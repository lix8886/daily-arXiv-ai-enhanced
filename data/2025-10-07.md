<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

> 通过分析Transformer模型中注意力分散且对内容依赖性弱的注意力头，研究提出了一种方法，在给定权重和少量校准文本的情况下，可以找到数百个对周围文本的高层次上下文属性敏感的第一层神经元。

<details>
  <summary>Details</summary>

**Motivation:** 理解注意力分散且对内容依赖性弱的注意力头的性质，探索在不依赖大量训练数据的情况下发现对文本高层次属性敏感的神经元。

**Method:** 从校准文本中采样softmax分母，结合多个稳定注意力头的输出，通过一个线性总结周围文本的过程来近似这些头的组合输出。

**Result:** 发现了许多第一层神经元，这些神经元对周围文本的高层次上下文属性敏感，即使它们在训练中未被激活。

**Conclusion:** 该方法可以通过少量信息（权重和一个校准文本）有效地发现模型中对高层次属性敏感的神经元，为理解 Transformer 模型提供了一种新方式。

**Abstract:** We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [2] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

> Introduces Graph-$S^3$ for improving LLM-based textual graph question answering by enhancing retriever performance.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of effective graph retrieval for LLM-based systems, moving beyond existing methods that use shallow embeddings or require excessive labeling and training.

**Method:** Graph-$S^3$, an agentic textual graph reasoning framework that uses an LLM-based retriever trained with synthetic stepwise supervision, evaluates each step based on offline-extracted golden subgraphs, and employs a two-stage training scheme to learn graph exploration policy.

**Result:** Achieves an average improvement of 8.1% in accuracy and 9.7% in F1 score over seven strong baselines on three common datasets, with a more significant advantage in complex multi-hop reasoning tasks.

**Conclusion:** Graph-$S^3$ demonstrates superior performance in extracting relevant graph content for LLMs, enhancing accuracy and F1 scores especially in complex multi-hop questions.

**Abstract:** A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [3] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

> 本研究通过比较六种流行的LLMs与100名人类在完成日常任务时的表现，发现LLMs的隐含价值观与其人类同行和其他LLMs均存在差异。

<details>
  <summary>Details</summary>

**Motivation:** 尽管AI助手有着巨大的潜力，但人们对于这些助手在完成主观日常任务时所展现的隐含价值观知之甚少。本研究旨在探讨大规模语言模型（LLMs）在完成日常任务时如何展现环境主义、慈善和多样性等价值观。

**Method:** 研究通过审计六种流行的LLMs在完成30项日常任务时的表现来回答这些问题，并将LLMs与来自美国的100名人类众包工作者进行比较。

**Result:** 研究发现LLMs在展示隐含价值观方面往往与人类不同，与其他LLMs之间也存在差异。

**Conclusion:** LLMs在完成日常任务时所展示的隐含价值观并不总与人类一致，并且不同的LLMs之间也有差异。

**Abstract:** Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [4] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

> CSAR算法通过结合形态和意义的互信息，从平行话语和意义的新兴语言语料库中有效地诱导出形态。

<details>
  <summary>Details</summary>

**Motivation:** 提出CSAR算法来从平行话语和意义的新兴语言语料库中诱导构成语素。

**Method:** CSAR算法通过计算形式和意义之间的互信息来加权构成语素，选择权重最高的配对，然后从语料库中移除，并重复此过程以诱导进一步的语素。

**Result:** 该算法首先在程序生成的数据集上验证其有效性，并与相关任务的基线进行比较。其次，在人类语言数据上验证其性能，表明该算法在相邻领域中做出了合理的预测。最后，通过分析几个新兴语言，量化了诸如同义程度和多义程度等语言特征。

**Conclusion:** CSAR算法在程序生成数据集和人类语言数据上都表现出了有效地诱导语素的能力，并且能够分析新兴语言，从而量化其语言特性，如同义和多义的程度。

**Abstract:** We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [5] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

> Omni-Embed-Nemotron is a unified multimodal retrieval embedding model that extends existing text-based retrieval systems to incorporate multimodal data types, including audio and video, demonstrating improved retrieval quality.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the limitations of existing retrieval systems which are primarily text-based and struggle with complex multimedia content. The goal is to improve the retrieval quality and capability to process unstructured and semantically rich data found in real-world documents such as PDFs, slides, and videos.

**Method:** The paper introduces Omni-Embed-Nemotron, a multimodal retrieval embedding model capable of handling various types of real-world information, including text, images, audio, and video. The model extends the capabilities of Retrieval-Augmented Generation (RAG) systems, which typically handle only textual data, by incorporating multimodal data using a unified approach.

**Result:** Omni-Embed-Nemotron is shown to be effective in text, image, and video retrieval. The results suggest an improvement in retrieval quality by incorporating multimodal data.

**Conclusion:** The paper concludes that Omni-Embed-Nemotron can effectively handle the increasing complexity of real-world information needs, providing a versatile solution for multimedia retrieval tasks.

**Abstract:** We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [6] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

> 研究设计了一种新兴语言生成方法，该方法能生成与人类语言相似的新兴语言。通过超参数优化和XferBench评估语言相似性，发现熵最小化有助于提高新兴语言的迁移学习性能，同时揭示了产生更真实新兴语言的超参数特性。

<details>
  <summary>Details</summary>

**Motivation:** 目的是生成类似于人类语言状态的新兴语言，并探索什么超参数能够产生更真实的新兴语言。

**Method:** 设计了一个基于信号博弈的新兴交流环境，利用超参数优化和XferBench作为目标函数来生成状态最先进的人类语言相似的新兴语言。XferBench通过测量新兴语言进行深度迁移学习到人类语言的适用性来量化其与人类语言的统计相似度。

**Result:** 验证了熵对新兴语言迁移学习性能的预测能力，并证实了关于新兴交流系统熵最小化的先前结果，报告了一般化发现，即哪些超参数能够生成更真实的新兴语言，这些语言能够更好地迁移到人类语言。

**Conclusion:** 研究表明，通过超参数优化可以生成与人类语言相似的新兴语言，并且熵的最小化有助于提高新兴语言的迁移学习性能。

**Abstract:** In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [7] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

> 介绍了SEER基准测试，用于评估语言模型识别表达情感的具体文本片段的能力，涵盖了单句及短文本段落两种任务，并通过误差分析揭示了模型的优势和不足。

<details>
  <summary>Details</summary>

**Motivation:** 针对传统情绪识别任务仅给整个句子打标签的局限，强调了准确识别情绪表达的具体词汇片段的重要性，对于共情对话和临床支持等应用场景尤为关键。

**Method:** Structure

**Result:** {
  "tldr": "介绍了SEER基准测试，用于评估语言模型识别表达情感的具体文本片段的能力，涵盖了单句及短文本段落两种任务，并通过误差分析揭示了模型的优势和不足。",
  "motivation": "针对传统情绪识别任务仅给整个句子打标签的局限，强调了准确识别情绪表达的具体词汇片段的重要性，对于共情对话和临床支持等应用场景尤为关键。",
  "method": "SEER包含两个任务：单句情绪证据识别和短段落情绪证据识别。该基准涵盖了1200条真实句子的新注释，对情绪和情绪证据进行了标注，并评估了14个开源语言模型的表现。",
  "result": "部分模型在单句输入上接近人类平均水平，但在更长的文本片段中准确性降低。错误分析揭示了模型过分依赖情绪相关词汇及在中性文本中的误报等问题。",
  "conclusion": "SEER为语言模型在情绪证据检测上的能力提供了新的评估视角，展示了模型的挑战和改进方向。")}
}


**Conclusion:** SEER为语言模型在情绪证据检测上的能力提供了新的评估视角，展示了模型的挑战和改进方向。

**Abstract:** We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [8] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

> ALHD数据集用于区分阿拉伯语的机器生成和人类生成的文本，研究发现微调的BERT模型性能突出，但在不同体裁（尤其是新闻）之间泛化能力存在挑战。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在建立一个能够有效区分人类和语言模型生成的文本的数据集，进而研究一般化的阿拉伯语言模型生成文本检测问题，并探索相关风险管控。

**Method:** 通过引入ALHD数据集，研究采用了传统的分类器、基于BERT的模型以及语言模型（零样本和少量样本）来进行基准实验，评估不同方法在检测阿拉伯语文本中生成人类文本和模型生成文本的能力。

**Result:** ALHD是一个专为区分人类和大型语言模型生成的阿拉伯文本而设计的首个大规模综合性数据集。它涵盖了三种体裁（新闻、社交媒体、评论），包括现代标准阿拉伯语和方言，并含有超过40万份由三个领先的语言模型生成的均衡样本，以及多个人类来源的文本。该数据集支持再现性研究，并对传统分类器、基于BERT的模型和零样本/少量样本的LLM进行了基准测试实验，得出基于BERT的微调模型能够达到较高的性能。实验表明在跨体裁泛化上仍存在挑战，尤其是在处理新闻文章时，因为语言模型生成的文本风格类似于人类文本。ALHD为阿拉伯语语言模型检测及其风险管控建立了研究基础。

**Conclusion:** 通过ALHD数据集的实验分析表明，基于BERT的微调模型在检测阿拉伯语文本中的人类和模型生成的文本时表现出色。但同时也指出模型泛化能力在跨体裁的文本分析中存在挑战，特别是在处理新闻类文本时。这些发现揭示了未来研究的方向。

**Abstract:** We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

> 研究提出了SoC-DT框架结合反应扩散模型和标准治疗方法，用于预测治疗后的肿瘤结构，该框架优于传统的PDE模型和纯数据驱动的方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的肿瘤反应-扩散模型因无法捕捉肿瘤在异质治疗下的动态变化而存在局限性，本研究旨在开发一个可以模拟真实世界中标准治疗干预，并考虑患者间基因组学、人口统计学和治疗方案差异的计算框架。

**Method:** SoC-DT框架结合反应扩散肿瘤生长模型、离散标准治疗干预、基因组和人口统计学的个性化信息，使用IMEX-SoC隐式-显式指数时间差分求解器来确保稳定性、正性和可扩展性。

**Result:** 该研究提出了一个名为标准治疗数字孪生（SoC-DT）的可微框架，结合了反应扩散肿瘤生长模型、离散标准治疗方法（如手术、化疗、放疗）以及基因组和人口统计学个性化元素，来预测治疗后的肿瘤结构。该框架使用隐式-显式指数时间差分求解器IMEX-SoC，确保了稳定性和扩展性。实验结果表明，SoC-DT在预测肿瘤动力学方面优于传统的偏微分方程基准和纯数据驱动的神经模型。这一研究建立了肿瘤数字孪生技术的原理性基础，有助于实现个性化肿瘤治疗方案的制定和疾病进展的预测。

**Conclusion:** SoC-DT以机制可解释性和现代可微分求解器相结合，为肿瘤学创建了一个个性化数字孪生的原理性基础，支持生物一致的肿瘤动态估计。

**Abstract:** Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [10] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

> 本文介绍一种新框架，结合多GPU分布式推理系统和互动可视化工具，用于分析视频中名人动态。

<details>
  <summary>Details</summary>

**Motivation:** 在以视频内容为主导的时代，了解视频内容的结构和动态变得越来越重要，因此提出此框架。

**Method:** 该论文提出了一种结合分布式多GPU推理系统和交互式可视化平台的混合框架，用于分析视频片段中名人动态。推理框架通过利用优化的ONNX模型、异构批处理推理和高吞吐量并行化，在视频数据处理中提高了效率，确保了带时间戳的出场记录的可扩展生成。然后，这些记录被转化为一系列详细的可视化，包括出场频率图表、持续时间分析、饼状图、共同出场矩阵、网络图形、堆叠面积图表、季节性比较以及热力图。

**Result:** 这些可视化提供了对视频内容的多维度洞察，可以揭示名人地位、屏幕时间分布、时间动态、共同出场关系以及跨剧集和赛季的活跃程度。系统的交互性质允许用户动态探索数据、识别关键时刻并揭示个人之间的关系演变。

**Conclusion:** 通过结合分布式识别与结构化、以视觉驱动的分析，此框架为娱乐分析、内容创作策略及观众互动研究开启了新的可能性。

**Abstract:** In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [11] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

> 研究评估了不同模型在跨域检测水下塑料污染的性能，发现轻量级的MobileNetV2表现最佳，同时指出预训练零样本模型有其独特的优缺点。

<details>
  <summary>Details</summary>

**Motivation:** 水下视觉系统的训练数据集转移导致性能下降，本研究旨在评估模型在跨域检测水下塑料污染的鲁棒性。

**Method:** 使用卷积神经网络（如MobileNetV2, ResNet-18, EfficientNet-B0）和视觉变换器（如DeiT-Tiny, ViT-B16）在标记的水下数据集上训练模型，并在一个来自不同数据源的均衡跨域测试集上进行评估，该测试集包括阳性塑料图像和来自训练域的阴性图像。同时评估了两个零样本模型CLIP ViT-L14和Google的Gemini 2.0 Flash。

**Result:** 轻量级的MobileNetV2在跨域性能（F1为0.97）方面表现最佳，优于较大模型。所有微调模型在精度方面表现高（约99%），但在召回率上有所差异。零样本模型CLIP具有较高的敏感性（召回率约80%），但容易产生假阳性（精度约56%）。Gemini则表现出相反的性能特点（精度约99%，召回率约81%）。错误分析表明，珊瑚纹理、悬浮颗粒和镜面眩光是常见混淆来源。

**Conclusion:** 紧凑的监督训练卷积神经网络能有效地跨域检测水下塑料污染，而大型预训练视觉-语言模型提供互补优势。

**Abstract:** Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [12] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

> 论文展示了VLCAP框架，使用多语言编码器提取视觉概念并通过多模态文本生成技术生成阿拉伯语图像字幕，在多种配置下实现了较高的BLEU和相似性评分。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在通过结合CLIP基础的视觉标签检索和多模态文本生成技术，开发一种更准确、文化和情境相关的阿拉伯语图像字幕生成框架。

**Method:** 本篇论文提出了VLCAP框架，结合CLIP基础的视觉标签检索和多模态文本生成技术，用来生成阿拉伯语图像字幕。该框架使用了三种多语言编码器：mCLIP、AraCLIP、Jina V4，分别用于标签检索。同时，构建了一个混合词汇表，并使用Visual Genome数据集中的标签进行扩充。在第二阶段，使用了Qwen-VL和Gemini Pro Vision两种模型进行字幕生成，共得到六种编码-解码配置。

**Result:** 实验结果表明，mCLIP + Gemini Pro Vision在BLEU-1（5.34%）和余弦相似性（60.01%）上取得了最佳性能，而AraCLIP + Qwen-VL在LLM-judge评分上取得了最高值（36.33%）。

**Conclusion:** 该研究的可解释性框架使得生成的文化一致且情境准确的阿拉伯语图像字幕成为可能。

**Abstract:** We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>
