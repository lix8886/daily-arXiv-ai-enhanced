{"id": "2507.21069", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.21069", "abs": "https://arxiv.org/abs/2507.21069", "authors": ["Andreas Spilz", "Heiko Oppel", "Jochen Werner", "Kathrin Stucke-Straub", "Felix Capanni", "Michael Munz"], "title": "GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data", "comment": null, "summary": "Wearable inertial measurement units (IMUs) offer a cost-effective and\nscalable means to assess human movement quality in clinical and everyday\nsettings. However, the development of robust sensor-based classification models\nfor physiotherapeutic exercises and gait analysis requires large, diverse\ndatasets, which are costly and time-consuming to collect. Here, we present a\nmultimodal dataset of physiotherapeutic exercises - including correct and\nclinically relevant variants - and gait-related exercises - including both\nnormal and impaired gait patterns - recorded from 19 participants using\nsynchronized IMUs and marker-based motion capture (MoCap). The dataset includes\nraw data from nine IMUs and thirty-five optical markers capturing full-body\nkinematics. Each IMU is additionally equipped with four optical markers,\nenabling precise comparison between IMU-derived orientation estimates and\nreference values from the MoCap system. To support further analysis, we also\nprovide processed IMU orientations aligned with common segment coordinate\nsystems, subject-specific OpenSim models, inverse kinematics results, and tools\nfor visualizing IMU orientations in the musculoskeletal context. Detailed\nannotations of movement execution quality and time-stamped segmentations\nsupport diverse analysis goals. This dataset supports the development and\nbenchmarking of machine learning models for tasks such as automatic exercise\nevaluation, gait analysis, temporal activity segmentation, and biomechanical\nparameter estimation. To facilitate reproducibility, we provide code for\npostprocessing, sensor-to-segment alignment, inverse kinematics computation,\nand technical validation. This resource is intended to accelerate research in\nmachine learning-driven human movement analysis.", "AI": {"tldr": "文章介绍了一个多模态数据集，结合了IMU和运动捕捉数据，用于支持机器学习模型在人体运动分析中的开发和基准测试，加速了该领域研究进程。", "motivation": "由于物理治疗练习和步态分析中的传感器分类模型的开发需要大规模且多样化的数据集，这既耗时又昂贵。为了应对这一挑战，本研究旨在提供一个用于训练和测试机器学习模型的数据集，支持自动化运动评估、步态分析、时间活动分割和生物力学参数估计等任务。", "method": "本研究建立了一个多模态数据集，记录了来自19名参与者的物理治疗练习和步态相关练习的正确和相关临床变体以及正常和受损步态模式。数据集包括来自九个惯性测量单元(IMU)和35个光学标记捕捉全身运动的数据，可以与基于标记的运动捕捉系统进行精确比较。此外，还提供了处理后的IMU方位数据、特定于主题的OpenSim模型、逆运动学结果以及用于在肌肉骨骼环境中可视化IMU方位的工具。", "result": "创建了一个包含原始IMU数据和运动捕捉(MoCap)数据的数据集，用于支持机器学习模型的开发，该模型能够分析和评估人体运动。", "conclusion": "这个资源加速了机器学习驱动的人体运动分析的研究，提供了处理后IMU方位、逆运动学结果等数据，支持研究者进行各种分析目标。"}}
{"id": "2507.21161", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21161", "abs": "https://arxiv.org/abs/2507.21161", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Ying Liu"], "title": "Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues", "comment": "Accepted in IEEE 3rd International Conference on Artificial\n  Intelligence, Blockchain, and Internet of Things (AIBThings 2025)", "summary": "Pedestrian intention prediction is essential for autonomous driving in\ncomplex urban environments. Conventional approaches depend on supervised\nlearning over frame sequences and require extensive retraining to adapt to new\nscenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention\nPrediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing\nintentions directly from short, continuous video clips enriched with structured\nJAAD metadata. In contrast to GPT-4V based methods that operate on discrete\nframes, BF-PIP processes uninterrupted temporal clips. It also incorporates\nbounding-box annotations and ego-vehicle speed via specialized multimodal\nprompts. Without any additional training, BF-PIP achieves 73% prediction\naccuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate\nthat combining temporal video inputs with contextual cues enhances\nspatiotemporal perception and improves intent inference under ambiguous\nconditions. This approach paves the way for agile, retraining-free perception\nmodule in intelligent transportation system.", "AI": {"tldr": "BF-PIP是一种零样本方法，可以从简短的连续视频片段中直接推断出行人的过街意图，并且在没有额外训练的情况下，其预测准确率比GPT-4V基线高出了18%。", "motivation": "鉴于传统的监督学习方法需要大量的再训练来适应新的驾驶场景，在复杂城市环境中进行行人意图预测显得尤为重要。", "method": "这种方法称为BF-PIP（超越帧的行人意图预测），它基于Gemini 2.5 Pro，可以从简短的连续视频片段中直接推断出过街意图，这些视频片段还结合了JAAD元数据。BF-PIP通过专门的多模态提示结合了边界框注释和自车速度信息，与基于GPT-4V的方法相比，它处理的是不间断的视频片段。", "result": "BF-PIP无需额外训练就能达到73%的预测准确率，比GPT-4V基线方法高出18%。", "conclusion": "将时间视频输入与上下文线索相结合，可以提高空间和时间的感知能力，并在模糊条件下改善意图推断。这种方法为智能运输系统中的敏捷且无需再训练的感知模块铺平了道路。"}}
{"id": "2507.21167", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21167", "abs": "https://arxiv.org/abs/2507.21167", "authors": ["Danglu Yang", "Liang Zhang", "Zihao Yue", "Liangyu Chen", "Yichen Xu", "Wenxuan Wang", "Qin Jin"], "title": "ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions", "comment": null, "summary": "Charts are a fundamental visualization format widely used in data analysis\nacross research and industry. While enabling users to edit charts based on\nhigh-level intentions is of great practical value, existing methods primarily\nrely on natural language instructions, which are often too ambiguous to support\nfine-grained editing. In this work, we introduce a novel paradigm for\nmultimodal chart editing, where user intent is expressed through a combination\nof natural language and visual indicators that explicitly highlight the\nelements to be modified. To support this paradigm, we present\nChart$\\text{M}^3$, a new benchmark for Multimodal chart editing with\nMulti-level complexity and Multi-perspective evaluation. Chart$\\text{M}^3$\ncontains 1,000 samples spanning four levels of editing difficulty. Each sample\nincludes triplets in the form of (chart, code, multimodal instructions). To\ncomprehensively evaluate chart editing models, Chart$\\text{M}^3$ provides\nmetrics that assess both visual appearance and code correctness. Our benchmark\nreveals significant limitations in current multimodal large language models\n(MLLMs), including GPT-4o, particularly in their ability to interpret and act\non visual indicators. To address this, we construct Chart$\\text{M}^3$-Train, a\nlarge-scale training set with 24,000 multimodal chart editing samples.\nFine-tuning MLLMs on this dataset leads to substantial improvements,\ndemonstrating the importance of multimodal supervision in building practical\nchart editing systems. Our datasets, codes, and evaluation tools are available\nat https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our\ndatasets, codes, and evaluation tools are available at\nhttps://github.com/yaolinli/VCE.", "AI": {"tldr": "本文介绍了一种新的表格编辑方法，通过结合自然语言和视觉指示符来更准确地编辑表格，并提出一个新的多模态表格编辑基准Chart$\\text{M}^3$。", "motivation": "由于现有方法主要依赖自然语言指令，这往往过于模糊，不能支持细粒度编辑。而新的多模态范式通过结合自然语言和视觉指示，有望更准确地进行表格编辑。", "method": "本文提出了一种新的多模态表格编辑范式，用户意图通过自然语言和视觉指示符的结合来明确表达。为支持该范式，提出了Chart$\\text{M}^3$，这是一个具有多级难度和多角度评估的多模态表格编辑的新基准，包含1,000个样本。", "result": "Chart$\\text{M}^3$的评估揭示了现有大规模语言模型在解释和应对视觉指示方面存在显著限制。通过在大规模训练集Chart$\\text{M}^3$-Train上进行微调，可以显著改进这些模型的表现。", "conclusion": "本研究表明，通过提供具有视觉指示的多模态监督，可以在构建实用的表格编辑系统方面取得重要进展。"}}
{"id": "2507.21200", "categories": ["cs.CV", "cs.ET", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.21200", "abs": "https://arxiv.org/abs/2507.21200", "authors": ["Soren Pedersen", "Sanyam Jain", "Mikkel Chavez", "Viktor Ladehoff", "Bruna Neves de Freitas", "Ruben Pauwels"], "title": "PanoGAN A Deep Generative Model for Panoramic Dental Radiographs", "comment": null, "summary": "This paper presents the development of a generative adversarial network (GAN)\nfor synthesizing dental panoramic radiographs. Although exploratory in nature,\nthe study aims to address the scarcity of data in dental research and\neducation. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss\nwith gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying\nquality. The focus was on the dentoalveolar regions, other anatomical\nstructures were cropped out. Extensive preprocessing and data cleaning were\nperformed to standardize the inputs while preserving anatomical variability. We\nexplored four candidate models by varying critic iterations, feature depth, and\nthe use of denoising prior to training. A clinical expert evaluated the\ngenerated radiographs based on anatomical visibility and realism, using a\n5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical\ndepiction, although some were degraded by artifacts. A trade-off was observed\nthe model trained on non-denoised data yielded finer details especially in\nstructures like the mandibular canal and trabecular bone, while a model trained\non denoised data offered superior overall image clarity and sharpness. These\nfindings provide a foundation for future work on GAN-based methods in dental\nimaging.", "AI": {"tldr": "本研究开发了一种用于合成牙全景X光片的生成对抗网络（GAN），通过探索不同的模型配置，取得了在图像清晰度与细节展示间的权衡，并为未来的牙科成像研究提供了基础。", "motivation": "本研究旨在解决牙科研究和教育中数据稀缺的问题。通过开发一种生成牙全景X光片的生成对抗网络（GAN），希望能够增加可供使用的数据量，从而促进相关领域的研究和教育。", "method": "本研究使用深度卷积生成对抗网络（DCGAN）结合Wasserstein损失函数和梯度惩罚（WGANGP），基于一个包含2322张牙片的数据库进行训练，这些牙片质量不一。重点放在了牙槽区，其他解剖结构被裁剪。进行了广泛的预处理和数据清洗以标准化输入，同时保留了解剖变异。探索了四个候选模型，通过调整判别器迭代次数、特征深度，以及在训练前是否采用去噪技术。", "result": "临床专家对生成的牙片从解剖可见性和真实性两个方面进行了5级评分评估（1表示非常差，5表示优秀）。大多数图像在解剖描绘上表现中等，但有一些图像受到伪影的影响。观察到模型之间的权衡关系：未使用去噪数据训练的模型在像下颌管和板障骨等结构上提供更精细的细节，而使用去噪数据训练的模型则在总体图像清晰度和锐度上表现更优。", "conclusion": "本研究为基于GAN的方法在牙科成像中的未来发展奠定了基础。通过比较不同设定下训练出的模型，研究揭示了模型在细节展示和图像清晰度之间的权衡。"}}
{"id": "2507.21058", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21058", "abs": "https://arxiv.org/abs/2507.21058", "authors": ["Kerem Keskin", "Mümine Kaya Keleş"], "title": "Categorical Classification of Book Summaries Using Word Embedding Techniques", "comment": "in Turkish language. This paper was published in the proceedings of\n  the 6th International Conference on Data Science and Applications ICONDATA24,\n  held on September between 2 and 6, 2024, in Pristina, Kosovo. For full text\n  book see https://www.icondata.org/en/proceedings-books", "summary": "In this study, book summaries and categories taken from book sites were\nclassified using word embedding methods, natural language processing techniques\nand machine learning algorithms. In addition, one hot encoding, Word2Vec and\nTerm Frequency - Inverse Document Frequency (TF-IDF) methods, which are\nfrequently used word embedding methods were used in this study and their\nsuccess was compared. Additionally, the combination table of the pre-processing\nmethods used is shown and added to the table. Looking at the results, it was\nobserved that Support Vector Machine, Naive Bayes and Logistic Regression\nModels and TF-IDF and One-Hot Encoder word embedding techniques gave more\nsuccessful results for Turkish texts.", "AI": {"tldr": "研究使用了词嵌入技术，自然语言处理技术和机器学习算法对图书摘要和分类进行了分类，并发现对于土耳其文本，SVM、朴素贝叶斯、逻辑回归模型和TF-IDF、One-Hot编码技术效果更好。", "motivation": "研究动机在于利用词嵌入技术和机器学习算法对图书摘要及类别进行分类，并比较不同词嵌入方法的成功率。", "method": "本研究采用了词嵌入方法（包括One-Hot编码，Word2Vec和TF-IDF），自然语言处理技术和机器学习算法对来自图书网站的图书摘要和类别进行了分类。", "result": "结果表明，支持向量机、朴素贝叶斯和逻辑回归模型以及TF-IDF和One-Hot编码词嵌入技术在土耳其文本上表现更佳。", "conclusion": "通过结果，可以看出对于土耳其文本的支持向量机、朴素贝叶斯和逻辑回归模型，以及TF-IDF和One-Hot编码词嵌入技术表现更佳。"}}
{"id": "2507.21246", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21246", "abs": "https://arxiv.org/abs/2507.21246", "authors": ["Monika Shah", "Somdeb Sarkhel", "Deepak Venugopal"], "title": "On Explaining Visual Captioning with Hybrid Markov Logic Networks", "comment": null, "summary": "Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks\nsuch as image captioning. However, explaining/interpreting how these models\nintegrate visual information, language information and knowledge representation\nto generate meaningful captions remains a challenging problem. Standard metrics\nto measure performance typically rely on comparing generated captions with\nhuman-written ones that may not provide a user with a deep insights into this\nintegration. In this work, we develop a novel explanation framework that is\neasily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language\nthat can combine symbolic rules with real-valued functions - where we\nhypothesize how relevant examples from the training data could have influenced\nthe generation of the observed caption. To do this, we learn a HMLN\ndistribution over the training instances and infer the shift in distributions\nover these instances when we condition on the generated sample which allows us\nto quantify which examples may have been a source of richer information to\ngenerate the observed caption. Our experiments on captions generated for\nseveral state-of-the-art captioning models using Amazon Mechanical Turk\nillustrate the interpretability of our explanations, and allow us to compare\nthese models along the dimension of explainability.", "AI": {"tldr": "本文开发了一种基于HMLNs的解释框架来说明训练数据中的示例如何影响生成的描述，从而增强了模型的解释能力。", "motivation": "DNN在多模态任务上的进展使得解释模型如何整合视觉信息、语言信息和知识表示来生成描述成为一个具有挑战性的问题，现有的度量标准无法提供深入的见解，因此需要开发一种可解释的解释框架。", "method": "本文提出了一种新的基于混合马尔可夫逻辑网络（HMLNs）的解释框架，以解释训练数据中的相关示例如何影响模型生成的描述。通过学习训练实例上的HMLN分布，并计算在条件生成样本下分布的变化，可以量化哪些示例可能是生成观测描述的更丰富的信息源。", "result": "实验表明该解释框架能够解释训练数据中哪些示例影响了描述的生成，并展示了模型生成的描述对于人类判读者的可解释性。", "conclusion": "通过Amazon Mechanical Turk进行的实验展示了所提解释框架的可解释性，并能够比较不同模型在可解释性方面的表现。"}}
{"id": "2507.21065", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.RO", "I.2.7, I.2.9, j.4,"], "pdf": "https://arxiv.org/pdf/2507.21065", "abs": "https://arxiv.org/abs/2507.21065", "authors": ["Sabrina Patania", "Luca Annese", "Cansu Koyuturk", "Azzurra Ruggeri", "Dimitri Ognibene"], "title": "Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions", "comment": "submitted to ICSR2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing extensive offline datasets. However, they often face challenges in\nacquiring and integrating complex, knowledge online. Traditional AI training\nparadigms, predominantly based on supervised learning or reinforcement\nlearning, mirror a 'Piagetian' model of independent exploration. These\napproaches typically rely on large datasets and sparse feedback signals,\nlimiting the models' ability to learn efficiently from interactions. Drawing\ninspiration from Vygotsky's sociocultural theory, this study explores the\npotential of socially mediated learning paradigms to address these limitations.\n  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI\nlearner agent engages in dyadic pedagogical dialogues with knowledgeable AI\nteacher agents. These interactions emphasize external, structured dialogue as a\ncore mechanism for knowledge acquisition, contrasting with methods that depend\nsolely on internal inference or pattern recognition.\n  Our investigation focuses on how different pedagogical strategies impact the\nAI learning process in the context of ontology acquisition. Empirical results\nindicate that such dialogic approaches-particularly those involving\nmixed-direction interactions combining top-down explanations with\nlearner-initiated questioning-significantly enhance the LLM's ability to\nacquire and apply new knowledge, outperforming both unidirectional\ninstructional methods and direct access to structured knowledge, formats\ntypically present in training datasets.\n  These findings suggest that integrating pedagogical and psychological\ninsights into AI and robot training can substantially improve post-training\nknowledge acquisition and response quality. This approach offers a\ncomplementary pathway to existing strategies like prompt engineering", "AI": {"tldr": "研究开发了一个动态环境，通过社会化的教学对话彰显了对大语言模型的学习改进，通过对社会中介学习范式的探讨，以期提高其获取知识的能力。", "motivation": "传统的AI训练范式，主要基于监督学习或强化学习，成为类似于'皮亚杰式的'独立探索模型。这些方法通常依赖大型数据集和稀疏反馈信号，限制了模型利用交互有效学习的能力。该研究旨在探索基于维果茨基的社会文化理论的中介社会化学习范式解决这些局限的潜力。", "method": "本研究开发了一个名为 'AI 社交竞技场' 的动态环境，其中AI学习代理与知识丰富的AI教师代理进行二元教学对话。这些互动强调以结构化的对话为核心机制来获取知识，而不是仅仅依赖内部推理或模式识别。研究关注了不同的教学策略对AI在本体论知识获取过程中学习的影响。", "result": "实证结果显示，这种对话方法，特别是结合自上而下的解释与学习者发起的问题以混合方向交互的方式，显著增强了大语言模型获取和应用新知识的能力，超越了单向教学方法和直接访问结构化知识的方法。", "conclusion": "这些发现表明，将教育学和心理学见解融入AI和机器人培训中可以显著提高训练后的知识获取和响应质量。此方法为诸如提示工程等现有策略提供了一条补充路线。"}}
{"id": "2507.21247", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21247", "abs": "https://arxiv.org/abs/2507.21247", "authors": ["Ankit Singh", "Efstratios Gavves", "Cees G. M. Snoek", "Hilde Kuehne"], "title": "Dual Guidance Semi-Supervised Action Detection", "comment": null, "summary": "Semi-Supervised Learning (SSL) has shown tremendous potential to improve the\npredictive performance of deep learning models when annotations are hard to\nobtain. However, the application of SSL has so far been mainly studied in the\ncontext of image classification. In this work, we present a semi-supervised\napproach for spatial-temporal action localization. We introduce a dual guidance\nnetwork to select better pseudo-bounding boxes. It combines a frame-level\nclassification with a bounding-box prediction to enforce action class\nconsistency across frames and boxes. Our evaluation across well-known\nspatial-temporal action localization datasets, namely UCF101-24 , J-HMDB-21 and\nAVA shows that the proposed module considerably enhances the model's\nperformance in limited labeled data settings. Our framework achieves superior\nresults compared to extended image-based semi-supervised baselines.", "AI": {"tldr": "本研究提出了一种新的半监督学习方法，用于时空动作定位，显著提高了有限标注数据情况下的模型性能。", "motivation": "虽然半监督学习已在图像分类中得到了广泛研究，但对于时空动作定位的应用研究较少。鉴于标注数据难以获取时，半监督学习具有改善深度学习模型预测性能的巨大潜力，本研究旨在填补这一空白。", "method": "提出了一种用于时空动作定位的半监督方法，引入了一个双指导网络来选择更好的伪边界框。该方法结合了帧级分类和边界框预测，以确保动作类别在帧和边界框之间的一致性。", "result": "实验结果表明，所提出的方法在UCF101-24、J-HMDB-21和AVA数据集上优于扩展的基于图像的半监督基线。", "conclusion": "研究结果表明，结合帧级分类和边界框预测的双指导网络对于提升时空动作定位任务性能是有效的。"}}
{"id": "2507.21073", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.21073", "abs": "https://arxiv.org/abs/2507.21073", "authors": ["David James Woo", "Yangyang Yu", "Kai Guo", "Yilin Huang", "April Ka Yeng Fung"], "title": "Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing", "comment": "45 pages, 11 figures", "summary": "Text generated by artificial intelligence (AI) chatbots is increasingly used\nin English as a foreign language (EFL) writing contexts, yet its impact on\nstudents' expository writing process and compositions remains understudied.\nThis research examines how EFL secondary students edit AI-generated text.\nExploring editing behaviors in their expository writing process and in\nexpository compositions, and their effect on human-rated scores for content,\norganization, language, and overall quality. Participants were 39 Hong Kong\nsecondary students who wrote an expository composition with AI chatbots in a\nworkshop. A convergent design was employed to analyze their screen recordings\nand compositions to examine students' editing behaviors and writing qualities.\nAnalytical methods included qualitative coding, descriptive statistics,\ntemporal sequence analysis, human-rated scoring, and multiple linear regression\nanalysis. We analyzed over 260 edits per dataset, and identified two editing\npatterns: one where students refined introductory units repeatedly before\nprogressing, and another where they quickly shifted to extensive edits in body\nunits (e.g., topic and supporting sentences). MLR analyses revealed that the\nnumber of AI-generated words positively predicted all score dimensions, while\nmost editing variables showed minimal impact. These results suggest a\ndisconnect between students' significant editing effort and improved\ncomposition quality, indicating AI supports but does not replace writing\nskills. The findings highlight the importance of genre-specific instruction and\nprocess-focused writing before AI integration. Educators should also develop\nassessments valuing both process and product to encourage critical engagement\nwith AI text.", "AI": {"tldr": "The study investigates the impact of AI text editing by EFL students on composition quality.", "motivation": "The study aims to explore how EFL secondary students edit AI-generated text and the impact of these editing behaviors on the quality of their expository compositions.", "method": "This research employs a convergent design to analyze screen recordings and compositions of EFL secondary students editing AI-generated text, utilizing qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis.", "result": "MLR analyses indicate a positive correlation between the number of AI-generated words and all score dimensions, while most editing variables show minimal impact on composition quality.", "conclusion": "The findings suggest a disconnect between students' editing efforts and improvements in composition quality, indicating AI supports but does not replace writing skills. It emphasizes the need for genre-specific instruction and process-focused writing before integrating AI, alongside developing assessments that value both process and product."}}
{"id": "2507.21256", "categories": ["cs.CV", "I.4.8"], "pdf": "https://arxiv.org/pdf/2507.21256", "abs": "https://arxiv.org/abs/2507.21256", "authors": ["Christopher Indris", "Raiyan Rahman", "Goetz Bramesfeld", "Guanghui Wang"], "title": "Tracking Moose using Aerial Object Detection", "comment": "18 pages, 6 figures, 8 tables", "summary": "Aerial wildlife tracking is critical for conservation efforts and relies on\ndetecting small objects on the ground below the aircraft. It presents technical\nchallenges: crewed aircraft are expensive, risky and disruptive; autonomous\ndrones have limited computational capacity for onboard AI systems. Since the\nobjects of interest may appear only a few pixels wide, small object detection\nis an inherently challenging computer vision subfield compounded by\ncomputational efficiency needs. This paper applies a patching augmentation to\ndatasets to study model performance under various settings. A comparative study\nof three common yet architecturally diverse object detectors is conducted using\nthe data, varying the patching method's hyperparameters against detection\naccuracy. Each model achieved at least 93\\% mAP@IoU=0.5 on at least one\npatching configuration. Statistical analyses provide an in-depth commentary on\nthe effects of various factors. Analysis also shows that faster, simpler models\nare about as effective as models that require more computational power for this\ntask and perform well given limited patch scales, encouraging UAV deployment.\nDatasets and models will be made available via\nhttps://github.com/chrisindris/Moose.", "AI": {"tldr": "研究应用patching数据增强并比较了三种不同架构的目标检测器性能，发现加速和简化模型在有限规模的patch下对于无人机的用途也是有效的，这使得无人机的应用更具前景。", "motivation": "鉴于传统的载人飞机成本高、风险大且干扰性强，而自主无人机的计算能力有限，本研究旨在通过计算机视觉的方法提高小型动物的检测精度，特别是那些在图像中仅占据几个像素宽的对象。", "method": "本研究通过应用patching增强技术到数据集中，比较了三种架构多样化的常见目标检测器在不同设置下的表现。研究调整了patching方法的超参数以获得最佳检测精度。", "result": "每个模型至少在一种patching配置下达到了93%的mAP@IoU=0.5。统计分析提供了各种因素影响的深入评论，表明加速和简化的模型在这个任务中与计算能力要求更高的模型一样有效。", "conclusion": "研究表明，在空中野生动物追踪任务中，加速和简化的模型也能够达到良好的检测效果，即使在有限的补丁尺度下亦是如此，从而鼓励无人机应用。数据集和模型将在GitHub上公开。"}}
{"id": "2507.21080", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21080", "abs": "https://arxiv.org/abs/2507.21080", "authors": ["Vincent C. Müller"], "title": "Which symbol grounding problem should we try to solve?", "comment": null, "summary": "Floridi and Taddeo propose a condition of \"zero semantic commitment\" for\nsolutions to the grounding problem, and a solution to it. I argue briefly that\ntheir condition cannot be fulfilled, not even by their own solution. After a\nlook at Luc Steels' very different competing suggestion, I suggest that we need\nto re-think what the problem is and what role the 'goals' in a system play in\nformulating the problem. On the basis of a proper understanding of computing, I\ncome to the conclusion that the only sensible grounding problem is how we can\nexplain and re-produce the behavioral ability and function of meaning in\nartificial computational agents", "AI": {"tldr": "文章批评了Floridi和Taddeo提出的“零语义承诺”条件，并提出需要重新定义接地问题，以解决人工计算代理的行为能力和意义功能的解释与再现问题。", "motivation": "动机在于评估Floridi和Taddeo提出的“零语义承诺”条件和解决方案的有效性，并探讨不同的方法论以解决接地问题。", "method": "本文通过分析Floridi和Taddeo提出的“零语义承诺”条件及其解决方案，指出该条件无法实现，并进一步探讨了Luc Steels的不同建议。通过对计算的恰当理解，作者重新审视目标在系统中所起的作用以及如何定义接地问题。", "result": "作者认为唯一合理的接地问题是，如何解释和再现人工计算代理中的行为能力和意义功能。", "conclusion": "作者得出结论，需要重新思考接地问题的定义以及系统中“目标”的作用，以寻求解决此问题的有效方法。"}}
{"id": "2507.21261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21261", "abs": "https://arxiv.org/abs/2507.21261", "authors": ["Jack Hilliard", "Adrian Hilton", "Jean-Yves Guillemaut"], "title": "HDR Environment Map Estimation with Latent Diffusion Models", "comment": null, "summary": "We advance the field of HDR environment map estimation from a single-view\nimage by establishing a novel approach leveraging the Latent Diffusion Model\n(LDM) to produce high-quality environment maps that can plausibly light\nmirror-reflective surfaces. A common issue when using the ERP representation,\nthe format used by the vast majority of approaches, is distortions at the poles\nand a seam at the sides of the environment map. We remove the border seam\nartefact by proposing an ERP convolutional padding in the latent autoencoder.\nAdditionally, we investigate whether adapting the diffusion network\narchitecture to the ERP format can improve the quality and accuracy of the\nestimated environment map by proposing a panoramically-adapted Diffusion\nTransformer architecture. Our proposed PanoDiT network reduces ERP distortions\nand artefacts, but at the cost of image quality and plausibility. We evaluate\nwith standard benchmarks to demonstrate that our models estimate high-quality\nenvironment maps that perform competitively with state-of-the-art approaches in\nboth image quality and lighting accuracy.", "AI": {"tldr": "本文介绍了一种基于LDM的新方法，用于从单张图像中生成HDR环境贴图，解决了ERP格式中的边界接缝问题，并提出了适应ERP格式的扩散变换器PanoDiT。实验结果表明方法的有效性，但仍存在图像质量损失问题。", "motivation": "动机是改进从单视图图像中生成HDR环境贴图的算法，以减少ERP格式的极点失真和边界接缝问题，同时提升环境贴图的质量和准确性。", "method": "本文提出了一种新的方法，利用潜在扩散模型（LDM）从单视图图像中生成高质量的HDR环境贴图，以真实地照亮镜面反射表面。为了解决ERP表示格式中的极点失真和边界接缝问题，提出了一种ERP卷积填充方法，并设计了一种全景适应的扩散变换器架构PanoDiT。", "result": "PanoDiT网络在减少ERP失真和伪影方面表现出色，但图像质量和可信度有所降低。实验结果显示，所提模型在图像质量和光照准确性方面能够与现有最优方法竞争。", "conclusion": "研究表明，通过适应ERP格式的扩散变换器架构，可以在一定程度上改善环境贴图的失真问题，但会对图像质量和环境的真实性造成一定程度的影响。"}}
{"id": "2507.21083", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21083", "abs": "https://arxiv.org/abs/2507.21083", "authors": ["Franck Bardol"], "title": "ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs", "comment": null, "summary": "Large Language Models like GPT-4 adjust their responses not only based on the\nquestion asked, but also on how it is emotionally phrased. We systematically\nvary the emotional tone of 156 prompts - spanning controversial and everyday\ntopics - and analyze how it affects model responses. Our findings show that\nGPT-4 is three times less likely to respond negatively to a negatively framed\nquestion than to a neutral one. This suggests a \"rebound\" bias where the model\novercorrects, often shifting toward neutrality or positivity. On sensitive\ntopics (e.g., justice or politics), this effect is even more pronounced:\ntone-based variation is suppressed, suggesting an alignment override. We\nintroduce concepts like the \"tone floor\" - a lower bound in response negativity\n- and use tone-valence transition matrices to quantify behavior. Visualizations\nbased on 1536-dimensional embeddings confirm semantic drift based on tone. Our\nwork highlights an underexplored class of biases driven by emotional framing in\nprompts, with implications for AI alignment and trust. Code and data are\navailable at: https://github.com/bardolfranck/llm-responses-viewer", "AI": {"tldr": "研究分析了GPT-4对情感调整问题的响应方式，发现GPT-4在面对负面问题时更倾向于采取中立或积极态度，特别是在敏感话题上，这表明模型存在一种情绪过度矫正现象。", "motivation": "研究大型语言模型如GPT-4如何根据问题的情感语气调整其回应，探索其情感处理和响应模式的偏差。", "method": "通过系统地改变156个提示语的情绪语气，探讨了GPT-4对不同情感调节问题的回答方式。这些提示语涵盖了有争议的主题和日常话题。", "result": "GPT-4对情绪负面的提问比对中性提问更少提供负面回答，表现出所谓的“反弹”偏差，并在使用语调-价值转换矩阵和基于1536维嵌入的可视化来量化行为时得到了证实。", "conclusion": "揭示了情感提示所驱动的一种未被充分探索的偏差，这类偏差对于AI的对齐和信任有着重要的意义。"}}
{"id": "2507.21291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21291", "abs": "https://arxiv.org/abs/2507.21291", "authors": ["Théo Sourget", "David Restrepo", "Céline Hudelot", "Enzo Ferrante", "Stergios Christodoulidis", "Maria Vakalopoulou"], "title": "Fairness and Robustness of CLIP-Based Models for Chest X-rays", "comment": "Accepted for publication at the FAIMI MICCAI workshop 2025", "summary": "Motivated by the strong performance of CLIP-based models in natural\nimage-text domains, recent efforts have adapted these architectures to medical\ntasks, particularly in radiology, where large paired datasets of images and\nreports, such as chest X-rays, are available. While these models have shown\nencouraging results in terms of accuracy and discriminative performance, their\nfairness and robustness in the different clinical tasks remain largely\nunderexplored. In this study, we extensively evaluate six widely used\nCLIP-based models on chest X-ray classification using three publicly available\ndatasets: MIMIC-CXR, NIH-CXR14, and NEATX. We assess the models fairness across\nsix conditions and patient subgroups based on age, sex, and race. Additionally,\nwe assess the robustness to shortcut learning by evaluating performance on\npneumothorax cases with and without chest drains. Our results indicate\nperformance gaps between patients of different ages, but more equitable results\nfor the other attributes. Moreover, all models exhibit lower performance on\nimages without chest drains, suggesting reliance on spurious correlations. We\nfurther complement the performance analysis with a study of the embeddings\ngenerated by the models. While the sensitive attributes could be classified\nfrom the embeddings, we do not see such patterns using PCA, showing the\nlimitations of these visualisation techniques when assessing models. Our code\nis available at https://github.com/TheoSourget/clip_cxr_fairness", "AI": {"tldr": "本研究评估了基于CLIP的模型在胸部X光图像分类中的公平性和鲁棒性，发现存在年龄差异和依赖虚假相关性的问题。", "motivation": "由于CLIP-based模型在自然图像-文本领域表现强劲，近期的一些研究将这些架构应用于医疗任务，尤其是在放射学方面，因为拥有大量成对的图像和报告数据集，比如胸部X线摄影。然而，这些模型在不同临床任务中的公平性和鲁棒性尚未被充分探索。", "method": "本研究通过使用六个广泛使用的基于CLIP的模型对胸部X光图像进行分类来评估模型的公平性和鲁棒性。评估使用了三个公开数据集：MIMIC-CXR、NIH-CXR14 和 NEATX，考察了包括年龄、性别和种族在内的六个条件和亚群的公平性。同时，通过评估胸部插管存在与否的气胸病例来检验模型对捷径学习的鲁棒性。", "result": "结果显示，不同年龄的患者之间存在性能差距，但对于其他属性则更具有公平性。所有模型在没有胸部插管的图像上的性能较低，表明它们依赖于虚假的相关性。尽管可以基于模型生成的嵌入来分类敏感属性，但在使用PCA进行评估时没有看到这些模式，显示出这些可视化技术在评估模型方面的局限性。", "conclusion": "研究表明，六个被广泛使用的基于CLIP的模型在胸部X光图像分类中表现出一定的公平性和鲁棒性问题，尤其是关于年龄差异和对虚相关性的依赖。强调需要进一步研究和改进这些基于CLIP的模型以更好的适应医疗任务。"}}
{"id": "2507.21084", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21084", "abs": "https://arxiv.org/abs/2507.21084", "authors": ["Aly M. Kassem", "Zhuan Shi", "Negar Rostamzadeh", "Golnoosh Farnadi"], "title": "Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing", "comment": null, "summary": "Large language models (LLMs) are frequently fine-tuned or unlearned to adapt\nto new tasks or eliminate undesirable behaviors. While existing evaluation\nmethods assess performance after such interventions, there remains no general\napproach for detecting unintended side effects, such as unlearning biology\ncontent degrading performance on chemistry tasks, particularly when these\neffects are unpredictable or emergent. To address this issue, we introduce\nMNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight\nframework for identifying these side effects using sparse model diffing. MNEME\ncompares base and fine-tuned models on task-agnostic data (for example, The\nPile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral\nshifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,\nemergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent\naccuracy in predicting side effects, aligning with known benchmarks and\nrequiring no custom heuristics. Furthermore, we show that retraining on\nhigh-activation samples can partially reverse these effects. Our results\ndemonstrate that sparse probing and diffing offer a scalable and automated lens\ninto fine-tuning-induced model changes, providing practical tools for\nunderstanding and managing LLM behavior.", "AI": {"tldr": "提出MNEME框架，用于检测LLM在微调或卸载后的意外副作用，提高了对这类模型变化的理解。", "motivation": "现有的评估方法无法检测LLM微调或卸载后不可预见的副作用。例如，卸载生物学知识可能会影响化学任务的表现。因此，提出了MNEME框架来检测这些副作用。", "method": "引入了MNEME（Model diffiNg for Evaluating Mechanistic Effects），一个轻量级框架，通过稀疏模型差异比较来识别LLM微调或卸载后的意外副作用。MNEME比较基线模型与微调后的模型在任务无关的数据上的表现，以隔离行为变化，无需访问微调数据。", "result": "MNEME在三种场景下应用于五个LLM，准确率达到95%，表明稀疏探测和比较能够提供一个可扩展且自动化的视角来理解微调引发的模型变化。此外，结果显示在高激活样本上进行再训练可以部分逆转这些效应。", "conclusion": "研究结果表明，稀疏探测和比较方法为理解和管理LLM行为提供了实用工具，并展示了高激活样本上的再训练可以部分逆转这些效应。"}}
{"id": "2507.21311", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.21311", "abs": "https://arxiv.org/abs/2507.21311", "authors": ["Martin de La Gorce", "Charlie Hewitt", "Tibor Takacs", "Robert Gerdisch", "Zafiirah Hosenie", "Givi Meishvili", "Marek Kowalski", "Thomas J. Cashman", "Antonio Criminisi"], "title": "VoluMe -- Authentic 3D Video Calls from Live Gaussian Splat Prediction", "comment": null, "summary": "Virtual 3D meetings offer the potential to enhance copresence, increase\nengagement and thus improve effectiveness of remote meetings compared to\nstandard 2D video calls. However, representing people in 3D meetings remains a\nchallenge; existing solutions achieve high quality by using complex hardware,\nmaking use of fixed appearance via enrolment, or by inverting a pre-trained\ngenerative model. These approaches lead to constraints that are unwelcome and\nill-fitting for videoconferencing applications. We present the first method to\npredict 3D Gaussian reconstructions in real time from a single 2D webcam feed,\nwhere the 3D representation is not only live and realistic, but also authentic\nto the input video. By conditioning the 3D representation on each video frame\nindependently, our reconstruction faithfully recreates the input video from the\ncaptured viewpoint (a property we call authenticity), while generalizing\nrealistically to novel viewpoints. Additionally, we introduce a stability loss\nto obtain reconstructions that are temporally stable on video sequences. We\nshow that our method delivers state-of-the-art accuracy in visual quality and\nstability metrics compared to existing methods, and demonstrate our approach in\nlive one-to-one 3D meetings using only a standard 2D camera and display. This\ndemonstrates that our approach can allow anyone to communicate volumetrically,\nvia a method for 3D videoconferencing that is not only highly accessible, but\nalso realistic and authentic.", "AI": {"tldr": "Real-time 3D reconstruction from 2D webcams for accessible and authentic 3D videoconferencing.", "motivation": "represent people in 3D meetings without the constraints of complex hardware, fixed appearance, or pre-trained models", "method": "predict 3D Gaussian reconstructions in real time from a single 2D webcam feed, with a stability loss for temporal stability", "result": "state-of-the-art accuracy in visual quality and stability metrics, demonstrated in live one-to-one 3D meetings using only a standard 2D camera and display", "conclusion": "method is highly accessible, realistic, and authentic for 3D videoconferencing"}}
{"id": "2507.21086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21086", "abs": "https://arxiv.org/abs/2507.21086", "authors": ["Jaydip Sen", "Subhasis Dasgupta", "Hetvi Waghela"], "title": "Multi-Amateur Contrastive Decoding for Text Generation", "comment": "This paper has been accepted for oral presentation and publication in\n  the proceedings of the IEEE I2ITCON 2025. The conference will be organized in\n  Pune, India, from July 4 to 5, 2025. This is the accepted version of the\n  paper and NOT the final camera-ready version. The paper is 11 pages long and\n  contains 5 figures and 6 tables", "summary": "Contrastive Decoding (CD) has emerged as an effective inference-time strategy\nfor enhancing open-ended text generation by exploiting the divergence in output\nprobabilities between a large expert language model and a smaller amateur\nmodel. Although CD improves coherence and fluency, its dependence on a single\namateur restricts its capacity to capture the diverse and multifaceted failure\nmodes of language generation, such as repetition, hallucination, and stylistic\ndrift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a\ngeneralization of the CD framework that employs an ensemble of amateur models\nto more comprehensively characterize undesirable generation patterns. MACD\nintegrates contrastive signals through both averaging and consensus\npenalization mechanisms and extends the plausibility constraint to operate\neffectively in the multi-amateur setting. Furthermore, the framework enables\ncontrollable generation by incorporating amateurs with targeted stylistic or\ncontent biases. Experimental results across multiple domains, such as news,\nencyclopedic, and narrative, demonstrate that MACD consistently surpasses\nconventional decoding methods and the original CD approach in terms of fluency,\ncoherence, diversity, and adaptability, all without requiring additional\ntraining or fine-tuning.", "AI": {"tldr": "MACD, a generalization of Contrastive Decoding, uses multiple amateur models to better identify and mitigate failure modes in language generation, resulting in improved text quality without additional training.", "motivation": "The motivation is to overcome the limitation of Contrastive Decoding (CD) that depends on a single amateur model, which restricts its ability to capture diverse failure modes of text generation.", "method": "This paper proposes Multi-Amateur Contrastive Decoding (MACD), which uses an ensemble of amateur models to identify undesirable generation patterns in text. It integrates contrastive signals using averaging and consensus penalization, and enhances controllability by integrating targeted stylistic or content biases.", "result": "Experimental results show that MACD surpasses traditional decoding methods and CD in fluency, coherence, diversity, and adaptability, without the need for additional training.", "conclusion": "MACD provides a more robust and versatile solution for enhancing open-ended text generation, capturing a wider range of undesirable patterns and allowing for controllable generation."}}
{"id": "2507.21328", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21328", "abs": "https://arxiv.org/abs/2507.21328", "authors": ["Feixiang Zhou", "Zhuangzhi Gao", "He Zhao", "Jianyang Xie", "Yanda Meng", "Yitian Zhao", "Gregory Y. H. Lip", "Yalin Zheng"], "title": "GLCP: Global-to-Local Connectivity Preservation for Tubular Structure Segmentation", "comment": "MICCAI 2025 (Oral)", "summary": "Accurate segmentation of tubular structures, such as vascular networks, plays\na critical role in various medical domains. A remaining significant challenge\nin this task is structural fragmentation, which can adversely impact downstream\napplications. Existing methods primarily focus on designing various loss\nfunctions to constrain global topological structures. However, they often\noverlook local discontinuity regions, leading to suboptimal segmentation\nresults. To overcome this limitation, we propose a novel Global-to-Local\nConnectivity Preservation (GLCP) framework that can simultaneously perceive\nglobal and local structural characteristics of tubular networks. Specifically,\nwe propose an Interactive Multi-head Segmentation (IMS) module to jointly learn\nglobal segmentation, skeleton maps, and local discontinuity maps, respectively.\nThis enables our model to explicitly target local discontinuity regions while\nmaintaining global topological integrity. In addition, we design a lightweight\nDual-Attention-based Refinement (DAR) module to further improve segmentation\nquality by refining the resulting segmentation maps. Extensive experiments on\nboth 2D and 3D datasets demonstrate that our GLCP achieves superior accuracy\nand continuity in tubular structure segmentation compared to several\nstate-of-the-art approaches. The source codes will be available at\nhttps://github.com/FeixiangZhou/GLCP.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "{\n  \"tldr\": \"The paper proposes a Global-to-Local Connectivity Preservation (GLCP) framework that addresses the segmentation challenges of tubular structures, especially in dealing with local discontinuity, which is often overlooked by existing methods.\", \n  \"motivation\": \"To address the persistent issue of structural fragmentation in tubular structure segmentation which affects medical applications and is not sufficiently resolved by existing methods focused on global topologies.\", \n  \"method\": \"The authors introduce a GLCP framework that includes an Interactive Multi-head Segmentation (IMS) module and a Dual-Attention-based Refinement (DAR) module. IMS learns global segmentation, skeleton maps, and local discontinuity maps, while DAR enhances segmentation quality through refinement.\", \n  \"result\": \"Experiments on 2D and 3D datasets show that the GLCP framework performs better than state-of-the-art methods in terms of accuracy and continuity of the segmented tubular structures.\", \n  \"conclusion\": \"The GLCP framework effectively improves the segmentation of tubular structures by explicitly addressing local discontinuities and maintaining global topological integrity. The model outperforms existing approaches as demonstrated across a range of datasets.\", \n  \"citation\": \"\"}\n", "conclusion": ""}}
{"id": "2507.21095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21095", "abs": "https://arxiv.org/abs/2507.21095", "authors": ["Mohammad AL-Smadi"], "title": "QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning", "comment": null, "summary": "This paper presents our approach to the CheckThat! 2025 Task 1 on\nsubjectivity detection, where systems are challenged to distinguish whether a\nsentence from a news article expresses the subjective view of the author or\npresents an objective view on the covered topic. We propose a feature-augmented\ntransformer architecture that combines contextual embeddings from pre-trained\nlanguage models with statistical and linguistic features. Our system leveraged\npre-trained transformers with additional lexical features: for Arabic we used\nAraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while\nfor the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined\nwith TF-IDF features through a gating mechanism. We evaluated our system in\nmonolingual, multilingual, and zero-shot settings across multiple languages\nincluding English, Arabic, German, Italian, and several unseen languages. The\nresults demonstrate the effectiveness of our approach, achieving competitive\nperformance across different languages with notable success in the monolingual\nsetting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with\nmacro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank\n1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an\nablation analysis that demonstrated the importance of combining TF-IDF features\nwith the gating mechanism and the cross-lingual transfer for subjectivity\ndetection. Furthermore, our analysis reveals the model's sensitivity to both\nthe order of cross-lingual fine-tuning and the linguistic proximity of the\ntraining languages.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.21335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21335", "abs": "https://arxiv.org/abs/2507.21335", "authors": ["Monika Shah", "Sudarshan Balaji", "Somdeb Sarkhel", "Sanorita Dey", "Deepak Venugopal"], "title": "Analyzing the Sensitivity of Vision Language Models in Visual Question Answering", "comment": null, "summary": "We can think of Visual Question Answering as a (multimodal) conversation\nbetween a human and an AI system. Here, we explore the sensitivity of Vision\nLanguage Models (VLMs) through the lens of cooperative principles of\nconversation proposed by Grice. Specifically, even when Grice's maxims of\nconversation are flouted, humans typically do not have much difficulty in\nunderstanding the conversation even though it requires more cognitive effort.\nHere, we study if VLMs are capable of handling violations to Grice's maxims in\na manner that is similar to humans. Specifically, we add modifiers to\nhuman-crafted questions and analyze the response of VLMs to these modifiers. We\nuse three state-of-the-art VLMs in our study, namely, GPT-4o, Claude-3.5-Sonnet\nand Gemini-1.5-Flash on questions from the VQA v2.0 dataset. Our initial\nresults seem to indicate that the performance of VLMs consistently diminish\nwith the addition of modifiers which indicates our approach as a promising\ndirection to understand the limitations of VLMs.", "AI": {"tldr": "研究显示，在违规格赖斯会话准则的情况下，视觉语言模型的表现会下降。", "motivation": "探索视觉语言模型在违反格赖斯会话准则时的能力，以及它们是否能以类似人类的方式处理这些违反情况。", "method": "通过人为在问题中添加修饰语来研究视觉语言模型（VLMs）在违反格赖斯会话准则时的表现。", "result": "初步结果显示，随着修饰语的增加，VLMs的表现一致性地下降。", "conclusion": "该研究提供了一个有前景的方法来理解视觉语言模型的限制。"}}
{"id": "2507.21099", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21099", "abs": "https://arxiv.org/abs/2507.21099", "authors": ["Chloe Ho", "Ishneet Sukhvinder Singh", "Diya Sharma", "Tanvi Reddy Anumandla", "Michael Lu", "Vasu Sharma", "Kevin Zhu"], "title": "Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting", "comment": null, "summary": "Search algorithms and user query relevance have given LLMs the ability to\nreturn relevant information, but the effect of content phrasing on ad\nvisibility remains underexplored. We investigate how LLM-based rewriting of\nadvertisements can improve their ranking in retrieval systems and inclusion in\ngenerated LLM responses, without modifying the retrieval model itself. We\nintroduce a supervised fine-tuning framework with a custom loss balancing\nsemantic relevance and content fidelity. To evaluate effectiveness, we propose\ntwo metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion\nfrequency improvement). Our approach presents a scalable method to optimize ad\nphrasing, enhancing visibility in retrieval-based LLM workflows. Experiments\nacross both instruction-based and few-shot prompting demonstrate that PPO\ntrained models outperform both prompt engineering and supervised fine-tuning in\nmost cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in\ninstruction-based prompting. These results highlight the importance of how the\nad is written before retrieval and prompt format and reinforcement learning in\neffective ad rewriting for LLM integrated retrieval systems.", "AI": {"tldr": "研究了LLM广告重写的效应，提出了一种新方法通过调整广告措辞来提高其在检索系统中的可见性，实验显示该方法在多个方面优于其他方法。", "motivation": "探索LLM广告重写如何改进广告在其检索系统中的排名以及在生成的LLM响应中的包含频率。", "method": "采用基于监督微调的框架，并使用自定义损失函数在语义相关性和内容保真度之间进行平衡。", "result": "实验显示，PPO训练的模型在大多数情况下优于模板工程和监督微调，在基于指令的提示下实现了最高2.79 DeltaDIR@5和0.0073 DeltaMRR@5。", "conclusion": "此方法为在LLM集成的检索系统中有效重写广告提供了一种可扩展的方法，突显了广告在检索前书写的重要性以及提示格式和强化学习的作用。"}}
{"id": "2507.21349", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.21349", "abs": "https://arxiv.org/abs/2507.21349", "authors": ["Amirmohammad Shamaei", "Alexander Stebner", "Salome", "Bosshart", "Johanna Ospel", "Gouri Ginde", "Mariana Bento", "Roberto Souza"], "title": "Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging", "comment": null, "summary": "Magnetic resonance imaging (MRI) is a crucial medical imaging modality.\nHowever, long acquisition times remain a significant challenge, leading to\nincreased costs, and reduced patient comfort. Recent studies have shown the\npotential of using deep learning models that incorporate information from prior\nsubject-specific MRI scans to improve reconstruction quality of present scans.\nIntegrating this prior information requires registration of the previous scan\nto the current image reconstruction, which can be time-consuming. We propose a\nnovel deep-learning-based MRI reconstruction framework which consists of an\ninitial reconstruction network, a deep registration model, and a\ntransformer-based enhancement network. We validated our method on a\nlongitudinal dataset of T1-weighted MRI scans with 2,808 images from 18\nsubjects at four acceleration factors (R5, R10, R15, R20). Quantitative metrics\nconfirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon\nsigned-rank test). Furthermore, we analyzed the impact of our MRI\nreconstruction method on the downstream task of brain segmentation and observed\nimproved accuracy and volumetric agreement with reference segmentations. Our\napproach also achieved a substantial reduction in total reconstruction time\ncompared to methods that use traditional registration algorithms, making it\nmore suitable for real-time clinical applications. The code associated with\nthis work is publicly available at\nhttps://github.com/amirshamaei/longitudinal-mri-deep-recon.", "AI": {"tldr": "A deep-learning-based MRI reconstruction framework is proposed to significantly improve scan quality and reduce acquisition time by integrating information from previous scans, demonstrating superior performance compared to existing methods.", "motivation": "The motivation stems from the need to reduce long MRI acquisition times which lead to increased costs and decreased patient comfort. The goal is to integrate prior MRI information more efficiently to enhance current scan quality without extending the acquisition time.", "method": "We propose a deep-learning-based MRI reconstruction framework that includes an initial reconstruction network, a deep registration model, and a transformer-based enhancement network. This method aims to overcome the time-consuming process of integrating prior information from previous scans to improve the quality of current scans.", "result": "Our validation on a longitudinal dataset (2,808 images from 18 subjects at 4 acceleration factors: R5, R10, R15, R20) showed significant improvement over existing methods (p < 0.05, Wilcoxon signed-rank test). The method was noted to improve brain segmentation accuracy and volumetric agreement while also reducing reconstruction time.", "conclusion": "The proposed framework successfully enhances MRI reconstruction quality and accuracy, and reduces the reconstruction time compared to traditional methods, making it a more suitable option for clinical applications."}}
{"id": "2507.21104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21104", "abs": "https://arxiv.org/abs/2507.21104", "authors": ["Ariel E. Stassi", "Yanina Boria", "J. Matías Di Martino", "Gregory Randall"], "title": "iLSU-T: an Open Dataset for Uruguayan Sign Language Translation", "comment": "10 pages, 5 figures, 19th International Conference on Automatic Face\n  and Gesture Recognition IEEE FG 2025", "summary": "Automatic sign language translation has gained particular interest in the\ncomputer vision and computational linguistics communities in recent years.\nGiven each sign language country particularities, machine translation requires\nlocal data to develop new techniques and adapt existing ones. This work\npresents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB\nvideos with audio and text transcriptions. This type of multimodal and curated\ndata is paramount for developing novel approaches to understand or generate\ntools for sign language processing. iLSU T comprises more than 185 hours of\ninterpreted sign language videos from public TV broadcasting. It covers diverse\ntopics and includes the participation of 18 professional interpreters of sign\nlanguage. A series of experiments using three state of the art translation\nalgorithms is presented. The aim is to establish a baseline for this dataset\nand evaluate its usefulness and the proposed pipeline for data processing. The\nexperiments highlight the need for more localized datasets for sign language\ntranslation and understanding, which are critical for developing novel tools to\nimprove accessibility and inclusion of all individuals. Our data and code can\nbe accessed.", "AI": {"tldr": "本文介绍了一个公开的手语视频数据集iLSU T，并通过使用三种最先进的翻译算法进行实验，展示了本地化数据集在手语翻译和理解中的重要性。", "motivation": "鉴于每个国家的手语特点，机器翻译需要本地数据来开发新技术和适应现有技术。为了开发新的手语理解和生成工具，需要这种多模式和精心策划的数据。", "method": "本研究构建了一个名为iLSU T的公开数据集，该数据集包含了超过185小时的来自公共电视广播的乌拉圭手语视频，这些视频带有音频和文本转录。通过使用三种最先进的翻译算法进行了一系列实验，目的是为该数据集建立基准，并评估其有用性和所提出的数据处理管线的有效性。", "result": "实验表明，需要更多的本地化数据集来进行手语翻译和理解，这对于开发提高可访问性和包容性的新工具至关重要。研究中的数据和代码均可获取。", "conclusion": "该数据集的建立和实验表明，更多本地化数据集对于开发新的手语处理工具和提高所有人的可访问性和包容性至关重要。"}}
{"id": "2507.21353", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21353", "abs": "https://arxiv.org/abs/2507.21353", "authors": ["Deep Anil Patel", "Iain Melvin", "Zachary Izzo", "Martin Renqiang Min"], "title": "Group Relative Augmentation for Data Efficient Action Detection", "comment": null, "summary": "Adapting large Video-Language Models (VLMs) for action detection using only a\nfew examples poses challenges like overfitting and the granularity mismatch\nbetween scene-level pre-training and required person-centric understanding. We\npropose an efficient adaptation strategy combining parameter-efficient tuning\n(LoRA) with a novel learnable internal feature augmentation. Applied within the\nfrozen VLM backbone using FiLM, these augmentations generate diverse feature\nvariations directly relevant to the task. Additionally, we introduce a\ngroup-weighted loss function that dynamically modulates the training\ncontribution of each augmented sample based on its prediction divergence\nrelative to the group average. This promotes robust learning by prioritizing\ninformative yet reasonable augmentations. We demonstrate our method's\neffectiveness on complex multi-label, multi-person action detection datasets\n(AVA, MOMA), achieving strong mAP performance and showcasing significant data\nefficiency for adapting VLMs from limited examples.", "AI": {"tldr": "研究提出了一种结合参数有效调优和内部特征增强的方法，以解决视频-语言模型在有限数据下的动作检测任务中的适应问题，并在多个数据集上验证了其有效性。", "motivation": "研究动机在于解决在有限训练数据下，将大型视频语言模型（VLM）有效适应到动作检测任务中时存在的过拟合及场景级预训练与所需的人体中心理解之间的粒度不匹配问题。", "method": "研究采用了结合参数有效调优（LoRA）和内部特征增强的方法。特征增强是在VLM主干内通过冻结的方法实现的，利用了FiLM技术。此外，提出了一种可以动态调节每种增强样本在训练中贡献的分组加权损失函数，该损失函数基于每个样本的预测结果与群体平均预测的离散程度。", "result": "该研究针对利用大型视频-语言模型（VLM）进行动作检测时面临的小数据集下的过拟合和粒度不匹配问题，提出了一个高效的适应策略，该策略结合了参数有效调优方法（如LoRA），同时引入了一种新的可学习内部特征增强。这些增强通过FiLM在冷冻的VLM主干中应用，生成直接相关的多样化特征变异。此外，研究还引入了一个基于预测离群度的动态训练贡献调整的分组加权损失函数，以促进学习更加健壮。该方法在复杂多标签、多对象动作检测数据集（如AVA，MOMA）上展示了强大的mAP性能，并在从少量范例中调整VLM方面显示出显著的数据效率。", "conclusion": "该方法的有效性已经在复杂的多标签、多对象动作检测数据集（如AVA和MOMA）上得到了验证，表现出了强健的mAP性能，并且在从少量范例中调整VLM时表现出显著的数据效率。"}}
{"id": "2507.21106", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21106", "abs": "https://arxiv.org/abs/2507.21106", "authors": ["Mandar Marathe"], "title": "Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype", "comment": "This dissertation was submitted by Mandar Marathe on 6 September\n  2022, in partial fulfilment of the requirements for the Master of Arts degree\n  in Advanced Arabic at the University of Exeter", "summary": "Arabic Rhetoric is the field of Arabic linguistics which governs the art and\nscience of conveying a message with greater beauty, impact and persuasiveness.\nThe field is as ancient as the Arabic language itself and is found extensively\nin classical and contemporary Arabic poetry, free verse and prose. In practical\nterms, it is the intelligent use of word order, figurative speech and\nlinguistic embellishments to enhance message delivery. Despite the volumes that\nhave been written about it and the high status accorded to it, there is no way\nto objectively know whether a speaker or writer has used Arabic rhetoric in a\ngiven text, to what extent, and why. There is no objective way to compare the\nuse of Arabic rhetoric across genres, authors or epochs. It is impossible to\nknow which of pre-Islamic poetry, Andalucian Arabic poetry, or modern literary\ngenres are richer in Arabic rhetoric. The aim of the current study was to\ndevise a way to measure the density of the literary devices which constitute\nArabic rhetoric in a given text, as a proxy marker for Arabic rhetoric itself.\nA comprehensive list of 84 of the commonest literary devices and their\ndefinitions was compiled. A system of identifying literary devices in texts was\nconstructed. A method of calculating the density of literary devices based on\nthe morpheme count of the text was utilised. Four electronic tools and an\nanalogue tool were created to support the calculation of an Arabic text's\nrhetorical literary device density, including a website and online calculator.\nAdditionally, a technique of reporting the distribution of literary devices\nused across the three sub-domains of Arabic rhetoric was created. The output of\nthis project is a working tool which can accurately report the density of\nArabic rhetoric in any Arabic text or speech.", "AI": {"tldr": "本研究旨在开发一种测量方法来评估阿语文本中阿拉伯修辞手法的使用情况，成功开发了包括网站和在线计算器在内的工具。", "motivation": "目前没有客观的方法来判断一个说话者或作者是否在一个给定的文本中使用了阿拉伯修辞手法，使用到什么程度，以及为什么使用。也没有客观的方法来跨体裁、作者或时期比较阿拉伯修辞的使用情况。本研究的目的是开发一种测量给定文本中构成阿拉伯修辞的文学设备的密度的方法，以作为一种替代标志。", "method": "通过编制84种常见的修辞手法及其定义的综合列表，构建了一个识别文本中修辞手法的系统，采用基于词素计数计算修辞手法密度的方法，并创建了四种电子工具和一个模拟工具来支持计算阿语文本的修辞手法密度，包括一个网站和一个在线计算器。还创造了一个报告阿拉伯修辞三个子领域中使用修辞手法分布的技术。", "result": "该研究开发了一个能够在任何阿语文本或演讲中准确报告阿拉伯修辞密度的实用工具。", "conclusion": "本研究成功开发了一种能够准确测量任何阿语文本中修辞密度的方法和工具，填补了阿拉伯语言学在客观测量修辞手法使用情况方面的空白。"}}
{"id": "2507.21358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21358", "abs": "https://arxiv.org/abs/2507.21358", "authors": ["Jicheng Yuan", "Manh Nguyen Duc", "Qian Liu", "Manfred Hauswirth", "Danh Le Phuoc"], "title": "Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy", "comment": null, "summary": "Vision-based bird's-eye-view (BEV) 3D object detection has advanced\nsignificantly in autonomous driving by offering cost-effectiveness and rich\ncontextual information. However, existing methods often construct BEV\nrepresentations by collapsing extracted object features, neglecting intrinsic\nenvironmental contexts, such as roads and pavements. This hinders detectors\nfrom comprehensively perceiving the characteristics of the physical world. To\nalleviate this, we introduce a multi-task learning framework, Collaborative\nPerceiver (CoP), that leverages spatial occupancy as auxiliary information to\nmine consistent structural and conceptual similarities shared between 3D object\ndetection and occupancy prediction tasks, bridging gaps in spatial\nrepresentations and feature refinement. To this end, we first propose a\npipeline to generate dense occupancy ground truths incorporating local density\ninformation (LDO) for reconstructing detailed environmental information. Next,\nwe employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained\nlocal features according to distinct object properties. Furthermore, we develop\na global-local collaborative feature fusion (CFF) module that seamlessly\nintegrates complementary knowledge between both tasks, thus composing more\nrobust BEV representations. Extensive experiments on the nuScenes benchmark\ndemonstrate that CoP outperforms existing vision-based frameworks, achieving\n49.5\\% mAP and 59.2\\% NDS on the test set. Code and supplementary materials are\navailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.", "AI": {"tldr": "研究提出了一种名为Collaborative Perceiver (CoP)的多任务学习框架，通过引入空间占用信息来提升3D目标检测的效果。", "motivation": "现有的方法通常通过合并提取的对象特征来构建BEV表示，忽视了道路和人行道等本征环境背景，这阻碍了检测器全面感知物理世界的特点。为此，提出了这种方法。", "method": "引入了一个多任务学习框架，称为Collaborative Perceiver (CoP)，该框架利用空间占用作为辅助信息，以挖掘3D目标检测和占用预测任务之间共享的结构和概念相似性，从而弥补空间表示和特征细化的缺口。首先提出了一种生成包含局部密度信息（LDO）的密集占用真值的管道，用于重建详细的环境信息。接着采用体素-高度引导的采样（VHS）策略，根据不同的物体特性提取细粒度的局部特征。此外，开发了一个全局-局部协作特征融合（CFF）模块，无缝结合两个任务间互补的知识，从而组成更稳健的鸟瞰视图（BEV）表示。", "result": "在nuScenes基准上的广泛实验表明，CoP优于现有的基于视觉的框架，在测试集上实现了49.5%的mAP和59.2%的NDS。", "conclusion": "研究提出的Collaborative Perceiver框架通过多任务学习，提升了鸟瞰视图3D物体检测的效果，证明了该方法的有效性和优越性。"}}
{"id": "2507.21107", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21107", "abs": "https://arxiv.org/abs/2507.21107", "authors": ["Rob Manson"], "title": "Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams", "comment": "29 pages, 22 figures", "summary": "We propose Curved Inference - a geometric Interpretability framework that\ntracks how the residual stream trajectory of a large language model bends in\nresponse to shifts in semantic concern. Across 20 matched prompts spanning\nemotional, moral, perspective, logical, identity, environmental, and nonsense\ndomains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics,\nwith a primary focus on curvature (\\k{appa}_i) and salience (S(t)). These\nmetrics are computed under a pullback semantic metric derived from the\nunembedding matrix, ensuring that all measurements reflect token-aligned\ngeometry rather than raw coordinate structure. We find that concern-shifted\nprompts reliably alter internal activation trajectories in both models - with\nLLaMA exhibiting consistent, statistically significant scaling in both\ncurvature and salience as concern intensity increases. Gemma also responds to\nconcern but shows weaker differentiation between moderate and strong variants.\nOur results support a two-layer view of LLM geometry - a latent conceptual\nstructure encoded in the embedding space, and a contextual trajectory shaped by\nprompt-specific inference. Curved Inference reveals how models navigate,\nreorient, or reinforce semantic meaning over depth, offering a principled\nmethod for diagnosing alignment, abstraction, and emergent inference dynamics.\nThese findings offer fresh insight into semantic abstraction and model\nalignment through the lens of Curved Inference.", "AI": {"tldr": "研究提出了一种名为Curved Inference的框架，分析大型语言模型在响应不同语义关注点变化时的行为，揭示了模型几何结构的变化。", "motivation": "该研究旨在分析大型语言模型在面对不同语义领域（情感、道德、视角、逻辑、身份、环境和无意义领域）的提示时，其内部激活轨迹如何变化。", "method": "我们提出了一种名为Curved Inference的几何可解释性框架，用于追踪大型语言模型的残差流轨迹在语义关注点变化时如何弯曲。", "result": "研究发现，对LLaMA和Gemma3-1b模型进行语义关注点变化的提示时，它们的内部激活轨迹会显著变化。LLaMA在关注点强度增加时，弯曲度和显著性会有持续且统计显著的提升。而Gemma在响应关注点变化时较弱，尤其在辨别中等和强变体方面。", "conclusion": "研究结果支持大型语言模型几何结构的两层观点：嵌入空间中编码的潜在概念结构，和由特定提示引起的上下文轨迹。Curved Inference揭示了模型如何在深度上导航、重新定向或加强语义意义，为诊断模型一致性、抽象和新兴推理动态提供了一个原则性的方法。"}}
{"id": "2507.21364", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21364", "abs": "https://arxiv.org/abs/2507.21364", "authors": ["Lukman Jibril Aliyu", "Umar Sani Muhammad", "Bilqisu Ismail", "Nasiru Muhammad", "Almustapha A Wakili", "Seid Muhie Yimam", "Shamsuddeen Hassan Muhammad", "Mustapha Abdullahi"], "title": "Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers", "comment": "Accepted as a camera-ready paper at Deep Learning Indaba 2025\n  (Kigali, Rwanda)", "summary": "Wildlife populations in Africa face severe threats, with vertebrate numbers\ndeclining by over 65% in the past five decades. In response, image\nclassification using deep learning has emerged as a promising tool for\nbiodiversity monitoring and conservation. This paper presents a comparative\nstudy of deep learning models for automatically classifying African wildlife\nimages, focusing on transfer learning with frozen feature extractors. Using a\npublic dataset of four species: buffalo, elephant, rhinoceros, and zebra; we\nevaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and\nVision Transformer ViT-H/14. DenseNet-201 achieved the best performance among\nconvolutional networks (67% accuracy), while ViT-H/14 achieved the highest\noverall accuracy (99%), but with significantly higher computational cost,\nraising deployment concerns. Our experiments highlight the trade-offs between\naccuracy, resource requirements, and deployability. The best-performing CNN\n(DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time\nfield use, demonstrating the feasibility of deploying lightweight models in\nconservation settings. This work contributes to African-grounded AI research by\noffering practical insights into model selection, dataset preparation, and\nresponsible deployment of deep learning tools for wildlife conservation.", "AI": {"tldr": "本研究通过比较深度学习模型在非洲野生动物图像分类中的性能，提出了在现实世界中应用深度学习进行野生动物保护的可行性和应注意的权衡问题。", "motivation": "鉴于非洲野生动物种群在过去五十年内面临严重威胁，数量下降超过65%，本研究旨在通过深度学习图像分类技术为生物多样性监测和保护提供一种有前景的工具。", "method": "本研究比较了四种深度学习模型（DenseNet-201, ResNet-152, EfficientNet-B4, Vision Transformer ViT-H/14）在非洲野生动物图像分类中的表现，使用了含有四种物种（水牛、大象、犀牛和斑马）的数据集，并且采用了迁移学习方法，利用冻结的特征提取器进行实验评估。", "result": "实验结果显示，DenseNet-201 在卷积神经网络中表现最好，准确率为67%；而 Vision Transformer ViT-H/14 表现出最高的准确率99%，但计算成本显著更高，导致部署成为问题。", "conclusion": "研究结果突显了在准确性、资源需求和部署之间的权衡关系。DenseNet-201 成功整合进入Hugging Face Gradio Space，实现了在实地应用中的实时使用，展示了在保护场景中部署轻量级模型的可行性。"}}
{"id": "2507.21108", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21108", "abs": "https://arxiv.org/abs/2507.21108", "authors": ["Amrita Singh", "Aditya Joshi", "Jiaojiao Jiang", "Hye-young Paik"], "title": "A Survey of Classification Tasks and Approaches for Legal Contracts", "comment": "Under review. 49 pages + references", "summary": "Given the large size and volumes of contracts and their underlying inherent\ncomplexity, manual reviews become inefficient and prone to errors, creating a\nclear need for automation. Automatic Legal Contract Classification (LCC)\nrevolutionizes the way legal contracts are analyzed, offering substantial\nimprovements in speed, accuracy, and accessibility. This survey delves into the\nchallenges of automatic LCC and a detailed examination of key tasks, datasets,\nand methodologies. We identify seven classification tasks within LCC, and\nreview fourteen datasets related to English-language contracts, including\npublic, proprietary, and non-public sources. We also introduce a methodology\ntaxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning,\nand Transformer-based approaches. Additionally, the survey discusses evaluation\ntechniques and highlights the best-performing results from the reviewed\nstudies. By providing a thorough overview of current methods and their\nlimitations, this survey suggests future research directions to improve the\nefficiency, accuracy, and scalability of LCC. As the first comprehensive survey\non LCC, it aims to support legal NLP researchers and practitioners in improving\nlegal processes, making legal information more accessible, and promoting a more\ninformed and equitable society.", "AI": {"tldr": "The paper provides a comprehensive survey of automatic legal contract classification (LCC), analyzing its methodologies, datasets, and challenges, to improve legal processes and information accessibility.", "motivation": "The motivation is to address the inefficiencies and errors in manual contract reviews by improving the speed, accuracy, and accessibility of contract analysis through automation.", "method": "Automatic Legal Contract Classification (LCC) involves the use of Traditional Machine Learning, Deep Learning, and Transformer-based approaches to classify contracts.", "result": "The paper identifies seven classification tasks, reviews fourteen datasets, and introduces a methodology taxonomy for LCC. It also discusses evaluation techniques and highlights the best-performing results.", "conclusion": "The survey suggests future research directions to further improve the efficiency, accuracy, and scalability of legal contract classification. It aims to enhance legal processes and make legal information more accessible."}}
{"id": "2507.21367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21367", "abs": "https://arxiv.org/abs/2507.21367", "authors": ["I-Hsiang Chen", "Hua-En Chang", "Wei-Ting Chen", "Jenq-Neng Hwang", "Sy-Yen Kuo"], "title": "Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation", "comment": "Accepted by ICCV2025", "summary": "Domain Generalized Semantic Segmentation (DGSS) is a critical yet challenging\ntask, as domain shifts in unseen environments can severely compromise model\nperformance. While recent studies enhance feature alignment by projecting\nfeatures into the source domain, they often neglect intrinsic latent domain\npriors, leading to suboptimal results. In this paper, we introduce PDAF, a\nProbabilistic Diffusion Alignment Framework that enhances the generalization of\nexisting segmentation networks through probabilistic diffusion modeling. PDAF\nintroduces a Latent Domain Prior (LDP) to capture domain shifts and uses this\nprior as a conditioning factor to align both source and unseen target domains.\nTo achieve this, PDAF integrates into a pre-trained segmentation model and\nutilizes paired source and pseudo-target images to simulate latent domain\nshifts, enabling LDP modeling. The framework comprises three modules: the\nLatent Prior Extractor (LPE) predicts the LDP by supervising domain shifts; the\nDomain Compensation Module (DCM) adjusts feature representations to mitigate\ndomain shifts; and the Diffusion Prior Estimator (DPE) leverages a diffusion\nprocess to estimate the LDP without requiring paired samples. This design\nenables PDAF to iteratively model domain shifts, progressively refining feature\nrepresentations to enhance generalization under complex target conditions.\nExtensive experiments validate the effectiveness of PDAF across diverse and\nchallenging urban scenes.", "AI": {"tldr": "引入PDAF框架，通过概率扩散建模增强分割模型对未见环境的泛化能力，解决了忽视隐域先验导致泛化能力下降的问题。", "motivation": "文章旨在解决现有方法忽视隐域先验，仅通过对特征进行投影来提升特征对齐，从而导致迁移至未见环境时性能下降的问题。", "method": "PDAF, 即概率扩散对齐框架，通过引入隐域先验来捕捉域偏移，并将其作为条件因素来对齐源域和未见的目标域。该框架包含三个模块：隐域先验提取器（LPE）预测隐域先验，域补偿模块（DCM）调整特征表示以减轻域偏移，扩散先验估计器（DPE）通过扩散过程估计隐域先验，无需配对样本。", "result": "通过模拟隐域偏移，PDAF能够有效地对预训练分割模型进行集成，提高了模型在复杂目标条件下的泛化能力。实验结果显示了PDAF框架在各种具有挑战性的城市场景中的有效性。", "conclusion": "PDAF框架通过迭代建模域偏移过程，逐步细化特征表示，从而提升了分割模型在未见环境下的性能。"}}
{"id": "2507.21110", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.21110", "abs": "https://arxiv.org/abs/2507.21110", "authors": ["Kezhen Zhong", "Basem Suleiman", "Abdelkarim Erradi", "Shijing Chen"], "title": "SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering", "comment": "16 pages, 12 figures", "summary": "This paper introduces SemRAG, an enhanced Retrieval Augmented Generation\n(RAG) framework that efficiently integrates domain-specific knowledge using\nsemantic chunking and knowledge graphs without extensive fine-tuning.\nIntegrating domain-specific knowledge into large language models (LLMs) is\ncrucial for improving their performance in specialized tasks. Yet, existing\nadaptations are computationally expensive, prone to overfitting and limit\nscalability. To address these challenges, SemRAG employs a semantic chunking\nalgorithm that segments documents based on the cosine similarity from sentence\nembeddings, preserving semantic coherence while reducing computational\noverhead. Additionally, by structuring retrieved information into knowledge\ngraphs, SemRAG captures relationships between entities, improving retrieval\naccuracy and contextual understanding. Experimental results on MultiHop RAG and\nWikipedia datasets demonstrate SemRAG has significantly enhances the relevance\nand correctness of retrieved information from the Knowledge Graph,\noutperforming traditional RAG methods. Furthermore, we investigate the\noptimization of buffer sizes for different data corpus, as optimizing buffer\nsizes tailored to specific datasets can further improve retrieval performance,\nas integration of knowledge graphs strengthens entity relationships for better\ncontextual comprehension. The primary advantage of SemRAG is its ability to\ncreate an efficient, accurate domain-specific LLM pipeline while avoiding\nresource-intensive fine-tuning. This makes it a practical and scalable approach\naligned with sustainability goals, offering a viable solution for AI\napplications in domain-specific fields.", "AI": {"tldr": "本文介绍了SemRAG，一种增强型的检索增强生成框架，通过语义切块和知识图谱的运用，高效地整合领域特定知识，避免了繁琐的微调过程。", "motivation": "将领域特定知识整合到大语言模型中对于提高其在专业任务中的表现至关重要，然而现有的适配方法计算消耗大且易过拟合，限制了其扩展性。", "method": "SemRAG采用语义切块算法，基于句子嵌入的余弦相似度对文档进行分割，同时运用知识图谱对检索出的信息进行结构化处理，以此提高检索精度和上下文理解。", "result": "实验结果表明，与传统RAG方法相比，SemRAG在MultiHop RAG和Wikipedia数据集上的实验中显著提升了从知识图谱中检索出的信息的相关性和准确性。", "conclusion": "SemRAG能够在避免资源密集型微调的情况下，创建高效的领域特定语言模型管道，是一种实用且可扩展的方法。"}}
{"id": "2507.21371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21371", "abs": "https://arxiv.org/abs/2507.21371", "authors": ["Zitong Zhang", "Suranjan Gautam", "Rui Yu"], "title": "Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View", "comment": "ICCV 2025. Project page: https://top2pano.github.io/", "summary": "Generating immersive 360{\\deg} indoor panoramas from 2D top-down views has\napplications in virtual reality, interior design, real estate, and robotics.\nThis task is challenging due to the lack of explicit 3D structure and the need\nfor geometric consistency and photorealism. We propose Top2Pano, an end-to-end\nmodel for synthesizing realistic indoor panoramas from top-down views. Our\nmethod estimates volumetric occupancy to infer 3D structures, then uses\nvolumetric rendering to generate coarse color and depth panoramas. These guide\na diffusion-based refinement stage using ControlNet, enhancing realism and\nstructural fidelity. Evaluations on two datasets show Top2Pano outperforms\nbaselines, effectively reconstructing geometry, occlusions, and spatial\narrangements. It also generalizes well, producing high-quality panoramas from\nschematic floorplans. Our results highlight Top2Pano's potential in bridging\ntop-down views with immersive indoor synthesis.", "AI": {"tldr": "This paper proposes Top2Pano, an end-to-end model for realistic indoor panoramic synthesis from top-down views, achieving better performance than baselines by enhancing realism and structural fidelity.", "motivation": "The motivation behind this paper is to address the challenge of generating immersive 360° indoor panoramas from 2D top-down views, which is difficult due to the lack of explicit 3D structure and the need for geometric consistency and photorealism. The application areas are broad, including virtual reality, interior design, real estate, and robotics.", "method": "Our method estimates volumetric occupancy to infer 3D structures from top-down 2D views, then uses volumetric rendering to generate coarse color and depth panoramas. These panoramas guide a diffusion-based refinement stage using ControlNet, enhancing the realism and structural fidelity of the final panoramic images.", "result": "The evaluation on two datasets demonstrates that Top2Pano outperforms the baselines in effectively reconstructing geometry, occlusions, and the spatial arrangement, and it also generalizes well, creating high-quality panoramas from schematic floorplans.", "conclusion": "The study concludes that Top2Pano shows potential for bridging the gap between top-down views and immersive indoor synthesis, contributing valuable solutions for fields like virtual interior design and real estate visualization."}}
{"id": "2507.21112", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21112", "abs": "https://arxiv.org/abs/2507.21112", "authors": ["Panyi Dong", "Zhiyu Quan"], "title": "InsurTech innovation using natural language processing", "comment": null, "summary": "With the rapid rise of InsurTech, traditional insurance companies are\nincreasingly exploring alternative data sources and advanced technologies to\nsustain their competitive edge. This paper provides both a conceptual overview\nand practical case studies of natural language processing (NLP) and its\nemerging applications within insurance operations with a focus on transforming\nraw, unstructured text into structured data suitable for actuarial analysis and\ndecision-making. Leveraging real-world alternative data provided by an\nInsurTech industry partner that enriches traditional insurance data sources, we\napply various NLP techniques to demonstrate practical use cases in the\ncommercial insurance context. These enriched, text-derived insights not only\nadd to and refine traditional rating factors for commercial insurance pricing\nbut also offer novel perspectives for assessing underlying risk by introducing\nnovel industry classifications. Through these demonstrations, we show that NLP\nis not merely a supplementary tool but a foundational element for modern,\ndata-driven insurance analytics.", "AI": {"tldr": "本文通过真实世界数据展示了NLP在保险行业的应用，特别是商业保险中的实际案例，说明NLP不仅是辅助工具，更是现代数据驱动保险分析的基础元素。", "motivation": "随着InsurTech的迅速崛起，传统保险公司正在探索替代数据源和先进科技以维持竞争优势。本文旨在概述自然语言处理（NLP）的概念及其在保险运营中的新兴应用，特别是将原始、非结构化的文本转化为可用于精算分析和决策的结构化数据。", "method": "本研究采用多种自然语言处理技术，处理来自InsurTech行业合作伙伴提供的实际替代数据，这些数据丰富了传统的保险数据来源。通过这些数据，展示了商业保险情境中的实际应用案例。", "result": "研究展示了如何利用文本衍生洞察，不仅丰富和调整了商业保险定价的传统评级因素，还通过引入新的行业分类来提供评估基础风险的新视角。", "conclusion": "通过演示，我们表明，NLP不仅是辅助工具，更是现代数据驱动保险分析的基石。"}}
{"id": "2507.21391", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21391", "abs": "https://arxiv.org/abs/2507.21391", "authors": ["Shijie Zhou", "Ruiyi Zhang", "Huaisheng Zhu", "Branislav Kveton", "Yufan Zhou", "Jiuxiang Gu", "Jian Chen", "Changyou Chen"], "title": "Multimodal LLMs as Customized Reward Models for Text-to-Image Generation", "comment": "Accepted at ICCV 2025. Code available at\n  https://github.com/sjz5202/LLaVA-Reward", "summary": "We introduce LLaVA-Reward, an efficient reward model designed to\nautomatically evaluate text-to-image (T2I) generations across multiple\nperspectives, leveraging pretrained multimodal large language models (MLLMs).\nExisting MLLM-based approaches require instruction-following data for\nsupervised fine-tuning and evaluate generation quality on analyzing text\nresponse, which is time-consuming and difficult to train. To address this\nproblem, we propose LLaVA-Reward, which directly utilizes the hidden states of\nMLLMs given text-image pairs. To enhance the bidirectional interaction between\nvisual and textual representations in decoder-only MLLMs, we further propose\nadding a Skip-connection Cross Attention (SkipCA) module. This design enhances\ntext-image correlation reasoning by connecting early-layer visual features with\nlater-layer hidden representations.In addition, LLaVA-Reward supports different\ntypes of preference data for efficient fine-tuning, including paired preference\ndata and unpaired data. We train LLaVA-Reward on four evaluation perspectives:\ntext-image alignment, fidelity/artifact, safety, and overall ranking. Empirical\nresults demonstrate that LLaVA-Reward outperforms conventional and MLLM-based\nmethods in generating human-aligned scores for automatic evaluations and\ninference-time scaling in text-to-image generations.", "AI": {"tldr": "LLaVA-Reward is an efficient reward model for evaluating text-to-image generations, outperforming existing methods by leveraging hidden states of multimodal language models and enhancing visual-textual interaction through a Skip-connection Cross Attention module.", "motivation": "Existing methods require instruction-following data for supervised fine-tuning and evaluate generation quality from text responses, which is time-consuming and difficult to train. LLaVA-Reward aims to address these issues by utilizing the hidden states of MLLMs and supporting different preference data types for efficient fine-tuning.", "method": "We propose LLaVA-Reward, an efficient reward model for automatically evaluating text-to-image generations using pretrained multimodal large language models (MLLMs). To improve the interaction between visual and textual representations in decoder-only MLLMs, a Skip-connection Cross Attention (SkipCA) module is introduced, which connects early-layer visual features with later-layer hidden representations.", "result": "Empirical results show that LLaVA-Reward surpasses conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.", "conclusion": "LLaVA-Reward is an effective and efficient method for automating the evaluation of text-to-image generations, resulting in improved quality and human-aligned scores compared to conventional and multimodal language model-based approaches."}}
{"id": "2507.21134", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21134", "abs": "https://arxiv.org/abs/2507.21134", "authors": ["Zheng Hui", "Yijiang River Dong", "Ehsan Shareghi", "Nigel Collier"], "title": "TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-risk\ndomains such as law, finance, and medicine, systematically evaluating their\ndomain-specific safety and compliance becomes critical. While prior work has\nlargely focused on improving LLM performance in these domains, it has often\nneglected the evaluation of domain-specific safety risks. To bridge this gap,\nwe first define domain-specific safety principles for LLMs based on the AMA\nPrinciples of Medical Ethics, the ABA Model Rules of Professional Conduct, and\nthe CFA Institute Code of Ethics. Building on this foundation, we introduce\nTrident-Bench, a benchmark specifically targeting LLM safety in the legal,\nfinancial, and medical domains. We evaluated 19 general-purpose and\ndomain-specialized models on Trident-Bench and show that it effectively reveals\nkey safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic\nexpectations, whereas domain-specialized models often struggle with subtle\nethical nuances. This highlights an urgent need for finer-grained\ndomain-specific safety improvements. By introducing Trident-Bench, our work\nprovides one of the first systematic resources for studying LLM safety in law\nand finance, and lays the groundwork for future research aimed at reducing the\nsafety risks of deploying LLMs in professionally regulated fields. Code and\nbenchmark will be released at: https://github.com/zackhuiiiii/TRIDENT", "AI": {"tldr": "本文定义了LLM领域内特定的安全原则，并引入了专门评估LLMs在法律、金融和医疗领域安全性的基准测试Trident-Bench。评估结果揭示了关键安全差距，强调了需要在领域内安全性方面进行更精细的改进。", "motivation": "随着大型语言模型（LLMs）越来越多地应用于法律、金融和医疗等高风险领域，系统性评估LLMs在这些领域的特定安全性变得至关重要。然而，之前的研究主要集中在改善LLMs在这些领域的性能，忽略了对LLMs领域安全性风险的评估，本研究旨在填补这一空白。", "method": "本研究首先定义了基于医学伦理原则、律师职业行为准则及金融伦理代码的LLM领域内特定的安全原则。接着，引入了专门针对LLM在法律、金融和医疗领域安全性的基准测试Trident-Bench。", "result": "通过对19个通用和领域专长模型在Trident-Bench上的评估，发现了关键的安全差距：强大的通才模型（如GPT、Gemini）能够满足基本期望，而领域专长模型在复杂的伦理细微问题上往往表现不佳。", "conclusion": "通过引入Trident-Bench，本研究为在法律和金融领域内研究LLMs的安全性提供了首个系统性资源，并为未来旨在减少LLMs在专业监管领域安全风险的研究奠定了基础。"}}
{"id": "2507.21420", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21420", "abs": "https://arxiv.org/abs/2507.21420", "authors": ["Chaoyu Li", "Yogesh Kulkarni", "Pooyan Fazli"], "title": "ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs", "comment": null, "summary": "The computational cost of training multimodal large language models (MLLMs)\nrapidly increases with the number of tokens involved. Existing efficiency\nmethods primarily target inference and rely on token reduction or merging,\noffering limited benefit during training. In this paper, we propose ReGATE\n(Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method\nfor accelerating MLLM training. Specifically, ReGATE adopts a teacher-student\nframework in which the MLLM being trained serves as the student, and a frozen\nreference large language model (LLM) acts as the teacher. The teacher computes\nper-token reference losses, which are combined with an exponential moving\naverage (EMA) of the student's own difficulty scores. This adaptive\ndifficulty-based scoring enables the selective processing of crucial tokens\nwhile bypassing less informative ones in the forward pass, significantly\nreducing computational overhead. Experiments demonstrate that ReGATE, when\napplied to VideoLLaMA2, matches the peak accuracy of standard training on\nMVBench up to 2$\\times$ faster, using only 35% of the tokens. With additional\ntraining, it even surpasses the baseline on several multimodal benchmarks, all\nwhile reducing the total token count by over 41%. Code and models will be\nreleased soon.", "AI": {"tldr": "ReGATE方法通过教师-学生框架在训练大型多模态语言模型时选择性处理重要标记来减少计算负担，显著提高了训练效率。", "motivation": "现存的效率方法主要集中在推理阶段，对训练阶段的效果有限。因此，研究提出了一种新的方法来优化训练过程中的计算成本。", "method": "ReGATE采用教师-学生框架，通过计算每个标记的参考损失并与学生模型的难度分数结合，实现选择性处理关键标记，降低计算负担。", "result": "实验结果显示，应用于VideoLLaMA2时，ReGATE能够在仅使用35%标记的情况下达到标准训练的峰值精度，并且速度提升2倍。", "conclusion": "ReGATE证明了在减少计算负担的同时还能够提高多模态基准测试的性能。"}}
{"id": "2507.21138", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.21138", "abs": "https://arxiv.org/abs/2507.21138", "authors": ["Oleg Atamanenko", "Anna Chalova", "Joseph Coombes", "Nikki Cope", "Phillip Dang", "Zhifeng Deng", "Jimmy Du", "Michael Ermolenko", "Feifan Fan", "Yufei Feng", "Cheryl Fichter", "Pavel Filimonov", "Louis Fischer", "Kylan Gibbs", "Valeria Gusarova", "Pavel Karpik", "Andreas Assad Kottner", "Ian Lee", "Oliver Louie", "Jasmine Mai", "Mikhail Mamontov", "Suri Mao", "Nurullah Morshed", "Igor Poletaev", "Florin Radu", "Dmytro Semernia", "Evgenii Shingarev", "Vikram Sivaraja", "Peter Skirko", "Rinat Takhautdinov", "Robert Villahermosa", "Jean Wang"], "title": "TTS-1 Technical Report", "comment": "20 pages, 10 figures. For associated modeling and training code, see\n  https://github.com/inworld-ai/tts", "summary": "We introduce Inworld TTS-1, a set of two Transformer-based autoregressive\ntext-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters\nand is designed for utmost quality and expressiveness in demanding\napplications. TTS-1 is our most efficient model, with 1.6B parameters, built\nfor real-time speech synthesis and on-device use cases. By scaling train-time\ncompute and applying a sequential process of pre-training, fine-tuning, and\nRL-alignment of the speech-language model (SpeechLM) component, both models\nachieve state-of-the-art performance on a variety of benchmarks, demonstrating\nexceptional quality relying purely on in-context learning of the speaker's\nvoice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech\nwith low latency, and support 11 languages with fine-grained emotional control\nand non-verbal vocalizations through audio markups. We additionally open-source\nour training and modeling code under an MIT license.", "AI": {"tldr": "Inworld TTS-1引入了两个高质量、高效的TTS模型，通过高级训练技术达到了顶尖性能，并支持多语言情感控制，同时开源了项目代码。", "motivation": "开发Inworld TTS-1的目的是为了在要求极高的应用中实现语音合成的极致质量和表现力，同时也通过较小的模型支持实时语音合成和设备上使用场景。", "method": "引入了两个基于Transformer的自回归文本到语音（TTS）模型，分别是拥有88亿参数的TTS-1-Max和16亿参数的TTS-1。通过扩大训练计算能力和采用预训练、微调以及语言模型部分的强化学习对齐的序列过程，两个模型在多种基准测试中均达到了最先进的性能。", "result": "Inworld TTS-1和TTS-1-Max能够生成高分辨率的48kHz语音，具有低延迟，并且支持11种语言，通过音频标记提供细致的情感控制和非语言声音。同时还开源了训练和模型代码。", "conclusion": "通过上述方法，Inworld TTS-1证明了只需通过上下文学习即可获得卓越的语音质量，同时通过开源项目促进了交流与进一步发展。"}}
{"id": "2507.21423", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21423", "abs": "https://arxiv.org/abs/2507.21423", "authors": ["Thomas Monninger", "Zihan Zhang", "Zhipeng Mo", "Md Zafar Anwar", "Steffen Staab", "Sihao Ding"], "title": "MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving", "comment": "Accepted for 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "summary": "Autonomous driving requires an understanding of the static environment from\nsensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse\nmultiple inputs, and a vector decoder predicts a vectorized map representation\nfrom the latent BEV grid. However, traditional map construction models provide\ndeterministic point estimates, failing to capture uncertainty and the inherent\nambiguities of real-world environments, such as occlusions and missing lane\nmarkings. We propose MapDiffusion, a novel generative approach that leverages\nthe diffusion paradigm to learn the full distribution of possible vectorized\nmaps. Instead of predicting a single deterministic output from learned queries,\nMapDiffusion iteratively refines randomly initialized queries, conditioned on a\nBEV latent grid, to generate multiple plausible map samples. This allows\naggregating samples to improve prediction accuracy and deriving uncertainty\nestimates that directly correlate with scene ambiguity. Extensive experiments\non the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art\nperformance in online map construction, surpassing the baseline by 5% in\nsingle-sample performance. We further show that aggregating multiple samples\nconsistently improves performance along the ROC curve, validating the benefit\nof distribution modeling. Additionally, our uncertainty estimates are\nsignificantly higher in occluded areas, reinforcing their value in identifying\nregions with ambiguous sensor input. By modeling the full map distribution,\nMapDiffusion enhances the robustness and reliability of online vectorized HD\nmap construction, enabling uncertainty-aware decision-making for autonomous\nvehicles in complex environments.", "AI": {"tldr": "本文提出了MapDiffusion，一种新的生成方法，用于提高自主驾驶系统中在线矢量化高清地图构建的不确定性和鲁棒性，同时提高性能。", "motivation": "传统的地图构建模型提供确定性的点估计，无法捕捉不确定性及真实世界环境中的固有模糊性，如遮挡和缺少车道标记。", "method": "MapDiffusion提出了一种新的生成方法，利用扩散范式从学习到的BEV潜在网格条件下来迭代细化随机初始化的查询，生成多个可能的地图样本。", "result": "在nuScenes数据集上的广泛实验表明，MapDiffusion在线地图构建方面达到了最先进的性能，基线模型的单个样本性能提高了5%，并且多个样本的一致改进验证了分布建模的好处。", "conclusion": "通过建模整个地图分布，MapDiffusion增强了在线矢量化高清地图构建的鲁棒性和可靠性，使自动驾驶汽车在复杂环境中实现不确定性感知决策。"}}
{"id": "2507.21168", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.0"], "pdf": "https://arxiv.org/pdf/2507.21168", "abs": "https://arxiv.org/abs/2507.21168", "authors": ["Rafael Rosales", "Santiago Miret"], "title": "Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question", "comment": null, "summary": "Effectively leveraging diversity has been shown to improve performance for\nvarious machine learning models, including large language models (LLMs).\nHowever, determining the most effective way of using diversity remains a\nchallenge. In this work, we compare two diversity approaches for answering\nbinary questions using LLMs: model diversity, which relies on multiple models\nanswering the same question, and question interpretation diversity, which\nrelies on using the same model to answer the same question framed in different\nways. For both cases, we apply majority voting as the ensemble consensus\nheuristic to determine the final answer. Our experiments on boolq, strategyqa,\nand pubmedqa show that question interpretation diversity consistently leads to\nbetter ensemble accuracy compared to model diversity. Furthermore, our analysis\nof GPT and LLaMa shows that model diversity typically produces results between\nthe best and the worst ensemble members without clear improvement.", "AI": {"tldr": "本文研究了两种多样性方法在大型语言模型中的应用，实验表明问题解释多样性优于模型多样性。", "motivation": "多样性已被证明可以提高包括大型语言模型在内的各种机器学习模型的性能，但确定最有效的多样性使用方式仍然具有挑战性。本文旨在探讨在二元问题回答上，模型多样性与问题解释多样性两种策略的有效性。", "method": "本文通过比较两种多样性方法来回答二元问题，一种是模型多样性，即使用多个模型回答同一个问题；另一种是问题解释多样性，即使用相同的模型以不同的方式回答同一个问题。在这两种情况下，都采用了多数投票作为集成共识机制来确定最终答案。", "result": "实验结果表明，问题解释多样性相比模型多样性，在boolq、strategyqa和pubmedqa数据集上，前者一贯地展现出更高的集成准确性。此外，对GPT和LLaMa的分析表明，模型多样性通常会产生介于最好和最差成员之间的结果，并无明显改进。", "conclusion": "研究表明，对于回答二元问题，问题解释多样性比模型多样性更有效。模型多样性在实验中并没有带来明显的性能提升。"}}
{"id": "2507.21440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21440", "abs": "https://arxiv.org/abs/2507.21440", "authors": ["Han Wu", "Chong Wang", "Zhiming Cui"], "title": "Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling for Semi-supervised Medical Image Segmentation", "comment": "IEEE TMI", "summary": "Semi-supervised learning has proven highly effective in tackling the\nchallenge of limited labeled training data in medical image segmentation. In\ngeneral, current approaches, which rely on intra-image pixel-wise consistency\ntraining via pseudo-labeling, overlook the consistency at more comprehensive\nsemantic levels (e.g., object region) and suffer from severe discrepancy of\nextracted features resulting from an imbalanced number of labeled and unlabeled\ndata. To overcome these limitations, we present a new \\underline{Du}al\n\\underline{C}ross-\\underline{i}mage \\underline{S}emantic\n\\underline{C}onsistency (DuCiSC) learning framework, for semi-supervised\nmedical image segmentation. Concretely, beyond enforcing pixel-wise semantic\nconsistency, DuCiSC proposes dual paradigms to encourage region-level semantic\nconsistency across: 1) labeled and unlabeled images; and 2) labeled and fused\nimages, by explicitly aligning their prototypes. Relying on the dual paradigms,\nDuCiSC can effectively establish consistent cross-image semantics via prototype\nrepresentations, thereby addressing the feature discrepancy issue. Moreover, we\ndevise a novel self-aware confidence estimation strategy to accurately select\nreliable pseudo labels, allowing for exploiting the training dynamics of\nunlabeled data. Our DuCiSC method is extensively validated on four datasets,\nincluding two popular binary benchmarks in segmenting the left atrium and\npancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a\nchallenging scenario of segmenting the inferior alveolar nerve that features\ncomplicated anatomical structures, showing superior segmentation results over\nprevious state-of-the-art approaches. Our code is publicly available at\n\\href{https://github.com/ShanghaiTech-IMPACT/DuCiSC}{https://github.com/ShanghaiTech-IMPACT/DuCiSC}.", "AI": {"tldr": "提出一种名为DuCiSC的框架，用于解决现有半监督学习方法在医学图像分割中存在的问题，例如像素级一致性训练、跨图区域一致性及标签与未标签数据不平衡产生的特征差异问题。结果显示该框架在多个数据集上的分割效果优于现有方法。", "motivation": "当前的半监督学习方法在处理医学图像分割时，主要依赖于通过伪标记实施的像素级一致性训练，但忽视了更全面的语义级别的一致性（如对象区域），并且受标签和未标签数据不平衡的影响，提取的特征有许多差异。为解决这些问题，研究提出了这种新的框架。", "method": "提出了一种名为DuCiSC（Dual Cross-image Semantic Consistency）的框架，用于半监督医学图像分割。该框架不仅强化了像素级别的语义一致性，还通过两个范式来鼓励图像间区域级别的语义一致性。这两个范式分别用于标签图像和未标签图像以及标签图像和融合图像之间的语义对齐，以此来解决由于标签和未标签数据不平衡所导致的特征差异问题。此外，还设计了一种新的自我感知置信度估计策略来准确筛选可靠的伪标签，从而利用未标签数据的训练动态。", "result": "该方法在四个数据集上进行了广泛验证，包括两个流行的二分类基准数据集用于分割左心房和胰腺、一个多类的自动心脏诊断挑战数据集和一个分割复杂的解剖结构中下牙槽神经的具有挑战性的情景，结果表明优于现有的最先进的方法。", "conclusion": "研究表明，DuCiSC框架能够有效处理医学图像分割中的半监督学习问题，通过区域级别的语义一致性增强了分割效果，并且在多个数据集上优于现有的最先进的方法。代码已公开。"}}
{"id": "2507.21186", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21186", "abs": "https://arxiv.org/abs/2507.21186", "authors": ["Sungmin Han", "Jeonghyun Lee", "Sangkyun Lee"], "title": "Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers", "comment": null, "summary": "Transformers have profoundly influenced AI research, but explaining their\ndecisions remains challenging -- even for relatively simpler tasks such as\nclassification -- which hinders trust and safe deployment in real-world\napplications. Although activation-based attribution methods effectively explain\ntransformer-based text classification models, our findings reveal that these\nmethods can be undermined by class-irrelevant features within activations,\nleading to less reliable interpretations. To address this limitation, we\npropose Contrast-CAT, a novel activation contrast-based attribution method that\nrefines token-level attributions by filtering out class-irrelevant features. By\ncontrasting the activations of an input sequence with reference activations,\nContrast-CAT generates clearer and more faithful attribution maps. Experimental\nresults across various datasets and models confirm that Contrast-CAT\nconsistently outperforms state-of-the-art methods. Notably, under the MoRF\nsetting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds\nover the most competing methods, demonstrating its effectiveness in enhancing\ninterpretability for transformer-based text classification.", "AI": {"tldr": "论文提出了一种改进的解释神经网络模型决策的方法，名为Contrast-CAT，它通过对比激活提高了神经网络模型的可解释性。", "motivation": "激活归因方法虽然能够有效地解释基于Transformer的文本分类模型，但这些方法可能受到激活中类别无关特征的影响，导致解释不可靠。为此，作者提出了一种改进的归因方法。", "method": "Contrast-CAT, 一种基于激活对比的归因方法，通过将输入序列的激活与参考激活对比，过滤出与类别无关的特征，生成更清晰、更忠实的归因图。", "result": "实验结果表明，Contrast-CAT在不同的数据集和模型上普遍优于现有方法，在MoRF设置下的AOPC和LOdds分别平均提高了1.30和2.25倍。", "conclusion": "Contrast-CAT的引入显著提升了基于Transformer的文本分类模型的可解释性。"}}
{"id": "2507.21450", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21450", "abs": "https://arxiv.org/abs/2507.21450", "authors": ["Bolei Chen", "Jiaxu Kang", "Yifei Wang", "Ping Zhong", "Qi Wu", "Jianxin Wang"], "title": "Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation", "comment": "Submitted to AAAI 2026", "summary": "Vision Language Navigation (VLN) typically requires agents to navigate to\nspecified objects or remote regions in unknown scenes by obeying linguistic\ncommands. Such tasks require organizing historical visual observations for\nlinguistic grounding, which is critical for long-sequence navigational\ndecisions. However, current agents suffer from overly detailed scene\nrepresentation and ambiguous vision-language alignment, which weaken their\ncomprehension of navigation-friendly high-level scene priors and easily lead to\nbehaviors that violate linguistic commands. To tackle these issues, we propose\na navigation policy by recursively summarizing along-the-way visual\nperceptions, which are adaptively aligned with commands to enhance linguistic\ngrounding. In particular, by structurally modeling historical trajectories as\ncompact neural grids, several Recursive Visual Imagination (RVI) techniques are\nproposed to motivate agents to focus on the regularity of visual transitions\nand semantic scene layouts, instead of dealing with misleading geometric\ndetails. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to\nalign the learned situational memories with different linguistic components\npurposefully. Such fine-grained semantic matching facilitates the accurate\nanticipation of navigation actions and progress. Our navigation policy\noutperforms the state-of-the-art methods on the challenging VLN-CE and\nObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.", "AI": {"tldr": "提出了递归视觉想象（RVI）技术和自适应语言对齐（ALG）方法，通过递归总结路径上的视觉感知并进行语言对齐，提高了Vision Language Navigation (VLN)任务中的导航准确性。", "motivation": "现有模型在VLN中面临过于详尽的场景表示和语言-视觉对齐的模糊性问题，导致对导航有利的高层次场景数据分析不足和违反语言指令的行为。", "method": "通过将历史轨迹建模为紧凑的神经网格并引入RVI技术，使代理关注视觉转换的规律性和语义场景布局；此外，提出ALG方法使学到的情景记忆与不同的语言成分精确匹配，以提升导航操作的准确性。", "result": "在具有挑战性的VLN-CE和ObjectNav任务中，新提出的导航策略优于现有最先进技术。", "conclusion": "研究显示，RVI和ALG技术改进了VLN中的语境理解和导航决策。"}}
{"id": "2507.21234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21234", "abs": "https://arxiv.org/abs/2507.21234", "authors": ["Fatema Binte Hassan", "Md Al Jubair", "Mohammad Mehadi Hasan", "Tahmid Hossain", "S M Mehebubur Rahman Khan Shuvo", "Mohammad Shamsul Arefin"], "title": "Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability", "comment": null, "summary": "In recent years, social media platforms have become prominent spaces for\nindividuals to express their opinions on ongoing events, including criminal\nincidents. As a result, public sentiment can shift dynamically over time. This\nstudy investigates the evolving public perception of crime-related news by\nclassifying user-generated comments into three categories: positive, negative,\nand neutral. A newly curated dataset comprising 28,528 Bangla-language social\nmedia comments was developed for this purpose. We propose a transformer-based\nmodel utilizing the XLM-RoBERTa Base architecture, which achieves a\nclassification accuracy of 97%, outperforming existing state-of-the-art methods\nin Bangla sentiment analysis. To enhance model interpretability, explainable AI\ntechnique is employed to identify the most influential features driving\nsentiment classification. The results underscore the effectiveness of\ntransformer-based models in processing low-resource languages such as Bengali\nand demonstrate their potential to extract actionable insights that can support\npublic policy formulation and crime prevention strategies.", "AI": {"tldr": "本文发展了一种基于XLM-RoBERTa的模型进行孟加拉语情感分析，分类准确率为97%，优于现有方法。研究展示了该模型在公众观点分析中提取行动性见解的潜力，可用于公共政策和犯罪预防。", "motivation": "本文旨在研究公众对犯罪相关新闻情感的变化，特别是在社交媒体平台上公众观点的演变。", "method": "本研究利用基于XLM-RoBERTa Base架构的Transformer模型，对包含28,528个孟加拉语社交媒体评论的新数据集进行情感分类，将其划分为正面、负面和中性三类。", "result": "所提出的模型在孟加拉语情感分析中达到了97%的分类准确率，超过了现有的最先进的方法，证明了在处理像孟加拉语这样的低资源语言方面，基于Transformer的模型的有效性。为了增强模型的可解释性，研究还采用了可解释的人工智能技术来识别对情感分类最具影响力的特征。", "conclusion": "研究结果表明Transformer模型在处理像孟加拉语这样的低资源语言方面的有效性，并展示了其提取行动性见解的潜力，以支持公共政策制定和犯罪预防策略。"}}
{"id": "2507.21455", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21455", "abs": "https://arxiv.org/abs/2507.21455", "authors": ["Sheng-Feng Yu", "Jia-Jiun Yao", "Wei-Chen Chiu"], "title": "Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation", "comment": null, "summary": "Although larger datasets are crucial for training large deep models, the\nrapid growth of dataset size has brought a significant challenge in terms of\nconsiderable training costs, which even results in prohibitive computational\nexpenses. Dataset Distillation becomes a popular technique recently to reduce\nthe dataset size via learning a highly compact set of representative exemplars,\nwhere the model trained with these exemplars ideally should have comparable\nperformance with respect to the one trained with the full dataset. While most\nof existing works upon dataset distillation focus on supervised datasets, we\ninstead aim to distill images and their self-supervisedly trained\nrepresentations into a distilled set. This procedure, named as Self-Supervised\nDataset Distillation, effectively extracts rich information from real datasets,\nyielding the distilled sets with enhanced cross-architecture generalizability.\nParticularly, in order to preserve the key characteristics of original dataset\nmore faithfully and compactly, several novel techniques are proposed: 1) we\nintroduce an innovative parameterization upon images and representations via\ndistinct low-dimensional bases, where the base selection for parameterization\nis experimentally shown to play a crucial role; 2) we tackle the instability\ninduced by the randomness of data augmentation -- a key component in\nself-supervised learning but being underestimated in the prior work of\nself-supervised dataset distillation -- by utilizing predetermined\naugmentations; 3) we further leverage a lightweight network to model the\nconnections among the representations of augmented views from the same image,\nleading to more compact pairs of distillation. Extensive experiments conducted\non various datasets validate the superiority of our approach in terms of\ndistillation efficiency, cross-architecture generalization, and transfer\nlearning performance.", "AI": {"tldr": "本文提出了一种自监督数据集蒸馏方法，通过引入低维基参数化、预定数据增强和轻量级网络建模，能够更高效且紧凑地蒸馏自监督数据集，实验验证了该方法的有效性。", "motivation": "随着数据集规模的快速增长，训练大型深度模型的计算成本变得非常高昂。本文旨在通过蒸馏图像及其自监督训练的表示来减轻这一问题，特别是为了比现有方法更真实和紧凑地保留原始数据集的关键特性。", "method": "本文提出了一种名为自监督数据集蒸馏的方法，它通过引入图像和表示的不同低维基的创新参数化来提取原始数据集的关键特性，使用预定的数据增强来应对数据增强随机性引起的不稳定性，并利用轻量级网络建模同一图像的增强视图的表示之间的联系，从而实现更紧凑的蒸馏对。", "result": "实验在多个数据集上验证了本文方法在蒸馏效率、跨架构泛化能力和迁移学习性能上的优越性。", "conclusion": "本文的方法能够在保持模型性能的同时，大幅减少所需数据集的大小，从而降低计算成本，并在跨架构泛化和迁移学习方面具有显著优势。"}}
{"id": "2507.21242", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21242", "abs": "https://arxiv.org/abs/2507.21242", "authors": ["Mohammad Mehadi Hasan", "Fatema Binte Hassan", "Md Al Jubair", "Zobayer Ahmed", "Sazzatul Yeakin", "Md Masum Billah"], "title": "Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach", "comment": null, "summary": "In the current digital landscape, misinformation circulates rapidly, shaping\npublic perception and causing societal divisions. It is difficult to identify\nhyperpartisan news in Bangla since there aren't many sophisticated natural\nlanguage processing methods available for this low-resource language. Without\neffective detection methods, biased content can spread unchecked, posing\nserious risks to informed discourse. To address this gap, our research\nfine-tunes Bangla BERT. This is a state-of-the-art transformer-based model,\ndesigned to enhance classification accuracy for hyperpartisan news. We evaluate\nits performance against traditional machine learning models and implement\nsemi-supervised learning to enhance predictions further. Not only that, we use\nLIME to provide transparent explanations of the model's decision-making\nprocess, which helps to build trust in its outcomes. With a remarkable accuracy\nscore of 95.65%, Bangla BERT outperforms conventional approaches, according to\nour trial data. The findings of this study demonstrate the usefulness of\ntransformer models even in environments with limited resources, which opens the\ndoor to further improvements in this area.", "AI": {"tldr": "本研究通过微调孟加拉语BERT模型来精确检测偏见新闻，实现了高准确率，为低资源语言环境下的自然语言处理提供了新方法。", "motivation": "由于缺乏针对孟加拉语的高级自然语言处理方法，使得辨别偏见新闻存在困难，进而导致误导性的信息广泛传播，该研究旨在解决这一问题。", "method": "通过微调针对孟加拉语的BERT模型来提高辨别偏见新闻的分类准确性，同时使用半监督学习提升预测效果，并应用LIME为模型决策提供透明解释。", "result": "在试用数据中，经过微调的孟加拉语BERT模型达到了95.65%的准确率，优于传统方法，证明了在资源较少的语言环境中使用变压器模型的有效性。", "conclusion": "研究展示了变压器模型在低资源语言中的有效性，从而为这一领域的进一步改进奠定了基础。"}}
{"id": "2507.21460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21460", "abs": "https://arxiv.org/abs/2507.21460", "authors": ["Mianzhao Wang", "Fan Shi", "Xu Cheng", "Feifei Zhang", "Shengyong Chen"], "title": "An Angular-Temporal Interaction Network for Light Field Object Tracking in Low-Light Scenes", "comment": null, "summary": "High-quality 4D light field representation with efficient angular feature\nmodeling is crucial for scene perception, as it can provide discriminative\nspatial-angular cues to identify moving targets. However, recent developments\nstill struggle to deliver reliable angular modeling in the temporal domain,\nparticularly in complex low-light scenes. In this paper, we propose a novel\nlight field epipolar-plane structure image (ESI) representation that explicitly\ndefines the geometric structure within the light field. By capitalizing on the\nabrupt changes in the angles of light rays within the epipolar plane, this\nrepresentation can enhance visual expression in low-light scenes and reduce\nredundancy in high-dimensional light fields. We further propose an\nangular-temporal interaction network (ATINet) for light field object tracking\nthat learns angular-aware representations from the geometric structural cues\nand angular-temporal interaction cues of light fields. Furthermore, ATINet can\nalso be optimized in a self-supervised manner to enhance the geometric feature\ninteraction across the temporal domain. Finally, we introduce a large-scale\nlight field low-light dataset for object tracking. Extensive experimentation\ndemonstrates that ATINet achieves state-of-the-art performance in single object\ntracking. Furthermore, we extend the proposed method to multiple object\ntracking, which also shows the effectiveness of high-quality light field\nangular-temporal modeling.", "AI": {"tldr": "本文提出了一种新颖的4D光场ESP表示方法和交互网络ATINet，它们通过利用极线平面内光线角度变化来增强低光场景中的视觉表现，并在物体跟踪任务中取得了先进性能。", "motivation": "高质量的4D光场表示与有效的角度特征建模对于场景感知至关重要，尤其是在复杂低光场景中的移动目标识别。然而，最近的发展在时间域内的角度建模可靠性方面仍然存在问题。", "method": "本研究提出了一个新颖的光场极线面结构图像（ESI）表示，它明确界定了光场中的几何结构，并利用极线平面内光线角度的突然变化来增强低光场景中的视觉表现，减少高维光场的空间冗余。另外，研究还提出了一种用于光场物体跟踪的角-时间交互网络（ATINet），该网络从光场的几何结构线索和角-时间交互线索中学习角度感知的表示，并可通过自我监督方式进行优化，以提高时间域内几何特征交互的质量。", "result": "实验表明，ATINet在单个物体跟踪中达到了最先进的性能，并且所提出的方法扩展到多物体跟踪中也显示了高质量光场角-时间建模的效果。", "conclusion": "基于高性能光场极线面结构图像（ESI）表示和角-时间交互网络（ATINet），研究人员表明他们的方法在物体跟踪中具有显著性能，并特别适用于低光场景。"}}
{"id": "2507.21302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21302", "abs": "https://arxiv.org/abs/2507.21302", "authors": ["Christoph Metzner", "Shang Gao", "Drahomira Herrmannova", "Heidi A. Hanson"], "title": "Can human clinical rationales improve the performance and explainability of clinical text classification models?", "comment": null, "summary": "AI-driven clinical text classification is vital for explainable automated\nretrieval of population-level health information. This work investigates\nwhether human-based clinical rationales can serve as additional supervision to\nimprove both performance and explainability of transformer-based models that\nautomatically encode clinical documents. We analyzed 99,125 human-based\nclinical rationales that provide plausible explanations for primary cancer site\ndiagnoses, using them as additional training samples alongside 128,649\nelectronic pathology reports to evaluate transformer-based models for\nextracting primary cancer sites. We also investigated sufficiency as a way to\nmeasure rationale quality for pre-selecting rationales. Our results showed that\nclinical rationales as additional training data can improve model performance\nin high-resource scenarios but produce inconsistent behavior when resources are\nlimited. Using sufficiency as an automatic metric to preselect rationales also\nleads to inconsistent results. Importantly, models trained on rationales were\nconsistently outperformed by models trained on additional reports instead. This\nsuggests that clinical rationales don't consistently improve model performance\nand are outperformed by simply using more reports. Therefore, if the goal is\noptimizing accuracy, annotation efforts should focus on labeling more reports\nrather than creating rationales. However, if explainability is the priority,\ntraining models on rationale-supplemented data may help them better identify\nrationale-like features. We conclude that using clinical rationales as\nadditional training data results in smaller performance improvements and only\nslightly better explainability (measured as average token-level rationale\ncoverage) compared to training on additional reports.", "AI": {"tldr": "研究发现，人类临床推理作为额外训练数据可提升模型性能（尤其是在资源充足的情况下），但在资源有限时效果不一致。此外，基于推理训练的模型在准确性上通常不如基于更多报告训练的模型。", "motivation": "研究人类临床推理作为额外监督以提高基于transformer的模型在编码临床文档时的性能和可解释性的潜力。", "method": "通过分析99,125个人类临床推理和128,649份电子病理报告来评估基于transformer的模型提取原发癌症部位的表现，并研究充分性作为预先选择推理的质量度量方法。", "result": "研究表明，在资源充足的情况下，临床推理作为额外训练数据可以提升模型性能，但在资源有限的情况下结果不一致。使用充分性衡量并不总是有效。", "conclusion": "使用临床推理作为额外训练数据能够带来较小的性能提升和略微更优的可解释性。然而，总的来说，它们不如使用更多的报告进行训练来得有效。"}}
{"id": "2507.21489", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21489", "abs": "https://arxiv.org/abs/2507.21489", "authors": ["Zhichuan Wang", "Yang Zhou", "Zhe Liu", "Rui Yu", "Song Bai", "Yulong Wang", "Xinwei He", "Xiang Bai"], "title": "Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval", "comment": "Accepted to ICCV 2025", "summary": "Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D\nobjects of unseen categories beyond the training set. Existing methods\ntypically utilize all modalities (i.e., voxels, point clouds, multi-view\nimages) and train specific backbones before fusion. However, they still\nstruggle to produce generalized representations due to insufficient 3D training\ndata. Being contrastively pre-trained on web-scale image-text pairs, CLIP\ninherently produces generalized representations for a wide range of downstream\ntasks. Building upon it, we present a simple yet effective framework named\nDescribe, Adapt and Combine (DAC) by taking only multi-view images for open-set\n3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large\nlanguage model (MLLM) to learn generalized 3D representations, where the MLLM\nis used for dual purposes. First, it describes the seen category information to\nalign with CLIP's training objective for adaptation during training. Second, it\nprovides external hints about unknown objects complementary to visual cues\nduring inference. To improve the synergy, we introduce an Additive-Bias\nLow-Rank adaptation (AB-LoRA), which alleviates overfitting and further\nenhances the generalization to unseen categories. With only multi-view images,\nDAC significantly surpasses prior arts by an average of +10.01\\% mAP on four\nopen-set 3DOR datasets. Moreover, its generalization is also validated on\nimage-based and cross-dataset setups. Code is available at\nhttps://github.com/wangzhichuan123/DAC.", "AI": {"tldr": "DAC框架结合CLIP和MLLM，利用多视角图像生成泛化的3D表示形式，实现了良好的开放集合3D对象检索效果，提高了对未知类别的泛化能力。", "motivation": "现有的3D对象检索方法无法生成泛化的表示形式，尤其是在3D训练数据不足的情况下。基于CLIP模型，作者提出了一种新的框架DAC，旨在使用多视角图像生成适用于开放集合下的3D对象检索的泛化表示。", "method": "DAC框架结合CLIP模型与多模态大语言模型（MLLM），利用多视角图像进行开放集合下的3D对象检索。在训练时，MLLM描述已知类别信息，以适应CLIP的训练目标；在推理时，提供未知对象的外部提示以补充视觉线索。此外，引入了Additive-Bias Low-Rank适应方法（AB-LoRA）以减轻过拟合并提高对未知类别的泛化能力。", "result": "DAC在四个开放集合3D对象检索数据集上平均mAP提高了10.01%，表现优于以往方法。而且，它的泛化性能也在基于图像和跨数据集的设置中得到验证。", "conclusion": "提出了一种结合CLIP和MLLM的简单而有效的3D对象检索框架DAC，证明了这种方法在开放集合3D对象检索任务中的有效性和泛化能力。"}}
{"id": "2507.21319", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21319", "abs": "https://arxiv.org/abs/2507.21319", "authors": ["Hadi Mohammadi", "Yasmeen F. S. S. Meijer", "Efthymia Papadopoulou", "Ayoub Bagheri"], "title": "Do Large Language Models Understand Morality Across Cultures?", "comment": null, "summary": "Recent advancements in large language models (LLMs) have established them as\npowerful tools across numerous domains. However, persistent concerns about\nembedded biases, such as gender, racial, and cultural biases arising from their\ntraining data, raise significant questions about the ethical use and societal\nconsequences of these technologies. This study investigates the extent to which\nLLMs capture cross-cultural differences and similarities in moral perspectives.\nSpecifically, we examine whether LLM outputs align with patterns observed in\ninternational survey data on moral attitudes. To this end, we employ three\ncomplementary methods: (1) comparing variances in moral scores produced by\nmodels versus those reported in surveys, (2) conducting cluster alignment\nanalyses to assess correspondence between country groupings derived from LLM\noutputs and survey data, and (3) directly probing models with comparative\nprompts using systematically chosen token pairs. Our results reveal that\ncurrent LLMs often fail to reproduce the full spectrum of cross-cultural moral\nvariation, tending to compress differences and exhibit low alignment with\nempirical survey patterns. These findings highlight a pressing need for more\nrobust approaches to mitigate biases and improve cultural representativeness in\nLLMs. We conclude by discussing the implications for the responsible\ndevelopment and global deployment of LLMs, emphasizing fairness and ethical\nalignment.", "AI": {"tldr": "本研究调查了LLMs在捕捉跨文化道德观点方面的表现，发现这些模型往往不能精确反映真实世界中的道德多样性，显示出对模型的改进和减少文化偏见的迫切需求。", "motivation": "由于LLMs潜在嵌入的偏见（如性别、种族和文化偏见）而引发了关于这些技术的伦理使用和社会后果的重大质疑，因此该研究的目的在于调查LLMs在捕捉跨文化道德观点的差异和相似性方面的表现。", "method": "本研究采用了三种方法来调查大型语言模型（LLMs）在捕捉跨文化道德观点的差异和相似性方面的表现：(1) 比较模型生成的道德分数的方差与国际调查数据报告的方差；(2) 进行聚类对齐分析，以评估从LLM输出国群组与调查数据之间的对应关系；(3) 使用系统选择的令牌对直接对模型提出对比询问。", "result": "研究发现，目前的LLMs往往不能再现跨文化道德变异的全部范围，倾向于压缩差异，并且与实证调查模式之间的对齐度较低。", "conclusion": "这些发现突显了需要更加强大的方法来减少偏见并提高LLMs的文化代表性。最后，研究讨论了LLMs负责任发展和全球部署的公平性和伦理一致性的重要意义。"}}
