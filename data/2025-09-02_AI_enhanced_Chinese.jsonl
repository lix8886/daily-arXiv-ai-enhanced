{"id": "2508.21080", "categories": ["cs.CV", "cs.RO", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.21080", "abs": "https://arxiv.org/abs/2508.21080", "authors": ["Ali K. AlShami", "Ryan Rabinowitz", "Maged Shoman", "Jianwu Fang", "Lukas Picek", "Shao-Yuan Lo", "Steve Cruz", "Khang Nhut Lam", "Nachiket Kamod", "Lei-Lei Li", "Jugal Kalita", "Terrance E. Boult"], "title": "2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving", "comment": "11 pages, 2 figures, Accepted to ICCV 2025 Workshop on Out-of-Label\n  Hazards in Autonomous Driving (2COOOL)", "summary": "As the computer vision community advances autonomous driving algorithms,\nintegrating vision-based insights with sensor data remains essential for\nimproving perception, decision making, planning, prediction, simulation, and\ncontrol. Yet we must ask: Why don't we have entirely safe self-driving cars\nyet? A key part of the answer lies in addressing novel scenarios, one of the\nmost critical barriers to real-world deployment. Our 2COOOL workshop provides a\ndedicated forum for researchers and industry experts to push the state of the\nart in novelty handling, including out-of-distribution hazard detection,\nvision-language models for hazard understanding, new benchmarking and\nmethodologies, and safe autonomous driving practices. The 2nd Workshop on the\nChallenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held\nat the International Conference on Computer Vision (ICCV) 2025 in Honolulu,\nHawaii, on October 19, 2025. We aim to inspire the development of new\nalgorithms and systems for hazard avoidance, drawing on ideas from anomaly\ndetection, open-set recognition, open-vocabulary modeling, domain adaptation,\nand related fields. Building on the success of its inaugural edition at the\nWinter Conference on Applications of Computer Vision (WACV) 2025, the workshop\nwill feature a mix of academic and industry participation.", "AI": {"tldr": "第2届2COOOL研讨会将于2025年在夏威夷举行，旨在推动自动驾驶技术在处理未知场景和危险识别方面的研究。", "motivation": "研讨会的动机在于解决自动驾驶技术在实际部署中遇到的未知场景问题，即遇到未见过的危险情况的处理能力不足。", "method": "该摘要描述了一个专注于处理自动驾驶中未知场景挑战的研讨会，但并未详细说明具体的方法。", "result": "该研讨会旨在成为一个跨学科交流平台，但具体的研究成果及实验结果未提及。", "conclusion": "通过此次研讨会，希望能促进新算法和系统的开发，提高自动驾驶车辆在面对未知危险时的识别和规避能力。"}}
{"id": "2508.21088", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21088", "abs": "https://arxiv.org/abs/2508.21088", "authors": ["Alireza Golkarieh", "Kiana Kiashemshaki", "Sajjad Rezvani Boroujeni"], "title": "Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images", "comment": "14 pages, 8 figures, 8 tables", "summary": "This study investigates deep learning methods for automated classification of\ndental conditions in panoramic X-ray images. A dataset of 1,512 radiographs\nwith 11,137 expert-verified annotations across four conditions fillings,\ncavities, implants, and impacted teeth was used. After preprocessing and class\nbalancing, three approaches were evaluated: a custom convolutional neural\nnetwork (CNN), hybrid models combining CNN feature extraction with traditional\nclassifiers, and fine-tuned pre-trained architectures. Experiments employed 5\nfold cross validation with accuracy, precision, recall, and F1 score as\nevaluation metrics. The hybrid CNN Random Forest model achieved the highest\nperformance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.\nAmong pre-trained models, VGG16 performed best at 82.3% accuracy, followed by\nXception and ResNet50. Results show that hybrid models improve discrimination\nof morphologically similar conditions and provide efficient, reliable\nperformance. These findings suggest that combining CNN-based feature extraction\nwith ensemble classifiers offers a practical path toward automated dental\ndiagnostic support, while also highlighting the need for larger datasets and\nfurther clinical validation.", "AI": {"tldr": "研究通过评估三种深度学习方法发现，结合CNN特征提取与传统分类器的混合模型在全景X光影像牙齿状况分类任务上表现最佳。", "motivation": "这篇论文的动机在于探究基于深度学习的方法用于自动分类全景X光图像的牙齿状况。", "method": "本研究评估了三种方法：自定义卷积神经网络(CNN)，结合CNN特征提取与传统分类器的混合模型，以及微调的预训练架构。", "result": "实验结果显示，混合CNN随机森林模型达到了最高的性能，准确率为85.4%，超过了自定义CNN基线模型的74.3%。在预训练模型中，VGG16表现出最优性能，准确率为82.3%，其次是Xception和ResNet50。", "conclusion": "研究结果表明，混合模型提升了对形态相似的牙齿状况的区分能力，并提供了一种高效、可靠的表现。这些发现表明，结合基于CNN的特征提取和集成分类器为实现自动化的牙齿诊断提供了一条实用路径，同时也强调了需要更大的数据集和进一步的临床验证。"}}
{"id": "2508.21090", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21090", "abs": "https://arxiv.org/abs/2508.21090", "authors": ["Namu Kim", "Wonbin Kweon", "Minsoo Kim", "Hwanjo Yu"], "title": "Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment", "comment": null, "summary": "We observe that zero-shot appearance transfer with large-scale image\ngeneration models faces a significant challenge: Attention Leakage. This\nchallenge arises when the semantic mapping between two images is captured by\nthe Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing\nQuery-Query alignment to mitigate attention leakage and improve the semantic\nalignment in zero-shot appearance transfer. Q-Align incorporates three core\ncontributions: (1) Query-Query alignment, facilitating the sophisticated\nspatial semantic mapping between two images; (2) Key-Value rearrangement,\nenhancing feature correspondence through realignment; and (3) Attention\nrefinement using rearranged keys and values to maintain semantic consistency.\nWe validate the effectiveness of Q-Align through extensive experiments and\nanalysis, and Q-Align outperforms state-of-the-art methods in appearance\nfidelity while maintaining competitive structure preservation.", "AI": {"tldr": "Q-Align is introduced to mitigate attention leakage in zero-shot appearance transfer through Query-Query alignment, Key-Value rearrangement, and Attention refinement, exhibiting superior appearance fidelity and structure preservation.", "motivation": "The paper aims to address the challenge of Attention Leakage in zero-shot appearance transfer with large-scale image generation models to improve semantic alignment.", "method": "Query-Query alignment, Key-Value rearrangement, and Attention refinement are the core techniques of Q-Align to solve the attention leakage issue in zero-shot appearance transfer.", "result": "Experiments and analysis validate Q-Align's effectiveness, showing superior appearance fidelity and competitive structure preservation over state-of-the-art methods.", "conclusion": "The proposed Q-Align method is shown to be effective in mitigating attention leakage and enhancing semantic alignment in zero-shot appearance transfer tasks."}}
{"id": "2508.21091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21091", "abs": "https://arxiv.org/abs/2508.21091", "authors": ["Xurui Peng", "Hong Liu", "Chenqian Yan", "Rui Ma", "Fangmin Chen", "Xing Wang", "Zhihua Wu", "Songwei Liu", "Mingbao Lin"], "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion", "comment": null, "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.", "AI": {"tldr": "ERTACache is a caching framework for diffusion models that analyzes and mitigates caching errors, achieving 2x speedup in inference while preserving visual quality in tasks like image and video generation.", "motivation": "The motivation stems from the computational inefficiency of diffusion models due to their iterative inference process. Feature caching can speed up the process, but naive reuse can lead to quality degradation. ERTACache aims to address this by formally analyzing and mitigating caching errors.", "method": "In this paper, the authors propose ERTACache, a caching framework for diffusion models. ERTACache analyzes and rectifies the cumulative error from feature caching, decomposing it into feature shift error and step amplification error. The method includes an offline residual profiling stage, dynamic integration interval adjustment using a trajectory-aware correction coefficient, and an analytical approximation of cache-induced errors using a residual linearization model.", "result": "ERTACache achieves significant speedups of up to 2x for inference, while also maintaining or improving visual quality and fidelity in image and video generation tasks.", "conclusion": "The paper concludes that ERTACache offers a robust caching strategy that accelerates inference in diffusion models without sacrificing quality, as demonstrated by its performance across various benchmarks."}}
{"id": "2508.21083", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21083", "abs": "https://arxiv.org/abs/2508.21083", "authors": ["Kyohoon Jin", "Juhwan Choi", "Jungmin Yun", "Junho Lee", "Soojin Jang", "Youngbin Kim"], "title": "CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples", "comment": "Accepted at EMNLP 2025", "summary": "Deep learning models often learn and exploit spurious correlations in\ntraining data, using these non-target features to inform their predictions.\nSuch reliance leads to performance degradation and poor generalization on\nunseen data. To address these limitations, we introduce a more general form of\ncounterfactual data augmentation, termed counterbias data augmentation, which\nsimultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and\nenhances out-of-distribution robustness. We present CoBA: CounterBias\nAugmentation, a unified framework that operates at the semantic triple level:\nfirst decomposing text into subject-predicate-object triples, then selectively\nmodifying these triples to disrupt spurious correlations. By reconstructing the\ntext from these adjusted triples, CoBA generates counterbias data that\nmitigates spurious patterns. Through extensive experiments, we demonstrate that\nCoBA not only improves downstream task performance, but also effectively\nreduces biases and strengthens out-of-distribution resilience, offering a\nversatile and robust solution to the challenges posed by spurious correlations.", "AI": {"tldr": "CoBA框架通过文本的三元组级别数据增强，同时处理多种偏差，显著提高了模型的鲁棒性和泛化能力。", "motivation": "解决深度学习模型由于学习和利用训练数据中的非目标特征导致的性能下降和泛化能力差的问题。", "method": "通过将文本分解成主语-谓语-宾语三元组，然后选择性地修改这些三元组来打断非目标特征的关联，最后从调整后的三元组重构文本，从而生成对抗偏差数据，这种方法有助于缓解模型对非目标特征的依赖。", "result": "实验表明，CoBA不仅提高了下游任务的表现，还有效地减少了偏差并增强了对分布外数据的韧性。", "conclusion": "CoBA提供了一个多用途且健壮的解决方案，以应对由非目标特征关联带来的挑战。"}}
