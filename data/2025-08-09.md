<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

> 论文探索了一种通过元数据标签补充对话转录的方法，以增强说话者的特征属性，使用音频和语言模型结合的方法，在保持高效的同时取得了良好的说话者特征识别效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于通过增强对话转录中的元数据标签（如说话者年龄、性别和情绪）来提高对话质量，而不是仅仅依靠大型语言模型进行后处理以改善语法规则、标点和可读性。

**Method:** 该方法结合了冻结的音频基础模型（如Whisper或WavLM）和冻结的LLAMA语言模型，来推断这些说话者属性，且无需对任何模型进行特定任务的微调。通过轻量级、高效的连接器来桥接音频和语言表示。

**Result:** {야매 요약}

**Conclusion:** 研究结论显示这种方法在说话者特征识别任务上取得了有竞争力的表现，同时保持了模块化和速度。此外，还表明冻结的LLAMA模型可以直接比较x-vectors，在某些场景下等误率（Equal Error Rate）为8.8%。

**Abstract:** In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [2] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

> 提出了Parity-aware BPE算法，以确保低资源语言的标记化质量和高资源语言相当，实现跨语言的公平性，对于语言模型的全局性能基本没有影响。

<details>
  <summary>Details</summary>

**Motivation:** 标准的分词算法依赖于基于频率的目标，这有利于训练数据中占主导地位的语言，并且会导致低资源语言的标记化问题。这种现象加剧了来自不同语言背景的用户之间的计算和财务不平等。为了改善这种情况，提出了新的分词方法。

**Method:** 提出了一种名为Parity-aware Byte Pair Encoding (BPE) 的变体，该算法在每个合并步骤中最大化当前压缩率最差的语言的压缩增益，以换取少量的全局压缩率，从而实现跨语言的均衡。

**Result:** 实验证明，Parity-aware BPE 能够在不影响全局压缩率和下游任务语言模型性能的情况下，实现跨语言更均衡的标记数量。

**Conclusion:** Parity-aware BPE 能够在不显著影响全局压缩率和语言模型性能的前提下，实现跨语言更均衡的标记化。这一发现表明在NLP的初期阶段也需要关注公平性。

**Abstract:** Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration](https://arxiv.org/abs/2508.04797)
*Mohab Kishawy,Ali Abdellatif Hussein,Jun Chen*

Main category: cs.CV

> RetinexDual是一种新的UHD图像恢复框架，针对传统方法的局限性，通过互补的子网络结构提供更有效的图像恢复解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 传统的极端下采样或空间到频率域转换方法在UHD图像恢复中存在明显缺陷。下采样会导致信息不可逆损失，而仅采用频率域方法未能有效处理局部图像失真。RetinexDual旨在克服这些限制。

**Method:** RetinexDual采用基于Retinex理论的框架，包含两个互补子网络：Scale-Attentive maMBA（SAMBA）和Frequency Illumination Adaptor（FIA）。SAMBA通过粗到细的机制来修正反射成分，而FIA在频率域中操作，利用全局上下文信息精确修正颜色和照明失真。

**Result:** 在四种UHD图像恢复任务（去雨、去模糊、去雾和低光图像增强）的评估中，RetinexDual的性能优于近期方法，无论是定性还是定量评估。

**Conclusion:** 实验结果表明，RetinexDual的两个分支的设计和各个组件的有效性为其在UHD图像恢复任务上优于其他方法做出了重要贡献。

**Abstract:** Advancements in image sensing have elevated the importance of
Ultra-High-Definition Image Restoration (UHD IR). Traditional methods, such as
extreme downsampling or transformation from the spatial to the frequency
domain, encounter significant drawbacks: downsampling induces irreversible
information loss in UHD images, while our frequency analysis reveals that pure
frequency-domain approaches are ineffective for spatially confined image
artifacts, primarily due to the loss of degradation locality. To overcome these
limitations, we present RetinexDual, a novel Retinex theory-based framework
designed for generalized UHD IR tasks. RetinexDual leverages two complementary
sub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination
Adaptor (FIA). SAMBA, responsible for correcting the reflectance component,
utilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,
which effectively reduces artifacts and restores intricate details. On the
other hand, FIA ensures precise correction of color and illumination
distortions by operating in the frequency domain and leveraging the global
context provided by it. Evaluating RetinexDual on four UHD IR tasks, namely
deraining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows
that it outperforms recent methods qualitatively and quantitatively. Ablation
studies demonstrate the importance of employing distinct designs for each
branch in RetinexDual, as well as the effectiveness of its various components.

</details>


### [4] [ACM Multimedia Grand Challenge on ENT Endoscopy Analysis](https://arxiv.org/abs/2508.04801)
*Trong-Thuan Nguyen,Viet-Tham Huynh,Thao Thi Phuong Dao,Ha Nguyen Thi,Tien To Vu Thuy,Uyen Hanh Tran,Tam V. Nguyen,Thanh Dinh Le,Minh-Triet Tran*

Main category: cs.CV

> 本文介绍了ENTRep挑战赛，以解决ENT内窥镜图像自动化分析中的关键问题，强调其基准任务、提交协议标准化以及使用服务器端评分的公共和私人测试拆分的性能评估。

<details>
  <summary>Details</summary>

**Motivation:** 内窥镜图像的自动化分析在ENT护理中是一个重要但尚未充分开发的组成部分。这项工作受到了设备和操作员差异，以及细微和局部发现，特别是细微区别如侧别性和声带状态的挑战。现有公开基准很少支持分类以外的能力，例如检索类似病例，无论是在视觉上还是通过简洁的文本描述。

**Method:** 本文介绍了ENTRep，这是ACM Multimedia 2025的ENT内窥镜分析大赛，集成了细粒度的解剖结构分类与图像到图像和文本到图像的检索功能，并配备了双语（越南语和英语）临床监督。

**Result:** 文中报道了顶尖参赛队伍的表现，并提供了深入的讨论。

**Conclusion:** 通过ENTRep挑战赛，该研究强调了在ENT内窥镜图像的分析中，实现细粒度的解剖结构分类和双语临床监督下的图像及文本检索的重要性。

**Abstract:** Automated analysis of endoscopic imagery is a critical yet underdeveloped
component of ENT (ear, nose, and throat) care, hindered by variability in
devices and operators, subtle and localized findings, and fine-grained
distinctions such as laterality and vocal-fold state. In addition to
classification, clinicians require reliable retrieval of similar cases, both
visually and through concise textual descriptions. These capabilities are
rarely supported by existing public benchmarks. To this end, we introduce
ENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,
which integrates fine-grained anatomical classification with image-to-image and
text-to-image retrieval under bilingual (Vietnamese and English) clinical
supervision. Specifically, the dataset comprises expert-annotated images,
labeled for anatomical region and normal or abnormal status, and accompanied by
dual-language narrative descriptions. In addition, we define three benchmark
tasks, standardize the submission protocol, and evaluate performance on public
and private test splits using server-side scoring. Moreover, we report results
from the top-performing teams and provide an insight discussion.

</details>


### [5] [CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework](https://arxiv.org/abs/2508.04816)
*Sriram Mandalika,Lalitha V*

Main category: cs.CV

> 研究提出了一种轻量级、无参数框架CoMAD，它将多个自监督视觉变压器的知识合并到一个紧凑的学生网络中，适用于资源有限的部署，同时保证性能。

<details>
  <summary>Details</summary>

**Motivation:** 许多自监督学习范式，如对比学习和掩码图像建模，可以从未标记的数据中学习强大的表征，但它们通常是在孤立的情况下进行预训练的，忽略了一致的见解，并产生大型模型，适用于资源有限的部署的模型，从而产生这些挑战。因此引入了CoMAD框架。

**Method:** 提出了一种名为共识导向掩码蒸馏（CoMAD）的轻量级、无参数框架，它将多个当前最先进的自监督视觉变压器的知识统一到一个紧凑的学生网络中。CoMAD从三个预训练的ViT-Base教师模型（MAE、MoCo v3和iBOT）中蒸馏知识，并采用非对称掩码策略，学生只能看到25%的补丁，而每个教师接收不断变轻的独特掩码，这迫使学生在更丰富的上下文中插补缺失的特征。教师嵌入通过线性适配器和层归一化对齐到学生的空间中，然后通过联合共识门融合。学生通过具有双层KL散度的训练来捕捉局部和全局结构，对可见的补丁和重建的特征图进行训练。

**Result:** 在ImageNet-1K上，CoMAD的ViT-Tiny达到了75.4%的top-1准确率，比之前最先进的方法提高了0.4%。在密集预测转移中，它在ADE20K上达到了47.3%的mIoU，在MS-COCO上达到了44.5%的框平均精度和40.5%的掩码平均精度，建立了紧凑SSL蒸馏的新水平。

**Conclusion:** 该研究表明，通过多源知识蒸馏，可以创建一个轻量级、性能优越的自监督学习模型，无需参数调整，适用于资源受限环境。

**Abstract:** Numerous self-supervised learning paradigms, such as contrastive learning and
masked image modeling, learn powerful representations from unlabeled data but
are typically pretrained in isolation, overlooking complementary insights and
yielding large models that are impractical for resource-constrained deployment.
To overcome these challenges, we introduce Consensus-oriented Masked
Distillation (CoMAD), a lightweight, parameter-free framework that unifies
knowledge from multiple current state-of-the-art self-supervised Vision
Transformers into a compact student network. CoMAD distills from three
pretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct
semantic and contextual priors. Rather than naively averaging teacher outputs,
we apply asymmetric masking: the student sees only 25 percent of patches while
each teacher receives a progressively lighter, unique mask, forcing the student
to interpolate missing features under richer contexts. Teacher embeddings are
aligned to the student's space via a linear adapter and layer normalization,
then fused through our joint consensus gating, which weights each token by
combining cosine affinity with inter-teacher agreement. The student is trained
with dual-level KL divergence on visible tokens and reconstructed feature maps,
capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny
achieves 75.4 percent Top-1, an increment of 0.4 percent over the previous
state-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU
on ADE20K, and 44.5 percent box average precision and 40.5 percent mask average
precision on MS-COCO, establishing a new state-of-the-art in compact SSL
distillation.

</details>


### [6] [Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models](https://arxiv.org/abs/2508.04818)
*Mehrdad Moradi,Marco Grasso,Bianca Maria Colosimo,Kamran Paynabar*

Main category: cs.CV

> 本文介绍了一种名为RADAR的新方法，它是一种基于注意力的扩散模型，适用于实时无重构的异常检测，提高了检测的准确性与效率，并在多个数据集上超过现有的SOTA方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统基于重构的异常检测方法面临几个主要挑战：计算成本高、复杂模式的图像重构可能导致与原图像不同的正常模式以及选择合适的中间噪声水平困难。为此，作者提出了一种新的方法以克服这些挑战。

**Method:** 本文提出了一种名为RADAR的无重构异常检测方法，该方法基于注意力扩散模型，能够直接从扩散模型中生成异常图，从而提高了检测效率和准确性。不同于现有的SOTA方法，RADAR不需要重构输入图像，因此能够实现实时应用。

**Result:** RADAR方法在MVTec-AD数据集和3D打印材料数据集上的多项关键指标测试中超越了状态-of-the-art的扩散模型和统计机器学习模型，尤其是在F1评分上分别提高了7%和13%。

**Conclusion:** RADAR方法有效解决了传统重构方法存在的计算成本高、图像重构可能导致与原图像不同的正常模式及选择合适的中间噪声水平的问题，实现了更高的检测准确性和实时性。

**Abstract:** Generative models have demonstrated significant success in anomaly detection
and segmentation over the past decade. Recently, diffusion models have emerged
as a powerful alternative, outperforming previous approaches such as GANs and
VAEs. In typical diffusion-based anomaly detection, a model is trained on
normal data, and during inference, anomalous images are perturbed to a
predefined intermediate step in the forward diffusion process. The
corresponding normal image is then reconstructed through iterative reverse
sampling.
  However, reconstruction-based approaches present three major challenges: (1)
the reconstruction process is computationally expensive due to multiple
sampling steps, making real-time applications impractical; (2) for complex or
subtle patterns, the reconstructed image may correspond to a different normal
pattern rather than the original input; and (3) Choosing an appropriate
intermediate noise level is challenging because it is application-dependent and
often assumes prior knowledge of anomalies, an assumption that does not hold in
unsupervised settings.
  We introduce Reconstruction-free Anomaly Detection with Attention-based
diffusion models in Real-time (RADAR), which overcomes the limitations of
reconstruction-based anomaly detection. Unlike current SOTA methods that
reconstruct the input image, RADAR directly produces anomaly maps from the
diffusion model, improving both detection accuracy and computational
efficiency. We evaluate RADAR on real-world 3D-printed material and the
MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and
statistical machine learning models across all key metrics, including accuracy,
precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on
MVTec-AD and 13% on the 3D-printed material dataset compared to the next best
model.
  Code available at: https://github.com/mehrdadmoradi124/RADAR

</details>


### [7] [A deep learning approach to track eye movements based on events](https://arxiv.org/abs/2508.04827)
*Chirag Seth,Divya Naiken,Keyan Lin*

Main category: cs.CV

> 研究利用事件相机和深度学习方法开发了一种成本效益高的眼球追踪算法，以定位眼睛中心位置并预测人类注意力，以改善消费电子产品的用户体验。主要使用了CNN_LSTM模型，达到了约81%的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于眼球运动速度非常快，传统精确眼球跟踪通常需要昂贵和高速相机。该研究针对这一挑战，旨在开发低成本、高效率的眼球追踪技术，以进一步提升消费电子产品（特别是VR和AR产品）的用户体验。

**Method:** 本研究利用了事件相机输入，并探索了不同的模型方法，模型中以CNN_LSTM模型表现最为出色，达到了大约81%的准确率。未来还会采用Layer-wise Relevance Propagation（LRP）技术来提升模型的可解释性和预测能力。

**Result:** 该研究旨在利用事件相机输入来准确跟踪特定事件期间的眼球运动，目标是定位眼睛中心位置（x，y）。由于眼球运动速度可达到300°/s，通常需要昂贵且高速的相机才能实现精确的眼球跟踪。本研究利用深度学习方法，特别是CNN_LSTM模型，实现了约81%的准确率，目标是开发一个可解释且成本效益较高的算法来预测人类注意力，从而改善设备舒适性和用户体验。未来工作将集中在使用Layer-wise Relevance Propagation (LRP) 进一步增强模型的可解释性和预测性能。

**Conclusion:** 本研究通过深度学习方法，特别是CNN_LSTM模型，实现了低成本、高效率的眼球追踪算法，达到了81%的准确率，有助于改善消费电子产品（尤其是VR和AR产品）的用户体验。未来的研究将继续提升该模型的可解释性和预测性能。

**Abstract:** This research project addresses the challenge of accurately tracking eye
movements during specific events by leveraging previous research. Given the
rapid movements of human eyes, which can reach speeds of 300{\deg}/s, precise
eye tracking typically requires expensive and high-speed cameras. Our primary
objective is to locate the eye center position (x, y) using inputs from an
event camera. Eye movement analysis has extensive applications in consumer
electronics, especially in VR and AR product development. Therefore, our
ultimate goal is to develop an interpretable and cost-effective algorithm using
deep learning methods to predict human attention, thereby improving device
comfort and enhancing overall user experience. To achieve this goal, we
explored various approaches, with the CNN\_LSTM model proving most effective,
achieving approximately 81\% accuracy. Additionally, we propose future work
focusing on Layer-wise Relevance Propagation (LRP) to further enhance the
model's interpretability and predictive performance.

</details>
