{"id": "2508.00079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00079", "abs": "https://arxiv.org/abs/2508.00079", "authors": ["Oshayer Siddique", "J. M Areeb Uzair Alam", "Md Jobayer Rahman Rafy", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "comment": "Under review, 18 pages, 4 figures, 7 tables", "summary": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval.", "AI": {"tldr": "本文评估前沿LLMs解决物理问题的能力，并引入一种新的物理问题评估基准PHYSICEVAL，其中包含19,609道来自各种教科书的问题及答案。", "motivation": "本文旨在评估前沿语言模型（LLMs）解决物理问题的能力，并探索如何通过代理框架和推理技术提升这些模型的表现。", "method": "本文采用多种推理时的技术和代理框架来提升前沿LLMs解决物理问题（包括数学和描述性问题）的能力。具体的改进措施包括通过其他较小的LLM代理对提出的解决方案进行累积的验证。", "result": "应用多代理框架后，在模型原本表现不佳的问题上有了显著改善。", "conclusion": "通过使用多代理框架和其他推理时间技术，模型在解决物理问题上的性能有了显著提高，尤其是原本处理不佳的问题类型。此外，公开了代码和数据，以便进一步的研究。"}}
{"id": "2508.00086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00086", "abs": "https://arxiv.org/abs/2508.00086", "authors": ["Kelly Kendro", "Jeffrey Maloney", "Scott Jarvis"], "title": "Do LLMs produce texts with \"human-like\" lexical diversity?", "comment": "35 pages; includes abstract", "summary": "The degree to which LLMs produce writing that is truly human-like remains\nunclear despite the extensive empirical attention that this question has\nreceived. The present study addresses this question from the perspective of\nlexical diversity. Specifically, the study investigates patterns of lexical\ndiversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,\nand -4.5) in comparison with texts written by L1 and L2 English participants (n\n= 240) across four education levels. Six dimensions of lexical diversity were\nmeasured in each text: volume, abundance, variety-repetition, evenness,\ndisparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and\nSupport Vector Machines revealed that the LLM-generated texts differed\nsignificantly from human-written texts for each variable, with ChatGPT-o4 mini\nand -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated\nhigher levels of lexical diversity despite producing fewer tokens. The human\nwriters' lexical diversity did not differ across subgroups (i.e., education,\nlanguage status). Altogether, the results indicate that LLMs do not produce\nhuman-like texts in relation to lexical diversity, and the newer LLMs produce\nless human-like texts than older models. We discuss the implications of these\nresults for language pedagogy and related applications.", "AI": {"tldr": "研究发现语言生成模型（LLMs）生成的文本在词汇多样性上与人类书写存在显著差异，且较新的LLMs生成的文本比较旧的模型更不类似于人类的写作。", "motivation": "尽管这个问题收到了大量的实证关注，但LLMs所写出的文字是否真正如人类写出的一样，仍存在不确定性。此项研究旨在探讨这个问题。", "method": "本研究从词汇多样性视角探讨LLMs生成的文本是否真正与人类书写相似。具体来说，研究比较了四款ChatGPT模型（-3.5、-4、-o4迷你版和-4.5）生成的文本与240名英语母语者和非母语者在四个教育级别级别的文本，测量了每种文本的六个词汇多样性维度，包括量、丰度、多样性-重复、均匀性、差异和分散度。", "result": "研究结果显示，通过一元多变量方差分析（MANOVAs）、一元方差分析（ANOVAS）和支持向量机（SVMs），LLMs生成的文本与人类书写的文本在每个变量上均有显著差异，特别是在ChatGPT-o4迷你版和-4.5中差异最大。ChatGPT-4.5虽然产生的标记较少，但表现出更高的词汇多样性。人类作者的词汇多样性在不同子群体间没有显著差异。", "conclusion": "总体而言，结果表明LLMs生成的文本在词汇多样性方面并不像人类书写那样。事实上，更新的LLMs生成的文本比老模型更不具有人写文本的特点。我们讨论了这些结果对语言教学和相关应用的影响。"}}
{"id": "2508.00095", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00095", "abs": "https://arxiv.org/abs/2508.00095", "authors": ["Zachary K. Stine", "James E. Deitrick"], "title": "Semiotic Complexity and Its Epistemological Implications for Modeling Culture", "comment": "Preprint. Manuscript currently under review", "summary": "Greater theorizing of methods in the computational humanities is needed for\nepistemological and interpretive clarity, and therefore the maturation of the\nfield. In this paper, we frame such modeling work as engaging in translation\nwork from a cultural, linguistic domain into a computational, mathematical\ndomain, and back again. Translators benefit from articulating the theory of\ntheir translation process, and so do computational humanists in their work --\nto ensure internal consistency, avoid subtle yet consequential translation\nerrors, and facilitate interpretive transparency. Our contribution in this\npaper is to lay out a particularly consequential dimension of the lack of\ntheorizing and the sorts of translation errors that emerge in our modeling\npractices as a result. Along these lines we introduce the idea of semiotic\ncomplexity as the degree to which the meaning of some text may vary across\ninterpretive lenses, and make the case that dominant modeling practices --\nespecially around evaluation -- commit a translation error by treating\nsemiotically complex data as semiotically simple when it seems\nepistemologically convenient by conferring superficial clarity. We then lay out\nseveral recommendations for researchers to better account for these\nepistemological issues in their own work.", "AI": {"tldr": "本文指出计算人文科学在知识论和解释清晰度方面需要更多的理论化，特别是强调了符号复杂性概念，指出将复杂数据看作简单数据是一种翻译错误，并提出了应对这些知识论问题的方法。", "motivation": "本文旨在为计算人文科学领域的知识论和解释的清晰度提供更多的理论化思考，以促进该领域的发展。作者认为计算人文学者在工作中需要像翻译工作者那样明确其过程的理论性，以确保内部一致性、避免微妙但重要的翻译错误以及提高解释透明度。", "method": "本文通过将计算人文科学中的建模工作比作一种翻译工作，来探讨计算人文科学中理论化的不足和由此产生的翻译错误。具体来说，作者提出了‘符号复杂性’的概念，即文本意义在不同解释视角下变化的程度，并指出在评价过程中将符号复杂性数据视为符号简单数据的做法是一种翻译错误。", "result": "本文指出了当前建模实践中忽略符号复杂性数据被视为符号简单数据的翻译错误，并认为这种做法是为了方便获取表面上的清晰性。", "conclusion": "最后，作者提出了一些关于如何在研究中更好地考虑这些知识论问题的建议。"}}
{"id": "2508.00109", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00109", "abs": "https://arxiv.org/abs/2508.00109", "authors": ["Mingda Chen", "Yang Li", "Xilun Chen", "Adina Williams", "Gargi Ghosh", "Scott Yih"], "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality", "comment": null, "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.", "AI": {"tldr": "本文介绍了FACTORY，一种通过人为验证扩大规模的提示集，旨在提高长篇事实评估的准确性。实验表明，FACTORY对于最新的语言模型来说是一个具有挑战性的基准。", "motivation": "现有的基准测试往往缺乏人为验证，这可能导致质量问题。为了弥补这一不足，本文提出了一种新方法。", "method": "本文提出了一种大规模的人工验证提示集FACTORY，使用了模型循环方法并由人类精炼，包含了具有挑战性的提示，这些提示是事实性寻找的、可回答的和明确的。", "result": "通过使用FACTORY和现有数据集进行的人类评估表明，对于最先进的语言模型，大约40%的响应是在非事实的，而在其他数据集中只有10%。", "conclusion": "分析表明，FACTORY在之前的基准测试中具有优势，强调了其可靠性和模型在长尾事实推理中的必要性。"}}
{"id": "2508.00053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00053", "abs": "https://arxiv.org/abs/2508.00053", "authors": ["Jie Zhu", "Yiyang Su", "Minchul Kim", "Anil Jain", "Xiaoming Liu"], "title": "A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition", "comment": "Accepted to ICCV 2025. 11 pages, 5 figures", "summary": "Whole-body biometric recognition is a challenging multimodal task that\nintegrates various biometric modalities, including face, gait, and body. This\nintegration is essential for overcoming the limitations of unimodal systems.\nTraditionally, whole-body recognition involves deploying different models to\nprocess multiple modalities, achieving the final outcome by score-fusion (e.g.,\nweighted averaging of similarity matrices from each model). However, these\nconventional methods may overlook the variations in score distributions of\nindividual modalities, making it challenging to improve final performance. In\nthis work, we present \\textbf{Q}uality-guided \\textbf{M}ixture of score-fusion\n\\textbf{E}xperts (QME), a novel framework designed for improving whole-body\nbiometric recognition performance through a learnable score-fusion strategy\nusing a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for\nquality estimation with a modality-specific Quality Estimator (QE), and a score\ntriplet loss to improve the metric performance. Extensive experiments on\nmultiple whole-body biometric datasets demonstrate the effectiveness of our\nproposed approach, achieving state-of-the-art results across various metrics\ncompared to baseline methods. Our method is effective for multimodal and\nmulti-model, addressing key challenges such as model misalignment in the\nsimilarity score domain and variability in data quality.", "AI": {"tldr": "本文介绍了一种新的框架QME，它可以通过学习的方式改进全身生物识别的分数融合策略，解决了传统方法中的多种问题，实验结果表明使用QME能取得更好的效果。", "motivation": "传统的全身生物识别系统使用独立模型处理多种模态，通过分数融合达到最终结果，但这种方法忽略了各模态分数分布的变化，提高最终性能较为困难。", "method": "提出了一种名为QME（Quality-guided Mixture of Experts）的新框架，用于通过可学习的分数融合策略改进全身生物识别性能。该方法引入了一种新的伪质量损失来进行质量评估，并使用分数三元组损失来提高度量性能。", "result": "在多个全身生物识别数据集上的广泛实验表明，所提出的方法能够达到最先进的结果，优于基线方法。该方法有效解决了多模态和多模型中的模型对齐问题及数据质量变化问题。", "conclusion": "实验结果证明，所提出的方法有效解决了挑战，如相似度分数领域中的模型不对齐和数据质量变化问题。"}}
{"id": "2508.00121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00121", "abs": "https://arxiv.org/abs/2508.00121", "authors": ["Xiao Zhang", "Johan bos"], "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?", "comment": "Accepted by 16th IWCS", "summary": "Neural semantic parsers have shown good overall performance for a variety of\nlinguistic phenomena, reaching semantic matching scores of more than 90%. But\nhow do such parsers perform on strongly context-sensitive phenomena, where\nlarge pieces of semantic information need to be duplicated to form a meaningful\nsemantic representation? A case in point is English verb phrase ellipsis, a\nconstruct where entire verb phrases can be abbreviated by a single auxiliary\nverb. Are the otherwise known as powerful semantic parsers able to deal with\nellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with\ntheir fully resolved meaning representation and used this as a challenge set\nfor a large battery of neural semantic parsers. Although these parsers\nperformed very well on the standard test set, they failed in the instances with\nellipsis. Data augmentation", "AI": {"tldr": "研究发现，尽管效果较好，神经语义解析器在处理英语动词短语省略时表现不佳。", "motivation": "研究神经语义解析器在处理需大量语义信息复制语境下的表现，特别是英语动词短语省略现象。", "method": "通过构建包含120个省略实例及其完整语义表示的数据集，将其作为挑战数据集用于测试一系列神经语义解析器。", "result": "尽管这些解析器在标准测试集上表现非常好，但它们在包含省略的实例上失败了。", "conclusion": "研究表明，尽管神经语义解析器在多种语言现象上表现出良好的整体性能，但对于需要大量语义信息复制的强上下文敏感现象（例如英语动词短语省略），它们的表现不佳。"}}
{"id": "2508.00085", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00085", "abs": "https://arxiv.org/abs/2508.00085", "authors": ["Raiyaan Abdullah", "Jared Claypoole", "Michael Cogswell", "Ajay Divakaran", "Yogesh Rawat"], "title": "Punching Bag vs. Punching Person: Motion Transferability in Videos", "comment": "Accepted to ICCV 2025 main conference", "summary": "Action recognition models demonstrate strong generalization, but can they\neffectively transfer high-level motion concepts across diverse contexts, even\nwithin similar distributions? For example, can a model recognize the broad\naction \"punching\" when presented with an unseen variation such as \"punching\nperson\"? To explore this, we introduce a motion transferability framework with\nthree datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)\nKinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural\nvideo datasets. We evaluate 13 state-of-the-art models on these benchmarks and\nobserve a significant drop in performance when recognizing high-level actions\nin novel contexts. Our analysis reveals: 1) Multimodal models struggle more\nwith fine-grained unknown actions than with coarse ones; 2) The bias-free\nSyn-TA proves as challenging as real-world datasets, with models showing\ngreater performance drops in controlled settings; 3) Larger models improve\ntransferability when spatial cues dominate but struggle with intensive temporal\nreasoning, while reliance on object and background cues hinders generalization.\nWe further explore how disentangling coarse and fine motions can improve\nrecognition in temporally challenging datasets. We believe this study\nestablishes a crucial benchmark for assessing motion transferability in action\nrecognition. Datasets and relevant code:\nhttps://github.com/raiyaan-abdullah/Motion-Transfer.", "AI": {"tldr": "研究评估了13个先进动作识别模型在不同数据集下的表现，发现这些模型在面新上下文时性能明显下降，并考察了三维物体运动数据集Syn-TA和两个自然视频数据集（Kinetics400-TA，Something-Something-v2-TA）在动作识别任务中的作用及模型在这些数据集上的表现，突出了空间线索和时间推理对模型性能的影响。", "motivation": "为了探究现有模型是否能在多样化和相似分布的情况下有效地转移高层次运动概念。", "method": "研究引入了一个动作转移性框架，包含三个数据集：Syn-TA（合成数据集，包含3D物体运动）、Kinetics400-TA和Something-Something-v2-TA（后者两个是从自然视频数据集修改而来）。", "result": "观察到模型在识别新型上下文中的高层次动作时性能显著下降。", "conclusion": "该研究确立了一个关键基准，用于评估动作识别中的运动转移性。"}}
{"id": "2508.00185", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00185", "abs": "https://arxiv.org/abs/2508.00185", "authors": ["Alper Yaman", "Jannik Schwab", "Christof Nitsche", "Abhirup Sinha", "Marco Huber"], "title": "Comparison of Large Language Models for Deployment Requirements", "comment": null, "summary": "Large Language Models (LLMs), such as Generative Pre-trained Transformers\n(GPTs) are revolutionizing the generation of human-like text, producing\ncontextually relevant and syntactically correct content. Despite challenges\nlike biases and hallucinations, these Artificial Intelligence (AI) models excel\nin tasks, such as content creation, translation, and code generation.\nFine-tuning and novel architectures, such as Mixture of Experts (MoE), address\nthese issues. Over the past two years, numerous open-source foundational and\nfine-tuned models have been introduced, complicating the selection of the\noptimal LLM for researchers and companies regarding licensing and hardware\nrequirements. To navigate the rapidly evolving LLM landscape and facilitate LLM\nselection, we present a comparative list of foundational and domain-specific\nmodels, focusing on features, such as release year, licensing, and hardware\nrequirements. This list is published on GitLab and will be continuously\nupdated.", "AI": {"tldr": "本文提供了一个不断更新的开源LLM比较列表，以帮助研究者和企业在选择合适的模型时，考虑许可证和硬件要求。", "motivation": "鉴于过去两年中开源基础模型和微调模型的大量涌现，选择合适的LLM变得复杂。为了应对这一挑战，帮助研究者和企业在许可和硬件要求上更好地选择最优的LLM，提出了这篇论文。", "method": "通过构建一个比较列表来帮助研究人员和企业选择合适的语言模型，该列表涵盖了基础模型和领域特定模型，并重点关注了发布时间、许可、硬件要求等特征。", "result": "提供了一个发布在GitLab上，包含基础模型和领域特定模型比较的列表，该列表将定期更新。", "conclusion": "通过列出模型的关键特性和硬件需求，这项工作为研究人员和企业提供了选择和部署适合其需求的语言模型的清晰指南。"}}
{"id": "2508.00088", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00088", "abs": "https://arxiv.org/abs/2508.00088", "authors": ["Mateo de Mayo", "Daniel Cremers", "Taihú Pire"], "title": "The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking", "comment": "Accepted to IROS 2025", "summary": "Humanoid robots and mixed reality headsets benefit from the use of\nhead-mounted sensors for tracking. While advancements in visual-inertial\nodometry (VIO) and simultaneous localization and mapping (SLAM) have produced\nnew and high-quality state-of-the-art tracking systems, we show that these are\nstill unable to gracefully handle many of the challenging settings presented in\nthe head-mounted use cases. Common scenarios like high-intensity motions,\ndynamic occlusions, long tracking sessions, low-textured areas, adverse\nlighting conditions, saturation of sensors, to name a few, continue to be\ncovered poorly by existing datasets in the literature. In this way, systems may\ninadvertently overlook these essential real-world issues. To address this, we\npresent the Monado SLAM dataset, a set of real sequences taken from multiple\nvirtual reality headsets. We release the dataset under a permissive CC BY 4.0\nlicense, to drive advancements in VIO/SLAM research and development.", "AI": {"tldr": "本文提出了Monado SLAM数据集，该数据集包含多个虚拟现实头戴设备的真实序列，以解决现有VIO/SLAM系统在复杂场景中表现不佳的问题。", "motivation": "本文的动机在于现有的VIO和SLAM系统无法很好地处理头戴设备应用场景中的许多挑战性设置，例如高强度运动、动态遮挡、长时间跟踪会话、低纹理区域、不利的光照条件以及传感器饱和等情况。因此，作者创建了Monado SLAM数据集，以解决现有数据集未能充分涵盖这些问题的现状。", "method": "分析论文摘要的方法部分，本文没有明确说明具体的方法，而是介绍了数据集的创建目的和重要性。", "result": "本文主要成果是创建并发布了一个新的SLAM数据集，该数据集来自多个虚拟现实头戴设备的真实序列，并且以CC BY 4.0许可条款发布，以推动VIO/SLAM的研究和发展。", "conclusion": "作者通过创建Monado SLAM数据集，希望能够加大对VIO/SLAM系统在复杂头戴设备应用场景中挑战性问题的关注，并推动相关领域的研究和开发。"}}
{"id": "2508.00217", "categories": ["cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00217", "abs": "https://arxiv.org/abs/2508.00217", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "comment": null, "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "AI": {"tldr": "本文介绍了表格理解任务中的关键概念，并指出该领域存在的几个研究空白。", "motivation": "由于表格结构的多样性和复杂性，传统方法不足以应对这些挑战，因此本文提出了关键概念。", "method": "本文通过创建表格输入表示的分类法和介绍表格理解任务来解决这些挑战。", "result": "文章指出当前领域存在的几个关键差距，即任务集中在检索，对复杂表格处理困难，以及模型在不同表格表示和格式上泛化能力有限。", "conclusion": "研究结果表明，需要进一步的研究来解决表格理解中的挑战，尤其在复杂结构、大规模表格处理及模型泛化能力方面。"}}
{"id": "2508.00135", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00135", "abs": "https://arxiv.org/abs/2508.00135", "authors": ["Basna Mohammed Salih Hasan", "Ramadhan J. Mstafa"], "title": "Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images", "comment": "12 pages, 18 figures, 5 tables", "summary": "Gender classification has emerged as a crucial aspect in various fields,\nincluding security, human-machine interaction, surveillance, and advertising.\nNonetheless, the accuracy of this classification can be influenced by factors\nsuch as cosmetics and disguise. Consequently, our study is dedicated to\naddressing this concern by concentrating on gender classification using color\nimages of the periocular region. The periocular region refers to the area\nsurrounding the eye, including the eyelids, eyebrows, and the region between\nthem. It contains valuable visual cues that can be used to extract key features\nfor gender classification. This paper introduces a sophisticated Convolutional\nNeural Network (CNN) model that utilizes color image databases to evaluate the\neffectiveness of the periocular region for gender classification. To validate\nthe model's performance, we conducted tests on two eye datasets, namely CVBL\nand (Female and Male). The recommended architecture achieved an outstanding\naccuracy of 99% on the previously unused CVBL dataset while attaining a\ncommendable accuracy of 96% with a small number of learnable parameters\n(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of\nour proposed model for gender classification using the periocular region, we\nevaluated its performance through an extensive range of metrics and compared it\nwith other state-of-the-art approaches. The results unequivocally demonstrate\nthe efficacy of our model, thereby suggesting its potential for practical\napplication in domains such as security and surveillance.", "AI": {"tldr": "本文提出了一种基于颜色图像的卷积神经网络模型，用于眼周区域的性别分类，实验结果表明该模型在CVBL和(Female and Male)数据集上分别达到了99%和96%的准确率。", "motivation": "性别分类在安全、人机交互、监控和广告等领域具有重要意义，但化妆和伪装可能影响分类的准确性。研究集中于利用眼周区域的颜色图像进行性别分类，以改善这种准确性。", "method": "研究中提出了一种使用颜色眼周图像数据库评估性别分类效果的卷积神经网络(CNN)模型。", "result": "模型在CVBL数据集上达到了99%的准确率，且在(Female and Male)数据集上使用少量的可学习参数(7,235,089)达到了96%的准确率。", "conclusion": "通过广泛的评估指标验证，提出模型的性别分类效果优于其他先进的方法，为实际应用提供了潜在可能。"}}
{"id": "2508.00220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00220", "abs": "https://arxiv.org/abs/2508.00220", "authors": ["Rana Aref Salama", "Abdou Youssef", "Mona Diab"], "title": "Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform", "comment": null, "summary": "Wavelet transforms, a powerful mathematical tool, have been widely used in\ndifferent domains, including Signal and Image processing, to unravel intricate\npatterns, enhance data representation, and extract meaningful features from\ndata. Tangible results from their application suggest that Wavelet transforms\ncan be applied to NLP capturing a variety of linguistic and semantic\nproperties. In this paper, we empirically leverage the application of Discrete\nWavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase\nthe capabilities of DWT in analyzing embedding representations at different\nlevels of resolution and compressing them while maintaining their overall\nquality. We assess the effectiveness of DWT embeddings on semantic similarity\ntasks to show how DWT can be used to consolidate important semantic information\nin an embedding vector. We show the efficacy of the proposed paradigm using\ndifferent embedding models, including large language models, on downstream\ntasks. Our results show that DWT can reduce the dimensionality of embeddings by\n50-93% with almost no change in performance for semantic similarity tasks,\nwhile achieving superior accuracy in most downstream tasks. Our findings pave\nthe way for applying DWT to improve NLP applications.", "AI": {"tldr": "本文研究了离散小波变换（DWT）应用于词和句子嵌入的可行性，发现DWT能够高效压缩嵌入表示，同时保持或提升自然语言处理任务的性能。", "motivation": "本文旨在展示DWT在分析嵌入表示和压缩表示以维持其整体质量方面的有效性。此外，评估DWT嵌入在语义相似性任务中的表现，以展现其在巩固嵌入向量中的重要语义信息方面的应用潜力。", "method": "本文采用了离散小波变换（DWT）应用于词和句子的嵌入表示，通过在不同分辨率级别分析嵌入表示，并在保持整体质量的同时进行压缩。", "result": "实验结果表明，DWT能够减少嵌入表示的维度50-93%，几乎不对语义相似性任务的表现产生变化，同时在大多数下游任务中达到更高的准确性。", "conclusion": "本文的发现为提升自然语言处理（NLP）应用中DWT的应用提供了可能性。"}}
{"id": "2508.00144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00144", "abs": "https://arxiv.org/abs/2508.00144", "authors": ["Akshat Rakheja", "Aarsh Ashdhir", "Aryan Bhattacharjee", "Vanshika Sharma"], "title": "World Consistency Score: A Unified Metric for Video Generation Quality", "comment": "27 pages, 1 figure", "summary": "We introduce World Consistency Score (WCS), a novel unified evaluation metric\nfor generative video models that emphasizes internal world consistency of the\ngenerated videos. WCS integrates four interpretable sub-components - object\npermanence, relation stability, causal compliance, and flicker penalty - each\nmeasuring a distinct aspect of temporal and physical coherence in a video.\nThese submetrics are combined via a learned weighted formula to produce a\nsingle consistency score that aligns with human judgments. We detail the\nmotivation for WCS in the context of existing video evaluation metrics,\nformalize each submetric and how it is computed with open-source tools\n(trackers, action recognizers, CLIP embeddings, optical flow), and describe how\nthe weights of the WCS combination are trained using human preference data. We\nalso outline an experimental validation blueprint: using benchmarks like\nVBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human\nevaluations, performing sensitivity analyses, and comparing WCS against\nestablished metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a\ncomprehensive and interpretable framework for evaluating video generation\nmodels on their ability to maintain a coherent \"world\" over time, addressing\ngaps left by prior metrics focused only on visual fidelity or prompt alignment.", "AI": {"tldr": "本文提出World Consistency Score (WCS)，一个新颖的视频评估指标，整合四个子组件以评估视频生成模型的连贯性，并通过实验验证其有效性。", "motivation": "旨在解决现有视频评估指标在仅关注视觉保真度或提示对齐方面的局限性，提出WCS来全面评估视频生成模型在一段时间内维持连贯“世界”的能力。", "method": "提出了World Consistency Score (WCS)，一个新颖的统一评估指标，用于评估生成视频的内在世界一致性。WCS整合了四个可解释的子组件——对象持久性、关系稳定性、因果符应性和闪烁处罚，每个子组件测量视频在时间和物理连贯性方面的不同方面。", "result": "通过使用VBench-2.0、EvalCrafter和LOVE等基准测试，以及敏感性分析，将WCS与现有指标（如FVD、CLIPScore、VBench、FVMD）进行比较和验证。", "conclusion": "WCS为评价视频生成模型提供了全面且可解释的框架，对于提高生成视频的连贯性和可理解性具有重要意义。"}}
{"id": "2508.00238", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.00238", "abs": "https://arxiv.org/abs/2508.00238", "authors": ["Bryce Anderson", "Riley Galpin", "Tom S. Juzek"], "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English", "comment": "Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2\n  figures, 2 tables. Licensed under CC BY-SA 4.0", "summary": "In recent years, written language, particularly in science and education, has\nundergone remarkable shifts in word usage. These changes are widely attributed\nto the growing influence of Large Language Models (LLMs), which frequently rely\non a distinct lexical style. Divergences between model output and target\naudience norms can be viewed as a form of misalignment. While these shifts are\noften linked to using Artificial Intelligence (AI) directly as a tool to\ngenerate text, it remains unclear whether the changes reflect broader changes\nin the human language system itself. To explore this question, we constructed a\ndataset of 22.1 million words from unscripted spoken language drawn from\nconversational science and technology podcasts. We analyzed lexical trends\nbefore and after ChatGPT's release in 2022, focusing on commonly LLM-associated\nwords. Our results show a moderate yet significant increase in the usage of\nthese words post-2022, suggesting a convergence between human word choices and\nLLM-associated patterns. In contrast, baseline synonym words exhibit no\nsignificant directional shift. Given the short time frame and the number of\nwords affected, this may indicate the onset of a remarkable shift in language\nuse. Whether this represents natural language change or a novel shift driven by\nAI exposure remains an open question. Similarly, although the shifts may stem\nfrom broader adoption patterns, it may also be that upstream training\nmisalignments ultimately contribute to changes in human language use. These\nfindings parallel ethical concerns that misaligned models may shape social and\nmoral beliefs.", "AI": {"tldr": "研究发现，2022年后，随着大型语言模型（如ChatGPT）的流行，人类在使用科学和技术播客时的词汇偏好开始与这些模型的输出相一致。", "motivation": "探讨语言生成中的词汇变化是否反映了人类语言系统的更广泛变化，特别是在大型语言模型（LLMs）的影响下。", "method": "构建了一个包含2210万个单词的数据集，这些单词来自未脚本化的科学和技术播客中的口语。分析了ChatGPT在2022年发布前后的词汇趋势，重点关注与大语言模型相关的词汇。", "result": "结果表明，在2022年后，与LLM相关的词汇使用显著增加，而基准同义词词汇没有显著的变化。", "conclusion": "这些发现表明，在短时间内有相当数量的词汇受到LLM的影响，可能是语言使用开始发生变化的迹象。这种变化可能是自然语言演化的结果，也可能是由于人工智能的使用导致的新变化。此外，这也可能反映了更广泛的采用模式，或训练中的上游错配最终导致了人类语言使用的变化。"}}
{"id": "2508.00152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00152", "abs": "https://arxiv.org/abs/2508.00152", "authors": ["Li Mi", "Manon Bechaz", "Zeming Chen", "Antoine Bosselut", "Devis Tuia"], "title": "GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration", "comment": "ICCV 2025. Project page at https://limirs.github.io/GeoExplorer/", "summary": "Active Geo-localization (AGL) is the task of localizing a goal, represented\nin various modalities (e.g., aerial images, ground-level images, or text),\nwithin a predefined search area. Current methods approach AGL as a\ngoal-reaching reinforcement learning (RL) problem with a distance-based reward.\nThey localize the goal by implicitly learning to minimize the relative distance\nfrom it. However, when distance estimation becomes challenging or when\nencountering unseen targets and environments, the agent exhibits reduced\nrobustness and generalization ability due to the less reliable exploration\nstrategy learned during training. In this paper, we propose GeoExplorer, an AGL\nagent that incorporates curiosity-driven exploration through intrinsic rewards.\nUnlike distance-based rewards, our curiosity-driven reward is goal-agnostic,\nenabling robust, diverse, and contextually relevant exploration based on\neffective environment modeling. These capabilities have been proven through\nextensive experiments across four AGL benchmarks, demonstrating the\neffectiveness and generalization ability of GeoExplorer in diverse settings,\nparticularly in localizing unfamiliar targets and environments.", "AI": {"tldr": "本文提出GeoExplorer，通过好奇心驱动的探索策略增强了AGL的鲁棒性和泛化能力。", "motivation": "当前的AGL方法依赖于基于距离的奖励来最小化相对距离，但是这种方法在遇到难以估计距离或未见过的目标和环境时，表现不佳，探索策略也不够可靠。", "method": "GeoExplorer, 一个结合了基于好奇心的内在奖励机制的主动地理定位（AGL）智能体，通过有效建模环境来实现鲁棒、多样化且与上下文相关的探索，以此区别于基于距离的奖励。", "result": "在四个AGL基准测试中进行了广泛的实验，证明了GeoExplorer在多样化设置中的有效性和泛化能力，尤其是在定位不熟悉的目标和环境时。", "conclusion": "GeoExplorer通过利用基于好奇心的内在奖励，提升了主动地理定位的鲁棒性和探索能力，使其在处理未知目标和环境时更为可靠。"}}
{"id": "2508.00285", "categories": ["cs.CL", "I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2508.00285", "abs": "https://arxiv.org/abs/2508.00285", "authors": ["Peixian Li", "Yu Tian", "Ruiqi Tu", "Chengkai Wu", "Jingjing Ren", "Jingsong Li"], "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering", "comment": "23 pages, 8 figures", "summary": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.00169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00169", "abs": "https://arxiv.org/abs/2508.00169", "authors": ["Bhavya Goyal", "Felipe Gutierrez-Barragan", "Wei Lin", "Andreas Velten", "Yin Li", "Mohit Gupta"], "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs", "comment": "ICCV 2025", "summary": "LiDAR-based 3D sensors provide point clouds, a canonical 3D representation\nused in various scene understanding tasks. Modern LiDARs face key challenges in\nseveral real-world scenarios, such as long-distance or low-albedo objects,\nproducing sparse or erroneous point clouds. These errors, which are rooted in\nthe noisy raw LiDAR measurements, get propagated to downstream perception\nmodels, resulting in potentially severe loss of accuracy. This is because\nconventional 3D processing pipelines do not retain any uncertainty information\nfrom the raw measurements when constructing point clouds.\n  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation\nwhere each point is augmented with a probability attribute that encapsulates\nthe measurement uncertainty (or confidence) in the raw data. We further\nintroduce inference approaches that leverage PPC for robust 3D object\ndetection; these methods are versatile and can be used as computationally\nlightweight drop-in modules in 3D inference pipelines. We demonstrate, via both\nsimulations and real captures, that PPC-based 3D inference methods outperform\nseveral baselines using LiDAR as well as camera-LiDAR fusion models, across\nchallenging indoor and outdoor scenarios involving small, distant, and\nlow-albedo objects, as well as strong ambient light.\n  Our project webpage is at https://bhavyagoyal.github.io/ppc .", "AI": {"tldr": "The paper introduces Probabilistic Point Clouds, a technique for enhancing 3D object detection in challenging scenarios by integrating uncertainty measurement into each point of the point cloud, demonstrating improved performance over conventional methods.", "motivation": "The motivation is to address the key challenges faced by modern LiDARs in producing sparse or erroneous point clouds, especially in challenging real-world scenarios such as long-distance or low-albedo objects. These errors can significantly affect downstream perception models, leading to potential loss of accuracy.", "method": "We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation where each point is augmented with a probability attribute that encapsulates the measurement uncertainty in the raw data. This representation is used in robust 3D object detection and can be implemented as computationally lightweight drop-in modules in 3D inference pipelines.", "result": "The methods based on PPC outperform several baselines using LiDAR and even camera-LiDAR fusion models, demonstrating effectiveness in challenging indoor and outdoor scenarios.", "conclusion": "The PPC-based approach is shown to improve the robustness and accuracy of 3D object detection in various challenging scenarios, underscoring the importance of integrating uncertainty estimation directly into the point cloud representation."}}
{"id": "2508.00305", "categories": ["cs.CL", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.00305", "abs": "https://arxiv.org/abs/2508.00305", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "comment": null, "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "AI": {"tldr": "研究了剪枝、量化和丢弃标记等技术对大型语言模型在长上下文场景下的资源需求和性能的影响，并探讨了这些优化方法的组合和可扩展性问题。", "motivation": "大型语言模型在处理多样性自然语言任务时表现出色，但面临着资源需求高和上下文窗口有限的问题。目前有关这些技术在长上下文场景中的效果和整体系统评估分析不足。", "method": "分析不同的优化方法（如剪枝、量化和丢弃标记）对支持长上下文的两种大型语言模型架构的影响，并系统评估这些技术的组合效果。还研究了这些优化方法在拥有700亿参数的更大模型上的可扩展性。", "result": "研究表明，单纯依赖F1指标会隐藏问答任务中的精度-召回率权衡问题，并启示了一种将系统级分析与任务特定洞察相结合的方法。", "conclusion": "这项研究通过结合系统级别的性能剖析与特定任务的洞察力，帮助大型语言模型实务工作者和研究人员探索并平衡不同任务和硬件配置下的效率、准确性和可扩展性。同时，研究揭示了简单组合推理优化算法对更大模型可能带来负面影响，并指出这一点是与较小模型不同的。"}}
{"id": "2508.00171", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00171", "abs": "https://arxiv.org/abs/2508.00171", "authors": ["David Restrepo", "Ira Ktena", "Maria Vakalopoulou", "Stergios Christodoulidis", "Enzo Ferrante"], "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI", "comment": "Accepted to MICCAI 2025 1st Workshop on Multimodal Large Language\n  Models (MLLMs) in Clinical Practice", "summary": "Clinical decision-making relies on the integrated analysis of medical images\nand the associated clinical reports. While Vision-Language Models (VLMs) can\noffer a unified framework for such tasks, they can exhibit strong biases toward\none modality, frequently overlooking critical visual cues in favor of textual\ninformation. In this work, we introduce Selective Modality Shifting (SMS), a\nperturbation-based approach to quantify a model's reliance on each modality in\nbinary classification tasks. By systematically swapping images or text between\nsamples with opposing labels, we expose modality-specific biases. We assess six\nopen-source VLMs-four generalist models and two fine-tuned for medical data-on\ntwo medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)\nand FairVLMed (scanning laser ophthalmoscopy). By assessing model performance\nand the calibration of every model in both unperturbed and perturbed settings,\nwe reveal a marked dependency on text input, which persists despite the\npresence of complementary visual information. We also perform a qualitative\nattention-based analysis which further confirms that image content is often\novershadowed by text details. Our findings highlight the importance of\ndesigning and evaluating multimodal medical models that genuinely integrate\nvisual and textual cues, rather than relying on single-modality signals.", "AI": {"tldr": "A study using Selective Modality Shifting shows Vision-Language Models tend to rely more on textual information than on visual cues in medical data, indicating the need for better integration of both modalities.", "motivation": "To assess the reliance of Vision-Language Models on text versus image data in medical decision-making and to reveal modality-specific biases.", "method": "Selective Modality Shifting (SMS), a perturbation-based approach to quantify a model's reliance on each modality in binary classification tasks by swapping images or text between samples with opposing labels.", "result": "Reveals a marked dependency on text input over visual information despite the presence of complementary visual data, identified through model performance and calibration analysis in unperturbed and perturbed settings.", "conclusion": "Highlights the need to genuinely integrate visual and textual cues in multimodal medical models to avoid relying on single-modality signals."}}
{"id": "2508.00332", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00332", "abs": "https://arxiv.org/abs/2508.00332", "authors": ["Kaiyan Zhao", "Zhongtao Miao", "Yoshimasa Tsuruoka"], "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment", "comment": "Work in progress", "summary": "Multimodal sentence embedding models typically leverage image-caption pairs\nin addition to textual data during training. However, such pairs often contain\nnoise, including redundant or irrelevant information on either the image or\ncaption side. To mitigate this issue, we propose MCSEO, a method that enhances\nmultimodal sentence embeddings by incorporating fine-grained object-phrase\nalignment alongside traditional image-caption alignment. Specifically, MCSEO\nutilizes existing segmentation and object detection models to extract accurate\nobject-phrase pairs, which are then used to optimize a contrastive learning\nobjective tailored to object-phrase correspondence. Experimental results on\nsemantic textual similarity (STS) tasks across different backbone models\ndemonstrate that MCSEO consistently outperforms strong baselines, highlighting\nthe significance of precise object-phrase alignment in multimodal\nrepresentation learning.", "AI": {"tldr": "MCSEO方法通过结合使用对象检测和分割技术，改进了传统图像-标题对齐的不足，提升了多模态句子嵌入性能。", "motivation": "研究旨在解决图像-标题对中存在的噪声问题，如冗余或无关信息，以提高多模态句子嵌入的质量。", "method": "MCSEO使用现有分割和对象检测模型提取精确的对象-短语对，利用对象-短语对应关系优化对比学习目标，从而增强了多模态句子嵌入。", "result": "实验结果显示，MCSEO在语义文本相似性任务上超越了强大的基线方法，证明了精确对象-短语对齐的重要性。", "conclusion": "MCSEO通过引入细粒度的对象-短语对齐改善了多模态句子嵌入，展示了在多模态表示学习中精确对齐是关键因素。"}}
{"id": "2508.00197", "categories": ["cs.CV", "cs.LG", "cs.NA", "math.CT", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.00197", "abs": "https://arxiv.org/abs/2508.00197", "authors": ["Eric Mjolsness", "Cory B. Scott"], "title": "Graph Lineages and Skeletal Graph Products", "comment": "42 pages. 33 Figures. Under review", "summary": "Graphs, and sequences of growing graphs, can be used to specify the\narchitecture of mathematical models in many fields including machine learning\nand computational science. Here we define structured graph \"lineages\" (ordered\nby level number) that grow in a hierarchical fashion, so that: (1) the number\nof graph vertices and edges increases exponentially in level number; (2)\nbipartite graphs connect successive levels within a graph lineage and, as in\nmultigrid methods, can constrain matrices relating successive levels; (3) using\nprolongation maps within a graph lineage, process-derived distance measures\nbetween graphs at successive levels can be defined; (4) a category of \"graded\ngraphs\" can be defined, and using it low-cost \"skeletal\" variants of standard\nalgebraic graph operations and type constructors (cross product, box product,\ndisjoint sum, and function types) can be derived for graded graphs and hence\nhierarchical graph lineages; (5) these skeletal binary operators have similar\nbut not identical algebraic and category-theoretic properties to their standard\ncounterparts; (6) graph lineages and their skeletal product constructors can\napproach continuum limit objects. Additional space-efficient unary operators on\ngraded graphs are also derived: thickening, which creates a graph lineage of\nmultiscale graphs, and escalation to a graph lineage of search frontiers\n(useful as a generalization of adaptive grids and in defining \"skeletal\"\nfunctions). The result is an algebraic type theory for graded graphs and\n(hierarchical) graph lineages. The approach is expected to be well suited to\ndefining hierarchical model architectures - \"hierarchitectures\" - and local\nsampling, search, or optimization algorithms on them. We demonstrate such\napplication to deep neural networks (including visual and feature scale spaces)\nand to multigrid numerical methods.", "AI": {"tldr": "本文定义了一种称为结构化图谱系的概念，展示了如何利用它们执行在多尺度图和层级模型架构（称为“等级结构”）上的局部采样、搜索或优化算法，并展示了在深度神经网络和多网格数值方法中的应用。", "motivation": "本文旨在通过定义结构化的图谱系，探索一个适用于多项领域的数学模型的架构描述方法，尤其关注机器学习和计算科学中的应用。", "method": "本文提出了一种结构化的图谱系概念，这些图谱系按照层级增长，并具备多项特性，如图节点和边的数量随层级号成指数增长，以及定义了过程衍生的距离度量等。通过它们，作者导出了用于分级图的骨骼变体的标准代数图操作和类型构造器，并探讨了这些操作的代数和范畴理论性质。此外，还定义了空间效率高的单元操作，如加厚、升级等，适用于多尺度图谱系和搜索前沿的构建。", "result": "研究结果表明，通过图谱系的方法，可以构建空间效率高且具有相似但不完全相同的代数与范畴性质的操作符，并可以用这种方法定义“等级结构”，进而促进在多尺度图谱系框架下执行的局部算法的应用。", "conclusion": "本文提出的代数类型理论对于分级图和层次图谱系的构建具有广泛的意义，并且预期为定义复杂的模型架构和执行局部采样、搜索或优化算法提供了强力工具。"}}
{"id": "2508.00344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00344", "abs": "https://arxiv.org/abs/2508.00344", "authors": ["Keer Lu", "Chong Chen", "Bin Cui", "Huang Leng", "Wentao Zhang"], "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.", "AI": {"tldr": "本文由于大语言模型(大语言模型)在一部分任务中的关键动态参与生成测试。输入：现有的任务特点参数要求参数。导向：AdaPlan及实验结果：PilotRL有效距离上GPT-4o-mini。", "motivation": "由于生成任务的大语言模型变概率无法较大级测试特点，在大语言模型输入不大级应用的任务时，竟然不能发现随后改进模型存在较大的变概率随后一定级测试特点(一步操作)。特点：本文的主要目，有三点：(1)提出一个第一级特定化值设置；(2)提出一个弹性的设计理念；(3)可注直通使用简单第一级存在模型", "method": "Structure", "result": "{ \"tldr\": \"\\u7b80\\u89e3\\uff1a\\u672c\\u6587\\u7531\\u4e8e\\u5927\\u8bed\\u6587\\u6a21\\u578b(Large\\u5927\\u8bed\\u6587\\u6a21\\u578b)\\u5728\\u4e00\\u90e8\\u5206\\u4efb\\u52a1\\u4e2d\\u7684\\u5173\\u952e\\u52a8\\u6001\\u4e2d\\u53c2\\u4e0e\\u751f\\u6210\\u6d4b\\u8bd5\\u3002\\u8f93\\u5165\\uff1a\\u5b58\\u5728\\u7684\\u4efb\\u52a1\\u7279\\u70b9\\u53c2\\u6570\\u9700\\u8981\\u53c2\\u6570\\u3002\\u5f00\\u5c04\\uff1aAdaPlan\\u53ca\\u5b9e\\u9a8c\\u7ed3\\u679c\\uff1aPilotRL\\u6709\\u6548\\u8ddd\\u79bb\\u4e0aGPT-4o-mini\\u3002\", \"motivation\": \"\\u6027\\u7279\\uff1a\\u7531\\u4e8e\\u751f\\u6210\\u4efb\\u52a1\\u7684\\u5927\\u8bed\\u6587\\u6a21\\u578b\\u53d8\\u76df\\u4e0d\\u80fd\\u8f83\\u5927\\u7ea7\\u6d4b\\u8bd5\\u7279\\u70b9\\uff0c\\u5728\\u5927\\u8bed\\u6587\\u6a21\\u578b\\u8f93\\u5165\\u4e0d\\u5927\\u7ea7\\u5e94\\u7528\\u7684\\u4efb\\u52a1\\u65f6\\uff0c\\u751a\\u81f3\\u4e0d\\u80fd\\u53d1\\u73b0\\u968f\\u6765\\u4fee\\u6539\\u6a21\\u578b\\u5b58\\u5728\\u7684\\u8f83\\u5927\\u53d8\\u76df\\u4e0d\\u80fd\\u53d1\\u73b0\\u968f\\u6765\\u4e00\\u5b9a\\u7ea7\\u6d4b\\u8bd5\\u7279\\u70b9\\uff08\\u4e00\\u6b65\\u64cd\\u4f5c\\uff09\\u3002\\u7279\\u70b9\\uff1a\\u672c\\u6587\\u76ee\\u7684\\u4e3b\\u8981\\u76ee\\uff0c\\u6709\\u4e09\\u70b9\\uff1a(1)\\u63a8\\u51fa\\u4e00\\u4e2a\\u7b2c\\u4e00\\u7ea7\\u6a21\\u7279\\u5b9a\\u5316\\u503c\\u8bbe\\u7f6e\\uff1b(2)\\u63a8\\u51fa\\u4e00\\u4e2a\\u5f39\\u548c\\u7684\\u8bbe\\u8ba1\\u601d\\u60f3\\uff1b(3)\\u53ef\\u6ce8\\u76f4\\u901a\\u4f7f\\u7528\\u7b80\\u5355\\u7b2c\\u4e00\\u7ea7\\u5b58\\u5728\\u6a21\\u578b\\u3002\", \"method\": \"\\u4f7f\\u7528\\u76f8\\u5173\\u4e0a\\u5c04\\u6a21\\u578bAdaPlan\\uff08\\u4e25\\u683c\\u7684\\u53d1\\u73b0\\uff09\\uff0c\\u7531\\u6b64\\u53d1\\u73b0\\u4e86\\u4e00\\u4e2a\\u9ad8\\u7ea7\\u64cd\\u4f5c\\u7684\\u4e0a\\u5c04\\u9879\\u76ee\\u5728\\u4e00\\u5b9a\\u6a21\\u578b\\u5e94\\u7528\\u5e02\\u573a\\uff08\\u7531\\u5145\\u58eb\\u8868\\u683c\\uff09\\uff0c\\u5e76\\u4e14\\u5728ReAct\\u9879\\u6240\\u6709\\u5e02\\u573a\\u4e0b\\u901a\\u8fc7\\u4ed8\\u5165\\u4e00\\u4e2a\\u65f6\\u5c1a\\u8868\\u793a\\u3002\\u6b64\\u65b9\\u6cd5\\u5728PilotRL\\u9879\\u800c\\u88ab\\u63d0\\u51fa\\uff0c\\u5e0c\\u814a\\u5b9e\\u9a8c\\u6709\\u6548\\u6027\\u8ddd\\u79bb\\u4e0a\\u4e86\\u6bcf\\u4e2a\\u6a21\\u578b\\uff0c\\u4e0d\\u8fc7\\u5982\\u540c\\u6837\\u5927\\u7684\\u6587\\u6a21\\u5b58\\u5728\\u590d\\u6742\\u72b6\\u60c5\\u800c\\u5bfc\\u81f4\\u6bcf\\u4e00\\u6b65\\u7684\\u6700\\u7ec8\\u6a21\\u5b58\\u5b58\\u5728\\u60c5\\u51b5\\u4e0b\\u901a\\u8fc7\\u5b9e\\u9a8c\\u5165\\u53e3\\u8868\\u793a\\u7684\\u8868\\u793a\\u3002\", \"result\": \"\\u5b9e\\u9a8c\\u65b9\\u6cd5\\uff1aPilotRL\\u8868\\u793a\\u4e86\\u9ad8\\u7ea7\\u52a8\\u6001\\u6d4b\\u8bd5\\uff1b\\u4e00\\u90e8\\u5206\\u8f83\\u7f51\\u7edc\\u65b9\\u6cd5\\u6d4b\\u8bd5\\uff1b\\u4e00\\u5b9a\\u6a21\\u578b\\u8868\\u793a\\u6027\\u8ddd\\u79bb\\u5927\\u7ea7\\u6a21\\u5173\\uff1b\\u9884\\u6d4b\\u5316\\u5927\\u7ea7\\u6a21\\u6d4b\\u8bd5\\u7cfb\\u7edf\\u7684\\u5b9e\\u9a8c\\uff0c\\u53e6\\u5916\\u8868\\u793a\\u4e86\\u8868\\u793a\\u5e02\\u573a\\u4e0b\\u7684\\u5f02\\u6d4b\\u76ee\\u7684\\u5b9e\\u9a8c\\u7b49\\u4e8b\\u9879\\u5b9e\\u9a8c\\u800c\\u5728\\u4e00\\u90e8\\u6a21\\u578b\\u8f6c\\u58eb\\u5185\\u90e8\\u6a21\\u6d4b\\u3002\", \"conclusion\": \"\\u7ed3\\u8bba\\uff1a\\u5b9e\\u9a8c\\u7ed3\\u679c\\u8868\\u660e\\u4e86PilotRL\\u5728\\u591a\\u65b9\\u9762\\u4e0a\\u4e0e\\u5c06\\u5f3a\\u5927\\u6d4b\\u8bd5\\u9879\\u800c\\u8d8a\\u51fa\\u73b0\\u5f3a\\u5927\\u8f6c\\u58eb\\u5185\\u7684\\u6a21\\u578b\\u5e94\\u7528\\u5728\\u5927\\u8bed\\u6587\\u6a21\\u578b\\u4e0a\\u6709\\u6548\\u7684\\u6a21\\u578b\\u5e94\\u7528\\u529b\\u3002\\u53e6\\u5916\\u8bb2\\u660e\\u4e86\\u4e00\\u4e9b\\u5bcc\\u575a\\u6d4b\\u8bd5\\u3001\\u4e00\\u90e8\\u5206\\u5bcc\\u575a\\u6d4b\\u8bd5\\u7b49\\u8d8a\\u51fa\\u73b0\\u7c7b\\u578b\\u5927\\u7ea7\\u6a21\\u6d4b\\u8bd5\\u3002\"}", "conclusion": "实验表明PilotRL在多方面上与将强大型测试项而越突出强势转轮内的模型应用在大语言模型上有有效的模型应用力。另外说明了一些广阔测试、一部分广阔测试等越出现类型大型测试"}}
{"id": "2508.00205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00205", "abs": "https://arxiv.org/abs/2508.00205", "authors": ["Xiangyu Kong", "Hengde Zhu", "Haoqin Sun", "Zhihao Guo", "Jiayan Gu", "Xinyi Ni", "Wei Zhang", "Shizhe Liu", "Siyang Song"], "title": "Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition", "comment": "10 pages, 4 figures", "summary": "Automatic real personality recognition (RPR) aims to evaluate human real\npersonality traits from their expressive behaviours. However, most existing\nsolutions generally act as external observers to infer observers' personality\nimpressions based on target individuals' expressive behaviours, which\nsignificantly deviate from their real personalities and consistently lead to\ninferior recognition performance. Inspired by the association between real\npersonality and human internal cognition underlying the generation of\nexpressive behaviours, we propose a novel RPR approach that efficiently\nsimulates personalised internal cognition from easy-accessible external short\naudio-visual behaviours expressed by the target individual. The simulated\npersonalised cognition, represented as a set of network weights that enforce\nthe personalised network to reproduce the individual-specific facial reactions,\nis further encoded as a novel graph containing two-dimensional node and edge\nfeature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for\ninferring real personality traits from it. To simulate real personality-related\ncognition, an end-to-end strategy is designed to jointly train our cognition\nsimulation, 2D graph construction, and personality recognition modules.", "AI": {"tldr": "本文提出了一种新的方法来模拟个体的内部认知过程，并使用二维图神经网络来提高自动真实个性识别的准确性。", "motivation": "现有大多数自动真实个性识别解决方案作为外部观察者，基于目标个体的表达行为来推断个性印象，这往往与目标个体的真实个性存在显著偏差，并导致较差的识别性能。为了避免这种情况，作者受到真实个性与人类内部认知之间的关联的启发，提出了一个新的自动真实个性识别方法。", "method": "本文提出了一种新的自动真实个性识别方法，该方法通过模拟个人的内部认知过程来识别其个性，这个过程基于可获取的外部短音频-视频行为。这种方法通过个性化网络再现个体特异性面部反应，并将这些反应编码成一个二维节点和边特征矩阵的图，使用一种新型的二维图神经网络（2D-GNN）来从该图中推断真实个性特征。为了模拟个性相关的认知，设计了一个端到端的策略，联合训练认知模拟、二维图构建和个人识别模块。", "result": "未在摘要是提供具体结果，但是方法引入了一种创新的方式，通过模拟内部认知过程来提高个性化识别的准确性。这种方法强调了通过视频和音频数据模拟个性化内部认知的必要性，并利用2D-GNN从这种认知模型中识别个性特征的有效性。", "conclusion": "本文通过引入个性化内部认知的模拟过程，提出了一种基于短音频视频行为的新型自动真实个性识别方法。这种方法使用2D图神经网络从解锁的内部认识到个性特征，具有潜在的改进普通识别性能的可能性。"}}
{"id": "2508.00360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00360", "abs": "https://arxiv.org/abs/2508.00360", "authors": ["Alan Dao", "Dinh Bach Vu", "Alex Nguyen", "Norapat Buppodom"], "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors", "comment": null, "summary": "Small language models (SLMs) are inherently limited in knowledge-intensive\ntasks due to their constrained capacity. While test-time computation offers a\npath to enhanced performance, most approaches treat reasoning as a fixed or\nheuristic process. In this work, we propose a new paradigm: viewing the model's\ninternal reasoning, delimited by <think> and </think> tags, as a dynamic task\nvector machine. Rather than treating the content inside these tags as a mere\ntrace of thought, we interpret the generation process itself as a mechanism\nthrough which the model \\textbf{constructs and refines its own task vectors} on\nthe fly. We developed a method to optimize this dynamic task vector machine\nthrough RLVR and successfully trained an agentic web-search model. We present\nLucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with\nMCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing\non par with much larger models such as DeepSeek-V3. This demonstrates that\nsmall models can rival large ones when equipped with structured,\nself-constructed task reasoning.", "AI": {"tldr": "本文提出了一种新的范式，即视模型内部思考为动态任务向量机，通过这种机制，小模型能够实时构建和优化任务向量，从而提升性能。本研究训练的Lucy模型，在SimpleQA基准上的表现与许多大型模型相当。", "motivation": "小语言模型（SLMs）在知识密集型任务中因容量受限而表现不足。尽管测试时计算提供了性能增强的途径，但大部分方法都将推理视为一个固定或基于启发式的过程。该研究动机在于通过动态任务向量机制改进小语言模型的性能，使其能够与大型模型竞争。", "method": "本研究提出了一种新的范式，即视模型内部思考为动态任务向量机。这种方法不同于将括在<think>和</think>标签之间的内容视为单纯的思想痕迹，而是将其生成过程视为一种机制，通过这种机制模型能够实时构建和优化自己的任务向量。研究还提到开发了一种使用RLVR优化这种动态任务向量机的方法，以及成功训练了一个具有代理功能的网络搜索模型。", "result": "本研究开发了一种方法，并成功训练了一个名为Lucy的具有17亿参数的小语言模型，该模型结合了这种动态推理机制和MCP集成，在SimpleQA基准上实现了78.3%的准确率，与许多大型模型表现相当。", "conclusion": "研究表明，当小语言模型装备有结构化的、自构建任务推理时，它们能够与较大的模型相匹敌。这表明通过合理设计和优化内部推理过程，小模型也能表现出良好的性能。"}}
{"id": "2508.00213", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00213", "abs": "https://arxiv.org/abs/2508.00213", "authors": ["Shayan Jalilian", "Abdul Bais"], "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters", "comment": null, "summary": "The Segment Anything Model (SAM) has demonstrated impressive generalization\nin prompt-based segmentation. Yet, the potential of semantic text prompts\nremains underexplored compared to traditional spatial prompts like points and\nboxes. This paper introduces SAM-PTx, a parameter-efficient approach for\nadapting SAM using frozen CLIP-derived text embeddings as class-level semantic\nguidance. Specifically, we propose a lightweight adapter design called\nParallel-Text that injects text embeddings into SAM's image encoder, enabling\nsemantics-guided segmentation while keeping most of the original architecture\nfrozen. Our adapter modifies only the MLP-parallel branch of each transformer\nblock, preserving the attention pathway for spatial reasoning. Through\nsupervised experiments and ablations on the COD10K dataset as well as low-data\nsubsets of COCO and ADE20K, we show that incorporating fixed text embeddings as\ninput improves segmentation performance over purely spatial prompt baselines.\nTo our knowledge, this is the first work to use text prompts for segmentation\non the COD10K dataset. These results suggest that integrating semantic\nconditioning into SAM's architecture offers a practical and scalable path for\nefficient adaptation with minimal computational complexity.", "AI": {"tldr": "本文提出了SAM-PTx，这是一种通过使用冻结的CLIP衍生文本嵌入来指导语义分割的参数高效方法，这使SAM能够更好地适应包含固定文本嵌入的输入，从而改进比纯粹基于空间提示的方法的分割性能。", "motivation": "段落任意模型（SAM）在基于提示的分割中表现出其强大的泛化性能，但相比于传统的基于点和框的空间提示，语义文本提示的潜力尚未得到充分研究。", "method": "本文提出了一种称为Parallel-Text的轻量级适配器设计，该设计将文本嵌入注入到SAM的图像编码器中，以便在保持大部分原始架构不变的情况下实现语义引导的分割。特别地，此适配器仅修改每个transformer块的MLP并行分支，保留了注意力路径用于空间推理。", "result": "实验验证了SAM-PTx在COD10K数据集以及COCO和ADE20K的低数据子集上的性能优于纯空间提示基线。", "conclusion": "研究表明，将语义条件整合到SAM的架构中是实现高效适应的实践且可扩展的方法，且计算复杂度极小。"}}
{"id": "2508.00370", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00370", "abs": "https://arxiv.org/abs/2508.00370", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "comment": "9 pages", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "AI": {"tldr": "论文提出了EdgeInfinite-Instruct，它通过分段监督微调和优化技术（如PTQ和固定形状计算图）解决了在边缘设备上部署长序列任务的效率和性能问题。", "motivation": "研究动机在于解决现有的大型语言模型在资源受限的边缘设备上的部署难题，特别是长序列任务中的计算和内存消耗问题，以及现有解决方案在减少首词生成时间（TTFT）和保持性能方面的不足。", "method": "该论文提出了一种名为EdgeInfinite-Instruct的方法，它采用了分段监督微调(S-SFT)策略来解决长序列任务（如摘要和问答）的挑战，并通过细粒度的后训练量化(PTQ)和固定形状的计算图来优化在边缘NPU上的部署，以降低计算需求并保持准确率。", "result": "实验表明，该方法在长期上下文基准和实际移动任务上提高了特定领域的性能，同时在NPU加速的边缘设备上保持了高效。", "conclusion": "EdgeInfinite-Instruct通过其创新策略和技术改进，在边缘设备上针对特定任务维持了性能和效率，克服了先前解决方案的局限性。"}}
{"id": "2508.00218", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00218", "abs": "https://arxiv.org/abs/2508.00218", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "title": "Object-Centric Cropping for Visual Few-Shot Classification", "comment": null, "summary": "In the domain of Few-Shot Image Classification, operating with as little as\none example per class, the presence of image ambiguities stemming from multiple\nobjects or complex backgrounds can significantly deteriorate performance. Our\nresearch demonstrates that incorporating additional information about the local\npositioning of an object within its image markedly enhances classification\nacross established benchmarks. More importantly, we show that a significant\nfraction of the improvement can be achieved through the use of the Segment\nAnything Model, requiring only a pixel of the object of interest to be pointed\nout, or by employing fully unsupervised foreground object extraction methods.", "AI": {"tldr": "研究显示，在少量样本图像分类任务中，利用目标对象在图像中的局部位置信息，可以显著提升分类性能。特别地，通过使用Segment Anything模型或无监督前景对象提取方法，可以实现大部分性能提升，而且只需要指出感兴趣对象的一个像素点即可。", "motivation": "旨在解决少量样本图像分类任务中由于图像歧义导致性能下降的问题，特别是当图像中包含多个对象或复杂背景的情况下。", "method": "通过引入目标对象在图像中的局部位置信息来改进图像分类性能。具体实现方法包括使用Segment Anything模型或无监督的前景对象提取技术。", "result": "在已建立的基准测试中，研究显示局部位置信息的引入显著提升了分类性能，而通过Segment Anything模型或无监督对象提取方法能够实现大部分提升效果，只需提供一个像素的信息。", "conclusion": "研究表明，在仅使用最少样本（一个实例/类别）的情况下，通过增加局部对象位置的信息可以极大地提高图像分类的准确性，而使用Segment Anything模型或无监督方法，甚至可以用最少的信息获取大幅度的性能改进。"}}
{"id": "2508.00385", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00385", "abs": "https://arxiv.org/abs/2508.00385", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "comment": null, "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "AI": {"tldr": "本文研究了示例无效的原因，提出了一种新的基于梯度流的示例选择方法GradS，并验证了其有效性。", "motivation": "现有的研究主要假设了上下文学习（ICL）中示例的有效性，但是并非所有示例都是有效的。本文旨在探究示例无效的原因，并提出一种新的示例选择方法。", "method": "基于梯度流动和线性自我注意力模型进行分析，并提出了一种新的方法GradS，使用示例对于给定用户查询的梯度流的大小作为选取标准。", "result": "实验证实了有效性差异随着模型层数增加而放大，并验证了GradS在四种LLMs和五个主流数据集上的有效性。", "conclusion": "结果表明，提出的GradS相比于最强的基线方法平均提高了6.8%，这证明了该方法的有效性。"}}
{"id": "2508.00248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00248", "abs": "https://arxiv.org/abs/2508.00248", "authors": ["Chenggang Guo", "Hao Xu", "XianMing Wan"], "title": "Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network", "comment": null, "summary": "Depth map super-resolution technology aims to improve the spatial resolution\nof low-resolution depth maps and effectively restore high-frequency detail\ninformation. Traditional convolutional neural network has limitations in\ndealing with long-range dependencies and are unable to fully model the global\ncontextual information in depth maps. Although transformer can model global\ndependencies, its computational complexity and memory consumption are\nquadratic, which significantly limits its ability to process high-resolution\ndepth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba\n(MSF-UM) model, a novel guided depth map super-resolution framework. The core\ninnovation of this model is to integrate Mamba's efficient state-space modeling\ncapabilities into a multi-scale U-shaped fusion structure guided by a color\nimage. The structure combining the residual dense channel attention block and\nthe Mamba state space module is designed, which combines the local feature\nextraction capability of the convolutional layer with the modeling advantage of\nthe state space model for long-distance dependencies. At the same time, the\nmodel adopts a multi-scale cross-modal fusion strategy to make full use of the\nhigh-frequency texture information from the color image to guide the\nsuper-resolution process of the depth map. Compared with existing mainstream\nmethods, the proposed MSF-UM significantly reduces the number of model\nparameters while achieving better reconstruction accuracy. Extensive\nexperiments on multiple publicly available datasets validate the effectiveness\nof the model, especially showing excellent generalization ability in the task\nof large-scale depth map super-resolution.", "AI": {"tldr": "The paper introduces the MSF-UM model for depth map super-resolution, which uses an innovative multi-scale fusion structure guided by color images to achieve better results with fewer parameters.", "motivation": "The motivation is to improve the spatial resolution and high-frequency detail restoration in depth maps while addressing the limitations of traditional CNNs and transformers in modeling global context and computational efficiency for high-resolution depth maps.", "method": "The paper proposes a multi-scale fusion U-shaped Mamba (MSF-UM) model for depth map super-resolution, integrating Mamba's efficient state-space modeling with a multi-scale U-shaped fusion structure guided by a color image.", "result": "Experiments demonstrate that the MSF-UM model achieves better reconstruction accuracy with fewer model parameters compared to existing methods and shows excellent performance in large-scale depth map super-resolution tasks.", "conclusion": "The conclusion is that the proposed MSF-UM model significantly enhances the accuracy and efficiency of depth map super-resolution, especially for high-resolution depth maps, and demonstrates strong generalization capabilities across various datasets."}}
{"id": "2508.00390", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00390", "abs": "https://arxiv.org/abs/2508.00390", "authors": ["Hengxing Cai", "Jinhan Dong", "Yijie Rao", "Jingcheng Deng", "Jingjun Tan", "Qien Chen", "Haidong Wang", "Zhen Wang", "Shiyu Huang", "Agachai Sumalee", "Renxin Zhong"], "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable\nagents to accurately localize targets and plan flight paths in complex\nenvironments based on natural language instructions, with broad applications in\nintelligent inspection, disaster rescue, and urban monitoring. Recent progress\nin Vision-Language Models (VLMs) has provided strong semantic understanding for\nthis task, while reinforcement learning (RL) has emerged as a promising\npost-training strategy to further improve generalization. However, existing RL\nmethods often suffer from inefficient use of training data, slow convergence,\nand insufficient consideration of the difficulty variation among training\nsamples, which limits further performance improvement. To address these\nchallenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling\n(SA-GCS)}, a novel training framework that systematically integrates Curriculum\nLearning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator\n(SA-DE) to quantify the complexity of training samples and a Gaussian\nCurriculum Scheduler (GCS) to dynamically adjust the sampling distribution,\nenabling a smooth progression from easy to challenging tasks. This design\nsignificantly improves training efficiency, accelerates convergence, and\nenhances overall model performance. Extensive experiments on the CityNav\nbenchmark demonstrate that SA-GCS consistently outperforms strong baselines\nacross all metrics, achieves faster and more stable convergence, and\ngeneralizes well across models of different scales, highlighting its robustness\nand scalability. The implementation of our approach is publicly available.", "AI": {"tldr": "本文针对UAV VLN任务，提出了SA-GCS方法，以实现更有效的训练、加速收敛并提升模型性能。", "motivation": "本文的动机是解决现有基于强化学习的方法在训练数据使用效率低、收敛慢以及训练样本难度差异考虑不足等问题，从而提高UAV VLN任务的性能改善。", "method": "本文提出了一种名为语义感知高斯课程调度（Semantic-Aware Gaussian Curriculum Scheduling，SA-GCS）的新训练框架，该框架系统地将课程学习（CL）整合到强化学习（RL）中。SA-GCS使用语义感知难度估计器（SA-DE）量化训练样本的复杂性，并使用高斯课程调度器（GCS）动态调整采样分布，实现从简单任务到复杂任务的平稳过渡。", "result": "大规模实验表明，SA-GCS在CityNav基准上一致优于强有力的基线，实现了更快更稳定的收敛，且在不同规模的模型上具有良好的泛化能力。", "conclusion": "该方法能够显著提高训练效率和模型性能，展现了其鲁棒性和可扩展性。其实现代码已公开。"}}
{"id": "2508.00259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00259", "abs": "https://arxiv.org/abs/2508.00259", "authors": ["Wentao Sun", "Hanqing Xu", "Quanyun Wu", "Dedong Zhang", "Yiping Chen", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting", "comment": "22 pages, 9 figures", "summary": "We introduce PointGauss, a novel point cloud-guided framework for real-time\nmulti-object segmentation in Gaussian Splatting representations. Unlike\nexisting methods that suffer from prolonged initialization and limited\nmulti-view consistency, our approach achieves efficient 3D segmentation by\ndirectly parsing Gaussian primitives through a point cloud segmentation-driven\npipeline. The key innovation lies in two aspects: (1) a point cloud-based\nGaussian primitive decoder that generates 3D instance masks within 1 minute,\nand (2) a GPU-accelerated 2D mask rendering system that ensures multi-view\nconsistency. Extensive experiments demonstrate significant improvements over\nprevious state-of-the-art methods, achieving performance gains of 1.89 to\n31.78% in multi-view mIoU, while maintaining superior computational efficiency.\nTo address the limitations of current benchmarks (single-object focus,\ninconsistent 3D evaluation, small scale, and partial coverage), we present\nDesktopObjects-360, a novel comprehensive dataset for 3D segmentation in\nradiance fields, featuring: (1) complex multi-object scenes, (2) globally\nconsistent 2D annotations, (3) large-scale training data (over 27 thousand 2D\nmasks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.", "AI": {"tldr": "PointGauss框架利用点云指导高斯溅射表示中的实时多目标分割，解决了现有方法的初始化慢和多视角一致性差的问题，通过实验验证了其在多视角mIoU上的性能提升。", "motivation": "现有方法在初始化和多视角一致性方面存在问题。因此，研究者提出了PointGauss框架，旨在实现高效的实时多目标分割，同时解决现有方法的局限性。", "method": "该框架主要包括两个关键创新点：(1)基于点云的高斯原语解码器，能够在一分钟内生成3D实例掩码；(2)加速2D掩码渲染系统，以确保多视角一致性。", "result": "实验结果显示，相较于先前的最佳方法，PointGauss在多视角mIoU上取得了1.89%到31.78%的性能提升，同时保持了极佳的计算效率。", "conclusion": "研究者还引入了DesktopObjects-360，这是一个用于辐射场中3D分割的新型综合数据集，旨在解决当前基准数据集中单一目标聚焦、不一致的3D评估、规模小和部分覆盖的问题。"}}
{"id": "2508.00420", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00420", "abs": "https://arxiv.org/abs/2508.00420", "authors": ["Rana Salama", "Abdou Youssef", "Mona Diab"], "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding", "comment": null, "summary": "Wavelets have emerged as a cutting edge technology in a number of fields.\nConcrete results of their application in Image and Signal processing suggest\nthat wavelets can be effectively applied to Natural Language Processing (NLP)\ntasks that capture a variety of linguistic properties. In this paper, we\nleverage the power of applying Discrete Wavelet Transforms (DWT) to word and\nsentence embeddings. We first evaluate, intrinsically and extrinsically, how\nwavelets can effectively be used to consolidate important information in a word\nvector while reducing its dimensionality. We further combine DWT with Discrete\nCosine Transform (DCT) to propose a non-parameterized model that compresses a\nsentence with a dense amount of information in a fixed size vector based on\nlocally varying word features. We show the efficacy of the proposed paradigm on\ndownstream applications models yielding comparable and even superior (in some\ntasks) results to original embeddings.", "AI": {"tldr": "研究提出并验证了一种基于小波变换和余弦变换的非参数模型，用于压缩NLP中的词和句子嵌入，实验证明该模型可以在减少嵌入维数的同时保持或提高任务效果。", "motivation": "由于小波变换在图像和信号处理方面取得了切实成果，研究旨在探索小波变换在自然语言处理中的应用潜力，特别是如何减少嵌入维数并保持信息。", "method": "该研究采用离散小波变换(DWT)应用于词和句子嵌入，同时结合离散余弦变换(DCT)，提出了一种无参数模型，用于压缩包含局部变化词汇特征的句子到一个固定大小的向量中，该模型可以有效地整合并减少词向量的维度，同时保留重要信息。", "result": "实验结果展示了该模型在下游任务上的有效性，且在某些任务中，其效果甚至优于原始嵌入。", "conclusion": "研究表明，通过结合DWT和DCT对词向量进行信息整合和维数压缩的方法是有效的，并且在一些NLP任务中可以取得与原始嵌入相当甚至更优的结果。"}}
{"id": "2508.00260", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00260", "abs": "https://arxiv.org/abs/2508.00260", "authors": ["Hyundong Jin", "Hyung Jin Chang", "Eunwoo Kim"], "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models", "comment": "Accepted to ICCV 2025", "summary": "Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.", "AI": {"tldr": "研究提出了一种新的框架和方法，通过混合视觉投影器以基于具体指令上下文进行视觉到语言的翻译，改进了传统的持续学习方法，有效提升了模型对新视觉-语言任务的适应性。", "motivation": "先前的方法在通过调整视觉投影器来适应新任务时，可能会导致模型过分依赖视觉输入而忽视语言指令，特别是在具有重复类型的文本指令的学习任务中。为了克服这一问题并更充分地利用语言指令，本研究提出了新的框架和方法。", "method": "本研究提出了一种新的框架，通过将视觉信息的翻译基于语言模型的指令来进行。该框架引入了一种视觉投影器的混合方式，每个投影器作为根据给定指令上下文的专门视觉到语言翻译专家，以适应新的任务。此外，还提出了一种专家推荐策略，用于重用与之前学习的任务相似的专家，并采用专家剪枝方法来避免之前任务中累积激活的专家造成干扰。", "result": "在多样化的视觉语言任务中的广泛实验表明，该方法在生成遵循指令的响应方面优于现有的持续学习方法。", "conclusion": "通过使用专家混合和剪枝方法，该研究在连续学习中实现了一种更有效的方式，并提高了生成指令响应的性能。"}}
{"id": "2508.00429", "categories": ["cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00429", "abs": "https://arxiv.org/abs/2508.00429", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "AI": {"tldr": "本文提出了Retrieval-augmented Graph Agentic Network (ReaGAN)，这是一个基于代理的框架，旨在解决图神经网络中节点信息不平衡和全局语义关系忽略的问题。通过节点级的自动决策和检索增强生成（RAG），ReaGAN在少样本情况下无需微调即可实现优越性能。", "motivation": "标准的图神经网络在处理节点信息不平衡及忽略全局语义关系的问题上表现不佳，这限制了模型捕捉远程但相关数据的能力。为了改进这些问题，作者提出了一种基于代理的方法来增强节点的信息处理能力。", "method": "作者提出了ReaGAN架构，该架构通过赋予每个节点代理角色来实现自主决策和信息传播。每个节点能够根据其内部存储的信息决定其下一步行动，并辅以检索增强生成（RAG）技术以整合全局信息。", "result": "{", "conclusion": "研究通过实验证明，即使使用冻结的预训练语言模型（LLM）而不进行微调，ReaGAN也能在少量示例的场景中取得良好效果，展示了代理规划和局部-全局检索在图学习中的应用潜力。"}}
{"id": "2508.00265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00265", "abs": "https://arxiv.org/abs/2508.00265", "authors": ["Henghui Ding", "Song Tang", "Shuting He", "Chang Liu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Multimodal Referring Segmentation: A Survey", "comment": "Project Page:\n  https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation", "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", "AI": {"tldr": "A comprehensive survey on multimodal referring segmentation detailing its background, datasets, unified architecture, and methods for images, videos, and 3D scenes, with focus on GREx for real-world complexity.", "motivation": "The purpose is to summarize the advancements in multimodal referring segmentation due to improvements in convolutional neural networks, transformers, and large language models, which have enhanced multimodal perception capabilities.", "method": "This paper provides a comprehensive survey of multimodal referring segmentation, focusing on a unified meta architecture for various visual scenes and Generalized Referring Expression (GREx) methods.", "result": "The paper outlines the background, definitions, datasets, and key methods of multimodal referring segmentation, providing performance evaluations on standard benchmarks.", "conclusion": "The survey highlights the importance of multimodal referring segmentation for practical applications, outlining the state-of-the-art methods and indicating the need for further research to address real-world complexity."}}
{"id": "2508.00454", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00454", "abs": "https://arxiv.org/abs/2508.00454", "authors": ["Yuqi Tang", "Kehua Feng", "Yunfeng Wang", "Zhiwen Chen", "Chengfei Lv", "Gang Yu", "Qiang Zhang", "Keyan Ding"], "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges", "comment": "15 pages, 2 pages, under review at AAAI 2026", "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.", "AI": {"tldr": "The paper introduces an efficient multi-turn dialogue evaluator that consolidates multiple LLMs' preferences into a single model, offering cost-effective and reliable dialogue quality assessment.", "motivation": "The motivation is to address the biases inherent in current evaluation methods by leveraging multiple LLMs' judgments efficiently, thus providing a more reliable and consistent evaluation at a lower computational cost.", "method": "The paper proposes a method that aggregates the preference knowledge of multiple LLMs into a single model to evaluate dialogue quality, aiming to retain diverse feedback while reducing computational cost.", "result": "Experiments on seven benchmarks showed that the proposed method outperformed existing baselines in both single rating and pairwise comparison dialogue evaluation, indicating its robustness and efficiency.", "conclusion": "In conclusion, the proposed method enhances the evaluation of dialogue quality by efficiently aggregating the wisdom of multiple LLMs into a single model, leading to more reliable assessments with reduced computational overhead."}}
{"id": "2508.00272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00272", "abs": "https://arxiv.org/abs/2508.00272", "authors": ["Wenyue Chong"], "title": "Towards Robust Semantic Correspondence: A Benchmark and Insights", "comment": null, "summary": "Semantic correspondence aims to identify semantically meaningful\nrelationships between different images and is a fundamental challenge in\ncomputer vision. It forms the foundation for numerous tasks such as 3D\nreconstruction, object tracking, and image editing. With the progress of\nlarge-scale vision models, semantic correspondence has achieved remarkable\nperformance in controlled and high-quality conditions. However, the robustness\nof semantic correspondence in challenging scenarios is much less investigated.\nIn this work, we establish a novel benchmark for evaluating semantic\ncorrespondence in adverse conditions. The benchmark dataset comprises 14\ndistinct challenging scenarios that reflect commonly encountered imaging\nissues, including geometric distortion, image blurring, digital artifacts, and\nenvironmental occlusion. Through extensive evaluations, we provide several key\ninsights into the robustness of semantic correspondence approaches: (1) All\nexisting methods suffer from noticeable performance drops under adverse\nconditions; (2) Using large-scale vision models can enhance overall robustness,\nbut fine-tuning on these models leads to a decline in relative robustness; (3)\nThe DINO model outperforms the Stable Diffusion in relative robustness, and\ntheir fusion achieves better absolute robustness; Moreover, We evaluate common\nrobustness enhancement strategies for semantic correspondence and find that\ngeneral data augmentations are ineffective, highlighting the need for\ntask-specific designs. These results are consistent across both our dataset and\nreal-world benchmarks.", "AI": {"tldr": "研究建立新基准评估语义对应关系在恶劣条件下的鲁棒性，发现所有现有方法表现不足，大规模视觉模型能提升鲁棒性，但微调会减弱，DINO与Stable Diffusion融合表现最佳，需针对任务加强鲁棒性设计。", "motivation": "语义对应在理想条件下取得了显著成果，但在具有挑战性的场景中的鲁棒性研究较少。为了弥补这一不足，本文提出一个在恶劣条件下评估语义对应关系的新基准。", "method": "本文构建了一个新的基准来评估在恶劣条件下的语义对应关系，该基准数据集包含了14种不同的具有挑战性的场景，例如几何失真、图像模糊、数字伪影和环境遮挡等。", "result": "经过广泛的评估，研究发现所有现有方法在恶劣条件下的表现都有所减弱，使用大规模视觉模型虽然有助于整体鲁棒性的提升，但在这些模型上的微调反而导致相对鲁棒性的减弱；DINO模型在相对鲁棒性上优于Stable Diffusion，而两者融合则在绝对鲁棒性上表现更佳。", "conclusion": "常规的数据增强策略对于提高语义对应的鲁棒性并无显著效果，提示需要进行任务特异性设计。这些结果在作者构建的数据集和真实世界基准中都是一致的。"}}
{"id": "2508.00476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00476", "abs": "https://arxiv.org/abs/2508.00476", "authors": ["Jeongwoo Kang", "Markarit Vartampetian", "Felix Herron", "Yongxin Zhou", "Diandra Fabre", "Gabriela Gonzalez-Saez"], "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts", "comment": null, "summary": "This paper documents GETALP's submission to the Third Run of the Automatic\nMinuting Shared Task at SIGDial 2025. We participated in Task B:\nquestion-answering based on meeting transcripts. Our method is based on a\nretrieval augmented generation (RAG) system and Abstract Meaning\nRepresentations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for\napproximately 35% of the questions and provides notable improvements in\nanswering questions that involve distinguishing between different participants\n(e.g., who questions).", "AI": {"tldr": "The paper describes GETALP's participation in a shared task, using a combination of RAG and AMR to improve question-answering on meeting transcripts.", "motivation": "The motivation of this paper is to participate in Task B of the Third Run of the Automatic Minuting Shared Task at SIGDial 2025, which is question-answering based on meeting transcripts.", "method": "Our method combines retrieval augmented generation (RAG) system with Abstract Meaning Representations (AMR) to propose three systems that aim to improve question-answering based on meeting transcripts.", "result": "The results indicate that the incorporation of AMR leads to high-quality responses for about 35% of the questions and improves the ability to distinguish between different participants, particularly for 'who' type questions.", "conclusion": "The conclusion is that combining RAG with AMR enhances the system's performance in answering questions from meeting transcripts, especially those involving participant distinction."}}
{"id": "2508.00287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00287", "abs": "https://arxiv.org/abs/2508.00287", "authors": ["Tran Viet Khoa", "Do Hai Son", "Mohammad Abu Alsheikh", "Yibeltal F Alem", "Dinh Thai Hoang"], "title": "Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning", "comment": null, "summary": "Driver drowsiness is one of the main causes of road accidents and is\nrecognized as a leading contributor to traffic-related fatalities. However,\ndetecting drowsiness accurately remains a challenging task, especially in\nreal-world settings where facial data from different individuals is\ndecentralized and highly diverse. In this paper, we propose a novel framework\nfor drowsiness detection that is designed to work effectively with\nheterogeneous and decentralized data. Our approach develops a new Spatial\nSelf-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)\nnetwork to better extract key facial features and improve detection\nperformance. To support federated learning, we employ a Gradient Similarity\nComparison (GSC) that selects the most relevant trained models from different\noperators before aggregation. This improves the accuracy and robustness of the\nglobal model while preserving user privacy. We also develop a customized tool\nthat automatically processes video data by extracting frames, detecting and\ncropping faces, and applying data augmentation techniques such as rotation,\nflipping, brightness adjustment, and zooming. Experimental results show that\nour framework achieves a detection accuracy of 89.9% in the federated learning\nsettings, outperforming existing methods under various deployment scenarios.\nThe results demonstrate the effectiveness of our approach in handling\nreal-world data variability and highlight its potential for deployment in\nintelligent transportation systems to enhance road safety through early and\nreliable drowsiness detection.", "AI": {"tldr": "A novel framework combines SSA and LSTM for drowsiness detection, using federated learning and a customized tool for video processing, achieving an accuracy of 89.9%.", "motivation": "The key motivation is to address the problem of accurately detecting driver drowsiness using decentralized and diverse facial data, which is critical for enhancing road safety.", "method": "Our approach includes a novel Spatial Self-Attention (SSA) mechanism combined with a Long Short-Term Memory (LSTM) network for improved drowsiness detection. We also use a Gradient Similarity Comparison (GSC) for federated learning and a customized tool for video data processing.", "result": "The detection accuracy achieved by our framework in the federated learning setting is 89.9%, outperforming existing methods under various deployment scenarios.", "conclusion": "The proposed framework effectively handles real-world data variability, highlighting its potential for deployment in intelligent transportation systems to improve road safety through early and reliable drowsiness detection."}}
{"id": "2508.00489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00489", "abs": "https://arxiv.org/abs/2508.00489", "authors": ["Yixuan Tang", "Jincheng Wang", "Anthony K. H. Tung"], "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth Detection", "comment": null, "summary": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.", "AI": {"tldr": "论文提出半真陈述检测任务，包含新的基准PolitiFact-Hidden和TRACER框架，用于识别基于省略的误导性陈述，提高了事实验证系统的性能。", "motivation": "现有事实验证系统通常假设一个声明的真实性完全依赖于其所述的事实内容。然而，很多现实世界中的声明是半真半假的，虽然事实正确但可能误导，因为忽略了关键背景。这些模型并不擅长处理疏忽关键背景的情况，所以需要开发新的方法。", "method": "TRACER是一个模块化再评估框架，它结合证据对齐、推断隐含意图和估计隐藏内容的因果影响来检测基于省略的不真实信息。", "result": "论文引入了半真半假检测任务，并提出了一个包含15000个政治声明的新基准PolitiFact-Hidden。这些声明都标注了句子级别的证据对齐和推断声明意图。为应对这一挑战，论文提出了TRACER，一个模块化再评估框架，它通过证据对齐、推断隐含意图以及估计隐藏内容的因果影响来识别以省略为基础的错误信息。TRACER可以嵌入到现有的事实核查管道中，并且在多个强基线上一致提高了性能，特别是在半真分类F1分数上提高了多达16个点。", "conclusion": "TRACER显著提升了半真半假声明检测的性能，表明为可信的事实验证建模遗漏内容的重要性。"}}
{"id": "2508.00289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00289", "abs": "https://arxiv.org/abs/2508.00289", "authors": ["Christian Simon", "Masato Ishii", "Akio Hayakawa", "Zhi Zhong", "Shusuke Takahashi", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models", "comment": "Accepted to ICCV 2025", "summary": "In the recent development of conditional diffusion models still require heavy\nsupervised fine-tuning for performing control on a category of tasks.\nTraining-free conditioning via guidance with off-the-shelf models is a\nfavorable alternative to avoid further fine-tuning on the base model. However,\nthe existing training-free guidance frameworks either have heavy memory\nrequirements or offer sub-optimal control due to rough estimation. These\nshortcomings limit the applicability to control diffusion models that require\nintense computation, such as Text-to-Video (T2V) diffusion models. In this\nwork, we propose Taming Inference Time Alignment for Guided Text-to-Video\nDiffusion Model, so-called TITAN-Guide, which overcomes memory space issues,\nand provides more optimal control in the guidance process compared to the\ncounterparts. In particular, we develop an efficient method for optimizing\ndiffusion latents without backpropagation from a discriminative guiding model.\nIn particular, we study forward gradient descents for guided diffusion tasks\nwith various options on directional directives. In our experiments, we\ndemonstrate the effectiveness of our approach in efficiently managing memory\nduring latent optimization, while previous methods fall short. Our proposed\napproach not only minimizes memory requirements but also significantly enhances\nT2V performance across a range of diffusion guidance benchmarks. Code, models,\nand demo are available at https://titanguide.github.io.", "AI": {"tldr": "提出了TITAN-Guide方法，解决了现有技术在文字到视频扩散模型应用场景下的内存和控制效果问题，优化了扩散潜变量的优化过程，提高了T2V模型的性能。", "motivation": "在条件扩散模型的发展中，仍然需要大量的监督微调来进行任务类别的控制。训练自由的引导框架要么内存需求巨大，要么由于粗略的估计导致控制效果不佳。这限制了对需要大量计算控制的扩散模型的应用，如文本到视频（T2V）扩散模型。为此我们提出了TITAN-Guide。", "method": "我们提出了一种名为TITAN-Guide的方法，该方法在推理过程中对齐指导，解决了内存空间问题，并且在引导过程中提供了更优的控制。特别地，我们开发了一种无需反向传播的高效方法来优化扩散潜变量。此外，我们研究了不同导向指令下的正向梯度下降在引导扩散任务中的应用。", "result": "实验表明，我们的方法能够在潜变量优化过程中有效地管理内存，而之前的方法在这方面有不足。我们提出的方法不仅降低了内存需求，而且在扩散指导基准上的T2V性能得到了显著提升。", "conclusion": "我们提出的方法有效地解决了现有技术在T2V模型应用中的内存问题和控制效果不佳的问题，其在扩散指导任务中的高效内存管理和优化效果显著优于现有方法。"}}
{"id": "2508.00522", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00522", "abs": "https://arxiv.org/abs/2508.00522", "authors": ["Jiaxin Deng", "Qingcheng Zhu", "Junbiao Pang", "Linlin Yang", "Zhongqian Fu", "Baochang Zhang"], "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond", "comment": null, "summary": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.", "AI": {"tldr": "This paper introduces Flat-LoRA and EFlat-LoRA methods to find flat minima for LoRA, improving generalization. EFlat-LoRA shows significant performance improvements over LoRA and full fine-tuning.", "motivation": "The correlation between expressive ability and generalization ability of LoRA has not been fully explored. This work aims to find flat minima for LoRA to improve its generalization.", "method": "Our method, Flat-LoRA, and its efficient version EFlat-LoRA, aim to seek flat minima for low-rank adaptation (LoRA). Theoretical demonstration and empirical verification are provided.", "result": "EFlat-LoRA achieves comparable or better performance than LoRA and full fine-tuning, with improvements of 1.0% and 0.5% on the GLUE dataset and 1.5% and 1.0% on SQA and VizWiz datasets for vision-language models.", "conclusion": "The method proposed, EFlat-LoRA, successfully finds flat minima and achieves better or comparable performance with LoRA and full fine-tuning while maintaining computational efficiency."}}
{"id": "2508.00298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00298", "abs": "https://arxiv.org/abs/2508.00298", "authors": ["Jin Lyu", "Liang An", "Li Lin", "Pujin Cheng", "Yebin Liu", "Xiaoying Tang"], "title": "AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer", "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00837", "summary": "In the era of foundation models, achieving a unified understanding of\ndifferent dynamic objects through a single network has the potential to empower\nstronger spatial intelligence. Moreover, accurate estimation of animal pose and\nshape across diverse species is essential for quantitative analysis in\nbiological research. However, this topic remains underexplored due to the\nlimited network capacity of previous methods and the scarcity of comprehensive\nmulti-species datasets. To address these limitations, we introduce AniMer+, an\nextended version of our scalable AniMer framework. In this paper, we focus on a\nunified approach for reconstructing mammals (mammalia) and birds (aves). A key\ninnovation of AniMer+ is its high-capacity, family-aware Vision Transformer\n(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture\npartitions network layers into taxa-specific components (for mammalia and aves)\nand taxa-shared components, enabling efficient learning of both distinct and\ncommon anatomical features within a single model. To overcome the critical\nshortage of 3D training data, especially for birds, we introduce a\ndiffusion-based conditional image generation pipeline. This pipeline produces\ntwo large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for\nbirds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for\nbirds, which is crucial for resolving single-view depth ambiguities. Trained on\nan aggregated collection of 41.3k mammalian and 12.4k avian images (combining\nreal and synthetic data), our method demonstrates superior performance over\nexisting approaches across a wide range of benchmarks, including the\nchallenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the\neffectiveness of both our novel network architecture and the generated\nsynthetic datasets in enhancing real-world application performance.", "AI": {"tldr": "AniMer+, an extended version of AniMer framework, uses a high-capacity, family-aware Vision Transformer (ViT) with a Mixture-of-Experts (MoE) design to reconstruct mammals and birds, introducing a diffusion-based pipeline for generating synthetic datasets, demonstrating superior performance on various benchmarks.", "motivation": "To develop a unified approach for accurately estimating the pose and shape of different species, addressing the limitations of previous methods in terms of network capacity and the lack of comprehensive multi-species datasets.", "method": "AniMer+ uses a high-capacity, family-aware Vision Transformer (ViT) with a Mixture-of-Experts (MoE) design. Networks are partitioned into taxa-specific and taxa-shared components. A diffusion-based conditional image generation pipeline produces synthetic datasets for training, namely CtrlAni3D for quadrupeds and CtrlAVES3D for birds.", "result": "The method demonstrates superior performance across a wide range of benchmarks, including the challenging out-of-domain Animal Kingdom dataset, indicating improved real-world application performance.", "conclusion": "The novel architecture and synthetic datasets generated by AniMer+ significantly enhance performance in pose and shape estimation for mammals and birds, opening new avenues for biological research and spatial intelligence applications."}}
{"id": "2508.00537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00537", "abs": "https://arxiv.org/abs/2508.00537", "authors": ["Giulio Zhou", "Tsz Kin Lam", "Alexandra Birch", "Barry Haddow"], "title": "The Prosody of Emojis", "comment": null, "summary": "Prosodic features such as pitch, timing, and intonation are central to spoken\ncommunication, conveying emotion, intent, and discourse structure. In\ntext-based settings, where these cues are absent, emojis act as visual\nsurrogates that add affective and pragmatic nuance. This study examines how\nemojis influence prosodic realisation in speech and how listeners interpret\nprosodic cues to recover emoji meanings. Unlike previous work, we directly link\nprosody and emoji by analysing actual human speech data, collected through\nstructured but open-ended production and perception tasks. This provides\nempirical evidence of how emoji semantics shape spoken delivery and perception.\nResults show that speakers adapt their prosody based on emoji cues, listeners\ncan often identify the intended emoji from prosodic variation alone, and\ngreater semantic differences between emojis correspond to increased prosodic\ndivergence. These findings suggest that emojis can act as meaningful carriers\nof prosodic intent, offering insight into their communicative role in digitally\nmediated contexts.", "AI": {"tldr": "研究表明，说话者依据表情符号调整韵律，而听者能通过韵律变化识别表情符号的含义。表情符号在线上交流中的语用功能得到了验证。", "motivation": "研究表情符号如何影响语音中的韵律实现以及听者如何通过韵律线索恢复表情符号的意义，填补了相关研究的空白。", "method": "通过结构化但开放式生产和感知任务收集的实际人类语音数据，直接分析了表情符号与韵律之间的联系。", "result": "结果表明说话者依据表情符号调整他们的韵律，听者经常能仅凭韵律变化识别出预期的表情符号，表情符号之间的语义差异越大，其韵律表现越不同。", "conclusion": "表情符号作为韵律意图的有意义载体作用明显，为研究其在线交流中的沟通角色提供了新的见解。"}}
