<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.CV](#cs.CV) [Total: 38]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

> 该论文探讨了量子自然语言处理在自然语言推理任务中的应用，表明量子模型在few-shot设置下，可以使用较少的参数实现高效率的语义理解和推理，展示了其在低资源环境中的应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索QNLP模型在NLI任务中的应用，并通过比较量子、混合及经典Transformer模型来评估它们在few-shot设置下的表现。

**Method:** 该研究使用lambeq库和DisCoCat框架构建用于句子对的参数化量子电路，旨在进行语义相关性和推理分类任务。同时引入了一个新的信息论度量标准IGPP来评估训练效率。

**Result:** 实验结果显示，量子模型在参数数量大幅减少的情况下仍然能够达到与经典模型相当的性能。量子模型在推理任务上优于随机初始化的Transformer，并且在相关性任务上测试误差更低。量子模型的每个参数学习效率比经典模型更高，最多高出五个数量级。

**Conclusion:** 该研究展示了QNLP在资源受限和结构敏感环境下的潜力，并提出了一种新的集群架构，通过将门参数与学习的词簇关联起来，而不是单个的词汇单位，以提高泛化能力。

**Abstract:** Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

> 研究者介绍了一种利用ChatGPT和Claude两种大型语言模型的多模态融合框架，此种框架能够通过结合图像与合成文本提高胸部X光片解读的准确性，将共识后的诊断准确率提升至91.3%。

<details>
  <summary>Details</summary>

**Motivation:** 此研究旨在利用多模态融合框架，通过强化不同输入模态的信息互补性和输出一致性，提高AI辅助放射学诊断的可靠性和临床效用，以降低诊断错误，并尽可能减少计算资源的消耗。

**Method:** 本研究提出了一个利用两个最先进的大型语言模型（LLMs）——ChatGPT和Claude的多模态融合框架，以提高CheXpert数据集中胸部X光片解读的可靠性。研究首先在224,316张胸部X光片中随机选取了234个放射科医生标注的研究案例，使用仅包含图像的提示评估单一模型的表现；随后，在多模态输入的情况下，生成了符合MIMIC-CXR模板的合成临床笔记，并评估了50个随机选择的样本与图像和合成文本配对后的表现。

**Result:** 单一模型使用图像提示时，ChatGPT和Claude的诊断准确率分别为62.8%和76.9%。采用95%输出相似性阈值的共识方法可将准确率提高至77.6%。而在多模态输入条件下，两者的准确率分别提高到84%和76%，共识准确率达到了91.3%。

**Conclusion:** 这些结果表明，将不同的信息模态相结合，并使用输出水平的一致性来提升AI辅助放射性诊断的信任度和临床实用性，为提高诊断准确率并减少诊断错误提供了一种实际的方法。

**Abstract:** This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

> Evaluation of self-correction strategies for LLMs shows promise but highlights ongoing efficiency challenges, suggesting a need for further optimization.

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of self-correction methods in LLMs and determine if LLMs can significantly improve their reasoning performance through self-correction.

**Method:** Self-correction strategies for LLMs are evaluated using CorrectBench across three tasks: commonsense reasoning, mathematical reasoning, and code generation. The strategies include intrinsic, external, and fine-tuned methods.

**Result:** Self-correction improves LLM accuracy, especially in complex reasoning tasks. Combining methods further improves accuracy but reduces efficiency. Some LLMs have limited improvement and high time costs with added self-correction. A simple CoT baseline shows good performance.

**Conclusion:** Self-correction holds potential for enhancing LLM reasoning performance but presents challenges in balancing with operational efficiency.

**Abstract:** Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

> 文章介绍了一个名为EvolveR的框架，该框架通过闭环学习过程，使代理能够从自己的经验中进行自我改进，在复杂基准测试中表现优异。代码在GitHub上可用。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型（LLM）表现出强大的工具使用能力，但缺乏至关重要地系统性学习自身经验的能力。虽然现有的框架主要集中在缓解外部知识缺口上，但它们未能解决更基础的局限性：即无法迭代改进问题解决策略。

**Method:** 本文提出了EvolveR框架，该框架旨在通过完整且闭环的经验生命周期使代理自我改进。这个生命周期包括两个关键阶段：离线自我蒸馏，即合成代理的交互轨迹，以形成一个结构化的、抽象的、可重复使用的战略原则存储库；在线交互，在此阶段，代理与任务互动，主动检索提炼出的原则指导其决策制定，并积累一组多样的行为轨迹。这一循环通过策略强化机制迭代更新代理的性能。

**Result:** 本文展示了EvolveR在复杂多跳问题解答基准测试中的有效性，其性能优于强大的代理基线。

**Conclusion:** 研究提出了一个全面的蓝图，供代理不仅从外部数据，也从自身行为后果中学习，为更自主且持续改进的系统铺平道路。

**Abstract:** Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

> 研究了六个LLMs在不同提示类型下的表现，发现CoT-少样本提示最可靠，GPT-4o和DeepSeek有较强的综合性能，GPT-4o-mini成本低且性能具有竞争力。推荐使用低成本模型带结构化提示进行初步筛选，并将边界情况分派给高容量模型。

<details>
  <summary>Details</summary>

**Motivation:** 量化提示策略如何与大型语言模型互动，以自动化系统文献回顾的筛选阶段。

**Method:** 评估六个大型语言模型(LLMs)在五种提示类型下的表现，包括零样本、少样本、链式思考(CoT)、CoT-少样本、自我反思，针对相关性分类和六个二级任务，使用准确率、精度、召回率和F1分数作为评价指标。

**Result:** 研究显示模型-提示交互效应显著：CoT-少样本提示在精度-召回率平衡中表现最可靠；零样本提示最大化高敏感性筛选中的召回率；自我反思由于过度包含性和模型间的波动表现较差。GPT-4o和DeepSeek具有较强的综合性能，而GPT-4o-mini在大幅降低成本的同时表现依然具有竞争力。针对相关性分类（每处理1000个摘要），不同模型-提示组合间存在显著的绝对差异，GPT-4o-mini在所有提示下都保持较低的成本，而结构化的提示（CoT/CoT-少样本）应用于GPT-4o-mini上则以微小的成本增量提供有吸引力的F1分数。

**Conclusion:** 这些发现强调了LLMs不平衡但有前景的潜力，可以用于自动化文献筛选。通过系统分析提示-模型间的相互作用，提供了任务适应型LLM部署的对比基准和实用指南。

**Abstract:** This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

> 通过灵活的合成测试床，本文研究了语言模型中统计规律和事实关联的相互作用，发现多样性对泛化能力的影响依赖于上下文结构，也强调了模型组件的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管近期研究表明统计规律和事实关联的可变性对泛化能力至关重要，但我们缺乏对这些影响的系统性分析。

**Method:** 本文介绍了结合通用标记的统计流与源目标标记对的事实流的灵活合成测试床，用于分析语言模型中的统计规律和事实关联之间的相互作用。通过调整流的组成和统计流中的事实出现情况，可以独立控制多样性特征和多样性水平。

**Result:** 研究表明，更高的上下文多样性虽然会延迟分布内（ID）事实的准确性，但其对分布外（OOD）事实泛化的影响取决于上下文结构。在某些情况下，多样性对非平凡的事实回忆至关重要，甚至在低多样性情况下也存在最优的多样性水平。

**Conclusion:** 本文展示了上下文设计和多样性水平之间的相互作用如何影响不同的泛化方面，并通过一系列对模型组件的控制干预，追踪OOD失败的原因并强调了嵌入层和解嵌层的重要性。

**Abstract:** Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

> 本文通过多年的Reddit数据研究了公众对生成式AI（GenAI）的信任和不信任，揭示了影响态度的技术维度和个体差异。

<details>
  <summary>Details</summary>

**Motivation:** 由于以往对AI的信任研究大多依赖于心理学和人机交互方法，且缺乏大规模的计算和纵向研究，这篇文章提出了首个对GenAI和大型语言模型的信任和不信任的计算研究。

**Method:** 使用了多年的Reddit数据集（2022-2025），涵盖了39个子版块和197,618篇帖子，结合众包注释和分类模型进行分析。

**Result:** 发现信任和不信任的态度在时间上几乎持平，并且在主要模型发布时发生变化。性能和技术的实用性是最主要的考量维度，而个人经验则是影响态度形成的最常见原因。不同对象（如专家、伦理学家、普通用户）也有各自不同的模式。

**Conclusion:** 研究结果为大规模信任分析提供了一个方法框架，并深入理解了公众对GenAI态度的演变。

**Abstract:** The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [8] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

> 我们介绍了EgMM-Corpus，一个专注于埃及文化的多模态数据集，用于评估和训练视觉语言模型。EgMM-Corpus显示了现有大规模视觉语言模型存在的文化偏差。

<details>
  <summary>Details</summary>

**Motivation:** 尽管人工智能领域取得了进展，但多模态文化多样性数据集仍然有限，特别是在中东和非洲地区。本文旨在引入一个专注于埃及文化的多模态数据集。

**Method:** 我们设计并执行了一个新的数据收集流程，收集了超过3000张图像，涵盖了313个概念，包括地标、食物和民间传说。每个数据条目都经过手动验证，确保文化真实性和多模态一致性。

**Result:** 我们评估了在EgMM-Corpus上CLIP的零样本性能，其在分类任务上的Top-1和Top-5准确性分别为21.2%和36.4%。这强调了大规模视觉语言模型中存在的文化偏见。

**Conclusion:** EgMM-Corpus作为一个基准数据集，对开发文化感知模型具有重要意义。

**Abstract:** Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [9] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

> 文章提出理论框架研究语言模型对语法的理解并验证了三个关键假设。

<details>
  <summary>Details</summary>

**Motivation:** 探讨语言模型对语法的理解程度，分析字符串概率如何揭示语言模型的语法知识。

**Method:** 基于语料库数据生成过程的简单假设，本文提出了一种理论分析框架，用以探讨语法、意义和字符串概率之间的关系。

**Result:** 通过分析英语和中文的280,000个最小对，并验证了三个预测：最小对字符串的概率间的相关性，模型和人类的最小对差异的相关性，以及无配对语法和不语法字符串在概率空间中的差分程度较低。

**Conclusion:** 此分析为使用概率了解语言模型的结构知识提供了理论依据，并为未来的语言模型语法评估研究指明了方向。

**Abstract:** What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [10] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

> Proposed pluralistic decoding and model steering to improve language model alignment and reflect diverse perspectives with minimal training resources.

<details>
  <summary>Details</summary>

**Motivation:** Modern language models often assume there is one optimal answer for every query, leading to generic responses and poor alignment. The aim is to enhance pluralistic alignment in a low-resource setting.

**Method:** Our proposed methods include pluralistic decoding and model steering to enhance pluralistic alignment in a low-resource setting.

**Result:** Empirically demonstrated consistent improvement over zero-shot and few-shot baselines with only 50 annotated samples. Decreased false positives in hate speech detection, misinformation detection, and improved distributional alignment to human values in GlobalOpinionQA.

**Conclusion:** Highlight the importance of diversity and how language models can be adapted to consider nuanced perspectives.

**Abstract:** As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [11] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

> 本文提出 Profile-to-PEFT 框架，使用超网络将用户的编码资料直接映射到适配器参数，实现快速适应、未见用户泛化和隐私保护，且在资源消耗上更高效。

<details>
  <summary>Details</summary>

**Motivation:** 现有的参数高效微调（PEFT）方法，比如为每个用户训练一个适配器的单个PEFT（OPPU）范式，计算成本高且不适用于实时更新。为解决此问题，引入了 Profile-to-PEFT 框架。

**Method:** 通过使用超网络，Profile-to-PEFT 能够直接将用户的编码资料映射到一组完整的适配器参数（例如 LoRA），从而在部署时无需针对每个用户进行单独训练。

**Result:** 实验结果显示，本方法在减少部署计算资源需求的同时，不仅超越了基于提示的个性化，也优于OPPU。

**Conclusion:** 通过 Profile-to-PEFT 框架，LLM 的个性化能够在大规模应用中实现高效、可扩展和适应性强的个性化服务。

**Abstract:** Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [12] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

> 本文探讨了通过后训练技术赋予大型语言模型的自我意识能力，研究发现强化学习训练的模型在认识学习行为和任务泛化能力方面表现较好，但推理过程与最终输出之间却存在弱对齐现象。

<details>
  <summary>Details</summary>

**Motivation:** 最近后训练技术的进步使大型语言模型能够通过生成辅助规划标记来应对复杂的逻辑密集型任务，此背景下提出对模型是否意识到其"学习"和"思考"的探讨。

**Method:** 本文通过定义三大核心能力来评估模型的自我意识：1) 对学习到的潜在策略的认识；2) 这些策略在不同领域的泛化能力；3) 内部推理轨迹与最终输出之间的对齐情况。这些能力在设计用于要求学习不同策略的任务上进行实证评估。

**Result:** 研究发现，通过强化学习训练的模型不仅在对学习行为的认识以及对于结构相似的新任务上的泛化能力方面优于监督微调的模型，还经常表现出推理轨迹与最终输出之间的弱对齐，这种现象在通过组相对策略优化训练的模型中最为明显。

**Conclusion:** 研究结论表明，虽然强化学习训练的模型（尤其是直接策略优化和组相对策略优化）在自我意识和任务泛化能力上表现较强，但也存在推理过程与最终输出之间不对齐的问题。

**Abstract:** Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [13] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

> 研究利用大语言模型生成针对疫苗错误信息的有效反驳策略，通过分类和微调方法优化反驳生成，评估表明该方法具有缓解疫苗错误信息的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 当前，社交媒体上疫苗怀疑论和错误信息泛滥，导致免疫接种率低并削弱了公众对健康建议的信任。尽管在检测错误信息方面取得了一定进展，但实时生成针对性反驳的领域仍待进一步探索。

**Method:** 研究通过各种提示策略和微调方法来优化针对疫苗错误信息的反驳生成，并通过训练分类器将反对疫苗的推文归类到多个类别中，如对疫苗功效、副作用及政治影响的担忧，用以生成更有针对性的反驳。

**Result:** 研究通过人类判断、基于LLM的评估和自动指标进行的评估显示，这些方法之间存在强烈的一致性。研究发现，结合标签描述和结构化微调可以提高反驳的有效性，为大规模缓解疫苗错误信息提供了一种有前景的方法。

**Conclusion:** 整合标签描述和结构化微调能够增强针对疫苗错误信息的反驳效果，有助于大规模缓解疫苗错误信息。

**Abstract:** In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [14] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

> 本文介绍了一种新的论证挖掘方法AASP框架，基于自回归结构预测，通过一系列预定义动作逐步构建论证结构，提升了论证推理的效果。

<details>
  <summary>Details</summary>

**Motivation:** 由于论证挖掘任务中内在推理的复杂性，建模论证组件和论证关系之间的依赖关系具有挑战性。当前大多数方法通过生成范式来处理该任务，而本文提出了一种新的方法。

**Method:** AASP框架使用自回归结构预测技术，基于条件预训练语言模型，通过预定义的一系列动作逐步构建论证结构，以有效捕捉论证推理流程。

**Result:** 在三个标准论证挖掘基准测试中，AASP框架在两个基准上实现了最先进的结果，在一个基准上也取得了很强的结果。

**Conclusion:** 实验结果表明，在三个标准论证挖掘基准测试中，AASP框架在两个基准上实现了最先进的结果，在一个基准上也取得了很强的结果。

**Abstract:** Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [15] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

> 本研究提出了一种轻量级方法，通过线性转换和引导向量来提高大型语言模型在心理健康评估中的效能，证明了这种方法在不使用计算密集型技术的情况下可以优化模型在相关任务中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型的发展正在改变AI领域，提供了在心理健康等敏感且高影响力领域的新机会，但研究证据表明，小型模型尚难以在特定领域应用中实现最佳性能。因此，本研究旨在提出一种成本效益高且有效的改进方法。

**Method:** 本研究提出了一种轻量级的方法，该方法通过对特定层的激活进行线性转换，并利用引导向量来指导模型的输出，从而在不依赖任何计算密集型技术的情况下提高大型语言模型在心理健康评估方面的能力。

**Result:** 该干预措施使模型在两个任务中取得了更好的结果：（1）判断Reddit帖子是否有助于检测抑郁症状的存在与否（相关预测任务），（2）基于用户的Reddit帖子历史完成标准化的心理筛查问卷来诊断抑郁症（问卷完成任务）。

**Conclusion:** 研究结果突显了引导机制作为计算效率工具，在大型语言模型心理健康领域适应中的未开发潜力。

**Abstract:** The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [16] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

> 该论文提出MoReBench和MoReBench-Theory，用于评估AI在道德困境中的程序性推理能力，强调了透明性和对多种道德框架的理解，发现现有评估方法不能准确预测AI进行道德推理的能力。

<details>
  <summary>Details</summary>

**Motivation:** 随着AI系统的发展，为了使AI决策更好地符合人类价值观，需要对AI决策过程进行透明化和评估，特别是在道德困境中。

**Method:** 作者创建了MoReBench数据集，包括1000个道德场景和23000多个评估标准，以及MoReBench-Theory数据集，包含150个测试AI是否能在五个主要伦理学框架中进行合理推理的示例。

**Result:** 实验结果表明，规模扩张和现有评估方法不能准确预测模型在道德推理任务上的表现，模型在某些道德框架上有偏好。

**Conclusion:** 这些基准测试推动了合理性和透明度在AI推理评估中的应用，有望促使AI更好地为人类道德决策提供建议或自主做出伦理选择。

**Abstract:** As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [17] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

> 论文介绍了一个名为ATA的神经符号方法，通过将问题转换成可验证的知识库并结合人类专家调整来提升大型语言模型的可靠性和透明度，实验表明该方法显著提升了模型的信任度和性能。

<details>
  <summary>Details</summary>

**Motivation:** 动机是解决大型语言模型在可靠性和透明度方面的限制，这些限制阻碍了它们在高风险领域中的应用。

**Method:** 我们的方法提出了一种称为自主可信代理（ATA）的神经符号方法来解决大型语言模型在高风险领域部署中的信任问题。方法包括两个主要阶段：离线知识摄入和在线任务处理。离线阶段中，LLM将非正式的问题规范转换成符号形式的知识库，并由人类专家验证和调整以保证其准确性和符合领域要求。在线阶段处理任务时，将每个输入编码为相同的符号形式，并通过符号决策引擎与知识库结合生成可靠结果。

**Result:** 实验结果表明，ATA的具体实现可以在全自动设置下与先进的端到端推理模型竞争，同时保持信任度。经过人类验证和修正的知识库使我们方法显著超越更大模型，表现出完美确定性，增强的输入扰动稳定性和对提示注入攻击的内在免疫力。

**Conclusion:** 结论是，ATA通过符号推理产生决策，提供了一个实用可靠的架构来构建下一代透明、可查核和可靠的自主代理。

**Abstract:** Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [18] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

> The paper explores the use of Whisper, an ASR foundation model, for L2 spoken language assessment (SLA) by extracting acoustic and linguistic features from its hidden representations, achieving strong performance without task-specific fine-tuning.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to understand the inherent capabilities of Whisper in SLA tasks, moving beyond previous extrinsic analysis and showcasing its potential in understanding language proficiency and semantic aspects of speech.

**Method:** The method involves extracting features from Whisper's hidden states and training a lightweight classifier on top of these features to assess L2 spoken language proficiency. Image and text-prompt information are used as auxiliary relevance cues.

**Result:** The results demonstrate superior performance compared to existing baselines, including a multimodal approach, on the GEPT picture-description dataset. An analysis of Whisper's embeddings shows intrinsic encoding of proficiency patterns and semantic aspects.

**Conclusion:** The paper concludes that Whisper exhibits strong potential as a foundational model for SLA and similar tasks without task-specific fine-tuning, highlighting its value in understanding spoken language proficiency and semantic richness.

**Abstract:** In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [19] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

> 论文介绍了FrugalPrompt，一种用于压缩大语言模型(LLMs)的提示框架，通过只保留语义上最重要的标记来减少LLMs的冗余并提高效率。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型（LLMs）的卓越性能很大程度上归功于其扩展的输入上下文，但这种冗长增加了经济成本、碳足迹和推理时间延迟。

**Method:** FrugalPrompt，一种新的提示压缩框架，用于大语言模型，该框架保留了语义上最重要的标记。通过使用两种最先进的标记归因方法（GlobEnc 和 DecompX），为输入序列中的每个标记分配重要性评分，然后对其进行排序，保留前k%的标记以获得稀疏化的简明提示。

**Result:** 实验在四个NLP任务（情感分析、常识问答、总结和数学推理）上进行，并使用了一系列前沿的LLMs。对于前三个任务，减少20%的提示只导致了轻微的性能下降，表明现代LLMs可以从高相关性线索中重建被删除的上下文。然而，在数学推理任务上的表现急剧下降，反映出对该任务需要完整的标记连续性。此外，使用低重要性和随机性的标记百分比进行进一步分析揭示了不对称的表现模式，可能是由于模型利用了在常规NLP任务上的浅层模式记忆。

**Conclusion:** 该工作为理解LLM在性能和效率之间的权衡提供了更细致的认识，并界定了哪些任务对上下文的稀疏性具有耐受性，以及哪些任务需要完整上下文。

**Abstract:** Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [20] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

> 本文引入了TrajSelector，一个有效利用LLM隐状态进行合理过程评分的Best-of-N框架，并在多个基准测试中证明了其高性能和较低的计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）在复杂推理任务中显示出显著的进步，这主要归功于测试时扩展（TTS）范式。然而，当前的TTS方法存在高计算开销和潜在表示利用不足的问题。

**Method:** 我们介绍了一个名为TrajSelector的有效框架，该框架利用采样器LLM中的隐藏状态进行过程评分。一个轻量级的验证器（仅0.6B参数）评估逐步轨迹的质量，然后聚合这些评分以识别最佳的推理轨迹。框架使用完全基于数据驱动的端到端训练配方，消除了对大量步骤级标注的依赖。

**Result:** 实验结果表明，在五个基准上，TrajSelector提供了持续的性能改进。在Best-of-32设置下，该框架比多数投票准确率高出4.61%，与现有过程奖励模型相比，提高了4.31%到12.21%，同时保持了较低的推理成本。

**Conclusion:** TrajSelector为解决外部TTS方法中的高计算开销和潜在表示利用不足的问题提供了一种高效且有效的解决方案。

**Abstract:** Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [21] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

> 文章引入RAVEN框架，通过课程强化学习和多模态大型语言模型相结合的方法，改善了广告视频违规检测中的时间定位精确性、注释噪声问题，并展示了良好的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的广告视频违规检测方法在精确时间定位、嘈杂注释和有限泛化能力方面存在挑战。因此，提出了一种结合课程强化学习和多模态大型语言模型的新颖框架RAVEN，以增强违规检测的推理和认知能力。

**Method:** RAVEN框架结合了课程强化学习与多模态大型语言模型（MLLMs），通过渐进式训练策略，结合精确和粗糙注释的数据，并利用Group Relative Policy Optimization（GRPO）来发展不需显式推理注释的新兴推理能力。采用多级复杂的奖励机制以确保精确的时间定位和一致的类别预测。

**Result:** 实验结果显示，RAVEN在工业数据集和公共基准上对违规类别的准确性以及时间间隔定位上的性能表现优异。在线A/B测试进一步验证了其实际应用的有效性，并显著提高了精度和召回率。

**Conclusion:** RAVEN具有较强的泛化能力，可以克服与监督微调相关的灾难性遗忘问题。在线A/B测试验证了它的实际应用价值。

**Abstract:** Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [22] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

> 本研究通过分析自然语言推理数据集中的注释者差异，发现注释者在解释上的共识比标签一致性更能反映自由文本解释的语义相似性。

<details>
  <summary>Details</summary>

**Motivation:** 过去的工作主要集中在标签内的变化，例如注释者在最终NLI标签上达成一致，但提供的解释不同。本文将视角拓宽，不仅考察注释者在推理类型上的分歧，还考察他们在标注步骤上的分歧。

**Method:** 通过将LiTEx分类法应用于两个英文NLI数据集，研究从多个角度（包括NLI标签一致性、解释相似性和分类法一致性以及注释者的选取偏差）来对注释者之间的差异进行对齐。

**Result:** 观察到在标签不一致的情况下，注释者提供高相似度的解释，这表明表面上的分歧可能掩盖了解释背后的共识。此外，分析揭示了注释者在解释策略和标签选择上的个体偏好。

**Conclusion:** 研究结果强调了基于推理的解释的丰富性，并强调需谨慎对待将标签当作绝对标准的做法。

**Abstract:** Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [23] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

> 研究了在多轮次交互场景中，通过使大语言模型主体具有退出机制来提高其安全性的方法。结果表明，这种方法在提高安全性的同时，几乎不影响模型的帮助性。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型代理在复杂环境中操作时，其安全性变得至关重要。然而，对于多轮次代理场景中的不确定性量化仍然是一个挑战。提出了一种简单的退出行为机制，以便代理在缺乏信心时能够认识到并退出这些情况。

**Method:** 通过使用ToolEmu框架，对12个最先进的大语言模型进行了系统性的退出行为评估。

**Result:** 结果显示，通过明确定义的退出指令，可以在所有模型的安全性上平均提高+0.39（在专有模型上提高了+0.64），同时在可用性上的平均降低仅为-0.03。

**Conclusion:** 简单地添加明确的退出指令是一种有效的安全机制，可以立即部署于现有的代理系统，并为高风险应用中的自主代理建立了退出作为一种有效的防御机制。

**Abstract:** As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [24] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

> 本文提出了一种新的代理系统组件组合框架，通过优化选择和组装，提升了系统在不同预算和领域中的表现及成功率，尤其在多代理情况下表现优越。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法依赖于静态的语义检索来发现工具或代理，这导致了现有组件的有效重用和组合的挑战，因为决策不是基于能力、成本和即时效用。因此，本文旨在解决这些挑战。

**Method:** 本文提出了一种结构化的自动化框架，用于代理系统的组合，灵感源自背包问题。通过系统性识别、选择和组装最优代理组件集，该框架考虑了性能、预算限制和兼容性。通过动态测试候选组件和实时建模其效用，本文的方法简化了代理系统的设计，并促进了资源的可扩展重用。

**Result:** 经验评估结果表明，在单代理设置下，相比检索基线，线性背包组合器可以提高高达31.6%的成功率。在多代理系统中，当从包含100+个代理的代理库存中选择代理时，线性背包组合器将成功率从37%提高到了87%。

**Conclusion:** 实验结果验证了该方法在不同域和预算约束下的强大适应性，证明了在线背包组合器的有效性和优越性。

**Abstract:** Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [25] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

> 为了解决日益增加的论文提交和大型语言模型对审稿系统的影响，研究团队开发了ReviewGuard系统，利用LLM识别不足的审稿评论，证实了其在学术诚信领域的应用前景。

<details>
  <summary>Details</summary>

**Motivation:** 随着提交论文的激增和大型语言模型在学术评价中的广泛使用，审稿质量面临前所未有的挑战。不可控的来自人类专家和AI系统的不足审稿威胁到了审稿生态系统，有损学术诚信，因此提出了ReviewGuard系统来解决这一问题。

**Method:** ReviewGuard 使用一个四阶段的LLM驱动框架来检测和分类不足的审稿意见。框架包括收集ICLR和NeurIPS论文及对应的审稿意见，使用GPT-4.1及其人类验证标注审稿类型，通过LLM驱动的数据增强来解决类别不平衡和数据不足问题，最后微调基于编码器的模型和开源的LLM。

**Result:** 研究表明，相比于充分的审稿意见，不足的审稿意见具有较低的评分、较高的自我评定信心、较低的结构复杂性和更高的负面情绪比例。AI生成的文本检测表明，自ChatGPT出现以来，AI生成的审稿意见明显增加。在不足审稿意见检测模型的评估中，混合使用合成和真实审稿数据训练显著增强了召回率和F1分数。

**Conclusion:** 这项研究介绍了首个通过LLM驱动的方法来检测不足同行评审意见的系统。这为AI在同行评审中的治理提供了证据，并提供了关于如何促进人与AI的合作以维持学术诚信的宝贵见解。

**Abstract:** Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [26] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

> 通过测量LLMs在不同文化情境下回答问题时的激活路径重叠，研究发现语言具体的模式比文化的情境更为显著，但仍存在有文化独特性的情况，特别是在语言相似的情况下。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于改进LLMs在跨文化情境下的准确文化理解，因为先前的评估主要集中于输出层面的表现，而忽略了影响不同回应的因素，同时使用电路分析的研究涵盖的语言很少，并且很少关注文化。

**Method:** 通过测量在两种条件下回答语义等价问题时的激活路径重叠来追踪LLMs的内部文化理解机制：改变目标国家同时固定问题语言，以及改变问题语言同时固定国家。还使用相同语言的国家对来区分语言和文化方面的影响。

**Result:** 结果显示，对于相同语言、不同国家的问题，内部路径的重叠比对于不同语言、同一国家的问题更多，表明有强烈的语言特定模式。值得注意的是，韩国和朝鲜对展现出低重叠和高变异性，表明语言相似性并不能保证内部表示的一致性。

**Conclusion:** LLMs在理解文化时表现出强烈依赖语言的特异性，但语言相似性并不能保证内部文化表示的一致性，特别是在涉及文化差异明显的情景时。

**Abstract:** Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [27] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

> 研究提出了SHALLOW框架，该框架系统地分类和量化ASR中的幻觉现象，提供了一套更详细的模型错误分析方法，超出了传统错误率的局限。

<details>
  <summary>Details</summary>

**Motivation:** 当前的自动语音识别（ASR）系统在解码时存在幻觉现象，即完全不相关的流畅且连贯的转录，这可能比传统的解码错误更具危害性。因此，需要新的评估框架来有效识别和评估产生幻觉内容率较高的模型。

**Method:** 提出了SHALLOW框架，该框架系统地分类和量化ASR中的幻觉现象，涵盖词汇、音素、形态和语义四个互补轴，每个类别内定义了针对性指标以生成可解释的模型行为档案。

**Result:** 通过SHALLOW评估不同架构和语音领域的模型发现，在高识别质量下，SHALLOW指标与WER有较强的关联，但在WER增加时关联性减弱，显示出捕捉精细错误模式的能力。

**Conclusion:** SHALLOW框架在高识别质量下与WER有较强的关联，但在WER增加时关联性减弱，显示出在降级和具有挑战性条件下捕捉WER不能区分的精细错误模式的能力。该框架支持对模型弱点的具体诊断，并提供超出聚合误差率的模型改进反馈。

**Abstract:** Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [28] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

> 本研究提出了一种针对乌尔都语的AI生成文本检测框架，利用了平衡的数据集和多语言转换模型进行微调，mDeBERTa-v3-base模型取得了最佳性能。

<details>
  <summary>Details</summary>

**Motivation:** 乌尔都语缺乏检测AI生成文本的工具，这是一个亟待解决的问题。随着大型语言模型生成文本的能力不断增强，很难辨别文本是由人类还是机器生成，尤其是对于像乌尔都语这样的语言。

**Method:** 本研究提出了一种针对乌尔都语的AI生成文本检测框架。该框架使用了一个平衡的数据集，包括1800篇人类写作和1800篇由AI生成的文本，这些文本来自像Gemini、GPT-4o-mini和Kimi AI这样的模型。研究进行了详细的语用学和统计分析，重点关注了特征如字符和词语数量、词汇丰富度（类型-词汇率）以及N-gram模式。显著性通过t检验和曼-惠特尼U检验评估。并利用了三个最先进的多语言转换模型：mdeberta-v3-base、distilbert-base-multilingualcased和xlm-roberta-base，对数据集进行了微调。

**Result:** 通过微调，mDeBERTa-v3-base模型在测试集上达到了最高的F1得分91.29和准确率91.26%。

**Conclusion:** 本研究通过提出专门针对乌尔都语的AI生成文本检测方法，不仅有助于打击乌尔都语社区中的不实信息和学术不端行为，同时也为低资源语言的NLP工具开发做出了贡献。

**Abstract:** Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [29] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

> 本文通过微调大型语言模型，提出了进行短语结构分析的新方法，并证实了这种方法在提高MiSintaxis工具能力方面的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 旨在扩展MiSintaxis工具的功能，该工具专用于教授西班牙语句法。

**Method:** 通过微调大型语言模型（LLMs），将输入句子转换为其对应的句法结构，从而探索短语结构分析的新方法。使用的模型来自于Hugging Face仓库，并使用AnCora-ES语料库生成的训练数据进行了微调。

**Result:** 结果表明，在短语结构分析中表现出了高准确率，并突显了此方法论的潜力。

**Conclusion:** 实验证明了使用大型神经模型进行句法分析的高效性，并表明该方法有潜力作为教学辅助工具。

**Abstract:** Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [30] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

> {
  "tldr": "本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。", 
  "motivation": "语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。", 
  "method": "DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。", 
  "result": "该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。", 
  "conclusion": "接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。", 
  "tool_calls": []
}

<details>
  <summary>Details</summary>

**Motivation:** {
  "tldr": "本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。", 
  "motivation": "语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。", 
  "method": "DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。", 
  "result": "该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。", 
  "conclusion": "接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。", 
  "tool_calls": []
}

**Method:** {
  "tldr": "本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。", 
  "motivation": "语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。", 
  "method": "DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。", 
  "result": "该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。", 
  "conclusion": "接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。", 
  "tool_calls": []
}

**Result:** {
  "tldr": "本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。", 
  "motivation": "语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。", 
  "method": "DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。", 
  "result": "该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。", 
  "conclusion": "接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。", 
  "tool_calls": []
}

**Conclusion:** {
  "tldr": "本文介绍的Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo)框架，通过让四个专门化的语言模型代理之间的结构化辩论，提高了语言模型的表现能力和解释性。", 
  "motivation": "语言模型虽然在性能上表现出色，但它们往往缺乏可解释的推理能力。本文旨在通过引入新的框架来改善这一状况。", 
  "method": "DiMo框架通过让四个代表不同推理模式的代理进行辩论，每个代理表示一个特定的推理范式。通过迭代辩论过程，代理们能够挑战和提高原先的回答，提供更强健的结论和明确可审计的推理链。", 
  "result": "该框架在六个标准上展示了比单一模型和辩论基线更高的准确性，尤其是在数学领域有显著的提高。", 
  "conclusion": "接下来将DiMo框架定位为一个Web原生多代理框架，利用语义感知语言模型代理生产语义类型的、具有URL注释的证据链，便于解释和用户友好交互，同时结合检索增强的推理和可审查的结构化解释。", 
  "tool_calls": []
}

**Abstract:** Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [31] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

> 提出了Capsule Prompt-Tuning (CaPT)，该方法将任务相关的提示与实例信息结合，不需要额外调优，实验结果显示在多个语言任务中表现出色，参数效率高。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现有基于提示的学习方法中需要大量手动搜索最优提示长度和需要较多提示的问题，并弥补这些方法中缺失实例信息的不足。

**Method:** 介绍了Capsule Prompt-Tuning (CaPT)，该方法创新性地将任务相关的提示与实例相关的信息结合，通过在序列早期位置加入实例相关的标记，解决了传统提示方法中缺失实例信息的问题，并且在几乎不增加模型参数的情况下提高了性能。

**Result:** 实验表明，该方法在多个语言任务中表现优异（如T5-Large平均准确率为84.03%），并且参数效率高（例如Llama3.2-1B模型参数的0.003%）。

**Conclusion:** Capsule Prompt-Tuning (CaPT)作为有效且高效的解决方案，将实例信息引入到基于提示的学习中，展示了其在语言任务上的优越性能，并且保持了较高的参数效率。

**Abstract:** Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [32] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

> 提出了TUuD（基于指示性时间参照框架的时间理解）来评估语言模型在动态参考时间点下的时间推理能力。结果显示，这些语言模型在短期内容上能部分适应人类的时间认知，但对更远的时间点变化敏感。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在自然语言理解方面取得了显著进展，但它们理解和推理时间的能力仍然有限。为了评估LLMs的时间推理能力，特别是他们如何适应和理解"现在"这一参考点在时间线上的动态变化。

**Method:** TUuD (Temporal Understanding under Deictic t-FoR)框架评估了LLMs在"现在"这一参考点在时间线上动态变化时如何理解和推理时间-事件和事件-事件的关系。LLMs被要求从0.00（完全不相似）到1.00（非常相似）来评定当前时刻与目标事件之间的相似性，以此量化二者之间感知的时间对齐程度。

**Result:** 评估结果表明，四种被评估的语言模型在当前时刻相似性的评估值会随时间点的变化而呈现出峰值，并在向过去或未来事件延展时减少。这表明虽然语言模型显示了部分人类时间认知，但其对时间点变化和时间距离的适应性仍有限。

**Conclusion:** 语言模型虽然能部分适应指示性的时态框架，在短期内展现出类似于人类的时间认知，但其对时间点转移和较长距离的时间点敏感。这表明当前的语言模型在时间推理方面仍有改进的空间。

**Abstract:** Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [33] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

> 本研究构建了一个包含理由的NLURC数据集，并探索了这些理由在自然语言理解（NLU）任务中的应用。研究发现，随着模型规模的增大，CoT推理从影响NLU性能转变为超越直接标签预测，某些特定设计的方法能够在训练中提高性能，并且使用理由训练的LLM在未见过的NLU任务上表现出显著的性能提升，达到比它大十倍模型的性能，同时具有与商业LLM相当的可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管已有研究表明在推理任务中加入理由对模型性能有好处，但大多数研究忽略了它们对NLU任务的影响。因此，本研究旨在探讨在NLU任务中使用理由的可能性及效果。

**Method:** Structure

**Result:** {
  "tldr": "本研究构建了一个包含理由的NLURC数据集，并探索了这些理由在自然语言理解（NLU）任务中的应用。研究发现，随着模型规模的增大，CoT推理从影响NLU性能转变为超越直接标签预测，某些特定设计的方法能够在训练中提高性能，并且使用理由训练的LLM在未见过的NLU任务上表现出显著的性能提升，达到比它大十倍模型的性能，同时具有与商业LLM相当的可解释性。",
  "motivation": "尽管已有研究表明在推理任务中加入理由对模型性能有好处，但大多数研究忽略了它们对NLU任务的影响。因此，本研究旨在探讨在NLU任务中使用理由的可能性及效果。",
  "method": "构建了一个包含理由的NLURC数据集，并提出了几种理由增强的方法来探讨这些方法在NLU任务上的可行性。",
  "result": "研究表明CoT推理随模型规模增大对NLU性能的影响发生了从负到正的转变，一些特设计的方法在训练中表现出了性能提升。使用理由进行训练的模型在未见过的NLU任务中表现出色，超越了比它大十倍的模型。",
  "conclusion": "理由在NLU任务的训练与预测中有潜在价值，特别是在模型规模较大时能够显著提升性能。然而，不同的理由增强方法表现不同，需要设计专门的方法才能有效提高NLU任务中的模型性能。")

**Conclusion:** 理由在NLU任务的训练与预测中有潜在价值，特别是在模型规模较大时能够显著提升性能。然而，不同的理由增强方法表现不同，需要设计专门的方法才能有效提高NLU任务中的模型性能。

**Abstract:** Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [34] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

> 这篇综述论文对2014至2025年间自然语言处理（NLP）技术在心脏病学中的应用进行了详细分析，涵盖文章筛选、多维度分析及时间趋势分析，显示出NLP在心脏病学中的广泛应用及其趋势。

<details>
  <summary>Details</summary>

**Motivation:** 由于心血管疾病在现代社会中越来越普遍，它对全球健康和福祉产生显著影响。本论文旨在通过综述NLP技术在心脏病学文献中的应用，展示对心脏病研究和治疗的潜在变革影响。

**Method:** 本论文综述了2014年至2025年间自然语言处理（NLP）技术在心脏病学中的应用研究。通过查询六个文献数据库，收集并筛选了265篇相关的文章。从多个维度，包括NLP范式类型、心脏病学相关的任务类型、心血管疾病类型和数据源类型，对每篇文章进行分析。

**Result:** 论文分析结果展示了NLP研究在心脏病学领域中显著的多样性。时间趋势分析揭示了过去十年间NLP方法的应用演进和变化趋势。

**Conclusion:** 论文认为该综述为NLP在心脏病学领域应用提供了迄今为止最为全面的概述，展示了NLP在心脏病研究中多样的应用及其在诊断、治疗和预防心病症方面潜在的变革潜力。

**Abstract:** Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [35] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

> 研究揭示了大型语言模型在多轮对话中处理矛盾问题时出现立场不一致的行为，并提出了Chameleon评分和源重用率两个指标用于评估该问题的严重性。研究结果表明模型在处理矛盾问题时表现不佳，显示出对查询框架的高度依赖。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型和搜索引擎的集成已经变得非常普遍，但其内部存在严重缺陷，影响了其可靠性。本研究针对大型语言模型在多轮对话的矛盾问题中表现出的立场飘忽不定的问题（特别是搜索赋能的大型语言模型）进行了系统的研究。

**Method:** 通过构建包含17,770个精心设计的问题回答对和1,180个多轮对话的Chameleon基准数据集，涵盖了12个有争议的领域，本研究首次系统性地探讨了大型语言模型（LLMs）在面对矛盾问题时改变立场的现象（Chameleon行为）。同时引入了两个理论基础的指标：Chameleon评分（0-1）量化立场的不稳定性和源重用率（0-1）衡量知识多样性。

**Result:** 对Llama-4-Maverick，GPT-4o-mini和Gemini-2.5-Flash的严格评估显示，所有模型都表现出了严重的Chameleon行为（评分0.391-0.511），其中GPT-4o-mini表现最差。此外，较小的跨温度变量（小于0.004）表明这并非是一种采样成品。分析揭示了机制：源重用率与置信度（r=0.627）和立场变化之间存在显著强相关性（r=0.429），且统计显著（p<0.05），这说明知识多样性有限使得模型过分依附于查询框架。

**Conclusion:** 这些发现表明，在大语言模型的部署之前进行一致性评估是非常必要的，特别是在医疗、法律和金融服务等领域，这时模型必须保持连贯的立场以确保可靠的支持决策。

**Abstract:** Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [36] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

> 研究了诗人如何在他们的作品中使用空白符，以此来理解空白符在诗歌中的重要性，并探讨其对大型语言模型预训练数据集构建的影响。

<details>
  <summary>Details</summary>

**Motivation:** 尽管诗歌作为一种长久以来的艺术形式和大型语言模型（LLMs）的生成任务而广泛流行，但空白符并未受到NLP社区足够的关注。为了解决这一问题，并促进未来的研究，研究者们发布了2.8k首公共领域的保留格式的诗歌。

**Method:** 通过分析来自Poetry Foundation的19k首英文出版诗歌中4k位诗人在作品中使用空白符的方式，探究空白符在诗歌中的作用。

**Result:** 比较了19k首出版诗歌与51k个LLMs生成的诗歌以及来自在线社区的12k首未发表诗歌中的空白符使用情况，探讨了不同时期、不同诗歌形式和数据源之间的空白符使用差异。

**Conclusion:** 这一研究指出不同的文本处理方法会显著影响诗歌数据中空白符的表现，并提出这些诗歌和空白符模式可以用来讨论组装LLMs预训练数据集处理策略的含义。

**Abstract:** Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [37] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

> Beacon 是一种用来评估和测量大型语言模型中奉承偏见的基准方法。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在奖励优化过程中存在一个内在的权衡，即把有用性与礼貌的关系混为一谈，这导致了一种叫做奉承的潜在偏见。

**Method:** 介绍Beacon，这是一个单回合强制选择基准，可以独立于对话上下文隔离出奉承偏见，精确测量事实准确性与服从偏见之间的紧张关系。

**Result:** 通过对十二个最先进的模型进行评估，发现奉承可以分解为稳定的语言和情感次偏见，每个次偏见都与模型容量成比例。

**Conclusion:** Beacon 将奉承重新定义为一种可量化的规范性过度泛化形式，为研究和缓解大规模生成系统中的对齐偏差提供了可重复的基础。

**Abstract:** Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [38] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

> 本文提出了一种新的方法SCO-PAL来改进语言代理在对抗环境中的策略推理能力。通过自我对抗实验，发现该方法比基线方法提高了约30%的平均胜率，并在对抗GPT-4的六场游戏中获得了54.76%的胜率。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有语言代理在动态对抗游戏中因策略推理能力不足导致的问题，通过自动从游戏交互中学习来避免依赖于昂贵的专家标注数据。然而，关于对抗环境中对手选择的讨论仍然是一个探索区域。

**Method:** 提出了一种名为SCO-PAL的分步策略优化方法，该方法通过玩与学习来进行。通过设置不同级别的对手进行了详细的对手选择分析，并发现自我对抗是提高此类对抗环境中策略推理能力的最有效方式。

**Result:** 利用SCO-PAL与自我对抗，平均胜率对比基线增加了约30%，对抗GPT-4的六场游戏中获得了54.76%的胜率。

**Conclusion:** SCO-PAL方法与自我对抗能够显著提高语言代理在动态对抗环境中的策略推理能力，且能有效提高胜率。

**Abstract:** Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [39] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

> 本文介绍了一种名为LC-Eval的双语多任务评估基准，旨在评估英语和阿拉伯语的长文本理解性能。通过四个新设计的任务，LC-Eval显示了即使是顶级语言模型也面临挑战。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型处理和理解长文本内容能力的提升，研究者需要更严格的评估方法来评判这些模型在长文本理解上的表现。

**Method:** 该研究提出了一种名为LC-Eval的新型评估基准，用于评估长文本理解能力。LC-Eval涵盖了四类任务：多文档问答、双语问答、段落内的主张验证及基于长上下文的多选题。这些任务旨在评估语言模型在深度推理、文档理解、信息追溯及双语信息提取与理解方面的能力。

**Result:** 评估表明，即使是像GPT-4o这样表现优异的模型，在某些任务上也表现出了挑战，这说明LC-Eval具有一定的复杂性和严谨性。

**Conclusion:** LC-Eval提供了一种有效的手段来评估语言模型在长文本理解能力上的性能，尤其是在涉及深度推理、信息追溯和多语言理解的任务上。

**Abstract:** Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [40] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

> 本论文引入了MOSAIC方法，通过结合掩码语言模型和对比目标的联合优化来实现句嵌入模型的领域适应，并在多个资源环境和领域中展示了相比通用领域基线模型的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 该论文旨在解决将在大规模通用领域句嵌入模型适应到专业领域中所面临的挑战。

**Method:** MOSAIC方法结合了联合领域特定的掩码监督进行多阶段框架的领域适应。通过在同一训练管道中联合优化掩码语言模型（MLM）和对比目标，该方法可以有效地学习领域相关的表示，同时保留原始模型的鲁棒语义区分特性。

**Result:** 实验验证表明，MOSAIC方法在多个资源环境下均能改善结果，在NDCG@10指标上比强基线模型最高提高了13.4%。此外，详尽的消融研究进一步表明各个组件的有效性，特别是平衡联合监督和阶段性适应的重要性。

**Conclusion:** MOSAIC方法对于领域适应的句嵌入模型是有效的，可以在保持鲁棒语义区分特性的同时学习到相关的领域表示。

**Abstract:** We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [41] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

> 本文研究了大型语言模型如何在数值知识和启发式偏见之间做出选择。发现较大型的模型能够在数值知识更可靠时优先依赖这种知识，而较小的模型则依赖启发式偏见。使用chain-of-thought提示可以改进所有模型的数值属性利用。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于探讨LLMs在知识推理任务中如何以及何时依赖真实的数值知识，而不是依赖于表面的启发式方法。

**Method:** 通过实体比较任务研究大型语言模型（LLMs）是否依赖真实的知识或只是表面的启发式方法。具体来说，任务要求模型比较实体的数值属性（例如，“多瑙河和尼罗河，哪条河更长？”），这类问题可以提供明确的ground truth以进行系统的分析。

**Result:** 研究揭示了三种强烈影响模型预测的启发式偏差：实体流行度、提及顺序、语义共现。对于较小的模型，仅使用这些表面线索的简单逻辑回归模型就能比这些小模型自身的数值预测更准确地预测模型的选择，说明启发式方法很多时候覆盖了基于原则的推理。然而，较大的模型在数值知识更可靠的情况下，优先依赖数值知识。

**Conclusion:** 研究得出的结论是：较大的模型（32B参数）在数值知识更可靠时，会更依赖数值知识而非启发式偏见；而较小的模型（7-8B参数）则没有这种区分能力。因此，即使小型模型拥有更准确的知识，大型模型依然能更好地表现。此外，通过chain-of-thought提示法，可以引导所有模型使用数值属性进行推理。

**Abstract:** Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [42] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

> 本文提出ESCA框架，核心为SGClip模型，用于生成场景图并提高具身智能体的性能，展示了在具身环境中的先进表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的训练流程主要依赖高层次的视觉-声音-文本配对，缺少像素级视觉内容与文本语义之间的细粒度、结构化的对齐。为解决这一挑战，提出了ESCA框架。

**Method:** 本文提出了ESCA框架，通过结构化时空理解为具身智能体提供上下文。其核心是SGClip，这是一种基于CLIP的新型开放式领域可提示模型，用于生成场景图。SGClip通过对超过87000个开放式领域视频进行训练，并采用了神经符号学习管道，利用视频-字幕对的模型驱动自我监督和结构化推理，从而避免了需要人工标注的场景图注释。

**Result:** SGClip在场景图生成和动作定位基准测试中表现出色，ESCA框架显著减少了具身智能体的感知错误，并提升了多模态大型语言模型的整体性能。

**Conclusion:** 实验表明，ESCA与SGClip能够持续改善开源和商业多模态大型语言模型的表现，在两个具身环境中均达到了最先进的性能。尤其是在减少智能体感知错误和提升开源模型超越专有基线方面有显著改进。

**Abstract:** Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [43] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

> This paper proposes CrossRay3D, an advanced sparse multi-modal detector, which improves token quality and performance while maintaining computational efficiency, outperforming other methods on the nuScenes benchmark.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to address the shortcomings of existing sparse cross-modality detectors, particularly in terms of token representation quality and overall detector performance.

**Method:** The paper proposes a Sparse Selector (SS) that includes two key components: Ray-Aware Supervision (RAS) and Class-Balanced Supervision. It also introduces Ray Positional Encoding (Ray PE) to help with the distribution differences between LiDAR and image modalities.

**Result:** The proposed model, CrossRay3D, achieves state-of-the-art performance on the nuScenes benchmark with 72.4 mAP and 74.7 NDS. It runs 1.84 times faster than other leading methods and shows strong robustness in scenarios with partial or complete loss of LiDAR or camera data.

**Conclusion:** The inclusion of Ray-Aware Supervision, Class-Balanced Supervision, and Ray Positional Encoding in the proposed framework leads to superior performance in sparse multi-modal detection tasks, demonstrating strong robustness and efficiency.

**Abstract:** The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [44] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

> 本文提出使用CCTV流进行多缺陷检测和分割，结合VLM生成结构化维修建议，改善城市基础设施的监测和维护。

<details>
  <summary>Details</summary>

**Motivation:** 现有的自动系统通常只针对个别缺陷类型或提供无法直接指导维修队伍的非结构化输出。手动检查成本高昂且危险。

**Method:** 本文提出了一种全面的管道，利用街道CCTV流进行多缺陷检测和分割，采用YOLO对象检测器家族，并将检测结果传递给视觉语言模型（VLM）进行场景感知总结，生成包括事件描述、建议工具、尺寸、修复计划和紧急警报的结构化行动计划。

**Result:** 实验评估显示，系统能够准确识别多种缺陷并生成连贯的报告。

**Conclusion:** 讨论了将系统扩展到城市范围部署的挑战和方向。

**Abstract:** Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [45] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

> 本文提出了一种新的基于多模态大规模语言模型IAD-GPT，用于工业异常检测，通过异常提示生成器，文本引导增强器及多掩码融合模块等改进了异常检测和分割的性能。实验表明方法有效。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大规模语言模型在工业异常检测中展现出强大的因果推理能力，但传统的工业异常检测方法缺乏提供多轮人类与机器对话和具体描述的能力。同时，基于大规模预训练模型的方法尚未完全激发这些模型在异常检测任务中的能力。因此，本文致力于丰富文本语义与图像级别、像素级别信息的结合，以提升异常检测的性能。

**Method:** 文章提出了一种新的基于多模态大规模语言模型（MLLM）进行工业异常检测(IAD)的方法，名为IAD-GPT。通过使用异常提示生成器（Abnormal Prompt Generator, APG）生成特定对象的详细异常提示，从大规模预训练语言模型中获取并激活视觉语言模型（如CLIP）的检测和分割功能。为了增强MLLM的视觉定位能力，提出了文本引导增强器(Text-Guided Enhancer)，使语言模型能够专注于视觉数据的特定方面，提高了对图像中异常的解释和响应准确性。此外，设计了一种多掩码融合模块，通过将掩码作为专家知识纳入，提升了模型对像素级别异常的感知能力。

**Result:** 在MVTec-AD和VisA数据集上的大量实验证明了该方法在自监督和少量样本异常检测和分割任务中的卓越性能。

**Conclusion:** 研究展示了通过结合丰富的文本语义与图像信息，使用多模态大规模语言模型进行工业异常检测的有效性。提出的IAD-GPT在自监督和少量样本的情况下，在异常检测和分割任务上表现出色，达到了当前最优水平。

**Abstract:** The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [46] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

> 研究显示，使用AI辅助的结构化报告（AI-SR）不仅提高了放射科医生的效率，缩短了报告时间，并提升了诊断准确性、用户体验，显示出比结构化报告（SR）和自由文本（FT）更佳的整体效果。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在探讨结构化报告（SR）和人工智能（AI）如何改变放射科医生的影像分析方式，特别是关注SR与AI相结合的模式在提高效率和诊断准确性方面的潜力。

**Method:** 研究采用前瞻性设计（2024年7月至12月），评估了三种报告模式（自由文本FT、结构化报告SR及AI辅助结构化报告AI-SR）对影像分析行为、诊断准确性、效率及用户体验的影响。四名初学者和四名非初学者（包括放射科医生及医学生）每人分析了35张床边胸部X光片。使用定制的查看器和眼动追踪系统进行分析。关键结果通过与专家共识对比计算的Cohen's $\	ext{kappa}$值衡量，以及报告时间、眼动追踪指标及问卷调查用户的体验进行评估。

**Result:** 诊断准确性方面，FT组（$\kappa = 0.58$）和SR组（$\kappa = 0.60$）相似，但AI-SR组（$\kappa = 0.71$，$P < .001$）更高。报告时间从FT组的$88 ± 38$秒减少至SR组的$37 ± 18$秒和AI-SR组的$25 ± 9$秒（$P < .001$）。初学者在SR模式下更专注于影像，而非初学者无论在哪种模式下都将目光集中在影像上。AI-SR是首选方式。

**Conclusion:** 结构化报告通过指导视觉注意力集中于影像提高了效率，而AI预填充的结构化报告进一步增强了诊断准确性和用户体验。

**Abstract:** Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [47] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

> 本文提出了一个数据驱动的框架来发现和缓解图像分类中由于多个属性交互导致的交叉偏见，提出Biased-Weighted Augmentation策略并证明其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决机器学习模型在训练不平衡数据集时出现的交互偏见问题，这些偏见是由多个属性之间的交互作用而产生的，如目标类别和环境条件。

**Method:** 介绍了一个名为Intersectional Fairness Evaluation Framework (IFEF)的数据驱动框架，用于分析和缓解图像分类中的交叉偏见。该框架结合了定量公平度量和可解释性工具，系统地识别模型预测中的偏见模式。基于此分析，提出了一种新的数据增强策略Bias-Weighted Augmentation (BWA)，该策略根据子组分布统计信息来调整变换强度。

**Result:** 实验表明，利用Open Images V7数据集在五大目标类别上的测试中，BWA策略能够提高代表性不足的类别-环境交叉的精确度达24个百分点，同时将公平度量差异减少35%。多种独立运行的统计分析证明了改进的显著性（p < 0.05）。

**Conclusion:** 该方法提供了一种可重复的方法，用于分析和解决图像分类系统中的交叉偏见问题。

**Abstract:** Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [48] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

> This paper presents a differentiable quantization method for neural networks, which maintains high accuracy with less computational and storage needs, and outperforms previous quantization techniques in terms of both accuracy and training efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of non-differentiable quantization approaches and lack of effective activation quantization while keeping the accuracy high with lower computational and memory needs.

**Method:** The paper proposes a differentiable quantization method for neural networks that converges to the optimal network and supports $n$ bits quantization, enabling both weight and activation quantization.

**Result:** The method, when applied to an image classification task with the ImageNet dataset and ResNet18, achieved slightly less than 1 percent less accuracy compared to full precision using only weight quantization, and achieved state-of-the-art accuracy with both weight and activation quantization, while requiring only 15 epochs to train.

**Conclusion:** The proposed quantization method provides a superior approach to quantize neural networks by ensuring differentiability, supporting convergence proof, and offering flexibility in bits quantization without sacrificing performance.

**Abstract:** Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [49] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

> StripRFNet通过三个特定模块提升道路损伤检测的准确性和实时性，在多个评估指标上超越了现有方法，为道路维护提供了先进的解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 研究背景在于准确识别道路损伤的挑战性，例如损伤多样形状、难以捕捉细长裂纹以及小损伤识别高误差率。

**Method:** StripRFNet包含三个模块：SPM增强了形状的多尺度特征聚合的区分度；SRFM捕捉到了细长裂纹的特征；SSEM提升了小孔径目标的检测性能。

**Result:** StripRFNet 是一种新型的深度神经网络，旨在提高道路损伤检测的准确性。该网络通过三大模块实现这一目标：1) 形状感知模块（SPM），通过大规模分离内核注意力提升多尺度特征聚合的形状区分度；2) 条带感受野模块（SRFM），使用大条带卷积和池化策略捕捉细长裂纹的特征；3) 小尺度增强模块（SSEM），通过高分辨率 P2 特征图、专业检测头及动态上采样技术提高小目标的检测性能。实验结果表明显RFNet在RDD2022基准上超越现有方法，分别提升F1分数、mAP50和mAP50:95 4.4、2.9和3.4个百分点。在中国的子集上同样取得最佳F1分数80.33%，同时也保持了实时推理速度。总体上，StripRFNet在准确性和实时性上都达到了先进水平，为道路智能维护和可持久基础设施管理提供了一个有希望的工具。

**Conclusion:** 实验结果证明StripRFNet在RDD2022基准上的性能优越，尤其是在中国的数据子集上的表现更为突出。StripRFNet在提升检测准确性的同时保留了实时性的优势，表现出了可持续基础设施管理的前景。

**Abstract:** Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [50] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

> 本论文引入了ObjectTransforms技术，用于量化和减少基于视觉的对象检测中的不确定性。该技术提高了鲁棒性，增强了精度召回曲线，且在训练和推断阶段表现出较低的不确定性和较高的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 视觉基础的对象检测神经网络容易受到如数据偏差和分布变化等不确定性的影响。我们旨在通过引入ObjectTransforms解决这一问题，提升自动驾驶中关键决策的安全性。

**Method:** ObjectTransforms技术通过在训练和推断阶段对特定对象进行变换来量化和减少基于视觉的对象检测中的不确定性。训练阶段，ObjectTransforms在对象上执行颜色空间扰动来提高对光照和颜色变化的鲁棒性，并使用扩散模型生成真实且多样的行人实例。推断阶段，对检测到的对象应用扰动，并使用检测评分的方差来量化实时预测不确定性。

**Result:** 实验结果显示，相较于基线模型，在NuImages 10K数据集上使用YOLOv8时，ObjectTransforms可以显著提升所有对象类别的训练准确性，减少不确定性，并在推断阶段为假正例预测出比真实正例更高的不确定性值。

**Conclusion:** 实验结果强调了ObjectTransforms作为一种轻量有效的方法，对于减少和量化基于视觉感知中训练和推断阶段的不确定性具有潜在价值。

**Abstract:** Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [51] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

> Aria Gen 2 Pilot Dataset（A2PD）是一个公开的第一人称多模态数据集，使用Aria Gen 2眼镜捕捉数据，适用于多种场景，具有良好的鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 创建Aria Gen 2 Pilot Dataset（A2PD）是为了提供一个及时更新的、公开的、第一人称视角的多模态数据集，以促进相关研究的发展。

**Method:** 该数据集使用尖端的Aria Gen 2眼镜捕捉第一人称多模式数据，包含了关于日常生活活动的多种传感器数据和机器感知算法输出。通过多个场景，展示了设备感知佩戴者、周围环境及两者之间交互的能力，并具有跨用户和条件的鲁棒性能。

**Result:** 该数据集的初始版本已发布，并可通过projectaria.com访问。提供了开源工具和使用示例，方便研究者理解和使用数据。

**Conclusion:** A2PD展示了尖端技术在捕捉和处理多模态数据方面的潜力，为未来的研究提供了一种宝贵的资源。

**Abstract:** The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [52] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

> 提出一种基于预训练修正流模型并周期性添加引导的方法，以改进外观转移到3D资产的效果，实验结果表明此方法优于基线。

<details>
  <summary>Details</summary>

**Motivation:** 外观转移至具有不同几何结构的3D资产时，现有的最先进方法往往效果不佳。而单纯采用3D生成模型又容易产生不理想的结果。因此，提出这种新的基于引导的方法以改进外观转移的效果。

**Method:** 我们的方法是基于预训练的修正流模型，在该模型采样过程中周期性地加入引导以改进3D资产的外观转移效果。引导被建模为可微分的损失函数，具体实验了两种类型的引导：针对外观的部件感知损失和自我相似性损失。

**Result:** 实验表明，这种方法在转移到3D资产的纹理和几何细节上超过基线方法，在定性定量分析中都有出色表现。此外，传统评估指标不适合此任务，我们采用了基于GPT的系统进行结果评估，该系统能更公正且符合人类评价标准。用户研究也进一步确认了这一评估方法的有效性。

**Conclusion:** 研究表明，我们提出的方法成功转移了纹理和几何细节，且超越了其他方法在定性和定量上的评估。该方法在不同扩散模型和引导函数上的通用性也为未来更多应用场景提供了可能性。

**Abstract:** Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [53] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

> 本文提出了一种使用深度学习的方法来自动化血栓切除术中的关键步骤，提高治疗效率和安全性，取得了显著成果。

<details>
  <summary>Details</summary>

**Motivation:** 由于血栓切除术资源和人力需求量大，本文提出通过自动化关键步骤来提高该治疗方式的效率和安全性。

**Method:** 使用深度学习和自监督框架，通过回归预设任务来分类不同的骨骼标记点，以此来自动化血栓切除术的关键步骤。

**Result:** 实验表明，该模型在回归和分类任务上均优于现有方法，并且位置预设任务显著提高了分类性能。

**Conclusion:** 未来工作的重点是扩展该框架以实现完全自主的C臂控制，优化缺血性脑卒中血栓切除术中从骨盆到头部的轨迹。

**Abstract:** Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [54] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

> DuetMatch, a novel semi-supervised learning framework for medical image segmentation, addresses challenges in robustness and convergence with asynchronous optimization techniques and diverse regularization methods.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of joint optimization and improve robustness in medical image segmentation under semi-supervised conditions.

**Method:** DuetMatch, a dual-branch semi-supervised framework with asynchronous optimization, Decoupled Dropout Perturbation, Pair-wise CutMix Cross-Guidance, and Consistency Matching techniques are introduced.

**Result:** DuetMatch outperforms state-of-the-art methods on benchmark brain MRI segmentation datasets (ISLES2022 and BraTS).

**Conclusion:** The proposed DuetMatch framework demonstrates superior performance and robustness in semi-supervised medical image segmentation tasks.

**Abstract:** The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [55] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

> 开发了一种新的C形臂自动导航系统，利用X光图像预测向预定义解剖标志的3D位移矢量，并结合不确定性校准以确保可靠部署，从而提高荧光镜引导手术的安全性和准确性。

<details>
  <summary>Details</summary>

**Motivation:** 减少手动对齐C形臂带来的辐射暴露和程序延迟，提高荧光镜引导手术的准确性和可靠性。

**Method:** 使用基于X光图像的模型预测向解剖标志的3D位移矢量，结合不确定性校正（both aleatoric and epistemic uncertainties），并通过利用DeepDRR生成的合成X光数据集进行训练。

**Result:** 方法在多种架构下显示出强大的定位精度和校准的预测边界，表明该方法在安全可靠的自主C形臂系统中的潜力。

**Conclusion:** 开发的系统在准确性、安全性以及降低辐射暴露方面有潜力，对于自主C形臂系统是一个有益的组成部分。

**Abstract:** Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [56] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

> The paper discusses reducing the cost of image quality assessment (IQA) in production pipelines using deep generative models by introducing an automatic pre-filtering stage, resulting in a significant cost saving of 51.61% in a background inpainting use case using a simple AutoML solution.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the research is the inefficiency and high cost of manual IQA in production pipelines using generated images, aiming to improve the overall quality and lower the cost by implementing an automatic pre-filtering system.

**Method:** The method involves developing a formula to estimate cost savings from an automatic pre-filtering stage and applying this formula in a background inpainting case study with an AutoML solution.

**Result:** The result shows a significant reduction in cost, specifically 51.61%, due to the application of the automatic pre-filtering stage with a simple AutoML solution in the background inpainting use case.

**Conclusion:** The conclusion is that automatic pre-filtering can notably reduce the cost and improve the efficiency of image quality assessment in deep generative models by filtering out poorer quality images before they reach the manual assessment stage.

**Abstract:** Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [57] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

> 研究发现，将fMRI信号转换到类似语言模型的文本空间比直接转换到视觉或联合文本图像空间更有效。通过提出将fMRI信号投影到一个结构化文本空间的PRISM模型，实验表明这种方法在视觉刺激重建中优于以往方法。

<details>
  <summary>Details</summary>

**Motivation:** 理解大脑如何编码视觉信息是神经科学和机器学习中的一个中心挑战。研究希望通过从功能磁共振成像(fMRI)信号中重建视觉刺激，如图像，来推进这一理解。这个过程涉及到将fMRI信号转换成潜在空间，然后使用预训练的生成模型来重建图像。

**Method:** 本研究提出了一个名为PRISM的模型，该模型通过将fMRI信号投影到一个结构化的文本空间中，以此作为视觉刺激重建过程中的中间表示。模型包括一个基于对象的扩散模块，通过组合单独的对象来减少对象检测错误，以及一个属性关系搜索模块，自动识别与神经活动最吻合的关键属性和关系。

**Result:** 实验结果表明，本研究的方法优于现有的方法，感知损失减少了高达8%。这些结果强调了使用结构化的文本空间作为中间表示形式，来连接fMRI信号和图像重建的重要性。

**Conclusion:** 研究结论指出，使用结构化的文本空间作为fMRI信号和图像重建之间的桥梁，具有显著的优越性，特别是对于理解大脑如何编码和重建视觉信息方面。

**Abstract:** Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [58] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

> 本文使用数据导向的人工智能方法应对热带农业遥感制图的挑战，提出并评估了几种关键的数据处理技术，并推荐了一个适合大规模热带农业制图的实用方法。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在通过数据导向的人工智能（DCAI）方法解决热带地区农业遥感制图面对的独特挑战，如高质量标注数据的缺乏、标注成本高、数据变异性以及区域泛化难题。

**Method:** 本文优先考虑并审查了诸如自信学习、核心集选择、数据增强和主动学习等关键数据处理技术。

**Result:** 在大规模农业制图管道中评估了25种不同策略的适用性和成熟度。

**Conclusion:** 提出了一个实用的管道，采用了9种最成熟和简单的方法，用于大规模热带农业遥感制图项目。

**Abstract:** Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [59] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

> A flexible training method named StretchySnake is introduced for SSMs in video understanding, enhancing their spatio-temporal adaptability and robustness to varying video resolutions, significantly outperforming transformer and SSM baselines.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the fact that current training methods for video understanding, which are primarily designed for transformers, do not fully leverage the unique capabilities of SSMs, particularly their linear complexity and hidden-state recurrence. This leads to a performance drop when the model is faced with unseen resolutions, a problem this paper aims to solve.

**Method:** The method proposes a flexible training approach for State Space Models (SSMs) in video understanding, involving training models on videos of varying spatial and temporal resolutions, and dynamically interpolating model weights to adapt to any spatio-temporal scale.

**Result:** The proposed StretchySnake outperforms transformer and traditional SSM baselines by up to 28% on various action recognition benchmarks, showcasing strong adaptability to both short- and long-form videos, and fine-grained actions.

**Conclusion:** The flexible training method effectively enhances SSMs' capabilities in video understanding by making them resolution-agnostic and more robust, providing a simple yet powerful solution that outperforms existing models across a range of benchmarks.

**Abstract:** State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [60] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

> 论文提出一种新的混合模型VM-BeautyNet用于面部美感预测，结合了视觉变换器和Mamba模型，实现了全球最佳性能。

<details>
  <summary>Details</summary>

**Motivation:** 面部美感预测是一个复杂的任务，深度学习模型在捕获整体面部特征方面存在困难。为了解决这个问题，论文提出了一种结合两种模型优势的新方法。

**Method:** 该论文提出了一种新型的异构集成架构 VM-BeautyNet，它将视觉变换器（ViT）和基于Mamba的视觉模型的优点相结合。ViT擅长捕捉面部的整体结构和对称性，而Mamba则在处理长距离依赖和纹理方面更有效，且复杂度更低。

**Result:** 在基准SCUT-FBP5500数据集上，VM-BeautyNet达到了最先进的性能，取得了Pearson相关系数（PC）0.9212，平均绝对误差（MAE）0.2085以及均方根误差（RMSE）0.2698的成果。

**Conclusion:** 通过Grad-CAM可视化提供的可解释性分析，证实了两种模型的互补特征提取能力，为计算美学提供了一个强大的新架构范例。

**Abstract:** Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [61] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

> 开发了一种用于识别口腔鳞状细胞癌（OCSCC）的卷积神经网络（CNN），结合物理硬件系统提高OCSCC检测效率。研究显示，图像分辨率越高，预测的准确性越高，但在对数尺度上显示出递减的收益。

<details>
  <summary>Details</summary>

**Motivation:** 由于OCSCC在早期阶段难以察觉，导致较高的死亡率。使用经过训练的CNN能有效提高OCSCC的早期检测率，从而降低死亡率。

**Method:** 使用卷积神经网络（CNN）对口腔鳞状细胞癌（OCSCC）进行识别。CNN通过精确的图像分割技术来修改图像的RGB值，提高图像模式识别的准确性。硬件系统用于图像捕捉和处理，以确定准确预测所需的图像质量。

**Result:** 使用4293张训练图像（包括良性、恶性肿瘤及阴性样本）训练的CNN，其预测的精度、召回率和均值平均精度（mAP）被评估。通过图像分辨率变换进行测试，发现图像分辨率越高，预测准确性越高，但呈现递减的收益。

**Conclusion:** 开发了用于识别OCSCC的CNN，并设计了硬件系统来捕捉和处理细节图像。结果显示，更高分辨率的图像能提高预测准确性，但是像素增加到一定程度后收益递减。

**Abstract:** Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [62] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

> Meta引入Embody 3D数据集，包含500小时的3D人体运动数据，旨在为多模态交互研究提供资源。

<details>
  <summary>Details</summary>

**Motivation:** Meta的Codec Avatars实验室引入了Embody 3D，旨在提供丰富的3D人体运动数据，包括手部追踪、人体形状、文本注释以及每个参与者的单独音频轨道，以推动相关领域的发展。

**Method:** Codec Avatars Lab通过多摄像头采集阶段收集了来自439名参与者的数据，生成了名为Embody 3D的多模式数据集，该数据集包含了500小时的3D运动数据，总计超过5400万帧追踪的3D运动数据。

**Result:** 数据集包含了个人运动数据，如动作指令、手势和移动动作，以及多人行为和会话数据，如讨论、不同情绪状态下的会话、协作活动和公寓环境下共同生活场景。

**Conclusion:** Embody 3D数据集的推出，为研究者提供了一个重要的资源，能够支持更深入的人体运动分析和多模态交互的研究和应用开发。

**Abstract:** The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [63] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

> 文章提出一种新的任务前瞻场景分解与重构，利用人与物体的互动，迭代地分解和重构环境。这种方法在动态环境中表现出有效性和优势。

<details>
  <summary>Details</summary>

**Motivation:** 文章指出，人类行为是场景动态变化的主要原因，本身就包含了丰富的动态线索。针对传统方法在动态环境中进行物体级别重建存在的固有模糊问题，提出了一个新的任务前瞻场景分解与重构。

**Method:** 该文提出了一种在线方法，利用人与物体的互动，迭代地分解和重构环境。通过观察这些有意图的互动，可以动态地调整分解与重构流程。系统整合了多个任务，如精确的摄像机和物体姿态估计，实例分解，以及在线地图更新，并使用高斯散射技术实现了准确且一致的动态场景建模。

**Result:** 该方法在多个现实世界场景中进行了验证，并展示出了显著的优势。

**Conclusion:** 通过利用人类与物体的互动，文章提出的方法提供了一种比传统物体级别重建更灵活且逐步推进的替代选择。

**Abstract:** Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [64] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

> 本文介绍了Cerberus，一个用于实时视频异常检测的两阶段级联系统，通过动作掩码提示和基于规则的偏差检测技术，有效提高了计算效率和检测准确性。

<details>
  <summary>Details</summary>

**Motivation:** 项目动机是解决视觉语言模型在视频异常检测中的计算成本高和视觉定位性能不稳定的问题，提高实时部署能力。

**Method:** Cerberus采用两阶段级联系统设计，首先离线学习正常行为规则，然后在线推断时结合轻量级过滤和细粒度的VLM推理。系统通过动作掩码提示和基于规则的偏差检测两种创新方法提高性能，前者引导VLM关注与运动相关的区域，后者通过识别与学习规范的偏差来检测异常，而非枚举所有可能的异常。

**Result:** 实验结果表明，Cerberus在四个数据集上平均实现了57.68 fps的速度，相比其他方法有151.79倍的速度提升，并且实现了97.2%的准确性，这与最先进的VLM基VAD方法相当。

**Conclusion:** Cerberus作为一项实用的解决方案，证明了其在实时视频分析中的有效性。

**Abstract:** Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [65] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

> OpenLVLM-MIA是一个新的、无偏差的基准测试，它揭示了在评估大规模视觉语言模型成员推断攻击时存在分布偏差的问题，并为开发更强的隐私保护技术提供了基础。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于解决现有方法评估大规模视觉语言模型（LVLM）成员推断攻击时存在的分布偏差问题。

**Method:** OpenLVLM-MIA是一个新的基准测试，它包含6,000张图像，其中成员和非成员样本的分布被仔细平衡，并提供了三个不同训练阶段的真实成员标签。

**Result:** 使用OpenLVLM-MIA的实验结果表明，在无偏差条件下，最先进的成员推断攻击方法的表现收敛于随机机会水平。

**Conclusion:** OpenLVLM-MIA澄清了当前MIA研究在LVLM上的局限性，并为开发更强的隐私保护技术提供了坚实的基础。

**Abstract:** OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [66] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

> 我们提出了一个无训练框架Stroke2Sketch，它使用跨图像笔触注意力机制来精确转移笔触属性，同时保留语义结构和内容的忠实度。

<details>
  <summary>Details</summary>

**Motivation:** 为了实现由参考样风格指导的素描生成，需要精确地转移笔触属性，如线粗、变形和纹理稀疏度，同时保留语义结构和内容的忠实度。

**Method:** 我们提出了Stroke2Sketch，一种无训练框架，它引入了跨图像的笔触注意力机制，该机制嵌入在自注意力层中，用于建立细粒度的语义对应性，从而实现精确的笔触属性转移。此外，我们还开发了自适应对比度增强和语义聚焦注意力，以加强内容的保存和前景的强调。

**Result:** 实验结果表明，Stroke2Sketch在笔触控制和语义一致性方面优于现有方法，能够根据参考风格生成忠实的素描，且效果几乎与手绘结果无异。

**Conclusion:** Stroke2Sketch能够有效地合成忠实于样风格的素描，非常接近于手绘结果，优于现有的方法，在表现笔触控制和语义一致性方面表现出色。

**Abstract:** Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [67] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

> 论文通过构建大规模数据集ScaleDF，分析了深度伪造检测模型的缩放规律，发现在增加真实图像领域或伪造方法数量时，检测错误率遵循幂律衰减规律。这一发现可用于预测达到目标性能所需的数据量，并指导对抗深度伪造技术的策略。此外，论文还探讨了预训练和数据增强在扩展规模下的作用及限制。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的数据集无法满足研究需求，作者构建了大规模数据集ScaleDF，旨在研究深度伪造检测中模型性能与真实图像领域、伪造方法、训练图像数量之间的关系。

**Method:** 使用包含超过580万真实图像和880万伪造图像的ScaleDF数据集进行实验，通过增加真实图像领域和伪造方法的数量来观察模型性能的变化。

**Result:** 发现深度伪造检测中的错误率遵循幂律衰减规律，即错误率随真实图像领域或伪造方法数量增加而按照幂律减少，这一规律类似于大语言模型中的现象。

**Conclusion:** 论文阐述了通过增加数据量可以预测模型性能提高的目标，为对抗深度伪造技术提供了一种数据为导向的方法，并提出了预训练和数据增强在扩展规模下的局限性。

**Abstract:** This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [68] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

> Scale-DiT是一种新的扩散框架，采用分层局部注意力机制和低分辨率全局引导，实现了高效、可扩展且语义连贯的超高分辨率图像合成。

<details>
  <summary>Details</summary>

**Motivation:** 根据摘要，作者提出Scale-DiT的动机是因为当前扩散模型在超高分辨率（超过1K x 1K）图像生成上受到注意力机制二次复杂度和缺乏原生4K训练数据的限制。他们希望开发出一种新的图像生成方法，能够在不增加额外高分辨率训练数据的情况下扩展到4K分辨率，且保持相对较高的效率。

**Method:** 通过引入分层局部注意力机制和低分辨率全局引导，Scale-DiT框架解决了现有扩散模型在超高分辨率文本到图像生成上的局限性。该方法将高分辨率潜在特征划分为固定大小的局部窗口，以将注意力复杂度从二次降低到近线性，并用带有缩放位置锚点的低分辨率潜在特征注入全局语义。此外，一种轻量级的LoRA自适应方法在去噪过程中连接全局和局部路径，确保结构和细节之间的连贯性。为了最大化推理效率，该模型重新排列了以希尔伯特曲线顺序排列的标记序列，并实现了融合内核以跳过屏蔽操作，从而获得更适合GPU的设计。

**Result:** 实验表明，Scale-DiT 在推理速度上比密集注意力基线方法快超过2倍，内存使用更少，并且在不依赖额外的高分辨率数据的情况下可扩展至4K x 4K分辨率。在定量（FID，IS，CLIP Score）和定性比较中，Scale-DiT 表现出了更好的全局连贯性和锐利的局部细节，与依赖原生4K训练数据的最先进技术相比，可匹配或超越其性能。

**Conclusion:** 这种方法展示了分层局部注意与指导性低分辨率锚点作为推进超高分辨率图像生成的前景和有效途径。通过这种方法，图像生成技术可以超越现有限制，在没有资源密集型高分辨率训练数据集的情况下实现高分辨率和高质量的图像合成。

**Abstract:** Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [69] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

> 本文提出了一种云边协作框架DiffusionX，用于高效地进行多轮提示图像生成，通过优化计算负载分布，减少了生成时间并提高了图像质量。

<details>
  <summary>Details</summary>

**Motivation:** 提出DiffusionX的动机在于解决扩散模型生成过程中的计算负担重，以及用户需要多次迭代调整提示以达到所需结果的问题，这些问题增加了延迟并加重了云资源的负担。

**Method:** 提出了一种名为DiffusionX的云边协作框架，旨在高效地进行多轮提示生成。该系统中，轻量级的设备扩散模型通过快速生成预览图像与用户互动，而在提示最终确定后，高容量的云模型执行最终的细化工作。此外，还引入了一个噪声级别预测器，动态平衡计算负载，优化了延迟和云工作负载之间的权衡。

**Result:** 实验结果表明，与Stable Diffusion v1.5相比，DiffusionX将平均生成时间减少了15.8%，同时保持了可比较的图像质量。此外，它仅比Tiny-SD慢0.9%，但图像质量有了显著提升。

**Conclusion:** 结论是DiffusionX展示了一种在减少计算资源消耗的情况下，能够高效生成高质量图像的方法，证明了其有效性和可扩展性。

**Abstract:** Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [70] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

> TokenAR通过强化基于Autoregressive (AR)模型的token处理机制解决多参考生成过程中的身份混淆问题，实现高质量多主体生成。

<details>
  <summary>Details</summary>

**Motivation:** 解决多重参考生成方法中不同身份去耦问题，提升图像生成质量与多样性。

**Method:** TokenAR框架通过增强token级别的表示来解决参考身份混合的问题，包括：1) Token Index Embedding聚类tokens索引以便更好地表示同一参考图像；2) Instruct Token Injection作为额外的视觉特征容器，注入详细的互补先验知识以增强参考token；3) 身份-标记解耦策略（ITD）使token表示直接关注独立身份特征。

**Result:** 实验显示，TokenAR在多参考图像生成任务中超越了目前的最先进模型。

**Conclusion:** TokenAR显著增强了基于AR模型的方法在条件图像生成中的能力，可实现良好的身份一致性和高质量背景重建。

**Abstract:** Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [71] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

> 研究对比了强化学习(RL)和监督微调(SFT)对多模态语言模型(MLLM)视觉编码器的影响，发现RL在视觉表征上有明显优势，并提出了PIVOT方法增强视觉编码器。

<details>
  <summary>Details</summary>

**Motivation:** 为了填补对MLLM视觉编码器理解不足的空白，特别是训练策略如何影响视觉编码器以及MLLM。

**Method:** 通过多样且深入的实验分析了多模态语言模型（MLLM）的视觉编码器，实验范围从ImageNet分类和分割到梯度可视化。

**Result:** 研究结果表明，RL训练策略产生的视觉表征更强且定位更精确，优于SFT策略，并提出了构建强大视觉编码器的简单方法PIVOT。

**Conclusion:** PIVOT训练的视觉编码器不仅优于更大、训练更充足的模型，而且计算成本低于标准视觉预训练的1%，为提高MLLM视觉骨干的有效性和效率开辟了路径。

**Abstract:** A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [72] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [73] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

> 提出了MIRAD数据集，专门用于社会制造中的缺陷检测，评估了多种异常检测方法，显示了这些方法在实际个性化生产中的性能下降。

<details>
  <summary>Details</summary>

**Motivation:** 社会制造利用社区协作和分散资源实现个性化生产，但也带来质量问题，特别是缺陷检测。MIRAD数据集旨在克服数据和算法稀缺，评估和优化这些方法以应对实际生产挑战。

**Method:** Social manufacturing利用社区协作和分散资源实现现代工业的个性化生产。然而，这一范式转变也带来了质量控制方面的重大挑战，尤其是缺陷检测。主要困难来自三个方面。首先，产品通常具有高度定制化的配置。其次，生产通常涉及分散的小批量订单。第三，图像环境在分布式站点之间差异很大。为了克服现实世界数据集和定制算法的稀缺性，我们引入了大规模个性化鲁棒异常检测（MIRAD）数据集。作为第一个专门为社会制造中的异常检测设计的基准，MIRAD捕捉了该领域的三个关键维度：（1）多样的个性化产品具有较大的类内变异，（2）数据来自六个地理上分散的制造节点，（3）大量的成像异质性，包括光照、背景和运动条件的变化。然后，我们在MIRAD上对最先进的（SOTA）异常检测方法进行了广泛的评估，涵盖了单类别、多类别和零样本方法。

**Result:** 对多种异常检测方法进行了评估，结果表明所有模型在MIRAD上的表现都显著下降，这表明在实际大规模个性化生产中缺陷检测仍然存在复杂难题。

**Conclusion:** MIRAD数据集为开发符合工业需求的稳健质量控制解决方案提供了现实基础，有助于连接工业需求和学术研究，推动Industry 5.0的发展。

**Abstract:** Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [74] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

> A study presents a dataset of 3,000 videos of cataract surgeries with detailed annotations for diverse aspects of surgical procedures, including a domain adaptation baseline for phase recognition task. The dataset is designed to support the training and evaluation of AI models for surgical applications.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to contribute a comprehensive, annotated dataset to the field of computer-assisted surgery, particularly for cataract surgery where current datasets are deemed insufficient in diversity and depth of annotation.

**Method:** The paper introduces a dataset of 3,000 phacoemulsification cataract surgery videos with annotations for diverse aspects of surgical procedures. Benchmarking experiments are conducted to assess the efficacy of the dataset in surgical AI tasks, with a particular highlight on phase recognition using domain adaptation techniques.

**Result:** Benchmarking experiments validate the dataset's effectiveness in tasks such as workflow recognition, scene segmentation, and automated skill assessment, with domain adaptation providing a valuable baseline for phase recognition.

**Conclusion:** The introduction of a large dataset with rich annotations significantly advances the field of AI-assisted cataract surgery, offering a robust testbed for the development and evaluation of surgical AI systems.

**Abstract:** The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [75] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

> iWatchRoadv2是一个全自动的实时道路坑洞检测、GPS定位和动态道路健康可视化平台，使用OpenStreetMap，针对印度道路条件进行了优化。

<details>
  <summary>Details</summary>

**Motivation:** 印度道路网络复杂且维护不足，道路坑洞带来了显著的安全隐患和维护挑战。

**Method:** 使用了7000多帧驾驶记录仪拍摄的多样化印度道路条件、天气模式和光照情况的数据集，用于微调Ultralytics YOLO模型进行准确的坑洞检测。系统通过OCR提取的视频时间戳与外部GPS日志同步来精确定位每个检测到的坑洞，并通过优化后的后台数据库来管理路段和承包商信息。

**Result:** 该系统实现了自动化的坑洞检测、定位和可视化，同时支持与承包商和官员的自动报警，有助于政府加强道路管理和维护，实现透明的治理和道路基础设施的持续改进。

**Conclusion:** iWatchRoadv2提供了一个成本效益高、可扩展的解决方案，实现了完整的坑洞监测生命周期的自动化，从检测到维修验证，支持智慧城市管理，提高道路健康管理水平。

**Abstract:** Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [76] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

> 介绍了一种名为Demeter的新数据驱动参数化模型，用于编码植物形态的关键因素，并展示了在植物形状合成、结构重构及生物物理模拟方面的应用效果。

<details>
  <summary>Details</summary>

**Motivation:** 尽管对于人类和动物而言存在着强大的模型，但对于植物来说却缺乏同样表现力的建模方法。为了推动农作物建模技术的发展，本研究提出了一个可以灵活处理植物形态特征的新模型。

**Method:** 本研究提出了一种名为Demeter的数据驱动参数化模型，该模型能够编码植物形态的关键因素，包括拓扑结构、形状、关节运动和非刚性变形，并将这些因素编码进一个紧凑的、学习到的表示中。与之前的方法不同，Demeter 能够处理不同种类植物之间形状拓扑的变化，并且模型可以捕捉到三种形状变异性：关节运动、子组件形状变化以及非刚性变形。

**Result:** 研究人员从大豆农场收集了大规模的真实数据作为实验的测试平台，实验结果表明，Demeter 模型能有效地合成形状、重构结构，并模拟生物物理过程。

**Conclusion:** 本研究开发的Demeter参数化模型能够在一定程度上填补植物建模领域的空白，并在合成形状、结构重构以及生物物理过程模拟方面显示出良好的效果。

**Abstract:** Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [77] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

> The paper presents a light framework using sparse convolution, SPLite decoder, and quantization-aware training to achieve real-time and low-latency inference on AR/VR devices, demonstrating 2.98x speed-up on Raspberry Pi 5 without losing accuracy.

<details>
  <summary>Details</summary>

**Motivation:** The challenge of deploying deep learning models on AR/VR devices, which require real-time inference, low power consumption, and low latency, motivates the design of an efficient and accurate model in this paper.

**Method:** We design a light framework with an encoder-decoder architecture. Sparse convolution is applied on a ResNet-18 backbone to handle the sparsity in hand pose images, leading to a 42% efficiency improvement. The SPLite decoder is introduced to enhance the decoding process's frame rate by 3.1x. Additionally, quantization-aware training is used to decrease memory usage without sacrificing accuracy.

**Result:** The proposed system achieves a 2.98x speed-up on a Raspberry Pi 5 CPU. It maintains comparable accuracy to the state-of-the-art methods but significantly enhances computational efficiency, as shown on the FreiHAND dataset.

**Conclusion:** This work introduces an efficient and accurate deep learning framework for AR/VR devices. By improving both the efficiency and accuracy, the proposed system overcomes the challenge of deploying deep learning models on edge devices with real-time requirements.

**Abstract:** With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [78] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

> 该研究提出了REALM框架，旨在解决3D对象定位问题，通过结合2D视觉语言模型和3D高斯散射表示，以及提出新的全局到局部空间定位策略，提升了在不同指令下的3D目标分割性能。

<details>
  <summary>Details</summary>

**Motivation:** 目前的3D分割方法难以应对模糊的、基于推理的指令，而2D视觉语言模型虽然在这样的推理上表现优异，但缺乏内禀的3D空间理解能力。

**Method:** 引入了一个称为REALM的新式MLLM（多模式语言-视觉模型）-agent框架，可以在开放世界环境下进行基于推理的分割，同时避免需要大量的3D特定后训练。利用3D高斯散射表示进行分割，并引入了全局到局部的空间定位策略以提高分割准确性。

**Result:** 实验表明，REALM在LERF、3D-OVS和新引入的REALM3D基准测试中，对明确和隐含指令的解释都表现出了出色的能力。

**Conclusion:** 该研究展示了一个能理解和执行复杂人类指令的系统框架，适用于3D交互任务，如对象移除、替换、样式转换等。

**Abstract:** Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [79] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

> SSL4RL框架解决了强化学习在视觉语言模型中应用的难题，利用自监督学习任务作为可靠奖励来源，获得了出色的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型虽然能够将大型语言模型与视觉输入结合展现出强大的能力，但仍经常会由于依赖语言先验或在推理中依赖文本捷径而未能充分利用视觉证据。尽管强化学习可以将模型对齐到期望的行为，但由于缺乏可扩展的和可靠的奖励机制，其在视觉语言模型中的应用受到阻碍。

**Method:** SSL4RL框架通过使用自监督学习任务作为强化学习中可靠的奖励机制来源，解决了强化学习在视觉语言模型中的应用难题。具体来说，它将自监督学习目标（如预测图像旋转或重建遮罩的补丁）转化为密集的自动奖励信号，避免了对人工偏好数据或不可靠的AI评估者的依赖。

**Result:** 实验结果表明，SSL4RL在视觉主导任务和视觉语言推理基准上显著提高了性能。通过系统的消融研究，我们还确定了关键影响因素，如任务难度、模型规模以及与目标领域的语义对齐，为未来的工作提供了新的设计原则。进一步证明了该框架的通用性，将其应用于图学习中也取得了显著的增益。

**Conclusion:** SSL4RL建立了一个灵活有效的范式，使用可验证的自监督目标来对齐多模态模型。

**Abstract:** Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>
