<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

> The paper introduces LUCARIO, a framework that uses Bayesian Networks and large language models to answer probabilistic questions over large tabular data, demonstrating improved performance over existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the shortcomings of current NL2SQL systems in handling probabilistic questions over tabular data that require reasoning under uncertainty.

**Method:** Our method induces Bayesian Networks from tables, translates natural language queries into probabilistic queries, and uses large language models (LLMs) to generate final answers.

**Result:** Empirical results demonstrate significant improvements over baselines.

**Conclusion:** The study highlights the benefits of hybrid symbolic-neural reasoning for probabilistic QA over large tabular data.

**Abstract:** Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

> 构建了新多语言功能性基准测试，更准确地评估大模型的多语言能力，发现现有静态基准测试功能表现捕捉能力不足且模型鲁棒性在不同语言间有显著差异。

<details>
  <summary>Details</summary>

**Motivation:** 多语言能力在大规模语言模型中通常通过静态数据基准进行评估，但这些评估往往无法充分反映模型在多语言设置中的实际性能和鲁棒性。为了解决这一问题，构建了多语言功能性基准测试。

**Method:** 基于现有功能基准测试模板从英语翻译到包括法语、西班牙语、印地语、阿拉伯语和约鲁巴语在内的五种语言，以此构建跨语言功能性基准测试CL-GSM Symbolic和CL-IFEval，以更准确地评估大规模语言模型在多语言环境中的实际表现和鲁棒性。

**Result:** 研究结果显示，某些静态多语言基准测试功能性能的捕捉能力远低于新构建的功能性基准测试。例如，M-GSM和CL-GSM Symbolic在英语、法语和西班牙语中有24%、17%和18%的性能下降；而Belebele和CL-IFEval之间的性能下降为15%-24%，而M-MMLU和CL-IFEval之间的性能下降仅为0.5%-3%。此外，不同语言中的模型鲁棒性表现差异显著，以阿拉伯语和英语为例，这两种语言在反复评估中表现最为稳定。

**Conclusion:** 新构建的跨语言功能性基准测试能更精准地衡量大规模语言模型在多语言环境中的性能表现和鲁棒性，指出了不同静态多语言基准测试在功能性能捕捉上的不一致性。

**Abstract:** Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

> 研究发现，尽管LLM生成的想法在创新度方面具有潜力，但在实际执行结果上并不如人类专家的想法有效。

<details>
  <summary>Details</summary>

**Motivation:** 探讨AI生成的想法是否能导致更好的研究成果，而不是仅仅看起来新颖。

**Method:** 通过招募43位专家研究人员执行由专家撰写或LLM生成的想法来进行执行研究。每个专家花费超过100小时实现想法，并撰写一篇4页的短论文来记录实验，并由专家NLP研究者进行匿名评审。

**Result:** 执行研究后，LLM生成的想法的评分在所有评估指标（新颖性、兴奋度、有效性及总体）上显著下降，甚至在许多指标上，人类的想法得分高于LLM的想法。

**Conclusion:** 此执行研究揭示了当前LLM在生成真正有效的研究想法方面的局限性，以及在没有执行结果的情况下评估研究想法的挑战。

**Abstract:** Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

> MultiFinRAG通过多模态提取策略和模态感知的精确检索技术，大幅提高了在财务问答任务上的精度表现。

<details>
  <summary>Details</summary>

**Motivation:** 传统的大型语言模型和增强检索生成管道在处理包含密集叙述文本、结构化表格和复杂图像的财务文档时存在代数限制、布局丢失和片段化跨模态上下文等问题。

**Method:** MultiFinRAG采用多模态提取策略，首先将表格和图像分批处理，通过轻量级且量化后的开源多模态LLM生成结构化JSON输出和简洁的文本摘要。然后将这些输出与叙述性文本嵌入并索引，设置模态感知的相似性阈值以实现精确检索。此外，采用分层降级策略以动态地从文本到文本+表格+图像的上下文转换，从而减少无关上下文并促进跨模态推理。

**Result:** 在涉及文本、表格、图像和多模态推理的复杂财务问答任务上，MultiFinRAG相比ChatGPT-4o（免费版），在普通硬件上运行时，能获得高出19个百分点的准确率。

**Conclusion:** MultiFinRAG是一个专门为财务问答设计的检索增强生成框架，能够解决传统大型语言模型和检索增强生成方法的局限性问题。

**Abstract:** Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

> 研究使用VBVQ评估了六个LLMs在检测暴力倾向上的能力，发现他们表面的文本生成表现与实际对暴力反应的偏好存在分歧，并且倾向于表现出与社会科学发现相矛盾的人口统计学差异。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型(LLMs)被越来越多地提议用于检测和响应在线暴力内容，但它们对道德模糊现实情景的推理能力尚未得到充分研究，因此本研究旨在填补这一空白。

**Method:** 使用经过验证的社会科学研究工具——暴力行为情境问卷(VBVQ)来评估LLMs，引入基于角色的提示以改变美国境内的种族、年龄和地区身份来进行潜在偏见评估。在统一的零样本设置下评估了六个在不同地缘政治和组织背景下开发的LLMs。

**Result:** 研究表明LLMs的表层文本生成经常与其对暴力反应的内部偏好相悖，而且他们的暴力倾向随不同的人口统计学特征而变化，这经常与既定的犯罪学、社会科学和心理学发现相矛盾。

**Conclusion:** 尽管LLMs在表层文本生成上有一定表现，但其内部偏好和人口统计学特征对暴力反应倾向的影响仍需进一步探讨，揭示了现有模型在应对复杂社会情景上的局限性。

**Abstract:** Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

> The abstract discusses challenges and considerations for developing end-to-end fact-checking systems specifically in the medical domain, suggesting an interactive approach over autonomous systems.

<details>
  <summary>Details</summary>

**Motivation:** To explore reasons why advanced fact-checking systems are not widely used in public health and medicine despite technological advancements, and to understand how clinical experts verify medical claims.

**Method:** Presents a study examining how clinical experts verify real medical claims from social media by synthesizing scientific evidence.

**Result:** Identifies fundamental challenges in applying end-to-end fact-checking to medicine, such as connecting claims to relevant evidence and dealing with ambiguous or subjective claims.

**Conclusion:** Proposes that fact-checking in medicine should be treated as an interactive communication problem, rather than relying solely on end-to-end systems.

**Abstract:** Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

> 论文提出了一系列方法以改进语言模型对下游应用的适应性，包括使用未标记数据的持续预训练技术、参数高效的微调方法、改进的监督微调方法以及开发新的评估基准，这些方法显著提高了语言模型的鲁棒性、效率和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决语言模型在特定任务中的有效和鲁棒性适应问题。随着模型规模和复杂性的增加，仅依赖标记数据的微调方法往往未能充分利用未标记数据，导致在小规模特定任务数据上过拟合，并且需要大量的计算资源。这些问题限制了语言模型在实际语言任务中的应用。

**Method:** 这是我们论文的方法部分摘要：该论文首先探讨了从未标记数据中提取任务相关知识的策略，并引入了一种新的持续预训练技术，该技术优于现有的半监督方法。其次，提出了一种参数高效的微调方法，该方法显著减少了内存和计算成本，同时保持了竞争力的性能。还介绍了改进的监督微调方法，使语言模型能够更好地遵循指令，尤其是在标记数据稀缺的情况下，提高了它们在各种NLP任务（包括开放式生成）上的性能。最后，开发了新的评估方法和基准（如多跳空间推理任务），以更全面地评估语言模型的能力和适应性。

**Result:** 实验结果显示，这些方法在各种NLP任务中显著提高了语言模型的鲁棒性、效率和泛化性能，使其能够更好地适应广泛的应用场景。

**Conclusion:** 这项研究标志着向更加健壮和高效的语言模型迈进了一大步，推动了人工智能迈向通用智能的目标。

**Abstract:** Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

> 本文提出了一种新的预训练数据集整理流水线，可以适应多种语言，并通过1000多种语言的数据集提升了多语言模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 训练高性能的多语言大规模语言模型仍然是一个挑战，尤其是在针对多种语言调整过滤和去重流水线时。为此，本文旨在创建一个能够适用于多种语言的预训练数据集整理方法，以解决这个问题并提高模型性能。

**Method:** 此论文提出了一种新的预训练数据集整理流水线，基于FineWeb，可以自动适应任何语言。此外，还提出了一种简单而原理性的方法来重新平衡数据集，考虑了复制数量和质量。

**Result:** 实验表明，该流水线创建的非英语语料库能够产生比先前数据集更优秀的模型。通过使用几乎100个Common Crawl快照，作者扩大了他们的流水线，使用超过1000种语言创建了新的20TB（50亿文档）的多语言数据集FineWeb2，并发布了他们的流水线，训练，以及评估代码库。

**Conclusion:** 通过上述方法，作者解决了多语言模型训练中遇到的挑战，成功地创建了一个适应多种语言的预训练数据集整理流水线，并验证了该方法的有效性。此外，还提出了一个简单且原理性的方法来重新平衡数据集，进一步提升了模型性能。

**Abstract:** Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

> KaLM-Embedding-V2 is a compact and versatile text embedding model with enhanced performance through improved architecture and training techniques, achieving competitive results against much larger models while using less parameters.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to introduce a compact and versatile embedding model, KaLM-Embedding-V2, with improved performance through architectural upgrades and a comprehensive training methodology.

**Method:** Our model, KaLM-Embedding-V2, removes the causal attention mask and adopts a fully bidirectional transformer with mean-pooling to produce embeddings. It uses a multi-stage training with pre-training and fine-tuning, includes a focal-style reweighting mechanism and online hard-negative mixing, and leverages extensive data from over 20 categories for pre-training and 100 for fine-tuning.

**Result:** Evaluations on MTEB in Chinese and English indicated significant outperformance compared to models of similar size and competitive performance with larger models, establishing a new standard for a versatile and compact embedding model.

**Conclusion:** KaLM-Embedding-V2 sets a new bar for small-size embedding models, achieving performance competitive with much larger models while using less than 1B parameters.

**Abstract:** In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [10] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

> 本文提出了一种新方法，使得语言模型的微调能够模仿基于提示的做法，这为优化模型的泛化能力和长上下文处理提供了新的思路。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索是否可以通过某种方式修改模型，使得微调能够模仿提示的效果。即改变模型参数（如通过微调）是否可以达到提示的效果。

**Method:** 研究提出了一种元训练方法，使语言模型的梯度更新能够模仿基于提示的效果。这种方法采用基于梯度的元学习工具，使用模型的提示预测作为目标，不需要真实标签。

**Result:** 实验结果显示，后续的梯度下降训练可以恢复部分（有时甚至是全部）提示模型的性能，包括在"反转诅咒"任务上的改进，以及在单次梯度更新后回答文本段落的问题。

**Conclusion:** 研究表明，适当的初始化下，梯度下降学习可以非常有表现力。此外，该研究为长上下文建模提供了新的途径，并提供了关于梯度学习泛化能力的洞见。

**Abstract:** There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [11] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

> 本文扩展了MPI模型，引入了16PF模型，开发了SAC框架，能够更精细地控制LLMs的人格特质，使LLM的行为更加具有一致性和可操控性。通过实验发现，多维度控制的人格特质强度能更好地模拟人类行为。

<details>
  <summary>Details</summary>

**Motivation:** 现有的LLM模型在展示人类性格方面存在两个主要的局限性：依赖于只提供粗宽性格维度的大五人格框架，并缺乏对特质强度的控制机制。因此，为了弥补这个缺口并提高LLM在模拟人类性格表达方面的灵活性和精确度，本文提出了新的方法和模型。

**Method:** 本研究将原有的Machine Personality Inventory (MPI)模型扩展，用16人格因素（16PF）模型取代了之前的大五人格框架，从而精确地控制了十六种不同的个性特质。并且提出了一个名为Specific Attribute Control (SAC)的结构化框架，用于评估和动态激发LLMs的人格特质强度。SAC方法通过基于形容词的语义锚定来指导人格特质强度的表达，并使用了五个强度因素：频率、深度、阈值、努力和意愿的行为问题来进行人格特质的操控。

**Result:** 实验表明，将人格强度建模为连续谱相比于二元特质切换，能够提供更为一致和可控的性格表达。此外，我们观察到目标特质强度的变化系统地影响了与之紧密相关的其他特质，这种影响在心理学意义上是连贯的，暗示LLMs内化了多维度的人格结构。

**Conclusion:** 这项工作开辟了新的途径，为控制人类与机器之间的交互打开了新机会，使LLMs更具有人类特性，这对于医疗、教育、面试等领域的应用有着重要意义。

**Abstract:** Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [12] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

> 本文通过CA-Ben基准测试评估了六个大型语言模型在印度财务背景下的性能，并发现这些模型在法律推理方面表现出色，但在数值计算和法律解释方面仍有改进空间。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在填补印度复杂金融背景下LLM评估领域的空白，探讨这些模型如何有效捕捉和应用特定领域的财务知识。

**Method:** 本文介绍了CA-Ben基准测试，该测试由基于印度注册会计师协会（ICAI）考试的结构化问答数据集组成，旨在评估LLM在财务、法律和定量推理方面的能力。评估了六个主要的LLM，包括使用标准化协议评估的GPT 4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4。

**Result:** 结果表明，不同模型的性能存在差异，Claude 3.5 Sonnet和GPT-4o在概念和法律推理方面表现尤为突出。然而，在数值计算和法律解释方面仍面临挑战。

**Conclusion:** 研究结果强调了当前LLM的优势和局限性，建议未来改进方法应集中在混合推理和检索增强生成上，特别是对于定量分析和法律解释的准确性。

**Abstract:** Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [13] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

> 本文提出了一种新的半监督可扩展统一框架(SSUF)，用以解决电子商务查询分类中的信息不足和标签依赖问题，并通过实验验证了其优越性。

<details>
  <summary>Details</summary>

**Motivation:** 查询分类，包括意图和类别预测等多个子任务，对电子商务应用至关重要。电子商务查询通常较短且缺乏上下文，导致标签之间的信息无法被充分利用，从而造成建模时的先验信息不足。现有的大多数工业查询分类方法依赖于用户的后点击行为来构建训练样本，这导致了一个马太效应的恶性循环。此外，查询分类的子任务缺乏统一框架，使得算法优化效率低下。

**Method:** 我们提出了一种新的半监督可扩展统一框架(SSUF)，该框架包含多个增强模块，用以统一查询分类任务。知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号减少对后标签的依赖。结构增强模块基于复杂的标签关系增强标签表示。每个模块的输入特征可根据每个子任务的需要灵活添加或移除。

**Result:** 我们进行了广泛的离线和在线A/B实验，结果显示，与当前的先进模型相比，SSUF模型具有显著的优势。

**Conclusion:** 通过广泛的离线和在线A/B实验，结果表明SSUF模型显著优于当前最先进的模型。

**Abstract:** Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [14] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

> 本文介绍了MT2-CSD，一个用于多目标多轮对话立场检测的大型数据集，并提出了Large Language Model增强的对话关系注意力网络（LLM-CRAN），实验表明LLM-CRAN在MT2-CSD数据集上的对话立场检测任务中显著优于基线模型。

<details>
  <summary>Details</summary>

**Motivation:** 传统立场检测方法主要针对单个实例进行，限制了其处理真实社交环境中多方面对话的能力，因此本文的动机是开发针对多目标多轮对话立场检测的更复杂和准确的模型。

**Method:** 提出了Large Language Model增强的对话关系注意力网络（LLM-CRAN），利用LLM的逻辑推理能力来提高对话理解能力。

**Result:** 实验结果显示，LLM-CRAN在多目标多轮对话立场检测任务上优于当前的几个基线模型。

**Conclusion:** 本文提出的方法LLM-CRAN在新发布的MT2-CSD数据集上的测试证明了其在多目标多轮对话立场检测上取得了显著的性能提升。

**Abstract:** In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [15] [OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport](https://arxiv.org/abs/2506.20741)
*Qin Ren,Yifan Wang,Ruogu Fang,Haibin Ling,Chenyu You*

Main category: cs.CV

> OTSurv利用最优传输方法解决病理图像中的异质性问题，提高了生存预测的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有MIL方法难以捕捉病理图像中的异质性。OTSurv旨在通过全局和局部约束来解决这一问题，以更好地预测生存率。

**Method:** OTSurv通过最优传输（OT）框架处理生存预测问题，引入了两个约束：全球长尾约束和局部不确定性约束，进而将问题转化为不平衡最优传输问题，使用矩阵缩放算法求解。

**Result:** OTSurv在六个基准测试中表现优于现有方法，平均C指数提高了3.6%。

**Conclusion:** OTSurv是一种高效的生存预测工具，在数字病理学中具有高可解释性。

**Abstract:** Survival prediction using whole slide images (WSIs) can be formulated as a
multiple instance learning (MIL) problem. However, existing MIL methods often
fail to explicitly capture pathological heterogeneity within WSIs, both
globally -- through long-tailed morphological distributions, and locally
through -- tile-level prediction uncertainty. Optimal transport (OT) provides a
principled way of modeling such heterogeneity by incorporating marginal
distribution constraints. Building on this insight, we propose OTSurv, a novel
MIL framework from an optimal transport perspective. Specifically, OTSurv
formulates survival predictions as a heterogeneity-aware OT problem with two
constraints: (1) global long-tail constraint that models prior morphological
distributions to avert both mode collapse and excessive uniformity by
regulating transport mass allocation, and (2) local uncertainty-aware
constraint that prioritizes high-confidence patches while suppressing noise by
progressively raising the total transport mass. We then recast the initial OT
problem, augmented by these constraints, into an unbalanced OT formulation that
can be solved with an efficient, hardware-friendly matrix scaling algorithm.
Empirically, OTSurv sets new state-of-the-art results across six popular
benchmarks, achieving an absolute 3.6% improvement in average C-index. In
addition, OTSurv achieves statistical significance in log-rank tests and offers
high interpretability, making it a powerful tool for survival prediction in
digital pathology. Our codes are available at
https://github.com/Y-Research-SBU/OTSurv.

</details>


### [16] [StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation](https://arxiv.org/abs/2506.20756)
*Haodong Li,Chen Wang,Jiahui Lei,Kostas Daniilidis,Lingjie Liu*

Main category: cs.CV

> A new approach, StereoDiff, combines stereo matching and video depth diffusion to better handle static and dynamic regions in video depth estimation, achieving state-of-the-art performance.

<details>
  <summary>Details</summary>

**Motivation:** To address the fundamental differences in temporal consistency requirements for static and dynamic regions in videos, which are not adequately addressed by simply extending image depth estimation methods.

**Method:** StereoDiff, a two-stage video depth estimator, synergizes stereo matching for static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas.

**Result:** StereoDiff shows superior consistency and accuracy in video depth estimation, achieving SoTA performance on zero-shot, real-world dynamic video depth benchmarks.

**Conclusion:** The synergistic approach of combining stereo matching and video depth diffusion effectively captures the advantages of both techniques, leading to better performance in video depth estimation.

**Abstract:** Recent video depth estimation methods achieve great performance by following
the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained
video diffusion models with massive data. However, we argue that video depth
estimation is not a naive extension of image depth estimation. The temporal
consistency requirements for dynamic and static regions in videos are
fundamentally different. Consistent video depth in static regions, typically
backgrounds, can be more effectively achieved via stereo matching across all
frames, which provides much stronger global 3D cues. While the consistency for
dynamic regions still should be learned from large-scale video depth data to
ensure smooth transitions, due to the violation of triangulation constraints.
Based on these insights, we introduce StereoDiff, a two-stage video depth
estimator that synergizes stereo matching for mainly the static areas with
video depth diffusion for maintaining consistent depth transitions in dynamic
areas. We mathematically demonstrate how stereo matching and video depth
diffusion offer complementary strengths through frequency domain analysis,
highlighting the effectiveness of their synergy in capturing the advantages of
both. Experimental results on zero-shot, real-world, dynamic video depth
benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,
showcasing its superior consistency and accuracy in video depth estimation.

</details>


### [17] [ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations](https://arxiv.org/abs/2506.20757)
*Zhiyuan Wu,Yongqiang Zhao,Shan Luo*

Main category: cs.CV

> The paper presents ConViTac, a network for visual-tactile representation learning using a novel Contrastive Embedding Conditioning mechanism that enables better aligned and integrated features, leading to superior performance over existing methods in robotic perception and manipulation tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of the paper is to improve the integration of visual and tactile information in robotic perception and manipulation tasks, as previous methods often rely on direct feature combination which results in poor integration.

**Method:** ConViTac is a visual-tactile representation learning network that introduces a Contrastive Embedding Conditioning (CEC) mechanism to enhance feature alignment during fusion. The CEC uses a contrastive encoder that has been pretrained through self-supervised contrastive learning to project visual and tactile inputs into unified latent embeddings. Visual-tactile feature fusion is then performed through cross-modal attention on these embeddings.

**Result:** Experiments show that ConViTac outperforms current state-of-the-art methods in real-world applications and the CEC mechanism improves accuracy by up to 12.0% in material classification and grasping prediction tasks.

**Conclusion:** The conclusion is that ConViTac, with its CEC mechanism, is an effective approach for fusing visual and tactile information, leading to improved performance in downstream tasks, such as material classification and grasping prediction, compared to previous methods.

**Abstract:** Vision and touch are two fundamental sensory modalities for robots, offering
complementary information that enhances perception and manipulation tasks.
Previous research has attempted to jointly learn visual-tactile representations
to extract more meaningful information. However, these approaches often rely on
direct combination, such as feature addition and concatenation, for modality
fusion, which tend to result in poor feature integration. In this paper, we
propose ConViTac, a visual-tactile representation learning network designed to
enhance the alignment of features during fusion using contrastive
representations. Our key contribution is a Contrastive Embedding Conditioning
(CEC) mechanism that leverages a contrastive encoder pretrained through
self-supervised contrastive learning to project visual and tactile inputs into
unified latent embeddings. These embeddings are used to couple visual-tactile
feature fusion through cross-modal attention, aiming at aligning the unified
representations and enhancing performance on downstream tasks. We conduct
extensive experiments to demonstrate the superiority of ConViTac in real world
over current state-of-the-art methods and the effectiveness of our proposed CEC
mechanism, which improves accuracy by up to 12.0% in material classification
and grasping prediction tasks.

</details>


### [18] [AI-Driven MRI-based Brain Tumour Segmentation Benchmarking](https://arxiv.org/abs/2506.20786)
*Connor Ludwig,Khashayar Namdar,Farzad Khalvati*

Main category: cs.CV

> 该研究评估了多种分割模型在BraTS 2023数据集上的表现，发现在给定高质量提示时，SAM和SAM 2的性能优于nnU-Net，但提供高准确度的提示不现实。

<details>
  <summary>Details</summary>

**Motivation:** 近年来，已经引入了许多通用的可提示模型和医疗领域的变化，但在共同的医疗数据集上跨各种提示质量对这些模型进行评估和比较存在不足。该研究旨在填补这一空白，并提供一些见解。

**Method:** 该研究使用了Segment Anything Model (SAM), Segment Anything Model 2 (SAM 2), MedSAM, SAM-Med-3D, 和 nnU-Net在BraTS 2023成人胶质瘤和儿科数据集上进行了零样本推理，评估了不同质量的提示（点和边界框）。此外，还对SAM、SAM 2、MedSAM和SAM-Med-3D进行了细调，以进一步提高儿科数据集上的点提示性能。

**Result:** 研究发现在给定高质量的边界框提示时，SAM和SAM 2在某些Dice系数上达到了0.894和0.893，超过了nnU-Net的分割性能。细化后，点提示的性能有了显著提升，但仍未超过边界框提示或nnU-Net的分割性能。

**Conclusion:** nnU-Net依然是领先的医学图像分割网络，因为提给其他模型提供精确的提示不切实际。尽管如此，一些模型如SAM和SAM 2在给定高度准确提示的情况下表现出色。

**Abstract:** Medical image segmentation has greatly aided medical diagnosis, with U-Net
based architectures and nnU-Net providing state-of-the-art performance. There
have been numerous general promptable models and medical variations introduced
in recent years, but there is currently a lack of evaluation and comparison of
these models across a variety of prompt qualities on a common medical dataset.
This research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM
2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS
2023 adult glioma and pediatrics dataset across multiple prompt qualities for
both points and bounding boxes. Several of these models exhibit promising Dice
scores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,
respectively when given extremely accurate bounding box prompts which exceeds
nnU-Net's segmentation performance. However, nnU-Net remains the dominant
medical image segmentation network due to the impracticality of providing
highly accurate prompts to the models. The model and prompt evaluation, as well
as the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and
SAM-Med-3D on the pediatrics dataset. The improvements in point prompt
performance after fine-tuning are substantial and show promise for future
investigation, but are unable to achieve better segmentation than bounding
boxes or nnU-Net.

</details>


### [19] [How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?](https://arxiv.org/abs/2506.20795)
*Stephanie Käs,Anton Burenko,Louis Markert,Onur Alp Culha,Dennis Mack,Timm Linder,Bastian Leibe*

Main category: cs.CV

> 研究探索VFMs和VLMs在全身体势识别中的应用，特别是在人机通信背景下。实验结果显示HD-GCN性能最佳，但V-JEPA作为一个较为通用的模型也表现良好，暗示了一种可能的系统简化途径。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于探索使用VFMs和VLMs简化大规模自动化生产环境中人机通信的复杂性，特别关注于基于通用模型而非特定任务模型的解决方案。

**Method:** Gesture识别技术在人机交互中的应用研究，特别是在嘈杂的生产环境中。传统的基于深度学习的gesture识别依赖于特定的任务架构，输入包括图像、视频或骨骼姿态估计。论文调查了Vision Foundation Models (VFMs)和Vision Language Models (VLMs)应用于动态全身体势识别的可行性，并对比了V-JEPA（一种先进的VFM）、Gemini Flash 2.0（一种多模态VLM）和HD-GCN（一种顶级的基于骨架的方法）。

**Result:** 实验结果表明HD-GCN表现出最佳性能，但是通过简单的任务特定分类头，V-JEPA表现接近，体现了减少系统复杂性的可能性。相反，Gemini在基于文本描述的zero-shot设置中难以区分手势，说明需要进一步研究适合手势识别的输入表示方式。

**Conclusion:** 研究结果提供了一种可能的途径，通过使用V-JEPA作为通用多任务模型来简化系统复杂性，但也指出对于基于文本描述的零样本手势识别需要更多的研究与改进。

**Abstract:** Gestures enable non-verbal human-robot communication, especially in noisy
environments like agile production. Traditional deep learning-based gesture
recognition relies on task-specific architectures using images, videos, or
skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)
and Vision Language Models (VLMs) with their strong generalization abilities
offer potential to reduce system complexity by replacing dedicated
task-specific modules. This study investigates adapting such models for
dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art
VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing
skeleton-based approach). We introduce NUGGET, a dataset tailored for
human-robot communication in intralogistics environments, to evaluate the
different gesture recognition approaches. In our experiments, HD-GCN achieves
best performance, but V-JEPA comes close with a simple, task-specific
classification head - thus paving a possible way towards reducing system
complexity, by using it as a shared multi-task model. In contrast, Gemini
struggles to differentiate gestures based solely on textual descriptions in the
zero-shot setting, highlighting the need of further research on suitable input
representations for gestures.

</details>


### [20] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

> 文章提出了一种结合视觉语言模型（VLMs）评估超分辨率图像的方法，通过提出新的信任度得分（TWS），解决了现有超分辨率模型存在的问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有超分辨率方法存在的问题，包括回归模型引入的伪影以及扩散模型中挑选最可信解决方案的挑战。

**Method:** 该论文提出了一种结合视觉语言模型（VLMs）评估超分辨率图像语义正确性、视觉质量和伪影存在的框架。特别是，通过BLIP-2、GPT-4o及其变体，使用结构化查询评估超分辨率图像。结合排名最高的超分辨率候选图像，以产生一个值得信赖的输出，同时保证成本效益。

**Result:** 提出了一种新的信任度得分（TWS），这是一个混合指标，可以基于语义相似度（通过CLIP嵌入）、结构完整性（通过边缘映射上的SSIM）和伪影敏感度（通过多层次小波分解）量化超分辨率的可靠性。在模糊图像和自然图像中，TWS与人类偏好高度相关。

**Conclusion:** 通过与人类期望和语义正确性对齐，本工作为生成性超分辨率设定了新的可信度基准，提供了一个原则化、可扩展且通用的方法来解决扩散SR空间的不确定性。

**Abstract:** Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [21] [FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization](https://arxiv.org/abs/2506.20841)
*Ha Min Son,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

> A new SSDG method, FixCLR, is introduced to improve domain generalization by explicitly regularizing for domain invariance using contrastive learning adapted with pseudo-labels and a repelling term.

<details>
  <summary>Details</summary>

**Motivation:** To improve the generalization of SSDG methods to out-of-distribution data, especially when only a few labels are available, and to achieve explicit domain invariance regularization which is lacking in current methods.

**Method:** Semi-supervised domain generalization (SSDG) aims to generalize to out-of-distribution data with scarce labels by combining semi-supervised learning and regularization, but fails to explicitly regularize for domain invariance. FixCLR addresses this by adapting contrastive learning with class information from pseudo-labels and using a repelling term only.

**Result:** FixCLR demonstrates complementary performance improvements when added to existing SSDG or semi-supervised methods and shows effectiveness through benchmarking different improvements, evaluating pretrained versus non-pretrained models, and testing on multi-domain datasets.

**Conclusion:** FixCLR effectively achieves explicit domain invariance regularization, proving to be an effective SSDG method, especially when combined with other semi-supervised methods.

**Abstract:** Semi-supervised domain generalization (SSDG) aims to solve the problem of
generalizing to out-of-distribution data when only a few labels are available.
Due to label scarcity, applying domain generalization methods often
underperform. Consequently, existing SSDG methods combine semi-supervised
learning methods with various regularization terms. However, these methods do
not explicitly regularize to learn domains invariant representations across all
domains, which is a key goal for domain generalization. To address this, we
introduce FixCLR. Inspired by success in self-supervised learning, we change
two crucial components to adapt contrastive learning for explicit domain
invariance regularization: utilization of class information from pseudo-labels
and using only a repelling term. FixCLR can also be added on top of most
existing SSDG and semi-supervised methods for complementary performance
improvements. Our research includes extensive experiments that have not been
previously explored in SSDG studies. These experiments include benchmarking
different improvements to semi-supervised methods, evaluating the performance
of pretrained versus non-pretrained models, and testing on datasets with many
domains. Overall, FixCLR proves to be an effective SSDG method, especially when
combined with other semi-supervised methods.

</details>


### [22] [Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision](https://arxiv.org/abs/2506.20850)
*Yuting He,Shuo Li*

Main category: cs.CV

> 向量对比学习框架（COVER）用于改善像素级自监督预训练，通过向量回归优化像素级特征相关性，实验结果表明该方法在多个任务中效果显著，推动了医学视觉基础模型的发展。

<details>
  <summary>Details</summary>

**Motivation:** 标准的对比学习应用于像素级表示时，因过度追求特征弥散性而导致像素间特征相关性的破坏，影响了类内分布，因此提出新的对比学习方法解决该问题。

**Method:** 向量对比学习（VECTOR CL）框架，将对比学习问题重新定义为向量回归问题，通过位移向量的回归来量化特征距离。具体的实现框架为COntrast in VEctor Regression (COVER)，具备可扩展的向量基础自我学习，并采用向量金字塔结构。

**Result:** 论文摘要主要探讨了对比学习（CL）在基础模型中自监督预训练（SSP）中的应用及其在像素级表示上的扩展问题。针对标准的二元对比学习可能导致像素级特征相关性的破坏，作者提出了一种向量对比学习的方法，即将对比学习重新定义为向量回归问题，通过回归位移向量来量化特征距离，进而保持像素级特征相关性。为此，作者提出了COntrast in VEctor Regression (COVER)框架，该框架通过可扩展的向量基础自我学习，确保从向量回归到距离模型的一致优化过程，并使用向量金字塔结构进行粒度调整。实验结果显示，COVER框架在多个维度和模态下的多项任务中，显著提高了像素级自监督预训练的表现，有助于发展具有泛化能力的医学视觉基础模型。

**Conclusion:** 通过实验验证，COVER框架显著提高了像素级自监督预训练的表现，为开发具有泛化能力的医学视觉基础模型提供了新的思路。

**Abstract:** Contrastive learning (CL) has become a cornerstone of self-supervised
pretraining (SSP) in foundation models, however, extending CL to pixel-wise
representation, crucial for medical vision, remains an open problem. Standard
CL formulates SSP as a binary optimization problem (binary CL) where the
excessive pursuit of feature dispersion leads to an over-dispersion problem,
breaking pixel-wise feature correlation thus disrupting the intra-class
distribution. Our vector CL reformulates CL as a vector regression problem,
enabling dispersion quantification in pixel-wise pretraining via modeling
feature distances in regressing displacement vectors. To implement this novel
paradigm, we propose the COntrast in VEctor Regression (COVER) framework. COVER
establishes an extendable vector-based self-learning, enforces a consistent
optimization flow from vector regression to distance modeling, and leverages a
vector pyramid architecture for granularity adaptation, thus preserving
pixel-wise feature correlations in SSP. Extensive experiments across 8 tasks,
spanning 2 dimensions and 4 modalities, show that COVER significantly improves
pixel-wise SSP, advancing generalizable medical visual foundation models.

</details>


### [23] [Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation](https://arxiv.org/abs/2506.20867)
*Ryosuke Kawamura,Hideaki Hayashi,Shunsuke Otake,Noriko Takemura,Hajime Nagahara*

Main category: cs.CV

> 研究提出了一种新的数据增强方法MIDAS，为模糊面部表情数据的动态面部表情识别提供了有效解决方案，实验结果表明，模型在MIDAS增强的数据上训练能获得比使用原始数据训练的先进方法更好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在解决实际应用中准确识别野外数据中经常遇到的模糊面部表情，这对于动态面部表情识别任务是至关重要的。

**Method:** MIDAS是一种数据增强方法，通过使用多个情感类别的概率软标签来增强模糊面部表情数据的动态面部表情识别（DFER）性能。MIDAS通过凸组合视频帧及其对应的情感类别标签来增强训练数据，这种方法扩展了mixup方法到软标签视频数据，是一种处理DFER中的模棱两可问题的简单而有效的方法。

**Result:** 实验在DFEW数据集以及一个新的FERV39k-Plus数据集（为现有DFER数据集分配软标签）上进行。结果表明，使用MIDAS增强的数据训练的模型性能优于使用原始数据训练的最先进方法。

**Conclusion:** 实验结果证明，使用MIDAS增强的数据进行训练的模型在DFER任务上达到了优越的性能，优于在原始数据上训练的最先进方法。

**Abstract:** Dynamic facial expression recognition (DFER) is a task that estimates
emotions from facial expression video sequences. For practical applications,
accurately recognizing ambiguous facial expressions -- frequently encountered
in in-the-wild data -- is essential. In this study, we propose MIDAS, a data
augmentation method designed to enhance DFER performance for ambiguous facial
expression data using soft labels representing probabilities of multiple
emotion classes. MIDAS augments training data by convexly combining pairs of
video frames and their corresponding emotion class labels. This approach
extends mixup to soft-labeled video data, offering a simple yet highly
effective method for handling ambiguity in DFER. To evaluate MIDAS, we
conducted experiments on both the DFEW dataset and FERV39k-Plus, a newly
constructed dataset that assigns soft labels to an existing DFER dataset. The
results demonstrate that models trained with MIDAS-augmented data achieve
superior performance compared to the state-of-the-art method trained on the
original dataset.

</details>


### [24] [THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion](https://arxiv.org/abs/2506.20877)
*Calin Teodor Ioan*

Main category: cs.CV

> ThirdEye提出了一种新的单目深度估计方法，明确地提供和结合单目线索，以提高深度估计的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的单目深度估计方法通常是隐式学习，忽视了人类视觉系统依赖的明确的单目线索。ThirdEye方法希望通过提供专门的线索网络来改进这一点。

**Method:** ThirdEye方法通过专门的、预训练的和冻结的网络来提供明确的单目线索，如遮挡边界、阴影和透视。这些线索在一个三阶段皮层层次（V1->V2->V3）中融合，并配备了一个根据可靠性加权这些线索的关键值工作记忆模块。最后，一个自适应bins的transformer头部生成高分辨率的视差图。

**Result:** 由于线索专家网络是冻结的，ThirdEye可以继承大量外部监督，只需适度微调。实验细节和效果将在未来修订中提供。

**Conclusion:** 通过集成明确的视觉线索，ThirdEye旨在提高单目深度估计性能，该方法利用了预先训练的专家网络，并在皮层层次结构中融合这些线索。

**Abstract:** Monocular depth estimation methods traditionally train deep models to infer
depth directly from RGB pixels. This implicit learning often overlooks explicit
monocular cues that the human visual system relies on, such as occlusion
boundaries, shading, and perspective. Rather than expecting a network to
discover these cues unaided, we present ThirdEye, a cue-aware pipeline that
deliberately supplies each cue through specialised, pre-trained, and frozen
networks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)
equipped with a key-value working-memory module that weights them by
reliability. An adaptive-bins transformer head then produces a high-resolution
disparity map. Because the cue experts are frozen, ThirdEye inherits large
amounts of external supervision while requiring only modest fine-tuning. This
extended version provides additional architectural detail, neuroscientific
motivation, and an expanded experimental protocol; quantitative results will
appear in a future revision.

</details>
