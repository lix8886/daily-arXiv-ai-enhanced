{"id": "2507.02074", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02074", "abs": "https://arxiv.org/abs/2507.02074", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "comment": null, "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "AI": {"tldr": "本文综述了利用大型语言模型（LLMs）和视觉-语言模型（VLMs）进行视频碰撞检测的方法，指出了该领域的挑战和机遇。", "motivation": "鉴于在智能交通系统中，从视频流中检测碰撞是一项关键任务，本文旨在综述利用大型语言模型（LLMs）进行视频数据碰撞检测的最新方法，以推动该领域的进一步发展。", "method": "本文通过构建融合策略的结构性分类，总结关键数据集，分析模型架构，比较性能基准来综述利用大型语言模型（LLMs）和视觉-语言模型（VLMs）进行视频碰撞检测的方法。", "result": "无具体结果描述，因为这是一篇综述性文章，主要提供了对当前研究的结构性分析和总结。", "conclusion": "本文为未来在视频理解与基础模型快速发展的交叉领域的研究提供了基础，指出了正在面临的挑战和机遇。"}}
{"id": "2507.02148", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02148", "abs": "https://arxiv.org/abs/2507.02148", "authors": ["Zijie Cai", "Christopher Metzler"], "title": "Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning", "comment": null, "summary": "Monocular depth estimation has recently advanced to provide not only relative\nbut also metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground-truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of\nstate-of-the-art models across a range of underwater conditions with different\nranges. Our results show that large-scale models trained on terrestrial (real\nor synthetic) data, while effective in in-air settings, perform poorly\nunderwater due to significant domain shifts. To address this, we fine-tune\nDepth Anything V2 with a ViT-S backbone encoder on a synthetic underwater\nvariant of the Hypersim dataset, which we generated using a physically based\nunderwater image formation model. We demonstrate our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. Our study provides\na detailed evaluation and visualization for monocular metric depth estimation\nin underwater scenes, highlighting the importance of domain adaptation and\nscale-aware supervision for achieving robust and generalizable metric depth\npredictions in challenging underwater environments for future research.", "AI": {"tldr": "本文对水下场景的单目度量深度估计进行了详细的评估和可视化，通过微调模型展示了领域自适应的重要性。", "motivation": "单目深度估计在水下环境中的可靠性受限，原因包括光衰减和散射、色彩失真、浑浊和缺乏高质量的度量真实数据。为了克服这些挑战，本研究对水下环境中的单目度量深度估计进行了详细的评估和可视化。", "method": "本文提出了一种在真实世界水下数据集（如FLSea和SQUID）上对零样本和微调的单目度量深度估计模型进行综合基准测试的方法。作者生成了Hypersim数据集的合成水下变体，并使用基于物理的水下图像形成模型对Depth Anything V2（ViT-S主干编码器）进行了微调。", "result": "研究结果显示，大规模模型在空中设置中有效，但在水下性能较差，因为存在显著的领域变化。然而，经过微调的模型在所有基准测试中都表现出色，并优于仅在干净的空中Hypersim数据集上训练的基线模型。", "conclusion": "本研究强调了领域自适应和尺度感知监督对于在具有挑战性的水下环境中实现鲁棒和通用的度量深度预测的重要性，并为未来的研究提供了指导。"}}
{"id": "2507.02200", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02200", "abs": "https://arxiv.org/abs/2507.02200", "authors": ["Xiao Wang", "Jingtao Jiang", "Qiang Chen", "Lan Chen", "Lin Zhu", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning", "comment": "A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition", "summary": "Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT.", "AI": {"tldr": "ESTR-CoT：一种基于链式推理的事件流场景文本识别框架，提供有效的文本识别、良好的解释性和逻辑推理能力，在挑战性场景下表现出色。", "motivation": "现有方法存在解释性不足和上下文逻辑推理能力较弱的问题，因此提出了一个新的框架来解决这些问题。", "method": "提出了基于链式推理的事件流场景文本识别框架ESTR-CoT，首先使用视觉编码器EVA-CLIP（ViT-G/14）将输入事件流转换为tokens，使用Llama tokenizer对给定的生成提示进行编码。通过Q-former将视觉token与预训练的大语言模型Vicuna-7B对齐，并同时输出答案和链式推理（CoT）过程。该框架可以通过监督微调进行端到端优化。此外，还提出了一个大规模的CoT数据集，通过三阶段处理（即生成、改进和专家验证）来训练框架。", "result": "在三个事件流STR基准数据集（即EventSTR、WordArt*、IC15*）上的广泛实验验证了所提出框架的有效性和解释性。", "conclusion": "ESTR-CoT框架在解决低光照、快速运动等极具挑战场景下的文本识别任务上表现出色，提供了解释性和逻辑推理能力，同时提出的数据集为后续基于推理的大模型的发展提供了坚实的数据基础。"}}
{"id": "2507.02205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02205", "abs": "https://arxiv.org/abs/2507.02205", "authors": ["Elena Ryumina", "Maxim Markitantov", "Alexandr Axyonov", "Dmitry Ryumin", "Mikhail Dolgushin", "Alexey Karpov"], "title": "Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach", "comment": "8", "summary": "Compound Expression Recognition (CER), a subfield of affective computing,\naims to detect complex emotional states formed by combinations of basic\nemotions. In this work, we present a novel zero-shot multimodal approach for\nCER that combines six heterogeneous modalities into a single pipeline: static\nand dynamic facial expressions, scene and label matching, scene context, audio,\nand text. Unlike previous approaches relying on task-specific training data,\nour approach uses zero-shot components, including Contrastive Language-Image\nPretraining (CLIP)-based label matching and Qwen-VL for semantic scene\nunderstanding. We further introduce a Multi-Head Probability Fusion (MHPF)\nmodule that dynamically weights modality-specific predictions, followed by a\nCompound Expressions (CE) transformation module that uses Pair-Wise Probability\nAggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods\nto produce interpretable compound emotion outputs. Evaluated under multi-corpus\ntraining, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%\non Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via\nzero-shot testing, which is comparable to the results of supervised approaches\ntrained on target data. This demonstrates the effectiveness of the proposed\napproach for capturing CE without domain adaptation. The source code is\npublicly available.", "AI": {"tldr": "研究提出了一种零样本多模态方法，用于复合表情识别，通过结合六种不同模态和零样本组件，该方法在多个公开数据集上表现出与监督学习相当的性能。", "motivation": "复合表情识别是情感计算的一个重要子领域，旨在检测由多种基础情绪组合而成的复杂情感状态。本方法的主要动机在于通过零样本学习技术，无需任务特定的训练数据，即可准确识别复合表情。", "method": "本研究提出了一种新颖的零样本多模态方法来处理复合表情识别（CER），结合了六种异构模态：静态和动态面部表情、场景与标签匹配、场景上下文、音频和文本。该方法使用了零样本组件，包括基于CLIP的标签匹配和Qwen-VL进行语义场景理解。同时还引入了多头概率融合（MHPF）模块，用于动态加权各个模态的预测结果，随后是复合情绪转换模块，利用成对概率聚合（PPA）和成对特征相似性聚合（PFSA）方法生成可解释的复合情绪输出。", "result": "该方法在AffWild2、AFEW和C-EXPR-DB三个数据集上的零样本测试中分别取得了46.95%、49.02%和34.85%的F1分数，测试结果与针对目标数据进行监督学习训练的方法相当。这表明该方法在无需领域适应的情况下，能有效地捕捉到复合情绪。", "conclusion": "本研究展示了一种有效的零样本多模态方法，无需特定训练数据即可识别复合情感状态，并且无需领域适应的情况下也能取得良好的性能，这为复合表情识别领域提供了一种新的研究方向。代码也是公开可用的。"}}
{"id": "2507.02088", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02088", "abs": "https://arxiv.org/abs/2507.02088", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "comment": "24 pages, 9 figures", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.", "AI": {"tldr": "研究者提出了一个多任务中文偏见评估基准（McBE）来评估大型语言模型中的偏见，覆盖多类别和多种评估任务，对多个流行LLMs进行了评估，并分析了结果。", "motivation": "由于大型语言模型(LLMs)逐渐应用于各种自然语言处理(NLP)任务，其内在偏见逐渐显现出来。现有的偏见评估数据集主要针对英语和北美文化，对于其他文化并不完全适用，且缺乏中文和中国文化背景的数据集。", "method": "为了应对现有偏见评估数据集的局限性，研究者提出了一个多任务中文偏见评估基准 (McBE)，其中包括4,077个偏见评估实例，覆盖了12个单一偏见类别和82个子类别，并引入了5个评估任务。", "result": "研究者评估了不同系列和参数大小的流行LLMs，发现这些模型表现出了不同程度的偏见，并深入分析了结果，提供了关于LLMs偏见的新见解。", "conclusion": "研究结果显示，多任务中文偏见评估基准可以全面测量多种偏见，对于理解LLM们的偏见有重要的贡献。"}}
{"id": "2507.02212", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02212", "abs": "https://arxiv.org/abs/2507.02212", "authors": ["Takuro Kawada", "Shunsuke Kitada", "Sota Nemoto", "Hitoshi Iyatomi"], "title": "SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers", "comment": "21 pages, 15 figures, 4 tables. Project Page:\n  https://iyatomilab.github.io/SciGA/", "summary": "Graphical Abstracts (GAs) play a crucial role in visually conveying the key\nfindings of scientific papers. While recent research has increasingly\nincorporated visual materials such as Figure 1 as de facto GAs, their potential\nto enhance scientific communication remains largely unexplored. Moreover,\ndesigning effective GAs requires advanced visualization skills, creating a\nbarrier to their widespread adoption. To tackle these challenges, we introduce\nSciGA-145k, a large-scale dataset comprising approximately 145,000 scientific\npapers and 1.14 million figures, explicitly designed for supporting GA\nselection and recommendation as well as facilitating research in automated GA\ngeneration. As a preliminary step toward GA design support, we define two\ntasks: 1) Intra-GA recommendation, which identifies figures within a given\npaper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,\nwhich retrieves GAs from other papers to inspire the creation of new GAs. We\nprovide reasonable baseline models for these tasks. Furthermore, we propose\nConfidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation\nmetric that offers a fine-grained analysis of model behavior. CAR addresses\nlimitations in traditional ranking-based metrics by considering cases where\nmultiple figures within a paper, beyond the explicitly labeled GA, may also\nserve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a\nfoundation for advancing visual scientific communication while contributing to\nthe development of AI for Science.", "AI": {"tldr": "研究提出了SciGA-145k数据集，包含大约145,000篇科学论文和1.14百万张图，旨在支持科学图解(GA)的选择、推荐和自动生成的研究。还定义了GA设计支持的两个任务，并提出了一种新的推荐评估指标CAR。", "motivation": "为了克服设计有效科学图解所需的专业可视化技能障碍，并探索其在增强科学研究传播中的潜力。", "method": "构建了一个大型数据集SciGA-145k，并定义了图解选择和推荐的两个任务。同时也设计了恰当的基线模型和新的推荐评估指标CAR。", "result": "提出了合理基线模型以及新的评估指标CAR，为科学图解的设计提供初步支持。", "conclusion": "SciGA-145k数据集为推动视觉化科学交流以及科学领域AI的发展奠定了基础。"}}
{"id": "2507.02145", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02145", "abs": "https://arxiv.org/abs/2507.02145", "authors": ["Keyan Jin", "Yapeng Wang", "Leonel Santos", "Tao Fang", "Xu Yang", "Sio Kei Im", "Hugo Gonçalo Oliveira"], "title": "Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization", "comment": null, "summary": "Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization.", "AI": {"tldr": "该研究首次系统评估了解决方案中的逐步推理大型语言模型以及非推理大型语言模型在对话摘要任务中的表现，结果显示推理模型并不总是能改善对话摘要质量，反而可能出现冗长、不一致等问题。", "motivation": "研究动机在于探索与评估在需要同时具备抽象能力与简洁性的对话场景中，基于逐步推理架构的语言模型表现。", "method": "研究涵盖了三种主要的对话摘要范式——通用型、角色导向型和查询导向型，并应用了多个强有力的基准测试和先进的评估协议，包括基于大型语言模型的自动指标和以人为灵感的标准。", "result": "研究表明，与非推理大型语言模型相比，推理大型语言模型往往会导致摘要冗长、事实不一致，并且不太简洁。", "conclusion": "研究提出了对于现实世界对话摘要任务中推理大型语言模型的局限性，并强调了针对该任务需要采取目标化的模型构建和评价策略。"}}
{"id": "2507.02217", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02217", "abs": "https://arxiv.org/abs/2507.02217", "authors": ["Brandon Trabucco", "Qasim Wani", "Benjamin Pikus", "Vasu Sharma"], "title": "Understanding Trade offs When Conditioning Synthetic Data", "comment": null, "summary": "Learning robust object detectors from only a handful of images is a critical\nchallenge in industrial vision systems, where collecting high quality training\ndata can take months. Synthetic data has emerged as a key solution for data\nefficient visual inspection and pick and place robotics. Current pipelines rely\non 3D engines such as Blender or Unreal, which offer fine control but still\nrequire weeks to render a small dataset, and the resulting images often suffer\nfrom a large gap between simulation and reality. Diffusion models promise a\nstep change because they can generate high quality images in minutes, yet\nprecise control, especially in low data regimes, remains difficult. Although\nmany adapters now extend diffusion beyond plain text prompts, the effect of\ndifferent conditioning schemes on synthetic data quality is poorly understood.\nWe study eighty diverse visual concepts drawn from four standard object\ndetection benchmarks and compare two conditioning strategies: prompt based and\nlayout based. When the set of conditioning cues is narrow, prompt conditioning\nyields higher quality synthetic data; as diversity grows, layout conditioning\nbecomes superior. When layout cues match the full training distribution,\nsynthetic data raises mean average precision by an average of thirty four\npercent and by as much as one hundred seventy seven percent compared with using\nreal data alone.", "AI": {"tldr": "研究比较了两种条件控制策略对合成数据质量的影响，结果表明合成数据可以显著提高物体检测器的性能，特别是在布局控制与训练数据分布相匹配的情况下。", "motivation": "研究动机在于解决工业视觉系统中从少量图像中学习强大对象检测器的难题，并探索扩散模型生成高质量图像的能力及其不同条件控制方案对合成数据质量的影响。", "method": "本研究通过对比两种条件控制策略（基于提示的和基于布局的）对合成数据质量的影响，来检验合成数据在仅有少量图像的情况下提高对象检测器鲁棒性的效果。", "result": "研究表明，在条件控制提示较窄的情况下，基于提示的条件控制生成的合成数据质量更高；随着多样性的增长，基于布局的条件控制成为更优选择。在布局暗示匹配完整训练分布时，合成数据可以将平均精度提高34%，在某些情况下可提高177%。", "conclusion": "结论为：基于合成数据中适当控制方案的应用可以显著提升物体检测器的鲁棒性和性能，特别适合于训练数据收集困难的情境。"}}
{"id": "2507.02199", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02199", "abs": "https://arxiv.org/abs/2507.02199", "authors": ["Wenquan Lu", "Yuechuan Yang", "Kyle Lee", "Yanshu Li", "Enqi Liu"], "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot.", "AI": {"tldr": "本文通过一套探测技术研究了深度递归Transformer Huginn-3.5B的内部推理行为，发现其内部存在有限的可解释潜在链式思维证据，递归深度增加的效果不大。", "motivation": "本文旨在探讨在推理时重复使用层而不增加参数数量的深度递归Transformer中是否能出现这样的推理结构。", "method": "我们使用了一套探测技术，包括Logit Lens和Coda Lens，来研究Huginn-3.5B在算术任务上的内部行为。", "result": "研究结果发现，通过追踪终结果和中间结果标记的排名轨迹，对可解释的潜在链式思维只存在有限的证据，并且在递归块之间的探测结果存在显著分歧。另外，递归深度的增加仅带来了微小的改进。", "conclusion": "最终，实验表明，增加递归深度的效果远远不如显式地外部化推理步骤的模型。"}}
{"id": "2507.02222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02222", "abs": "https://arxiv.org/abs/2507.02222", "authors": ["Tian Gao", "Zhiyuan Zhang", "Kaijie Yin", "Xu-Cheng Zhong", "Hui Kong"], "title": "High-Fidelity Differential-information Driven Binary Vision Transformer", "comment": null, "summary": "The binarization of vision transformers (ViTs) offers a promising approach to\naddressing the trade-off between high computational/storage demands and the\nconstraints of edge-device deployment. However, existing binary ViT methods\noften suffer from severe performance degradation or rely heavily on\nfull-precision modules. To address these issues, we propose DIDB-ViT, a novel\nbinary ViT that is highly informative while maintaining the original ViT\narchitecture and computational efficiency. Specifically, we design an\ninformative attention module incorporating differential information to mitigate\ninformation loss caused by binarization and enhance high-frequency retention.\nTo preserve the fidelity of the similarity calculations between binary Q and K\ntensors, we apply frequency decomposition using the discrete Haar wavelet and\nintegrate similarities across different frequencies. Additionally, we introduce\nan improved RPReLU activation function to restructure the activation\ndistribution, expanding the model's representational capacity. Experimental\nresults demonstrate that our DIDB-ViT significantly outperforms\nstate-of-the-art network quantization methods in multiple ViT architectures,\nachieving superior image classification and segmentation performance.", "AI": {"tldr": "DIDB-ViT 提出了一种新的二值视觉变换器，通过设计信息丰富的注意力模块和改进的RPReLU激活函数来减少信息损失并扩展模型的表示能力，实验结果显示其在多种视觉变换器架构中表现优于现有的网络量化方法。", "motivation": "文章动机在于解决现有二值视觉变换器方法中存在的性能退化问题或对全精度模块的高度依赖，旨在创建一个计算效率高且信息丰富的二值视觉变换器。", "method": "Structure", "result": "{\n  \"tldr\": \"DIDB-ViT 提出了一种新的二值视觉变换器，通过设计信息丰富的注意力模块和改进的RPReLU激活函数来减少信息损失并扩展模型的表示能力，实验结果显示其在多种视觉变换器架构中表现优于现有的网络量化方法。\",\n  \"motivation\": \"文章动机在于解决现有二值视觉变换器方法中存在的性能退化问题或对全精度模块的高度依赖，旨在创建一个计算效率高且信息丰富的二值视觉变换器。\",\n  \"method\": \"方法包括设计一种包含差异信息的信息丰富注意力模块、使用离散哈默尔波分解来保持二值Q和K张量之间相似性的保真度以及引入改进的RPReLU激活函数以改进激活函数分布。\",\n  \"result\": \"实验结果表明，DIDB-ViT在多种视觉变换器架构中显著优于现有网络量化方法，特别是在图像分类和分割任务中。\",\n  \"conclusion\": \"结论是DIDB-ViT通过其创新设计和改进激活函数在二值网络中实现了更高的信息保留和模型表现，是该领域的一个重要进展。\",\n  \"paper_content\": \"The binarization of vision transformers (ViTs) offers a promising approach to addressing the trade-off between high computational/storage demands and the constraints of edge-device deployment. However, existing binary ViT methods often suffer from severe performance degradation or rely heavily on full-precision modules. To address these issues, we propose DIDB-ViT, a novel binary ViT that is highly informative while maintaining the original ViT architecture and computational efficiency. Specifically, we design an informative attention module incorporating differential information to mitigate information loss caused by binarization and enhance high-frequency retention. To preserve the fidelity of the similarity calculations between binary Q and K tensors, we apply frequency decomposition using the discrete Haar wavelet and integrate similarities across different frequencies. Additionally, we introduce an improved RPReLU activation function to restructure the activation distribution, expanding the model's representational capacity. Experimental results demonstrate that our DIDB-ViT significantly outperforms state-of-the-art network quantization methods in multiple ViT architectures, achieving superior image classification and segmentation performance. \"\n}", "conclusion": "结论是DIDB-ViT通过其创新设计和改进激活函数在二值网络中实现了更高的信息保留和模型表现，是该领域的一个重要进展。"}}
{"id": "2507.02221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02221", "abs": "https://arxiv.org/abs/2507.02221", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "comment": "11 pages, 1 figure, 7 tables", "summary": "Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds.", "AI": {"tldr": "介绍了一种名为GDC Cohort Copilot的开源工具，其功能是将用户自然语言描述的队列需求转化为GDC中可使用的队列筛选条件，且表现优于其他语言模型。", "motivation": "Genomic Data Commons (GDC) 提供高质量的癌症基因组数据。尽管用户可以通过图形化Cohort Builder创建复杂的队列，但用户（尤其是新用户）可能难以在众多可能的领域和属性中找到具体的队列描述符。然而，用户更可能通过自由文本自然语言来描述他们所需的队列。", "method": "开发并评估了多个大型语言模型（LLMs）来支持GDC Cohort Copilot。其中，一个本地服务的开源GDC Cohort LLM在生成GDC队列方面表现优于GPT-4o。", "result": "GDC Cohort Copilot是一个开源的协同工具，用于根据用户输入的自然语言描述自动生成GDC队列筛选条件，之后将队列导出到GDC进行进一步分析。交互式用户界面允许用户调整生成的队列。研究展示了本地服务的开源GDC Cohort LLM比GPT-4o在生成GDC队列方面表现更好。", "conclusion": "GDC Cohort Copilot提供了一个开源的解决方案，能自动生成GDC队列筛选条件，并允许用户通过交互式界面进一步调整队列。该工具能显著提高GDC用户的队列构建效率和准确性。"}}
{"id": "2507.02250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02250", "abs": "https://arxiv.org/abs/2507.02250", "authors": ["Jiangxia Chen", "Tongyuan Huang", "Ke Song"], "title": "FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model", "comment": null, "summary": "3D semantic occupancy prediction plays a pivotal role in autonomous driving.\nHowever, inherent limitations of fewframe images and redundancy in 3D space\ncompromise prediction accuracy for occluded and distant scenes. Existing\nmethods enhance performance by fusing historical frame data, which need\nadditional data and significant computational resources. To address these\nissues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement\noccupancy network with flow matching selective state space model for few-frame\n3D occupancy prediction. Firstly, to generate missing features, we designed a\nfeature refinement module based on a flow matching model, which is called Flow\nMatching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and\nPlane Selective SSM (PS3M), we selectively filter TPV features to reduce the\nimpact of air voxels on non-air voxels, thereby enhancing the overall\nefficiency of the model and prediction capability for distant scenes. Finally,\nwe design the Mask Training (MT) method to enhance the robustness of FMOcc and\naddress the issue of sensor data loss. Experimental results on the\nOcc3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing\nstate-of-theart methods. Our FMOcc with two frame input achieves notable scores\nof 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on\nOpenOcc with 5.4 G inference memory and 330ms inference time.", "AI": {"tldr": "提出FMOcc网络，通过流匹配选择状态空间模型解决了依赖多帧图像进行3D场景占用预测的计算复杂性和数据需求问题，提高了远处场景的预测精度和模型效率。", "motivation": "现有方法依赖多帧数据提高3D占用预测的准确性，需要更多的计算资源和数据，而本文旨在解决这些问题，提出一种适用于少量帧的3D占用预测方法。", "method": "设计了基于流匹配模型的特征精炼模块FMSSM，以及TPV SSM层和平面选择性SSM（PS3M），减少空气体素对非空气体素的影响，以提高模型效率和远处场景的预测能力，并设计了掩码训练法（MT）增强模型鲁棒性。", "result": "实验结果表明，FMOcc在Occ3D-nuScenes和OpenOcc数据集上超越了现有方法，使用两帧输入在Occ3D-nuScenes验证上获得了43.1%的RayIoU和39.8%mIoU，而在OpenOcc上获得了42.6%的RayIoU，推理内存为5.4G，推理时间为330ms。", "conclusion": "提出的FMOcc网络提高了少帧3D占用预测的精度和效率，同时不需要额外的数据和极大的计算资源。"}}
{"id": "2507.02259", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02259", "abs": "https://arxiv.org/abs/2507.02259", "authors": ["Hongli Yu", "Tinghong Chen", "Jiangtao Feng", "Jiangjie Chen", "Weinan Dai", "Qiying Yu", "Ya-Qin Zhang", "Wei-Ying Ma", "Jingjing Liu", "Mingxuan Wang", "Hao Zhou"], "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent", "comment": "Project Page: https://memagent-sialab.github.io/", "summary": "Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.", "AI": {"tldr": "本文提出MemAgent，一种新型代理工作流，通过分段阅读和覆盖策略更新内存来优化长文本任务，展示了在长上下文处理中的卓越能力。", "motivation": "尽管长度外推、高效注意力和内存模块有所改进，但在处理长文本时，仍然面临在时间复杂度线性的情况下处理无限长文档而不降低性能的关键挑战。我们的动机在于解决这一挑战。", "method": "我们介绍了一种新型的代理工作流MemAgent，该方法通过分段阅读文本并使用覆盖策略更新记忆来处理长文本任务。我们扩展了DAPO算法，通过独立上下文的多会话生成来促进训练。", "result": "MemAgent在长上下文能力方面表现出色，从基于32K文本训练的8K上下文外推到3.5M QA任务时，性能下降小于5%，在512K RULER测试中达到了95%以上的准确率。", "conclusion": "研究结果表明，MemAgent能够高效地处理长文本任务，并在不同长度的测试中表现出色，证明了其在长文本处理中的潜力。"}}
{"id": "2507.02252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02252", "abs": "https://arxiv.org/abs/2507.02252", "authors": ["Zeyu Lei", "Hongyuan Yu", "Jinlin Wu", "Zhen Chen"], "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement", "comment": null, "summary": "Precise surgical interventions are vital to patient safety, and advanced\nenhancement algorithms have been developed to assist surgeons in\ndecision-making. Despite significant progress, these algorithms are typically\ndesigned for single tasks in specific scenarios, limiting their effectiveness\nin complex real-world situations. To address this limitation, we propose\nSurgVisAgent, an end-to-end intelligent surgical vision agent built on\nmultimodal large language models (MLLMs). SurgVisAgent dynamically identifies\ndistortion categories and severity levels in endoscopic images, enabling it to\nperform a variety of enhancement tasks such as low-light enhancement,\noverexposure correction, motion blur elimination, and smoke removal.\nSpecifically, to achieve superior surgical scenario understanding, we design a\nprior model that provides domain-specific knowledge. Additionally, through\nin-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent\ndelivers customized image enhancements tailored to a wide range of distortion\ntypes and severity levels, thereby addressing the diverse requirements of\nsurgeons. Furthermore, we construct a comprehensive benchmark simulating\nreal-world surgical distortions, on which extensive experiments demonstrate\nthat SurgVisAgent surpasses traditional single-task models, highlighting its\npotential as a unified solution for surgical assistance.", "AI": {"tldr": "SurgVisAgent 是一个基于多模态大型语言模型的智能手术视觉代理，可以动态识别内窥镜图像中的失真类别和严重程度，并执行多种图像增强任务，如低光增强、过曝校正、运动模糊消除和烟雾去除，显示出了优于传统单任务模型的潜力。", "motivation": "尽管医疗图像增强算法取得了显著进展，但它们通常仅针对特定场景中的单个任务设计，限制了其在复杂现实情况下的有效性。为了克服这一局限性，开发了SurgVisAgent。", "method": "Structure", "result": "{\n  \"tldr\": \"SurgVisAgent 是一个基于多模态大型语言模型的智能手术视觉代理，可以动态识别内窥镜图像中的失真类别和严重程度，并执行多种图像增强任务，如低光增强、过曝校正、运动模糊消除和烟雾去除，显示出了优于传统单任务模型的潜力。\",\n  \"motivation\": \"尽管医疗图像增强算法取得了显著进展，但它们通常仅针对特定场景中的单个任务设计，限制了其在复杂现实情况下的有效性。为了克服这一局限性，开发了SurgVisAgent。\",\n  \"method\": \"通过设计提供领域特定知识的prior模型，并采用上下文中的少样学习和链式推理(CoT)，SurgVisAgent能够适应广泛失真类型和严重程度的定制图像增强。\",\n  \"result\": \"大量实验表明SurgVisAgent超越了传统单任务模型，在一个综合基准上模拟了真实的手术失真情况。\",\n  \"conclusion\": \"SurgVisAgent展示出作为手术辅助统一解决方案的潜力。\n}\n", "conclusion": "SurgVisAgent展示出作为手术辅助统一解决方案的潜力。"}}
{"id": "2507.02302", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02302", "abs": "https://arxiv.org/abs/2507.02302", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "comment": "22 pages, 5 figures, ACL 2025 Main", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX.", "AI": {"tldr": "本论文提出了DoMIX方法，利用LoRA模块解决了持续领域自适应预训练中效率低、敏感于数据顺序及无法提供特定任务定制模型的问题，实现高效并行的自适应预训练，适用于具体任务且展示出在标准LLM微调场景中的应用潜力。", "motivation": "尽管持续领域自适应预训练在发展预训练模型以增量地融合不同领域的数据集方面已经展现潜力，但现有方法有高计算成本和GPU使用量高、对增量数据顺序敏感以及无法为所有终任务提供定制模型等局限。本论文旨在克服这些局限。", "method": "本论文提出了DoMIX方法，利用参数高效的微调（PEFT）方法中的LoRA模块来应对持续领域自适应预训练中的挑战。此方法能够实现高效且并行的领域自适应预训练，并且能够有效利用累积的知识，针对特定任务提供定制化的预训练模型。", "result": "DoMIX方法证实了其在标准LLM微调场景之外的适用性，并展示了其在持续领域自适应预训练背景下解决现有方法局限性的有效性。", "conclusion": "提出的方法展示了在持续领域自适应预训练背景下解决现有方法局限性的有效性，并且具有在标准LLM微调场景中的应用潜力。"}}
{"id": "2507.02265", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02265", "abs": "https://arxiv.org/abs/2507.02265", "authors": ["Zhangding Liu", "Neda Mohammadi", "John E. Taylor"], "title": "Multi-Label Classification Framework for Hurricane Damage Assessment", "comment": "9 pages, 3 figures. Accepted at the ASCE International Conference on\n  Computing in Civil Engineering (i3CE 2025)", "summary": "Hurricanes cause widespread destruction, resulting in diverse damage types\nand severities that require timely and accurate assessment for effective\ndisaster response. While traditional single-label classification methods fall\nshort of capturing the complexity of post-hurricane damage, this study\nintroduces a novel multi-label classification framework for assessing damage\nusing aerial imagery. The proposed approach integrates a feature extraction\nmodule based on ResNet and a class-specific attention mechanism to identify\nmultiple damage types within a single image. Using the Rescuenet dataset from\nHurricane Michael, the proposed method achieves a mean average precision of\n90.23%, outperforming existing baseline methods. This framework enhances\npost-hurricane damage assessment, enabling more targeted and efficient disaster\nresponse and contributing to future strategies for disaster mitigation and\nresilience. This paper has been accepted at the ASCE International Conference\non Computing in Civil Engineering (i3CE 2025), and the camera-ready version\nwill appear in the official conference proceedings.", "AI": {"tldr": "该研究提出了一种基于ResNet和特定类别注意力机制的多标签分类框架，用于评估飓风后的建筑物损伤，通过Rescuenet数据集从飓风迈克尔事件中验证，达到了90.23%的平均精度，有效提升灾害响应速度和精准度。", "motivation": "传统单标签分类方法难以准确捕捉飓风造成的复杂损伤类型，因此研究采用了多标签分类框架来改善损伤评估的准确性和效率。", "method": "框架结合了基于ResNet的特征提取模块和类别特定的注意力机制来识别单张图像中的多种损伤类型。", "result": "在使用Rescuenet数据集实验的基础上，提出的方法相较现有基线方法达到了90.23%的平均精确度，表现出色。", "conclusion": "该多标签框架增强了飓风后损伤评估的质量，能够支持更针对性和高效的灾害响应，并对未来灾害减缓策略提供帮助。"}}
{"id": "2507.02357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02357", "abs": "https://arxiv.org/abs/2507.02357", "authors": ["Christian Jaumann", "Annemarie Friedrich", "Rainer Lienhart"], "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models", "comment": "Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.", "AI": {"tldr": "本文介绍了一个在科学视觉问答任务中表现出色的系统，该系统使用了两种多模态大型语言模型与多个少样本策略相结合的方法。", "motivation": "本文描述了我们为SciVQA 2025共享任务——科学视觉问答任务开发的系统。", "method": "本研究使用了两个多模态大型语言模型的集成，并结合了多种少样本示例检索策略。模型和少样本设置的选择取决于图形和问题类型。", "result": "该系统在盲测数据上的表现优异，整体排名第三，且在多个评估指标上达到了85.12的平均F1分数。", "conclusion": "在盲测数据上，该系统在七个系统中排名第三，ROUGE-1、ROUGE-L和BERTS的平均F1分数为85.12。代码已公开。"}}
{"id": "2507.02268", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.02268", "abs": "https://arxiv.org/abs/2507.02268", "authors": ["Yuxiang Zhang", "Wei Li", "Wen Jia", "Mengmeng Zhang", "Ran Tao", "Shunlin Liang"], "title": "Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation", "comment": null, "summary": "Utilizing hyperspectral remote sensing technology enables the extraction of\nfine-grained land cover classes. Typically, satellite or airborne images used\nfor training and testing are acquired from different regions or times, where\nthe same class has significant spectral shifts in different scenes. In this\npaper, we propose a Bi-directional Domain Adaptation (BiDA) framework for\ncross-domain hyperspectral image (HSI) classification, which focuses on\nextracting both domain-invariant features and domain-specific information in\nthe independent adaptive space, thereby enhancing the adaptability and\nseparability to the target scene. In the proposed BiDA, a triple-branch\ntransformer architecture (the source branch, target branch, and coupled branch)\nwith semantic tokenizer is designed as the backbone. Specifically, the source\nbranch and target branch independently learn the adaptive space of source and\ntarget domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is\ndeveloped in coupled branch for feature interaction and inter-domain\ncorrelation mining. Furthermore, a bi-directional distillation loss is designed\nto guide adaptive space learning using inter-domain correlation. Finally, we\npropose an Adaptive Reinforcement Strategy (ARS) to encourage the model to\nfocus on specific generalized feature extraction within both source and target\nscenes in noise condition. Experimental results on cross-temporal/scene\nairborne and satellite datasets demonstrate that the proposed BiDA performs\nsignificantly better than some state-of-the-art domain adaptation approaches.\nIn the cross-temporal tree species classification task, the proposed BiDA is\nmore than 3\\%$\\sim$5\\% higher than the most advanced method. The codes will be\navailable from the website:\nhttps://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.", "AI": {"tldr": "本论文提出了一种新的BiDA框架，通过独立适应空间学习和跨域相关性挖掘，解决了高光谱图像在跨域条件下的分类问题，实验表明其性能优于现有方法。", "motivation": "利用高光谱遥感技术可提取精细地表覆盖分类，由于训练和测试数据通常来自不同的区域或时间，同一类别的光谱在不同场景下有显著变化，本研究旨在解决这一跨域分类问题。", "method": "本论文提出了一种双向领域适应（BiDA）框架，用于跨域高光谱图像分类，该框架通过在独立适应空间中提取领域不变特征和领域特定信息，提升了目标场景下的适应性和可分离性。BiDA包括一个三分支变压器架构（源分支、目标分支和耦合分支），采用语义分词器作为骨架结构。其中，源分支和目标分支分别独立学习源域和目标域的适应空间；耦合分支中开发了一种耦合多头交叉注意力（CMCA）机制，用于特征交互和跨域相关性挖掘。此外，设计了一种双向蒸馏损失以利用域间相关性指导适应空间学习。为进一步优化在噪声条件下的泛化特征提取，提出了一种自适应强化策略（ARS）。", "result": "实验结果表明，该论文提出的BiDA框架在跨时域/场景的机载和卫星数据集上的分类性能显著优于一些最先进的领域适应方法。在跨时域树种分类任务中，BiDA优于最先进的方法，性能提高了约3%~5%。", "conclusion": "该论文提出了一种新的双向领域适应框架（BiDA），通过开发三分支变换架构、耦合多头交叉注意力机制以及自适应强化策略等，提升了高光谱图像在跨域条件下的分类性能。其在跨时域和跨场景的实验中均展现出了优越的性能。"}}
{"id": "2507.02364", "categories": ["cs.CL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.02364", "abs": "https://arxiv.org/abs/2507.02364", "authors": ["Pilsung Kang"], "title": "QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers", "comment": null, "summary": "Parameterized quantum circuits (PQCs) have recently emerged as promising\ncomponents for enhancing the expressibility of neural architectures. In this\nwork, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the\nfeedforward network (FFN) modules of a compact BERT variant are replaced by\nPQC-based layers. This design is motivated by the dominant parameter\ncontribution of FFNs, which account for approximately two-thirds of the\nparameters within standard Transformer encoder blocks. While prior studies have\nprimarily integrated PQCs into self-attention modules, our work focuses on the\nFFN and systematically investigates the trade-offs between PQC depth,\nexpressibility, and trainability. Our final PQC architecture incorporates a\nresidual connection, both $R_Y$ and $R_Z$ rotations, and an alternating\nentanglement strategy to ensure stable training and high expressibility. Our\nexperiments, conducted on a classical simulator, on the SST-2 and DBpedia\nbenchmarks demonstrate two key findings. First, a carefully configured\nQFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its\nclassical counterpart in a full-data setting while reducing FFN-specific\nparameters by over 99%. Second, our model exhibits a consistent and competitive\nedge in few-shot learning scenarios, confirming its potential for superior data\nefficiency. These results, supported by an ablation study on a non-optimized\nPQC that failed to learn, confirm that PQCs can serve as powerful and\nparameter-efficient alternatives to classical FFNs when co-designed with\nfoundational deep learning principles.", "AI": {"tldr": "本研究展示了QFFN-BERT——一种将FFN模块替换为PQC层的混合量子经典Transformer，在参数高效性和数据效率方面都有不错的性能。", "motivation": "由于FFN参数在标准Transformer编码器模块中占比达到约三分之二，本研究寻求通过替换这些模块中的FFN为PQC层来改善神经网络结构的表现。", "method": "本研究将参数化量子电路（PQC）整合到BERT变种的前馈神经网络（FFN）模块中，形成QFFN-BERT。重点在于探究PQC深度、表现力和可训练性之间的权衡。最终的PQC架构包括残差连接、$R_Y$和$R_Z$旋转，以及交替的纠缠策略。", "result": "实验在SST-2和DBpedia基准测试上进行，结果显示QFFN-BERT在全数据场景中达到基线精度的102.0%，并将FFN相关的参数减少了超过99%。此外，该模型在少量样本学习情景中表现优异，展示出更高的数据效率。", "conclusion": "实验结果及非优化PQC组件无法学习的消融研究结果，证实了当PQC与基础深度学习原则共同设计时，它们可以成为强大的、参数高效的经典FFN替代品。"}}
{"id": "2507.02270", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02270", "abs": "https://arxiv.org/abs/2507.02270", "authors": ["Fanghai Yi", "Zehong Zheng", "Zexiao Liang", "Yihang Dong", "Xiyang Fang", "Wangyu Wu", "Xuhang Chen"], "title": "MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement", "comment": "Accepted by IEEE SMC 2025", "summary": "Enhancing underwater images is crucial for exploration. These images face\nvisibility and color issues due to light changes, water turbidity, and bubbles.\nTraditional prior-based methods and pixel-based methods often fail, while deep\nlearning lacks sufficient high-quality datasets. We introduce the Multi-Axis\nConditional Lookup (MAC-Lookup) model, which enhances visual quality by\nimproving color accuracy, sharpness, and contrast. It includes Conditional 3D\nLookup Table Color Correction (CLTCC) for preliminary color and quality\ncorrection and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.\nThis model prevents over-enhancement and saturation while handling underwater\nchallenges. Extensive experiments show that MAC-Lookup excels in enhancing\nunderwater images by restoring details and colors better than existing methods.\nThe code is https://github.com/onlycatdoraemon/MAC-Lookup.", "AI": {"tldr": "研究介绍了MAC-Lookup模型，该模型通过两种技术（CLTCC和MAAE）来提高水下图像的颜色准确性和清晰度，实验结果表明其性能优于现有方法。", "motivation": "传统的基于先验的方法和基于像素的方法在增强水下图像时效果不佳，而深度学习由于缺乏高质量的数据集而受到限制。该研究旨在解决水下图像的可见性和颜色问题。", "method": "引入了Multi-Axis Conditional Lookup (MAC-Lookup)模型，其中包括Conditional 3D Lookup Table Color Correction (CLTCC)进行初步的颜色和质量校正，以及Multi-Axis Adaptive Enhancement (MAAE)进行细节细化。", "result": "广泛的实验表明，MAC-Lookup模型在增强水下图像方面表现出色，尤其是在恢复细节和颜色方面优于现有的方法。", "conclusion": "MAC-Lookup模型能够有效增强水下图像，同时避免过增强和饱和问题，表现优于现有方法。"}}
{"id": "2507.02378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02378", "abs": "https://arxiv.org/abs/2507.02378", "authors": ["Weijie Lyu", "Sheng-Jun Huang", "Xuan Xia"], "title": "Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection", "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.", "AI": {"tldr": "The paper presents a parametric model for efficient selection of high-quality training data for code generation, achieving better performance and efficiency with fewer samples compared to full datasets.", "motivation": "To improve training efficiency and model performance of large language models (LLMs) in code generation, the paper addresses the issue of data quality being overlooked in current methods that focus on quantity.", "method": "The authors utilize a parametric model for selecting code data, optimizing it to guarantee distribution consistency and diversity within the subset chosen for training.", "result": "Experimental results show that the method achieves a 2.4% and 2.3% performance gain in HumanEval and MBPP respectively, using only 10K samples against a 92K-sample baseline, surpassing other sampling methods in performance and efficiency.", "conclusion": "The method effectively enhances model performance in code generation tasks while significantly reducing the computational cost by selecting high-quality training data efficiently."}}
{"id": "2507.02271", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.02271", "abs": "https://arxiv.org/abs/2507.02271", "authors": ["Feizhen Huang", "Yu Wu", "Yutian Lin", "Bo Du"], "title": "Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation", "comment": "Accepted by IJCAI 2025", "summary": "Video-to-Audio (V2A) Generation achieves significant progress and plays a\ncrucial role in film and video post-production. However, current methods\noverlook the cinematic language, a critical component of artistic expression in\nfilmmaking. As a result, their performance deteriorates in scenarios where\nFoley targets are only partially visible. To address this challenge, we propose\na simple self-distillation approach to extend V2A models to cinematic language\nscenarios. By simulating the cinematic language variations, the student model\nlearns to align the video features of training pairs with the same audio-visual\ncorrespondences, enabling it to effectively capture the associations between\nsounds and partial visual information. Our method not only achieves impressive\nimprovements under partial visibility across all evaluation metrics, but also\nenhances performance on the large-scale V2A dataset, VGGSound.", "AI": {"tldr": "论文提出了一种自蒸馏方法，改进了视频到音频生成模型在对象部分可见情况下的性能，并提升了其在VGGSound数据集上的表现。", "motivation": "现有的视频到音频生成方法忽略了电影语言这一关键的艺术表达组件，导致在对象仅部分可见的场景下性能下降。", "method": "论文采用了一种自蒸馏的方法，通过模拟电影语言的变化，使学生模型学习训练对中的视频特征与相同音视频对应关系之间的对齐。", "result": "该论文提出了一种简单自蒸馏方法，旨在将视频到音频生成模型扩展到电影语言场景中。通过模拟电影语言的变化，学生模型能够更好地捕捉声音与部分视觉信息之间的关联，从而在对象部分可见的情况下显著提升了模型的表现。此外，该方法在大规模视频到音频数据集VGGSound上的性能也得到了增强。", "conclusion": "提出的自蒸馏方法在部分目标可见的场景下，所有评估指标都取得显著改进，并且在大规模V2A数据集VGGSound上的性能也得以提升。"}}
{"id": "2507.02407", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02407", "abs": "https://arxiv.org/abs/2507.02407", "authors": ["Mark Atta Mensah", "Isaac Wiafe", "Akon Ekpezu", "Justice Kwame Appati", "Jamal-Deen Abdulai", "Akosua Nyarkoa Wiafe-Akenten", "Frank Ernest Yeboah", "Gifty Odame"], "title": "Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability", "comment": "This version has been reviewed and accepted for presentation at the\n  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025\n  in Munich, Germany. 17 pages, 4 figures, 1 table", "summary": "Most existing automatic speech recognition (ASR) research evaluate models\nusing in-domain datasets. However, they seldom evaluate how they generalize\nacross diverse speech contexts. This study addresses this gap by benchmarking\nseven Akan ASR models built on transformer architectures, such as Whisper and\nWav2Vec2, using four Akan speech corpora to determine their performance. These\ndatasets encompass various domains, including culturally relevant image\ndescriptions, informal conversations, biblical scripture readings, and\nspontaneous financial dialogues. A comparison of the word error rate and\ncharacter error rate highlighted domain dependency, with models performing\noptimally only within their training domains while showing marked accuracy\ndegradation in mismatched scenarios. This study also identified distinct error\nbehaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned\nWhisper Akan models led to more fluent but potentially misleading transcription\nerrors, Wav2Vec2 produced more obvious yet less interpretable outputs when\nencountering unfamiliar inputs. This trade-off between readability and\ntransparency in ASR errors should be considered when selecting architectures\nfor low-resource language (LRL) applications. These findings highlight the need\nfor targeted domain adaptation techniques, adaptive routing strategies, and\nmultilingual training frameworks for Akan and other LRLs.", "AI": {"tldr": "该研究通过四个包含不同领域的阿坎语语料库来评估七种阿坎语ASR模型，发现模型在训练领域以外表现不佳，提出需开发特定领域适应技术、自适应路由策略和多语言训练框架。", "motivation": "大多数现有的ASR研究利用领域内的数据集来评估模型，很少评估这些模型在多样化语音环境下的泛化能力。这项研究旨在填补这一空白。", "method": "该研究通过使用四个包含不同领域数据的阿坎语语音语料库来评估七种基于变压器架构的阿坎语自动语音识别（ASR）模型的性能，其中包括文化相关的图像描述、非正式对话、圣经经文阅读和自发性金融对话。", "result": "通过比较单词错误率和字符错误率，研究表明这些模型只在其训练领域内表现出较好的性能，而在不匹配场景中的精度显著下降。这个研究还发现Whisper和Wav2Vec2架构在面对不熟悉输入时，会表现出不同的错误行为。", "conclusion": "根据该研究，选择低资源语言ASR架构时，应该考虑准确性与可解释性之间的权衡。这些发现强调了需要针对阿坎语和其他低资源语言开发特定领域的适应技术、自适应路由策略和多语言训练框架。"}}
{"id": "2507.02279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02279", "abs": "https://arxiv.org/abs/2507.02279", "authors": ["Juntao Liu", "Liqiang Niu", "Wenchao Chen", "Jie Zhou", "Fandong Meng"], "title": "LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models", "comment": null, "summary": "Existing visual token compression methods for Multimodal Large Language\nModels (MLLMs) predominantly operate as post-encoder modules, limiting their\npotential for efficiency gains. To address this limitation, we propose LaCo\n(Layer-wise Visual Token Compression), a novel framework that enables effective\ntoken compression within the intermediate layers of the vision encoder. LaCo\nintroduces two core components: 1) a layer-wise pixel-shuffle mechanism that\nsystematically merges adjacent tokens through space-to-channel transformations,\nand 2) a residual learning architecture with non-parametric shortcuts that\npreserves critical visual information during compression. Extensive experiments\nindicate that our LaCo outperforms all existing methods when compressing tokens\nin the intermediate layers of the vision encoder, demonstrating superior\neffectiveness. In addition, compared to external compression, our method\nimproves training efficiency beyond 20% and inference throughput over 15% while\nmaintaining strong performance.", "AI": {"tldr": "LaCo (Layer-wise Visual Token Compression) 是一种可以提高Multimodal Large Language Models (MLLMs) 效率的新框架，能够在视觉编码器的中间层实现有效的token压缩。", "motivation": "当前的视觉token压缩方法主要用于多模态大语言模型(MLLMs)的后编码模块，限制了其在效率提升方面的潜力。为了克服这一局限性，提出了LaCo方法。", "method": "LaCo (Layer-wise Visual Token Compression) 是一种新的框架，旨在在视觉编码器的中间层实现有效的token压缩。该框架包括两个核心组件：1) 层级像素混洗机制，通过空间到通道变换系统地合并相邻token；2) 残差学习架构，配有非参数捷径，以在压缩过程中保存关键视觉信息。", "result": "广泛实验表明，当压缩视觉编码器中间层的token时，LaCo优于所有现有方法，显示了更强的有效性。此外，与外部压缩相比，该方法提高了超过20%的训练效率和超过15%的推理吞吐量，同时保持了强大的性能。", "conclusion": "LaCo不仅优于现有方法，而且在提高训练效率和推理吞吐量方面表现出色，同时保持了强大的性能。"}}
{"id": "2507.02428", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02428", "abs": "https://arxiv.org/abs/2507.02428", "authors": ["Sumaya Ahmed Salihs", "Isaac Wiafe", "Jamal-Deen Abdulai", "Elikem Doe Atsakpo", "Gifty Ayoka", "Richard Cave", "Akon Obu Ekpezu", "Catherine Holloway", "Katrin Tomanek", "Fiifi Baffoe Payin Winful"], "title": "A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages", "comment": "This version has been reviewed and accepted for presentation at the\n  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5\n  pages and 3 tables", "summary": "This study presents an approach for collecting speech samples to build\nAutomatic Speech Recognition (ASR) models for impaired speech, particularly,\nlow-resource languages. It aims to democratize ASR technology and data\ncollection by developing a \"cookbook\" of best practices and training for\ncommunity-driven data collection and ASR model building. As a proof-of-concept,\nthis study curated the first open-source dataset of impaired speech in Akan: a\nwidely spoken indigenous language in Ghana. The study involved participants\nfrom diverse backgrounds with speech impairments. The resulting dataset, along\nwith the cookbook and open-source tools, are publicly available to enable\nresearchers and practitioners to create inclusive ASR technologies tailored to\nthe unique needs of speech impaired individuals. In addition, this study\npresents the initial results of fine-tuning open-source ASR models to better\nrecognize impaired speech in Akan.", "AI": {"tldr": "本研究开发了一种方法来收集有语言障碍者的语音样本，以建立低资源语言的ASR模型，特别是在阿肯语上进行了实践。同时还创建了相关开放资源来促进这项工作的发展。初步结果显示，这项工作对于改善ASR技术的包容性具有潜力。", "motivation": "该研究旨在解决有语言障碍的个体在使用ASR技术方面的不平等问题，特别是对于那些使用低资源语言的人群。通过开放和共享资源，促进更多人能够参与到ASR技术的开发和应用中来。", "method": "本研究提出了一种收集用于建立自动语音识别（ASR）模型的语音样本的方法，特别针对有语言障碍的人群，尤其是低资源语言。研究旨在通过开发数据收集和ASR模型构建的最佳实践指南和培训，普及ASR技术和数据收集。作为概念验证，本研究整理了第一份开放源代码的阿肯语（加纳广泛使用的土著语言）有语言障碍的语音数据集。", "result": "研究结果包括公开发布的数据集、最佳实践指南和开源工具，以供研究人员和实践者创建适合有语言障碍个体需求的包容性ASR技术。此外，本研究还展示了对开源ASR模型进行微调以更好地识别阿肯语中障碍语音的初步结果。", "conclusion": "本研究通过开发最佳实践和工具，使得科学研究和社会实践人员能够创建适合有语言障碍个体需求的ASR技术。研究展示了这些工具和实践的初步成果，以及其对未来ASR技术发展的潜在影响。"}}
{"id": "2507.02288", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02288", "abs": "https://arxiv.org/abs/2507.02288", "authors": ["De Cheng", "Zhipeng Xu", "Xinyang Jiang", "Dongsheng Li", "Nannan Wang", "Xinbo Gao"], "title": "Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization", "comment": null, "summary": "Domain Generalization (DG) seeks to develop a versatile model capable of\nperforming effectively on unseen target domains. Notably, recent advances in\npre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated\nconsiderable potential in enhancing the generalization capabilities of deep\nlearning models. Despite the increasing attention toward VFM-based domain\nprompt tuning within DG, the effective design of prompts capable of\ndisentangling invariant features across diverse domains remains a critical\nchallenge. In this paper, we propose addressing this challenge by leveraging\nthe controllable and flexible language prompt of the VFM. Noting that the text\nmodality of VFMs is naturally easier to disentangle, we introduce a novel\nframework for text feature-guided visual prompt tuning. This framework first\nautomatically disentangles the text prompt using a large language model (LLM)\nand then learns domain-invariant visual representation guided by the\ndisentangled text feature. However, relying solely on language to guide visual\nfeature disentanglement has limitations, as visual features can sometimes be\ntoo complex or nuanced to be fully captured by descriptive text. To address\nthis, we introduce Worst Explicit Representation Alignment (WERA), which\nextends text-guided visual prompts by incorporating an additional set of\nabstract prompts. These prompts enhance source domain diversity through\nstylized image augmentations, while alignment constraints ensure that visual\nrepresentations remain consistent across both the original and augmented\ndistributions. Experiments conducted on major DG datasets, including PACS,\nVLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method\noutperforms state-of-the-art DG methods.", "AI": {"tldr": "本文提出了一个通过文本特征引导视觉提示调优的新框架，并引入了WERA机制以增强方法的跨领域泛化能力，该方法在多个数据集上超过了现有最优方法。", "motivation": "近年来，预训练视觉基础模型（VFMs）在领域泛化（DG）中表现出色，但设计能够解耦跨域不变特征的提示仍然具有挑战性。", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出了一个通过文本特征引导视觉提示调优的新框架，并引入了WERA机制以增强方法的跨领域泛化能力，该方法在多个数据集上超过了现有最优方法。\", \n  \"motivation\": \"近年来，预训练视觉基础模型（VFMs）在领域泛化（DG）中表现出色，但设计能够解耦跨域不变特征的提示仍然具有挑战性。\", \n  \"method\": \"方法包括使用大语言模型自动解耦文本提示，然后基于解耦的文本特征学习领域不变的视觉表示。还引入了WERA机制，通过增加抽象提示和样式化的图像增强来优化这一过程。\", \n  \"result\": \"实验结果显示，本文方法在PACS、VLCS、OfficeHome、DomainNet和TerraInc等多个DG数据集上超过了现有最优方法。\", \n  \"conclusion\": \"本文证明了文本引导的视觉提示优化在DG中的有效性，并展示了WERA为增强模型的泛化能力提供了一个有效途径。\"}\n}", "conclusion": "本文证明了文本引导的视觉提示优化在DG中的有效性，并展示了WERA为增强模型的泛化能力提供了一个有效途径。"}}
{"id": "2507.02506", "categories": ["cs.CL", "cs.AI", "cs.LG", "91B14, 68T50", "I.2.7; K.4.1; K.5.2"], "pdf": "https://arxiv.org/pdf/2507.02506", "abs": "https://arxiv.org/abs/2507.02506", "authors": ["Sneha Deshmukh", "Prathmesh Kamble"], "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders", "comment": "9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and\n  GitHub. Submitted to arXiv for open access", "summary": "Legal NLP remains underdeveloped in regions like India due to the scarcity of\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\npipeline and verified for consistency. This resource supports a wide range of\nlegal NLP tasks such as outcome prediction, summarization, and fairness\nanalysis, and is the first publicly available dataset focused specifically on\nIndian bail jurisprudence.", "AI": {"tldr": "本文构建了名为IndianBailJudgments-1200的数据集，包含1200份印度法庭判决书，旨在促进印度法律NLP研究，支持保释结果预测等任务。", "motivation": "鉴于印度等区域在法律NLP领域的数据集稀缺，本研究旨在填补这一空白，为法律NLP任务提供支持。", "method": "本文介绍了一个名为IndianBailJudgments-1200的新数据集，该数据集包含1200份有关印度保释决定的法院判决，标注了20多个属性。这些注释通过使用经过提示工程的GPT-4管道生成，并验证了一致性。", "result": "成功构建了第一个专门针对印度保释司法的公开数据集，可用于保释结果预测、总结和公平性分析等多种任务。", "conclusion": "本研究构建的数据集对促进印度法律NLP研究具有重要意义，为进一步的研究和应用提供了宝贵的资源。"}}
{"id": "2507.02294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02294", "abs": "https://arxiv.org/abs/2507.02294", "authors": ["Hanbo Bi", "Yulong Xu", "Ya Li", "Yongqiang Mao", "Boyuan Tong", "Chongyang Li", "Chunbo Lang", "Wenhui Diao", "Hongqi Wang", "Yingchao Feng", "Xian Sun"], "title": "ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation", "comment": null, "summary": "The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits\nstrong generalization in generic segmentation tasks. However, applying SAM to\nremote sensing (RS) images still faces two major challenges. First, manually\nconstructing precise prompts for each image (e.g., points or boxes) is\nlabor-intensive and inefficient, especially in RS scenarios with dense small\nobjects or spatially fragmented distributions. Second, SAM lacks domain\nadaptability, as it is pre-trained primarily on natural images and struggles to\ncapture RS-specific semantics and spatial characteristics, especially when\nsegmenting novel or unseen classes. To address these issues, inspired by\nfew-shot learning, we propose ViRefSAM, a novel framework that guides SAM\nutilizing only a few annotated reference images that contain class-specific\nobjects. Without requiring manual prompts, ViRefSAM enables automatic\nsegmentation of class-consistent objects across RS images. Specifically,\nViRefSAM introduces two key components while keeping SAM's original\narchitecture intact: (1) a Visual Contextual Prompt Encoder that extracts\nclass-specific semantic clues from reference images and generates object-aware\nprompts via contextual interaction with target images; and (2) a Dynamic Target\nAlignment Adapter, integrated into SAM's image encoder, which mitigates the\ndomain gap by injecting class-specific semantics into target image features,\nenabling SAM to dynamically focus on task-relevant regions. Extensive\nexperiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,\nLoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and\nautomatic segmentation of unseen classes by leveraging only a few reference\nimages and consistently outperforms existing few-shot segmentation methods\nacross diverse datasets.", "AI": {"tldr": "为了解决SAM在遥感图像上的手动提示构建和领域适应性问题，研究人员提出使用少量标注参考图像来自动分割特定类别的对象。实验表明，该方法在多个少样本分割基准中表现优异。", "motivation": "尽管SAM在一般分割任务上显示出强大的泛化能力，但将其应用于遥感图像时仍面临两个重大挑战：1）为每张图像手动构建精确的提示（如点或框）工作量大且效率低，特别是在存在密集小对象或空间分布碎片化的遥感环境中。2）SAM缺乏领域适应性，主要是在自然图像上预训练的，难以捕捉遥感特定的语义和空间特性，尤其是当分割新的或未见过的类别时。", "method": "为了解决SAM在遥感图像中的手动提示构建问题和领域适应性问题，ViRefSAM框架通过利用少量包含特定类别对象的标注参考图像来引导SAM。ViRefSAM具体包括两个关键组件：1）视觉上下文提示编码器，从参考图像中提取类别特定的语义线索，并通过与目标图像的上下文交互生成对象感知提示；2）动态目标对齐适配器，集成在SAM的图像编码器中，通过注入类别特定语义到目标图像特征中来减少领域差距，使SAM能够动态地专注于任务相关区域。", "result": "在iSAID-5$^i$、LoveDA-2$^i$和COCO-20$^i$三个少样本分割基准上的广泛实验表明，ViRefSAM通过利用少量参考图像能够实现未见过类别的准确自动分割，并在各种数据集上超过现有的少样本分割方法。", "conclusion": "实验显示，ViRefSAM通过利用少量参考图像能够准确且自动地分割未见过的类别，且在各种数据集上一致优于现有的少样本分割方法。"}}
{"id": "2507.02592", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02592", "abs": "https://arxiv.org/abs/2507.02592", "authors": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Liwen Zhang", "Litu Ou", "Jialong Wu", "Wenbiao Yin", "Baixuan Li", "Zhengwei Tao", "Xinyu Wang", "Weizhou Shen", "Junkai Zhang", "Dingchu Zhang", "Xixi Wu", "Yong Jiang", "Ming Yan", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebSailor: Navigating Super-human Reasoning for Web Agent", "comment": null, "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.", "AI": {"tldr": "WebSailor是一种后训练方法，通过新的训练技巧和算法以增强开源语言模型在复杂信息搜寻任务上处理高度不确定性的能力，达成在这些任务上的表现与专有系统旗鼓相当。", "motivation": "由于专有代理系统在处理复杂信息搜索任务中展现出了超越开源模型的表现，因此研究旨在通过引入WebSailor来关闭开源模型与专有模型之间的能力差距。", "method": "WebSailor采用了一种完整的后训练方法，利用结构化采样和信息模糊化生成高不确定性任务，通过RFT冷启动和高效代理强化学习训练算法（复制采样策略优化，DUPO）来实现这一目标。", "result": "采用WebSailor训练出来的代理在复杂的搜索任务上显著超越了所有开源代理的表现，达到了与专有代理相匹配的水平，缩小了它们之间的能力差距。", "conclusion": "研究证明，通过WebSailor这种特定的训练方法，能够赋予开源模型处理复杂信息环境中的高度不确定性的能力，从而提升其在复杂任务中的性能，可望达到接近专有系统的水平。"}}
{"id": "2507.02299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02299", "abs": "https://arxiv.org/abs/2507.02299", "authors": ["Yunhan Yang", "Shuo Chen", "Yukun Huang", "Xiaoyang Wu", "Yuan-Chen Guo", "Edmund Y. Lam", "Hengshuang Zhao", "Tong He", "Xihui Liu"], "title": "DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation", "comment": "Accepted by TPAMI, extension of CVPR 2024 paper DreamComposer", "summary": "Recent advancements in leveraging pre-trained 2D diffusion models achieve the\ngeneration of high-quality novel views from a single in-the-wild image.\nHowever, existing works face challenges in producing controllable novel views\ndue to the lack of information from multiple views. In this paper, we present\nDreamComposer++, a flexible and scalable framework designed to improve current\nview-aware diffusion models by incorporating multi-view conditions.\nSpecifically, DreamComposer++ utilizes a view-aware 3D lifting module to\nextract 3D representations of an object from various views. These\nrepresentations are then aggregated and rendered into the latent features of\ntarget view through the multi-view feature fusion module. Finally, the obtained\nfeatures of target view are integrated into pre-trained image or video\ndiffusion models for novel view synthesis. Experimental results demonstrate\nthat DreamComposer++ seamlessly integrates with cutting-edge view-aware\ndiffusion models and enhances their abilities to generate controllable novel\nviews from multi-view conditions. This advancement facilitates controllable 3D\nobject reconstruction and enables a wide range of applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.02593", "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.02593", "abs": "https://arxiv.org/abs/2507.02593", "authors": ["Cornelia Gruber", "Helen Alber", "Bernd Bischl", "Göran Kauermann", "Barbara Plank", "Matthias Aßenmacher"], "title": "Revisiting Active Learning under (Human) Label Variation", "comment": null, "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.", "AI": {"tldr": "本文提出了一种将人类标签变化（HLV）整合进主动学习（AL）过程的概念框架，目的在于更好地反映现实世界标注的复杂性。", "motivation": "调研现有的主动学习（AL）和（H）标签变化（LV）社区对于信号（例如，HLV）和噪声（例如，标注错误）之间的区别的处理情况，强调了需要将观察到的LV分解为信号和噪声。", "method": "文中提出了将人类标签变化（HLV）纳入主动学习（AL）循环的概念框架，包括实例选择、标注者选择和标签表示。此外，还讨论了将大型语言模型（LLM）作为标注者的整合。", "result": "强调了现有AL和LV社区在处理HLV和噪声方面存在的缺陷，提出了一个新的理论框架来处理这些区别。", "conclusion": "工作旨在为HLV感知的主动学习建立一个概念基础，更好地反映现实世界标注的复杂性。"}}
{"id": "2507.02307", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02307", "abs": "https://arxiv.org/abs/2507.02307", "authors": ["Haoxuan Li", "Chenxu Wei", "Haodong Wang", "Xiaomeng Hu", "Boyuan An", "Lingyan Ran", "Baosen Zhang", "Jin Jin", "Omirzhan Taukebayev", "Amirkhan Temirbayev", "Junrui Liu", "Xiuwei Zhang"], "title": "Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images", "comment": "18 pages, 8 figures", "summary": "Change detection typically involves identifying regions with changes between\nbitemporal images taken at the same location. Besides significant changes, slow\nchanges in bitemporal images are also important in real-life scenarios. For\ninstance, weak changes often serve as precursors to major hazards in scenarios\nlike slopes, dams, and tailings ponds. Therefore, designing a change detection\nnetwork that simultaneously detects slow and fast changes presents a novel\nchallenge. In this paper, to address this challenge, we propose a change\ndetection network named Flow-CDNet, consisting of two branches: optical flow\nbranch and binary change detection branch. The first branch utilizes a pyramid\nstructure to extract displacement changes at multiple scales. The second one\ncombines a ResNet-based network with the optical flow branch's output to\ngenerate fast change outputs. Subsequently, to supervise and evaluate this new\nchange detection framework, a self-built change detection dataset Flow-Change,\na loss function combining binary tversky loss and L2 norm loss, along with a\nnew evaluation metric called FEPE are designed. Quantitative experiments\nconducted on Flow-Change dataset demonstrated that our approach outperforms the\nexisting methods. Furthermore, ablation experiments verified that the two\nbranches can promote each other to enhance the detection performance.", "AI": {"tldr": "The paper presents Flow-CDNet, a change detection network capable of detecting both slow and fast changes in bitemporal images, featuring an optical flow branch and a binary change detection branch.", "motivation": "To develop a change detection network that can simultaneously detect slow and fast changes in bitemporal images, particularly recognizing the importance of weak changes that can indicate major hazards in real-life scenarios such as slopes, dams, and tailings ponds.", "method": "Flow-CDNet includes two branches: one for extracting displacement changes at multiple scales using a pyramid structure, and another that combines a ResNet-based network with optical flow outputs for fast change detection. A self-built dataset (Flow-Change), a hybrid loss function, and a new evaluation metric (FEPE) were designed for supervision and evaluation.", "result": "Experiments demonstrated that Flow-CDNet outperformed existing methods on the Flow-Change dataset, and ablation studies confirmed that the two branches can enhance each other's performance.", "conclusion": "The Flow-CDNet, designed to detect both slow and fast changes in bitemporal images, achieves better performance than current change detection methods, with the potential to be a valuable tool in monitoring hazardous scenarios that require early detection of small changes."}}
{"id": "2507.02595", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02595", "abs": "https://arxiv.org/abs/2507.02595", "authors": ["Xin Guan", "PeiHsin Lin", "Zekun Wu", "Ze Wang", "Ruibo Zhang", "Emre Kazim", "Adriano Koshiyama"], "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion", "comment": "Accepted at ICML 2025 AIW Workshop", "summary": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning.", "AI": {"tldr": "MPF是一种利用多视角生成来缓解大语言模型偏见的后训练对齐框架，能够自动构建偏见基准并减少偏见，无需复杂的提示工程或微调。", "motivation": "为了应对日益增长的轻松缓解偏见的需求，MPF框架被开发出来作为LLM的一种后训练校准框架。", "method": "MPF框架利用多视角生成来揭示和校准LLM输出中的偏见，通过将其分解为可解释的视角组件，并通过采样和平衡响应来进行引导，响应的权重基于分解中获得的概率。", "result": "实验表明，MPF可以将LLM的情感分布与反事实基线（如绝对平等）和HR基线（偏向Top University）对齐，实现了小的KL散度，减少了校准误差，并且能够推广到未见过的问题上。", "conclusion": "MPF框架提供了一种可扩展和可解释的方法，用于LLM的对齐和偏见缓解，并且能够与部署的LLM兼容。"}}
{"id": "2507.02308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02308", "abs": "https://arxiv.org/abs/2507.02308", "authors": ["Pei Guo", "Ryan Farrell"], "title": "LMPNet for Weakly-supervised Keypoint Discovery", "comment": null, "summary": "In this work, we explore the task of semantic object keypoint discovery\nweakly-supervised by only category labels. This is achieved by transforming\ndiscriminatively-trained intermediate layer filters into keypoint detectors. We\nbegin by identifying three preferred characteristics of keypoint detectors: (i)\nspatially sparse activations, (ii) consistency and (iii) diversity. Instead of\nrelying on hand-crafted loss terms, a novel computationally-efficient leaky max\npooling (LMP) layer is proposed to explicitly encourage final conv-layer\nfilters to learn \"non-repeatable local patterns\" that are well aligned with\nobject keypoints. Informed by visualizations, a simple yet effective selection\nstrategy is proposed to ensure consistent filter activations and attention\nmask-out is then applied to force the network to distribute its attention to\nthe whole object instead of just the most discriminative region. For the final\nkeypoint prediction, a learnable clustering layer is proposed to group keypoint\nproposals into keypoint predictions. The final model, named LMPNet, is highly\ninterpretable in that it directly manipulates network filters to detect\npredefined concepts. Our experiments show that LMPNet can (i) automatically\ndiscover semantic keypoints that are robust to object pose and (ii) achieves\nstrong prediction accuracy comparable to a supervised pose estimation model.", "AI": {"tldr": "本文提出了一种高效的新模型LMPNet来实现弱监督下的语义物体关键点检测。通过使用漏极最大池化层和注意力遮罩机制，模型实现了与监督方法相近的预测准确性。", "motivation": "本文旨在研究仅通过类别标签进行弱监督的物体语义关键点发现任务。", "method": "本文提出了LMPNet模型，通过在卷积层后加入新颖的计算高效漏极最大池化（LMP）层，将有区分性的训练中间层滤波器转化为关键点检测器。LMP层鼓励学习“非重复性局部模式”，这些模式与物体关键点对齐。此外，通过可视化指导，提出了一种简单的选择策略，并且应用注意力遮罩使网络将注意力分配到整个物体而不是最区分的区域。最后关键点预测是通过一个可学习的聚类层将关键点提案分组完成的。", "result": "实验表明LMPNet能够自动发现对物体姿态鲁棒的语义关键点并获得了与监督姿势估计模型相当的强预测准确性。", "conclusion": "LMPNet模型具有高度可解释性，因为它直接操纵网络滤波器来检测预定义的概念，并取得了良好的关键点预测效果。"}}
{"id": "2507.02679", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02679", "abs": "https://arxiv.org/abs/2507.02679", "authors": ["Ahmed Sabir", "Rajesh Sharama"], "title": "Exploring Gender Bias Beyond Occupational Titles", "comment": "Work in progress", "summary": "In this work, we investigate the correlation between gender and contextual\nbiases, focusing on elements such as action verbs, object nouns, and\nparticularly on occupations. We introduce a novel dataset, GenderLexicon, and a\nframework that can estimate contextual bias and its related gender bias. Our\nmodel can interpret the bias with a score and thus improve the explainability\nof gender bias. Also, our findings confirm the existence of gender biases\nbeyond occupational stereotypes. To validate our approach and demonstrate its\neffectiveness, we conduct evaluations on five diverse datasets, including a\nJapanese dataset.", "AI": {"tldr": "本文研究性别和情境偏差之间的关联，引入了新数据集和框架来评估和解释性别偏差，发现性别偏见存在超出职业刻板印象的证据。", "motivation": "研究性别和情境偏差之间的关联，特别关注动作动词、物体名词和职业上的差异。", "method": "我们引入了一个新数据集GenderLexicon和一个可以估计语境偏差及其相关性别偏差的框架。我们的模型可以通过评分来解释性别偏见，从而提高性别偏见的解释性。", "result": "研究确认了存在超越职业刻板印象的性别偏见。在包括日语数据在内的五个不同的数据集上验证了方法的有效性。", "conclusion": "研究证明了所提出的方法能够有效地评估和解释性别偏差，并且识别出一种超越传统职业刻板印象的性别偏见形式。"}}
{"id": "2507.02311", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02311", "abs": "https://arxiv.org/abs/2507.02311", "authors": ["Le Xu", "Qi Zhang", "Qixian Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "title": "Perception Activator: An intuitive and portable framework for brain cognitive exploration", "comment": null, "summary": "Recent advances in brain-vision decoding have driven significant progress,\nreconstructing with high fidelity perceived visual stimuli from neural\nactivity, e.g., functional magnetic resonance imaging (fMRI), in the human\nvisual cortex. Most existing methods decode the brain signal using a two-level\nstrategy, i.e., pixel-level and semantic-level. However, these methods rely\nheavily on low-level pixel alignment yet lack sufficient and fine-grained\nsemantic alignment, resulting in obvious reconstruction distortions of multiple\nsemantic objects. To better understand the brain's visual perception patterns\nand how current decoding models process semantic objects, we have developed an\nexperimental framework that uses fMRI representations as intervention\nconditions. By injecting these representations into multi-scale image features\nvia cross-attention, we compare both downstream performance and intermediate\nfeature changes on object detection and instance segmentation tasks with and\nwithout fMRI information. Our results demonstrate that incorporating fMRI\nsignals enhances the accuracy of downstream detection and segmentation,\nconfirming that fMRI contains rich multi-object semantic cues and coarse\nspatial localization information-elements that current models have yet to fully\nexploit or integrate.", "AI": {"tldr": "本文提出了一种利用fMRI表示改进视觉目标检测和分割的方法，并证明了fMRI信号提供的多对象语义信息和空间信息对于此类任务的有效性。", "motivation": "为了更好地理解大脑的视觉感知模式以及当前解码模型如何处理语义对象，本文作者提出了一种新的实验方法。", "method": "本文提出了一种实验框架，该框架使用功能磁共振成像(fMRI)表示作为干预条件，并通过交叉注意力将这些表示注入多尺度图像特征中。研究人员通过对比有无fMRI信息对目标检测和实例分割任务的影响，来研究其下游性能和中间特征的变化。", "result": "研究结果表明，加入fMRI信号可以提高下游检测和分割任务的准确性，这证实了fMRI信号包含了丰富的多个对象的语义线索和粗略的空间定位信息。", "conclusion": "通过将fMRI表示与图像特征相融合，可以改善目标检测和实例分割模型的性能。这些研究进一步揭示了fMRI信号在视觉理解中的作用及其尚未被充分利用的潜力。"}}
{"id": "2507.02694", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02694", "abs": "https://arxiv.org/abs/2507.02694", "authors": ["Zhijian Xu", "Yilun Zhao", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers", "comment": null, "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.", "AI": {"tldr": "该研究开发了LimitGen，一个用于评估大型语言模型（LLMs）识别研究论文局限性能力的基准测试，包含合成数据集和人类编写的局限性数据集，并通过文献检索增强LLMs的能力，使其能够更准确地提供反馈。", "motivation": "随着科研论文数量的增加，同行评审的挑战也随之增大。尽管大型语言模型在各种科学任务上显示出潜力，但对于同行评审的帮助，特别是识别论文局限性的潜力仍未充分研究。", "method": "研究首先提出了一个关于科学研究局限性的全面分类，专注于AI领域。基于此分类，构建了LimitGen基准测试，包括两部分：LimitGen-Syn，用于研究局限性的合成数据集；LimitGen-Human，一组真实人类编写的局限性。通过文献检索增强大型语言模型，提升其识别局限性的能力。", "result": "该方法增强了大型语言模型系统生成研究论文局限性陈述的能力，使其能够提供更加具体和建设性的反馈。", "conclusion": "LimitGen基准测试和文献检索的结合使用提升了大型语言模型在早期阶段为同行评审提供反馈和支持的能力。"}}
{"id": "2507.02314", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02314", "abs": "https://arxiv.org/abs/2507.02314", "authors": ["JaeHyuck Choi", "MinJun Kim", "JeHyeong Hong"], "title": "MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation", "comment": "10 pages, 6 figures", "summary": "Few-shot anomaly generation is emerging as a practical solution for\naugmenting the scarce anomaly data in industrial quality control settings. An\nideal generator would meet three demands at once, namely (i) keep the normal\nbackground intact, (ii) inpaint anomalous regions to tightly overlap with the\ncorresponding anomaly masks, and (iii) generate anomalous regions in a\nsemantically valid location, while still producing realistic, diverse\nappearances from only a handful of real examples. Existing diffusion-based\nmethods usually satisfy at most two of these requirements: global anomaly\ngenerators corrupt the background, whereas mask-guided ones often falter when\nthe mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting\nwith multi-level perturbations and Context-aware alignment--to resolve all\nthree issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting\nbackbone that preserves normal regions and ensures strict adherence of the\nsynthesized anomaly to the supplied mask, directly addressing background\ncorruption and misalignment. To offset the diversity loss that fine-tuning can\ncause, MAGIC adds two complementary perturbation strategies: (i) Gaussian\nprompt-level perturbation applied during fine-tuning and inference that\nbroadens the global appearance of anomalies while avoiding low-fidelity textual\nappearances, and (ii) mask-guided spatial noise injection that enriches local\ntexture variations. Additionally, the context-aware mask alignment module forms\nsemantic correspondences and relocates masks so that every anomaly remains\nplausibly contained within the host object, eliminating out-of-boundary\nartifacts. Under a consistent identical evaluation protocol on the MVTec-AD\ndataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly\ntasks.", "AI": {"tldr": "MAGIC is proposed to generate high-quality anomalies while preserving the background and strictly adhering to anomaly masks, surpassing existing methods on the MVTec-AD dataset.", "motivation": "To address the need for realistic, diverse, and precisely located anomaly generation from limited data for industrial quality control.", "method": "Mask-guided inpainting with multi-level perturbations and context-aware alignment (MAGIC) to generate anomalies while preserving the background and precisely aligning with anomaly masks.", "result": "Outperforms previous methods on MVTec-AD dataset under a consistent evaluation protocol.", "conclusion": "MAGIC effectively resolves issues with background corruption, mask misalignment, and diversity loss in anomaly generation."}}
{"id": "2507.02744", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02744", "abs": "https://arxiv.org/abs/2507.02744", "authors": ["Peter Viechnicki"], "title": "Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens", "comment": null, "summary": "A body of work over the past several decades has demonstrated that the\ncomplex and coordinated articulatory movements of human vowel production are\ngoverned (at least in part)by control mechanisms whose targets are regions of\nauditory space. Within the target region control at the sub-phonemic level has\nalso been demonstrated. But the degree of accuracy of that control is unknown.\nThe current work investigates this question by asking how far apart must two\nvowel stimuli lie in auditory space in order to yield reliably different\nimitations? This distance is termed 'Just Producible Difference' (JPD). The\ncurrent study uses a vowel mimicry paradigm to derive the first measurement of\nJPD among two sets of English speakers during front vowel production. JPD is\nestimated at between 14 and 51 mels in F1 X F2 space. This finding has\nimplications for episodic theories of speech production. It also clarifies the\npossible structures of human vowel systems, by setting a theoretical lower\nbound for how close two vowel phonemes may be in a speaker's formant space, and\nhence a psychophysical explanation of observed trends in number and patterns of\npossible vowel phonemes.", "AI": {"tldr": "This study measures the 'Just Producible Difference' (JPD) in auditory space for English speakers' front vowel production, finding values between 14 and 51 mels.", "motivation": "The research aims to understand the accuracy of control mechanisms involved in human vowel production targeted at regions of auditory space by determining the JPD.", "method": "The study uses a vowel mimicry paradigm to investigate the 'Just Producible Difference' (JPD) among English speakers during front vowel production.", "result": "JPD is estimated to be between 14 and 51 mels in F1 X F2 space, suggesting the minimum distance needed to produce reliably distinct vowel sounds.", "conclusion": "The findings clarify the possible structures of human vowel systems and support episodic theories of speech production by setting a limit for vowel closeness in formant space."}}
{"id": "2507.02316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02316", "abs": "https://arxiv.org/abs/2507.02316", "authors": ["Zecheng Zhao", "Selena Song", "Tong Chen", "Zhi Chen", "Shazia Sadiq", "Yadan Luo"], "title": "Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos", "comment": "7 pages, 10 figures", "summary": "Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation\nmetrics primarily capture visual quality and temporal consistency, offering\nlimited insight into how synthetic videos perform in downstream tasks such as\ntext-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset\nand benchmark designed to evaluate the utility of synthetic videos for building\nretrieval models. Based on 800 diverse user queries derived from MSRVTT\ntraining split, we generate synthetic videos using state-of-the-art T2V models\nand annotate each video-text pair along four key semantic alignment dimensions:\nObject \\& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation\nframework correlates general video quality assessment (VQA) metrics with these\nalignment scores, and examines their predictive power for downstream TVR\nperformance. To explore pathways of scaling up, we further develop an\nAuto-Evaluator to estimate alignment quality from existing metrics. Beyond\nbenchmarking, our results show that SynTVA is a valuable asset for dataset\naugmentation, enabling the selection of high-utility synthetic samples that\nmeasurably improve TVR outcomes. Project page and dataset can be found at\nhttps://jasoncodemaker.github.io/SynTVA/.", "AI": {"tldr": "The authors present SynTVA, a benchmark and dataset designed to assess synthetic video quality for text-to-video retrieval, along with annotations in four semantic dimensions and an Auto-Evaluator for estimating alignment quality.", "motivation": "The motivation for this work is the realization that current evaluation metrics for text-to-video synthesis primarily focus on visual quality and temporal consistency, neglecting the performance of synthetic videos in downstream tasks such as text-to-video retrieval. The authors seek to address this gap by introducing a new dataset and benchmark that provides insights into the utility of synthetic videos for such tasks.", "method": "In this work, the authors develop SynTVA, a new dataset and benchmark for evaluating the utility of synthetic videos in text-to-video retrieval tasks. They generate synthetic videos using state-of-the-art T2V models based on 800 diverse user queries and annotate these along four key semantic dimensions: Object & Scene, Action, Attribute, and Prompt Fidelity. An Auto-Evaluator is also created to estimate alignment quality from existing metrics.", "result": "The SynTVA dataset and its evaluation framework correlate general video quality with alignment scores, highlighting the predictive power of these metrics for downstream TVR performance. Furthermore, it is shown that SynTVA can be used for dataset augmentation to improve TVR outcomes.", "conclusion": "SynTVA is concluded to be a valuable resource for benchmarking and dataset augmentation in the field of text-to-video synthesis, particularly for improving text-to-video retrieval performance. The work suggests a pathway for scaling up the evaluation process through the use of the Auto-Evaluator."}}
{"id": "2507.02778", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02778", "abs": "https://arxiv.org/abs/2507.02778", "authors": ["Ken Tsui"], "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs", "comment": "31 pages, 18 figures", "summary": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.", "AI": {"tldr": "研究发现LLMs在自我纠正方面存在系统性盲点，提出Self-Correction Bench框架并通过实验展示修正这一问题的初步结果和潜在改进方向。", "motivation": "研究LLMs的自我纠正能力，解决其在自我输出中纠正错误的系统性盲点问题。", "method": "引入Self-Correction Bench框架，通过在三个复杂度级别上注入受控错误，系统地测量LLMs的自我纠正盲点现象。", "result": "测试14个模型，平均盲点率为64.5%。通过在训练数据中加入错误纠正序列可以减轻盲点现象；简单加入“等待”指令可以使盲点现象降低89.3%。", "conclusion": "当前的LLMs在自我输出纠正方面存在一定局限性，通过改进数据训练方式可以提高其可靠性和可信度，指出了改进潜在方向。"}}
{"id": "2507.02321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02321", "abs": "https://arxiv.org/abs/2507.02321", "authors": ["Nina Konovalova", "Maxim Nikolaev", "Andrey Kuznetsov", "Aibek Alanov"], "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback", "comment": "code available at https://github.com/ControlGenAI/InnerControl", "summary": "Despite significant progress in text-to-image diffusion models, achieving\nprecise spatial control over generated outputs remains challenging. ControlNet\naddresses this by introducing an auxiliary conditioning module, while\nControlNet++ further refines alignment through a cycle consistency loss applied\nonly to the final denoising steps. However, this approach neglects intermediate\ngeneration stages, limiting its effectiveness. We propose InnerControl, a\ntraining strategy that enforces spatial consistency across all diffusion steps.\nOur method trains lightweight convolutional probes to reconstruct input control\nsignals (e.g., edges, depth) from intermediate UNet features at every denoising\nstep. These probes efficiently extract signals even from highly noisy latents,\nenabling pseudo ground truth controls for training. By minimizing the\ndiscrepancy between predicted and target conditions throughout the entire\ndiffusion process, our alignment loss improves both control fidelity and\ngeneration quality. Combined with established techniques like ControlNet++,\nInnerControl achieves state-of-the-art performance across diverse conditioning\nmethods (e.g., edges, depth).", "AI": {"tldr": "我们提出了InnerControl，一种新的训练策略，通过在整个扩散过程中强制执行空间一致性，改善生成图像的空间控制精度和质量。", "motivation": "尽管在文本到图像的扩散模型方面取得了显著进展，但精确控制生成输出的空间布局仍然具有挑战性。现有的方法，如ControlNet和ControlNet++，虽然有所改进，但忽视了中间生成阶段，限制了它们的效果。我们提出了一种新的方法来解决这个问题。", "method": "我们提出了一种名为InnerControl的训练策略，其通过在整个扩散过程中强制执行空间一致性来改善图像生成的控制。具体来说，InnerControl训练轻量级的卷积探测器从中间UNet特征中在每个去噪步骤中重建输入控制信号（如边缘、深度）。这些探测器可以从高度噪点的潜在空间中高效地提取信号，从而提供用于训练的伪地面真实控制信号。通过在整个扩散过程中最小化预测条件和目标条件之间的差异，我们的对齐损失提高了控制精度和生成质量。", "result": "实验表明，我们的方法不仅显著提高了控制精度和生成质量，还能够与其他现有技术（如ControlNet++）结合使用，以实现最先进的性能。", "conclusion": "InnerControl在不同的控制方法（如边缘、深度）上均达到了最先进的性能，展示了其有效性和广泛应用性。"}}
{"id": "2507.02799", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02799", "abs": "https://arxiv.org/abs/2507.02799", "authors": ["Riccardo Cantini", "Nicola Gabriele", "Alessio Orsino", "Domenico Talia"], "title": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models", "comment": null, "summary": "Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design.", "AI": {"tldr": "通过CLEAR-Bias基准测试评估显示，推理语言模型在面对偏见诱惑时的鲁棒性更低，尤其是依赖CoT提示的模型更容易受到攻击。", "motivation": "研究引入推理能力对模型公平性和鲁棒性的影响，比较了精细调优推理的模型与依赖推理提示（CoT）的模型在安全上的表现差异，以及不同的推理机制如何影响偏见掠取攻击的成功率。", "method": "通过CLEAR-Bias基准测试，系统地评估了最先进的推理语言模型（RLMs）在面对偏见诱惑时的鲁棒性。采用了语言模型作为裁判的自动化安全评分方法，并利用越狱技术来评估内置安全机制的强度。", "result": "研究表明，具有显式推理能力的模型，无论是通过CoT提示还是细调推理轨迹，都比没有这些机制的基本模型更容易受到偏见的触发。这表明推理可能无意中开启了强化刻板印象的新途径。依赖CoT提示的模型特别容易受到通过故事情节提示、虚构人设或奖励成形指令进行的情境重构攻击。", "conclusion": "结果挑战了推理能力能自动提升鲁棒性的假设，强调了需要开发更注重减少偏见的推理设计方法。"}}
{"id": "2507.02322", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02322", "abs": "https://arxiv.org/abs/2507.02322", "authors": ["Farida Siddiqi Prity", "Mirza Raquib", "Saydul Akbar Murad", "Md. Jubayar Alam Rafi", "Md. Khairul Bashar Bhuiyan", "Anupam Kumar Bairagi"], "title": "Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model", "comment": null, "summary": "Rice leaf diseases significantly reduce productivity and cause economic\nlosses, highlighting the need for early detection to enable effective\nmanagement and improve yields. This study proposes Artificial Neural Network\n(ANN)-based image-processing techniques for timely classification and\nrecognition of rice diseases. Despite the prevailing approach of directly\ninputting images of rice leaves into ANNs, there is a noticeable absence of\nthorough comparative analysis between the Feature Analysis Detection Model\n(FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it\ncomes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs).\nHence, this research presents initial experiments on the Feature Analysis\nDetection Model, utilizing various image Feature Extraction Algorithms,\nDimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms\n(FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on\ndatasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf\nscald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation\nmethod. A Direct Image-Centric Detection Model is established without the\nutilization of any FEA, and the evaluation of classification performance relies\non different metrics. Ultimately, an exhaustive contrast is performed between\nthe achievements of the Feature Analysis Detection Model and Direct\nImage-Centric Detection Model in classifying rice leaf diseases. The results\nreveal that the highest performance is attained using the Feature Analysis\nDetection Model. The adoption of the proposed Feature Analysis Detection Model\nfor detecting rice leaf diseases holds excellent potential for improving crop\nhealth, minimizing yield losses, and enhancing overall productivity and\nsustainability of rice farming.", "AI": {"tldr": "研究提出了使用特征分析检测模型(FADM)和直接图像为中心的检测模型(DICDM)进行水稻叶片疾病的分类，并通过实验表明FADM表现更优，有助于改善水稻作物的健康状况。", "motivation": "水稻叶片疾病的早期检测对于管理疾病和提高产量至关重要，但缺乏对FADM和DICDM的系统对比。", "method": "实验采用了不同的特征提取算法（FEA）、降维算法（DRA）和特征选择算法（FSA），并与不采用FEA的DICDM模型进行对比。", "result": "实验结果表明使用FADM的性能最佳。", "conclusion": "提出的FADM在水稻叶片疾病检测方面具有较大潜力，可以改善作物健康，减少产量损失，提高水稻种植的整体生产力和可持续性。"}}
{"id": "2507.02804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02804", "abs": "https://arxiv.org/abs/2507.02804", "authors": ["Wenhao Shi", "Zhiqiang Hu", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Heng Tao Shen"], "title": "Multimodal Mathematical Reasoning with Diverse Solving Perspective", "comment": "8 pages", "summary": "Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning.", "AI": {"tldr": "提出了MathV-DP数据集和Qwen-VL-DP模型，利用多种解题路径和强化学习方法，以提高数学推理中的准确性和多样性。", "motivation": "现有的多模态大语言模型在数学推理中依赖一对一的图像-文本对和单解决方案监督，缺乏对有效推理视角多样性的考虑和内部思考。", "method": "引入了MathV-DP数据集，该数据集为每个图像-问题对添加了多种不同的解题路径，并基于Qwen-VL模型提出了Qwen-VL-DP模型。Qwen-VL-DP通过监督学习进行微调，并通过基于规则的强化学习方法——群组相对策略优化（GRPO），整合正确性区分和多样性意识奖励函数。", "result": "在MathVista迷你测试和Math-V基准测试中，Qwen-VL-DP在准确性和生成多样性方面显著优于之前的多模态大语言模型。", "conclusion": "实验表明，在数学推理中整合多样化的视角和反思对于提高准确性和创造性生成是非常重要的。"}}
{"id": "2507.02349", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02349", "abs": "https://arxiv.org/abs/2507.02349", "authors": ["Rafic Nader", "Vincent L'Allinec", "Romain Bourcier", "Florent Autrusseau"], "title": "Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection", "comment": null, "summary": "Intracranial aneurysms (ICA) commonly occur in specific segments of the\nCircle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.\nAn accurate detection of these critical landmarks is necessary for a prompt and\nefficient diagnosis. We introduce a fully automated landmark detection approach\nfor CoW bifurcations using a two-step neural networks process. Initially, an\nobject detection network identifies regions of interest (ROIs) proximal to the\nlandmark locations. Subsequently, a modified U-Net with deep supervision is\nexploited to accurately locate the bifurcations. This two-step method reduces\nvarious problems, such as the missed detections caused by two landmarks being\nclose to each other and having similar visual characteristics, especially when\nprocessing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for\nthe anatomical variability of the CoW, which affects the number of detectable\nlandmarks per scan. We assessed the effectiveness of our approach using two\ncerebral MRA datasets: our In-House dataset which had varying numbers of\nlandmarks, and a public dataset with standardized landmark configuration. Our\nexperimental results demonstrate that our method achieves the highest level of\nperformance on a bifurcation detection task.", "AI": {"tldr": "Developed and validated a two-step neural network for automated detection of bifurcations in the Circle of Willis, achieving superior results in aneurysm diagnosis.", "motivation": "To accurately detect critical landmarks of intracranial aneurysms (ICA) in the Circle of Willis (CoW) for prompt and efficient diagnosis.", "method": "We introduce a two-step neural network approach using an object detection network to identify ROIs and a modified U-Net with deep supervision for accurate bifurcation localization.", "result": "Our method achieves the highest level of performance in bifurcation detection, outperforming existing approaches.", "conclusion": "The proposed two-step neural network approach effectively addresses challenges such as anatomical variability and landmark proximity, improving the accuracy and reliability of bifurcation detection."}}
{"id": "2507.02822", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02822", "abs": "https://arxiv.org/abs/2507.02822", "authors": ["Wencheng Zhang", "Shiqin Qiao", "Lingjie Luo", "Yinfeng Li", "Chuanyang Zheng", "Qian Xu", "Meng Li", "Yong Gui", "Yijun He", "Jianing Qiu", "Jindong Hong", "Jiankai Sun"], "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model", "comment": null, "summary": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost.", "AI": {"tldr": "研究展示了动态地将医疗问题查询分配到适合的模式以优化准确性和成本效益的方法，提出并验证了SynapseRoute框架。", "motivation": "随着具有推理能力的大型语言模型（LLMs）的广泛应用，合理选择模型不仅要平衡性能，还需兼顾操作成本。研究表明，大约58%的医疗问题可以通过“非思考”模式准确回答，而不需要耗时的推理过程，这表明根据问题复杂性动态分配查询模式可以优化准度、成本效率和用户体验。", "method": "提出了SynapseRoute，这是一个基于机器学习的动态路由框架，能够智能地将输入查询分配到“思考”或“非思考”模式。", "result": "在几个医疗数据集上的实验结果表明，SynapseRoute不仅提高了整体准确度（0.8390对0.8272），还减少了推理时间36.8%和令牌使用39.66%。", "conclusion": "该工作进一步引入了准确度-推理-令牌（AIT）指数来全面评估准确度、延迟和令牌成本之间的权衡。"}}
{"id": "2507.02354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02354", "abs": "https://arxiv.org/abs/2507.02354", "authors": ["Fei Yuhuan", "Wang Gengchen", "Liu Fenghao", "Zang Ran", "Sun Xufei", "Chang Hao"], "title": "Lightweight Shrimp Disease Detection Research Based on YOLOv8n", "comment": "in Chinese language", "summary": "Shrimp diseases are one of the primary causes of economic losses in shrimp\naquaculture. To prevent disease transmission and enhance intelligent detection\nefficiency in shrimp farming, this paper proposes a lightweight network\narchitecture based on YOLOv8n. First, by designing the RLDD detection head and\nC2f-EMCM module, the model reduces computational complexity while maintaining\ndetection accuracy, improving computational efficiency. Subsequently, an\nimproved SegNext_Attention self-attention mechanism is introduced to further\nenhance the model's feature extraction capability, enabling more precise\nidentification of disease characteristics. Extensive experiments, including\nablation studies and comparative evaluations, are conducted on a\nself-constructed shrimp disease dataset, with generalization tests extended to\nthe URPC2020 dataset. Results demonstrate that the proposed model achieves a\n32.3% reduction in parameters compared to the original YOLOv8n, with a mAP@0.5\nof 92.7% (3% improvement over YOLOv8n). Additionally, the model outperforms\nother lightweight YOLO-series models in mAP@0.5, parameter count, and model\nsize. Generalization experiments on the URPC2020 dataset further validate the\nmodel's robustness, showing a 4.1% increase in mAP@0.5 compared to YOLOv8n. The\nproposed method achieves an optimal balance between accuracy and efficiency,\nproviding reliable technical support for intelligent disease detection in\nshrimp aquaculture.", "AI": {"tldr": "本文提出了一种基于YOLOv8n轻量化网络模型，通过引入RLDD检测头和C2f-EMCM模块，提高了计算效率并减少了计算复杂度，同时引入改进的SegNext_Attention自注意力机制，增强了模型特征提取能力。实验结果表明，所提模型相比原文模型参数减少32.3%，且在mAP@0.5指标上提高了3%。同时在模型大小和参数数量上也优于其他轻量级YOLO模型，在泛化性测试中也有良好表现。", "motivation": "鉴于对虾养殖中疾病造成的经济损失，为了提高智能检测效率和减少疾病传播，提出了一种新的基于YOLOv8n的轻量化网络模型。", "method": "该论文通过设计RLDD检测头和C2f-EMCM模块来优化模型，同时引入了改进的SegNext_Attention自注意力机制。", "result": "模型相比原版本YOLOv8n，参数减少了32.3%，mAP@0.5提升了3%，并在其他轻量级YOLO模型中表现出更优异的mAP@0.5指标、参数数量和模型大小，且在泛化性测试中也有显著的mAP@0.5提升，达到4.1%。", "conclusion": "该模型实现了准确性和效率之间的最优平衡，为对虾养殖的智能疾病检测提供了可靠的技术支持。"}}
{"id": "2507.02833", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02833", "abs": "https://arxiv.org/abs/2507.02833", "authors": ["Valentina Pyatkin", "Saumya Malik", "Victoria Graf", "Hamish Ivison", "Shengyi Huang", "Pradeep Dasigi", "Nathan Lambert", "Hannaneh Hajishirzi"], "title": "Generalizing Verifiable Instruction Following", "comment": "11 pages", "summary": "A crucial factor for successful human and AI interaction is the ability of\nlanguage models or chatbots to follow human instructions precisely. A common\nfeature of instructions are output constraints like ``only answer with yes or\nno\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to\ncraft a more useful answer. Even today's strongest models struggle with\nfulfilling such constraints. We find that most models strongly overfit on a\nsmall set of verifiable constraints from the benchmarks that test these\nabilities, a skill called precise instruction following, and are not able to\ngeneralize well to unseen output constraints. We introduce a new benchmark,\nIFBench, to evaluate precise instruction following generalization on 58 new,\ndiverse, and challenging verifiable out-of-domain constraints. In addition, we\nperform an extensive analysis of how and on what data models can be trained to\nimprove precise instruction following generalization. Specifically, we\ncarefully design constraint verification modules and show that reinforcement\nlearning with verifiable rewards (RLVR) significantly improves instruction\nfollowing. In addition to IFBench, we release 29 additional new hand-annotated\ntraining constraints and verification functions, RLVR training prompts, and\ncode.", "AI": {"tldr": "本文提出了一个新的评估基准IFBench以及RLVR方法以改进语言模型对于用户指定的精确输出约束的遵循能力，解决当前模型在小规模基准测试上的过拟合和泛化性能差的问题。", "motivation": "研究的主要动机在于解决现有模型在遵循用户指定的输出约束（如特定回答方式或关键词的重复次数）方面普遍存在的问题，以及这些模型在小规模基准测试上的过拟合现象。", "method": "研究者提出了一个新的基准测试IFBench，用于评估模型在处理58种新的、多样化的且具有挑战性的输出约束时的能力。他们通过强化学习与可验证奖励（RLVR）的方法改进了模型对精确指令的理解，并提供了额外的手动标注的训练约束条件和验证函数，以及相关的训练提示和代码。", "result": "研究结果表明，当前最强的语言模型在处理来自基准测试的验证约束上存在较强的过拟合现象，使得它们无法很好地泛化到未见过的约束上。研究所提出的方法能显著改善这一情况。", "conclusion": "研究显示，通过使用新的基准测试IFBench及所设计的约束验证模块与RLVR方法，模型在面对多样化的输出约束时，其精确指令跟随能力得到了显著改善。"}}
{"id": "2507.02358", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02358", "abs": "https://arxiv.org/abs/2507.02358", "authors": ["Anlin Zheng", "Haochen Wang", "Yucheng Zhao", "Weipeng Deng", "Tiancai Wang", "Xiangyu Zhang", "Xiaojuan Qi"], "title": "Holistic Tokenizer for Autoregressive Image Generation", "comment": "17 pages, 10 figures", "summary": "The vanilla autoregressive image generation model generates visual tokens in\na step-by-step fashion, which limits the ability to capture holistic\nrelationships among token sequences. Moreover, most visual tokenizers map local\nimage patches into latent tokens, leading to limited global information. To\naddress this, we introduce \\textit{Hita}, a novel image tokenizer for\nautoregressive (AR) image generation. It introduces a holistic-to-local\ntokenization scheme with learnable holistic queries and local patch tokens.\nBesides, Hita incorporates two key strategies for improved alignment with the\nAR generation process: 1) it arranges a sequential structure with holistic\ntokens at the beginning followed by patch-level tokens while using causal\nattention to maintain awareness of previous tokens; and 2) before feeding the\nde-quantized tokens into the decoder, Hita adopts a lightweight fusion module\nto control information flow to prioritize holistic tokens. Extensive\nexperiments show that Hita accelerates the training speed of AR generators and\noutperforms those trained with vanilla tokenizers, achieving \\textbf{2.59 FID}\nand \\textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the\nholistic representation highlights its ability to capture global image\nproperties such as textures, materials, and shapes. Additionally, Hita also\ndemonstrates effectiveness in zero-shot style transfer and image in-painting.\nThe code is available at\n\\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}", "AI": {"tldr": "Hita, an innovative image tokenizer, accelerates training speed and outperforms vanilla tokenizers in autoregressive image generation, achieving significant improvements in FID and IS scores on ImageNet.", "motivation": "To overcome the limitations of vanilla autoregressive models in capturing holistic relationships among token sequences and the lack of global information in most visual tokenizers.", "method": "Hita, a novel image tokenizer for autoregressive (AR) image generation that uses a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens, and a sequential structure with causal attention to maintain alignment with the AR generation process.", "result": "Hita achieves 2.59 FID and 281.9 IS on ImageNet, outperforming vanilla tokenizers, and demonstrates effectiveness in zero-shot style transfer and image in-painting.", "conclusion": "Hita's holistic representation allows it to effectively capture global image properties, making it a promising approach for autoregressive image generation, with open-source code for further exploration and use."}}
{"id": "2507.02850", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02850", "abs": "https://arxiv.org/abs/2507.02850", "authors": ["Almog Hilel", "Idan Shenfeld", "Leshem Choshen", "Jacob Andreas"], "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users", "comment": null, "summary": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection).", "AI": {"tldr": "论文展示了攻击者如何利用用户反馈改变语言模型的知识和行为，以引入虚假信息或安全漏洞。", "motivation": "动机是揭示了语言模型在偏好调优过程中的新特征，并展示了一种新的针对使用用户反馈训练的语言模型的攻击方式。", "method": "描述了一种针对使用用户反馈训练的语言模型（LM）的漏洞利用方法。攻击者通过提供提示和对LM输出进行点赞或点差反馈来持续改变LM的知识和行为。", "result": "研究结果显示攻击者可以(1)插入模型之前不具有的事实知识，(2)以引入可利用的安全漏洞的方式改变代码生成模式，(3)注入虚假财经新闻。", "conclusion": "此发现确定了语言模型偏好调优的新特点，并且提出了针对用户反馈训练的语言模型的新攻击手段，即使用小范围的偏好数据能够对模型行为施加细粒度控制。"}}
{"id": "2507.02363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02363", "abs": "https://arxiv.org/abs/2507.02363", "authors": ["Jiahao Wu", "Rui Peng", "Jianbo Jiao", "Jiayu Yang", "Luyang Tang", "Kaiqiang Xiong", "Jie Liang", "Jinbo Yan", "Runling Liu", "Ronggang Wang"], "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling", "comment": "Accepted by ICCV 2025", "summary": "Due to the complex and highly dynamic motions in the real world, synthesizing\ndynamic videos from multi-view inputs for arbitrary viewpoints is challenging.\nPrevious works based on neural radiance field or 3D Gaussian splatting are\nlimited to modeling fine-scale motion, greatly restricting their application.\nIn this paper, we introduce LocalDyGS, which consists of two parts to adapt our\nmethod to both large-scale and fine-scale motion scenes: 1) We decompose a\ncomplex dynamic scene into streamlined local spaces defined by seeds, enabling\nglobal modeling by capturing motion within each local space. 2) We decouple\nstatic and dynamic features for local space motion modeling. A static feature\nshared across time steps captures static information, while a dynamic residual\nfield provides time-specific features. These are combined and decoded to\ngenerate Temporal Gaussians, modeling motion within each local space. As a\nresult, we propose a novel dynamic scene reconstruction framework to model\nhighly dynamic real-world scenes more realistically. Our method not only\ndemonstrates competitive performance on various fine-scale datasets compared to\nstate-of-the-art (SOTA) methods, but also represents the first attempt to model\nlarger and more complex highly dynamic scenes. Project page:\nhttps://wujh2001.github.io/LocalDyGS/.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.02851", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.02851", "abs": "https://arxiv.org/abs/2507.02851", "authors": ["Purbesh Mitra", "Sennur Ulukus"], "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs", "comment": null, "summary": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.", "AI": {"tldr": "本文提出了MOTIF，这是一种通过强化学习分批优化生成思考标记的方法，解决了大语言模型（LLM）在处理超出其上下文限制的推理时的问题，相比传统的GRPO方法，该方法在GSM8K数据集上进行训练后，对MATH500和AIME2024基准的测试准确率分别提高了3.8%和3.3%，并展示了样本效率的优势。", "motivation": "大语言模型在推理过程中需要使用更多的思考标记来生成更好的响应，但是受到上下文限制的制约，迫切需要一种方法来分批生成思考标记从而拓宽模型的推理能力。", "method": "作者提出了名为MOTIF的强化学习微调方法，该方法通过模块化思想策略分批生成思考标记，进而扩展了模型的上下文处理能力。", "result": "在GSM8K数据集上进行微调训练后，所提出的MOTIF方法在MATH500和AIME2024基准测试中分别实现了3.8%和3.3%的性能提升，并且该改进是在使用少于15%的样本量的情况下得到的。", "conclusion": "MOTIF作为强化学习训练方法能够帮助大型语言模型处理超出传统上下文限制的推理任务，通过分批生成思考标记的方式提高了训练效果和样本效率，展示出了更好的性能提升。"}}
{"id": "2507.02373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02373", "abs": "https://arxiv.org/abs/2507.02373", "authors": ["Xizhe Xue", "Yang Zhou", "Dawei Yan", "Ying Li", "Haokui Zhang", "Rong Xiao"], "title": "UVLM: Benchmarking Video Language Model for Underwater World Understanding", "comment": "13 pages, 4 figures, 3 tables", "summary": "Recently, the remarkable success of large language models (LLMs) has achieved\na profound impact on the field of artificial intelligence. Numerous advanced\nworks based on LLMs have been proposed and applied in various scenarios. Among\nthem, video language models (VidLMs) are particularly widely used. However,\nexisting works primarily focus on terrestrial scenarios, overlooking the highly\ndemanding application needs of underwater observation. To overcome this gap, we\nintroduce UVLM, an under water observation benchmark which is build through a\ncollaborative approach combining human expertise and AI models. To ensure data\nquality, we have conducted in-depth considerations from multiple perspectives.\nFirst, to address the unique challenges of underwater environments, we selected\nvideos that represent typical underwater challenges including light variations,\nwater turbidity, and diverse viewing angles to construct the dataset. Second,\nto ensure data diversity, the dataset covers a wide range of frame rates,\nresolutions, 419 classes of marine animals, and various static plants and\nterrains. Next, for task diversity, we adopted a structured design where\nobservation targets are categorized into two major classes: biological and\nenvironmental. Each category includes content observation and change/action\nobservation, totaling 20 distinct task types. Finally, we designed several\nchallenging evaluation metrics to enable quantitative comparison and analysis\nof different methods. Experiments on two representative VidLMs demonstrate that\nfine-tuning VidLMs on UVLM significantly improves underwater world\nunderstanding while also showing potential for slight improvements on existing\nin-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and\nprompt engineering will be released publicly.", "AI": {"tldr": "This paper presents UVLM, an underwater observation benchmark created from a collaborative approach combining human expertise and AI models, demonstrating improved underwater understanding compared to generic VidLMs.", "motivation": "To fill the gap in existing works that primarily focus on terrestrial scenarios, this paper aims to address the high demand application needs of underwater observation.", "method": "Recently, the remarkable success of large language models (LLMs) has inspired the development of underwater video language models (VidLMs). The authors introduce UVLM, an underwater observation benchmark specifically designed for underwater scenarios. The dataset and prompt engineering will be publicly released.", "result": "Experiments on two representative VidLMs show that fine-tuning VidLMs on UVLM significantly enhances underwater world understanding and slightly improves performance on existing in-air VidLM benchmarks.", "conclusion": "The UVLM benchmark, with its detailed underwater challenges and diverse task design, is a valuable resource for advancing the understanding of underwater observation using VidLMs."}}
{"id": "2507.02856", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02856", "abs": "https://arxiv.org/abs/2507.02856", "authors": ["Nikhil Chandak", "Shashwat Goel", "Ameya Prabhu", "Moritz Hardt", "Jonas Geiping"], "title": "Answer Matching Outperforms Multiple Choice for Language Model Evaluation", "comment": "34 pages, Code is available at\n  https://github.com/nikhilchandak/answer-matching", "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.", "AI": {"tldr": "研究提出了一种新的评估方式——答案匹配，能够有效评估语言模型的自由回答，避免了多选题评估中的捷径问题，并且显示出比多选题评估更好的一致性。", "motivation": "克服多选题评估中存在的捷径问题，寻找一种有效且可扩展的替代方案，改善语言模型的评估方式。", "method": "通过一种称为答案匹配的生成评估方法来评估模型：给候选模型提供没有选项的问题，让其生成自由形式的回答，然后使用现代语言模型和参考答案来判断回答是否匹配参考答案。", "result": "使用答案匹配方法进行评估，即便是使用较小的现代语言模型，也能够达成接近完美的评估一致性，这与多选题评估或没有参考答案的LLM评估的效果相比有了显著改进。", "conclusion": "作者建议在评估生态系统中从多选题评估向答案匹配评估迁移，并且指出这种评估方式的改进不仅是理论上的，还会改变多个模型的排名。"}}
{"id": "2507.02393", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02393", "abs": "https://arxiv.org/abs/2507.02393", "authors": ["Seokyeong Lee", "Sithu Aung", "Junyong Choi", "Seungryong Kim", "Ig-Jae Kim", "Junghyun Cho"], "title": "PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection", "comment": "18 pages, 16 figures", "summary": "Monocular 3D object detection (M3OD) has long faced challenges due to data\nscarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.\nAlthough various weakly supervised methods and pseudo-labeling methods have\nbeen proposed to address these issues, they are mostly limited by\ndomain-specific learning or rely solely on shape information from a single\nobservation. In this paper, we propose a novel pseudo-labeling framework that\nuses only video data and is more robust to occlusion, without requiring a\nmulti-view setup, additional sensors, camera poses, or domain-specific\ntraining. Specifically, we explore a technique for aggregating the\npseudo-LiDARs of both static and dynamic objects across temporally adjacent\nframes using object point tracking, enabling 3D attribute extraction in\nscenarios where 3D data acquisition is infeasible. Extensive experiments\ndemonstrate that our method ensures reliable accuracy and strong scalability,\nmaking it a practical and effective solution for M3OD.", "AI": {"tldr": "本研究旨在解决单目3D目标检测中的数据稀缺和2D到3D不确定性挑战，提出了一种基于视频数据和伪LiDAR聚合的新伪标签框架，增强遮挡鲁棒性和3D属性提取能力。", "motivation": "单目3D目标检测长期面临由于标注成本高和内在2D到3D不确定性而造成的数据稀缺问题。尽管已经提出了各种弱监督方法和伪标签方法来解决这些问题，但它们大多受特定领域学习的限制，或者仅仅依赖于单次观察的形状信息。", "method": "本研究提出了一种新颖的伪标签框架，仅使用视频数据，并且对遮挡具有更强的鲁棒性，无需多视角设置、额外传感器、相机姿态或特定领域的训练。具体而言，该方法利用对象点追踪技术，在时间相邻帧上聚合静态和动态对象的伪LiDAR数据，以便在难以获取3D数据的场景中提取3D属性。", "result": "广泛的实验表明，该方法在单目3D目标检测中能够确保可靠的精度和强可扩展性。", "conclusion": "实验证明，该方法能够在缺乏3D数据获取的情况下，确保可靠的精度和强大的可扩展性，成为单目3D目标检测（M3OD）的一种实用且有效的方法。"}}
{"id": "2507.02395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02395", "abs": "https://arxiv.org/abs/2507.02395", "authors": ["Byung Hyun Lee", "Wongi Jeong", "Woojae Han", "Kyoungbun Lee", "Se Young Chun"], "title": "Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis", "comment": "Accepted at ICCV 2025", "summary": "Multiple instance learning (MIL) significantly reduced annotation costs via\nbag-level weak labels for large-scale images, such as histopathological whole\nslide images (WSIs). However, its adaptability to continual tasks with minimal\nforgetting has been rarely explored, especially on instance classification for\nlocalization. Weakly incremental learning for semantic segmentation has been\nstudied for continual localization, but it focused on natural images,\nleveraging global relationships among hundreds of small patches (e.g., $16\n\\times 16$) using pre-trained models. This approach seems infeasible for MIL\nlocalization due to enormous amounts ($\\sim 10^5$) of large patches (e.g., $256\n\\times 256$) and no available global relationships such as cancer cells. To\naddress these challenges, we propose Continual Multiple Instance Learning with\nEnhanced Localization (CoMEL), an MIL framework for both localization and\nadaptability with minimal forgetting. CoMEL consists of (1) Grouped Double\nAttention Transformer (GDAT) for efficient instance encoding, (2) Bag\nPrototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling,\nand (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting\nin both bag and instance classification. Extensive experiments on three public\nWSI datasets demonstrate superior performance of CoMEL, outperforming the prior\narts by up to $11.00\\%$ in bag-level accuracy and up to $23.4\\%$ in\nlocalization accuracy under the continual MIL setup.", "AI": {"tldr": "本文提出了用于本地化和最小遗忘适应的持续多实例学习框架CoMEL，在三个WSI数据集上实验表明优于先前工作。", "motivation": "虽然多实例学习在大型图像上通过弱标签显著降低了注释成本，但其对持续性任务中最小遗忘问题的适应性却鲜有研究，特别是在实例分类本地化上。此外，已经研究的弱增量学习语义分割针对的是自然图像，这种方法对于多实例学习的本地化可能存在挑战。", "method": "CoMEL框架包括：(1) Grouped Double Attention Transformer (GDAT)用于高效的实例编码，(2) Bag Prototypes-based Pseudo-Labeling (BPPL)用于可靠实例伪标签，(3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA)用于减轻在包和实例分类中的遗忘问题。", "result": "在三个公开的WSI数据集上的广泛实验表明，CoMEL在持续多实例学习设置下的表现优于先前的工作，最高提升了11.00%的包级准确率和23.4%的本地化准确率。", "conclusion": "CoMEL框架，在不牺牲性能的情况下有效地解决了多实例学习在持续性任务中最小遗忘的问题，并且在包级准确率和定位准确率上都有显著提升。"}}
{"id": "2507.02398", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02398", "abs": "https://arxiv.org/abs/2507.02398", "authors": ["Taehoon Kim", "Jongwook Choi", "Yonghyun Jeong", "Haeun Noh", "Jaejun Yoo", "Seungryul Baek", "Jongwon Choi"], "title": "Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection", "comment": "accepted by iccv 2025. code is will be available at\n  https://github.com/rama0126/PwTF-DVD", "summary": "We introduce a deepfake video detection approach that exploits pixel-wise\ntemporal inconsistencies, which traditional spatial frequency-based detectors\noften overlook. Traditional detectors represent temporal information merely by\nstacking spatial frequency spectra across frames, resulting in the failure to\ndetect temporal artifacts in the pixel plane. Our approach performs a 1D\nFourier transform on the time axis for each pixel, extracting features highly\nsensitive to temporal inconsistencies, especially in areas prone to unnatural\nmovements. To precisely locate regions containing the temporal artifacts, we\nintroduce an attention proposal module trained in an end-to-end manner.\nAdditionally, our joint transformer module effectively integrates pixel-wise\ntemporal frequency features with spatio-temporal context features, expanding\nthe range of detectable forgery artifacts. Our framework represents a\nsignificant advancement in deepfake video detection, providing robust\nperformance across diverse and challenging detection scenarios.", "AI": {"tldr": "提出了一种新的深度伪造视频检测方法，通过利用像素级时间不一致性，达到精确定位和检测伪造伪影的目的。这种方法显著提升了在复杂背景下的伪造视频检测性能。", "motivation": "传统的基于空间频率的检测器往往忽略像素级的时间不一致性，因为它们仅通过堆叠帧间空间频率光谱来表示时间信息，导致无法检测像素级别的伪影。本研究的动机是改进这种检测方法，以识别这些传统检测器无法捕捉到的时间伪影。", "method": "提出了一种基于像素级时间不一致性的深度伪造视频检测方法，这种方法利用了一维傅里叶变换时间轴上每个像素的特征，特别关注不自然运动区域。还引入了一个注意力提案模块来精确定位包含时间伪影的区域。最后，联合变压器模块整合了像素级时间频率特征和时空上下文特征，扩大了检测伪造伪影的范围。", "result": "该框架在多样化的、具有挑战性的检测场景中展示了强大的性能，代表了深度伪造视频检测的重大进步。", "conclusion": "本研究提出了利用像素级时间不一致性特征进行深度伪造视频检测的新方法，这在多样的检测场景中展示了强劲的性能，为深度伪造视频检测技术带来重大进展。"}}
{"id": "2507.02790", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02790", "abs": "https://arxiv.org/abs/2507.02790", "authors": ["Xiangfeng Wang", "Xiao Li", "Yadong Wei", "Xueyu Song", "Yang Song", "Xiaoqiang Xia", "Fangrui Zeng", "Zaiyi Chen", "Liu Liu", "Gu Xu", "Tong Xu"], "title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding", "comment": null, "summary": "The rapid growth of online video content, especially on short video\nplatforms, has created a growing demand for efficient video editing techniques\nthat can condense long-form videos into concise and engaging clips. Existing\nautomatic editing methods predominantly rely on textual cues from ASR\ntranscripts and end-to-end segment selection, often neglecting the rich visual\ncontext and leading to incoherent outputs. In this paper, we propose a\nhuman-inspired automatic video editing framework (HIVE) that leverages\nmultimodal narrative understanding to address these limitations. Our approach\nincorporates character extraction, dialogue analysis, and narrative\nsummarization through multimodal large language models, enabling a holistic\nunderstanding of the video content. To further enhance coherence, we apply\nscene-level segmentation and decompose the editing process into three subtasks:\nhighlight detection, opening/ending selection, and pruning of irrelevant\ncontent. To facilitate research in this area, we introduce DramaAD, a novel\nbenchmark dataset comprising over 800 short drama episodes and 500\nprofessionally edited advertisement clips. Experimental results demonstrate\nthat our framework consistently outperforms existing baselines across both\ngeneral and advertisement-oriented editing tasks, significantly narrowing the\nquality gap between automatic and human-edited videos.", "AI": {"tldr": "本文提出了一种基于多模态叙事理解的人类启发式自动视频编辑框架（HIVE），通过引入DramaAD数据集来改进现有自动编辑方法，提高了生成视频的连贯性和质量。", "motivation": "随着在线视频内容特别是短视频平台内容的快速增长，现有的自动编辑方法主要依赖于ASR转录的文本线索和端到端片段选择，忽略了丰富的视觉上下文，导致输出不连贯。", "method": "我们的方法（HIVE）通过结合字符提取、对话分析和叙述总结，利用多模态大型语言模型来实现对视频内容的整体理解。为了进一步提高连贯性，我们采用场景级分割，并将编辑过程分解为三个子任务：高光检测、开头/结尾选择和无关内容的筛选。", "result": "实验结果显示，我们的框架在通用和广告导向的编辑任务中都一直优于现有基线，显著缩小了自动编辑与人工编辑视频之间的质量差距。", "conclusion": "我们的研究表明，通过结合多模态理解和场景级编辑策略，可以在自动视频编辑任务上取得显著的进步。"}}
{"id": "2507.02399", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02399", "abs": "https://arxiv.org/abs/2507.02399", "authors": ["Peilin Zhang", "Shaouxan Wua", "Jun Feng", "Zhuo Jin", "Zhizezhang Gao", "Jingkun Chen", "Yaqiong Xing", "Xiao Zhang"], "title": "TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation", "comment": null, "summary": "Background and objective: Medical image segmentation is a core task in\nvarious clinical applications. However, acquiring large-scale, fully annotated\nmedical image datasets is both time-consuming and costly. Scribble annotations,\nas a form of sparse labeling, provide an efficient and cost-effective\nalternative for medical image segmentation. However, the sparsity of scribble\nannotations limits the feature learning of the target region and lacks\nsufficient boundary supervision, which poses significant challenges for\ntraining segmentation networks. Methods: We propose TAB Net, a novel\nweakly-supervised medical image segmentation framework, consisting of two key\ncomponents: the triplet augmentation self-recovery (TAS) module and the\nboundary-aware pseudo-label supervision (BAP) module. The TAS module enhances\nfeature learning through three complementary augmentation strategies: intensity\ntransformation improves the model's sensitivity to texture and contrast\nvariations, cutout forces the network to capture local anatomical structures by\nmasking key regions, and jigsaw augmentation strengthens the modeling of global\nanatomical layout by disrupting spatial continuity. By guiding the network to\nrecover complete masks from diverse augmented inputs, TAS promotes a deeper\nsemantic understanding of medical images under sparse supervision. The BAP\nmodule enhances pseudo-supervision accuracy and boundary modeling by fusing\ndual-branch predictions into a loss-weighted pseudo-label and introducing a\nboundary-aware loss for fine-grained contour refinement. Results: Experimental\nevaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB\nNet significantly outperforms state-of-the-art methods for scribble-based\nweakly supervised segmentation. Moreover, it achieves performance comparable to\nthat of fully supervised methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.02844", "categories": ["cs.CV", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.02844", "abs": "https://arxiv.org/abs/2507.02844", "authors": ["Ziqi Miao", "Yi Ding", "Lijun Li", "Jing Shao"], "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection", "comment": "16 pages", "summary": "With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack.", "AI": {"tldr": "本研究定义了一种新的视觉中心的破解模式，并提出名为'VisCo'（视觉情境）的攻击方法，通过利用视觉信息加强现实场景的建构，从而更有效地对多模态大型语言模型进行触发有害反应的攻击。", "motivation": "鉴于多模态大型语言模型(MMLMs)的广泛潜力及其在视觉模态中安全性方面的挑战，本篇论文旨在提出一种新的攻击策略，该策略充分利用视觉信息以更安全有效的方式对MMLMs进行攻击。特别关注的是解决现有方法中存在的视觉模态作为触发因素的语义模糊性问题，以及缺乏现实情境的挑战。", "method": "通过定义一种新颖的情境：视觉中心的突破，其中视觉信息是构建完整且现实的突破情境的必要组成部分。在此基础上，我们提出了VisCo（视觉情境）攻击。VisCo利用了四种不同的视觉聚焦策略来伪造情境对话，并在必要时动态生成辅助图像以构建视觉中心的突破情境。为了最大化攻击效果，VisCo还引入了自动毒性模糊和语义优化，以生成能够可靠触发目标黑盒MLLMs有害反应的最终攻击提示。", "result": "通过实验，我们验证了VisCo攻击的有效性，在MM-SafetyBench上针对GPT-4o模型，其毒性评分为4.78，攻击成功率为85%，显著优于基线方法的2.48毒性评分和22.2%的攻击成功率。", "conclusion": "研究结果表明，VisCo攻击方法能够显著提高攻击成功率和毒性评分，展示了通过优化视觉信息构建攻击情景在提升攻击效率方面的潜力，这为攻击多模态语言模型提供了一种全新的视角和手段。"}}
{"id": "2507.02403", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02403", "abs": "https://arxiv.org/abs/2507.02403", "authors": ["Mufhumudzi Muthivhi", "Terence L. van Zyl"], "title": "Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings", "comment": "Accepted for publication in IEEE Xplore and ISIF FUSION 2025\n  proceedings:", "summary": "Wildlife re-identification aims to match individuals of the same species\nacross different observations. Current state-of-the-art (SOTA) models rely on\nclass labels to train supervised models for individual classification. This\ndependence on annotated data has driven the curation of numerous large-scale\nwildlife datasets. This study investigates self-supervised learning\nSelf-Supervised Learning (SSL) for wildlife re-identification. We automatically\nextract two distinct views of an individual using temporal image pairs from\ncamera trap data without supervision. The image pairs train a self-supervised\nmodel from a potentially endless stream of video data. We evaluate the learnt\nrepresentations against supervised features on open-world scenarios and\ntransfer learning in various wildlife downstream tasks. The analysis of the\nexperimental results shows that self-supervised models are more robust even\nwith limited data. Moreover, self-supervised features outperform supervision\nacross all downstream tasks. The code is available here\nhttps://github.com/pxpana/SSLWildlife.", "AI": {"tldr": "本研究探索自我监督学习在野生动物再识别中的应用，实验表明自我监督方法在数据受限的情况下表现更好，并且在所有下游任务中超越了监督学习方法。", "motivation": "当前最先进的模型依赖于标注数据来训练用于个体分类的监督模型，而本研究旨在探索自我监督学习（SSL）在野生动物再识别中的应用，以减少对标注数据的依赖。", "method": "本研究通过自动提取来自相机陷阱数据的两幅图像对，无需监督即可学习个体的不同视图，从而训练一个自我监督模型。这种方法利用了潜在无限的数据流。", "result": "实验结果表明，自我监督模型在数据有限的情况下更具有鲁棒性，并且在所有下游任务中，自我监督特征的表现优于监督方法。", "conclusion": "研究表明自我监督学习方法在野生动物再识别任务中具有较大的应用潜力，可减少对标注数据的依赖，并且在各种下游任务中表现优异。"}}
{"id": "2507.02405", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02405", "abs": "https://arxiv.org/abs/2507.02405", "authors": ["Ayantika Das", "Moitreya Chaudhuri", "Koushik Bhat", "Keerthi Ram", "Mihail Bota", "Mohanasankar Sivaprakasam"], "title": "PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration", "comment": "Published in IEEE Journal of Biomedical and Health Informatics (Early\n  Access Available) https://ieeexplore.ieee.org/document/10989734", "summary": "Denoising diffusion models produce high-fidelity image samples by capturing\nthe image distribution in a progressive manner while initializing with a simple\ndistribution and compounding the distribution complexity. Although these models\nhave unlocked new applicabilities, the sampling mechanism of diffusion does not\noffer means to extract image-specific semantic representation, which is\ninherently provided by auto-encoders. The encoding component of auto-encoders\nenables mapping between a specific image and its latent space, thereby offering\nexplicit means of enforcing structures in the latent space. By integrating an\nencoder with the diffusion model, we establish an auto-encoding formulation,\nwhich learns image-specific representations and offers means to organize the\nlatent space. In this work, First, we devise a mechanism to structure the\nlatent space of a diffusion auto-encoding model, towards recognizing\nregion-specific cellular patterns in brain images. We enforce the\nrepresentations to regress positional information of the patches from\nhigh-resolution images. This creates a conducive latent space for\ndifferentiating tissue types of the brain. Second, we devise an unsupervised\ntear artifact restoration technique based on neighborhood awareness, utilizing\nlatent representations and the constrained generation capability of diffusion\nmodels during inference. Third, through representational guidance and\nleveraging the inference time steerable noising and denoising capability of\ndiffusion, we devise an unsupervised JPEG artifact restoration technique.", "AI": {"tldr": "该论文提出了一种结合扩散模型与自动编码器的方法，通过结构化潜在空间，实现大脑图像中特定区域细胞模式的识别以及无监督的 artifact 恢复技术。", "motivation": "扩散模型虽然能够生成高清度的图像，但不能直接提取出图像的语义表示，因此与自然提供的自动编码器结合可以解决这一问题。", "method": "通过将编码器与扩散模型结合，提出了一种自动编码器格式，该格式可以学习图像特定的表示，并提供手段来组织潜在空间。", "result": "该方法主要提出了三种技术：（1）结构化扩散自动编码模型的潜在空间，以识别大脑图像中的特定区域细胞模式；（2）基于邻域意识的无监督撕裂 artifact 恢复技术；（3）利用表征引导和扩散模型在推断时间可操纵的噪声和去噪能力的无监督 JPEG artifact 恢复技术。", "conclusion": "通过结合自动编码器与扩散模型的优点，该方法在学习图像特定表示和潜在空间组织方面获得了有效的成果。"}}
{"id": "2507.02408", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02408", "abs": "https://arxiv.org/abs/2507.02408", "authors": ["Duong Nguyen-Ngoc Tran", "Long Hoang Pham", "Chi Dai Tran", "Quoc Pham-Nam Ho", "Huy-Hung Nguyen", "Jae Wook Jeon"], "title": "A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern", "comment": null, "summary": "Multi-Object Tracking in thermal images is essential for surveillance\nsystems, particularly in challenging environments where RGB cameras struggle\ndue to low visibility or poor lighting conditions. Thermal sensors enhance\nrecognition tasks by capturing infrared signatures, but a major challenge is\ntheir low-level feature representation, which makes it difficult to accurately\ndetect and track pedestrians. To address this, the paper introduces a novel\ntuning method for pedestrian tracking, specifically designed to handle the\ncomplex motion patterns in thermal imagery. The proposed framework optimizes\ntwo-stages, ensuring that each stage is tuned with the most suitable\nhyperparameters to maximize tracking performance. By fine-tuning\nhyperparameters for real-time tracking, the method achieves high accuracy\nwithout relying on complex reidentification or motion models. Extensive\nexperiments on PBVS Thermal MOT dataset demonstrate that the approach is highly\neffective across various thermal camera conditions, making it a robust solution\nfor real-world surveillance applications.", "AI": {"tldr": "A novel tuning method for pedestrian tracking in thermal images optimizes two stages with suitable hyperparameters, achieving high accuracy in surveillance applications.", "motivation": "The motivation is to improve multi-object tracking in thermal images for surveillance systems, especially in challenging environments where traditional RGB cameras are less effective due to poor visibility.", "method": "The paper introduces a novel tuning method for pedestrian tracking in thermal images, which optimizes two stages by tuning hyperparameters to handle complex motion patterns and improve tracking accuracy.", "result": "Experiments on the PBVS Thermal MOT dataset show that the proposed method achieves high accuracy in various thermal camera conditions without the need for complex reidentification or motion models.", "conclusion": "The conclusion is that the proposed method is robust and effective for real-world surveillance applications, as it demonstrates high performance in tracking pedestrians in thermal imagery."}}
{"id": "2507.02414", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.02414", "abs": "https://arxiv.org/abs/2507.02414", "authors": ["Rundong Xin", "Taotao Wang", "Jin Wang", "Chonghe Zhao", "Jing Wang"], "title": "Privacy-preserving Preselection for Face Identification Based on Packing", "comment": "This paper has been accepted for publication in SecureComm 2025", "summary": "Face identification systems operating in the ciphertext domain have garnered\nsignificant attention due to increasing privacy concerns and the potential\nrecovery of original facial data. However, as the size of ciphertext template\nlibraries grows, the face retrieval process becomes progressively more\ntime-intensive. To address this challenge, we propose a novel and efficient\nscheme for face retrieval in the ciphertext domain, termed Privacy-Preserving\nPreselection for Face Identification Based on Packing (PFIP). PFIP incorporates\nan innovative preselection mechanism to reduce computational overhead and a\npacking module to enhance the flexibility of biometric systems during the\nenrollment stage. Extensive experiments conducted on the LFW and CASIA datasets\ndemonstrate that PFIP preserves the accuracy of the original face recognition\nmodel, achieving a 100% hit rate while retrieving 1,000 ciphertext face\ntemplates within 300 milliseconds. Compared to existing approaches, PFIP\nachieves a nearly 50x improvement in retrieval efficiency.", "AI": {"tldr": "A novel scheme, PFIP, is proposed to enhance the efficiency of face retrieval in the ciphertext domain for privacy-preserving face identification, achieving high accuracy and significant reduction in retrieval time.", "motivation": "The increasing concerns over privacy and the need to prevent the recovery of original facial data drive the research on privacy-preserving face identification systems which operate in the ciphertext domain.", "method": "PFIP utilizes an innovative preselection mechanism and a packing module in the enrollment stage to reduce computational burdens and enhance the flexibility of face identification systems in the ciphertext domain.", "result": "Experiments on LFW and CASIA datasets show that PFIP maintains a 100% hit rate while significantly improving retrieval efficiency by 50 times compared to existing methods.", "conclusion": "PFIP effectively improves the efficiency of face retrieval in the ciphertext domain without compromising the accuracy of face identification."}}
{"id": "2507.02416", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.02416", "abs": "https://arxiv.org/abs/2507.02416", "authors": ["Subhasis Dasgupta", "Jaydip Sen", "Tuhina Halder"], "title": "Determination Of Structural Cracks Using Deep Learning Frameworks", "comment": "This is the accepted version of the paper presented in IEEE CONIT\n  2025 held on 20th June 2025. This is not the camera-ready version. There are\n  6 pages in this paper and it contains 7 figures and 1 table", "summary": "Structural crack detection is a critical task for public safety as it helps\nin preventing potential structural failures that could endanger lives. Manual\ndetection by inexperienced personnel can be slow, inconsistent, and prone to\nhuman error, which may compromise the reliability of assessments. The current\nstudy addresses these challenges by introducing a novel deep-learning\narchitecture designed to enhance the accuracy and efficiency of structural\ncrack detection. In this research, various configurations of residual U-Net\nmodels were utilized. These models, due to their robustness in capturing fine\ndetails, were further integrated into an ensemble with a meta-model comprising\nconvolutional blocks. This unique combination aimed to boost prediction\nefficiency beyond what individual models could achieve. The ensemble's\nperformance was evaluated against well-established architectures such as SegNet\nand the traditional U-Net. Results demonstrated that the residual U-Net models\noutperformed their predecessors, particularly with low-resolution imagery, and\nthe ensemble model exceeded the performance of individual models, proving it as\nthe most effective. The assessment was based on the Intersection over Union\n(IoU) metric and DICE coefficient. The ensemble model achieved the highest\nscores, signifying superior accuracy. This advancement suggests way for more\nreliable automated systems in structural defects monitoring tasks.", "AI": {"tldr": "本文介绍了一种新的深度学习架构，通过集成残差U-Net模型和元模型来提高结构裂缝检测的性能，实验表明该方法具有更高的准确性和效率。", "motivation": "通过提高裂纹检测的可靠性和效率，研究旨在解决人工检测缓慢、不一致和容易出错的问题，从而提高公共安全。", "method": "本研究使用了多种残差U-Net模型配置，并将其与包含卷积块的元模型集成，以提高结构裂缝检测的准确性和效率。", "result": "实验结果表明，残差U-Net模型，特别是在低分辨率图像方面，优于SegNet和传统U-Net模型。集成模型取得了最高的IoU和DICE系数得分，显示了更高的准确性。", "conclusion": "集成模型的性能超越所有单独模型，表明在结构缺陷监控任务中实现更可靠的自动化系统的方向。"}}
{"id": "2507.02419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02419", "abs": "https://arxiv.org/abs/2507.02419", "authors": ["Yiming Zhong", "Xiaolin Zhang", "Ligang Liu", "Yao Zhao", "Yunchao Wei"], "title": "AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars", "comment": null, "summary": "Similar to facial beautification in real life, 3D virtual avatars require\npersonalized customization to enhance their visual appeal, yet this area\nremains insufficiently explored. Although current 3D Gaussian editing methods\ncan be adapted for facial makeup purposes, these methods fail to meet the\nfundamental requirements for achieving realistic makeup effects: 1) ensuring a\nconsistent appearance during drivable expressions, 2) preserving the identity\nthroughout the makeup process, and 3) enabling precise control over fine\ndetails. To address these, we propose a specialized 3D makeup method named\nAvatarMakeup, leveraging a pretrained diffusion model to transfer makeup\npatterns from a single reference photo of any individual. We adopt a\ncoarse-to-fine idea to first maintain the consistent appearance and identity,\nand then to refine the details. In particular, the diffusion model is employed\nto generate makeup images as supervision. Due to the uncertainties in diffusion\nprocess, the generated images are inconsistent across different viewpoints and\nexpressions. Therefore, we propose a Coherent Duplication method to coarsely\napply makeup to the target while ensuring consistency across dynamic and\nmultiview effects. Coherent Duplication optimizes a global UV map by recoding\nthe averaged facial attributes among the generated makeup images. By querying\nthe global UV map, it easily synthesizes coherent makeup guidance from\narbitrary views and expressions to optimize the target avatar. Given the coarse\nmakeup avatar, we further enhance the makeup by incorporating a Refinement\nModule into the diffusion model to achieve high makeup quality. Experiments\ndemonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality\nand consistency throughout animation.", "AI": {"tldr": "We introduce AvatarMakeup, a method for applying realistic 3D makeup to virtual avatars using a diffusion model and a coarse-to-fine approach, which outperforms existing techniques.", "motivation": "Addressing the challenges in achieving realistic makeup effects for 3D virtual avatars, such as maintaining consistency across expressions, preserving identity, and controlling fine details, while using currently available Gaussian editing methods which fall short.", "method": "We propose AvatarMakeup, a specialized 3D makeup method that uses a pretrained diffusion model to transfer makeup patterns from a single reference photo. The method employs a coarse-to-fine strategy, first ensuring consistent appearance and identity with the Coherent Duplication method and then refining the details with a refinement module.", "result": "AvatarMakeup achieves state-of-the-art makeup transfer quality and consistency throughout animation, overcoming limitations of previous methods.", "conclusion": "The proposed method, AvatarMakeup, successfully overcomes the fundamental challenges in 3D makeup, providing high-quality makeup transfer that is consistent across different expressions and multiple viewpoints."}}
{"id": "2507.02437", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.02437", "abs": "https://arxiv.org/abs/2507.02437", "authors": ["Wei Li", "Jingyang Zhang", "Lihao Liu", "Guoan Wang", "Junjun He", "Yang Chen", "Lixu Gu"], "title": "F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning", "comment": "This paper has been submitted to relevant journals", "summary": "Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a\nsource model to unseen medical sites using unlabeled test data, due to the high\ncost of data annotation. Existing TTA methods consider scenarios where data\nfrom one or multiple domains arrives in complete domain units. However, in\nclinical practice, data usually arrives in domain fragments of arbitrary\nlengths and in random arrival orders, due to resource constraints and patient\nvariability. This paper investigates a practical Free-Form Test-Time Adaptation\n(F$^{2}$TTA) task, where a source model is adapted to such free-form domain\nfragments, with shifts occurring between fragments unpredictably. In this\nsetting, these shifts could distort the adaptation process. To address this\nproblem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT)\nframework. I-DiPT employs an image-invariant prompt to explore domain-invariant\nrepresentations for mitigating the unpredictable shifts, and an image-specific\nprompt to adapt the source model to each test image from the incoming\nfragments. The prompts may suffer from insufficient knowledge representation\nsince only one image is available for training. To overcome this limitation, we\nfirst introduce Uncertainty-oriented Masking (UoM), which encourages the\nprompts to extract sufficient information from the incoming image via masked\nconsistency learning driven by the uncertainty of the source model\nrepresentations. Then, we further propose a Parallel Graph Distillation (PGD)\nmethod that reuses knowledge from historical image-specific and image-invariant\nprompts through parallel graph networks. Experiments on breast cancer and\nglaucoma classification demonstrate the superiority of our method over existing\nTTA approaches in F$^{2}$TTA. Code is available at\nhttps://github.com/mar-cry/F2TTA.", "AI": {"tldr": "本文提出了针对临床实践中自由形式领域片段适应问题的I-DiPT框架，实验证明其在F$^{2}$TTA任务上优于现有方法。", "motivation": "在临床实践中，数据以任意长度的领域片段随机到达，这限制了现有的Test-Time Adaptation方法的适用性。因此，提出了F$^{2}$TTA任务以及针对该任务的解决方案。", "method": "I-DiPT框架采用图像不变提示以探索领域不变表示，用以缓解不可预测的偏移，并采用图像特定提示以适应来自传入片段的每个测试图像。此外，提出不确定性导向遮罩（UoM）和并行图蒸馏（PGD）方法来克服提示知识表示不足的问题。", "result": "实验结果证明，所提方法在乳腺癌及青光眼分类任务上优于现有Test-Time Adaptation方法。", "conclusion": "通过引入图像不可变提示与图像特定提示，以及提出UoM和PGD方法，本文成功解决了临床数据自由形式传入的问题，提高了模型在F$^{2}$TTA任务上的性能。"}}
{"id": "2507.02443", "categories": ["cs.CV", "cs.AI", "cs.DC", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02443", "abs": "https://arxiv.org/abs/2507.02443", "authors": ["Sandro Costa Magalhães", "Marco Almeida", "Filipe Neves dos Santos", "António Paulo Moreira", "Jorge Dias"], "title": "Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic", "comment": "Submitted to ROBOT'2025", "summary": "Robots usually slow down for canning to detect objects while moving.\nAdditionally, the robot's camera is configured with a low framerate to track\nthe velocity of the detection algorithms. This would be constrained while\nexecuting tasks and exploring, making robots increase the task execution time.\nAMD has developed the Vitis-AI framework to deploy detection algorithms into\nFPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we\nuse the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit\nquantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation\n(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This\nis a self-acquired dataset released in open access. MobileNet v1 performed\nbetter, reaching a success rate of 98 % and an inference speed of 6611 FPS. In\nthis work, we proved that we can use FPGAs to speed up ANNs and make them\nsuitable for attention mechanisms.", "AI": {"tldr": "本研究通过在FPGA的PL中部署ANNs，解决了机器人在移动检测时遇到的速度问题，并展示了FPGA在加速ANNs方面的潜力。", "motivation": "由于机器人在移动过程中需要减速进行物体检测，并且低帧率的相机配置也影响了检测算法的速度，这在执行任务和探索时会受到限制，导致任务执行时间增加。现有的Vitis-AI框架在FPGA上的应用没有充分利用其PL部分。", "method": "本研究使用FINN架构在FPGA的PL中部署了三个ANNs，即4位量化的MobileNet v1、2位量化的CNV和1位量化的CNV（BNN），并且这些模型是在一个公开的数据集(RG2C)上进行训练的。", "result": "实验结果显示，MobileNet v1达到了98%的成功率和6611 FPS的推理速度。", "conclusion": "研究表明，FPGA可以加速ANNs，使其适用于注意力机制。"}}
{"id": "2507.02445", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.02445", "abs": "https://arxiv.org/abs/2507.02445", "authors": ["Hailong Yan", "Junjian Huang", "Tingwen Huang"], "title": "IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising", "comment": "Submitted to IEEE Transactions on Artificial Intelligence (TAI) on\n  Oct.31, 2024", "summary": "Current methods for restoring underexposed images typically rely on\nsupervised learning with paired underexposed and well-illuminated images.\nHowever, collecting such datasets is often impractical in real-world scenarios.\nMoreover, these methods can lead to over-enhancement, distorting\nwell-illuminated regions. To address these issues, we propose IGDNet, a\nZero-Shot enhancement method that operates solely on a single test image,\nwithout requiring guiding priors or training data. IGDNet exhibits strong\ngeneralization ability and effectively suppresses noise while restoring\nillumination. The framework comprises a decomposition module and a denoising\nmodule. The former separates the image into illumination and reflection\ncomponents via a dense connection network, while the latter enhances\nnon-uniformly illuminated regions using an illumination-guided pixel adaptive\ncorrection method. A noise pair is generated through downsampling and refined\niteratively to produce the final result. Extensive experiments on four public\ndatasets demonstrate that IGDNet significantly improves visual quality under\ncomplex lighting conditions. Quantitative results on metrics like PSNR\n(20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art\nunsupervised methods. The code will be released soon.", "AI": {"tldr": "提出新的图像去噪和照明恢复方法IGDNet，该方法不依赖训练数据，实验表明其在复杂光照下的性能优于现有方法。", "motivation": "现有的曝光不足图像恢复方法依赖监督学习且容易导致过度增强，本方法旨在解决这些问题。", "method": "IGDNet采用零样本方法，不需要引导先验或训练数据。该框架包括分解模块和去噪模块。分解模块使用密集连接网络将图像分离为照明和反射成分。去噪模块使用基于光照的像素自适应校正方法增强非均匀光照区域。通过降采样生成噪声对并迭代优化以生成最终结果。", "result": "实验结果表明IGDNet在复杂光照条件下显著提升视觉质量。在PSNR(20.41dB)和SSIM(0.860dB)等指标上优于14种最先进的无监督方法。", "conclusion": "IGDNet作为一种零样本方法，在不需要引导先验或训练数据的情况下，有效增强了非均匀照明区域并抑制了噪声，大幅提升了图像质量。"}}
{"id": "2507.02454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02454", "abs": "https://arxiv.org/abs/2507.02454", "authors": ["Weiwei Duan", "Luping Ji", "Shengjia Chen", "Sicheng Zhu", "Jianghong Huang", "Mao Ye"], "title": "Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection", "comment": null, "summary": "Different from general object detection, moving infrared small target\ndetection faces huge challenges due to tiny target size and weak background\ncontrast.Currently, most existing methods are fully-supervised, heavily relying\non a large number of manual target-wise annotations. However, manually\nannotating video sequences is often expensive and time-consuming, especially\nfor low-quality infrared frame images. Inspired by general object detection,\nnon-fully supervised strategies ($e.g.$, weakly supervised) are believed to be\npotential in reducing annotation requirements. To break through traditional\nfully-supervised frameworks, as the first exploration work, this paper proposes\na new weakly-supervised contrastive learning (WeCoL) scheme, only requires\nsimple target quantity prompts during model training.Specifically, in our\nscheme, based on the pretrained segment anything model (SAM), a potential\ntarget mining strategy is designed to integrate target activation maps and\nmulti-frame energy accumulation.Besides, contrastive learning is adopted to\nfurther improve the reliability of pseudo-labels, by calculating the similarity\nbetween positive and negative samples in feature subspace.Moreover, we propose\na long-short term motion-aware learning scheme to simultaneously model the\nlocal motion patterns and global motion trajectory of small targets.The\nextensive experiments on two public datasets (DAUB and ITSDT-15K) verify that\nour weakly-supervised scheme could often outperform early fully-supervised\nmethods. Even, its performance could reach over 90\\% of state-of-the-art (SOTA)\nfully-supervised ones.", "AI": {"tldr": "The paper introduces a weakly-supervised method for detecting small moving infrared targets that uses minimal manual annotations.", "motivation": "The paper aims to address the challenges of detecting small moving infrared targets with weak background contrast, particularly the high cost and time required for fully-supervised methods relying on manual annotations.", "method": "The paper proposes a new weakly-supervised contrastive learning (WeCoL) scheme that uses a pretrained segment anything model (SAM) and a potential target mining strategy to improve detection reliability.", "result": "The proposed scheme outperforms early fully-supervised methods and even reaches over 90% of state-of-the-art fully-supervised methods on public datasets.", "conclusion": "The proposed weakly-supervised contrastive learning scheme demonstrates superior performance and reduces the annotation requirements for small moving infrared target detection."}}
{"id": "2507.02477", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.02477", "abs": "https://arxiv.org/abs/2507.02477", "authors": ["Gaochao Song", "Zibo Zhao", "Haohan Weng", "Jingbo Zeng", "Rongfei Jia", "Shenghua Gao"], "title": "Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk", "comment": "9 pages main text, 14 pages appendix, 23 figures", "summary": "We introduce Mesh Silksong, a compact and efficient mesh representation\ntailored to generate the polygon mesh in an auto-regressive manner akin to silk\nweaving. Existing mesh tokenization methods always produce token sequences with\nrepeated vertex tokens, wasting the network capability. Therefore, our approach\ntokenizes mesh vertices by accessing each mesh vertice only once, reduces the\ntoken sequence's redundancy by 50\\%, and achieves a state-of-the-art\ncompression rate of approximately 22\\%. Furthermore, Mesh Silksong produces\npolygon meshes with superior geometric properties, including manifold topology,\nwatertight detection, and consistent face normals, which are critical for\npractical applications. Experimental results demonstrate the effectiveness of\nour approach, showcasing not only intricate mesh generation but also\nsignificantly improved geometric integrity.", "AI": {"tldr": "Mesh Silksong 提出了一种新的网格表示方法，它能有效地减少标记序列的冗余并提高生成几何网格的质量，展现了优于现有方法的效果。", "motivation": "旨在解决现有网格标记方法中存在的标记序列冗余度较高的问题，提高网格生成的效率和几何完整性。", "method": "通过仅访问每个网格顶点一次减少标记序列冗余度，并以类似于丝绸编织的自回归方式生成网格。", "result": "Mesh Silksong 是一种紧凑高效的网格表示方法，能够以类似于丝绸编织的自回归方式生成多边形网格。与现有方法不同，它通过仅访问每个网格顶点一次来标记顶点，减少了冗余，将标记序列的冗余度降低了50%，并实现了约22%的压缩率。此外，Mesh Silksong 生成的多边形网格具有流形拓扑、防水检测和一致的面法线等优越的几何属性，这些属性对于实际应用至关重要。实验结果表明，该方法不仅能生成复杂的网格，还显著提高了几何完整性。", "conclusion": "实验结果显示，该方法能够有效减少标记序列的冗余，并生成具有高几何完整性且复杂的多边形网格。"}}
{"id": "2507.02479", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02479", "abs": "https://arxiv.org/abs/2507.02479", "authors": ["Teng Fu", "Yuwen Chen", "Zhuofan Chen", "Mengyang Zhao", "Bin Li", "Xiangyang Xue"], "title": "CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios", "comment": null, "summary": "Multi-object tracking is a classic field in computer vision. Among them,\npedestrian tracking has extremely high application value and has become the\nmost popular research category. Existing methods mainly use motion or\nappearance information for tracking, which is often difficult in complex\nscenarios. For the motion information, mutual occlusions between objects often\nprevent updating of the motion state; for the appearance information,\nnon-robust results are often obtained due to reasons such as only partial\nvisibility of the object or blurred images. Although learning how to perform\ntracking in these situations from the annotated data is the simplest solution,\nthe existing MOT dataset fails to satisfy this solution. Existing methods\nmainly have two drawbacks: relatively simple scene composition and\nnon-realistic scenarios. Although some of the video sequences in existing\ndataset do not have the above-mentioned drawbacks, the number is far from\nadequate for research purposes. To this end, we propose a difficult large-scale\ndataset for multi-pedestrian tracking, shot mainly from the first-person view\nand all from real-life complex scenarios. We name it ``CrowdTrack'' because\nthere are numerous objects in most of the sequences. Our dataset consists of 33\nvideos, containing a total of 5,185 trajectories. Each object is annotated with\na complete bounding box and a unique object ID. The dataset will provide a\nplatform to facilitate the development of algorithms that remain effective in\ncomplex situations. We analyzed the dataset comprehensively and tested multiple\nSOTA models on our dataset. Besides, we analyzed the performance of the\nfoundation models on our dataset. The dataset and project code is released at:\nhttps://github.com/loseevaya/CrowdTrack .", "AI": {"tldr": "研究提出了一种新的多行人跟踪数据集（CrowdTrack），以促进算法在复杂场景下保持有效性。", "motivation": "现有的多目标跟踪数据集场景构成都较为简单，且缺乏现实性，这限制了在复杂场景下的跟踪研究。因此，本研究旨在提出一个适合复杂场景下多行人跟踪的数据集。", "method": "本研究提出了一种名为CrowdTrack的大型困难多行人跟踪数据集，主要从第一人称视角拍摄，涵盖了现实生活中的复杂场景。此数据集由33个视频组成，总共有5,185条轨迹，每个对象都有完整的边界框和唯一的对象ID标注。", "result": "研究者们对数据集进行了全面分析，并在该数据集上测试了多个最先进模型。此外，还分析了基础模型在该数据集上的表现。", "conclusion": "CrowdTrack数据集提供了一个促进算法在复杂条件下保持有效性的平台。此数据集和项目代码已公开发布。"}}
{"id": "2507.02488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02488", "abs": "https://arxiv.org/abs/2507.02488", "authors": ["Zunhui Xia", "Hongxing Li", "Libin Lan"], "title": "MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention", "comment": "13 pages, 9 figures, 9 tables", "summary": "Medical image recognition serves as a key way to aid in clinical diagnosis,\nenabling more accurate and timely identification of diseases and abnormalities.\nVision transformer-based approaches have proven effective in handling various\nmedical recognition tasks. However, these methods encounter two primary\nchallenges. First, they are often task-specific and architecture-tailored,\nlimiting their general applicability. Second, they usually either adopt full\nattention to model long-range dependencies, resulting in high computational\ncosts, or rely on handcrafted sparse attention, potentially leading to\nsuboptimal performance. To tackle these issues, we present MedFormer, an\nefficient medical vision transformer with two key ideas. First, it employs a\npyramid scaling structure as a versatile backbone for various medical image\nrecognition tasks, including image classification and dense prediction tasks\nsuch as semantic segmentation and lesion detection. This structure facilitates\nhierarchical feature representation while reducing the computation load of\nfeature maps, highly beneficial for boosting performance. Second, it introduces\na novel Dual Sparse Selection Attention (DSSA) with content awareness to\nimprove computational efficiency and robustness against noise while maintaining\nhigh performance. As the core building technique of MedFormer, DSSA is\nexplicitly designed to attend to the most relevant content. In addition, a\ndetailed theoretical analysis has been conducted, demonstrating that MedFormer\nhas superior generality and efficiency in comparison to existing medical vision\ntransformers. Extensive experiments on a variety of imaging modality datasets\nconsistently show that MedFormer is highly effective in enhancing performance\nacross all three above-mentioned medical image recognition tasks. The code is\navailable at https://github.com/XiaZunhui/MedFormer.", "AI": {"tldr": "由于传统的医学图像识别方法在通用性和计算效率上存在不足，MedFormer 通过其独特的金字塔缩放结构和内容感知双稀疏选择注意力机制，在各种任务上展示了优越的性能。", "motivation": "传统的基于视觉转换器的方法在医学图像识别中面临着任务专有性和架构定制的问题，这限制了它们的通用性。此外，这些方法要么采用全注意力机制处理长距离依赖，导致高计算成本，要么依赖手工稀疏注意力机制，可能导致次优性能。因此，MedFormer旨在解决这些挑战，提供一种更具通用性和效率的方法。", "method": "MedFormer 提出了一种高效的医学视觉转换器，其核心方法包括两个部分：1) 采用金字塔缩放结构作为各种医学图像识别任务的通用骨干结构，包括图像分类和稠密预测任务（如语义分割和病变检测）。这有助于分层特征表示，减少特征图的计算负担，从而提升性能。2) 引入了一种新的内容感知双稀疏选择注意力机制（DSSA），以提高计算效率和鲁棒性，并保持高性能。", "result": "通过广泛的数据集实验，MedFormer 在所有三种医学图像识别任务（图像分类、语义分割和病变检测）中表现出了高度的有效性。", "conclusion": "MedFormer 在医学图像识别任务上展示了优于现有医学视觉转换器的通用性和效率，通过结合金字塔缩放结构和内容感知双稀疏选择注意力机制，有效提升了性能和计算效率。"}}
{"id": "2507.02493", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02493", "abs": "https://arxiv.org/abs/2507.02493", "authors": ["Luca Parolari", "Andrea Cherubini", "Lamberto Ballan", "Carlo Biffi"], "title": "Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy", "comment": "Accepted at MICCAI 2025", "summary": "Automated polyp counting in colonoscopy is a crucial step toward automated\nprocedure reporting and quality control, aiming to enhance the\ncost-effectiveness of colonoscopy screening. Counting polyps in a procedure\ninvolves detecting and tracking polyps, and then clustering tracklets that\nbelong to the same polyp entity. Existing methods for polyp counting rely on\nself-supervised learning and primarily leverage visual appearance, neglecting\ntemporal relationships in both tracklet feature learning and clustering stages.\nIn this work, we introduce a paradigm shift by proposing a supervised\ncontrastive loss that incorporates temporally-aware soft targets. Our approach\ncaptures intra-polyp variability while preserving inter-polyp discriminability,\nleading to more robust clustering. Additionally, we improve tracklet clustering\nby integrating a temporal adjacency constraint, reducing false positive\nre-associations between visually similar but temporally distant tracklets. We\ntrain and validate our method on publicly available datasets and evaluate its\nperformance with a leave-one-out cross-validation strategy. Results demonstrate\na 2.2x reduction in fragmentation rate compared to prior approaches. Our\nresults highlight the importance of temporal awareness in polyp counting,\nestablishing a new state-of-the-art. Code is available at\nhttps://github.com/lparolari/temporally-aware-polyp-counting.", "AI": {"tldr": "本文提出了一种新的息肉计数方法，利用监督对比损失和时间感知软目标，提高了结肠镜检查中息肉计数的准确性，并降低了碎片化率。", "motivation": "研究动机在于提高结肠镜检查的自动化过程报告和质量控制，通过更精确的息肉计数来增强结肠镜筛查的成本效益。", "method": "本文提出了一种利用监督对比损失函数结合时间感知软目标的方法，旨在解决现有聚类方法中忽视时间关系的问题。这种方法能够捕捉到同一息肉内的变化同时保持息肉之间的判别性。同时，通过引入时间邻接约束，进一步优化了轨迹聚类，减少了时间上距离较远但视觉上相似的轨迹间的错误重新关联。", "result": "实验结果表明，采用该方法后，碎片化率比先前的方法降低了2.2倍，确立了在息肉计数上的新标准。", "conclusion": "研究结论证明了在息肉计数中考虑时间意识的重要性，并表明所提出的方法通过减少时间上距离较远但视觉上相似的轨迹间的错误重新关联，提高了息肉计数的准确性和稳定性。"}}
{"id": "2507.02494", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02494", "abs": "https://arxiv.org/abs/2507.02494", "authors": ["Hyunsoo Son", "Jeonghyun Noh", "Suemin Jeon", "Chaoli Wang", "Won-Ki Jeong"], "title": "MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations", "comment": "5 pages", "summary": "Implicit Neural Representations (INRs) are widely used to encode data as\ncontinuous functions, enabling the visualization of large-scale multivariate\nscientific simulation data with reduced memory usage. However, existing\nINR-based methods face three main limitations: (1) inflexible representation of\ncomplex structures, (2) primarily focusing on single-variable data, and (3)\ndependence on structured grids. Thus, their performance degrades when applied\nto complex real-world datasets. To address these limitations, we propose a\nnovel neural network-based framework, MC-INR, which handles multivariate data\non unstructured grids. It combines meta-learning and clustering to enable\nflexible encoding of complex structures. To further improve performance, we\nintroduce a residual-based dynamic re-clustering mechanism that adaptively\npartitions clusters based on local error. We also propose a branched layer to\nleverage multivariate data through independent branches simultaneously.\nExperimental results demonstrate that MC-INR outperforms existing methods on\nscientific data encoding tasks.", "AI": {"tldr": "MC-INR克服了现有隐式神经表示技术的限制，能够在处理无结构网格上的多变量科学模拟数据时提供更强的灵活性和更好的性能。", "motivation": "现有的隐式神经表示方法存在三个主要局限：无法灵活表示复杂结构、主要针对单变量数据以及依赖于结构化网格。因此，这些方法在应用于复杂现实数据集时性能会降低。为了克服这些问题，提出了MC-INR。", "method": "结合元学习和聚类技术，提出了一种新型神经网络框架MC-INR，用于处理无结构网格上的多变量数据。该框架还引入了一种基于残差的动态重聚类机制，以根据局部误差自适应地划分聚类，并提出了一种分支层来同时利用多变量数据。", "result": "实验结果表明，MC-INR在科学数据编码任务上优于现有方法。", "conclusion": "MC-INR通过结合元学习和聚类技术，并引入基于残差的动态重聚类机制，解决了现有方法在复杂结构和多变量数据处理上的局限性，显示出其潜在的应用前景。"}}
{"id": "2507.02513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02513", "abs": "https://arxiv.org/abs/2507.02513", "authors": ["Dimitrios Bouzoulas", "Eerik Alamikkotervo", "Risto Ojala"], "title": "Automatic Labelling for Low-Light Pedestrian Detection", "comment": null, "summary": "Pedestrian detection in RGB images is a key task in pedestrian safety, as the\nmost common sensor in autonomous vehicles and advanced driver assistance\nsystems is the RGB camera. A challenge in RGB pedestrian detection, that does\nnot appear to have large public datasets, is low-light conditions. As a\nsolution, in this research, we propose an automated infrared-RGB labeling\npipeline. The proposed pipeline consists of 1) Infrared detection, where a\nfine-tuned model for infrared pedestrian detection is used 2) Label transfer\nprocess from the infrared detections to their RGB counterparts 3) Training\nobject detection models using the generated labels for low-light RGB pedestrian\ndetection. The research was performed using the KAIST dataset. For the\nevaluation, object detection models were trained on the generated autolabels\nand ground truth labels. When compared on a previously unseen image sequence,\nthe results showed that the models trained on generated labels outperformed the\nones trained on ground-truth labels in 6 out of 9 cases for the mAP@50 and\nmAP@50-95 metrics. The source code for this research is available at\nhttps://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling", "AI": {"tldr": "本文提出了一种自动标注管道，用于在低光条件下改进RGB图像中的行人检测，实验结果表明该方法优于传统方法。", "motivation": "在行人安全中，行人检测在RGB图像中是一个关键任务，但缺乏大规模公共数据集导致在低光条件下检测存在问题。为了解决这个问题，作者提出了一种自动标注管道，以改善低光条件下的行人检测性能。", "method": "本文提出了一种自动红外-RGB标注管道，包含三个步骤：1) 红外检测，使用了细调后的红外行人检测模型；2) 将红外检测标签转移到其RGB对应图像；3) 使用生成的标签训练低光RGB行人检测对象检测模型。", "result": "在使用KAIST数据集进行的研究中，使用生成的标签和真实标签分别训练了对象检测模型。当在之前未见过的图像序列上进行比较时，结果显示，在6个案例中的mAP@50和mAP@50-95度量上，使用生成标签训练的模型优于使用真实标签训练的模型。", "conclusion": "研究表明，通过自动化红外-RGB标签转移过程，可以提高低光条件下RGB图像行人检测的性能。"}}
{"id": "2507.02517", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.02517", "abs": "https://arxiv.org/abs/2507.02517", "authors": ["Vivek Yadav", "Anugrah Jain"], "title": "Detecting Multiple Diseases in Multiple Crops Using Deep Learning", "comment": null, "summary": "India, as a predominantly agrarian economy, faces significant challenges in\nagriculture, including substantial crop losses caused by diseases, pests, and\nenvironmental stress. Early detection and accurate identification of diseases\nacross different crops are critical for improving yield and ensuring food\nsecurity. This paper proposes a deep learning based solution for detecting\nmultiple diseases in multiple crops, aimed to cover India's diverse\nagricultural landscape. We first create a unified dataset encompassing images\nof 17 different crops and 34 different diseases from various available\nrepositories. Proposed deep learning model is trained on this dataset and\noutperforms the state-of-the-art in terms of accuracy and the number of crops,\ndiseases covered. We achieve a significant detection accuracy, i.e., 99 percent\nfor our unified dataset which is 7 percent more when compared to\nstate-of-the-art handling 14 crops and 26 different diseases only. By improving\nthe number of crops and types of diseases that can be detected, proposed\nsolution aims to provide a better product for Indian farmers.", "AI": {"tldr": "A deep learning solution is developed to detect multiple diseases in multiple crops, outperforming the state-of-the-art with an accuracy of 99% on a dataset of 17 crops and 34 diseases.", "motivation": "Addressing significant crop losses in India due to diseases, pests, and environmental stress, the solution aims to improve agricultural yield and ensure food security.", "method": "The proposed solution involves creating a unified dataset of 17 crops and 34 diseases and training a deep learning model on it.", "result": "The model shows a 99% accuracy, which is 7% higher compared to state-of-the-art models that handle only 14 crops and 26 diseases.", "conclusion": "The solution enhances the range of crops and diseases that can be accurately detected, aiming to provide a valuable tool for Indian farmers."}}
{"id": "2507.02519", "categories": ["cs.CV", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.02519", "abs": "https://arxiv.org/abs/2507.02519", "authors": ["Abiam Remache González", "Meriem Chagour", "Timon Bijan Rüth", "Raúl Trapiella Cañedo", "Marina Martínez Soler", "Álvaro Lorenzo Felipe", "Hyun-Suk Shin", "María-Jesús Zamorano Serrano", "Ricardo Torres", "Juan-Antonio Castillo Parra", "Eduardo Reyes Abad", "Miguel-Ángel Ferrer Ballester", "Juan-Manuel Afonso López", "Francisco-Mario Hernández Tejera", "Adrian Penate-Sanchez"], "title": "IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning", "comment": "14 pages, 7 figures", "summary": "This paper introduces IMASHRIMP, an adapted system for the automated\nmorphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing\ngenetic selection tasks in aquaculture. Existing deep learning and computer\nvision techniques were modified to address the specific challenges of shrimp\nmorphology analysis from RGBD images. IMASHRIMP incorporates two discrimination\nmodules, based on a modified ResNet-50 architecture, to classify images by the\npoint of view and determine rostrum integrity. It is proposed a \"two-factor\nauthentication (human and IA)\" system, it reduces human error in view\nclassification from 0.97% to 0% and in rostrum detection from 12.46% to 3.64%.\nAdditionally, a pose estimation module was adapted from VitPose to predict 23\nkey points on the shrimp's skeleton, with separate networks for lateral and\ndorsal views. A morphological regression module, using a Support Vector Machine\n(SVM) model, was integrated to convert pixel measurements to centimeter units.\nExperimental results show that the system effectively reduces human error,\nachieving a mean average precision (mAP) of 97.94% for pose estimation and a\npixel-to-centimeter conversion error of 0.07 (+/- 0.1) cm. IMASHRIMP\ndemonstrates the potential to automate and accelerate shrimp morphological\nanalysis, enhancing the efficiency of genetic selection and contributing to\nmore sustainable aquaculture practices.The code are available at\nhttps://github.com/AbiamRemacheGonzalez/ImaShrimp-public", "AI": {"tldr": "IMASHRIMP系统利用深度学习和计算机视觉技术，对白虾的形态进行自动化分析，以优化水产养殖中的遗传选育工作。系统包含基于修改版ResNet-50的分类模块、“双因素认证”系统、姿态估计算法以及形态回归模块，显著减少了人工错误，并实现了高效准确的虾形态测量。", "motivation": "旨在优化白虾在水产养殖中的遗传选育工作，减少人工错误，提高效率。", "method": "采用修改版的深度学习和计算机视觉技术，包括基于ResNet-50的分类模块、\"双因素认证\"系统、从VitPose改编的姿态估计算法以及使用SVM的回归模块。", "result": "系统大幅减少了人工错误，实现了97.94%的平均精确度（mAP）和0.07 cm (+/- 0.1)的像素到厘米转换误差。", "conclusion": "IMASHRIMP系统展示了其在虾形态自动化分析中的潜力，有助于提高遗传选育效率，并促进更加可持续的水产养殖实践。"}}
{"id": "2507.02546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02546", "abs": "https://arxiv.org/abs/2507.02546", "authors": ["Ruicheng Wang", "Sicheng Xu", "Yue Dong", "Yu Deng", "Jianfeng Xiang", "Zelong Lv", "Guangzhong Sun", "Xin Tong", "Jiaolong Yang"], "title": "MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details", "comment": "Project page: https://wangrc.site/MoGe2Page/", "summary": "We propose MoGe-2, an advanced open-domain geometry estimation model that\nrecovers a metric scale 3D point map of a scene from a single image. Our method\nbuilds upon the recent monocular geometry estimation approach, MoGe, which\npredicts affine-invariant point maps with unknown scales. We explore effective\nstrategies to extend MoGe for metric geometry prediction without compromising\nthe relative geometry accuracy provided by the affine-invariant point\nrepresentation. Additionally, we discover that noise and errors in real data\ndiminish fine-grained detail in the predicted geometry. We address this by\ndeveloping a unified data refinement approach that filters and completes real\ndata from different sources using sharp synthetic labels, significantly\nenhancing the granularity of the reconstructed geometry while maintaining the\noverall accuracy. We train our model on a large corpus of mixed datasets and\nconducted comprehensive evaluations, demonstrating its superior performance in\nachieving accurate relative geometry, precise metric scale, and fine-grained\ndetail recovery -- capabilities that no previous methods have simultaneously\nachieved.", "AI": {"tldr": "MoGe-2 enhances MoGe by predicting metric scale 3D maps from single images, improving accuracy and detail recovery through a data refinement technique.", "motivation": "To extend MoGe for metric geometry prediction on a large scale, and to address the noise and errors in real data which diminish fine-grained detail.", "method": "We propose MoGe-2, which builds on MoGe to predict metric scale 3D point maps from single images, addressing scale ambiguity while maintaining relative geometry accuracy. We also introduce a data refinement technique to improve fine-grained detail accuracy.", "result": "Our evaluations show that MoGe-2 achieves superior performance in relative geometry accuracy, precise metric scale, and fine-grained detail recovery.", "conclusion": "MoGe-2 accomplishes the simultaneous achievement of accurate relative geometry, precise metric scale, and fine-grained detail recovery, outperforming previous methods."}}
{"id": "2507.02565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02565", "abs": "https://arxiv.org/abs/2507.02565", "authors": ["Buzhen Huang", "Chen Li", "Chongyang Xu", "Dongyue Lu", "Jinnan Chen", "Yangang Wang", "Gim Hee Lee"], "title": "Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning", "comment": null, "summary": "Due to visual ambiguities and inter-person occlusions, existing human pose\nestimation methods cannot recover plausible close interactions from in-the-wild\nvideos. Even state-of-the-art large foundation models~(\\eg, SAM) cannot\naccurately distinguish human semantics in such challenging scenarios. In this\nwork, we find that human appearance can provide a straightforward cue to\naddress these obstacles. Based on this observation, we propose a dual-branch\noptimization framework to reconstruct accurate interactive motions with\nplausible body contacts constrained by human appearances, social proxemics, and\nphysical laws. Specifically, we first train a diffusion model to learn the\nhuman proxemic behavior and pose prior knowledge. The trained network and two\noptimizable tensors are then incorporated into a dual-branch optimization\nframework to reconstruct human motions and appearances. Several constraints\nbased on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to\nassist the optimization. With the proxemics prior and diverse constraints, our\nmethod is capable of estimating accurate interactions from in-the-wild videos\ncaptured in complex environments. We further build a dataset with pseudo\nground-truth interaction annotations, which may promote future research on pose\nestimation and human behavior understanding. Experimental results on several\nbenchmarks demonstrate that our method outperforms existing approaches. The\ncode and data are available at https://www.buzhenhuang.com/works/CloseApp.html.", "AI": {"tldr": "A dual-branch optimization framework is proposed for accurate interactive motion reconstruction, leveraging a diffusion model and various constraints to overcome the limitations of existing human pose estimation methods in challenging scenarios.", "motivation": "Existing human pose estimation methods fail to recover plausible close interactions and cannot distinguish human semantics in challenging scenarios due to visual ambiguities and inter-person occlusions.", "method": "The paper adopts a dual-branch optimization framework to reconstruct accurate interactive motions with plausible body contacts. It includes a diffusion model trained to learn human proxemic behavior and pose priors, along with various constraints based on 3D Gaussians, 2D keypoints, and mesh penetrations.", "result": "The method is capable of estimating accurate interactions from in-the-wild videos. Experiments on several benchmarks show it outperforms existing approaches.", "conclusion": "The proposed method demonstrates superior performance in human poses and interactions reconstruction with the use of dual-branch optimization and proxemics prior."}}
{"id": "2507.02576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02576", "abs": "https://arxiv.org/abs/2507.02576", "authors": ["Alina F. Dima", "Suprosanna Shit", "Huaqi Qiu", "Robbie Holland", "Tamara T. Mueller", "Fabio Antonio Musio", "Kaiyuan Yang", "Bjoern Menze", "Rickmer Braren", "Marcus Makowski", "Daniel Rueckert"], "title": "Parametric shape models for vessels learned from segmentations via differentiable voxelization", "comment": "15 pages, 6 figures", "summary": "Vessels are complex structures in the body that have been studied extensively\nin multiple representations. While voxelization is the most common of them,\nmeshes and parametric models are critical in various applications due to their\ndesirable properties. However, these representations are typically extracted\nthrough segmentations and used disjointly from each other. We propose a\nframework that joins the three representations under differentiable\ntransformations. By leveraging differentiable voxelization, we automatically\nextract a parametric shape model of the vessels through shape-to-segmentation\nfitting, where we learn shape parameters from segmentations without the\nexplicit need for ground-truth shape parameters. The vessel is parametrized as\ncenterlines and radii using cubic B-splines, ensuring smoothness and continuity\nby construction. Meshes are differentiably extracted from the learned shape\nparameters, resulting in high-fidelity meshes that can be manipulated post-fit.\nOur method can accurately capture the geometry of complex vessels, as\ndemonstrated by the volumetric fits in experiments on aortas, aneurysms, and\nbrain vessels.", "AI": {"tldr": "本文提出了一种将体素化、网格和参数化模型结合的框架，通过可微分变换，从分割直接学习形状参数，来提高复杂血管几何结构的捕捉准确性。", "motivation": "虽然体素化是研究血管的最常见表示方法，但网格和参数化模型在众多应用中也非常重要。然而，这些表示方法通常通过分割独立获得。我们提出的方法旨在将这三种表示方法结合起来，以更准确地捕捉复杂的血管几何结构。", "method": "我们提出了一种框架，将体素化、网格和参数化模型这三种表示方法通过可微分变换结合起来。通过利用可微分体素化，我们能够从分割到形状的匹配中自动提取血管的参数化形状模型，从而无需显式的地面真实形状参数就能学习到形状参数。血管被参数化为中心线和半径，使用三次B样条确保光滑性和连续性。网格可以通过学习到的形状参数被不同地提取，从而可以进行高保真度的网格操作。", "result": "实验结果表明，该方法能够准确捕捉复杂血管的几何形状，包括主动脉、动脉瘤和脑血管的体积拟合。", "conclusion": "研究揭示了结合不同表示方法并通过可微分变换学习形状参数的方法可以有效地提高血管几何结构的捕捉精度。这种方法在不同类型的血管研究中表现出色。"}}
{"id": "2507.02581", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02581", "abs": "https://arxiv.org/abs/2507.02581", "authors": ["Tan Pan", "Zhaorui Tan", "Kaiyu Guo", "Dongli Xu", "Weidi Xu", "Chen Jiang", "Xin Guo", "Yuan Qi", "Yuan Cheng"], "title": "Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning", "comment": "Accepted by ICCV25", "summary": "3D medical image self-supervised learning (mSSL) holds great promise for\nmedical analysis. Effectively supporting broader applications requires\nconsidering anatomical structure variations in location, scale, and morphology,\nwhich are crucial for capturing meaningful distinctions. However, previous mSSL\nmethods partition images with fixed-size patches, often ignoring the structure\nvariations. In this work, we introduce a novel perspective on 3D medical images\nwith the goal of learning structure-aware representations. We assume that\npatches within the same structure share the same semantics (semantic\nconsistency) while those from different structures exhibit distinct semantics\n(semantic discrepancy). Based on this assumption, we propose an mSSL framework\nnamed $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency\nin two steps. First, $S^2DC$ enforces distinct representations for different\npatches to increase semantic discrepancy by leveraging an optimal transport\nstrategy. Second, $S^2DC$ advances semantic consistency at the structural level\nbased on neighborhood similarity distribution. By bridging patch-level and\nstructure-level representations, $S^2DC$ achieves structure-aware\nrepresentations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3\nmodalities, our proposed method consistently outperforms the state-of-the-art\nmethods in mSSL.", "AI": {"tldr": "This paper presents a novel 3D medical image self-supervised learning framework ($S^2DC) that achieves structure-aware representations by addressing limitations in prior methods that ignore anatomical structure variations. Evaluations demonstrate it outperforms existing methods.", "motivation": "The motivation is to improve limitations in previous self-supervised learning methods for 3D medical imaging by accounting for variations in anatomical structures' location, scale, and morphology, which are critical for capturing meaningful distinctions in medical analysis.", "method": "The method introduces a new framework called $S^2DC$ that aims to learn structure-aware representations in 3D medical images. The framework has two steps: 1. Enforcing distinct representations for different patches to increase semantic discrepancy using an optimal transport strategy. 2. Promoting semantic consistency at the structural level based on neighborhood similarity distribution.", "result": "The method was evaluated across 10 datasets, 4 tasks, and 3 modalities, consistently outperforming existing state-of-the-art methods in self-supervised learning.", "conclusion": "The conclusion is that the proposed $S^2DC$ framework successfully captures structure-aware representations in 3D medical images, leading to superior performance compared to state-of-the-art self-supervised learning methods."}}
{"id": "2507.02591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02591", "abs": "https://arxiv.org/abs/2507.02591", "authors": ["Weili Xu", "Enxin Song", "Wenhao Chai", "Xuexiang Wen", "Tian Ye", "Gaoang Wang"], "title": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding", "comment": "Accepted to ICCV 2025", "summary": "The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding.", "AI": {"tldr": "提出了AuroraLong模型，采用线性RNN来处理长视频理解问题，实现了与更大模型相似的性能，同时显著降低了计算复杂度。", "motivation": "旨在解决长视频理解面临的高计算复杂度和高昂内存成本问题，特别是对于基于Transformer的LLMs，其内存和计算需求与输入序列长度呈二次增长。", "method": "采用AuroraLong方法，以线性RNN语言模型替换传统的基于Transformer的LLMs组件，适用于任意长度输入序列且具有固定大小的隐藏状态。此外，通过将视觉标记按大小升序重新排序，进一步结合视觉标记合并以提高吞吐量和效率。", "result": "尽管AuroraLong仅有20亿参数，并且仅基于公共数据进行训练，但它在多个视频基准测试中的表现与基于Transformer的模型相当，后者具有相似的参数量但基于私有数据集训练。", "conclusion": "研究表明，高效的线性RNN能够通过降低计算门槛来普及长视频理解。首次在类似LLaVA的模型中采用线性RNN作为LLM主干，实现开放式视频理解。"}}
{"id": "2507.02602", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02602", "abs": "https://arxiv.org/abs/2507.02602", "authors": ["Riccardo Gallon", "Fabian Schiemenz", "Alessandra Menicucci", "Eberhard Gill"], "title": "Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development", "comment": "Submitted to Acta Astronautica", "summary": "The increasing importance of Vision-Based Navigation (VBN) algorithms in\nspace missions raises numerous challenges in ensuring their reliability and\noperational robustness. Sensor faults can lead to inaccurate outputs from\nnavigation algorithms or even complete data processing faults, potentially\ncompromising mission objectives. Artificial Intelligence (AI) offers a powerful\nsolution for detecting such faults, overcoming many of the limitations\nassociated with traditional fault detection methods. However, the primary\nobstacle to the adoption of AI in this context is the lack of sufficient and\nrepresentative datasets containing faulty image data.\n  This study addresses these challenges by focusing on an interplanetary\nexploration mission scenario. A comprehensive analysis of potential fault cases\nin camera sensors used within the VBN pipeline is presented. The causes and\neffects of these faults are systematically characterized, including their\nimpact on image quality and navigation algorithm performance, as well as\ncommonly employed mitigation strategies. To support this analysis, a simulation\nframework is introduced to recreate faulty conditions in synthetically\ngenerated images, enabling a systematic and controlled reproduction of faulty\ndata. The resulting dataset of fault-injected images provides a valuable tool\nfor training and testing AI-based fault detection algorithms. The final link to\nthe dataset will be added after an embargo period. For peer-reviewers, this\nprivate link is available.", "AI": {"tldr": "研究解决了视觉导航中传感器故障检测的数据缺乏问题，生成故障图像数据集用于AI训练。", "motivation": "解决视觉导航算法中传感器故障检测的挑战，推动AI在故障检测中的应用。", "method": "通过模拟框架生成包含故障图像的数据集，用于训练和测试基于AI的故障检测算法。", "result": "创建了一个包含故障图像的数据集，为基于AI的故障检测算法提供支持。", "conclusion": "研究展示了如何通过模拟框架生成故障数据，并为未来的AI故障检测提供了有价值的数据集。"}}
{"id": "2507.02664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02664", "abs": "https://arxiv.org/abs/2507.02664", "authors": ["Ziyin Zhou", "Yunpeng Luo", "Yuanchen Wu", "Ke Sun", "Jiayi Ji", "Ke Yan", "Shouhong Ding", "Xiaoshuai Sun", "Yunsheng Wu", "Rongrong Ji"], "title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models", "comment": "Accepted to ICCV 2025", "summary": "The rapid development of AI-generated content (AIGC) technology has led to\nthe misuse of highly realistic AI-generated images (AIGI) in spreading\nmisinformation, posing a threat to public information security. Although\nexisting AIGI detection techniques are generally effective, they face two\nissues: 1) a lack of human-verifiable explanations, and 2) a lack of\ngeneralization in the latest generation technology. To address these issues, we\nintroduce a large-scale and comprehensive dataset, Holmes-Set, which includes\nthe Holmes-SFTSet, an instruction-tuning dataset with explanations on whether\nimages are AI-generated, and the Holmes-DPOSet, a human-aligned preference\ndataset. Our work introduces an efficient data annotation method called the\nMulti-Expert Jury, enhancing data generation through structured MLLM\nexplanations and quality control via cross-model evaluation, expert defect\nfiltering, and human preference modification. In addition, we propose Holmes\nPipeline, a meticulously designed three-stage training framework comprising\nvisual expert pre-training, supervised fine-tuning, and direct preference\noptimization. Holmes Pipeline adapts multimodal large language models (MLLMs)\nfor AIGI detection while generating human-verifiable and human-aligned\nexplanations, ultimately yielding our model AIGI-Holmes. During the inference\nstage, we introduce a collaborative decoding strategy that integrates the model\nperception of the visual expert with the semantic reasoning of MLLMs, further\nenhancing the generalization capabilities. Extensive experiments on three\nbenchmarks validate the effectiveness of our AIGI-Holmes.", "AI": {"tldr": "本研究提出了Holmes-Set数据集和Holmes Pipeline训练框架来提高AIGI检测技术的解释能力和泛化性，以应对高逼真AI生成图像导致的信息安全威胁。", "motivation": "解决现有AI生成图像(AIGI)检测技术缺乏人类可验证解释和在最新生成技术中泛化能力不足的问题，以应对高逼真度AI生成图像在传播误导信息时对公众信息安全造成的威胁。", "method": "通过引入一个大规模综合数据集Holmes-Set来解决现有AI生成图像(AIGI)检测技术的两个问题：缺乏可由人类验证的解释以及在最新生成技术中的泛化能力不足。Holmes-Set包括带有图像是否为AI生成解释的指令微调数据集Holmes-SFTSet和人类对齐偏好数据集Holmes-DPOSet。此外，还提出了一种高效的数据标注方法Multi-Expert Jury，通过结构化MLLM解释和质量控制增强数据生成。在这个过程中，采用三阶段训练框架Holmes Pipeline(视觉专家预训练、监督微调和直接偏好优化)，适应多模态大型语言模型进行AIGI检测，并生成可验证和对齐人类的解释。推理阶段，通过集成视觉专家模型感知与MLLM语义推理的合作解码策略进一步提高泛化能力。", "result": "广泛的实验证明了提出的AIGI-Holmes模型在三个基准测试中的有效性。", "conclusion": "本研究的AIGI-Holmes模型在提高AI生成图像检测的可解释性和泛化能力方面取得了显著效果。"}}
{"id": "2507.02686", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02686", "abs": "https://arxiv.org/abs/2507.02686", "authors": ["Charlesquin Kemajou Mbakam", "Jonathan Spence", "Marcelo Pereyra"], "title": "Learning few-step posterior samplers by unfolding and distillation of diffusion models", "comment": "28 pages, 16 figures, 10 tables", "summary": "Diffusion models (DMs) have emerged as powerful image priors in Bayesian\ncomputational imaging. Two primary strategies have been proposed for leveraging\nDMs in this context: Plug-and-Play methods, which are zero-shot and highly\nflexible but rely on approximations; and specialized conditional DMs, which\nachieve higher accuracy and faster inference for specific tasks through\nsupervised training. In this work, we introduce a novel framework that\nintegrates deep unfolding and model distillation to transform a DM image prior\ninto a few-step conditional model for posterior sampling. A central innovation\nof our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm\n- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et\nal., 2025) - representing the first known instance of deep unfolding applied to\na Monte Carlo sampling scheme. We demonstrate our proposed unfolded and\ndistilled samplers through extensive experiments and comparisons with the state\nof the art, where they achieve excellent accuracy and computational efficiency,\nwhile retaining the flexibility to adapt to variations in the forward model at\ninference time.", "AI": {"tldr": "本文提出了一种将扩散模型图像先验转换为几步条件模型的新框架，该框架通过深度展开和模型蒸馏将最近提出的LATINO兰格文采样器集成到MCMC算法中，从而在保持灵活适应性的同时提高准确性和计算效率。", "motivation": "扩散模型（DMs）作为贝叶斯计算成像中的图像先验显示出其强大的能力。提出的两种主要策略——Plug-and-Play方法（零样本、高度灵活但依赖于近似）和专门的条件扩散模型（针对特定任务通过监督训练获得更高的准确性和更快的推理速度）仍存在局限。通过提出新的框架，旨在结合二者优势，解决现有方法的不足。", "method": "通过深度展开和模型蒸馏，将扩散模型图像先验转换为用于后验采样的几步条件模型。核心创新是展开马尔可夫链蒙特卡洛（MCMC）算法——具体为最近提出的LATINO兰格文采样器（Spagnoletti等，2025），这是首次将深度展开应用于蒙特卡洛采样方案。", "result": "该方法在各种实验环境中相较于现有技术水平展示出了显著的准确性和计算效率，证明了其方法的有效性。", "conclusion": "提出的展开和蒸馏采样器在大量实验和与现有技术水平的比较中展示出了优秀的准确性和计算效率，同时保留在推理时间适应正向模型变化的能力。"}}
{"id": "2507.02687", "categories": ["cs.CV", "cs.AI", "60J60, 68T07", "I.2.6; I.2.10; I.4.9"], "pdf": "https://arxiv.org/pdf/2507.02687", "abs": "https://arxiv.org/abs/2507.02687", "authors": ["JungWoo Chae", "Jiyoon Kim", "JaeWoong Choi", "Kyungyul Kim", "Sangheum Hwang"], "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data", "comment": "CVPR 2025 camera ready. Project page: https://lgcnsai.github.io/apt", "summary": "Personalizing diffusion models using limited data presents significant\nchallenges, including overfitting, loss of prior knowledge, and degradation of\ntext alignment. Overfitting leads to shifts in the noise prediction\ndistribution, disrupting the denoising trajectory and causing the model to lose\nsemantic coherence. In this paper, we propose Adaptive Personalized Training\n(APT), a novel framework that mitigates overfitting by employing adaptive\ntraining strategies and regularizing the model's internal representations\nduring fine-tuning. APT consists of three key components: (1) Adaptive Training\nAdjustment, which introduces an overfitting indicator to detect the degree of\noverfitting at each time step bin and applies adaptive data augmentation and\nadaptive loss weighting based on this indicator; (2)Representation\nStabilization, which regularizes the mean and variance of intermediate feature\nmaps to prevent excessive shifts in noise prediction; and (3) Attention\nAlignment for Prior Knowledge Preservation, which aligns the cross-attention\nmaps of the fine-tuned model with those of the pretrained model to maintain\nprior knowledge and semantic coherence. Through extensive experiments, we\ndemonstrate that APT effectively mitigates overfitting, preserves prior\nknowledge, and outperforms existing methods in generating high-quality, diverse\nimages with limited reference data.", "AI": {"tldr": "APT addresses overfitting in personalized diffusion models, preserving prior knowledge and text alignment, leading to improved image generation quality and diversity with limited reference data.", "motivation": "To address the challenges of personalizing diffusion models using limited data, such as overfitting, loss of prior knowledge, and poor text alignment.", "method": "Adaptive Personalized Training (APT), a novel framework with three components: Adaptive Training Adjustment, Representation Stabilization, and Attention Alignment for Prior Knowledge Preservation.", "result": "Experiments show APT effectively mitigates overfitting, preserves prior knowledge, and outperforms existing methods in generating high-quality, diverse images with limited reference data.", "conclusion": "APT framework effectively addresses overfitting and preserves prior knowledge in personalized diffusion models, leading to the generation of high-quality, diverse images with limited reference data."}}
{"id": "2507.02691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02691", "abs": "https://arxiv.org/abs/2507.02691", "authors": ["Xiangyang Luo", "Ye Zhu", "Yunfei Liu", "Lijian Lin", "Cong Wan", "Zijian Cai", "Shao-Lun Huang", "Yu Li"], "title": "CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation", "comment": "ICCV Accepted", "summary": "Video face swapping aims to address two primary challenges: effectively\ntransferring the source identity to the target video and accurately preserving\nthe dynamic attributes of the target face, such as head poses, facial\nexpressions, lip-sync, \\etc. Existing methods mainly focus on achieving\nhigh-quality identity transfer but often fall short in maintaining the dynamic\nattributes of the target face, leading to inconsistent results. We attribute\nthis issue to the inherent coupling of facial appearance and motion in videos.\nTo address this, we propose CanonSwap, a novel video face-swapping framework\nthat decouples motion information from appearance information. Specifically,\nCanonSwap first eliminates motion-related information, enabling identity\nmodification within a unified canonical space. Subsequently, the swapped\nfeature is reintegrated into the original video space, ensuring the\npreservation of the target face's dynamic attributes. To further achieve\nprecise identity transfer with minimal artifacts and enhanced realism, we\ndesign a Partial Identity Modulation module that adaptively integrates source\nidentity features using a spatial mask to restrict modifications to facial\nregions. Additionally, we introduce several fine-grained synchronization\nmetrics to comprehensively evaluate the performance of video face swapping\nmethods. Extensive experiments demonstrate that our method significantly\noutperforms existing approaches in terms of visual quality, temporal\nconsistency, and identity preservation. Our project page are publicly available\nat https://luoxyhappy.github.io/CanonSwap/.", "AI": {"tldr": "CanonSwap is a video face-swapping method that decouples motion and appearance to achieve high-quality identity transfer while preserving target face dynamics.", "motivation": "The motivation behind CanonSwap is to address the limitations of current face-swapping methods that, while achieving high-quality identity transfer, often fail to maintain the dynamic attributes of the target face, such as head poses and facial expressions.", "method": "The paper proposes CanonSwap, a new framework for video face swapping that aims to improve on existing methods by decoupling motion and appearance information. It achieves this by first eliminating motion-related information to allow for identity modification in a unified canonical space, followed by reintegration into the original video to preserve dynamic attributes. Additionally, it introduces a Partial Identity Modulation module that uses a spatial mask to adaptively integrate source identity features.", "result": "Extensive experiments show that CanonSwap significantly outperforms existing methods in visual quality, temporal consistency, and identity preservation.", "conclusion": "CanonSwap effectively addresses the main challenges of video face swapping by decoupling motion and appearance information and applying adaptive partial identity modulation, leading to superior results in both identity transfer and dynamic attribute preservation."}}
{"id": "2507.02705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02705", "abs": "https://arxiv.org/abs/2507.02705", "authors": ["Qi Xu", "Dongxu Wei", "Lingzhe Zhao", "Wenpu Li", "Zhangchi Huang", "Shunping Ji", "Peidong Liu"], "title": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment", "comment": null, "summary": "Simultaneous understanding and 3D reconstruction plays an important role in\ndeveloping end-to-end embodied intelligent systems. To achieve this, recent\napproaches resort to 2D-to-3D feature alignment paradigm, which leads to\nlimited 3D understanding capability and potential semantic information loss. In\nlight of this, we propose SIU3R, the first alignment-free framework for\ngeneralizable simultaneous understanding and 3D reconstruction from unposed\nimages. Specifically, SIU3R bridges reconstruction and understanding tasks via\npixel-aligned 3D representation, and unifies multiple understanding tasks into\na set of unified learnable queries, enabling native 3D understanding without\nthe need of alignment with 2D models. To encourage collaboration between the\ntwo tasks with shared representation, we further conduct in-depth analyses of\ntheir mutual benefits, and propose two lightweight modules to facilitate their\ninteraction. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance not only on the individual tasks of 3D\nreconstruction and understanding, but also on the task of simultaneous\nunderstanding and 3D reconstruction, highlighting the advantages of our\nalignment-free framework and the effectiveness of the mutual benefit designs.", "AI": {"tldr": "The paper presents SIU3R, an innovative alignment-free framework for simultaneous 3D reconstruction and understanding, significantly enhancing performance over previous methods.", "motivation": "The motivation is to overcome the limitations and potential semantic information loss caused by the current 2D-to-3D feature alignment paradigm used in embodied intelligent systems.", "method": "The paper proposes SIU3R, an alignment-free framework for 3D reconstruction and understanding from unposed images, using a pixel-aligned 3D representation and unified learnable queries.", "result": "The proposed method achieves state-of-the-art performance on the individual tasks of 3D reconstruction and understanding, as well as on simultaneous understanding and 3D reconstruction.", "conclusion": "The alignment-free framework of SIU3R and the designed mutual benefit modules are effective in enhancing the performance of simultaneous 3D reconstruction and understanding."}}
{"id": "2507.02713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02713", "abs": "https://arxiv.org/abs/2507.02713", "authors": ["Qin Guo", "Ailing Zeng", "Dongxu Yue", "Ceyuan Yang", "Yang Cao", "Hanzhong Guo", "Fei Shen", "Wei Liu", "Xihui Liu", "Dan Xu"], "title": "UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation", "comment": null, "summary": "Although significant advancements have been achieved in the progress of\nkeypoint-guided Text-to-Image diffusion models, existing mainstream\nkeypoint-guided models encounter challenges in controlling the generation of\nmore general non-rigid objects beyond humans (e.g., animals). Moreover, it is\ndifficult to generate multiple overlapping humans and animals based on keypoint\ncontrols solely. These challenges arise from two main aspects: the inherent\nlimitations of existing controllable methods and the lack of suitable datasets.\nFirst, we design a DiT-based framework, named UniMC, to explore unifying\ncontrollable multi-class image generation. UniMC integrates instance- and\nkeypoint-level conditions into compact tokens, incorporating attributes such as\nclass, bounding box, and keypoint coordinates. This approach overcomes the\nlimitations of previous methods that struggled to distinguish instances and\nclasses due to their reliance on skeleton images as conditions. Second, we\npropose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed\nfor keypoint-guided human and animal image generation. HAIG-2.9M includes 786K\nimages with 2.9M instances. This dataset features extensive annotations such as\nkeypoints, bounding boxes, and fine-grained captions for both humans and\nanimals, along with rigorous manual inspection to ensure annotation accuracy.\nExtensive experiments demonstrate the high quality of HAIG-2.9M and the\neffectiveness of UniMC, particularly in heavy occlusions and multi-class\nscenarios.", "AI": {"tldr": "Although significant advancements have been achieved in the progress of keypoint-guided Text-to-Image diffusion models, existing mainstream keypoint-guided models encounter challenges in controlling the generation of more general non-rigid objects beyond humans and animals. Extensive experiments demonstrate the high quality of HAIG-2.9M and the effectiveness of UniMC, particularly under heavy occlusions and multi-class scenarios.", "motivation": "To address the problems arising from the limitations of controllable methods for generating images with multiple overlapping humans and animals and the lack of suitable datasets for this task, the authors propose a new framework, UniMC, and a new dataset, HAIG-2.9M.", "method": "Structure", "result": "{\"tldr\": \"\\u8be5\\u70b9\\u5b9a\\u5f00\\u53d1\\u6a21\\u578b\\u4e0e\\u8d44\\u6599\\u96c6\\u79ef\\u5408\\u5f62\\u6210\\u529b\\u5f0f\\u63a8\\u51fa\\u4e86UniMC\\uff0c\\u4ee5\\u53ca\\u4e00\\u4e2a\\u4e2d\\u5927\\u8981\\u70b9\\u5f00\\u53d1\\u8d44\\u6599\\u96c6\\u5728\\u4eba\\u7269\\u548c\\u5c0f\\u5b69\\u56fe\\u50cf\\u751f\\u6210\\u4e2d\\u7684\\u4e00\\u7247\\uff1aHAIG-2.9M\\uff0e\", \"motivation\": \"\\u5bf9\\u4e0e\\u73b0\\u6709\\u7684\\u4e00\\u4e9b\\u4e0d\\u8db3\\u7b2c\\u96c6\\u4e0e\\u7b2c\\u4e00\\u4e2a\\u9879\\u76ee\\uff0c\\u4ee5\\u53ca\\u4e0d\\u8db3\\u4eba\\u7269\\u548c\\u5c0f\\u5b69\\u7684\\u793e\\u4f1a\\u5206\\u6210\\u65b9\\u9762\\u7684\\u4e0a\\u5c01\\u4e0d\\u53ca\\u591a\\u7c7b\\u8d44\\u6599\\u96c6\\uff0c\\u8be5\\u7406\\u8bba\\u751f\\u6210\\u51fa\\u4e86UniMC\\u548cHAIG-2.9M\\uff0e\", \"method\": \"\\u8be5\\u7ba1\\u7406\\u5b8c\\u6574\\u5305\\u542b\\u4e86\\u7269\\u4f53\\u7ea7\\u548c\\u70b9\\u7ea7\\u6761\\u4ef6\\u7684\\u8f6c\\u5316\\uff0c\\u4ee5\\u53ca\\u4e00\\u4e2a\\u4e2d\\u5927\\u8d44\\u6599\\u96c6\\u5e26\\u6709\\u7a7a\\u95f4\\u4fe1\\u606f\\u548c\\u70b9\\u5b9a\\u660e\\u70b9\\uff0e\", \"result\": \"\\u5b9e\\u9a8c\\u7ed3\\u679c\\u63d0\\u793a\\u4e86HAIG-2.9M\\u6216\\u8005UniMC\\u5728\\u591a\\u7c7b\\u548c\\u4e0a\\u5c01\\u8d8a\\u7ea7\\u573a\\u666f\\u4e2d\\u7684\\u6709\\u6548\\u6027\\uff0e\", \"conclusion\": \"\\u8be5\\u7406\\u8bba\\u4e3b\\u8981\\u6d4b\\u8bd5\\u4e86\\u4e2d\\u5927\\u8d44\\u6599\\u96c6\\u548c\\u5176\\u4e0a\\u5c01\\u8d8a\\u7ea7\\u51b3\\u4e49\\u5177\\u4f53\\u7684\\u975e\\u5e38\\u6709\\u529b\\u3002\"}", "conclusion": "The paper concludes by evaluating the performance of the proposed dataset and framework specifically under challenging conditions such as heavy occlusions and multi-class scenarios. The results indicate that the proposed methods handle such conditions effectively."}}
{"id": "2507.02714", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02714", "abs": "https://arxiv.org/abs/2507.02714", "authors": ["Yuxuan Wang", "Tianwei Cao", "Huayu Zhang", "Zhongjiang He", "Kongming Liang", "Zhanyu Ma"], "title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models", "comment": "ICCV 2025", "summary": "Image generation has achieved remarkable progress with the development of\nlarge-scale text-to-image models, especially diffusion-based models. However,\ngenerating human images with plausible details, such as faces or hands, remains\nchallenging due to insufficient supervision of local regions during training.\nTo address this issue, we propose FairHuman, a multi-objective fine-tuning\napproach designed to enhance both global and local generation quality fairly.\nSpecifically, we first construct three learning objectives: a global objective\nderived from the default diffusion objective function and two local objectives\nfor hands and faces based on pre-annotated positional priors. Subsequently, we\nderive the optimal parameter updating strategy under the guidance of the\nMinimum Potential Delay (MPD) criterion, thereby attaining fairness-ware\noptimization for this multi-objective problem. Based on this, our proposed\nmethod can achieve significant improvements in generating challenging local\ndetails while maintaining overall quality. Extensive experiments showcase the\neffectiveness of our method in improving the performance of human image\ngeneration under different scenarios.", "AI": {"tldr": "本文提出FairHuman方法解决人像生成中局部细节生成质量低的问题，通过多目标优化策略，提升了人像生成的整体质量，尤其是在手部和面部细节生成方面。", "motivation": "由于训练过程中局部区域的监督不足，使用大规模文本到图像模型，特别是基于扩散的模型，生成具有高度细节（如面部或手部）的人像仍然是一个挑战。", "method": "本文提出了一种名为FairHuman的多目标微调方法，旨在公平提升全局和局部图像生成的质量。具体来说，构建了三个学习目标：一个全局目标，是从默认的扩散模型目标函数中衍生；两个局部目标，分别针对手部和面部，基于预注释的位置先验。随后，根据最小潜在延迟(MPD)标准导出了最优参数更新策略，实现了多目标问题的公平优化。", "result": "实验结果显示，该方法在不同场景下提升了人类图像生成的整体性能，特别是在生成具有挑战性的局部细节时，同时保持了整体质量。", "conclusion": "该研究通过FairHuman方法在人像生成中取得了重要的进展，特别是在改善局部细节生成方面，同时保持了整体图像质量。"}}
{"id": "2507.02743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02743", "abs": "https://arxiv.org/abs/2507.02743", "authors": ["Mélanie Gaillochet", "Mehrdad Noori", "Sahar Dastani", "Christian Desrosiers", "Hervé Lombaert"], "title": "Prompt learning with bounding box constraints for medical image segmentation", "comment": "Accepted to IEEE Transactions on Biomedical Engineering (TMBE), 14\n  pages", "summary": "Pixel-wise annotations are notoriously labourious and costly to obtain in the\nmedical domain. To mitigate this burden, weakly supervised approaches based on\nbounding box annotations-much easier to acquire-offer a practical alternative.\nVision foundation models have recently shown noteworthy segmentation\nperformance when provided with prompts such as points or bounding boxes. Prompt\nlearning exploits these models by adapting them to downstream tasks and\nautomating segmentation, thereby reducing user intervention. However, existing\nprompt learning approaches depend on fully annotated segmentation masks. This\npaper proposes a novel framework that combines the representational power of\nfoundation models with the annotation efficiency of weakly supervised\nsegmentation. More specifically, our approach automates prompt generation for\nfoundation models using only bounding box annotations. Our proposed\noptimization scheme integrates multiple constraints derived from box\nannotations with pseudo-labels generated by the prompted foundation model.\nExtensive experiments across multimodal datasets reveal that our weakly\nsupervised method achieves an average Dice score of 84.90% in a limited data\nsetting, outperforming existing fully-supervised and weakly-supervised\napproaches. The code is available at\nhttps://github.com/Minimel/box-prompt-learning-VFM.git", "AI": {"tldr": "本文提出一种新的框架，结合基础视觉模型和弱监督分割的效率，使用边界框注释自动生成提示并优化，显著降低了医疗图像分割中的注释成本。", "motivation": "医疗领域的像素级注释劳动强度大且成本高。本文旨在通过结合基础模型和弱监督分割的优势，提出一种仅依赖边界框注释的自动化提示生成框架，以减少用户干预并提高效率。", "method": "本文提出了一种结合基础模型表示能力和弱监督分割注释效率的新框架。具体来说，该方法使用仅基于边界框注释自动生成提示，并整合了多个从框注释导出的约束以及由提示基础模型生成的伪标签的优化方案。", "result": "实验结果显示，在有限数据集上，该弱监督方法的平均Dice评分为84.90%，优于现有的全监督和弱监督方法。", "conclusion": "所提出的方法在多重模态数据集上进行了验证，结果表明提出的弱监督方法具有良好的效果，并且优于现有的方法。"}}
{"id": "2507.02747", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.02747", "abs": "https://arxiv.org/abs/2507.02747", "authors": ["Jiawei He", "Danshi Li", "Xinqiang Yu", "Zekun Qi", "Wenyao Zhang", "Jiayi Chen", "Zhaoxiang Zhang", "Zhizheng Zhang", "Li Yi", "He Wang"], "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale", "comment": null, "summary": "As large models gain traction, vision-language-action (VLA) systems are\nenabling robots to tackle increasingly complex tasks. However, limited by the\ndifficulty of data collection, progress has mainly focused on controlling\nsimple gripper end-effectors. There is little research on functional grasping\nwith large models for human-like dexterous hands. In this paper, we introduce\nDexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction\naligned with language instructions using single-view RGBD input. To accomplish\nthis, we generate a dataset of 170 million dexterous grasp poses mapped to\nsemantic parts across 174,000 objects in simulation, paired with detailed\npart-level captions. This large-scale dataset, named DexGraspNet 3.0, is used\nto train a VLM and flow-matching-based pose head capable of producing\ninstruction-aligned grasp poses for tabletop objects. To assess DexVLG's\nperformance, we create benchmarks in physics-based simulations and conduct\nreal-world experiments. Extensive testing demonstrates DexVLG's strong\nzero-shot generalization capabilities-achieving over 76% zero-shot execution\nsuccess rate and state-of-the-art part-grasp accuracy in simulation-and\nsuccessful part-aligned grasps on physical objects in real-world scenarios.", "AI": {"tldr": "研究团队提出了DexVLG模型，用于解决大规模模型在类似人类灵巧手的抓取任务上的挑战，并通过大规模数据集训练模型，在模拟和现实世界实验中验证了模型的有效性。", "motivation": "鉴于复杂数据收集的难度，当前研究主要集中在使用大模型对简单抓取器执行抓取任务，而对于类似人类灵巧手的大模型抓取功能的研究较少。因此，该研究旨在提出一种新方法来填补这一空白，即通过视觉-语言-抓取模型对复杂的手指动作进行语言指令引导的抓取姿态预测。", "method": "本文介绍了DexVLG，这是一种用于灵巧抓取姿态预测的大规模视觉-语言-抓取模型，能够根据单视角RGBD输入和语言指令对桌面上的物体进行抓取姿态预测。为了实现这一目标，研究团队构建了一个包含1.7亿个灵巧抓取姿态的数据集DexGraspNet 3.0，该数据集包含了17.4万个物体上的语义部位，并配以详细的部位级标注。", "result": "该模型在基于物理的模拟环境下的基准测试以及现实世界的实验中均取得了优异的成绩，实现了超过76%的零样本执行成功率和模拟中的同类最佳部分抓取准确率，并且在真实场景中成功实现了部分对齐的抓取。", "conclusion": "DexVLG展示了出色的零样本泛化能力，在模拟实验中表现出色，同时在真实物体上展示了成功抓取的能力，说明该模型可以用于解决复杂环境中的人类灵巧手抓取任务。"}}
{"id": "2507.02748", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02748", "abs": "https://arxiv.org/abs/2507.02748", "authors": ["Alex Colagrande", "Paul Caillon", "Eva Feillet", "Alexandre Allauzen"], "title": "Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics", "comment": "Accepted at ECLR Workshop at ICCV 2025", "summary": "Transformers have become the de facto standard for a wide range of tasks,\nfrom image classification to physics simulations. Despite their impressive\nperformance, the quadratic complexity of standard Transformers in both memory\nand time with respect to the input length makes them impractical for processing\nhigh-resolution inputs. Therefore, several variants have been proposed, the\nmost successful relying on patchification, downsampling, or coarsening\ntechniques, often at the cost of losing the finest-scale details. In this work,\nwe take a different approach. Inspired by state-of-the-art techniques in\n$n$-body numerical simulations, we cast attention as an interaction problem\nbetween grid points. We introduce the Multipole Attention Neural Operator\n(MANO), which computes attention in a distance-based multiscale fashion. MANO\nmaintains, in each attention head, a global receptive field and achieves linear\ntime and memory complexity with respect to the number of grid points. Empirical\nresults on image classification and Darcy flows demonstrate that MANO rivals\nstate-of-the-art models such as ViT and Swin Transformer, while reducing\nruntime and peak memory usage by orders of magnitude. We open source our code\nfor reproducibility at https://github.com/AlexColagrande/MANO.", "AI": {"tldr": "本文提出了MANO，这是一种线性时间和内存复杂度的注意力机制，适用于高分辨率输入，与现有先进模型性能相当，但资源消耗更低。", "motivation": "传统的Transformer在处理高分辨率输入时，由于二次内存和时间复杂度，在实际情况中不切实际。本文旨在提出一种新方法，减少计算和内存使用，同时保持高性能。", "method": "本文介绍了Multipole Attention Neural Operator (MANO)，其灵感来源于$n$-body数值模拟技术，通过将注意力视为网格点之间的交互问题，以距离为基础的多尺度方式计算注意力。这种方法在每个注意力头中保持全局感受野，实现了与网格点数成线性的计算和内存复杂度。", "result": "实验结果表明，MANO在图像分类和达西流问题上表现优异，与ViT和Swin Transformer等模型持平，同时大幅减少运行时间和峰值内存占用。", "conclusion": "MANO通过新的注意力机制设计，在保持高性能的同时显著减少了计算资源的消耗，为处理高分辨率输入提供了新的解决方案。"}}
{"id": "2507.02751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02751", "abs": "https://arxiv.org/abs/2507.02751", "authors": ["Mingxin Liu", "Peiyuan Zhang", "Yuan Liu", "Wei Zhang", "Yue Zhou", "Ning Liao", "Ziyang Gong", "Junwei Luo", "Zhirui Wang", "Yi Yu", "Xue Yang"], "title": "Partial Weakly-Supervised Oriented Object Detection", "comment": "10 pages, 5 figures, 4 tables, source code:\n  https://github.com/VisionXLab/PWOOD", "summary": "The growing demand for oriented object detection (OOD) across various domains\nhas driven significant research in this area. However, the high cost of dataset\nannotation remains a major concern. Current mainstream OOD algorithms can be\nmainly categorized into three types: (1) fully supervised methods using\ncomplete oriented bounding box (OBB) annotations, (2) semi-supervised methods\nusing partial OBB annotations, and (3) weakly supervised methods using weak\nannotations such as horizontal boxes or points. However, these algorithms\ninevitably increase the cost of models in terms of annotation speed or\nannotation cost. To address this issue, we propose:(1) the first Partial\nWeakly-Supervised Oriented Object Detection (PWOOD) framework based on\npartially weak annotations (horizontal boxes or single points), which can\nefficiently leverage large amounts of unlabeled data, significantly\noutperforming weakly supervised algorithms trained with partially weak\nannotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware\nStudent (OS-Student) model capable of learning orientation and scale\ninformation with only a small amount of orientation-agnostic or scale-agnostic\nweak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF)\nto reduce the model's sensitivity to static filtering thresholds. Comprehensive\nexperiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD\nframework performs comparably to, or even surpasses, traditional\nsemi-supervised algorithms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.02781", "categories": ["cs.CV", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.02781", "abs": "https://arxiv.org/abs/2507.02781", "authors": ["Danrong Zhang", "Huili Huang", "N. Simrill Smith", "Nimisha Roy", "J. David Frost"], "title": "From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images", "comment": null, "summary": "In the aftermath of earthquakes, social media images have become a crucial\nresource for disaster reconnaissance, providing immediate insights into the\nextent of damage. Traditional approaches to damage severity assessment in\npost-earthquake social media images often rely on classification methods, which\nare inherently subjective and incapable of accounting for the varying extents\nof damage within an image. Addressing these limitations, this study proposes a\nnovel approach by framing damage severity assessment as a semantic segmentation\nproblem, aiming for a more objective analysis of damage in earthquake-affected\nareas. The methodology involves the construction of a segmented damage severity\ndataset, categorizing damage into three degrees: undamaged structures, damaged\nstructures, and debris. Utilizing this dataset, the study fine-tunes a\nSegFormer model to generate damage severity segmentations for post-earthquake\nsocial media images. Furthermore, a new damage severity scoring system is\nintroduced, quantifying damage by considering the varying degrees of damage\nacross different areas within images, adjusted for depth estimation. The\napplication of this approach allows for the quantification of damage severity\nin social media images in a more objective and comprehensive manner. By\nproviding a nuanced understanding of damage, this study enhances the ability to\noffer precise guidance to disaster reconnaissance teams, facilitating more\neffective and targeted response efforts in the aftermath of earthquakes.", "AI": {"tldr": "本研究提出了一种新的方法，通过语义分割技术解决地震后社交媒体图片损害评估的主观性问题，提升了损害评估的客观性和全面性。", "motivation": "传统方法依靠分类方法评估地震后的损害程度，这种方法具有主观性且无法考虑到图片中不同程度的损害。为了解决这一问题，研究人员提出了一种新的方法，使损害评估更加客观和全面。", "method": "本研究将地震后社交媒体图片的损害程度评估问题重新定义为语义分割问题，通过构建一个细分的损害程度数据集来解决现有方法的主观性和不足。该数据集将损害分为三个等级：未受损结构、受损结构和废墟。研究中使用了SegFormer模型对此数据集进行微调，从而生成地震后社交媒体图片的损害程度分割图。此外，还引入了一个新的损害程度评分系统，通过考虑图片中不同区域不同程度的损害并调整深度估计来量化损害。", "result": "通过这种方法，研究能够更客观地量化社交媒体图片中的损害程度，提供了一种更加细致的理解方式，从而能为灾难侦察团队提供精确的指导。", "conclusion": "该研究通过构建新的数据集和引入新的损害评分系统，增强了为灾难侦察团队提供精确指导的能力，使得地震后续响应更加有效和精准。"}}
{"id": "2507.02792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02792", "abs": "https://arxiv.org/abs/2507.02792", "authors": ["Liheng Zhang", "Lexi Pang", "Hang Ye", "Xiaoxuan Ma", "Yizhou Wang"], "title": "RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation", "comment": null, "summary": "Text-to-image (T2I) diffusion models have shown remarkable success in\ngenerating high-quality images from text prompts. Recent efforts extend these\nmodels to incorporate conditional images (e.g., depth or pose maps) for\nfine-grained spatial control. Among them, feature injection methods have\nemerged as a training-free alternative to traditional fine-tuning approaches.\nHowever, they often suffer from structural misalignment, condition leakage, and\nvisual artifacts, especially when the condition image diverges significantly\nfrom natural RGB distributions. By revisiting existing methods, we identify a\ncore limitation: the synchronous injection of condition features fails to\naccount for the trade-off between domain alignment and structural preservation\nduring denoising. Inspired by this observation, we propose a flexible feature\ninjection framework that decouples the injection timestep from the denoising\nprocess. At its core is a structure-rich injection module, which enables the\nmodel to better adapt to the evolving interplay between alignment and structure\npreservation throughout the diffusion steps, resulting in more faithful\nstructural generation. In addition, we introduce appearance-rich prompting and\na restart refinement strategy to further enhance appearance control and visual\nquality. Together, these designs enable training-free generation that is both\nstructure-rich and appearance-rich. Extensive experiments show that our\napproach achieves state-of-the-art performance across diverse zero-shot\nconditioning scenarios.", "AI": {"tldr": "论文提出了一种新的特征注入框架，通过解耦注入时间步骤与去噪过程，并引入其它策略，有效解决了现有方法中存在的问题，提高了图像生成质量。", "motivation": "研究动机在于克服现有特征注入方法在结构对齐、条件泄漏和视觉伪影方面的限制，特别是在条件图像与自然RGB分布差异较大时的问题。", "method": "该论文提出了一种灵活的特征注入框架，该框架将注入时间步骤与去噪过程解耦。其核心是一个结构丰富的注入模块，能够更好地适应对齐和结构保存之间的相互作用。此外还引入了外观丰富的提示和重启精炼策略以进一步增强外观控制和视觉质量。", "result": "实验结果表明，该方法在各种零样本条件场景中实现了最先进的性能。", "conclusion": "该论文通过提出灵活的特征注入框架和相关策略，实现了结构丰富和外观丰富的无训练图像生成。"}}
{"id": "2507.02798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02798", "abs": "https://arxiv.org/abs/2507.02798", "authors": ["Miguel Espinosa", "Chenhongyi Yang", "Linus Ericsson", "Steven McDonagh", "Elliot J. Crowley"], "title": "No time to train! Training-Free Reference-Based Instance Segmentation", "comment": "Preprint", "summary": "The performance of image segmentation models has historically been\nconstrained by the high cost of collecting large-scale annotated data. The\nSegment Anything Model (SAM) alleviates this original problem through a\npromptable, semantics-agnostic, segmentation paradigm and yet still requires\nmanual visual-prompts or complex domain-dependent prompt-generation rules to\nprocess a new image. Towards reducing this new burden, our work investigates\nthe task of object segmentation when provided with, alternatively, only a small\nset of reference images. Our key insight is to leverage strong semantic priors,\nas learned by foundation models, to identify corresponding regions between a\nreference and a target image. We find that correspondences enable automatic\ngeneration of instance-level segmentation masks for downstream tasks and\ninstantiate our ideas via a multi-stage, training-free method incorporating (1)\nmemory bank construction; (2) representation aggregation and (3) semantic-aware\nfeature matching. Our experiments show significant improvements on segmentation\nmetrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP),\nPASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free\napproaches on the Cross-Domain FSOD benchmark (22.4% nAP).", "AI": {"tldr": "This paper introduces a training-free method for object segmentation using a small set of reference images. By leveraging semantic priors and feature matching, it achieves state-of-the-art segmentation results on multiple benchmarks without manual prompts or complex rules.", "motivation": "The motivation is to address the limitation of the Segment Anything Model (SAM) in generating automatic prompts by utilizing a small set of reference images, thereby reducing the burden of manual visual-prompts or complex prompt-generation rules for processing a new image.", "method": "Our method consists of three stages: (1) memory bank construction; (2) representation aggregation; and (3) semantic-aware feature matching, which allows automatic generation of instance-level segmentation masks using a small set of reference images.", "result": "Experiments demonstrate significant improvements on segmentation metrics, achieving state-of-the-art performance on COCO FSOD (36.8% nAP), PASCAL VOC Few-Shot (71.2% nAP50), and outperforming other training-free approaches on the Cross-Domain FSOD benchmark (22.4% nAP).", "conclusion": "The research demonstrates that leveraging strong semantic priors and automatic generation of visual prompts through semantic-aware feature matching can effectively improve instance-level segmentation performance without needing complex prompt-generation rules or manual annotations."}}
{"id": "2507.02803", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.02803", "abs": "https://arxiv.org/abs/2507.02803", "authors": ["Gent Serifi", "Marcel C. Bühler"], "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars", "comment": "Project page: https://gserifi.github.io/HyperGaussians", "summary": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for\nhigh-quality animatable face avatars. Creating such detailed face avatars from\nvideos is a challenging problem and has numerous applications in augmented and\nvirtual reality. While tremendous successes have been achieved for static\nfaces, animatable avatars from monocular videos still fall in the uncanny\nvalley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face\nthrough a collection of 3D Gaussian primitives. 3DGS excels at rendering static\nfaces, but the state-of-the-art still struggles with nonlinear deformations,\ncomplex lighting effects, and fine details. While most related works focus on\npredicting better Gaussian parameters from expression codes, we rethink the 3D\nGaussian representation itself and how to make it more expressive. Our insights\nlead to a novel extension of 3D Gaussians to high-dimensional multivariate\nGaussians, dubbed 'HyperGaussians'. The higher dimensionality increases\nexpressivity through conditioning on a learnable local embedding. However,\nsplatting HyperGaussians is computationally expensive because it requires\ninverting a high-dimensional covariance matrix. We solve this by\nreparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.\nThis trick boosts the efficiency so that HyperGaussians can be seamlessly\nintegrated into existing models. To demonstrate this, we plug in HyperGaussians\ninto the state-of-the-art in fast monocular face avatars: FlashAvatar. Our\nevaluation on 19 subjects from 4 face datasets shows that HyperGaussians\noutperform 3DGS numerically and visually, particularly for high-frequency\ndetails like eyeglass frames, teeth, complex facial movements, and specular\nreflections.", "AI": {"tldr": "提出了一种新的扩展模型HyperGaussians，用于提高单目视频中可动画化人脸模型的品质，特别是在捕捉高频率细节方面表现出色。", "motivation": "虽然在静态人脸图像的制作上取得了很大成功，但单目视频中可动画化的人脸模型仍然存在挑战，比如难以精确捕捉非线性变形、复杂光照效果和细微特征，因此提出了一种新的解决方案。", "method": "引入了HyperGaussians，即高维多变量高斯，作为3D高斯点云表示法的新扩展，以此来提升可动画化人脸模型的制作。HyperGaussians通过学习到的局部嵌入对条件进行编码，从而提高了表达能力。为了解决高维协方差矩阵求逆带来的计算复杂度问题，提出了‘逆协方差技巧’，通过重参数化协方差矩阵提高效率。", "result": "在19位来自4个人脸数据集的主体上进行评估，结果显示HyperGaussians在数值和视觉上均超过了3DGS，尤其是对于高频率的细节如眼镜框、牙齿、复杂的面部运动和镜面反射表现更优。", "conclusion": "HyperGaussians通过提高3D高斯表示的表达能力以及解决计算复杂度问题，显著改进了从单目视频创建高质量可动画化人脸模型的效果。"}}
{"id": "2507.02813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02813", "abs": "https://arxiv.org/abs/2507.02813", "authors": ["Fangfu Liu", "Hao Li", "Jiawei Chi", "Hanyang Wang", "Minghui Yang", "Fudong Wang", "Yueqi Duan"], "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion", "comment": "Project page: https://liuff19.github.io/LangScene-X", "summary": "Recovering 3D structures with open-vocabulary scene understanding from 2D\nimages is a fundamental but daunting task. Recent developments have achieved\nthis by performing per-scene optimization with embedded language information.\nHowever, they heavily rely on the calibrated dense-view reconstruction\nparadigm, thereby suffering from severe rendering artifacts and implausible\nsemantic synthesis when limited views are available. In this paper, we\nintroduce a novel generative framework, coined LangScene-X, to unify and\ngenerate 3D consistent multi-modality information for reconstruction and\nunderstanding. Powered by the generative capability of creating more consistent\nnovel observations, we can build generalizable 3D language-embedded scenes from\nonly sparse views. Specifically, we first train a TriMap video diffusion model\nthat can generate appearance (RGBs), geometry (normals), and semantics\n(segmentation maps) from sparse inputs through progressive knowledge\nintegration. Furthermore, we propose a Language Quantized Compressor (LQC),\ntrained on large-scale image datasets, to efficiently encode language\nembeddings, enabling cross-scene generalization without per-scene retraining.\nFinally, we reconstruct the language surface fields by aligning language\ninformation onto the surface of 3D scenes, enabling open-ended language\nqueries. Extensive experiments on real-world data demonstrate the superiority\nof our LangScene-X over state-of-the-art methods in terms of quality and\ngeneralizability. Project Page: https://liuff19.github.io/LangScene-X.", "AI": {"tldr": "The paper presents LangScene-X, an innovative generative framework for 3D reconstruction that addresses the issues of rendering artifacts and poor semantic synthesis in sparse-view scenarios.", "motivation": "The goal is to overcome the limitations of current methods that rely on dense, calibrated views and suffer from rendering artifacts and implausible semantic synthesis, especially with limited view data.", "method": "We introduce LangScene-X, a generative framework that can create consistent multi-modality 3D information from sparse 2D images. It uses a TriMap video diffusion model to generate RGBs, normals, and segmentation maps, and a Language Quantized Compressor (LQC) to efficiently encode language embeddings for cross-scene generalization.", "result": "LangScene-X shows superior quality and generalizability in 3D reconstruction and understanding compared to state-of-the-art methods, demonstrated through experiments with real-world data.", "conclusion": "The proposed method, LangScene-X, significantly advances 3D structure recovery and open-vocabulary scene understanding from 2D images, particularly in scenarios with sparse views."}}
{"id": "2507.02826", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02826", "abs": "https://arxiv.org/abs/2507.02826", "authors": ["Panpan Ji", "Junni Song", "Hang Xiao", "Hanyu Liu", "Chao Li"], "title": "Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach", "comment": null, "summary": "Sensor-based Human Activity Recognition (HAR) is a core technology that\nenables intelligent systems to perceive and interact with their environment.\nHowever, multimodal HAR systems still encounter key challenges, such as\ndifficulties in cross-modal feature alignment and imbalanced modality\ncontributions. To address these issues, we propose a novel framework called the\nDynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three\nkey components. First, a dual-path feature extraction architecture is employed,\nwhere ResNet and DenseNet branches collaboratively process multimodal sensor\ndata. Second, a multi-stage contrastive learning mechanism is introduced to\nachieve progressive alignment from local perception to semantic abstraction.\nThird, we present a confidence-driven gradient modulation strategy that\ndynamically monitors and adjusts the learning intensity of each modality branch\nduring backpropagation, effectively alleviating modality competition. In\naddition, a momentum-based gradient accumulation strategy is adopted to enhance\ntraining stability. We conduct ablation studies to validate the effectiveness\nof each component and perform extensive comparative experiments on four public\nbenchmark datasets.", "AI": {"tldr": "为了解决跨模态特征对齐和模态贡献不平衡问题，提出一个新的多模态人体活动识别框架DCDP-HAR，并进行了有效性验证。", "motivation": "解决多模态人体活动识别系统中的跨模态特征对齐困难和模态贡献不平衡问题。", "method": "提出了一种称为动态对比双路径网络(DCDP-HAR)的新框架，包含三个关键组成部分：1. 采用ResNet和DenseNet分支协作处理多模态传感器数据的双路径特征提取架构；2. 引入了多阶段对比学习机制，实现从局部感知到语义抽象的逐步对齐；3. 采用基于置信度的梯度调制策略，在反向传播过程中动态监控和调整每个模态分支的学习强度，有效缓解模态竞争。同时采用了基于动量的梯度累积策略以增强训练稳定性。", "result": "通过消融研究验证了每个组件的有效性，并在四个公共基准数据集上进行了广泛的比较实验。", "conclusion": ""}}
{"id": "2507.02827", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02827", "abs": "https://arxiv.org/abs/2507.02827", "authors": ["Ying Yu", "Hang Xiao", "Siyao Li", "Jiarui Li", "Haotian Tang", "Hanyu Liu", "Chao Li"], "title": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network", "comment": null, "summary": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.", "AI": {"tldr": "针对人类活动识别中的挑战，本文提出了一种新的优化方法，利用无监督数据扩增和多注意力机制，实现在多数据集上的高性能和在轻量设备上的高效运行。", "motivation": "人类活动识别（HAR）的主要目标是从传感器数据中推断出正在进行的人类行为。然而，HAR仍然面临着关键挑战，如罕见活动的标记样本稀缺、高级特征提取不足以及在轻量设备上的模型性能不佳。", "method": "本文提出了一种基于多注意力交互机制的综合优化方法。首先，通过无监督、统计引导的扩散模型进行数据扩增，缓解标签数据稀缺和类别不平衡问题。其次，设计了一个多分支时空交互网络，捕捉序列数据的多尺度特征。同时，引入时序注意力机制以识别关键时间点，而空间注意力增强传感器间的交互。最后，引入自适应多损失函数融合策略以动态调整损失权重并优化整体模型。", "result": "实验结果表明，在WISDM、PAMAP2和OPPORTUNITY三个公开数据集上，所提出的USAD网络分别实现了98.84%、93.81%和80.92%的准确率，显著优于现有方法。", "conclusion": "此外，在嵌入式设备上的实际部署验证了该方法的高效性和可行性。"}}
{"id": "2507.02857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02857", "abs": "https://arxiv.org/abs/2507.02857", "authors": ["Ziye Li", "Hao Luo", "Xincheng Shuai", "Henghui Ding"], "title": "AnyI2V: Animating Any Conditional Image with Motion Control", "comment": "ICCV 2025, Project Page: https://henghuiding.com/AnyI2V/", "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.", "AI": {"tldr": "AnyI2V提出了一种无需训练的框架，可以基于用户自定义的运动轨迹对任意条件图像进行动画处理，使得视频生成更加灵活和多样。", "motivation": "为了解决现有T2V和I2V方法中存在的问题，如缺乏对生成内容的空间布局精准控制以及实时图像依赖限制了生成内容的编辑性，AnyI2V被提出以提供更灵活和多样化的视频生成方式。", "method": "Structure", "result": "{\\n  \\\"tldr\\\": \\\"AnyI2V提出了一种无需训练的框架，可以基于用户自定义的动...\",\\n  \\\"motivation\\\": \\\"为了解决现有T2V和I2V方法中存在的问题，如缺乏对生成内容的空间布局精准控制以及实时图像依赖限制了生成内容的编辑性，AnyI2V被提出以提供更灵活和多样化的视频生成方式。\\\",\\n  \\\"method\\\": \\\"AnyI2V支持经过用户定义的运动轨迹对任意条件图像进行动画处理，不仅限于传统图像，还拓展到了不被ControlNet支持的数据类型如网格和点云等。幸运的是，它还支持通过LoRA和文本提示进行风格转换和编辑。\\\",\\n  \\\"result\\\": \\\"通过广泛的实验验证了AnyI2V在空间-运动控制视频生成中的优越性能和提供了新的视角。\\\",\\n  \\\"conclusion\\\": \\\"AnyI2V以其独特的训练自由框架和对多种类型数据的支持，为视频生成提供了一个全新的、灵活多变的思路。\\\"\\n}", "conclusion": "AnyI2V以其独特的训练自由框架和对多种类型数据的支持，为视频生成提供了一个全新的、灵活多变的思路。"}}
{"id": "2507.02859", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02859", "abs": "https://arxiv.org/abs/2507.02859", "authors": ["Jiaer Xia", "Bingkui Tong", "Yuhang Zang", "Rui Shao", "Kaiyang Zhou"], "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation", "comment": "Accepted by ICCV2025", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in interpreting images using natural language. However, without\nusing large-scale datasets for retraining, these models are difficult to adapt\nto specialized vision tasks, e.g., chart understanding. This problem is caused\nby a mismatch between pre-training and downstream datasets: pre-training\ndatasets primarily concentrate on scenes and objects but contain limited\ninformation about specialized, non-object images, such as charts and tables. In\nthis paper, we share an interesting finding that training an MLLM with\nChain-of-Thought (CoT) reasoning data can facilitate model adaptation in\nspecialized vision tasks, especially under data-limited regimes. However, we\nidentify a critical issue within CoT data distilled from pre-trained MLLMs,\ni.e., the data often contains multiple factual errors in the reasoning steps.\nTo address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple\nbootstrapping-based approach that aims to inject grounding information (i.e.,\nbounding boxes) into CoT data, essentially making the reasoning steps more\nfaithful to input images. We evaluate our approach on five specialized vision\ntasks, which cover a variety of visual formats including charts, tables,\nreceipts, and reports. The results demonstrate that under data-limited regimes\nour approach significantly improves upon fine-tuning and distillation.", "AI": {"tldr": "本文研究通过使用包含Chain-of-Thought (CoT) 推理数据训练MLLMs，以改善其在专业视觉任务中的适应性。提出的方法Grounded Chain-of-Thought (GCoT)，在多种视觉格式的任务中显示了显著的改进效果。", "motivation": "本文旨在解决现有的多模态大型语言模型（MLLMs）难以适应专业视觉任务的问题，特别是在数据受限的情况下训练MLLMs。", "method": "提出了一种基于自举的方法——Grounded Chain-of-Thought (GCoT)，旨在将绑定信息（即，边界框）注入到CoT数据中，使推理步骤更忠于输入图像。", "result": "<tool_call>", "conclusion": "实验结果表明，在数据受限的情况下，该方法相比微调和蒸馏方法显著提高了性能。"}}
{"id": "2507.02860", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02860", "abs": "https://arxiv.org/abs/2507.02860", "authors": ["Xin Zhou", "Dingkang Liang", "Kaijin Chen", "Tianrui Feng", "Xiwu Chen", "Hongkai Lin", "Yikang Ding", "Feiyang Tan", "Hengshuang Zhao", "Xiang Bai"], "title": "Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching", "comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/", "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.", "AI": {"tldr": "EasyCache is a framework that speeds up video generation models by reusing computation without requiring additional training or complex tuning, making high-quality video synthesis faster and more accessible.", "motivation": "The motivation behind this paper is to address the slow inference speeds and high computational costs associated with current video generation models, which limit their practical use. By improving these aspects, the technology can be more widely adopted.", "method": "The paper introduces EasyCache, a training-free acceleration framework for video diffusion models. It employs a lightweight, runtime-adaptive caching mechanism to avoid redundant computations by reusing previous transformation vectors.", "result": "The study shows that EasyCache can reduce inference time by a factor of 2.1-3.3x across different large-scale video generation models while improving PSNR (fidelity) by up to 36% over the previous state-of-the-art method.", "conclusion": "EasyCache stands out as a highly efficient and accessible option for accelerating high-quality video generation processes, making advanced video synthesis techniques more available for both research and real-world applications."}}
{"id": "2507.02861", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.02861", "abs": "https://arxiv.org/abs/2507.02861", "authors": ["Zhening Huang", "Xiaoyang Wu", "Fangcheng Zhong", "Hengshuang Zhao", "Matthias Nießner", "Joan Lasenby"], "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans", "comment": "Project Page: https://litereality.github.io; Video:\n  https://www.youtube.com/watch?v=ecK9m3LXg2c&feature=youtu.be", "summary": "We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor\nenvironments into compact, realistic, and interactive 3D virtual replicas.\nLiteReality not only reconstructs scenes that visually resemble reality but\nalso supports key features essential for graphics pipelines -- such as object\nindividuality, articulation, high-quality physically based rendering materials,\nand physically based interaction. At its core, LiteReality first performs scene\nunderstanding and parses the results into a coherent 3D layout and objects with\nthe help of a structured scene graph. It then reconstructs the scene by\nretrieving the most visually similar 3D artist-crafted models from a curated\nasset database. Next, the Material Painting module enhances realism by\nrecovering high-quality, spatially varying materials. Finally, the\nreconstructed scene is integrated into a simulation engine with basic physical\nproperties to enable interactive behavior. The resulting scenes are compact,\neditable, and fully compatible with standard graphics pipelines, making them\nsuitable for applications in AR/VR, gaming, robotics, and digital twins. In\naddition, LiteReality introduces a training-free object retrieval module that\nachieves state-of-the-art similarity performance on the Scan2CAD benchmark,\nalong with a robust material painting module capable of transferring\nappearances from images of any style to 3D assets -- even under severe\nmisalignment, occlusion, and poor lighting. We demonstrate the effectiveness of\nLiteReality on both real-life scans and public datasets. Project page:\nhttps://litereality.github.io; Video:\nhttps://www.youtube.com/watch?v=ecK9m3LXg2c", "AI": {"tldr": "LiteReality 是通过 RGB-D 扫描创建逼真 3D 虚拟环境的新技术，集成了大量图形处理和物理交互特性和优化策略，可在不经过训练的前提下完成高质量模型的检索。", "motivation": "此项研究机动于开发一种能够创建真实交互式 3D 虚拟环境的技术，旨在填补现有技术在对象独立性、材质质量和交互性等方面的不足。", "method": "其技术方法包括场景理解、检索 3D 模型以补充场景内容、增强材料的材质绘制以及将重建的场景整合进一个物理模拟引擎中。", "result": "LiteReality 是一种将室内环境的 RGB-D 扫描转化为逼真、紧凑且可交互的 3D 虚拟副本的新型流程。它不仅能够还原视觉上接近现实的场景，还支持图形管线必须的关键特性，如对象独立性、处理细节、高质量基于物理渲染材质以及真实物理交互。LiteReality 通过一个结构化的场景图进行场景理解，并将其解析为一个连贯的 3D 布局和对象。后续步骤从精心挑选的资产库中检索出最匹配视觉的 3D 模型进行场景重建，再通过材质绘制模块恢复高质量的空间变化材质。最后，将重建的场景整合到一个具有基本物理特性的模拟引擎中，以实现交互性行为。生成的场景不仅紧凑且可编辑，还与标准图形管线兼容，适合用来进行增强现实/虚拟现实 (AR/VR)、游戏、机器人以及数字孪生等应用。此外，LiteReality 还引入了一个无需训练的对象检索模块，在 Scan2CAD 标准上的表现在同类技术中处于领先。需要注意的是，该材质绘制模块能够在存在严重错位、遮挡和光照不良等情况下依然能够从任意风格的图像中转移外观效果。实验结果显示，LiteReality 能够有效处理真实环境扫描的场景和公共数据集。", "conclusion": "LiteReality 证明了其在创建从真实扫描到可交互 3D 虚拟场景转换的能力，并且其处理效果优于同类方法，表现出强大的材料转移能力和稳健的对象检索性能。"}}
{"id": "2507.02862", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02862", "abs": "https://arxiv.org/abs/2507.02862", "authors": ["Xiang Fan", "Xiaohang Sun", "Kushan Thakkar", "Zhu Liu", "Vimal Bhat", "Ranjay Krishna", "Xiang Hao"], "title": "RefTok: Reference-Based Tokenization for Video Generation", "comment": null, "summary": "Effectively handling temporal redundancy remains a key challenge in learning\nvideo models. Prevailing approaches often treat each set of frames\nindependently, failing to effectively capture the temporal dependencies and\nredundancies inherent in videos. To address this limitation, we introduce\nRefTok, a novel reference-based tokenization method capable of capturing\ncomplex temporal dynamics and contextual information. Our method encodes and\ndecodes sets of frames conditioned on an unquantized reference frame. When\ndecoded, RefTok preserves the continuity of motion and the appearance of\nobjects across frames. For example, RefTok retains facial details despite head\nmotion, reconstructs text correctly, preserves small patterns, and maintains\nthe legibility of handwriting from the context. Across 4 video datasets (K600,\nUCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms\ncurrent state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all\nevaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or\nhigher compression ratios. When a video generation model is trained using\nRefTok's latents on the BAIR Robot Pushing task, the generations not only\noutperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters,\nacross all generation metrics by an average of 27.9%.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.02863", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02863", "abs": "https://arxiv.org/abs/2507.02863", "authors": ["Yuqi Wu", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory", "comment": "Code is available at: https://github.com/YkiWu/Point3R", "summary": "Dense 3D scene reconstruction from an ordered sequence or unordered image\ncollections is a critical step when bringing research in computer vision into\npractical scenarios. Following the paradigm introduced by DUSt3R, which unifies\nan image pair densely into a shared coordinate system, subsequent methods\nmaintain an implicit memory to achieve dense 3D reconstruction from more\nimages. However, such implicit memory is limited in capacity and may suffer\nfrom information loss of earlier frames. We propose Point3R, an online\nframework targeting dense streaming 3D reconstruction. To be specific, we\nmaintain an explicit spatial pointer memory directly associated with the 3D\nstructure of the current scene. Each pointer in this memory is assigned a\nspecific 3D position and aggregates scene information nearby in the global\ncoordinate system into a changing spatial feature. Information extracted from\nthe latest frame interacts explicitly with this pointer memory, enabling dense\nintegration of the current observation into the global coordinate system. We\ndesign a 3D hierarchical position embedding to promote this interaction and\ndesign a simple yet effective fusion mechanism to ensure that our pointer\nmemory is uniform and efficient. Our method achieves competitive or\nstate-of-the-art performance on various tasks with low training costs. Code is\navailable at: https://github.com/YkiWu/Point3R.", "AI": {"tldr": "利用显式的空间指针记忆技术实现了在线密集流式3D重建，表现出优越性能和较低训练成本。", "motivation": "现有的密集3D重建方法依赖于一个隐式记忆，这在容量上存在限制并且可能会早期帧的信息丢失。因此，需要一种新方法以适应密集流式3D重建。", "method": "Point3R方法是一种面向在线密集流式3D重建的框架，它包含一个显式的空间指针记忆（spatial pointer memory），该记忆与当前场景的3D结构直接关联。每个指针被分配一个特定的3D位置，并将全球坐标系附近的情景信息聚集为一个变化的空间特征。通过设计一个3D分层位置嵌入以促进这些指针之间的交互，并设计了一种简单但有效的融合机制，确保了其统一性和效率。", "result": "该方法在多种任务中实现了具有竞争力或最先进的性能，并且训练成本较低。", "conclusion": "Point3R方法利用显式空间指针记忆，确保了信息有效地融入到全局坐标系中，并通过低位姿态嵌入和融合机制提高了效率和性能，证明了算法的有效性。实现上，在不同任务中表现出优异性能，同时保持低训练成本。"}}
