{"id": "2512.20735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20735", "abs": "https://arxiv.org/abs/2512.20735", "authors": ["Shijing Wang", "Chaoqun Cui", "Yaping Huang", "Hyung Jin Chang", "Yihua Cheng"], "title": "VL4Gaze: Unleashing Vision-Language Models for Gaze Following", "comment": null, "summary": "Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.", "AI": {"tldr": "Introducing VL4Gaze, the first large-scale benchmark for evaluating and training vision-language models on human gaze understanding tasks, showing significant performance gains through targeted training.", "motivation": "To explore and improve the capability of vision-language models (VLMs) in interpreting human gaze, which is crucial for understanding attention, intention, and social behaviors, by creating a systematic benchmark for gaze analysis.", "method": "VL4Gaze, a large-scale benchmark that consists of 489K automatically generated question-answer pairs across 124K images, aiming to evaluate and enhance the gaze understanding abilities of vision-language models through four specific tasks.", "result": "Evaluation demonstrates that VLMs without special training struggle with gaze interpretation tasks. Training on VL4Gaze, however, significantly improves performance across all four gaze understanding tasks, indicating the necessity of specialized task supervision.", "conclusion": "Targeted multi-task supervision, as provided by the VL4Gaze benchmark, is essential for enhancing gaze understanding in VLMs, suggesting that the general vision-language pre-training alone is insufficient for reliable gaze interpretation."}}
{"id": "2512.20746", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20746", "abs": "https://arxiv.org/abs/2512.20746", "authors": ["Tony Tran", "Bin Hu"], "title": "TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection", "comment": "10 pages. The paper has been accepted by the WACV 2026 workshop", "summary": "This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.", "AI": {"tldr": "该研究提出了一种硬件感知的神经架构搜索框架，用于在TinyML设备上实现垃圾检测，提高了检测准确性并减少了参数量。", "motivation": "为了解决在TinyML约束下，边缘和物联网设备上进行TACO数据集下的垃圾检测问题。", "method": "采用了一种迭代的硬件感知神经架构搜索框架，构建了一种Once-for-All风格的ResDets超网络，并通过交替优化主干和-neck/-head部分来寻找最优解，同时还使用了种群通过机制和准确性预测来降低成本和提高稳定性。", "result": "最强版本TrashDet-l，在五个类别中达到了19.5 mAP50，并且参数量比之前的检测器减少了数千个。其他的TrashDet变种参数量在1.2M到30.5M之间，mAP50值从11.4到19.5不等。在MAX78002微控制器上，两种特化变种TrashDet-ResNet和TrashDet-MBNet不仅减少了能源消耗，也减少了延迟和平均功率。", "conclusion": "这项研究证明了他们的方法可以在TinyML设备上提供高性能的垃圾检测模型。它的垃圾检测模型（TrashDets）为资源有限的设备在不同的部署预算中提供了可伸缩的检测器选择。"}}
{"id": "2512.20770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20770", "abs": "https://arxiv.org/abs/2512.20770", "authors": ["Markus Gross", "Sai B. Matha", "Aya Fahmy", "Rui Song", "Daniel Cremers", "Henri Meess"], "title": "OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective", "comment": null, "summary": "Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.", "AI": {"tldr": "本文提出了OccuFly，一个基于摄像头的航空SSC数据集，并提出了一种无LiDAR的3D场景理解数据生成方法。", "motivation": "传统的SSC研究主要集中在陆地上，而在空中场景方面研究不足，同时，由于空中飞行器的限制，LiDAR传感器的应用受到限制。因此，作者希望通过提出基于摄像头的SSC数据生成框架来弥补这一领域空白。", "method": "通过利用传统3D重建技术，作者提出了一种基于相机的无LiDAR数据生成框架，该框架可以通过将部分注释的2D掩模提升到重建的点云中来自动转移标签，从而大幅度减少手动的3D标注工作。", "result": "作者引入了OccuFly，这是一个基于摄像头的航空SSC真实世界基准，该基准在50米、40米和30米高度拍摄，涵盖春、夏、秋、冬四个季节。", "conclusion": "通过在OccuFly基准上进行的评估，作者讨论了高空视角带来的具体挑战，并提出了一套完整的方法来促进对整体航空三维场景理解的研究。"}}
{"id": "2512.20638", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20638", "abs": "https://arxiv.org/abs/2512.20638", "authors": ["Matyas Bohacek", "Nino Scherrer", "Nicholas Dufour", "Thomas Leung", "Christoph Bregler", "Stephanie C. Y. Chan"], "title": "Uncovering Competency Gaps in Large Language Models and Their Benchmarks", "comment": null, "summary": "The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak (\"model gaps\") and (ii) imbalanced coverage in the benchmarks themselves (\"benchmark gaps\"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.", "AI": {"tldr": "本文通过一种基于稀疏自动编码器的新方法，自动识别大型语言模型中的概念级弱点和现有基准体系的覆盖不足，为理解和改进模型提供了新的视角。", "motivation": "现有的大型语言模型评估因为依赖于标准化的基准，往往忽略了模型在特定子领域中的弱点以及基准本身在覆盖范围上的不平衡问题。因此，本文的研究动机在于探索新的方法来自动识别和填补这些模型和基准的差距，从而改进评估的全面性和细致性。", "method": "该研究采用了稀疏自动编码器（SAEs）来自动分析大型语言模型的内部表征，通过计算每一个概念的显着性能得分，系统地发现模型在特定概念上的弱点以及现有基准体系的不足。", "result": "{\"tldr\": \"本文提出一种新的方法，利用稀疏自动编码器（SAEs）自动发现大型语言模型中的模型差距和基准差距。该方法通过在基准数据上计算每个概念的显着性能得分，提供了基准分数的概念级分解，不仅补充了传统的聚合指标，而且揭示了模型得分背后的原因和基准应如何发展以更好地反映它们的预期范围。\", \"motivation\": \"现有的评估大型语言模型的方法依赖于标准化的基准，但这些聚合度量可能会模糊模型在特定子领域的弱点和基准本身的不平衡覆盖范围。因此，本文旨在提出一种新的自动发现模型差距和基准差距的方法。\", \"method\": \"本文提出的方法使用稀疏自动编码器（SAEs）来提取概念激活，并在基准数据上计算显着性能得分。这使得评估可以基于模型的内部表示来进行，并允许在不同基准之间进行比较。\", \"result\": \"该方法应用于两个流行的开源模型和十个基准，发现这些模型在与谄媚行为对立的概念上表现出一致的低分（例如，礼貌地拒绝请求或维护界限）和与安全性讨论有关的概念上也表现欠佳。此外还观察到许多评估的基准超代表与服从、权威或指令遵循相关概念，而忽略了应在它们预期范围内的核心概念。\", \"conclusion\": \"本文提出的方法提供了一种表示基础的评估方法，它不仅补充了传统的聚合度量，提供了概念级别分解，揭示了模型得分背后的原因，还如何改进基准以更好地反映预期范围提供了见解。\"}", "conclusion": "本文提出的方法提供了一种基于模型内部表示的评估方法，使得分析可以深入到概念级别。这种方法不仅可以填补传统聚合指标的不足，还可以揭示出导致模型得分的原因，以及如何改进现有的基准体系以更好地体现其预期的目标范围。"}}
{"id": "2512.20783", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20783", "abs": "https://arxiv.org/abs/2512.20783", "authors": ["Raja Mallina", "Bryar Shareef"], "title": "NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts", "comment": "5 pages, 2 figures, and 4 tables", "summary": "Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.", "AI": {"tldr": "本文提出了一种名为NullBUS的框架，可以有效地处理带有和不带有提示的乳腺超声图像，显著提高了分割精度和模型的鲁棒性。", "motivation": "当前，许多用于乳腺超声（BUS）分割的可提示方法由于公共数据集缺乏可靠的元数据或报告而受到限制，这限制了训练模型所需的多模态子集，并降低了鲁棒性。", "method": "提出了一种名为NullBUS的多模态混合监督框架，该框架可以从带有和不带有提示（prompt）的图像中学习。为了处理缺失的文本信息，引入了可空提示，通过可学习的空值嵌入和存在掩码实现，可以在没有元数据时回退到仅基于图像的证据，并在有文本时使用文本信息。", "result": "在包含三个公共BUS数据集的统一池中进行评估，NullBUS实现了0.8568的平均IoU和0.9103的平均Dice值，显示出在提示可用性混合情况下的先进性能。", "conclusion": "NullBUS框架通过引入可空提示解决了现有数据集中元数据短缺的问题，提高了乳腺超声图像分割的鲁棒性和性能。"}}
{"id": "2512.20724", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20724", "abs": "https://arxiv.org/abs/2512.20724", "authors": ["Alexandros Christoforos", "Chadbourne Davis"], "title": "SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention", "comment": "Under submission", "summary": "Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.", "AI": {"tldr": "The paper introduces SA-DiffuSeq, a diffusion-based framework with sparse attention for efficient long text generation, showing improvements over state-of-the-art methods in scalability and sampling speed.", "motivation": "The motivation of this paper is to address the high computational cost and memory overhead involved in generating long texts using diffusion models.", "method": "SA-DiffuSeq uses sparse attention to reduce computational complexity in the diffusion process. It includes a soft absorbing state to stabilize the diffusion process and improve sampling efficiency.", "result": "The experiments show SA-DiffuSeq outperforms existing diffusion baselines in terms of both training efficiency and sampling speed, especially for longer sequences.", "conclusion": "The incorporation of structured sparsity into diffusion models, as demonstrated with SA-DiffuSeq, is effective for generating long form texts efficiently and accurately, suitable for applications like scientific writing, large-scale code generation, and long-context dialogues."}}
{"id": "2512.20815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20815", "abs": "https://arxiv.org/abs/2512.20815", "authors": ["Reeshad Khan amd John Gauch"], "title": "Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation", "comment": null, "summary": "Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.", "AI": {"tldr": "文章提出了一种任务驱动的协同设计框架，该框架将光学、传感器建模和语义分割网络集成到一个端到端的RAW到任务管道中，显著提高了语义分割性能，并证明了在边缘设备上的可部署性。", "motivation": "传统的自动驾驶管道将相机设计与下游感知分离，依赖于固定光学和手工设计的ISP，这优先考虑人类可视图像，而不是机器语义。这种分离在插值、降噪或量化过程中会丢失信息，同时迫使模型适应传感器的伪像。因此，本文提出了一个集成光学、传感器和网络协同优化框架，针对机器视觉任务进行优化。", "method": "传统自动驾驶管道将相机设计与下游感知分离，而本文提出了一种任务驱动的协同设计框架，将光学、传感器建模和轻量级语义分割网络整合进一个端到端的RAW至任务管道中。该框架在DeepLens的基础上，整合了现实的手机级镜头模型、可学习的颜色滤光阵列、泊松-高斯噪声过程和量化，并直接针对分割目标进行优化。", "result": "在KITTI-360上的评估显示，与固定管道相比，该方法在mIoU上有一致的提高，尤其是在光学建模和CFA学习方面，对薄或低光敏感类别的提升尤为显著。此外，该模型参数量少，运行速度快，达到了约28 FPS，证明了其在边缘设备上的可部署性。", "conclusion": "通过视觉和定量分析，文章表明，协同设计的传感器可以将采集过程适应于语义结构，从而提高边界清晰度并维持在模糊、噪声和低比特深度下的准确性。研究结果表明，光学、传感器和网络的全栈协同优化是实现高效、可靠和可部署感知的自动驾驶系统的基础。"}}
{"id": "2512.20757", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20757", "abs": "https://arxiv.org/abs/2512.20757", "authors": ["Gül Sena Altıntaş", "Malikeh Ehghaghi", "Brian Lester", "Fengyuan Liu", "Wanru Zhao", "Marco Ciccone", "Colin Raffel"], "title": "TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior", "comment": null, "summary": "Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.", "AI": {"tldr": "研究通过TokSuite，涵盖了模型训练和基准测试，展示了不同分词器对语言模型的影响，揭示了它们的优缺点。", "motivation": "分词方法对于语言模型的表现和行为很关键，但由于难以独立测量其作用，当前对其影响了解不深入。本研究旨在深入理解分词器的影响。", "method": "使用了14种不同的分词器来训练模型，同时保持其他条件如架构、数据集、训练预算和初始化一致，来研究分词方法对语言模型的影响。", "result": "研究提供了详细的分词器在不同条件下的表现，揭示了广泛使用的分词器的各自优点和不足。", "conclusion": "TokSuite为研究分词器对语言模型的影响提供了一套强大而稳健的方法，促进了对分词器选择的理解。"}}
{"id": "2512.20833", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.20833", "abs": "https://arxiv.org/abs/2512.20833", "authors": ["Vidit Agrawal", "John Peters", "Tyler N. Thompson", "Mohammad Vali Sanian", "Chau Pham", "Nikita Moshkov", "Arshad Kazi", "Aditya Pillai", "Jack Freeman", "Byunguk Kang", "Samouil L. Farhi", "Ernest Fraenkel", "Ron Stewart", "Lassi Paavolainen", "Bryan A. Plummer", "Juan C. Caicedo"], "title": "CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images", "comment": "47 Pages, 23 Figures, 26 Tables", "summary": "Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.", "AI": {"tldr": "CHAMMI-75, an open-access dataset of diverse microscopy images, enhances the creation of adaptable cellular morphology models that perform well across different imaging modalities.", "motivation": "The motivation stems from the need to develop more adaptable and universally applicable cellular morphology models that can work across different microscopy imaging types and experimental conditions.", "method": "The method involves the creation and use of CHAMMI-75, an open-access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. The goal is to develop channel-adaptive cellular morphology models that can process any type of microscopy image.", "result": "Training with CHAMMI-75 improved performance in multi-channel bioimaging tasks, primarily due to the high diversity in microscopy modalities.", "conclusion": "The work lays the foundation for the development of the next generation of cellular morphology models for biological studies, highlighting the importance of diverse training datasets."}}
{"id": "2512.20773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20773", "abs": "https://arxiv.org/abs/2512.20773", "authors": ["Ziyi Zhu", "Olivier Tieleman", "Caitlin A. Stamatis", "Luka Smyth", "Thomas D. Hull", "Daniel R. Cahn", "Matteo Malgaroli"], "title": "Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization", "comment": null, "summary": "Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.", "AI": {"tldr": "通过对抗训练迭代提高用户模拟器的现实性，应用于心理健康支持聊天机器人，显示出相比初始模型在揭示系统问题方面具有显著优势。", "motivation": "研究的动机在于，现实的用户模拟对于训练和评估任务导向对话系统至关重要，但准确复制人类行为始终具有挑战性。有效的模拟器应能揭示系统失败模式。", "method": "本研究提出了一种对抗训练框架，通过生成器（用户模拟器）和判别器之间的竞争动态，迭代地提高了用户模拟器的现实性。", "result": "经过对抗训练的模拟器在多样性和模拟结果与现实数据的分布一致性及预测有效性方面都有显著提高。判别器的准确度在三次对抗迭代后显著下降，表明模拟器更加现实。", "conclusion": "研究提供了证据，表明对抗训练是创造心理健康支持任务导向对话系统中现实用户模拟器的一种有前景的方法。它可以高效、可靠且低成本地对系统进行评估。"}}
{"id": "2512.20839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20839", "abs": "https://arxiv.org/abs/2512.20839", "authors": ["Putu Indah Githa Cahyani", "Komang David Dananjaya Suartana", "Novanto Yudistira"], "title": "Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.20780", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.20780", "abs": "https://arxiv.org/abs/2512.20780", "authors": ["Ramatu Oiza Abdulsalam", "Segun Aroyehun"], "title": "Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles", "comment": null, "summary": "Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.", "AI": {"tldr": "对大型语言模型的数学辅导回应与人类专家的行为进行了对比分析，发现模型在教学质量和语言特征上接近但有所不同。", "motivation": "探究大型语言模型生成的数学辅导回应与专家人类实践的教学行为有多接近。", "method": "采用对照实验，将专家人类导师、新手人类导师和多个大型语言模型对同一组数学辅导对话回合进行回应，从教学策略和语言特点（如重新陈述和再声表达、要求准确性、词汇多样性、易读性、礼貌和代理性）两个方面进行分析比较。", "result": "大型语言模型在感知教学质量水平上接近专家水平，但存在教学和语言配置上的系统差异。特别指出的是，大型语言模型较少使用像专家人类导师那样的重新陈述和再声表达策略，但会产生更长、更具词汇多样性和更礼貌的回应。", "conclusion": "近期的大型语言模型在感知教学质量水平上与专家人类导师相当，但使用了不同的教学和语言策略。这些发现强调了在评估人类导师和智能辅导系统的辅导回应时，分析教学策略和语言特征的重要性。"}}
{"id": "2512.20858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.20858", "abs": "https://arxiv.org/abs/2512.20858", "authors": ["Md Zabirul Islam", "Md Motaleb Hossen Manik", "Ge Wang"], "title": "ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction", "comment": null, "summary": "Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.\n  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.\n  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.\n  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.", "AI": {"tldr": "ALIVE系统改善了传统讲座视频缺乏实时互动的不足，提供了准确、内容相关的实时支持，以增强学习体验。", "motivation": "传统讲座视频提供灵活性但缺乏实时澄清机制，而ALIVE通过本地、隐私保护的统一管道将被动讲座观看转变为动态、实时的学习体验。", "method": "ALIVE采用本地硬件操作，集成了通过ASR转录、LLM优化和神经 talking-head 合成生成的Avatar讲座、基于语义相似性和时间戳对齐的内容感知检索机制、以及实时多模态交互，使学生能够暂停课程、通过文本或语音提问并获得文本或Avatar交付的解释。", "result": "ALIVE在完整医学影像课程上实现了高准确率的内容感知实时支持，评价结果显示其检索准确性、延迟特性及用户体验良好。", "conclusion": "ALIVE展示了结合内容感知检索和本地部署的多模态AI如何显著提升录播讲座的教育价值，为下一代互动学习环境提供了一条可扩展的途径。"}}
{"id": "2512.20794", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20794", "abs": "https://arxiv.org/abs/2512.20794", "authors": ["Shariqah Hossain", "Lalana Kagal"], "title": "Investigating Model Editing for Unlearning in Large Language Models", "comment": null, "summary": "Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.", "AI": {"tldr": "通过探索模型编辑算法（ROME, IKE, WISE），研究在遗忘场景下的新编辑目标，证明模型编辑方法在某些设置下优于传统的遗忘方法，但也继承了限定遗忘范围的难题。", "motivation": "解决大规模参数语言模型中高效的遗忘问题，对比分析模型编辑方法与传统遗忘方法在遗忘属性上的差异。", "method": "研究并设计模型编辑算法（ROME, IKE, WISE）在遗忘场景下的新编辑目标。", "result": "在某些设置下，模型编辑方法的遗忘质量超过了传统的遗忘方法。", "conclusion": "模型编辑方法虽然在某些场景下能更好地解决信息遗忘问题，但在不损害模型整体性能的前提下精确限定被遗忘的范围仍然具有挑战性。"}}
{"id": "2512.20866", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.20866", "abs": "https://arxiv.org/abs/2512.20866", "authors": ["Haotian Lv", "Chao Li", "Jiangbo Dai", "Yuhui Zhang", "Zepeng Fan", "Yiqiu Tan", "Dawei Wang", "Binglei Xie"], "title": "Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images", "comment": null, "summary": "To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.", "AI": {"tldr": "本文提出了一种综合利用多视角联合分析策略和深度学习优化机制的三维管道检测框架，并通过实验证明了其在复杂场景中具有更高的识别精度。", "motivation": "解决多视角特征弱相关性、小型目标识别精度低以及在复杂场景下鲁棒性不足的问题，特别是在地下管道三维地质雷达检测中。", "method": "提出了一种三维管道智能检测框架，包括基于FDTD方法的三维管道三视角特征评估方法、整合了DySample、CGLU和OutlookAttention机制的DCO-YOLO框架，以及3D-DIoU空间特征匹配算法。", "result": "在基于真实城市地下管道数据的实验中，提出的方法在复杂多管道场景中实现了96.2%的精度、93.3%的召回率和96.7%的平均精度均值，分别比基准模型高出2.0%、2.1%和0.9%。", "conclusion": "该研究将深度学习优化策略与三维地质雷达的物理特性结合，为地下管道的智能识别和定位提供了一种高效可靠的新技术框架。"}}
{"id": "2512.20796", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20796", "abs": "https://arxiv.org/abs/2512.20796", "authors": ["Zhengyang Shan", "Aaron Mueller"], "title": "Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?", "comment": null, "summary": "We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.", "AI": {"tldr": "研究指出，通过针对性的算法干预可以实现语言模型的去偏见同时保持其性别相关的任务性能。", "motivation": "探讨独立的人口统计偏见机制与语言模型中通用的人口统计识别之间的关系。", "method": "通过多任务评估设置，将人口统计学特征与名字、职业和教育水平相关联，以衡量是否可以在保持人口统计检测能力的同时减少偏见。比较了基于属性和基于相关性的方法来定位偏见特征。", "result": "针对性的稀疏自动编码器功能消减能够减少Gemma-2-9B模型中的偏见，而不影响识别性能：基于属性的消减减轻了职业中的种族和性别刻板印象，同时保持了名字识别准确性；基于相关性的消减则对教育偏差更有效。", "conclusion": "结果表明人口统计偏见源自任务特定机制，而非绝对的人口统计标记，并且可以在不损害核心建模能力的情况下通过机制化的推断时干预实现手术性去偏见。"}}
{"id": "2512.20871", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.20871", "abs": "https://arxiv.org/abs/2512.20871", "authors": ["Daichi Arai", "Kyohei Unno", "Yasuko Sugito", "Yuichi Kusakabe"], "title": "NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder", "comment": "2026 IIEEJ International Conference on Image Electronics and Visual Computing (IEVC)", "summary": "Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.", "AI": {"tldr": "NeRV360 is introduced as an end-to-end framework that improves efficiency in decoding high-resolution 360-degree videos by focusing on the user-selected viewport, thus reducing memory consumption and increasing speed, while also achieving better image quality.", "motivation": "To address the excessive memory usage and slow decoding speed issues that occur when applying implicit neural representations (NeRV) for high-resolution 360-degree videos, making real-time applications impractical.", "method": "NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. It integrates viewport extraction into the decoding process and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time.", "result": "Experiments on 6K-resolution videos demonstrate a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to a representative prior work (HNeRV). Moreover, NeRV360 provides better image quality as measured by objective metrics.", "conclusion": "The proposed NeRV360 framework is effective in overcoming the limitations of using implicit neural representations for video compression in high-resolution 360-degree videos, making real-time applications feasible with enhanced decoding efficiency and image quality."}}
