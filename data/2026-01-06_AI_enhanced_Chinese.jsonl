{"id": "2601.00797", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.00797", "abs": "https://arxiv.org/abs/2601.00797", "authors": ["Hugues Draelants"], "title": "The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models", "comment": "26 pages, 3 tables. Manuscript submitted for peer-reviewed journal publication", "summary": "A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a \"qualitative laboratory\". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a \"simulation then validation\" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.", "AI": {"tldr": "本文介绍了通过大型语言模型进行社会人物模拟的新方法，这种方法克服了传统方法论中的局限，能够生成深度纹理化的假设，用于后续实证测试。", "motivation": "本文的动机在于解决社会科学中的一个中心挑战，即生成关于不同社会群体如何解读新信息的丰富定性假设。通过这种方法，可以克服传统方法论中的局限性。", "method": "本文提出了一种新颖的方法论方法，即通过大型语言模型（LLMs）进行社会人物模拟，以生成关于不同社会群体如何解读新信息的丰富定性假设。这种方法被称为“定性实验室”。", "result": "通过让源于气候接受社会理论的人物角色对政策信息作出反应，模拟产生了细致入微且出乎意料的假设，例如保守角色对国家安全框架的拒绝，这挑战了理论上的假设。", "conclusion": "作为“先模拟后验证”工作流程的一部分，这种方法被认为是一种优越的工具，用于生成在后续实证测试中需要的深度纹理化假设。"}}
{"id": "2601.00938", "categories": ["cs.CL", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00938", "abs": "https://arxiv.org/abs/2601.00938", "authors": ["Faruk Alpay", "Bugra Kilictas"], "title": "Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian Updates", "comment": "9 pages", "summary": "Bounded-context agents fail when intermediate reasoning exceeds an effective working-memory budget. We study compressed query delegation (CQD): (i) compress a high-dimensional latent reasoning state into a low-rank tensor query, (ii) delegate the minimal query to an external oracle, and (iii) update the latent state via Riemannian optimization on fixed-rank manifolds. We give a math-first formulation: CQD is a constrained stochastic program with a query-budget functional and an oracle modeled as a noisy operator. We connect CQD to classical rate-distortion and information bottleneck principles, showing that spectral hard-thresholding is optimal for a natural constrained quadratic distortion problem, and we derive convergence guarantees for Riemannian stochastic approximation under bounded oracle noise and smoothness assumptions. Empirically, we report (A) a 2,500-item bounded-context reasoning suite (BBH-derived tasks plus curated paradox instances) comparing CQD against chain-of-thought baselines under fixed compute and context; and (B) a human \"cognitive mirror\" benchmark (N=200) measuring epistemic gain and semantic drift across modern oracles.", "AI": {"tldr": "本文研究了压缩查询委托（CQD）技术来解决高维度推理状态下高维工作记忆限制问题。该研究提供了CQD的理论分析和实验结果。", "motivation": "该研究的动机在于解决高维推理状态下高维工作记忆预算限制的问题，通过压缩查询委托以减轻工作记忆负担。", "method": "该研究提出了压缩查询委托（CQD）的方法，该方法包括三个步骤：将高维潜在推理状态压缩到低秩张量查询，将最小查询委托给外部预言机，然后通过固定秩流形上的黎曼优化更新潜在状态。该方法被设计为一个带有查询预算功能的约束随机规划问题，其中预言机被视为一个噪声操作符。", "result": "理论上，该研究将CQD与经典的信息瓶、压缩理论相联系，证明了对于一个自然的约束二次失真问题，光谱阈值硬是最佳方案，并且在有界预言机噪音和光滑度假设下，研究还提供了黎曼随机逼近的收敛性保证。实验上，研究通过一个包含2,500个任务的数据集将CQD与链式思考基线方法进行比较，并提供了关于现代预言机知识获取和语义漂移的人类认知镜子基准测试。", "conclusion": "实验表明，CQD方法在处理BBH衍生任务和已精选悖论实例的范围内表现出色，并具备了与现代预言机相比较的认知优势。"}}
{"id": "2601.01011", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01011", "abs": "https://arxiv.org/abs/2601.01011", "authors": ["Patricio Vera"], "title": "Intention Collapse: Intention-Level Metrics for Reasoning in Language Models", "comment": "21 pages, 4 figures, 3 tables. Code: https://github.com/patriciomvera/intention-collapse-experiments", "summary": "Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies", "AI": {"tldr": "文章探讨了语言生成过程中意向崩溃的现象，并通过实验展示了CoT（思考过程）方法在提高生成准确性和保持意向清晰度方面的效果。", "motivation": "动机在于理解语言生成中从高维意向空间到外部语言空间的映射过程，以及这种映射对生成结果的影响。", "method": "通过定义三个与模型无关的意向度量（意向熵Hint、有效维度dimeff和潜在知识可恢复性Recov），本文形式化了当代语言模型中的意向崩溃过程，并提出了一个关于推理时间计算如何塑造内部意向的实证研究议程。", "result": "通过一个小型实验，本文表明CoT（思考过程）方法提高了准确率，同时显著降低了崩溃前的意向熵，并展示出比其他方法更高的全局有效维度数，尽管产生的token较少。", "conclusion": "初步结果显示，意向层度量能够区分不同的推理模式，并揭示在生成过程中部分丢失的潜在信息，但同时也表明我们当前的度量标准存在一些限制。"}}
{"id": "2601.01015", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.01015", "abs": "https://arxiv.org/abs/2601.01015", "authors": ["Shiyuan Liu", "Jianwei Wang", "Xuemin Lin", "Lu Qin", "Wenjie Zhang", "Ying Zhang"], "title": "HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery", "comment": null, "summary": "As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.", "AI": {"tldr": "Developed HyperJoin, an LLM-augmented Hypergraph framework for enhanced joinable table discovery in data lakes, demonstrating superior performance over existing methods.", "motivation": "To improve upon existing methods by capturing rich structural interactions and ensuring coherent result sets in joinable table discovery.", "method": "HyperJoin, which uses a large language model (LLM)-augmented Hypergraph to capture inter-table and intra-table structural information for joinable table discovery.", "result": "Achieved average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.", "conclusion": "The introduction of HyperJoin, a hierarchical interaction network enhanced by a hypergraph structure, provides a more effective approach to joinable table discovery."}}
{"id": "2601.00812", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00812", "abs": "https://arxiv.org/abs/2601.00812", "authors": ["Takashi Ushio", "Kazuhiro Onishi", "Hideyoshi Yanagisawa"], "title": "Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements", "comment": "This article has been accepted for publication in IEEE Access and will be published shortly", "summary": "Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified \"pleasantness,\" \"surprise,\" and \"habituation\" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected \"pleasantness\" associated with brand presentation, BS has captured \"surprise\" arising from informational complexity, and UN has reflected \"surprise\" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.", "AI": {"tldr": "研究从广告视频中通过量化特征来估计解释性情绪，不依赖外部信息，结合FE理论，通过特定的计算方法，有效识别出不同情绪的特征，同时证明了方法的稳定性和适用性。", "motivation": "由于情绪反应在理解媒体效应中起到关键作用，特别是在广告视频观看过程中，研究旨在通过仅依赖于广告视频的场景级表情特征来建立一种方法论基础，实现情感估计，以避免对生理信号或主观评分的依赖。", "method": "本研究采用自由能（FE）原理，通过量化的Kullback-Leibler散度（KLD）、贝叶斯惊讶（BS）和不确定性（UN）从广告视频的场景级表情特征中分别捕捉\"pleasantness\"、\"surprise\"和\"habituation\"情绪反应。", "result": "实验结果表明KLD与品牌呈现有关的\"pleasantness\"相关，BS能够捕捉由信息复杂性引起\"surprise\"，而UN则反映了由元素类型、空间排列的不确定性驱动\"surprise\"，以及由所呈现元素的变化性和数量引起的因素。研究还识别出了三种典型的情绪模式，这一方法的鲁棒性和一般性测试在不同类型的广告视频中均得到验证。", "conclusion": "本研究提出的方法在不同的广告视频类型中具有稳定性和通用性，未来可以扩展识别更多的情绪表达元素，通过主观评分验证这一方法，为更吸引人的广告视频制作技术提供指导。"}}
{"id": "2601.01037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01037", "abs": "https://arxiv.org/abs/2601.01037", "authors": ["Livia Leong Hui Teng"], "title": "Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation", "comment": null, "summary": "Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.", "AI": {"tldr": "论文提出了一种多维框架，目标是提升小型语言模型在开放对话中的质量，并通过自动和人工评估，显示出显著提升效果，尤其使Llama-2-7B的表现接近大型模型。", "motivation": "小型语言模型尽管有显著的部署优势，但在开放领域对话质量上经常难以匹敌更大的模型。研究旨在通过引入多维框架来提升小型语言模型的对话质量。", "method": "提出了一种多维prompt-chaining框架，包括自然度、连贯性和吸引力三个维度，旨在提高小型语言模型在开放领域对话生成中的人类相似性。将该框架应用于TinyLlama和Llama-2-7B两种小型语言模型，并将其性能与更大规模模型（如Llama-2-70B和GPT-3.5 Turbo）进行比较。", "result": "结果显示，完整框架可以使响应多样性提高最多29%，上下文连贯性提高最多28%，并且在吸引性和自然度上的提升最多可达29%。", "conclusion": "精心设计的基于prompt的策略为提高小型语言模型在开放领域对话质量提供了一种有效且资源高效的方法。"}}
{"id": "2601.00829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00829", "abs": "https://arxiv.org/abs/2601.00829", "authors": ["Alexander Vinogradov"], "title": "Can Generative Models Actually Forge Realistic Identity Documents?", "comment": "11 pages, 16 figures", "summary": "Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.", "AI": {"tldr": "该研究评估了开源的生成模型在伪造身份文件上的能力，发现这些模型虽然能模仿文件的表面特性，但无法复制其结构和取证真实性，因此相关风险可能被高估。", "motivation": "鉴于生成图像模型在图像真实感方面的显著进展，引发了公众对这些模型可能被用于伪造文件的担忧。本研究旨在评估现有公开的生成模型是否能创建出可以绕过人工或自动化验证系统的伪造身份文件。", "method": "研究方法涉及使用多个公开的生成模型家族，对文本到图像和图像到图像的生成管道进行评估，以测试生成模型创建身份文件假冒品的能力。", "result": "该论文探讨了当代开源和公共访问的基于扩散的生成模型是否可以生产出能够真实地绕过人工或自动化验证系统的身份文档伪造品。研究通过评估文本到图像和图像到图像生成流水线，使用多个公开可用的生成模型家族，包括稳定扩散、Qwen、Flux、Nano-Banana等。研究发现，尽管当前生成模型可以模拟文档的表面美学，但无法复制结构和取证真实性。因此，生成身份文档深度伪造物达到取证水平真实性的风险可能被高估，强调了机器学习从业者与文档取证专家在现实风险评估中合作的价值。", "conclusion": "研究结论指出，生成身份文档深伪的风险可能被夸大，实际生成的文档无法通过正式验证。这需要机器学习领域专家与文件鉴伪专家共同参与风险评估。"}}
{"id": "2601.01046", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01046", "abs": "https://arxiv.org/abs/2601.01046", "authors": ["Yixuan Tang", "Yi Yang"], "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs", "comment": null, "summary": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.", "AI": {"tldr": "提出KV-Embedding框架，以解决LLMs在无训练设置下的限制问题，通过内部状态操纵提升模型表现，实验结果证明其有效性。", "motivation": "解决LLMs在无训练设置下的结构挑战，包括因果注意力限制和下一个标记预测目标的偏差，从而挖掘冻结LLMs的潜在表示能力。", "method": "提出KV-Embedding框架解决LLMs在无训练设置下因因果注意力和下一步预测目标而遇到的限制问题。该框架利用最终标记在每一层中的键值状态作为压缩序列视图，并将其作为前缀重新路由，以使所有标记在一个前向传递中可以访问序列级上下文。此外，引入基于内在维度的自动化层选择策略，确保模型无关的应用性。", "result": "在MTEB上的评估表明，KV-Embedding在Qwen、Mistral和Llama这三个模型上都超过了现有的无训练基线方法，性能提升高达10%，并且在长达4,096个标记的序列上保持了健壮的性能。", "conclusion": "该工作表明，内部状态操纵为表示学习提供了一种有效的替代方案，鼓励对未来探索LLMs内部结构的深入研究。"}}
{"id": "2601.00837", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00837", "abs": "https://arxiv.org/abs/2601.00837", "authors": ["Agniv Roy Choudhury"], "title": "Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs", "comment": null, "summary": "Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.\n  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.\n  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.\n  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\\% accuracy, 99.61\\% F1-score, and 99.93\\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.\n  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.\n  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.", "AI": {"tldr": "研究比较了从头训练的卷积神经网络(CNN)与使用迁移学习(ResNet50, DenseNet121, EfficientNet-B0)的模型在儿童肺炎检测上的效果，发现微调的ResNet50模型达到了最佳性能，显示出近乎完美的准确性。这种方法在资源有限的环境下作为筛查工具具有很强的潜力。", "motivation": "肺炎是五岁以下儿童的主要死因，每年导致超过70万名儿童死亡。准确地从胸部X光片诊断肺炎往往受到放射科医生资源限制和诊断上的不确定性的影响。", "method": "使用了一个包含5216张儿童胸部X光片的数据集，该数据集被划分为80%用于训练，10%用于验证，10%用于测试。训练了七个模型，并根据准确率、F1分数和AUC进行评估。Grad-CAM可视化提供了可解释性。", "result": "微调的ResNet50模型表现最佳：准确率达99.43%，F1分数为99.61%，AUC值为99.93%，仅有3次误分类。平均而言，微调模型比冻结骨架(stem)的模型高出约5.5个百分点。Grad-CAM确认临床相关的肺部区域指导了预测。", "conclusion": "使用迁移学习并进行微调的模型相较于从头训练的CNN在儿童肺炎检测上表现出显著优势，显示出近乎完美的准确性。这种系统在资源有限的环境中作为筛查工具表现出了很大的前景。未来的研究需要在多中心和成人数据集上验证这些发现。"}}
{"id": "2601.01060", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01060", "abs": "https://arxiv.org/abs/2601.01060", "authors": ["Shuhuan Gu", "Wenbiao Tao", "Xinchen Ma", "Kangkang He", "Ye Guo", "Xiang Li", "Yunshi Lan"], "title": "Unsupervised Text Style Transfer for Controllable Intensity", "comment": null, "summary": "Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.", "AI": {"tldr": "本研究提出了一种基于SFT-then-PPO范式的无监督文本风格转换方法，解决了相邻强度级别之间风格难以区分的挑战，实验结果表明在不同评估指标下均有良好的性能提升。", "motivation": "这项研究的主要动机是解决无监督文本风格转换（UTST）中的一个关键挑战：在缺乏并行数据和相邻强度级别之间的差异难以区分的情况下，实现可控的风格强度转换。", "method": "我们提出了一种名为SFT-then-PPO的范式来对大型语言模型（LLM）进行微调。首先使用合成的并行数据对LLM进行训练，其次通过精心设计奖励函数以区分不同层次的风格强度的PPO进一步训练LLM。", "result": "实验表明，提出的方法在两个无监督文本风格转换基准上取得了良好的效果，验证了所设计的奖励函数的有效性。", "conclusion": "研究表明，通过结合奖励函数的全局和局部风格特征，可以有效提升基于LLM的模型在不同评估指标下的性能，即使是强度相近的相邻级别，也能观察到生成文本的风格差异。"}}
{"id": "2601.00839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00839", "abs": "https://arxiv.org/abs/2601.00839", "authors": ["Zahid Ullah", "Muhammad Hilal", "Eunsoo Lee", "Dragan Pamucar", "Jihie Kim"], "title": "Unified Review and Benchmark of Deep Segmentation Architectures for Cardiac Ultrasound on CAMUS", "comment": null, "summary": "Several review papers summarize cardiac imaging and DL advances, few works connect this overview to a unified and reproducible experimental benchmark. In this study, we combine a focused review of cardiac ultrasound segmentation literature with a controlled comparison of three influential architectures, U-Net, Attention U-Net, and TransUNet, on the Cardiac Acquisitions for Multi-Structure Ultrasound Segmentation (CAMUS) echocardiography dataset. Our benchmark spans multiple preprocessing routes, including native NIfTI volumes, 16-bit PNG exports, GPT-assisted polygon-based pseudo-labels, and self-supervised pretraining (SSL) on thousands of unlabeled cine frames. Using identical training splits, losses, and evaluation criteria, a plain U-Net achieved a 94% mean Dice when trained directly on NIfTI data (preserving native dynamic range), while the PNG-16-bit workflow reached 91% under similar conditions. Attention U-Net provided modest improvements on small or low-contrast regions, reducing boundary leakage, whereas TransUNet demonstrated the strongest generalization on challenging frames due to its ability to model global spatial context, particularly when initialized with SSL. Pseudo-labeling expanded the training set and improved robustness after confidence filtering. Overall, our contributions are threefold: a harmonized, apples-to-apples benchmark of U-Net, Attention U-Net, and TransUNet under standardized CAMUS preprocessing and evaluation; practical guidance on maintaining intensity fidelity, resolution consistency, and alignment when preparing ultrasound data; and an outlook on scalable self-supervision and emerging multimodal GPT-based annotation pipelines for rapid labeling, quality assurance, and targeted dataset curation.", "AI": {"tldr": "本文提供了U-Net、注意力U-Net、TransUNet在心脏超声分割上的对比结果，强调了数据预处理和自我监督学习的重要性，并提供了实用的数据准备指南以及对未来方向的展望。", "motivation": "本文的动机在于为心脏成像及深度学习领域的综述性研究提供一个统一且可重现的实验基准。在此之前的工作中，很少有研究将这些综述与实验基准连接起来。", "method": "本文结合了心脏超声分割文献的专题回顾，并在一个受控的实验环境中，比较了三种具有影响力的架构（U-Net、注意力U-Net、TransUNet）在CAMUS超声心动图数据集上的表现。实验涵盖了多种预处理流程，包括原始NIfTI格式、16位PNG导出、GPT辅助的基于多边形的伪标签以及在数千个未标记的电影帧上的自我监督预训练。", "result": "实验结果显示，在相同的训练拆分、损失函数和评估标准下，直接在NIfTI数据上训练的U-Net达到平均Dice值94%，而16位PNG工作流程则达到了91%。注意力U-Net在小或者低对比度区域表现有所提升，而TransUNet通过自我监督预训练，在困难帧段表现出色。伪标签技术通过增加训练集和过滤信心，也提升了模型的鲁棒性。", "conclusion": "本文的贡献包括标准化的CAMUS数据预处理和评估下，对U-Net、注意力U-Net及TransUNet进行统一的基准测试；提供了维持超声数据强度保真性、分辨率一致性及对齐策略的实用指南；并展望了扩大自我监督学习和支持多模式GPT注释管线的潜力。"}}
