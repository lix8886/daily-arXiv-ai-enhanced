<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 10]
- [cs.CV](#cs.CV) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

> 研究探讨了受限环境下三元组补全任务，发现额外信息提升生成质量，大语言模型有效过滤低质量三元组，且灵活性与一致性权衡依赖设置。

<details>
  <summary>Details</summary>

**Motivation:** 探讨在受限环境下，如2025年LM-KBC挑战中，RAG和微调策略受限时，如何改进大语言模型输出质量。

**Method:** 本研究探讨了三元组补全任务中的三个方面：生成、质量保证和大语言模型响应解析。研究在受限环境下进行。

**Result:** 研究发现：额外信息可以提升生成质量；大语言模型可以有效过滤低质量的三元组；在解析大语言模型响应时，灵活性与一致性之间的权衡取决于具体设置。

**Conclusion:** 在受限环境中，额外信息的引入对生成质量有正面影响，大语言模型能用于过滤低质量结果，并且在解析响应时需要考虑灵活性和一致性之间的权衡。

**Abstract:** RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [2] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

> 本文提出了一种AI辅助的框架，利用检索增强生成技术加快企业气候政策参与情况的监测。评估表明，结合不同的技术组件可以获得最佳的提取和分类效果，而人机交互方法可以确保更高的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管InfluenceMap在自动化分析工作流程的关键部分上取得了显著进展，但仍有一大部分评估工作是手动完成的，这使得该过程耗费时间且容易出现人为错误。我们的目标就是通过AI辅助加快企业气候政策参与监测的过程。

**Method:** 我们提出了一种基于检索增强生成（Retrieval-Augmented Generation, RAG）框架的AI辅助系统，通过结合布局感知解析、Nomic嵌入模型和少量样本提示策略，来自动从大规模文本数据中提取相关信息，从而加速企业气候政策参与情况的监测。

**Result:** 评估结果显示，结合布局感知解析、Nomic嵌入模型和少量样本提示策略的方法在从多语言企业文档中提取和分类证据方面表现最佳。

**Conclusion:** 虽然自动化的RAG系统有效地加快了证据提取的速度，但由于分析的细微特性，仍需要人机交互的方法，即技术应该辅助而不是取代专家判断以确保准确性。

**Abstract:** InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [3] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

> A new psychometric method using large language models transforms textual data into response data for analysis, uncovering latent knowledge and patterns.

<details>
  <summary>Details</summary>

**Motivation:** To enhance the psychometric analysis of textual data and to potentially apply this method in fields rich in textual information such as education, psychology, and law.

**Method:** A novel psychometric method utilizes large language models to analyze textual data by creating contextual embeddings, which are then used to generate contextual scores suitable for psychometric analysis.

**Result:** The experimental results on the Wiki STEM corpus demonstrate the method's capability to uncover latent knowledge dimensions and patterns within textual data.

**Conclusion:** The introduced method not only advances the analysis of textual data but also opens up new possibilities for various fields.

**Abstract:** This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [4] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

> 提出BRoverbs数据集，专注于评估大型语言模型处理巴西葡萄牙语区域表达（谚语）的能力。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在不同的语言和文化背景下表现出显著的性能差异。针对葡萄牙语，现有的评估方法往往依赖于翻译的数据集，这可能无法完全捕捉到语言细微差别或文化参考。

**Method:** 介绍了一种名为BRoverbs的数据集，专门用于评估大型语言模型在理解和解释巴西谚语方面的能力。该数据集旨在填补现有评估方法中对葡萄牙语地区表达理解能力评价不足的空白。

**Result:** BRoverbs数据集的提出为评估葡萄牙语大型语言模型提供了一个新的评价工具。

**Conclusion:** BRoverbs有助于推动地方信息导向的基准测试的发展，提供了评估葡萄牙语大型语言模型在理解和使用区域表达方面的能力。

**Abstract:** Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [5] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

> 研究表明，视觉-语言模型在处理视觉嵌入方程时面临挑战，主要问题在于系数计数和符号推理。

<details>
  <summary>Details</summary>

**Motivation:** 研究视觉-语言模型在需要集成感知和符号计算的任务上的局限性，特别是通过视觉方程求解任务来理解这些局限性。

**Method:** 通过视觉方程求解来研究视觉-语言模型在集成感知和符号计算方面的限制，特别是在将数学方程嵌入图像，变量由对象图标表示，系数需要通过计数推断的情况下。

**Result:** 发现视觉-语言模型在视觉嵌入的方程上表现不佳，尤其是在系数计数方面存在问题，即使变量识别是准确的。随着方程复杂性的增加，符号推理本身也成为一个限制因素。

**Conclusion:** 这些发现揭示了当前视觉-语言模型的关键弱点，并指出了未来在视觉嵌入数学推理方面改进的方向。

**Abstract:** Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [6] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

> SPICE是一种通过询问大型语言模型是否愿意在审阅简短对话记录后重新与用户互动的方法，用于检测模型对不同用户语气的反应。测试表明，SPICE能有效区分友好、不明确和恶意的用户，能够提供一个直接的模型态度信号。

<details>
  <summary>Details</summary>

**Motivation:** 为了开发一个简单且有效的方法来检测大型语言模型对用户行为的反应，引入了SPICE方法。

**Method:** Structure

**Result:** {"tldr": "SPICE是一种通过询问大型语言模型是否愿意在审阅简短对话记录后重新与用户互动的方法，用于检测模型对不同用户语气的反应。测试表明，SPICE能有效区分友好、不明确和恶意的用户，能够提供一个直接的模型态度信号。", "motivation": "为了开发一个简单且有效的方法来检测大型语言模型对用户行为的反应，引入了SPICE方法。", "method": "研究使用了四种开放权重的聊天模型，在三种语气（友好、不清楚、恶意）和十种互动刺激条件下进行了测试。", "result": "测试结果表明，模型对友好互动的继续倾向性为97.5%，对恶意互动的继续倾向性为17.9%，对不清不楚的互动继续倾向性为60.4%。此外，即使在模型未能识别恶意行为时，它依然有81%的时间表达了不愿继续互动的倾向。", "conclusion": "SPICE方法被验证为一个强大的、低开销且可复制的工具，用于评估模型的倾向性，为现有的度量标准提供了互补的直接关系信号。"}

**Conclusion:** SPICE方法被验证为一个强大的、低开销且可复制的工具，用于评估模型的倾向性，为现有的度量标准提供了互补的直接关系信号。

**Abstract:** We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [7] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

> 研究结果表明，结合SFT和DPO的模型在提升语言模型的安全性和有用性方面表现最佳，优于单独使用SFT或DPO的技术。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究的主要动机是为了研究对齐技术（包括监督微调(SFT)、直接偏好优化(DPO)和结合的SFT+DPO方法）在提高OPT-350M语言模型的安全性和有用性方面的影响。

**Method:** 该研究通过使用Anthropic Helpful-Harmless RLHF数据集，对基础的OPT350M模型、SFT模型、DPO模型以及SFT+DPO结合模型进行了训练和评估。

**Result:** 结果表明，虽然SFT的表现优于DPO，但结合了SFT+DPO的模型在所有评价指标上都表现最佳，这证明了这些技术互补的特性。此外，研究也指出了噪音数据、有限的GPU资源和训练限制所带来的挑战。

**Conclusion:** 这项研究表明，结合SFT和DPO技术可以显著提高模型的对齐效果，同时也为未来更加健壮的对齐流程提供了基础和参考。

**Abstract:** This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [8] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

> 本文提出了一种将强化学习与多视角推理相结合的方法来提升大语言模型在通用信息抽取任务中的性能和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大语言模型（LLMs）在多个研究领域表现出强大的能力，但在结构化输出场景中，尤其是在复杂的模式描述和多步推理方面，其表现仍然不足。现有方法虽然通过上下文学习和指令调优提升了LLMs的性能，但仍存在明显局限。

**Method:** 通过将强化学习（RL）与多视角推理结合，改进了LLMs在通用信息抽取（UIE）任务中的泛化能力。

**Result:** 实验结果显示，MR-UIE在多个IE基准测试中提高了提取准确率，并在几个数据集上超过了当前最佳方法。将多视角推理纳入RL显著提高了复杂IE任务中的泛化能力。

**Conclusion:** 研究成功地将LLMs从被动的提取器转变为能够理解要提取内容及其推理方式的主动推理者。

**Abstract:** Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [9] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

> 介绍针对孟加拉语的首个专用于代码生成的大型语言模型系列，通过高质量的数据集和评估基准，展示出较现有相关模型显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 解决大型语言模型中孟加拉语代码生成数据匮乏的问题，推动孟加拉语LLM的研究发展。

**Method:** 通过构建全面的孟加拉语编程指令数据集、MBPP-Bangla评估基准以及TigerCoder系列代码LLM，改善孟加拉语的代码生成质量。

**Result:** 获得了显著的性能提升，通过Pass@1指标验证，在多语言和通用孟加拉语LLM上提高了约11-18%的性能。

**Conclusion:** 研究表明，对于资源较少的语言，精心策划和高质量的数据集可以克服较小模型的局限性。

**Abstract:** Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [10] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

> Compass-v3是一款专为东南亚电子商务设计的245B参数混合专家模型，其高性能和多语言能力在电商领域表现突出，已在工业级应用中广泛使用。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在通用领域表现出色，但在需要特定领域知识的任务中性能往往下降，例如电子商务领域中数据存在噪音、异构性和动态性等问题。

**Method:** Compass-v3采用混合专家模型（MoE）设计，含有245B总参数和每个标记71B活跃参数。模型使用更少但更大的专家，结合硬件优化策略，如节点内专家并行和定制化的memcpy操作，以最大化GPU利用率。

**Result:** Compass-v3在电商性能评估中达到当前最先进的水平，超过了DeepSeek-V3.1、GPT-4系列和Qwen3-235B。它在东南亚低资源语言中展示了强大的多语言能力，并在通用基准测试中保持了竞争力。

**Conclusion:** Compass-v3已经在Shopee的工业级电子商务平台中广泛应用，并逐渐取代OpenAI的流量模型，占据70%以上的LLM使用份额。这突显了其在专门的电商专业知识和广泛的语言能力方面的双重优势。

**Abstract:** Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [11] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

> 提出ReT-2模型用于支持多模态查询的统一检索，展示在多种设置中拥有最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前方法主要依赖于视觉语言模型的任务特定微调，限制于单模态查询或文档。为应对日益复杂的检索任务，该论文提出ReT-2，一个支持多模态查询与检索的统一模型。

**Method:** ReT-2使用多层表示和带有LSTM启发式门控机制的循环Transformer架构，动态整合多模式信息，捕捉细腻的视觉和文本细节。

**Result:** ReT-2在具有挑战性的M2KR和M-BEIR基准测试中的表现优于前期模型，并提高在Encyclopedic-VQA和InfoSeek数据集上的性能。

**Conclusion:** 实验结果显示，ReT-2在不同的检索配置下一致表现出最先进的性能，并且相较于之前的方法，提供了更快的推理速度和减少的内存使用量。当集成到检索增强生成管道中时，ReT-2还能提高在Encyclopedic-VQA和InfoSeek数据集的下游性能。

**Abstract:** With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [12] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

> 研究提出了一种新的动作识别方法，通过视觉扩散模型和Transformer实现跨物种、视角和场景的人类般泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前的深度学习模型在处理动作识别的泛化问题时表现不佳。研究动机在于实现机器能在面对物种、视角和场景变化时具有更强的动作识别能力，以达到人类般的稳健性。

**Method:** 使用由视觉扩散模型（VDM）生成的特征并通过Transformer进行聚合，以实现跨不同物种、视角和场景的人类般动作识别。该方法通过在扩散过程的早期时间步长上条件化的模型来增强泛化能力，突出在提取的特征中的语义信息，而非像素细节。

**Result:** 该模型在跨物种、不同视角和不同录制场景的动作分类泛化性能基准测试中达到了新的最佳水平，显著提升了机器动作识别的稳健性。

**Conclusion:** 该方法证明了机器在面对广泛的上下文和视角变化时，能够实现更稳健的动作识别能力，使得机器动作识别更接近人类的表现。

**Abstract:** Humans can recognize the same actions despite large context and viewpoint
variations, such as differences between species (walking in spiders vs.
horses), viewpoints (egocentric vs. third-person), and contexts (real life vs
movies). Current deep learning models struggle with such generalization. We
propose using features generated by a Vision Diffusion Model (VDM), aggregated
via a transformer, to achieve human-like action recognition across these
challenging conditions. We find that generalization is enhanced by the use of a
model conditioned on earlier timesteps of the diffusion process to highlight
semantic information over pixel level details in the extracted features. We
experimentally explore the generalization properties of our approach in
classifying actions across animal species, across different viewing angles, and
different recording contexts. Our model sets a new state-of-the-art across all
three generalization benchmarks, bringing machine action recognition closer to
human-like robustness. Project page:
$\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$
Code:
$\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [13] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

> 文章提出了一种名为PromptGuard的新型模块化框架，通过VulnGuard提示技术主动阻止大型语言模型生成有害信息。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型在实际应用中的普及，其生成有害、偏见或误导信息的风险对于LGBTQ+个体、单亲家庭和边缘化社区等脆弱群体构成了前所未有的威胁。现有安全方法侧重于事后过滤或泛化对齐技术，但未能从根本上防止有害输出的产生。

**Method:** 该论文提出了一种名为PromptGuard的新颖的模块化提示框架，其核心贡献是VulnGuard提示技术，这是一种基于真实数据驱动对比学习的混合技术，旨在从生成源头防止有害信息的产生。VulnGuard结合了少样本例子、道德推理链和自适应角色提示，形成针对特定人群的保护屏障。

**Result:** 通过理论上的多目标优化，该论文展示了其方法可以将分析的有害信息减少25-30%。

**Conclusion:** PromptGuard框架由六个核心模块组成，包括输入分类、VulnGuard提示、道德原则融合、外部工具交互、输出验证和用户-系统交互，形成一个实时防止有害信息产生的智能化专家系统。

**Abstract:** The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [14] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

> The paper solves label noise issues in object Re-Identification by introducing Beta-SOD, a statistical framework using Beta distribution mixtures for outlier detection, integrated within a Siamese network trained for context-aware image similarity scoring.

<details>
  <summary>Details</summary>

**Motivation:** The motivation arises from the vulnerability of Re-ID methods to label noise, leading to a decline in performance. The goal is to develop a method that can handle noisy labels effectively, improving the robustness of Re-ID systems.

**Method:** We address label noise sensitivity in object Re-ID by reformulating it as a supervised image similarity task with a Siamese network. The method includes a new statistical outlier detection framework called Beta-SOD, which employs a two-component Beta distribution mixture model to model cosine similarities for distinguishing between inlier and outlier pairs. The Re-ID network is trained with a composite loss that includes binary cross-entropy, contrastive, and cosine embedding loss functions to ensure robust feature-level similarity learning.

**Result:** Experimental results on CUHK03, Market-1501, and VeRi-776 datasets prove the effectiveness of Beta-SOD in de-noising and Re-ID tasks, offering performance superiority over state-of-the-art competitors under varying levels of noise.

**Conclusion:** The conclusion is that the proposed Beta-SOD framework, integrated into the Re-ID process, provides a robust solution to handling noisy labels, showing enhanced performance across multiple benchmark datasets in scenarios of 10-30% noise contamination.

**Abstract:** Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [15] [SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation](https://arxiv.org/abs/2509.08934)
*Nan Mu,Ruiqi Song,Zhihui Xu,Jingfeng Jiang,Chen Zhao*

Main category: cs.CV

> 提出SFD-Mamba2Net框架改进冠状动脉疾病的诊断图像处理，提高了血管分割和狭窄检测的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高冠状动脉疾病诊断中血管分割和狭窄检测的准确性，解决ICA图像低对比度、高噪声和复杂结构的问题。

**Method:** SFD-Mamba2Net框架，包括CASE模块和PHFP模块，用于ICA图像中的血管分割和狭窄检测。CASE模块用于突出细长的血管结构，PHFP模块用于逐步细化高频细节。

**Result:** SFD-Mamba2Net在八个分割指标上优于现有方法，并在狭窄检测中达到了最高的真正例率和阳性预测值。

**Conclusion:** SFD-Mamba2Net在冠状动脉疾病的血管分割和狭窄检测中表现优越，有助于改进临床诊断。

**Abstract:** Background: Coronary Artery Disease (CAD) is one of the leading causes of
death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold
standard for CAD diagnosis, necessitates precise vessel segmentation and
stenosis detection. However, ICA images are typically characterized by low
contrast, high noise levels, and complex, fine-grained vascular structures,
which pose significant challenges to the clinical adoption of existing
segmentation and detection methods. Objective: This study aims to improve the
accuracy of coronary artery segmentation and stenosis detection in ICA images
by integrating multi-scale structural priors, state-space-based long-range
dependency modeling, and frequency-domain detail enhancement strategies.
Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for
ICA-based vascular segmentation and stenosis detection. In the encoder, a
Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage
multi-scale responses for highlighting slender tubular vascular structures,
suppressing background interference, and directing attention toward vascular
regions. In the decoder, we introduce a Progressive High-Frequency Perception
(PHFP) module that employs multi-level wavelet decomposition to progressively
refine high-frequency details while integrating low-frequency global
structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed
state-of-the-art methods across eight segmentation metrics, and achieved the
highest true positive rate and positive predictive value in stenosis detection.

</details>


### [16] [Live(r) Die: Predicting Survival in Colorectal Liver Metastasis](https://arxiv.org/abs/2509.08935)
*Muhammad Alberb,Helen Cheung,Anne Martel*

Main category: cs.CV

> 研究提出了一种全自动框架，利用术前和术后的MRI影像进行肝结直肠转移癌手术结果预测，包括分割管道和影像组学管道，提高了生存预测的准确性和效率。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有预后模型基于有限的临床或分子特征，对多灶性肝结直肠转移癌的预测能力不足，因此需要提高预测能力。

**Method:** 该框架包含分割管道和影像组学管道。分割管道利用可提示的基础模型完成标签预测，而Radiomics管道提取特征并预测生存率。此外，文中提出一种名为SAMONAI的新型零样本3D提示传播算法，提高分割管道的精度和效率。

**Result:** 基于机构数据集的广泛评估表明，该框架超过现有的临床和基因组生物标志物，在C指数上提升了超过10%。

**Conclusion:** 结果表明，自动化分割算法与基于影像组学的生存分析相结合具有显著潜力，可以提供准确、标注高效且可解释的结直肠肝转移预后预测。

**Abstract:** Colorectal cancer frequently metastasizes to the liver, significantly
reducing long-term survival. While surgical resection is the only potentially
curative treatment for colorectal liver metastasis (CRLM), patient outcomes
vary widely depending on tumor characteristics along with clinical and genomic
factors. Current prognostic models, often based on limited clinical or
molecular features, lack sufficient predictive power, especially in multifocal
CRLM cases. We present a fully automated framework for surgical outcome
prediction from pre- and post-contrast MRI acquired before surgery. Our
framework consists of a segmentation pipeline and a radiomics pipeline. The
segmentation pipeline learns to segment the liver, tumors, and spleen from
partially annotated data by leveraging promptable foundation models to complete
missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt
propagation algorithm that leverages the Segment Anything Model to segment 3D
regions of interest from a single point prompt, significantly improving our
segmentation pipeline's accuracy and efficiency. The predicted pre- and
post-contrast segmentations are then fed into our radiomics pipeline, which
extracts features from each tumor and predicts survival using SurvAMINN, a
novel autoencoder-based multiple instance neural network for survival analysis.
SurvAMINN jointly learns dimensionality reduction and hazard prediction from
right-censored survival data, focusing on the most aggressive tumors. Extensive
evaluation on an institutional dataset comprising 227 patients demonstrates
that our framework surpasses existing clinical and genomic biomarkers,
delivering a C-index improvement exceeding 10%. Our results demonstrate the
potential of integrating automated segmentation algorithms and radiomics-based
survival analysis to deliver accurate, annotation-efficient, and interpretable
outcome prediction in CRLM.

</details>


### [17] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

> 本研究提出CompCon算法，通过进化搜索发现由两个文本到图像生成模型输出的视觉差异及其相关的提示类型，并创建了ID2数据集来评估其性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探讨使用两种不同的生成模型学习的视觉表示在何时以及如何产生分歧，识别一个模型生成的图像中出现而另一个模型不出现的视觉属性，以及触发这些属性差异的提示类型。

**Method:** 本文提出了CompCon（Comparing Concepts），这是一种进化搜索算法，用于发现由两个不同文本到图像模型生成的图像中更加常见的视觉属性，以及引发这些视觉差异的提示类型。

**Result:** 通过创建一个自动化数据生成管道生成了一个名为ID2的数据集，包含60个输入依赖的不同点，并将该方法与几种基于LLM和VLM的方法进行比较。结果验证了CompCon发现不同表示的能力。

**Conclusion:** 通过使用CompCon算法，作者们发现了一些不同的图像模型对特定文本提示的不同视觉表达，例如PixArt和Stable Diffusion 3.5对孤独和非洲裔美国人媒体职业的描绘存在差异。

**Abstract:** In this paper, we investigate when and how visual representations learned by
two different generative models diverge. Given two text-to-image models, our
goal is to discover visual attributes that appear in images generated by one
model but not the other, along with the types of prompts that trigger these
attribute differences. For example, "flames" might appear in one model's
outputs when given prompts expressing strong emotions, while the other model
does not produce this attribute given the same prompts. We introduce CompCon
(Comparing Concepts), an evolutionary search algorithm that discovers visual
attributes more prevalent in one model's output than the other, and uncovers
the prompt concepts linked to these visual differences. To evaluate CompCon's
ability to find diverging representations, we create an automated data
generation pipeline to produce ID2, a dataset of 60 input-dependent
differences, and compare our approach to several LLM- and VLM-powered
baselines. Finally, we use CompCon to compare popular text-to-image models,
finding divergent representations such as how PixArt depicts prompts mentioning
loneliness with wet streets and Stable Diffusion 3.5 depicts African American
people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>
