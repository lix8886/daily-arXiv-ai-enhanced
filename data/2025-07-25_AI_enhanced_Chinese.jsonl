{"id": "2507.17842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17842", "abs": "https://arxiv.org/abs/2507.17842", "authors": ["Yimeng Zhang", "Tian Wang", "Jiri Gesi", "Ziyi Wang", "Yuxuan Lu", "Jiacheng Lin", "Sinong Zhan", "Vianne Gao", "Ruochen Jiao", "Junze Liu", "Kun Qian", "Yuxin Tang", "Ran Xue", "Houyu Zhang", "Qingjun Cui", "Yufan Guo", "Dakuo Wang"], "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in\ngenerating 'believable human-like' behavior in web environments. Prior work has\nexplored augmenting training data with LLM-synthesized rationales and applying\nsupervised fine-tuning (SFT) to enhance reasoning ability, which in turn can\nimprove downstream action prediction. However, the performance of such\napproaches remains inherently bounded by the reasoning capabilities of the\nmodel used to generate the rationales. In this paper, we introduce Shop-R1, a\nnovel reinforcement learning (RL) framework aimed at enhancing the reasoning\nability of LLMs for simulation of real human behavior in online shopping\nenvironments Specifically, Shop-R1 decomposes the human behavior simulation\ntask into two stages: rationale generation and action prediction, each guided\nby distinct reward signals. For rationale generation, we leverage internal\nmodel signals (e.g., logit distributions) to guide the reasoning process in a\nself-supervised manner. For action prediction, we propose a hierarchical reward\nstructure with difficulty-aware scaling to prevent reward hacking and enable\nfine-grained reward assignment. This design evaluates both high-level action\ntypes and the correctness of fine-grained sub-action details (attributes and\nvalues), rewarding outputs proportionally to their difficulty. Experimental\nresults show that our method achieves a relative improvement of over 65%\ncompared to the baseline.", "AI": {"tldr": "本研究提出Shop-R1框架以改善大型语言模型在模拟线上购物环境中人类行为时的推理能力，通过自监督的推理过程和分层奖励结构来实现。实验表明，相比基线模型，本方法相对提升了65%以上。", "motivation": "先前的研究已经探索通过增强训练数据和应用监督微调来提高模型的推理能力，从而改善下游行为预测。然而，利用模型生成的理由的方法性能本质上受生成这些理由的模型推理能力的限制。为了突破这个限制，本研究提出了新的框架Shop-R1，以提高大型语言模型的推理能力，更好地模拟人类行为。", "method": "本研究提出了一个名为Shop-R1的新型强化学习框架，旨在提高大型语言模型在模拟线上购物环境中人类行为时的推理能力。Shop-R1将人类行为模拟任务分解为两个阶段：理由生成和行为预测，每个阶段都由不同的奖励信号引导。在理由生成阶段，利用内部模型信号（例如，logit分布）以自监督的方式引导推理过程。在行为预测阶段，提出了一种具有难度感知缩放的分层奖励结构，以防止奖励作弊并实现详细的奖励分配。这种设计评估了高层行为类型和精细动作细节（属性和值）的正确性，并根据其难度比例奖励输出。", "result": "实验结果显示，本方法相较于基线模型获得了超过65%的相对提升。", "conclusion": "Shop-R1框架通过引入自监督的推理过程和防止奖励作弊的分层奖励结构，显著提高了大型语言模型在模拟人类行为时的表现。这种方法在提升模型推理能力方面显示出了巨大潜力。"}}
{"id": "2507.17849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17849", "abs": "https://arxiv.org/abs/2507.17849", "authors": ["Zhangyue Yin", "Qiushi Sun", "Zhiyuan Zeng", "Qinyuan Cheng", "Xipeng Qiu", "Xuanjing Huang"], "title": "Dynamic and Generalizable Process Reward Modeling", "comment": "Accepted by ACL 2025 Main", "summary": "Process Reward Models (PRMs) are crucial for guiding Large Language Models\n(LLMs) in complex scenarios by providing dense reward signals. However,\nexisting PRMs primarily rely on heuristic approaches, which struggle with\ncross-domain generalization. While LLM-as-judge has been proposed to provide\ngeneralized rewards, current research has focused mainly on feedback results,\noverlooking the meaningful guidance embedded within the text. Additionally,\nstatic and coarse-grained evaluation criteria struggle to adapt to complex\nprocess supervision. To tackle these challenges, we propose Dynamic and\nGeneralizable Process Reward Modeling (DG-PRM), which features a reward tree to\ncapture and store fine-grained, multi-dimensional reward criteria. DG-PRM\ndynamically selects reward signals for step-wise reward scoring. To handle\nmultifaceted reward signals, we pioneeringly adopt Pareto dominance estimation\nto identify discriminative positive and negative pairs. Experimental results\nshow that DG-PRM achieves stunning performance on prevailing benchmarks,\nsignificantly boosting model performance across tasks with dense rewards.\nFurther analysis reveals that DG-PRM adapts well to out-of-distribution\nscenarios, demonstrating exceptional generalizability.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.17896", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.17896", "abs": "https://arxiv.org/abs/2507.17896", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL", "comment": null, "summary": "Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity.", "AI": {"tldr": "VeriMinder 是一个交互式系统，旨在帮助用户在使用自然语言接口进行数据分析时检测和缓解认知偏见，并通过三大创新实现这一目标，包括语义映射框架、系统化的数据分析指导和优化的LLM驱动系统。用户测试显示其显著提升了分析的质量。", "motivation": "随着自然语言接口在数据库应用系统中的普及，非统计学背景的用户面临着提出无偏见的分析问题的挑战。研究多集中在提高文本到SQL的转换准确度，而在分析问题中的认知偏见方面仍有待探索。", "method": "VeriMinder 采用三个主要创新：语义映射框架为特定分析环境定义偏差，实现系统化数据分析的方法论框架，以及通过多候选、批评反馈和自我反思过程生成高质量、任务特定提示的优化的LLM驱动系统。", "result": "在用户测试中，82.5%的参与者认为该系统正面影响了分析的质量。在对比评估中，VeriMinder 在分析的明确性、全面性和准确性方面的得分比其他方法至少高出20%。系统作为网络应用程序实现，旨在帮助用户在数据分析中避免提出“错误的问题”。", "conclusion": "VeriMinder 以开放源码的形式提供，有助于社区进一步的研究和使用。它在帮助用户提出无偏差的分析问题方面做出了显著贡献。"}}
{"id": "2507.17918", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17918", "abs": "https://arxiv.org/abs/2507.17918", "authors": ["Nhan Phan", "Anusha Porwal", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tamás Grósz", "Mikko Kurimo"], "title": "One Whisper to Grade Them All", "comment": "Accepted to SLaTE 2025 workshop", "summary": "We present an efficient end-to-end approach for holistic Automatic Speaking\nAssessment (ASA) of multi-part second-language tests, developed for the 2025\nSpeak & Improve Challenge. Our system's main novelty is the ability to process\nall four spoken responses with a single Whisper-small encoder, combine all\ninformation via a lightweight aggregator, and predict the final score. This\narchitecture removes the need for transcription and per-part models, cuts\ninference time, and makes ASA practical for large-scale Computer-Assisted\nLanguage Learning systems.\n  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming\nthe text-based baseline (0.44) while using at most 168M parameters (about 70%\nof Whisper-small). Furthermore, we propose a data sampling strategy, allowing\nthe model to train on only 44.8% of the speakers in the corpus and still reach\n0.383 RMSE, demonstrating improved performance on imbalanced classes and strong\ndata efficiency.", "AI": {"tldr": "The paper presents an efficient end-to-end Automatic Speaking Assessment system using a single encoder for multi-part language tests, achieving better results than baselines with fewer parameters.", "motivation": "The motivation behind this paper is to develop an end-to-end system for holistic Automatic Speaking Assessment (ASA) of multi-part second-language tests that is efficient, practical, and scalable for Computer-Assisted Language Learning systems.", "method": "Our system uses a single Whisper-small encoder to process all four spoken responses and a lightweight aggregator to combine the information and predict the final score. This avoids the need for transcription and separate models for each part of the test.", "result": "The system achieved a RMSE of 0.384, outperforming the text-based baseline while using fewer parameters. It also demonstrates good data efficiency with a data sampling strategy.", "conclusion": "The proposed system effectively reduces inference time, outperforms a text-based baseline, and shows strong performance when trained with a subset of the speakers, demonstrating the effectiveness and efficiency of the method."}}
{"id": "2507.17801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17801", "abs": "https://arxiv.org/abs/2507.17801", "authors": ["Yi Xin", "Juncheng Yan", "Qi Qin", "Zhen Li", "Dongyang Liu", "Shicheng Li", "Victor Shea-Jay Huang", "Yupeng Zhou", "Renrui Zhang", "Le Zhuo", "Tiancheng Han", "Xiaoqing Sun", "Siqi Luo", "Mengmeng Wang", "Bin Fu", "Yuewen Cao", "Hongsheng Li", "Guangtao Zhai", "Xiaohong Liu", "Yu Qiao", "Peng Gao"], "title": "Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling", "comment": "Tech Report, 23 pages, 11 figures, 7 tables", "summary": "We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model\nthat revisits and revitalizes the autoregressive paradigm for high-quality\nimage generation and beyond. Unlike existing approaches that rely on pretrained\ncomponents or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from\nscratch, enabling unrestricted architectural design and licensing freedom. It\nachieves generation quality on par with state-of-the-art diffusion models such\nas DALL-E 3 and SANA, while preserving the inherent flexibility and\ncompositionality of autoregressive modeling. Our unified tokenization scheme\nallows the model to seamlessly handle a wide spectrum of tasks-including\nsubject-driven generation, image editing, controllable synthesis, and dense\nprediction-within a single generative framework. To further boost usability, we\nincorporate efficient decoding strategies like inference-time scaling and\nspeculative Jacobi sampling to improve quality and speed, respectively.\nExtensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)\ndemonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses\ndiffusion-based models. Moreover, we confirm its multi-task capabilities on the\nGraph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally\nwell. These results position Lumina-mGPT 2.0 as a strong, flexible foundation\nmodel for unified multimodal generation. We have released our training details,\ncode, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.", "AI": {"tldr": "Lumina-mGPT 2.0 是一个独立的、仅解码的自回归模型，专门用于高质量图像生成及其他任务。该模型从零开始训练，具有无限制的架构设计和许可证自由。它在生成质量上与DALL-E 3和SANA等领先的扩散模型相当，同时保持了自回归建模灵活性和组成性。通过统一的标记方案，它能够处理包括主题驱动生成、图像编辑、可控合成和密集预测等任务。此外，增加高效的解码策略进一步提升质量和速度。它在标准的文本到图像基准测试上表现优秀，并在多任务能力上有出色表现。", "motivation": "推动图像生成和其他任务的高质量和灵活性，同时解决现有依赖于预训练组件或混合架构的方法的限制。Lumina-mGPT 2.0旨在提供一个不受约束的架构设计和许可证自由度，同时保持与顶级扩散模型相当的生成质量。", "method": "Lumina-mGPT 2.0是一个自回归模型，使用统一的标记方案进行训练，可以从零开始，不依赖预训练组件或混构结构。该方法采用了高效的解码策略，如推理时尺度和投机性的雅克比采样，以提高质量和速度。", "result": "Lumina-mGPT 2.0在标准的文本到图像基准测试和Graph200K基准上表现良好，不仅与扩散模型相匹敌，在某些情况下甚至超过了它们，证明了其作为统一多模式生成基础模型的潜力。", "conclusion": "Lumina-mGPT 2.0是一个强大的、灵活的统一多模态生成基础模型，它展示了通过自回归模型实现高质量图像生成和执行多种任务的能力。通过开放训练细节、代码和模型，进一步推动了本领域的研究和应用。"}}
{"id": "2507.17944", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17944", "abs": "https://arxiv.org/abs/2507.17944", "authors": ["Hulayyil Alshammari", "Praveen Rao"], "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text", "comment": null, "summary": "Large language models (LLMs) have rapidly transformed the creation of written\nmaterials. LLMs have led to questions about writing integrity, thereby driving\nthe creation of artificial intelligence (AI) detection technologies.\nAdversarial attacks, such as standard and humanized paraphrasing, inhibit\ndetectors' ability to detect machine-generated text. Previous studies have\nmainly focused on ChatGPT and other well-known LLMs and have shown varying\naccuracy across detectors. However, there is a clear gap in the literature\nabout DeepSeek, a recently published LLM. Therefore, in this work, we\ninvestigate whether six generally accessible AI detection tools -- AI Text\nClassifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can\nconsistently recognize text generated by DeepSeek. The detectors were exposed\nto the aforementioned adversarial attacks. We also considered DeepSeek as a\ndetector by performing few-shot prompting and chain-of-thought reasoning (CoT)\nfor classifying AI and human-written text. We collected 49 human-authored\nquestion-answer pairs from before the LLM era and generated matching responses\nusing DeepSeek-v3, producing 49 AI-generated samples. Then, we applied\nadversarial techniques such as paraphrasing and humanizing to add 196 more\nsamples. These were used to challenge detector robustness and assess accuracy\nimpact. While QuillBot and Copyleaks showed near-perfect performance on\noriginal and paraphrased DeepSeek text, others -- particularly AI Text\nClassifier and GPT-2 -- showed inconsistent results. The most effective attack\nwas humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and\n52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best\nfive-shot result misclassifying only one of 49 samples (AI recall 96%, human\nrecall 100%).", "AI": {"tldr": "研究对象是六个公开的AI检测工具对DeepSeek生成文本的检测能力，特别是遭受humanization这类攻击后的表现，发现QuillBot和Copyleaks在原始和改写文本检测上有良好表现，GPT2和AI Text Classifier则表现不稳定。DeepSeek使用few-shot和CoT提示对文本分类展现出较高准确性。", "motivation": "当前大多数研究主要集中在知名的LLM如ChatGPT上，而DeepSeek作为一个新公布的语言模型，其生成文本的检测成为研究空白。本研究旨在探索六种公开的AI检测工具能否准确识别DeepSeek生成的文本，特别是在遭受对手攻击时的表现。", "method": "本研究通过使用六个可获取的AI检测工具对DeepSeek生成的文本进行检测，这些工具分别是AI Text Classifier、Content Detector AI、Copyleaks、QuillBot、GPT-2和GPTZero。此外，研究还使用了DeepSeek自身的few-shot prompting和链式思维推理(CoT)对AI和人类撰写的文本进行分类。", "result": "QuillBot和Copyleaks在原始和改写的DeepSeek文本上表现出近乎完美的性能，但其他工具如AI Text Classifier和GPT-2的结果则不够稳定。最有效的攻击手段是humanization，它将Copyleaks的准确率降至71%，QuillBot降至58%，GPTZero降至52%。同时，few-shot和CoT提示方法展示了较高的准确率，特别是在五次射击中仅误判了一次样本，AI召回率为96%，人类召回率为100%。", "conclusion": "研究表明，多数AI检测工具在识别DeepSeek的改写和humanization文本时存在挑战，特别是依赖于GPT-2的工具表现不佳。而five-shot CoT方法展现了高准确率，表明它可能是识别DeepSeek文本的有效方法。"}}
{"id": "2507.17844", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17844", "abs": "https://arxiv.org/abs/2507.17844", "authors": ["Sai Varun Kodathala", "Yashwanth Reddy Vutukoori", "Rakesh Vunnam"], "title": "SV3.3B: A Sports Video Understanding Model for Action Recognition", "comment": "8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025", "summary": "This paper addresses the challenge of automated sports video analysis, which\nhas traditionally been limited by computationally intensive models requiring\nserver-side processing and lacking fine-grained understanding of athletic\nmovements. Current approaches struggle to capture the nuanced biomechanical\ntransitions essential for meaningful sports analysis, often missing critical\nphases like preparation, execution, and follow-through that occur within\nseconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B\nparameter video understanding model that combines novel temporal motion\ndifference sampling with self-supervised learning for efficient on-device\ndeployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction\nmechanism that intelligently identifies the 16 most representative frames from\nsports sequences, followed by a V-DWT-JEPA2 encoder pretrained through\nmask-denoising objectives and an LLM decoder fine-tuned for sports action\ndescription generation. Evaluated on a subset of the NSVA basketball dataset,\nSV3.3B achieves superior performance across both traditional text generation\nmetrics and sports-specific evaluation criteria, outperforming larger\nclosed-source models including GPT-4o variants while maintaining significantly\nlower computational requirements. Our model demonstrates exceptional capability\nin generating technically detailed and analytically rich sports descriptions,\nachieving 29.2% improvement over GPT-4o in ground truth validation metrics,\nwith substantial improvements in information density, action complexity, and\nmeasurement precision metrics essential for comprehensive athletic analysis.\nModel Available at https://huggingface.co/sportsvision/SV3.3B.", "AI": {"tldr": "提出了SV3.3B，一个轻量级的视频理解模型，用以解决传统自动体育视频分析中计算密集型模型和缺乏精细运动理解的问题。该模型在NSVA篮球数据集的子集上评估，展示了能够生成技术详细和分析丰富的体育描述的能力，优于较大的闭源模型如GPT-4o变种模型。", "motivation": "旨在解决自动化体育视频分析中模型计算量大、需服务器端处理以及缺乏对运动生物力学细微过渡的理解等问题。", "method": "SV3.3B模型结合了新颖的时域运动差异采样和自监督学习，并采用DWT-VGG16-LDA机制提取关键帧，使用V-DWT-JEPA2编码器和LLM解码器进行预训练和针对体育动作描述生成的微调。", "result": "在NSVA篮球数据集的子集上进行评估，展示了比GPT-4o更高的性能，特别是在技术细节和动作复杂度方面有显著改善。", "conclusion": "SV3.3B模型在生成技术详细和分析丰富的体育描述方面表现优越，优于较大的闭源模型，并且具有更低的计算需求。"}}
{"id": "2507.17951", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17951", "abs": "https://arxiv.org/abs/2507.17951", "authors": ["Sohaib Imran", "Ihor Kendiukhov", "Matthew Broerman", "Aditya Thomas", "Riccardo Campanella", "Rob Lamb", "Peter M. Atkinson"], "title": "Are LLM Belief Updates Consistent with Bayes' Theorem?", "comment": "Accepted at the ICML 2025 Workshop on Assessing World Models", "summary": "Do larger and more capable language models learn to update their \"beliefs\"\nabout propositions more consistently with Bayes' theorem when presented with\nevidence in-context? To test this, we formulate a Bayesian Coherence\nCoefficient (BCC) metric and generate a dataset with which to measure the BCC.\nWe measure BCC for multiple pre-trained-only language models across five model\nfamilies, comparing against the number of model parameters, the amount of\ntraining data, and model scores on common benchmarks. Our results provide\nevidence for our hypothesis that larger and more capable pre-trained language\nmodels assign credences that are more coherent with Bayes' theorem. These\nresults have important implications for our understanding and governance of\nLLMs.", "AI": {"tldr": "研究发现更大、更强大的语言模型在更新信念方面更加符合贝叶斯定理。", "motivation": "研究更大的、能力更强的语言模型在面对上下文证据时是否能更一致地遵循贝叶斯定理更新其关于命题的'信念'。", "method": "我们提出了一种贝叶斯一致系数（BCC）指标，并生成了相应的数据集来测量BCC。", "result": "结果表明，更大的和更强大的预训练语言模型在分配概率时更符合贝叶斯定理。", "conclusion": "这些结果对我们理解和管理语言模型具有重要意义。"}}
{"id": "2507.17853", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17853", "abs": "https://arxiv.org/abs/2507.17853", "authors": ["Lifeng Chen", "Jiner Wang", "Zihao Pan", "Beier Zhu", "Xiaofeng Yang", "Chi Zhang"], "title": "Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models", "comment": null, "summary": "Recent advances in text-to-image (T2I) generation have led to impressive\nvisual results. However, these models still face significant challenges when\nhandling complex prompt, particularly those involving multiple subjects with\ndistinct attributes. Inspired by the human drawing process, which first\noutlines the composition and then incrementally adds details, we propose\nDetail++, a training-free framework that introduces a novel Progressive Detail\nInjection (PDI) strategy to address this limitation. Specifically, we decompose\na complex prompt into a sequence of simplified sub-prompts, guiding the\ngeneration process in stages. This staged generation leverages the inherent\nlayout-controlling capacity of self-attention to first ensure global\ncomposition, followed by precise refinement. To achieve accurate binding\nbetween attributes and corresponding subjects, we exploit cross-attention\nmechanisms and further introduce a Centroid Alignment Loss at test time to\nreduce binding noise and enhance attribute consistency. Extensive experiments\non T2I-CompBench and a newly constructed style composition benchmark\ndemonstrate that Detail++ significantly outperforms existing methods,\nparticularly in scenarios involving multiple objects and complex stylistic\nconditions.", "AI": {"tldr": "The paper proposes Detail++, a training-free framework for text-to-image generation that uses a Progressive Detail Injection (PDI) strategy to handle complex prompts with multiple subjects and distinct attributes.", "motivation": "The motivation is to address the difficulty of current text-to-image generation models in handling complex prompts with multiple subjects and attributes.", "method": "Detail++ decomposes complex prompts into a sequence of simplified sub-prompts, using a staged generation approach that first ensures global composition and then refines details, leveraging the layout-controlling capacity of self-attention and cross-attention mechanisms with a Centroid Alignment Loss for accurate attribute binding.", "result": "The experiments on T2I-CompBench and a new style composition benchmark show that Detail++ outperforms existing methods, especially in scenarios with multiple objects and complex styles.", "conclusion": "The paper concludes that the proposed Detail++ framework effectively improves the performance of text-to-image generation in complex prompt scenarios, demonstrating the potential of staged detail injection and attribute alignment mechanisms."}}
{"id": "2507.17974", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17974", "abs": "https://arxiv.org/abs/2507.17974", "authors": ["Fitsum Gaim", "Jong C. Park"], "title": "Natural Language Processing for Tigrinya: Current State and Future Directions", "comment": null, "summary": "Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable.\\footnote{Tigrinya NLP Anthology:\nhttps://github.com/fgaim/tigrinya-nlp-anthology.", "AI": {"tldr": "论文综述了提格烈尼亚语的NLP研究，涵盖了十年以上的研究成果，分析了资源创建的里程碑对研究进展的影响，并指出了主要挑战和研究方向。", "motivation": "研究提格烈尼亚语这种虽然被数百万人使用，但在自然语言处理（NLP）研究中代表性不足的语言。", "method": "该论文通过系统性回顾2011年至2025年超过40项研究，分析了提格烈尼亚语在计算资源、模型和应用程序在内的十个不同下游任务中的当前状态。", "result": "研究表明，从基于规则的基础系统到现代神经架构有一个清晰的发展轨迹，资源创建的里程碑一直是推动研究进展的关键。研究识别了提格烈尼亚语在形态复杂性和资源稀缺性方面的主要挑战，并提出了有前景的研究方向。", "conclusion": "该工作不仅为研究人员提供了一个全面的参考，而且为推进提格烈尼亚语NLP提供了路线图。"}}
{"id": "2507.17859", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17859", "abs": "https://arxiv.org/abs/2507.17859", "authors": ["Muayad Abujabal", "Lyes Saad Saoud", "Irfan Hussain"], "title": "FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains", "comment": null, "summary": "Accurate fish detection in underwater imagery is essential for ecological\nmonitoring, aquaculture automation, and robotic perception. However, practical\ndeployment remains limited by fragmented datasets, heterogeneous imaging\nconditions, and inconsistent evaluation protocols. To address these gaps, we\npresent \\textit{FishDet-M}, the largest unified benchmark for fish detection,\ncomprising 13 publicly available datasets spanning diverse aquatic environments\nincluding marine, brackish, occluded, and aquarium scenes. All data are\nharmonized using COCO-style annotations with both bounding boxes and\nsegmentation masks, enabling consistent and scalable cross-domain evaluation.\nWe systematically benchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.\nEvaluations are conducted using standard metrics including mAP, mAP@50, and\nmAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and\ninference profiling in terms of latency and parameter count. The results\nhighlight the varying detection performance across models trained on FishDet-M,\nas well as the trade-off between accuracy and efficiency across models of\ndifferent architectures. To support adaptive deployment, we introduce a\nCLIP-based model selection framework that leverages vision-language alignment\nto dynamically identify the most semantically appropriate detector for each\ninput image. This zero-shot selection strategy achieves high performance\nwithout requiring ensemble computation, offering a scalable solution for\nreal-time applications. FishDet-M establishes a standardized and reproducible\nplatform for evaluating object detection in complex aquatic scenes. All\ndatasets, pretrained models, and evaluation tools are publicly available to\nfacilitate future research in underwater computer vision and intelligent marine\nsystems.", "AI": {"tldr": "本研究提出了FishDet-M，一个用于鱼类检测的最大统一数据集，并评估了不同模型的性能，同时引入了一种零样本的模型选择框架，提高了系统适应性。", "motivation": "本研究的动机在于现有的鱼类检测技术由于数据集零散、成像条件各异和评估协议不一致而难以实际部署。", "method": "我们提出了FishDet-M，这是一个用于鱼类检测的最大统一基准，包含13个公开数据集，涵盖了多种水生环境。所有的数据都采用COCO样式的标注，包括边界框和分割掩模，以实现一致性和规模化的跨域评估。我们系统地评估了28种现代目标检测模型，包括YOLOv8到YOLOv12系列、基于R-CNN的检测器和基于DETR的模型。", "result": "结果表明，通过FishDet-M训练的不同模型展现出各异的检测性能，并揭示了不同架构模型在精度和效率之间的权衡。通过引入一种基于CLIP的模型选择框架，利用视觉-语言对齐来动态识别最适合每个输入图像的检测器，我们实现了高效率的性能，且无需进行集成计算。", "conclusion": "FishDet-M为复杂水下场景的对象检测提供了一个标准化和可复现的平台。所有的数据集、预训练模型和评估工具都已公开，促进了未来水下计算机视觉和智能海洋系统的研究。"}}
{"id": "2507.18013", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.18013", "abs": "https://arxiv.org/abs/2507.18013", "authors": ["Zihan Wang", "Xinzhang Liu", "Yitong Yao", "Chao Wang", "Yu Zhao", "Zhihao Yang", "Wenmin Deng", "Kaipeng Jia", "Jiaxin Peng", "Yuyao Huang", "Sishi Xiong", "Zhuo Jiang", "Kaidong Yu", "Xiaohui Hu", "Fubei Yao", "Ruiyu Fang", "Zhuoru Jiang", "Ruiting Song", "Qiyi Xie", "Rui Xue", "Xuewei He", "Yanlei Xue", "Zhu Yuan", "Zhaoxi Zhang", "Zilu Huang", "Shiquan Wang", "Xin Wang", "Hanming Wu", "Mingyuan Wang", "Xufeng Zhan", "Yuhan Sun", "Zhaohu Xing", "Yuhao Jiang", "Bingkai Yang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "Technical Report of TeleChat2, TeleChat2.5 and T1", "comment": "32 pages, 5 figures", "summary": "We introduce the latest series of TeleChat models: \\textbf{TeleChat2},\n\\textbf{TeleChat2.5}, and \\textbf{T1}, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith \\textbf{TeleChat2}, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. \\textbf{TeleChat2.5} and \\textbf{T1} expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The \\textbf{T1} variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, \\textbf{TeleChat2.5} prioritizes speed, delivering rapid\ninference. Both flagship models of \\textbf{T1} and \\textbf{TeleChat2.5} are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, \\textbf{T1-115B} outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release \\textbf{TeleChat2},\n\\textbf{TeleChat2.5} and \\textbf{T1}, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications.", "AI": {"tldr": "本论文介绍了TeleChat系列的最新模型TeleChat2, TeleChat2.5和T1，通过对训练策略的改进而提升了性能，尤其在复杂推理和代码生成方面。", "motivation": "旨在通过改进训练策略而不是改变模型架构来提升语言模型的性能，尤其是在代码生成和数学推理任务上。", "method": "首先通过大规模高质量数据进行预训练，随后进行监督微调和直接偏好优化。T1和TeleChat2.5进一步通过持续预训练和强化学习提升特定领域的性能。", "result": "T1在复杂推理和数学、代码任务上表现出色，而TeleChat2.5则更注重推理速度。", "conclusion": "TeleChat系列的最新模型展示了显著的推理能力提升，并且已经公开发布，以支持开发者和研究人员的应用。"}}
{"id": "2507.17860", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17860", "abs": "https://arxiv.org/abs/2507.17860", "authors": ["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"], "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "comment": null, "summary": "Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.", "AI": {"tldr": "研究利用LightningDiT模型评估黑色素瘤分类器的公平性，发现合成数据在公平性评估中很有前景，但不同训练数据集下的模型会带来验证上的挑战。", "motivation": "该研究的主要动机是评估和提高应用于皮肤癌筛查中的AI系统的公平性，特别是考虑到潜在的未预见和固有的偏见。", "method": "本研究利用了最先进的生成式人工智能（GenAI）LightningDiT模型来评估公开的黑色素瘤分类器的公平性。", "result": "研究结果表明，使用高度逼真的合成数据进行公平性评估是一个有前景的方向。然而，当用于评估的黑色素瘤检测模型是基于与合成图像数据集不同的数据集训练时，验证公平性就变得困难。", "conclusion": "我们提出的方法为利用合成数据来衡量和增强医学成像GenAI系统的公平性提供了一个有价值的途径。"}}
{"id": "2507.18028", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18028", "abs": "https://arxiv.org/abs/2507.18028", "authors": ["Weizhi Fei", "Hao Shi", "Jing Xu", "Jingchen Peng", "Jiazheng Li", "Jingzhao Zhang", "Bo Bai", "Wei Han", "Zhenyuan Chen", "Xueyan Niu"], "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database", "comment": null, "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).", "AI": {"tldr": "NeuralDB is a novel editing framework for large language models that uses a neural KV database and a gated retrieval module to enhance editing efficacy without compromising the model's general abilities, demonstrating success in scaling up to 100,000 facts.", "motivation": "To enable efficient editing of large language models without compromising their general abilities or resulting in forgetting edited facts when scaling up the number of edits.", "method": "NeuralDB, a framework that represents edited facts as a neural Key-Value database with a non-linear gated retrieval module, ensuring the preservation of the LLM's general abilities when editing.", "result": "NeuralDB outperforms existing methods in editing efficacy, generalization, specificity, fluency, and consistency. It preserves the overall performance across various text understanding and generation tasks and maintains effectiveness even when scaling to 100,000 facts.", "conclusion": "NeuralDB effectively addresses the challenges of large-scale fact editing in LLMs, ensuring that their general abilities are preserved without significant degradation in performance."}}
{"id": "2507.17892", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17892", "abs": "https://arxiv.org/abs/2507.17892", "authors": ["Hanzhou Liu", "Binghan Li", "Chengkai Liu", "Mi Lu"], "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration", "comment": null, "summary": "Transformers, with their self-attention mechanisms for modeling long-range\ndependencies, have become a dominant paradigm in image restoration tasks.\nHowever, the high computational cost of self-attention limits scalability to\nhigh-resolution images, making efficiency-quality trade-offs a key research\nfocus. To address this, Restormer employs channel-wise self-attention, which\ncomputes attention across channels instead of spatial dimensions. While\neffective, this approach may overlook localized artifacts that are crucial for\nhigh-quality image restoration. To bridge this gap, we explore Dilated\nNeighborhood Attention (DiNA) as a promising alternative, inspired by its\nsuccess in high-level vision tasks. DiNA balances global context and local\nprecision by integrating sliding-window attention with mixed dilation factors,\neffectively expanding the receptive field without excessive overhead. However,\nour preliminary experiments indicate that directly applying this global-local\ndesign to the classic deblurring task hinders accurate visual restoration,\nprimarily due to the constrained global context understanding within local\nattention. To address this, we introduce a channel-aware module that\ncomplements local attention, effectively integrating global context without\nsacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based\narchitecture specifically designed for image restoration, achieves competitive\nresults across multiple benchmarks, offering a high-quality solution for\ndiverse low-level computer vision problems.", "AI": {"tldr": "本文提出DiNAT-IR，一种结合滑动窗口注意力与混合扩张因子的Transformer架构，实现了全局上下文与局部细节的平衡，适用于图像修复任务，表现出色。", "motivation": "鉴于传统的基于通道的自注意力机制在处理高分辨率图像时面临计算成本过高的问题，本文旨在提出一种新的注意力机制来平衡全局上下文理解与局部细节精准度，以实现高效的图像复原效果。", "method": "DiNAT-IR 采用通道感知模块结合局部注意力机制来集成全局上下文，这是一款针对图像恢复任务的Transformer架构。", "result": "提出的DiNAT-IR在多个基准数据集中取得了具有竞争力的结果。", "conclusion": "研究结果表明，结合局部注意力机制与全局上下文理解的DiNAT-IR是解决低层次视觉问题提供了一种高效且高质量的解决方案。"}}
{"id": "2507.18043", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18043", "abs": "https://arxiv.org/abs/2507.18043", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs", "comment": "21 pages. Code: https://github.com/duykhuongnguyen/GrAInS", "summary": "Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.", "AI": {"tldr": "GrAInS是一种在推理时间调整语言模型和视觉语言模型行为的方法，通过识别最有影响力的令牌并构造方向性引导向量来改进模型的行为，这种方法在无需重新训练模型的情况下提供了精细、可解释且具有模块化特性的模型行为控制。", "motivation": "现有的推理时间引导方法依赖于不变的、全局的干预向量，忽视了单个输入令牌的因果影响，且未能善加利用模型logits中的有用梯度，特别是在视觉和文本输入贡献不均的多模态场景下。", "method": "GrAInS通过对比性和基于梯度的归因方法来识别最有影响力的top-k个令牌，依据它们对优选或非优选输出的贡献，并构建方向性引导向量以捕捉从不理想到理想行为的语义变化。在推理过程中，GrAInS根据令牌级归因信号调整变压器层中的隐藏激活，并对激活进行归一化处理以保持表示尺度。", "result": "实验结果显示，相较于微调和其他现有引导基线，GrAInS在多个评估任务和模型中表现出更高的性能，例如在TruthfulQA任务中使用Llama-3.1-8B时获得了13.22%的准确性提升。", "conclusion": "GrAInS提供了一种无需重新训练或辅助监督即可进行精细、可解释和模块化模型行为控制的方法，实验表明这种方法在多种任务和评估指标上均优于现有的微调和引导方法。"}}
{"id": "2507.17957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17957", "abs": "https://arxiv.org/abs/2507.17957", "authors": ["Md. Al-Masrur Khan", "Durgakant Pushp", "Lantao Liu"], "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation", "comment": null, "summary": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is\ntrained on labeled source domain data (e.g., synthetic images) and adapted to\nan unlabeled target domain (e.g., real-world images) without access to target\nannotations. Existing UDA-SS methods often struggle to balance fine-grained\nlocal details with global contextual information, leading to segmentation\nerrors in complex regions. To address this, we introduce the Adaptive Feature\nRefinement (AFR) module, which enhances segmentation accuracy by refining\nhighresolution features using semantic priors from low-resolution logits. AFR\nalso integrates high-frequency components, which capture fine-grained\nstructures and provide crucial boundary information, improving object\ndelineation. Additionally, AFR adaptively balances local and global information\nthrough uncertaintydriven attention, reducing misclassifications. Its\nlightweight design allows seamless integration into HRDA-based UDA methods,\nleading to state-of-the-art segmentation performance. Our approach improves\nexisting UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on\nSynthia-->Cityscapes. The implementation of our framework is available at:\nhttps://github.com/Masrur02/AFRDA", "AI": {"tldr": "提出了一种自适应特征细化(AFR)模块，以解决UDA-SS中的分割问题，它通过不确定驱动注意力平衡局部和全局信息，增强了细粒度结构和边界信息，从而提高分割性能。", "motivation": "现有的UDA-SS方法往往难以平衡细粒度局部细节和全局上下文信息，导致复杂区域的分割错误。", "method": "AFR模块通过使用来自低分辨率logits的语义先验来细化高分辨率特征，从而增强分割准确性。AFR还结合了捕获细粒度结构和提供关键边界信息的高频分量，改善了物体描绘。此外，AFR通过不确定驱动注意力自适应地平衡局部和全局信息，减少了误分类。其轻量级设计允许无缝集成到基于HRDA的UDA方法中。", "result": "在GTA V-->Cityscapes和Synthia-->Cityscapes上，分割性能(mIoU)分别提高了1.05%和1.04%。", "conclusion": "AFR模块改进了现有的UDA-SS方法的分割性能，达到了新的状态。"}}
{"id": "2507.18044", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18044", "abs": "https://arxiv.org/abs/2507.18044", "authors": ["Hoyeon Lee", "Sejung Son", "Ye-Eun Kang", "Jong-Hwan Kim"], "title": "Synthetic Data Generation for Phrase Break Prediction with Large Language Model", "comment": "Accepted at Interspeech 2025", "summary": "Current approaches to phrase break prediction address crucial prosodic\naspects of text-to-speech systems but heavily rely on vast human annotations\nfrom audio or text, incurring significant manual effort and cost. Inherent\nvariability in the speech domain, driven by phonetic factors, further\ncomplicates acquiring consistent, high-quality data. Recently, large language\nmodels (LLMs) have shown success in addressing data challenges in NLP by\ngenerating tailored synthetic data while reducing manual annotation needs.\nMotivated by this, we explore leveraging LLM to generate synthetic phrase break\nannotations, addressing the challenges of both manual annotation and\nspeech-related tasks by comparing with traditional annotations and assessing\neffectiveness across multiple languages. Our findings suggest that LLM-based\nsynthetic data generation effectively mitigates data challenges in phrase break\nprediction and highlights the potential of LLMs as a viable solution for the\nspeech domain.", "AI": {"tldr": "通过使用大型语言模型生成合成短语断点注释，减少文本转语音系统中的手动注释需求并改善语音相关任务的处理效果。", "motivation": "减少文本转语音系统中手动注释的需求，同时解决语音领域内在变化带来的问题", "method": "利用大型语言模型生成合成短语断点注释，并将其与传统注释进行比较", "result": "发现基于大型语言模型的合成数据生成方法能够有效地缓解短语断点预测的数据挑战", "conclusion": "大型语言模型作为生成合成数据的方法在语音领域显示出潜力。"}}
{"id": "2507.17959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17959", "abs": "https://arxiv.org/abs/2507.17959", "authors": ["Ali Abedi", "Sadaf Safa", "Tracey J. F. Colella", "Shehroz S. Khan"], "title": "OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments", "comment": "14 pages, 3 figures, 7 tables", "summary": "Engagement in virtual learning is essential for participant satisfaction,\nperformance, and adherence, particularly in online education and virtual\nrehabilitation, where interactive communication plays a key role. Yet,\naccurately measuring engagement in virtual group settings remains a challenge.\nThere is increasing interest in using artificial intelligence (AI) for\nlarge-scale, real-world, automated engagement recognition. While engagement has\nbeen widely studied in younger academic populations, research and datasets\nfocused on older adults in virtual and telehealth learning settings remain\nlimited. Existing methods often neglect contextual relevance and the\nlongitudinal nature of engagement across sessions. This paper introduces OPEN\n(Older adult Patient ENgagement), a novel dataset supporting AI-driven\nengagement recognition. It was collected from eleven older adults participating\nin weekly virtual group learning sessions over six weeks as part of cardiac\nrehabilitation, producing over 35 hours of data, making it the largest dataset\nof its kind. To protect privacy, raw video is withheld; instead, the released\ndata include facial, hand, and body joint landmarks, along with affective and\nbehavioral features extracted from video. Annotations include binary engagement\nstates, affective and behavioral labels, and context-type indicators, such as\nwhether the instructor addressed the group or an individual. The dataset offers\nversions with 5-, 10-, 30-second, and variable-length samples. To demonstrate\nutility, multiple machine learning and deep learning models were trained,\nachieving engagement recognition accuracy of up to 81 percent. OPEN provides a\nscalable foundation for personalized engagement modeling in aging populations\nand contributes to broader engagement recognition research.", "AI": {"tldr": "OPEN数据集支持AI驱动的老年人参与度识别，是一个包含11位老年人在六个星期内的虚拟学习会话产生的35小时数据的大型数据集。", "motivation": "该论文的动机在于，现有针对老年人在虚拟和远程医疗学习环境中参与度的研究和数据集非常有限，并且现有方法往往忽视了参与度的背景相关性和跨会话的纵向性质。通过引入OPEN数据集，作者旨在为老年人的参与度建模和更广泛地参与度识别研究提供一个可扩展的基础。", "method": "此论文介绍了OPEN数据集，这是一个专为老年人在虚拟和远程医疗学习环境中参与度识别而设计的数据集。数据集通过在六周内每周对11位老年人进行虚拟小组学习会话来收集，生成了超过35小时的数据，是同类最大的数据集。为了保护隐私，未公开原始视频，而是提供了从视频中提取的面部、手部和身体关节标记，以及情感和行为特征。", "result": "使用多个机器学习和深度学习模型进行了训练，实现了最高81%的参与度识别准确率。", "conclusion": "该数据集对老年人群参与度的个性化模型提供了可扩展的基础，推动了更广泛的参与度识别研究。"}}
{"id": "2507.18055", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18055", "abs": "https://arxiv.org/abs/2507.18055", "authors": ["Tevin Atwal", "Chan Nam Tieu", "Yefeng Yuan", "Zhan Shi", "Yuhong Liu", "Liang Cheng"], "title": "Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs", "comment": null, "summary": "The increasing use of synthetic data generated by Large Language Models\n(LLMs) presents both opportunities and challenges in data-driven applications.\nWhile synthetic data provides a cost-effective, scalable alternative to\nreal-world data to facilitate model training, its diversity and privacy risks\nremain underexplored. Focusing on text-based synthetic data, we propose a\ncomprehensive set of metrics to quantitatively assess the diversity (i.e.,\nlinguistic expression, sentiment, and user perspective), and privacy (i.e.,\nre-identification risk and stylistic outliers) of synthetic datasets generated\nby several state-of-the-art LLMs. Experiment results reveal significant\nlimitations in LLMs' capabilities in generating diverse and privacy-preserving\nsynthetic data. Guided by the evaluation results, a prompt-based approach is\nproposed to enhance the diversity of synthetic reviews while preserving\nreviewer privacy.", "AI": {"tldr": "本文针对由大规模语言模型生成的合成文本数据，提出了一套量化评估其多样性和隐私风险的指标，并通过实验揭示了这些模型生成多样化且保护隐私的合成数据的局限性，最终提出了一种基于提示的方法来提高合成评论的多样性同时保护评论者的隐私。", "motivation": "随着大规模语言模型生成的合成数据在数据驱动的应用中越来越受欢迎，其多样性和隐私风险尚未被充分探索。本文旨在通过专门的评估指标来量化合成文本数据的这些方面。", "method": "本文提出了一整套指标来评估合成数据集的多样性（包括语言表达、情感和用户视角）和隐私风险（包括重新识别风险和风格异常）。", "result": "实验结果显示，目前最先进的大规模语言模型生成的合成数据存在多样性和隐私保护方面的显著局限性。", "conclusion": "基于实验结果，本文提出了一种基于提示的方法以提高合成评论的多样性，同时确保用户的隐私。"}}
{"id": "2507.17987", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17987", "abs": "https://arxiv.org/abs/2507.17987", "authors": ["Arsen Yermukan", "Pedro Machado", "Feliciano Domingos", "Isibor Kennedy Ihianle", "Jordan J. Bird", "Stefano S. K. Kaburu", "Samantha J. Ward"], "title": "Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring", "comment": null, "summary": "Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is\ntime-consuming and prone to errors. This project introduces an automated system\nfor real-time video analysis, using You Only Look Once (YOLO) object detection\nmodels to identify two key behaviours: basking and hunting. We trained five\nYOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of\n1200 images, encompassing bearded dragons (600), heating lamps (500), and\ncrickets (100). YOLOv8s was selected as the optimal model due to its superior\nbalance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes\nvideo footage by extracting per-frame object coordinates, applying temporal\ninterpolation for continuity, and using rule-based logic to classify specific\nbehaviours. Basking detection proved reliable. However, hunting detection was\nless accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).\nFuture improvements will focus on enhancing cricket detection through expanded\ndatasets or specialised small-object detectors. This automated system offers a\nscalable solution for monitoring reptile behaviour in controlled environments,\nsignificantly improving research efficiency and data quality.", "AI": {"tldr": "本研究开发了一个自动化的食火鸡龙行为分析系统，使用YOLO模型识别其行为。虽然在检测狩猎行为上存在一定挑战，但该系统提高了效率和数据准确度。", "motivation": "传统的食火鸡龙行为监控是耗时且容易出错的。本项目引入了一种自动化系统以进行实时视频分析，用以提高监控效率和数据质量。", "method": "使用You Only Look Once (YOLO)对象检测模型对食火鸡龙（Pogona Viticeps）的两个关键行为：晒太阳和狩猎，进行了实时视频分析。训练了五个YOLO变体（v5, v7, v8, v11, v12）在一个包含600张食火鸡龙图像、500张加热灯图像和100张蟋蟀图像的自定义公开数据集上。检测出来的对象坐标通过帧提取，时间插值确保连续性，并使用规则逻辑分类特定行为。", "result": "YOLOv8s模型因其在准确性和速度之间的优越平衡被选为最佳模型（mAP@0.5:0.95=0.855）。晒太阳行为检测的可靠性较好，而狩猎行为检测相对不那么准确，主要是因为蟋蟀检测的准确性较低（mAP@0.5=0.392）。", "conclusion": "该自动化系统为受控环境下的爬行动物行为监控提供了一种可扩展的解决方案，大大提高了研究效率和数据质量。"}}
{"id": "2507.18061", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18061", "abs": "https://arxiv.org/abs/2507.18061", "authors": ["Zehan Li", "Hongjie Chen", "Yuxin Zhang", "Jing Zhou", "Xuening Wang", "Hang Lv", "Mengjie Du", "Yaodong Song", "Jie Lian", "Jian Kang", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios", "comment": null, "summary": "Spoken language models (SLMs) have seen rapid progress in recent years, along\nwith the development of numerous benchmarks for evaluating their performance.\nHowever, most existing benchmarks primarily focus on evaluating whether SLMs\ncan perform complex tasks comparable to those tackled by large language models\n(LLMs), often failing to align with how users naturally interact in real-world\nconversational scenarios. In this paper, we propose TELEVAL, a dynamic\nbenchmark specifically designed to evaluate SLMs' effectiveness as\nconversational agents in realistic Chinese interactive settings. TELEVAL\ndefines three evaluation dimensions: Explicit Semantics, Paralinguistic and\nImplicit Semantics, and System Abilities. It adopts a dialogue format\nconsistent with real-world usage and evaluates text and audio outputs\nseparately. TELEVAL particularly focuses on the model's ability to extract\nimplicit cues from user speech and respond appropriately without additional\ninstructions. Our experiments demonstrate that despite recent progress,\nexisting SLMs still have considerable room for improvement in natural\nconversational tasks. We hope that TELEVAL can serve as a user-centered\nevaluation framework that directly reflects the user experience and contributes\nto the development of more capable dialogue-oriented SLMs.", "AI": {"tldr": "本文提出了TELEVAl，一种专为评估中文环境下SLM对话能力而设计的动态基准。通过三个维度评估模型表现：显性语义、副语言和隐性语义、系统能力。实验表明现有模型仍需改进。", "motivation": "大多数现有的基准测试主要是评估SLM是否能执行类似于大型语言模型处理的任务，常与用户在现实世界中自然交互的方式不一致。", "method": "提出TELEVAl，这是一个动态基准测试，旨在评估SLM在现实中文交互场景中的有效性。TELEVAl定义了三个评估维度：显性语义、副语言和隐性语义、系统能力。它采用与现实世界使用一致的对话格式，并分别评估文本和音频输出。重点是模型能够从用户言语中提取隐含线索并适当回应的能力，无需额外指令。", "result": "实验表明，尽管近期有进展，现有的SLM在自然对话任务上仍有很大的改进空间。", "conclusion": "希望TELEVAl可以作为一个以用户为中心的评估框架，直接反映用户体验，有助于开发出更强大、面向对话的SLM。"}}
{"id": "2507.17995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17995", "abs": "https://arxiv.org/abs/2507.17995", "authors": ["Huy Nguyen", "Kien Nguyen", "Akila Pemasiri", "Akmal Jahan", "Clinton Fookes", "Sridha Sridharan"], "title": "AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID", "comment": "Accepted atIEEE International Joint Conference on Biometrics (IJCB)\n  2025", "summary": "Person re-identification (Re-ID) across visible and infrared modalities is\ncrucial for 24-hour surveillance systems, but existing datasets primarily focus\non ground-level perspectives. While ground-based IR systems offer nighttime\ncapabilities, they suffer from occlusions, limited coverage, and vulnerability\nto obstructions--problems that aerial perspectives uniquely solve. To address\nthese limitations, we introduce AG-VPReID.VIR, the first aerial-ground\ncross-modality video-based person Re-ID dataset. This dataset captures 1,837\nidentities across 4,861 tracklets (124,855 frames) using both UAV-mounted and\nfixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents\nunique challenges including cross-viewpoint variations, modality discrepancies,\nand temporal dynamics. Additionally, we propose TCC-VPReID, a novel\nthree-stream architecture designed to address the joint challenges of\ncross-platform and cross-modality person Re-ID. Our approach bridges the domain\ngaps between aerial-ground perspectives and RGB-IR modalities, through\nstyle-robust feature learning, memory-based cross-view adaptation, and\nintermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR\npresents distinctive challenges compared to existing datasets, with our\nTCC-VPReID framework achieving significant performance gains across multiple\nevaluation protocols. Dataset and code are available at\nhttps://github.com/agvpreid25/AG-VPReID.VIR.", "AI": {"tldr": "提出AG-VPReID.VIR数据集和TCC-VPReID框架，解决空地跨视角和RGB-IR跨模态行人重识别挑战。", "motivation": "为解决地面红外系统在遮挡、覆盖范围有限和易受阻塞等问题，提出了AG-VPReID.VIR，这是一个首个空地跨模态视频行人重识别数据集。", "method": "介绍了一种名为TCC-VPReID的三流架构，旨在解决跨平台和跨模态行人重识别的联合挑战，通过风格鲁棒特征学习、基于记忆的跨视图适应和中间指导时间建模来缩小空域和模态差异。", "result": "通过实验展示了所提数据集和方法的有效性，证明方法在多种评估协议中性能优异。", "conclusion": "实验表明AG-VPReID.VIR数据集相较于现有数据集具独特挑战，TCC-VPReID框架在多种评估协议中显著提高性能。"}}
{"id": "2507.18076", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18076", "abs": "https://arxiv.org/abs/2507.18076", "authors": ["Haomin Qi", "Zihan Dai", "Chengbo Huang"], "title": "Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints", "comment": "10 pages, 2 figures and 1 table", "summary": "Fine-tuning large language models (LLMs) remains a computational bottleneck\ndue to their scale and memory demands. This paper presents a comprehensive\nevaluation of parameter-efficient fine-tuning (PEFT) techniques, including\nLoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that\ndynamically integrates BOFT's orthogonal stability with LoRA-GA's\ngradient-aligned rapid convergence. By computing per-layer adaptive updates\nguided by gradient norms, the hybrid method achieves superior convergence\nefficiency and generalization across diverse tasks. We also explore, for the\nfirst time, the adaptation of unitary RNN (uRNN) principles to\ntransformer-based LLMs, enhancing gradient stability through structured unitary\nconstraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,\nand HumanEval -- using models ranging from 7B to 405B parameters demonstrate\nthat our hybrid method consistently outperforms individual PEFT baselines,\napproaching full fine-tuning accuracy while reducing resource consumption by up\nto 2.1 times in training time and 50 percent in memory usage. These findings\nestablish the hybrid approach as a practical and scalable fine-tuning solution\nfor real-world deployment of LLMs under resource constraints.", "AI": {"tldr": "A hybrid PEFT method is introduced that combines BOFT and LoRA-GA techniques, and adds uRNN principles to enhance gradient stability for efficient large language model fine-tuning with lower resource use.", "motivation": "The motivation is to address the computational bottleneck of fine-tuning large language models, which includes high scale and memory demands. The aim is to find an efficient fine-tuning method that can reduce resource consumption while maintaining or improving performance quality.", "method": "The paper introduces a hybrid strategy that dynamically integrates BOFT's orthogonal stability with LoRA-GA's gradient-aligned rapid convergence. It also adapts uRNN principles to transformer-based LLMs for enhanced gradient stability.", "result": "The hybrid method achieves superior convergence efficiency and generalization across diverse tasks with a reduction of up to 2.1 times in training time and 50 percent in memory usage compared to full fine-tuning while maintaining high accuracy.", "conclusion": "The hybrid method outperforms individual PEFT baselines and approaches full fine-tuning accuracy with significant reductions in training time and memory usage. It is considered a practical and scalable fine-tuning solution for LLMs deployed under resource constraints."}}
{"id": "2507.17996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17996", "abs": "https://arxiv.org/abs/2507.17996", "authors": ["Emma A. M. Stanley", "Raghav Mehta", "Mélanie Roschewitz", "Nils D. Forkert", "Ben Glocker"], "title": "Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification", "comment": "Accepted at MICCAI Workshop on Fairness of AI in Medical Imaging\n  (FAIMI) 2025", "summary": "Systematic mislabelling affecting specific subgroups (i.e., label bias) in\nmedical imaging datasets represents an understudied issue concerning the\nfairness of medical AI systems. In this work, we investigated how size and\nseparability of subgroups affected by label bias influence the learned features\nand performance of a deep learning model. Therefore, we trained deep learning\nmodels for binary tissue density classification using the EMory BrEast imaging\nDataset (EMBED), where label bias affected separable subgroups (based on\nimaging manufacturer) or non-separable \"pseudo-subgroups\". We found that\nsimulated subgroup label bias led to prominent shifts in the learned feature\nrepresentations of the models. Importantly, these shifts within the feature\nspace were dependent on both the relative size and the separability of the\nsubgroup affected by label bias. We also observed notable differences in\nsubgroup performance depending on whether a validation set with clean labels\nwas used to define the classification threshold for the model. For instance,\nwith label bias affecting the majority separable subgroup, the true positive\nrate for that subgroup fell from 0.898, when the validation set had clean\nlabels, to 0.518, when the validation set had biased labels. Our work\nrepresents a key contribution toward understanding the consequences of label\nbias on subgroup fairness in medical imaging AI.", "AI": {"tldr": "标签偏差影响医学影像中的特定子组，导致深度学习模型特征表示偏移和性能差异，强调了使用无偏标签在验证集中的重要性。", "motivation": "探讨系统标签偏差对特定子组在医学影像数据集中的影响，以研究其对医疗AI系统公正性的影响。", "method": "使用EMBED数据集训练用于二分类组织密度的深度学习模型，分析标签偏差对可分和非可分子组的影响。", "result": "本次研究探讨了标签偏差对医疗AI系统公正性的影响，特别是在医学影像数据集中，标签偏差影响特定子组的情况。研究使用EMBED数据集，分析了子组大小和可分性如何影响深度学习模型的学习特征和性能。研究结果显示，标签偏差导致模型学习特征表示显著偏移，这种偏移依赖于受影响子组的相对大小和可分性。使用无偏标签的验证集定义分类阈值时，可观察到子组性能的显著差异。例如，当标签偏差影响大多数可分子组时，该子组的真实阳性率从0.898下降到0.518。本研究为进一步理解标签偏差对医学影像AI中子组公平性的后果做出了重要贡献。", "conclusion": "标签偏差导致深度学习模型的学习特征表示偏移，偏移依赖于受影响子组的大小和可分性。使用无偏标签的验证集可以显著改善子组的分类性能。"}}
{"id": "2507.18103", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18103", "abs": "https://arxiv.org/abs/2507.18103", "authors": ["Riley Carlson", "John Bauer", "Christopher D. Manning"], "title": "A New Pair of GloVes", "comment": null, "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.", "AI": {"tldr": "本报告介绍了新的2024年版GloVe模型，它使用了更新的数据集训练，并且在文档化和模型性能上有所提升，特别是在处理当前内容相关的词汇任务上表现出色。", "motivation": "由于2014年版的GloVe模型虽然被广泛使用，但语言和世界一直在变化，新的用法可能从中受益。并且2014年的模型没有详细记录所用的数据版本和预处理步骤，本研究对此进行了补充。", "method": "本研究使用了Wikipedia、Gigaword以及Dolma的一部分数据集来训练两组词嵌入模型，并通过词汇对比、直接测试和命名实体识别（NER）任务对这些新模型进行了评估。", "result": "评估结果显示，2024年版的词向量模型包含了新的文化及语言相关的词汇，并且在结构任务如类比和相似度测试中表现良好，尤其在非西方新闻数据集的NER任务上表现优于之前的数据集。", "conclusion": "2024年版的GloVe模型不仅更好地捕捉了语言的变化，而且通过详尽的文档化，让模型的使用更加透明和可靠。"}}
{"id": "2507.17998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17998", "abs": "https://arxiv.org/abs/2507.17998", "authors": ["Jaeho Shin", "Hyeonjae Gil", "Junwoo Jang", "Maani Ghaffari", "Ayoung Kim"], "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold", "comment": null, "summary": "Affine Grassmannian has been favored for expressing proximity between lines\nand planes due to its theoretical exactness in measuring distances among\nfeatures. Despite this advantage, the existing method can only measure the\nproximity without yielding the distance as an explicit function of rigid body\ntransformation. Thus, an optimizable distance function on the manifold has\nremained underdeveloped, stifling its application in registration problems.\nThis paper is the first to explicitly derive an optimizable cost function\nbetween two Grassmannian features with respect to rigid body transformation\n($\\mathbf{R}$ and $\\mathbf{t}$). Specifically, we present a rigorous\nmathematical proof demonstrating that the bases of high-dimensional linear\nsubspaces can serve as an explicit representation of the cost. Finally, we\npropose an optimizable cost function based on the transformed bases that can be\napplied to the registration problem of any affine subspace. Compared to vector\nparameter-based approaches, our method is able to find a globally optimal\nsolution by directly minimizing the geodesic distance which is agnostic to\nrepresentation ambiguity. The resulting cost function and its extension to the\ninlier-set maximizing \\ac{BnB} solver have been demonstrated to improve the\nconvergence of existing solutions or outperform them in various computer vision\ntasks. The code is available on\nhttps://github.com/joomeok/GrassmannRegistration.", "AI": {"tldr": "本文为解决现有的Affine Grassmannian方法在刚体变换下的距离不可显式优化问题，提出了一种新的、基于高维线性子空间基的优化代价函数，并证明其在配准问题中可找到全局最优解，已在多项计算机视觉任务中验证了其优越性。", "motivation": "尽管Affine Grassmannian在表示线和平面之间的接近性方面具有理论上的精确性，但现有方法只能测量接近度而不能直接得到距离作为刚体变换的显式函数。这导致了流形上的可优化距离函数尚未得到充分开发，从而限制了在配准问题的应用。", "method": "本文提出了一种新的方法，通过严格的数学证明，用高维线性子空间的基作为代价函数的显式表示。该方法导出了两个Grassmann特征之间的可优化代价函数，该函数基于刚体变换（$\\mathbf{R}$ 和 $\\mathbf{t}$）。", "result": "通过实验验证，提出的最优代价函数及其向内点集最大化的BnB求解器扩展，已经在提高现有解决方案的收敛性或在各种计算机视觉任务中超越它们方面得到了证明。", "conclusion": "该研究克服了现有方法的局限，提出了一种新的基于转换基的优化代价函数，可以在任意仿射子空间的配准问题中应用。相比于基于向量参数的方法，新方法能够通过直接最小化测地距离找到全局最优解，且具有无表示歧义的特点。"}}
{"id": "2507.18119", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18119", "abs": "https://arxiv.org/abs/2507.18119", "authors": ["Hongjie Chen", "Zehan Li", "Yaodong Song", "Wenming Deng", "Yitong Yao", "Yuxin Zhang", "Hang Lv", "Xuechao Zhu", "Jian Kang", "Jie Lian", "Jie Li", "Chao Wang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness", "comment": null, "summary": "Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken\ninteractions. However, most existing models treat speech merely as a vehicle\nfor linguistic content, often overlooking the rich paralinguistic and speaker\ncharacteristic cues embedded in human speech, such as dialect, age, emotion,\nand non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel\nspoken language model with paralinguistic and speaker characteristic awareness,\ndesigned to extend spoken language modeling beyond text semantics. GOAT-SLM\nadopts a dual-modality head architecture that decouples linguistic modeling\nfrom acoustic realization, enabling robust language understanding while\nsupporting expressive and adaptive speech generation. To enhance model\nefficiency and versatility, we propose a modular, staged training strategy that\nprogressively aligns linguistic, paralinguistic, and speaker characteristic\ninformation using large-scale speech-text corpora. Experimental results on\nTELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM\nachieves well-balanced performance across both semantic and non-semantic tasks,\nand outperforms existing open-source models in handling emotion, dialectal\nvariation, and age-sensitive interactions. This work highlights the importance\nof modeling beyond linguistic content and advances the development of more\nnatural, adaptive, and socially aware spoken language systems.", "AI": {"tldr": "GOAT-SLM是一个具备伴随语言和说话人特征意识的新型语音语言模型，采用双模式头部架构和分阶段训练策略，以提升语言理解和语音生成的表达性与适应性。", "motivation": "现有的大部分模型仅将语音视为传递语言内容的载体，忽视了人类语音中嵌入的丰富伴随语言和说话人特征线索。为了克服这一局限性，提出了GOAT-SLM，以扩展语音语言建模的范围，超出文本语义的范畴。", "method": "GOAT-SLM采用双模式头部架构，将语言建模与声学实现解耦，从而实现稳健的语言理解和表达性、适应性的语音生成。此外，还提出了一种分阶段的训练策略，通过大规模语音-文本语料库逐渐对齐语言、伴随语言和说话人特征信息。", "result": "实验结果表明，GOAT-SLM在TELEVAL多维评估基准上实现了平衡出色的性能，涵盖了语义和非语义任务，并在处理情感、方言变异和与年龄相关的交互方面优于现有的开源模型。", "conclusion": "这项工作强调了建模超越语言内容的重要性，并推进了更自然、更具适应性和社交意识的语音语言系统的发展。"}}
{"id": "2507.18009", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18009", "abs": "https://arxiv.org/abs/2507.18009", "authors": ["Jake R. Patock", "Nicole Catherine Lewis", "Kevin McCoy", "Christina Gomez", "Canling Chen", "Lorenzo Luzi"], "title": "GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures", "comment": "12 pages, 2 figures", "summary": "State-of-the-art (SOTA) image and text generation models are multimodal\nmodels that have many similarities to large language models (LLMs). Despite\nachieving strong performances, leading foundational multimodal model\narchitectures frequently lag behind the architectural sophistication of\ncontemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner\n(CoCa) model that incorporates Gaussian error gated linear units, root mean\nsquared normalization, and rotary positional embedding into the textual\ndecoders and the vision transformer (ViT) encoder. Each architectural\nmodification has been shown to improve model performance in LLMs, but has yet\nto be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model\nwith the same modified textual decoders but with CoCa's original ViT encoder.\nWe used standard pretraining and fine-tuning workflows to benchmark the models\non contrastive and generative tasks. Our GRR-CoCa significantly outperformed\nBaseline CoCa on the pretraining dataset and three diverse fine-tuning\ndatasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in\nperplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were\n13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We\nshow that GRR-CoCa's modified architecture improves performance and\ngeneralization across vision-language domains.", "AI": {"tldr": "研究通过引入在LLMs中已证明有效的架构改进，开发了一种改进的SOTA对比式标题生成模型GRR-CoCa，并展示了其在预训练和微调任务中的显著性能提升。", "motivation": "尽管大型多模态模型在多个领域中表现良好，但其架构复杂度仍然落后于当代大型语言模型。为了应对这一挑战，该研究希望通过在多模态模型中引入LLMs中已验证有效的架构改进以进一步提高模型性能和泛化能力。", "method": "提出了一种改进的SOTA对比式标题生成模型GRR-CoCa。该模型在文本解码器和视觉Transformer (ViT) 编码器中引入了高斯误差门控线性单元、均方根归一化以及旋转位置编码，这些都是LLMs中表现优异但在CoCa中尚未采用的架构改进。", "result": "GRR-CoCa相比Baseline CoCa（具有相同的修改后的文本解码器但使用原始ViT编码器）在预训练数据集和三个不同领域的微调数据集上显著提高了性能。预训练结果的改进包括对比损失减少27.25%，困惑度降低3.71%，以及CoCa损失减少7.15%。微调结果的改进平均为对比损失降低13.66%，困惑度降低5.18%，CoCa损失减少5.55%。", "conclusion": "GRR-CoCa的修改架构不仅提高了性能，还增强了在视觉-语言领域中的泛化能力。"}}
{"id": "2507.18140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18140", "abs": "https://arxiv.org/abs/2507.18140", "authors": ["Xiaoyuan Li", "Moxin Li", "Wenjie Wang", "Rui Men", "Yichang Zhang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "title": "MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning", "comment": "Under Review", "summary": "Recent progress in Multi-modal Large Language Models (MLLMs) has enabled\nstep-by-step multi-modal mathematical reasoning by performing visual operations\nbased on the textual instructions. A promising approach uses code as an\nintermediate representation to precisely express and manipulate the images in\nthe reasoning steps. However, existing evaluations focus mainly on text-only\nreasoning outputs, leaving the MLLM's ability to perform accurate visual\noperations via code largely unexplored. This work takes a first step toward\naddressing that gap by evaluating MLLM's code-based capabilities in multi-modal\nmathematical reasoning.Specifically, our framework focuses on two key\nevaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's\nability to accurately understand and construct visualizations from scratch. (2)\nMulti-modal Code Editing (MCE) assesses the model's capacity for fine-grained\noperations, which include three types: Deletion, Modification and Annotation.\nTo evaluate the above tasks, we incorporate a dataset that covers the five most\npopular types of mathematical figures, including geometric diagrams, function\nplots, and three types of statistical charts, to provide a comprehensive and\neffective measurement of existing MLLMs. Our experimental evaluation involves\nnine mainstream MLLMs, and the results reveal that existing models still lag\nsignificantly behind human performance in performing fine-grained visual\noperations.", "AI": {"tldr": "研究提出了一种评估MLLM多模态数学推理能力的方法，专注于评估模型通过代码进行视觉操作的能力，结果表明现有模型在这方面落后于人类。", "motivation": "现有的评估主要集中在仅基于文本的推理输出上，忽视了MLLM通过代码执行精确视觉操作的能力。", "method": "使用代码作为中间表示来精确表达和操纵图像中的推理步骤。", "result": "实验结果表明，现有的模型在执行细粒度的视觉操作方面仍然远远落后于人类的表现。", "conclusion": "研究提出了一个框架，用于评估MLLM在多模态数学推理中基于代码的能力，指出了模型在执行细粒度视觉操作方面的不足。"}}
{"id": "2507.18015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18015", "abs": "https://arxiv.org/abs/2507.18015", "authors": ["Yuezun Li", "Delong Zhu", "Xinjie Cui", "Siwei Lyu"], "title": "Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics", "comment": "https://github.com/OUC-VAS/Celeb-DF-PP", "summary": "The rapid advancement of AI technologies has significantly increased the\ndiversity of DeepFake videos circulating online, posing a pressing challenge\nfor \\textit{generalizable forensics}, \\ie, detecting a wide range of unseen\nDeepFake types using a single model. Addressing this challenge requires\ndatasets that are not only large-scale but also rich in forgery diversity.\nHowever, most existing datasets, despite their scale, include only a limited\nvariety of forgery types, making them insufficient for developing generalizable\ndetection methods. Therefore, we build upon our earlier Celeb-DF dataset and\nintroduce {Celeb-DF++}, a new large-scale and challenging video DeepFake\nbenchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers\nthree commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment\n(FR), and Talking-face (TF). Each scenario contains a substantial number of\nhigh-quality forged videos, generated using a total of 22 various recent\nDeepFake methods. These methods differ in terms of architectures, generation\npipelines, and targeted facial regions, covering the most prevalent DeepFake\ncases witnessed in the wild. We also introduce evaluation protocols for\nmeasuring the generalizability of 24 recent detection methods, highlighting the\nlimitations of existing detection methods and the difficulty of our new\ndataset.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.18143", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18143", "abs": "https://arxiv.org/abs/2507.18143", "authors": ["Gonzalo Cardenal Antolin", "Jacques Fellay", "Bashkim Jaha", "Roger Kouyos", "Niko Beerenwinkel", "Diane Duroux"], "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support", "comment": null, "summary": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care.", "AI": {"tldr": "本研究通过对LLMs在HIV管理中的表现进行全面评估，提出了HIVMedQA基准测试，结果显示在特定临床情境中，某些专有模型表现更优。", "motivation": "研究目的是评估LLM在HIV管理中的当前能力，特别是在复杂性情境下的应用。考虑到HIV管理的复杂性、多种治疗选择、合并症和遵循挑战，研究希望能通过评估揭示LLM在临床应用中的优势和局限。", "method": "本研究设计了一个名为HIVMedQA的基准测试，用于评估在HIV护理中的开放式医学问题回答。研究使用了七个通用型和三个医学专业化的LLM，并通过提示工程技术来提升性能。评估框架结合了词法相似度和“LLM作为评判者”的方法，扩展了临床相关性的反映。", "result": "研究发现，Gemini 2.5 Pro在大多数维度上表现最佳。值得注意的是，排名前三位的模型中有两个是专有的。随着问题复杂性的增加，性能下降。专门调整用于医学的模型并不总是优于通用型模型，而模型规模更大的也不总是性能更好。此外，推理和理解能力相比于事实回想更具挑战性，并观察到了如最近认知偏见和现状偏见的认知偏差。", "conclusion": "研究强调了为了确保LLM的安全性和有效性整合到临床护理中，必须有目标地进行开发和评估。尽管LLM在HIV管理中有潜力成为一个有价值的支持工具，但在临床应用中仍需关注其准确性和潜在的风险。"}}
{"id": "2507.18023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18023", "abs": "https://arxiv.org/abs/2507.18023", "authors": ["Jun Zhou", "Dinghao Li", "Nannan Li", "Mingjie Wang"], "title": "High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details", "comment": null, "summary": "Recent advancements in multi-view 3D reconstruction and novel-view synthesis,\nparticularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting\n(3DGS), have greatly enhanced the fidelity and efficiency of 3D content\ncreation. However, inpainting 3D scenes remains a challenging task due to the\ninherent irregularity of 3D structures and the critical need for maintaining\nmulti-view consistency. In this work, we propose a novel 3D Gaussian inpainting\nframework that reconstructs complete 3D scenes by leveraging sparse inpainted\nviews. Our framework incorporates an automatic Mask Refinement Process and\nregion-wise Uncertainty-guided Optimization. Specifically, we refine the\ninpainting mask using a series of operations, including Gaussian scene\nfiltering and back-projection, enabling more accurate localization of occluded\nregions and realistic boundary restoration. Furthermore, our Uncertainty-guided\nFine-grained Optimization strategy, which estimates the importance of each\nregion across multi-view images during training, alleviates multi-view\ninconsistencies and enhances the fidelity of fine details in the inpainted\nresults. Comprehensive experiments conducted on diverse datasets demonstrate\nthat our approach outperforms existing state-of-the-art methods in both visual\nquality and view consistency.", "AI": {"tldr": "We propose a novel 3D Gaussian inpainting framework for reconstructing complete 3D scenes by leveraging sparse inpainted views.", "motivation": "Inpainting 3D scenes is a challenging task due to the irregularity of 3D structures and the need for multi-view consistency, which our method addresses.", "method": "Our framework incorporates an automatic Mask Refinement Process and region-wise Uncertainty-guided Optimization for inpainting 3D scenes, using sparse inpainted views and estimating the importance of each region across multi-view images.", "result": "Comprehensive experiments show that our approach outperforms existing state-of-the-art methods in both visual quality and view consistency.", "conclusion": "The proposed 3D Gaussian inpainting framework achieves superior performance in visual quality and multi-view consistency over existing methods."}}
