<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion](https://arxiv.org/abs/2506.15747)
*Fangzhou Lin,Zilin Dai,Rigved Sanku,Songlin Hou,Kazunori D Yamada,Haichong K. Zhang,Ziming Zhang*

Main category: cs.CV

> 本文探讨了单视图图像在点云补全任务中的必要性，提出了一种无需图像引导的点云补全方法，实验显示其优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 探讨在单视图图像引导的点云补全任务中，图像引导的基本必要性，提出一种无视角的基线方法来检验其效果。

**Method:** 采用基于注意力机制的多分支编码-解码网络，该网络只使用部分点云作为输入，通过层次自融合机制，跨多个流融合信息，增强特征表示，提高捕捉几何结构的能力。

**Result:** 该研究旨在探讨基于单视图图像引导下的点云补全任务，其提出的无视角基线方法在无需单视角图像输入的情况下，基于注意力机制的多分支编码-解码网络，实现了优于当前最先进的单视图图像引导点云补全方法的性能。实验验证了这种方法在ShapeNet-ViPC数据集上的有效性。

**Conclusion:** 研究发现，提出的无视角框架在单视图图像引导的点云补全任务上表现出色，可能为这一领域的多模态学习提供新的见解。

**Abstract:** The single-view image guided point cloud completion (SVIPC) task aims to
reconstruct a complete point cloud from a partial input with the help of a
single-view image. While previous works have demonstrated the effectiveness of
this multimodal approach, the fundamental necessity of image guidance remains
largely unexamined. To explore this, we propose a strong baseline approach for
SVIPC based on an attention-based multi-branch encoder-decoder network that
only takes partial point clouds as input, view-free. Our hierarchical
self-fusion mechanism, driven by cross-attention and self-attention layers,
effectively integrates information across multiple streams, enriching feature
representations and strengthening the networks ability to capture geometric
structures. Extensive experiments and ablation studies on the ShapeNet-ViPC
dataset demonstrate that our view-free framework performs superiorly to
state-of-the-art SVIPC methods. We hope our findings provide new insights into
the development of multimodal learning in SVIPC. Our demo code will be
available at https://github.com/Zhang-VISLab.

</details>


### [2] [VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service](https://arxiv.org/abs/2506.15755)
*Xiasi Wang,Tianliang Yao,Simin Chen,Runqi Wang,Lei YE,Kuofeng Gao,Yi Huang,Yuan Yao*

Main category: cs.CV

> 研究提出了一种方法VLMInferSlow，用于在实际部署中有效评估视觉语言模型的效率鲁棒性，特别关注在对抗样本影响下的模型表现。

<details>
  <summary>Details</summary>

**Motivation:** 传统的研究主要集中在提高视觉语言模型的准确性，而效率问题较少受到关注。现有评估方法在机器学习作为服务的部署方式下存在不切实际的假设，因此需要一种新的评估方法。

**Method:** 提出了一种新的方法VLMInferSlow，用于在实际的黑盒环境下评估视觉语言模型的效率鲁棒性。该方法采用了针对视觉语言推理的精细效率建模，并利用零阶优化来搜索对抗样本。

**Result:** 实验结果表明，VLMInferSlow可以生成具有不明显扰动的对抗图像，将计算成本最多提高128.47%。

**Conclusion:** 此研究旨在引起社区对视觉语言模型效率鲁棒性的关注，并提供了在实际黑盒设置下评估其效率的新途径。

**Abstract:** Vision-Language Models (VLMs) have demonstrated great potential in real-world
applications. While existing research primarily focuses on improving their
accuracy, the efficiency remains underexplored. Given the real-time demands of
many applications and the high inference overhead of VLMs, efficiency
robustness is a critical issue. However, previous studies evaluate efficiency
robustness under unrealistic assumptions, requiring access to the model
architecture and parameters -- an impractical scenario in ML-as-a-service
settings, where VLMs are deployed via inference APIs. To address this gap, we
propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness
in a realistic black-box setting. VLMInferSlow incorporates fine-grained
efficiency modeling tailored to VLM inference and leverages zero-order
optimization to search for adversarial examples. Experimental results show that
VLMInferSlow generates adversarial images with imperceptible perturbations,
increasing the computational cost by up to 128.47%. We hope this research
raises the community's awareness about the efficiency robustness of VLMs.

</details>
