{"id": "2510.21740", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21740", "abs": "https://arxiv.org/abs/2510.21740", "authors": ["Alexa R. Tartaglini", "Satchel Grant", "Daniel Wurgaft", "Christopher Potts", "Judith E. Fan"], "title": "Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models", "comment": null, "summary": "Data visualizations are vital components of many scientific articles and news\nstories. Current vision-language models (VLMs) still struggle on basic data\nvisualization understanding tasks, but the causes of failure remain unclear.\nAre VLM failures attributable to limitations in how visual information in the\ndata visualization is encoded, how information is transferred between the\nvision and language modules, or how information is processed within the\nlanguage module? We developed FUGU, a suite of data visualization understanding\ntasks, to precisely characterize potential sources of difficulty (e.g.,\nextracting the position of data points, distances between them, and other\nsummary statistics). We used FUGU to investigate three widely used VLMs. To\ndiagnose the sources of errors produced by these models, we used activation\npatching and linear probes to trace information flow through models across a\nvariety of prompting strategies. We found that some models fail to generate the\ncoordinates of individual data points correctly, and these initial errors often\nlead to erroneous final responses. When these models are provided with the\ncorrect coordinates, performance improves substantially. Moreover, even when\nthe model generates an incorrect response, the correct coordinates can be\nsuccessfully read out from the latent representations in the vision encoder,\nsuggesting that the source of these errors lies in the vision-language handoff.\nWe further found that while providing correct coordinates helps with tasks\ninvolving one or a small number of data points, it generally worsens\nperformance for tasks that require extracting statistical relationships across\nmany data points. Fine-tuning models on FUGU also fails to yield ceiling\nperformance. These findings point to architectural constraints in current VLMs\nthat might pose significant challenges for reliable data visualization\nunderstanding.", "AI": {"tldr": "研究发现，一些模型在生成单个数据点的坐标时存在问题，这些问题常常导致最终错误响应。虽然模型在生成错误响应时，可以从视觉编码器的隐藏表示中成功读取出正确的坐标，但这些错误通常源于视觉和语言模块间的交接问题。精调模型以解决统计关系的性能提升有限，表明现有的VLM架构限制了对数据可视化的可靠理解能力。", "motivation": "当前视觉语言模型在处理基础的数据可视化理解任务时表现不佳，但其失败的原因尚不明确，目的是确定失败是否来自于数据可视化中视觉信息编码的局限，视觉模块与自然语言模块间信息传输的问题，还是自然语言模块内的信息处理问题。", "method": "提出了FUGU，一套用于精确表征数据可视化理解任务潜在难度的测试套件，包括数据点的位置、间距和总结统计数据等任务。通过多种提示策略，采用激活修补技术和线性探针技术诊断现有VLM模型错误的来源。", "result": "研究发现在模型中正确地提供坐标可以显著提高性能。但是当任务需要提取大量数据点间的统计关系时，提供正确的坐标反而会降低性能。即使是通过对FUGU进行微调，模型也没有达到理想的性能表现。", "conclusion": "这些发现表明现有的视觉语言模型在处理数据可视化时存在固有的架构限制，这对可靠的数据可视化理解构成了重大挑战。"}}
{"id": "2510.21757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21757", "abs": "https://arxiv.org/abs/2510.21757", "authors": ["Mihir Gupta", "Pratik Desai", "Ross Greer"], "title": "Agro-Consensus: Semantic Self-Consistency in Vision-Language Models for Crop Disease Management in Developing Countries", "comment": null, "summary": "Agricultural disease management in developing countries such as India, Kenya,\nand Nigeria faces significant challenges due to limited access to expert plant\npathologists, unreliable internet connectivity, and cost constraints that\nhinder the deployment of large-scale AI systems. This work introduces a\ncost-effective self-consistency framework to improve vision-language model\n(VLM) reliability for agricultural image captioning. The proposed method\nemploys semantic clustering, using a lightweight (80MB) pre-trained embedding\nmodel to group multiple candidate responses. It then selects the most coherent\ncaption -- containing a diagnosis, symptoms, analysis, treatment, and\nprevention recommendations -- through a cosine similarity-based consensus. A\npractical human-in-the-loop (HITL) component is incorporated, wherein user\nconfirmation of the crop type filters erroneous generations, ensuring\nhigher-quality input for the consensus mechanism. Applied to the publicly\navailable PlantVillage dataset using a fine-tuned 3B-parameter PaliGemma model,\nour framework demonstrates improvements over standard decoding methods.\nEvaluated on 800 crop disease images with up to 21 generations per image, our\nsingle-cluster consensus method achieves a peak accuracy of 83.1% with 10\ncandidate generations, compared to the 77.5% baseline accuracy of greedy\ndecoding. The framework's effectiveness is further demonstrated when\nconsidering multiple clusters; accuracy rises to 94.0% when a correct response\nis found within any of the top four candidate clusters, outperforming the 88.5%\nachieved by a top-4 selection from the baseline.", "AI": {"tldr": "本文提出了一种成本效益高的自我一致性框架，用于改善农业图像字幕的视觉语言模型(VLM)的可靠性，特别针对发展中国家的农业疾病管理问题。", "motivation": "针对发展中国家由于缺乏植物病理专家、不可靠的互联网连接和成本限制，难以部署大规模AI系统，本文旨在提供一种经济有效的解决方案，以提高农业病害图像字幕的准确性。", "method": "该方法利用语义聚类，通过一个轻量级的预训练嵌入模型（80MB）对多个候选答案进行分组，并通过基于余弦相似性的共识机制选择最连贯的诊断报告。同时还结合了人类介入的组件，以提高输入质量。", "result": "使用3B参数的PaliGemma模型微调并在PlantVillage公开数据集上应用该框架，结果显示单簇共识方法的峰值准确率为83.1%，而采用多簇共识机制时，准确率进一步提高到94.0%。", "conclusion": "这种方法展示了在成本效益和可靠性上优于传统解码方法，对于提高发展中国家农业病害管理的水平具有重要意义。"}}
{"id": "2510.21763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21763", "abs": "https://arxiv.org/abs/2510.21763", "authors": ["Julien Boudier", "Hugo Caselles-Dupré"], "title": "Proportion and Perspective Control for Flow-Based Image Generation", "comment": "Technical report after open-source release", "summary": "While modern text-to-image diffusion models generate high-fidelity images,\nthey offer limited control over the spatial and geometric structure of the\noutput. To address this, we introduce and evaluate two ControlNets specialized\nfor artistic control: (1) a proportion ControlNet that uses bounding boxes to\ndictate the position and scale of objects, and (2) a perspective ControlNet\nthat employs vanishing lines to control the 3D geometry of the scene. We\nsupport the training of these modules with data pipelines that leverage\nvision-language models for annotation and specialized algorithms for\nconditioning image synthesis. Our experiments demonstrate that both modules\nprovide effective control but exhibit limitations with complex constraints.\nBoth models are released on HuggingFace:\nhttps://huggingface.co/obvious-research", "AI": {"tldr": "研究专注于改善文本到图像生成模型在图像的空间与几何结构控制效果，通过引入比例和透视ControlNets模块实现更精确的图像生成控制。", "motivation": "现代文本到图像的扩散模型虽然生成了高保真度的图像，但是对生成图像的空间和几何结构的控制有限。", "method": "引入并评估了两种专门用于艺术控制的ControlNets：（1）比例ControlNet，它使用边界框来控制对象的位置和大小；（2）透视ControlNet，它利用消失线来控制场景的三维几何结构。", "result": "实验表明，这两种模块提供了有效的控制手段，但对复杂条件的处理能力有限。", "conclusion": "两种模块都提供了有效控制，但在处理复杂约束方面表现出局限性。"}}
{"id": "2510.21769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.21769", "abs": "https://arxiv.org/abs/2510.21769", "authors": ["Harry Zhang", "Luca Carlone"], "title": "H2OFlow: Grounding Human-Object Affordances with 3D Generative Models and Dense Diffused Flows", "comment": null, "summary": "Understanding how humans interact with the surrounding environment, and\nspecifically reasoning about object interactions and affordances, is a critical\nchallenge in computer vision, robotics, and AI. Current approaches often depend\non labor-intensive, hand-labeled datasets capturing real-world or simulated\nhuman-object interaction (HOI) tasks, which are costly and time-consuming to\nproduce. Furthermore, most existing methods for 3D affordance understanding are\nlimited to contact-based analysis, neglecting other essential aspects of\nhuman-object interactions, such as orientation (\\eg, humans might have a\npreferential orientation with respect certain objects, such as a TV) and\nspatial occupancy (\\eg, humans are more likely to occupy certain regions around\nan object, like the front of a microwave rather than its back). To address\nthese limitations, we introduce \\emph{H2OFlow}, a novel framework that\ncomprehensively learns 3D HOI affordances -- encompassing contact, orientation,\nand spatial occupancy -- using only synthetic data generated from 3D generative\nmodels. H2OFlow employs a dense 3D-flow-based representation, learned through a\ndense diffusion process operating on point clouds. This learned flow enables\nthe discovery of rich 3D affordances without the need for human annotations.\nThrough extensive quantitative and qualitative evaluations, we demonstrate that\nH2OFlow generalizes effectively to real-world objects and surpasses prior\nmethods that rely on manual annotations or mesh-based representations in\nmodeling 3D affordance.", "AI": {"tldr": "H2OFlow通过合成数据学习全面的3D人-物互动属性，包括接触、方向和空间占用。这种方法避免了手工标注数据的需求，并且在真实物体上的表现优于现有的方法。", "motivation": "当前的方法依赖于耗时且成本高昂的手工标记数据集来捕捉真实世界或模拟的人-物互动任务，而且大多数现有的3D人-物互动理解方法仅限于基于接触的分析，忽略了一些重要方面，例如人体对物体的偏好方位和空间占用。", "method": "H2OFlow, 一种新的框架，该框架使用仅由3D生成模型生成的合成数据全面学习3D人-物互动属性，包括接触、方向和空间占用。H2OFlow使用一种基于密集3D流的表示方法，通过在点云上操作的密集扩散过程学习此流。", "result": "通过广泛的定量和定性评估，展示了H2OFlow在学习3D人-物互动属性方面不仅有效泛化到现实世界的物体，而且在建模方面超越了依赖于手动注释或网格表示的方法。", "conclusion": "该研究提出了一种新的3D人-物互动理解框架H2OFlow，它通过使用仅由3D生成模型生成的合成数据，解决了现有方法对数据的过度依赖及在互动属性上的局限性，并展示了其在真实物体上的有效性和优越性。"}}
{"id": "2510.21762", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.21762", "abs": "https://arxiv.org/abs/2510.21762", "authors": ["Eric Jeangirard"], "title": "A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications", "comment": null, "summary": "We present a dataset of 833k paragraphs extracted from CC-BY licensed\nscientific publications, classified into four categories: acknowledgments, data\nmentions, software/code mentions, and clinical trial mentions. The paragraphs\nare primarily in English and French, with additional European languages\nrepresented. Each paragraph is annotated with language identification (using\nfastText) and scientific domain (from OpenAlex). This dataset, derived from the\nFrench Open Science Monitor corpus and processed using GROBID, enables training\nof text classification models and development of named entity recognition\nsystems for scientific literature mining. The dataset is publicly available on\nHuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.", "AI": {"tldr": "基于833k段落构建了一个分类科学文献数据集，并公开发布于HuggingFace，有利于文本分类和命名实体识别系统的开发。", "motivation": "此数据集的目的是提供一种资源，以促进有关科学文献的数据分类和命名实体识别的研究。", "method": "我们通过从CC-BY许可的科学出版物中提取833K段落，并使用fastText进行语言识别以及使用OpenAlex进行科学领域标注，构建了一个数据集。该数据集主要用于训练文本分类模型和开发针对科学文献的命名实体识别系统。", "result": "构建了一个包含833k段落的多语言、多领域的科学文献数据集，为后续研究提供了基础资源。", "conclusion": "该数据集公开发布在HuggingFace上，数据集中段落的分类和标注有利于后续的研究工作，特别是在文本分类和命名实体识别方面。"}}
