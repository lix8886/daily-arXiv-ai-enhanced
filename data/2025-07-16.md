<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 36]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

> 本文介绍了一种基于AI的系统来对抗误导性信息，该系统可以检查YouTube视频中的声明并在评论区生成有说服力的评论。实验结果证实了该系统的事实核查能力和改善在线环境的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决当今数字世界中迅速蔓延的错误信息问题。通过开发一种AI驱动的系统来检查YouTube视频中的声明并在评论区与用户互动，作者希望挑战误导性的叙述，从而帮助创建更加知情的在线环境。

**Method:** 本文提出了一种基于AI的系统，该系统不仅可以检查YouTube视频中的陈述的真实性，还可以通过与用户在评论区互动来挑战误导性的叙述。该系统由两个主要组件组成：Truth Sleuth和Trend Bender。Truth Sleuth使用检索增强生成(RAG)方法评估视频中的陈述，并生成详细的报告。Trend Bender根据这些报告生成有见地和有说服力的评论，以促进有意义的讨论。

**Result:** 实验结果显示，该系统的事实核查代理具有高准确性，并展示了AI驱动干预在对抗错误信息方面的潜力，同时也证实了它在促进更知情的在线空间方面的潜力。

**Conclusion:** 研究表明，AI系统在事实核查和与用户互动方面有很高的潜力，这对于创建更加知情和有包容性的在线社群是非常重要的。

**Abstract:** Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [2] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

> EmoSApp是一个完全离线的智能手机心理支持应用，利用大型语言模型为用户提供心理健康支持，克服了在线心理健康平台面临的问题。

<details>
  <summary>Details</summary>

**Motivation:** 由于心理健康对于个体的整体福祉至关重要，且数字平台面临用户访问性、互联网连接和数据隐私等方面的挑战，因此迫切需要开发一种离线、基于智能手机的心理健康和情感支持解决方案。

**Method:** 本文提出了一种名为EmoSApp的离线智能手机应用，利用大型语言模型(LLM)，特别针对资源有限的设备进行了微调、量化和部署。通过在包含14,582个心理健康问答对的定制数据集上进行微调，EmoSApp能够提供连贯、富有同情心、互动性强且相关信息的对话支持。

**Result:** 定性的人类评估显示EmoSApp能连贯、富有同情心地对话，并能提供相关建议；定量评估则表明在资源有限的环境中，经过微调和量化的模型依然具有高效性。

**Conclusion:** 通过定性和定量评估证明了EmoSApp在提供心理健康支持方面的能力。这表明，EmoSApp作为一种携带便捷、安全且高度定制的AI驱动心理健康解决方案，为未来的创新提供了蓝图。

**Abstract:** Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [3] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

> 本文提出了一种工具链，旨在通过自动化的标准化、摘要和匿名化处理，使得在保护隐私的前提下进行无结构文本数据的大规模分析成为可能，为先前由于隐私和异质性约束而无法访问的文本数据研究打开新途径。

<details>
  <summary>Details</summary>

**Motivation:** 无结构文本（如法律、医疗和行政来源）为公共卫生和社会科学研究提供了丰富但未充分利用的资源，但大规模分析受个人敏感信息和结构语言显著异质性的阻碍，本研究旨在解决这些问题。

**Method:** 我们提出了一种模块化的工具链，用于准备基于嵌入分析的无结构文本数据。该工具链依赖完全开放权重的模型运行于本地硬件，只需工作站级别的GPU，并支持隐私敏感性的研究。该工具链运用大型语言模型（LLM）提示来标准化、摘要化文本，并在需要时将其翻译成英文以提高可比性。通过LLM的红行动作、命名实体识别和基于规则的方法来实现匿名化，减少暴露风险。

**Result:** 通过手动检查、自动化扫描和预测评估验证证明，工具链有效地删除了识别信息，同时保留了语义内容。我们使用从少量手动标签摘要中得出的嵌入向量训练预测模型，证明了工具链在大规模半自动化内容分析中的能力。

**Conclusion:** 通过启用结构化、隐私意识分析的敏感文档，我们的工具链为以前因隐私和异质性约束而无法访问文本数据的领域的大规模研究打开了新的可能性。

**Abstract:** Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [4] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

> 本文提出了一种针对基于提示的自然语言解释（NLE）的更新的XAI分类法，旨在提供一个框架，以便研究者、审核员和政策制定者能够对透明的AI系统进行表征、设计和增强NLEs。

<details>
  <summary>Details</summary>

**Motivation:** 有效的AI治理需要结构化的方法让利益相关者可以访问和验证AI系统的行为。随着大型语言模型的兴起，自然语言解释(NLEs)现在成为表达模型行为的关键，这需要对NLEs的特征和治理影响进行专门研究。

**Method:** 我们基于可解释AI(XAI)文献，创建了一个更新的XAI分类法，该分类法专门针对基于提示的自然语言解释(NLE)，涵盖了三个维度：(1)上下文，包括任务、数据、受众和目标；(2)生成与呈现，涵盖生成方法、输入、交互性、输出和形式；以及(3)评估，重点是内容、呈现和以用户为中心的特性，以及评估的设置。

**Result:** 创建了一个专门针对基于提示的自然语言解释(NLEs)的XAI分类法，该分类法提供了透明AI系统的表征、设计和增强的框架。

**Conclusion:** 这个分类法提供了一个框架，帮助研究人员、审计员和政策制定者对透明的AI系统进行表征、设计和增强NLEs。

**Abstract:** Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [5] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

> 提出AutoRAG-LoRA框架，通过综合利用多种技术手段减少大语言模型的幻觉现象，提高模型在实际应用中的可信度。

<details>
  <summary>Details</summary>

**Motivation:** 解决大语言模型在生成过程中出现的事实不准确问题，这会降低模型在实际部署中的信任度。

**Method:** AutoRAG-LoRA框架综合利用轻量级LoRA适配器、KL正则化训练以及自动化提示重写、混合检索和低秩适配器微调等技术，将生成的回答与检索到的证据相结合，从而减少大语言模型的幻觉现象。

**Result:** AutoRAG-LoRA显著减少了模型的幻觉现象，同时保持了模型的效率和模块化特性。

**Conclusion:** 该方法通过引入轻量级适配器和KL正则化训练等技术，实现在不过度影响生成效率的前提下减少大语言模型的幻觉。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [6] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

> 本文讨论了在自然语言处理中，通过语言手段表达语言模型的不确定性，提出了拟人化的不确定性方法，以增强人类对机器的信任。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型的输出常常在准确性受到质疑的情况下表达得很自信，这影响了用户对这些模型的信任。因此，有必要通过语言手段表达语言模型的信心，并将这种信心传达给用户，以从人机协作中获益，并减少潜在的危害。

**Method:** 本文通过综述人类不确定性交流的研究，调查了正在进行的研究，并进行了进一步的分析，展示了迄今为止被忽视的口头化不确定性偏差。

**Result:** 研究表明在人类不确定性交流中存在数据偏差，机器的不确定性交流受到这些偏差的影响。

**Conclusion:** 文章总结了人类与机器之间不确定性交流的独特因素，并将拟人化不确定性分解为NLP未来研究方向。

**Abstract:** Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [7] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

> PLEX是一种基于从LLM提取的上下文嵌入和「Siamese网络」风格的神经网络的新方法，旨在提供更高效且计算成本更低的局部解释。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）在文本分类任务中表现出色，但其复杂性阻碍了模型的可解释性。传统解释性AI方法如LIME和SHAP虽然提供局部解释，但依赖于计算成本高昂的扰动操作。为了缓解这一问题，提出了一个新的方法。

**Method:** PLEX方法利用从LLM提取的上下文嵌入和类似「Siamese网络」的神经网络来对特征重要性评分进行对齐，从而避免了在每次解释时需要进行扰动的步骤。这种方法仅需一次性的训练过程即可对任意新的句子提供高效的解释。

**Result:** PLEX在四个分类任务（情感分析、假新闻检测、虚假新冠疫情新闻检测和抑郁分析）上得到了超过92%的与LIME和SHAP的一致性。此外，通过「压力测试」，PLEX证明了它准确识别重要词的能力，并且在某些情况下，比LIME和SHAP表现更优，减少了两到四个数量级的解释时间和计算开销。

**Conclusion:** PLEX为LLM驱动的可解释文本分类提供了一种有效和高效的解决方案，显著降低了计算时间和计算成本，同时保持了高精度的数据解释。

**Abstract:** Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [8] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

> 研究发现大型语言模型在情绪识别上存在系统偏差，尤其对一些交叉身份的群体辨识有较大误差，并强调了需以认知模型优化模型评价。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型越来越多地被用于会话代理，理解这些模型是如何建模用户的情绪状态变得至关重要，这关系到伦理部署的问题。同时，研究还希望能够深入了解模型内部情绪推理机制及其相关偏差，以及探索认知理论在改进模型评估中的潜在应用。

**Method:** 该研究受情绪轮模型（一种心理学框架）的启发，分析了模型输出中情绪状态之间的概率依赖关系。通过比较模型输出和真实人类的数据，探讨了LLMs如何形成分层情绪树并与人类心理模型相比较。此外，还进行了人类研究，以揭示LLMs内部化社会感知的方面及其误分类的特征。

**Result:** 研究揭示了大型语言模型(LLMs)如何自然形成与人类心理模型相吻合的分层情绪树，并且较大的模型会发展出更复杂的层次结构。此外，研究还发现模型在识别不同社会经济身份的情绪时存在系统性偏差，特别是对于交叉和代表性不足群体的误分类现象更为严重。这些发现不仅展示了LLMs中情绪推理的出现，还暗示了可以利用认知理论来改进模型评估的潜力。

**Conclusion:** 大型语言模型能够自然地模拟分层情绪树，反映出类似人类的情感组织方式，模型越大，其情绪层次结构越复杂。但是，模型在识别不同社会经济背景用户情绪时存在系统性偏差，尤其是在识别交叉身份群体时，这种偏差更为明显，这提示我们应进一步优化模型以减少偏见，并且可以借助认知理论来改进模型评估。

**Abstract:** As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [9] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

> 本文通过研究语言模型方法改进成人服务网站（ASWs）文本分析，展示了定制变压器模型在准确率、召回率、F1 分数和 ROC AUC 方面优于其他模型。

<details>
  <summary>Details</summary>

**Motivation:** 本文动机在于解决成人服务网站（ASWs）文本数据中的挑战，提高识别潜在性 trafficking 受害者的能力。

**Method:** 本文研究了成人服务网站（ASWs）文本数据分析的语言模型方法，包括简单信息检索方法、预训练变压器模型和定制变压器模型。

**Result:** 本文结果显示定制变压器模型在关键任务中优于其他预训练模型，如图中巨大组件分解、广告文本聚类和非法环境中表情符号使用的理解。

**Conclusion:** 本文的发展代表了成人服务网站（ASWs）文本分析的显著进步，该模型可在多种下游应用和研究中发挥作用。

**Abstract:** Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [10] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

> 本研究探索了预训练的文本嵌入模型在含丰富文本属性的标记属性图中的应用，提升了图分析的准确性和可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于，通过有效利用属性图中的文本属性，可以增强分析任务的效果。

**Method:** 本研究采用预训练的文本嵌入模型来增强具有丰富文本属性的标记属性图的语义分析。通过对节点和边的文本属性进行嵌入处理，支持后续节点分类和关系预测任务，从而提高上下文理解能力。

**Result:** 方法将语言模型嵌入集成到图数据处理管道中，而不改变图的结构，从而显著提升属性图分析的准确性和可解释性。

**Conclusion:** 结论表明，文本语义可以在不改变图结构的情况下，显著增强属性图分析的准确性与可解释性。

**Abstract:** Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [11] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

> 研究引入了MISS-QA基准测试来评估模型解释科学文献图表的能力，并评估了18个多模态模型，结果显示模型与人类专家之间存在显著差距。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是评估和改进现有模型在解释科学文献图表中的表现，特别是在多模态理解和信息查询方面的能力。

**Method:** 该研究使用了专门设计的MISS-QA基准测试，其中包括大量标注的例子和多种多模态模型的性能评估。

**Result:** 该研究引入了MISS-QA，这是一个专门用于评估模型解释科学文献中图表能力的基准。MISS-QA包含1,500个经过专家标注的例子，涵盖了465篇科学论文。在这个基准测试中，模型的任务是解释展示研究概览的图表并回答相关的信息查询问题。研究评估了18个前沿的多模态基础模型，包括o4-mini、Gemini-2.5-Flash和Qwen2.5-VL。结果显示这些模型与人类专家在MISS-QA上的表现存在显著差距。对无法回答的问题和详细的错误分析进一步突显了当前模型的优势和局限性，为提高模型理解多模态科学文献的能力提供了关键见解。

**Conclusion:** 研究发现现有的多模态模型在解释科学文献中的图表方面仍然存在显著的性能差距，人类专家的表现明显优于模型。研究还揭示了模型在处理特定类型任务时的优势和局限性。

**Abstract:** This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [12] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

> The study tested the link between social approval and online hate by examining posts from Parler and found mixed results, indicating different dynamics on niche platforms.

<details>
  <summary>Details</summary>

**Motivation:** To investigate how receiving social approval influences the creation and escalation of online hate messages based on Walther's (2024) social approval theory.

**Method:** We analyzed over 110 million posts from Parler (2018-2021) to test the relationship between social approval and online hate speech.

**Result:** The number of upvotes on hate speech posts did not predict an increase in hate speech in future posts. The relationship between social approval and hate speech was mixed over different time intervals.

**Conclusion:** Social approval reinforcement mechanisms of online hate may function differently on niche social media platforms like Parler compared to the predictions of Walther's (2024) social approval theory.

**Abstract:** In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [13] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

> 研究构建了一套评估LLM公平性的框架和数据集JudiFair，揭示了LLM在司法判断中的公平性问题，公布了支持未来研究的工具包。

<details>
  <summary>Details</summary>

**Motivation:** LLM在影响权利和公平的高风险领域应用越来越广泛，但是其司法公平性及对社会正义的影响尚未得到充分研究。本文旨在解决这个问题。

**Method:** 基于司法公平理论，构建了一个全面的框架来衡量LLM的公平性，选择了65个标签及161个对应值，针对司法系统收集了包含177,100个独特案件事实的数据集JudiFair，并开发了3个评估指标：不一致度、偏见和不平衡不准确性。

**Result:** 实验揭示了LLM在司法判断中的普遍不一致、偏见和不平衡不准确性，且在对人口统计标签上的偏见尤其明显，而相较于程序标签，物质标签上的偏见稍小。温度参数的调整对LLM的公平性有影响，但模型大小、发布时间、国家起源等对司法公平性没有显著影响。

**Conclusion:** 提出了一个全面的评估框架和数据集来衡量和改善LLM的司法公平性，并指出温度参数调整可能改善其公平性，同时也强调了未来需要更多研究来进一步优化LLM。

**Abstract:** Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [14] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

> 研究发现用户偏好的对话风格主观相似性与第三方评定的客观相似性存在差异，强调了在分析风格相似性与用户偏好时区分这两种评估的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 探讨风格相似性（主观与客观）与用户偏好的关系，以及它们在人机对话中的作用。

**Method:** 构建了一个包含用户偏好、用户自我感知的主观风格相似性和第三方评定的客观风格相似性的数据集，进行了相关分析。

**Result:** 分析结果发现了用户对主观风格相似性的偏好与第三方评估的客观相似性存在差异。

**Conclusion:** 这一发现突显了在分析风格相似性与用户偏好时区分主观和客观评估的重要性。

**Abstract:** Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [15] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

> 为了解决低资源语言韩语中的语义模糊问题，本文提出了一种名为HanjaBridge的新技术，通过在持续预训练框架中提供所有可能的汉字候选来增强模型的上下文消歧能力。实验表明，这种技术显著提高了韩语模型的理解能力和跨语言迁移效果。

<details>
  <summary>Details</summary>

**Motivation:** 解决大型语言模型在像韩语这样的低资源语言中表现不佳的问题，尤其是由于音同形异的韩汉词汇在韩文书写系统中无法区分而造成的语义模糊问题。

**Method:** 提出HanjaBridge，这是一种新的意义注入技术，集成在持续预训练(CPT)框架中。HanjaBridge不是将一个词确定地映射到单一的汉字，而是为给定的多义词展示所有可能的汉字候选，鼓励模型学习上下文消歧。此过程与基于标记的知识蒸馏相结合，以防止灾难性 forgetting.

**Result:** 实验结果显示，HanjaBridge显著提高了韩语理解能力，在KoBALT基准测试中实现了21%的相对改进。通过加强韩语和汉语之间通过共享汉字的语义对齐，发现有强烈的跨语言积极迁移。这些改进在推理时即使忽略汉字增强也得以持续，确保了实际应用中的高效性，且没有任何额外的运行时间成本。

**Conclusion:** 通过集成HanjaBridge技术，可以有效解决低资源语言韩语中的语义模糊问题，提高韩语模型的理解和跨语言迁移能力。

**Abstract:** Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [16] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

> 研究通过故事为基础的类比映射任务，评估大型语言模型的类比推理能力，特别是模型大小和架构对性能的影响。结果表明，LLMs在类比推理方面具有一定的人类推理模型潜力。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在探索大型语言模型与人类性能在检测和映射类比方面的对齐程度。先前的研究表明，尽管LLMs能够从类比问题中提取相似性，但它们缺乏稳健的人类样化推理能力。

**Method:** 该研究采用了故事为基础的类比映射任务，通过句子嵌入评估大型语言模型对类比源文本和目标文本相似性的捕捉能力，以及它们对源文本和干扰文本之间差异性的理解。此外，还探讨了通过显式提示改进LLMs解释类比能力的效果，并评估了模型大小(8B vs. 70B参数)和不同先进模型架构（如GPT-4和LLaMA3）之间的表现差异。

**Result:** 通过对句子嵌入的评估，该研究发现LLMs在捕捉类比中的相似性和捕捉源文本与干扰文本之间的差异方面具有一定的能力。此外，研究评估了大型语言模型在不同模型大小和架构条件下的表现差异。

**Conclusion:** 该研究推进了我们对大型语言模型类比推理能力的理解，并展示了它们作为人类推理模型的潜在能力。尽管存在一些限制，但LLMs在类比推理方面展示了与人类类似的性能特征。

**Abstract:** Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [17] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

> 本文总结了DS@GT团队在eRisk 2025挑战中的参与情况，特别是在对话抑郁症的LLMs方面取得了较好的成绩。

<details>
  <summary>Details</summary>

**Motivation:** 由于没有可用的真实标签，我们评估了跨模型的一致性和内部一致性。

**Method:** 采用了prompt工程策略，不同的大语言模型（LLMs）进行了BDI-II基于的评估并生成了结构化的JSON输出。

**Result:** 我们在官方排行榜上获得了第二名，其指标为DCHR = 0.50，ADODL = 0.89，ASHR = 0.27。

**Conclusion:** 我们的prompt设计方法使模型输出与BDI-II标准保持一致，并能够分析影响症状预测的对话线索。

**Abstract:** This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [18] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

> 提出TEAch Me Sign（TEAM-Sign）通过微调LLM学习文本与手语间的对应关系，采用逐步提示策略进行手语知识的提取与生成，实验显示可以有效处理手语和口语间的规则差异。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在许多AI任务上带来了革命性的变化，但它们对手语生成的影响仍然有限，主要原因在于手语的复杂性和独特规则。

**Method:** 通过微调大型语言模型（LLM），将其视为另一种自然语言，从而实现文本与手语间的对应关系学习与生成。考虑到手语和口语之间的差异，采用逐步提示策略提取LLM中的内在手语知识，以支持学习和生成过程。

**Result:** 在How2Sign和Phoenix14T数据集上的实验结果显示，我们的方法能有效利用LLM中的手语知识和推理能力，以协调手语与口语间不同的分布和语法规则。

**Conclusion:** 我们的方法在手语生成任务上展现出了显著效果，特别是在涉及手语与口语之间差异的情况下。

**Abstract:** Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [19] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

> The paper describes a hierarchical Low-Rank Adaptation (LoRA) method for efficient sexism detection in English and Spanish tweets, achieving strong performance with minimal trainable parameters.

<details>
  <summary>Details</summary>

**Motivation:** To develop an efficient and scalable approach for sexism detection in tweets across multiple languages, aiming to improve upon traditional methods that involve complex data processing and ensembles.

**Method:** Our method introduces conditional adapter routing for hierarchical task adaptation using LoRA, targeting all linear transformations of Llama 3.1 8B, and applying unified multilingual training for English and Spanish sexism detection.

**Result:** Achieved competitive performance with 1.67% trainable parameters, reducing training time by 75% and model storage by 98%, with F1 improvements through cross-lingual transfer.

**Conclusion:** The hierarchical LoRA approach provides an effective and efficient method for sexism detection in tweets, demonstrating that simple parameter-efficient fine-tuning can be effective without complex data preprocessing or ensembles.

**Abstract:** This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [20] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

> 论文介绍了一个名为HerO 2的系统，该系统在计算约束下通过文档摘要、答案重构和量化优化提升了事实验证性能，效率高且效果好。

<details>
  <summary>Details</summary>

**Motivation:** 为了提升去年挑战中最好的开源模型HerO的性能，研究团队开发了HerO 2，旨在改善证据质量、优化真实性预测以及提升系统效率。

**Method:** HerO 2系统通过文档摘要和答案重构提升证据质量，通过量化优化模型以提高在计算约束下的真实性预测性能，并通过集成更新的语言模型来提高整体系统性能。

**Result:** HerO 2在排行榜上位列第二，并且在前三个系统中的运行时间最短，展示了在现实世界的事实验证中的高效率和强大潜力。

**Conclusion:** HerO 2展示了高效性和在实际应用中的潜力，证明了通过文档摘要、量化优化和集成更新的语言模型可以提升事实验证系统的性能。

**Abstract:** This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [21] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

> 研究提出了JoA-ICL框架，用于处理长篇新闻文章的立场检测问题，并展示了其在促进新闻推荐观点多样性和揭示媒体偏见方面的应用价值。

<details>
  <summary>Details</summary>

**Motivation:** 随着在线新闻消费的增长，个性化推荐系统已成为数字新闻报道不可或缺的一部分。然而，这些系统存在加剧过滤气泡和政治极化的风险，因为它们未能纳入多元视角。为了缓解这些问题，该研究通过引入K-News-Stance数据集来填补现有立场检测研究的空白，这个数据集包含了针对2000篇新闻文章的立场标注。

**Method:** 本研究提出了一个名为JoA-ICL的框架，该框架利用语言模型代理来预测文章中关键结构部分（例如引言、引语）的观点，并将其聚合以推断整篇文章的观点。该框架用于处理长形式新闻文章中的立场检测问题。

**Result:** 实验表明，JoA-ICL在立场检测方面优于现有的方法，强调了分段代理在捕获长篇新闻文章整体立场上的优势。实验结果还通过两个案例研究进一步证明其在促进新闻推荐中的观点多样性及揭示媒体偏见方面的广泛应用价值。

**Conclusion:** JoA-ICL在立场检测方面的优越性能及其在促进观点多样性和揭示媒体偏见方面的应用，表明该框架在长篇新闻文章中捕获整体立场的有效性。

**Abstract:** As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [22] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [23] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

> 该研究提出了一种基于变压器的混合情感分析框架，针对孟加拉语社会媒体评论进行情感分析，采用了包括XMB-BERT在内的多种方法，结合投票分类器实现了83.7%的高精度。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是在社会媒体上放大公众情绪和塑造话语的历史性群众运动中，特别是在孟加拉国七月革命期间和之后，分析公共意见。

**Method:** 该研究提出了一种基于变压器的混合情感分析框架，使用包括BanglaBERT、mBERT、XLM-RoBERTa以及提出的混合XMB-BERT在内的先进变压器特征抽取技术来捕获文本数据中的细微模式。同时采用了主成分分析（PCA）以提高计算效率，探讨了11种经典和先进的机器学习分类器用于情感识别。

**Result:** 提出的混合XMB-BERT结合投票分类器取得了83.7%的高精度，优于其他模型。

**Conclusion:** 这项研究表明了机器学习技术（特别是提出的混合XMB-BERT模型）分析低资源语言（如孟加拉语）中的社会情绪的潜力。

**Abstract:** The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [24] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

> The paper explores the use of Large Language Models (LLMs) to enhance entity-matching accuracy in cross-border financial activities, demonstrating improved performance over traditional methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the accuracy and reduce false positives in entity-matching tasks, which are critical for risk management, regulatory compliance, and fraud prevention in the Spanish financial system.

**Method:** Content involves comparing traditional entity-matching algorithms (Jaccard, cosine, Levenshtein) with Large Language Models (LLMs) for improving the accuracy of entity-matching tasks in the context of cross-border financial activities.

**Result:** The study finds that traditional methods have over 92% accuracy but high false positive rates (20-40%). Interface-based LLMs perform the best with accuracies above 93%, F1 scores over 96%, and false positive rates of 40-80%.

**Conclusion:** The conclusion is that interface-based LLMs significantly outperform traditional methods in entity-matching tasks, offering a more accurate and reliable solution for the financial sector, particularly for foreign entity identification and classification.

**Abstract:** The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [25] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

> DIJA is a framework that exploits dLLMs safety weaknesses by crafting adversarial prompts, demonstrating significant vulnerabilities that current alignment methods cannot adequately address.

<details>
  <summary>Details</summary>

**Motivation:** To highlight the safety concerns associated with diffusion-based large language models (dLLMs) when dealing with context-aware adversarial prompts, leading to the development of a new systematic attack framework.

**Method:** DIJA constructs specific adversarial prompts using interleaved mask-text inputs that capitalize on bidirectional modeling and parallel decoding mechanisms of dLLMs, thereby bypassing standard safety alignments.

**Result:** DIJA achieved high attack success rates, notably surpassing previous methods in both evaluator-based and keyword-based assessment measures.

**Conclusion:** The study indicates a critical need for re-examining the safety mechanisms in dLLMs, as current methods are ineffective against the developed adversarial attack framework.

**Abstract:** Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [26] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

> The paper introduces a framework to study data poisoning attacks in Large Language Models (LLMs) with multiple backdoor triggers that can coexist without interference. It also offers a post hoc recovery method that targets specific model components for retraining to defend against such attacks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to understand and mitigate the vulnerability of LLMs to data poisoning attacks, particularly the coexistence and interaction of multiple backdoor triggers within the same model.

**Method:** The method involves creating a framework to study multiple distinct backdoor triggers within LLMs and demonstrating their ability to coexist without mutual interference. A post hoc recovery method selectively retrains model components based on layer-wise weight differences to remove trigger behavior.

**Result:** The results show that multiple triggers can coexist in a model and exhibit robust activation even under token substitution and separation. The proposed recovery method successfully eliminates trigger behavior with minimal parameter updates.

**Conclusion:** The paper concludes by highlighting the vulnerability of LLMs to multi-trigger poisoning attacks and proposes an effective and practical defense mechanism to mitigate the threat.

**Abstract:** Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [27] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

> 研究团队提出了一种基于集成系统的多语言多模态推理系统，用于处理ImageCLEF 2025 EXAMS V挑战中的问题。系统集成了多个Gemini模型，通过精心设计的提示策略在多种语言任务中表现出色，尤其在多语言轨道中取得了81.4%的准确率，显示了轻量级OCR-VLM集成和精确提示策略的优势。

<details>
  <summary>Details</summary>

**Motivation:** 此研究针对的是在多语言情况下进行有效且精确推理的挑战，尤其是涉及到教育设置中的高风险任务。多语言推理要求系统能够理解多模态输入并给出准确的回答。

**Method:** 该系统采用Gemini 2.5 Flash提供描述，Gemini 1.5 Pro进行改进和一致性检查，以及Gemini 2.5 Pro作为推理器，指导最终答案的选择。此外，通过精心设计的提示策略优化了模型性能。

**Result:** 系统在官方排行榜中以81.4%的准确率获得了多语言赛道的第一名，并且在13种语言中领先了11个。具体表现如克罗地亚语的95.07%和意大利语的92.12%。

**Conclusion:** 研究得出轻量级OCR-VLM集成结合精确提示策略和跨语言数据增强不仅可以克服多语言推理中的挑战，而且能够在复杂的教育环境中与更重的端到端模型竞争中胜出。

**Abstract:** We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [28] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

> 本文提出WikiMem数据集及一种新的度量标准，用于评估LLM中人类事实的关联性，这为实现大规模语言模型的个别数据遗忘请求提供了潜在方案。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决大型语言模型中个人资料的识别及记忆问题，尤其是针对欧盟GDPR中的被遗忘权。现有的机器遗忘方法假设需要遗忘的数据是已知的，并不能识别模型中存储的个人事实关联，而传统的隐私审计技术也存在局限性。

**Method:** 我们引入了WikiMem数据集，该数据集包含超过5,000个自然语言的防护措施，并涵盖了来自Wikidata的243个人类相关的属性。我们还提出了一种模型无关的度量标准，用于量化LLM中的人类事实关联。通过使用校准的负对数似然来对不同事实的真值进行排序以应对各种不同措辞的问题。

**Result:** 我们在15种不同参数规模的LLM中对200个个体进行了评估，发现记忆与个体在网上的存在以及模型规模相关。

**Conclusion:** 这一方法可为识别LLM中个人内存数据提供基础，从而为机器遗忘和被遗忘权请求提供了动态构造遗忘集的可能性。

**Abstract:** Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [29] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

> 研究结果表明，温度显著地影响了所有六种大语言模型中共识的达成和达成的时间长短。存在多个角色的多代理系统比统一角色的多代理系统在四分之六个模型中更延迟达成共识。只有在一个拥有70亿参数的大语言模型（OpenHermesV2:7B）和某一种编码类别中，多代理系统的商议在特定条件下能带来高于机会水平的改善。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索多代理系统（MAS）对编码工作流的模拟，以及其相对于单代理编码的好处。目前对于后者的优势还不甚明了，因此这项研究旨在填补这一知识空白。

**Method:** 本研究通过实验研究了代理角色和温度如何影响基于对话片段的共识构建和编码准确性，这些对话片段基于一个包含8个编码的代码簿。研究使用了六个开源的大语言模型（参数数量从30亿到320亿不等）和18种实验配置，分析了超过77,000个编码决策。

**Result:** 结果表明，温度显著地影响了共识的达成和时间长短。多角色的MAS在四个大语言模型中比单角色MAS更延迟达成共识，但在三个模型中，较高温度减弱了角色多样性对共识效果的影响。MAS在大多数情况下未能在编码准确性上有显著改善，与单代理系统持平或低于单代理系统。

**Conclusion:** 研究结果挑战了多样化的MAS角色能带来更好结果的观念。即便如此，对于特定配置的MAS合作可能有助于缩小编码应用中的模糊性，从而改进代码簿和人机编码质量。整体而言，该研究提供了关于基于大语言模型的定性方法的局限性的新见解。

**Abstract:** Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [30] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

> 该论文引入了EsBBQ和CaBBQ，即西班牙语和加泰罗尼亚语偏见基准，用于评估未经英语和美国社会环境以外的语言模型的社会偏见。

<details>
  <summary>Details</summary>

**Motivation:** 目前关于语言模型社会偏见的资源在英语之外的语言和美国之外的社会环境里匮乏。

**Method:** 基于原始BBQ（偏见基准），设计适用于西班牙语和加泰罗尼亚语，评估跨10个社会类别偏见的平行数据集。

**Result:** 结果显示，较大的语言模型在具有社会偏见的模棱两可的情境中倾向于表现不佳，且高QA准确率经常与更大程度上依赖社会偏见相关。

**Conclusion:** 这项工作说明了评估非英语语言模型中社会偏见的重要性和方法。

**Abstract:** Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [31] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

> 本研究提出了FlowFSM，利用大型语言模型和代理输出链接技术从协议的原始文档中准确提取FSMs，解决了现有技术在FSM提取中的问题。

<details>
  <summary>Details</summary>

**Motivation:** 论文指出现有的FSM抽取技术存在可扩展性差、覆盖率低、自然语言规范模糊等问题，因此提出了一种新的方法来解决这些问题。这种方法旨在改善网络协议的状态机建模，使其在验证、分析和发现漏洞方面更加准确和有效。

**Method:** 本论文提出了一种新的框架FlowFSM，该框架利用大型语言模型（LLMs）、提示链和链式思维推理技术，从原始RFC文档中准确提取有限状态机（FSMs）。FlowFSM系统地处理协议规范，识别状态转换，并通过链接代理输出来构建结构化的规约书。

**Result:** 实验评估表明，FlowFSM在FTP和RTSP协议上实现了高精度的FSM提取，减少了幻觉状态转移，显示了其在协议分析中的潜力。

**Conclusion:** 该研究证实了基于代理的LLM系统在协议分析和FSM推断中的应用潜力，特别是在网络安全和逆向工程领域。

**Abstract:** Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [32] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

> 研究提出了一种基于稀疏自编码器的SAE-LAPE方法，用于识别大语言模型中的语言特异性特征，这些特征在模型中到末层出现，并可用于语言识别，性能类似fastText但更易解释。

<details>
  <summary>Details</summary>

**Motivation:** 理解大语言模型中的多语言机制有助于洞察它们如何处理不同的语言，但现有的研究由于神经元的多义性难以将语言特异单元与跨语言表征区分开。

**Method:** 采用稀疏自编码器（SAEs）学习多语种大语言模型（LLMs）中具有一致性的具体和抽象概念特征。特别是引入了基于特征激活概率的SAE-LAPE方法，在前馈网络中识别语言特异性特征。

**Result:** 发现了大量语言特异性特征主要出现在模型的中到末层，并且这些特征可以用于语言识别，其性能与fastText相近，但更具可解释性。

**Conclusion:** 通过使用SAE-LAPE方法，可以识别并利用大语言模型中的语言特异性特征，这些特征对于模型的多语言性能和输出具有重要影响。

**Abstract:** Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [33] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

> 研究提出KV-Latent范式，通过降采样Key-Value向量维度到潜在空间，改进了Rotary Positional Embedding的频率采样机制，减少KV缓存足迹，提高推理速度，实验结果证明了该方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基于Transformer Decoders的语言模型在对话生成AI中占主导地位，但在推理过程中，Key-Value (KV)缓存的逐渐增加成为主要的效率瓶颈，尤其是在内存消耗和数据传输带宽方面。为了解决这些问题，提出了本研究。

**Method:** 本研究提出了一种称为KV-Latent的新范式，通过将Key-Value向量维度降采样到潜在空间，显著减少KV缓存的足迹，并提高推理速度。同时改进了在低维向量上应用的Rotary Positional Embedding的频率采样机制，以增强其稳定性。

**Result:** 实验结果，包括具有Grouped Query Attention的模型和没有它的模型，都取得了令人满意的结果。此外，还进行了对比实验，研究单独减少Key和Value组件对模型性能的影响。

**Conclusion:** 这种方法允许构建更有效的语言模型系统，开启了在KV缓存节省和高效大规模语言模型的新可能性。

**Abstract:** Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

> CWNet 提出了一种新的架构，结合因果推理和小波变换，以改善传统的低光图像增强方法，显著提升了图像增强效果。

<details>
  <summary>Details</summary>

**Motivation:** 传统的低光图像增强方法主要集中在亮度的均匀调整，忽视了实例级别的语义信息和不同特征的内在特性。

**Method:** CWNet 采用了基于因果推理和小波变换的网络架构，包含两个关键组件：因果推理方法和基于小波变换的骨干网络。

**Result:** 实验结果显示，CWNet 在多个数据集上显著优于目前最先进的方法，表现出强大的跨场景性能表现。

**Conclusion:** 通过全球视角的因果嵌入学习和局部语义损失，CWNet 有效保留了因果因素一致性，并实现针对具体小波变换属性的精准增强。

**Abstract:** Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [35] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

> 提出了一种新的框架，整合外部生物知识来改善显微镜成像模型处理从头细胞系时的表现，实验表明该方法在药物发现中有效。

<details>
  <summary>Details</summary>

**Motivation:** 高通量筛选技术对药物发现和生物医学研究至关重要，但是针对从头细胞系的鲁棒性扰动筛选由于不同细胞系之间存在显著的形态和生物学异质性，仍然是一个挑战。

**Method:** 我们提出了一种新框架，该框架将外部生物知识与现有预训练策略结合，以增强显微镜成像模型的配置文件化效果。我们的方法明确分离了扰动特定和细胞系特定表示，利用STRING和Hetionet数据库中的蛋白质相互作用数据构建知识图谱，以指导模型在预训练期间聚焦于扰动特定特征。此外，我们整合了来自单细胞基础模型的转录组特征，以捕捉细胞系特定表示。

**Result:** 我们在RxRx数据库上进行了一次性的微调（针对RxRx1细胞系）和几次微调（针对RxRx19a数据集中的细胞系），实验结果表明，我们的方法提高了显微镜成像模型对从头细胞系的泛化能力，展示了其在基于表型的药物发现应用中的有效性。

**Conclusion:** 我们的方法提高了显微镜成像模型对从头细胞系的适应能力，对于基于表型的药物发现应用具有重要意义。

**Abstract:** High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [36] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

> The study audits two FER datasets, revealing issues with posed images in supposed in-the-wild datasets, and racial biases in model predictions towards darker skin tones.

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the performance and ethical implications of facial expression recognition models and datasets by auditing the source of their data and testing their biases.

**Method:** Random samples from two FER datasets are examined to determine if images are spontaneous or posed, and models are tested on their ability to predict emotions for various races and skin tones.

**Result:** A significant number of images in the datasets are posed, and the models show bias towards predicting negative emotions for individuals with darker skin tones.

**Conclusion:** The performance of FER models varies greatly between spontaneous and posed images, and these models have racial biases perpetuating harm if applied in the wild.

**Abstract:** Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [37] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

> A new interest point matching technique that eliminates the need for descriptors, reducing memory usage.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to reduce memory usage in localization systems by completely eliminating the need for descriptors, despite the marginal decrease in matching accuracy compared to conventional methods.

**Method:** The paper introduces a technique for the extraction and matching of interest points that inherently associates points during detection, eliminating the need for descriptors.

**Result:** The method was assessed by comparing it against both classical handcrafted and modern learned approaches.

**Conclusion:** This approach leads to a drastic reduction in memory usage, offering a viable alternative to traditional methods.

**Abstract:** The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [38] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [39] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

> Developed a system that enhances spatial understanding using a combination of LLMs and specialized tools, demonstrating superior performance in warehouse scenario tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to present an efficient method to improve the spatial understanding ability of Multi-modal Large Language Models in challenging environments without relying on extensive finetuning.

**Method:** We propose a LLM agent system integrating multiple tools for spatial reasoning and API interaction, aiming to enhance spatial understanding in complex indoor warehouse scenarios.

**Result:** Extensive evaluations on the AI City Challenge dataset show that our system achieves high accuracy and efficiency in tasks involving object retrieval, counting, and distance estimation.

**Conclusion:** The proposed system successfully addresses complex spatial reasoning tasks within a warehouse setting, providing a robust solution that surpasses the spatial capabilities of existing Multi-modal Large Language Models.

**Abstract:** Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [40] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

> ThinkingViT is a nested Vision Transformer architecture that utilizes a Token Recycling mechanism to dynamically adjust computation based on the complexity of the input, improving efficiency and accuracy on ImageNet-1K.

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficiency and scalability of Vision Transformers across various hardware types by dynamically adjusting the amount of computation according to the difficulty of the input, thus avoiding unnecessary computational expenditures.

**Method:** Proposes a nested ViT architecture (ThinkingViT) with progressive thinking stages and a Token Recycling mechanism. It starts with a limited number of attention heads, expands for complex inputs, and terminates when prediction certainty is high.

**Result:** On ImageNet-1K, ThinkingViT demonstrates a 2.0 percentage point increase in accuracy compared to nested baselines at the same throughput and up to 2.9 percentage points at equal GMACs.

**Conclusion:** ThinkingViT presents a backbone-preserving upgrade to vanilla Vision Transformers, significantly boosting efficiency and accuracy by intelligently allocating computational resources based on input complexity.

**Abstract:** Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [41] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

> We propose an LLM-guided framework for open-world object detection that enhances autonomy and adaptability, outperforming traditional methods by generating names for unknown objects without retraining.

<details>
  <summary>Details</summary>

**Motivation:** Object detection traditionally relies on fixed category sets, requiring costly re-training to handle novel objects. While Open-World and Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting autonomy.

**Method:** We propose an LLM-guided agentic object detection (LAOD) framework that enables fully label-free, zero-shot detection by prompting a Large Language Model (LLM) to generate scene-specific object names.

**Result:** Experiments on LVIS, COCO, and COCO-OOD validate our approach, showing strong performance in detecting and naming novel objects.

**Conclusion:** Our method offers enhanced autonomy and adaptability for open-world understanding.

**Abstract:** Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [42] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

> 本文提出了一种新颖的、可调的Grad-CAM扩展方法——Winsor-CAM，它生成健壮且连贯的显著性图，并在多个标准指标和模型上验证了其在可解释性和定位性能上的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 解释卷积神经网络（CNNs）的决策过程对于在高风险领域部署模型至关重要，而广泛使用的Grad-CAM方法通常关注于最终卷积层或简单地跨层平均，这可能会遮蔽重要的语义线索或放大无关的噪声。

**Method:** 通过应用Winsorization（一种基于百分位数的异常值衰减技术），Winsor-CAM方法聚合了所有卷积层的信息，生成健壮且连贯的显著性图，以减少噪声或极端属性值的影响。用户可控制的阈值允许进行语义级别的调整，可以在表征层次结构中灵活地探索模型行为。

**Result:** 在使用PASCAL VOC 2012数据集评估中，Winsor-CAM与Grad-CAM和均匀层平均基准相比，在定位指标（如交并比和质心对齐）上表现更好，产生了更具解释性的热图。

**Conclusion:** Winsor-CAM通过提供具有人类参与控制的可解释、多层次洞察，推进了可信AI的目标。

**Abstract:** Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [43] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

> A sparse coding inspired fine-tuning framework is introduced to improve interpretability and adaptability of pretrained models for various tasks, showing better performance than existing methods.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of understanding model adaptation in fine-tuning by using sparse representation, enhancing interpretability.

**Method:** Sparse coding inspired fine-tuning framework, where features are represented as a sparse combination of dictionary atoms for better interpretability and adaptation.

**Result:** Improved performance in image editing with text alignment and text-to-image concept customization.

**Conclusion:** The sparse coding method offers a better way to understand how pretrained models adapt to new tasks and performs well in image editing and concept customization tasks, outperforming baseline methods.

**Abstract:** Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [44] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

> 研究提出了一种结合局部离群因子(LOF)算法和YOLO-v11n深度学习模型的轻量级结肠息肉检测框架，并在五个公开数据集上进行了测试，结果显示该方法在息肉检测任务上表现出高精度和效率。

<details>
  <summary>Details</summary>

**Motivation:** 及时和准确地检测结肠息肉对于诊断和预防结肠癌至关重要，而结肠癌是全球主要死亡原因之一。因此，研究旨在提出一种轻量级且高效的息肉检测框架，以支持临床结肠镜检查。

**Method:** 研究在五个不同的公开数据集：CVC-ColonDB、CVC-ClinicDB、Kvasir-SEG、ETIS和EndoScene上测试了一种融合了局部离群因子(LOF)算法和YOLO-v11n深度学习模型的方法。为了增强模型的鲁棒性和泛化能力，研究采用了5折交叉验证法和LOF方法来移除异常样本，将分割描边转换成适用于检测的目标标签，并使用了多种现代图像增广策略进行训练。

**Result:** 该方法显著提高了息肉的定位性能，达到了95.83%的精确率、91.85%的查全率、93.48%的F1分数、0.5时的MAP值为96.48%，以及0.5:0.95区间内的mAP值为77.75%。相比于先前基于YOLO的方法，该模型展示了更高的检测准确性和效率。

**Conclusion:** 研究结果表明，所提出的框架在临床场景中的结肠镜检查实时支持方面具有巨大的应用潜力。此外，该研究强调了数据预处理和模型效率在设计有效的医学影像AI系统中的重要性。

**Abstract:** Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [45] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

> 本文介绍了一种改进的中心线追踪模型Trexplorer Super，它可以更准确地追踪复杂结构并展示了优于现有模型的表现。同时也强调了在评估算法性能时真实数据的重要性。

<details>
  <summary>Details</summary>

**Motivation:** Tubular tree结构，如血管和气道，是人体解剖学中的重要部分。准确追踪这些结构并保持其拓扑结构对于很多下游任务至关重要。现有的Trexplorer算法在预测重复分支和过早终止的问题上效果不佳。

**Method:** 本文提出了Trexplorer Super，这是Trexplorer算法的改进版，主要用于改善中心线追踪过程中预测重复分支和过早终止的问题，通过一些创新进步来提高性能。

**Result:** 通过使用开发的三个中心线数据集（一个合成数据集，两个真实数据集）对现有的SOTA模型进行全面评估，结果表明Trexplorer Super在所有数据集上都超过了先前的SOTA模型。此外，研究还发现，在合成数据上表现出色并不一定意味着在真实数据上也能表现出色。

**Conclusion:** 研究证明了Trexplorer Super在追踪人体内部的重要结构时能够提供比现有SOTA模型更好的追踪效果，同时也揭示了在合成数据上表现出色的模型不一定能在真实数据集上得到同样好的性能。

**Abstract:** Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [46] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

> 本文介绍了一种基于CNN的新型全球天气预报模型KAI-a，该模型在保持高准确性的前提下大幅降低了计算成本，适合用于捕捉极端天气事件。

<details>
  <summary>Details</summary>

**Motivation:** 由于许多现有的天气预报模型基于Transformer架构，导致训练复杂度和资源需求高，因此本文旨在提出一种能够在保证预测精度的同时显著减少计算要求的CNN模型。

**Method:** 本文提出了一种基于现代CNN的全球天气预报模型，该模型在其设计中融入了尺度不变架构和基于InceptionNeXt的模块，并考虑了地球系统数据的特性。该模型在一个NVIDIA L40s GPU上仅需12小时即可完成训练，参数量约为7百万。

**Result:** 实验表明，KAI-a在中短期天气预报中达到了现有最优模型的性能，同时具有轻量级设计。此外，KAI-a在2018年欧洲热浪和东亚夏季风的案例研究中表现出色，能够精准捕捉极端天气事件。

**Conclusion:** KAI-a证明了在资源高效利用的情况下，依然可以获得高质量的天气预报性能，特别适用于极端天气事件的捕捉，展示了其实际应用的潜力。

**Abstract:** Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [47] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

> 本文针对基于EEG的情感识别训练中神经网络模型遇到的时间尺度依赖性标签不一致问题，提出了两种新策略：LVL和LGCL，通过图论框架和经典数学原理，取得了优于现有方法的实验结果。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决神经网络模型在基于EEG的人类情感识别训练中的一个常常被忽视的问题：时间尺度依赖性标签不一致（TsDLI）。

**Method:** 两种新的正则化策略——局部变化损失（LVL）和局部-全局一致性损失（LGCL）被提出，以缓解由于时间尺度依赖性标签不一致（TsDLI）引起的问题。这两种方法都结合了有界变函数和通勤时间距离的经典数学原理，在图论框架内使用。

**Result:** 通过在广泛使用的EEG情感数据集DREAMER和DEAP上进行全面的实验，验证了所提出方法的有效性。实验结果表明，所提出的方法在多种神经架构中均优于现有最先进的基线方法，实现了更好的总体性能。

**Conclusion:** 所提出的方法提供了一种在标签不一致性背景下，在可解释性和预测能力之间达成原则性权衡的方法。

**Abstract:** In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [48] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

> GeoDistill是一种几何引导的弱监督自蒸馏框架，通过FoV-based masking实现跨视角定位，提高定位精度并减少不确定性，适用于全景图和有限视角图像。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法通常依赖于需要昂贵的地面真实姿态注释的全监督学习。GeoDistill旨在解决这一问题，通过自蒸馏框架来提高跨视角定位的性能。

**Method:** GeoDistill框架通过教师-学生学习的方式，并利用Field-of-View (FoV)-based masking方法来增强局部特征学习，以实现稳健的跨视角定位。教师模型定位全景图像，而学生模型预测通过FoV-based masking生成的有限视角图像的位置。

**Result:** 实验结果表明，GeoDistill在不同的框架中显著提高了定位性能。同时，还引入了一种新的方向估计网络，可以在无需精确平面位置真实值情况下预测相对方向。

**Conclusion:** GeoDistill提供了一种可扩展且高效的解决实际跨视角定位挑战的方法。

**Abstract:** Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [49] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

> A method called GAPL-SCD is developed to enhance semantic change detection in remote sensing data by optimizing multiple tasks and using graph aggregation to improve category-level domain alignment, achieving state-of-the-art performance in accuracy and robustness.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the challenge of negative transfer in multi-task learning for semantic change detection in remote sensing data, providing a more robust and accurate method for detecting and categorizing changes in a detailed manner.

**Method:** Graph Aggregation Prototype Learning for Semantic Change Detection (GAPL-SCD) is proposed to address the issue of negative transfer in multi-task learning for SCD. It involves optimizing semantic segmentation and change detection alongside a graph aggregation prototype learning module, which constructs an interaction graph and uses prototypes for category-level domain alignment.

**Result:** Experimental results on the SECOND and Landsat-SCD datasets show state-of-the-art performance, with significant improvements in accuracy and robustness for the SCD task.

**Conclusion:** The proposed method, GAPL-SCD, improves multi-task learning capabilities in semantic change detection by effectively combining semantic segmentation with graph aggregation prototype learning, leading to better performance in complex scenes.

**Abstract:** Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [50] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

> RIDFR is a novel ID-specific face restoration framework using diffusion models that improves identity fidelity and robustness in face restoration.

<details>
  <summary>Details</summary>

**Motivation:** To address the uncertainty of face identity in face restoration due to identity-obscure inputs and stochastic generative processes.

**Method:** Uses a pre-trained diffusion model with two parallel conditioning modules for content and identity injection, and Alignment Learning to suppress ID-irrelevant face semantics.

**Result:** Experiments show high-quality ID-specific face restoration with high identity fidelity and strong robustness, outperforming state-of-the-art methods.

**Conclusion:** RIDFR demonstrates significant improvements in face restoration, providing a robust solution for preserving specific identities in severely degraded images.

**Abstract:** The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [51] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

> 本研究创建了WomenSports数据集，并提出了一种利用通道注意力机制的CNN模型来改进女性体育动作分类。在该数据集上，达到了89.15%的分类准确率。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服现有研究中缺乏足够表示女性体育动作且包含足够类间和类内变异性图像数据集的限制，本研究旨在开发新的WomenSports数据集来实现基于小样本训练数据的女性体育动作分类。

**Method:** 本研究提出了一种用于深度特征提取的卷积神经网络(CNN)，并通过局部上下文区域的通道注意力机制来优化和增强特征表示。

**Result:** 实验在三个不同的体育数据集和一个舞蹈数据集上进行，结果表明该算法具有良好的泛化能力。使用ResNet-50在提出的WomenSports数据集上达到了89.15%的Top-1分类准确率。

**Conclusion:** 提出的WomenSports数据集和基于CNN的通道注意力方案显示出在女性体育动作分类方面的有效性和先进性。

**Abstract:** Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [52] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

> The paper presents an efficient HOI detection method using a wavelet attention backbone and ray-based encoder, achieving better performance on standard datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the efficiency and accuracy of HOI detection by addressing the limitations of existing methods that are inefficient and resource-intensive.

**Method:** The paper introduces a wavelet attention-like backbone and a ray-based encoder architecture for HOI detection. The wavelet backbone aggregates discriminative features from interactions of different orders, while the ray-based encoder optimizes attention on relevant regions to reduce computational load.

**Result:** The proposed architecture demonstrates promising results on benchmark datasets like ImageNet and HICO-DET.

**Conclusion:** The wavelet attention and ray-based encoder show potential in enhancing HOI detection performance while reducing computational costs.

**Abstract:** Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [53] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [54] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

> SpaRTAN, a novel, efficient CNN architecture, outperforms existing models by effectively integrating varying receptive fields and wave-based channel aggregation, reducing redundancy and enhancing spatial and channel-wise information.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the simplicity bias and information redundancy issues found in modern CNNs and transformer architectures, while improving their efficiency and performance.

**Method:** The method involves a lightweight architecture named SpaRTAN, which employs kernels with varying receptive fields and a wave-based channel aggregation module to enhance information processing and reduce redundancy.

**Result:** On the ImageNet-1k benchmark, SpaRTAN achieved 77.7% accuracy with 3.8M parameters and 1.0 GFLOPs. On the COCO benchmark, it achieved 50.0% AP, outperforming previous models with fewer parameters.

**Conclusion:** The study concludes that SPAARTN efficiently gathers and contextualizes discriminative features, achieving remarkable performance with fewer parameters compared to existing architectures, demonstrating the potential of its design approach.

**Abstract:** The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [55] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

> FiSeCLIP enhances CLIP's capability for ZSAD by utilizing batch image referencing and text-based noise filtering, showing better performance in anomaly detection tasks compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind FiSeCLIP is to develop a more effective zero-shot anomaly detection mechanism that leverages the strengths of CLIP while addressing its limitations in handling rare classes and noisy data, making it applicable to industrial needs.

**Method:** The method FiSeCLIP integrates CLIP's feature matching with cross-modal alignment, using other images in the same batch as reference points to address the challenges in zero-shot anomaly detection (ZSAD). To refine filters and enhance accuracy, it employs text information to filter out noise and recovers CLIP's local semantic correlation.

**Result:** FiSeCLIP outperforms the state-of-the-art (SOTA) AdaCLIP on the MVTec-AD benchmark, demonstrating superior performance improvements of +4.6% and +5.7% in segmentation metrics (AU-ROC/$F_1$-max).

**Conclusion:** The paper concludes that FiSeCLIP represents a significant advancement in zero-shot anomaly detection, providing a strong baseline for future research and applications.

**Abstract:** With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [56] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

> 该研究提出了一种名为SISRNet的新方法，通过识别具有医学关键特征的显著区域，用于生成准确的胸部X光报告，从而解决现有方法中由于数据偏差导致的误报问题。在标准数据集上表现优于其它方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有自动放射学报告生成方法存在因放射学图像中的大量数据偏差导致的医疗准确性问题，提出新方法来解决这一挑战。

**Method:** Structure

**Result:** {"tldr": "该研究提出了一种名为SISRNet的新方法，通过识别具有医学关键特征的显著区域，用于生成准确的胸部X光报告，从而解决现有方法中由于数据偏差导致的误报问题。在标准数据集上表现优于其它方法。", "motivation": "现有自动放射学报告生成方法存在因放射学图像中的大量数据偏差导致的医疗准确性问题，提出新方法来解决这一挑战。", "method": "SISRNet通过细粒度的跨模态语义，明确识别出具有医学关键特征的显著区域，并在这两个阶段中专注于这些高信息量的区域。", "result": "相比其他方法，SISRNet在常用的IU-Xray和MIMIC-CXR数据集上展现出更优越的性能。", "conclusion": "SISRNet通过聚焦于高信息量区域，有效捕捉细微异常发现，生成具有临床准确性的报告，解决数据偏差问题。"}

**Conclusion:** SISRNet通过聚焦于高信息量区域，有效捕捉细微异常发现，生成具有临床准确性的报告，解决数据偏差问题。

**Abstract:** Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [57] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

> A novel CBCT-to-MDCT translation framework based on the Schrodinger Bridge formulation outperforms existing methods, providing superior anatomical fidelity and fine control.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to improve the quality and controllability of medical image translation from CBCT to MDCT, ensuring both anatomical fidelity and perceptual controllability.

**Method:** We present a novel framework for CBCT-to-MDCT translation using the Schrodinger Bridge formulation that incorporates GAN-derived priors and human-guided conditional diffusion. This method enforces boundary consistency between CBCT inputs and pseudo targets and uses binary human feedback via classifier-free guidance.

**Result:** The proposed method effectively selectively attenuates shade artifacts in key anatomical regions while preserving fine structural detail. It also outperforms prior GAN- and fine-tuning-based feedback methods on RMSE, SSIM, LPIPS, and Dice metrics.

**Conclusion:** The paper concludes that the proposed framework demonstrates superior performance for real-time, preference-aligned medical image translation, with only 10 sampling steps required.

**Abstract:** We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [58] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

> 本研究解决了OVSS无法准确识别个性化用户文本描述的问题，提出了个性化OVSS方法，通过负掩码提议和视觉嵌入增强了方法的性能，在新基准测试中表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 开放词汇语义分割（OVSS）虽能分割基于任意文本描述的图像，但难以识别个性化文本（例如，用户特定的“我的马克杯”），尤其是当面对多个类似对象时。该研究动机在于解决这类个性化识别的挑战。

**Method:** 本研究提出了一种基于文本提示调优的插件方法，旨在通过少量图像和掩码对识别个性化视觉概念，同时保持原有开放词汇语义分割（OVSS）的性能。该方法通过引入“负掩码提议”来减少错误预测，并通过将个性化概念的视觉嵌入注入到文本提示中来丰富其表示，从而提高了个性化OVSS的性能。

**Result:** 实验表明，该方法在建立的新基准测试（包括FSS$^{	ext{per}}$、CUB$^{	ext{per}}$和ADE$^{	ext{per}}$）中表现出色。

**Conclusion:** 研究结论是，通过引入“负掩码提议”和注入视觉嵌入到文本提示中，能够有效提升个性化OVSS的性能，同时保持传统OVSS的功能。实验验证了方法的有效性。

**Abstract:** While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [59] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

> DGFDNet is proposed to improve single-image dehazing by integrating spatial and frequency domain strategies, achieving strong performance with high efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to develop a method for single-image dehazing that can effectively model long-range dependencies, handle complex haze conditions, and achieve real-time applicability while maintaining high performance.

**Method:** Transformer-based models are criticized for their high computational cost in single-image dehazing. DGFDNet addresses this by introducing a dual-domain approach, using a Haze-Aware Frequency Modulator (HAFM) to generate a haze confidence map and adjust frequency components. It also includes a Multi-level Gating Aggregation Module (MGAM) for fusing features, and a Prior Correction Guidance Branch (PCGB) for iterative improvement of haze localization.

**Result:** Extensive experiments on four benchmark datasets show that DGFDNet outperforms existing methods, achieving superior robustness and real-time efficiency.

**Conclusion:** DGFDNet demonstrates state-of-the-art performance on benchmark haze datasets, with superior robustness and real-time efficiency, effectively overcoming limitations of spatial and frequency domain approaches.

**Abstract:** Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [60] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

> 本文介绍了一个新的多视角高分辨率三维足-踝点云数据集FootGait3D，用于在自然步态情况下评估三维点云建模方法，并为生物力学和机器人应用提供重要工具。

<details>
  <summary>Details</summary>

**Motivation:** 足踝复合体在步态中的运动学分析对于推进生物力学研究和临床评估至关重要，但采集动态步态下足踝区域的准确表面几何数据极具挑战性。此数据集旨在解决这一问题，提供细粒度的足踝运动数据。

**Method:** 本文介绍了FootGait3D，一个专注于足踝区域动态步态下的高分辨率三维点云数据集。数据集由8,403帧点云画面构成，采集自46名受试者，使用了自定义的五相机深度感知系统。每帧包括完整的五个视角重建画面（作为真实值）以及四种部分视角的点云。

**Result:** FootGait3D提供了一个有结构的变异，可以评估在不同遮挡水平和视角下的三维点云完成方法。数据集适用于形状补全任务，可以用于现有方法的基准测试。

**Conclusion:** 这项研究的成果包含了一个新颖的数据集，该数据集有望推动足踝复合体步态的生物机械研究，为步态分析、假肢设计和需要精细三维模型的机器人应用建立了一个宝贵的测试平台。

**Abstract:** The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [61] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

> GLOD是一个用于高分辨率卫星图像的基于Transformer的对象检测模型，相比现有的方法有显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于提高高分辨率卫星图像中物体检测的准确性和效率。

**Method:** GLOD采用基于Transformer的架构，用Swin Transformer取代传统的CNN主干网，结合新型的UpConvMixer模块进行稳健的上采样，并使用Fusion Blocks进行多尺度特征整合。

**Result:** 实验结果表明，在xView数据集上，GLOD达到了32.95%的性能，相较于现有最优方法提高了11.46%。

**Conclusion:** 该架构通过引入非对称融合与CBAM注意力机制及多路径头设计，特别是在捕捉不同尺度的物体方面具有创新性，且在维护计算效率的同时利用了空间先验知识。

**Abstract:** We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [62] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

> 本论文提出了ProLearn框架，该框架通过原型驱动语义近似(PSA)模块减轻了对文本报告的依赖性，显示出了在有限文本条件下超过现有语言引导分割方法的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有文献引导的分割方法依赖于图像-文本配对输入，存在两个限制：一是很多医疗分割数据集缺乏配对报告，导致大量图像数据未被充分利用；二是推断限制在有配对报告的病例的回顾性分析中，限制了其在大多数临床场景中的可适用性。为此，本论文提出了新型原型驱动学习框架。

**Method:** Medical language-guided segmentation方法通过结合文本临床报告作为辅助指导来提高图像分割性能。本研究提出了一种名为ProLearn的新型原型驱动学习框架，该框架能够减轻对文本的依赖。核心组件是原型驱动语义近似(PSA)模块，该模块通过从文本报告中提取与分割相关的语义，来初始化离散紧凑的原型空间，为没有文本输入的图像提供语义指导。

**Result:** 通过在QaTa-COV19，MosMedData+和Kvasir-SEG数据集上的广泛实验表明，ProLearn在有限文本可用时优于现有的语言引导方法。

**Conclusion:** ProLearn框架通过引入PSA模块，显著降低了对带有文本报告图像的依赖性，促进了医疗图像分割技术的进步，使其更加适应实际临床环境。

**Abstract:** Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [63] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

> 该论文提出了RoMaP，一种用于3D高斯表示局部精确编辑的框架，通过引入3D面具生成模块和正则化SDS损失来解决现有方法的局限性，从而达成更精准和丰富的局部编辑。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决现有3D神经表示和实例级编辑模型在实现精准局部3D编辑中遇到的挑战，尤其是高斯模型在局部编辑上的局限性。

**Method:** 提出RoMaP框架，包括使用球谐系数预测3D部分分割的3D-Geometry Aware Label Prediction（3D-GALP）模块和结合额外正则化的SDS损失。

**Result:** 实验表明，RoMaP在重建和生成的高斯场景与对象上的局部3D编辑中达到最先进水平，定性和定量实验结果均证明了这一点。

**Conclusion:** RoMaP框架允许用户进行更加稳健和灵活的3D高斯模型的部分级编辑。

**Abstract:** Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [64] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

> 本文提出了一种新颖的基于关节角度的建模方法来改进人体姿态估计，使用该方法进行训练的数据集使算法在识别错误修正与时空轨迹平滑方面表现出色。实验证明在像花样滑冰这样的复杂场景下，此种方法优于现有的最优姿态估计算法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的人体姿态估计算法往往会因关键点识别偶尔出错以及关键点轨迹随机波动而受到限制。尤其现有深度学习模型的训练数据集中关键点手动标注的不准确性也极大限制了其性能。

**Method:** 该论文提出了一种基于关节角度建模的新方法。关键技术包括：(i) 一种用于描述运动中人体姿态的关节角度模型；(ii) 通过高阶傅里叶级数拟合关节角度的时间变化以获得可靠的“真实值”；(iii) 设计双向循环网络作为后处理模块来优化已有的HRNet的估算结果。

**Result:** 利用本方法构建的高质量数据集训练，该网络在修正错误识别的关键点和平滑其时空轨迹方面表现出色。测试表明，在像花样滑冰这样的具有挑战性的情况下，关节角度优化(JAR)优于最先进的HPE优化网络。

**Conclusion:** 该研究提出了一种新的基于关节角度的方法来改进人体姿态估计算法，并通过实验验证了其在困难场景下的有效性。

**Abstract:** Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [65] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

> 本文提出了一种新的、基于图的、用于单目估计非合作航天器姿态的方法，即GKNet，该方法使用了空间几何约束，并提供了一个航天器关键点检测的中等规模数据集SKD来评估其性能。

<details>
  <summary>Details</summary>

**Motivation:** 非合作航天器的单目姿态估计对于在轨服务（如卫星维护、空间碎片清除和站组装）任务至关重要。现有的关键点检测器在面对非合作航天器的结构对称性和部分遮挡时依然脆弱。我们的目标是提高这类姿态估计的准确性。

**Method:** 我们的方法是一种基于图的、用于非合作航天器单目姿态估计算法，称为GKNet，它利用了关键点图的几何约束。为了更好地验证关键点检测器，我们还提出了一种适度规模的航天器关键点检测数据集，名为SKD，包括3个航天器目标、90,000张模拟图像及其对应的精确关键点注释。

**Result:** 广泛的实验和消融研究表明，相较于最先进的航天器关键点检测器，我们的GKNet在准确性和有效性上表现优异。

**Conclusion:** 研究表明，GKNet和SKD数据集提供了一个有效的解决方案，可用于提高非合作航天器的单目姿态估计的准确性。代码和数据集已公开。

**Abstract:** Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [66] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

> 本文提出一种新的针对地面穿透雷达（GPR）图像中RSD识别的交叉验证策略，该策略基于高质量3D GPR数据集和深度学习技术，在实际应用中大幅减少检测工作量并提高准确性。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有RSD识别劳动强度大、对专家依赖度高以及深度学习中缺少高质量训练数据集和网络对RSD区分能力不足的问题。

**Method:** 本研究构建了一个严格验证的3D GPR数据集，包含2134个多样本，并提出了一种基于YOLO模型的交叉验证策略，该策略在特定类型道路路面下层病害（RSD）识别上具有不同敏感度，从而提高了识别准确性。

**Result:** 通过实施这种识别策略，在实地测试中实现了超过98.6%的召回率。

**Conclusion:** 该方法集成到一个在线RSD检测系统中，可将检查工作量减少90%左右。

**Abstract:** Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [67] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

> 本文提出了一个新的3D大气恢复方法FourCastX，它基于频率增强的空间-时间专家混合网络，解决了现有方法依赖辅助输入和简化物理近似的不足，建立了Atmos-Bench作为3D大气基准。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法依赖于辅助输入和简化的基于物理的近似，缺乏标准化的3D基准进行公平评估。然而，这些方法可能会引入额外的不确定性，并且无法充分捕捉真实的辐射传输和大气散射吸收效应。

**Method:** 提出了Atmos-Bench：第一个3D大气基准，以及一种新的FourCastX：频率增强空间-时间专家混合网络，该网络（a）从通过将WRF与增强的COSP模拟器耦合在532纳米和355纳米波长下模拟的3D散射体积中生成921,600张图像切片，产生高质体素参考；（b）将ATB-BC物理约束嵌入模型架构中，促进恢复过程中的能量一致性。

**Result:** <tool_call>
cmpleted the analysis and retrieved the information.
<tool_call>",

**Conclusion:** FourCastX在355纳米和532纳米波段上的Atmos-Bench数据集上均取得了持续的改进，优于现有最先进的基线模型，并且不需要依赖辅助输入。Atmos-Bench为基于卫星的3D大气结构恢复建立了新标准，开启了更深层次的气候见解。

**Abstract:** Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [68] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

> This paper reviews the interpretability of visual recognition models, proposes a human-centered taxonomy and discusses new opportunities for research.

<details>
  <summary>Details</summary>

**Motivation:** To better understand the mechanisms of visual recognition models and to diagnose failures in critical applications, this paper aims to review existing research and inspire future work.

**Method:** Structure

**Result:** A taxonomy of methods for interpretable visual recognition models based on Intent, Object, Presentation, and Methodology is proposed, along with evaluation criteria and new research opportunities.

**Conclusion:** The taxonomy and proposed criteria provide a structured approach to understand and evaluate interpretability methods, aimed at advancing the field of visual recognition models.

**Abstract:** In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [69] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

> 本文提出了KptLLM++，一种专为通用关键点理解设计的多模态大型语言模型，通过结合多样化的输入模式和用户定义的指令，实现了对关键点位置的精确定位，显著提高了细粒度图像理解的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态大型语言模型在捕捉细粒度语义信息，特别是对关键点对象的精确识别和分析上有局限性，因此提出KptLLM++来提升关键点检测的效率和准确性。

**Method:** KptLLM++采用识别-检测范式，首先理解和解释关键点的语义，然后通过结构化推理机制精确定位关键点位置，并通过规模扩大的训练数据集提高模型性能。

**Result:** 在多个关键点检测基准实验中展示了KptLLM++的优越性能，显著提升了细粒度图像理解和复杂环境下的对象识别准确性。

**Conclusion:** KptLLM++通过整合不同模式输入和扩展训练数据资源，证明了其作为细粒度图像理解统一解决方案的能力，对人机交互模式具有变革性的影响。

**Abstract:** The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>
