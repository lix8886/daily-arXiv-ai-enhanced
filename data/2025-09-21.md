<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Tokenization Strategies for Low-Resource Agglutinative Languages in Word2Vec: Case Study on Turkish and Finnish](https://arxiv.org/abs/2509.14238)
*Jinfan Frank Hu*

Main category: cs.CL

> 对土耳其语和芬兰语的分词策略实验表明，在生成高质量词嵌入方面，词级分词优于子词分词等其他策略。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于探究不同的分词策略对生成高质量词嵌入的影响，特别是在处理含有大量形态信息的黏着语（如土耳其语和芬兰语）时，不同分词策略的效果。

**Method:** 此研究评估了不同分词策略（包括词级分词、字符级分词、n元语法分词和字节对编码BPE）对由Word2Vec生成的土耳其语和芬兰语静态词嵌入质量的影响。研究使用了包含10,000篇文章的维基百科语料库，并在低资源条件下训练模型，评估其在命名实体识别（NER）任务上的表现。

**Result:** 研究结果显示，尽管在理论上子词分段方法有其吸引力，但在所有测试的分词策略中，词级分词始终表现最好。

**Conclusion:** 研究结论表明，在低资源语境及黏着语中，通过词级分词保持边界可能会比复杂统计方法产生更好的嵌入效果。

**Abstract:** Tokenization plays a critical role in processing agglutinative languages,
where a single word can encode multiple morphemes carrying syntactic and
semantic information. This study evaluates the impact of various tokenization
strategies - word-level, character-level, n-gram, and Byte Pair Encoding (BPE)
- on the quality of static word embeddings generated by Word2Vec for Turkish
and Finnish. Using a 10,000-article Wikipedia corpus, we trained models under
low-resource conditions and evaluated them on a Named Entity Recognition (NER)
task. Despite the theoretical appeal of subword segmentation, word-level
tokenization consistently outperformed all alternatives across all tokenization
strategies tested. These findings suggest that in agglutinative, low-resource
contexts, preserving boundaries via word-level tokenization may yield better
embedding performance than complex statistical methods. This has practical
implications for developing NLP pipelines for under-resourced languages where
annotated data and computing power are limited.

</details>


### [2] [Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion](https://arxiv.org/abs/2509.14249)
*Happymore Masoka*

Main category: cs.CL

> 该研究引入了一个新型的绍纳语-英语俚语数据集，并使用此数据集调整了一个多语言DistilBERT分类器，用于意图识别，准确率达到96.4%，F1评分为96.3%，并将其整合进了一个混合聊天机器人中，辅助学生获取研究生项目信息，展示了与仅使用检索增强生成(RAG)模型相比，混合系统在文化相关性和用户参与方面的优势。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于通过引入一个来自匿名社交媒体对话的绍纳语-英语俚语数据集，解决非洲语言在自然语言处理中代表性不足的问题，尤其是日常交流方面的不足。

**Method:** 通过整理一个带有意图、情感、对话行为、混合编码和音调注释的绍纳语-英语俚语数据集，并使用多语言DistilBERT分类器进行调整，实现意图识别，同时开发了一种结合基于规则的响应和检索增强生成（RAG）方法的混合聊天机器人。

**Result:** 调整后的多语言DistilBERT分类器在意图识别上达到了96.4%的准确率和96.3%的F1评分。此外，该研究展示了混合聊天机器人在协助学生获取研究生项目信息这一具体应用场景中，在文化相关性和用户参与度上优于仅使用RAG模型的系统。

**Conclusion:** 通过发布数据集、模型和方法，这项工作推进了非英语自然语言处理资源的发展，促进了包容性和文化共鸣的对话型人工智能系统的构建。

**Abstract:** African languages remain underrepresented in natural language processing
(NLP), with most corpora limited to formal registers that fail to capture the
vibrancy of everyday communication. This work addresses this gap for Shona, a
Bantu language spoken in Zimbabwe and Zambia, by introducing a novel
Shona--English slang dataset curated from anonymized social media
conversations. The dataset is annotated for intent, sentiment, dialogue acts,
code-mixing, and tone, and is publicly available at
https://github.com/HappymoreMasoka/Working_with_shona-slang. We fine-tuned a
multilingual DistilBERT classifier for intent recognition, achieving 96.4\%
accuracy and 96.3\% F1-score, hosted at https://huggingface.co/HappymoreMasoka.
This classifier is integrated into a hybrid chatbot that combines rule-based
responses with retrieval-augmented generation (RAG) to handle domain-specific
queries, demonstrated through a use case assisting prospective students with
graduate program information at Pace University. Qualitative evaluation shows
the hybrid system outperforms a RAG-only baseline in cultural relevance and
user engagement. By releasing the dataset, model, and methodology, this work
advances NLP resources for African languages, promoting inclusive and
culturally resonant conversational AI.

</details>


### [3] [The meaning of prompts and the prompts of meaning: Semiotic reflections and modelling](https://arxiv.org/abs/2509.14250)
*Martin Thellefsen,Amalia Nurma Dewi,Bent Sorensen*

Main category: cs.CL

> 此论文重新思考提示方法，提出了提示不仅仅是技术输入，而是一种涉及符号形成、解释和改进的交流与知识性行为。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于重新概念化提示的作用，将其视为一种更广泛的交流和知识性行为，而不仅仅是技术输入。

**Method:** 论文基于皮尔士的符号学理论，包括代表物、对象和解释项之间的相互作用，以及符号的多种类型，来分析大语言模型中的提示。

**Result:** 研究表明，提示是一种符号学和交流过程，这种过程重新定义了在数字环境中如何组织、搜索、解释和协同构建知识。

**Conclusion:** 论文提出将知识组织和信息检索的理论与方法重新构思，特别是在计算符号学的时代。

**Abstract:** This paper explores prompts and prompting in large language models (LLMs) as
dynamic semiotic phenomena, drawing on Peirce's triadic model of signs, his
nine sign types, and the Dynacom model of communication. The aim is to
reconceptualize prompting not as a technical input mechanism but as a
communicative and epistemic act involving an iterative process of sign
formation, interpretation, and refinement. The theoretical foundation rests on
Peirce's semiotics, particularly the interplay between representamen, object,
and interpretant, and the typological richness of signs: qualisign, sinsign,
legisign; icon, index, symbol; rheme, dicent, argument - alongside the
interpretant triad captured in the Dynacom model. Analytically, the paper
positions the LLM as a semiotic resource that generates interpretants in
response to user prompts, thereby participating in meaning-making within shared
universes of discourse. The findings suggest that prompting is a semiotic and
communicative process that redefines how knowledge is organized, searched,
interpreted, and co-constructed in digital environments. This perspective
invites a reimagining of the theoretical and methodological foundations of
knowledge organization and information seeking in the age of computational
semiosis

</details>


### [4] [LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures](https://arxiv.org/abs/2509.14252)
*Hai Huang,Yann LeCun,Randall Balestriero*

Main category: cs.CL

> 本研究提出了LLM-JEPA，一种基于联合嵌入预测架构（JEPA）的解决方案，适用于语言模型的微调和预训练，并在多个数据集和模型上显示出优于标准训练目标的效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索语言训练方法是否可以从视觉训练方法中吸取教训，尤其是嵌入空间训练目标是否比输入空间训练目标更有效。

**Method:** 提出了一种基于JEPA的解决方案，称为LLM-JEPA，应用于语言模型的微调和预训练。

**Result:** 实验结果显示，LLM-JEPA在多个数据集（NL-RX、GSM8K、Spider、RottenTomatoes）和不同模型（如Llama3、OpenELM、Gemma2、Olmo）上，均显著优于标准的训练目标，并且具有较强的过拟合鲁棒性。

**Conclusion:** 结论是LLM-JEPA为语言训练目标设计提供了一个成功的实例，并验证了JEPA类方法在语言模型上的潜力。

**Abstract:** Large Language Model (LLM) pretraining, finetuning, and evaluation rely on
input-space reconstruction and generative capabilities. Yet, it has been
observed in vision that embedding-space training objectives, e.g., with Joint
Embedding Predictive Architectures (JEPAs), are far superior to their
input-space counterpart. That mismatch in how training is achieved between
language and vision opens up a natural question: {\em can language training
methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is
a testimony of the challenge in designing such objectives for language. In this
work, we propose a first step in that direction where we develop LLM-JEPA, a
JEPA based solution for LLMs applicable both to finetuning and pretraining.
Thus far, LLM-JEPA is able to outperform the standard LLM training objectives
by a significant margin across models, all while being robust to overfiting.
Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider,
RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo
families. Code: https://github.com/rbalestr-lab/llm-jepa.

</details>


### [5] [CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning](https://arxiv.org/abs/2509.14253)
*Ahmad Pouramini,Hesham Faili*

Main category: cs.CL

> CrossPT 是一个多任务提示调优框架，通过学习注意力机制实现跨任务知识迁移和专业化，提高了多任务设置的准确性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大多数提示调优方法是针对单一任务设置设计的，未能实现跨相关任务的知识共享。为此，研究人员提出了CrossPT。

**Method:** Cross-task Prompt Tuning (CrossPT) 是一个多任务提示调优框架，该框架通过学习注意力机制将每个目标提示分解为共享的预训练源提示和特定任务的私有提示，从而实现可控的知识迁移，同时保持任务特定的专业化。

**Result:** 在GLUE和其他相关基准测试中，实证结果表明，与传统的提示调优方法相比，CrossPT在低资源场景中实现了更高的准确性和鲁棒性，同时保持了良好的参数效率。

**Conclusion:** 研究证明了CrossPT在多任务设置下的有效性和鲁棒性，尤其是在低资源场景中，同时保持了参数效率。

**Abstract:** Prompt tuning offers a parameter-efficient way to adapt large pre-trained
language models to new tasks, but most existing approaches are designed for
single-task settings, failing to share knowledge across related tasks. We
propose Cross-task Prompt Tuning (CrossPT), a modular framework for multi-task
prompt tuning that enables controlled knowledge transfer while maintaining
task-specific specialization. CrossPT decomposes each target prompt into
shared, pre-trained source prompts and task-specific private prompts, combined
via a learned attention mechanism. To support robust transfer, we
systematically investigate key design factors including prompt initialization,
balancing shared and private prompts, number of source prompts, learning rates,
task prefixes, and label semantics. Empirical results on GLUE and related
benchmarks show that CrossPT achieves higher accuracy and robustness compared
to traditional prompt tuning and related methods, particularly in low-resource
scenarios, while maintaining strong parameter efficiency.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Class-invariant Test-Time Augmentation for Domain Generalization](https://arxiv.org/abs/2509.14420)
*Zhicheng Lin,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

> 本文提出了轻量级测试时间增强（CI-TTA）技术来提高深度模型在不同数据分布上的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 深度模型在数据分布变化时性能显著下降，传统的域泛化方法依赖于多域训练或计算密集型的测试时间适应。

**Method:** 轻量级的测试时间增强（CI-TTA）技术，生成多样的输入图像变体，这些变体通过弹性变形和网格变形来保持与原始图像相同的类别。

**Result:** 在PACS和Office-Home数据集上的大量实验表明，该方法在不同的域泛化算法和骨干网络中都表现出一致的性能提升。

**Conclusion:** CI-TTA技术展示了其有效性和普适性，可以有效地应用于提高深度模型的域泛化性能。

**Abstract:** Deep models often suffer significant performance degradation under
distribution shifts. Domain generalization (DG) seeks to mitigate this
challenge by enabling models to generalize to unseen domains. Most prior
approaches rely on multi-domain training or computationally intensive test-time
adaptation. In contrast, we propose a complementary strategy: lightweight
test-time augmentation. Specifically, we develop a novel Class-Invariant
Test-Time Augmentation (CI-TTA) technique. The idea is to generate multiple
variants of each input image through elastic and grid deformations that
nevertheless belong to the same class as the original input. Their predictions
are aggregated through a confidence-guided filtering scheme that remove
unreliable outputs, ensuring the final decision relies on consistent and
trustworthy cues. Extensive Experiments on PACS and Office-Home datasets
demonstrate consistent gains across different DG algorithms and backbones,
highlighting the effectiveness and generality of our approach.

</details>
