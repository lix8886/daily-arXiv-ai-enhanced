{"id": "2508.19268", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19268", "abs": "https://arxiv.org/abs/2508.19268", "authors": ["Qing Wang", "Xue Han", "Jiahui Wang", "Lehao Xing", "Qian Hu", "Lianlian Zhang", "Chao Deng", "Junlan Feng"], "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts", "comment": null, "summary": "Despite LLMs' excellent code creation capabilities, multilingual code\ngeneration remains extremely challenging. To address this, we intent to improve\nthe multi-programming-lingual (MultiPL) performance of the base LLMs while\nretaining the most popular ones using restricted computational resources. We\nconsider MultiPL to be a special case of multiple natural languages and propose\na MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called\nMultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize\nexpert selection at both the token and segment levels. The token-level MoE is a\nstandard upcycling MoE structure with a shared expert and a novel gate weight\nnormalization approach that aids in the final fusion with the segment-level\nMoE. The segment-level MoE incorporates two innovative designs to better\ncapture the syntactic structure and contextual patterns of programming\nlanguages: First, using a sliding window to partition the input token sequence\ninto multiple segments; Then, adopting an expert-choice routing strategy that\nallows experts to select the top-k segments. The results of the experiment\nproved the effectiveness of MultiPL-MoE.", "AI": {"tldr": "提出MultiPL-MoE模型，提高了大型语言模型的多编程语言生成能力。", "motivation": "为了解决在有限计算资源下提升大型语言模型（LLM）多编程语言性能的难题。", "method": "提出了一种名为MultiPL-MoE的多编程语言扩展，结合了两级专家混合模型，即在标记级和片段级优化专家选择。标记级MoE采用标准的MoE结构并引入了一种新颖的门权重规范化方法。片段级MoE使用滑动窗口划分输入标记序列，并采用一种专家选择路由策略。", "result": "实验结果证明了MultiPL-MoE的有效性。", "conclusion": "MultiPL-MoE在多编程语言性能方面展现了有效性。"}}
{"id": "2508.19270", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19270", "abs": "https://arxiv.org/abs/2508.19270", "authors": ["Nguyen Huu Nhat Minh", "Tran Nguyen Anh", "Truong Dinh Dung", "Vo Van Nam", "Le Pham Tuyen"], "title": "Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English", "comment": null, "summary": "Cross-lingual phoneme recognition has emerged as a significant challenge for\naccurate automatic speech recognition (ASR) when mixing Vietnamese and English\npronunciations. Unlike many languages, Vietnamese relies on tonal variations to\ndistinguish word meanings, whereas English features stress patterns and\nnon-standard pronunciations that hinder phoneme alignment between the two\nlanguages. To address this challenge, we propose a novel bilingual speech\nrecognition approach with two primary contributions: (1) constructing a\nrepresentative bilingual phoneme set that bridges the differences between\nVietnamese and English phonetic systems; (2) designing an end-to-end system\nthat leverages the PhoWhisper pre-trained encoder for deep high-level\nrepresentations to improve phoneme recognition. Our extensive experiments\ndemonstrate that the proposed approach not only improves recognition accuracy\nin bilingual speech recognition for Vietnamese but also provides a robust\nframework for addressing the complexities of tonal and stress-based phoneme\nrecognition", "AI": {"tldr": "本研究针对越南语和英语的跨语言语音素识别挑战，提出了一个利用预训练编码器的双语语音识别系统，旨在提高识别精度。", "motivation": "研究动机在于解决越南语和英语混合发音情况下的准确性自动语音识别(ASR)面临的跨语言语音素识别挑战，尤其是涉及声调变化和重音模式。", "method": "本研究提出了一种新型的双语语音识别方法，旨在解决越南语和英语语音识别的跨语言发音识别问题。方法包括：(1) 构建一套代表性的双语音素集，以弥合越南语和英语语音系统的差异；(2) 设计了一个端到端系统，利用预先训练的PhoWhisper编码器来提高语音素识别的深度高层次表征。", "result": "实验结果显示，提出的方法不仅在越南语双语音识别中提高了识别准确率，也为解决声调和重音语音素识别的复杂性提供了坚实基础。", "conclusion": "该研究证明了所提出方法在提高越南语双语音识别准确性方面的有效性，并为处理声调和重音语音素识别的复杂性提供了一个坚固的框架。"}}
{"id": "2508.19271", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19271", "abs": "https://arxiv.org/abs/2508.19271", "authors": ["Rushitha Santhoshi Mamidala", "Anshuman Chhabra", "Ankur Mali"], "title": "Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT", "comment": null, "summary": "Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and\nIn-Context Learning (ICL) have become widely used for eliciting reasoning\ncapabilities in large language models (LLMs). However, these methods rely on\nfragile, implicit mechanisms often yielding inconsistent outputs across seeds,\nformats, or minor prompt variations making them fundamentally unreliable for\ntasks requiring stable, interpretable reasoning. In contrast, automata-based\nneuro-symbolic frameworks like RetoMaton offer a more structured and\ntrustworthy alternative by grounding retrieval in symbolic memory with\ndeterministic transitions. In this work, we extend RetoMaton by replacing its\nglobal datastore with a local, task-adaptive Weighted Finite Automaton (WFA),\nconstructed directly from external domain corpora. This local automaton\nstructure promotes robust, context-aware retrieval while preserving symbolic\ntraceability and low inference overhead. Unlike prompting, which entangles\ncontext and memory in opaque ways, our approach leverages the explicit\nstructure of WFAs to provide verifiable and modular retrieval behavior, making\nit better suited for domain transfer and interoperability. We evaluate this\nlocal RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT\nacross three reasoning tasks: TriviaQA (reading comprehension), GSM8K\n(multi-step math), and MMLU (domain knowledge). Compared to the base model and\nprompting-based methods, augmenting these setups with local RetoMaton\nconsistently improves performance while enabling transparent and reproducible\nretrieval dynamics. Our results highlight a promising shift toward trustworthy,\nsymbolic reasoning in modern LLMs via lightweight, automaton-guided memory.", "AI": {"tldr": "本文通过用局部的加权有限自动机（WFA）替换全局数据存储，改进了基于符号的神经自动机框架RetoMaton。这种方法在促进健壮的上下文感知检索的同时，还保持了符号可追溯性和低推理开销。实验结果显示，这种方法在三个推理任务上优于基础模型和基于提示的方法。", "motivation": "通过链式思维(CoT)和上下文学习(ICL)等基于提示的推理策略为大语言模型(LLMs)的推理能力提供了启发。然而，这些方法依赖于脆弱的、隐性的机制，往往导致跨种子、格式或轻微提示变化的输出不一致，这使得它们基本不可靠，不适合需要稳定、可解释推理的任务。本文旨在通过提出一种结构化和更值得信赖的替代方案，解决这些问题。", "method": "本文提出通过用从外部领域语料库直接构建的局部加权有限自动机(WFA)替换RetoMaton的全局数据存储，来改进RetoMaton。这种方法促进健壮的上下文感知检索，同时保持符号可追溯性和低推理开销。与将上下文和记忆在不透明方式中纠缠的提示方法不同，本文的方法利用WFA的显式结构提供可验证和模块化的检索行为。", "result": "在两个预训练的语言模型LLaMA-3.2-1B和Gemma-3-1B-PT上进行评估，三个推理任务分别是TriviaQA (阅读理解)，GSM8K (多步数学)和MMLU (领域知识)。结果显示，在这些设置中加入局部RetoMaton可以持续提高性能，同时使检索动态透明和可重复。", "conclusion": "研究结果显示了一种利用轻量级自动机引导记忆来实现可信赖符号推理的现代大语言模型的有希望的发展方向。通过局部WFA增强可解释性和推理性能，为领域迁移和互操作性提供了更好的支持。"}}
{"id": "2508.19272", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19272", "abs": "https://arxiv.org/abs/2508.19272", "authors": ["Kshitij Fadnis", "Sara Rosenthal", "Maeda Hanafi", "Yannis Katsis", "Marina Danilevsky"], "title": "RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits", "comment": null, "summary": "Retrieval Augmented Generation (RAG) is an important aspect of conversing\nwith Large Language Models (LLMs) when factually correct information is\nimportant. LLMs may provide answers that appear correct, but could contain\nhallucinated information. Thus, building benchmarks that can evaluate LLMs on\nmulti-turn RAG conversations has become an increasingly important task.\nSimulating real-world conversations is vital for producing high quality\nevaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform\nthat enables annotators to simulate real-world conversations for benchmarking\nand evaluating LLMs. RAGAPHENE has been successfully used by approximately 40\nannotators to build thousands of real-world conversations.", "AI": {"tldr": "RAGAPHENE是一个用于模拟真实世界对话的聊天注释平台，旨在为评估LLM在多轮次检索增强生成对话中的表现提供高质量的基准。", "motivation": "应对LLMs可能生成的事实性错误信息，通过模拟真实世界对话来评估和基准化LLM性能。", "method": "开发了RAGAPHENE平台，供约40名注释者使用以创建数千个真实世界的对话。", "result": "RAGAPHENE已被约40名注释者用来构建了数千个真实世界对话。", "conclusion": "RAGAPHENE平台证明了其在为多轮次RAG对话创建基准对话中的有效性。"}}
{"id": "2508.19254", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19254", "abs": "https://arxiv.org/abs/2508.19254", "authors": ["Jookyung Song", "Mookyoung Kang", "Nojun Kwak"], "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration", "comment": "6 pages, 4 figures, NeurIPS Creative AI Track 2025", "summary": "This paper presents a real-time generative drawing system that interprets and\nintegrates both formal intent - the structural, compositional, and stylistic\nattributes of a sketch - and contextual intent - the semantic and thematic\nmeaning inferred from its visual content - into a unified transformation\nprocess. Unlike conventional text-prompt-based generative systems, which\nprimarily capture high-level contextual descriptions, our approach\nsimultaneously analyzes ground-level intuitive geometric features such as line\ntrajectories, proportions, and spatial arrangement, and high-level semantic\ncues extracted via vision-language models. These dual intent signals are\njointly conditioned in a multi-stage generation pipeline that combines\ncontour-preserving structural control with style- and content-aware image\nsynthesis. Implemented with a touchscreen-based interface and distributed\ninference architecture, the system achieves low-latency, two-stage\ntransformation while supporting multi-user collaboration on shared canvases.\nThe resulting platform enables participants, regardless of artistic expertise,\nto engage in synchronous, co-authored visual creation, redefining human-AI\ninteraction as a process of co-creation and mutual enhancement.", "AI": {"tldr": "本文介绍了一种新的实时生成绘画系统，该系统通过整合视觉内容的形式意图及语义意图，来支持多人参与同步创作，算法结合底层几何分析和高层语义信息，实现更流畅、高效、合作的视觉创作过程。", "motivation": "本研究旨在开发一个能够将视觉内容的形式意图与语义意图相结合的实时生成绘画系统，以实现人类与人工智能在创作过程中的更友好、深入的合作。", "method": "研究提出了一种新的生成架构，它能够分析线条轨迹、比例和空间排列等底层几何特征，同时结合视觉-语言模型提取的高层次语义信息，通过一个结合轮廓保持结构控制及内容感知图像合成的多阶段生成管道，实现高效的生成绘画转换。", "result": "该论文提出了一种实时生成绘画系统，该系统能够解释并整合草图的形式意图（包括结构、组合和风格属性）以及上下文意图（从其视觉内容推断出的语义和主题意义），将其统一到转换过程中。与主要捕捉高层次上下文描述的传统文本提示生成系统不同，我们的方法同时分析底层的直观几何特征，如线条轨迹、比例和空间排列，并通过视觉-语言模型提取高层次的语义线索。这些双重意图信号在一个多级生成管道中以联合方式处理，该管道结合了轮廓保持的结构性控制以及风格和内容感知的图像合成。该系统实现了一个基于触摸屏的界面和分布式推理架构，能够在支持多人在共享画布上合作的同时，实现低延迟的两阶段转换。由此产生的平台使参与者，不论其艺术造诣如何，都能够参与同步、共同创作的视觉创作，重新定义人类与人工智能交互的过程为一种共同创作和相互增强的过程。", "conclusion": "研究证明了在艺术创作中，结合底层几何特征分析和高层语义信息提取，能够显著提升人工智能与人类之间的合作与创作体验，此系统为协同创作平台的发展提供了新的可能。"}}
{"id": "2508.19274", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19274", "abs": "https://arxiv.org/abs/2508.19274", "authors": ["Yue Chu"], "title": "Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis", "comment": "Ph.D. dissertation submitted to The Ohio State University, August\n  2025", "summary": "In countries without civil registration and vital statistics, verbal autopsy\n(VA) is a critical tool for estimating cause of death (COD) and inform policy\npriorities. In VA, interviewers ask proximal informants for details on the\ncircumstances preceding a death, in the form of unstructured narratives and\nstructured questions. Existing automated VA cause classification algorithms\nonly use the questions and ignore the information in the narratives. In this\nthesis, we investigate how the VA narrative can be used for automated COD\nclassification using pretrained language models (PLMs) and machine learning\n(ML) techniques. Using empirical data from South Africa, we demonstrate that\nwith the narrative alone, transformer-based PLMs with task-specific fine-tuning\noutperform leading question-only algorithms at both the individual and\npopulation levels, particularly in identifying non-communicable diseases. We\nexplore various multimodal fusion strategies combining narratives and questions\nin unified frameworks. Multimodal approaches further improve performance in COD\nclassification, confirming that each modality has unique contributions and may\ncapture valuable information that is not present in the other modality. We also\ncharacterize physician-perceived information sufficiency in VA. We describe\nvariations in sufficiency levels by age and COD and demonstrate that\nclassification accuracy is affected by sufficiency for both physicians and\nmodels. Overall, this thesis advances the growing body of knowledge at the\nintersection of natural language processing, epidemiology, and global health.\nIt demonstrates the value of narrative in enhancing COD classification. Our\nfindings underscore the need for more high-quality data from more diverse\nsettings to use in training and fine-tuning PLM/ML methods, and offer valuable\ninsights to guide the rethinking and redesign of the VA instrument and\ninterview.", "AI": {"tldr": "利用迁移学习的语言模型和机器学习技术，通过分析口头自述和结构化问题，提高了死亡原因分类的准确性，特别对于非传染性疾病的识别。", "motivation": "在缺乏民事登记和生命统计的国家，口头自述（VA）是估计死亡原因（COD）和制定政策优先事项的关键工具。目前的自动化VA死亡原因分类算法只使用结构化问题，忽略了叙事中的信息。本论文旨在解决这一问题。", "method": "本论文通过使用南非的实证数据，利用迁移学习的语言模型（PLMs）和机器学习（ML）技术，探索了如何利用口头自述（VA叙事）进行自动化死亡原因分类。特别研究了多模态融合策略，即整合叙事和结构化问题以创建统一的框架。", "result": "研究表明，使用叙事信息，经过任务特异性微调的基于转换器的PLMs在个体和群体水平上都优于现有的仅依赖于问题的算法。多模态方法进一步提高了死亡原因分类的性能。", "conclusion": "本论文展示了口头自述在提升死亡原因分类中的价值，并强调了需要更多高质量、多样化环境的数据用于PLM/ML方法的训练和微调，为重新思考和重新设计VA工具提供了宝贵见解。"}}
{"id": "2508.19257", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.19257", "abs": "https://arxiv.org/abs/2508.19257", "authors": ["Chenghao Liu", "Jiachen Zhang", "Chengxuan Li", "Zhimu Zhou", "Shixin Wu", "Songfang Huang", "Huiling Duan"], "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models", "comment": "Manuscript submitted to AAAI 2026, currently under review", "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.", "AI": {"tldr": "文章提出了一种无训练的方法（TTF）以提高VLA模型在机器人操作任务上的性能，通过整合历史和当前的视觉表示，实验结果显示在多个基准和实际任务上效果显著。", "motivation": "传统的VLA模型每一步独立处理视觉输入，忽略了宝贵的时序信息，易受视觉噪声的影响。TTF旨在解决这些问题，提高模型性能。", "method": "提出了一种无训练的方法——时间标记融合（TTF），通过智能整合历史和当前的视觉表示来提高Vision-Language-Action (VLA) 模型的推断质量。此方法结合了高效的灰度像素差异分析与基于注意力的语义相关性评估，采用硬融合策略和关键帧锚定防止错误累积。", "result": "在LIBERO、SimplerEnv和实际机器人任务上进行的全面实验表明，TTF方法有一致的改进效果：LIBERO数据集上平均提高4.0个百分点（72.4% vs 68.4%基准），SimplerEnv数据集上相对提升了4.8%，实际机器人任务上相对提升了8.7%。", "conclusion": "TTF方法证明了它在OpenVLA和VLA-Cache架构上的通用性，并揭示了选择性查询矩阵在注意力机制中的重复使用可以提高而不是降低性能，表明直接KVQ矩阵的重用策略有望实现计算加速的同时提高任务成功率。"}}
{"id": "2508.19279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19279", "abs": "https://arxiv.org/abs/2508.19279", "authors": ["Gunjan Jalori", "Preetika Verma", "Sercan Ö Arık"], "title": "FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series", "comment": "EMNLP", "summary": "Time series Forecasting with large languagemodels (LLMs) requires bridging\nnumericalpatterns and natural language. Effective fore-casting on LLM often\nrelies on extensive pre-processing and fine-tuning.Recent studiesshow that a\nfrozen LLM can rival specializedforecasters when supplied with a carefully\nen-gineered natural-language prompt, but craft-ing such a prompt for each task\nis itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt\noptimization framework thatutilizes an agentic system: a\nForecaster-agentgenerates forecasts using an initial prompt,which is then\nrefined by a refiner agent, in-formed by past outputs and retrieved\nanalogs.This adaptive prompting generalizes across do-mains using creative\nprompt templates andgenerates high-quality forecasts without inter-mediate code\ngeneration.Experiments onbenchmark datasets show improved accuracyover static\nprompting and retrieval-augmentedbaselines, approaching the performance\nofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,\nachievingstrong performance via its agentic approach toadaptive prompt\nrefinement and retrieval.", "AI": {"tldr": "FLAIRR-TS是一种通过代理系统的创造性提示模板和检索方法进行适应性提示优化的时间序列预测框架，无需生成中间代码，其在基准数据集上表现超出静态提示和检索增强的基线，接近于专门提示的性能。", "motivation": "时间序列预测需要解决大型语言模型（LLMs）中数值模式和自然语言之间的差距。虽然研究表明冻结的LLM可以与专门的预测器相媲美，但如果为每项任务手工设计提示则困难且不具有一般性。因此，提出了一种自适应提示优化框架来提高预测质量和泛化能力。", "method": "本文提出了一种称为FLAIRR-TS的时间序列预测框架，该框架利用代理系统进行测试时间的提示优化。具体来说，预测器代理使用初始提示生成预测，然后由调整器代理根据过去的输出和检索到的类似物来进一步优化提示。", "result": "实验表明，FLAIRR-TS在基准数据集上的预测精度超过了静态提示和检索增强基线模型，并接近专门提示的性能。", "conclusion": "FLAIRR-TS为时间序列预测提供了一种实用的替代方案，通过其代理系统的适应性提示优化和检索方法，展现了强大的性能。"}}
{"id": "2508.19289", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19289", "abs": "https://arxiv.org/abs/2508.19289", "authors": ["Tai Inui", "Steven Oh", "Magdeline Kuan"], "title": "Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation", "comment": "6 pages", "summary": "We present an unsupervised slide-quality assessment pipeline that combines\nseven expert-inspired visual-design metrics (whitespace, colorfulness, edge\ndensity, brightness contrast, text density, color harmony, layout balance) with\nCLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate\npresentation slides. Trained on 12k professional lecture slides and evaluated\non six academic talks (115 slides), our method achieved Pearson correlations up\nto 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores\nfrom leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude\nSonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual\nratings, discriminant validity against speaker-delivery scores, and exploratory\nalignment with overall impressions. Our results show that augmenting low-level\ndesign cues with multimodal embeddings closely approximates audience\nperceptions of slide quality, enabling scalable, objective feedback in real\ntime.", "AI": {"tldr": "本文提出了一种结合专家设计指标和CLIP-ViT嵌入向量的幻灯片质量评估方法，效果优于现有视觉-语言模型，与人类评分高度相关。", "motivation": "本文旨在提出一个自动化的幻灯片质量评估系统，以在没有人工干预的情况下提供大规模、客观的反馈。", "method": "本文介绍了一个结合七种视觉设计指标（空白量、色彩丰富度、边缘密度、亮度对比度、文字密度、色彩和谐度、布局平衡）和CLIP-ViT嵌入向量的无监督幻灯片质量评估流水线。采用Isolation Forest算法基于异常评分来评估幻灯片。", "result": "模型在接受12,000张专业讲座幻灯片训练，并在六次学术演讲的共计115张幻灯片上进行了评估，与人类视觉质量评分的皮尔逊相关性达到0.83，是领先视觉-语言模型得分的1.79至3.23倍。", "conclusion": "研究表明，结合低级别设计线索和多模态嵌入可以近似观众对幻灯片质量的感知，并在实时中提供了可扩展的客观反馈。"}}
{"id": "2508.19282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19282", "abs": "https://arxiv.org/abs/2508.19282", "authors": ["Ziqiang Cui", "Yunpeng Weng", "Xing Tang", "Peiyang Liu", "Shiwei Li", "Bowei He", "Jiamin Chen", "Xiuqiang He", "Chen Ma"], "title": "CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the timeliness of knowledge and the factual accuracy of responses in\nLarge Language Models (LLMs). However, the inclusion of excessive retrieved\ndocuments substantially increases the input length, leading to higher\ncomputational costs. Previous studies have attempted to compress retrieved\ndocuments into shorter texts before in-context integration, but such methods\noften compromise end-task performance. The lack of well-defined compression\ntargets forces many approaches to rely on fixed heuristics, which cannot\nguarantee that the compressed content will effectively support the end task. To\naddress these limitations, we propose CORE, a novel method designed to achieve\nlossless context compression for RAG. CORE employs reinforcement learning to\noptimize the compression process without relying on predefined compression\nlabels. Specifically, it utilizes end-task performance as a reward signal and\napplies Generalized Reinforcement Learning Policy Optimization (GRPO) to train\nthe compressor. This end-to-end training framework enables the compressor to\ngenerate summaries that maximize the accuracy of answers generated by the LLM.\nExtensive experiments on four datasets demonstrate the superiority of our\napproach. With a high compression ratio of 3\\%, our method not only avoids\nperformance degradation compared to prepending full documents across all\ndatasets but also improves the average Exact Match (EM) score by 3.3 points.\nThe code will be released soon.", "AI": {"tldr": "该论文介绍了一种名为CORE的新方法，旨在实现无损上下文压缩，以优化Retrieval-Augmented Generation (RAG)的压缩过程。使用强化学习算法，CORE能训练出生成有效摘要的压缩器，提升了大型语言模型在多个数据集上的性能。", "motivation": "该论文的动机在于解决前人研究在压缩检索到的文档时，由于缺乏明确的压缩目标，导致许多方法依赖于固定启发式方法，这不能保证压缩内容有效支持最终任务。", "method": "CORE方法采用强化学习来优化压缩过程，不依赖预定义的压缩标签。它利用最终任务的性能作为奖励信号，应用广义强化学习策略优化（GRPO）来训练压缩器。这种端到端的训练框架使压缩器能够生成最大化LLM生成答案准确性的摘要。", "result": "实验结果表明，该方法不仅避免了性能下降，还提高了平均Exact Match评分。在四个数据集上的广泛实验展示了这种方法的优越性。", "conclusion": "CORE可以实现无损的上下文压缩，有效提高Retrieval-Augmented Generation (RAG)的性能，同时保持高度压缩比。"}}
{"id": "2508.19290", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19290", "abs": "https://arxiv.org/abs/2508.19290", "authors": ["Alexandros Gkillas", "Ioulia Kapsali", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation", "comment": null, "summary": "LiDAR-based segmentation is essential for reliable perception in autonomous\nvehicles, yet modern segmentation networks are highly susceptible to\nadversarial attacks that can compromise safety. Most existing defenses are\ndesigned for networks operating directly on raw 3D point clouds and rely on\nlarge, computationally intensive generative models. However, many\nstate-of-the-art LiDAR segmentation pipelines operate on more efficient 2D\nrange view representations. Despite their widespread adoption, dedicated\nlightweight adversarial defenses for this domain remain largely unexplored. We\nintroduce an efficient model-based purification framework tailored for\nadversarial defense in 2D range-view LiDAR segmentation. We propose a direct\nattack formulation in the range-view domain and develop an explainable\npurification network based on a mathematical justified optimization problem,\nachieving strong adversarial resilience with minimal computational overhead.\nOur method achieves competitive performance on open benchmarks, consistently\noutperforming generative and adversarial training baselines. More importantly,\nreal-world deployment on a demo vehicle demonstrates the framework's ability to\ndeliver accurate operation in practical autonomous driving scenarios.", "AI": {"tldr": "本文提出了一种针对2D范围视图LiDAR分割的高效模型净化框架，以在对抗攻击中保持高安全性同时减少计算复杂度。", "motivation": "现有的大多数防御措施针对的是直接作用于原始3D点云的网络，这些网络计算密集且不适合于基于2D范围视图工作流程的高效LiDAR分割。这些流程在自动驾驶中广泛采用，但针对对抗攻击的高效防御措施仍然较少。", "method": "提出了一种针对2D范围视图的直接攻击形式，并基于数学论证的优化问题开发了一个可解释的净化网络。该方法在对抗防御中实现了强大的韧性，并且计算开销极小。", "result": "该方法在公开基准上的性能与生成式和对抗训练基线方法相比具有竞争力，并且在实际自动驾驶场景中也表现出了准确的操作能力。", "conclusion": "我们提出的方法可以有效抵御对抗攻击，并能在实际部署中确保安全的感知，特别是在2D范围视图的LiDAR分割场景中。"}}
{"id": "2508.19357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19357", "abs": "https://arxiv.org/abs/2508.19357", "authors": ["Peiran Zhou", "Junnan Zhu", "Yichen Shen", "Ruoxi Yu"], "title": "Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains", "comment": null, "summary": "Large Language Models (LLMs) excel in language tasks but are prone to\nhallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)\nmitigates these by grounding LLMs in external knowledge. However, in complex\ndomains involving multiple, lengthy, or conflicting documents, traditional RAG\nsuffers from information overload and inefficient synthesis, leading to\ninaccurate and untrustworthy answers. To address this, we propose CASC\n(Context-Adaptive Synthesis and Compression), a novel framework that\nintelligently processes retrieved contexts. CASC introduces a Context Analyzer\n& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs\nkey information extraction, cross-document consistency checking and conflict\nresolution, and question-oriented structured synthesis. This process transforms\nraw, scattered information into a highly condensed, structured, and\nsemantically rich context, significantly reducing the token count and cognitive\nload for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new\nchallenging multi-document question answering dataset designed for complex\nscientific domains with inherent redundancies and conflicts. Our extensive\nexperiments demonstrate that CASC consistently outperforms strong baselines.", "AI": {"tldr": "提出CASC框架解决复杂领域的文档检索与生成问题，通过智能处理检索到的上下文，显著提高了生成答案的准确性与可信度。", "motivation": "传统的检索增强生成(Retrieval-Augmented Generation, RAG)方法在处理复杂领域涉及多个长篇幅或相互矛盾的文档时存在问题，导致信息过载和无效合成，从而生成不准确和不值得信赖的答案。", "method": "CASC (Context-Adaptive Synthesis and Compression)框架，包括一个Context Analyzer & Synthesizer (CAS)模块，该模块通过一个小的微调LLM实现关键信息提取、跨文档一致性检查和冲突解决，以及面向问题的结构化合成过程。", "result": "CASC框架能够智能处理检索到的上下文，将原始分散的信息转换为高度浓缩、结构化且语义丰富的情境，显著减少了最终Reader LLM的认知负荷。", "conclusion": "实验结果表明，CASC框架在新设计的SciDocs-QA数据集上优于强基线，该数据集专为具有内在冗余和冲突的复杂科学领域设计。"}}
{"id": "2508.19294", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19294", "abs": "https://arxiv.org/abs/2508.19294", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review", "comment": "First Peer Reviewed Review Paper for Object Detection with\n  Vision-Language Models (VLMs)", "summary": "The fusion of language and vision in large vision-language models (LVLMs) has\nrevolutionized deep learning-based object detection by enhancing adaptability,\ncontextual reasoning, and generalization beyond traditional architectures. This\nin-depth review presents a structured exploration of the state-of-the-art in\nLVLMs, systematically organized through a three-step research review process.\nFirst, we discuss the functioning of vision language models (VLMs) for object\ndetection, describing how these models harness natural language processing\n(NLP) and computer vision (CV) techniques to revolutionize object detection and\nlocalization. We then explain the architectural innovations, training\nparadigms, and output flexibility of recent LVLMs for object detection,\nhighlighting how they achieve advanced contextual understanding for object\ndetection. The review thoroughly examines the approaches used in integration of\nvisual and textual information, demonstrating the progress made in object\ndetection using VLMs that facilitate more sophisticated object detection and\nlocalization strategies. This review presents comprehensive visualizations\ndemonstrating LVLMs' effectiveness in diverse scenarios including localization\nand segmentation, and then compares their real-time performance, adaptability,\nand complexity to traditional deep learning systems. Based on the review, its\nis expected that LVLMs will soon meet or surpass the performance of\nconventional methods in object detection. The review also identifies a few\nmajor limitations of the current LVLM modes, proposes solutions to address\nthose challenges, and presents a clear roadmap for the future advancement in\nthis field. We conclude, based on this study, that the recent advancement in\nLVLMs have made and will continue to make a transformative impact on object\ndetection and robotic applications in the future.", "AI": {"tldr": "这篇文章回顾了视觉语言模型（LVLMs）在目标检测中的应用，展示了其在适应性、上下文推理和超越传统架构的泛化能力方面的进步，并提出了未来的发展方向。", "motivation": "本文的动机是深入探讨视觉语言模型（LVLMs）如何重新定义基于深度学习的目标检测，包括这些模型如何融合自然语言处理（NLP）和计算机视觉（CV）技术。", "method": "通过一个三步骤的研究回顾流程讨论视觉语言模型的运作机制，描述了最新LVLMs在目标检测中的架构创新、训练范式和输出灵活性，并分析用于整合视觉和文本信息的方法。", "result": "展示了LVLMs在目标检测和定位上的先进上下文理解能力，提供了LVLMs在不同场景包括定位和分割中的有效性的全面可视化，比较了这些模型和传统深度学习系统的实时性能、适应性和复杂度。", "conclusion": "总结显示，最近LVLMs的进步以及未来的持续改进，将对目标检测和机器人应用产生变革性的影响，尽管还存在一些重大限制，但研究指出了这些挑战的解决方案，并提出未来发展的路线图。"}}
{"id": "2508.19359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19359", "abs": "https://arxiv.org/abs/2508.19359", "authors": ["Fatemeh Haji", "Mazal Bethany", "Cho-Yu Jason Chiang", "Anthony Rios", "Peyman Najafirad"], "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction", "comment": null, "summary": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets.", "AI": {"tldr": "提出ARIS系统，结合多种方法优势，改进事件抽取的召回率和准确性。", "motivation": "解决传统判别模型召回率低和生成式方法如大语言模型在事件抽取中的幻觉和不一致预测问题。", "method": "ARIS, 一种结合了Self Mixture of Agents和判别式序列标注器的混合方法，利用结构模型共识、基于置信度的过滤以及LLM反事实推理模块来提高事件预测的可靠性。", "result": "实验表明，该方法在三个基准数据集上的表现优于现有的最先进的事件抽取方法。", "conclusion": "ARIS系统通过综合应用多种技术，能够有效提高事件抽取的质量和一致性，是一项有前景的技术。"}}
{"id": "2508.19295", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19295", "abs": "https://arxiv.org/abs/2508.19295", "authors": ["Sauptik Dhar", "Nicholas Buoncristiani", "Joe Anakata", "Haoyu Zhang", "Michelle Munson"], "title": "Large VLM-based Stylized Sports Captioning", "comment": null, "summary": "The advent of large (visual) language models (LLM / LVLM) have led to a\ndeluge of automated human-like systems in several domains including social\nmedia content generation, search and recommendation, healthcare prognosis, AI\nassistants for cognitive tasks etc. Although these systems have been\nsuccessfully integrated in production; very little focus has been placed on\nsports, particularly accurate identification and natural language description\nof the game play. Most existing LLM/LVLMs can explain generic sports\nactivities, but lack sufficient domain-centric sports' jargon to create natural\n(human-like) descriptions. This work highlights the limitations of existing\nSoTA LLM/LVLMs for generating production-grade sports captions from images in a\ndesired stylized format, and proposes a two-level fine-tuned LVLM pipeline to\naddress that. The proposed pipeline yields an improvement > 8-10% in the F1,\nand > 2-10% in BERT score compared to alternative approaches. In addition, it\nhas a small runtime memory footprint and fast execution time. During Super Bowl\nLIX the pipeline proved its practical application for live professional sports\njournalism; generating highly accurate and stylized captions at the rate of 6\nimages per 3-5 seconds for over 1000 images during the game play.", "AI": {"tldr": "This paper presents a refined LVLM pipeline that boosts the performance of automated sports commentary by enhancing accuracy and incorporating sports-specific jargon in natural language descriptions, with significant improvements in F1 and BERT scores over existing methods.", "motivation": "The motivation for this paper is to address the limitations of existing state-of-the-art LLM and LVLM models when applied to sports content, particularly in generating human-like descriptions that are accurate and rich in domain-specific language.", "method": "This paper proposes a two-level fine-tuned LVLM pipeline to improve the generation of sports captions from images in a stylized format. The pipeline is designed to enhance the natural language descriptions by incorporating domain-specific sports jargon, which existing LLM/LVLMs lack.", "result": "The proposed pipeline yields a notable improvement in performance metrics such as an increase of more than 8-10% in F1 score and over 2-10% in BERT score compared to other approaches. It was successfully used in a live professional sports setting, the Super Bowl LIX, demonstrating high accuracy and speed.", "conclusion": "The paper concludes that the newly proposed two-level fine-tuned LVLM pipeline not only provides significant improvements in accuracy and natural language generation for sports commentary, but also does so with a low memory footprint and high-speed execution, proving its practicality in real-time applications like live sports journalism."}}
{"id": "2508.19363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19363", "abs": "https://arxiv.org/abs/2508.19363", "authors": ["Jiayu Ding", "Shuming Ma", "Lei Cui", "Nanning Zheng", "Furu Wei"], "title": "LongReasonArena: A Long Reasoning Benchmark for Large Language Models", "comment": null, "summary": "Existing long-context benchmarks for Large Language Models (LLMs) focus on\nevaluating comprehension of long inputs, while overlooking the evaluation of\nlong reasoning abilities. To address this gap, we introduce LongReasonArena, a\nbenchmark specifically designed to assess the long reasoning capabilities of\nLLMs. Our tasks require models to solve problems by executing multi-step\nalgorithms that reflect key aspects of long reasoning, such as retrieval and\nbacktracking. By controlling the inputs, the required reasoning length can be\narbitrarily scaled, reaching up to 1 million tokens of reasoning for the most\nchallenging tasks. Extensive evaluation results demonstrate that\nLongReasonArena presents a significant challenge for both open-source and\nproprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our\ntask. Further analysis also reveals that the accuracy exhibits a linear decline\nwith respect to the logarithm of the expected number of reasoning steps. Our\ncode and data is available at\nhttps://github.com/LongReasonArena/LongReasonArena.", "AI": {"tldr": "提出了LongReasonArena基准测试，专门用来评估LLM的长推理能力，发现了这类模型在处理多步骤长推理任务时的挑战。", "motivation": "现有的针对大语言模型（LLMs）的长上下文基准测试侧重于评估对长输入的理解，而忽视了对长推理能力的评估。为了弥补这一不足，本研究引入了LongReasonArena这一基准，专门用于评估LLMs的长推理能力。", "method": "研究中采用了需要执行多步骤算法来解决问题的任务，这些任务反映了长推理的关键方面，如检索和回溯。输入可以被控制，使得所需的推理长度可以自由缩放，最高可达100万标记的推理步骤，用以应对最具有挑战性的任务。", "result": "广泛的评估结果显示，LongReasonArena对于开源和专有LLMs来说都是一个重大挑战。例如，Deepseek-R1在任务上的准确率仅为7.5%。进一步分析表明，准确性与预期推理步骤对数之间存在线性下降关系。", "conclusion": "引入LongReasonArena基准测试以评估LLM的长推理能力，揭示了LLMs在长推理任务方面的不足。这为未来的研究提供了改进方向。"}}
{"id": "2508.19298", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19298", "abs": "https://arxiv.org/abs/2508.19298", "authors": ["Abu Sufian", "Anirudha Ghosh", "Debaditya Barman", "Marco Leo", "Cosimo Distante"], "title": "DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models", "comment": "6 pages, 4 figures, 13th International Workshop on Biometrics and\n  Forensics (IWBF)", "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable\ncapabilities across various downstream tasks, including biometric face\nrecognition (FR) with description. However, demographic biases remain a\ncritical concern in FR, as these foundation models often fail to perform\nequitably across diverse demographic groups, considering ethnicity/race,\ngender, and age. Therefore, through our work DemoBias, we conduct an empirical\nevaluation to investigate the extent of demographic biases in LVLMs for\nbiometric FR with textual token generation tasks. We fine-tuned and evaluated\nthree widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own\ngenerated demographic-balanced dataset. We utilize several evaluation metrics,\nlike group-specific BERTScores and the Fairness Discrepancy Rate, to quantify\nand trace the performance disparities. The experimental results deliver\ncompelling insights into the fairness and reliability of LVLMs across diverse\ndemographic groups. Our empirical study uncovered demographic biases in LVLMs,\nwith PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,\nCaucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably\nconsistent. Repository: https://github.com/Sufianlab/DemoBias.", "AI": {"tldr": "研究展示了大视觉语言模型中的群体偏见，表明LLaVA和PaliGemma在某些群体中存在较高偏见，而BLIP-2更为稳定。", "motivation": "尽管大视觉语言模型（Large Vision Language Models，LVLMs）在多种下游任务中展现了强大的能力，包括带有描述的生物特征人脸识别（FR），但人口统计偏见仍然是FR中的一个关键问题。由于这些基础模型在不同人口统计群体中往往无法公平地表现，因此我们的工作旨在探讨LVLMs在带有文本标记生成任务的生物特征FR中的人口统计偏见程度。", "method": "通过DemoBias研究，我们对三种广泛使用的预训练大视觉语言模型（LVLMs）——LLaVA、BLIP-2 和 PaliGemma进行了微调和评估，使用了我们自动生成的人口统计平衡数据集。我们使用了多种评估指标，如特定群体的BERTScores和公平性差异率，来量化和追踪性能差异。", "result": "实验结果显示了LVLMs在不同人口统计群体间的公平性和可靠性。我们的实证研究表明，LVLMs存在人口统计偏见，特别是对于西班牙裔/拉丁裔、高加索人和南亚群体，PaliGemma和LLaVA表现出更高的差异性，而BLIP-2表现出相对稳定的性能。", "conclusion": "本研究揭示了大视觉语言模型在不同人口统计群体中存在公平性问题，这为未来改进模型公平性和可靠性提供了重要见解。"}}
{"id": "2508.19372", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19372", "abs": "https://arxiv.org/abs/2508.19372", "authors": ["Zikun Fu", "Chen Yang", "Kourosh Davoudi", "Ken Q. Pu"], "title": "Database Entity Recognition with Data Augmentation and Deep Learning", "comment": "6 pages, 5 figures. Accepted at IEEE 26th International Conference on\n  Information Reuse and Integration for Data Science (IRI 2025), San Jose,\n  California, August 6-8, 2025", "summary": "This paper addresses the challenge of Database Entity Recognition (DB-ER) in\nNatural Language Queries (NLQ). We present several key contributions to advance\nthis field: (1) a human-annotated benchmark for DB-ER task, derived from\npopular text-to-sql benchmarks, (2) a novel data augmentation procedure that\nleverages automatic annotation of NLQs based on the corresponding SQL queries\nwhich are available in popular text-to-SQL benchmarks, (3) a specialized\nlanguage model based entity recognition model using T5 as a backbone and two\ndown-stream DB-ER tasks: sequence tagging and token classification for\nfine-tuning of backend and performing DB-ER respectively. We compared our DB-ER\ntagger with two state-of-the-art NER taggers, and observed better performance\nin both precision and recall for our model. The ablation evaluation shows that\ndata augmentation boosts precision and recall by over 10%, while fine-tuning of\nthe T5 backbone boosts these metrics by 5-10%.", "AI": {"tldr": "本文通过开发新的DB-ER基准、数据增强方案和基于T5的实体识别模型，显著提高了自然语言查询中的数据库实体识别任务的精确度和召回率。", "motivation": "该研究旨在解决自然语言查询（NLQ）中的数据库实体识别（DB-ER）挑战，通过在这一领域推进多个贡献，包括建立更适合该任务的基准和改进数据增强技术，以提升实体识别的精确度和召回率。", "method": "此项研究提出了一个用于数据库实体识别（DB-ER）任务的人工标注基准，该基准源自流行的文本到SQL基准；一种新颖的数据增强程序，利用基于相应SQL查询的自然语言查询(NLQ)的自动标注；一个基于T5的特化语言模型实体识别模型，使用两种下游任务（序列标注和令牌分类）来微调后台并执行数据库实体识别。", "result": "通过对比实验，研究者发现他们开发的DB-ER标注器优于两种现有的最先进的NER标注器，在精确度和召回率上都有更好的表现。消融研究显示，数据增强将精确度和召回率提高了超过10%，而微调T5骨干结构则将这些指标提升了5-10%。", "conclusion": "研究结果表明，新型数据增强程序和模型的专项微调能够显著提高NLQ中DB-ER任务的表现。这为未来的相关研究提供了坚实的理论支撑和实践指导。"}}
{"id": "2508.19305", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19305", "abs": "https://arxiv.org/abs/2508.19305", "authors": ["Chen Chu", "Cyrus Shahabi"], "title": "Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities", "comment": null, "summary": "Spatial representation learning is essential for GeoAI applications such as\nurban analytics, enabling the encoding of shapes, locations, and spatial\nrelationships (topological and distance-based) of geo-entities like points,\npolylines, and polygons. Existing methods either target a single geo-entity\ntype or, like Poly2Vec, decompose entities into simpler components to enable\nFourier transformation, introducing high computational cost. Moreover, since\nthe transformed space lacks geometric alignment, these methods rely on uniform,\nnon-adaptive sampling, which blurs fine-grained features like edges and\nboundaries. To address these limitations, we introduce Geo2Vec, a novel method\ninspired by signed distance fields (SDF) that operates directly in the original\nspace. Geo2Vec adaptively samples points and encodes their signed distances\n(positive outside, negative inside), capturing geometry without decomposition.\nA neural network trained to approximate the SDF produces compact,\ngeometry-aware, and unified representations for all geo-entity types.\nAdditionally, we propose a rotation-invariant positional encoding to model\nhigh-frequency spatial variations and construct a structured and robust\nembedding space for downstream GeoAI models. Empirical results show that\nGeo2Vec consistently outperforms existing methods in representing shape and\nlocation, capturing topological and distance relationships, and achieving\ngreater efficiency in real-world GeoAI applications. Code and Data can be found\nat: https://github.com/chuchen2017/GeoNeuralRepresentation.", "AI": {"tldr": "提出了一种新的Geo2Vec方法，该方法通过符号距离场直接在原始空间中工作，有效解决了现有方法在处理地理实体时的计算成本和样本缺乏精确性等问题，从而在GeoAI应用方面表现出色。", "motivation": "现有的空间表示学习方法要么只针对单一类型的地理位置实体，要么通过将实体分解为更简单的组件以启用傅里叶变换，导致极高的计算成本。而且由于变换空间缺乏几何对齐，这些方法依赖于统一的非自适应采样，这会模糊精细的特征如边缘和边界。为解决这些问题，提出了Geo2Vec。", "method": "Geo2Vec通过采用符号距离场（SDF）的原理直接在原始空间中操作，自适应采样点并编码其符号距离（外部为正，内部为负），捕捉几何形状而不需分解实体。此外，提出了一种旋转不变的位置编码来建模高频空间变化，并为下游GeoAI模型构造结构化且稳健的嵌入空间。", "result": "实证结果表明，Geo2Vec在表示形状和位置、捕捉拓扑和距离关系以及在实际GeoAI应用中实现更高效率方面始终优于现有方法。", "conclusion": "Geo2Vec为地理实体的空间表示提供了一种紧凑、几何感知且统一的解决方案，展示了在各种GeoAI任务中的优越性能和更高的处理效率。"}}
{"id": "2508.19402", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19402", "abs": "https://arxiv.org/abs/2508.19402", "authors": ["Mor Turgeman", "Chen Shani", "Dafna Shahaf"], "title": "One Joke to Rule them All? On the (Im)possibility of Generalizing Humor", "comment": null, "summary": "Humor is a broad and complex form of communication that remains challenging\nfor machines. Despite its broadness, most existing research on computational\nhumor traditionally focused on modeling a specific type of humor. In this work,\nwe wish to understand whether competence on one or more specific humor tasks\nconfers any ability to transfer to novel, unseen types; in other words, is this\nfragmentation inevitable? This question is especially timely as new humor types\ncontinuously emerge in online and social media contexts (e.g., memes,\nanti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this\nevolving landscape, they must be able to generalize across humor types by\ncapturing deeper, transferable mechanisms. To investigate this, we conduct a\nseries of transfer learning experiments across four datasets, representing\ndifferent humor tasks. We train LLMs under varied diversity settings (1-3\ndatasets in training, testing on a novel task). Experiments reveal that models\nare capable of some transfer, and can reach up to 75% accuracy on unseen\ndatasets; training on diverse sources improves transferability (1.88-4.05%)\nwith minimal-to-no drop in in-domain performance. Further analysis suggests\nrelations between humor types, with Dad Jokes surprisingly emerging as the best\nenabler of transfer (but is difficult to transfer to). We release data and\ncode.", "AI": {"tldr": "该研究探讨了在不同的幽默类型之间，大语言模型（LLMs）的能力能否进行转移，并通过转移学习实验发现模型可以在未曾见过的数据上达到75%的准确率，且在多样化的训练设置下表现更好。", "motivation": "随着在社交媒体上不断出现新的幽默类型，例如梗图、反幽默和AI失败等，想要让LLMs跟上这种不断变化的环境，它们必须能够跨幽默类型进行泛化，捕捉更深层次、可转移的机制。研究的动机在于探讨是否可以跨越特定的幽默任务进行转移学习以及这种分裂能否避免。", "method": "研究者进行了一系列的转移学习实验，涉及四个代表不同幽默任务的数据集。他们使用了一到三个数据集进行训练，并在新的任务上进行了测试。", "result": "实验结果显示了模型之间存在一些共通性，能够达到75%的转移学习准确率，并且多样性训练设置极大地提高了模型的可转移性（提高了1.88%-4.05%），同时对域内性能几乎没有影响。此外，研究发现Dad Jokes是最佳的模型转移动力，但难以转移到其他类型上。", "conclusion": "LLMs在幽默类型之间的转移学习表现出了一定的潜力，且多样性的训练能够有效提升模型的泛化能力。这些成果证明了在保持原领域性能的同时提高跨领域性能的可能性。"}}
{"id": "2508.19307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19307", "abs": "https://arxiv.org/abs/2508.19307", "authors": ["Hamza Khan"], "title": "Advancements in Crop Analysis through Deep Learning and Explainable AI", "comment": "Master's thesis", "summary": "Rice is a staple food of global importance in terms of trade, nutrition, and\neconomic growth. Among Asian nations such as China, India, Pakistan, Thailand,\nVietnam and Indonesia are leading producers of both long and short grain\nvarieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To\nensure consumer satisfaction and strengthen national reputations, monitoring\nrice crops and grain quality is essential. Manual inspection, however, is\nlabour intensive, time consuming and error prone, highlighting the need for\nautomated solutions for quality control and yield improvement. This study\nproposes an automated approach to classify five rice grain varieties using\nConvolutional Neural Networks (CNN). A publicly available dataset of 75000\nimages was used for training and testing. Model evaluation employed accuracy,\nrecall, precision, F1-score, ROC curves, and confusion matrices. Results\ndemonstrated high classification accuracy with minimal misclassifications,\nconfirming the model effectiveness in distinguishing rice varieties. In\naddition, an accurate diagnostic method for rice leaf diseases such as Brown\nSpot, Blast, Bacterial Blight, and Tungro was developed. The framework combined\nexplainable artificial intelligence (XAI) with deep learning models including\nCNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP\n(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic\nExplanations) revealed how specific grain and leaf features influenced\npredictions, enhancing model transparency and reliability. The findings\ndemonstrate the strong potential of deep learning in agricultural applications,\npaving the way for robust, interpretable systems that can support automated\ncrop quality inspection and disease diagnosis, ultimately benefiting farmers,\nconsumers, and the agricultural economy.", "AI": {"tldr": "", "motivation": "", "method": "Structure", "result": "{\n  \"tldr\": \"该研究提出了一种使用卷积神经网络（CNN）自动分类五种水稻品种的方法，并开发了一种准确诊断水稻叶部疾病的诊断方法，结合了解释性人工智能（XAI）技术，展示了深度学习在农业应用中的潜力。\",\n  \"motivation\": \"为了确保消费者满意并加强国家声誉，监测水稻作物和谷物质量至关重要。然而，人工检查劳动密集、耗时且易出错，因此需要自动化解决方案以改善质量和产量。\",\n  \"method\": \"本研究采用卷积神经网络（CNN）自动分类五种水稻品种，使用包含75000张图像的公开数据集进行训练和测试。模型评估采用了准确性、召回率、精度、F1得分、ROC曲线和混淆矩阵等指标。\",\n  \"result\": \"结果展示了高分类准确性，并且错误分类最小，确认了模型在区分水稻品种方面的能力。此外，开发了一种用于诊断水稻叶部疾病（如褐斑、稻瘟病、细菌性条斑病、丛病）的准确诊断方法。\",\n  \"conclusion\": \"研究结果表明，深度学习在农业应用中具有强大的潜力，促进了其在支持自动化作物质量检查和疾病诊断方面的发展，最终将有益于农民、消费者和农业经济。\\n\"}\n}", "conclusion": ""}}
{"id": "2508.19427", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.19427", "abs": "https://arxiv.org/abs/2508.19427", "authors": ["Evandro L. T. P. Cunha"], "title": "A perishable ability? The future of writing in the face of generative artificial intelligence", "comment": "10 pages", "summary": "The 2020s have been witnessing a very significant advance in the development\nof generative artificial intelligence tools, including text generation systems\nbased on large language models. These tools have been increasingly used to\ngenerate texts in the most diverse domains -- from technical texts to literary\ntexts --, which might eventually lead to a lower volume of written text\nproduction by humans. This article discusses the possibility of a future in\nwhich human beings will have lost or significantly decreased their ability to\nwrite due to the outsourcing of this activity to machines. This possibility\nparallels the loss of the ability to write in other moments of human history,\nsuch as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).", "AI": {"tldr": "文章探讨了因将写作文本的任务外包给机器，人类未来可能失去或显著减少写作能力的可能性，并将其与人类历史上其他时期写作能力的丧失相类比，例如所谓的希腊黑暗时代。", "motivation": "近期生成式人工智能工具，尤其是基于大型语言模型的文本生成系统的发展，引发了人类可能在未来减少或丧失写作能力的讨论，类似于希腊黑暗时期的经历。", "method": "通过比较当前技术进步与历史上写作能力变化的情况，文章探讨了未来人类写作能力可能的变化。", "result": "文章指出了由于先进AI写作工具的普及，人类将来可能减少自身写作的情况。", "conclusion": "随着机器在文本生成上的使用越来越广泛，未来人类写作能力的变化是一个值得关注的问题。"}}
{"id": "2508.19312", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19312", "abs": "https://arxiv.org/abs/2508.19312", "authors": ["Ander Galván", "Marivi Higuero", "Jorge Sasiain", "Eduardo Jacob"], "title": "Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax", "comment": "Aceptado para publicaci\\'on, in Spanish language. XVII Jornadas de\n  Ingenier\\'ia Telem\\'atica (JITEL 2025)", "summary": "Facial recognition powered by Artificial Intelligence has achieved high\naccuracy in specific scenarios and applications. Nevertheless, it faces\nsignificant challenges regarding privacy and identity management, particularly\nwhen unknown individuals appear in the operational context. This paper presents\nthe design, implementation, and evaluation of a facial recognition system\nwithin a federated learning framework tailored to open-set scenarios. The\nproposed approach integrates the OpenMax algorithm into federated learning,\nleveraging the exchange of mean activation vectors and local distance measures\nto reliably distinguish between known and unknown subjects. Experimental\nresults validate the effectiveness of the proposed solution, demonstrating its\npotential for enhancing privacy-aware and robust facial recognition in\ndistributed environments.\n  --\n  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado\nuna alta precisi\\'on en algunos escenarios y aplicaciones. Sin embargo,\npresenta desaf\\'ios relacionados con la privacidad y la identificaci\\'on de\npersonas, especialmente considerando que pueden aparecer sujetos desconocidos\npara el sistema que lo implementa. En este trabajo, se propone el dise\\~no,\nimplementaci\\'on y evaluaci\\'on de un sistema de reconocimiento facial en un\nescenario de aprendizaje federado, orientado a conjuntos abiertos.\nConcretamente, se dise\\~na una soluci\\'on basada en el algoritmo OpenMax para\nescenarios de aprendizaje federado. La propuesta emplea el intercambio de los\nvectores de activaci\\'on promedio y distancias locales para identificar de\nmanera eficaz tanto personas conocidas como desconocidas. Los experimentos\nrealizados demuestran la implementaci\\'on efectiva de la soluci\\'on propuesta.", "AI": {"tldr": "本文介绍了一种新的面部识别系统设计，该系统在联邦学习框架中得到实现，特别适用于开放集合场景。", "motivation": "鉴于面部识别在隐私和身份管理方面的挑战，尤其是在出现未知个体的情况下，本文提出了一种新的解决方案。", "method": "设计并实现了一种基于OpenMax算法的联邦学习框架下的面部识别系统，该系统通过交换平均激活向量和局部距离度量来有效地区分已知和未知对象。", "result": "实验结果验证了所提解决方案的有效性，证明了其在分布式环境中增强隐私感知和鲁棒面部识别的潜力。", "conclusion": "实验表明，所提方案在通过联邦学习集成OpenMax算法来增强隐私感知的面部识别中具有潜力。"}}
{"id": "2508.19428", "categories": ["cs.CL", "cs.LO", "cs.SC", "68T30, 68T50, 68T07, 68U15", "I.2.4; I.2.7; H.3.1; H.3.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.19428", "abs": "https://arxiv.org/abs/2508.19428", "authors": ["Aleksandra Beliaeva", "Temurbek Rahmatullaev"], "title": "Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)", "comment": null, "summary": "We present a comprehensive system for addressing Tasks A, B, and C of the\nLLMs4OL 2025 challenge, which together span the full ontology construction\npipeline: term extraction, typing, and taxonomy discovery. Our approach\ncombines retrieval-augmented prompting, zero-shot classification, and\nattention-based graph modeling -- each tailored to the demands of the\nrespective task. For Task A, we jointly extract domain-specific terms and their\nontological types using a retrieval-augmented generation (RAG) pipeline.\nTraining data was reformulated into a document to terms and types\ncorrespondence, while test-time inference leverages semantically similar\ntraining examples. This single-pass method requires no model finetuning and\nimproves overall performance through lexical augmentation Task B, which\ninvolves assigning types to given terms, is handled via a dual strategy. In the\nfew-shot setting (for domains with labeled training data), we reuse the RAG\nscheme with few-shot prompting. In the zero-shot setting (for previously unseen\ndomains), we use a zero-shot classifier that combines cosine similarity scores\nfrom multiple embedding models using confidence-based weighting. In Task C, we\nmodel taxonomy discovery as graph inference. Using embeddings of type labels,\nwe train a lightweight cross-attention layer to predict is-a relations by\napproximating a soft adjacency matrix. These modular, task-specific solutions\nenabled us to achieve top-ranking results in the official leaderboard across\nall three tasks. Taken together these strategies showcase the scalability,\nadaptability, and robustness of LLM-based architectures for ontology learning\nacross heterogeneous domains.\n  Code is available at:\nhttps://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek", "AI": {"tldr": "该研究提出了一个全面的系统来解决术语抽取、类型判定及分类法发现的挑战任务，并使用了检索增强提示、零样本分类和注意力图建模等结合的方法，取得了显著的效果。", "motivation": "研究旨在解决在不同领域中创建本体库的需求，以及在有限或无训练数据情况下进行术语抽取和分类的挑战。", "method": "Structure", "result": "{\n  \"tldr\": \"该研究提出了一个全面的系统来解决术语抽取、类型判定及分类法发现的挑战任务，并使用了检索增强提示、零样本分类和注意力图建模等结合的方法，取得了显著的效果。\",\n  \"motivation\": \"研究旨在解决在不同领域中创建本体库的需求，以及在有限或无训练数据情况下进行术语抽取和分类的挑战。\",\n  \"method\": \"采用了检索增强生成管道用于术语抽取及类型识别，针对不同类型的数据量情况使用不同的策略，并通过轻量级交叉注意力层预测类目间的继承关系来发现分类法结构。\",\n  \"result\": \"该方法在三项任务中均获得了官方排行榜上的顶级成绩，证明了LLM架构在异质领域本体学习中的可扩展性、适应性和健壮性。\",\n  \"conclusion\": \"研究结果表明，通过组合使用检索增强生成、零样本分类和注意力图建模等方法，可以有效提升术语抽取以及本体构建任务的性能。\",\n  \"code\": \"https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek。请访问该链接以获取代码。\"}\n}", "conclusion": "研究结果表明，通过组合使用检索增强生成、零样本分类和注意力图建模等方法，可以有效提升术语抽取以及本体构建任务的性能。"}}
{"id": "2508.19314", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19314", "abs": "https://arxiv.org/abs/2508.19314", "authors": ["Mahdis Tourian", "Sareh Rowlands", "Remy Vandaele", "Max Fancourt", "Rebecca Mein", "Hywel T. P. Williams"], "title": "Automated classification of natural habitats using ground-level imagery", "comment": "15 pages, 6 figures, 2 tables", "summary": "Accurate classification of terrestrial habitats is critical for biodiversity\nconservation, ecological monitoring, and land-use planning. Several habitat\nclassification schemes are in use, typically based on analysis of satellite\nimagery with validation by field ecologists. Here we present a methodology for\nclassification of habitats based solely on ground-level imagery (photographs),\noffering improved validation and the ability to classify habitats at scale (for\nexample using citizen-science imagery). In collaboration with Natural England,\na public sector organisation responsible for nature conservation in England,\nthis study develops a classification system that applies deep learning to\nground-level habitat photographs, categorising each image into one of 18\nclasses defined by the 'Living England' framework. Images were pre-processed\nusing resizing, normalisation, and augmentation; re-sampling was used to\nbalance classes in the training data and enhance model robustness. We developed\nand fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label\nto each photograph. Using five-fold cross-validation, the model demonstrated\nstrong overall performance across 18 habitat classes, with accuracy and\nF1-scores varying between classes. Across all folds, the model achieved a mean\nF1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and\nPeat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or\nambiguous classes scoring lower. These findings demonstrate the potential of\nthis approach for ecological monitoring. Ground-level imagery is readily\nobtained, and accurate computational methods for habitat classification based\non such data have many potential applications. To support use by practitioners,\nwe also provide a simple web application that classifies uploaded images using\nour model.", "AI": {"tldr": "本文提出了一种新的栖息地分类方法，利用地面级图像（如照片）并通过深度学习模型进行分类。与传统方法相比，这种方法可以更方便地进行规模化分类，并已证明在识别某些栖息地方面表现良好。", "motivation": "准确的陆地栖息地分类对于生物多样性保护、生态监测和土地使用规划至关重要。现有的栖息地分类方案通常是基于卫星图像的分析，然后由田野生态学家进行验证。本研究提出了一种基于地面级图像（照片）进行栖息地分类的方法，提供了一种改进的验证方法，可以对栖息地进行规模化分类（例如使用公民科学图像集）。", "method": "研究开发了一种基于深度学习的分类系统，应用于地面级栖息地照片分类，将每张图像分类为18个类别之一，这些类别是根据'Living England'框架定义的。图像进行了预处理，包括调整大小、归一化和数据增强。使用重采样平衡训练数据中的类别，以增强模型的鲁棒性。研究开发并微调了DeepLabV3-ResNet101分类器，用于给每张照片分配栖息地类别标签。使用五折交叉验证，模型在18个栖息地类别上展示了强大的整体表现，准确率和F1分值在类别之间有所不同。", "result": "跨所有折叠，模型实现了平均F1得分为0.61。在视觉上区别明显的栖息地（如裸露土壤、淤泥和泥炭（BSSP）以及裸露沙地（BS））得分超过0.90，而混合或模糊的类别得分较低。这些发现证明了这种方法在生态系统监测方面的潜力。地面级图像易于获得，基于这些数据的准确计算分类方法有许多潜在的应用。", "conclusion": "为了支持实践者的使用，我们还提供了一个简单的网络应用程序，使用我们的模型对上传的图像进行分类。"}}
{"id": "2508.19464", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19464", "abs": "https://arxiv.org/abs/2508.19464", "authors": ["Philipp Borchert", "Jochen De Weerdt", "Marie-Francine Moens"], "title": "Bridging Language Gaps: Enhancing Few-Shot Language Adaptation", "comment": "17 pages", "summary": "The disparity in language resources poses a challenge in multilingual NLP,\nwith high-resource languages benefiting from extensive data, while low-resource\nlanguages lack sufficient data for effective training. Our Contrastive Language\nAlignment with Prompting (CoLAP) method addresses this gap by integrating\ncontrastive learning with cross-lingual representations, facilitating\ntask-specific knowledge transfer from high-resource to lower-resource\nlanguages. The primary advantage of our approach is its data efficiency,\nenabling rapid adaptation to new languages and reducing the need for large\nlabeled datasets. We conduct experiments with multilingual encoder-only and\ndecoder-only language models on natural language understanding tasks, including\nnatural language inference and relation extraction, evaluating performance\nacross both high- and low-resource languages. Our results demonstrate that\nCoLAP outperforms few-shot cross-lingual transfer baselines and in-context\nlearning, even with limited available data. This effectively narrows the\ncross-lingual performance gap, contributing to the development of more\nefficient multilingual NLP techniques.", "AI": {"tldr": "提出CoLAP方法来解决语言资源不均衡所带来的多语言NLP挑战，主要优势在于数据效率，即使在数据有限的情况下也能缩小跨语言性能差距。", "motivation": "解决因语言资源差异带给多语言自然语言处理的挑战，特别是高资源语言和低资源语言之间数据量的不均衡问题。", "method": "对比语言对齐提示（CoLAP）方法结合对比学习与跨语言表示，促进从高资源语言到低资源语言的任务特定知识转移。", "result": "实验结果显示CoLAP方法在多种自然语言理解任务上表现优于少样本跨语言转移基准和上下文学习方法，即使在数据有限的情况下也能如此。", "conclusion": "所提出的CoLAP方法有助于多语言NLP技术的发展，尤其是在数据量较少的低资源语言中的应用效率得到了提升。"}}
{"id": "2508.19320", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19320", "abs": "https://arxiv.org/abs/2508.19320", "authors": ["Ming Chen", "Liyuan Cui", "Wenyuan Zhang", "Haoxian Zhang", "Yan Zhou", "Xiaohan Li", "Xiaoqiang Liu", "Pengfei Wan"], "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation", "comment": "Technical Report. Project Page: https://chenmingthu.github.io/milm/", "summary": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with high latency, heavy\ncomputational cost, and limited controllability. In this work, we introduce an\nautoregressive video generation framework that enables interactive multimodal\ncontrol and low-latency extrapolation in a streaming manner. With minimal\nmodifications to a standard large language model (LLM), our framework accepts\nmultimodal condition encodings including audio, pose, and text, and outputs\nspatially and semantically coherent representations to guide the denoising\nprocess of a diffusion head. To support this, we construct a large-scale\ndialogue dataset of approximately 20,000 hours from multiple sources, providing\nrich conversational scenarios for training. We further introduce a deep\ncompression autoencoder with up to 64$\\times$ reduction ratio, which\neffectively alleviates the long-horizon inference burden of the autoregressive\nmodel. Extensive experiments on duplex conversation, multilingual human\nsynthesis, and interactive world model highlight the advantages of our approach\nin low latency, high efficiency, and fine-grained multimodal controllability.", "AI": {"tldr": "本文介绍了一种新型自回归视频生成框架，通过微调大型语言模型和引入高效的深度压缩自动编码器，实现了低延迟和高效率的互动多模态视频生成。", "motivation": "现有的互动数字人类视频生成系统在即时处理多样输入信号方面仍然面临高延迟、计算成本重和控制效果有限等挑战。本研究旨在解决这些问题，介绍一种可以实现实时互动多模态控制和低延迟外推的视频生成框架。", "method": "本研究提出了一种自回归视频生成框架，该框架在流式生成时支持互动多模态控制和低延迟外推。通过仅对标准大型语言模型(LLM)进行最小的修改，该框架可以接受多种模式的条件编码（包括音频、姿势和文本），并输出空间和语义连贯的表示来引导扩散模型的去噪过程。为了支持这一框架，我们构建了一个来自多个来源的大型对话数据集，大约有20000小时的对话，提供了丰富的训练对话场景。此外，我们引入了一个深度压缩自动编码器，压缩率高达64倍，有效地缓解了自回归模型长远预测的计算负担。", "result": "通过广泛的实验，包括双工对话、多语言人类合成和互动世界模型，展示了本方法在低延迟、高效性和多模态精细控制方面的优势。", "conclusion": "本研究展示了一种在处理多模态输入信号方面突破了现有技术限制的互动数字人类视频生成框架。该方法通过引入简单的大型语言模型修改和一个高效的深度压缩自动编码器，实现了在互动环境下的实时、低延迟视频生成能力，并在多模态精细控制方面表现出了显著优势。"}}
{"id": "2508.19467", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19467", "abs": "https://arxiv.org/abs/2508.19467", "authors": ["Sumon Kanti Dey", "Jeanne M. Powell", "Azra Ismail", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset", "comment": "Dataset and code: https://github.com/SumonKantiDey/Reddit_Impacts_NER", "summary": "Nonmedical opioid use is an urgent public health challenge, with far-reaching\nclinical and social consequences that are often underreported in traditional\nhealthcare settings. Social media platforms, where individuals candidly share\nfirst-person experiences, offer a valuable yet underutilized source of insight\ninto these impacts. In this study, we present a named entity recognition (NER)\nframework to extract two categories of self-reported consequences from social\nmedia narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,\ndepression) and SocialImpacts (e.g., job loss). To support this task, we\nintroduce RedditImpacts 2.0, a high-quality dataset with refined annotation\nguidelines and a focus on first-person disclosures, addressing key limitations\nof prior work. We evaluate both fine-tuned encoder-based models and\nstate-of-the-art large language models (LLMs) under zero- and few-shot\nin-context learning settings. Our fine-tuned DeBERTa-large model achieves a\nrelaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming\nLLMs in precision, span accuracy, and adherence to task-specific guidelines.\nFurthermore, we show that strong NER performance can be achieved with\nsubstantially less labeled data, emphasizing the feasibility of deploying\nrobust models in resource-limited settings. Our findings underscore the value\nof domain-specific fine-tuning for clinical NLP tasks and contribute to the\nresponsible development of AI tools that may enhance addiction surveillance,\nimprove interpretability, and support real-world healthcare decision-making.\nThe best performing model, however, still significantly underperforms compared\nto inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap\npersists between expert intelligence and current state-of-the-art NER/AI\ncapabilities for tasks requiring deep domain knowledge.", "AI": {"tldr": "研究提出了一种从社交媒体提取阿片类药物使用自述影响的NER框架，并推出了RedditImpacts 2.0数据集，结果显示领域特定微调对临床NLP任务的重要性。", "motivation": "非医疗阿片类药物使用是个公共健康的紧迫挑战，传统的医疗保健设置下往往报告不足。社交媒体提供了宝贵的见解来源，因为个人会在这些平台上公开分享此类问题的个人经历。", "method": "本研究提出了一种命名实体识别（NER）框架，用于从与阿片类药物使用相关的社交媒体叙述中提取两类自我报告的影响：临床影响（如戒断症状、抑郁）和社会影响（如失业）。", "result": "研究采用微调后的DeBERTa-large模型，在放松的标记级别F1分数上达到了0.61（95% CI：0.43-0.62），高于大语言模型（LLMs）的精度、跨度准确性和任务特定指南的遵守程度。此外，研究还显示使用较少的标注数据也能获得较强的NER性能。", "conclusion": "研究发现强调了特定领域微调对于临床自然语言处理任务的价值，并为发展可增强成瘾监视、提高可解释性并支持现实世界医疗决策的AI工具有积极贡献。不过，最佳模型的表现仍从显著不及专家之间的共识（Cohen's kappa: 0.81），显示出要求深度领域知识的任务中专家智能和当前最先进的NER/AI能力之间的差距依然存在。"}}
{"id": "2508.19324", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.19324", "abs": "https://arxiv.org/abs/2508.19324", "authors": ["Jefferson David Rodriguez Chivata", "Davide Ghiani", "Simone Maurizio La Cava", "Marco Micheletto", "Giulia Orrù", "Federico Lama", "Gian Luca Marcialis"], "title": "Deep Data Hiding for ICAO-Compliant Face Images: A Survey", "comment": "In 2025 IEEE International Joint Conference on Biometrics (IJCB)", "summary": "ICAO-compliant facial images, initially designed for secure biometric\npassports, are increasingly becoming central to identity verification in a wide\nrange of application contexts, including border control, digital travel\ncredentials, and financial services. While their standardization enables global\ninteroperability, it also facilitates practices such as morphing and deepfakes,\nwhich can be exploited for harmful purposes like identity theft and illegal\nsharing of identity documents. Traditional countermeasures like Presentation\nAttack Detection (PAD) are limited to real-time capture and offer no\npost-capture protection. This survey paper investigates digital watermarking\nand steganography as complementary solutions that embed tamper-evident signals\ndirectly into the image, enabling persistent verification without compromising\nICAO compliance. We provide the first comprehensive analysis of\nstate-of-the-art techniques to evaluate the potential and drawbacks of the\nunderlying approaches concerning the applications involving ICAO-compliant\nimages and their suitability under standard constraints. We highlight key\ntrade-offs, offering guidance for secure deployment in real-world identity\nsystems.", "AI": {"tldr": "作者调查了数字水印和信息隐藏技术，以提供拍摄后的保护机制，使得ICAO合规面部图像在网络数字化时代依旧可信。", "motivation": "传统的反措施，如呈现攻击检测（PAD）只能在实时捕捉中实现，并不能提供拍摄后的保护。随着ICAO合规面部图像在各种应用情境中中心地位的加强，防止像身份盗用和非法分发身份文档等有害行为成为迫切需要解决的问题。", "method": "分析抽象内容，并提取主要信息。", "result": "本文调查了数字水印和信息隐藏作为补充解决方案，以嵌入防篡改信号直接进入ICAO合规面部图像，使得验证持久化同时不违背ICAO法规。这是首次全面分析最先进的技术，以评估这些技术在涉及到ICAO合规图像的应用场景下的可行性和存在的短板。作者提出了一些关键的权衡，并提供了有关在现实世界的身份系统中安全部署的指导。", "conclusion": "本文指出了一些关键权衡，提供了关于在现实世界的身份系统中安全部署的指导。这为安全保护ICAO合规面部图像提供了新思路。"}}
{"id": "2508.19475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19475", "abs": "https://arxiv.org/abs/2508.19475", "authors": ["Md. Alvee Ehsan", "A. S. M Mehedi Hasan", "Kefaya Benta Shahnoor", "Syeda Sumaiya Tasneem"], "title": "Automatic Question & Answer Generation Using Generative Large Language Model (LLM)", "comment": null, "summary": "\\Abstract{In the realm of education, student evaluation holds equal\nsignificance as imparting knowledge. To be evaluated, students usually need to\ngo through text-based academic assessment methods. Instructors need to make\ndiverse sets of questions that need to be fair for all students to prove their\nadequacy over a particular topic. This can prove to be quite challenging as\nthey may need to manually go through several different lecture materials. Our\nobjective is to make this whole process much easier by implementing Automatic\nQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. For\ntailoring the instructor's preferred question style (MCQ, conceptual, or\nfactual questions), prompt Engineering (PE) is being utilized. In this\nresearch, we propose to leverage unsupervised learning methods in NLP,\nprimarily focusing on the English language. This approach empowers the base\nMeta-Llama 2-7B model to integrate RACE dataset as training data for the\nfine-tuning process. Creating a customized model that will offer efficient\nsolutions for educators, instructors, and individuals engaged in text-based\nevaluations. A reliable and efficient tool for generating questions and answers\ncan free up valuable time and resources, thus streamlining their evaluation\nprocesses.}", "AI": {"tldr": "本研究提出了一种利用非监督学习方法和提示工程来自动化生成多样问题类型的方法，使用Meta-Llama 2-7B模型和RACE数据集进行微调，以辅助教育者简化评估过程。", "motivation": "在教育领域，学生评估与传授知识同样重要。然而，教师创建多样、公平的问题集以评估学生对特定主题的掌握情况通常是颇具挑战性的。本研究旨在通过自动问答生成技术简化这一过程。", "method": "本次研究利用了非监督学习方法在自然语言处理领域，主要集中在英语上，采用提示工程来定制不同的问题风格（如选择题、概念题或事实题），并使用了Meta-Llama 2-7B模型作为基础模型，使用RACE数据集进行微调。", "result": "本研究开发的定制模型能够为教育者和评估者提供高效的自动问题生成工具，帮助节省时间和资源，优化评估流程。", "conclusion": "通过提出基于提示工程和非监督学习方法的自动问答生成系统，本研究为教育领域的评估过程提供了新的高效解决方案。"}}
{"id": "2508.19325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19325", "abs": "https://arxiv.org/abs/2508.19325", "authors": ["Haoyang Su", "Jin-Yi Xiang", "Shaohao Rui", "Yifan Gao", "Xingyu Chen", "Tingxuan Yin", "Xiaosong Wang", "Lian-Ming Wu"], "title": "PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI", "comment": null, "summary": "Accurate prediction of major adverse cardiac events (MACE) remains a central\nchallenge in cardiovascular prognosis. We present PRISM (Prompt-guided\nRepresentation Integration for Survival Modeling), a self-supervised framework\nthat integrates visual representations from non-contrast cardiac cine magnetic\nresonance imaging with structured electronic health records (EHRs) for survival\nanalysis. PRISM extracts temporally synchronized imaging features through\nmotion-aware multi-view distillation and modulates them using medically\ninformed textual prompts to enable fine-grained risk prediction. Across four\nindependent clinical cohorts, PRISM consistently surpasses classical survival\nprediction models and state-of-the-art (SOTA) deep learning baselines under\ninternal and external validation. Further clinical findings demonstrate that\nthe combined imaging and EHR representations derived from PRISM provide\nvaluable insights into cardiac risk across diverse cohorts. Three distinct\nimaging signatures associated with elevated MACE risk are uncovered, including\nlateral wall dyssynchrony, inferior wall hypersensitivity, and anterior\nelevated focus during diastole. Prompt-guided attribution further identifies\nhypertension, diabetes, and smoking as dominant contributors among clinical and\nphysiological EHR factors.", "AI": {"tldr": "PRISM, a framework that integrates cardiac imagery and EHR data, improves the prediction of major adverse cardiac events and identifies key risk signatures and contributors.", "motivation": "The central challenge in cardiovascular prognosis is the accurate prediction of major adverse cardiac events (MACE).", "method": "We present PRISM, a self-supervised framework for survival analysis that integrates visual representations from cardiac imaging and EHRs. PRISM extracts temporally synchronized imaging features through motion-aware multi-view distillation and uses medically informed textual prompts for fine-grained risk prediction.", "result": "PRISM outperforms classical survival prediction models and SOTA deep learning baselines in both internal and external validation across four clinical cohorts.", "conclusion": "PRISM demonstrates consistent superiority over classical and SOTA deep learning models in MACE risk prediction. It also uncovers three imaging signatures and identifies hypertension, diabetes, and smoking as dominant contributors to MACE risk."}}
{"id": "2508.19481", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19481", "abs": "https://arxiv.org/abs/2508.19481", "authors": ["Manuel Mosquera", "Melissa Robles", "Johan Rodriguez", "Ruben Manrique"], "title": "Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study", "comment": null, "summary": "Low-resource machine translation remains a significant challenge for large\nlanguage models (LLMs), which often lack exposure to these languages during\npretraining and have limited parallel data for fine-tuning. We propose a novel\napproach that enhances translation for low-resource languages by integrating an\nexternal dictionary tool and training models end-to-end using reinforcement\nlearning, in addition to supervised fine-tuning. Focusing on the\nSpanish-Wayuunaiki language pair, we frame translation as a tool-augmented\ndecision-making problem in which the model can selectively consult a bilingual\ndictionary during generation. Our method combines supervised instruction tuning\nwith Guided Reward Policy Optimization (GRPO), enabling the model to learn both\nwhen and how to use the tool effectively. BLEU similarity scores are used as\nrewards to guide this learning process. Preliminary results show that our\ntool-augmented models achieve up to +3.37 BLEU improvement over previous work,\nand a 18% relative gain compared to a supervised baseline without dictionary\naccess, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared\nTask. We also conduct ablation studies to assess the effects of model\narchitecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other\nmodels such as LLaMA and a prior NLLB-based system. These findings highlight\nthe promise of combining LLMs with external tools and the role of reinforcement\nlearning in improving translation quality in low-resource language settings.", "AI": {"tldr": "本研究提出了一种结合外部词典工具和强化学习方法，用于改善低资源语言（如西班牙语-Wayuunaiki）的机器翻译。", "motivation": "大型语言模型在低资源语言的翻译上面临挑战，缺乏对这些语言的预训练数据和双语数据。", "method": "将翻译问题视为结合词典辅助的决策问题，通过监督微调和引导奖励策略优化（GRPO）来共同训练模型。", "result": "在西班牙语-Wayuunaiki翻译任务上，与先前工作相比，BLEU分数提高了+3.37，与无词典访问的监督基线相比提高了18%。", "conclusion": "该研究展示了结合大型语言模型与外部工具以及强化学习方法在低资源语言翻译中的潜力。"}}
{"id": "2508.19349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19349", "abs": "https://arxiv.org/abs/2508.19349", "authors": ["Mahdieh Behjat Khatooni", "Mohsen Soryani"], "title": "EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis", "comment": null, "summary": "Alzheimer's disease (AD) is one of the most prevalent neurodegenerative\ndisorders worldwide. As it progresses, it leads to the deterioration of\ncognitive functions. Since AD is irreversible, early diagnosis is crucial for\nmanaging its progression. Mild Cognitive Impairment (MCI) represents an\nintermediate stage between Cognitively Normal (CN) individuals and those with\nAD, and is considered a transitional phase from normal cognition to Alzheimer's\ndisease. Diagnosing MCI is particularly challenging due to the subtle\ndifferences between adjacent diagnostic categories. In this study, we propose\nEffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole\nAlzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging\n(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a\nVision Transformer (ViT) to capture both local and global features from MRI\nimages. Unlike previous studies that rely on limited subsets of data, our\napproach is trained on the full T1-weighted MRI dataset from ADNI, resulting in\na more robust and unbiased model. This comprehensive methodology enhances the\nmodel's clinical reliability. Furthermore, fine-tuning large pretrained models\noften yields suboptimal results when source and target dataset domains differ.\nTo address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt\nthe pretrained ViT model to our target domain. This method enables efficient\nknowledge transfer and reduces the risk of overfitting. Our model achieves a\nclassification accuracy of 92.52% and an F1-score of 92.76% across three\ndiagnostic categories: AD, MCI, and CN for full ADNI dataset.", "AI": {"tldr": "本研究提出了一种使用EffNetViTLoRA模型的AD诊断方法，基于全部ADNI MRI数据集，实现了对三个诊断类别(AD, MCI和CN)的高精度分类。", "motivation": "阿尔茨海默病(AD)是一种常见的神经退行性疾病，早期诊断对管理其进展至关重要。轻度认知障碍(MCI)是认知正常(CN)个体与AD患者之间的中间阶段，鉴别诊断MCI具有挑战性。因此，研究旨在开发一个稳健、抗偏差的AD诊断模型。", "method": "该研究提出了一种名为EffNetViTLoRA的模型，结合卷积神经网络(CNN)和视觉变换器(ViT)，用于从MRI图像中捕捉局部和全局特征。此外，该研究还采用了低秩适应(LoRA)技术，以有效适应目标领域，解决跨领域数据差异带来的预训练模型微调次优解问题。", "result": "EffNetViTLoRA模型在三个诊断类别(AD, MCI和CN)上实现了92.52%的分类准确率和92.76%的F1得分。", "conclusion": "EffNetViTLoRA模型通过对全T1加权MRI数据集的训练和低秩适应技术的使用，避免了以往方法依赖少量子集数据的不足，提高了临床可靠性，实现了92.52%的分类准确率和92.76%的F1得分。"}}
{"id": "2508.19484", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19484", "abs": "https://arxiv.org/abs/2508.19484", "authors": ["Bahar Bateni", "Benjamin Pratt", "Jim Whitehead"], "title": "Rule Synergy Analysis using LLMs: State of the Art and Implications", "comment": "Submitted for publication at the IEEE Transactions on Games 2024,\n  Special Issue on Large Language Models and Games (10 pages excluding\n  appendix, 3 figures)", "summary": "Large language models (LLMs) have demonstrated strong performance across a\nvariety of domains, including logical reasoning, mathematics, and more. In this\npaper, we investigate how well LLMs understand and reason about complex rule\ninteractions in dynamic environments, such as card games. We introduce a\ndataset of card synergies from the game Slay the Spire, where pairs of cards\nare classified based on their positive, negative, or neutral interactions. Our\nevaluation shows that while LLMs excel at identifying non-synergistic pairs,\nthey struggle with detecting positive and, particularly, negative synergies. We\ncategorize common error types, including issues with timing, defining game\nstates, and following game rules. Our findings suggest directions for future\nresearch to improve model performance in predicting the effect of rules and\ntheir interactions.", "AI": {"tldr": "本研究调查了大语言模型在理解动态环境（如卡牌游戏）中复杂规则交互方面的表现，发现他们在识别非协同的卡片对表现出色，但在检测协同卡片对时存在问题。", "motivation": "大语言模型已在多种领域展现出强大的性能，包括逻辑推理、数学等。然而，它们在理解动态环境如纸牌游戏中复杂规则交互方面的能力尚不清楚。本研究旨在探究LLMs在这一领域的能力。", "method": "我们通过引入一个来自游戏Slay the Spire的卡片协同数据集来评估大语言模型（LLMs）在动态环境下的复杂规则交互理解与推理能力。数据集中卡片对依据其正、负或中性交互分类。", "result": "评估显示，LLMs在识别非协同卡片对方面表现出色，但在检测正协同和特别是负协同方面存在问题。我们还归类了常见的错误类型，包括时间问题、定义游戏状态和遵循游戏规则方面的问题。", "conclusion": "我们的研究发现指出了未来研究的方向，即改进模型预测规则及其交互效果的能力。"}}
{"id": "2508.19477", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19477", "abs": "https://arxiv.org/abs/2508.19477", "authors": ["Zachary L. Crang", "Rich D. Johnston", "Katie L. Mills", "Johsan Billingham", "Sam Robertson", "Michael H. Cole", "Jonathon Weakley", "Adam Hewitt and", "Grant M. Duthie"], "title": "Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage", "comment": null, "summary": "This study aimed to: (1) understand whether commercially available\ncomputer-vision and artificial intelligence (AI) player tracking software can\naccurately measure player position, speed and distance using broadcast footage\nand (2) determine the impact of camera feed and resolution on accuracy. Data\nwere obtained from one match at the 2022 Qatar Federation Internationale de\nFootball Association (FIFA) World Cup. Tactical, programme and camera 1 feeds\nwere used. Three commercial tracking providers that use computer-vision and AI\nparticipated. Providers analysed instantaneous position (x, y coordinates) and\nspeed (m\\,s^{-1}) of each player. Their data were compared with a\nhigh-definition multi-camera tracking system (TRACAB Gen 5). Root mean square\nerror (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to\n16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\\,s^{-1}. Total match\ndistance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across\nproviders. Computer-vision and AI player tracking software offer the ability to\ntrack players with fair precision when players are detected by the software.\nProviders should use a tactical feed when tracking position and speed, which\nwill maximise player detection, improving accuracy. Both 720p and 1080p\nresolutions are suitable, assuming appropriate computer-vision and AI models\nare implemented.", "AI": {"tldr": "本研究旨在评估商用计算机视觉和人工智能（AI）球员追踪软件使用广播视频素材测量球员位置、速度和距离的准确性，并确定摄像机信号和分辨率对准确性的影响。结果表明，计算机视觉和AI球员追踪软件在检测到球员时能提供合理的精确度。为了提高准确性，对位置和速度进行追踪时应使用战术信号，且720p和1080p分辨率都适合，只要运用适当的计算机视觉和AI模型。", "motivation": "研究动机是评估商用计算机视觉和人工智能（AI）球员追踪软件在广播视频基础上测量球员数据的准确性，并探讨摄像机信号类型和分辨率如何影响其准确性。", "method": "本研究数据来自2022年国际足联世界杯卡塔尔比赛中获取的实时资料。研究使用战术、程序和相机1信号，三项商用追踪提供者参与了研究，参与方分析了每个球员的实时位置（x，y坐标）和速度（m/s）。他们的数据与高清晰度多摄像机追踪系统（TRACAB Gen 5）的数据进行了对比。", "result": "位置的均方根误差（RMSE）范围从1.68到16.39米，速度的RMSE范围在0.34到2.38米/秒。总体比赛距离平均偏差范围为-1745米（-21.8%）到1945米（24.3%），取决于供应商。", "conclusion": "结果表明，计算机视觉和AI播放跟踪软件有能力在检测到运动员时提供合理水平的精度。为了提高准确性，合同供应商应使用战术信号来跟踪位置和速度，并且720p和1080p的分辨率都是合适的，只要实施适当的计算机视觉和AI模型。"}}
{"id": "2508.19529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19529", "abs": "https://arxiv.org/abs/2508.19529", "authors": ["Bowen Sun", "Yujun Cai", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding", "comment": null, "summary": "Discrete diffusion language models have shown strong potential for text\ngeneration, yet standard supervised fine-tuning (SFT) misaligns with their\nsemi-autoregressive inference: training randomly masks tokens across the entire\nresponse, while inference generates fixed-size blocks sequentially. This\nmismatch introduces noisy prefixes and leaky suffixes, biasing gradients away\nfrom the desired blockwise likelihood. We propose Blockwise SFT, which\npartitions responses into fixed-size blocks, selects one active block per step\nfor stochastic masking, freezes all preceding tokens, and fully hides future\nones. Loss is computed only over the active block, directly mirroring the\nblockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show\nconsistent gains over classical SFT under equal compute or token budgets. Block\nsize consistency studies and ablations confirm that improvements stem from\nfaithful training-inference alignment rather than incidental masking effects.\nOur results highlight the importance of matching supervision granularity to the\ndecoding procedure in diffusion-based language models.", "AI": {"tldr": "提出了Blockwise SFT方法来解决离散扩散语言模型中的训练-推理不一致问题，实验表明该方法优于经典SFT。", "motivation": "由于标准的监督微调(SFT)与离散扩散语言模型的半自回归推理不匹配，产生了噪声前缀和泄漏后缀，使得梯度偏离了所需的块级可能性。因此，提出了Blockwise SFT来解决这一问题。", "method": "我们提出了一种名为Blockwise SFT的方法，该方法将响应划分为固定大小的块，在每一步中随机选择一个活动块进行遮蔽，冻结所有先前的标记，并完全隐藏未来的标记。损失仅在活动块上计算，直接反映块级解码过程。", "result": "实验结果表明，在GSM8K、MATH和MetaMathQA数据集上，Blockwise SFT在相同的计算或标记预算下，相比经典SFT显示出一致的性能提升。研究证实，其改进源于训练和推理过程的对齐，而不是偶然的遮蔽效果。", "conclusion": "我们的结果强调了在扩散型语言模型中，将监督粒度与解码过程相匹配的重要性。"}}
{"id": "2508.19485", "categories": ["cs.CV", "68T45 (Primary), 68T07 (Secondary)", "I.2.10; I.4.6"], "pdf": "https://arxiv.org/pdf/2508.19485", "abs": "https://arxiv.org/abs/2508.19485", "authors": ["Xinlong Zhao", "Qixiang Pang", "Shan Du"], "title": "JVLGS: Joint Vision-Language Gas Leak Segmentation", "comment": "19 pages, 13 figures", "summary": "Gas leaks pose serious threats to human health and contribute significantly\nto atmospheric pollution, drawing increasing public concern. However, the lack\nof effective detection methods hampers timely and accurate identification of\ngas leaks. While some vision-based techniques leverage infrared videos for leak\ndetection, the blurry and non-rigid nature of gas clouds often limits their\neffectiveness. To address these challenges, we propose a novel framework called\nJoint Vision-Language Gas leak Segmentation (JVLGS), which integrates the\ncomplementary strengths of visual and textual modalities to enhance gas leak\nrepresentation and segmentation. Recognizing that gas leaks are sporadic and\nmany video frames may contain no leak at all, our method incorporates a\npost-processing step to reduce false positives caused by noise and non-target\nobjects, an issue that affects many existing approaches. Extensive experiments\nconducted across diverse scenarios show that JVLGS significantly outperforms\nstate-of-the-art gas leak segmentation methods. We evaluate our model under\nboth supervised and few-shot learning settings, and it consistently achieves\nstrong performance in both, whereas competing methods tend to perform well in\nonly one setting or poorly in both. Code available at:\nhttps://github.com/GeekEagle/JVLGS", "AI": {"tldr": "为解决气体泄露检测问题，提出JVLGS框架，结合视觉和文本信息，提高检测的准确性并减少误报。", "motivation": "现有的气体泄漏检测方法效果有限，特别是在模糊和非刚性的气体云情况下。", "method": "JVLGS框架结合视觉和文本模态来增强气体泄漏的表示和分割，使用后处理步骤减少假阳性。", "result": "JVLGS在多种场景下实验中显著优于现有的气体泄漏分割方法，并在有监督和少样本学习设置下都表现出色。", "conclusion": "提出的JVLGS框架能有效解决现有方法在气体泄漏检测中的问题，展示出在不同学习设置下的优越性能。"}}
{"id": "2508.19532", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19532", "abs": "https://arxiv.org/abs/2508.19532", "authors": ["Houxing Ren", "Zimu Lu", "Weikang Shi", "Haotian Hou", "Yunqiao Yang", "Ke Wang", "Aojun Zhou", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "title": "Alignment with Fill-In-the-Middle for Enhancing Code Generation", "comment": "Accepted to EMNLP 2025 (main conference)", "summary": "The code generation capabilities of Large Language Models (LLMs) have\nadvanced applications like tool invocation and problem-solving. However,\nimproving performance in code-related tasks remains challenging due to limited\ntraining data that is verifiable with accurate test cases. While Direct\nPreference Optimization (DPO) has shown promise, existing methods for\ngenerating test cases still face limitations. In this paper, we propose a novel\napproach that splits code snippets into smaller, granular blocks, creating more\ndiverse DPO pairs from the same test cases. Additionally, we introduce the\nAbstract Syntax Tree (AST) splitting and curriculum training method to enhance\nthe DPO training. Our approach demonstrates significant improvements in code\ngeneration tasks, as validated by experiments on benchmark datasets such as\nHumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data\nare available at https://github.com/SenseLLM/StructureCoder.", "AI": {"tldr": "提出了一种新颖的方法来改善代码生成能力，通过细化代码片段和AST拆分提升DPO训练效果。", "motivation": "研究动机包括提高LLMs在与代码相关的任务上的性能，尽管直接偏好优化(DPO)展示出潜力，但现有的生成测试用例的方法仍有限制。", "method": "通过将代码片段拆分为更小的粒度块来创建更多的DPO对，并引入抽象语法树（AST）拆分和课程训练方法以增强DPO训练。", "result": "实验结果在HumanEval (+), MBPP (+), APPS, LiveCodeBench 和 BigCodeBench基准数据集上验证了方法的有效性。", "conclusion": "研究展示了一种创新的方法用于提升代码生成任务的表现，特别是在改进DPO训练方法上取得了显著效果。"}}
{"id": "2508.19498", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19498", "abs": "https://arxiv.org/abs/2508.19498", "authors": ["Yimu Wang", "Weiming Zhuang", "Chen Chen", "Jiabo Huang", "Jingtao Li", "Lingjuan Lyu"], "title": "UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models", "comment": null, "summary": "In the era of deep learning, the increasing number of pre-trained models\navailable online presents a wealth of knowledge. These models, developed with\ndiverse architectures and trained on varied datasets for different tasks,\nprovide unique interpretations of the real world. Their collective consensus is\nlikely universal and generalizable to unseen data. However, effectively\nharnessing this collective knowledge poses a fundamental challenge due to the\nheterogeneity of pre-trained models. Existing knowledge integration solutions\ntypically rely on strong assumptions about training data distributions and\nnetwork architectures, limiting them to learning only from specific types of\nmodels and resulting in data and/or inductive biases. In this work, we\nintroduce a novel framework, namely UNIFORM, for knowledge transfer from a\ndiverse set of off-the-shelf models into one student model without such\nconstraints. Specifically, we propose a dedicated voting mechanism to capture\nthe consensus of knowledge both at the logit level -- incorporating teacher\nmodels that are capable of predicting target classes of interest -- and at the\nfeature level, utilizing visual representations learned on arbitrary label\nspaces. Extensive experiments demonstrate that UNIFORM effectively enhances\nunsupervised object recognition performance compared to strong knowledge\ntransfer baselines. Notably, it exhibits remarkable scalability by benefiting\nfrom over one hundred teachers, while existing methods saturate at a much\nsmaller scale.", "AI": {"tldr": "本文提出了一种名为UNIFORM的新框架，用于从多样化的预训练模型中无约束地转移知识到一个学生模型中，通过一种专门的投票机制在logit和特征层面捕捉知识的一致性，实验表明该方法有效提升了无监督物体识别性能。", "motivation": "在深度学习时代，越来越多的预训练模型在线可用，这些模型具有不同的架构并针对不同的任务进行了训练，它们提供了对现实世界的独特解释。然而，这些模型的异质性使得有效利用它们的知识成为了一个基本挑战。现有的知识整合解决方案通常依赖于关于训练数据分布和网络架构的强假设，这限制了它们只能从特定类型的模型中学习，导致数据和/或归纳偏见。因此，本文旨在提出一种新的方法来解决这个问题。", "method": "本文提出了一种称为UNIFORM的新框架，用于从多样化的现成模型中将知识转移到一个学生模型中，而不受现有方法的约束。具体来说，作者提出了一种专门的投票机制，在logit层面（包括能够预测目标类别的教师模型）和特征层面（利用任意标签空间学习的视觉表示）来捕捉知识的一致性。", "result": "广泛的实验表明，与强大的知识转移基线相比，UNIFORM可以有效提升无监督对象识别的性能。特别地，它展示了显著的可扩展性，通过利用超过一百个教师模型获得收益，而现有方法在较小规模时就趋于饱和。", "conclusion": "UNIFORM框架能够有效解决将多样化预训练模型的知识转移到一个学生模型中的挑战，它通过一种专门的投票机制在logit和特征层面捕捉知识的一致性，从而在无监督物体识别性能上提升了性能并展现出显著的可扩展性。"}}
{"id": "2508.19533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19533", "abs": "https://arxiv.org/abs/2508.19533", "authors": ["Kun Peng", "Cong Cao", "Hao Peng", "Guanlin Wu", "Zhifeng Hao", "Lei Jiang", "Yanbing Liu", "Philip S. Yu"], "title": "Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation", "comment": "Accepted at EMNLP2025", "summary": "Current Emotion Recognition in Conversation (ERC) research follows a\nclosed-domain assumption. However, there is no clear consensus on emotion\nclassification in psychology, which presents a challenge for models when it\ncomes to recognizing previously unseen emotions in real-world applications. To\nbridge this gap, we introduce the Unseen Emotion Recognition in Conversation\n(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based\nemotion transfer framework. This prototype-based approach shows promise but\nstill faces key challenges: First, implicit expressions complicate emotion\ndefinition, which we address by proposing an LLM-enhanced description approach.\nSecond, utterance encoding in long conversations is difficult, which we tackle\nwith a proposed parameter-free mechanism for efficient encoding and overfitting\nprevention. Finally, the Markovian flow nature of emotions is hard to transfer,\nwhich we address with an improved Attention Viterbi Decoding (AVD) method to\ntransfer seen emotion transitions to unseen emotions. Extensive experiments on\nthree datasets show that our method serves as a strong baseline for preliminary\nexploration in this new area.", "AI": {"tldr": "研究提出了一种新的情感识别任务并开发了基于原型框架的解决方案，该框架在实验中证明了其在未见情感识别中的有效性。", "motivation": "当前的情感识别研究大多基于封闭领域假设，但在心理学中情感分类并没有明确的共识，这给模型识别新情感带来了挑战。本研究旨在解决这一问题。", "method": "本研究首次提出了对话中的未见情感识别（URC）任务，并提出了基于原型的情感转换框架ProEmoTrans。该框架通过LLM增强描述方法解决隐含表情复杂的问题，通过无参数机制解决长对话中的话语编码难题，使用改进的注意力维特比解码（AVD）方法解决情感转移问题。", "result": "在三个数据集上的广泛实验表明，该方法为这一新领域的初步探索提供了一个强有力的基础。", "conclusion": "研究提出的方法为解决情感识别中的开放域问题提供了一个新的方向和潜在解决方案。"}}
{"id": "2508.19499", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19499", "abs": "https://arxiv.org/abs/2508.19499", "authors": ["Xiangxu Wang", "Tianhong Zhao", "Wei Tu", "Bowen Zhang", "Guanzhou Chen", "Jinzhou Cao"], "title": "Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery", "comment": null, "summary": "Origin-Destination (OD) flow matrices are essential for urban mobility\nanalysis, underpinning applications in traffic forecasting, infrastructure\nplanning, and policy design. However, existing methods suffer from two critical\nlimitations: (1) reliance on auxiliary features (e.g., Points of Interest,\nsocioeconomic statistics) that are costly to collect and have limited spatial\ncoverage; and (2) sensitivity to spatial topology, where minor index reordering\nof urban regions (e.g., census tract relabeling) disrupts structural coherence\nin generated flows. To address these challenges, we propose Sat2Flow, a latent\nstructure-aware diffusion-based framework that generates structurally coherent\nOD flows using solely satellite imagery as input. Our approach introduces a\nmulti-kernel encoder to capture diverse regional interactions and employs a\npermutation-aware diffusion process that aligns latent representations across\ndifferent regional orderings. Through a joint contrastive training objective\nthat bridges satellite-derived features with OD patterns, combined with\nequivariant diffusion training that enforces structural consistency, Sat2Flow\nensures topological robustness under arbitrary regional reindexing.\nExperimental results on real-world urban datasets demonstrate that Sat2Flow\noutperforms both physics-based and data-driven baselines in numerical accuracy\nwhile preserving empirical distributions and spatial structures under index\npermutations. Sat2Flow offers a globally scalable solution for OD flow\ngeneration in data-scarce urban environments, eliminating region-specific\nauxiliary data dependencies while maintaining structural invariance for robust\nmobility modeling.", "AI": {"tldr": "Sat2Flow是一种基于卫星图像的OD流生成框架，可以生成结构一致的OD流，并且在不同的城市区域有序重排下保持结构的一致性和鲁棒性。", "motivation": "现有方法在生成OD流动时依赖于辅助特征（例如兴趣点、社会经济统计数据）且对空间拓扑敏感。Sat2Flow旨在消除这些限制，仅依靠卫星图像生成结构上一致的OD流，并确保拓扑结构的鲁棒性。", "method": "Sat2Flow采用多核编码器来捕捉不同区域之间的互动，并使用一种对排列敏感的扩散过程来对齐不同区域排列下的潜在表示。通过结合来自卫星图像的特征和OD流动模式的联合对比训练目标以及等变扩散训练，确保在任意区域重索引下拓扑结构的鲁棒性。", "result": "实验结果表明，Sat2Flow在真实世界的城市数据集上，无论在数值准确性还是在索引排列下的分布和空间结构保留方面，都优于基于物理的方法和数据驱动的基线方法。", "conclusion": "Sat2Flow提供了一种在数据稀缺的城市环境中全球可扩展的OD流生成解决方案，无需依赖地区特定的辅助数据，同时保持结构不变性，适合于稳健的流动性建模。"}}
{"id": "2508.19546", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19546", "abs": "https://arxiv.org/abs/2508.19546", "authors": ["Jio Choi", "Mohit Bansal", "Elias Stengel-Eskin"], "title": "Language Models Identify Ambiguities and Exploit Loopholes", "comment": "EMNLP 2025 camera-ready; Code:\n  https://github.com/esteng/ambiguous-loophole-exploitation", "summary": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals.", "AI": {"tldr": "研究大型语言模型对漏洞的反应，发现在面对冲突目标时模型能识别并利用歧义，揭示了潜在的AI安全风险。", "motivation": "研究大型语言模型对漏洞的反应提供了两个机遇：考察LLMs中的歧义和语用推理；并提出一个对齐问题，即模型面临冲突目标时利用歧义为自己谋利。", "method": "设计了包含量词隐涵、结构二义性和权力动态的场景，让大型语言模型（LLMs）在有矛盾用户指令的情况下实现目标，并测量它们利用漏洞满足其目标的能力。", "result": "发现闭源和较强开源模型可以识别歧义并利用其产生的漏洞，这可能构成AI安全风险。", "conclusion": "分析表明，利用漏洞的模型能够明确识别和推理歧义及冲突目标。"}}
{"id": "2508.19511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19511", "abs": "https://arxiv.org/abs/2508.19511", "authors": ["Alzayat Saleh", "Shunsuke Hatano", "Mostafa Rahimi Azghadi"], "title": "Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity", "comment": "19 pages, 10 figures, 6 tables", "summary": "The automated management of invasive weeds is critical for sustainable\nagriculture, yet the performance of deep learning models in real-world fields\nis often compromised by two factors: challenging environmental conditions and\nthe high cost of data annotation. This study tackles both issues through a\ndiagnostic-driven, semi-supervised framework. Using a unique dataset of\napproximately 975 labeled and 10,000 unlabeled images of Guinea Grass in\nsugarcane, we first establish strong supervised baselines for classification\n(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and\nmAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by\ninterpretability tools, uncovered a pervasive \"shadow bias,\" where models\nlearned to misidentify shadows as vegetation. This diagnostic insight motivated\nour primary contribution: a semi-supervised pipeline that leverages unlabeled\ndata to enhance model robustness. By training models on a more diverse set of\nvisual information through pseudo-labeling, this framework not only helps\nmitigate the shadow bias but also provides a tangible boost in recall, a\ncritical metric for minimizing weed escapes in automated spraying systems. To\nvalidate our methodology, we demonstrate its effectiveness in a low-data regime\non a public crop-weed benchmark. Our work provides a clear and field-tested\nframework for developing, diagnosing, and improving robust computer vision\nsystems for the complex realities of precision agriculture.", "AI": {"tldr": "本研究提出了一种针对复杂实际农业环境中的入侵杂草管理问题的诊断驱动半监督框架，提升了模型的鲁棒性和关键指标召回率，以提高自动化喷洒系统的效率。", "motivation": "研究的动机在于解决自动化管理入侵杂草的问题，这是可持续农业的关键。目前，深度学习模型在现实农田中的表现经常受到挑战性环境条件和高成本数据标注的影响。", "method": "本研究通过诊断驱动的半监督框架解决了环境条件挑战和高数据标注成本的问题。使用了一组独特的数据集，包括约975个标注图象和10,000个非标注图象，包含甘蔗中的 Guinea Grass 图象。首先建立了分类（ResNet）和检测（YOLO, RF-DETR）的监督基准线，实现了0.90的F1分数和0.82以上的mAP50分数。通过解释工具发现了一个普遍的“影子偏差”，即模型学会了将阴影误认为植被。为了解决这一问题，提出了一个利用非标注数据提升模型鲁棒性的半监督流水线。通过伪标注训练模型，提高了模型的召回率，这对于减少自动化喷洒系统中的杂草逃脱至关重要。", "result": "本研究实现了高达0.90的F1分数和超过0.82的mAP50分数，并发现并解决了模型的“影子偏差”问题。研究展示了该方法在公共作物-杂草基准测试中的有效性，特别是在低数据量下的表现。", "conclusion": "研究提供了一个明确且经过实地测试的框架，用于开发、诊断和改善复杂的现实农业中的计算机视觉系统。这为提升精准农业中的模型鲁棒性和召回率提供了新的途径。"}}
{"id": "2508.19578", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19578", "abs": "https://arxiv.org/abs/2508.19578", "authors": ["Jiaqi Deng", "Yuho Lee", "Nicole Hee-Yeon Kim", "Hyangsuk Min", "Taewon Yun", "Minjeong Ban", "Kim Yul", "Hwanjun Song"], "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts", "comment": "Accepted to EMNLP 2025 (Main)", "summary": "We introduce HAMLET, a holistic and automated framework for evaluating the\nlong-context comprehension of large language models (LLMs). HAMLET structures\nsource texts into a three-level key-fact hierarchy at root-, branch-, and\nleaf-levels, and employs query-focused summarization to evaluate how well\nmodels recall and faithfully represent information at each level. To validate\nthe reliability of our fully automated pipeline, we conduct a systematic human\nstudy, showing that our automatic evaluation achieves over 90% agreement with\nexpert human judgments, while reducing the cost by up to 25 times. HAMLET\nreveals that LLMs struggle with fine-grained comprehension, especially at the\nleaf level, and are sensitive to positional effects like the\nlost-in-the-middle. Analytical queries pose greater challenges than narrative\nones, and consistent performance gaps emerge between open-source and\nproprietary models, as well as across model scales. Our code and dataset are\npublicly available at https://github.com/DISL-Lab/HAMLET.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19527", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19527", "abs": "https://arxiv.org/abs/2508.19527", "authors": ["Zhiting Gao", "Dan Song", "Diqiong Jiang", "Chao Xue", "An-An Liu"], "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment", "comment": "11 pages, 5 figures", "summary": "Motion generation is essential for animating virtual characters and embodied\nagents. While recent text-driven methods have made significant strides, they\noften struggle with achieving precise alignment between linguistic descriptions\nand motion semantics, as well as with the inefficiencies of slow, multi-step\ninference. To address these issues, we introduce TMR++ Aligned Preference\nOptimization (TAPO), an innovative framework that aligns subtle motion\nvariations with textual modifiers and incorporates iterative adjustments to\nreinforce semantic grounding. To further enable real-time synthesis, we propose\nMotionFLUX, a high-speed generation framework based on deterministic rectified\nflow matching. Unlike traditional diffusion models, which require hundreds of\ndenoising steps, MotionFLUX constructs optimal transport paths between noise\ndistributions and motion spaces, facilitating real-time synthesis. The\nlinearized probability paths reduce the need for multi-step sampling typical of\nsequential methods, significantly accelerating inference time without\nsacrificing motion quality. Experimental results demonstrate that, together,\nTAPO and MotionFLUX form a unified system that outperforms state-of-the-art\napproaches in both semantic consistency and motion quality, while also\naccelerating generation speed. The code and pretrained models will be released.", "AI": {"tldr": "提出TAPO和MotionFLUX框架，提高动画中运动生成的语义一致性、运动质量和实时生成速度。", "motivation": "提升虚拟角色和具身代理的动画效果，解决现有文本驱动方法在语义对齐上存在的问题，以及多步骤推理低效的问题。", "method": "TAPO框架结合MotionFLUX生成框架，前者通过与文本描述的精确对齐和迭代调整来增强语义基础，后者基于确定性校正流匹配实现高速实时合成。", "result": "实验结果表明，TAPO和MotionFLUX组成的统一系统在语义一致性、运动质量上超越现有最先进技术，同时加速了生成速度。", "conclusion": "TAPO和MotionFLUX联合框架能够有效提升虚拟角色的动作生成效果，实现了高效的实时合成，交互性和运动质量得到显著改进。"}}
{"id": "2508.19580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19580", "abs": "https://arxiv.org/abs/2508.19580", "authors": ["Omkar Gurjar", "Agam Goyal", "Eshwar Chandrasekharan"], "title": "ArgCMV: An Argument Summarization Benchmark for the LLM-era", "comment": null, "summary": "Key point extraction is an important task in argument summarization which\ninvolves extracting high-level short summaries from arguments. Existing\napproaches for KP extraction have been mostly evaluated on the popular ArgKP21\ndataset. In this paper, we highlight some of the major limitations of the\nArgKP21 dataset and demonstrate the need for new benchmarks that are more\nrepresentative of actual human conversations. Using SoTA large language models\n(LLMs), we curate a new argument key point extraction dataset called ArgCMV\ncomprising of around 12K arguments from actual online human debates spread\nacross over 3K topics. Our dataset exhibits higher complexity such as longer,\nco-referencing arguments, higher presence of subjective discourse units, and a\nlarger range of topics over ArgKP21. We show that existing methods do not adapt\nwell to ArgCMV and provide extensive benchmark results by experimenting with\nexisting baselines and latest open source models. This work introduces a novel\nKP extraction dataset for long-context online discussions, setting the stage\nfor the next generation of LLM-driven summarization research.", "AI": {"tldr": "本文提出了一个新的从在线人类辩论中提取关键点的数据集ArgCMV，该数据集包含约12K条论据，覆盖超过3K个主题。相较于现有的数据集ArgKP21，ArgCMV展现了更高的复杂性，包括较长、有指代关系的论据，更多的主观对话单元，以及主题范围更广。本文通过实验展示了现有方法在ArgCMV数据集上的表现不佳，为进一步的研究奠定了基础。", "motivation": "旨在解决现有ArgKP21数据集在代表真实人类对话方面存在的限制，提出并验证一个更为复杂和全面的数据集ArgCMV，推动关键点提取任务的发展。", "method": "使用最先进的大语言模型（LLMs）整理了ArgCMV数据集，并在该数据集上测试了多种现有的基线方法和最新的开源模型。", "result": "研究表明，现有的关键点提取方法在新的ArgCMV数据集上没有达到很好的效果。", "conclusion": "本文提出的ArgCMV数据集为长上下文在线讨论中的关键点提取提供了重要的研究资源，推进了基于LLM的总结研究的下一代发展。"}}
{"id": "2508.19542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19542", "abs": "https://arxiv.org/abs/2508.19542", "authors": ["Nannan Zhu", "Yonghao Dong", "Teng Wang", "Xueqian Li", "Shengjun Deng", "Yijia Wang", "Zheng Hong", "Tiantian Geng", "Guo Niu", "Hanyan Huang", "Xiongfei Yao", "Shuaiwei Jiao"], "title": "CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning", "comment": null, "summary": "While multimodal large language models (MLLMs) exhibit strong performance on\nsingle-video tasks (e.g., video question answering), their ability across\nmultiple videos remains critically underexplored. However, this capability is\nessential for real-world applications, including multi-camera surveillance and\ncross-video procedural learning. To bridge this gap, we present CVBench, the\nfirst comprehensive benchmark designed to assess cross-video relational\nreasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning\nthree hierarchical tiers: cross-video object association (identifying shared\nentities), cross-video event association (linking temporal or causal event\nchains), and cross-video complex reasoning (integrating commonsense and domain\nknowledge). Built from five domain-diverse video clusters (e.g., sports, life\nrecords), the benchmark challenges models to synthesise information across\ndynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including\nGPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought\nprompting paradigms. Key findings reveal stark performance gaps: even top\nmodels, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,\ncompared to the 91% accuracy of human performance. Crucially, our analysis\nreveals fundamental bottlenecks inherent in current MLLM architectures, notably\ndeficient inter-video context retention and poor disambiguation of overlapping\nentities. CVBench establishes a rigorous framework for diagnosing and advancing\nmulti-video reasoning, offering architectural insights for next-generation\nMLLMs.The data and evaluation code are available at\nhttps://github.com/Hokhim2/CVBench.", "AI": {"tldr": "CVBench是首个评估多视频关系推理能力的全面基准，发现尽管大型多模态语言模型在单一视频任务中表现出色，但在处理多视频问题时存在显著性能差距，尤其是在跨视频因果推理任务中的表现。", "motivation": "当前多模态大型语言模型在多视频任务中的能力未得到充分探索，但这种能力对于实现实用场景至关重要，如多摄像头监控和跨视频程序性学习。", "method": "CVBench包含1000个问题回答对，涵盖三个层次：跨视频对象关联、跨视频事件关联和跨视频复杂推理。从五个不同领域的视频集群构建，挑战模型在动态视觉上下文中的信息合成能力。", "result": "对10种以上领先MLLM模型（包括GPT-4o、Gemini-2.0-flash、Qwen2.5-VL）进行评估，发现顶级模型如GPT-4o在因果推理任务上仅达到约60%的准确率，明显低于91%的人类效果。", "conclusion": "这些结果揭示了现有MLLM架构的基本瓶颈，如跨视频内容保留不足和重叠实体区分不佳。CVBench为多视频推理的能力诊断和下一代MLLM的架构改进提供了框架。"}}
{"id": "2508.19587", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19587", "abs": "https://arxiv.org/abs/2508.19587", "authors": ["Hadi Zaatiti", "Hatem Hajri", "Osama Abdullah", "Nader Masmoudi"], "title": "Towards stable AI systems for Evaluating Arabic Pronunciations", "comment": null, "summary": "Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and\nsentence-level transcription, yet struggle to classify isolated letters. In\nthis study, we show that this phoneme-level task, crucial for language\nlearning, speech therapy, and phonetic research, is challenging because\nisolated letters lack co-articulatory cues, provide no lexical context, and\nlast only a few hundred milliseconds. Recogniser systems must therefore rely\nsolely on variable acoustic cues, a difficulty heightened by Arabic's emphatic\n(pharyngealized) consonants and other sounds with no close analogues in many\nlanguages. This study introduces a diverse, diacritised corpus of isolated\nArabic letters and demonstrates that state-of-the-art wav2vec 2.0 models\nachieve only 35% accuracy on it. Training a lightweight neural network on\nwav2vec embeddings raises performance to 65%. However, adding a small amplitude\nperturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we\napply adversarial training, limiting the noisy-speech drop to 9% while\npreserving clean-speech accuracy. We detail the corpus, training pipeline, and\nevaluation protocol, and release, on demand, data and code for reproducibility.\nFinally, we outline future work extending these methods to word- and\nsentence-level frameworks, where precise letter pronunciation remains critical.", "AI": {"tldr": "研究展示了wav2vec 2.0模型在孤立阿拉伯字母识别上的困难，并提出通过轻量级神经网络和对抗性训练方法提高了识别的准确性和鲁棒性。", "motivation": "本研究旨在解决现代阿拉伯ASR系统在分类孤立字母方面存在的困难，这对于语言学习、语音治疗以及语音学研究至关重要的音素级别任务具有挑战性，因为孤立字母缺乏协同发音线索、提供的词汇上下文很少且持续时间很短。", "method": "在本研究中，我们引入了多样化的标音孤立阿拉伯字母语料库，并展示了最先进wav2vec 2.0模型对于孤立字母的识别准确度仅为35%。通过在wav2vec嵌入上训练一个轻量级神经网络，性能提高到65%。为了提高鲁棒性，我们应用了对抗性训练，限制了噪声语音的准确度下降至9%，同时保持了干净语音的准确性。", "result": "研究结果表明，通过在wav2vec嵌入上训练轻量级神经网络，性能从原来的35%提高到65%。然而，引入一个小的幅度扰动会使准确度下降到32%。应用对抗性训练后，在保持清晰语音准确性的同时，噪声语音的准确度下降被限制在9%以内。", "conclusion": "研究详细介绍了语料库、训练流程及评估标准，并表示在未来的研究中将把该方法扩展到字词及句子级别框架中，以确保精确的字母发音。"}}
{"id": "2508.19544", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19544", "abs": "https://arxiv.org/abs/2508.19544", "authors": ["Eduardo Davalos", "Yike Zhang", "Namrata Srivastava", "Yashvitha Thatigotla", "Jorge A. Salas", "Sara McFadden", "Sun-Joo Cho", "Amanda Goodwin", "Ashwin TS", "Gautam Biswas"], "title": "WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization", "comment": "9 pages, 7 figures, 1 table", "summary": "With advancements in AI, new gaze estimation methods are exceeding\nstate-of-the-art (SOTA) benchmarks, but their real-world application reveals a\ngap with commercial eye-tracking solutions. Factors like model size, inference\ntime, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking\nmethods lack sufficient accuracy, in particular due to head movement. To tackle\nthese issues, we introduce We bEyeTrack, a framework that integrates\nlightweight SOTA gaze estimation models directly in the browser. It\nincorporates model-based head pose estimation and on-device few-shot learning\nwith as few as nine calibration samples (k < 9). WebEyeTrack adapts to new\nusers, achieving SOTA performance with an error margin of 2.32 cm on\nGazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.\nOur open-source code is available at\nhttps://github.com/RedForestAi/WebEyeTrack.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19594", "abs": "https://arxiv.org/abs/2508.19594", "authors": ["Jun Bai", "Minghao Tong", "Yang Liu", "Zixia Jia", "Zilong Zheng"], "title": "Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs", "comment": "Accepted by EMNLP 2025 Main", "summary": "Context faithfulness is essential for reliable reasoning in context-dependent\nscenarios. However, large language models often struggle to ground their\noutputs in the provided context, resulting in irrelevant responses. Inspired by\nthe emergent expert specialization observed in mixture-of-experts\narchitectures, this work investigates whether certain experts exhibit\nspecialization in context utilization, offering a potential pathway toward\ntargeted optimization for improved context faithfulness. To explore this, we\npropose Router Lens, a method that accurately identifies context-faithful\nexperts. Our analysis reveals that these experts progressively amplify\nattention to relevant contextual information, thereby enhancing context\ngrounding. Building on this insight, we introduce Context-faithful Expert\nFine-Tuning (CEFT), a lightweight optimization approach that selectively\nfine-tunes context-faithful experts. Experiments across a wide range of\nbenchmarks and models demonstrate that CEFT matches or surpasses the\nperformance of full fine-tuning while being significantly more efficient.", "AI": {"tldr": "本研究提出通过优化上下文忠实专家来提升大型语言模型的上下文忠实性，方法包括Router Lens和CEFT。结果证明，该方法可以匹配全微调的性能，同时更省时。", "motivation": "大型语言模型往往难以将其输出与给定的上下文相吻合，这导致了不相关的响应。这项工作的动机是研究是否可以通过优化能够更好地利用上下文的专家来提高上下文忠实度。", "method": "Router Lens 方法被提出以准确识别忠实于上下文的专家，并通过专注于这些专家进行轻量级优化，称为上下文忠实专家微调（CEFT）。", "result": "实验表明，CEFT 在各种基准测试和模型中与全微调表现相当或更好，同时效率更高。", "conclusion": "通过专注于提升上下文忠实专家的性能，可以有效提高整体模型的上下文忠实性，且保持更高的效率。"}}
{"id": "2508.19555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19555", "abs": "https://arxiv.org/abs/2508.19555", "authors": ["Yu-Wei Zhang", "Tongju Han", "Lipeng Gao", "Mingqiang Wei", "Hui Liu", "Changbao Li", "Caiming Zhang"], "title": "MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery", "comment": null, "summary": "This paper presents MonoRelief V2, an end-to-end model designed for directly\nrecovering 2.5D reliefs from single images under complex material and\nillumination variations. In contrast to its predecessor, MonoRelief V1 [1],\nwhich was solely trained on synthetic data, MonoRelief V2 incorporates real\ndata to achieve improved robustness, accuracy and efficiency. To overcome the\nchallenge of acquiring large-scale real-world dataset, we generate\napproximately 15,000 pseudo real images using a text-to-image generative model,\nand derive corresponding depth pseudo-labels through fusion of depth and normal\npredictions. Furthermore, we construct a small-scale real-world dataset (800\nsamples) via multi-view reconstruction and detail refinement. MonoRelief V2 is\nthen progressively trained on the pseudo-real and real-world datasets.\nComprehensive experiments demonstrate its state-of-the-art performance both in\ndepth and normal predictions, highlighting its strong potential for a range of\ndownstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.", "AI": {"tldr": "MonoRelief V2是一种直接从单张图像中恢复2.5D浮雕的端到端模型，它通过将真实数据与合成数据一起用于训练，进一步提高了模型的鲁棒性和性能。", "motivation": "为了提升从前作MonoRelief V1中仅使用合成数据训练的模型到在复杂材质和照明变化下的直接恢复2.5D浮雕的能力，引入真实数据训练以解决数据集获取的难题。", "method": "MonoRelief V2结合真实数据训练，克服了获取大规模真实世界数据集的挑战。通过使用文本到图像生成模型生成约15000张伪真实图像，并通过深度和法线预测融合生成相应的深度伪标签。同时，通过多视图重建和细节改进构建了一个小规模的真实世界数据集（800个样本）。MonoRelief V2在伪真实和真实世界数据集上进行渐进训练。", "result": "实验表明，MonoRelief V2在深度和法线预测方面都达到了最先进的性能，展示了其在一系列下游应用中的强大力量。", "conclusion": "MonoRelief V2通过利用真实数据来加强训练过程，显著提高了2.5D浮雕从单一图像中恢复的鲁棒性、准确性和效率，并展现出其在广泛应用中的巨大潜力。"}}
{"id": "2508.19614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19614", "abs": "https://arxiv.org/abs/2508.19614", "authors": ["Yang Sun", "Lixin Zou", "Dan Luo", "Zhiyong Xie", "Long Zhang", "Liming Dong", "Yunwei Zhao", "Xixun Lin", "Yanxiong Lu", "Chenliang Li"], "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) incorporates external knowledge into\nlarge language models (LLMs), improving their adaptability to downstream tasks\nand enabling information updates. Surprisingly, recent empirical evidence\ndemonstrates that injecting noise into retrieved relevant documents\nparadoxically facilitates exploitation of external knowledge and improves\ngeneration quality. Although counterintuitive and challenging to apply in\npractice, this phenomenon enables granular control and rigorous analysis of how\nLLMs integrate external knowledge. Therefore, in this paper, we intervene on\nnoise injection and establish a layer-specific functional demarcation within\nthe LLM: shallow layers specialize in local context modeling, intermediate\nlayers focus on integrating long-range external factual knowledge, and deeper\nlayers primarily rely on parametric internal knowledge. Building on this\ninsight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that\ndirectly combines representations from an intermediate layer with final-layer\ndecoding outputs to fully exploit the external factual knowledge. To identify\nthe optimal intermediate layer, we introduce an internal knowledge score (IKS)\ncriterion that selects the layer with the lowest IKS value in the latter half\nof layers. Experimental results across multiple benchmarks demonstrate that LFD\nhelps RAG systems more effectively surface retrieved context knowledge with\nminimal cost.", "AI": {"tldr": "本文揭示了在检索增强生成系统（RAG）中，向检索文档加入噪声，能够提高文本生成质量。研究提出了层融合解码（LFD）策略，通过结合中间层的知识表示和最终层的输出来优化外部知识的利用。这种方法在多个实验中表现优异。", "motivation": "研究目的是探讨在大语言模型中检索增强生成技术（RAG）如何通过整合外部知识来提高其适应性，并优化信息更新能力。", "method": "通过引入噪声到检索的相关文档中，作者分析了大语言模型（LLMs）如何整合外部知识，并提出了一种名为层融合解码（LFD）的新策略。LFD直接结合中间层的表示与最终层的解码输出，以充分利用外部事实知识。为了找到最佳的中间层，他们引入了一个内部知识评分（IKS）准则。", "result": "实验结果在多个基准测试中表明，LFD帮助RAG系统更有效地呈现检索到的上下文知识，并且成本极低。", "conclusion": "研究结论是，通过噪声注入来干预，可以实现对LLMs整合外部知识过程的精细控制和深入分析，并验证了提出的LFD策略能更高效地利用外部知识。"}}
{"id": "2508.19565", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.19565", "abs": "https://arxiv.org/abs/2508.19565", "authors": ["Yuhang Zhao", "Zixing Wang"], "title": "FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection", "comment": "Accepted by PRCV 2025. Project page with code and dataset:\n  https://github.com/AstronZh/Intersection-Flow-5K", "summary": "End-to-end object detectors offer a promising NMS-free paradigm for real-time\napplications, yet their high computational cost remains a significant barrier,\nparticularly for complex scenarios like intersection traffic monitoring. To\naddress this challenge, we propose FlowDet, a high-speed detector featuring a\ndecoupled encoder optimization strategy applied to the DETR architecture.\nSpecifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for\ntraffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to\nmaintain high representational power across extreme scale variations. To\nrigorously evaluate the model's performance in environments with severe\nocclusion and high object density, we collected the Intersection-Flow-5k\ndataset, a new challenging scene for this task. Evaluated on\nIntersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to\nthe strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by\n1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference\nspeed by 16.2%. Our work demonstrates a new path towards building highly\nefficient and accurate detectors for demanding, real-world perception systems.\nThe Intersection-Flow-5k dataset is available at\nhttps://github.com/AstronZh/Intersection-Flow-5K.", "AI": {"tldr": "FlowDet相较于RT-DETR，在Intersection-Flow-5k数据集上AP(test)提升1.5%，AP50(test)提升1.6%，GFLOPs减少63.2%，平均成绩大大改善推理速度6.2%。", "motivation": "通过FlowDet首先解决极端计算单元消耗问题，为实现平均成绩在高且密度数据情况下做好资源平分化；提高应用在新型模型实施配置的面对真实世界的高分辨率任务。", "method": "Structure", "result": "{\"tldr\": \"\\u7531\\u65b9\\u63d0\\u51fa\\u4e86FlowDet\\u672c\\u4eba\\u6a21\\u578b\\uff0c\\u6bd4\\u8d5b\\u4e8ERR-DETR\\uff0c\\u5728Intersection-Flow-5k\\u6570\\u636e\\u96c6\\u4e0aAP(test)\\u53d8\\u52a81.5%\\uff0cAP50(test)\\u53d8\\u52a81.6%\\uff0cGFLOPs\\u7ed9\\u4e0b\\u964d\\u901263.2%\\uff0c\\u5e73\\u5747\\u7ed3\\u679c\\u5927\\u5927\\u53d8\\u5fkd8\\u901f\\u5ea66.2%\\u3002\", \"motivation\": \"\\u7528FlowDet\\u5148\\u89e3\\u51b3\\u7ee7\\u6781\\u8d85\\u7ba1\\u80a1\\u7684\\u9ad8\\u8ba1\\u7b97\\u5143\\u7ec4\\u6027\\u529b\\uff0c\\u4e3a\\u73b0\\u5b9e\\u4e00\\u4e0b\\u5fkd8\\u6570\\u636e\\u96c6\\u5916\\u4e00\\u5b9a\\u5bf9\\u5e73\\u5747\\u7ed3\\u679c\\u8fdb\\u884c\\u9ad8\\u6548\\u7387\\u53ca\\u7b80\\u5316\\u7684\\u8d44\\u6e90\\u5e73\\u5206\\u5224\\u4e0d\\uff1b\\u63d0\\u9ad8\\u5e94\\u7528\\u5728\\u9ad8\\u65b0\\u578b\\u4f26\\u529b\\u7b49\\u5b9e\\u9a8c\\u5b58\\u5728\\u7684\\u9ad8\\u5cf0\\u5143\\u5143\\u6574\\u7684\\u9ad8\\u89e3\\u529b\\u3002\", \"method\": \"\\u5b9e\\u9a8cFlowDet\\u7528\\u529f\\u673a\\u6a21\\u578b\\u7684DETR\\uff0c\\u4f7f\\u7528\\u5e26\\u6709\\u4e00\\u79cd\\u65b0\\u578b\\u529f\\u673a\\u4f5c\\u4e3aGDU(\\u8c03\\u7528\\u8d8b\\u52a8\\u5143\\u4ef6)\\u7684Scale-Aware\\u6ce8\\uff1b\\u5b9e\\u9a8c\\u5728Intersection-Flow-5k\\u6570\\u636e\\u96c6\\u4e0awithy\\u7ecf\\u5178\\u578b\\u7b56\\u7565\\u4f8bDETErr\\u5bf9\\u6bd4\\u540e\\u66f4\\u65b0\\u7684\\u529f\\u80fd\\u5e73\\u5747\\u7ed3\\u679c\\u8def\\u5fbd\\u6a21\\u578b\\u7b80\\u5316\\u7684\\u65b0\\u8d44\\u6e90\\u5e73\\u5206\\u5224\\u711c\\u529f\\uff0c\", \"result\": \"FlowDet\\u6bd4\\u8d5b\\u4e8ERR-DETR\\uff0c\\u5728Intersection-Flow-5k\\u6570\\u636e\\u96c6\\u4e0aAP(test)\\u53d8\\u52a81.5%\\uff0cAP50(test)\\u53d8\\u52a81.6%\\uff0cGFLOPs\\u7ed9\\u4e0b\\u964d\\u901263.2%\\uff0c\\u5e73\\u5747\\u7ed3\\u679c\\u5927\\u5927\\u53d8\\u5fkd8\\u901f\\u5ea66.2%\\u3002\", \"conclusion\": \"\\u5e94\\u7528FlowDet\\u5728\\u6b63\\u596f\\u8001\\u662f\\u7279\\u522b\\u5728\\u73b0\\u5b9e\\u4e00\\u4e0b\\u5fkd8\\u901f\\u5ea6\\u5e94\\u7528\\u91cf\\u91cf\\u7684\\u5b9e\\u9a8c\\u5b58\\u5728\\u7684\\u9ad8\\u5cf0\\u5143\\u5143\\u6574\\u7684\\u9ad8\\u89e3\\u529b\\uff0c\\u4f7f\\u7528\\u5728\\u529f\\u673a\\u6a21\\u578b\\u9ad8\\u529f\\u80fd\\u7684\\u5b9e\\u9a8c\\u53ef\\u975e\\u4e00\\u5b9a\\u89c1\\u3002\"}", "conclusion": "在实际高分辨率应用实验中，大量采用FlowDet会使资源更加平分和提高计算单元与资源性能的提高有着极大的实际作用。"}}
{"id": "2508.19633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19633", "abs": "https://arxiv.org/abs/2508.19633", "authors": ["Chong Tian", "Qirong Ho", "Xiuying Chen"], "title": "A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Rapid LLM advancements heighten fake news risks by enabling the automatic\ngeneration of increasingly sophisticated misinformation. Previous detection\nmethods, including fine-tuned small models or LLM-based detectors, often\nstruggle with its dynamically evolving nature. In this work, we propose a novel\nframework called the Symbolic Adversarial Learning Framework (SALF), which\nimplements an adversarial training paradigm by an agent symbolic learning\noptimization process, rather than relying on numerical updates. SALF introduces\na paradigm where the generation agent crafts deceptive narratives, and the\ndetection agent uses structured debates to identify logical and factual flaws\nfor detection, and they iteratively refine themselves through such adversarial\ninteractions. Unlike traditional neural updates, we represent agents using\nagent symbolic learning, where learnable weights are defined by agent prompts,\nand simulate back-propagation and gradient descent by operating on natural\nlanguage representations of weights, loss, and gradients. Experiments on two\nmultilingual benchmark datasets demonstrate SALF's effectiveness, showing it\ngenerates sophisticated fake news that degrades state-of-the-art detection\nperformance by up to 53.4% in Chinese and 34.2% in English on average. SALF\nalso refines detectors, improving detection of refined content by up to 7.7%.\nWe hope our work inspires further exploration into more robust, adaptable fake\nnews detection systems.", "AI": {"tldr": "提出SALF框架，采用符号代理学习对抗训练范式，可生成复杂假新闻并改进现有检测能力。", "motivation": "由于大语言模型的快速发展，假新闻风险提高。以往的检测方法，包含微调小型模型或基于大语言模型的检测器，常常难以应对不断变化的假新闻。因此，提出SALF框架，期望其能够对不断进化性质的假新闻采用更有效的方法进行检测。", "method": "SALF采用对抗训练范式，通过代理符号学习优化过程实现，不依赖数值更新。生成代理会制作欺骗性叙事，检测代理通过结构化辩论识别逻辑和事实错误来进行检测，两者通过这种对抗性交互迭代地提高自身。代理通过代理提示定义可学习权重，并通过在权重、损失和梯度的自然语言表示上操作来模拟反向传播和梯度下降。", "result": "实验结果表明，SALF在两个多语言基准数据集上的表现都非常优秀，生成的假新闻将现有的检测性能降低53.4%（中文）和34.2%（英语）。同时，SALF也能通过对抗性训练改进检测器，提高对复杂内容的检测精度达7.7%。", "conclusion": "该框架有望激励未来更强大且适应性强的假新闻检测系统的研发。"}}
{"id": "2508.19573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19573", "abs": "https://arxiv.org/abs/2508.19573", "authors": ["Luhu Li", "Bowen Lin", "Mukhtiar Khan", "Shujun Fu"], "title": "DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection", "comment": null, "summary": "Anomaly detection in medical images is challenging due to limited annotations\nand a domain gap compared to natural images. Existing reconstruction methods\noften rely on frozen pre-trained encoders, which limits adaptation to\ndomain-specific features and reduces localization accuracy. Prototype-based\nlearning offers interpretability and clustering benefits but suffers from\nprototype collapse, where few prototypes dominate training, harming diversity\nand generalization. To address this, we propose a unified framework combining a\ntrainable encoder with prototype-guided reconstruction and a novel\nDiversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum\nbranch, enables stable domain-adaptive feature learning. A lightweight\nPrototype Extractor mines informative normal prototypes to guide the decoder\nvia attention for precise reconstruction. Our loss enforces balanced prototype\nuse through diversity constraints and per-prototype normalization, effectively\npreventing collapse. Experiments on multiple medical imaging benchmarks show\nsignificant improvements in representation quality and anomaly localization,\noutperforming prior methods. Visualizations and prototype assignment analyses\nfurther validate the effectiveness of our anti-collapse mechanism and enhanced\ninterpretability.", "AI": {"tldr": "研究提出了一种医学图像异常检测的新框架，该框架结合了可训练编码器、原型引导的重建和多样性感知对齐损失，实现了防止原型崩溃、增强可解释性和提高异常定位的性能。", "motivation": "在医学图像中的异常检测因标注信息有限和与自然图像领域的差异而具有挑战性。现有的重建方法往往依赖于冻结的预训练的编码器，这限制了其对特定领域特征的适应并降低了定位准确性。基于原型的学习提供了解释性和聚类优势，但因少数原型主导训练而遭受原型崩溃，损害了多样性和泛化能力。因此，提出了这种框架以解决现有方法存在的问题。", "method": "提出了一种将可训练编码器与原型引导的重建相结合的统一框架，该框架包括动量分支以实现稳定的领域自适应特征学习。轻量级的原型提取器挖掘有用的正常原型，通过注意力机制指导解码器实现精确重建。一种新颖的多样性感知对齐损失通过多样性约束和每个原型的归一化来防止原型崩溃。", "result": "实验结果表明，在多个医学图像基准测试上，该方法在表示质量和异常定位方面表现出显著改进，超越了先前方法。可视化和原型分配分析进一步验证了防止原型崩溃机制的有效性和增强了解释性。", "conclusion": "通过实验与可视化分析验证了所提方法在医学图像异常检测中表现优异，相比于先前的方法，它不仅能提高表示质量和定位精度，还能增强模型的解释性。"}}
{"id": "2508.19665", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19665", "abs": "https://arxiv.org/abs/2508.19665", "authors": ["Giovanni Pollo", "Andrei Mihai Albu", "Alessio Burrello", "Daniele Jahier Pagliari", "Cristian Tesconi", "Loris Panaro", "Dario Soldi", "Fabio Autieri", "Sara Vinco"], "title": "Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design", "comment": null, "summary": "The recent advancements of the automotive sector demand robust co-simulation\nmethodologies that enable early validation and seamless integration across\nhardware and software domains. However, the lack of standardized interfaces and\nthe dominance of proprietary simulation platforms pose significant challenges\nto collaboration, scalability, and IP protection. To address these limitations,\nthis paper presents an approach for automatically wrapping SystemC models by\nusing the Functional Mock-up Interface (FMI) standard. This method combines the\nmodeling accuracy and fast time-to-market of SystemC with the interoperability\nand encapsulation benefits of FMI, enabling secure and portable integration of\nembedded components into co-simulation workflows. We validate the proposed\nmethodology on real-world case studies, demonstrating its effectiveness with\ncomplex designs.", "AI": {"tldr": "本文提出了通过FMI标准自动封装SystemC模型的方法，解决了汽车行业协同仿真中缺乏标准化接口和专有平台问题，实现了嵌入式组件的安全可移植集成。", "motivation": "汽车行业的需求需要强健的协同仿真方法，以便尽早验证并实现硬件和软件领域的无缝集成，但标准化接口的缺失和专有仿真平台的主导地位带来了合作、扩展性及知识产权保护方面的挑战。", "method": "本文提出了一种使用功能性模拟接口（FMI）标准自动封装SystemC模型的方法。这种方法结合了SystemC的建模精度和快速上市时间以及FMI的互操作性和封装优势，实现了嵌入式组件的安全和可移植集成到协同仿真工作流程中。", "result": "作者通过实际案例研究验证了所提出方法的有效性，证明了其在复杂设计中的适用性。", "conclusion": "结合SystemC和FMI标准的自动封装方法证明了其在汽车行业内改善协同仿真方面的能力，旨在推动更无缝的集成和保护知识产权。"}}
{"id": "2508.19574", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19574", "abs": "https://arxiv.org/abs/2508.19574", "authors": ["Mingxi Fu", "Fanglei Fu", "Xitong Ling", "Huaitian Yuan", "Tian Guan", "Yonghong He", "Lianghui Zhu"], "title": "Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation", "comment": null, "summary": "Pathological image segmentation faces numerous challenges, particularly due\nto ambiguous semantic boundaries and the high cost of pixel-level annotations.\nAlthough recent semi-supervised methods based on consistency regularization\n(e.g., UniMatch) have made notable progress, they mainly rely on\nperturbation-based consistency within the image modality, making it difficult\nto capture high-level semantic priors, especially in structurally complex\npathology images. To address these limitations, we propose MPAMatch - a novel\nsegmentation framework that performs pixel-level contrastive learning under a\nmultimodal prototype-guided supervision paradigm. The core innovation of\nMPAMatch lies in the dual contrastive learning scheme between image prototypes\nand pixel labels, and between text prototypes and pixel labels, providing\nsupervision at both structural and semantic levels. This coarse-to-fine\nsupervisory strategy not only enhances the discriminative capability on\nunlabeled samples but also introduces the text prototype supervision into\nsegmentation for the first time, significantly improving semantic boundary\nmodeling. In addition, we reconstruct the classic segmentation architecture\n(TransUNet) by replacing its ViT backbone with a pathology-pretrained\nfoundation model (Uni), enabling more effective extraction of\npathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,\nEBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art\nmethods, validating its dual advantages in structural and semantic modeling.", "AI": {"tldr": "MPAMatch通过像素级对比学习结合多模态原型指导，改进病理图像分割，特别是在结构和语义层面的监督更有效，实验结果优于现有技术。", "motivation": "现有的半监督方法主要依赖于基于扰动的一致性正则化，难以捕捉病理图像中的高层次语义先验信息。因此提出了MPAMatch 来克服当前方法的局限，更好地处理结构复杂且语义边界模糊的病理图像。", "method": "MPAMatch提出了一种新的分割框架，该框架在多模态原型引导的监督范式下进行像素级对比学习。其核心创新在于图像原型与像素标签之间的双对比学习方案和文本原型与像素标签之间的对比学习，为分割提供结构和语义层面的监督。此外，该方法还通过用病理预训练基础模型(如Uni)替换经典分割架构(如TransUNet)中的ViT主干，提高病理性特征的提取效果。", "result": "实验表明，MPAMatch 在GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER 和 KPI 数据集上表现优于现有的最先进方法，验证了该方法在结构和语义建模方面的双优效果。", "conclusion": "MPAMatch 证明了引入文本原型监督到分割任务中的有效性，使未标记的样本具备更强的辨别能力，改进病理图像的语义边界建模。通过用病理预训练模型替换ViT主干，还提高了病理图像分割的性能。"}}
{"id": "2508.19667", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19667", "abs": "https://arxiv.org/abs/2508.19667", "authors": ["Chenghan Yang", "Ruiyu Zhao", "Yang Liu", "Ling Jiang"], "title": "Survey of Specialized Large Language Model", "comment": "9 pages, 1 figures", "summary": "The rapid evolution of specialized large language models (LLMs) has\ntransitioned from simple domain adaptation to sophisticated native\narchitectures, marking a paradigm shift in AI development. This survey\nsystematically examines this progression across healthcare, finance, legal, and\ntechnical domains. Besides the wide use of specialized LLMs, technical\nbreakthrough such as the emergence of domain-native designs beyond fine-tuning,\ngrowing emphasis on parameter efficiency through sparse computation and\nquantization, increasing integration of multimodal capabilities and so on are\napplied to recent LLM agent. Our analysis reveals how these innovations address\nfundamental limitations of general-purpose LLMs in professional applications,\nwith specialized models consistently performance gains on domain-specific\nbenchmarks. The survey further highlights the implications for E-Commerce field\nto fill gaps in the field.", "AI": {"tldr": "本文综述了专业大型语言模型（LLMs）从简单领域适应到复杂原生架构的演变，特别是在专业应用中的技术突破和性能提升，并指出其在电子商务领域的潜在价值。", "motivation": "该研究旨在审查专业大型语言模型（LLMs）的发展，特别是技术突破如领域原生设计的出现、参数效率的提高以及多模态能力的整合等，对于专业应用中的局限性解决之道。", "method": "本文通过系统性地考察LLMs在医疗、金融、法律和技术领域从简单的领域适应到复杂原生架构的演变过程，分析了这些创新如何克服通用LLMs在专业应用中的根本限制。", "result": "专业化LLMs在特定领域基准测试中表现出持续的性能提升，并指出了电子商务领域中的潜在应用前景。", "conclusion": "研究揭示了专业LLMs在多领域中的进展，强调了这些进展对未来在电子商务等领域应用的积极影响。"}}
{"id": "2508.19575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19575", "abs": "https://arxiv.org/abs/2508.19575", "authors": ["Zhu Xu", "Zhaowen Wang", "Yuxin Peng", "Yang Liu"], "title": "Interact-Custom: Customized Human Object Interaction Image Generation", "comment": null, "summary": "Compositional Customized Image Generation aims to customize multiple target\nconcepts within generation content, which has gained attention for its wild\napplication.Existing approaches mainly concentrate on the target entity's\nappearance preservation, while neglecting the fine-grained interaction control\namong target entities.To enable the model of such interaction control\ncapability, we focus on human object interaction scenario and propose the task\nof Customized Human Object Interaction Image Generation(CHOI), which\nsimultaneously requires identity preservation for target human object and the\ninteraction semantic control between them.Two primary challenges exist for\nCHOI:(1)simultaneous identity preservation and interaction control demands\nrequire the model to decompose the human object into self-contained identity\nfeatures and pose-oriented interaction features, while the current HOI image\ndatasets fail to provide ideal samples for such feature-decomposed\nlearning.(2)inappropriate spatial configuration between human and object may\nlead to the lack of desired interaction semantics.To tackle it, we first\nprocess a large-scale dataset, where each sample encompasses the same pair of\nhuman object involving different interactive poses.Then we design a two-stage\nmodel Interact-Custom, which firstly explicitly models the spatial\nconfiguration by generating a foreground mask depicting the interaction\nbehavior, then under the guidance of this mask, we generate the target human\nobject interacting while preserving their identities features.Furthermore, if\nthe background image and the union location of where the target human object\nshould appear are provided by users, Interact-Custom also provides the optional\nfunctionality to specify them, offering high content controllability. Extensive\nexperiments on our tailored metrics for CHOI task demonstrate the effectiveness\nof our approach.", "AI": {"tldr": "研究提出了自定义的人物体互动图像生成任务（CHOI），挑战在于同时保持身份及互动行为控制。为此，研究设计了一个两阶段模型Interact-Custom，可以生成同时保持身份特征的互动目标人与物，并提供高内容可控性。实验结果证明方法的有效性。", "motivation": "研究动机在于现有的方法主要集中在目标实体外观的保持上，忽略了目标实体之间的精细互动控制。现有的HOI图像数据集也未能提供理想的学习样本，故作者提出了自定义的人物体互动图像生成任务，并设计了相应的方法来解决现有问题。", "method": "提出了一个名为Interact-Custom的两阶段模型，该模型首先通过生成描绘互动行为的前景掩码来显式建模空间配置，然后在掩码的指导下生成保留身份特征的互动目标人和物。此外，该模型还提供了根据用户指定的背景图像和目标人物的位置进行生成的功能，从而提供高内容可控性。", "result": "在专为CHOI任务设计的度量标准上的广泛实验证明了该方法的有效性。", "conclusion": "研究结论表明，所提出的Interact-Custom模型能够有效地解决人物体互动图像生成中的同时保持身份特征及互动控制的挑战，实现了高内容可控性。实验验证了模型的有效性。"}}
{"id": "2508.19689", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19689", "abs": "https://arxiv.org/abs/2508.19689", "authors": ["Xiaoying Zhang"], "title": "Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality", "comment": "179 pages", "summary": "Developing adaptable, extensible, and accurate task bots with minimal or zero\nhuman intervention is a significant challenge in dialog research. This thesis\nexamines the obstacles and potential solutions for creating such bots, focusing\non innovative techniques that enable bots to learn and adapt autonomously in\nconstantly changing environments.", "AI": {"tldr": "The thesis investigates the challenges and solutions for creating autonomous, adaptable task bots that can function with minimal human intervention in dynamic environments.", "motivation": "The motivation is to address the challenge of developing adaptable, extensible, and accurate task bots that can operate with minimal or zero human intervention.", "method": "This thesis focuses on exploring innovative techniques that enable bots to learn and adapt autonomously in constantly changing environments.", "result": "Not detailed in the abstract.", "conclusion": "Not detailed in the abstract."}}
{"id": "2508.19579", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19579", "abs": "https://arxiv.org/abs/2508.19579", "authors": ["Haomiao Zhang", "Miao Cao", "Xuan Yu", "Hui Luo", "Yanling Piao", "Mengjie Qin", "Zhangyuan Li", "Ping Wang", "Xin Yuan"], "title": "High-Speed FHD Full-Color Video Computer-Generated Holography", "comment": null, "summary": "Computer-generated holography (CGH) is a promising technology for\nnext-generation displays. However, generating high-speed, high-quality\nholographic video requires both high frame rate display and efficient\ncomputation, but is constrained by two key limitations: ($i$) Learning-based\nmodels often produce over-smoothed phases with narrow angular spectra, causing\nsevere color crosstalk in high frame rate full-color displays such as\ndepth-division multiplexing and thus resulting in a trade-off between frame\nrate and color fidelity. ($ii$) Existing frame-by-frame optimization methods\ntypically optimize frames independently, neglecting spatial-temporal\ncorrelations between consecutive frames and leading to computationally\ninefficient solutions. To overcome these challenges, in this paper, we propose\na novel high-speed full-color video CGH generation scheme. First, we introduce\nSpectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase\ndistributions via frequency modulation, enabling high-fidelity full-color\ndisplay at high frame rates. Second, we present HoloMamba, a lightweight\nasymmetric Mamba-Unet architecture that explicitly models spatial-temporal\ncorrelations across video sequences to enhance reconstruction quality and\ncomputational efficiency. Extensive simulated and real-world experiments\ndemonstrate that SGDDM achieves high-fidelity full-color display without\ncompromise in frame rate, while HoloMamba generates FHD (1080p) full-color\nholographic video at over 260 FPS, more than 2.6$\\times$ faster than the prior\nstate-of-the-art Divide-Conquer-and-Merge Strategy.", "AI": {"tldr": "本文提出一种新型CGH方案，通过SGDDM和HoloMamba解决了现有技术的限制，实现了高速高质量全彩色全息视频的生成。", "motivation": "当前CGH技术在生成高速高质量全息视频方面面临两个主要限制：基于学习的模型生成的相位往往过于光滑，导致高帧率全彩色显示中的颜色串扰问题；现有的逐帧优化方法忽略了连续帧之间的时空相关性，导致计算效率低下。", "method": "本论文提出了一种新颖的高速全彩色视频计算机生成全息图（CGH）生成方案。该方案包括两个关键部分：一是光谱引导深度分割复用（SGDDM），通过频率调制优化相位分布，实现高速全彩色显示；二是HoloMamba，一种轻量级非对称Mamba-Unet架构，明确建模视频序列间的时空相关性以提升重建质量和计算效率。", "result": "通过广泛的模拟和真实世界实验表明，SGDDM实现了无需降低帧率就能保证的高保真全彩色显示，而HoloMamba可以生成每秒超过260帧的FHD（1080p）全彩色全息视频，比以前的领先技术快2.6倍以上。", "conclusion": "所提方法克服了现有CGH技术的局限性，不仅提高了全彩色全息视频的显示质量，同时提升了计算效率，为下一代显示技术提供了新方案。"}}
{"id": "2508.19720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19720", "abs": "https://arxiv.org/abs/2508.19720", "authors": ["Yilin Wang", "Heng Wang", "Yuyang Bai", "Minnan Luo"], "title": "Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models", "comment": null, "summary": "In Large Language Models (LLMs) generation, there exist knowledge conflicts\nand scenarios where parametric knowledge contradicts knowledge provided in the\ncontext. Previous works studied tuning, decoding algorithms, or locating and\nediting context-aware neurons to adapt LLMs to be faithful to new contextual\nknowledge. However, they are usually inefficient or ineffective for large\nmodels, not workable for black-box models, or unable to continuously adjust\nLLMs' sensitivity to the knowledge provided in the context. To mitigate these\nproblems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a\nsimple framework that can steer LLMs' sensitivity to contextual knowledge\ncontinuously at a lightweight cost. Specifically, we tune two small LMs (i.e.\nproxy models) and use the difference in their output distributions to shift the\noriginal distribution of an LLM without modifying the LLM weights. In the\nevaluation process, we not only design synthetic data and fine-grained metrics\nto measure models' sensitivity to contextual knowledge but also use a real\nconflict dataset to validate CSKS's practical efficacy. Extensive experiments\ndemonstrate that our framework achieves continuous and precise control over\nLLMs' sensitivity to contextual knowledge, enabling both increased sensitivity\nand reduced sensitivity, thereby allowing LLMs to prioritize either contextual\nor parametric knowledge as needed flexibly. Our data and code are available at\nhttps://github.com/OliveJuiceLin/CSKS.", "AI": {"tldr": "文章提出了一个简单的框架CSKS，可以以轻量级的成本连续调整LLMs对上下文知识的敏感度。实验结果表明这个方法能够精确地控制模型敏感度，具备实用价值。", "motivation": "现有的方法要么不适用于大型模型，要么无法持续调整模型对上下文知识的敏感度。CSKS框架旨在解决这些问题，通过一种更加高效且有效的方法来适应LLMs对新上下文知识的忠实度。", "method": "提出了一种轻量级的成本框架CSKS（持续引导知识敏感性），通过调整两个小型模型（代理模型）的输出分布差异来改变原始模型（LLM）的分布，从而实现对LLM对上下文知识敏感性的连续控制，而不需要修改LLM的权重。", "result": "评估过程中，设计了合成数据和细粒度度量来衡量模型对上下文知识的敏感度，并使用了一个真正的冲突数据集来验证CSKS的实际效果。", "conclusion": "通过广泛的实验表明，框架实现了对LLMs对上下文知识敏感性的连续和精准控制，既可增加敏感度，也可以减少敏感度，使LLMs能够灵活地优先考虑上下文知识或参数知识。"}}
{"id": "2508.19581", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19581", "abs": "https://arxiv.org/abs/2508.19581", "authors": ["Dat Nguyen Cong", "Hieu Tran Bao", "Hoang Thanh-Tung"], "title": "Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction", "comment": "21 pages, 16 figures", "summary": "Diffusion models have gained prominence as state-of-the-art techniques for\nsynthesizing images and videos, particularly due to their ability to scale\neffectively with large datasets. Recent studies have uncovered that these\nextensive datasets often contain mistakes from manual labeling processes.\nHowever, the extent to which such errors compromise the generative capabilities\nand controllability of diffusion models is not well studied. This paper\nintroduces Score-based Discriminator Correction (SBDC), a guidance technique\nfor aligning noisy pre-trained conditional diffusion models. The guidance is\nbuilt on discriminator training using adversarial loss, drawing on prior noise\ndetection techniques to assess the authenticity of each sample. We further show\nthat limiting the usage of our guidance to the early phase of the generation\nprocess leads to better performance. Our method is computationally efficient,\nonly marginally increases inference time, and does not require retraining\ndiffusion models. Experiments on different noise settings demonstrate the\nsuperiority of our method over previous state-of-the-art methods.", "AI": {"tldr": "本文提出了一种改进扩散模型针对大规模数据集中的标注错误影响的技术，此技术在无需重新训练模型的条件下，提高了模型的生成能力和可控性。", "motivation": "最近的研究表明，大规模的数据集中通常含有由于手动标注过程产生的错误，而这些错误对扩散模型的生成能力和可控性的影响尚未得到充分研究。", "method": "提出了一种基于评分的判别器校正（SBDC）技术，用于对预训练的条件扩散模型进行噪声校正。此技术是在判别器训练的基础上，使用对抗性损失来检测每个样本的真实性的。", "result": "实验表明，在不同的噪声设置下，本方法优于现有的最先进的方法。", "conclusion": "该方法计算效率高，仅边际增加推理时间，且无需重新训练扩散模型。"}}
{"id": "2508.19721", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.19721", "abs": "https://arxiv.org/abs/2508.19721", "authors": ["Carlos Carvalho", "Francisco Teixeira", "Catarina Botelho", "Anna Pompili", "Rubén Solera-Ureña", "Sérgio Paulo", "Mariana Julião", "Thomas Rolland", "John Mendonça", "Diogo Pereira", "Isabel Trancoso", "Alberto Abad"], "title": "CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese", "comment": "Accepted to ASRU 2025", "summary": "Existing resources for Automatic Speech Recognition in Portuguese are mostly\nfocused on Brazilian Portuguese, leaving European Portuguese (EP) and other\nvarieties under-explored. To bridge this gap, we introduce CAM\\~OES, the first\nopen framework for EP and other Portuguese varieties. It consists of (1) a\ncomprehensive evaluation benchmark, including 46h of EP test data spanning\nmultiple domains; and (2) a collection of state-of-the-art models. For the\nlatter, we consider multiple foundation models, evaluating their zero-shot and\nfine-tuned performances, as well as E-Branchformer models trained from scratch.\nA curated set of 425h of EP was used for both fine-tuning and training. Our\nresults show comparable performance for EP between fine-tuned foundation models\nand the E-Branchformer. Furthermore, the best-performing models achieve\nrelative improvements above 35% WER, compared to the strongest zero-shot\nfoundation model, establishing a new state-of-the-art for EP and other\nvarieties.", "AI": {"tldr": "研究提出了CAM\\~OES框架，解决了欧洲葡萄牙语及其他变体在自动语音识别上的资源不足问题，最佳模型相比零样本模型有超过35%的WER相对改善。", "motivation": "目前针对葡萄牙语的自动语音识别资源主要集中在巴西葡萄牙语上，对于欧洲葡萄牙语（EP）和其他变体研究不足。该研究旨在填补这一空白。", "method": "通过引入CAM\\~OES框架来填补葡萄牙语自动语音识别资源的空白，该框架包含一个全面的评估基准和一系列最先进的模型。评估基准包括46小时的EP测试数据，覆盖多个领域。模型方面，研究了多个基础模型的零样本和微调性能，以及从头开始训练的E-Branchformer模型。", "result": "微调后的基础模型和E-Branchformer模型在EP上的表现相当，最佳模型与最强零样本基础模型相比，WER相对改善超过35%，建立了EP和其他变体的新最先进水平。", "conclusion": "CAM\\~OES框架为葡萄牙语自动语音识别提供了宝贵资源，尤其是对欧洲葡萄牙语和其他变种的识别性能有了显著改善。"}}
{"id": "2508.19593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19593", "abs": "https://arxiv.org/abs/2508.19593", "authors": ["Abhinav Kumar"], "title": "Generalizing Monocular 3D Object Detection", "comment": "PhD Thesis submitted to MSU", "summary": "Monocular 3D object detection (Mono3D) is a fundamental computer vision task\nthat estimates an object's class, 3D position, dimensions, and orientation from\na single image. Its applications, including autonomous driving, augmented\nreality, and robotics, critically rely on accurate 3D environmental\nunderstanding. This thesis addresses the challenge of generalizing Mono3D\nmodels to diverse scenarios, including occlusions, datasets, object sizes, and\ncamera parameters. To enhance occlusion robustness, we propose a mathematically\ndifferentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we\nexplore depth equivariant (DEVIANT) backbones. We address the issue of large\nobject detection, demonstrating that it's not solely a data imbalance or\nreceptive field problem but also a noise sensitivity issue. To mitigate this,\nwe introduce a segmentation-based approach in bird's-eye view with dice loss\n(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D\nmodels to unseen camera heights and improve Mono3D generalization in such\nout-of-distribution settings.", "AI": {"tldr": "本文针对单目3D目标检测提出了用于增强遮挡鲁棒性、提升泛化能力、解决大型物体检测以及在不同相机高度下的泛化问题的多种方法。", "motivation": "本文解决的是将单目3D目标检测模型推广到各种场景中的挑战，这些场景包括遮挡、数据集多样性、物体尺寸以及相机参数的不同。", "method": "本文提出了一种可微分的NMS方法（GrooMeD-NMS）来增强遮挡鲁棒性。为了提高模型在新数据集上的泛化能力，探索了深度等变（DEVIANT）的主干网络。针对大型物体检测的问题，指出其不仅仅是数据不平衡或感受野问题，而是噪声敏感性问题，并引入了一种基于分割的方法（SeaBird）。最后，进行了数学分析论证以改进超出分布设定中的Mono3D模型的泛化能力。", "result": "提出了解决遮挡、泛化和大型物体检测问题的有效方法，并提高了单目3D检测模型在不同应用场景下的性能。", "conclusion": "通过改进Mono3D模型的遮挡鲁棒性、泛化能力以及在不同场景下的性能，增强了模型在复杂环境中的应用能力。"}}
{"id": "2508.19724", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19724", "abs": "https://arxiv.org/abs/2508.19724", "authors": ["Aritra Dutta", "Swapnanil Mukherjee", "Deepanway Ghosal", "Somak Aditya"], "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks", "comment": null, "summary": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.", "AI": {"tldr": "该论文提出了一种集成常识知识的视觉问题回答框架NLKI，提高了小规模视觉语言模型在视觉问题回答任务上的性能。", "motivation": "小型视觉语言模型在常识视觉问题回答任务上的表现通常差于大型模型，作者希望通过集成常识知识提升它们的性能。", "method": "该框架包括三个步骤：1. 使用ColBERTv2检索自然语言事实；2. 使大语言模型生成自然语言解释；3. 将这两种信号分别输入到小型视觉语言模型中。", "result": "NLKI框架在三个数据集上将FLAVA和其他小型视觉语言模型的准确性提高了最多7%，并且在对带有标签噪声的数据集进行额外的鲁棒性训练后，又进一步提高了2.5%-5.5%的准确性。", "conclusion": "研究结果表明，大语言模型生成的常识解释在减少幻觉方面有效，且噪声感知训练有助于小型模型在常识知识增强的背景下保持稳定，说明高效使用常识推理对小型模型（例如2.5亿参数）是可行的。"}}
{"id": "2508.19600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19600", "abs": "https://arxiv.org/abs/2508.19600", "authors": ["Toghrul Karimov", "Hassan Imani", "Allan Kazakov"], "title": "Quantization Robustness to Input Degradations for Object Detection", "comment": null, "summary": "Post-training quantization (PTQ) is crucial for deploying efficient object\ndetection models, like YOLO, on resource-constrained devices. However, the\nimpact of reduced precision on model robustness to real-world input\ndegradations such as noise, blur, and compression artifacts is a significant\nconcern. This paper presents a comprehensive empirical study evaluating the\nrobustness of YOLO models (nano to extra-large scales) across multiple\nprecision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8\n(TensorRT). We introduce and evaluate a degradation-aware calibration strategy\nfor Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix\nof clean and synthetically degraded images. Models were benchmarked on the COCO\ndataset under seven distinct degradation conditions (including various types\nand levels of noise, blur, low contrast, and JPEG compression) and a\nmixed-degradation scenario. Results indicate that while Static INT8 TensorRT\nengines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop\n(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did\nnot yield consistent, broad improvements in robustness over standard clean-data\ncalibration across most models and degradations. A notable exception was\nobserved for larger model scales under specific noise conditions, suggesting\nmodel capacity may influence the efficacy of this calibration approach. These\nfindings highlight the challenges in enhancing PTQ robustness and provide\ninsights for deploying quantized detectors in uncontrolled environments. All\ncode and evaluation tables are available at https://github.com/AllanK24/QRID.", "AI": {"tldr": "This paper evaluates the robustness of quantized YOLO models under various image degradations, finding that while Static INT8 quantization provides speed benefits, a degradation-aware calibration strategy only inconsistently improves robustness.", "motivation": "To understand how different quantization methods and calibration strategies affect the robustness of object detection models under real-world degradations.", "method": "The models are evaluated using a mix of clean and synthetically degraded images under multiple precision formats and degradation conditions.", "result": "Static INT8 quantization provides speedup but with a moderate accuracy drop on clean data. The degradation-aware calibration yields limited improvements in robustness, except in specific cases for larger models.", "conclusion": "The study reveals the challenges in improving the robustness of quantized models and offers insights for deploying such models in uncontrolled settings."}}
{"id": "2508.19740", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19740", "abs": "https://arxiv.org/abs/2508.19740", "authors": ["Wenhao Li", "Yuxin Zhang", "Gen Luo", "Haiyuan Wan", "Ziyang Gong", "Fei Chao", "Rongrong Ji"], "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval", "comment": null, "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.", "AI": {"tldr": "本文提出了Spotlight Attention，一种使用非线性哈希函数的方法，以提高编码效率并降低大语言模型的键值缓存负担。它还实现了一种在较小的GPU内存上高效训练的框架，并优化了哈希检索的速度。", "motivation": "传统的随机线性哈希方法由于查询和键在大语言模型中的正交分布，效率低下。本文旨在减少大语言模型中的键值缓存负担，以加速推理过程。", "method": "Spotlight Attention方法使用非线性哈希函数，优化了查询和键的嵌入分布，提高了编码效率和鲁棒性。此外，使用一种轻量级的、基于Bradley-Terry排名损失的稳定训练框架，使得能够在16GB内存的GPU上进行训练。", "result": "实验结果显示，Spotlight Attention方法相较于传统线性哈希，能够将检索精度大幅提高并缩短至少5倍的哈希码长度。此外，在A100 GPU上实现了高效的哈希检索速度，吞吐量提高了3倍。", "conclusion": "Spotlight Attention显著提高了检索的精度，缩短了哈希码的长度，并且利用专门的CUDA内核实现，在单个A100 GPU上实现了512K令牌的哈希检索在100微秒内完成，并且端到端吞吐量比普通解码提高了3倍。"}}
{"id": "2508.19604", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19604", "abs": "https://arxiv.org/abs/2508.19604", "authors": ["Qizhe Fan", "Chaoyu Liu", "Zhonghua Qiao", "Xiaoqin Shen"], "title": "IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation", "comment": null, "summary": "Domain Generalized Semantic Segmentation (DGSS) focuses on training a model\nusing labeled data from a source domain, with the goal of achieving robust\ngeneralization to unseen target domains during inference. A common approach to\nimprove generalization is to augment the source domain with synthetic data\ngenerated by diffusion models (DMs). However, the generated images often\ncontain structural or semantic defects due to training imperfections. Training\nsegmentation models with such flawed data can lead to performance degradation\nand error accumulation. To address this issue, we propose to integrate inverse\nevolution layers (IELs) into the generative process. IELs are designed to\nhighlight spatial discontinuities and semantic inconsistencies using\nLaplacian-based priors, enabling more effective filtering of undesirable\ngenerative patterns. Based on this mechanism, we introduce IELDM, an enhanced\ndiffusion-based data augmentation framework that can produce higher-quality\nimages. Furthermore, we observe that the defect-suppression capability of IELs\ncan also benefit the segmentation network by suppressing artifact propagation.\nBased on this insight, we embed IELs into the decoder of the DGSS model and\npropose IELFormer to strengthen generalization capability in cross-domain\nscenarios. To further strengthen the model's semantic consistency across\nscales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,\nwhich performs frequency-domain analysis to achieve structured integration of\nmulti-resolution features, thereby improving cross-scale coherence. Extensive\nexperiments on benchmark datasets demonstrate that our approach achieves\nsuperior generalization performance compared to existing methods.", "AI": {"tldr": "本文提出了一种改进的扩散模型生成方法（IELDM）和一种新的DGSS模型（IELFormer），通过引入逆演层（IELs）来提高图像质量和跨域泛化能力。", "motivation": "解决传统方法中由于训练不完美导致的合成图像中的结构或语义缺陷问题，从而避免性能下降和误差累积。", "method": "本文提出了一种新的方法，将逆演层（IELs）集成到生成过程中，以提高生成图像的质量，并将其嵌入到DGSS模型的解码器中，提出了一种新的模型IELFormer。此外，IELFormer还加入了一个多尺度频率融合（MFF）模块，以提高跨尺度的语义一致性。", "result": "实验结果表明，该方法在基准数据集上实现了优于现有方法的跨域泛化性能。", "conclusion": "基于逆演层和多尺度频率融合模块的迭代改进方法，可以有效提高模型在不同域中的泛化性能，特别是通过减少合成图像中的缺陷来提升语义分割的性能。"}}
{"id": "2508.19758", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.19758", "abs": "https://arxiv.org/abs/2508.19758", "authors": ["Yixuan Tang", "Yuanyuan Shi", "Yiqun Sun", "Anthony Kum Hoe Tung"], "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval", "comment": "Accepted by EMNLP 2025", "summary": "Access to diverse perspectives is essential for understanding real-world\nevents, yet most news retrieval systems prioritize textual relevance, leading\nto redundant results and limited viewpoint exposure. We propose NEWSCOPE, a\ntwo-stage framework for diverse news retrieval that enhances event coverage by\nexplicitly modeling semantic variation at the sentence level. The first stage\nretrieves topically relevant content using dense retrieval, while the second\nstage applies sentence-level clustering and diversity-aware re-ranking to\nsurface complementary information. To evaluate retrieval diversity, we\nintroduce three interpretable metrics, namely Average Pairwise Distance,\nPositive Cluster Coverage, and Information Density Ratio, and construct two\nparagraph-level benchmarks: LocalNews and DSGlobal. Experiments show that\nNEWSCOPE consistently outperforms strong baselines, achieving significantly\nhigher diversity without compromising relevance. Our results demonstrate the\neffectiveness of fine-grained, interpretable modeling in mitigating redundancy\nand promoting comprehensive event understanding. The data and code are\navailable at https://github.com/tangyixuan/NEWSCOPE.", "AI": {"tldr": "我们提出了NEWSCOPE框架，通过两阶段方法提升新闻检索的多样性，实验显示其效果优于现有方法。", "motivation": "大多数新闻检索系统注重文本相关性，因而产生冗余结果，限制了视角的多样性。为了更好地理解现实世界的事件，需要访问不同的视角。", "method": "我们提出了一种名为NEWSCOPE的两阶段框架，旨在提升新闻检索的多样性。第一阶段通过密集检索获取主题相关的新闻内容，第二阶段则利用句子级别的聚类和多样性的重新排序方法来揭示互补信息。", "result": "实验表明，NEWSCOPE在多样性和相关性方面均优于强大的基线方法，实现了显著更高的多样性，同时没有影响相关性。", "conclusion": "研究结果表明，细粒度、可解释性建模在减少冗余和提高事件理解的全面性方面是有效的。"}}
{"id": "2508.19626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19626", "abs": "https://arxiv.org/abs/2508.19626", "authors": ["Jiajun Sun", "Zhen Yu", "Siyuan Yan", "Jason J. Ong", "Zongyuan Ge", "Lei Zhang"], "title": "Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model", "comment": "11 pages, 4 figures", "summary": "Skin images from real-world clinical practice are often limited, resulting in\na shortage of training data for deep-learning models. While many studies have\nexplored skin image synthesis, existing methods often generate low-quality\nimages and lack control over the lesion's location and type. To address these\nlimitations, we present LF-VAR, a model leveraging quantified lesion\nmeasurement scores and lesion type labels to guide the clinically relevant and\ncontrollable synthesis of skin images. It enables controlled skin synthesis\nwith specific lesion characteristics based on language prompts. We train a\nmultiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to\nencode images into discrete latent representations for structured tokenization.\nThen, a Visual AutoRegressive (VAR) Transformer trained on tokenized\nrepresentations facilitates image synthesis. Lesion measurement from the lesion\nregion and types as conditional embeddings are integrated to enhance synthesis\nfidelity. Our method achieves the best overall FID score (average 0.74) among\nseven lesion types, improving upon the previous state-of-the-art (SOTA) by\n6.3%. The study highlights our controllable skin synthesis model's\neffectiveness in generating high-fidelity, clinically relevant synthetic skin\nimages. Our framework code is available at\nhttps://github.com/echosun1996/LF-VAR.", "AI": {"tldr": "提出LF-VAR模型，用于生成高保真、临床相关的合成皮肤图像，尤其强调在病变位置和类型上的可控性，改进了之前的最佳技术水平并达到了最优的整体FID评分。", "motivation": "解决现有皮肤图像合成方法生成的图像质量低以及对病变位置和类型的控制不足的问题。现有的训练数据不足，需要更多的训练数据来优化深度学习模型。", "method": "利用量化病变测量分数和病变类型标签指导临床相关的可控皮肤图像合成。该方法使用多尺度病变聚焦矢量量化变分自动编码器（VQVAE）将图像编码成离散的潜表示，再使用在结构化标记表示上训练的视觉自回归（VAR）Transformer进行图像生成。病变区域的测量值和类型作为条件嵌入整合以提高合成的保真度。", "result": "在七种病变类型中的总体FID评分达到0.74，比先前的最佳技术水平提高了6.3%。", "conclusion": "该研究证明了可控皮肤合成模型在生成高保真、临床相关的合成皮肤图像方面的有效性。代码框架在https://github.com/echosun1996/LF-VAR上可供访问。"}}
{"id": "2508.19764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19764", "abs": "https://arxiv.org/abs/2508.19764", "authors": ["Pedro Henrique Luz de Araujo", "Paul Röttger", "Dirk Hovy", "Benjamin Roth"], "title": "Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance", "comment": "30 pages, 29 figures, accepted to EMNLP 2025", "summary": "Expert persona prompting -- assigning roles such as expert in math to\nlanguage models -- is widely used for task improvement. However, prior work\nshows mixed results on its effectiveness, and does not consider when and why\npersonas should improve performance. We analyze the literature on persona\nprompting for task improvement and distill three desiderata: 1) performance\nadvantage of expert personas, 2) robustness to irrelevant persona attributes,\nand 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs\nacross 27 tasks with respect to these desiderata. We find that expert personas\nusually lead to positive or non-significant performance changes. Surprisingly,\nmodels are highly sensitive to irrelevant persona details, with performance\ndrops of almost 30 percentage points. In terms of fidelity, we find that while\nhigher education, specialization, and domain-relatedness can boost performance,\ntheir effects are often inconsistent or negligible across tasks. We propose\nmitigation strategies to improve robustness -- but find they only work for the\nlargest, most capable models. Our findings underscore the need for more careful\npersona design and for evaluation schemes that reflect the intended effects of\npersona usage.", "AI": {"tldr": "研究了角色引导在任务改进上的效果，发现专家角色通常有助于改善性能，但对无关角色细节高度敏感。建议更加谨慎地设计角色及采用更有效的评估方案。", "motivation": "现有研究表明角色引导对语言模型的改进效果不一，而我们的研究旨在探讨何时及为何角色引导能够提高模型性能。", "method": "我们分析了关于角色引导对任务改进的文献，并提炼了三个关键点：1) 专家角色的优势；2) 对无关角色属性的鲁棒性；3) 对角色属性的保真度。随后，我们评估了9个最先进的语言模型在27项任务上的表现，基于这些关键点进行考量。", "result": "研究发现专家角色通常导致积极的性能变化或无显著差异。模型对无关角色细节极其敏感，性能几乎下降了30个百分点。在保持角色属性一致性方面，较高的教育水平、专业化和与领域的相关性能够提高性能，但其效应在不同任务中往往是不一致的或影响力很小。", "conclusion": "研究发现专家角色通常会导致性能提升或无显著差异。令人惊讶的是，模型对无关角色细节非常敏感，性能因此下降几乎30个百分点。此外，我们提出了一些改进鲁棒性的策略，但这些策略只对最大的、最有能力的模型有效。这表明需要更加认真地设计角色，并采用能反映角色使用意图的评估方案。"}}
{"id": "2508.19630", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19630", "abs": "https://arxiv.org/abs/2508.19630", "authors": ["Xiaolei Wei", "Yi Ouyang", "Haibo Ye"], "title": "Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition", "comment": "This paper has been accepted to PRCV 2025", "summary": "Long-tailed visual recognition is challenging not only due to class imbalance\nbut also because of varying classification difficulty across categories. Simply\nreweighting classes by frequency often overlooks those that are intrinsically\nhard to learn. To address this, we propose \\textbf{DQRoute}, a modular\nframework that combines difficulty-aware optimization with dynamic expert\ncollaboration. DQRoute first estimates class-wise difficulty based on\nprediction uncertainty and historical performance, and uses this signal to\nguide training with adaptive loss weighting. On the architectural side, DQRoute\nemploys a mixture-of-experts design, where each expert specializes in a\ndifferent region of the class distribution. At inference time, expert\npredictions are weighted by confidence scores derived from expert-specific OOD\ndetectors, enabling input-adaptive routing without the need for a centralized\nrouter. All components are trained jointly in an end-to-end manner. Experiments\non standard long-tailed benchmarks demonstrate that DQRoute significantly\nimproves performance, particularly on rare and difficult classes, highlighting\nthe benefit of integrating difficulty modeling with decentralized expert\nrouting.", "AI": {"tldr": "DQRoute是一种针对长尾视觉识别难题的框架，它通过难度感知优化和动态专家协作提高对罕见和难以分类类别的识别性能。", "motivation": "长尾视觉识别由于类别不平衡和类别间分类难度不同而具有挑战性。简单地按频率重新加权类别往往忽略那些本质上难以学习的类别。DQRoute旨在通过难度感知方法解决这一问题。", "method": "DQRoute是一种集成难度感知优化和动态专家协作的模块化框架。它首先基于预测不确定性和历史性能估计类别难度，并据此引导适应性损失加权的训练过程。在架构方面，DQRoute采用专家混合设计，每个专家专注于不同的类别分布区域。推理时，专家的预测根据来自专家特定的OOD检测器得出的信心分数加权，从而实现在不需要中心路由器的情况下自适应路由。所有组件都以端到端的方式进行联合训练。", "result": "实验结果显示，DQRoute在标准长尾基准上显著提高了性能，特别是在稀有和困难的类别上，这突出了集成难度建模和去中心化专家路由的优势。", "conclusion": "DQRoute显著改善了长尾数据集上的分类性能，尤其是在罕见和困难的类别上，这表明难度建模与去中心化专家路由相结合的有效性。"}}
{"id": "2508.19813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19813", "abs": "https://arxiv.org/abs/2508.19813", "authors": ["Jie Zhang", "Changzai Pan", "Kaiwen Wei", "Sishi Xiong", "Yu Zhao", "Xiangyu Li", "Jiaxin Peng", "Xiaoyan Gu", "Jian Yang", "Wenhan Chang", "Zhenhe Wu", "Jiang Zhong", "Shuangyong Song", "Yongxiang Li", "Xuelong Li"], "title": "T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables", "comment": null, "summary": "Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench. Source code and data will be available after acceptance.", "AI": {"tldr": "提出了将表格转化为报告的任务，并构建了一个新基准T2R-bench用以评估此任务，实验结果表明现有的LLM还有很大的改进空间。", "motivation": "大量的研究已经探索了大语言模型在表格推理方面的功能，但是将表格中的信息转化为报告这一任务在工业应用中仍然是一个重大挑战。此问题主要由表格的复杂性和多样性导致的推理结果不佳，以及现有表格基准无法充分评估该任务的实际应用情况所引起。", "method": "我们提出了一个新的任务——将表格转化为报告，并且构建了一个名为T2R-bench的双语基准数据集，该数据集包含来自45个行业的真实场景中的457张表格。同时，我们还提出了一组评估标准来公平地衡量报告生成的质量。", "result": "通过对25种广泛使用的大型语言模型进行实验，结果显示即使是处于前沿的Deepseek-R1模型也只能达到62.71的总分，表明LLM在此数据集上的表现还有很大的提升空间。", "conclusion": "实验结果显示，现有的大语言模型在我们构建的T2R-bench上仍有较大的改进空间，表明将表格转化为报告的任务仍然是一项具有挑战性的任务。我们将源代码和数据在被接受后提供。"}}
{"id": "2508.19638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19638", "abs": "https://arxiv.org/abs/2508.19638", "authors": ["Yang Li", "Quan Yuan", "Guiyang Luo", "Xiaoyuan Fu", "Rui Pan", "Yujia Yang", "Congzhang Shao", "Yuewen Liu", "Jinglin Li"], "title": "Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception", "comment": null, "summary": "Collaborative perception allows agents to enhance their perceptual\ncapabilities by exchanging intermediate features. Existing methods typically\norganize these intermediate features as 2D bird's-eye-view (BEV)\nrepresentations, which discard critical fine-grained 3D structural cues\nessential for accurate object recognition and localization. To this end, we\nfirst introduce point-level tokens as intermediate representations for\ncollaborative perception. However, point-cloud data are inherently unordered,\nmassive, and position-sensitive, making it challenging to produce compact and\naligned point-level token sequences that preserve detailed structural\ninformation. Therefore, we present CoPLOT, a novel Collaborative perception\nframework that utilizes Point-Level Optimized Tokens. It incorporates a\npoint-native processing pipeline, including token reordering, sequence\nmodeling, and multi-agent spatial alignment. A semantic-aware token reordering\nmodule generates adaptive 1D reorderings by leveraging scene-level and\ntoken-level semantic information. A frequency-enhanced state space model\ncaptures long-range sequence dependencies across both spatial and spectral\ndomains, improving the differentiation between foreground tokens and background\nclutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop\nprocess, combining global agent-level correction with local token-level\nrefinement to mitigate localization noise. Extensive experiments on both\nsimulated and real-world datasets show that CoPLOT outperforms state-of-the-art\nmodels, with even lower communication and computation overhead. Code will be\navailable at https://github.com/CheeryLeeyy/CoPLOT.", "AI": {"tldr": "本文提出CoPLOT框架，通过点级令牌处理点云以实现更好的协作感知，实验结果表明其性能优于现有方法。", "motivation": "现有的协作感知方法通常将中间特征组织成2D BEV表示，丢失了重要的3D结构线索，这项研究旨在通过点级令牌保留结构信息，用于更精确的目标识别和定位。", "method": "引入了点级令牌作为协作感知的中间表示形式，并提出了CoPLOT框架，该框架使用点级优化令牌来处理点云数据。主要包括令牌重新排序、序列建模和多智能体空间对齐等模块。", "result": "在模拟和真实世界的数据集上，CoPLOT模型不仅优于现有的最先进的模型，还具有更低的通信和计算开销。", "conclusion": "CoPLOT框架展示了一种有效的方法来解决点云数据处理中的挑战，并提高了基于点云的协作感知性能。"}}
{"id": "2508.19828", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19828", "abs": "https://arxiv.org/abs/2508.19828", "authors": ["Sikuan Yan", "Xiufeng Yang", "Zuchao Huang", "Ercong Nie", "Zifeng Ding", "Zonggen Li", "Xiaowen Ma", "Hinrich Schütze", "Volker Tresp", "Yunpu Ma"], "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations\n{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant\nentries and reasons over them to produce an answer. Both agents are fine-tuned\nwith outcome-driven RL (PPO and GRPO), enabling adaptive memory management and\nuse with minimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the most\ncompetitive existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behaviors in LLMs, pointing toward richer, more persistent\nreasoning systems.", "AI": {"tldr": "Memory-R1是一种强化学习框架，它赋予大型语言模型使用外部记忆的能力，以改善长期推理能力。", "motivation": "针对大型语言模型缺乏状态和较长推理能力的限制，该研究希望通过引入主动管理的外部记忆来改善此问题。", "method": "Memory-R1包含两个专门的代理：记忆管理器负责执行结构化的记忆操作，回答代理负责挑选相关条目并依据它们生成答案。两个代理通过强化学习进行微调。", "result": "Memory-R1在少量训练数据下表现优于现有基线，并且在多样化的问句类型和语言模型基础上具有良好的泛化能力。", "conclusion": "该研究为大型语言模型如何通过强化学习获得更具代理性和记忆的运作方式提供了洞见。"}}
{"id": "2508.19647", "categories": ["cs.CV", "I.2.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.19647", "abs": "https://arxiv.org/abs/2508.19647", "authors": ["Bikash Kumar Badatya", "Vipul Baghel", "Ravi Hegde"], "title": "UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks", "comment": "This paper has been accepted at the ICIP Satellite Workshop 2025", "summary": "Fine-grained action localization in untrimmed sports videos presents a\nsignificant challenge due to rapid and subtle motion transitions over short\ndurations. Existing supervised and weakly supervised solutions often rely on\nextensive annotated datasets and high-capacity models, making them\ncomputationally intensive and less adaptable to real-world scenarios. In this\nwork, we introduce a lightweight and unsupervised skeleton-based action\nlocalization pipeline that leverages spatio-temporal graph neural\nrepresentations. Our approach pre-trains an Attention-based Spatio-Temporal\nGraph Convolutional Network (ASTGCN) on a pose-sequence denoising task with\nblockwise partitions, enabling it to learn intrinsic motion dynamics without\nany manual labeling. At inference, we define a novel Action Dynamics Metric\n(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects\nmotion boundaries by identifying inflection points in its curvature profile.\nOur method achieves a mean Average Precision (mAP) of 82.66% and average\nlocalization latency of 29.09 ms on the DSV Diving dataset, matching\nstate-of-the-art supervised performance while maintaining computational\nefficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving\nfootage without retraining, demonstrating its practical applicability for\nlightweight, real-time action analysis systems in embedded or dynamic\nenvironments.", "AI": {"tldr": "本文提出了一种轻量级、无监督的基于骨架的动作定位方法，利用ASTGCN和ADM实现了高精度和低延迟的动作定位，计算效率高，对未见过的数据具有良好的泛化能力。", "motivation": "现有的监督和弱监督解决方案依赖于广泛的注释数据集和高性能模型，计算成本高，且对现实世界场景适应性不足。本文旨在提改善这种情况，提出了一种轻量级、计算效率高的无监督方法。", "method": "引入了一种轻量级、无监督的基于骨架的动作定位流水线，利用时空图神经表示。该方法通过在分块的姿势序列去噪任务上预训练注意力机制的时空图卷积网络（ASTGCN），学习内在运动动态，无需任何手动标注。在推理阶段，通过低维ASTGCN嵌入直接计算新型动作动态度量（ADM），通过识别其曲率轮廓中的拐点来检测运动边界。", "result": "该方法在DSV跳水数据集上实现了82.66%的平均精度（mAP）和29.09毫秒的平均定位延迟，达到了最优监督性能，保持了计算效率。", "conclusion": "该方法不仅精度高、延迟低，而且在未见跳水比赛中表现出强大的泛化能力，展示了其在轻量级、实时动作分析系统中的实际应用潜力。"}}
{"id": "2508.19831", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19831", "abs": "https://arxiv.org/abs/2508.19831", "authors": ["Anusha Kamath", "Kanishk Singla", "Rakesh Paul", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis", "comment": null, "summary": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages.", "AI": {"tldr": "为了解决印地语大型语言模型缺乏高质量基准的问题，通过结合人工注释和翻译验证来创建了五个新的评估数据集，并进行全面的基准测试。", "motivation": "评估针对印地语调整的大型语言模型具有挑战性，因为高质量的基准测试不存在，直接将英文数据集翻译为印地语无法捕捉到关键的语言和文化细微差别。", "method": "通过从零开始的人工注释与翻译和验证过程的结合，创建了五个用于评估印地语大型语言模型的数据集：IFEval-Hi、MT-Bench-Hi、GSM8K-Hi、ChatRAG-Hi 和 BFCL-Hi。", "result": "使用这些数据集对支持印地语的开源大型语言模型进行了广泛的基准测试，提供了它们当前能力的详细比较分析。", "conclusion": "数据集的编制过程也提供了一种可复制的方法，用于开发其他低资源语言的基准测试。"}}
{"id": "2508.19649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19649", "abs": "https://arxiv.org/abs/2508.19649", "authors": ["Dongjin Kim", "Jaekyun Ko", "Muhammad Kashif Ali", "Tae Hyun Kim"], "title": "IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising", "comment": "ICCV 2025. Project Page: https://dongjinkim9.github.io/projects/idf/", "summary": "Image denoising is a fundamental challenge in computer vision, with\napplications in photography and medical imaging. While deep learning-based\nmethods have shown remarkable success, their reliance on specific noise\ndistributions limits generalization to unseen noise types and levels. Existing\napproaches attempt to address this with extensive training data and high\ncomputational resources but they still suffer from overfitting. To address\nthese issues, we conduct image denoising by utilizing dynamically generated\nkernels via efficient operations. This approach helps prevent overfitting and\nimproves resilience to unseen noise. Specifically, our method leverages a\nFeature Extraction Module for robust noise-invariant features, Global\nStatistics and Local Correlation Modules to capture comprehensive noise\ncharacteristics and structural correlations. The Kernel Prediction Module then\nemploys these cues to produce pixel-wise varying kernels adapted to local\nstructures, which are then applied iteratively for denoising. This ensures both\nefficiency and superior restoration quality. Despite being trained on\nsingle-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse\nnoise types and levels, demonstrating the promise of iterative dynamic\nfiltering for practical image denoising.", "AI": {"tldr": "本文提出的新图像去噪方法利用动态生成的内核，通过高效的运算避免过拟合并提高对未知噪声的鲁棒性，展示出在泛化能力上的优越性。", "motivation": "虽然基于深度学习的方法在图像去噪方面取得了显著的成功，但它们对特定噪声分布的依赖性限制了它们对未知噪声类型和水平的泛化能力。现有的方法试图用大量训练数据和高计算资源来解决这个问题，但这仍然会遭受过拟合。", "method": "本文提出了一种利用动态生成的内核进行图像去噪的方法，通过高效的运算避免过拟合，提高对未知噪声的鲁棒性。该方法包含特征提取模块以提取鲁棒的噪声不变特征，全局统计和局部相关模块以捕捉全面的噪声特征和结构关联，以及内核预测模块以生成适应局部结构的像素级变化内核。", "result": "尽管模型是在单级高斯噪声上训练的，但该紧凑模型(~0.04M)在各种噪声类型和水平上表现出色，展示了迭代动态滤波在实际图像去噪中的潜力。", "conclusion": "该论文展示了迭代动态滤波在实际图像去噪中的潜力，提出的新方法在鲁棒性和高效性方面更加优越。"}}
{"id": "2508.19836", "categories": ["cs.CL", "physics.ed-ph"], "pdf": "https://arxiv.org/pdf/2508.19836", "abs": "https://arxiv.org/abs/2508.19836", "authors": ["Jonas Timmann Mjaaland", "Markus Fleten Kreutzer", "Halvor Tyseng", "Rebeckah K. Fussell", "Gina Passante", "N. G. Holmes", "Anders Malthe-Sørenssen", "Tor Ole B. Odden"], "title": "Scalable and consistent few-shot classification of survey responses using text embeddings", "comment": null, "summary": "Qualitative analysis of open-ended survey responses is a commonly-used\nresearch method in the social sciences, but traditional coding approaches are\noften time-consuming and prone to inconsistency. Existing solutions from\nNatural Language Processing such as supervised classifiers, topic modeling\ntechniques, and generative large language models have limited applicability in\nqualitative analysis, since they demand extensive labeled data, disrupt\nestablished qualitative workflows, and/or yield variable results. In this\npaper, we introduce a text embedding-based classification framework that\nrequires only a handful of examples per category and fits well with standard\nqualitative workflows. When benchmarked against human analysis of a conceptual\nphysics survey consisting of 2899 open-ended responses, our framework achieves\na Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in\nan exhaustive coding scheme. We further show how performance of this framework\nimproves with fine-tuning of the text embedding model, and how the method can\nbe used to audit previously-analyzed datasets. These findings demonstrate that\ntext embedding-assisted coding can flexibly scale to thousands of responses\nwithout sacrificing interpretability, opening avenues for deductive qualitative\nanalysis at scale.", "AI": {"tldr": "本文提出了一种基于文本嵌入的分类框架，用于加速和提高定性分析的准确性，且在无需大量标注数据下达到了与人类专家相似的效果。", "motivation": "传统编码方法耗时且容易出现不一致，现有的NLP解决方案（如监督分类器、主题建模技术、生成型语言模型）在定性分析中的适用性有限。", "method": "介绍了一种基于文本嵌入的分类框架，该框架只需要每个类别少量示例，并且能很好地适应标准的定性工作流程。", "result": "该框架在对一个包含2899条开放式回答的概念物理调查进行基准测试时，与专家人类编码者的全面编码方案相比，Cohen's Kappa值介于0.74到0.83之间。", "conclusion": "这种方法可以灵活扩展到数千个响应，同时保持解释性，为大规模的定性分析开辟了新的途径。"}}
