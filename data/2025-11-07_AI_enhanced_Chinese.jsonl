{"id": "2511.03765", "categories": ["cs.CV", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03765", "abs": "https://arxiv.org/abs/2511.03765", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Jae-Jin Lee", "Woojoo Lee"], "title": "LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices", "comment": "8 pages, 6 figures, 2 tables, DATE 2026 accepted paper", "summary": "On-device fine-tuning of CNNs is essential to withstand domain shift in edge\napplications such as Human Activity Recognition (HAR), yet full fine-tuning is\ninfeasible under strict memory, compute, and energy budgets. We present\nLoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on\nLow-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies\nTensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional\nlayers, (ii) selectively updates only the output-side core with\nzero-initialization to keep the auxiliary path inactive at the start, and (iii)\nfuses the update back into dense kernels, leaving inference cost unchanged.\nThis design preserves convolutional structure and reduces the number of\ntrainable parameters by up to two orders of magnitude compared to full\nfine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves\naccuracy within 4.7% of full fine-tuning while updating at most 1.49% of\nparameters, consistently outperforming prior parameter-efficient baselines\nunder similar budgets. On a Jetson Orin Nano, TT-SVD initialization and\nselective-core training yield 1.4-3.8x faster convergence to target F1.\nLoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN\nadaptation practical for edge platforms.", "AI": {"tldr": "LoRA-Edge是一种参数高效的微调方法，它通过结合低秩适配（LoRA）和张量列车辅助，再采用TT-SVD、选择性核心更新和参数融合技术，在保持推理成本不变的同时大幅减少了可训练参数的数量，提高了边缘设备上CNN适应的效率与性能。", "motivation": "由于内存、计算力和能耗受限，完整的迁移学习在边缘应用中不可行。针对设备端CNN的领域转移问题，需要更高效的微调方法以适应边缘应用程序，如人体活动识别（HAR）。", "method": "该论文提出了LoRA-Edge方法，它基于低秩适配（LoRA）并结合张量列车辅助。该方法(i)使用张量列车奇异值分解（TT-SVD）应用于预训练卷积层，(ii)选择性地仅更新输出侧核心并初始化为零以使辅助路径在开始时处于非活动状态，(iii)将更新融合回密集核中，从而不影响推理成本。这种方法保持了卷积结构，并将可训练的参数数量减少了两个数量级，相比于完全微调。", "result": "在不同的人体活动数据集和CNN体系结构上，LoRA-Edge的准确率相较于全微调仅差4.7%，而更新的参数量仅为全微调的1.49%以内，全面优于相似预算下的先前基准，且在Jetson Orin Nano上，TT-SVD初始化和选择性核心训练使收敛速度提高了1.4-3.8倍。", "conclusion": "LoRA-Edge方法实现了参数高效的设备端CNN适应性调整，使其适用于边缘设备，并在识别任务上保持了高的准确性。"}}
{"id": "2511.03819", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.03819", "abs": "https://arxiv.org/abs/2511.03819", "authors": ["Ozan Kanbertay", "Richard Vogg", "Elif Karakoc", "Peter M. Kappeler", "Claudia Fichtel", "Alexander S. Ecker"], "title": "SILVI: Simple Interface for Labeling Video Interactions", "comment": null, "summary": "Computer vision methods are increasingly used for the automated analysis of\nlarge volumes of video data collected through camera traps, drones, or direct\nobservations of animals in the wild. While recent advances have focused\nprimarily on detecting individual actions, much less work has addressed the\ndetection and annotation of interactions -- a crucial aspect for understanding\nsocial and individualized animal behavior. Existing open-source annotation\ntools support either behavioral labeling without localization of individuals,\nor localization without the capacity to capture interactions. To bridge this\ngap, we present SILVI, an open-source labeling software that integrates both\nfunctionalities. SILVI enables researchers to annotate behaviors and\ninteractions directly within video data, generating structured outputs suitable\nfor training and validating computer vision models. By linking behavioral\necology with computer vision, SILVI facilitates the development of automated\napproaches for fine-grained behavioral analyses. Although developed primarily\nin the context of animal behavior, SILVI could be useful more broadly to\nannotate human interactions in other videos that require extracting dynamic\nscene graphs. The software, along with documentation and download instructions,\nis available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.", "AI": {"tldr": "本文介绍了一款名为SILVI的开源软件，它可以同时进行行为标注和个体定位，并能生成结构化输出，方便训练计算机视觉模型。这项工具填补了现有工具在注释交互行为上的空缺。", "motivation": "研究动机是为了解决现有的标注工具无法同时支持行为标注和个体定位的能力，尤其是在理解动物的社会和个体化行为时，这些工具显得不足。SILVI旨在填补这一空白，使研究人员能够全面地注释行为和交互。", "method": "内容中提及的研究方法是一种结合行为标注与个体定位功能的开源标注软件SILVI，它可以直接在视频数据中注释行为和交互，生成适用于训练和验证计算机视觉模型的结构化输出。", "result": "研究结果表明SILVI可以有效地整合行为标注和个体定位功能，生成结构化输出，适用于行为生态学和计算机视觉的自动化方法开发。此外，该工具具有更广泛的适用性，可以用于注释其他视频中的人际互动，尤其是那些需要提取动态场景图的视频。", "conclusion": "结论是SILVI为研究动物行为提供了一个强有力的工具，能够帮助研究人员更好地理解和分析动物的社会和个体化行为，且具有应用于人类互动标注的潜力。"}}
