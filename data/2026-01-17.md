<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-Driven Preference Data Synthesis for Proactive Prediction of the Next User Utterance in Human-Machine Dialogue](https://arxiv.org/abs/2601.09713)
*Jinqiang Wang,Huansheng Ning,Jianguo Ding,Tao Zhu,Liming Chen,Chris Nugent*

Main category: cs.CL

> 提出ProUtt方法，改进了下一个用户发言预测的能力，解决了现有方法在隐私和计算成本上的问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有API方法的隐私问题，以及通用LLM本地部署的计算成本高问题；现有方法难以显式建模导致用户下一次发言的意图推理，且难以定义和合成偏好和非偏好推理过程。

**Method:** 训练紧凑的任务特定语言模型（LLM）用于预测用户的下一个发言，提出ProUtt方法，该方法将对话历史转换为意图树并显式地建模意图推理轨迹，从利用和探索的角度预测下一个可能轨迹。此外，通过扰动或修正意图树路径来构建偏好和非偏好推理过程。

**Result:** 使用'LLM作为评判'和人类判断进行的广泛评估中，ProUtt在四个基准数据集上优于现有的数据综合方法、用户模拟器和商用LLM API。

**Conclusion:** ProUtt方法通过显式建模意图推理轨迹和构建偏好与非偏好推理过程，能够有效改进用户的下一次发言预测。公开了代码和合成的数据集以促进未来研究。

**Abstract:** Proactively predicting a users next utterance in human-machine dialogue can streamline interaction and improve user experience. Existing commercial API-based solutions are subject to privacy concerns while deploying general-purpose LLMs locally remains computationally expensive. As such, training a compact, task-specific LLM provides a practical alternative. Although user simulator methods can predict a user's next utterance, they mainly imitate their speaking style rather than advancing the dialogue. Preference data synthesis has been investigated to generate data for proactive next utterance prediction and help align LLMs with user preferences. Yet existing methods lack the ability to explicitly model the intent reasoning that leads to the user's next utterance and to define and synthesize preference and non-preference reasoning processes for predicting the user's next utterance.To address these challenges, we propose ProUtt, an LLM-driven preference data synthesis method for proactive next utterance prediction. ProUtt converts dialogue history into an intent tree and explicitly models intent reasoning trajectories by predicting the next plausible path from both exploitation and exploration perspectives. It then constructs preference and non-preference reasoning processes by perturbing or revising intent tree paths at different future turns. Extensive evaluations using LLM-as-a-judge and human judgments demonstrate that ProUtt consistently outperforms existing data synthesis methods, user simulators, and commercial LLM APIs across four benchmark datasets. We release both the code and the synthesized datasets to facilitate future research.

</details>


### [2] [Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines](https://arxiv.org/abs/2601.09714)
*Devesh Saraogi,Rohit Singhee,Dhruv Kumar*

Main category: cs.CL

> 本文评估了多步骤AI代理系统生成科研计划的能力，发现某些特定的多步骤系统能更有效地创造新颖而合理的研究提案。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型（LLMs）集成到科研生态系统中，产生了关于AI生成研究的创造性和原创性的重要问题。通过对多步骤系统（采用迭代推理、演化搜索和递归分解的代理工作流）的研究，探讨这些系统是否能够生成更为新颖和可行的研究计划。

**Method:** 本研究对比了五种推理架构在生成科研计划的新颖性、可行性和影响力方面的表现，包括基于反思的迭代优化、Sakana AI v2演化算法、Google Co-Scientist多智能体框架、GPT Deep Research（GPT-5.1）的递归分解和Gemini 3 Pro的多模态长上下文流程。

**Result:** 研究结果显示，基于分解的和长上下文流程的架构在新颖性方面得分平均为4.17/5, 而基于反思的方法得分显著较低，为2.33/5。跨科研领域的表现不一，但高性能的工作流程均保持了可行性并且没有牺牲创造力。

**Conclusion:** 这些研究结果证明，精心设计的多阶段代理工作流程能够推进AI辅助的研究构思过程。

**Abstract:** The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.

</details>


### [3] [Introducing Axlerod: An LLM-based Chatbot for Assisting Independent Insurance Agents](https://arxiv.org/abs/2601.09715)
*Adam Bradley,John Hastings,Khandaker Mamun Ahmed*

Main category: cs.CL

> 本研究展示了一个名为Axlerod的高级AI驱动会话系统的开发，它能够解析用户意图，访问政策数据库，并提供实时反馈，显著提高了独立保险代理人的运营效率。

<details>
  <summary>Details</summary>

**Motivation:** 随着人工智能技术的采用，特别是智能聊天机器人的使用，保险行业正经历着一场范式转变。本研究旨在通过设计能够自动执行复杂工作流并提高用户参与度的AI驱动系统来改进保险业务运营。

**Method:** 本研究设计并实现了一个名为Axlerod的AI驱动的会话界面，利用自然语言处理（NLP）、检索增强生成（RAG）以及领域知识整合，来提高独立保险代理人的运营效率。Axlerod在解析用户意图、访问结构化政策数据库和提供实时相关响应方面显示出了强大能力。

**Result:** 实验结果表明，Axlerod在解析用户意图、访问结构化政策数据库以及提供实时相关响应方面都表现出了较高的准确率和效率，整体准确率为93.18%，平均搜索时间减少了2.42秒。

**Conclusion:** 本研究结果表明，Axlerod在政策检索任务中表现出色，实现了93.18%的整体准确率，同时将平均检索时间缩短了2.42秒。该研究为企业级AI在保险科技中的应用贡献了一篇重要文章，特别侧重于辅助代理的人工智能架构。

**Abstract:** The insurance industry is undergoing a paradigm shift through the adoption of artificial intelligence (AI) technologies, particularly in the realm of intelligent conversational agents. Chatbots have evolved into sophisticated AI-driven systems capable of automating complex workflows, including policy recommendation and claims triage, while simultaneously enabling dynamic, context-aware user engagement. This paper presents the design, implementation, and empirical evaluation of Axlerod, an AI-powered conversational interface designed to improve the operational efficiency of independent insurance agents. Leveraging natural language processing (NLP), retrieval-augmented generation (RAG), and domain-specific knowledge integration, Axlerod demonstrates robust capabilities in parsing user intent, accessing structured policy databases, and delivering real-time, contextually relevant responses. Experimental results underscore Axlerod's effectiveness, achieving an overall accuracy of 93.18% in policy retrieval tasks while reducing the average search time by 2.42 seconds. This work contributes to the growing body of research on enterprise-grade AI applications in insurtech, with a particular focus on agent-assistive rather than consumer-facing architectures.

</details>


### [4] [Opportunities and Challenges of Natural Language Processing for Low-Resource Senegalese Languages in Social Science Research](https://arxiv.org/abs/2601.09716)
*Derguene Mbaye,Tatiana D. P. Mbengue,Madoune R. Seye,Moussa Diallo,Mamadou L. Ndiaye,Dimitri S. Adjanohoun,Cheikh S. Wade,Djiby Sow,Jean-Claude B. Munyaka,Jerome Chenal*

Main category: cs.CL

> 本文对塞内加尔的六种官方语言在自然语言处理领域的发展进行了全面回顾，总结了语言学、社会技术和基础设施方面的因素，同时创建了一个集中式的资源库来支持资源的共享和研究的可重复性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于非洲语言在NLP技术进步中的代表性不足，本文旨在提供塞内加尔宪法认可的六种官方语言在NLP领域的首个全面审视，识别并解决数据、工具和基准方面的问题。

**Method:** 综合分析了塞内加尔六种官方语言在自然语言处理（NLP）领域的进展和挑战，考虑了语言学、社会技术以及基础设施因素，并针对文本规整化、机器翻译和语音处理的现有举措进行了分析。

**Result:** 创建了一个集中的GitHub存储库，汇集了面向这些语言的一系列NLP任务的公共可用资源，以促进合作和可重复性，强调伦理数据治理、开放资源以及跨学科合作的重要性。

**Conclusion:** 文章提出了一条通往可持续、以社区为中心的塞内加尔语言NLP生态系统的路线图，特别强调社会科学研究多语言转录、翻译和检索管道的应用，旨在提高田野研究的效率和包容性。

**Abstract:** Natural Language Processing (NLP) is rapidly transforming research methodologies across disciplines, yet African languages remain largely underrepresented in this technological shift. This paper provides the first comprehensive overview of NLP progress and challenges for the six national languages officially recognized by the Senegalese Constitution: Wolof, Pulaar, Sereer, Joola, Mandingue, and Soninke. We synthesize linguistic, sociotechnical, and infrastructural factors that shape their digital readiness and identify gaps in data, tools, and benchmarks. Building on existing initiatives and research works, we analyze ongoing efforts in text normalization, machine translation, and speech processing. We also provide a centralized GitHub repository that compiles publicly accessible resources for a range of NLP tasks across these languages, designed to facilitate collaboration and reproducibility. A special focus is devoted to the application of NLP to the social sciences, where multilingual transcription, translation, and retrieval pipelines can significantly enhance the efficiency and inclusiveness of field research. The paper concludes by outlining a roadmap toward sustainable, community-centered NLP ecosystems for Senegalese languages, emphasizing ethical data governance, open resources, and interdisciplinary collaboration.

</details>


### [5] [SALP-CG: Standard-Aligned LLM Pipeline for Classifying and Grading Large Volumes of Online Conversational Health Data](https://arxiv.org/abs/2601.09717)
*Yiwei Yan,Hao Li,Hua He,Gong Kai,Zhengyi Yang,Guanfeng Liu*

Main category: cs.CL

> SALP-CG运用大型语言模型分类并评估在线医疗对话数据中的隐私风险，有效解决了现有方法在这一领域的不足。

<details>
  <summary>Details</summary>

**Motivation:** 在线医疗咨询产生了大量的对话健康数据，这些数据通常包含受保护的健康信息，需要一种统一的标准和可靠的自动方法来分类这些数据并根据政策和实践分配风险等级。而现有方法在这方面尚存在不足。

**Method:** 本研究提出了一种基于大型语言模型的数据提取管道SALP-CG，用于分类和评估在线医疗对话数据中的隐私风险。采用了少样本指导、JSON Schema约束解码和确定的高风险规则来实现这一目标。

**Result:** 在MedDialog-CN基准测试中，该模型获得了强大的实体计数、模式合规性和敏感性分级，最强的模型在最高级预测中获得了微F1得分为0.900。

**Conclusion:** SALP-CG提供了一种实用的方法，可以帮助跨语言模型分类在线医疗对话数据，并对其敏感性进行分级。

**Abstract:** Online medical consultations generate large volumes of conversational health data that often embed protected health information, requiring robust methods to classify data categories and assign risk levels in line with policies and practice. However, existing approaches lack unified standards and reliable automated methods to fulfill sensitivity classification for such conversational health data. This study presents a large language model-based extraction pipeline, SALP-CG, for classifying and grading privacy risks in online conversational health data. We concluded health-data classification and grading rules in accordance with GB/T 39725-2020. Combining few-shot guidance, JSON Schema constrained decoding, and deterministic high-risk rules, the backend-agnostic extraction pipeline achieves strong category compliance and reliable sensitivity across diverse LLMs. On the MedDialog-CN benchmark, models yields robust entity counts, high schema compliance, and accurate sensitivity grading, while the strongest model attains micro-F1=0.900 for maximum-level prediction. The category landscape stratified by sensitivity shows that Level 2-3 items dominate, enabling re-identification when combined; Level 4-5 items are less frequent but carry outsize harm. SALP-CG reliably helps classify categories and grading sensitivity in online conversational health data across LLMs, offering a practical method for health data governance. Code is available at https://github.com/dommii1218/SALP-CG.

</details>


### [6] [StatLLaMA: A multi-stage training framework for building a domain-optimized statistical language model](https://arxiv.org/abs/2601.09718)
*Jing-Yi Zeng,Guan-Hua Huang*

Main category: cs.CL

> 研究了如何使用轻量级的LLaMA-3.2-3B作为基础模型，来有效地构建一个专门用于统计学的大型语言模型。研究结果表明，直接使用基础模型难以发展出有意义的统计推理能力，而从一个已经接受了指令调优的基础模型开始，可以获得有效的领域专业化能力。最终模型StatLLaMA在数学推理、常识推理和统计学专业知识的基准测试中表现出色，提供了开发资源高效的统计学大型语言模型的实际蓝图。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是为了寻找一种方法，能够高效地构建一个专门针对统计学领域的大型语言模型，同时保持良好的资源效率和模型性能。

**Method:** 研究采用了系统对比三种多阶段训练方法的策略，这些方法从基础模型的训练开始，到指令调优，再到监督微调（SFT）、从人类反馈中进行强化学习（RLHF）的偏好对齐，以及下游任务适应。

**Result:** 研究表明，直接从基础模型开始进行训练难以发展出有意义的统计推理能力，即使经过大量的指令调优、监督微调或RLHF偏好对齐也无法显著改进。而从LLaMA-3.2-3B指令调优版本开始进行训练则能有效实现领域专业化。

**Conclusion:** 研究结论是，为了开发出一种既具有领域专业知识又具备良好通用推理能力的统计学大型语言模型，应该选择从已经接受过指令调优的基础模型开始进行训练。此外，证实了直接偏好优化可以提供稳定且有效的RLHF偏好对齐，并指出下游微调强度需极低以避免高度优化模型的灾难性遗忘问题。

**Abstract:** This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines, starting from a base FM with no instruction-following capability, a base FM augmented with post-hoc instruction tuning, and an instruction-tuned FM with strong general reasoning abilities across continual pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) preference alignment, and downstream task adaptation. Results show that pipelines beginning with a base FM fail to develop meaningful statistical reasoning, even after extensive instruction tuning, SFT, or RLHF alignment. In contrast, starting from LLaMA-3.2-3B-Instruct enables effective domain specialization. A comprehensive evaluation of SFT variants reveals clear trade-offs between domain expertise and general reasoning ability. We further demonstrate that direct preference optimization provides stable and effective RLHF preference alignment. Finally, we show that downstream fine-tuning must be performed with extremely low intensity to avoid catastrophic forgetting in highly optimized models. The final model, StatLLaMA, achieves strong and balanced performance on benchmarks of mathematical reasoning, common-sense reasoning, and statistical expertise, offering a practical blueprint for developing resource-efficient statistical LLMs. The code is available at https://github.com/HuangDLab/StatLLaMA.

</details>


### [7] [Bounded Hyperbolic Tangent: A Stable and Efficient Alternative to Pre-Layer Normalization in Large Language Models](https://arxiv.org/abs/2601.09719)
*Hoyoon Byun,Youngjun Choi,Taero Kim,Sungrae Park,Kyungwoo Song*

Main category: cs.CL

> 本文提出了Bounded Hyperbolic Tanh (BHyT)，作为一种宏替换方法，解决了预先层归一化(Pre-LN)无法处理深度增长问题，提供了更好的稳定性和效率,平均训练速度和token生成吞吐量都有所提高。

<details>
  <summary>Details</summary>

**Motivation:** 改进大型语言模型的训练稳定性和效率，解决预先层归一化(Pre-LN)方法在深度增长时不稳定和效率低下的问题。

**Method:** 提出了Bounded Hyperbolic Tanh (BHyT)，它结合了tanh非线性和显式的数据驱动输入边界，将激活值保持在非饱和范围内，并在整个深度中阻止激活值的幅度和方差的增长。

**Result:** 实验结果表明，BHyT在预训练期间提高了稳定性和效率，平均训练速度提高了15.8%，平均token生成吞吐量提高了4.2%，而且在多个语言理解和推理基准上与RMSNorm相比，匹配或超越了其推理性能和鲁棒性。

**Conclusion:** BHyT在保持模型性能的同时，显著提高了大型语言模型的训练效率和稳定性，为未来的模型优化提供了新的路径。

**Abstract:** Pre-Layer Normalization (Pre-LN) is the de facto choice for large language models (LLMs) and is crucial for stable pretraining and effective transfer learning. However, Pre-LN is inefficient due to repeated statistical calculations and suffers from the curse of depth. As layers grow, the magnitude and variance of the hidden state escalate, destabilizing training. Efficiency-oriented normalization-free methods such as Dynamic Tanh (DyT) improve speed but remain fragile at depth. To jointly address stability and efficiency, we propose Bounded Hyperbolic Tanh (BHyT), a drop-in replacement for Pre-LN. BHyT couples a tanh nonlinearity with explicit, data-driven input bounding to keep activations within a non-saturating range. It prevents depth-wise growth in activation magnitude and variance and comes with a theoretical stability guarantee. For efficiency, BHyT computes exact statistics once per block and replaces a second normalization with a lightweight variance approximation, enhancing efficiency. Empirically, BHyT demonstrates improved stability and efficiency during pretraining, achieving an average of 15.8% faster training and an average of 4.2% higher token generation throughput compared to RMSNorm., while matching or surpassing its inference performance and robustness across language understanding and reasoning benchmarks. Our code is available at: https://anonymous.4open.science/r/BHyT

</details>


### [8] [Uncertainty-Aware Dynamic Knowledge Graphs for Reliable Question Answering](https://arxiv.org/abs/2601.09720)
*Yu Takahashi,Shun Takeuchi,Kexuan Xin,Guillaume Pelat,Yoshiaki Ikai,Junya Saito,Jonathan Vitale,Shlomo Berkovsky,Amin Beheshti*

Main category: cs.CL

> 本论文展示了通过不确定性感知动态知识图谱(KG)提高问答系统可靠性和透明性的框架，特别在医疗场景中进行实例化和验证。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于KG的问答体系往往忽略了知识随着时间变化和推理中的不确定性，本工作旨在构建一个能够应对这些挑战的不确定性感知动态KG框架。

**Method:** 框架包含动态KG构建、不确定性感知检索和置信度评分、以及交互式界面设计。同时在医疗领域，利用电子健康记录来构建个性化KG，并展示不确定性在就诊期间的可视化。

**Result:** 通过在医疗领域的实例化和验证，如在死亡率预测任务上的表现，展示了这一框架如何提高问答系统的可靠性和透明度。

**Conclusion:** 不确定性感知动态KG框架在高风险应用中的问答系统中展现出增强可靠性的潜力，特别是在医疗领域。

**Abstract:** Question answering (QA) systems are increasingly deployed across domains. However, their reliability is undermined when retrieved evidence is incomplete, noisy, or uncertain. Existing knowledge graph (KG) based QA frameworks typically represent facts as static and deterministic, failing to capture the evolving nature of information and the uncertainty inherent in reasoning. We present a demonstration of uncertainty-aware dynamic KGs, a framework that combines (i) dynamic construction of evolving KGs, (ii) confidence scoring and uncertainty-aware retrieval, and (iii) an interactive interface for reliable and interpretable QA. Our system highlights how uncertainty modeling can make QA more robust and transparent by enabling users to explore dynamic graphs, inspect confidence-annotated triples, and compare baseline versus confidence-aware answers. The target users of this demo are clinical data scientists and clinicians, and we instantiate the framework in healthcare: constructing personalized KGs from electronic health records, visualizing uncertainty across patient visits, and evaluating its impact on a mortality prediction task. This use case demonstrates the broader promise of uncertainty-aware dynamic KGs for enhancing QA reliability in high-stakes applications.

</details>


### [9] [Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox](https://arxiv.org/abs/2601.09721)
*Vahideh Zolfaghari*

Main category: cs.CL

> 研究评估了大型语言模型在儿童医疗咨询服务中面对家长焦虑时的行为表现，结果显示小型模型的安全性优于大型模型。

<details>
  <summary>Details</summary>

**Motivation:** 研究大型语言模型（LLM）在儿科医疗咨询中面对父母焦虑时的安全性，因为这种场景可能揭示模型的潜在漏洞。

**Method:** 使用PediatricAnxietyBench基准测试集，包含300个查询（150个真实和150个对抗性查询），测试了三个模型：Llama-3.3-70B和Llama-3.1-8B（Groq）以及Mistral-7B（HuggingFace）。使用0-15评分体系评估模型的安全性。

**Result:** Mistral-7B得分最高，平均得分10.39，其次是Llama-3.1-8B，平均得分9.70，Llama-3.3-70B表现最弱。Mistral-7B在对抗性查询中表现出更强的安全性。

**Conclusion:** 研究表明，模型的安全性更多取决于其对齐和架构而不是其规模。结果强调了具有针对性的安全性训练和对抗性测试的重要性。

**Abstract:** Background Large language models (LLMs) are increasingly deployed in medical consultations, yet their safety under realistic user pressures remains understudied. Prior assessments focused on neutral conditions, overlooking vulnerabilities from anxious users challenging safeguards. This study evaluated LLM safety under parental anxiety-driven adversarial pressures in pediatric consultations across models and platforms. Methods PediatricAnxietyBench, from a prior evaluation, includes 300 queries (150 authentic, 150 adversarial) spanning 10 topics. Three models were assessed via APIs: Llama-3.3-70B and Llama-3.1-8B (Groq), Mistral-7B (HuggingFace), yielding 900 responses. Safety used a 0-15 scale for restraint, referral, hedging, emergency recognition, and non-prescriptive behavior. Analyses employed paired t-tests with bootstrapped CIs. Results Mean scores: 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B). Llama-3.1-8B outperformed Llama-3.3-70B by +0.66 (p=0.0001, d=0.225). Models showed positive adversarial effects, Mistral-7B strongest (+1.09, p=0.0002). Safety generalized across platforms; Llama-3.3-70B had 8% failures. Seizures vulnerable (33% inappropriate diagnoses). Hedging predicted safety (r=0.68, p<0.001). Conclusions Evaluation shows safety depends on alignment and architecture over scale, with smaller models outperforming larger. Evolution to robustness across releases suggests targeted training progress. Vulnerabilities and no emergency recognition indicate unsuitability for triage. Findings guide selection, stress adversarial testing, and provide open benchmark for medical AI safety.

</details>


### [10] [ADMEDTAGGER: an annotation framework for distillation of expert knowledge for the Polish medical language](https://arxiv.org/abs/2601.09722)
*Franciszek Górski,Andrzej Czyżewski*

Main category: cs.CL

> 该研究利用多语言预训练模型Llama3.1对波兰语医学文本进行标注，解决了标注资源匮乏的问题，并基于标注数据训练了多个分类器模型，其中DistilBERT模型在临床分类任务中表现最佳。

<details>
  <summary>Details</summary>

**Motivation:** 项目需要标注大量波兰语医学文本资源，但现有标注资源不足，因此利用预训练语言模型进行自动标注。

**Method:** 使用多语言预训练模型Llama3.1对五类临床医学文本进行标注，并基于标注数据训练了三种基于BERT架构的分类器模型。

**Result:** DistilBERT模型在五个临床分类任务中表现出色，F1值均大于0.80，在三个分类中F1值超过0.93。

**Conclusion:** 研究发现，对于医学文本分类任务，小型的DistilBERT模型不仅具有很好的分类效果，还具有比大型语言模型更小的模型尺寸和更低的资源消耗。

**Abstract:** In this work, we present an annotation framework that demonstrates how a multilingual LLM pretrained on a large corpus can be used as a teacher model to distill the expert knowledge needed for tagging medical texts in Polish. This work is part of a larger project called ADMEDVOICE, within which we collected an extensive corpus of medical texts representing five clinical categories - Radiology, Oncology, Cardiology, Hypertension, and Pathology. Using this data, we had to develop a multi-class classifier, but the fundamental problem turned out to be the lack of resources for annotating an adequate number of texts. Therefore, in our solution, we used the multilingual Llama3.1 model to annotate an extensive corpus of medical texts in Polish. Using our limited annotation resources, we verified only a portion of these labels, creating a test set from them. The data annotated in this way were then used for training and validation of 3 different types of classifiers based on the BERT architecture - the distilled DistilBERT model, BioBERT fine-tuned on medical data, and HerBERT fine-tuned on the Polish language corpus. Among the models we trained, the DistilBERT model achieved the best results, reaching an F1 score > 0.80 for each clinical category and an F1 score > 0.93 for 3 of them. In this way, we obtained a series of highly effective classifiers that represent an alternative to large language models, due to their nearly 500 times smaller size, 300 times lower GPU VRAM consumption, and several hundred times faster inference.

</details>


### [11] [SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels](https://arxiv.org/abs/2601.09723)
*Guancheng Du,Yong Hu,Wenqing Wang,Yaming Yang,Jiaheng Gao*

Main category: cs.CL

> 研究团队引入了SagaScale，这是一个基于全本小说的真实、可扩展且高质量的长上下文基准测试，可以用来更好地理解和评估LLMs在处理复杂长文上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型（LLMs）取得了显著进展，但对于理解和处理长且复杂的文档仍然具有挑战性。许多长上下文基准存在任务真实性、数据可扩展性及数据质量等方面的限制。为此，研究团队开发了SagaScale基准。

**Method:** SagaScale是一个基于全本小说的真实、可扩展且高质量的长上下文基准测试，利用自动化数据收集管道和外部资源（如维基百科页面）生成问题答案对，但这些外部资源仅用于基准测试构建，不用于评估过程中。

**Result:** 评估了12个前沿LLMs和三种长上下文方法：Naïve RAG，Agentic RAG，Long Context。关键发现包括直接向LLMs提供完整上下文可以大幅胜过其他方法，大部分LLMs对长上下文的表现仍不尽如人意，尤其是在Gemini-2.5-Pro中表现更为突出以及Agentic RAG有效解决了Naïve RAG中的检索瓶颈。

**Conclusion:** 文章介绍了SagaScale长上下文基准测试，展示了其与其他方法相比的关键洞察，并公开释放了SagaScale基准测试和数据收集代码库，以促进未来的研究。

**Abstract:** Large Language Models (LLMs) have shown significant progress, but understanding long and complex documents remains challenging. Many long-context benchmarks have been proposed, but they face several limitations, including task realism, data scalability, and data quality. To this end, we introduce SagaScale, a realistic, scalable, and high-quality long-context benchmark built from full-length novels. The entire benchmark is constructed using an automated data collection pipeline that utilizes external resources (e.g., Wikipedia pages) to curate question-answer pairs. Critically, these external resources are provided only for benchmark construction and not during evaluation, which allows LLMs to curate complex questions that go beyond what they can answer during evaluation. SagaScale is also bilingual and offers the largest context length to date, with average token counts exceeding 250K for English novels and 320K for Chinese novels. Our evaluation across 12 frontier LLMs and three long-context methods -- Naïve RAG, Agentic RAG, and Long Context -- yields key insights, including: (1) Directly supplying the full context to the LLM can outperform other methods by a large margin; (2) Most LLMs still struggle with lengthy contexts, but Gemini-2.5-Pro stands out as an exception; and (3) Agentic RAG effectively addresses the retrieval bottleneck in Naïve RAG. Finally, we publicly release the SagaScale benchmark and our data collection codebase to facilitate future research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [12] [Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification](https://arxiv.org/abs/2601.09806)
*Shahrzad Sayyafzadeh,Hongmei Chi,Shonda Bernadin*

Main category: cs.CV

> The paper outlines a pipeline for creating imperceptible adversarial patches against facial recognition systems using FGSM and a diffusion model, with applications in forensic analysis and system security testing. The effectiveness and subtlety of the adversarial patches were evaluated, achieving high SSIM scores.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to study and develop methods that can potentially compromise facial biometric systems, which can be valuable for forensic analysis and security testing to improve the robustness of recognition systems against adversarial attacks.

**Method:** The paper introduces an end-to-end pipeline for creating, refining, and assessing adversarial patches aimed at undermining facial biometric systems. The technique starts by using FGSM to generate adversarial patches targeting an identity classifier and then employs a diffusion model combined with reverse diffusion to improve the imperceptibility of these patches by means of Gaussian smoothing and dynamic brightness adjustment. The refined patches are tested on facial images to measure evasion of recognition while still maintaining a natural appearance.

**Result:** The study evaluates the refined adversarial patches through changes in identity classification, captioning outcomes from a Vision Transformer (ViT)-GPT2 model, and vulnerability testing under adversarial conditions. Additionally, the paper showcases successful detection and analysis of adversarial patches and samples using perceptual hashing and segmentation with an SSIM (Structural Similarity Index) of 0.95, indicating high fidelity in similarity between adversarial and genuine images.

**Conclusion:** The research concludes with the presentation of an effective pipeline that can generate high-fidelity adversarial patches, assess their ability to evade facial recognition systems, and analyze their characteristics using advanced detection methods such as perceptual hashing and segmentation.

**Abstract:** This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems, with applications in forensic analysis and security testing. We utilize FGSM to generate adversarial noise targeting an identity classifier and employ a diffusion model with reverse diffusion to enhance imperceptibility through Gaussian smoothing and adaptive brightness correction, thereby facilitating synthetic adversarial patch evasion. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person's identity for adversarial images, supporting forensic interpretation and documentation for identity evasion and recognition attacks. The pipeline evaluates changes in identity classification, captioning results, and vulnerabilities in facial identity verification and expression recognition under adversarial conditions. We further demonstrate effective detection and analysis of adversarial patches and adversarial samples using perceptual hashing and segmentation, achieving an SSIM of 0.95.

</details>


### [13] [LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2601.09812)
*Carlo Sgaravatti,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

> 研究提出了一种新的融合框架LCF3D，用于结合RGB相机和LiDAR传感器，以提升自主驾驶汽车中的3D物体检测性能。该方法在多个数据集中显示了优于单纯使用LiDAR的效果，尤其是在行人和骑行者等困难类别上的检测。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高自主车辆中3D物体检测的性能，有必要将RGB相机与LiDAR传感器有效结合起来以达到更准确的物体定位，如行人、骑自行车者以及其他车辆。然而，如何有效地集成这两种数据源进行3D物体检测仍然具有挑战。

**Method:** 我们提出了一种名为LCF3D的新颖传感器融合框架，该框架将RGB图像上的2D对象检测器与LiDAR点云上的3D对象检测器相结合，以提升3D物体检测的准确性。我们的方法包含两个主要原则：(i)晚期融合，通过匹配LiDAR 3D检测与RGB 2D检测以减少LiDAR假正例；(ii)级联融合，通过生成新的3D锥体提案来恢复遗漏的LiDAR对象检测。

**Result:** 实验表明，LCF3D在处理不同训练和测试传感配置方面具有良好的领域泛化能力。LCF3D的方法在行人和骑车人等具有挑战性的类别（KITTI数据集）以及摩托车和自行车（nuScenes数据集）上，显著优于基于LiDAR的方法。

**Conclusion:** 我们的研究表明，通过结合RGB图像和LiDAR点云数据的融合方法，LCF3D可以显著提升在多个数据集上的3D物体检测性能，特别是在困难类别上，如行人和骑行者等。

**Abstract:** Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.

</details>


### [14] [Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images](https://arxiv.org/abs/2601.09814)
*Adil O. Khadidos,Aziida Nanyonga,Alaa O. Khadidos,Olfat M. Mirza,Mustafa Tahsin Yilmaz*

Main category: cs.CV

> 研究通过比较DenseNet121和EfficientNet-B0模型，发现EfficientNet-B0在儿童肺炎检测方面表现更佳，并且模型解释结果显示其能够可靠地关注肺部相关区域，提高了人工智能辅助诊断的透明度和可信度。

<details>
  <summary>Details</summary>

**Motivation:** 肺炎仍然是全球儿童发病率和死亡率的主要原因之一，迫切需要准确和有效的诊断支持工具。深度学习在医学图像分析中，特别是对于胸部X光片解释方面展现了强大的潜力。本文研究比较了两种最先进的卷积神经网络（CNN）架构，用于自动的儿童肺炎检测。

**Method:** 本研究使用了5,863张儿童胸部X光图像的公开数据集。图像通过标准化、调整大小和数据增强进行预处理以增强泛化能力。使用了具有预训练ImageNet权重的DenseNet121和EfficientNet-B0模型进行微调。性能评估指标包括准确度、F1得分、Matthews相关系数（MCC）和召回率。同时使用Grad-CAM和LIME进行模型解释，以可视化影响预测的图像区域。

**Result:** EfficientNet-B0表现优于DenseNet121，准确度为84.6%，F1得分为0.8899，MCC为0.6849。DenseNet121准确度为79.7%，F1得分为0.8597，MCC为0.5852。两种模型的召回率均超过0.99，表明其对肺炎检测的高敏感性。使用Grad-CAM和LIME的可视化结果指出，模型重点关注了具有临床意义的肺部区域。

**Conclusion:** 与DenseNet121相比，EfficientNet-B0提供了更有平衡性和计算效率更高的性能，是临床应用的一个有力候选者。模型解释技术的结合增强了人工智能辅助儿童肺炎诊断技术的透明度和可信赖性。

**Abstract:** Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.

</details>


### [15] [NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration](https://arxiv.org/abs/2601.09823)
*Subhajit Sanyal,Srinivas Soumitri Miriyala,Akshay Janardan Bankar,Sravanth Kodavanti,Harshit,Abhishek Ameta,Shreyas Pandith,Amit Satish Unde*

Main category: cs.CV

> 提出了一种帕累托最优的扩散基础模型系列NanoSD，该模型系列保留了生成先验，并且能够在边缘设备上实现实时视觉生成和修复。

<details>
  <summary>Details</summary>

**Motivation:** 现有的轻量级变体主要是压缩去噪U-Net或减少扩散轨迹，这会破坏潜在流形，限制其在单一任务之外的泛化能力。因此，提出了NanoSD，旨在保留生成先验的同时，可以在边缘设备上实现实时推理。

**Method:** 通过网络手术、特征生成蒸馏和结构化架构扩展，从Stable Diffusion 1.5中提炼出NanoSD，这是一种帕累托最优的扩散基础模型系列。这些改进同时应用于U-Net和VAE编码器-解码器。

**Result:** NanoSD系列模型参数从1.3亿到3.15亿不等，可以在移动类NPUs上实现低至20毫秒的实时推理。实验表明，NanoSD在图像超分辨率、图像去模糊、人脸修复和单目深度估计等任务上，达到了感知质量与实际部署性的最佳表现。

**Conclusion:** NanoSD通过改进的全管道协同设计方法，建立了一个通用的扩散基础模型系列，适用于边缘设备上的实时视觉生成和修复。

**Abstract:** Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.

</details>


### [16] [UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval](https://arxiv.org/abs/2601.09828)
*Xiaoxu Ma,Runhao Li,Hanwen Liu,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CV

> UniHash is a novel image retrieval method that unifies pointwise and pairwise paradigms to achieve balanced performance in seen and unseen categories with a dual-branch framework and bidirectional knowledge transfer.

<details>
  <summary>Details</summary>

**Motivation:** Most deep hashing methods are limited to either pointwise or pairwise training, excelling in either seen or unseen categories but not both. This paper aims to balance retrieval performance across both by unifying these paradigms.

**Method:** Unified Hashing (UniHash) introduces a dual-branch framework combining pointwise and pairwise paradigms with bidirectional knowledge transfer to improve hash code discriminability and generalization.

**Result:** Theoretical analysis validates UniHash's effectiveness, and experiments show UniHash achieves state-of-the-art performance in seen and unseen image retrieval on datasets like CIFAR-10, MSCOCO, and ImageNet.

**Conclusion:** UniHash successfully combines the strengths of both pointwise and pairwise training paradigms, enhancing the retrieval performance for both seen and unseen categories.

**Abstract:** Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.

</details>


### [17] [ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning](https://arxiv.org/abs/2601.09851)
*Po-han Li,Shenghui Chen,Ufuk Topcu,Sandeep Chinchali*

Main category: cs.CV

> 提出ViSIL评分方法来解决跨不同模式的信息量化问题，用于评估和优化视频摘要的质量，并且在VQA任务上的表现优于传统文本摘要。

<details>
  <summary>Details</summary>

**Motivation:** 传统的评价指标，如BLEU或ROUGE，无法量化跨不同模式的信息覆盖情况，比如将一段文字与一系列关键帧进行比较。因此，我们提出了ViSIL评分来解决这个问题。

**Method:** 我们提出了一种称为Video Summary Information Loss (ViSIL)的评分方法，这是一种信息论框架，通过视觉-语言模型(VLM)推断来量化视频中未被摘要捕获的信息。

**Result:** 实验结果表明，ViSIL评分与人类和VLM在视频问答（VQA）任务上的性能存在显著的相关性。此外，ViSIL使得摘要选择能够优化信息损失与处理速度之间的平衡，在不增加处理负担的情况下，比文本摘要提高了7%的VQA准确率。

**Conclusion:** ViSIL作为一种统一分析跨模态摘要格式的度量标准，它克服了传统方法在评估多模态视频摘要时的不足，并对VQA任务的性能表现有显著提升。

**Abstract:** Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.

</details>


### [18] [Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP](https://arxiv.org/abs/2601.09859)
*Anant Mehta,Xiyuan Wei,Xingyu Chen,Tianbao Yang*

Main category: cs.CV

> TuneCLIP 提出了一种自监督微调框架，旨在改善开源 CLIP 模型在多项下游任务中的性能。通过解决冷启动偏见和优化新的对比损失，TuneCLIP 增强了模型的性能，在 ImageNet 和 DataComp 等基准测试上取得了显著提高。

<details>
  <summary>Details</summary>

**Motivation:** 尽管 CLIP 模型在多模态表示学习中作用显著，但仅通过现有的自监督数据集来改进其性能仍是一个挑战。标准的微调方法往往导致性能下降。因此，研究者希望探索一种方法来克服此问题，提高模型在多个任务中的表现。

**Method:** TuneCLIP 包括两个关键阶段：一个用于恢复优化统计以减少冷启动偏见的预热阶段；一个优化新的对比损失以减轻对假阴性对惩罚的微调阶段。

**Result:** 实验表明，TuneCLIP 一致提高了模型性能，特别是在 SigLIP (ViT-B/16) 模型上，在 ImageNet 及相关的分布外基准测试中提升了高达 +2.5%，在竞争激烈的 DataComp 基准上提升了 +1.2%。

**Conclusion:** TuneCLIP 作为高效的后预训练适应的方法，提供了一个强有力的基准，在不修改大规模训练过程的前提下，提升了现有 CLIP 模型的性能。这一研究为未来探索改进模型性能提供了新的视角。

**Abstract:** CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.

</details>


### [19] [VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching](https://arxiv.org/abs/2601.09866)
*Kiarie Ndegwa,Andreas Gros,Tony Chang,David Diaz,Vincent A. Landau,Nathan E. Rutenbeck,Luke J. Zachmann,Guy Bayes,Scott Conway*

Main category: cs.CV

> 本文提出的VibrantSR框架能够从Sentinel-2影像中估算高分辨率的冠层高度模型，实现季节至年度频率的大范围森林监测，其MAE性能优于现有的卫星基准方法。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机在于开发一种不依赖于频繁和规则采集日程的空基方法，实现对大范围地区进行一致的森林监控和碳核算。

**Method:** 我们提出了VibrantSR（活力超分辨率）框架，用于从10米分辨率的Sentinel-2影像中估算0.5米分辨率的冠层高度模型（CHM）。该方法利用了全球可用的Sentinel-2季节合成影像，能够在季节到年度的时间尺度上实现一致的监测。

**Result:** 在对美国西部22个EPA第三级生态区域进行的空间分离验证分割测试中，VibrantSR达到了4.39米的平均绝对误差（MAE）（适用于冠层高度大于等于2米的情况），优于Meta（4.83米）、LANDFIRE（5.96米）和ETH（7.05米）的卫星基准。值得注意的是，尽管基于空中的VibrantVS（2.71米MAE）具有更高的精度优势，但VibrantSR实现了在大陆范围内进行森林监测和碳核算的功能，而且无需依赖昂贵且时间不频繁的航空采集。

**Conclusion:** VibrantSR在没有使用昂贵和时间稀疏的航空采集的情况下，在大陆范围内实现了有效的森林监测和碳核算。

**Abstract:** We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.

</details>
