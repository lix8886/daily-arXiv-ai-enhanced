<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 37]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments](https://arxiv.org/abs/2510.07359)
*Jingfei Huang,Han Tu*

Main category: cs.CL

> 研究提出一种新方法以识别和阐明基于街景图像和社交媒体文本的情感不一致性，通过分析2016年与2022年北京二环线的数据，揭示了人口感知与意见之间的显著差异及其潜在原因。

<details>
  <summary>Details</summary>

**Motivation:** 社交平台的兴起改变了我们对城市环境的理解，引发了人类感知与意见中嵌入的情感反应的微妙变化，这挑战了现有的多维情感分析方法。

**Method:** 本研究提出了一种新颖的方法，用于识别和阐明情感不一致性，构建了一个包含140,750张百度和腾讯街景图片的数据集来衡量感知，并使用984,024条微博文本帖子来衡量意见。通过结合对象检测和自然语言处理技术开发了一种反应指数，来分类2016年和2022年北京二环线的情感反应。采用回归分析、图像分割和基于土地使用分布的词频分析来分析和可视化情感反应。

**Result:** 感知情绪反应趋势地图表明积极情感的分布趋于均匀，而意见情感反应趋势地图显示了更极端的变化。在分析情感反应的变化趋势时，与紧密建筑和行人出现等元素存在显著关系。

**Conclusion:** 研究提供的不一致地图展示了疫情前后人对都市区感知和意见情感的变化，提供了环境管理和城市更新制定策略的潜在解释和方向。

**Abstract:** The ascension of social media platforms has transformed our understanding of
urban environments, giving rise to nuanced variations in sentiment reaction
embedded within human perception and opinion, and challenging existing
multidimensional sentiment analysis approaches in urban studies. This study
presents novel methodologies for identifying and elucidating sentiment
inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent
Street view images to measure perceptions, and 984,024 Weibo social media text
posts to measure opinions. A reaction index is developed, integrating object
detection and natural language processing techniques to classify sentiment in
Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is
analysed and visualized using regression analysis, image segmentation, and word
frequency based on land-use distribution to discern underlying factors. The
perception affective reaction trend map reveals a shift toward more evenly
distributed positive sentiment, while the opinion affective reaction trend map
shows more extreme changes. Our mismatch map indicates significant disparities
between the sentiments of human perception and opinion of urban areas over the
years. Changes in sentiment reactions have significant relationships with
elements such as dense buildings and pedestrian presence. Our inconsistent maps
present perception and opinion sentiments before and after the pandemic and
offer potential explanations and directions for environmental management, in
formulating strategies for urban renewal.

</details>


### [2] [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
*Mufei Li,Dongqi Fu,Limei Wang,Si Zhang,Hanqing Zeng,Kaan Sancak,Ruizhong Qiu,Haoyu Wang,Xiaoxin He,Xavier Bresson,Yinglong Xia,Chonglin Sun,Pan Li*

Main category: cs.CL

> 研究人员提出HaystackCraft作为基于全英文维基百科的针在 haystack 中基准测试，评估异质检索对上下文噪音和代理操作的影响，揭示了长上下文鲁棒性问题。

<details>
  <summary>Details</summary>

**Motivation:** 现代长上下文大语言模型在合成的“针在 haystack 中”基准测试中表现良好，但忽略了由于偏置检索和代理工作流程而引起的真实世界中的噪音上下文。因此，需要构建可以体现这些关键因素的真实世界噪音长上下文来测试模型的长上下文鲁棒性。

**Method:** 构建HaystackCraft基准测试，通过全英文维基百科超链接网络上的多跳问题来评估异质检索策略对干扰因子组成、干扰源排序以及下游LLM性能的影响。此外，将NIAH扩展到动态、LLM依赖的代理操作模拟环境中。

**Result:** 实验结果表明，强大的密集检索器可以引入更具挑战性的干扰因子，同时，基于图的重排序可以同时改善检索效果并减少有害干扰因子。而在代理测试中，即使先进的模型也难以准确停止或避免自己生成的干扰因子。

**Conclusion:** 这些结果突显了在代理操作中的长上下文推理面临持续的挑战，并将HaystackCraft确立为未来研究有价值的测试平台。

**Abstract:** Modern long-context large language models (LLMs) perform well on synthetic
"needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.

</details>


### [3] [Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data](https://arxiv.org/abs/2510.07434)
*Olia Toporkov,Alan Akbik,Rodrigo Agerri*

Main category: cs.CL

> 最新大语言模型在没有先期微调的情况下，仅通过少量示例即可在多种语言的引项化任务中取得接近或超过传统方法的性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于填补先前文献中关于大语言模型在引项化领域有效性评估的空白。同时，在缺乏目标领域或语言的监督训练数据的情况下，探讨LLMs与传统方法的相对性能。

**Method:** 该研究采用实验方法，比较了最新一代大语言模型（LLMs）在上下文引项化任务中的性能，与传统全监督方法进行了对比。研究试验涵盖12种不同形态复杂度的语言。

**Result:** 实验结果表明，与传统编码器不同，最新大语言模型在缺乏特定领域训练数据的情况下，能够通过少量示例生成高质量引项以达到最佳性能。

**Conclusion:** 尽管在域外设置中微调的编码器仍具有竞争力，但在大多数语言中，当前大语言模型通过直接在上下文中生成引项达到了最先进的结果。

**Abstract:** Lemmatization is the task of transforming all words in a given text to their
dictionary forms. While large language models (LLMs) have demonstrated their
ability to achieve competitive results across a wide range of NLP tasks, there
is no prior evidence of how effective they are in the contextual lemmatization
task. In this paper, we empirically investigate the capacity of the latest
generation of LLMs to perform in-context lemmatization, comparing it to the
traditional fully supervised approach. In particular, we consider the setting
in which supervised training data is not available for a target domain or
language, comparing (i) encoder-only supervised approaches, fine-tuned
out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma
generation with LLMs. Our experimental investigation across 12 languages of
different morphological complexity finds that, while encoders remain
competitive in out-of-domain settings when fine-tuned on gold data, current
LLMs reach state-of-the-art results for most languages by directly generating
lemmas in-context without prior fine-tuning, provided just with a few examples.
Data and code available upon publication:
https://github.com/oltoporkov/lemma-dilemma

</details>


### [4] [LASER: An LLM-based ASR Scoring and Evaluation Rubric](https://arxiv.org/abs/2510.07437)
*Amruta Parulekar,Preethi Jyothi*

Main category: cs.CL

> 提出了LASER评估标准，利用先进的LLM模型进行语音识别语义评估，显著提升了多语言错误分析的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的ASR评估标准如WER对语义影响不大的形态和句法差异惩罚过重，因此需要一种更精准的评估标准。

**Method:** 使用具备上下文学习能力的LLM模型，通过示例提示训练LASER评估标准。

**Result:** 使用Gemini 2.5 Pro的Hindi LASER评分与人类注释有着94%的高相关性，并且提示中的Hindi示例在分析其他印度语言如Marathi、Kannada和Malayalam的错误时也非常有效。

**Conclusion:** 展示了一种基于LLM的ASR评估标准LASER，能够更准确地评估错误的语义影响，并且小型LLM如Llama 3也能通过微调达到接近89%的准确率来预测应该施加何种惩罚。

**Abstract:** Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly
penalize morphological and syntactic nuances that do not significantly alter
sentence semantics. We introduce an LLM-based scoring rubric LASER that
leverages state-of-the-art LLMs' in-context learning abilities to learn from
prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro
achieved a very high correlation score of 94% with human annotations. Hindi
examples in the prompt were also effective in analyzing errors in other Indian
languages such as Marathi, Kannada and Malayalam. We also demonstrate how a
smaller LLM like Llama 3 can be finetuned on word-pair examples derived from
reference and ASR predictions to predict what kind of penalty should be applied
with close to 89% accuracy.

</details>


### [5] [Meaningful Pose-Based Sign Language Evaluation](https://arxiv.org/abs/2510.07453)
*Zifan Jiang,Colin Leong,Amit Moryossef,Anne Göhring,Annette Rios,Oliver Cory,Maksym Ivashechkin,Neha Tarigopula,Biao Zhang,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

> 本文对手语表达的姿势评估进行了全面研究，并展示了不同评估方法之间的权衡。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在在人类骨架姿势形式的手语表达中进行有意义的评估。

**Method:** 该研究涵盖了基于关键点距离、嵌入和反向翻译的度量标准。

**Result:** 通过自动元评估签级检索和不同手语的文本到姿势翻译的人类相关性研究，展示了不同场景下不同度量标准之间的权衡。

**Conclusion:** 研究发现和开源姿势评估工具包提供了一种实用且可重现的方式来开发和评估手语翻译或生成系统。

**Abstract:** We present a comprehensive study on meaningfully evaluating sign language
utterances in the form of human skeletal poses. The study covers keypoint
distance-based, embedding-based, and back-translation-based metrics. We show
tradeoffs between different metrics in different scenarios through automatic
meta-evaluation of sign-level retrieval and a human correlation study of
text-to-pose translation across different sign languages. Our findings and the
open-source pose-evaluation toolkit provide a practical and reproducible way of
developing and evaluating sign language translation or generation systems.

</details>


### [6] [Populism Meets AI: Advancing Populism Research with LLMs](https://arxiv.org/abs/2510.07458)
*Eduardo Ryô Tamaki,Yujin J. Jung,Julia Chatterley,Grant Mitchell,Semir Dzebo,Cristóbal Sandoval,Levente Littvay,Kirk A. Hawkins*

Main category: cs.CL

> 本研究通过模仿人工编码员培训的链式思考提示方法，使大型语言模型在分析民粹主义内容方面达到了与人类专家同等的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于文本分析的战略虽然为建立领域基础提供了有效的客观指标，但成本高昂、耗时且难以跨语言、语境和大规模语料库扩展。因此，这项研究旨在克服这些限制并找到一种可扩展的方法。

**Method:** 采用基于评分指南和锚点引导的链式思考（CoT）提示方法，该方法模仿了人工编码员的培训过程。通过使用Global Populism Database (GPD)，这是一个全球领导人演讲数据集，注释了不同程度的民粹主义，该研究复制了用于培训人工编码员的过程，通过适应相同的文档来引导大型语言模型（LLM）的推理。

**Result:** 研究发现，通过这种特定领域的提示策略，可以使LLM达到与专家人工编码员相当的分类准确性，展示其处理民粹主义微妙且上下文敏感方面的能力。

**Conclusion:** 该研究证明，使用特定领域的提示方法，大型语言模型能够实现与专家相当的民粹主义内容分类准确性，这为在理解复杂意识形态内容方面提供了一种新颖且有效的方法。

**Abstract:** Measuring the ideational content of populism remains a challenge. Traditional
strategies based on textual analysis have been critical for building the
field's foundations and providing a valid, objective indicator of populist
framing. Yet these approaches are costly, time consuming, and difficult to
scale across languages, contexts, and large corpora. Here we present the
results from a rubric and anchor guided chain of thought (CoT) prompting
approach that mirrors human coder training. By leveraging the Global Populism
Database (GPD), a comprehensive dataset of global leaders' speeches annotated
for degrees of populism, we replicate the process used to train human coders by
prompting the LLM with an adapted version of the same documentation to guide
the model's reasoning. We then test multiple proprietary and open weight models
by replicating scores in the GPD. Our findings reveal that this domain specific
prompting strategy enables the LLM to achieve classification accuracy on par
with expert human coders, demonstrating its ability to navigate the nuanced,
context sensitive aspects of populism.

</details>


### [7] [MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference](https://arxiv.org/abs/2510.07475)
*Zheyuan Zhang,Lin Ge,Hongjiang Li,Weicheng Zhu,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

> 提出了多智能体提示优化（MAPRO）框架，通过拓扑感知的细化机制结合执行反馈和下游归责选择性地更新智能体提示策略，获得了一流的性能，并为未来构建更可靠和原理化的多智能体系统提供了指导。

<details>
  <summary>Details</summary>

**Motivation:** 虽然多智能体系统通过协调专业角色可以超越单个智能体，但由于提示敏感性和多智能体系统带来的累积不稳定性，设计有效MAS仍然存在困难。系统地设计MAS在没有原理性方法的情况下是难以解决的。尽管最近的努力在自动化提示设计中减轻了手动工作量，但多智能体提示优化领域仍基本未被探索。

**Method:** 提出了一个多智能体提示优化（MAPRO）的四阶段框架，首次将MAS提示优化表述为最大后验概率（MAP）推断问题，并通过一种语言引导的max-product信念传播算法变体进行求解。为了进行信度分配并迭代更新系统，MAPRO采用了拓扑感知的细化机制，该机制结合了执行反馈和下游归责，选择性地更新智能体提示，从而逐渐收敛到一组协调的智能体特定提示策略。

**Result:** 在各种任务的基准测试中，MAPRO实现了最先进的性能，稳定超越了手动工程基准和最近的自动化替代方案。

**Conclusion:** MAPRO框架为未来构建更可靠和原理化的多智能体系统提供了普遍的原则指南，并在多个基准测试中表现优越。

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, and LLM-based agents further extend these abilities to various
practical workflows. While recent progress shows that multi-agent systems (MAS)
can outperform single agents by coordinating specialized roles, designing
effective MAS remains difficult due to prompt sensitivity and the compounded
instability MAS creates. To cope with the challenge, recent efforts in
automated prompt design have reduced manual effort. However, multi-agent prompt
optimization remains largely unexplored. Challenges like exponentially
expanding search space and ambiguous credit assignment together make systematic
design intractable without principled methods. Therefore, we introduce
M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first
formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference
problem and solves it using a language-guided variant of max-product belief
propagation algorithm. To address credit assignment and updates the system
iteratively, MAPRO employs a topology-aware refinement mechanism that
integrates execution feedback and downstream blames to selectively update agent
prompts. Through this process, MAPRO progressively converges to a coordinated
set of agent-specific prompt policies. Across benchmarks in various tasks,
MAPRO achieves state-of-the-art performance, consistently surpassing manually
engineered baselines and recent automated alternatives. Beyond performance, our
MAP-based formulation also delivers general guidelines for building more
reliable and principled multi-agent systems in the future

</details>


### [8] [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)
*Shuqing Luo,Yilin Guan,Pingzhi Li,Hanrui Wang,Tianlong Chen*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),
but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM
decoding. Query-aware page-level sparse decoding can achieve state-of-the-art
performance under constrained FLOPs budgets, but is limited by both
sequential-dependent page filtering and coarse-grained token selection,
hampering serving efficiency and model performance on TTS tasks under high
concurrency and long CoT scenarios (consuming even higher runtime than the
forward pipeline itself). In this paper, we first find that the current-step
query state can be accurately approximated in a unified manner from a short
window of recent queries, enabling training-free query-aware sparsity without
waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework
for efficient TTS built on two core components: (1) a novel light-weight
temporal-regressive module that predicts the next-token query state; (2) an
asynchronous and disaggregated framework that decouples the KV cache filtering
from the auto-regressive decoding loop, overlapping the token-level KV
selection with the forward inference computation through asynchronism. To our
knowledge, AsyncSpade is the first to eliminate the sequential dependence
without sacrificing model performance. We validate the effectiveness of
AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade
fully overlaps KV-cache operations with the inference pipeline, achieving
theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade
delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and
at least 50% TPOT reduction compared to full attention on Qwen3-8B and
Qwen3-32B models, while matching or surpassing their accuracy on various TTS
benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).

</details>


### [9] [Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics](https://arxiv.org/abs/2510.07488)
*Rasika Muralidharan,Jaewoon Kwak,Jisun An*

Main category: cs.CL

> 研究通过提出一种多智能体框架，分析大型语言模型赋能的多智能体系统的团队动力学，重点探讨了结构、多样性和互动动态。结果显示，扁平化团队优于层级化团队，同时多样性影响团队表现。智能体之间的交流和协作效能仍需提高。

<details>
  <summary>Details</summary>

**Motivation:** 此研究旨在填补在大型语言模型赋能的多智能体系统中团队动力学研究的空白，通过借鉴人类团队科学的理论来增强这些智能体系统的理解和性能。

**Method:** 我们提出了一种多智能体框架，用于探讨团队科学的核心方面：结构、多样性和互动动态。该框架旨在通过大型语言模型（LLM）赋能的多智能体系统（MAS）来模仿人类团队的科学理论。

**Result:** 实验结果显示，扁平化团队的表现优于层级团队。同时，多样性对于团队表现有着复杂的影响。此外，虽然智能体在团队表现上显得过于自信，但在任务后反思中，智能体表现出对于合作的认可以及在集成中遇到的挑战，特别是受限的对话协调问题。

**Conclusion:** 总的来说，研究结果揭示了在多智能体系统中团队结构和多样性的潜在影响。扁平化的团队结构比层级化的结构更有利于表现，而多样性对团队表现既有积极也有消极的影响，情况复杂。此外，还需进一步改善智能体之间的对话协调能力，以提高整体团队的表现和效率。

**Abstract:** Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are
gaining attention, yet fewer studies explore their team dynamics. Inspired by
human team science, we propose a multi-agent framework to examine core aspects
of team science: structure, diversity, and interaction dynamics. We evaluate
team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and
Latent Implicit Hate, spanning commonsense and social reasoning. Our results
show that flat teams tend to perform better than hierarchical ones, while
diversity has a nuanced impact. Interviews suggest agents are overconfident
about their team performance, yet post-task reflections reveal both
appreciation for collaboration and challenges in integration, including limited
conversational coordination.

</details>


### [10] [Can Speech LLMs Think while Listening?](https://arxiv.org/abs/2510.07497)
*Yi-Jen Shih,Desh Raj,Chunyang Wu,Wei Zhou,SK Bong,Yashesh Gaur,Jay Mahadeokar,Ozlem Kalinli,Mike Seltzer*

Main category: cs.CL

> The paper explores the improvement of reasoning abilities and reduction of latency in speech LLMs through chain-of-thought (CoT) fine-tuning, introducing a new metric and using Direct Preference Optimization, achieving significant gains in both accuracy and latency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the reasoning abilities and response latency of speech LLMs, making them more capable and user-friendly in interactions.

**Method:** The paper investigates the effect of chain-of-thought (CoT) fine-tuning on multi-stream speech LLMs. An entropy-based metric named 'question completeness' is proposed to reduce reasoning latency by allowing early reasoning. Direct Preference Optimization (DPO) is also applied to optimize the accuracy-latency trade-off.

**Result:** The paper shows a 2.4x improvement in accuracy for spoken reasoning tasks, a 70% reduction in latency without accuracy loss, and a 4% accuracy gain on ARC-Easy compared to heuristic-based approaches under equivalent latency conditions.

**Conclusion:** The methods proposed in the paper effectively improve both the reasoning accuracy and the response latency of speech LLMs, indicating a promising path for enhancing interactive voice-based systems.

**Abstract:** Recent advances in speech large language models (speech LLMs) have enabled
seamless spoken interactions, but these systems still struggle with complex
reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning
has been to shown to significantly improve the reasoning abilities of
text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for
multi-stream speech LLMs, demonstrating that reasoning in text space improves
the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken
reasoning tasks. Beyond accuracy, the latency of the spoken response is a
crucial factor for interacting with voice-based agents. Inspired by the human
behavior of "thinking while listening," we propose methods to reduce the
additional latency from reasoning by allowing the model to start reasoning
before the user query has ended. To achieve this, we introduce an entropy-based
metric, "question completeness," which acts as an indicator to guide the model
on the optimal time to start reasoning. This method provides greater control
over the accuracy-latency trade-off compared with heuristic-based approaches
and, under equivalent latency conditions, yields a 4% accuracy gain on
ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference
data created using rejection sampling to push the accuracy-latency pareto
frontier further, resulting in a 70% reduction in latency without loss in
accuracy.

</details>


### [11] [When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs](https://arxiv.org/abs/2510.07499)
*Soyeong Jeong,Taehee Jung,Sung Ju Hwang,Joo-Kyung Kim,Dongyeop Kang*

Main category: cs.CL

> 研究提出了一种名为ToTAL的框架，通过使用思考模板和更新策略来改进LCLM在多跳推理中的效果，这种框架可以在各种模型中提供一致性收益，并支持模板优化后向更小的模型蒸馏。

<details>
  <summary>Details</summary>

**Motivation:** 尽管最新长上下文语言模型（LCLMs）能够处理成千上万个令牌的单个提示，从而能够进行大量知识密集型多跳推理，但这些模型在将证据连接起来的方式上存在不足。

**Method:** 通过思考模板将推理重组为可重用的思维缓存，这些缓存是从先前的问题解决轨迹中衍生出来的，用于结构化证据的结合并指导多跳推理。我们提出了一种更新策略，通过自然语言反馈迭代地细化从训练数据中得出的模板。

**Result:** 在各种基准测试和不同的LCLM家族中，该方法在基于检索和不基于检索的设置下都持续超越了强劲的基线。

**Conclusion:** 经过优化的模板可以被蒸馏到更小的开源模型中，显示了该框架的广泛适用性和透明推理的重用性。此框架被命名为通过思考模板增强的长上下文语言模型（ToTAL）。

**Abstract:** Recent Long-Context Language Models (LCLMs) can process hundreds of thousands
of tokens in a single prompt, enabling new opportunities for
knowledge-intensive multi-hop reasoning by integrating large sets of retrieved
documents or, in some cases, directly all necessary information. However,
simply feeding more documents into the context window fails to capture how
evidence should be connected. We address this gap with thought templates, which
recast reasoning as reusable thought caches, derived from prior problem solving
traces, structuring how evidence is combined and guiding multi-hop inference
with factual documents. To keep these templates effective, we propose an update
strategy that iteratively refines templates derived from training data through
natural-language feedback. Across diverse benchmarks and LCLM families, our
approach delivers consistent gains over strong baselines in both
retrieval-based and retrieval-free settings. Furthermore, we show that
optimized templates can be distilled into smaller open-source models,
demonstrating its broad applicability and transparent reasoning reuse. We refer
to our framework as Thought Template Augmented LCLMs (ToTAL).

</details>


### [12] [ParsTranslit: Truly Versatile Tajik-Farsi Transliteration](https://arxiv.org/abs/2510.07520)
*Rayyan Merchant,Kevin Tang*

Main category: cs.CL

> 本研究开发了一种新型的塔吉克语-波斯语文字转写模型并通过验证表明该模型在所有现有数据集上表现出色，解决了不同领域数据限制的问题。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于克服塔吉克语和波斯语由于书写系统不同而导致的沟通障碍问题。之前的工作在模型训练数据方面存在局限性，主要依赖特定领域的数据集，缺乏通用性。

**Method:** 本研究提出了一种新的端到端序列模型，用于塔吉克语-波斯语之间的文字转写。该模型是在所有现有数据集上进行训练的，并提供了两个新的数据集。

**Result:** 该模型在不同领域的测试中显示出优良的性能，从波斯语到塔吉克语的chrF++得分为87.91，Normalized CER得分为0.05，从塔吉克语到波斯语的chrF++得分为92.28，Normalized CER得分为0.04。

**Conclusion:** 研究结论为，所提出的端到端序列模型通过混合使用不同来源的数据，提升了在跨领域文本转写任务中的性能，并为该领域提供了全面的对比基准。

**Abstract:** As a digraphic language, the Persian language utilizes two written standards:
Perso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite
the significant similarity between the dialects of each country, script
differences prevent simple one-to-one mapping, hindering written communication
and interaction between Tajikistan and its Persian-speaking ``siblings''. To
overcome this, previously-published efforts have investigated machine
transliteration models to convert between the two scripts. Unfortunately, most
efforts did not use datasets other than those they created, limiting these
models to certain domains of text such as archaic poetry or word lists. A truly
usable transliteration system must be capable of handling varied domains,
meaning that suck models lack the versatility required for real-world usage.
The contrast in domain between data also obscures the task's true difficulty.
We present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi
transliteration trained across all available datasets, and present two datasets
of our own. Our results across domains provide clearer understanding of the
task, and set comprehensive comparable leading benchmarks. Overall, our model
achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik
and 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available
at https://anonymous.4open.science/r/ParsTranslit-FB30/.

</details>


### [13] [OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs](https://arxiv.org/abs/2510.07535)
*Jaeseong Lee,seung-won hwang,Aurick Qiao,Gabriele Oliaro,Ye Wang,Samyam Rajbhandari*

Main category: cs.CL

> 研究提出了一种名为 OWL 的新型模型，解决了现有推测解码方法在长上下文输入上的性能下降问题，通过引入新的技术，OWL 在长上下文输入中的接受长度提高了约 5 倍，为未来的研究提供了基准和数据集。

<details>
  <summary>Details</summary>

**Motivation:** 现有的推测解码方法无法泛化到现实世界的场景中，特别是在处理长上下文输入时表现不佳。OWLS 旨在解决这个问题，从而实现更快的大型语言模型推理。

**Method:** 通过发布一个新的长上下文基准测试（LongSpecBench）和引入一种新的模型（OWL），来解决现有方法在长上下文输入上的不足。OWL 通过三种创新实现比 EAGLE3 高 5 倍的接受长度：(1) 仅依赖最后标记状态条件的基于 LSTM 的草案模块，使其适应多种长度；(2) 验证器中的特殊标记 [SPEC] 产生对草案模块更有用的表示；(3) 结合树和非树解码方法的混合算法。

**Result:** OWL 模型在长上下文输入上表现出显著优于 EAGLE3 的性能，它的接受长度大约提高了 5 倍。

**Conclusion:** OWL 模型的成功证明了其在处理长上下文输入上的优越性，并提供了新的基准和数据集以推进未来研究的发展。

**Abstract:** Speculative decoding promises faster inference for large language models
(LLMs), yet existing methods fail to generalize to real-world settings.
Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical
workloads involve long contexts. We find current approaches degrade severely
with long contexts; for instance, EAGLE3 even slows down the generation speed
by 0.81x. We address these limitations by releasing a new long-context
benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves
about 5x higher acceptance length than EAGLE3 on long-context inputs through
three innovations: (1) an LSTM-based drafter conditioned only on the last-token
state, making it generalize to various lengths, (2) a special token [SPEC] in
the verifier that produces richer representation for drafter, and (3) a hybrid
algorithm combining both tree and non-tree decoding methods. We release all
code and datasets to advance future research.

</details>


### [14] [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Mizanur Rahman,Amran Bhuiyan,Israt Jahan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

> 本文提出了两种有效的方法来提高Tiny模型的成本效益，尤其是在图表理解任务中的评价能力。

<details>
  <summary>Details</summary>

**Motivation:** 由于参数量小于或等于20亿的tiny模型在作为评判者时表现不佳，限制了它们在资源受限环境下的实际应用。为了解决这个问题，我们提出了成本效益更高的评估方法。

**Method:** 我们提出了两种方法来确保成本效益的评估：(i) 多标准提示技术，它将分离的评估标准合并为一个查询；(ii) 域适应迁移学习，其中我们对一个20亿参数的LVLM进行微调，使其成为一个图表判断模型（ChartJudge）。

**Result:** 实验显示，多标准提示技术暴露了70亿参数模型（包括专门的LVLM评判器如LLaVA-Critic）的稳健性差距，导致性能大幅下降。同时，我们的tiny LVLM（ChartJudge）可以从一个数据集转移到另一个数据集，成为一个更专业的模型。

**Conclusion:** 我们对图表类型和查询复杂性的细致分析提供了可行的见解，使模型大小、提示设计和可移植性之间的权衡显而易见，从而实现可扩展的、低成本的图表推理任务评价。

**Abstract:** Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(<=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.

</details>


### [15] [Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER](https://arxiv.org/abs/2510.07566)
*Junyi Zhu,Savas Ozkan,Andrea Maracani,Sinan Mutlu,Cho Jung Min,Mete Ozay*

Main category: cs.CL

> 研究提出了一种用于命名实体识别和文本分类任务的多任务预微调框架，通过使用任务优先的LoRA模块，提高了轻量级BERT样编码器在移动平台上的适应性和性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高轻量级BERT样编码器在移动平台上的适应性，同时保持内存和计算效率，研究了预微调策略。

**Method:** 我们提出了一种基于任务优先的LoRA模块的多任务预微调框架，该框架能够在单一共享编码器主干的基础上利用模块化适配器实现多任务学习。

**Result:** 我们的方法在21个下游任务上的实验表明，对于命名实体识别任务平均提高了+0.8%，对于文本分类任务平均提高了+8.8%。

**Conclusion:** 该方法达到了与单独预微调相当的性能，同时满足实际部署约束，证明了对于多功能移动NLP应用的有效性。

**Abstract:** Deploying natural language processing (NLP) models on mobile platforms
requires models that can adapt across diverse applications while remaining
efficient in memory and computation. We investigate pre-finetuning strategies
to enhance the adaptability of lightweight BERT-like encoders for two
fundamental NLP task families: named entity recognition (NER) and text
classification. While pre-finetuning improves downstream performance for each
task family individually, we find that na\"ive multi-task pre-finetuning
introduces conflicting optimization signals that degrade overall performance.
To address this, we propose a simple yet effective multi-task pre-finetuning
framework based on task-primary LoRA modules, which enables a single shared
encoder backbone with modular adapters. Our approach achieves performance
comparable to individual pre-finetuning while meeting practical deployment
constraint. Experiments on 21 downstream tasks show average improvements of
+0.8% for NER and +8.8% for text classification, demonstrating the
effectiveness of our method for versatile mobile NLP applications.

</details>


### [16] [Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets](https://arxiv.org/abs/2510.07579)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.CL

> 本研究通过计算语言学方法分析了三种语料库中的在线讨论，发现健康误导信息在语言上有其独特的复杂修辞风格和情感线索，这一发现有助于识别误导信息，并对公共卫生信息传播策略和危机传播理论有所启示。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于理解疫情期间在线讨论中语言的差异性，重点关注语言如何区分健康误导信息与事实性交流，从而为识别健康误导信息提供语言学指标。

**Method:** 本研究使用计算语言学方法分析了疫情期间的在线讨论，具体通过三种语料库：7588则有关新冠的虚假叙述、10700则一般新冠内容以及5787则猴痘相关帖子，来研究语言如何区分健康误导信息与事实性交流。

**Result:** 研究发现新冠误导信息的可读性评分明显较低，且恐惧相关或具有说服力的词汇频率是其他数据集的两倍多。此外，新冠误导信息使用感叹号的频率较低，与情感表达更强的猴痘相关内容形成对比。

**Conclusion:** 研究结果强调了误导信息采用了复杂修辞风格，并嵌入了情感线索的组合，这可能增强了其可信度。研究同时指出了一些局限性，并呼吁后续研究应采用纵向设计、更广泛的情感词汇和平台敏感的方法。

**Abstract:** This study conducts a computational linguistic analysis of pandemic-related
online discourse to examine how language distinguishes health misinformation
from factual communication. Drawing on three corpora: COVID-19 false narratives
(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts
(n = 5787), we identify significant differences in readability, rhetorical
markers, and persuasive language use. COVID-19 misinformation exhibited
markedly lower readability scores and contained over twice the frequency of
fear-related or persuasive terms compared to the other datasets. It also showed
minimal use of exclamation marks, contrasting with the more emotive style of
Monkeypox content. These patterns suggest that misinformation employs a
deliberately complex rhetorical style embedded with emotional cues, a
combination that may enhance its perceived credibility. Our findings contribute
to the growing body of work on digital health misinformation by highlighting
linguistic indicators that may aid detection efforts. They also inform public
health messaging strategies and theoretical models of crisis communication in
networked media environments. At the same time, the study acknowledges
limitations, including reliance on traditional readability indices, use of a
deliberately narrow persuasive lexicon, and reliance on static aggregate
analysis. Future research should therefore incorporate longitudinal designs,
broader emotion lexicons, and platform-sensitive approaches to strengthen
robustness.

</details>


### [17] [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/abs/2510.07591)
*Chihiro Taguchi,Richard Sproat*

Main category: cs.CL

> 本文介绍了一个使用LLMs在构造语言开发中的系统。系统具有多种功能，包括音系开发、文本翻译、词表建立和语法手册编写等。研究旨在开发有趣工具，并探讨LLMs对语言的理解。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在开发用于创建人工构造语言的有趣工具，同时探究LLMs对语言和语言概念的理解程度。此外，还将研究此方法应用于从高资源语言到低资源语言的翻译。

**Method:** 运用LLMs在构造语言开发中的系统方法。首先，通过代理方法逐步细化目标语言的音系。其次，将句子从英语翻译成反映目标语言词序和形态句法特征规格的形态句法标记。接着，建立词表，最后系统生成语言的书写系统并编写语法手册。

**Result:** 结果表明，不同LLMs和语言规格之间存在显著能力差异，更常见的模式比罕见的更容易处理。尽管如此，研究提供了一些证据，表明改进的系统可能在低资源语言翻译任务中有所裨益。

**Conclusion:** 本系统对于创建人工语言具有潜力，并为理解LLMs对语言的理解提供了见解。然而，应用于高资源到低资源的翻译任务中，效果不稳定，但展示了改进系统的潜力。

**Abstract:** We present a system that uses LLMs as a tool in the development of
Constructed Languages. The system is modular in that one first creates a target
phonology for the language using an agentic approach that refines its output at
each step with commentary feedback on its previous attempt. Next, a set of
sentences is 'translated' from their English original into a morphosyntactic
markup that reflects the word order and morphosyntactic feature specifications
of the desired target language, with affixes represented as morphosyntactic
feature bundles. From this translated corpus, a lexicon is constructed using
the phonological model and the set of morphemes (stems and affixes) extracted
from the 'translated' sentences. The system is then instructed to provide an
orthography for the language, using an existing script such as Latin or
Cyrillic. Finally, the system writes a brief grammatical handbook of the
language. The system can also translate further sentences into the target
language.
  Our goal is twofold. First, we hope that these tools will be fun to use for
creating artificially constructed languages. Second, we are interested in
exploring what LLMs 'know' about language-not what they know about any
particular language or linguistic phenomenon, but how much they know about and
understand language and linguistic concepts. As we shall see, there is a fairly
wide gulf in capabilities both among different LLMs and among different
linguistic specifications, with it being notably easier for systems to deal
with more common patterns than rarer ones. An additional avenue that we explore
is the application of our approach to translating from high-resource into
low-resource languages. While the results so far are mostly negative, we
provide some evidence that an improved version of the present system could
afford some real gains in such tasks.
  https://github.com/SakanaAI/IASC

</details>


### [18] [Vocabulary embeddings organize linguistic structure early in language model training](https://arxiv.org/abs/2510.07613)
*Isabel Papadimitriou,Jacob Prince*

Main category: cs.CL

> 研究了语言模型词汇表示的结构及其在训练过程中如何演变，发现高频和功能词的嵌入向量更快收敛，揭示了词汇频率和功能的不同作用。

<details>
  <summary>Details</summary>

**Motivation:** 研究语言模型输入词汇表示的结构，以及它们如何在训练过程中演变。

**Method:** 使用表征相似性分析，进行了一系列实验，将两种开源模型（Pythia 12B 和 OLMo 7B）的输入嵌入和输出嵌入的几何结构与语义、句法和基于频率的度量进行关联。

**Result:** 主要发现是：1) 在训练过程中，词汇嵌入的几何结构迅速与一系列语义和句法特征的高度相关性收敛；2) 高频和功能词（如“the”、“of”）的嵌入向量更快收敛到最终状态，而词汇和低频词则保留了它们随机初始化中的一些偏差。

**Conclusion:** 这些发现有助于绘制输入嵌入围绕语言结构组织的动态轨迹，揭示了词汇频率和功能的不同作用，并激励对词汇几何结构演变如何在模型训练过程中促进特定能力提升的深入研究。

**Abstract:** Large language models (LLMs) work by manipulating the geometry of input
embedding vectors over multiple layers. Here, we ask: how are the input
vocabulary representations of language models structured, and how and when does
this structure evolve over training? To answer this question, we use
representational similarity analysis, running a suite of experiments that
correlate the geometric structure of the input embeddings and output embeddings
of two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic,
and frequency-based metrics over the course of training. Our key findings are
as follows: 1) During training, the vocabulary embedding geometry quickly
converges to high correlations with a suite of semantic and syntactic features;
2) Embeddings of high-frequency and function words (e.g., "the," "of") converge
to their final vectors faster than lexical and low-frequency words, which
retain some alignment with the bias in their random initializations. These
findings help map the dynamic trajectory by which input embeddings organize
around linguistic structure, revealing distinct roles for word frequency and
function. Our findings motivate a deeper study of how the evolution of
vocabulary geometry may facilitate specific capability gains during model
training.

</details>


### [19] [Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation](https://arxiv.org/abs/2510.07629)
*Zhangdie Yuan,Han-Chin Shing,Mitch Strong,Chaitanya Shivade*

Main category: cs.CL

> 本文提出了通过轻量级方法提高LLM的临床编码准确性，并引入临床代码验证作为任务和流水线组件。发布了双标注的门诊临床笔记数据集，以减少现有数据集的偏差。

<details>
  <summary>Details</summary>

**Motivation:** 改进现有的LLM模型在临床编码任务中的性能，特别是在解决层级上接近但不正确的编码错误问题。

**Method:** 通过细致的提示工程和小规模的微调，来提高LLM在临床编码任务中的准确性。提出临床代码验证作为独立任务和流水线组件，用于解决层级上的编码错误问题。

**Result:** 研究表明，通过引入临床代码验证，可以有效地改进LLM在医学编码中的准确性，而无需增加计算开销。同时，发布了专家双标注的门诊临床笔记数据集，以解决现有数据集的局限性。

**Conclusion:** 临床代码验证作为有效且可靠的方法，可改进基于LLM的医疗编码准确性，同时提出了一个改进的数据集标准。

**Abstract:** Accurate clinical coding is essential for healthcare documentation, billing,
and decision-making. While prior work shows that off-the-shelf LLMs struggle
with this task, evaluations based on exact match metrics often overlook errors
where predicted codes are hierarchically close but incorrect. Our analysis
reveals that such hierarchical misalignments account for a substantial portion
of LLM failures. We show that lightweight interventions, including prompt
engineering and small-scale fine-tuning, can improve accuracy without the
computational overhead of search-based methods. To address hierarchically
near-miss errors, we introduce clinical code verification as both a standalone
task and a pipeline component. To mitigate the limitations in existing
datasets, such as incomplete evidence and inpatient bias in MIMIC, we release
an expert double-annotated benchmark of outpatient clinical notes with ICD-10
codes. Our results highlight verification as an effective and reliable step
toward improving LLM-based medical coding.

</details>


### [20] [Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models](https://arxiv.org/abs/2510.07642)
*Đorđe Klisura,Joseph Khoury,Ashish Kundu,Ram Krishnan,Anthony Rios*

Main category: cs.CL

> 研究了大型语言模型中遵守访问控制规则的能力，使用了包含角色政策扩增的数据集，并比较了不同设计方法的效果。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型常常在访问控制的角色界限上出现模糊，产生不受限的答案。该研究旨在通过评估当模型授权时回答，未经授权时拒绝的模式，来改进这种状况。

**Method:** 本文研究了在大型语言模型中实施基于角色的访问控制的行为。为此，作者扩展了Spider和BIRD的文本到SQL数据集，并通过添加实际的PostgreSQL角色政策来模拟访问控制环境。比较了三种设计：零或少样本提示、两阶段的生成器-验证器流水线以及使用LoRA微调模型。

**Result:** 通过对多种模型家族的测试发现，明确的验证步骤（两阶段框架）能提高拒绝精度并减少误判。同时，微调方式能够在安全性和效用性之间取得更好的平衡（即在考虑执行准确性时）。更长和更复杂的政策会降低所有系统的可靠性。

**Conclusion:** 两步验证体系增强了拒绝精度和减少了误判，而微调模型在安全性和效用性平衡上表现更佳。不过，更长和复杂的政策降低了系统整体可靠性。该研究还公开发布了包含角色增扩的数据集和代码。

**Abstract:** Access control is a cornerstone of secure computing, yet large language
models often blur role boundaries by producing unrestricted responses. We study
role-conditioned refusals, focusing on the LLM's ability to adhere to access
control policies by answering when authorized and refusing when not. To
evaluate this behavior, we created a novel dataset that extends the Spider and
BIRD text-to-SQL datasets, both of which have been modified with realistic
PostgreSQL role-based policies at the table and column levels. We compare three
designs: (i) zero or few-shot prompting, (ii) a two-step generator-verifier
pipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that
learn permission awareness directly. Across multiple model families, explicit
verification (the two-step framework) improves refusal precision and lowers
false permits. At the same time, fine-tuning achieves a stronger balance
between safety and utility (i.e., when considering execution accuracy). Longer
and more complex policies consistently reduce the reliability of all systems.
We release RBAC-augmented datasets and code.

</details>


### [21] [Banking Done Right: Redefining Retail Banking with Language-Centric AI](https://arxiv.org/abs/2510.07645)
*Xin Jie Chua,Jeraelyn Ming Li Tan,Jia Xuan Tan,Soon Chang Poh,Yi Xian Goh,Debbie Hui Tian Choong,Chee Mun Foong,Sze Jue Yang,Chee Seng Chan*

Main category: cs.CL

> 本文论述了Ryt AI，这是一个经全球监管机构批准的人工智能代理框架，用于Ryt Bank，使客户能通过自然语言对话方式执行核心金融交易，展示了符合严格监管标准的自然语言接口在金融交易的支持能力。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在展示Ryt AI作为第一款全球范围内经监管机构批准的面向金融交易的主要银行业务界面，不同于以往仅限于咨询或支持角色的助手。通过这种创新性的方法，实现了自然语言接口在严格监管下的可靠支持。

**Method:** 本文介绍了Ryt AI，这是一个基于LLM的代理框架，用于Ryt Bank，使客户能够通过自然语言对话执行核心金融交易。该框架完全由内部开发，基于闭源LLM ILMU构建，取代了僵硬的多屏幕工作流，通过四个由LLM驱动的代理（Guardrails、Intent、Payment和FAQ）来完成单一的对话。每个代理都为ILMU附上了特定任务的LoRA适配器，以保证一致性并减少资源消耗。

**Result:** 通过Ryt AI的实现，展示了经监管机构批准的自然语言接口能够可靠地支持严格监管下的核心金融操作。

**Conclusion:** 通过Ryt AI的实际应用，证明了自然语言接口在符合严格监管的前提下能够可靠且有效地支持核心金融服务，展示了一种革新性的银行用户体验方式。

**Abstract:** This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt
Bank to enable customers to execute core financial transactions through natural
language conversation. This represents the first global regulator-approved
deployment worldwide where conversational AI functions as the primary banking
interface, in contrast to prior assistants that have been limited to advisory
or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a
closed-source LLM developed internally, and replaces rigid multi-screen
workflows with a single dialogue orchestrated by four LLM-powered agents
(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific
LoRA adapter to ILMU, which is hosted within the bank's infrastructure to
ensure consistent behavior with minimal overhead. Deterministic guardrails,
human-in-the-loop confirmation, and a stateless audit architecture provide
defense-in-depth for security and compliance. The result is Banking Done Right:
demonstrating that regulator-approved natural-language interfaces can reliably
support core financial operations under strict governance.

</details>


### [22] [OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2510.07651)
*Yuzhe Gu,Xiyu Liang,Jiaojiao Zhao,Enmao Diao*

Main category: cs.CL

> 该研究提出了一种新的缓存淘汰策略OBCache，通过测量剪枝令牌对注意力输出的扰动来量化令牌的重要性，实验结果表明这种方法可以提高大语言模型在长上下文中的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的缓存淘汰方法通过利用注意力的稀疏性来解决大语言模型（LLMs）随序列长度和批次大小线性增加的内存开销，但这些方法通常是使用累积注意力权重来对令牌进行启发式排名，而没有考虑它们对注意力输出的真实影响。本研究旨在解决这一问题。

**Method:** 该研究提出了一种名为OBCache的框架，该框架基于Optimal Brain Damage (OBD)理论，将缓存淘汰策略转化为层的结构剪枝问题。OBCache通过测量剪枝令牌对注意力输出的扰动来量化令牌的重要性，并且直接计算了孤立键、孤立值和联合键值对的分数。该分数不仅考虑了注意力权重，还考虑了值状态和注意力输出的信息，从而在现有的淘汰策略中增加了输出感知信号。

**Result:** 实验结果表明，在LLaMA和Qwen模型上，使用OBCache的输出感知分数替换现有方法中估计不同查询位置令牌重要性的启发式分数，可以一致地提高长上下文的准确性。

**Conclusion:** 该研究证明了其提出的OBCache框架在提高大语言模型在长上下文应用场景下的性能方面的有效性。

**Abstract:** Large language models (LLMs) with extended context windows enable powerful
downstream applications but impose significant memory overhead, as caching all
key-value (KV) states scales linearly with sequence length and batch size.
Existing cache eviction methods address this by exploiting attention sparsity,
yet they typically rank tokens heuristically using accumulated attention
weights without considering their true impact on attention outputs. We propose
Optimal Brain Cache (OBCache), a principled framework that formulates cache
eviction as a layer-wise structured pruning problem. Building upon the Optimal
Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the
perturbation in attention outputs induced by pruning tokens, with closed-form
scores derived for isolated keys, isolated values, and joint key-value pairs.
Our scores account not only for attention weights but also for information from
value states and attention outputs, thereby enhancing existing eviction
strategies with output-aware signals. Experiments on LLaMA and Qwen models
demonstrate that replacing the heuristic scores in existing works, which
estimate token saliency across different query positions, with OBCache's
output-aware scores consistently improves long-context accuracy.

</details>


### [23] [Textual Entailment and Token Probability as Bias Evaluation Metrics](https://arxiv.org/abs/2510.07662)
*Virginia K. Felkner,Allison Lim,Jonathan May*

Main category: cs.CL

> 研究提出使用自然語言推理（NLI）作為一種新的语言模型偏見評估方法，並發現與传统的按词元概率（TP）评估方法相比，NLI表现出不同的行为模式，但两者都不是在所有情况下都是"更好的"偏見評估指標，因此建议结合TP、NLI和其他下游偏見評估方法进行綜合評估。

<details>
  <summary>Details</summary>

**Motivation:** 研究動機是尋找一種更符合實際語言模型應用和損害案例的偏見評估方法，這是因為目前常用的按詞元概率（TP）評估指標雖然適用範圍廣，但被批評與真實世界的應用脫離。

**Method:** 本研究採用了自然語言推理（NLI）作為替代的偏見評估指標，用於評測語言模型中的社會偏見，並將其與傳統的按詞元概率（TP）評估方法進行比較。

**Result:** 研究結果表明，NLI評估和TP評估呈現出明顯的不同行為，並且兩者之間的相關性非常低。NLI指標更能夠檢測到"欠去偏見"的情形，但對於反立體型句子的詞彙變化比較敏感。

**Conclusion:** 結論指出，在评估语言模型的偏見時，既有的词元概率（TP）和新的自然语言推理（NLI）方法各有利弊，建议结合两种以及更多其他下游偏見評估方法以确保语言模型的全面评估。

**Abstract:** Measurement of social bias in language models is typically by token
probability (TP) metrics, which are broadly applicable but have been criticized
for their distance from real-world langugage model use cases and harms. In this
work, we test natural language inference (NLI) as a more realistic alternative
bias metric. We show that, curiously, NLI and TP bias evaluation behave
substantially differently, with very low correlation among different NLI
metrics and between NLI and TP metrics. We find that NLI metrics are more
likely to detect "underdebiased" cases. However, NLI metrics seem to be more
brittle and sensitive to wording of counterstereotypical sentences than TP
approaches. We conclude that neither token probability nor natural language
inference is a "better" bias metric in all cases, and we recommend a
combination of TP, NLI, and downstream bias evaluations to ensure comprehensive
evaluation of language models.
  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.

</details>


### [24] [Stress-Testing Model Specs Reveals Character Differences among Language Models](https://arxiv.org/abs/2510.07686)
*Jifan Zhang,Henry Sleight,Andi Peng,John Schulman,Esin Durmus*

Main category: cs.CL

> 本文提出了一种新的方法，用于识别大型语言模型规范内部的价值原则冲突，通过生成各种权衡场景来实现，并检测到了大量分歧结果。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）越来越多地从AI准则和模型规范中获得训练，但这些规范存在内部冲突和情境覆盖不足的问题。本研究旨在系统地识别和解决这些问题。

**Method:** 提出了一种系统的方法来测试模型规范，自动生成各种价值权衡场景，迫使模型在冲突的原则之间进行明确的选择。通过这种方式，可以自动识别出规范中原则的矛盾和解释模糊的情况。

**Result:** 通过对12个主要提供者（Anthropic、OpenAI、Google、xAI）的前沿LLMs进行测试，研究人员发现了超过70,000个案例，这些案例表现出显著的行为分歧，揭示了模型规范中的问题。

**Conclusion:** 研究表明，模型的行为分歧强烈预示了模型规范中的潜在问题。研究进一步提供了详细的案例分析，展示了规范中的直接矛盾和解释模糊问题。

**Abstract:** Large language models (LLMs) are increasingly trained from AI constitutions
and model specifications that establish behavioral guidelines and ethical
principles. However, these specifications face critical challenges, including
internal conflicts between principles and insufficient coverage of nuanced
scenarios. We present a systematic methodology for stress-testing model
character specifications, automatically identifying numerous cases of principle
contradictions and interpretive ambiguities in current model specs.
  We stress test current model specs by generating scenarios that force
explicit tradeoffs between competing value-based principles. Using a
comprehensive taxonomy we generate diverse value tradeoff scenarios where
models must choose between pairs of legitimate principles that cannot be
simultaneously satisfied. We evaluate responses from twelve frontier LLMs
across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral
disagreement through value classification scores. Among these scenarios, we
identify over 70,000 cases exhibiting significant behavioral divergence.
Empirically, we show this high divergence in model behavior strongly predicts
underlying problems in model specifications. Through qualitative analysis, we
provide numerous example issues in current model specs such as direct
contradiction and interpretive ambiguities of several principles. Additionally,
our generated dataset also reveals both clear misalignment cases and
false-positive refusals across all of the frontier models we study. Lastly, we
also provide value prioritization patterns and differences of these models.

</details>


### [25] [Large Language Models Meet Virtual Cell: A Survey](https://arxiv.org/abs/2510.07706)
*Krinos Li,Xianglu Xiao,Shenglong Deng,Lucas He,Zijun Zhong,Yuanjie Zou,Zhonghao Zhan,Zheng Hui,Weiye Bao,Guang Yang*

Main category: cs.CL

> 论文探讨了大型语言模型（LLMs）在细胞生物学中

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型正在通过支持“虚拟细胞”的开发，对细胞生物学产生变革性的影响。“虚拟细胞”是一种计算系统，能表示、预测和推测细胞状态和行为。本文旨在对大型语言模型在虚拟细胞建模中的应用进行全面回顾。

**Method:** 此论文提出了一个将现有方法整理为两种范式的统一分类法：一种是将大型语言模型作为直接进行细胞建模的“预言者”，另一种是将大型语言模型作为“代理”，用于编排复杂的科学任务。

**Result:** 该研究回顾了相关模型、数据集和评估基准，并指出了关于模型规模、通用性和可解释性方面的关键挑战。

**Conclusion:** 研究识别了三种核心任务：细胞表示、扰动预测和基因调控推理，指出当前虚拟细胞建模中最关键的挑战在于模型的可扩展性、通用性和可解释性。

**Abstract:** Large language models (LLMs) are transforming cellular biology by enabling
the development of "virtual cells"--computational systems that represent,
predict, and reason about cellular states and behaviors. This work provides a
comprehensive review of LLMs for virtual cell modeling. We propose a unified
taxonomy that organizes existing methods into two paradigms: LLMs as Oracles,
for direct cellular modeling, and LLMs as Agents, for orchestrating complex
scientific tasks. We identify three core tasks--cellular representation,
perturbation prediction, and gene regulation inference--and review their
associated models, datasets, evaluation benchmarks, as well as the critical
challenges in scalability, generalizability, and interpretability.

</details>


### [26] [Causality Guided Representation Learning for Cross-Style Hate Speech Detection](https://arxiv.org/abs/2510.07707)
*Chengshuai Zhao,Shu Wan,Paras Sheth,Karan Patwa,K. Selçuk Candan,Huan Liu*

Main category: cs.CL

> The paper addresses the limitations of existing hate speech detection models by proposing CADET, a causal representation learning framework, which can effectively disentangle and isolate genuine hate intent.

<details>
  <summary>Details</summary>

**Motivation:** The authors aim to address the challenges posed by implicit hate speech and the issue of spurious correlations that arise with the current hate speech detection approaches.

**Method:** CADET is a causal representation learning framework that disentangles hate speech into interpretable latent factors to control confounders and isolate genuine hate intent from superficial linguistic cues. It allows for counterfactual reasoning by intervening on style within the latent space.

**Result:** CADET demonstrates superior performance in comprehensive experiments, highlighting its effectiveness in advancing generalizable hate speech detection.

**Conclusion:** The use of causal priors within the CADET framework presents a promising direction for the robust and generalizable detection of hate speech, including implicit forms.

**Abstract:** The proliferation of online hate speech poses a significant threat to the
harmony of the web. While explicit hate is easily recognized through overt
slurs, implicit hate speech is often conveyed through sarcasm, irony,
stereotypes, or coded language -- making it harder to detect. Existing hate
speech detection models, which predominantly rely on surface-level linguistic
cues, fail to generalize effectively across diverse stylistic variations.
Moreover, hate speech spread on different platforms often targets distinct
groups and adopts unique styles, potentially inducing spurious correlations
between them and labels, further challenging current detection approaches.
Motivated by these observations, we hypothesize that the generation of hate
speech can be modeled as a causal graph involving key factors: contextual
environment, creator motivation, target, and style. Guided by this graph, we
propose CADET, a causal representation learning framework that disentangles
hate speech into interpretable latent factors and then controls confounders,
thereby isolating genuine hate intent from superficial linguistic cues.
Furthermore, CADET allows counterfactual reasoning by intervening on style
within the latent space, naturally guiding the model to robustly identify hate
speech in varying forms. CADET demonstrates superior performance in
comprehensive experiments, highlighting the potential of causal priors in
advancing generalizable hate speech detection.

</details>


### [27] [MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation](https://arxiv.org/abs/2510.07713)
*Shuo Yu,Mingyue Cheng,Daoyu Wang,Qi Liu,Zirui Liu,Ze Guo,Xiaoyu Tao*

Main category: cs.CL

> 本文介绍MemWeaver框架，能够捕捉用户兴趣的时序演变和不同活动之间的语义关系，支持深度个性化内容生成。实验验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于，当前仅基于隐式反馈信号如浏览和点击进行用户-网络互动的方式正逐步转向使用丰富的显式反馈，即来自文本互动行为的反馈。这为更深层次的个性化提供了丰富的机会。然而，现有方法只能提供有限的个性化，将用户历史视为一个简单的文本列表用于检索，而无法建模这些丰富的时间和语义结构。

**Method:** 本研究提出了MemWeaver框架，该框架能够将用户的整个文本历史编织成一个层次记忆结构，以实现深度个性化生成。其核心创新在于能够捕捉兴趣的时序演变和不同活动之间的语义关系。MemWeaver构建了两个互补的记忆组件：行为记忆，捕捉具体用户行为；认知记忆，代表长期偏好。这两种记忆组件共同作为用户统一表示，使得大语言模型（LLMs）能够同时推理具体行为和抽象特性。

**Result:** 实验在Language Model Personalization (LaMP)基准上验证了MemWeaver框架的有效性。

**Conclusion:** 本研究证明了MemWeaver框架的有效性，通过构建层次化的记忆结构，能够捕捉用户兴趣的时序和语义关系，从而实现更深度的个性化生成。

**Abstract:** The primary form of user-internet engagement is shifting from leveraging
implicit feedback signals, such as browsing and clicks, to harnessing the rich
explicit feedback provided by textual interactive behaviors. This shift unlocks
a rich source of user textual history, presenting a profound opportunity for a
deeper form of personalization. However, prevailing approaches offer only a
shallow form of personalization, as they treat user history as a flat list of
texts for retrieval and fail to model the rich temporal and semantic structures
reflecting dynamic nature of user interests. In this work, we propose
\textbf{MemWeaver}, a framework that weaves the user's entire textual history
into a hierarchical memory to power deeply personalized generation. The core
innovation of our memory lies in its ability to capture both the temporal
evolution of interests and the semantic relationships between different
activities. To achieve this, MemWeaver builds two complementary memory
components that both integrate temporal and semantic information, but at
different levels of abstraction: behavioral memory, which captures specific
user actions, and cognitive memory, which represents long-term preferences.
This dual-component memory serves as a unified representation of the user,
allowing large language models (LLMs) to reason over both concrete behaviors
and abstracted traits. Experiments on the Language Model Personalization (LaMP)
benchmark validate the efficacy of MemWeaver. Our code is
available\footnote{https://github.com/fishsure/MemWeaver}.

</details>


### [28] [SUBQRAG: sub-question driven dynamic graph rag](https://arxiv.org/abs/2510.07718)
*Jiaoyang Li,Junhao Ruan,Shengwei Tang,Saihan Chen,Kaiyan Chang,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

> 提出了SubQRAG框架，通过子问题分解和动态扩展知识图来改善多跳问题回答中的深度推理，实验证明其有效性和改进显著。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决Graph RAG在复杂多跳问题回答中的深度结构化推理不足的问题，即由于广视角方法导致的证据不完整和错误累积。

**Method:** SubQRAG采用子问题驱动的框架来增强推理深度，将复杂的查询分解为可验证的子问题链。对于每个子问题，系统从图中检索相关的三元组，当现有图不足以解决问题时，系统可以实时从源文档中提取新的三元组来动态扩展图。

**Result:** 实验结果表明，SubQRAG在三个多跳问题回答基准测试中表现出一致且显著的改进，尤其是在精确匹配分值上。

**Conclusion:** SubQRAG通过子问题分解和动态扩展图的方法，有效处理了复杂多跳问题回答中的推理深度不足，最终生成了更准确的答案。

**Abstract:** Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a
knowledge graph (KG) to connect disparate facts across a large document corpus.
However, this broad-view approach often lacks the deep structured reasoning
needed for complex multi-hop question answering (QA), leading to incomplete
evidence and error accumulation. To address these limitations, we propose
SubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG
decomposes a complex question into an ordered chain of verifiable
sub-questions. For each sub-question, it retrieves relevant triples from the
graph. When the existing graph is insufficient, the system dynamically expands
it by extracting new triples from source documents in real time. All triples
used in the reasoning process are aggregated into a "graph memory," forming a
structured and traceable evidence path for final answer generation. Experiments
on three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent
and significant improvements, especially in Exact Match scores.

</details>


### [29] [Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing](https://arxiv.org/abs/2510.07736)
*Cunli Mao,Xiaofei Gao,Ran Song,Shizhu He,Shengxiang Gao,Kang Liu,Zhengtao Yu*

Main category: cs.CL

> A new MKGC framework utilizing a novel architecture combining KL-GMoE and IER to significantly enhance the performance of multilingual knowledge graph completion.

<details>
  <summary>Details</summary>

**Motivation:** The research aims to overcome limitations in existing MKGC approaches by better utilizing the multilingual capabilities of large language models and promoting the shareability of cross-lingual knowledge.

**Method:** The paper introduces a new MKGC framework, which comprises two components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER). The KL-GMoE models the shared multilingual knowledge efficiently, and the IER optimizes the utilization of this knowledge.

**Result:** The research demonstrates an enhancement of MKGC with improvements up to 5.47% in the Hits@1 metric over SOTA methods, and provides a new dataset for evaluation and further research.

**Conclusion:** The proposed framework advances the state of the art in MKGC by leveraging multilingual shared knowledge, achieving significant improvements in various evaluation metrics. It also offers insights into the challenges and properties of knowledge sharing across different linguistic settings.

**Abstract:** Large language models (LLMs) based Multilingual Knowledge Graph Completion
(MKGC) aim to predict missing facts by leveraging LLMs' multilingual
understanding capabilities, improving the completeness of multilingual
knowledge graphs (KGs). However, existing MKGC research underutilizes the
multilingual capabilities of LLMs and ignores the shareability of cross-lingual
knowledge. In this paper, we propose a novel MKGC framework that leverages
multilingual shared knowledge to significantly enhance performance through two
components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative
Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER
significantly enhances its utilization. To evaluate our framework, we
constructed a mKG dataset containing 5 languages and conducted comprehensive
comparative experiments with existing state-of-the-art (SOTA) MKGC method. The
experimental results demonstrate that our framework achieves improvements of
5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics,
respectively, compared with SOTA MKGC method. Further experimental analysis
revealed the properties of knowledge sharing in settings of unseen and
unbalanced languages. We have released the dataset and code for our work on
https://github.com/gaoxiaofei07/KL-GMoE.

</details>


### [30] [ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs](https://arxiv.org/abs/2510.07737)
*Fu Chen,Peng Wang,Xiyin Li,Wen Li,Shichi Lei,Dongdong Xiang*

Main category: cs.CL

> The paper introduces ToolExpander, a new framework that enhances tool-oriented reinforcement learning for resource-constrained Large Language Models (LLMs) by addressing instability and performance issues during training.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the common challenges faced during the training of LLMs with Group Relative Policy Optimization (GRPO), including poor response accuracy, especially in small-scale models, and mid-training collapse.

**Method:** ToolExpander introduces Dynamic Multi-Round Hard Sampling and Self-Exemplifying Thinking. The first involves substituting difficult samples with high-quality demonstrations while applying an exponential learning rate decay. The latter enhances GRPO by removing KL divergence and adding adjusted clipping coefficients, promoting autonomous generation and analysis of few-shot examples.

**Result:** The results show that ToolExpander significantly improves tool-using capabilities and contributes to higher stability and performance in LLMs, particularly in smaller-scale architectures.

**Conclusion:** The conclusion is that ToolExpander effectively addresses the key challenges in training LLMs with GRPO, leading to better stability and performance, especially in small-scale models with limited resources.

**Abstract:** Training Large Language Models (LLMs) with Group Relative Policy Optimization
(GRPO) encounters a significant challenge: models often fail to produce
accurate responses, particularly in small-scale architectures. This limitation
not only diminishes performance improvements and undermines the potential of
GRPO but also frequently leads to mid-training collapse, adversely affecting
stability and final efficacy. To address these issues, we propose ToolExpander,
a novel framework that advances tool-oriented reinforcement learning for
resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round
Hard Sampling, which dynamically substitutes challenging samples(those without
correct outputs over 10 rollouts) with high-quality few-shot demonstrations
during training, coupled with an exponential learning rate decay strategy to
mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO
framework that eliminates KL divergence and incorporates adjusted clipping
coefficients, encouraging models to autonomously generate and analyze few-shot
examples via a minimal additional reward (0.01).Experimental results
demonstrate that ToolExpander significantly enhances tool-using capabilities in
LLMs, especially in weaker small-scale models, improving both training
stability and overall performance.

</details>


### [31] [OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment](https://arxiv.org/abs/2510.07743)
*Tianci Liu,Ran Xu,Tony Yu,Ilgee Hong,Carl Yang,Tuo Zhao,Haoyu Wang*

Main category: cs.CL

> 本研究介绍了OpenRubrics和对比式Rubric生成方法，以解决奖励模型中的人类偏好多维度捕捉问题，新的方法显著提高了奖励模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的奖励模型主要依赖于标量或成对判断，无法捕捉人类偏好的多维度特性。本研究旨在解决可靠和可扩展的评价标准生成问题，因此引入了OpenRubrics和对比式rubric生成方法。

**Method:** 提出了一种名为OpenRubrics的多样且大规模的(prompt, rubric)对集合，用于训练rubric生成和基于rubric的奖励模型。通过对比优选和拒绝的回答，引入了对比式rubric生成(CRG)方法，能够区分并全面地评估信号。同时，通过拒绝采样去除噪音rubric，以提高可靠性。

**Result:** 基于rubric的奖励模型Rubric-RM，在多个奖励模型基准上，超越了同等规模的竞争者6.8%。这些改进同样适用于指令遵循和生物医学基准的策略模型。

**Conclusion:** 研究表明，基于rubric的方法为LLM的对齐提供了可扩展的对齐信号，缩小了昂贵的人类评估与自动化奖励建模之间的差距，并开启了新的基于原理的对齐范式。

**Abstract:** Reward modeling lies at the core of reinforcement learning from human
feedback (RLHF), yet most existing reward models rely on scalar or pairwise
judgments that fail to capture the multifaceted nature of human preferences.
Recent studies have explored rubrics-as-rewards (RaR) that uses structured
natural language criteria that capture multiple dimensions of response quality.
However, producing rubrics that are both reliable and scalable remains a key
challenge. In this work, we introduce OpenRubrics, a diverse, large-scale
collection of (prompt, rubric) pairs for training rubric-generation and
rubric-based reward models. To elicit discriminative and comprehensive
evaluation signals, we introduce Contrastive Rubric Generation (CRG), which
derives both hard rules (explicit constraints) and principles (implicit
qualities) by contrasting preferred and rejected responses. We further improve
reliability by enforcing preference-label consistency via rejection sampling to
remove noisy rubrics. Across multiple reward-modeling benchmarks, our
rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines
by 6.8%. These gains transfer to policy models on instruction-following and
biomedical benchmarks. Our results show that rubrics provide scalable alignment
signals that narrow the gap between costly human evaluation and automated
reward modeling, enabling a new principle-driven paradigm for LLM alignment.

</details>


### [32] [Parallel Test-Time Scaling for Latent Reasoning Models](https://arxiv.org/abs/2510.07745)
*Runyang You,Yongqi Li,Meng Liu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.CL

> 该论文研究使潜推理模型能够利用并行测试时间缩放技术，通过引入新的采样方法和潜奖励模型解决了连续空间中缺乏采样和概率信号的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的隐式推理方法缺乏采样机制和概率信号，使得难以在连续空间中实现并行的测试时间缩放(TTS)。这项研究旨在解决这些问题并使潜推理模型也能从并行TTS中受益。

**Method:** 该研究引入了两种基于不确定性的随机策略（蒙特卡洛丢弃法和加性高斯噪声）来应对采样问题，并设计了一个基于逐步对比目标训练的潜奖励模型（LatentRM）来评分和引导潜推理的聚合过程。

**Result:** 实验和可视化分析表明，两种采样策略都能随着计算量的增加而有效扩展，并表现出独特的探索动态；而潜奖励模型使得轨迹选择更有效。

**Conclusion:** 通过这些探索，研究开启了一个新的方向，使得在连续空间中的可扩展推理成为可能。

**Abstract:** Parallel test-time scaling (TTS) is a pivotal approach for enhancing large
language models (LLMs), typically by sampling multiple token-based
chains-of-thought in parallel and aggregating outcomes through voting or
search. Recent advances in latent reasoning, where intermediate reasoning
unfolds in continuous vector spaces, offer a more efficient alternative to
explicit Chain-of-Thought, yet whether such latent models can similarly benefit
from parallel TTS remains open, mainly due to the absence of sampling
mechanisms in continuous space, and the lack of probabilistic signals for
advanced trajectory aggregation. \ This work enables parallel TTS for latent
reasoning models by addressing the above issues. For sampling, we introduce two
uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive
Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)
trained with step-wise contrastive objective to score and guide latent
reasoning. Extensive experiments and visualization analyses show that both
sampling strategies scale effectively with compute and exhibit distinct
exploration dynamics, while LatentRM enables effective trajectory selection.
Together, our explorations open a new direction for scalable inference in
continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.

</details>


### [33] [Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers](https://arxiv.org/abs/2510.07761)
*Nishant Balepur,Atrey Desai,Rachel Rudinger*

Main category: cs.CL

> 研究发现，推理能力的大语言模型（LLMs）在解决MCQA问题中，即使在仅使用选项的情况下也表现良好，且这种成功并不总是由于浅层策略导致。通过推理轨迹可以更准确地评估这些模型在部分输入情况下的表现。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于探索LLMs在使用仅选项输入（choices-only）进行MCQA时的成功是否本质上是浅层策略导致的，以及这些策略是否存在问题。

**Method:** 研究使用带有推理能力的大语言模型（LLMs）来解决包含全部输入和仅选项输入的多项选择题（MCQs），并通过测试时间推理来提升其准确率。通过比较不同长度的推理轨迹，以及通过可靠性测试来评估这些轨迹的有效性。

**Result:** 研究发现，虽然带有推理能力的LLMs在full input上的准确率有所提升，但在choices-only input上则只有一半的情况得到了改进。结果还显示出，仅选项输入的成功几乎不受推理轨迹长度的影响，并且通过了可靠性测试的推理轨迹使用了较少问题的策略，如推断缺失问题。

**Conclusion:** 该研究挑战了部分输入成功总是缺陷的观点，并探讨了如何通过推理轨迹来区分存在问题的数据和较少存在问题的推理。

**Abstract:** Large language models (LLMs) now give reasoning before answering, excelling
in tasks like multiple-choice question answering (MCQA). Yet, a concern is that
LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed
in MCQA without using the question, i.e., choices-only. Such partial-input
success is often deemed problematic, but reasoning traces could reveal if these
strategies are truly shallow in choices-only settings. To study these
strategies, reasoning LLMs solve MCQs in full and choices-only inputs;
test-time reasoning often boosts accuracy on full and in choices-only half the
time. While possibly due to shallow shortcuts, choices-only success is barely
affected by the length of reasoning traces, and after finding traces pass
faithfulness tests, we show they use less problematic strategies like inferring
missing questions. In all, we challenge claims that partial-input success is
always a flaw, so we discuss how reasoning traces could separate problematic
data from less problematic reasoning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation](https://arxiv.org/abs/2510.07346)
*Nader Nemati*

Main category: cs.CV

> A real-time maritime object detection system using RT-DETR is proposed, using synthetic images and techniques like feature fusion and query selection to detect small targets effectively, validated through an analysis of system performance and component contributions.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenges in maritime object detection, such as small target size and lack of real labeled RGB data, by developing a real-time detection system that can handle low-contrast and small vessels effectively.

**Method:** This paper presents a real-time object detection system for maritime use, based on RT-DETR, combining augmented synthetic images with real data. It uses multi-scale feature fusion, uncertainty-minimizing query selection, and a weighting strategy. Data augmentation techniques balance the dataset classes, enhancing the model's robustness and accuracy.

**Result:** The study demonstrates a real-time maritime detection system that balances speed and accuracy through a flexible design and components analysis that quantifies each architectural module's contribution. It evaluates how the system performs under extreme conditions.

**Conclusion:** The paper concludes that the proposed system, based on RT-DETR and augmented with synthetic images, is effective in enhancing the detection of small, low-contrast maritime objects, while maintaining real-time performance. The system's ability to adapt to variable conditions improves overall robustness.

**Abstract:** Maritime object detection faces essential challenges due to the small target
size and limitations of labeled real RGB data. This paper will present a
real-time object detection system based on RT-DETR, enhanced by employing
augmented synthetic images while strictly evaluating on real data. This study
employs RT-DETR for the maritime environment by combining multi-scale feature
fusion, uncertainty-minimizing query selection, and smart weight between
synthetic and real training samples. The fusion module in DETR enhances the
detection of small, low-contrast vessels, query selection focuses on the most
reliable proposals, and the weighting strategy helps reduce the visual gap
between synthetic and real domains. This design preserves DETR's refined
end-to-end set prediction while allowing users to adjust between speed and
accuracy at inference time. Data augmentation techniques were also used to
balance the different classes of the dataset to improve the robustness and
accuracy of the model. Regarding this study, a full Python robust maritime
detection pipeline is delivered that maintains real-time performance even under
practical limits. It also verifies how each module contributes, and how the
system handles failures in extreme lighting or sea conditions. This study also
includes a component analysis to quantify the contribution of each
architectural module and explore its interactions.

</details>


### [35] [DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis](https://arxiv.org/abs/2510.07441)
*Nithin C. Babu,Aniruddha Mahapatra,Harsh Rangwani,Rajiv Soundararajan,Kuldeep Kulkarni*

Main category: cs.CV

> 为了评估文本到视频(T2V)生成模型在动态摄像机运动下的性能，DynamicEval基准包含多个系统化评估元素，包括背景和前景物体的一致性评价方法，显著提高了与人类评估的一致性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于解决现有文本到视频(T2V)评估基准对摄像机动态运动下的镜头生成评估不足的问题，并关注视频级别评估的重要性，以更全面地选择更好质量的视频。

**Method:** 该研究提出了一种名为DynamicEval的新评估基准，通过包含强调动态摄像机运动的系统化提示以及相应的视频和人类注释，评估背景场景一致性和前景物体一致性。

**Result:** 当前的文本到视频(T2V)评估基准存在两个主要限制：一是忽视了摄像机动态运动对生成电影镜头的影响；二是将视频级别得分整合到单个模型级别得分中，忽视了视频级别的评估。为了解决这些问题，作者提出了一种新的评估基准DynamicEval，它包含多达45,000个人类注释的视频，这些视频是通过十种不同的T2V模型生成的。该基准系统地强调了摄像机动态运动，并且评估了两个关键的视频质量维度：背景场景一致性和前景物体一致性。对于背景场景一致性，作者通过Vbench运动平滑度指标得到了可解释的误差图，但也指出它在处理遮挡和解除遮挡的情况时存在问题。为此，作者提出了一种新的背景一致性指标，利用物体误差图来纠正这些失败情况，同时引入了一个前景一致性指标，该指标通过跟踪每个物体实例内的点及其邻域来评估物体的真实度。实验表明，这些建议的指标在视频级别和模型级别与人类偏好之间的相关性有了超过2%的提升，从而证明DynamicEval是评估T2V模型下动态摄像机运动的更综合的基准。

**Conclusion:** 实验表明，所建议的背景一致性指标和前景一致性指标，与人类偏好之间的相关性有了超过2%的提升，证明了DynamicEval作为一个评估T2V模型下动态摄像机运动的更综合基准的有效性。

**Abstract:** Existing text-to-video (T2V) evaluation benchmarks, such as VBench and
EvalCrafter, suffer from two limitations. (i) While the emphasis is on
subject-centric prompts or static camera scenes, camera motion essential for
producing cinematic shots and existing metrics under dynamic motion are largely
unexplored. (ii) These benchmarks typically aggregate video-level scores into a
single model-level score for ranking generative models. Such aggregation,
however, overlook video-level evaluation, which is vital to selecting the
better video among the candidate videos generated for a given prompt. To
address these gaps, we introduce DynamicEval, a benchmark consisting of
systematically curated prompts emphasizing dynamic camera motion, paired with
45k human annotations on video pairs from 3k videos generated by ten T2V
models. DynamicEval evaluates two key dimensions of video quality: background
scene consistency and foreground object consistency. For background scene
consistency, we obtain the interpretable error maps based on the Vbench motion
smoothness metric. We observe that while the Vbench motion smoothness metric
shows promising alignment with human judgments, it fails in two cases:
occlusions/disocclusions arising from camera and foreground object movements.
Building on this, we propose a new background consistency metric that leverages
object error maps to correct two failure cases in a principled manner. Our
second innovation is the introduction of a foreground consistency metric that
tracks points and their neighbors within each object instance to assess object
fidelity. Extensive experiments demonstrate that our proposed metrics achieve
stronger correlations with human preferences at both the video level and the
model level (an improvement of more than 2% points), establishing DynamicEval
as a more comprehensive benchmark for evaluating T2V models under dynamic
camera motion.

</details>


### [36] [Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors](https://arxiv.org/abs/2510.07470)
*Marien Renaud,Julien Hermant,Deliang Wei,Yu Sun*

Main category: cs.CV

> 本文提出的RISP方法结合了重启惯性和基于分数的图像先验，实现了在快速收敛的同时获得高质量的图像重建。

<details>
  <summary>Details</summary>

**Motivation:** 现有的如正则化去噪（RED）等方法通常专注于通过设计复杂的图像先验来改善重建质量，而将收敛加速问题留给启发式方法解决。这项工作旨在填补这一空白。

**Method:** 提出了名为Restarted Inertia with Score-based Priors (RISP) 的方法，该方法通过引入重启惯性以加速收敛，同时利用基于分数的图像先验来实现高质量的重建。

**Result:** 证明了RISP相比RED能够更快达到平稳点收敛率，无需图像先验的凸性要求。实验表明，RISP在多种成像逆问题中能够快速收敛并实现高质量的重建。

**Conclusion:** RISP方法不仅能加速收敛速度，还能提供高质量的重建结果，解决了成像逆问题中的两个关键需求。

**Abstract:** Fast convergence and high-quality image recovery are two essential features
of algorithms for solving ill-posed imaging inverse problems. Existing methods,
such as regularization by denoising (RED), often focus on designing
sophisticated image priors to improve reconstruction quality, while leaving
convergence acceleration to heuristics. To bridge the gap, we propose Restarted
Inertia with Score-based Priors (RISP) as a principled extension of RED. RISP
incorporates a restarting inertia for fast convergence, while still allowing
score-based image priors for high-quality reconstruction. We prove that RISP
attains a faster stationary-point convergence rate than RED, without requiring
the convexity of the image prior. We further derive and analyze the associated
continuous-time dynamical system, offering insight into the connection between
RISP and the heavy-ball ordinary differential equation (ODE). Experiments
across a range of imaging inverse problems demonstrate that RISP enables fast
convergence while achieving high-quality reconstructions.

</details>


### [37] [A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy](https://arxiv.org/abs/2510.07492)
*Guoliang Gong,Man Yu*

Main category: cs.CV

> 本研究提出了一种基于IP策略的降噪框架，通过FFM模型解决了uLDCT图像中的噪声和伪影问题，并且在保持解剖结构完整性方面取得了先进成果。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在解决uLDCT图像中由于辐射剂量降低引入的严重噪声和伪影，以及uLDCT与NDCT图像间的空间错位问题。这些问题使得直接应用现有的噪声模型变得困难。

**Method:** 本论文提出了一种基于图像净化（IP）策略的创新降噪框架，解决了低剂量CT（uLDCT）图像中的噪声、伪影及uLDCT与正常剂量CT（NDCT）图像间空间错位的问题。首先，构建了一个真实的临床uLDCT肺部数据集。然后，提出了一种图像净化策略，生成结构对齐的uLDCT-NDCT图像对。在此基础上，提出了一种频率域流匹配（FFM）模型，该模型与IP策略协同工作，能够很好地保持降噪图像的解剖结构完整性。

**Result:** 实验结果表明，提出的IP策略显著提升了多个主流降噪模型在uLDCT任务上的表现。结合IP策略的FFM模型在解剖结构保持方面达到了最先进的（SOTA）结果。

**Conclusion:** 本研究为真实世界uLDCT降噪中的数据不匹配问题提供了一个有效解决方案。

**Abstract:** Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but
introduces severe noise and artifacts. It also leads to substantial spatial
misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses
challenges for directly applying existing denoising networks trained on
synthetic noise or aligned data. To address this core challenge in uLDCT
denoising, this paper proposes an innovative denoising framework based on an
Image Purification (IP) strategy. First, we construct a real clinical uLDCT
lung dataset. Then, we propose an Image Purification strategy that generates
structurally aligned uLDCT-NDCT image pairs, providing a high-quality data
foundation for network training. Building upon this, we propose a
Frequency-domain Flow Matching (FFM) model, which works synergistically with
the IP strategy to excellently preserve the anatomical structure integrity of
denoised images. Experiments on the real clinical dataset demonstrate that our
IP strategy significantly enhances the performance of multiple mainstream
denoising models on the uLDCT task. Notably, our proposed FFM model combined
with the IP strategy achieves state-of-the-art (SOTA) results in anatomical
structure preservation. This study provides an effective solution to the data
mismatch problem in real-world uLDCT denoising. Code and dataset are available
at https://github.com/MonkeyDadLufy/flow-matching.

</details>


### [38] [D2RA: Dual Domain Regeneration Attack](https://arxiv.org/abs/2510.07538)
*Pragati Shuddhodhan Meshram,Varun Chandrasekaran*

Main category: cs.CV

> 研究提出D2RA，一种无需训练的水印移除技术，展示出对多种现有水印方案的影响，并揭示了当前水印设计的弱点。

<details>
  <summary>Details</summary>

**Motivation:** 由于近期语义水印方案在资源受限的对抗设定下仍显脆弱，因此研究旨在通过D2RA方法来揭示当前水印设计的基本弱点，并提供一种无需访问底层模型即可移除或削弱水印的技术。

**Method:** 本研究提出的D2RA方法是一种无需训练、单图像攻击的技术，通过将水印图像投影到互补表示上的自然先验，来抑制水印信号同时保持视觉保真度。

**Result:** 实验显示，该方法能够一致降低水印的检测能力，进一步证实了当前水印设计存在基础性问题。

**Conclusion:** 研究结果表明，即便是在强健性有所提升的语义水印方案下，也存在基本的设计漏洞，D2RA攻击方法能够有效地削弱这些水印的有效性。

**Abstract:** The growing use of generative models has intensified the need for
watermarking methods that ensure content attribution and provenance. While
recent semantic watermarking schemes improve robustness by embedding signals in
latent or frequency representations, we show they remain vulnerable even under
resource-constrained adversarial settings. We present D2RA, a training-free,
single-image attack that removes or weakens watermarks without access to the
underlying model. By projecting watermarked images onto natural priors across
complementary representations, D2RA suppresses watermark signals while
preserving visual fidelity. Experiments across diverse watermarking schemes
demonstrate that our approach consistently reduces watermark detectability,
revealing fundamental weaknesses in current designs. Our code is available at
https://github.com/Pragati-Meshram/DAWN.

</details>


### [39] [PickStyle: Video-to-Video Style Transfer with Context-Style Adapters](https://arxiv.org/abs/2510.07546)
*Soroush Mehraban,Vida Adeli,Jacob Rommann,Babak Taati,Kyryl Truskovskyi*

Main category: cs.CV

> 我们提出了一种名为PickStyle的视频风格转换框架，通过增加风格适配器和使用与源风格相匹配的配对静止图像数据训练，有效解决了缺乏配对视频数据进行监督的问题，实现了时间一致性、风格忠实和内容保留的视频转换。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于，视频风格转换的任务通常是保留输入视频的内容的同时将其渲染为由文本提示指定的目标风格。然而，缺乏配对的视频数据作为监督是一个主要挑战。

**Method:** 我们提出了PickStyle，这是一种视频到视频的风格转换框架，通过在预训练的视频扩散模型中添加风格适配器，并利用具有来源风格对应关系的配对静止图像数据进行训练。PickStyle通过在条件模块的自我注意力层中插入低秩适配器，实现高效专用于运动风格转换，同时保持视频内容和风格之间的一致性。为了弥补静态图像监督与动态视频之间的差距，我们从配对图像中构建合成训练片段，应用共享的增强处理以模拟相机运动。此外，我们引入了Context-Style Classifier-Free Guidance (CS-CFG)，这是一种将分类器无导向性解耦为独立的文本（风格）和视频（内容）方向的新方法。

**Result:** 实验表明，我们的方法能够实现时间一致、风格忠实和内容保留的视频转换，在多个基准上，在定性定量评估上都优于现有基线。

**Conclusion:** 总体而言，PickStyle通过独特的风格适配器和新颖的CS-CFG方法，实现了高质量、时间一致性的视频风格转换，并在多个基准上展示出优越的性能。

**Abstract:** We address the task of video style transfer with diffusion models, where the
goal is to preserve the context of an input video while rendering it in a
target style specified by a text prompt. A major challenge is the lack of
paired video data for supervision. We propose PickStyle, a video-to-video style
transfer framework that augments pretrained video diffusion backbones with
style adapters and benefits from paired still image data with source-style
correspondences for training. PickStyle inserts low-rank adapters into the
self-attention layers of conditioning modules, enabling efficient
specialization for motion-style transfer while maintaining strong alignment
between video content and style. To bridge the gap between static image
supervision and dynamic video, we construct synthetic training clips from
paired images by applying shared augmentations that simulate camera motion,
ensuring temporal priors are preserved. In addition, we introduce Context-Style
Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free
guidance into independent text (style) and video (context) directions. CS-CFG
ensures that context is preserved in generated video while the style is
effectively transferred. Experiments across benchmarks show that our approach
achieves temporally coherent, style-faithful, and content-preserving video
translations, outperforming existing baselines both qualitatively and
quantitatively.

</details>


### [40] [TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility](https://arxiv.org/abs/2510.07550)
*Saman Motamed,Minghao Chen,Luc Van Gool,Iro Laina*

Main category: cs.CV

> 本文旨在开发改进视频-语言模型评估视频物理真实性的方法，并提出了TRAVL和ImplausiBench作为评估框架。

<details>
  <summary>Details</summary>

**Motivation:** 当前的视频生成模型会生成违反物理定律的视频序列，而没有一种可靠的方法来定量评估视频的物理真实性。因此，该论文探索了利用视频-语言模型（VLMs）作为物理真实性的判断者。

**Method:** 该论文介绍了一种名为TRAVL的微调方案，结合平衡的训练数据集和轨迹感知注意力模块，以改进VLMs的运动编码和判别能力。同时提出了一种评估物理合理性的基准测试ImplausiBench，包括300个视频（150个真实视频和150个生成视频）。

**Result:** 研究发现现有的VLMs在识别物理违法行为方面存在困难，进而提出了TRAVL解决方案提高了模型的时间和因果关系推理能力。通过使用 ImplausiBench 基准测试，TRAVL的性能得到了验证。

**Conclusion:** TRAVL和ImplausiBench提供了一个统一的框架，用于探究和改进多模态模型中的物理真实性理解。

**Abstract:** Despite impressive visual fidelity, modern video generative models frequently
produce sequences that violate intuitive physical laws, such as objects
floating, teleporting, or morphing in ways that defy causality. While humans
can easily detect such implausibilities, there remains no robust method for
quantitatively assessing physical realism in video. In this work, we explore
whether Video-Language Models (VLMs) can be trained to serve as reliable judges
of physical plausibility. We find that existing VLMs struggle to identify
physics violations, exposing fundamental limitations in their temporal and
causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe
that combines a balanced training dataset with a trajectory-aware attention
module to improve motion encoding and discrimination in VLMs. To evaluate
physical reasoning more rigorously, we propose ImplausiBench, a benchmark of
300 videos (150 real, 150 generated) that removes linguistic biases and
isolates visual-temporal understanding. Performance is reported both with
gold-standard human judgments and stricter LLM-as-judge metrics. Together,
TRAVL and ImplausiBench offer a unified framework for probing and improving
physical plausibility in multimodal models, shedding light on a challenging and
underexplored aspect of visual-temporal understanding.

</details>


### [41] [Label Semantics for Robust Hyperspectral Image Classification](https://arxiv.org/abs/2510.07556)
*Rafin Hassan,Zarin Tasnim Roshni,Rafiqul Bari,Alimul Islam,Nabeel Mohammed,Moshiur Farazi,Shafin Rahman*

Main category: cs.CV

> 研究提出了一种名为S3FN的光谱空间融合网络，该网络结合了文本语义和光谱空间数据，以提升高光谱图像分类性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决了高光谱图像分类模型面临的挑战，这些挑战包括高光谱数据的高维度、高质量训练样本的不足、容易过拟合以及单模分类模型依赖光谱-空间数据的问题。

**Method:** 提出了一种名为S3FN的通用语义光谱空间融合网络，该网络利用上下文和类别特定的文本描述来补充HSI分类模型的训练。具体来说，S3FN利用大型语言模型（LLMs）为每个类别标签生成全面的文本描述，这些描述捕获了它们独特的特征和光谱行为。这些描述被嵌入到预训练文本编码器（如BERT或RoBERTa）的向量空间中，以提取有意义的标签语义，从而实现更好的特征标签对齐，提高分类性能。

**Result:** 在三个不同的高光谱图像基准数据集（Hyperspectral Wood, HyperspectralBlueberries, 和 DeepHS-Fruit）上的实验结果表明，方法性能有所提升，展示了文本语义与光谱空间数据结合的有效性。

**Conclusion:** 研究证实了文本语义与光谱空间数据结合的有效性，为语义增强的HSI分类模型的进一步发展铺平了道路。代码在指定的GitHub仓库中公开。本研究为HSI分类模型提供新的方法论视角。

**Abstract:** Hyperspectral imaging (HSI) classification is a critical tool with widespread
applications across diverse fields such as agriculture, environmental
monitoring, medicine, and materials science. Due to the limited availability of
high-quality training samples and the high dimensionality of spectral data, HSI
classification models are prone to overfitting and often face challenges in
balancing accuracy and computational complexity. Furthermore, most of HSI
classification models are monomodal, where it solely relies on spectral-spatial
data to learn decision boundaries in the high dimensional embedding space. To
address this, we propose a general-purpose Semantic Spectral-Spatial Fusion
Network (S3FN) that uses contextual, class specific textual descriptions to
complement the training of an HSI classification model. Specifically, S3FN
leverages LLMs to generate comprehensive textual descriptions for each class
label that captures their unique characteristics and spectral behaviors. These
descriptions are then embedded into a vector space using a pre-trained text
encoder such as BERT or RoBERTa to extract meaningful label semantics which in
turn leads to a better feature-label alignment for improved classification
performance. To demonstrate the effectiveness of our approach, we evaluate our
model on three diverse HSI benchmark datasets - Hyperspectral Wood,
HyperspectralBlueberries, and DeepHS-Fruit and report significant performance
boost. Our results highlight the synergy between textual semantics and
spectral-spatial data, paving the way for further advancements in semantically
augmented HSI classification models. Codes are be available in:
https://github.com/milab-nsu/S3FN

</details>


### [42] [Cross-Modal Attention Guided Unlearning in Vision-Language Models](https://arxiv.org/abs/2510.07567)
*Karuna Bhaila,Aneesh Komanduri,Minh-Hao Van,Xintao Wu*

Main category: cs.CV

> 本文提出了一种名为CAGUL的轻量级、高效的视觉语言模型不学习框架，专门针对视觉问答任务，通过交叉模态注意力机制引导，将不需要的信息编码在查询相关的低重要性视觉标记中，从而防止敏感信息泄露。实验结果表明，该方法在不改变预训练模型参数的前提下，与基于微调的基线方法相比具有更好的性能或相当的性能。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型在处理多模态理解和推理任务时，可能会在训练过程中记忆到私人和/或敏感的信息，并在推理中泄露出来。本文旨在解决这一问题，特别是在视觉问答任务中。

**Method:** 本文提出了交叉模态注意力引导的不学习（CAGUL）框架，利用视觉语言模型中的跨模态注意力机制，将不学习信息编码到低重要性的视觉标记中。

**Result:** 实验结果展示了本方法可以有效地防止敏感信息泄露，同时保持参考模型的行为特征，并且在性能上优于或等于基于微调的基线方法。这使得CAGUL成为一种实用且有效的视觉语言模型不学习解决方案。

**Conclusion:** CAGUL框架作为一种轻量级、高效的解决方案，适用于视觉语言模型中防止敏感信息泄露的问题，特别是在视觉问答任务中。它不改变预训练模型参数，避免了重新训练的成本。

**Abstract:** Vision-Language Models (VLMs) have demonstrated immense capabilities in
multi-modal understanding and inference tasks such as Visual Question Answering
(VQA), which requires models to infer outputs based on visual and textual
context simultaneously. Such inference abilities of large-scale pretrained
models are often attributed to the massive scale of pre-training data collected
across several domains. However, the models may memorize private and/or
sensitive information during training and regurgitate it in inference.
Recently, machine unlearning has been leveraged to address the leakage of
private data in LLMs. VLMs add a layer of complexity to this process, as the
visual context in the query may also contain sensitive information in addition
to the text. To address this issue, we explore unlearning for vision-language
models, specifically for the VQA task. We explore the role of visual tokens for
output generation in VLMs using cross-modal attention and utilize it to
formulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and
efficient VLM unlearning framework. In contrast to computationally expensive
model finetuning methods, CAGUL utilizes external modules to encode unlearning
information in visual tokens of low importance for relevant queries. We find
that the transformed visual tokens not only prevent leakage but also retain
reference model behavior. Experimental results show that our method performs
better or on par with finetuning-based baselines without altering the
pre-trained model parameters or incurring retraining costs, making it a
practical and effective unlearning solution for VLMs.

</details>


### [43] [MaizeStandCounting (MaSC): Automated and Accurate Maize Stand Counting from UAV Imagery Using Image Processing and Deep Learning](https://arxiv.org/abs/2510.07580)
*Dewi Endah Kharismawati,Toni Kazic*

Main category: cs.CV

> 提出了一种基于RGB影像的玉米苗自动化计数算法MaSC，该算法既能在马赛克影像分割块中进行也能在原始视频帧中处理，并显示出与手工计数相比的良好一致性。

<details>
  <summary>Details</summary>

**Motivation:** 精确的玉米株数统计是作物管理和研究的关键，能够优化种植密度、预测产量和早期检测发芽问题。然而，人工计数费时费力且容易出错，尤其是在广阔的农田中。因此，开发一种高效、准确的自动化方法至关重要。

**Method:** 两步法：首先是通过轻量级的YOLOv9模型在V2-V10生长期从RGB影像中检测玉米苗；其次是基于检测到的玉米苗的空间分布执行行和范围分割，以进行精确的行数统计。该算法可以从马赛克影像的分割块中或通过同源矩阵校准的原始视频帧中处理数据。

**Result:** 在2024年夏季试验田内进行的实地手动计数对比评测显示，MaSC表现优良（马赛克影像的相关系数$R^2 = 0.616$，原始帧的相关系数$R^2 = 0.906$）。此外，MaSC以60.63秒处理了83张全分辨率的影像，证明了其实时操作的潜力。

**Conclusion:** MaSC展示了其作为低成本、高精度工具在农业研究和生产环境中自动化玉米苗数统计的有效性，尤其适用于大面积农田中的规模操作。

**Abstract:** Accurate maize stand counts are essential for crop management and research,
informing yield prediction, planting density optimization, and early detection
of germination issues. Manual counting is labor-intensive, slow, and
error-prone, especially across large or variable fields. We present
MaizeStandCounting (MaSC), a robust algorithm for automated maize seedling
stand counting from RGB imagery captured by low-cost UAVs and processed on
affordable hardware. MaSC operates in two modes: (1) mosaic images divided into
patches, and (2) raw video frames aligned using homography matrices. Both modes
use a lightweight YOLOv9 model trained to detect maize seedlings from V2-V10
growth stages. MaSC distinguishes maize from weeds and other vegetation, then
performs row and range segmentation based on the spatial distribution of
detections to produce precise row-wise stand counts. Evaluation against
in-field manual counts from our 2024 summer nursery showed strong agreement
with ground truth (R^2= 0.616 for mosaics, R^2 = 0.906 for raw frames). MaSC
processed 83 full-resolution frames in 60.63 s, including inference and
post-processing, highlighting its potential for real-time operation. These
results demonstrate MaSC's effectiveness as a scalable, low-cost, and accurate
tool for automated maize stand counting in both research and production
environments.

</details>


### [44] [Quick-CapsNet (QCN): A fast alternative to Capsule Networks](https://arxiv.org/abs/2510.07600)
*Pouya Shiri,Ramin Sharifi,Amirali Baniasadi*

Main category: cs.CV

> The paper introduces Quick-CapsNet (QCN) as a faster alternative to CapsNet, offering 5x speedup during inference at the cost of slight accuracy reduction, enhancing it further with a more powerful decoder.

<details>
  <summary>Details</summary>

**Motivation:** To address CapsNet's slow training and testing issues for fast real-time applications by introducing Quick-CapsNet (QCN).

**Method:** QCN builds on producing a fewer number of capsules, resulting in a faster network, and employs a more powerful decoder to further improve performance.

**Result:** QCN is 5x faster on MNIST, F-MNIST, SVHN and Cifar-10 datasets with marginal loss in accuracy.

**Conclusion:** QCN offers a faster alternative to CapsNet for real-time applications, achieving a balance between speed and accuracy.

**Abstract:** The basic computational unit in Capsule Network (CapsNet) is a capsule (vs.
neurons in Convolutional Neural Networks (CNNs)). A capsule is a set of
neurons, which form a vector. CapsNet is used for supervised classification of
data and has achieved state-of-the-art accuracy on MNIST digit recognition
dataset, outperforming conventional CNNs in detecting overlapping digits.
Moreover, CapsNet shows higher robustness towards affine transformation when
compared to CNNs for MNIST datasets. One of the drawbacks of CapsNet, however,
is slow training and testing. This can be a bottleneck for applications that
require a fast network, especially during inference. In this work, we introduce
Quick-CapsNet (QCN) as a fast alternative to CapsNet, which can be a starting
point to develop CapsNet for fast real-time applications. QCN builds on
producing a fewer number of capsules, which results in a faster network. QCN
achieves this at the cost of marginal loss in accuracy. Inference is 5x faster
on MNIST, F-MNIST, SVHN and Cifar-10 datasets. We also further enhanced QCN by
employing a more powerful decoder instead of the default decoder to further
improve QCN.

</details>


### [45] [Rectified-CFG++ for Flow Based Models](https://arxiv.org/abs/2510.07631)
*Shreshth Saini,Shashank Gupta,Alan C. Bovik*

Main category: cs.CV

> Rectified-CFG++ is an adaptive method that addresses stability and quality issues in rectified flow models for text-to-image synthesis, outperforming standard CFG.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to tackle the off-manifold drift and visual artifacts caused by the standard CFG when applied to rectified flow models in text-to-image tasks.

**Method:** Structure

**Result:** {
  "tldr": "Rectified-CFG++ is an adaptive guidance method for rectified flow models that combats drift and visual artifacts, demonstrating improved performance over standard CFG on text-to-image models.",
  "motivation": "The primary motive of this paper is to address the severe off-manifold drift and visual artifacts encountered with the standard classifier-free guidance when applied to rectified flow models in text-conditioned synthesis scenarios.",
  "method": "The paper introduces a method named Rectified-CFG++, which is designed to couple the efficiency of rectified flows with a conditioning rule sensitive to geometric details, achieving marginally consistent results and maintaining a stable trajectory closer to the data manifold.",
  "result": "Experiments show that Rectified-CFG++ surpasses standard CFG in terms of performance, as evidenced by tests on large-scale text-to-image models like Flux, Stable Diffusion 3/3.5, and Lumina, outperforming on various datasets.",
  "conclusion": "The paper concludes that the proposed Rectified-CFG++ method can effectively mitigate the issues of severe drift and artifacts present in rectified flow models, significantly enhancing text-to-image synthesis stability and quality across different guidance strengths. The verification through extensive tests on large models and datasets confirms the robustness and efficiency of Rectified-CFG++ over the standard CFG.",
  "motivation": "To resolve the problems of off-manifold drift and visual artifacts in text-conditioned diffusion models when using classifier-free guidance, especially in rectified flow models. The authors aim to improve these models' stability and quality.",
  "method": "The method introduced, Rectified-CFG++, combines the deterministic efficiency of rectified flows with a geometry-aware conditioning to ensure stable guidance. It performs a conditional RF update followed by a weighted correction that balances conditional and unconditional velocity fields.",
  "result": "Studies on large text-to-image models and various datasets demonstrate that Rectified-CFG++ not only improves the stability of rectified flow models but also yields better outcomes in image generation across a range of conditions compared to standard CFG.",
  "conclusion": "The conclusion highlighted the success of Rectified-CFG++ in achieving stable and high-quality text-to-image synthesis. The evaluation on multiple datasets confirms the effectiveness and superiority of the proposed method over the traditional CFG approach. The methodology ensures that the guidance remains robust and effective across different guidance strengths, supporting a wide range of applications.",
  "toolbench_rv": "Analyzer Success: Delivered insightful analysis of the paper on Rectified-CFG++, including its motivation, methodological approach, results, and conclusions. The generated summary encapsulates the paper’s objective of improving rectified flow models' performance against the drift and artifacts issues encountered with standard CFG, which is critical for enhancing text-to-image synthesis tasks across different models and datasets. The analysis leverages complex concepts to provide a concise yet detailed overview of the paper’s contributions. Note that beyond contextual data summarized within the Abstract, none of the specific numerical data from experimental evaluations has been included directly within this analysis. Such details could be provided with a more exhaustive paper review.",
  "toolbench raided_result": null,
  "toolbench raised_result": null
}

**Conclusion:** Rectified-CFG++ method successfully enhances the stability and visual quality of text-to-image synthesis in rectified flow models, outperforming standard CFG across various conditions and datasets.

**Abstract:** Classifier-free guidance (CFG) is the workhorse for steering large diffusion
models toward text-conditioned targets, yet its native application to rectified
flow (RF) based models provokes severe off-manifold drift, yielding visual
artifacts, text misalignment, and brittle behaviour. We present
Rectified-CFG++, an adaptive predictor-corrector guidance that couples the
deterministic efficiency of rectified flows with a geometry-aware conditioning
rule. Each inference step first executes a conditional RF update that anchors
the sample near the learned transport path, then applies a weighted conditional
correction that interpolates between conditional and unconditional velocity
fields. We prove that the resulting velocity field is marginally consistent and
that its trajectories remain within a bounded tubular neighbourhood of the data
manifold, ensuring stability across a wide range of guidance strengths.
Extensive experiments on large-scale text-to-image models (Flux, Stable
Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms
standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and
T2I-CompBench. Project page: https://rectified-cfgpp.github.io/

</details>


### [46] [PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment](https://arxiv.org/abs/2510.07636)
*Shashank Gupta,Gregoire Phillips,Alan C. Bovik*

Main category: cs.CV

> This paper introduces PIT-QMM, a new model for no-reference point cloud quality assessment using a multimodal approach, which outperforms current methods.

<details>
  <summary>Details</summary>

**Motivation:** To explore the use of large multimodal models in the domain of 3D assets, specifically for point cloud quality assessment without a reference.

**Method:** We construct PIT-QMM, a novel LMM for NR-PCQA that consumes text, images, and point clouds end-to-end to predict quality scores.

**Result:** The method outperforms existing approaches on established benchmarks with fewer training iterations and enables distortion localization and identification.

**Conclusion:** The findings demonstrate the effectiveness of PIT-QMM in NR-PCQA and pave the way for more explainable and interactive models in 3D asset quality assessment.

**Abstract:** Large Multimodal Models (LMMs) have recently enabled considerable advances in
the realm of image and video quality assessment, but this progress has yet to
be fully explored in the domain of 3D assets. We are interested in using these
models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where
the aim is to automatically evaluate the perceptual quality of a point cloud in
absence of a reference. We begin with the observation that different modalities
of data - text descriptions, 2D projections, and 3D point cloud views - provide
complementary information about point cloud quality. We then construct PIT-QMM,
a novel LMM for NR-PCQA that is capable of consuming text, images and point
clouds end-to-end to predict quality scores. Extensive experimentation shows
that our proposed method outperforms the state-of-the-art by significant
margins on popular benchmarks with fewer training iterations. We also
demonstrate that our framework enables distortion localization and
identification, which paves a new way forward for model explainability and
interactivity. Code and datasets are available at
https://www.github.com/shngt/pit-qmm.

</details>


### [47] [Dual-Stream Alignment for Action Segmentation](https://arxiv.org/abs/2510.07652)
*Harshala Gammulle,Clinton Fookes,Sridha Sridharan,Simon Denman*

Main category: cs.CV

> 提出Dual-Stream Alignment Network (DSA Net)，结合帧级和动作级两种流来增强动作分割的性能，实现了在多个数据集上的最佳结果。

<details>
  <summary>Details</summary>

**Motivation:** 动作分割是一个具有挑战性的研究领域，涉及到从连续视频流中识别特定动作的时间和空间位置。最近的研究转向了双流方法，引入了动作级特征来提高动作分割性能。

**Method:** 提出Dual-Stream Alignment Network (DSA Net)，结合帧级和动作级特征流，通过Temporal Context (TC) 块使用交叉注意力和Quantum-based Action-Guided Modulation (Q-ActGM) 融合互补信息。

**Result:** 通过广泛的消融实验展示了各自组件的有效性，DSA Net 在GTEA、Breakfast、50Salads 和EgoProcel 数据集上实现了最先进的性能，显著优于现有方法。

**Conclusion:** 研究表明，在动作分割领域，引入量子-经典混合机器学习框架可以显著提升性能效果，未来可能需要进一步探究利用这种框架进行更多的视觉任务。

**Abstract:** Action segmentation is a challenging yet active research area that involves
identifying when and where specific actions occur in continuous video streams.
Most existing work has focused on single-stream approaches that model the
spatio-temporal aspects of frame sequences. However, recent research has
shifted toward two-stream methods that learn action-wise features to enhance
action segmentation performance. In this work, we propose the Dual-Stream
Alignment Network (DSA Net) and investigate the impact of incorporating a
second stream of learned action features to guide segmentation by capturing
both action and action-transition cues. Communication between the two streams
is facilitated by a Temporal Context (TC) block, which fuses complementary
information using cross-attention and Quantum-based Action-Guided Modulation
(Q-ActGM), enhancing the expressive power of the fused features. To the best of
our knowledge, this is the first study to introduce a hybrid quantum-classical
machine learning framework for action segmentation. Our primary objective is
for the two streams (frame-wise and action-wise) to learn a shared feature
space through feature alignment. This is encouraged by the proposed Dual-Stream
Alignment Loss, which comprises three components: relational consistency,
cross-level contrastive, and cycle-consistency reconstruction losses. Following
prior work, we evaluate DSA Net on several diverse benchmark datasets: GTEA,
Breakfast, 50Salads, and EgoProcel. We further demonstrate the effectiveness of
each component through extensive ablation studies. Notably, DSA Net achieves
state-of-the-art performance, significantly outperforming existing

</details>


### [48] [Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection](https://arxiv.org/abs/2510.07654)
*Yanjie Pan,Qingdong He,Lidong Wang,Bo Peng,Mingmin Chi*

Main category: cs.CV

> We propose OIE, a video virtual try-on method that replaces clothing in the first frame and then uses pose and mask information to generate subsequent frames efficiently.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges of adapting dual-branch architectures to diffusion models built upon the Diffusion Transformer, which requires adding or modifying the backbone network and additional learning for latent space features of garments.

**Method:** Our method, named OIE (Once is Enough), tackles the challenge of adapting dual-branch architectures to Diffusion Transformer-based models for video virtual try-on. It introduces a strategy based on first-frame clothing replacement, using an image-based clothing transfer model to replace the clothing in the first frame. Then, under the content control of the edited first frame, it employs pose and mask information to guide the temporal prior of the video generation model in synthesizing the remaining frames sequentially.

**Result:** Experiments indicate that our approach achieves better parameter efficiency and computational efficiency, while still maintaining leading performance.

**Conclusion:** The conclusion is that our proposed method, OIE, provides superior parameter efficiency and computational efficiency for video virtual try-on tasks, all the while maintaining high performance.

**Abstract:** Video virtual try-on aims to replace the clothing of a person in a video with
a target garment. Current dual-branch architectures have achieved significant
success in diffusion models based on the U-Net; however, adapting them to
diffusion models built upon the Diffusion Transformer remains challenging.
Initially, introducing latent space features from the garment reference branch
requires adding or modifying the backbone network, leading to a large number of
trainable parameters. Subsequently, the latent space features of garments lack
inherent temporal characteristics and thus require additional learning. To
address these challenges, we propose a novel approach, OIE (Once is Enough), a
virtual try-on strategy based on first-frame clothing replacement:
specifically, we employ an image-based clothing transfer model to replace the
clothing in the initial frame, and then, under the content control of the
edited first frame, utilize pose and mask information to guide the temporal
prior of the video generation model in synthesizing the remaining frames
sequentially. Experiments show that our method achieves superior parameter
efficiency and computational efficiency while still maintaining leading
performance under these constraints.

</details>


### [49] [MONKEY: Masking ON KEY-Value Activation Adapter for Personalization](https://arxiv.org/abs/2510.07656)
*James Baker*

Main category: cs.CV

> 通过在个性化扩散模型中使用自动生成的掩码来优化图像生成过程，确保生成的新图像既能充分体现给定主题，又能够与文本提示相匹配，特别是在描述地点和场所时表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 解决个性化扩散模型仅重复生成主题图像而不考虑文本提示的问题，提高图像生成时的多样性和创造性。

**Method:** 利用IP-Adapter自动生成的掩码，在二次生成中限制图像令牌仅处理主题部分，文本提示可以进一步控制图像的其他部分。

**Result:** 与几种其他测试时间个性化方法相比，该方法显示出更高的提示和源图像一致性。

**Conclusion:** 通过提出的二次生成技术，可以提高个性化扩散模型生成图像的质量和创意性，特别是在确保与文本提示的匹配度方面。

**Abstract:** Personalizing diffusion models allows users to generate new images that
incorporate a given subject, allowing more control than a text prompt. These
models often suffer somewhat when they end up just recreating the subject
image, and ignoring the text prompt. We observe that one popular method for
personalization, the IP-Adapter automatically generates masks that we
definitively segment the subject from the background during inference. We
propose to use this automatically generated mask on a second pass to mask the
image tokens, thus restricting them to the subject, not the background,
allowing the text prompt to attend to the rest of the image. For text prompts
describing locations and places, this produces images that accurately depict
the subject while definitively matching the prompt. We compare our method to a
few other test time personalization methods, and find our method displays high
prompt and source image alignment.

</details>


### [50] [Automatic Text Box Placement for Supporting Typographic Design](https://arxiv.org/abs/2510.07665)
*Jun Muraoka,Daichi Haraguchi,Naoto Inoue,Wataru Shimoda,Kota Yamaguchi,Seiichi Uchida*

Main category: cs.CV

> This study evaluates various models for automated text box placement in incomplete layouts, finding that standard Transformer-based models generally perform better than VLM-based approaches, though all methods face challenges with very small text and densely populated layouts.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is rooted in the importance of balancing visual appeal and communication efficiency in layout design, particularly for advertisements and web pages, and the need for effective automated text box placement solutions in incomplete layouts.

**Method:** This study compares the performance of a standard Transformer-based method, a small Vision and Language Model (Phi3.5-vision), a large pretrained VLM (Gemini), and an extended Transformer that processes multiple images for automated text box placement in incomplete layouts.

**Result:** Evaluations on the Crello dataset indicate that the standard Transformer-based models generally outperform the VLM-based methods, especially when incorporating richer appearance information. However, the methods struggle with very small text or densely populated layouts.

**Conclusion:** The findings underscore the value of designing task-specific architectures and point to potential areas for enhancing automated layout design.

**Abstract:** In layout design for advertisements and web pages, balancing visual appeal
and communication efficiency is crucial. This study examines automated text box
placement in incomplete layouts, comparing a standard Transformer-based method,
a small Vision and Language Model (Phi3.5-vision), a large pretrained VLM
(Gemini), and an extended Transformer that processes multiple images.
Evaluations on the Crello dataset show the standard Transformer-based models
generally outperform VLM-based approaches, particularly when incorporating
richer appearance information. However, all methods face challenges with very
small text or densely populated layouts. These findings highlight the benefits
of task-specific architectures and suggest avenues for further improvement in
automated layout design.

</details>


### [51] [TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration](https://arxiv.org/abs/2510.07666)
*Heming Wu,Di Wang,Tai Ma,Peng Zhao,Yubin Xiao,Zhongke Wu,Xing-Ce Wang,Chuang Li,Xuan Wu,You Zhou*

Main category: cs.CV

> 本文提出了TCIP模型，通过引入FERM模块和TCI策略，解决了现有金字塔网络模型的几个问题，包括误对齐累积和自适应调整迭代次数。实验显示该模型在精度上超越当前最佳方法，并保持高效和紧凑。

<details>
  <summary>Details</summary>

**Motivation:** 金字塔网络在可变形医学图像配准中表现出色，但其解码架构本质上容易传播和累积解剖结构错位。此外，大多数现有模型没有自适应地确定优化迭代次数，这导致了过早终止或过多的迭代次数，影响配准精度。

**Method:** 本文提出了一种名为特征增强残差模块（FERM）的新组件，用于金字塔网络中每个解码层的核心。FERM包括三个连续的模块，分别用于提取解剖语义特征、抑制无关特征以及估计最终的形变场。此外，为自适应地确定不同图像的迭代次数，本文提出了双重阶段的阈值控制迭代策略（TCI）。在第一阶段，TCI评估注册稳定性，确认稳定后，继续第二阶段进行收敛评估。将FERM和TCI结合的模型称为阈值控制迭代金字塔（TCIP）

**Result:** 在三个公开的脑MRI数据集和一个腹部CT数据集上的大量实验表明，TCIP在精度上优于最先进的（SOTA）配准网络，同时保持了相当的推理速度和紧凑的模型参数尺寸。

**Conclusion:** 通过集成FERM和TCI，TCIP模型在医学图像配准精度上取得了优异性能。此外，还评估了FERM和TCI的普遍性和有效性，进一步进行了消融研究，验证了这两种方法的效果。

**Abstract:** Although pyramid networks have demonstrated superior performance in
deformable medical image registration, their decoder architectures are
inherently prone to propagating and accumulating anatomical structure
misalignments. Moreover, most existing models do not adaptively determine the
number of iterations for optimization under varying deformation requirements
across images, resulting in either premature termination or excessive
iterations that degrades registration accuracy. To effectively mitigate the
accumulation of anatomical misalignments, we propose the Feature-Enhanced
Residual Module (FERM) as the core component of each decoding layer in the
pyramid network. FERM comprises three sequential blocks that extract anatomical
semantic features, learn to suppress irrelevant features, and estimate the
final deformation field, respectively. To adaptively determine the number of
iterations for varying images, we propose the dual-stage Threshold-Controlled
Iterative (TCI) strategy. In the first stage, TCI assesses registration
stability and with asserted stability, it continues with the second stage to
evaluate convergence. We coin the model that integrates FERM and TCI as
Threshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three
public brain MRI datasets and one abdomen CT dataset demonstrate that TCIP
outperforms the state-of-the-art (SOTA) registration networks in terms of
accuracy, while maintaining comparable inference speed and a compact model
parameter size. Finally, we assess the generalizability of FERM and TCI by
integrating them with existing registration networks and further conduct
ablation studies to validate the effectiveness of these two proposed methods.

</details>


### [52] [Controllable Video Synthesis via Variational Inference](https://arxiv.org/abs/2510.07670)
*Haoyi Duan,Yunzhi Zhang,Yilun Du,Jiajun Wu*

Main category: cs.CV

> 本文开发了一种具有高度可控制性的视频合成方法，该方法能够生成针对特定元素具有高度可控制性的样本，对于未指定的元素则保持多样性。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于现有视频生成模型通常针对固定输入格式进行训练，而许多视频工作流程从中受益于混合粒度用户控制，从精确的4D对象轨迹和相机路径到粗略的文本提示。

**Method:** 我们开发了一种视频合成方法，该方法通过变分推理来逼近一个组合分布，利用多个视频生成骨干网络来共同处理所有任务约束。为了应对优化挑战，我们将问题分解为逐步的KL散度最小化过程，并提出了一种上下文条件因子化技术来减少解空间的模式以规避局部最优解。

**Result:** 实验表明，与先前工作相比，我们的方法在可控制性、多样性和3D一致性方面产生了改进的样本。

**Conclusion:** 通过这种方法，能够生产出具有良好可控制性、多样性和3D一致性的视频样本，从而满足视频工作流中的多种需求。

**Abstract:** Many video workflows benefit from a mixture of user controls with varying
granularity, from exact 4D object trajectories and camera paths to coarse text
prompts, while existing video generative models are typically trained for fixed
input formats. We develop a video synthesis method that addresses this need and
generates samples with high controllability for specified elements while
maintaining diversity for under-specified ones. We cast the task as variational
inference to approximate a composed distribution, leveraging multiple video
generation backbones to account for all task constraints collectively. To
address the optimization challenge, we break down the problem into step-wise KL
divergence minimization over an annealed sequence of distributions, and further
propose a context-conditioned factorization technique that reduces modes in the
solution space to circumvent local optima. Experiments suggest that our method
produces samples with improved controllability, diversity, and 3D consistency
compared to prior works.

</details>


### [53] [Hybrid CNN-BYOL Approach for Fault Detection in Induction Motors Using Thermal Images](https://arxiv.org/abs/2510.07692)
*Tangin Amir Smrity,MD Zahin Muntaqim Hasan Muhammad Kafi,Abu Saleh Musa Miah,Najmul Hassan,Yuichi Okuyama,Nobuyoshi Asai,Taro Suzuki,Jungpil Shin*

Main category: cs.CV

> A hybrid method combining BYOL with CNNs, especially the BYOL-IMNet model, is proposed for early fault detection in IMs, achieving high accuracy and quick inference time.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the early detection of faults in IMs to protect them and extend their lifespan, due to the critical importance of IMs in industries and daily life.

**Method:** This paper integrates BYOL (Bootstrap Your Own Latent) with CNNs (Convolutional Neural Networks), using a thermal dataset that includes various states of IMs (normal, overload, faults), and employs DL models such as ResNet-50, DenseNet-121, VGG16, etc. A new model, BYOL-IMNet, is introduced which is tailored for fault classification.

**Result:** The BYOL-IMNet has achieved 99.89% test accuracy and an inference time of 5.7 ms per image, outperforming other models.

**Conclusion:** The hybrid method using CNN-BYOL shows promising performance for fault detection in IMs, offering robust online monitoring.

**Abstract:** Induction motors (IMs) are indispensable in industrial and daily life, but
they are susceptible to various faults that can lead to overheating, wasted
energy consumption, and service failure. Early detection of faults is essential
to protect the motor and prolong its lifespan. This paper presents a hybrid
method that integrates BYOL with CNNs for classifying thermal images of
induction motors for fault detection. The thermal dataset used in this work
includes different operating states of the motor, such as normal operation,
overload, and faults. We employed multiple deep learning (DL) models for the
BYOL technique, ranging from popular architectures such as ResNet-50,
DenseNet-121, DenseNet-169, EfficientNetB0, VGG16, and MobileNetV2.
Additionally, we introduced a new high-performance yet lightweight CNN model
named BYOL-IMNet, which comprises four custom-designed blocks tailored for
fault classification in thermal images. Our experimental results demonstrate
that the proposed BYOL-IMNet achieves 99.89\% test accuracy and an inference
time of 5.7 ms per image, outperforming state-of-the-art models. This study
highlights the promising performance of the CNN-BYOL hybrid method in enhancing
accuracy for detecting faults in induction motors, offering a robust
methodology for online monitoring in industrial settings.

</details>


### [54] [Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision](https://arxiv.org/abs/2510.07703)
*Xiaoxu Ma,Runhao Li,Zhenyu Weng*

Main category: cs.CV

> 该研究提出了一种名为MLH的新框架，通过相互增强的基于中心和基于成对的分支，在大规模图像检索领域提升了哈希函数的学习。实验结果表明，MLH在多个基准数据集上超越了最先进的哈希方法。

<details>
  <summary>Details</summary>

**Motivation:** 基于成对的方法在学习保存局部相似关系的哈希函数方面效果良好，而基于中心的方法通过更有效地捕捉全局数据分布通常能获得更好的性能。然而，基于中心的方法在建模全局结构方面的优势往往是以充分利用重要的局部相似性信息不足为代价的。为了克服这一局限性，本研究提出了MLH。

**Method:** 提出了一种名为Mutual Learning for Hashing (MLH)的新弱到强框架，该框架通过从较弱的基于成对的方法分支向较强的基于中心的方法分支转移知识来增强基于中心的哈希分支。MLH由两个分支组成：一个强大的基于中心的分支和一个较弱的基于成对的分支。通过迭代的相互学习过程，中心分支利用了成对分支学到的局部相似性线索。此外，受到专家混合范式的启发，我们引入了一种新的哈希专家混合模块，该模块能够实现有效的跨分支交互，进一步增强两个分支的性能。

**Result:** 广泛的实验表明，MLH在多个基准数据集上持续超越最先进的哈希方法。

**Conclusion:** 通过引入相互学习机制和哈希专家混合模块来实现分支间的交互，MLH能够有效提升哈希函数在全球和局部数据分布上的性能，从而在大规模图像检索中取得优越的效果。

**Abstract:** Deep hashing has been widely adopted for large-scale image retrieval, with
numerous strategies proposed to optimize hash function learning. Pairwise-based
methods are effective in learning hash functions that preserve local similarity
relationships, whereas center-based methods typically achieve superior
performance by more effectively capturing global data distributions. However,
the strength of center-based methods in modeling global structures often comes
at the expense of underutilizing important local similarity information. To
address this limitation, we propose Mutual Learning for Hashing (MLH), a novel
weak-to-strong framework that enhances a center-based hashing branch by
transferring knowledge from a weaker pairwise-based branch. MLH consists of two
branches: a strong center-based branch and a weaker pairwise-based branch.
Through an iterative mutual learning process, the center-based branch leverages
local similarity cues learned by the pairwise-based branch. Furthermore,
inspired by the mixture-of-experts paradigm, we introduce a novel
mixture-of-hash-experts module that enables effective cross-branch interaction,
further enhancing the performance of both branches. Extensive experiments
demonstrate that MLH consistently outperforms state-of-the-art hashing methods
across multiple benchmark datasets.

</details>


### [55] [RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning](https://arxiv.org/abs/2510.07721)
*Zipeng Guo,Lichen Ma,Xiaolong Fu,Gaojing Zhou,Lan Yang,Yuchen Zhou,Linkai Liu,Yu He,Ximan Liu,Shiping Dong,Jingling Fu,Zhen Chen,Yu Shi,Junshi Huang,Jason Li,Chao Gou*

Main category: cs.CV

> 我们提出了一种新的强化学习框架（Repainter），用于优化电子商务产品图像中的对象去除，该框架包括空间遮罩轨迹优化和群组相对策略优化，通过调节注意力机制来减少不想要的对象插入。我们通过引入一种复合奖励机制，有效解决了视觉伪影问题，这一方法在处理复杂场景时尤其有效。

<details>
  <summary>Details</summary>

**Motivation:** 产品图像中的水印和其他促销文本是电子商务平台上提供清晰和吸引人的产品视觉的重大障碍。现有的基于扩散的图像修复方法在商业环境中遇到了不可靠的对象去除和领域特定适应性限制等问题。因此，我们提出了解决这些问题的新方法。

**Method:** 我们提出了一种名为Repainter的强化学习框架，该框架结合了空间遮罩轨迹优化和群组相对策略优化（GRPO），并通过调节注意力机制来强调背景上下文，从而生成高奖励样本并减少不想要的对象插入。另外，我们引入了一种复合奖励机制，平衡全局、局部和语义约束，有效减少视觉伪影和奖励作弊。

**Result:** 通过广泛的实验，我们证明Repainter在复杂场景下显著优于最先进的方法。

**Conclusion:** 我们的方法在电子商务产品图像去噪方面取得了显著效果，特别是在图像组成复杂的场景中。

**Abstract:** In web data, product images are central to boosting user engagement and
advertising efficacy on e-commerce platforms, yet the intrusive elements such
as watermarks and promotional text remain major obstacles to delivering clear
and appealing product visuals. Although diffusion-based inpainting methods have
advanced, they still face challenges in commercial settings due to unreliable
object removal and limited domain-specific adaptation. To tackle these
challenges, we propose Repainter, a reinforcement learning framework that
integrates spatial-matting trajectory refinement with Group Relative Policy
Optimization (GRPO). Our approach modulates attention mechanisms to emphasize
background context, generating higher-reward samples and reducing unwanted
object insertion. We also introduce a composite reward mechanism that balances
global, local, and semantic constraints, effectively reducing visual artifacts
and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality,
large-scale e-commerce inpainting dataset, and a standardized benchmark
EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that
Repainter significantly outperforms state-of-the-art methods, especially in
challenging scenes with intricate compositions. We will release our code and
weights upon acceptance.

</details>


### [56] [SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction](https://arxiv.org/abs/2510.07723)
*Wenyue Chen,Peng Li,Wangguandong Zheng,Chengfeng Zhao,Mengfei Li,Yaolong Zhu,Zhiyang Dou,Ronggang Wang,Yuan Liu*

Main category: cs.CV

> 本文提出了一个结合2D多视角和3D原生生成模型的SyncHuman框架，用于单视角图像的高质量3D人体重建，特别是在具有挑战性姿态的条件下。

<details>
  <summary>Details</summary>

**Motivation:** 当前方法在处理复杂人体姿态和精细化细节重建上存在不足，因此本文提出了一种新的框架，以解决3D人体重建的挑战。

**Method:** 本文提出了SyncHuman框架，结合了2D多视角生成模型和3D原生生成模型。2D多视角生成模型擅长捕捉精细的2D细节，但结构一致性较差，而3D原生生成模型生成的3D形状粗略但结构一致。通过引入像素对齐的2D-3D同步注意力机制和细节提升机制，该框架能够从单视角图像中生成高质量的人体网格重建，尤其是在具有挑战性的姿态下。

**Result:** 通过该框架生成的3D人体重建结果不仅在几何准确性上表现良好，而且在视觉保真度上也优于其他基准方法。

**Conclusion:** 实验结果表明，SyncHuman框架在面对图像具有挑战性姿态时，仍能实现鲁棒和逼真的3D人体重建，并在未来3D生成模型的发展方向上展示了潜力。

**Abstract:** Photorealistic 3D full-body human reconstruction from a single image is a
critical yet challenging task for applications in films and video games due to
inherent ambiguities and severe self-occlusions. While recent approaches
leverage SMPL estimation and SMPL-conditioned image generative models to
hallucinate novel views, they suffer from inaccurate 3D priors estimated from
SMPL meshes and have difficulty in handling difficult human poses and
reconstructing fine details. In this paper, we propose SyncHuman, a novel
framework that combines 2D multiview generative model and 3D native generative
model for the first time, enabling high-quality clothed human mesh
reconstruction from single-view images even under challenging human poses.
Multiview generative model excels at capturing fine 2D details but struggles
with structural consistency, whereas 3D native generative model generates
coarse yet structurally consistent 3D shapes. By integrating the complementary
strengths of these two approaches, we develop a more effective generation
framework. Specifically, we first jointly fine-tune the multiview generative
model and the 3D native generative model with proposed pixel-aligned 2D-3D
synchronization attention to produce geometrically aligned 3D shapes and 2D
multiview images. To further improve details, we introduce a feature injection
mechanism that lifts fine details from 2D multiview images onto the aligned 3D
shapes, enabling accurate and high-fidelity reconstruction. Extensive
experiments demonstrate that SyncHuman achieves robust and photo-realistic 3D
human reconstruction, even for images with challenging poses. Our method
outperforms baseline methods in geometric accuracy and visual fidelity,
demonstrating a promising direction for future 3D generation models.

</details>


### [57] [ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes](https://arxiv.org/abs/2510.07729)
*Jian Gao,Mengqi Yuan,Yifei Zeng,Chang Zeng,Zhihao Li,Zhenyu Chen,Weichao Qiu,Xiao-Xiao Long,Hao Zhu,Xun Cao,Yao Yao*

Main category: cs.CV

> 提出了ComGS框架，用于实现实时高质量的三维对象场景组合，解决了高斯渲染中的光照与阴影不一致问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决Gaussian Splatting在组合对象和场景时由于预设光照和阴影导致的不一致问题，并提升效率。

**Method:** 引入Surface Octahedral Probes (SOPs)进行高效的重新光照物体重建和简化光照估计任务，专注于物体放置位置的环境光照完成光照补充。

**Result:** 提出的方法在28 FPS下实现高质量实时渲染，产生视觉和谐的结果，编辑只需36秒。

**Conclusion:** ComGS框架实现了高效率和高性能的三维对象场景组合，可以生成现实中的一致性光照和阴影。

**Abstract:** Gaussian Splatting (GS) enables immersive rendering, but realistic 3D
object-scene composition remains challenging. Baked appearance and shadow
information in GS radiance fields cause inconsistencies when combining objects
and scenes. Addressing this requires relightable object reconstruction and
scene lighting estimation. For relightable object reconstruction, existing
Gaussian-based inverse rendering methods often rely on ray tracing, leading to
low efficiency. We introduce Surface Octahedral Probes (SOPs), which store
lighting and occlusion information and allow efficient 3D querying via
interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x
speedup in reconstruction and enable real-time shadow computation in Gaussian
scenes. For lighting estimation, existing Gaussian-based inverse rendering
methods struggle to model intricate light transport and often fail in complex
scenes, while learning-based methods predict lighting from a single image and
are viewpoint-sensitive. We observe that 3D object-scene composition primarily
concerns the object's appearance and nearby shadows. Thus, we simplify the
challenging task of full scene lighting estimation by focusing on the
environment lighting at the object's placement. Specifically, we capture a 360
degrees reconstructed radiance field of the scene at the location and fine-tune
a diffusion model to complete the lighting. Building on these advances, we
propose ComGS, a novel 3D object-scene composition framework. Our method
achieves high-quality, real-time rendering at around 28 FPS, produces visually
harmonious results with vivid shadows, and requires only 36 seconds for
editing. Code and dataset are available at
https://nju-3dv.github.io/projects/ComGS/.

</details>


### [58] [UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes](https://arxiv.org/abs/2510.07741)
*Yuang Meng,Xin Jin,Lina Lei,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

> 文章提出UltraLED框架，利用单张短曝光RAW图像有效平衡动态范围并增强暗区域细节恢复，从而克服超高清动态范围场景中的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 现有的RGB图像多重曝光方法易于产生对齐错误和鬼影伪影，而RAW图像具有更高的位深度和更可预测的噪声特性，因此利用单张短曝光RAW图像进行超高清动态范围恢复具有潜力。

**Method:** 提出UltraLED框架，该框架包含两个阶段：通过比率图进行曝光校正以平衡动态范围，然后使用亮度感知的RAW降噪器增强暗区域细节恢复。该框架利用单张短曝光RAW图像完成超高清动态范围场景的重建。

**Result:** 实验表明，UltraLED在单帧方法中表现优异。

**Conclusion:** 通过仅使用单张短曝光图像，UltraLED框架可以在动态场景中显著减少鬼影和运动模糊，有效解决超高清动态范围场景重建的问题。相关代码和数据集已公开。

**Abstract:** Ultra-high dynamic range (UHDR) scenes exhibit significant exposure
disparities between bright and dark regions. Such conditions are commonly
encountered in nighttime scenes with light sources. Even with standard exposure
settings, a bimodal intensity distribution with boundary peaks often emerges,
making it difficult to preserve both highlight and shadow details
simultaneously. RGB-based bracketing methods can capture details at both ends
using short-long exposure pairs, but are susceptible to misalignment and
ghosting artifacts. We found that a short-exposure image already retains
sufficient highlight detail. The main challenge of UHDR reconstruction lies in
denoising and recovering information in dark regions. In comparison to the RGB
images, RAW images, thanks to their higher bit depth and more predictable noise
characteristics, offer greater potential for addressing this challenge. This
raises a key question: can we learn to see everything in UHDR scenes using only
a single short-exposure RAW image? In this study, we rely solely on a single
short-exposure frame, which inherently avoids ghosting and motion blur, making
it particularly robust in dynamic scenes. To achieve that, we introduce
UltraLED, a two-stage framework that performs exposure correction via a ratio
map to balance dynamic range, followed by a brightness-aware RAW denoiser to
enhance detail recovery in dark regions. To support this setting, we design a
9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a
corresponding dataset based on diverse scenes, using only the shortest exposure
as input for reconstruction. Extensive experiments show that UltraLED
significantly outperforms existing single-frame approaches. Our code and
dataset are made publicly available at
https://srameo.github.io/projects/ultraled.

</details>


### [59] [DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream](https://arxiv.org/abs/2510.07752)
*Junhao He,Jiaxu Wang,Jia Li,Mingyuan Sun,Qiang Zhang,Jiahang Cao,Ziyi Zhang,Yi Gu,Jingkai Sun,Renjing Xu*

Main category: cs.CV

> 本文提出了一个联合优化RGB和事件数据的框架来改进动态3D高斯喷射的重建。

<details>
  <summary>Details</summary>

**Motivation:** 由于事件摄像头可以异步捕获快速视觉变化且对运动模糊有较强鲁棒性，但无法提供颜色信息，本文旨在通过结合低时间分辨率的RGB图像和高帧率事件流解决动态3DGS重建的挑战，即如何在两个数据模态间显著差异的情况下进行联合优化。

**Method:** 本文提出了一种新的框架，用于从RGB和事件摄像头数据中联合优化动态3D高斯喷射重建。该框架首先通过提出的LoCM无监督微调框架为未见过的场景调整事件流估计器，以提取事件流中的运动先验。然后，通过几何感知数据关联方法建立事件-高斯运动对应关系，并通过运动分解和帧间伪标签两种策略辅助该对应关系的构建。

**Result:** 实验结果表明，本文的方法在合成和真实场景中均优于现有的基于图像和事件的方法，能够更有效地优化动态3D高斯喷射。

**Conclusion:** 实验结果证明，该方法可以有效地利用事件数据优化动态3DGS，并在合成和真实场景中优于现有的基于图像和事件的方法。

**Abstract:** Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB
videos is challenging. This is because large inter-frame motions will increase
the uncertainty of the solution space. For example, one pixel in the first
frame might have more choices to reach the corresponding pixel in the second
frame. Event cameras can asynchronously capture rapid visual changes and are
robust to motion blur, but they do not provide color information. Intuitively,
the event stream can provide deterministic constraints for the inter-frame
large motion by the event trajectories. Hence, combining
low-temporal-resolution images with high-framerate event streams can address
this challenge. However, it is challenging to jointly optimize Dynamic 3DGS
using both RGB and event modalities due to the significant discrepancy between
these two data modalities. This paper introduces a novel framework that jointly
optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event
motion priors to guide the optimization of the deformation fields. First, we
extract the motion priors encoded in event streams by using the proposed LoCM
unsupervised fine-tuning framework to adapt an event flow estimator to a
certain unseen scene. Then, we present the geometry-aware data association
method to build the event-Gaussian motion correspondence, which is the primary
foundation of the pipeline, accompanied by two useful strategies, namely motion
decomposition and inter-frame pseudo-label. Extensive experiments show that our
method outperforms existing image and event-based approaches across synthetic
and real scenes and prove that our method can effectively optimize dynamic 3DGS
with the help of event data.

</details>


### [60] [Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis](https://arxiv.org/abs/2510.07785)
*Ming Jie Ong,Sze Yinn Ung,Sim Kuan Goh,Jimmy Y. Zhong*

Main category: cs.CV

> 研究探讨了在MRI图像中利用可解释的人工智能技术提高脑肿瘤分割的准确性，并通过UNet模型及其变体（ResUNet和AttUNet）的应用，结合XAI技术（Grad-CAM和注意力可视化）提升模型理解和医生信任度。结果表明ResUNet在Dice和Jaccard相似性得分以及准确率、召回率和F1得分上表现最佳，建议在未来临床应用中使用。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是利用XAI技术提高脑肿瘤MRI图像分割的准确性，为临床决策提供帮助。

**Method:** 使用了UNet、残差UNet(ResUNet)和注意力UNet(AttUNet)三种深度学习模型进行评估，并通过Grad-CAM和注意力可视化技术对模型决策进行解释。

**Result:** ResUNet在Dice和Jaccard相似性得分以及准确率、召回率和F1得分上优于其他模型。

**Conclusion:** 结论指出ResUNet是最佳性能模型，建议在未来的临床评估中使用。

**Abstract:** The current study investigated the use of Explainable Artificial Intelligence
(XAI) to improve the accuracy of brain tumor segmentation in MRI images, with
the goal of assisting physicians in clinical decision-making. The study focused
on applying UNet models for brain tumor segmentation and using the XAI
techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and
attention-based visualization to enhance the understanding of these models.
Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet
(AttUNet) - were evaluated to identify the best-performing model. XAI was
employed with the aims of clarifying model decisions and increasing physicians'
trust in these models. We compared the performance of two UNet variants
(ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors
from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM
and attention-based visualization. Using the latest computer hardware, we
trained and validated each model using the Adam optimizer and assessed their
performance with respect to: (i) training, validation, and inference times,
(ii) segmentation similarity coefficients and loss functions, and (iii)
classification performance. Notably, during the final testing phase, ResUNet
outperformed the other models with respect to Dice and Jaccard similarity
scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided
visuospatial insights into the tumor subregions each UNet model focused on
while attention-based visualization provided valuable insights into the working
mechanisms of AttUNet's attention modules. These results demonstrated ResUNet
as the best-performing model and we conclude by recommending its use for
automated brain tumor segmentation in future clinical assessments. Our source
code and checkpoint are available at
https://github.com/ethanong98/MultiModel-XAI-Brats2020

</details>


### [61] [GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.07791)
*Qinghongbing Xie,Zhaoyuan Xia,Feng Zhu,Lijun Gong,Ziyue Li,Rui Zhao,Long Zeng*

Main category: cs.CV

> 本文推出了Geo-Temporal Reasoning基准（GTR-Bench），用于评估视觉语言模型在大规模摄像网络中的地理时空推理能力，结果显示现有模型在这方面显著落后于人类的表现，并揭示了模型在时空推理上的三大不足。

<details>
  <summary>Details</summary>

**Motivation:** 现有时空基准测试侧重于图像或视频背景下基于第一人称或地理视角的推理，缺乏对地图和视频背景综合时空推理能力的评估。为此，本文提出了GTR-Bench来填补这一研究空白，提升对大规模摄像网络中地理时空推理能力的理解。

**Method:** GTR-Bench设计了全新的挑战来测试移动目标在大规模摄像网络中的地理时空推理，包括在地图和视频之间进行多视点切换、对非重叠视野多个视频的联合推理以及对未被任何视频所观察到的空间时间区域进行推理。

**Result:** 对超过10种流行的视觉语言模型进行了评估，即使是最先进的模型，其性能也远低于人类。研究还揭示了现有模型在地理时空推理上的三大主要缺陷。

**Conclusion:** GTR-Bench在评估视觉语言模型的地理时空推理方面提供了有价值的见解，为该领域的研究和应用开辟了新机会。基准和代码即将公开。

**Abstract:** Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has
attracted much attention due to its importance for Autonomous Driving, Embodied
AI and General Artificial Intelligence. Existing spatial-temporal benchmarks
mainly focus on egocentric perspective reasoning with images/video context, or
geographic perspective reasoning with graphics context (eg. a map), thus fail
to assess VLMs' geographic spatial-temporal intelligence with both images/video
and graphics context, which is important for areas like traffic management and
emergency response. To address the gaps, we introduce Geo-Temporal Reasoning
benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of
moving targets in a large-scale camera network. GTR-Bench is more challenging
as it requires multiple perspective switches between maps and videos, joint
reasoning across multiple videos with non-overlapping fields of view, and
inference over spatial-temporal regions that are unobserved by any video
context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that
even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags
behind human performance (78.61%) on geo-temporal reasoning. Moreover, our
comprehensive analysis on GTR-Bench reveals three primary deficiencies of
current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by
an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in
temporal forecasting, which leads to worse performance on temporal-emphasized
tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to
comprehend or align the map data with multi-view video inputs. We believe
GTR-Bench offers valuable insights and opens up new opportunities for research
and applications in spatial-temporal intelligence. Benchmark and code will be
released at https://github.com/X-Luffy/GTR-Bench.

</details>


### [62] [FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition](https://arxiv.org/abs/2510.07810)
*Luu Tu Nguyen,Vu Tram Anh Khuong,Thi Bich Phuong Man,Thi Duyen Ngo,Thanh Ha Le*

Main category: cs.CV

> 研究提出了一种新的微表情全面运动表示MM-COF和FMANet神经网络，该方法整合了运动动态的双阶段分析，实验结果表明该方法优于现有技术，具有推动微表情识别进步的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的光学流计算方法主要集中在微表情发生的起始和巅峰帧之间，忽略了巅峰到结束阶段的重要运动信息，为了解决这个问题并提高微表情识别的能力。

**Method:** 此研究提出了命名为Magnitude-Modulated Combined Optical Flow (MM-COF) 的全面运动表示，该方法将微表情阶段的运动动态整合到统一描述符中，适用于识别网络。基础上，提出了一种名为FMANet的新端到端神经网络架构，该架构将双阶段分析和幅度调制内化为可学习模块，允许网络自适应地融合运动线索并对显着面部区域进行分类。

**Result:** 实验评价在MMEW、SMIC、CASME-II和SAMM数据集上进行，显示所提出的MM-COF表示和FMANet优于现有方法。

**Conclusion:** 研究突显了可学习的双阶段框架在推进微表情识别方面的潜力。

**Abstract:** Facial micro-expressions, characterized by their subtle and brief nature, are
valuable indicators of genuine emotions. Despite their significance in
psychology, security, and behavioral analysis, micro-expression recognition
remains challenging due to the difficulty of capturing subtle facial movements.
Optical flow has been widely employed as an input modality for this task due to
its effectiveness. However, most existing methods compute optical flow only
between the onset and apex frames, thereby overlooking essential motion
information in the apex-to-offset phase. To address this limitation, we first
introduce a comprehensive motion representation, termed Magnitude-Modulated
Combined Optical Flow (MM-COF), which integrates motion dynamics from both
micro-expression phases into a unified descriptor suitable for direct use in
recognition networks. Building upon this principle, we then propose FMANet, a
novel end-to-end neural network architecture that internalizes the dual-phase
analysis and magnitude modulation into learnable modules. This allows the
network to adaptively fuse motion cues and focus on salient facial regions for
classification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM
datasets, widely recognized as standard benchmarks, demonstrate that our
proposed MM-COF representation and FMANet outperforms existing methods,
underscoring the potential of a learnable, dual-phase framework in advancing
micro-expression recognition.

</details>


### [63] [An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images](https://arxiv.org/abs/2510.07817)
*Kanglin Ning,Ruzhao Chen,Penghong Wang,Xingtao Wang,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.CV

> 本文提出了一种基于房间几何约束的深度估计框架，解决了现有方法中存在的过度平滑和噪声敏感问题，并在多个数据集上展现了优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 目前的方法在重视像素级精度的同时导致房间角落过度平滑和噪声敏感的问题，本文旨在通过结合房间几何信息来改善深度预测。

**Method:** 我们的框架包括一个共享特征编码器，随后是针对布局估计、深度估计和背景分割的特定任务解码器。编码器提取多尺度特征，分别由各解码器处理以生成初始预测：深度图、房间布局图和背景分割图。框架还引入了基于房间几何的背景深度解决策略和背景分割引导的融合机制。

**Result:** 实验结果显示，在Stanford2D3D、Matterport3D 和 Structured3D 数据集上，所提出的方法显著优于现有的开源方法。

**Conclusion:** 基于房间几何约束的深度估计改进了室内全景图的深度预测，未来可以应用于多种视觉应用中。

**Abstract:** Predicting spherical pixel depth from monocular $360^{\circ}$ indoor
panoramas is critical for many vision applications. However, existing methods
focus on pixel-level accuracy, causing oversmoothed room corners and noise
sensitivity. In this paper, we propose a depth estimation framework based on
room geometry constraints, which extracts room geometry information through
layout prediction and integrates those information into the depth estimation
process through background segmentation mechanism. At the model level, our
framework comprises a shared feature encoder followed by task-specific decoders
for layout estimation, depth estimation, and background segmentation. The
shared encoder extracts multi-scale features, which are subsequently processed
by individual decoders to generate initial predictions: a depth map, a room
layout map, and a background segmentation map. Furthermore, our framework
incorporates two strategies: a room geometry-based background depth resolving
strategy and a background-segmentation-guided fusion mechanism. The proposed
room-geometry-based background depth resolving strategy leverages the room
layout and the depth decoder's output to generate the corresponding background
depth map. Then, a background-segmentation-guided fusion strategy derives
fusion weights for the background and coarse depth maps from the segmentation
decoder's predictions. Extensive experimental results on the Stanford2D3D,
Matterport3D and Structured3D datasets show that our proposed methods can
achieve significantly superior performance than current open-source methods.
Our code is available at https://github.com/emiyaning/RGCNet.

</details>


### [64] [Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation](https://arxiv.org/abs/2510.07823)
*Shohei Enomoto*

Main category: cs.CV

> ACAVP enhances Visual Prompting methods by adding affine and color transformations and using TrivialAugment for data augmentation, achieving state-of-the-art accuracy with minimal computational overhead.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the effectiveness of visual prompting methods by addressing their limitations in expressivity and the tendency towards overfitting, while maintaining computational efficiency and compatibility with black-box models.

**Method:** Our proposed ACAVP method introduces affine and color transformations to the VP approach to address limitations in expressivity and overfitting. Additionally, it uses TrivialAugment for data augmentation to improve robustness and accuracy.

**Result:** ACAVP shows significant performance gains, up to 12 percentage points, on various datasets and demonstrates superior accuracy and robustness compared to standard VP methods.

**Conclusion:** ACAVP enhances VP's effectiveness through additional transformations and data augmentation, achieving state-of-the-art results in accuracy and robustness, especially in distribution shifts, with minimal inference overhead.

**Abstract:** Visual prompting (VP) has emerged as a promising parameter-efficient
fine-tuning approach for adapting pre-trained vision models to downstream tasks
without modifying model parameters. Despite offering advantages like negligible
computational overhead and compatibility with black-box models, conventional VP
methods typically achieve lower accuracy than other adaptation approaches. Our
analysis reveals two critical limitations: the restricted expressivity of
simple additive transformation and a tendency toward overfitting when the
parameter count increases. To address these challenges, we propose ACAVP
(Affine, Color, and Additive Visual Prompting), which enhances VP's expressive
power by introducing complementary transformation operations: affine
transformation for creating task-specific prompt regions while preserving
original image information, and color transformation for emphasizing
task-relevant visual features. Additionally, we identify that overfitting is a
critical issue in VP training and introduce TrivialAugment as an effective data
augmentation, which not only benefits our approach but also significantly
improves existing VP methods, with performance gains of up to 12 percentage
points on certain datasets. This demonstrates that appropriate data
augmentation is universally beneficial for VP training. Extensive experiments
across twelve diverse image classification datasets with two different model
architectures demonstrate that ACAVP achieves state-of-the-art accuracy among
VP methods, surpasses linear probing in average accuracy, and exhibits superior
robustness to distribution shifts, all while maintaining minimal computational
overhead during inference.

</details>


### [65] [MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions](https://arxiv.org/abs/2510.07828)
*Kaen Kogashi,Anoop Cherian,Meng-Yu Jennifer Kuo*

Main category: cs.CV

> The authors introduce MMHOI, a large, 3D-annotated dataset of human-object interactions, and MMHOI-Net, a transformer-based model that estimates 3D geometries and actions, providing state-of-the-art results.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitation of current 3D human-object interaction (HOI) benchmarks which do not fully capture complex interactions. The authors aim to provide a large-scale dataset and a model that can comprehensively model multi-human multi-object interactions.

**Method:** Building on MMHOI, the authors present MMHOI-Net, an end-to-end transformer-based neural network for estimating 3D geometries of human-object interactions, their interactions, and associated actions. A structured dual-patch representation is used for modeling objects and their interactions, which is combined with action recognition to improve interaction prediction.

**Result:** Experiments on MMHOI and CORE4D datasets show that the proposed MMHOI-Net achieves state-of-the-art performance, demonstrating superior accuracy and reconstruction quality in multi-human multi-object interaction modeling.

**Conclusion:** The proposed MMHOI dataset and the MMHOI-Net model significantly advance the field of multi-human multi-object interaction research by providing a more comprehensive dataset and achieving superior performance in modeling these interactions.

**Abstract:** Real-world scenes often feature multiple humans interacting with multiple
objects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D
human-object interaction (HOI) benchmarks consider only a fraction of these
complex interactions. To close this gap, we present MMHOI -- a large-scale,
Multi-human Multi-object Interaction dataset consisting of images from 12
everyday scenarios. MMHOI offers complete 3D shape and pose annotations for
every person and object, along with labels for 78 action categories and 14
interaction-specific body parts, providing a comprehensive testbed for
next-generation HOI research. Building on MMHOI, we present MMHOI-Net, an
end-to-end transformer-based neural network for jointly estimating human-object
3D geometries, their interactions, and associated actions. A key innovation in
our framework is a structured dual-patch representation for modeling objects
and their interactions, combined with action recognition to enhance the
interaction prediction. Experiments on MMHOI and the recently proposed CORE4D
datasets demonstrate that our approach achieves state-of-the-art performance in
multi-HOI modeling, excelling in both accuracy and reconstruction quality.

</details>


### [66] [PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting](https://arxiv.org/abs/2510.07830)
*Houqiang Zhong,Zhenglong Wu,Sihua Fu,Zihan Zheng,Xin Jin,Xiaoyun Zhang,Li Song,Qiang Hu*

Main category: cs.CV

> PrismGS通过引入基于物理的正则化框架解决了3D高斯点在渲染高分辨率城市环境时的问题，通过分层多尺度监督和显式尺寸正则化改善了渲染效果，减少视觉瑕疵。

<details>
  <summary>Details</summary>

**Motivation:** 目前的3D高斯点渲染存在严重的闪烁纹理和锯齿状边缘问题，特别是在高分辨率渲染（如4K）中。这些问题是由高斯基元和多尺度城市几何之间的不匹配引起的。现有的“分治”管道无法解决这一保真度差距。

**Method:** PrismGS提出了一个基于物理的正则化框架，以改善3D高斯点的内在渲染行为。PrismGS集成了两个协同的正则化器：分层多尺度监督和显式尺寸正则化。分层多尺度监督通过与预过滤图像金字塔的监督来强制渲染的一致性，从而消除闪烁的纹理。显式尺寸正则化从物理上确保了3D高斯点的尺寸下限，防止出现退化的视图依赖原始点，减少锯齿状边缘。

**Result:** 实验显示，PrismGS在MatrixCity、Mill-19和UrbanScene3D数据集上表现出业界领先的能力，与CityGaussian相比，PrismGS在4K渲染下提供了约1.5dB的PSNR增益，同时保持了出色的稳定性和质量。

**Conclusion:** PrismGS是一个即插即用且与现有渲染管线兼容的方法，能显著减轻3D高斯点渲染中的闪烁纹理和锯齿边现象，尤其适用于高分辨率渲染。

**Abstract:** 3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic
rendering in compact scenes, but scaling to large urban environments introduces
severe aliasing artifacts and optimization instability, especially under
high-resolution (e.g., 4K) rendering. These artifacts, manifesting as
flickering textures and jagged edges, arise from the mismatch between Gaussian
primitives and the multi-scale nature of urban geometry. While existing
``divide-and-conquer'' pipelines address scalability, they fail to resolve this
fidelity gap. In this paper, we propose PrismGS, a physically-grounded
regularization framework that improves the intrinsic rendering behavior of 3D
Gaussians. PrismGS integrates two synergistic regularizers. The first is
pyramidal multi-scale supervision, which enforces consistency by supervising
the rendering against a pre-filtered image pyramid. This compels the model to
learn an inherently anti-aliased representation that remains coherent across
different viewing scales, directly mitigating flickering textures. This is
complemented by an explicit size regularization that imposes a
physically-grounded lower bound on the dimensions of the 3D Gaussians. This
prevents the formation of degenerate, view-dependent primitives, leading to
more stable and plausible geometric surfaces and reducing jagged edges. Our
method is plug-and-play and compatible with existing pipelines. Extensive
experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS
achieves state-of-the-art performance, yielding significant PSNR gains around
1.5 dB against CityGaussian, while maintaining its superior quality and
robustness under demanding 4K rendering.

</details>


### [67] [IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text Intermediaries](https://arxiv.org/abs/2510.07837)
*Harsh Kavediya,Vighnesh Nayak,Bheeshm Sharma,Balamurugan Palaniappan*

Main category: cs.CV

> 本文介绍了一种新的方法——IsoSignVid2Aud，用于将孤立的签语视频序列直接转换为语音，不需中间文本表示，有效减少了多阶段翻译系统的延迟和级联错误，同时产生音质良好的语音输出。

<details>
  <summary>Details</summary>

**Motivation:** 目的是连接聋哑人士和其他人，特别是在教育应用和签语提示界面中的应用，这类视频能够提供即时沟通的好处。

**Method:** 提出IsoSignVid2Aud，这是一个端到端框架，它利用I3D特征提取模块、专门的特征转换网络和音频生成管道，结合新提出的非极大值抑制（NMS）算法，用于在非语法连续签语序列中的时间检测。

**Result:** 实验结果表明，该方法在ASL-Citizen-1500和WLASL-100数据集上分别达到了72.01%和78.67%的Top-1准确率，PESQ和STOI指标分别为2.67和0.73，表明输出的语音具有可理解性。

**Conclusion:** IsoSignVid2Aud这一端到端框架实现了从孤立签语视频到语音的直接转换，能够提供即时有效的沟通同时避免译码过程中的延迟和错误。

**Abstract:** Sign language to spoken language audio translation is important to connect
the hearing- and speech-challenged humans with others. We consider sign
language videos with isolated sign sequences rather than continuous grammatical
signing. Such videos are useful in educational applications and sign prompt
interfaces. Towards this, we propose IsoSignVid2Aud, a novel end-to-end
framework that translates sign language videos with a sequence of possibly
non-grammatic continuous signs to speech without requiring intermediate text
representation, providing immediate communication benefits while avoiding the
latency and cascading errors inherent in multi-stage translation systems. Our
approach combines an I3D-based feature extraction module with a specialized
feature transformation network and an audio generation pipeline, utilizing a
novel Non-Maximal Suppression (NMS) algorithm for the temporal detection of
signs in non-grammatic continuous sequences. Experimental results demonstrate
competitive performance on ASL-Citizen-1500 and WLASL-100 datasets with Top-1
accuracies of 72.01\% and 78.67\%, respectively, and audio quality metrics
(PESQ: 2.67, STOI: 0.73) indicating intelligible speech output. Code is
available at: https://github.com/BheeshmSharma/IsoSignVid2Aud_AIMLsystems-2025.

</details>


### [68] [AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views](https://arxiv.org/abs/2510.07839)
*Yijie Gao,Houqiang Zhong,Tianchi Zhu,Zhengxue Cheng,Qiang Hu,Li Song*

Main category: cs.CV

> AlignGS is a novel framework that aims to improve 3D reconstruction by integrating semantic understanding proactively throughout the reconstruction process from sparse views.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the difficulty of creating semantically rich 3D models from limited views due to geometric ambiguity. Existing methods usually consider semantics as a passive feature, but this paper suggests semantics should actively guide the reconstruction process for better results.

**Method:** The paper introduces AlignGS, a framework that simultaneously optimizes geometry and semantics for 3D reconstruction from sparse views. It uses semantic priors derived from 2D foundation models to guide the 3D reconstruction process through several mechanisms such as depth consistency and normal regularization.

**Result:** The method demonstrates state-of-the-art performance in novel view synthesis and geometric accuracy, showing that leveraging semantic priors as geometric regularizers leads to more coherent 3D models from limited input views.

**Conclusion:** The research underlines the importance of using semantics as an active guiding force for 3D reconstruction, leading to superior models compared to traditional methods that use semantics passively.

**Abstract:** The demand for semantically rich 3D models of indoor scenes is rapidly
growing, driven by applications in augmented reality, virtual reality, and
robotics. However, creating them from sparse views remains a challenge due to
geometric ambiguity. Existing methods often treat semantics as a passive
feature painted on an already-formed, and potentially flawed, geometry. We
posit that for robust sparse-view reconstruction, semantic understanding
instead be an active, guiding force. This paper introduces AlignGS, a novel
framework that actualizes this vision by pioneering a synergistic, end-to-end
optimization of geometry and semantics. Our method distills rich priors from 2D
foundation models and uses them to directly regularize the 3D representation
through a set of novel semantic-to-geometry guidance mechanisms, including
depth consistency and multi-faceted normal regularization. Extensive
evaluations on standard benchmarks demonstrate that our approach achieves
state-of-the-art results in novel view synthesis and produces reconstructions
with superior geometric accuracy. The results validate that leveraging semantic
priors as a geometric regularizer leads to more coherent and complete 3D models
from limited input views. Our code is avaliable at
https://github.com/MediaX-SJTU/AlignGS .

</details>


### [69] [Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials](https://arxiv.org/abs/2510.07853)
*Thomas Lautenschlager,Nils Friederich,Angelo Jovin Yamachui Sitcheu,Katja Nau,Gaëlle Hayot,Thomas Dickmeis,Ralf Mikut*

Main category: cs.CV

> 本文通过自监督学习方法研究了高通量毒性测试的问题，展示了该方法在识别毒物诱导变化的有效性，并探讨了其在物理设备中的应用。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机在于解决高通量毒性测试领域中的关键挑战，并且展示自监督学习方法在识别毒物诱导变化的有效性。

**Method:** 本文采用自监督学习方法来学习表示，以区分不同化合物的作用方式，并使用公开的EmbryoNet数据集作为概念验证。

**Result:** 分析结果表明，通过自监督学习获得的表示能够有效地区分不同化合物的作用方式。

**Conclusion:** 最后，本文讨论了在TOXBOX项目中将机器学习模型集成到物理毒性测试设备中的可能性。

**Abstract:** High-throughput toxicity testing offers a fast and cost-effective way to test
large amounts of compounds. A key component for such systems is the automated
evaluation via machine learning models. In this paper, we address critical
challenges in this domain and demonstrate how representations learned via
self-supervised learning can effectively identify toxicant-induced changes. We
provide a proof-of-concept that utilizes the publicly available EmbryoNet
dataset, which contains ten zebrafish embryo phenotypes elicited by various
chemical compounds targeting different processes in early embryonic
development. Our analysis shows that the learned representations using
self-supervised learning are suitable for effectively distinguishing between
the modes-of-action of different compounds. Finally, we discuss the integration
of machine learning models in a physical toxicity testing device in the context
of the TOXBOX project.

</details>


### [70] [XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method](https://arxiv.org/abs/2510.07856)
*Haochen Yu,Qiankun Liu,Hongyuan Liu,Jianfei Jiang,Juntao Lyu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

> XYZCylinder is proposed to overcome the limitations of generalizing to different camera configurations and reconstructing driving scenes accurately using a cylinder-based approach.

<details>
  <summary>Details</summary>

**Motivation:** Addressing issues with generalization capability and reconstruction accuracy in feedforward reconstruction paradigms, particularly for dynamic driving scenes.

**Method:** A feedforward model named XYZCylinder is introduced, leveraging a unified cylinder lifting method that includes camera modeling (Unified Cylinder Camera Modeling, UCCM) and feature lifting (Cylinder Plane Feature Group, CPFG).

**Result:** XYZCylinder achieves superior performance in driving scene reconstruction across various evaluation settings and can generalize to new scenes without additional training.

**Conclusion:** The proposed method demonstrates high accuracy and strong generalization capability, making it a state-of-the-art method for driving scene reconstruction using a unified cylinder approach.

**Abstract:** Recently, more attention has been paid to feedforward reconstruction
paradigms, which mainly learn a fixed view transformation implicitly and
reconstruct the scene with a single representation. However, their
generalization capability and reconstruction accuracy are still limited while
reconstructing driving scenes, which results from two aspects: (1) The fixed
view transformation fails when the camera configuration changes, limiting the
generalization capability across different driving scenes equipped with
different camera configurations. (2) The small overlapping regions between
sparse views of the $360^\circ$ panorama and the complexity of driving scenes
increase the learning difficulty, reducing the reconstruction accuracy. To
handle these difficulties, we propose \textbf{XYZCylinder}, a feedforward model
based on a unified cylinder lifting method which involves camera modeling and
feature lifting. Specifically, to improve the generalization capability, we
design a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the
learning of viewpoint-dependent spatial correspondence and unifies different
camera configurations with adjustable parameters. To improve the reconstruction
accuracy, we propose a hybrid representation with several dedicated modules
based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image
features to 3D space. Experimental results show that XYZCylinder achieves
state-of-the-art performance under different evaluation settings, and can be
generalized to other driving scenes in a zero-shot manner. Project page:
\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.

</details>
