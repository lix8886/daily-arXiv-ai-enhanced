<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.CV](#cs.CV) [Total: 28]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings](https://arxiv.org/abs/2506.14900)
*Imane Guellil,Salomé Andres,Atul Anand,Bruce Guthrie,Huayu Zhang,Abul Hasan,Honghan Wu,Beatrice Alex*

Main category: cs.CL

> 研究创建了适用于老年人不良事件（AE）提取的手动标注数据集，并使用多个模型进行了评估。研究结果表明，尽管模型在粗粒度提取上表现良好，但在细粒度实体级别上的表现还有待提高。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机是开发一个专门针对老年患者在出院总结中的不良事件进行标注的数据集，因为老年人群体在临床自然语言处理资源中经常被忽视。

**Method:** 本论文介绍了为老年人出院总结中不良事件（AE）提取手动标注的数据集。该数据集包含14种临床意义上的不良事件，并带有上下文属性如否定、诊断类型和住院期间发生情况。注释模式独特地支持不连续和重叠实体，解决了现有研究中很少触及的挑战。

**Result:** 研究使用FlairNLP对多个模型进行评估，发现在三个注释颗粒度级别上的表现有所差异。特别提及变压器模型（如BERT-cased）在粗粒度提取任务上表现出色，但在细粒度和复杂属性上的精细实体级别任务表现较差。

**Conclusion:** 研究结果显示，尽管基于文档级别的粗粒度提取任务上取得了F1分数为0.943的良好表现，但在细粒度实体级别任务上的表现则显著下降，提示现有模型在检测未充分表示的不良事件和捕捉细微的临床语言方面还存在挑战。

**Abstract:** In this work, we present a manually annotated corpus for Adverse Event (AE)
extraction from discharge summaries of elderly patients, a population often
underrepresented in clinical NLP resources. The dataset includes 14 clinically
significant AEs-such as falls, delirium, and intracranial haemorrhage, along
with contextual attributes like negation, diagnosis type, and in-hospital
occurrence. Uniquely, the annotation schema supports both discontinuous and
overlapping entities, addressing challenges rarely tackled in prior work. We
evaluate multiple models using FlairNLP across three annotation granularities:
fine-grained, coarse-grained, and coarse-grained with negation. While
transformer-based models (e.g., BERT-cased) achieve strong performance on
document-level coarse-grained extraction (F1 = 0.943), performance drops
notably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly
for rare events and complex attributes. These results demonstrate that despite
high-level scores, significant challenges remain in detecting underrepresented
AEs and capturing nuanced clinical language. Developed within a Trusted
Research Environment (TRE), the dataset is available upon request via DataLoch
and serves as a robust benchmark for evaluating AE extraction methods and
supporting future cross-dataset generalisation.

</details>


### [2] [Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction](https://arxiv.org/abs/2506.14901)
*Marija Šakota,Robert West*

Main category: cs.CL

> BoostCD combines constrained and unconstrained decoding to improve the performance of structured NLP tasks like closed information extraction without needing retraining for new constraints.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of low-quality outputs during constrained decoding and to improve performance in structured NLP tasks such as closed information extraction without the need for retraining due to dynamic constraints.

**Method:** Boosted Constrained Decoding (BoostCD) is used to combine constrained and unconstrained decoding in two phases to improve the output quality for structured NLP tasks. Phase 1 obtains two weak predictions. Phase 2 uses a learned boosted model to combine these predictions.

**Result:** The model, BoostIE, outperforms previous approaches both in and out-of-distribution for closed information extraction, addressing common errors found in those approaches.

**Conclusion:** BoostCD effectively enhances the output quality and addresses common errors found in previous methods by leveraging the complementary weaknesses in constrained and unconstrained model predictions.

**Abstract:** Many recent approaches to structured NLP tasks use an autoregressive language
model $M$ to map unstructured input text $x$ to output text $y$ representing
structured objects (such as tuples, lists, trees, code, etc.), where the
desired output structure is enforced via constrained decoding. During training,
these approaches do not require the model to be aware of the constraints, which
are merely implicit in the training outputs $y$. This is advantageous as it
allows for dynamic constraints without requiring retraining, but can lead to
low-quality output during constrained decoding at test time. We overcome this
problem with Boosted Constrained Decoding (BoostCD), which combines constrained
and unconstrained decoding in two phases: Phase 1 decodes from the base model
$M$ twice, in constrained and unconstrained mode, obtaining two weak
predictions. In phase 2, a learned autoregressive boosted model combines the
two weak predictions into one final prediction. The mistakes made by the base
model with vs. without constraints tend to be complementary, which the boosted
model learns to exploit for improved performance. We demonstrate the power of
BoostCD by applying it to closed information extraction. Our model, BoostIE,
outperforms prior approaches both in and out of distribution, addressing
several common errors identified in those approaches.

</details>


### [3] [CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision](https://arxiv.org/abs/2506.14912)
*Dyah Adila,Shuai Zhang,Boran Han,Bonan Min,Yuyang Wang*

Main category: cs.CL

> 我们提出了CrEst框架，用于评估大型语言模型（LLM）推理过程中上下文文件的可信度，通过文档间的语义一致性进行自动化评估，并提出了两种集成策略，明显优于基线模型。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法往往忽视了一个关键挑战：上下文文件的可信度差异可能很大，这可能导致不可靠信息的传播。我们的研究旨在提出一种解决方案，以解决这个问题。

**Method:** 我们提出了CrEst，这是一种新颖的弱监督框架，用于在大型语言模型（LLM）推理过程中评估上下文文档的可信度，且不需要手动标注。该方法基于这样的见解，即可信的文件通常与其他可信文件在语义上高度一致，从而可以通过文档间的一致性来自动评估可信度。为了将可信度纳入LLM的推理过程中，我们提出了两种集成策略：一种是黑盒方法，适用于无法访问内部权重或激活值的模型；另一种是白盒方法，直接修改注意力机制。

**Result:** 实验显示，CrEst在三种模型架构和五个数据集上均优于强基线模型，准确率最高提升了26.86%，F1分数提高了3.49%。进一步的分析表明，即使在高噪声条件下，CrEst也能保持稳健的性能。

**Conclusion:** CrEst框架证明了在大型语言模型的推理过程中评估和利用上下文文档的可信度是可行的，并且能显著提高模型在知识密集型任务上的表现。

**Abstract:** The integration of contextual information has significantly enhanced the
performance of large language models (LLMs) on knowledge-intensive tasks.
However, existing methods often overlook a critical challenge: the credibility
of context documents can vary widely, potentially leading to the propagation of
unreliable information. In this paper, we introduce CrEst, a novel weakly
supervised framework for assessing the credibility of context documents during
LLM inference--without requiring manual annotations. Our approach is grounded
in the insight that credible documents tend to exhibit higher semantic
coherence with other credible documents, enabling automated credibility
estimation through inter-document agreement. To incorporate credibility into
LLM inference, we propose two integration strategies: a black-box approach for
models without access to internal weights or activations, and a white-box
method that directly modifies attention mechanisms. Extensive experiments
across three model architectures and five datasets demonstrate that CrEst
consistently outperforms strong baselines, achieving up to a 26.86% improvement
in accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst
maintains robust performance even under high-noise conditions.

</details>


### [4] [MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance](https://arxiv.org/abs/2506.14927)
*Joseph J. Peper,Wenzhao Qiu,Ali Payani,Lu Wang*

Main category: cs.CL

> MDBench, a novel synthetic dataset for evaluating LLMs in multi-document reasoning, is introduced, showing significant challenges for all methods and the effectiveness of a knowledge-guided generation technique.

<details>
  <summary>Details</summary>

**Motivation:** The expansion of LLMs' reasoning capabilities has made new evaluation benchmarks increasingly important, especially for multi-document reasoning, which is both highly relevant and historically challenging to benchmark due to the high cost of annotating long inputs.

**Method:** MDBench, a new dataset for evaluating LLMs on multi-document reasoning, is created through a novel synthetic generation process, using condensed structured seed knowledge modified through LLM-assisted edits and then converted into natural text.

**Result:** The analysis shows that MDBENCH poses significant challenges for all LLM methods, even for relatively short document sets, and the knowledge-guided generation technique allows for targeted analysis of MD-specific reasoning capabilities and quick adaption to new challenges.

**Conclusion:** MDBench addresses the need for rigorous evaluation of LLMs in multi-document reasoning tasks and demonstrates the potential of synthetic generation methods for creating complex and purposeful benchmark datasets.

**Abstract:** Natural language processing evaluation has made significant progress, largely
driven by the proliferation of powerful large language mod-els (LLMs). New
evaluation benchmarks are of increasing priority as the reasoning capabilities
of LLMs are expanding at a rapid pace. In particular, while multi-document (MD)
reasoning is an area of extreme relevance given LLM capabilities in handling
longer-context inputs, few benchmarks exist to rigorously examine model
behavior in this setting. Moreover, the multi-document setting is historically
challenging for benchmark creation due to the expensive cost of annotating long
inputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs
on the task of multi-document reasoning. Notably, MDBench is created through a
novel synthetic generation process, allowing us to controllably and efficiently
generate challenging document sets and the corresponding question-answer (QA)
examples. Our novel technique operates on condensed structured seed knowledge,
modifying it through LLM-assisted edits to induce MD-specific reasoning
challenges. We then convert this structured knowledge into a natural text
surface form, generating a document set and corresponding QA example. We
analyze the behavior of popular LLMs and prompting techniques, finding that
MDBENCH poses significant challenges for all methods, even with relatively
short document sets. We also see our knowledge-guided generation technique (1)
allows us to readily perform targeted analysis of MD-specific reasoning
capabilities and (2) can be adapted quickly to account for new challenges and
future modeling improvements.

</details>


### [5] [From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?](https://arxiv.org/abs/2506.14949)
*Shadman Sakib,Oishy Fatema Akhand,Ajwad Abrar*

Main category: cs.CL

> 本研究通过使用多种提示方法对六个语言模型进行糖尿病预测测试，相较于传统机器学习模型，专有和大型语言模型表现更佳，尤其在少量样本情况下。

<details>
  <summary>Details</summary>

**Motivation:** 尽管机器学习和深度学习模型在糖尿病预测中被广泛应用，但大型语言模型（LLMs）在结构化数值数据上的应用尚未被深入探索。因此，本研究旨在通过实证分析探索LLMs在糖尿病预测任务中的有效性。

**Method:** 本研究测试了大型语言模型（LLMs）在使用零样本、单样本和三样本提示方法预测糖尿病方面的有效性。研究使用了Pima印度糖尿病数据库（PIDD），并对六个LLMs模型进行了评估，包括四个开源模型：Gemma-2-27B，Mistral-7B，Llama-3.1-8B，Llama-3.2-2B，以及两个专有模型：GPT-4o和Gemini Flash 2.0。同时，将其与三种传统机器学习模型（随机森林，逻辑回归和支持向量机(SVM)）的表现进行了比较。实验结果用准确性，精确度，召回率和F1分数作为评估指标。

**Result:** 专有LLMs模型的表现优于开源模型，GPT-4o和Gemma-2-27B在少量样本情况下达到了最高的准确性。值得注意的是，Gemma-2-27B的F1分数也超过了传统的ML模型。不过，研究还指出存在提示策略间性能波动，以及需要领域特定微调的问题。

**Conclusion:** 该研究表明，LLMs可以在医疗预测任务中发挥作用，并鼓励对未来工作在提示工程和混合方法上的研究以改善医疗预测。

**Abstract:** While Machine Learning (ML) and Deep Learning (DL) models have been widely
used for diabetes prediction, the use of Large Language Models (LLMs) for
structured numerical data is still not well explored. In this study, we test
the effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and
three-shot prompting methods. We conduct an empirical analysis using the Pima
Indian Diabetes Database (PIDD). We evaluate six LLMs, including four
open-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We
also test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we
compare their performance with three traditional machine learning models:
Random Forest, Logistic Regression, and Support Vector Machine (SVM). We use
accuracy, precision, recall, and F1-score as evaluation metrics. Our results
show that proprietary LLMs perform better than open-source ones, with GPT-4o
and Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,
Gemma-2-27B also outperforms the traditional ML models in terms of F1-score.
However, there are still issues such as performance variation across prompting
strategies and the need for domain-specific fine-tuning. This study shows that
LLMs can be useful for medical prediction tasks and encourages future work on
prompt engineering and hybrid approaches to improve healthcare predictions.

</details>


### [6] [Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings](https://arxiv.org/abs/2506.15001)
*Ignacio Sastre,Aiala Rosá*

Main category: cs.CL

> 研究发现可以生成一种特殊的嵌入，能让大语言模型精确重建原始文本，而无需修改模型权重，这一现象在不同语言、序列和模型尺寸上进行了验证。

<details>
  <summary>Details</summary>

**Motivation:** 观察到可以生成可逆的句子嵌入，使得大语言模型（LLM）在不改变模型权重的情况下精确重建原始文本的现象，以此揭示LLM的能力并探索其潜在应用。

**Method:** 通过引入特殊的记忆令牌，该令牌的嵌入是在固定序列上训练优化得到的。当模型被提示这个嵌入时，可以准确地重建固定序列。

**Result:** 实验表明，对于英语和西班牙语数据集、最多约240个令牌的序列以及从1亿到80亿参数的模型规模范围内，Llama 3.1 8B能够成功重建所有测试的序列。

**Conclusion:** 这一发现揭示了LLM的有趣能力，并暗示了在基于记忆的检索、压缩和受控文本生成方面潜在的应用。

**Abstract:** In this work, we observe an interesting phenomenon: it is possible to
generate reversible sentence embeddings that allow an LLM to reconstruct the
original text exactly, without modifying the model's weights. This is achieved
by introducing a special memory token, whose embedding is optimized through
training on a fixed sequence. When prompted with this embedding, the model
reconstructs the fixed sequence exactly. We evaluate this phenomenon across
English and Spanish datasets, sequences of up to approximately 240 tokens, and
model scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B
successfully reconstructs all tested sequences. Our findings highlight an
interesting capability of LLMs and suggest potential applications in
memory-based retrieval, compression, and controlled text generation.

</details>


### [7] [Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods](https://arxiv.org/abs/2506.15030)
*Drew Walker,Swati Rajwal,Sudeshna Das,Snigdha Peddireddy,Abeed Sarker*

Main category: cs.CL

> Identified social isolation and loneliness in suicide cases using NLP techniques, with significant predictors including gender, sexual orientation, and marital status, enhancing surveillance and prevention efforts.

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing issue of social isolation and loneliness, and their strong contribution to suicide rates, despite their lack of recording in the US NVDRS structured variables.

**Method:** Using topic modeling for lexicon development and supervised learning classifiers to identify social isolation and loneliness in law enforcement and coroner medical examiner narratives within the NVDRS.

**Result:** Developed high-quality classifiers with an average F1 score of .86 and accuracy of .82. Identified 1,198 cases mentioning chronic social isolation, finding higher odds of classification among men, gay individuals, and those who were divorced.

**Conclusion:** The developed methods can potentially improve the surveillance and prevention efforts against social isolation and loneliness in the United States.

**Abstract:** Social isolation and loneliness, which have been increasing in recent years
strongly contribute toward suicide rates. Although social isolation and
loneliness are not currently recorded within the US National Violent Death
Reporting System's (NVDRS) structured variables, natural language processing
(NLP) techniques can be used to identify these constructs in law enforcement
and coroner medical examiner narratives. Using topic modeling to generate
lexicon development and supervised learning classifiers, we developed
high-quality classifiers (average F1: .86, accuracy: .82). Evaluating over
300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic
social isolation. Decedents had higher odds of chronic social isolation
classification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =
3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).
We found significant predictors for other social isolation topics of recent or
impending divorce, child custody loss, eviction or recent move, and break-up.
Our methods can improve surveillance and prevention of social isolation and
loneliness in the United States.

</details>


### [8] [Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation](https://arxiv.org/abs/2506.15068)
*Zongxia Li,Yapei Chang,Yuhang Zhou,Xiyang Wu,Zichao Liang,Yoo Yeon Sung,Jordan Lee Boyd-Graber*

Main category: cs.CL

> 引入PrefBERT评估开放性长文本生成，优于传统指标，并在多个评估标准中验证了其可靠性和与人类偏好的一致性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估方法经常忽略连贯性、风格或相关性等关键要素，或受到预训练数据的偏见影响，这使得开放性长文本评估成为一个未充分探索的问题。

**Method:** 我们提出PrefBERT，一种用于评估开放性长文本生成的评分模型，它通过在GRPO中使用不同的奖励来区分好与差的输出，从而指导训练。

**Result:** 通过包括语言模型作为裁判、人工评分和定性分析在内的全面评估，我们证明PrefBERT在多句子和段落长度的响应训练中保持了可靠性，并且与GRPO所需的可验证奖励有很好的一致性。人工评估确认使用PrefBERT作为奖励信号来训练策略模型生成的响应更符合人类偏好。

**Conclusion:** PrefBERT在评估开放性长文本生成方面提供了比传统指标ROUGE-L和BERTScore更好的语义反馈支持，其代码在https://github.com/zli12321/long_form_rl可获得。

**Abstract:** Evaluating open-ended long-form generation is challenging because it is hard
to define what clearly separates good from bad outputs. Existing methods often
miss key aspects like coherence, style, or relevance, or are biased by
pretraining data, making open-ended long-form evaluation an underexplored
problem. To address this gap, we propose PrefBERT, a scoring model for
evaluating open-ended long-form generation in GRPO and guiding its training
with distinct rewards for good and bad outputs. Trained on two response
evaluation datasets with diverse long-form styles and Likert-rated quality,
PrefBERT effectively supports GRPO by offering better semantic reward feedback
than traditional metrics ROUGE-L and BERTScore do. Through comprehensive
evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,
we show that PrefBERT, trained on multi-sentence and paragraph-length
responses, remains reliable across varied long passages and aligns well with
the verifiable rewards GRPO needs. Human evaluations confirm that using
PrefBERT as the reward signal to train policy models yields responses better
aligned with human preferences than those trained with traditional metrics. Our
code is available at https://github.com/zli12321/long_form_rl.

</details>


### [9] [Learning-Time Encoding Shapes Unlearning in LLMs](https://arxiv.org/abs/2506.15076)
*Ruihan Wu,Konstantin Garov,Kamalika Chaudhuri*

Main category: cs.CL

> 研究发现，利用改写的短语描述提升删除效果并且从一段文本中单独删除信息具有挑战性，这暗示了在实现可靠的后置删除中，学习时的知识编码方式发挥着关键作用。

<details>
  <summary>Details</summary>

**Motivation:** 随着大规模语言模型在现实世界中的应用越来越广泛，删除特定知识片段的能力变得非常重要，其原因涵盖隐私规定到纠正过时或有害内容等。

**Method:** 该研究通过实证调查学习时知识编码的选择是如何影响事实性知识的“删除”效果的。

**Result:** 该研究的摘要主要探讨了在大规模语言模型中删除特定知识片段的“删除”能力的重要性。研究通过实验揭示了使用同义改写短语描述可以提升“删除”效果，以及从一段文本中删除单一知识点具有挑战性。这表明，知识编码的学习方式在实现可靠的后置删除中可能发挥关键作用。

**Conclusion:** 学习时的知识编码方式可能在实现可靠的后置删除中扮演关键角色。

**Abstract:** As large language models (LLMs) are increasingly deployed in the real world,
the ability to ``unlearn'', or remove specific pieces of knowledge post hoc,
has become essential for a variety of reasons ranging from privacy regulations
to correcting outdated or harmful content. Prior work has proposed unlearning
benchmarks and algorithms, and has typically assumed that the training process
and the target model are fixed. In this work, we empirically investigate how
learning-time choices in knowledge encoding impact the effectiveness of
unlearning factual knowledge. Our experiments reveal two key findings: (1)
learning with paraphrased descriptions improves unlearning performance and (2)
unlearning individual piece of knowledge from a chunk of text is challenging.
Our results suggest that learning-time knowledge encoding may play a central
role in enabling reliable post-hoc unlearning.

</details>


### [10] [Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification](https://arxiv.org/abs/2506.15081)
*Yaxin Fan,Peifeng Li,Qiaoming Zhu*

Main category: cs.CL

> 本文提出了一种话语感知澄清模块 (DCM) 以提高对话话语解析器的性能。通过澄清类型推理和话语目标推理两种不同的推理过程来处理对话中的歧义。同时引入贡献感知偏好优化 (CPO) 来降低错误澄清的风险。实验结果显示，该方法显著优于现有最先进的模型。

<details>
  <summary>Details</summary>

**Motivation:** 对话话语解析中，口语语言特征如省略和成语等会造成歧义，干扰话语关系的识别，增加了句子解析的难度。此研究旨在解决这些问题。

**Method:** 提出了一个包含澄清类型推理和话语目标推理的DCM来解决话语歧义问题，并引入了CPO来降低错误澄清的风险，优化DCM。

**Result:** 实验结果表明，该方法在STAC和Molweni数据集上能有效解决歧义问题，并在性能上超越当前最先进的模型。

**Conclusion:** 研究表明口头话语模块（DCM）和贡献感知偏好优化（CPO）能够增强解析器对对话中歧义处理的能力，总体性能优于现有方法。

**Abstract:** Dialogue discourse parsing aims to identify and analyze discourse relations
between the utterances within dialogues. However, linguistic features in
dialogues, such as omission and idiom, frequently introduce ambiguities that
obscure the intended discourse relations, posing significant challenges for
parsers. To address this issue, we propose a Discourse-aware Clarification
Module (DCM) to enhance the performance of the dialogue discourse parser. DCM
employs two distinct reasoning processes: clarification type reasoning and
discourse goal reasoning. The former analyzes linguistic features, while the
latter distinguishes the intended relation from the ambiguous one. Furthermore,
we introduce Contribution-aware Preference Optimization (CPO) to mitigate the
risk of erroneous clarifications, thereby reducing cascading errors. CPO
enables the parser to assess the contributions of the clarifications from DCM
and provide feedback to optimize the DCM, enhancing its adaptability and
alignment with the parser's requirements. Extensive experiments on the STAC and
Molweni datasets demonstrate that our approach effectively resolves ambiguities
and significantly outperforms the state-of-the-art (SOTA) baselines.

</details>


### [11] [CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records](https://arxiv.org/abs/2506.15118)
*Junke Wang,Hongshun Ling,Li Zhang,Longqian Zhang,Fang Wang,Yuan Gao,Zhi Li*

Main category: cs.CL

> CKD-EHR框架采用知识蒸馏技术，提升基于电子健康记录的疾病预测模型的效率和准确性，显著优于基线模型。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型在医学知识表示和临床应用效率方面面临挑战，这促使研究者提出一个更高效和准确的病患风险预测框架。

**Method:** 通过知识蒸馏技术，首先在增强医学知识的数据上对大型语言模型Qwen2.5-7B进行微调，作为教师模型。然后，通过多粒度注意力蒸馏机制生成可解释的软标签。最后，将知识转移给轻量级的BERT学生模型。

**Result:** 实验结果显示，CKD-EHR框架在MIMIC-III数据集上的诊断准确率提高了9%，F1值提高了27%，推理速度提升了22.2倍。

**Conclusion:** 该方法不仅提高了资源利用率，还显著提高了诊断的准确性和及时性，为临床资源优化提供了实际的技术方案。

**Abstract:** Electronic Health Records (EHR)-based disease prediction models have
demonstrated significant clinical value in promoting precision medicine and
enabling early intervention. However, existing large language models face two
major challenges: insufficient representation of medical knowledge and low
efficiency in clinical deployment. To address these challenges, this study
proposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which
achieves efficient and accurate disease risk prediction through knowledge
distillation techniques. Specifically, the large language model Qwen2.5-7B is
first fine-tuned on medical knowledge-enhanced data to serve as the teacher
model.It then generates interpretable soft labels through a multi-granularity
attention distillation mechanism. Finally, the distilled knowledge is
transferred to a lightweight BERT student model. Experimental results show that
on the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline
model:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and
a 22.2 times inference speedup is achieved. This innovative solution not only
greatly improves resource utilization efficiency but also significantly
enhances the accuracy and timeliness of diagnosis, providing a practical
technical approach for resource optimization in clinical settings. The code and
data for this research are available athttps://github.com/209506702/CKD_EHR.

</details>


### [12] [Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs](https://arxiv.org/abs/2506.15131)
*Jing Yang Lee,Kong-Aik Lee,Woon-Seng Gan*

Main category: cs.CL

> 本文通过分解开放域对话生成为多响应生成(MRG)和基于偏好的选择(PS)两个步骤，采用了新的策略和评估指标来改进对话模型的能力，尤其是提高了小规模语言模型的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管之前的研究表明，建模开放域对话的多对一（o2m）属性可以提高响应的多样性，但大多数现代基于大型语言模型的对话代理并未明确这样做。因此，本文旨在通过分解OD生成为两个关键任务来提升响应的多样性和质量。

**Method:** 本文提出了一种两阶段框架来优化开放域对话系统的多样性响应生成。首先是多响应生成（MRG），生成给定对话上下文的n个语义和词汇多样的高质量响应；然后再通过基于偏好的选择（PS）选择一个单一响应。

**Result:** 通过应用所提的方法到小规模语言模型上，其开放域对话生成能够显著提升响应多样性，同时保持上下文连贯性，并将响应质量提升90%。

**Conclusion:** 实验证明，此两阶段框架应用于较小的大型语言模型可以显著提高开放域对话生成的响应多样性，同时保持上下文连贯性，并将响应质量提高约90%，使得小模型的表现接近于大模型。

**Abstract:** Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby
multiple appropriate responses exist for a single dialogue context. Despite
prior research showing that modeling this property boosts response diversity,
most modern LLM-based dialogue agents do not explicitly do so. In this work, we
model the o2m property of OD in LLMs by decomposing OD generation into two key
tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),
which entail generating a set of n semantically and lexically diverse
high-quality responses for a given dialogue context, followed by selecting a
single response based on human preference, respectively. To facilitate MRG and
PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the
o2m property by featuring multiple plausible responses for each context.
Leveraging o2mDial, we propose new in-context learning and instruction-tuning
strategies, as well as novel evaluation metrics for MRG, alongside a
model-based approach for PS. Empirical results demonstrate that applying the
proposed two-stage framework to smaller LLMs for OD generation enhances overall
response diversity while maintaining contextual coherence, improving response
quality by up to 90%, bringing them closer to the performance of larger models.

</details>


### [13] [Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models](https://arxiv.org/abs/2506.15138)
*Gyeongje Cho,Yeonkyoun So,Chanwoo Park,Sangmin Lee,Sungmok Jung,Jaejin Lee*

Main category: cs.CL

> 论文介绍了Thunder-Tok，一种新的韩语Tokenizer，通过增加平均Token长度来减少多产性，实验表明它可以提高10%的推理速度同时保持性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前分词器在某些语言上的多产性高，论文旨在为韩语开发一种既能减少多产性又能保持模型性能的Tokenizer。

**Method:** 采用基于规则的预分词方法，构建种子词汇表，并使用二叉熵选择算法优化Token，以增加平均Token长度，减少多产性。

**Result:** 该论文引入了Thunder-Tok，这是一种新的韩语分词器，旨在减少token的多产性而不影响模型性能。该研究采用了一种与韩语语言结构相匹配的基于规则的预分词方法，并创建了一个包含类似语言单元token的种子词汇表，同时运用二叉熵选择算法。这些技术提高了平均token长度，从而减少了多产性，同时保留了语言信息。实验结果显示，与BPE相比，Thunder-Tok将多产性降低了大约10%，即减少了10%的token数量，提高了10%的推理速度，同时在各种下游任务中不牺牲性能。这些发现表明，我们以语言为指导的方法对于设计高效的分词器是有效的且可行的。

**Conclusion:** 该研究证明了一种基于语言学的Tokenizer设计方法能够有效减少多产性，提高推理速度，同时保持语言模型的性能。Thunder-Tok在不影响任务性能的前提下，改善了韩语模型的效率。

**Abstract:** This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce
token fertility without compromising model performance. Our approach uses a
rule-based pre-tokenization method that aligns with the linguistic structure of
the Korean language. We also create a seed vocabulary containing tokens that
resemble linguistic units and employ a branching entropy-based selection
algorithm. These techniques increase the average token length, thus lowering
fertility while preserving linguistic information. Experimental results
indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces
the number of tokens by 10%, improving the inference speed by 10%) compared to
BPE without compromising performance across various downstream tasks. These
findings demonstrate that our linguistically informed approach is effective and
practical for designing efficient tokenizers for language models.

</details>


### [14] [Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View](https://arxiv.org/abs/2506.15156)
*Muhammad Cendekia Airlangga,Hilal AlQuabeh,Munachiso S Nwadike,Kentaro Inui*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** We study memory in state-space language models using primacy and recency
effects as behavioral tools to uncover how information is retained and
forgotten over time. Applying structured recall tasks to the Mamba
architecture, we observe a consistent U-shaped accuracy profile, indicating
strong performance at the beginning and end of input sequences. We identify
three mechanisms that give rise to this pattern. First, long-term memory is
supported by a sparse subset of channels within the model's selective state
space block, which persistently encode early input tokens and are causally
linked to primacy effects. Second, short-term memory is governed by
delta-modulated recurrence: recent inputs receive more weight due to
exponential decay, but this recency advantage collapses when distractor items
are introduced, revealing a clear limit to memory depth. Third, we find that
memory allocation is dynamically modulated by semantic regularity: repeated
relations in the input sequence shift the delta gating behavior, increasing the
tendency to forget intermediate items. We validate these findings via targeted
ablations and input perturbations on two large-scale Mamba-based language
models: one with 1.4B and another with 7B parameters.

</details>


### [15] [A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals](https://arxiv.org/abs/2506.15208)
*Andrea Cadeddu,Alessandro Chessa,Vincenzo De Leo,Gianni Fenu,Enrico Motta,Francesco Osborne,Diego Reforgiato Recupero,Angelo Salatino,Luca Secchi*

Main category: cs.CL

> \u8be5\u7a7a\u95f4\u7528\u4e8e\u5206\u6790\u6bcd\u7248\u672c\u548c\u5f00\u653e\u6765\u6e90\u7684\u5927\u8fbe\u4e4b\u591a\u8bed\u6587\u6a21\u578b\u5bf9\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u76ee\u6807\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u529b\u529b\uff0c\u5e76\u8d5b\u52a8\u4e86\u4e00\u5b9e\u767b\u8bbf\u5b9e\u9a8c\u548c\u5c0f\u989f\u767b\u8bbf\u5b9e\u9a8c\u7b49\u65b9\u6cd5\u7684\u6548\u679c\u3002\u7ed3\u679c\u8868\u793a\uff0c\u9009\u62e9\u4e00\u5b9a\u7684\u63d0\u9192\u5de5\u7a0b\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u53d6\u4ee3\u5c0f\u5927\u7684\u6a21\u578b\u3002

<details>
  <summary>Details</summary>

**Motivation:** \u5bf9\u5360\u7528\u4e00\u5b9a\u7684\u63d0\u9192\u5de5\u7a0b\uff0c\u5c0f\u6a21\u578b\u5728\u5bf9\u5e94\u8d5b\u52a8\u4e00\u5b9e\u9a8c\u548c\u5c0f\u989f\u767b\u8bbf\u5b9e\u9a8c\u7b49\u65b9\u6cd5\u4e4b\u540e\uff0c\u53ef\u4ee5\u5f62\u6210\u4e00\u79cd\u7b49\u540c\u5c0f\u5927\u6a21\u578b\u7684\u6548\u679c\u3002

**Method:** Structure

**Result:** {"tldr": "\u8be5\u7a7a\u95f4\u7528\u4e8e\u5206\u6790\u6bcd\u7248\u672c\u548c\u5f00\u653e\u6765\u6e90\u7684\u5927\u8fbe\u4e4b\u591a\u8bed\u6587\u6a21\u578b\u5bf9\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u76f8\u5e94\u76ee\u6807\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u529b\u529b\uff0c\u5e76\u8d5b\u52a8\u4e86\u4e00\u5148\u767b\u8bbf\u5b9e\u9a8c\u548c\u5c0f\u9891\u767b\u8bbf\u5b9e\u9a8c\u7b49\u65b9\u6cd5\u7684\u6548\u679c\u3002\u7ed3\u679c\u8868\u793a\uff0c\u9009\u62e9\u4e00\u5b9a\u7684\u63d0\u9192\u5de5\u7a0b\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u53d6\u4ee3\u5c0f\u5927\u7684\u6a21\u578b\u3002", "motivation": "\u5bf9\u5360\u7528\u4e00\u5b9a\u7684\u63d0\u9192\u5de5\u7a0b\uff0c\u5c0f\u6a21\u578b\u5728\u5bf9\u5e94\u8d5b\u52a8\u4e00\u5b9e\u9a8c\u548c\u5c0f\u9891\u767b\u8bbf\u5b9e\u9a8c\u7b49\u65b9\u6cd5\u4e4b\u540e\uff0c\u53ef\u4ee5\u5f62\u6210\u4e00\u79cd\u7b49\u540c\u5c0f\u5927\u6a21\u578b\u7684\u6548\u679c\u3002", "method": "\u7a0b\u5e8f\u4f7f\u7528\u4e86\u51e0\u4e2a\u8f93\u5165\u5c40\u90e8\u5f00\u5c40\u4e8e\u5927\u8fbe\u4e4b\u591a\u8bed\u6587\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u8d5b\u52a8\u4e00\u5b9e\u9a8c\u548c\u5c0f\u989f\u5b9e\u9a8c\u4e2d\u7684\u6548\u679c\uff0c\u5e76\u8fdb\u884c\u4e86\u8c03\u7528\u548c\u6bd5\u987a\u5b9a\u5750\u4e4b\u540e\u7684\u7ec4\u5408\u64cd\u4f5c\u3002", "result": "\u5c0f\u6a21\u578b\u5728\u7ec4\u5408\u64cd\u4f5c\u4e4b\u540e\u53ef\u5f62\u6210\u4e00\u79cd\u7b49\u540c\u5927\u6a21\u578b\u7684\u6548\u679c\u3002", "conclusion": "\u5c0f\u6a21\u578b\u901a\u8fc7\u4e00\u5b9e\u9a8c\u548c\u5c0f\u989f\u9a8c\u8bc1\u53ef\u4ee5\u5f62\u6210\u8d8a\u52a8\u8d5b\u52a8\u6240\u83b7\u5f97\u6a21\u578b\u7684\u7ed3\u679c\uff0c\u6b63\u786e\u89e3\u9898\u80fd\u529b\u76f8\u540c\u3002"}

**Conclusion:** \u5c0f\u6a21\u578b\u901a\u8fc7\u4e00\u5b9e\u9a8c\u548c\u5c0f\u989f\u9a8c\u8bc1\u53ef\u4ee5\u5f62\u6210\u8d8a\u52a8\u8d5b\u52a8\u6240\u83b7\u5f97\u6a21\u578b\u7684\u7ed3\u679c\uff0c\u6b63\u786e\u89e3\u9898\u80fd\u529b\u76f8\u540c\u3002

**Abstract:** In 2012, the United Nations introduced 17 Sustainable Development Goals
(SDGs) aimed at creating a more sustainable and improved future by 2030.
However, tracking progress toward these goals is difficult because of the
extensive scale and complexity of the data involved. Text classification models
have become vital tools in this area, automating the analysis of vast amounts
of text from a variety of sources. Additionally, large language models (LLMs)
have recently proven indispensable for many natural language processing tasks,
including text classification, thanks to their ability to recognize complex
linguistic patterns and semantics. This study analyzes various proprietary and
open-source LLMs for a single-label, multi-class text classification task
focused on the SDGs. Then, it also evaluates the effectiveness of task
adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot
and Few-Shot Learning, as well as Fine-Tuning within this domain. The results
reveal that smaller models, when optimized through prompt engineering, can
perform on par with larger models like OpenAI's GPT (Generative Pre-trained
Transformer).

</details>


### [16] [ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs](https://arxiv.org/abs/2506.15211)
*Feng He,Zijun Chen,Xinnian Liang,Tingting Ma,Yunqi Qiu,Shuangzhi Wu,Junchi Yan*

Main category: cs.CL

> 本文假设大型推理模型的跨领域泛化能力源自于跨任务共享的抽象推理原型。基于此假设，我们提出了一个增强推理能力的框架——ProtoReasoning，该框架通过转换问题为原型表示、利用验证系统以及在原型空间内合成问题，实现了显著的性能提升，并验证了我们的假设。

<details>
  <summary>Details</summary>

**Motivation:** 大型推理模型在跨领域泛化方面的机制理解尚不充分。我们假设跨领域的泛化是由于在不同领域的推理任务中存在共有的抽象推理原型，这些原型抽象出了问题的本质，揭示了看似不同的任务所具有的共通的推理结构。

**Method:** 我们提出了一个名为ProtoReasoning的框架，该框架通过利用可扩展且可验证的原型表示（通过Prolog进行逻辑推理，通过PDDL进行规划）来增强大模型的推理能力。ProtoReasoning包括：(1)一个自动构建原型的流水线，可以将问题转换为对应的原型表示；(2)一个综合验证系统，通过Prolog和PDDL解释器提供可靠的反馈；(3)在原型空间内合成问题的可扩展性，并确保正确性。

**Result:** 实验证明，ProtoReasoning在逻辑推理（Enigmata-Eval）上比基线模型提高了4.7%，在规划任务上提高了6.3%，在综合推理（MMLU）上提高了4.0%，在数学（AIME24）上提高了1.0%。显著的是，我们的消融研究表明，在原型空间中学习的模型在面对结构相似的问题时，展示了比仅在自然语言表示上训练的模型更好的泛化能力。

**Conclusion:** 实验结果验证了我们的假设，即推理原型是大型语言模型中可泛化推理的基础。

**Abstract:** Recent advances in Large Reasoning Models (LRMs) trained with Long
Chain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain
generalization capabilities. However, the underlying mechanisms supporting such
transfer remain poorly understood. We hypothesize that cross-domain
generalization arises from shared abstract reasoning prototypes -- fundamental
reasoning patterns that capture the essence of problems across domains. These
prototypes minimize the nuances of the representation, revealing that seemingly
diverse tasks are grounded in shared reasoning structures.Based on this
hypothesis, we propose ProtoReasoning, a framework that enhances the reasoning
ability of LLMs by leveraging scalable and verifiable prototypical
representations (Prolog for logical reasoning, PDDL for
planning).ProtoReasoning features: (1) an automated prototype construction
pipeline that transforms problems into corresponding prototype representations;
(2) a comprehensive verification system providing reliable feedback through
Prolog/PDDL interpreters; (3) the scalability to synthesize problems
arbitrarily within prototype space while ensuring correctness. Extensive
experiments show that ProtoReasoning achieves 4.7% improvement over baseline
models on logical reasoning (Enigmata-Eval), 6.3% improvement on planning
tasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics
(AIME24). Significantly, our ablation studies confirm that learning in
prototype space also demonstrates enhanced generalization to structurally
similar problems compared to training solely on natural language
representations, validating our hypothesis that reasoning prototypes serve as
the foundation for generalizable reasoning in large language models.

</details>


### [17] [MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs](https://arxiv.org/abs/2506.15215)
*Yongqi Fan,Yating Wang,Guandong Wang,Jie Zhai,Jingping Liu,Qi Ye,Tong Ruan*

Main category: cs.CL

> 论文提出MinosEval方法，针对开放式问题的回答评估，根据不同类型问题采用不同评估策略，实验结果表明其评估效果较好且结果可解释。

<details>
  <summary>Details</summary>

**Motivation:** 开放式问题回答评估在传统的评估方法（如ROUGE和BERTScore）中面临困难，这些方法难以捕捉语义相似度且不具备直观的可解释性。同时，现有评估方法未区分事实型和非事实型问题，这影响了评估的效果和合理性。因此，需要一种能够有效区分和评估开放式问题的回答的新方法。

**Method:** 文章提出了MinosEval方法，首先通过问题类型区分模块来区分开放性问题（包括事实型和非事实型），然后根据问题类型选择合适的评分策略：事实型问题采用关键词评分策略，非事实型问题采用实例感知的列表排名策略。

**Result:** 该论文提出一种新颖的开放式问题回答评估方法MinosEval，该方法首先区分事实型和非事实型问题，然后采用不同的评估策略对候选答案进行排序。对于事实型问题，采用自适应关键点评分策略；对于非事实型问题，采用实例感知的列表排名策略。实验表明，MinosEval评估结果更符合人类标注，并且具有更好的可解释性。

**Conclusion:** 实验结果表明，MinosEval在多种开放式问题回答数据集上，尤其是自建的数据集上，能更好地与人类标注保持一致，并提供更具有可解释性的评估结果。

**Abstract:** Open-ended question answering (QA) is a key task for evaluating the
capabilities of large language models (LLMs). Compared to closed-ended QA, it
demands longer answer statements, more nuanced reasoning processes, and diverse
expressions, making refined and interpretable automatic evaluation both crucial
and challenging. Traditional metrics like ROUGE and BERTScore struggle to
capture semantic similarities due to different patterns between model responses
and reference answers. Current LLM-based evaluation approaches, such as
pairwise or listwise comparisons of candidate answers, lack intuitive
interpretability. While pointwise scoring of each response provides some
descriptions, it fails to adapt across different question contents. Most
notably, existing methods overlook the distinction between factoid and
non-factoid questions. To address these challenges, we propose
\textbf{MinosEval}, a novel evaluation method that first distinguishes
open-ended questions and then ranks candidate answers using different
evaluation strategies. For factoid questions, it applies an adaptive key-point
scoring strategy, while for non-factoid questions, it uses an instance-aware
listwise ranking strategy. Experiments on multiple open-ended QA datasets,
including self-built ones with more candidate responses to complement community
resources, show that MinosEval better aligns with human annotations and offers
more interpretable results.

</details>


### [18] [Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants](https://arxiv.org/abs/2506.15239)
*Jaione Bengoetxea,Itziar Gonzalez-Dios,Rodrigo Agerri*

Main category: cs.CL

> 本文评估了当前语言技术处理巴斯克语和西班牙语变体的能力，并发现这些技术在处理某些语言变体时性能下降，尤其是巴斯克语方言。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在评估当今语言技术理解巴斯克语和西班牙语变体的能力。

**Method:** 本文采用自然语言推理（NLI）作为关键任务，并引入了一个新颖的手动整理的平行数据集，包含巴斯克语和西班牙语及其各自变体。

**Result:** 实证分析显示，当处理语言变体，尤其是在巴斯克语时，跨语言和上下文学习实验中的大型语言模型（LLMs）的性能下降。错误分析表明这种下降并非由于词汇重叠问题，而是由于语言变体问题本身。进一步的消融实验表明，基于编码器的模型在处理西部巴斯克语变体时尤其困难，并且这与认为边缘方言（如西部方言）离标准语言更远的语言理论相吻合。

**Conclusion:** 所有的数据和代码都是公开可用的，这有助于进一步的研究和验证本文的结果。

**Abstract:** In this paper, we evaluate the capacity of current language technologies to
understand Basque and Spanish language varieties. We use Natural Language
Inference (NLI) as a pivot task and introduce a novel, manually-curated
parallel dataset in Basque and Spanish, along with their respective variants.
Our empirical analysis of crosslingual and in-context learning experiments
using encoder-only and decoder-based Large Language Models (LLMs) shows a
performance drop when handling linguistic variation, especially in Basque.
Error analysis suggests that this decline is not due to lexical overlap, but
rather to the linguistic variation itself. Further ablation experiments
indicate that encoder-only models particularly struggle with Western Basque,
which aligns with linguistic theory that identifies peripheral dialects (e.g.,
Western) as more distant from the standard. All data and code are publicly
available.

</details>


### [19] [Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs](https://arxiv.org/abs/2506.15241)
*Yang Fan,Zhang Qi,Xing Wenqian,Liu Chang,Liu Liu*

Main category: cs.CL

> 通过Graph RAG框架，本文提出了一种低资源的解决方案，用于历史文本分析，特别是人物关系的数据集和知识抽取，显著改善了模型性能，减少了幻觉现象，提高了可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在填补一般大型语言模型在历史文本分析方面的领域知识空白，特别是在计算人文和AIGC技术的背景下。

**Method:** 本文提出了Graph RAG框架，结合了链式思维提示、自指令生成和过程监督，用于创建《史记》中的人物关系数据集，极大地减少了手动标注的工作量。

**Result:** 实验表明，特定领域的模型Xunzi-Qwen1.5-14B在关系抽取上取得了最佳性能（F1=0.68）。而集成了GraphRAG的DeepSeek模型在开放领域的C-CLUE关系抽取数据集上，F1值提升了11%（0.08-0.19），超过了Xunzi-Qwen1.5-14B的F1值0.12。

**Conclusion:** 本框架提供了一种低资源的解决方案，可应用于古典文本知识抽取，从而推动历史知识服务和人文研究的发展。

**Abstract:** This article addresses domain knowledge gaps in general large language models
for historical text analysis in the context of computational humanities and
AIGC technology. We propose the Graph RAG framework, combining chain-of-thought
prompting, self-instruction generation, and process supervision to create a The
First Four Histories character relationship dataset with minimal manual
annotation. This dataset supports automated historical knowledge extraction,
reducing labor costs. In the graph-augmented generation phase, we introduce a
collaborative mechanism between knowledge graphs and retrieval-augmented
generation, improving the alignment of general models with historical
knowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,
with Simplified Chinese input and chain-of-thought prompting, achieves optimal
performance in relation extraction (F1 = 0.68). The DeepSeek model integrated
with GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation
extraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),
effectively alleviating hallucinations phenomenon, and improving
interpretability. This framework offers a low-resource solution for classical
text knowledge extraction, advancing historical knowledge services and
humanities research.

</details>


### [20] [TopClustRAG at SIGIR 2025 LiveRAG Challenge](https://arxiv.org/abs/2506.15246)
*Juli Bakagianni,John Pavlopoulos,Aristidis Likas*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** We present TopClustRAG, a retrieval-augmented generation (RAG) system
developed for the LiveRAG Challenge, which evaluates end-to-end question
answering over large-scale web corpora. Our system employs a hybrid retrieval
strategy combining sparse and dense indices, followed by K-Means clustering to
group semantically similar passages. Representative passages from each cluster
are used to construct cluster-specific prompts for a large language model
(LLM), generating intermediate answers that are filtered, reranked, and finally
synthesized into a single, comprehensive response. This multi-stage pipeline
enhances answer diversity, relevance, and faithfulness to retrieved evidence.
Evaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in
faithfulness and 7th in correctness on the official leaderboard, demonstrating
the effectiveness of clustering-based context filtering and prompt aggregation
in large-scale RAG systems.

</details>


### [21] [Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments](https://arxiv.org/abs/2506.15266)
*Sungen Hahm,Heejin Kim,Gyuseong Lee,Hyunji Park,Jaejin Lee*

Main category: cs.CL

> 文章提出了一种新的去标识化框架Thunder-DeID，设计了特定的韩语法律数据集并引入了一套系统化的个人身份信息分类方法，并采用基于深度神经网络的去标识化流程，实验数据证明，该模型达到了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了在开放司法访问和个人数据保护之间取得平衡，韩国司法体系要求在公共公开之前必须对法院判决进行去标识化，但目前的去标识化过程无法在遵守严格法律要求的同时处理大量法院判决，并且现有关于个人标识符的法律定义和技术解决方案之间的契合度不高。

**Method:** 开发了一个名为Thunder-DeID的去标识化框架，该框架包含三大组成部分：1) 构建并发布了首个包含注释判决和对应的实体提及列表的韩语法律数据集；2) 引入了一种系统性的个人身份信息类别划分；3) 开发了一个端到端的基于深度神经网络的去标识化流程。

**Result:** 实验结果表明，该模型在法院判决的去标识化任务中达到了最先进的性能。

**Conclusion:** Thunder-DeID框架的成功实施，说明了通过创造性的技术和法律定义改进现有去标识化方法的可能性及其高效性。

**Abstract:** To ensure a balance between open access to justice and personal data
protection, the South Korean judiciary mandates the de-identification of court
judgments before they can be publicly disclosed. However, the current
de-identification process is inadequate for handling court judgments at scale
while adhering to strict legal requirements. Additionally, the legal
definitions and categorizations of personal identifiers are vague and not
well-suited for technical solutions. To tackle these challenges, we propose a
de-identification framework called Thunder-DeID, which aligns with relevant
laws and practices. Specifically, we (i) construct and release the first Korean
legal dataset containing annotated judgments along with corresponding lists of
entity mentions, (ii) introduce a systematic categorization of Personally
Identifiable Information (PII), and (iii) develop an end-to-end deep neural
network (DNN)-based de-identification pipeline. Our experimental results
demonstrate that our model achieves state-of-the-art performance in the
de-identification of court judgments.

</details>


### [22] [Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment](https://arxiv.org/abs/2506.15301)
*Shrestha Ghosh,Moritz Schneider,Carina Reinicke,Carsten Eickhoff*

Main category: cs.CL

> 本文是关于使用LLM（大型语言模型）在临床试验患者匹配中的应用的第一项综述性研究。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大型语言模型在通用领域表现出色，但在临床试验这样的关键领域其应用仍有限。该领域的自然语言处理任务可以从LLM的知识汇总和推理能力中受益。

**Method:** 分析最近的论文摘要，提取其主要信息。

**Result:** 本论文是第一个全面分析文献中临床试验患者匹配任务并考量新兴LLM方法的研究。它检查了现有基准、方法和评估框架，指出了采用LLM技术面临的挑战。

**Conclusion:** 本综述为未来在临床试验患者匹配任务中采用LLM技术开辟了新方向，并指出了该领域的一些挑战。

**Abstract:** Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,
their adoption in critical domains, such as clinical trial recruitment, remains
limited. As trials are designed in natural language and patient data is
represented as both structured and unstructured text, the task of matching
trials and patients benefits from knowledge aggregation and reasoning abilities
of LLMs. Classical approaches are trial-specific and LLMs with their ability to
consolidate distributed knowledge hold the potential to build a more general
solution. Yet recent applications of LLM-assisted methods rely on proprietary
models and weak evaluation benchmarks. In this survey, we are the first to
analyze the task of trial-patient matching and contextualize emerging LLM-based
approaches in clinical trial recruitment. We critically examine existing
benchmarks, approaches and evaluation frameworks, the challenges to adopting
LLM technologies in clinical research and exciting future directions.

</details>


### [23] [ConLID: Supervised Contrastive Learning for Low-Resource Language Identification](https://arxiv.org/abs/2506.15304)
*Negar Foroutan,Jakhongir Saydaliev,Ye Eun Kim,Antoine Bosselut*

Main category: cs.CL

> 本文提出了一个监督对比学习方法来解决低资源语言在语言识别任务上的性能问题，特别是在外域数据上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管许多研究集中在收集多样的训练数据来改进语言识别模型的表现，低资源语言的表现仍然不理想，通常局限于单一领域数据。本文旨在解决这类语言的类别不平衡和偏置问题。

**Method:** 我们提出了一种新的监督对比学习（SCL）方法，以学习低资源语言的领域不变表示。

**Result:** 通过广泛的分析，我们展示了我们的方法提高了低资源语言在外域数据上的语言识别性能，提升了3.2%。

**Conclusion:** 我们的方法有效地提升了低资源语言在语言识别模型中的表现，特别是在外域数据上的性能有所提升。

**Abstract:** Language identification (LID) is a critical step in curating multilingual LLM
pretraining corpora from web crawls. While many studies on LID model training
focus on collecting diverse training data to improve performance, low-resource
languages -- often limited to single-domain data, such as the Bible -- continue
to perform poorly. To resolve these class imbalance and bias issues, we propose
a novel supervised contrastive learning (SCL) approach to learn
domain-invariant representations for low-resource languages. Through an
extensive analysis, we show that our approach improves LID performance on
out-of-domain data for low-resource languages by 3.2%, demonstrating its
effectiveness in enhancing LID models.

</details>


### [24] [DeVisE: Behavioral Testing of Medical Large Language Models](https://arxiv.org/abs/2506.15339)
*Camila Zurdo Tagliabue,Heloisa Oss Boll,Aykut Erdem,Erkut Erdem,Iacer Calixto*

Main category: cs.CL

> 研究通过DeVisE框架评估五种LLMs在医疗决策中的表现，并展示了零样本模型与微调模型的不同特点。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在临床决策支持中的应用日益增加，但目前的评估方法往往无法区分真正意义上的医疗推理与表面模式。

**Method:** 本研究引入了DeVisE（人口统计和生命体征评估）行为测试框架，用于检测细微的临床理解。数据集由MIMIC-IV的ICU出院总结构成，包括现实世界数据和基于模板的合成数据，后者通过单一变量反事实场景专注于人口统计（年龄、性别、种族）和生命体征属性。研究评估了五种LLMs，包括通用型和医疗微调型模型，在零样本和微调环境下进行测试。评估模型行为通过输入层面的敏感性（反事实如何改变笔记出现概率），以及下游推理（它们如何影响预测住院时长）进行。

**Result:** 研究结果显示，零样本模型在反事实推理模式上表现得更为连贯，而微调模型则更稳定但对临床有意义的变化响应较小。值得注意的是，人口统计因素细微但一致地影响模型输出，强调了公平评估的重要性。

**Conclusion:** 这项工作强调了行为测试在揭示临床LLMs推理策略和设计更安全、透明的医疗AI系统方面的实用性。

**Abstract:** Large language models (LLMs) are increasingly used in clinical decision
support, yet current evaluation methods often fail to distinguish genuine
medical reasoning from superficial patterns. We introduce DeVisE (Demographics
and Vital signs Evaluation), a behavioral testing framework for probing
fine-grained clinical understanding. We construct a dataset of ICU discharge
notes from MIMIC-IV, generating both raw (real-world) and template-based
(synthetic) versions with controlled single-variable counterfactuals targeting
demographic (age, gender, ethnicity) and vital sign attributes. We evaluate
five LLMs spanning general-purpose and medically fine-tuned variants, under
both zero-shot and fine-tuned settings. We assess model behavior via (1)
input-level sensitivity - how counterfactuals alter the likelihood of a note;
and (2) downstream reasoning - how they affect predicted hospital
length-of-stay. Our results show that zero-shot models exhibit more coherent
counterfactual reasoning patterns, while fine-tuned models tend to be more
stable yet less responsive to clinically meaningful changes. Notably,
demographic factors subtly but consistently influence outputs, emphasizing the
importance of fairness-aware evaluation. This work highlights the utility of
behavioral testing in exposing the reasoning strategies of clinical LLMs and
informing the design of safer, more transparent medical AI systems.

</details>


### [25] [SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture](https://arxiv.org/abs/2506.15355)
*Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Sriparna Saha*

Main category: cs.CL

> 论文介绍了一个名为SANSKRITI的基准测试，用于评估语言模型对印度丰富文化多样性理解的能力。该基准包含21,853个精心策划的问题和答案对，涵盖印度28个州和8个中央直辖地区的文化知识。SANSKRITI旨在解决语言模型在处理文化细微差别方面的不足，并为评估和改进语言模型的文化理解力设定了新标准。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决语言模型对不同地区文化背景理解不足的问题，尤其是在处理文化细微差别的能力方面。印度文化非常丰富且多样化，通过制定这个基准测试，可以更好地测试和改进语言模型对这种复杂性的理解和应用能力。

**Method:** 该研究通过设计SANSKRITI基准测试来评估语言模型，特别是大型语言模型、印地语语言模型和小型语言模型对印度文化多样性的理解情况。SANSKRITI包括16个关键的文化属性，涵盖了印度文化和生活方式的不同方面。

**Result:** 研究表明，不同规模的语言模型在处理文化细腻的查询上存在显著差距，尤其在处理地区特定情境下表现较差，这表明当前的语言模型在文化理解方面还有很大的提升空间。

**Conclusion:** SANSKRITI提供了一个大规模、文化丰富的数据集，用于评估语言模型对印度文化理解的能力。此基准测试将助力于进一步提升语言模型在处理文化细微差别上的性能。

**Abstract:** Language Models (LMs) are indispensable tools shaping modern workflows, but
their global effectiveness depends on understanding local socio-cultural
contexts. To address this, we introduce SANSKRITI, a benchmark designed to
evaluate language models' comprehension of India's rich cultural diversity.
Comprising 21,853 meticulously curated question-answer pairs spanning 28 states
and 8 union territories, SANSKRITI is the largest dataset for testing Indian
cultural knowledge. It covers sixteen key attributes of Indian culture: rituals
and ceremonies, history, tourism, cuisine, dance and music, costume, language,
art, festivals, religion, medicine, transport, sports, nightlife, and
personalities, providing a comprehensive representation of India's cultural
tapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic
Language Models (ILMs), and Small Language Models (SLMs), revealing significant
disparities in their ability to handle culturally nuanced queries, with many
models struggling in region-specific contexts. By offering an extensive,
culturally rich, and diverse dataset, SANSKRITI sets a new standard for
assessing and improving the cultural understanding of LMs.

</details>


### [26] [COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation](https://arxiv.org/abs/2506.15372)
*Raghvendra Kumar,S. A. Mohammed Salman,Aryan Sahu,Tridib Nandi,Pragathi Y. P.,Sriparna Saha,Jose G. Moreno*

Main category: cs.CL

> 本研究通过介绍COSMMIC数据集解决了印度语言在评论意识下的多媒体和多语言摘要生成研究的空白，此数据集包含文章文本、用户评论和图片信息，并进行了基于LLama3和GPT-4等语言模型的评估以找到最有效的配置。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于填补印度语言在评论相关的多媒体和多语言摘要生成研究方面的空白。现有数据集大多专注于英文和中文，而在处理其他语言，特别是印度各类语言的方法和发展还相当有限。COSMMIC旨在通过构建一个具有代表性的数据集来弥补这一空白，促进NLP领域的研究，并增进其包容性。

**Method:** 本文介绍并使用了COSMMIC数据集，这是一个专为九种主要印度语言设计的、带有评论的多语言多媒体数据集。数据集包含了文章-图片对以及读者评论。研究中探索了四种不同的摘要生成配置：仅使用文章文本，整合用户评论，利用图片信息以及结合文本、评论和图片的综合方法。为了评估数据集的有效性，研究采用了最先进的语言模型如LLama3和GPT-4。同时，研究中还使用了IndicBERT进行评论分类以筛选出有价值的评论，并利用多语言CLIP分类器从图片中提取有意义的信息。

**Result:** 通过结合文章文本、用户评论和图片信息，本文的方法为印度语言的NLP任务提供了新视角，并在不同模型和配置下进行了评估，证明了该方法的可行性和高效性。

**Conclusion:** COSMMIC数据集填补了印度语言资源在NLP研究中的空白，并提供了处理用户评论和多媒体数据的新方式。其综合策略促进了NLP领域的发展，并且对于多语言、多媒体的自然语言生成任务具有重要贡献。

**Abstract:** Despite progress in comment-aware multimodal and multilingual summarization
for English and Chinese, research in Indian languages remains limited. This
study addresses this gap by introducing COSMMIC, a pioneering comment-sensitive
multimodal, multilingual dataset featuring nine major Indian languages. COSMMIC
comprises 4,959 article-image pairs and 24,484 reader comments, with
ground-truth summaries available in all included languages. Our approach
enhances summaries by integrating reader insights and feedback. We explore
summarization and headline generation across four configurations: (1) using
article text alone, (2) incorporating user comments, (3) utilizing images, and
(4) combining text, comments, and images. To assess the dataset's
effectiveness, we employ state-of-the-art language models such as LLama3 and
GPT-4. We conduct a comprehensive study to evaluate different component
combinations, including identifying supportive comments, filtering out noise
using a dedicated comment classifier using IndicBERT, and extracting valuable
insights from images with a multilingual CLIP-based classifier. This helps
determine the most effective configurations for natural language generation
(NLG) tasks. Unlike many existing datasets that are either text-only or lack
user comments in multimodal settings, COSMMIC uniquely integrates text, images,
and user feedback. This holistic approach bridges gaps in Indian language
resources, advancing NLP research and fostering inclusivity.

</details>


### [27] [Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning](https://arxiv.org/abs/2506.15415)
*Stanley Ngugi*

Main category: cs.CL

> 本研究针对资源不足语言在大语言模型中词汇对齐不佳的问题，提出了一种名为Targeted Lexical Injection (TLI) 的新方法，实验表明该方法显著提升了Swahili-English词汇对的对齐效果，而且对于未见过的单词对也有很好的泛化效果。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型在资源不足的语言上的表现较差，主要是由于这些语言的数据稀缺和预训练中的代表性不足所导致。研究旨在通过改进词汇对齐来提升这些模型在诸如翻译及跨语言信息检索等任务上的性能。

**Method:** 本研究提出了一种名为Targeted Lexical Injection (TLI) 的新方法，利用Low-Rank Adaptation (LoRA) 和对比学习目标，专注于Lugha-Llama-8B-wura模型的第2层词汇对齐优化，该层在Swahili-English词汇对的对齐上表现出色。

**Result:** 实验表明，通过TLI方法，623组训练过的Swahili-English词汇对的相似度从0.3211提升到了0.4113，增加了28.08%。同时，对于63个未见过的单词对也表现出良好的泛化能力，相似度从0.3143提升到了0.4033，增幅为28.32%。

**Conclusion:** TLI方法能够有效提升目标语言对的词汇对齐性能，尤其体现在从模型内部早期层向最终输出表示的传播方面。这一方法为资源不足语言的模型优化提供了一种参数有效和高效的方法。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their performance in low-resource languages (LRLs), such as Swahili, often lags
due to data scarcity and underrepresentation in pre-training. A key challenge
is achieving robust cross-lingual lexical alignment, crucial for tasks like
translation and cross-lingual information retrieval. This paper introduces
Targeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.
We first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits
strong, near-perfect lexical alignment for Swahili-English word pairs in its
early internal layers (specifically Layer 2, with ~0.99998 average cosine
similarity based on a pilot study), a capability not fully reflected in its
final output representations (baseline ~0.32 similarity on our evaluation set).
TLI leverages this insight by using Low-Rank Adaptation (LoRA) and a
contrastive learning objective to fine-tune the model, specifically targeting
embeddings from this empirically identified optimal early layer. Our
experiments show that TLI significantly improves the output-level lexical
alignment for 623 trained Swahili-English word pairs, increasing average cosine
similarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More
importantly, these improvements generalize remarkably well to 63 unseen control
word pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17
x 10^-27). These findings suggest TLI enhances the model's ability to preserve
and propagate its inherent early-layer cross-lingual knowledge, offering a
parameter-efficient and effective strategy for improving lexical alignment in
LRL-focused LLMs.

</details>


### [28] [Understanding GUI Agent Localization Biases through Logit Sharpness](https://arxiv.org/abs/2506.15425)
*Xingjian Tao,Yiwei Wang,Yujun Cai,Zhicheng Yang,Jing Tang*

Main category: cs.CL

> 文章提出了一种细粒度评估框架和Context-Aware Cropping技术，用于提升多模态大语言模型在GUI交互中的性能和可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 针对多模态大语言模型在GUI操作中常出现的系统性定位错误问题，此研究旨在提供更精细的评估方法和提高模型可靠性的策略。

**Method:** 我们提出了一种细粒度的评估框架，将模型预测分类为四种不同的类型，并引入Peak Sharpness Score（PSS）来更好地量化模型的不确定性。此外，我们提出了一种无需训练的技术——Context-Aware Cropping，通过自适应地调整输入上下文来提高模型性能。

**Result:** 实验广泛展示了我们的框架和方法提供了可操作的见解，增强了GUI代理行为的解释性和鲁棒性。

**Conclusion:** 我们的工作不仅揭示了多模态语言模型在GUI操作中的失败机制，还提供了一种提高其可靠性的有效途径。

**Abstract:** Multimodal large language models (MLLMs) have enabled GUI agents to interact
with operating systems by grounding language into spatial actions. Despite
their promising performance, these models frequently exhibit
hallucinations-systematic localization errors that compromise reliability. We
propose a fine-grained evaluation framework that categorizes model predictions
into four distinct types, revealing nuanced failure modes beyond traditional
accuracy metrics. To better quantify model uncertainty, we introduce the Peak
Sharpness Score (PSS), a metric that evaluates the alignment between semantic
continuity and logits distribution in coordinate prediction. Building on this
insight, we further propose Context-Aware Cropping, a training-free technique
that improves model performance by adaptively refining input context. Extensive
experiments demonstrate that our framework and methods provide actionable
insights and enhance the interpretability and robustness of GUI agent behavior.

</details>


### [29] [AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need](https://arxiv.org/abs/2506.15451)
*Zhouhong Gu,Xiaoxuan Zhu,Yin Cai,Hao Shen,Xingzhou Chen,Qingyi Wang,Jialin Li,Xiaoran Shi,Haoran Guo,Wenxuan Huang,Hongwei Feng,Yanghua Xiao,Zheyu Ye,Yao Hu,Shaosheng Cao*

Main category: cs.CL

> AgentGroupChat-V2是一个克服现有框架在系统架构设计、跨域通用性和性能保证方面挑战的新型框架，特别适用于任务复杂度和代理数增加的场景。它通过三种核心创新机制表现出了优异的性能：全并行架构、自适应协作引擎、以及高效的代理组织优化策略。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服基于大型语言模型的多代理系统在系统架构设计、跨域通用性和性能保证方面的挑战，尤其是在任务复杂性和代理数量增加时出现的问题，引入了AgentGroupChat-V2框架。

**Method:** AgentGroupChat-V2通过三种核心设计实现此目的：(1)全并行架构，用于分解用户查询并管理依赖关系；(2)自适应协作引擎，根据任务特性动态选择不同类型的大型语言模型组合及其互动方式；(3)代理组织优化策略，以提高问题分解效率。

**Result:** 实验表明，AgentGroupChat-V2在不同领域表现出优异性能：在GSM8K上准确率达到91.50%（超过最佳基线5.6个百分点），在AIME比赛中准确率达到30.4%，在HumanEval上通过率达到79.20%。随着任务难度增加，其性能优势更为明显，特别是在Level 5 MATH问题上，其改进超过了11个百分点。

**Conclusion:** 这些结果证实了AgentGroupChat-V2在构建高效、通用的多代理系统上提供了全面解决方案，特别是在复杂推理场景下有显著优势。代码可以在此项目中获取：https://github.com/MikeGu721/AgentGroupChat-V2。

**Abstract:** Large language model based multi-agent systems have demonstrated significant
potential in social simulation and complex task resolution domains. However,
current frameworks face critical challenges in system architecture design,
cross-domain generalizability, and performance guarantees, particularly as task
complexity and number of agents increases. We introduces AgentGroupChat-V2, a
novel framework addressing these challenges through three core innovations: (1)
a divide-and-conquer fully parallel architecture that decomposes user queries
into hierarchical task forest structures enabling dependency management and
distributed concurrent processing. (2) an adaptive collaboration engine that
dynamically selects heterogeneous LLM combinations and interaction modes based
on task characteristics. (3) agent organization optimization strategies
combining divide-and-conquer approaches for efficient problem decomposition.
Extensive experiments demonstrate AgentGroupChat-V2's superior performance
across diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best
baseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME
(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance
advantages become increasingly pronounced with higher task difficulty,
particularly on Level 5 MATH problems where improvements exceed 11 percentage
points compared to state-of-the-art baselines. These results confirm that
AgentGroupChat-V2 provides a comprehensive solution for building efficient,
general-purpose LLM multi-agent systems with significant advantages in complex
reasoning scenarios. Code is available at
https://github.com/MikeGu721/AgentGroupChat-V2.

</details>


### [30] [RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation](https://arxiv.org/abs/2506.15455)
*Xinnuo Xu,Rachel Lawrence,Kshitij Dubey,Atharva Pandey,Risa Ueno,Fabian Falck,Aditya V. Nori,Rahul Sharma,Amit Sharma,Javier Gonzalez*

Main category: cs.CL

> RE-IMAGINE框架用于评估大语言模型的真实推理能力，通过生成不可单纯依靠记忆解决的问题来测试其性能。

<details>
  <summary>Details</summary>

**Motivation:** 旨在区分大语言模型的高基准成绩是源于真正的推理能力还是训练集的统计记忆。

**Method:** Structure

**Result:** {
  "tldr": "RE-IMAGINE框架用于评估大语言模型的真实推理能力，通过生成不可单纯依靠记忆解决的问题来测试其性能。",
  "motivation": "旨在区分大语言模型的高基准成绩是源于真正的推理能力还是训练集的统计记忆。",
  "method": "基于因果关系的三个层次设计了RE-IMAGINE框架，并通过一个自动化管道产生不同层次的问题变化。",
  "result": "在四个广泛使用的基准测试上评估了多个家族的大型语言模型，并观察到在问题变化时性能降低。",
  "conclusion": "该框架揭示了大型语言模型过去成绩中的记忆依赖，并为不同推理层次的模型能力研究提供了方向。 "
}

**Conclusion:** 该框架揭示了大型语言模型过去成绩中的记忆依赖，并为不同推理层次的模型能力研究提供了方向。

**Abstract:** Recent Large Language Models (LLMs) have reported high accuracy on reasoning
benchmarks. However, it is still unclear whether the observed results arise
from true reasoning or from statistical recall of the training set. Inspired by
the ladder of causation (Pearl, 2009) and its three levels (associations,
interventions and counterfactuals), this paper introduces RE-IMAGINE, a
framework to characterize a hierarchy of reasoning ability in LLMs, alongside
an automated pipeline to generate problem variations at different levels of the
hierarchy. By altering problems in an intermediate symbolic representation,
RE-IMAGINE generates arbitrarily many problems that are not solvable using
memorization alone. Moreover, the framework is general and can work across
reasoning domains, including math, code, and logic. We demonstrate our
framework on four widely-used benchmarks to evaluate several families of LLMs,
and observe reductions in performance when the models are queried with problem
variations. These assessments indicate a degree of reliance on statistical
recall for past performance, and open the door to further research targeting
skills across the reasoning hierarchy.

</details>


### [31] [Context-Informed Grounding Supervision](https://arxiv.org/abs/2506.15480)
*Hyunji Lee,Seunghyun Yoon,Yunjae Won,Hanseok Oh,Geewook Kim,Trung Bui,Franck Dernoncourt,Elias Stengel-Eskin,Mohit Bansal,Minjoon Seo*

Main category: cs.CL

> 本文提出了一种CINGS训练方式，用以提高大型语言模型在文本和视觉语言任务中的内容一致性，而不影响其泛化性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机源于现有方法在推理过程中仅仅添加上下文并不能确保生成的响应是有根据的。希望通过CINGS方法加强模型输出与提供的外部上下文之间的一致性。

**Method:** 提出了一种称为上下文引导监督(CINGS)的方法，在模型训练时将相关上下文添加到响应的前面，但在计算损失时仅计算响应标记，而忽略上下文。

**Result:** 研究表明，经过CINGS训练的模型在文本和视觉语言两个领域都表现出了强烈的内容一致性。在文本领域，CINGS在11个寻求信息的数据集上超过了其他训练方法。在视觉语言领域，CINGS减少了四个基准的幻觉，同时保持了生成响应中的事实一致性。

**Conclusion:** 研究表明，在不影响一般性能的前提下，CINGS可以在文本和视觉语言任务中提供更强的反应定性和更少的幻觉现象，提升了模型对提供的外部内容的依赖和一致性。同时，CINGS导致了模型在先前知识和行为方面的变化。

**Abstract:** Large language models (LLMs) are often supplemented with external knowledge
to provide information not encoded in their parameters or to reduce
hallucination. In such cases, we expect the model to generate responses by
grounding its response in the provided external context. However, prior work
has shown that simply appending context at inference time does not ensure
grounded generation. To address this, we propose Context-INformed Grounding
Supervision (CINGS), a post-training supervision in which the model is trained
with relevant context prepended to the response, while computing the loss only
over the response tokens and masking out the context. Our experiments
demonstrate that models trained with CINGS exhibit stronger grounding in both
textual and visual domains compared to standard instruction-tuned models. In
the text domain, CINGS outperforms other training methods across 11
information-seeking datasets and is complementary to inference-time grounding
techniques. In the vision-language domain, replacing a vision-language model's
LLM backbone with a CINGS-trained model reduces hallucinations across four
benchmarks and maintains factual consistency throughout the generated response.
This improved grounding comes without degradation in general downstream
performance. Finally, we analyze the mechanism underlying the enhanced
grounding in CINGS and find that it induces a shift in the model's prior
knowledge and behavior, implicitly encouraging greater reliance on the external
context.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [32] [SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2506.14791)
*Jingxuan Zhou,Yuehao Wu,Yibo Zhang,Yeyubei Zhang,Yunchong Liu,Bolin Huang,Chunhong Yuan*

Main category: cs.CV

> This paper proposes the Semantic Irony Recognition Network (SemIRNet) to improve the accuracy in multimodal irony detection by integrating ConceptNet knowledge and cross-modal semantic similarity detection, enhancing the performance through a contrastive learning loss function.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of accurately identifying graphical implicit correlations in multimodal irony detection tasks, a novel network SemIRNet is proposed.

**Method:** The paper introduces a Semantic Irony Recognition Network (SemIRNet) with three key innovations: (1) Utilization of the ConceptNet knowledge base to boost the common-sense reasoning capability of the model; (2) Implementation of cross-modal semantic similarity detection modules at the word and sample levels for better understanding of graphic-textual correlations; and (3) Integration of a contrastive learning loss function to enhance the separability of positive and negative samples.

**Result:** Experiments on a publicly available multimodal irony detection dataset demonstrate an improvement of 1.64% and 2.88% in accuracy and F1 value to 88.87% and 86.33%, respectively, over existing optimal methods. Ablation studies confirm the significance of knowledge fusion and semantic similarity detection in performance enhancement.

**Conclusion:** The Semantic Irony Recognition Network (SemIRNet) proves to be effective in identifying graphical implicit correlations in multimodal irony detection tasks, showcasing improvements in accuracy and F1 score, with ablation experiments validating the benefits of knowledge fusion and semantic similarity detection.

**Abstract:** Aiming at the problem of difficulty in accurately identifying graphical
implicit correlations in multimodal irony detection tasks, this paper proposes
a Semantic Irony Recognition Network (SemIRNet). The model contains three main
innovations: (1) The ConceptNet knowledge base is introduced for the first time
to acquire conceptual knowledge, which enhances the model's common-sense
reasoning ability; (2) Two cross-modal semantic similarity detection modules at
the word level and sample level are designed to model graphic-textual
correlations at different granularities; and (3) A contrastive learning loss
function is introduced to optimize the spatial distribution of the sample
features, which improves the separability of positive and negative samples.
Experiments on a publicly available multimodal irony detection benchmark
dataset show that the accuracy and F1 value of this model are improved by 1.64%
and 2.88% to 88.87% and 86.33%, respectively, compared with the existing
optimal methods. Further ablation experiments verify the important role of
knowledge fusion and semantic similarity detection in improving the model
performance.

</details>


### [33] [Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?](https://arxiv.org/abs/2506.14805)
*Yang Yao,Lingyu Li,Jiaxin Song,Chiyu Chen,Zhenqi He,Yixu Wang,Xin Wang,Tianle Gu,Jie Li,Yan Teng,Yingchun Wang*

Main category: cs.CV

> 论文介绍了Argus Inspection基准测试和Eye of Panoptes框架，以评估多模态大语言模型在视觉细粒度感知和常识因果推理方面的能力，发现这些模型的性能有限，指出未来研究的方向和潜力。

<details>
  <summary>Details</summary>

**Motivation:** 多模态大语言模型（MLLMs）在认知和推理能力上取得了显著进步，但仍面临视觉细粒度感知和常识因果推理的挑战。

**Method:** 提出Argus Inspection多模态基准测试，分为两个难度级别，强调详细视觉识别，同时纳入现实世界的常识理解来评估因果推理能力。并介绍了Panoptes之眼框架，该框架整合了一个二元参数逻辑斯蒂度量和一个指示函数，以更全面地评估MLLMs在基于观点的推理任务中的表现。

**Result:** 

**Conclusion:** 实验结果显示，26个主流MLLMs中，视觉细粒度推理的最佳性能仅为0.46，表明有很大的改进空间。研究提供了对MLLMs进一步精进的宝贵见解。

**Abstract:** As Multimodal Large Language Models (MLLMs) continue to evolve, their
cognitive and reasoning capabilities have seen remarkable progress. However,
challenges in visual fine-grained perception and commonsense causal inference
persist. This paper introduces Argus Inspection, a multimodal benchmark with
two levels of difficulty, emphasizing detailed visual recognition while
incorporating real-world commonsense understanding to evaluate causal reasoning
abilities. Expanding on it, we present the Eye of Panoptes framework, which
integrates a binary parametric Sigmoid metric with an indicator function,
enabling a more holistic evaluation of MLLMs' responses in opinion-based
reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the
highest performance in visual fine-grained reasoning reaches only 0.46,
highlighting considerable potential for enhancement. Our research offers
valuable perspectives for the continued refinement of MLLMs.

</details>


### [34] [A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease Detection](https://arxiv.org/abs/2506.14816)
*Alavikunhu Panthakkan,Zubair Medammal,S M Anzar,Fatma Taher,Hussain Al-Ahmad*

Main category: cs.CV

> 本文提出了一种结合ConvNeXt和EfficientNet的AI模型，用于猎鹰疾病的精准分类与诊断，展示了该模型在疾病检测效能上的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 在猎鹰训练和狩猎中，对猎鹰进行严格的健康管理是必要的。本文旨在提出一种AI模型来提升猎鹰疾病的诊断精度，保障猎鹰健康。

**Method:** 研究采用了ConvNeXt和EfficientNet两种AI模型的结合，形成一个混合AI模型，用于猎鹰疾病的分类。

**Result:** 本次研究提出了一种结合ConvNeXt和EfficientNet的AI模型，用于准确分类猎鹰的疾病状况，包括正常状态、肝病和曲霉菌病。通过大规模数据集的训练和验证，研究展示了该模型在准确率、精确度、召回率和F1分数上的出色表现，优于传统诊断方法及单一模型结构。这代表了在猎鹰疾病检测中实现精准医疗的重要进展，并为未来基于AI的鸟类健康解决方案铺平了道路。

**Conclusion:** 这项研究通过混合AI模型大幅提升了猎鹰疾病的诊断精度，为猎鹰及其他鸟类的健康监测提供了有效的技术支持，对于未来进一步发展AI在动物健康领域的应用具有重要意义。

**Abstract:** Falconry, a revered tradition involving the training and hunting with
falcons, requires meticulous health surveillance to ensure the health and
safety of these prized birds, particularly in hunting scenarios. This paper
presents an innovative method employing a hybrid of ConvNeXt and EfficientNet
AI models for the classification of falcon diseases. The study focuses on
accurately identifying three conditions: Normal, Liver Disease and
'Aspergillosis'. A substantial dataset was utilized for training and validating
the model, with an emphasis on key performance metrics such as accuracy,
precision, recall, and F1-score. Extensive testing and analysis have shown that
our concatenated AI model outperforms traditional diagnostic methods and
individual model architectures. The successful implementation of this hybrid AI
model marks a significant step forward in precise falcon disease detection and
paves the way for future developments in AI-powered avian healthcare solutions.

</details>


### [35] [ViLLa: A Neuro-Symbolic approach for Animal Monitoring](https://arxiv.org/abs/2506.14823)
*Harsha Koduri*

Main category: cs.CV

> ViLLa是一个用于动物监测的神经符号框架，能够理解和回答有关动物数量、存在以及位置的自然语言查询。

<details>
  <summary>Details</summary>

**Motivation:** 动物种群监测需要系统能够解读视觉数据和人类语言查询，ViLLa旨在提供一个模块化且透明的解决方案，以解决现有黑盒模型的不可解释性问题。

**Method:** 本论文提出了ViLLa，一个基于视觉、语言和逻辑的神经符号框架，用于可解释的动物监测。它整合了视觉检测模块、语言解析器和符号推理层三个核心组件，以理解和回答关于动物的自然语言查询。

**Result:** 该系统在一系列动物图像任务中进行了评估，展示了将视觉内容与结构化的、人类可解读的查询相连接的能力。

**Conclusion:** 研究表明ViLLa能够有效地处理视觉内容的理解和自然语言查询的回应，为动物监测提供了一种模块化和透明的方法。

**Abstract:** Monitoring animal populations in natural environments requires systems that
can interpret both visual data and human language queries. This work introduces
ViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for
interpretable animal monitoring. ViLLa integrates three core components: a
visual detection module for identifying animals and their spatial locations in
images, a language parser for understanding natural language queries, and a
symbolic reasoning layer that applies logic-based inference to answer those
queries. Given an image and a question such as "How many dogs are in the
scene?" or "Where is the buffalo?", the system grounds visual detections into
symbolic facts and uses predefined rules to compute accurate answers related to
count, presence, and location. Unlike end-to-end black-box models, ViLLa
separates perception, understanding, and reasoning, offering modularity and
transparency. The system was evaluated on a range of animal imagery tasks and
demonstrates the ability to bridge visual content with structured,
human-interpretable queries.

</details>


### [36] [GraphGSOcc: Semantic and Geometric Graph Transformer for 3D Gaussian Splating-based Occupancy Prediction](https://arxiv.org/abs/2506.14825)
*Ke Song,Yunhe Wu,Chunchit Siu,Huiyuan Xiong*

Main category: cs.CV

> 研究开发了GraphGSOcc模型解决3D高斯渲染的占用预测问题，提升了性能和降低了内存使用。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现有的3D高斯渲染方法中的两个关键问题：统一特征聚集不考虑语义相关性，以及由于缺乏几何约束导致边界模糊，影响了自动驾驶中的3D语义占用预测。

**Method:** 我们提出了GraphGSOcc模型，这是一种结合语义和几何图Transformer的3D高斯渲染占用预测新框架。通过Dual Gaussians Graph Attention，动态构建了两种图结构：一种几何图表基于高斯姿态自适应地计算k近邻搜索半径，实现了大尺度的高斯特征聚集；另一种语义图表通过余弦相似性保留最相关的节点，以编码实例内的语义关系。结合多尺度图注意力框架，低层进行细粒度注意力优化边界细节，高层进行粗粒度注意力建模对象拓扑。

**Result:** 在SurroundOcc数据集的实验实现了24.10%的mIoU，减少了GPU内存至6.1GB，相比GaussianWorld提升了1.97%的mIoU和13.7%的内存使用。

**Conclusion:** GraphGSOcc模型通过结合几何和语义图变换器，有效提升了3D高斯渲染的占用预测准确性和效率，减少了内存使用，展示了在自动驾驶应用中的潜力。

**Abstract:** Addressing the task of 3D semantic occupancy prediction for autonomous
driving, we tackle two key issues in existing 3D Gaussian Splating (3DGS)
methods: (1) unified feature aggregation neglecting semantic correlations among
similar categories and across regions, and (2) boundary ambiguities caused by
the lack of geometric constraints in MLP iterative optimization. We propose the
GraphGSOcc model, a novel framework that combines semantic and geometric graph
Transformer for 3D Gaussian Splating-based Occupancy Prediction. We propose the
Dual Gaussians Graph Attenntion, which dynamically constructs dual graph
structures: a geometric graph adaptively calculating KNN search radii based on
Gaussian poses, enabling large-scale Gaussians to aggregate features from
broader neighborhoods while compact Gaussians focus on local geometric
consistency; a semantic graph retaining top-M highly correlated nodes via
cosine similarity to explicitly encode semantic relationships within and across
instances. Coupled with the Multi-scale Graph Attention framework, fine-grained
attention at lower layers optimizes boundary details, while coarse-grained
attention at higher layers models object-level topology. Experiments on the
SurroundOcc dataset achieve an mIoU of 24.10%, reducing GPU memory to 6.1 GB,
demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to
GaussianWorld

</details>


### [37] [DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning](https://arxiv.org/abs/2506.14827)
*Yifeng Gao,Yifan Ding,Hongyu Su,Juncheng Li,Yunhan Zhao,Lin Luo,Zixing Chen,Li Wang,Xin Wang,Yixu Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

> 本论文介绍了DAVID-X数据集和DAVID-XR1模型，用于增强对AI生成视频的可解释性检测方法，使其成为一个透明且可验证的诊断过程。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法主要将识别AI生成视频视为一个二元分类任务，这限制了识别过程中的信息深度。本文旨在提供更加精细和具有说服力的证据，使审核者和终端用户都能信服。

**Method:** 提出了DAVID-XR1，这是一个视频-语言模型，旨在提供一个可解释的视觉推理链，包括缺陷分类、时空定位和自然语言解释。该方法利用了DAVID-X数据集，此数据集首次为AI生成的视频提供详细的缺陷级、时空注释和书面理由。

**Result:** 研究表明，通过泛化到我们的紧凑型数据集并增强链式思维蒸馏，一个通用的主干网络能够很好地泛化到多种生成器和生成模式。

**Conclusion:** 研究结果强调了可信识别AI生成视频内容的可解释性检测方法的潜力。

**Abstract:** As AI-generated video becomes increasingly pervasive across media platforms,
the ability to reliably distinguish synthetic content from authentic footage
has become both urgent and essential. Existing approaches have primarily
treated this challenge as a binary classification task, offering limited
insight into where or why a model identifies a video as AI-generated. However,
the core challenge extends beyond simply detecting subtle artifacts; it
requires providing fine-grained, persuasive evidence that can convince auditors
and end-users alike. To address this critical gap, we introduce DAVID-X, the
first dataset to pair AI-generated videos with detailed defect-level,
temporal-spatial annotations and written rationales. Leveraging these rich
annotations, we present DAVID-XR1, a video-language model designed to deliver
an interpretable chain of visual reasoning-including defect categorization,
temporal-spatial localization, and natural language explanations. This approach
fundamentally transforms AI-generated video detection from an opaque black-box
decision into a transparent and verifiable diagnostic process. We demonstrate
that a general-purpose backbone, fine-tuned on our compact dataset and enhanced
with chain-of-thought distillation, achieves strong generalization across a
variety of generators and generation modes. Our results highlight the promise
of explainable detection methods for trustworthy identification of AI-generated
video content.

</details>


### [38] [Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review](https://arxiv.org/abs/2506.14831)
*Céline Finet,Stephane Da Silva Martins,Jean-Bernard Hayet,Ioannis Karamouzas,Javad Amirian,Sylvie Le Hégarat-Mascle,Julien Pettré,Emanuel Aldea*

Main category: cs.CV

> 这篇论文综述了2020年至2024年之间发布的深度学习在多智能体轨迹预测上的最新进展，重点分析了这些模型的架构设计、输入表示及预测策略，并集中在ETH/UCY基准上的模型评估，还指出了该领域的关键挑战和未来研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于通过综述近年来深度学习在多智能体轨迹预测方面的进展，以深化对多智能体交互的理解并推动自动驾驶导航和人群建模等领域的技术进步。

**Method:** 论文通过分类现有的方法，基于它们的架构设计、输入表示和整体预测策略进行综述，特别是关注在ETH/UCY基准上的实验评估。

**Result:** 结果部分重点讨论了分类的模型和策略，以及它们在多智能体轨迹预测领域中的应用。

**Conclusion:** 结论指出，尽管在多智能体轨迹预测方面有显著进展，但仍面临诸多挑战，提出了未来的研究方向。

**Abstract:** With the emergence of powerful data-driven methods in human trajectory
prediction (HTP), gaining a finer understanding of multi-agent interactions
lies within hand's reach, with important implications in areas such as
autonomous navigation and crowd modeling. This survey reviews some of the most
recent advancements in deep learning-based multi-agent trajectory prediction,
focusing on studies published between 2020 and 2024. We categorize the existing
methods based on their architectural design, their input representations, and
their overall prediction strategies, placing a particular emphasis on models
evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges
and future research directions in the field of multi-agent HTP.

</details>


### [39] [ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes](https://arxiv.org/abs/2506.14832)
*Jun Yin,Jing Zhong,Pengyu Zeng,Peilin Li,Zixuan Dai,Miao Zhang,Shuai Lu*

Main category: cs.CV

> 该研究提出了一种新的模型ArchShapeNet，用于分类和分析建筑3D形式，并展示了它在区分人类设计和机器生成形式方面的优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 在当代建筑设计中，设计需求的复杂性和多样性日益增长，使得生成式插件工具对于快速产生初步概念和探索新型3D形式变得至关重要。然而，客观分析人类设计和机器生成的3D形式之间的差异仍然是一个挑战，这限制了我们对各自优势的理解，并阻碍了生成式工具的发展。

**Method:** 构建了ArchForms-4000数据集，包含2000个人类设计的和2000个Evomass生成的3D形式；提出了ArchShapeNet，一种专门用于分类和分析建筑形状的3D卷积神经网络，包含了一个能够突出关键空间特征的显着性模块，这些特征与建筑设计逻辑相匹配；进行了对比实验，结果显示模型在区分形式来源方面优于人类专家，准确率为94.29%，精确率为96.2%，召回率为98.51%。

**Result:** 研究表明，模型在区分形状来源方面优于人类专家，达到94.29%的准确率，96.2%的精确率和98.51%的召回率。

**Conclusion:** 该研究不仅显现了人类设计形式在空间组织、比例和谐及细节优化中的独特优势，也为未来增强生成设计工具提供了宝贵见解。

**Abstract:** In contemporary architectural design, the growing complexity and diversity of
design demands have made generative plugin tools essential for quickly
producing initial concepts and exploring novel 3D forms. However, objectively
analyzing the differences between human-designed and machine-generated 3D forms
remains a challenge, limiting our understanding of their respective strengths
and hindering the advancement of generative tools.
  To address this, we built ArchForms-4000, a dataset containing 2,000
architect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet,
a 3D convolutional neural network tailored for classifying and analyzing
architectural forms, incorporating a saliency module to highlight key spatial
features aligned with architectural reasoning; And conducted comparative
experiments showing our model outperforms human experts in distinguishing form
origins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall.
  This study not only highlights the distinctive advantages of human-designed
forms in spatial organization, proportional harmony, and detail refinement but
also provides valuable insights for enhancing generative design tools in the
future.

</details>


### [40] [Real-Time, Low-Latency Surveillance Using Entropy-Based Adaptive Buffering and MobileNetV2 on Edge Devices](https://arxiv.org/abs/2506.14833)
*Poojashree Chandrashekar Pankaj M Sajjanar*

Main category: cs.CV

> The paper introduces a high-performance, low-latency surveillance system adapted for resource-limited devices with high accuracy in varying conditions, offering scalability, cost-effectiveness, and better privacy.

<details>
  <summary>Details</summary>

**Motivation:** The aim is to develop a scalable, inexpensive, and privacy-compliant video surveillance system suitable for smart cities or embedded security architecture, designed for use on limited resource devices.

**Method:** This paper proposes a formal entropy-based adaptive frame buffering algorithm integrated with MobileNetV2 for high-performance and low-latency video surveillance.

**Result:** The system maintains over 92% detection accuracy on standard datasets and demonstrates robustness with varying conditions. It operates with sub-50ms latency on resource-constrained devices.

**Conclusion:** The proposed system offers high throughput, low latency, and robust surveillance capabilities on resource-limited devices, making it suitable for smart city and embedded security scenarios.

**Abstract:** This paper describes a high-performance, low-latency video surveillance
system designed for resource-constrained environments. We have proposed a
formal entropy-based adaptive frame buffering algorithm and integrated that
with MobileNetV2 to achieve high throughput with low latency. The system is
capable of processing live streams of video with sub-50ms end-to-end inference
latency on resource-constrained devices (embedding platforms) such as Raspberry
Pi, Amazon, and NVIDIA Jetson Nano. Our method maintains over 92% detection
accuracy on standard datasets focused on video surveillance and exhibits
robustness to varying lighting, backgrounds, and speeds. A number of
comparative and ablation experiments validate the effectiveness of our design.
Finally, our architecture is scalable, inexpensive, and compliant with stricter
data privacy regulations than common surveillance systems, so that the system
could coexist in a smart city or embedded security architecture.

</details>


### [41] [MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation](https://arxiv.org/abs/2506.14835)
*Kiet Dang Vu,Trung Thai Tran,Duc Dung Nguyen*

Main category: cs.CV

> 论文介绍了 MonoVQD，一种用于改进 DETR 基于的单目 3D 检测框架，引入了新的自注意力机制和去噪技术，并提出了一种高级的自我蒸馏策略，从而在多个数据集上提升了 3D 检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于 DETR 类似架构直接应用于单目 3D 检测时遇到固有限制，无法达到最优性能，因此提出了 MonoVQD 以克服这些挑战。

**Method:** Mask Separated Self-Attention 机制和 Variational Query Denoising 技术被引入来改进 DETR 框架，以增强其在单目 3D 检测中的性能。此外，还开发了一种高级自我蒸馏策略来进一步优化查询质量。

**Result:** 实验表明，MonoVQD 在具有挑战性的 KITTI 单目基准测试中达到了优越的性能。同时，MonoVQD 的核心组件也可以无缝整合到其他架构中，甚至在 nuScenes 数据集中的多视角 3D 检测场景中也展现出了显著的性能提升。

**Conclusion:** MonoVQD 的提出不仅提升了单目 3D 检测的效果，而且展示了其广泛的适用性和良好的泛化能力。

**Abstract:** Precisely localizing 3D objects from a single image constitutes a central
challenge in monocular 3D detection. While DETR-like architectures offer a
powerful paradigm, their direct application in this domain encounters inherent
limitations, preventing optimal performance. Our work addresses these
challenges by introducing MonoVQD, a novel framework designed to fundamentally
advance DETR-based monocular 3D detection. We propose three main contributions.
First, we propose the Mask Separated Self-Attention mechanism that enables the
integration of the denoising process into a DETR architecture. This improves
the stability of Hungarian matching to achieve a consistent optimization
objective. Second, we present the Variational Query Denoising technique to
address the gradient vanishing problem of conventional denoising methods, which
severely restricts the efficiency of the denoising process. This explicitly
introduces stochastic properties to mitigate this fundamental limitation and
unlock substantial performance gains. Finally, we introduce a sophisticated
self-distillation strategy, leveraging insights from later decoder layers to
synergistically improve query quality in earlier layers, thereby amplifying the
iterative refinement process. Rigorous experimentation demonstrates that
MonoVQD achieves superior performance on the challenging KITTI monocular
benchmark. Highlighting its broad applicability, MonoVQD's core components
seamlessly integrate into other architectures, delivering significant
performance gains even in multi-view 3D detection scenarios on the nuScenes
dataset and underscoring its robust generalization capabilities.

</details>


### [42] [Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction](https://arxiv.org/abs/2506.14837)
*Chengzhi Xu,Yuyang Wang,Lai Wei,Lichao Sun,Weiran Huang*

Main category: cs.CV

> 本文提出了ChartIR方法，该方法通过区分视觉理解和代码翻译两个子任务，并引入结构化指令，有效提升了多模态大语言模型在图表生成任务上的表现，实验结果显示其在对比方法中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大语言模型在各种视觉任务中表现出色，但在图表生成任务上的性能仍然不尽人意。这促使研究人员提出ChartIR方法，以改进图表理解和代码翻译的精确性。

**Method:** ChartIR方法通过对结构化指令的设计，实现了图表生成任务的逐步优化。该方法首先将视觉理解和代码翻译两个任务区分开，并设计了描述和差异两种结构化指令来完成视觉理解阶段。描述指令用于捕捉参考图表的视觉元素，差异指令用于描述参考图表与生成图表之间的差异。通过这种方式，能够有效地将视觉特征转化为语言表示，进而促进后續的代码翻译流程。其次，该方法将整体的图表生成管道分解成初始代码生成和迭代优化两个阶段，以逐步提升最终输出的质量。

**Result:** 实验结果表明，在开源模型Qwen2-VL和闭源模型GPT-4o上，相比于其他方法，ChartIR方法均展现出了更优的性能。

**Conclusion:** ChartIR方法通过结构化指令实现了对图表生成任务的优化，证明了其在提升视觉理解和代码翻译任务中的有效性。

**Abstract:** Recently, multimodal large language models (MLLMs) have attracted increasing
research attention due to their powerful visual understanding capabilities.
While they have achieved impressive results on various vision tasks, their
performance on chart-to-code generation remains suboptimal. This task requires
MLLMs to generate executable code that can reproduce a given chart, demanding
not only precise visual understanding but also accurate translation of visual
elements into structured code. Directly prompting MLLMs to perform this complex
task often yields unsatisfactory results. To address this challenge, we propose
{ChartIR}, an iterative refinement method based on structured instruction.
First, we distinguish two tasks: visual understanding and code translation. To
accomplish the visual understanding component, we design two types of
structured instructions: description and difference. The description
instruction captures the visual elements of the reference chart, while the
difference instruction characterizes the discrepancies between the reference
chart and the generated chart. These instructions effectively transform visual
features into language representations, thereby facilitating the subsequent
code translation process. Second, we decompose the overall chart generation
pipeline into two stages: initial code generation and iterative refinement,
enabling progressive enhancement of the final output. Experimental results show
that, compared to other method, our method achieves superior performance on
both the open-source model Qwen2-VL and the closed-source model GPT-4o.

</details>


### [43] [PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers](https://arxiv.org/abs/2506.14842)
*Lukas Schiesser,Cornelius Wolff,Sophie Haas,Simon Pukrop*

Main category: cs.CV

> PictSure框架优化嵌入模型的预训练和训练方式，显着提升了少量样本图像分类在跨域任务中的表现，同时在域内任务中表现良好。

<details>
  <summary>Details</summary>

**Motivation:** 本文的研究动机在于解决数据稀缺领域构建图像分类模型时的挑战，并通过在上下文学习（ICL）框架下优化图像嵌入模型的性能来提升少量样本图像分类（FSIC）的能力。

**Method:** 本文通过PictSure框架系统性地分析了不同视觉编码器类型、预训练目标和微调策略对下游少量样本图像分类任务性能的影响。PictSure框架将嵌入模型及其架构、预训练和训练动态作为中心分析对象。

**Result:** 实验结果显示，训练成功和跨域性能的提升高度依赖于嵌入模型的预训练方式。PictSure可以超越现有的ICL基础FSIC模型在显著不同于训练分布的跨域基准测试上的表现，同时在域内任务上保持可比的结果。

**Conclusion:** 研究表明，正确的预训练策略能够大大提升嵌入模型在少量样本（few-shot）分类中的表现。PictSure通过优化这些策略，证明了其在跨域图像分类任务上的优越性能。

**Abstract:** Building image classification models remains cumbersome in data-scarce
domains, where collecting large labeled datasets is impractical. In-context
learning (ICL) has emerged as a promising paradigm for few-shot image
classification (FSIC), enabling models to generalize across domains without
gradient-based adaptation. However, prior work has largely overlooked a
critical component of ICL-based FSIC pipelines: the role of image embeddings.
In this work, we present PictSure, an ICL framework that places the embedding
model -- its architecture, pretraining, and training dynamics -- at the center
of analysis. We systematically examine the effects of different visual encoder
types, pretraining objectives, and fine-tuning strategies on downstream FSIC
performance. Our experiments show that the training success and the
out-of-domain performance are highly dependent on how the embedding models are
pretrained. Consequently, PictSure manages to outperform existing ICL-based
FSIC models on out-of-domain benchmarks that differ significantly from the
training distribution, while maintaining comparable results on in-domain tasks.
Code can be found at https://github.com/PictSure/pictsure-library.

</details>


### [44] [Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach](https://arxiv.org/abs/2506.14846)
*Shreyas Rajeev,B Sathish Babu*

Main category: cs.CV

> 本文提出了BKSEF，一个数学上和经验上被验证的框架，用于确定CNN中层间的最优核尺寸，实验结果表明相比传统的使用统一3x3核的模型，BKSEF指导的架构在准确性和FLOPs减少方面都有所提高。

<details>
  <summary>Details</summary>

**Motivation:** 卷积神经网络（CNN）中的核尺寸选择是一个关键但常被忽视的设计决策，它影响感受野、特征提取、计算成本和模型准确性。

**Method:** 提出了最佳核尺寸估算函数（BKSEF），通过结合信息论、信号处理和学习理论的原则，实现层间最优核尺寸的确定，平衡信息增益、计算效率和准确性提高。

**Result:** 在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上的实验表明，BKSEF指导的架构相比传统模型实现了最高3.1%的准确度提升和42.8%的FLOPs减少。而且，两个实际案例研究--一个用于基于云的医学图像分类，另一个用于边缘设备上的交通标志识别--进一步验证了这种方法的有效性。

**Conclusion:** BKSEF提供了一种在CNN优化方面的全新视角，适用于集成到神经架构搜索管道和实时系统中。

**Abstract:** Kernel size selection in Convolutional Neural Networks (CNNs) is a critical
but often overlooked design decision that affects receptive field, feature
extraction, computational cost, and model accuracy. This paper proposes the
Best Kernel Size Estimation Function (BKSEF), a mathematically grounded and
empirically validated framework for optimal, layer-wise kernel size
determination. BKSEF balances information gain, computational efficiency, and
accuracy improvements by integrating principles from information theory, signal
processing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100,
ImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided
architectures achieve up to 3.1 percent accuracy improvement and 42.8 percent
reduction in FLOPs compared to traditional models using uniform 3x3 kernels.
Two real-world case studies further validate the approach: one for medical
image classification in a cloud-based setup, and another for traffic sign
recognition on edge devices. The former achieved enhanced interpretability and
accuracy, while the latter reduced latency and model size significantly, with
minimal accuracy trade-off. These results show that kernel size can be an
active, optimizable parameter rather than a fixed heuristic. BKSEF provides
practical heuristics and theoretical support for researchers and developers
seeking efficient and application-aware CNN designs. It is suitable for
integration into neural architecture search pipelines and real-time systems,
offering a new perspective on CNN optimization.

</details>


### [45] [Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis](https://arxiv.org/abs/2506.14854)
*Varun Mannam,Zhenyu Shi*

Main category: cs.CV

> 简而言之：本文提出一种基于深度学习的算法来自动识别零售视频中的关键帧并进行标注，这比传统的人工方法更加高效同时节省成本。

<details>
  <summary>Details</summary>

**Motivation:** 动机：传统视频注释方法依赖人工标注，导致成本高、效率低和标注不准确。本文旨在通过自动化视频关键帧识别和自动注释减少运营成本并提高效率。

**Method:** 方法：本文利用深度神经网络从视频帧中学习判别特征，并采用专为零售环境设计的对象检测技术，实现零售视频中关键帧的自动识别与注释。

**Result:** 内容概述：本文提出了一种基于深度学习的自动关键帧识别方法，用于零售视频中的产品和客户自动注释，这种方法解决了传统方法中人工标注耗时、成本高的问题。实验表明，该方法的准确率可与人工标注相媲美，并且能够大幅提高零售视频标注的工作效率，平均节省两倍的标注成本。通过仅让人工标注员审核/调整不到5%的检测到的帧，该方法实现了高效且高质量的自动注释。这项技术对多种零售应用具有重大价值，如顾客行为分析、产品互动检测和店内安全监控。

**Conclusion:** 结论：提出的自动化方法能够显著提高零售视频标注的效率，并确保与人工标注相同的高质量。通过自动化的关键帧检测，显著提升了时间和人力成本效益，为多种零售应用提供了高价值的解决方案。

**Abstract:** Accurate video annotation plays a vital role in modern retail applications,
including customer behavior analysis, product interaction detection, and
in-store activity recognition. However, conventional annotation methods heavily
rely on time-consuming manual labeling by human annotators, introducing
non-robust frame selection and increasing operational costs. To address these
challenges in the retail domain, we propose a deep learning-based approach that
automates key-frame identification in retail videos and provides automatic
annotations of products and customers. Our method leverages deep neural
networks to learn discriminative features by embedding video frames and
incorporating object detection-based techniques tailored for retail
environments. Experimental results showcase the superiority of our approach
over traditional methods, achieving accuracy comparable to human annotator
labeling while enhancing the overall efficiency of retail video annotation.
Remarkably, our approach leads to an average of 2 times cost savings in video
annotation. By allowing human annotators to verify/adjust less than 5% of
detected frames in the video dataset, while automating the annotation process
for the remaining frames without reducing annotation quality, retailers can
significantly reduce operational costs. The automation of key-frame detection
enables substantial time and effort savings in retail video labeling tasks,
proving highly valuable for diverse retail applications such as shopper journey
analysis, product interaction detection, and in-store security monitoring.

</details>


### [46] [Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction](https://arxiv.org/abs/2506.14856)
*Zhengquan Zhang,Feng Xu,Mengmi Zhang*

Main category: cs.CV

> 本文提出一种基于UPNet预测的神经不确定性地图引导的新AVS方法，实现了减少计算开销和保持高质量重建，同时具有良好的新对象类别的概括性。

<details>
  <summary>Details</summary>

**Motivation:** 主动视点选择是对3D重建的基本挑战。作者尝试通过提出一个新的方法来确定构建精准且高效的3D物体重建，最少视点组合提供了最大的信息。这一方法改变了现有的神经网络，从当前观察学出辐射场并计算每个候选视点的不确定性。

**Method:** 本文提出了一种新的主动视点选择（AVS）方法，该方法由UPNet预测的神经不确定地图引导。UPNet是一个轻量级的前馈深度神经网络，它接收3D物体的单个输入图像，并输出表示所有可能候选视点的不确定值的预测不确定地图。通过利用来自观察许多自然物体及其相关不确定性模式的启发式方法，训练UPNet学习直接将视点外观映射到底层体积表示的不确定性。然后，该方法聚合所有已预测的神经不确定地图，压缩冗余的候选视点，有效地选择最具信息量的视点。

**Result:** 相较于基线方法，使用新方法最多可以降低计算负担400倍，并且可以减少50%的CPU、RAM和GPU的使用。此方法在使用仅为上界一半的视点情况下仍能达到可比较的重建准确性。

**Conclusion:** 研究表明，由于使用UPNet预测视点不确定性，该方法能够减少计算负担，并有效选择最具信息量的视点，提高了3D重建效率和质量。而无需额外训练，该方法具有良好的通用性。

**Abstract:** Some perspectives naturally provide more information than others. How can an
AI system determine which viewpoint offers the most valuable insight for
accurate and efficient 3D object reconstruction? Active view selection (AVS)
for 3D reconstruction remains a fundamental challenge in computer vision. The
aim is to identify the minimal set of views that yields the most accurate 3D
reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian
Splatting, from a current observation and computing uncertainty for each
candidate viewpoint, we introduce a novel AVS approach guided by neural
uncertainty maps predicted by a lightweight feedforward deep neural network,
named UPNet. UPNet takes a single input image of a 3D object and outputs a
predicted uncertainty map, representing uncertainty values across all possible
candidate viewpoints. By leveraging heuristics derived from observing many
natural objects and their associated uncertainty patterns, we train UPNet to
learn a direct mapping from viewpoint appearance to uncertainty in the
underlying volumetric representations. Next, our approach aggregates all
previously predicted neural uncertainty maps to suppress redundant candidate
viewpoints and effectively select the most informative one. Using these
selected viewpoints, we train 3D neural rendering models and evaluate the
quality of novel view synthesis against other competitive AVS methods.
Remarkably, despite using half of the viewpoints than the upper bound, our
method achieves comparable reconstruction accuracy. In addition, it
significantly reduces computational overhead during AVS, achieving up to a 400
times speedup along with over 50\% reductions in CPU, RAM, and GPU usage
compared to baseline methods. Notably, our approach generalizes effectively to
AVS tasks involving novel object categories, without requiring any additional
training.

</details>


### [47] [DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization](https://arxiv.org/abs/2506.14903)
*Renjith Prasad,Abhilekh Borah,Hasnat Md Abdullah,Chathurangi Shyalika,Gurpreet Singh,Ritvik Garimella,Rajarshi Roy,Harshul Surana,Nasrin Imanpour,Suranjana Trivedy,Amit Sheth,Amitava Das*

Main category: cs.CV

> DPO-Kernels introduced for T2I models to enhance alignment with improved optimization techniques, evaluated on a large-scale benchmark DETONATE.

<details>
  <summary>Details</summary>

**Motivation:** To improve alignment in text-to-image models, ensuring safety and fairness, by extending the principles of Direct Preference Optimization (DPO) used in large language models.

**Method:** DPO-Kernels, a novel extension to enhance alignment in T2I models, introduces three dimensions: Hybrid Loss, Kernelized Representations, and Divergence Selection.

**Result:** The performance of DPO-Kernels is evaluated on DETONATE, a large-scale benchmark, and it maintains strong generalization bounds via Heavy-Tailed Self-Regularization (HT-SR).

**Conclusion:** DPO-Kernels effectively improve alignment in T2I models, addressing safety and fairness through the introduction of new optimization techniques and a novel benchmark.

**Abstract:** Alignment is crucial for text-to-image (T2I) models to ensure that generated
images faithfully capture user intent while maintaining safety and fairness.
Direct Preference Optimization (DPO), prominent in large language models
(LLMs), is extending its influence to T2I systems. This paper introduces
DPO-Kernels for T2I models, a novel extension enhancing alignment across three
dimensions: (i) Hybrid Loss, integrating embedding-based objectives with
traditional probability-based loss for improved optimization; (ii) Kernelized
Representations, employing Radial Basis Function (RBF), Polynomial, and Wavelet
kernels for richer feature transformations and better separation between safe
and unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's
default Kullback-Leibler (KL) regularizer by incorporating Wasserstein and
R'enyi divergences for enhanced stability and robustness. We introduce
DETONATE, the first large-scale benchmark of its kind, comprising approximately
100K curated image pairs categorized as chosen and rejected. DETONATE
encapsulates three axes of social bias and discrimination: Race, Gender, and
Disability. Prompts are sourced from hate speech datasets, with images
generated by leading T2I models including Stable Diffusion 3.5 Large, Stable
Diffusion XL, and Midjourney. Additionally, we propose the Alignment Quality
Index (AQI), a novel geometric measure quantifying latent-space separability of
safe/unsafe image activations, revealing hidden vulnerabilities. Empirically,
we demonstrate that DPO-Kernels maintain strong generalization bounds via
Heavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are
publicly released.

</details>


### [48] [PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2506.14907)
*Yizhen Zhang,Yang Ding,Shuoshuo Zhang,Xinchen Zhang,Haoling Li,Zhong-zhi Li,Peijie Wang,Jie Wu,Lei Ji,Yelong Shen,Yujiu Yang,Yeyun Gong*

Main category: cs.CV

> 为了应对现有的多模态强化学习方法难以推广到多图像位置推理问题，作者提出了新方法PeRL，展示其在多图像基准测试上达到最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态强化学习方法仍然局限于单图像情境中的空间推理，难以推广到涉及多图像位置推理的更复杂、更真实的场景。这些场景需要理解图像之间的关系。为了应对这一挑战，提出了PeRL方法。

**Method:** 文中提出了一种针对交错多模态任务的通用强化学习方法PeRL，以及多阶段策略来优化探索与利用之间的权衡，提高学习效率和任务性能。具体来说，引入了图像序列的排列来模拟不同的位置关系，探索更多的空间和位置多样性，并设计了一种回放缓冲过滤机制来重新采样，专注于有助于学习最优行为的轨迹。

**Result:** 在五个广泛使用的多图像基准测试和三个单图像基准测试上对模型进行了评估。实验结果证实，经过PeRL训练的模型在多图像基准测试上超过了R1相关和其他交错VLM基线，实现了最先进的性能，同时在单图像任务上保持了相当的性能。

**Conclusion:** PeRL方法成功地提升了在涉及多图像位置推理任务上的性能，不仅在多图像基准上表现优异，也在单图像任务中保持了竞争力。

**Abstract:** Inspired by the impressive reasoning capabilities demonstrated by
reinforcement learning approaches like DeepSeek-R1, recent emerging research
has begun exploring the use of reinforcement learning (RL) to enhance
vision-language models (VLMs) for multimodal reasoning tasks. However, most
existing multimodal reinforcement learning approaches remain limited to spatial
reasoning within single-image contexts, yet still struggle to generalize to
more complex and real-world scenarios involving multi-image positional
reasoning, where understanding the relationships across images is crucial. To
address this challenge, we propose a general reinforcement learning approach
PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy
designed to enhance the exploration-exploitation trade-off, thereby improving
learning efficiency and task performance. Specifically, we introduce
permutation of image sequences to simulate varied positional relationships to
explore more spatial and positional diversity. Furthermore, we design a rollout
filtering mechanism for resampling to focus on trajectories that contribute
most to learning optimal behaviors to exploit learned policies effectively. We
evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image
benchmarks. Our experiments confirm that PeRL trained model consistently
surpasses R1-related and interleaved VLM baselines by a large margin, achieving
state-of-the-art performance on multi-image benchmarks, while preserving
comparable performance on single-image tasks.

</details>


### [49] [Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models](https://arxiv.org/abs/2506.14919)
*Xinkai Zhao,Yuta Tokuoka,Junichiro Iwasawa,Keita Oda*

Main category: cs.CV

> 论文提出了一种新的会员推断攻击（MIA）方法，即频率校准重建误差（FCRE），专门用于医学图像扩散模型，该方法通过专注于中频重建误差，提高了隐私风险的量化能力。实验表明该方法优于现有的MIA方法。

<details>
  <summary>Details</summary>

**Motivation:** 随着扩散模型在图像生成，尤其是在医学成像等敏感领域的应用增加，隐私问题变得越来越显著。会员推断攻击（MIA）作为一种方法，被用来确定特定的图像是否被用来训练扩散模型，从而量化隐私风险。然而，在将这些方法应用于医学图像时遇到了挑战。现有方法通常依赖于扩散重建误差，其中成员图像的重建误差预期低于非成员图像。然而，重建误差受图像本身难度的影响，而且扩散模型在高频率细节重建方面存在困难。为此，作者提出了针对医学图像扩散模型的频率校准重建误差（FCRE）方法来解决这些问题。

**Method:** 作者提出了一种针对医学图像扩散模型的频率校准重建误差（FCRE）方法，通过专注于特定中频段的重建误差，排除高频率（难以重建）和低频率（不具信息量）的区域，来减轻图像内在难度影响。具体地说，他们分析了逆扩散过程，获得了中频重建误差，并计算了重建图像和原始图像之间的结构相似性指数得分。

**Result:** 实验结果表明，作者提出的FCRE方法在几个医学图像数据集上优于现有的MIA方法，其效果在相同设定下得以验证。

**Conclusion:** 作者通过引入频率校准重建误差（FCRE）方法，解决了现有MIA方法在医学图像扩散模型中应用时遇到的问题，证明了该方法在提升隐私风险量化能力方面具有优势。

**Abstract:** The increasing use of diffusion models for image generation, especially in
sensitive areas like medical imaging, has raised significant privacy concerns.
Membership Inference Attack (MIA) has emerged as a potential approach to
determine if a specific image was used to train a diffusion model, thus
quantifying privacy risks. Existing MIA methods often rely on diffusion
reconstruction errors, where member images are expected to have lower
reconstruction errors than non-member images. However, applying these methods
directly to medical images faces challenges. Reconstruction error is influenced
by inherent image difficulty, and diffusion models struggle with high-frequency
detail reconstruction. To address these issues, we propose a
Frequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical
image diffusion models. By focusing on reconstruction errors within a specific
mid-frequency range and excluding both high-frequency (difficult to
reconstruct) and low-frequency (less informative) regions, our
frequency-selective approach mitigates the confounding factor of inherent image
difficulty. Specifically, we analyze the reverse diffusion process, obtain the
mid-frequency reconstruction error, and compute the structural similarity index
score between the reconstructed and original images. Membership is determined
by comparing this score to a threshold. Experiments on several medical image
datasets demonstrate that our FCRE method outperforms existing MIA methods.

</details>


### [50] [Vision Transformers for End-to-End Quark-Gluon Jet Classification from Calorimeter Images](https://arxiv.org/abs/2506.14934)
*Md Abrar Jahin,Shahriar Soudeep,Arian Rahman Aditta,M. F. Mridha,Nafiz Fahad,Md. Jakir Hossen*

Main category: cs.CV

> 该论文评估了Vision Transformer (ViT)及与其结合的CNN混合模型对喷注分类的性能，实验证明ViT模型由于能够更好地捕获喷注子结构的长距离空间关联性，其优于现有的CNN基线模型。

<details>
  <summary>Details</summary>

**Motivation:** 区别夸克和胶子引发的喷注对于大型强子对撞机中的新物理学搜索和精确测量来说至关重要但充满挑战。虽然深度学习，特别是使用基于图像表示方式的卷积神经网络，已经推动了喷注标记的进步，但ViT架构在直接分析计数器图像下特别是在模拟的检测器和堆积条件下的潜力尚未得到充分探索。

**Method:** 该论文通过使用Vision Transformer (ViT)及其与Convolutional Neural Networks (CNN)的混合模型，对夸克-胶子喷注分类进行了系统评估。采用模拟的2012年CMS开放数据集，构建了来自探测器级别能量沉积（电磁量能器、强子量能器）和重建轨迹的多通道喷注视图图像，实现了端到端学习方法。

**Result:** 论文表明基于ViT的模型，特别是ViT+MaxViT和ViT+ConvNeXt混合模型，在F1得分、ROC-AUC和准确性方面始终优于现有的CNN基线模型。这表明ViT模型在捕捉喷注子结构中的长距离空间相关性方面具有显著优势。

**Conclusion:** 该工作为使用公共碰撞数据应用ViT架构对计数器图像基于的喷注分类建立了第一个系统框架，并建立了稳健的性能基准。同时也构建了适合该领域进一步深度学习研究的结构化数据集。

**Abstract:** Distinguishing between quark- and gluon-initiated jets is a critical and
challenging task in high-energy physics, pivotal for improving new physics
searches and precision measurements at the Large Hadron Collider. While deep
learning, particularly Convolutional Neural Networks (CNNs), has advanced jet
tagging using image-based representations, the potential of Vision Transformer
(ViT) architectures, renowned for modeling global contextual information,
remains largely underexplored for direct calorimeter image analysis, especially
under realistic detector and pileup conditions. This paper presents a
systematic evaluation of ViTs and ViT-CNN hybrid models for quark-gluon jet
classification using simulated 2012 CMS Open Data. We construct multi-channel
jet-view images from detector-level energy deposits (ECAL, HCAL) and
reconstructed tracks, enabling an end-to-end learning approach. Our
comprehensive benchmarking demonstrates that ViT-based models, notably
ViT+MaxViT and ViT+ConvNeXt hybrids, consistently outperform established CNN
baselines in F1-score, ROC-AUC, and accuracy, highlighting the advantage of
capturing long-range spatial correlations within jet substructure. This work
establishes the first systematic framework and robust performance baselines for
applying ViT architectures to calorimeter image-based jet classification using
public collider data, alongside a structured dataset suitable for further deep
learning research in this domain.

</details>


### [51] [Advances in Compliance Detection: Novel Models Using Vision-Based Tactile Sensors](https://arxiv.org/abs/2506.14980)
*Ziteng Li,Malte Kuhlmann,Ilana Nisky,Nicolás Navarro-Guerrero*

Main category: cs.CV

> 本研究提出两种基于长短期递归卷积网络（LRCNs）和Transformer架构的模型，结合RGB触觉图像和其他GelSight传感器信息来准确预测合规性指标，模型在多个评估指标上表现优于基线模型，特别是对于比传感器更硬的对象的预测更具挑战性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的合规性检测方法存在便携性和可扩展性问题，且依赖昂贵的专用设备；现有基于视觉的触觉传感器的神经网络方法预测精度不足。

**Method:** 研究设计了两种模型，分别基于LRCNs和Transformer架构，利用RGB触觉图像及其他GelSight传感器捕获的信息进行合规性指标预测。

**Result:** 实验结果表明，这两个模型在多个评估指标上的表现优于基线模型。

**Conclusion:** 提出的模型对于预测合规性指标表现出优越的性能，特别是在探索传感器与对象合规性之间的相关性时发现，对于比传感器硬度大的对象，其合规性更难以预测。

**Abstract:** Compliance is a critical parameter for describing objects in engineering,
agriculture, and biomedical applications. Traditional compliance detection
methods are limited by their lack of portability and scalability, rely on
specialized, often expensive equipment, and are unsuitable for robotic
applications. Moreover, existing neural network-based approaches using
vision-based tactile sensors still suffer from insufficient prediction
accuracy. In this paper, we propose two models based on Long-term Recurrent
Convolutional Networks (LRCNs) and Transformer architectures that leverage RGB
tactile images and other information captured by the vision-based sensor
GelSight to predict compliance metrics accurately. We validate the performance
of these models using multiple metrics and demonstrate their effectiveness in
accurately estimating compliance. The proposed models exhibit significant
performance improvement over the baseline. Additionally, we investigated the
correlation between sensor compliance and object compliance estimation, which
revealed that objects that are harder than the sensor are more challenging to
estimate.

</details>


### [52] [Hyper-Local Deformable Transformers for Text Spotting on Historical Maps](https://arxiv.org/abs/2506.15010)
*Yijun Lin,Yao-Yi Chiang*

Main category: cs.CV

> PALETTE is developed to enhance text spotting in historical maps, addressing the issues of varied map styles and insufficient training data, and demonstrates superior performance on historical map datasets.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of text extraction from historical maps, including the lack of effective methods and training data, while improving precision in detecting text in complex, varied map styles.

**Method:** PALETTE, a novel text spotting model that introduces a hyper-local sampling module for learning localized image features and positional embeddings for spatial interactions.

**Result:** PALETTE with SynthMap+ outperforms state-of-the-art text spotters on new historical map benchmark datasets and processes a large number of maps.

**Conclusion:** PALETTE effectively spots text in historical maps with high precision, especially for long and angled text, facilitating the processing of extensive map collections and enabling more effective map searching.

**Abstract:** Text on historical maps contains valuable information providing georeferenced
historical, political, and cultural contexts. However, text extraction from
historical maps is challenging due to the lack of (1) effective methods and (2)
training data. Previous approaches use ad-hoc steps tailored to only specific
map styles. Recent machine learning-based text spotters (e.g., for scene
images) have the potential to solve these challenges because of their
flexibility in supporting various types of text instances. However, these
methods remain challenges in extracting precise image features for predicting
every sub-component (boundary points and characters) in a text instance. This
is critical because map text can be lengthy and highly rotated with complex
backgrounds, posing difficulties in detecting relevant image features from a
rough text region. This paper proposes PALETTE, an end-to-end text spotter for
scanned historical maps of a wide variety. PALETTE introduces a novel
hyper-local sampling module to explicitly learn localized image features around
the target boundary points and characters of a text instance for detection and
recognition. PALETTE also enables hyper-local positional embeddings to learn
spatial interactions between boundary points and characters within and across
text instances. In addition, this paper presents a novel approach to
automatically generate synthetic map images, SynthMap+, for training text
spotters for historical maps. The experiment shows that PALETTE with SynthMap+
outperforms SOTA text spotters on two new benchmark datasets of historical
maps, particularly for long and angled text. We have deployed PALETTE with
SynthMap+ to process over 60,000 maps in the David Rumsey Historical Map
collection and generated over 100 million text labels to support map searching.
The project is released at
https://github.com/kartta-foundation/mapkurator-palette-doc.

</details>


### [53] [Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?](https://arxiv.org/abs/2506.15033)
*Gary Song Yan,Yusen Zhang,Jinyu Zhao,Hao Zhang,Zhangping Yang,Guanye Xiong,Yanfei Liu,Tao Zhang,Yujie He,Siyuan Tian,Yao Gou,Min Li*

Main category: cs.CV

> 研究介绍了一个创新的统一风格转移框架StyleWallfacer，利用多项策略实现了高质量的风格转移，保持原图内容，并首次实现了在风格转移过程中的颜色编辑。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服传统风格转移方法中存在的种种问题，并统一不同任务的框架，该研究引入了一个革命性的统一训练和推理框架。旨在实现艺术家级别的风格转移和基于文本的样式化。

**Method:** 论文中介绍了StyleWallfacer框架，该框架包含三个主要部分：基于语义的风格注入方法、基于人类反馈的数据增强策略、以及无训练三扩散过程。首先，提出了一种基于语义的风格注入方法，通过利用BLIP为其样式图像生成语义描述，并使用大语言模型移除与风格相关的描述从而创建语义差距，以此细化模型。其次，数据增强策略基于人类反馈，将早期训练样本纳入训练集以促进渐进学习。最后，设计了一个无训练的三扩散过程，使用细调后的模型对自注意力层进行特征变换，以实现风格注入同时保持文本控制。此外，还引入查询保持以减少对原始内容的干扰。

**Result:** 该框架在高质量的图像驱动风格转移和文本驱动风格化中取得了艺术家级别的效果，同时保持了原图内容，实现了在风格迁移过程中的图像色彩编辑。

**Conclusion:** StyleWallfacer框架通过一系列创新方法，如基于语义的风格注入、数据增强策略和无训练的三扩散过程，成功实现了高质量的风格转移，并保持了原图内容的完整性，同时还实现了在风格转移过程中的颜色编辑，实现了一次技术上的突破。

**Abstract:** In this pioneering study, we introduce StyleWallfacer, a groundbreaking
unified training and inference framework, which not only addresses various
issues encountered in the style transfer process of traditional methods but
also unifies the framework for different tasks. This framework is designed to
revolutionize the field by enabling artist level style transfer and text driven
stylization. First, we propose a semantic-based style injection method that
uses BLIP to generate text descriptions strictly aligned with the semantics of
the style image in CLIP space. By leveraging a large language model to remove
style-related descriptions from these descriptions, we create a semantic gap.
This gap is then used to fine-tune the model, enabling efficient and drift-free
injection of style knowledge. Second, we propose a data augmentation strategy
based on human feedback, incorporating high-quality samples generated early in
the fine-tuning process into the training set to facilitate progressive
learning and significantly reduce its overfitting. Finally, we design a
training-free triple diffusion process using the fine-tuned model, which
manipulates the features of self-attention layers in a manner similar to the
cross-attention mechanism. Specifically, in the generation process, the key and
value of the content-related process are replaced with those of the
style-related process to inject style while maintaining text control over the
model. We also introduce query preservation to mitigate disruptions to the
original content. Under such a design, we have achieved high-quality
image-driven style transfer and text-driven stylization, delivering
artist-level style transfer results while preserving the original image
content. Moreover, we achieve image color editing during the style transfer
process for the first time.

</details>


### [54] [Enhancing Vector Quantization with Distributional Matching: A Theoretical and Empirical Study](https://arxiv.org/abs/2506.15078)
*Xianghong Fang,Litao Guo,Hengchao Chen,Yuxuan Zhang,XiaofanXia,Dingjie Song,Yexin Liu,Hao Wang,Harry Yang,Yuan Yuan,Qiang Sun*

Main category: cs.CV

> 本文提出了一种新的向量化方法，采用Wasserstein距离来解决训练不稳定性和码本崩溃问题，提高了码本利用率，减小了量化误差。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机在于解决现有向量化技术中的两个关键问题，即训练不稳定性和码本崩溃，这些问题主要由特征分布与码本向量分布之间的不匹配引起。

**Method:** 本文提出使用Wasserstein距离来对齐特征分布和代码向量分布，以解决训练不稳定性和码本崩溃问题，这有助于实现接近100%的码本利用率和显著降低量化误差。

**Result:** 通过对提出的方案进行实证和理论分析，结果显示该方法有效解决了训练不稳定性和码本崩溃的问题。

**Conclusion:** 通过引入Wasserstein距离，论文成功改善了向量化过程中的数据信息保留，验证了该方法的有效性。

**Abstract:** The success of autoregressive models largely depends on the effectiveness of
vector quantization, a technique that discretizes continuous features by
mapping them to the nearest code vectors within a learnable codebook. Two
critical issues in existing vector quantization methods are training
instability and codebook collapse. Training instability arises from the
gradient discrepancy introduced by the straight-through estimator, especially
in the presence of significant quantization errors, while codebook collapse
occurs when only a small subset of code vectors are utilized during training. A
closer examination of these issues reveals that they are primarily driven by a
mismatch between the distributions of the features and code vectors, leading to
unrepresentative code vectors and significant data information loss during
compression. To address this, we employ the Wasserstein distance to align these
two distributions, achieving near 100\% codebook utilization and significantly
reducing the quantization error. Both empirical and theoretical analyses
validate the effectiveness of the proposed approach.

</details>


### [55] [SynPo: Boosting Training-Free Few-Shot Medical Segmentation via High-Quality Negative Prompts](https://arxiv.org/abs/2506.15153)
*Yufei Liu,Haoke Xiao,Jiaxing Chai,Yongcun Zhang,Rong Wang,Zijie Meng,Zhiming Luo*

Main category: cs.CV

> 针对现有基于LVM的零训练方法在低对比度医学图像上表现不佳的问题，提出了一种改进负提示质量的方法SynPo，在零训练下实现了与有训练的少样本方法相当的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于大视觉模型（LVMs）的零训练方法未能有效利用负提示，导致在低对比度的医学图像上表现不佳。因此，提出了一种改进负提示质量的新方法。

**Method:** 通过结合DINOv2和SAM的优势设计了一个新的置信图协同模块，来更可靠地选择点提示。基于置信图，选择前k个像素作为正点集，并使用高斯分布选择负点集，然后分别对两个集合进行独立的K-means聚类。最终将这些选定的点作为高质量提示用于SAM进行分割。

**Result:** 大量的实验表明，SynPo的性能与最先进的训练基础上的少样本方法相当。

**Conclusion:** 该研究提出的方法SynPo在零训练环境下通过提高负提示的质量，显著增强了对低对比度医疗图像的分割效果，达到了与训练方法相近的性能水平。

**Abstract:** The advent of Large Vision Models (LVMs) offers new opportunities for
few-shot medical image segmentation. However, existing training-free methods
based on LVMs fail to effectively utilize negative prompts, leading to poor
performance on low-contrast medical images. To address this issue, we propose
SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core
insight: improving the quality of negative prompts. To select point prompts in
a more reliable confidence map, we design a novel Confidence Map Synergy Module
by combining the strengths of DINOv2 and SAM. Based on the confidence map, we
select the top-k pixels as the positive points set and choose the negative
points set using a Gaussian distribution, followed by independent K-means
clustering for both sets. Then, these selected points are leveraged as
high-quality prompts for SAM to get the segmentation results. Extensive
experiments demonstrate that SynPo achieves performance comparable to
state-of-the-art training-based few-shot methods.

</details>


### [56] [Enhancing point cloud analysis via neighbor aggregation correction based on cross-stage structure correlation](https://arxiv.org/abs/2506.15160)
*Jiaqi Shi,Jin Xiao,Xiaoguang Hu,Boyang Song,Hao Jiang,Tianyou Chen,Baochang Zhang*

Main category: cs.CV

> 提出了一种用于点云数据处理的新模块 PDSA，它通过高维空间中的相关性修正来进行特征聚合，显著提高了计算效率和鲁棒性，并在不同任务上展示出了优良的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于三维相对坐标聚合同邻点的方法存在无关点干扰和特征层次差距问题。虽然有些工作通过建模跨阶段结构来改进空间描述，但这些基于直接几何结构编码的方法存在高计算开销和噪声敏感性的问题。

**Method:** 提出了一种名为点分布集抽象模块（PDSA）的新方法，该方法利用高维空间中的相关性来修正聚合过程中的特征分布，从而提高计算效率和鲁棒性。PDSA 通过基于轻量级的跨阶段结构描述符，增强点关联，并通过消除特征矩阵的方差和增强类别可分性来改进结构同质性。此外，还引入了关键词机制来优化计算开销。

**Result:** 在基于不同基线的语义分割和分类任务中，实验结果验证了所提出方法的泛化能力，并且在参数消耗较少的情况下性能显著提高。

**Conclusion:** PDSA 的有效性和合理性通过消融实验和可视化结果得到了验证。

**Abstract:** Point cloud analysis is the cornerstone of many downstream tasks, among which
aggregating local structures is the basis for understanding point cloud data.
While numerous works aggregate neighbor using three-dimensional relative
coordinates, there are irrelevant point interference and feature hierarchy gap
problems due to the limitation of local coordinates. Although some works
address this limitation by refining spatial description though explicit
modeling of cross-stage structure, these enhancement methods based on direct
geometric structure encoding have problems of high computational overhead and
noise sensitivity. To overcome these problems, we propose the Point
Distribution Set Abstraction module (PDSA) that utilizes the correlation in the
high-dimensional space to correct the feature distribution during aggregation,
which improves the computational efficiency and robustness. PDSA distinguishes
the point correlation based on a lightweight cross-stage structural descriptor,
and enhances structural homogeneity by reducing the variance of the neighbor
feature matrix and increasing classes separability though long-distance
modeling. Additionally, we introducing a key point mechanism to optimize the
computational overhead. The experimental result on semantic segmentation and
classification tasks based on different baselines verify the generalization of
the method we proposed, and achieve significant performance improvement with
less parameter cost. The corresponding ablation and visualization results
demonstrate the effectiveness and rationality of our method. The code and
training weight is available at: https://github.com/AGENT9717/PointDistribution

</details>


### [57] [Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography](https://arxiv.org/abs/2506.15166)
*Abdur Rahman,Keerthiveena Balraj,Manojkumar Ramteke,Anurag Singh Rathore*

Main category: cs.CV

> Echo-DND在提高超声图像中左心室分割精度方面取得了显著进步，其在两个基准数据集上的实验结果优于现有最先进的方法。

<details>
  <summary>Details</summary>

**Motivation:** 超声图像由于其噪声大、对比度低以及左心室边界模糊，给准确分割带来了极大的挑战。准确的左心室分割对诊断过程和必要的治疗至关重要。

**Method:** Echo-DND采用了一种结合高斯噪声和伯努利噪声的独特方法，同时引入了多尺度融合条件模块和空间一致性校准机制，以提高对左心室的分割精度并保持空间完整性。

**Result:** 该论文提出了一种新颖的双噪声扩散模型Echo-DND，用于解决超声图像中左心室（LV）边界模糊和噪声大的问题。该模型结合了高斯噪声和伯努利噪声，并引入了多尺度融合条件模块和空间一致性校准机制，用以提高分割精度和保持空间完整性。实验结果表明，该方法在CAMUS和EchoNet-Dynamic数据集上表现优于现有SOTA模型，Dice系数分别为0.962和0.939，显示了其强大的分割性能。Echo-DND模型不仅在超声图像分割中建立了新标准，而且在其他医学影像任务中也具有广泛的适用前景。

**Conclusion:** Echo-DND模型建立了超声心动图分割的新标准，并且其架构有潜力在更广泛的医学图像任务中应用，从而可能提高各类医学诊断的准确性。

**Abstract:** Recent advancements in diffusion probabilistic models (DPMs) have
revolutionized image processing, demonstrating significant potential in medical
applications. Accurate segmentation of the left ventricle (LV) in
echocardiograms is crucial for diagnostic procedures and necessary treatments.
However, ultrasound images are notoriously noisy with low contrast and
ambiguous LV boundaries, thereby complicating the segmentation process. To
address these challenges, this paper introduces Echo-DND, a novel dual-noise
diffusion model specifically designed for this task. Echo-DND leverages a
unique combination of Gaussian and Bernoulli noises. It also incorporates a
multi-scale fusion conditioning module to improve segmentation precision.
Furthermore, it utilizes spatial coherence calibration to maintain spatial
integrity in segmentation masks. The model's performance was rigorously
validated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations
demonstrate that the proposed framework outperforms existing SOTA models. It
achieves high Dice scores of 0.962 and 0.939 on these datasets, respectively.
The proposed Echo-DND model establishes a new standard in echocardiogram
segmentation, and its architecture holds promise for broader applicability in
other medical imaging tasks, potentially improving diagnostic accuracy across
various medical domains. Project page: https://abdur75648.github.io/Echo-DND

</details>


### [58] [ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections](https://arxiv.org/abs/2506.15180)
*Ziling Huang,Yidan Zhang,Shin'ichi Satoh*

Main category: cs.CV

>  YYS直指同时解决大规模视觉搜索引擎中的两个问题：找到完全包含查询对象的图片并找出对象的具体位置。YS提出Referring Search and Discovery (ReSeDis)任务，结合了语料级别检索与像素级别定位。研究构建了一个新的基准数据集，为该任务设计了一个特定指标，并提出了一种简单直接的零样本基线模型。YS表明这一任务为构建下一代鲁棒且可扩展的多模态搜索系统提供了现实的测试平台。

<details>
  <summary>Details</summary>

**Motivation:**  YYS为了克服现有技术只解决上述问题之一的局限，提出了结合语料级别检索与像素级别定位的任务ReSeDis。

**Method:** 욧지와 동일

**Result:**  YYS提出了ReSeDis任务，开发了一个基准数据集，设计了一种特定任务的指标，并展示了零样本基线模型的性能。

**Conclusion:**  YYS表明ReSeDis为构建鲁棒且可扩展的多模态搜索系统提供了现实的测试平台。

**Abstract:** Large-scale visual search engines are expected to solve a dual problem at
once: (i) locate every image that truly contains the object described by a
sentence and (ii) identify the object's bounding box or exact pixels within
each hit. Existing techniques address only one side of this challenge. Visual
grounding yields tight boxes and masks but rests on the unrealistic assumption
that the object is present in every test image, producing a flood of false
alarms when applied to web-scale collections. Text-to-image retrieval excels at
sifting through massive databases to rank relevant images, yet it stops at
whole-image matches and offers no fine-grained localization. We introduce
Referring Search and Discovery (ReSeDis), the first task that unifies
corpus-level retrieval with pixel-level grounding. Given a free-form
description, a ReSeDis model must decide whether the queried object appears in
each image and, if so, where it is, returning bounding boxes or segmentation
masks. To enable rigorous study, we curate a benchmark in which every
description maps uniquely to object instances scattered across a large, diverse
corpus, eliminating unintended matches. We further design a task-specific
metric that jointly scores retrieval recall and localization precision.
Finally, we provide a straightforward zero-shot baseline using a frozen
vision-language model, revealing significant headroom for future study. ReSeDis
offers a realistic, end-to-end testbed for building the next generation of
robust and scalable multimodal search systems.

</details>


### [59] [Conquering the Retina: Bringing Visual in-Context Learning to OCT](https://arxiv.org/abs/2506.15200)
*Alessio Negrini,Simon Reiß*

Main category: cs.CV

> 本文探讨了使用视觉上下文学习（VICL）训练通用模型以实现视网膜OCT图像分析的方法，并提出了评估该方法的一个全面协议，建立了第一个基线来展示上下文学习在OCT中的潜力与局限性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于专门模型在医学图像分析中的局限性，本文提出了一种通过训练通用模型来实现视网膜光学相干断层扫描任务的方法，以满足临床需求。

**Method:** Structure

**Result:** {"tldr": "本文探讨了使用视觉上下文学习（VICL）训练通用模型以实现视网膜OCT图像分析的方法，并提出了评估该方法的一个全面协议，建立了第一个基线来展示上下文学习在OCT中的潜力与局限性。", "motivation": "鉴于专门模型在医学图像分析中的局限性，本文提出了一种通过训练通用模型来实现视网膜光学相干断层扫描任务的方法，以满足临床需求。", "method": "该研究使用视觉上下文学习（VICL）技术训练模型以实现跨任务的泛化能力，即基于推理时提供的少量示例进行任务定义。", "result": "研究在多个视网膜OCT数据集上广泛评估了当前先进的医疗上文学习方法，建立了第一个基线，展示了其在OCT中的潜力与当前局限性。", "conclusion": "研究公开发布了代码，旨在促进更深入的研究和实际应用，而评估协议和基线数据将为未来的改进提供重要参考。"}

**Conclusion:** 研究公开发布了代码，旨在促进更深入的研究和实际应用，而评估协议和基线数据将为未来的改进提供重要参考。

**Abstract:** Recent advancements in medical image analysis have led to the development of
highly specialized models tailored to specific clinical tasks. These models
have demonstrated exceptional performance and remain a crucial research
direction. Yet, their applicability is limited to predefined tasks, requiring
expertise and extensive resources for development and adaptation. In contrast,
generalist models offer a different form of utility: allowing medical
practitioners to define tasks on the fly without the need for task-specific
model development. In this work, we explore how to train generalist models for
the domain of retinal optical coherence tomography using visual in-context
learning (VICL), i.e., training models to generalize across tasks based on a
few examples provided at inference time. To facilitate rigorous assessment, we
propose a broad evaluation protocol tailored to VICL in OCT. We extensively
evaluate a state-of-the-art medical VICL approach on multiple retinal OCT
datasets, establishing a first baseline to highlight the potential and current
limitations of in-context learning for OCT. To foster further research and
practical adoption, we openly release our code.

</details>
