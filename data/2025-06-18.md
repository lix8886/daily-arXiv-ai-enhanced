<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/abs/2506.13796)
*Zhou Chen,Xiao Wang,Yuanhong Liao,Ming Lin,Yuqi Bai*

Main category: cs.CL

> 研究提出了一种自动化方法来构建气候变化指令数据集，通过这种方法构建的ClimateChat-Corpus数据集，能够显著提升语言模型在气候变化问答任务上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管一些研究通过构建气候变化相关的指令数据提高了模型在特定任务上的性能，当前研究在高效生成大量高精度的气候变化指令数据方面仍显不足，限制了气候变化LLMs的发展。

**Method:** 本研究提出了一种自动化构建指令数据的方法，该方法利用文档中的事实和背景知识生成指令，并通过网络抓取和收集种子指令来提高指令数据的多样性。

**Result:** 使用该方法构建了一个名为ClimateChat-Corpus的气候变化指令数据集，用于微调开源语言模型，生成了一个名为ClimateChat的模型。评估结果显示，ClimateChat在气候变化问答任务上性能有了显著提升。

**Conclusion:** 本研究为构建气候变化指令数据和训练专门针对气候变化的LLMs提供了宝贵的参考资料和实证支持。

**Abstract:** As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.

</details>


### [2] [Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles](https://arxiv.org/abs/2506.13886)
*Antara Raaghavi Bhattacharya,Isabel Papadimitriou,Kathryn Davidson,David Alvarez-Melis*

Main category: cs.CL

> 研究发现，LLMs需要明确的数学符号来解决数字相关的语言数学问题，并指出理解数字的组合规则对LLMs而言仍是一个挑战。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨为什么大型语言模型（LLMs）在解决跨语言数字系统中的语言数学谜题时会遇到困难，而人类却可以成功学习解决这些问题。

**Method:** 本研究通过一系列实验来解开语言和数学在数字中的作用，并进行了消融研究来探究数字构建和组合中的各个参数是如何影响模型表现的。

**Result:** 实验表明，除非数学操作用已知符号明确标记，否则模型无法一致地解决此类问题。人类能通过语言理解数字的隐含组合结构，而LLMs似乎缺乏这种能力。

**Conclusion:** 研究得出，从人类规模数据中的隐含模式灵活推理组合规则的能力，对当前推理模型而言仍然是一个开放的问题。

**Abstract:** Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Non-planar Object Detection and Identification by Features Matching and Triangulation Growth](https://arxiv.org/abs/2506.13769)
*Filippo Leveni*

Main category: cs.CV

> 本文提出了一种基于特征的方法，用于检测和识别场景图像中给定模板的扭曲实例。该方法通过增量分组图像与模板之间的特征匹配来进行检测和识别，使用Delaunay三角剖分作为指导。在严重变形的情况下，该方法比使用基于单应性的RANSAC方法表现更好。

<details>
  <summary>Details</summary>

**Motivation:** 在计算机视觉领域，对象检测和识别是一项基本任务，它在许多应用中扮演着重要角色。面对几何模型不适用或模板存在变形的情况，本研究希望通过改进检测方法，以提高对象识别的性能。

**Method:** 文中提出的方法使用Delaunay三角形的剖分作为工具，通过迭代的方式，从单个三角形开始，考虑邻近节点的特征识别及其匹配评估，来确定它们是否适合分组。这个评估基于局部一致性标准，标准来自于局部特征的几何和光度特性。

**Result:** 实验表明，本方法在稍微变形的情况下与基于单应性的RANSAC方法性能相当，但在变形严重的情况下表现更佳，特别是对于非平面模板或平面模板出现扭曲的情况。

**Conclusion:** 该研究提出的方法适用于检测和识别场景图像中具有扭曲形象的目标。当目标模板因变形变得较难识别时，提出的方法可以提供更佳的描述性能。

**Abstract:** Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.

</details>


### [4] [CDST: Color Disentangled Style Transfer for Universal Style Reference Customization](https://arxiv.org/abs/2506.13770)
*Shiwen Zhang,Zhuowei Chen,Lang Chen,Yanze Wu*

Main category: cs.CV

> 介绍了Color Disentangled Style Transfer (CDST)，一种能够完全将颜色与风格隔离的双流风格迁移技术，它无需调整即可实现广泛的风格迁移任务，并且提高了风格相似性。

<details>
  <summary>Details</summary>

**Motivation:** 这项工作的动机是实现一种无需调整即可处理多种风格迁移任务的方法，在保持内容特征的同时提高风格相似性。传统的风格迁移方法存在风格和内容混杂的问题，而且往往需要特定的调整才能达到理想效果。

**Method:** 我们介绍了Color Disentangled Style Transfer (CDST)，这是一种新的双流风格迁移训练范式，它完全将颜色与风格隔离，并使风格流失去对颜色的依赖。CDST使用同一个模型，无需调整即可实现通用的风格迁移能力。特别是，它首次以无需调整的方式解决了带有风格和内容参考的特征保持风格迁移问题。CDST通过多特征图像嵌入压缩显著提高了风格相似性，并通过受Diffusion UNet解缠定律启发的新的CDST风格定义，保留了强大的编辑能力。

**Result:** 通过广泛的质量和数量实验以及人类评价，作者证明了CDST在各种风格迁移任务中达到了最新技术水平。

**Conclusion:** 该研究展示了CDST在风格迁移领域的前沿地位，验证了其在保持内容特征的同时提高风格相似性的能力，为未来的风格迁移技术提供了新的方向。

**Abstract:** We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks.

</details>


### [5] [Hidden Bias in the Machine: Stereotypes in Text-to-Image Models](https://arxiv.org/abs/2506.13780)
*Sedat Porikli,Vedat Porikli*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** Structure

**Result:** {"tldr": "\u8be5\u7406\u8bba\u6587\u53d1\u73b0\uff0c\u6587\u5b57\u8f6c\u56fe\u6a21\u578b\u5728\u5f62\u6210\u8be5\u53d8\u4f53\u5305\u542b\u7684\u795e\u7ecf\u538b\u529b\u5f71\u54cd\u53ca\u51cf\u5c11\u8f83\u5e73\u5177\u4e49\u8868\u793a\u3002\u6587\u7ae0\u5c55\u5f00\u4e86\u4e00\u7cfb\u7edf\u7684\u7a7a\u95f4\u8868\u793a\u68c0\u67e5\uff0c\u4e3a\u51cf\u5c11\u6a21\u578b\u4e2d\u8f83\u5e73\u5177\u4e49\u7684\u5f71\u54cd\u8d4b\u4e3b\uff0c\u5e76\u5f15\u51fa\u8981\u6c42\u5728\u6570\u636e\u96c6\u548c\u5f00\u53d1\u65b9\u5411\u4e0a\u5c06\u5173\u957f\u7684\u53d1\u5c55\u5b8c\u7f8e\u7684\u89e3\u51b3\u65b9\u5411\u3002", "motivation": "\u6587\u7ae0\u7684\u52a8\u67机\uff0c在于研究和揭示文本转图像模型中潜在的社会偏见问题，特别是与性别、种族、年龄等方面的人类中心因素有关的社会偏见，并探索减少这些偏见的方法。", "method": "\u7528\u591a\u4e2a\u4e0d\u540c\u7684\u6587\u672c\u8bc4\u4ef6\u53d1\u751f\u8d28\u91cf\u5927\u91cf\u7684\u56fe\u50cf\uff0c\u5e76\u4e0e\u5728\u7ebf\u67e5\u8be2\u5927\u5c0f\u76f8\u540c\u7684\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u67e5\u627e\u672c\u6587\u6027\u8d28\u5728\u56fe\u50cf\u4e2d\u7684\u5b58\u5728\u60c5\u51b5\u3002", "result": "\u6570\u636e\u5360\u5e7f\u7a81\u70b9\uff0c\u53d8\u91cf\u6240\u5c06\u793e\u4f1a\u5bf9\u8c61\u7684\u5b9e\u9645\u53d8\u5316\u4e00\u8def\u4e2d\u7684\u7f81\u751f\u548c\u5f71\u54cd\u3002\u6587\u7ae0\u662f\u6709\u65f6\u5c06\u7f81\u751f\u4e00\u5b9a\u81ea\u7531\u7684\u53bb\u9664\u672c\u6587\u6027\u8d28\u5355\u72ec\u5b58\u5728\u7a81\u70b9\u6216\u5728\u56fe\u50cf\u7684\u5360\u5e7f\u7f81\u751f\u72b6\u51b5\u3002", "conclusion": "\u6587\u7ae0\u8ba4\u4e3a\uff0c\u8f93\u5165\u7684\u6587\u672c\u5177\u6709\u7c7b\u522b\u4e0a\u7684\u795e\u7ecf\u5b58\u5728\u8f7b\u5b9e\uff0c\u5bfc\u81f4\u53d1\u751f\u7684\u56fe\u50cf\u4e2d\u5b58\u5728\u8f83\u5e73\u5177\u4e49\u8868\u793a\u7684\u7834\u7b49\uff0c\u6b63\u5728\u7edf\u4e00\u5b9e\u4f5c\u6b63\u786e\u63a5\u53d7\u7684\u7a81\u70b9\uff0c\u5e76\u63d0\u51fa\u5e94\u8be5\u4e0d\u65ad\u9010\u6b65\u8d70\u5411\u89e3\u51b3\u8fd9\u4e00\u7c7b\u60c5\u51b5\u3002"}

**Conclusion:** 

**Abstract:** Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.

</details>


### [6] [Fake it till You Make it: Reward Modeling as Discriminative Prediction](https://arxiv.org/abs/2506.13846)
*Runtao Liu,Jiahao Zhan,Yingqing He,Chen Wei,Alan Yuille,Qifeng Chen*

Main category: cs.CV

> The paper introduces GAN-RM, a reward modeling framework inspired by GANs for enhancing visual generative models, which simplifies the process by avoiding manual preference annotation and using a small set of unpaired target samples.

<details>
  <summary>Details</summary>

**Motivation:** To reduce the complexity brought by the reliance on human-annotated preference data or engineered quality dimensions in reward models, which are often incomplete or require significant effort.

**Method:** The method utilizes a GAN-like framework, called GAN-RM, which discriminates between a small set of representative, unpaired target samples and the model-generated outputs to train the reward model.

**Result:** Experiments show that the GAN-RM framework is effective in improving key applications such as Best-of-N sample filtering, Supervised Fine-Tuning (SFT), and Direct Preference Optimization (DPO).

**Conclusion:** GAN-RM provides an efficient and simplified reward modeling solution for enhancing visual generative models, which is effective across several reinforcement learning scenarios.

**Abstract:** An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

</details>


### [7] [DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding](https://arxiv.org/abs/2506.13897)
*Thomas Kreutz,Max Mühlhäuser,Alejandro Sanchez Guinea*

Main category: cs.CV

> DeSPITE, a model that learns a joint embedding space across LiDAR point clouds, human skeleton poses, IMU data, and text, demonstrates novel human activity understanding tasks and effective pre-training strategy for point cloud human activity recognition, through noise contrastive estimation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to explore the application of LiDAR in multi-modal contrastive pre-training for human activity understanding, as it is less explored compared to RGB cameras.

**Method:** Our work introduces DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model that utilizes noise contrastive estimation to learn a joint embedding space across LiDAR point clouds, human skeleton poses, IMU data, and text.

**Result:** The experiments show that DeSPITE effectively enables new tasks for point cloud sequences and serves as a useful pre-training strategy for point cloud human activity recognition.

**Conclusion:** The conclusion of the paper is that DeSPITE successfully learns a joint embedding space, which is beneficial for various human activity understanding tasks and provides a strong pre-training strategy for point cloud human activity recognition.

**Abstract:** Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding (e.g., human activity recognition (HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model, which effectively learns a joint embedding space across these four modalities through noise contrastive estimation. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR.

</details>


### [8] [OPTIMUS: Observing Persistent Transformations in Multi-temporal Unlabeled Satellite-data](https://arxiv.org/abs/2506.13902)
*Raymond Yu,Paul Han,Josh Myers-Dean,Piper Wolters,Favyen Bastani*

Main category: cs.CV

> Introduced OPTIMUS, a self-supervised method for change detection in satellite imagery with improved performance over baselines.

<details>
  <summary>Details</summary>

**Motivation:** To address the difficulty of using supervised methods for change detection due to the lack of satellite data annotated with change labels, especially for rare categories.

**Method:** OPTIMUS, a self-supervised learning method based on an intuitive principle that models can recover information about the relative order of images in the time series to detect long-lasting changes.

**Result:** Achieved an improvement in AUROC score from 56.3% to 87.6% in distinguishing changed series from unchanged ones.

**Conclusion:** OPTIMUS can effectively detect interesting changes in satellite images, demonstrating better performance compared to baseline methods.

**Abstract:** In the face of pressing environmental issues in the 21st century, monitoring surface changes on Earth is more important than ever. Large-scale remote sensing, such as satellite imagery, is an important tool for this task. However, using supervised methods to detect changes is difficult because of the lack of satellite data annotated with change labels, especially for rare categories of change. Annotation proves challenging due to the sparse occurrence of changes in satellite images. Even within a vast collection of images, only a small fraction may exhibit persistent changes of interest. To address this challenge, we introduce OPTIMUS, a self-supervised learning method based on an intuitive principle: if a model can recover information about the relative order of images in the time series, then that implies that there are long-lasting changes in the images. OPTIMUS demonstrates this principle by using change point detection methods on model outputs in a time series. We demonstrate that OPTIMUS can directly detect interesting changes in satellite images, achieving an improvement in AUROC score from 56.3% to 87.6% at distinguishing changed time series from unchanged ones compared to baselines. Our code and dataset are available at https://huggingface.co/datasets/optimus-change/optimus-dataset/.

</details>
