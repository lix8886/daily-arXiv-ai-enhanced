<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [cs.CV](#cs.CV) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning](https://arxiv.org/abs/2511.15886)
*Jeremias Ferrao,Ezgi Basar,Khondoker Ittehadul Islam,Mahrokh Hassani*

Main category: cs.CL

> 该研究探讨了多语言大语言模型（LLM）中链式思考（CoT）推理的归因模式，指出CoT提示可能引起生成推理链不忠实和解释性差的问题。实验结果显示，归因分数过分强调最终推理步骤，结构化CoT提示对高资源拉丁文脚本语言的准确率提高显著，且否定句和干扰句会降低模型准确性和归因一致性。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在评估多语言背景下CoT推理生成的忠实性和可解释性，发现并探讨了CoT提示法在多语言LLM模型中的局限性。

**Method:** 采用ContextCite进行步骤级别的归因分析和Inseq进行令牌级别的归因分析，应用于Qwen2.5 1.5B-Instruct模型，并使用MGSM基准进行测试。

**Result:** 实验结果表明，归因分数对最终推理步骤的强调过度，特别是在生成错误时；结构化CoT提示主要提高高资源拉丁文脚本语言的准确性；否定句和干扰句降低了模型准确性和归因的一致性。

**Conclusion:** 这些发现强调了多语言环境下的CoT提示方法在多项指标上的局限性，特别是对于提高准确性和解释透明度。

**Abstract:** This study investigates the attribution patterns underlying Chain-of-Thought (CoT) reasoning in multilingual LLMs. While prior works demonstrate the role of CoT prompting in improving task performance, there are concerns regarding the faithfulness and interpretability of the generated reasoning chains. To assess these properties across languages, we applied two complementary attribution methods--ContextCite for step-level attribution and Inseq for token-level attribution--to the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Our experimental results highlight key findings such as: (1) attribution scores excessively emphasize the final reasoning step, particularly in incorrect generations; (2) structured CoT prompting significantly improves accuracy primarily for high-resource Latin-script languages; and (3) controlled perturbations via negation and distractor sentences reduce model accuracy and attribution coherence. These findings highlight the limitations of CoT prompting, particularly in terms of multilingual robustness and interpretive transparency.

</details>


### [2] [Mind the Motions: Benchmarking Theory-of-Mind in Everyday Body Language](https://arxiv.org/abs/2511.15887)
*Seungbeen Lee,Jinhong Jeong,Donghyun Kim,Yejin Son,Youngjae Yu*

Main category: cs.CL

> 提出了Motion2Mind框架，通过一个精心构建的视频数据集来评估AI系统在解释非语言线索方面的心智理论能力，发现AI系统存在检测和解释上的显著问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的心智理论基准主要集中在虚假信念任务和带有非对称信息的推理上，忽略了其他心理状态和丰富的人类非语言交流方式。因此，我们提出了一种新的框架来评估机器在解释非语言线索方面的心智理论能力。

**Method:** 我们提出了Motion2Mind框架，该框架使用专家编纂的身体语言参考作为知识库，并构建了一个包含222种非言语线索和397种心理状态的精细标注视频数据集。

**Result:** 评估结果显示，当前的AI系统在非语言线索解释方面面临巨大挑战，不仅在检测方面存在显著性能差距，而且在解释方面也表现出过度解读的模式，这与人类注释者不同。

**Conclusion:** 我们的研究结果强调了当前AI系统在理解和解释非言语线索方面的局限性，这对于未来AI系统的心智理论能力的提高至关重要。

**Abstract:** Our ability to interpret others' mental states through nonverbal cues (NVCs) is fundamental to our survival and social cohesion. While existing Theory of Mind (ToM) benchmarks have primarily focused on false-belief tasks and reasoning with asymmetric information, they overlook other mental states beyond belief and the rich tapestry of human nonverbal communication. We present Motion2Mind, a framework for evaluating the ToM capabilities of machines in interpreting NVCs. Leveraging an expert-curated body-language reference as a proxy knowledge base, we build Motion2Mind, a carefully curated video dataset with fine-grained nonverbal cue annotations paired with manually verified psychological interpretations. It encompasses 222 types of nonverbal cues and 397 mind states. Our evaluation reveals that current AI systems struggle significantly with NVC interpretation, exhibiting not only a substantial performance gap in Detection, as well as patterns of over-interpretation in Explanation compared to human annotators.

</details>


### [3] [TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues](https://arxiv.org/abs/2511.15976)
*Sarik Ghazarian,Abhinav Gullapalli,Swair Shah,Anurag Beniwal,Nanyun Peng,Narayanan Sadagopan,Zhou Yu*

Main category: cs.CL

> 本文介绍了TOD-ProcBench，这是一个专为评估LLM在多轮任务导向型对话中遵循复杂指令能力而设计的新基准。通过三个任务评估LLM识别相关说明、预测行动、检测违反指令响应及根据复杂指令生成条件性响应的能力，并探讨了多语言和不同指令格式对性能的影响。

<details>
  <summary>Details</summary>

**Motivation:** 目前的任务导向型对话（TOD）基准测试因简化复杂指令，不足以全面评估LLM的能力。此研究提出了TOD-ProcBench，以更真实地反映复杂指令的执行情况。

**Method:** 该基准基于高质数据集构建，包括复杂的指令和精细的约束条件，设计了三个任务来评估LLM的能力：提取相关指令并预测行动、识别违反指令的响应、以及生成遵守指令的答复。

**Result:** 通过该基准测试，可以衡量LLM在真实多轮对话场景下对复杂指令的理解与执行能力，涵盖多个语言环境和指令格式。

**Conclusion:** TOD-ProcBench提供了一个系统评估LLM指令执行能力的方法，指出未来研究可以在多语言支持和指令格式间进行深入探讨。

**Abstract:** In real-world task-oriented dialogue (TOD) settings, agents are required to strictly adhere to complex instructions while conducting multi-turn conversations with customers. These instructions are typically presented in natural language format and include general guidelines and step-by-step procedures with complex constraints. Existing TOD benchmarks often oversimplify the complex nature of these instructions by reducing them to simple schemas composed of intents, slots, and API call configurations. To address this gap and systematically benchmark LLMs' instruction-following capabilities, we propose TOD-ProcBench, a challenging benchmark featuring complex process instructions with intricate, fine-grained constraints that evaluates various LLMs' abilities to understand and follow instructions in multi-turn TODs. Our benchmark dataset comprises instruction documents derived from the high-quality ABCD dataset with corresponding conversations under human quality control. We formulate fine-grained constraints and action procedures as multi-level condition-action instruction statements. We design three tasks to comprehensively benchmark LLMs' complex instruction-following capabilities in multi-turn TODs. Task 1 evaluates how LLMs retrieve the most relevant statement from a complex instruction and predict the corresponding next action. In Task 2, we synthesize instruction-violating responses by injecting inconsistencies and manipulating the original instructions, and then we analyze how effectively LLMs can identify instruction-violating responses. Task 3 investigates LLMs' abilities in conditional generation of instruction-following responses based on the original complex instructions. Additionally, we conduct studies on the impact of multilingual settings and different instruction text formats on compliance performance. We release our benchmark under the Llama 3.3 Community License Agreement.

</details>


### [4] [Liars' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/abs/2511.16035)
*Kieron Kretschmar,Walter Laurito,Sharan Maiya,Samuel Marks*

Main category: cs.CL

> 本文提出了一个测试平台LIARS' BENCH，包含大量的谎言和诚实回答样本，用来评估谎言检测技术的性能，发现现有技术在特定场景下识别谎言的局限。

<details>
  <summary>Details</summary>

**Motivation:** 作者的动机是发现现有技术在识别谎言方面的局限性。这些技术通常在狭窄的场景下进行验证，而未能捕捉到语言模型可以生成的各种谎言。通过引入LIARS' BENCH，作者希望能够提供一个实用的测试平台，推动谎言检测技术的进步。

**Method:** 本文介绍了一个名为LIARS' BENCH的测试平台，该平台包含72,863个由四个开放权重模型生成的谎言和诚实回答，涵盖了七种数据集。通过控制模型撒谎的原因和谎言针对的信念目标，LIARS' BENCH旨在涵盖不同类型的谎言。

**Result:** 通过评估三类黑色和白色盒子谎言检测技术在LIARS' BENCH上的表现，研究发现现有技术在识别某些类型的谎言上系统性地失败了，尤其是在仅靠对话记录无法确定模型是否撒谎的情境下。

**Conclusion:** LIARS' BENCH揭示了先前技术的限制，并为推进谎言检测技术提供了实用的测试平台。

**Abstract:** Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.

</details>


### [5] [Learning Tractable Distributions Of Language Model Continuations](https://arxiv.org/abs/2511.16054)
*Gwen Yidou-Weng,Ian Li,Anji Liu,Oliver Broadrick,Guy Van den Broeck,Benjie Wang*

Main category: cs.CL

> 本研究提出了一种名为LTLA的方法，通过结合语言模型和可计算模型来改进受控语言生成中的上下文感知能力，提高了查询质量和效率。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于现有方法使用HMM等模型作为代理来近似后续分布，调整语言模型的下一个词预测，但是这些方法上下文感知能力较弱，降低了查询质量。

**Method:** 本研究提出了Learning to Look Ahead (LTLA) 方法，结合基础语言模型以实现丰富的前缀编码，并利用一个固定的可计算模型来计算精确的后续概率。通过批量的HMM更新一次性处理所有候选下一个词，并仅对代理模型的潜在状态先验进行条件处理，从而避免了效率问题，实现了计算的重用。

**Result:** 实验结果显示，LTLA在条件似然性上高于无条件HMM，能够近似视觉语言模型的延续分布，并且在相似的流畅度下提高了约束满意度，同时推理开销极小。

**Conclusion:** LTLA方法不仅提升了条件似然性，还能更好地处理视觉语言模型的延续分布，提高了受控语言生成任务中的约束满意度，同时保持了较低的推理成本。

**Abstract:** Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.

</details>


### [6] [Early science acceleration experiments with GPT-5](https://arxiv.org/abs/2511.16072)
*Sébastien Bubeck,Christian Coester,Ronen Eldan,Timothy Gowers,Yin Tat Lee,Alexandru Lupsasca,Mehtaab Sawhney,Robert Scherrer,Mark Sellke,Brian K. Spears,Derya Unutmaz,Kevin Weil,Steven Yin,Nikita Zhivotovskiy*

Main category: cs.CL

> GPT-5 is shown to provide valuable assistance in advancing scientific research across multiple disciplines, particularly in mathematics, with four new results confirmed.

<details>
  <summary>Details</summary>

**Motivation:** To demonstrate and document the practical value of GPT-5 in scientific research by providing concrete examples and highlighting both its benefits and limitations.

**Method:** The paper uses case studies in various scientific fields as practical demonstrations of GPT-5's capabilities and limitations, with a focus on documenting the interaction and collaboration process between human scientists and GPT-5.

**Result:** Four modest but significant mathematical results are produced, demonstrating GPT-5's ability to contribute to problem-solving in complex scientific areas.

**Conclusion:** GPT-5 can significantly accelerate certain aspects of scientific inquiry, but human oversight and input remain essential. The findings suggest a promising future for AI-assisted scientific research.

**Abstract:** AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.

</details>


### [7] [ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models](https://arxiv.org/abs/2511.16122)
*Qing Zhang,Bing Xu,Xudong Zhang,Yifan Shi,Yang Li,Chen Zhang,Yik Chung Wu,Ngai Wong,Yijie Chen,Hong Dai,Xiansen Chen,Mian Zhang*

Main category: cs.CL

> 文章提出了一个基于集合学习的自动提示优化新框架ELPO，通过集成不同的策略和机制得到更为准确和稳定的优化效果。实验结果显示，ELPO在多个任务上表现优越，特别是在ArSarcasm数据集上，F1得分提高了7.6。

<details>
  <summary>Details</summary>

**Motivation:** 现有自动提示优化方法侧重于单一模型或算法的生成策略和优化过程，限制了处理复杂任务时的性能。因此，我们提出了一个名为集合学习基于提示优化（ELPO）的新框架。

**Method:** 我们提出了一种名为ELPO的新型框架，该框架基于集合学习理念，采用投票机制，并引入了共享的生成策略和不同的搜索方法以寻找更优质的提示。此外，ELPO还创造性地提出了更高效的提示生成和搜索算法。

**Result:** 实验结果表明，ELPO在不同任务上优于最先进的提示优化方法，例如在ArSarcasm数据集上的F1得分提高了7.6。

**Conclusion:** ELPO方法通过集合学习的方法引入了更有效的生成和搜索机制，从而在多种任务中取得了优异的表现。

**Abstract:** The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.

</details>


### [8] [TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating](https://arxiv.org/abs/2511.16147)
*Dabiao Ma,Ziming Dai,Zhimin Xin,Shu Wang,Ye Wang,Haojun Fei*

Main category: cs.CL

> 本研究提出了Token-Selective PEFT，选择性地应用于特定位置索引，提高了性能并优化了大规模模型的微调过程。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨传统的PEFT方法，提出是否需要对所有位置索引都应用PEFT，并为此提出了一种更高效的策略。

**Method:** 本研究提出了一种新的范式，称为Token-Selective PEFT（TS-PEFT），其中函数S选择性地对序列中的一组位置索引应用PEFT修改，以改善下游任务中的性能而非对所有位置索引无差别应用PEFT。

**Result:** 实验结果表明，对所有位置索引无差别应用PEFT不仅没有必要，而且可能是有害的。

**Conclusion:** 研究提供了一种新的视角，推荐对大规模模型微调方式采用更有针对性的方法，为未来的研究提供了框架。

**Abstract:** In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.

</details>


### [9] [SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning](https://arxiv.org/abs/2511.16198)
*Sebastian Haan*

Main category: cs.CL

> SemanticCite 是一个利用AI验证学术引用准确性的系统，通过全文源分析，并提供丰富的上下文信息，包括详细的推理和相关文本片段。

<details>
  <summary>Details</summary>

**Motivation:** 学术文献面临语义引用错误、AI生成的虚假引用和传统引用格式不精确等问题，这些问题影响了科学研究的可信度。

**Method:** 使用了多种检索方法与四类分类系统（支持，部分支持，不支持，不确定）来捕捉引用和声明之间的微妙关系。

**Result:** 经过微调的轻量级语言模型在大规模引用校验中达到了与大型商用系统相当的性能，且计算需求显著降低。

**Conclusion:** SemanticCite 提供了一个开放源码的框架，解决了研究准确性中的关键问题，实现了可扩展的引用核实、简化了同行评审过程，同时对AI生成内容进行质量控制。

**Abstract:** Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.

</details>


### [10] [SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs](https://arxiv.org/abs/2511.16275)
*Xingtao Zhao,Hao Peng,Dingli Su,Xianghua Zeng,Chunyang Liu,Jinzhi Liao,Philip S. Yu*

Main category: cs.CL

> SeSE is a novel UQ method that uses semantic structural entropy to accurately quantify uncertainty in large language models, leading to improved safety and reliability in their applications.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the reliability of large language models in safety-critical scenarios by providing a more precise uncertainty estimation method that can better prevent the models from generating falsehoods when uncertain.

**Method:** Semantic Structural Entropy (SeSE) is a method that quantifies the intrinsic uncertainty of large language models from a semantic structural perspective, using an adaptively sparsified directed graph and hierarchical abstraction to model the semantic space and detect hallucinations.

**Result:** Experiments with 29 different models and datasets show that SeSE outperforms other advanced uncertainty quantification methods, including supervised learning approaches and a recently proposed method named KLE.

**Conclusion:** SeSE provides a principled framework for uncertainty quantification that enhances the reliability and safety of large language models, particularly in scenarios where exactitude is paramount.

**Abstract:** Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.

</details>


### [11] [SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning](https://arxiv.org/abs/2511.16324)
*Wei Xia,Zhi-Hong Deng*

Main category: cs.CL

> The paper introduces SDA, a framework that improves the alignment of large language model behavior with human intent during inference without retraining, achieving notable performance gains in helpfulness, honesty, and harmlessness across tested models.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenge of aligning LLM behavior with human intent during inference without the need for retraining, providing a lightweight and efficient solution.

**Method:** SDA (Steering-Driven Distribution Alignment) is a training-free and model-agnostic framework designed for aligning open-source large language models with human intent during inference. It dynamically redistributes model output probabilities based on predefined instructions without requiring fine-tuning.

**Result:** Empirical results show that SDA consistently improves alignment performance, achieving average gains of 64.4% in helpfulness, 30% in honesty, and 11.5% in harmlessness across 8 diverse open-source LLMs.

**Conclusion:** The proposed SDA method demonstrates significant improvements in alignment performance and is shown to be effective across a wide range of open-source language models and application scenarios.

**Abstract:** With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.

</details>


### [12] [Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement](https://arxiv.org/abs/2511.16331)
*Jiashu Yao,Heyan Huang,Shuang Zeng,Chuwei Luo,WangJie You,Jie Tang,Qingsong Liu,Yuhang Guo,Yangyang Kang*

Main category: cs.CL

> The paper introduces a self-rewriting framework using reinforcement learning to improve the internal reasoning quality of large reasoning models, achieving higher accuracy and better reasoning quality.

<details>
  <summary>Details</summary>

**Motivation:** The one-sided reward focus on final correctness limits the ability to provide detailed supervision for internal reasoning process, leading to various reasoning flaws.

**Method:** Structure

**Result:** {
  "tldr": "The paper introduces a self-rewriting framework using reinforcement learning to improve the internal reasoning quality of large reasoning models, achieving higher accuracy and better reasoning quality.", 
  "motivation": "The one-sided reward focus on final correctness limits the ability to provide detailed supervision for internal reasoning process, leading to various reasoning flaws.", 
  "method": "The method proposed is a self-rewriting framework based on reinforcement learning where the model rewrites its own reasoning texts. A selective rewriting approach is used that only rewrites 'simple' samples defined by the model's consistent correctness, maintaining original reward signals.", 
  "result": "Experiments show that self-rewriting approach achieves improved accuracy (+0.6) with shorter reasoning (-46%) and higher internal reasoning quality (+7.2) under the LLM-as-a-judge metric, effectively mitigating reasoning flaws.", 
  "conclusion": "The self-rewriting framework successfully enhances the internal reasoning process quality of large reasoning models, leading to improved reasoning accuracy and reduced flaws without explicit length reduction instructions. This mitigates the internal reasoning defects and can potentially scale well.", 
  "name": "Structure", 
  "arguments": ""}


**Conclusion:** The self-rewriting framework successfully enhances the internal reasoning process quality of large reasoning models, leading to improved reasoning accuracy and reduced flaws without explicit length reduction instructions. This mitigates the internal reasoning defects and can potentially scale well.

**Abstract:** Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only "simple" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.

</details>


### [13] [NLP Datasets for Idiom and Figurative Language Tasks](https://arxiv.org/abs/2511.16345)
*Blake Matheny,Phuong Minh Nguyen,Minh Le Nguyen,Stephanie Reynolds*

Main category: cs.CL

> 论文提出了一种通过使用多样化类别的惯用语和比喻语言数据集来帮助大型语言模型更好地理解这些语言表达的方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语料库对所有机器学习和自然语言处理问题似乎都是解决方案，但惯用语和比喻语言继续逃避大型语言模型的理解。针对这一问题，论文提出改进和扩大数据集可以帮助进一步缩小这一差距。

**Method:** 通过选择最近的惯用语和比喻语言数据集来构建一个综合的惯用语列表，并使用这个列表从大型语料库中检索上下文序列，创建了一个大规模的潜在惯用语和比喻语言表达的数据集，以及两个额外的人工标注的确定性惯用语和比喻语言表达的数据集用于评估预训练语言模型识别惯用语（检测）任务中的隐喻意义的基准能力。随后对这些数据集进行了后处理，以确保其适用于不特定模型的训练，并在标签和序列标记任务上进行了训练和评估。

**Result:** 未在摘要中明确提及具体结果。

**Conclusion:** 该论文提供了一种解决方案，即使用多样化的类别构建新模型和发展新方法，以此帮助弥补大型语言模型在理解惯用语和比喻语言方面存在的差距。

**Abstract:** Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.

</details>


### [14] [Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies](https://arxiv.org/abs/2511.16353)
*Jonathan Kamp,Lisa Beinborn,Antske Fokkens*

Main category: cs.CL

> 研究了充分性与模型识别理由及理由信息改善模型性能的关系，并发现其间的复杂性。

<details>
  <summary>Details</summary>

**Motivation:** 解决理由充分性估计理由信息量的局限性，并评估模型是基于正确的原因学习标签，还是依赖于数据集特定的捷径。

**Method:** 通过模型识别哪些标记属于理由（通过标记分类）和通过引入注意力正则化来改进性能（通过在输入中加入理由信息）的能力，我们将理由的充分性与两种建模范式联系起来。

**Result:** 实验表明高信息量的理由并不可能帮助模型正确分类实例。充分性相反反映了非理由化上下文对分类的影响，并且在输入信息中的理由信息会受到干扰。此外，在模型输入中加入理由信息可以提高跨域分类，但结果因任务和模型类型而不一致。充分性和标记分类似乎无关。

**Conclusion:** 结果突显了理由的复杂性，表明能够系统地捕捉这种类型信息的度量标准值得进一步研究。

**Abstract:** Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [15] [UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment](https://arxiv.org/abs/2511.15831)
*Wei Zhang,Yeying Jin,Xin Li,Yan Zhang,Xiaofeng Cong,Cong Wang,Fengcai Qiao,zhichao Lian*

Main category: cs.CV

> 本文提出UniFit，一种基于多模态大型语言模型（MLLM）驱动的通用虚拟试衣（VTON）框架，该框架通过新型MGSA模块和两阶段训练策略，有效减少语义差距并且能够从有限数据中学习，支持多种虚拟试衣任务并且达到最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管虚拟试衣领域取得了显著进步，但建立一个通用的虚拟试衣框架以灵活应对多样和复杂的任务仍然是一项重大挑战。现有方法通过文本指令引导的多任务虚拟试衣框架，仍然存在语义差距和复杂数据场景下数据稀缺两大局限性。为了应对这些挑战，本文提出了一个解决方案。

**Method:** 本文提出了一种名为UniFit的通用虚拟试衣框架，该框架由多模态大型语言模型（MLLM）驱动。具体来说，引入了MLLM引导的语义对齐模块（MGSA），该模块通过MLLM和一组可学习的查询集成多模态输入。通过施加语义对齐损失，MGSA捕捉跨模态语义关系，为生成过程提供连贯且明确的语义指导，从而减少语义差距。此外，通过设计两阶段渐进式训练策略，并结合自我合成管道，UniFit能够从有限的数据中学习复杂任务。

**Result:** 实验结果表明，UniFit不仅支持广泛的任务，包括多服装和模型到模型试穿，而且达到了最先进的性能。

**Conclusion:** UniFit通过引入MGSA模块和两阶段渐进式训练策略，解决了文本指令与参考图像之间的语义差距以及复杂数据场景下的数据稀缺等问题，实现了对广泛虚拟试衣任务的支持，并达到了最先进的性能。

**Abstract:** Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.

</details>


### [16] [EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation from SAM1, 2, and 3](https://arxiv.org/abs/2511.15833)
*Chengxi Zeng,Yuxuan Jiang,Aaron Zhang*

Main category: cs.CV

> 本文提出EfficientSAM3，通过对SAM3进行渐进分层蒸馏得到一系列轻量级的学生模型，实现在设备上高效的概念分割和跟踪，同时保持与SAM3类似的性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管SAM3在图像和视频中的概念分割上表现出色，但其统一架构对于设备上使用来说仍是负担过重。因此，本研究旨在开发一个更轻量化的模型系列，既能保持SAM3的功能，又能在设备上高效运行。

**Method:** 本研究提出了EfficientSAM3模型家族，通过渐进分层蒸馏（PHD）技术，将SAM3的能力转移到轻量级的学生模型上，分为三个阶段进行：编码器蒸馏、时间记忆蒸馏和端到端微调。这些阶段确保了学生模型具备与SAM3相当的概念分割和跟踪能力，同时解决了在设备上使用的问题。

**Result:** 在多个流行的视频对象分割数据集上进行基准测试，并与其他相关工作进行比较，结果表明EfficientSAM3在性能和效率之间实现了很好的平衡。

**Conclusion:** EfficientSAM3通过渐进分层蒸馏成功地将SAM3的能力传递给了一组轻量级的学生模型，从而实现在设备上的高效概念分割和跟踪，同时保持了高保真的教师行为。

**Abstract:** The Segment Anything Model 3 (SAM3) advances visual understanding with Promptable Concept Segmentation (PCS) across images and videos, but its unified architecture (shared vision backbone, DETR-style detector, dense-memory tracker) remains prohibitive for on-device use. We present EfficientSAM3, a family of efficient models built on Progressive Hierarchical Distillation (PHD) that transfers capability from SAM3 to lightweight students in three stages: (1) Encoder Distillation aligns image features via prompt-in-the-loop training on SA-1B; (2) Temporal Memory Distillation replaces dense memory with a compact Perceiver-based module trained on SA-V to compress and retrieve spatiotemporal features efficiently; and (3) End-to-End Fine-Tuning refines the full pipeline on the official SAM3 PCS data to preserve concept-level performance. PHD yields a spectrum of student variants using RepViT, TinyViT, and EfficientViT backbones, enabling on-device concept segmentation and tracking while maintaining high fidelity to teacher behavior. We benchmark on popular VOS datasets, and compare with varies of releated work, achieing strong performance-efficiency trade-offs.

</details>


### [17] [WALDO: Where Unseen Model-based 6D Pose Estimation Meets Occlusion](https://arxiv.org/abs/2511.15874)
*Sajjad Pakdamansavoji,Yintao Ma,Amir Rasouli,Tongtong Cao*

Main category: cs.CV

> 本文针对遮挡问题提出改进的6D物体姿态估计方法，并通过实验显示了在ICBIN和BOP数据集上准确性和速度的提升。

<details>
  <summary>Details</summary>

**Motivation:** 6D物体姿态估计对于机器人、增强现实和场景理解至关重要，但对未见物体的泛化仍具挑战。

**Method:** 我们提出了四个模型的6D姿态估计方法扩展：(i) 动态非均匀密集采样策略，(ii) 多假设推断机制，(iii) 逐步细化，和(iv) 遮挡关注的训练增强，并提出一个新的基于可见性的评价指标。

**Result:** 在ICBIN和BOP数据集上实现了超过5%和2%的准确性提升，同时推理速度提升大约3倍。

**Conclusion:** 这些改进和新的评价指标使模型在遮挡情况下表现更好。

**Abstract:** Accurate 6D object pose estimation is vital for robotics, augmented reality, and scene understanding. For seen objects, high accuracy is often attainable via per-object fine-tuning but generalizing to unseen objects remains a challenge. To address this problem, past arts assume access to CAD models at test time and typically follow a multi-stage pipeline to estimate poses: detect and segment the object, propose an initial pose, and then refine it. Under occlusion, however, the early-stage of such pipelines are prone to errors, which can propagate through the sequential processing, and consequently degrade the performance. To remedy this shortcoming, we propose four novel extensions to model-based 6D pose estimation methods: (i) a dynamic non-uniform dense sampling strategy that focuses computation on visible regions, reducing occlusion-induced errors; (ii) a multi-hypothesis inference mechanism that retains several confidence-ranked pose candidates, mitigating brittle single-path failures; (iii) iterative refinement to progressively improve pose accuracy; and (iv) series of occlusion-focused training augmentations that strengthen robustness and generalization. Furthermore, we propose a new weighted by visibility metric for evaluation under occlusion to minimize the bias in the existing protocols. Via extensive empirical evaluations, we show that our proposed approach achieves more than 5% improvement in accuracy on ICBIN and more than 2% on BOP dataset benchmarks, while achieving approximately 3 times faster inference.

</details>


### [18] [Automatic Uncertainty-Aware Synthetic Data Bootstrapping for Historical Map Segmentation](https://arxiv.org/abs/2511.15875)
*Lukas Arzoumanidis,Julius Knechtel,Jan-Henrik Haunert,Youness Dehbi*

Main category: cs.CV

> The paper presents innovative methods to generate synthetic historical maps, which serve as effective training data for deep learning models in the analysis of historical maps. It addresses the limitations of data availability and realism in this niche domain.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to overcome the scarcity of suitable annotated training data for deep learning-based methods, particularly in the specialized domain of historical maps, by synthesizing an effectively unlimited number of training samples that maintain the authenticity and diversity needed for effective model training.

**Method:** The paper introduces an automated deep generative approach and an alternative manual stochastic degradation technique to create synthetic historical maps that mimic the visual qualities and uncertainties of real historical maps. This addresses the challenge of creating sufficient annotated training data for training deep learning models in the context of historical map analysis.

**Result:** The generated synthetic maps were used for domain-adaptive semantic segmentation, demonstrating the effectiveness of the proposed method in creating high-quality training data for deep learning models in the analysis of homogeneous historical map corpuses.

**Conclusion:** The research concludes that synthetic data generated through the proposed approaches can effectively supplement the limited real-world training data available for historical maps, leading to improved performance in deep learning-based land-cover interpretation tasks.

**Abstract:** The automated analysis of historical documents, particularly maps, has drastically benefited from advances in deep learning and its success across various computer vision applications. However, most deep learning-based methods heavily rely on large amounts of annotated training data, which are typically unavailable for historical maps, especially for those belonging to specific, homogeneous cartographic domains, also known as corpora. Creating high-quality training data suitable for machine learning often takes a significant amount of time and involves extensive manual effort. While synthetic training data can alleviate the scarcity of real-world samples, it often lacks the affinity (realism) and diversity (variation) necessary for effective learning. By transferring the cartographic style of an original historical map corpus onto vector data, we bootstrap an effectively unlimited number of synthetic historical maps suitable for tasks such as land-cover interpretation of a homogeneous historical map corpus. We propose an automatic deep generative approach and a alternative manual stochastic degradation technique to emulate the visual uncertainty and noise, also known as data-dependent uncertainty, commonly observed in historical map scans. To quantitatively evaluate the effectiveness and applicability of our approach, the generated training datasets were employed for domain-adaptive semantic segmentation on a homogeneous map corpus using a Self-Constructing Graph Convolutional Network, enabling a comprehensive assessment of the impact of our data bootstrapping methods.

</details>


### [19] [Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes](https://arxiv.org/abs/2511.15884)
*Yintao Ma,Sajjad Pakdamansavoji,Amir Rasouli,Tongtong Cao*

Main category: cs.CV

> 提出Box6D方法，专为仓库环境中存储箱的6D姿态估计设计，速度快，精度高。

<details>
  <summary>Details</summary>

**Motivation:** 该方法旨在解决准确且高效的6D姿态估计问题，特别是在杂乱和遮挡的情况下，这是仓库自动化、拣选、物流和电子商务履行等机器人操纵领域的关键挑战。现有的三种方法（基于模型、无模型、类别级）都有其局限性，Box6D则是为了克服这些局限性，提供一个在工业环境中既灵活又精确的解决方案。

**Method:** Box6D, 一种专为仓库环境中的存储箱设计的类别级6D姿态估计方法。该方法从单个RGB-D观察中推断出箱子的尺寸，通过快速二分查找，并使用类别CAD模板而不是实例特定模型来估计姿态。通过使用基于深度的合理性过滤器和早期停止策略，Box6D可以拒绝不合理的假设，从而降低计算成本。

**Result:** 在对实际存储场景和公共基准进行评估后，研究表明该方法在提供具有竞争力或更优的6D姿态精度的同时，减少了大约76%的推理时间。

**Conclusion:** Box6D通过使用类别CAD模板和高效算法，能够在减少计算成本的同时，提供准确的6D姿态估计，适用于实际工业环境。

**Abstract:** Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.
  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.

</details>


### [20] [RB-FT: Rationale-Bootstrapped Fine-Tuning for Video Classification](https://arxiv.org/abs/2511.15923)
*Meilong Xu,Di Fu,Jiaxing Zhang,Gong Yu,Jiayu Zheng,Xiaoling Hu,Dongdi Zhao,Feiyang Li,Chao Chen,Yong Cao*

Main category: cs.CV

> 提出了一个两阶段自改进策略，使用VLM自生成理由进行微调，有效改善了VLM在特定领域视频分类任务上的表现。

<details>
  <summary>Details</summary>

**Motivation:** VLM在特定领域的视频分类任务上常常表现不佳，特别是在数据量有限的情况下。这源于一个关键的‘理由差距’，即稀疏领域的数据不足以桥接复杂时空内容和抽象分类标签之间的语义距离。

**Method:** 我们提出了一种两阶段的自我改进范式，首先通过提示VLM生成视频的详细文本理由，然后利用这些自我生成的理由对VLM进行微调。第二阶段在任务标签上进行传统的监督微调。

**Result:** 广泛的数据集实验表明，我们的方法显著优于直接的监督微调，验证了自我生成理由作为适应特定领域视频分析的注解有效的范式。

**Conclusion:** 通过使用自我生成的理由，我们的方法能够有效地填补这一差距，提高了模型在特定领域视频分析任务上的表现。

**Abstract:** Vision Language Models (VLMs) are becoming increasingly integral to multimedia understanding; however, they often struggle with domain-specific video classification tasks, particularly in cases with limited data. This stems from a critical \textit{rationale gap}, where sparse domain data is insufficient to bridge the semantic distance between complex spatio-temporal content and abstract classification labels. We propose a two-stage self-improvement paradigm to bridge this gap without new annotations. First, we prompt the VLMs to generate detailed textual rationales for each video, compelling them to articulate the domain-specific logic. The VLM is then fine-tuned on these self-generated rationales, utilizing this intermediate supervision to align its representations with the nuances of the target domain. Second, conventional supervised fine-tuning (SFT) is performed on the task labels, achieving markedly higher effectiveness as a result of the model's pre-acquired domain reasoning. Extensive experiments on diverse datasets demonstrate that our method significantly outperforms direct SFT, validating self-generated rationale as an effective, annotation-efficient paradigm for adapting VLMs to domain-specific video analysis.

</details>


### [21] [Boosting Medical Visual Understanding From Multi-Granular Language Learning](https://arxiv.org/abs/2511.15943)
*Zihan Li,Yiqing Wang,Sina Farsiu,Paul Kinahan*

Main category: cs.CV

> MGLL improves visual understanding in complex domains by enhancing cross-granularity and multi-label alignment between images and text.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of CLIP in aligning visual and textual representations for complex domains, such as medical imaging, where images often have multi-level, multi-granularity labels.

**Method:** Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to enhance multi-label and cross-granularity alignment. It uses structured multi-label supervision, leverages descriptions across different granularities, and applies soft-label supervision with smooth KL divergence for consistency.

**Result:** Outperforms other state-of-the-art methods in downstream tasks after pretraining on large-scale datasets.

**Conclusion:** MGLL shows significant performance gains in multimodal learning, especially in complex areas requiring multi-label and cross-granularity alignments.

**Abstract:** Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.

</details>


### [22] [Automated Interpretable 2D Video Extraction from 3D Echocardiography](https://arxiv.org/abs/2511.15946)
*Milos Vukadinovic,Hirotaka Ieki,Yuki Sahasi,David Ouyang,Bryan He*

Main category: cs.CV

> 研究通过自动化方法从3D心脏超声数据中提取标准的2D视图，并采用了深度学习与心脏病专家提供的启发式规则，证明了这种方法在临床应用中的准确性和实用性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管心脏具有复杂的三维解剖结构，但传统的医学成像仍依赖于一系列2D视频来显示单独的心脏结构，本研究旨在通过利用3D超声心动图提高获取速度和诊断的准确性，以适应临床需求。

**Method:** 本研究提出了一种自动化方法，从3D心脏超声体积中选择标准的2D视图，结合深度学习视角分类器和基于解剖标志点的下游启发式规则，重现标准超声心动图视图。

**Result:** 该方法通过三名心脏病专家在蒙蔽评估中的验证（1,600个视频，来自两家医院，准确率为96%），证明了其有效性和可靠性。此外，通过使用AI超声心动图模型（EchoPrime和PanEcho）和临床级别的心脏解剖测量（EchoNet-Measurement）验证了下游2D视频在检测心脏异常中的能力。

**Conclusion:** 研究表明，所提取的2D视频保留了空间校准和诊断特征，使得临床医生可以从3D体积中获得准确的实际解读。这种方法能够使医生在受益于3D扫描的速度和便利性的同时，也能以他们习惯的方式解释数据。

**Abstract:** Although the heart has complex three-dimensional (3D) anatomy, conventional medical imaging with cardiac ultrasound relies on a series of 2D videos showing individual cardiac structures. 3D echocardiography is a developing modality that now offers adequate image quality for clinical use, with potential to streamline acquisition and improve assessment of off-axis features. We propose an automated method to select standard 2D views from 3D cardiac ultrasound volumes, allowing physicians to interpret the data in their usual format while benefiting from the speed and usability of 3D scanning. Applying a deep learning view classifier and downstream heuristics based on anatomical landmarks together with heuristics provided by cardiologists, we reconstruct standard echocardiography views. This approach was validated by three cardiologists in blinded evaluation (96\% accuracy in 1,600 videos from 2 hospitals). The downstream 2D videos were also validated in their ability to detect cardiac abnormalities using AI echocardiography models (EchoPrime and PanEcho) as well as ability to generate clinical-grade measurements of cardiac anatomy (EchoNet-Measurement). We demonstrated that the extracted 2D videos preserve spatial calibration and diagnostic features, allowing clinicians to obtain accurate real-world interpretations from 3D volumes. We release the code and a dataset of 29 3D echocardiography videos https://github.com/echonet/3d-echo .

</details>


### [23] [Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click](https://arxiv.org/abs/2511.15948)
*Raphael Ruschel,Hardikkumar Prajapati,Awsafur Rahman,B. S. Manjunath*

Main category: cs.CV

> Click2Graph 是首个交互式全景视频场景图生成框架，它可以接收用户提示以生成时间一致的场景图，展示了如何结合用户提示、全景定位和关系推理来实现可控和可解释的视频场景理解。

<details>
  <summary>Details</summary>

**Motivation:** 最先进的视频场景图生成系统缺乏接受人类指导的能力，而像SAM2这样的可提示分割模型虽然能够实现精确的用户交互，却缺乏语义或关系推理能力。为此，提出了Click2Graph，旨在通过引入人类指导来增强视频场景理解。

**Method:** Click2Graph 是一种交互式框架，用于全景视频场景图生成，它结合了视觉提示、空间、时间和语义理解。用户可以使用单个提示（如点击或边界框）来分割并跟踪主题，自动发现交互对象，并预测三元组以形成一个时间一致的场景图。该框架包含两个关键组件：动态交互发现模块，用于生成基于主题的对象提示；语义分类头部，用于进行联合实体和谓词推理。

**Result:** 在OpenPVSG基准测试中，Click2Graph建立了坚实的基础，展示了其在用户引导全景视频场景图生成方面的能力。

**Conclusion:** 研究结果表明，结合视觉提示和语义理解可以促进视频场景中物体和关系的可控制和可解释的理解。这种方法为用户引导的全景视频场景图生成提供了新的视角。

**Abstract:** State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.

</details>


### [24] [InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer](https://arxiv.org/abs/2511.15967)
*Muyao Yuan,Yuanhong Zhang,Weizhan Zhang,Lan Ma,Yuan Gao,Jiangyong Ying,Yudeng Xin*

Main category: cs.CV

> 本文提出InfoCLIP方法，通过信息论的视角优化CLIP在开放词汇语义分割中的效果，解决过拟合和视觉-语言对齐的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在有限的已见类别上通过微调CLIP进行分割常常会导致过拟合并且降低预先训练好的视觉-语言对齐效果，以此来稳定微调期间的模态对齐，以此为动机提出了该方法。

**Method:** 提出了一种称为InfoCLIP的方法，该方法通过采用信息论的角度来转移预训练CLIP的对齐知识到分割任务。具体来说，转移过程由两个基于互信息的新目标引导。首先，通过压缩预训练CLIP中的像素-文本对齐来减少噪声，这些噪声来源于其在图像-文本监督下的粗粒度局部语义表示。其次，最大化预训练CLIP的对齐知识与微调模型之间的互信息，以转移适合分割任务的紧凑局部语义关系。

**Result:** 多项评估基准的广泛测试证明了InfoCLIP在通过微调CLIP进行开放词汇语义分割的有效性。

**Conclusion:** 评估证明所提出的InfoCLIP在增强CLIP的开放词汇语义分割上的微调过程中的效果，表现出其适应性和在不对称转移中的优越性。

**Abstract:** Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.

</details>


### [25] [Externally Validated Multi-Task Learning via Consistency Regularization Using Differentiable BI-RADS Features for Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2511.15968)
*Jingru Zhang,Saed Moradi,Ashirbani Saha*

Main category: cs.CV

> 在分析乳腺肿瘤的超声图像时，研究提出了一种新的方法来改善多任务学习中的任务干扰和分割效果。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高基于乳腺超声的肿瘤分割任务中的多任务学习的泛化性能，因为我们发现多任务学习会受到任务之间破坏性干扰的影响，使得联合训练的模型表现不如单任务基线模型。

**Method:** 我们提出了一种新颖的一致性正则化方法，以减轻分割和分类之间的破坏性干扰。该方法基于可微分的BI-RADS启发的形态特征。

**Result:** 与基线方法相比，所提出的方法在三个外部数据集上显著提高了分割任务的泛化性能：UDIAT, BUSI 和 BUS-UCLM (Dice系数分别为0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49)。

**Conclusion:** 我们提出的方法在分割任务中的泛化性能得到了显著提高，并且在严格的外部验证中实现了最新的分割性能。

**Abstract:** Multi-task learning can suffer from destructive task interference, where jointly trained models underperform single-task baselines and limit generalization. To improve generalization performance in breast ultrasound-based tumor segmentation via multi-task learning, we propose a novel consistency regularization approach that mitigates destructive interference between segmentation and classification. The consistency regularization approach is composed of differentiable BI-RADS-inspired morphological features. We validated this approach by training all models on the BrEaST dataset (Poland) and evaluating them on three external datasets: UDIAT (Spain), BUSI (Egypt), and BUS-UCLM (Spain). Our comprehensive analysis demonstrates statistically significant (p<0.001) improvements in generalization for segmentation task of the proposed multi-task approach vs. the baseline one: UDIAT, BUSI, BUS-UCLM (Dice coefficient=0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49, resp.). The proposed approach also achieves state-of-the-art segmentation performance under rigorous external validation on the UDIAT dataset.

</details>


### [26] [UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition](https://arxiv.org/abs/2511.15984)
*Xinyu Nan,Lingtao Mao,Huangyu Dai,Zexin Zheng,Xinyu Sun,Zihan Liang,Ben Chen,Yuqing Ding,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.CV

> Achieving visual semantic understanding requires a unified framework that can simultaneously perform object detection, category prediction, and attribute recognition. The method introduced in this paper uses a BART-based generator, which performs well on external datasets, indicating strong fine-grained recognition ability and coherent unified inference.

<details>
  <summary>Details</summary>

**Motivation:** Amidst challenges in simultaneously achieving tasks like object detection, category prediction, and attribute recognition, particularly in dealing with large-scale datasets and commercial data environments, the existing approaches face difficulties in fine-grained recognition and specific attribute diversity, motivating the proposal of an enhanced structured generation model.

**Method:** The proposed system leverages a BART-based generator to produce semantic tokens for the detected objects from extracted ROI features, capturing category hierarchies and property-value pairs in a sequential manner.

**Result:** {"tldr": "\u8be5\u7b79\u7ae0\u4e3b\u8981\u4ecb\u7ecf\u4e86\u4e00\u79cd\u53d1\u73b0\u63a7\u5236\u7684\u4f53\u73b0\u8bed\u6f14\u7406\u8def\u5f84\uff0c\u8be5\u8def\u5f84\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u5bf9\u58eb\u7269\u7684\u63a8\u6307\uff0c\u544a\u73b0\u5206\u7c7b\u4e4b\u95f4\u7684\u7ec4\u5c5e\u5206\u7ea7\u548c\u5c5e\u6027\u5206\u7c7b\u3002\u5b83\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8eBART\u7684\u751f\u6210\u5668\uff0c\u5728\u5916\u90e8\u8d44\u6599\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8868\u793a\u51fa\u4e00\u79cd\u66f4\u5f3a\u7684\u7ec4\u5c5e\u5206\u8bb8\u548c\u6807\u51c6\u7684\u65cb\u8f6c\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u5bf9\u4e8e\u540c\u6b65\u5b9e\u73b0\u6570\u636e\u5bf9\u58eb\u7269\u7684\u5b58\u5728\u3001\u7c7b\u578b\u9886\u5b50\u9a8c\u8bc1\u4e0e\u5c5e\u6027\u8bed\u6f14\u8ba1\u5206\u8bb8\u3001\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u5f00\u5a92\u4fe1\u606f\u7684\u5b9e\u9a8c\u5e76\u975e\u5927\u53d8\u7684\u9700\u6c42\uff0c\u524d\u767b\u7684\u5173\u952e\u9020\u5f0f\u62c5\u5373\u7ec4\u5708\u4e0e\u5c5e\u6027\u4e4b\u95f4\u7684\u7ed3\u5408\u6d4b\u91cf\u4e00\u822c\u5e76\u4e0d\u751f\u611b\u8eab\u5904\u7406\u81ea\u7531\u7684\u7ec4\u5207\u884c\u5206\u4e0e\u7279\u5b9a\u7684\u5c5e\u6027\u52b1\u52a8\u53ca\u5206\u79bb\u7684\u95ee\u9898\uff0c\u4eba\u4eec\u5f88\u5dee\u5fc3\u53d1\u73b0\u4e00\u4e2a\u53ef\u5b9e\u73b0\u5206\u7c7b\u5bb3\u5206\u7684\u4e13\u4e1a\u8def\u5f84\u3002", "method": "\u53d1\u73b0\u63a7\u5236\u7684\u53d1\u73b0\u7cfb\u7edf\u5e26\u6765\u4e00\u4e2a\u57fa\u4e8eBART\u7684\u751f\u6210\u5668\u3002\u5bf9\u6bcf\u4e2a\u63a8\u6307\u7684\u58eb\u7269\uff0c\u4ece\u5185\u90e8\u53d1\u73b0\u5b9e\u9a8c\u77f3\u4e0a\u53d1\u73b0\u5b9e\u9a8c\u77f3\uff0c\u5efa\u7acb\u4e00\u4e2a\u7cfb\u7edf\u4ee5\u5f00\u59cb\uff0c\u4e00\u822c\u6309\u7167\u6761\u5217\u7ea7\u7684\u987a\u5e8f\u53bb\u4ea4\u53c9\u548c\u5e2e\u52a9\u7684\u7cfb\u7edf\u53d1\u73b0\u57f9\u517b\u3002", "result": "\u5728\u6570\u636e\u8d44\u6e90\u4e0a\u8bad\u7ec3\u7684\u5b9e\u9a8c\u7ed3\u679c\u5c31\u662f\uff1a\u5bf9\u6bd4\u751f\u6210\u7684\u8d28\u7269\u6709\u89e3\u6b63\u7279\u522b\u7684\u8bb8\u7ed9\uff0c\u5e76\u4e14\u5728\u4e00\u4e9b\u5f00\u653e\u5e95\u6570\u636e\u7248\u672c\u4e0a\uff0c\u4ea6\u5e76\u8868\u793a\u51fa\u4e86\u66f4\u5f3a\u7684\u8d28\u7269\u4e0e\u517c\u5207\u52b1\u52a8\u8fd0\u9a8c\u7a7a\u95f4\u7684\u6784\u5efa\u7b97\u6cd5\u3002", "conclusion": "\u6b63\u5929\u8d28\u7269\u57f9\u517b\u4e2d\u7684\u95ee\u9898\uff0c\u5373\u7b7e\u4e0a\u8f6c\u6362\u5927\u7ea7\u58eb\u4e1b\u6570\u636e\u4e4b\u95f4\u88ab\u6d41\u7545\u7b49\u4e0a\u4e00\u79cd\u7c7b\u7684\u5927\u95ee\u9898\uff0c\u4f7f\u7528\u76f8\u5173\u7684\u6a21\u5f0f\u5e94\u7528\u53ef\u4ee5\u52a0\u5feb\u89e3\u51b3\u95ee\u9898\u3002"}

**Conclusion:** The presented model demonstrates significant improvement in recognition capabilities when trained on extensive proprietary and open-source datasets, providing a pathway for addressing large-scale visual semantic understanding in commercial applications.

**Abstract:** Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.

</details>


### [27] [Fairness in Multi-modal Medical Diagnosis with Demonstration Selection](https://arxiv.org/abs/2511.15986)
*Dawei Li,Zijian Gu,Peng Wang,Chuhan Song,Zhen Tan,Mohan Zhang,Tianlong Chen,Yu Tian,Song Wang*

Main category: cs.CV

> 该研究提出了一种公平性意识的上下文学习（FADS）方法，用于在不进行微调的情况下提高医学图像推理模型的公平性，尤其在性别、种族和族裔相关的差异方面取得了显著效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于解决医学图像推理模型在不同人口统计学群体间公平性的问题，现有去偏方法依赖大量的标注数据集或是微调，这些方法对基础模型来说并不实际。

**Method:** 研究采用了一种无调优、轻量级的上下文学习方法（ICL）来改进公平性，提出了公平性意识的示范选择（FADS），通过基于聚类的采样构建了人口统计学平衡和语义相关的示范。

**Result:** 实验在多个医学图像识别基准上显示，FADS可以持续降低性别、种族及族裔相关的差异，同时保持良好的准确性。

**Conclusion:** 研究结果强调了公平性上下文学习作为一个可扩展且数据高效的解决方案，对于实现公平的医学图像推理模型的潜力。

**Abstract:** Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.

</details>


### [28] [Exploiting Inter-Sample Information for Long-tailed Out-of-Distribution Detection](https://arxiv.org/abs/2511.16015)
*Nimeshika Udayangani,Hadi M. Dolatabadi,Sarah Erfani,Christopher Leckie*

Main category: cs.CV

> 论文提出了一种基于图形结构的方法，通过高斯化激活层分布并使用图卷积网络优化特征表示来改进长尾数据集中的异常检测，实现更高的分类准确率和更低的误报率。

<details>
  <summary>Details</summary>

**Motivation:** 解决长尾数据集中异常检测问题，提高少样本类别识别准确率并降低误报率。

**Method:** 利用图表示法捕捉样本间关系，并使用预训练模型的特征空间初始化图结构；通过高斯化处理缓解预训练和训练数据激活层分布差异；使用图卷积网络（GCN）精细调整图表示，以适应长尾数据集上的异常检测。

**Result:** 在CIFAR10-LT、CIFAR100-LT和ImageNet-LT三个基准数据集上，实验结果表明，该方法在误报率和少样本类别分类准确率上显著优于现有方法。

**Conclusion:** 该方法通过引入图形结构和高斯化激活层缓解预训练数据与训练数据间分布差异性，并利用图卷积网络提升模型检测性能，从而有效地提高了少样本类别上的准确率。

**Abstract:** Detecting out-of-distribution (OOD) data is essential for safe deployment of deep neural networks (DNNs). This problem becomes particularly challenging in the presence of long-tailed in-distribution (ID) datasets, often leading to high false positive rates (FPR) and low tail-class ID classification accuracy. In this paper, we demonstrate that exploiting inter-sample relationships using a graph-based representation can significantly improve OOD detection in long-tailed recognition of vision datasets. To this end, we use the feature space of a pre-trained model to initialize our graph structure. We account for the differences between the activation layer distribution of the pre-training vs. training data, and actively introduce Gaussianization to alleviate any deviations from a standard normal distribution in the activation layers of the pre-trained model. We then refine this initial graph representation using graph convolutional networks (GCNs) to arrive at a feature space suitable for long-tailed OOD detection. This leads us to address the inferior performance observed in ID tail-classes within existing OOD detection methods. Experiments over three benchmarks CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrate that our method outperforms the state-of-the-art approaches by a large margin in terms of FPR and tail-class ID classification accuracy.

</details>


### [29] [Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion](https://arxiv.org/abs/2511.16020)
*Dingkun Zhou,Patrick P. K. Chan,Hengxu Wu,Shikang Zheng,Ruiqi Huang,Yuanjie Zhao*

Main category: cs.CV

> 本篇论文提出了一种生成自然且可打印的对抗纹理的框架，可用于衣物以在整段行走视频中隐蔽地降低检测置信度，通过基于物理的人体-衣物管道和时间加权变换期望目标进行优化，实验验证了其稳健性和跨模型转移性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的用于对抗性操作的方法通常是逐帧优化纹理，因此无法在带有运动、姿态变化和衣物变形的长时间视频序列中保持隐蔽性。本研究旨在生成在整段行走视频中均有效的自然可打印对抗纹理，提高真实环境下的隐蔽性和稳健性。

**Method:** 通过将产品图像映射到UV空间并转换为紧凑的调色板和控制点参数化，以生成自然且可打印的对抗纹理。使用基于物理的人体-衣物管道来模拟运动、多角度摄像机视角、布料动力学和照明变化，使用带时间加权的变换期望目标来优化控制点，以在整段视频序列中最小化检测置信度。

**Result:** 实验结果展示了强大的稳定隐蔽性、高视图变化抗差异、优异的跨模型转移性能，物理制作的衣物经过升华印刷在室内和室外视频录制中实现了可靠的抑制效果。

**Conclusion:** 该研究提出的方法证明其在对抗性操作中的实际可行性，能够在真实世界环境下影响目标检测的准确性，且具有跨模型的稳健性和可靠性。

**Abstract:** Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.

</details>


### [30] [Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution](https://arxiv.org/abs/2511.16024)
*Xiao He,Zhijun Tu,Kun Cheng,Mingrui Zhu,Jie Hu,Nannan Wang,Xinbo Gao*

Main category: cs.CV

> 本文提出了应用于图像超分辨率的Mixture-of-Ranks (MoR)架构，通过专家划分策略、退化估计模块和退化感知负载平衡损失提升图像超分辨的性能，并在实验中展示了框架的有效性和先进性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于细调预训练扩散模型的图像超分辨率方法在捕捉复杂现实世界降质样本的异质特性或在等效计算预算下实现知识共享方面能力有限。

**Method:** 本文提出了Mixture-of-Ranks (MoR)架构，用于单步图像超分辨率。通过细粒度的专家划分策略将LoRA中的每个秩视为独立专家，同时保持固定位置的秩作为共享专家以保存通用特征，减少路由冗余。此外，引入退化估计模块使用CLIP嵌入和预定义的正负文本对计算相对退化分数，动态引导专家激活，并结合零专家槽和退化感知负载平衡损失，动态调整活跃专家数量，确保资源的最优分配。

**Result:** 广泛的实验验证了该框架的有效性和最先进的性能。

**Conclusion:** 本文提出的Mixture-of-Ranks (MoR)架构在图像超分辨率任务中表现出色，通过引入多个创新模块和策略，有效提升了图像重建的质量，为图像超分辨率领域提供了新的见解。

**Abstract:** The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.

</details>


### [31] [Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning](https://arxiv.org/abs/2511.16026)
*Mohamed Abdallah Salem,Hamdy Ahmed Ashur,Ahmed Elshinnawy*

Main category: cs.CV

> The paper presents a material classification technique for laser cutting using speckle sensing and deep learning, achieving high accuracy and robustness across different laser colors, improving environmental and occupational health.

<details>
  <summary>Details</summary>

**Motivation:** To monitor and identify material types in real-time during laser cutting to ensure environmental and health safety, while addressing the limitations of previous speckle sensing methods in handling changes in laser color.

**Method:** material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process.

**Result:** The model achieved an accuracy of 98.30% on the training set and 96.88% on the validation set. When evaluated on a set of 3000 new images for 30 different materials, the F1-score was 0.9643.

**Conclusion:** The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing, demonstrating high performance even with changes in laser color.

**Abstract:** Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.

</details>


### [32] [CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis](https://arxiv.org/abs/2511.16030)
*Zijian Wu,Mingfeng Jiang,Zidian Lin,Ying Song,Hanjie Ma,Qun Wu,Dongping Zhang,Guiyang Pu*

Main category: cs.CV

> CuriGS, a novel framework for 3D reconstruction under sparse-view conditions, surpasses current benchmarks in rendering and geometric consistency using 3D Gaussian Splatting with curriculum learning and a variety of regularization techniques.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to extend 3D Gaussian Splatting (3DGS) to sparse-view settings which is challenging due to supervision scarcity and overfitting caused by limited viewpoint coverage.

**Method:** CuriGS introduces student views: pseudo-views sampled around ground-truth poses (teacher). It generates multiple groups of student views with different perturbation levels and uses a curriculum schedule, depth-correlation, co-regularization, and a multi-signal metric. Periodically, the best-performing students are retained and those that satisfy a quality threshold are promoted to enhance the training set.

**Result:** Experimental results demonstrate that CuriGS surpasses state-of-the-art baselines in both rendering fidelity and geometric consistency for various synthetic and real sparse-view scenes.

**Conclusion:** CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS, outperforms state-of-the-art baselines in rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes.

**Abstract:** 3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/

</details>
