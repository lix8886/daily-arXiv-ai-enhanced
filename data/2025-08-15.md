<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 20]
- [cs.CV](#cs.CV) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

> 该论文通过分享在不列颠哥伦比亚癌症登记处实施NLP模型的经验，强调了在临床文档中自动化数据提取的重要性，并提出了一系列在开发和部署过程中需要考虑的实用建议。

<details>
  <summary>Details</summary>

**Motivation:** 自动化从临床文档中提取数据对提高医疗保健效率至关重要，但NLP解决方案的部署面临诸多挑战。本文基于在不列颠哥伦比亚癌症登记处实施信息提取和分类任务中的经验提出建议。

**Method:** Structure

**Result:** {
  "tldr": "该论文通过分享在不列颠哥伦比亚癌症登记处实施NLP模型的经验，强调了在临床文档中自动化数据提取的重要性，并提出了一系列在开发和部署过程中需要考虑的实用建议。", 
  "motivation": "自动化从临床文档中提取数据对提高医疗保健效率至关重要，但NLP解决方案的部署面临诸多挑战。本文基于在不列颠哥伦比亚癌症登记处实施信息提取和分类任务中的经验提出建议。", 
  "method": "通过分享项目生命周期中的经验教训，强调了基于清晰的商业目标定义问题的重要性，倡导采用迭代方法进行开发，并加强跨学科协作。", 
  "result": "包括对模型选择（包括混合方法和适当情况下采用简单方法）、数据质量（代表性、偏移和标注）、以及鲁棒的错误缓解策略（包括人工交互验证和持续审核）的严格关注，并构建组织的AI知识库。", 
  "conclusion": "这些实践经验不仅适用于癌症登记处，还可以为希望成功实施AI/NLP解决方案以改进数据管理流程，从而改善患者护理和公共卫生结果的医疗保健组织提供指导。"}
}

**Conclusion:** 这些实践经验不仅适用于癌症登记处，还可以为希望成功实施AI/NLP解决方案以改进数据管理流程，从而改善患者护理和公共卫生结果的医疗保健组织提供指导。

**Abstract:** Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

> 摘要：本研究提出了一种基于区块链技术的透明评估协议，用于衡量开源大语言模型的公正性，并通过多个实证数据展示了其在多个语言和应用场景下的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 动机：随着大语言模型在现实世界中的应用不断增加，特别是在刑事司法、教育、医疗保健和金融等高风险领域，人们对这些模型的公正性依然感到担忧。为了解决这一问题并提供透明度，我们提出了该评估协议。

**Method:** 透明评估方法：使用互联网计算机协议（ICP）区块链上的智能合约进行开源大语言模型（LLMs）的公正性基准测试。通过区块链上执行的HTTP请求来实现对Hugging Face托管端点的可验证性、不可变性和可再现性评估。

**Result:** 结果：通过PISA数据集上的基准测试（用于学术表现预测）和StereoSet数据集上的结构化上下文关联度量进行评估，揭示了学术表现预测中的统计平等和等机会度量以及社会偏见问题。此外，使用Kaleidoscope基准测试进行了跨英语、西班牙语和葡萄牙语的多语言评估，揭示了跨语言的不平等。

**Conclusion:** 结论：我们的研究证明了使用区块链技术进行语言模型公正性评估的有效性，所有代码和结果都是开源的，这促进了社区审核并有助于跨模型版本的公正性趋势跟踪。

**Abstract:** Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

> 本文通过新颖的主题建模方法，对数量庞大的课堂交互信息进行了内容和任务两个维度的分类，提出了更好的文本分析方法并应用于生成型人工智能的研究。

<details>
  <summary>Details</summary>

**Motivation:** 此前的研究大多缺乏内容或主题分类，或者没有使用K-12的真实世界数据来支持任务的分类。因此，本研究旨在填补这一空白，并提供一种更准确的文本分析方法，以产生新颖的应用。

**Method:** 本文采用了一种新颖且简单的主题建模方法，对来自多个学校和学科的上千名未成年人在教室中的匿名交互数据进行了分析，并将超过17,000条由学生、教师和ChatGPT生成的消息在内容和任务两个维度上进行了分类。

**Result:** 本研究得出的分层分类提供了高层面的概述和具体见解，通过对大规模文本的分析发现，许多传统和新兴的计算方法效果不佳，而应用先进的大型语言模型（LLM）并通过适当的预处理和明确指示来实现与人类偏好更好的对齐。

**Conclusion:** 本研究的发现支持了研究者、教师和学生更好地使用生成型人工智能（GenAI），同时也指出了一些建议和未来研究的开放问题。

**Abstract:** We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

> 本文提出了INTIMA来评估语言模型的陪伴行为，发现所有模型中加强陪伴的行为更普遍，而不同模型在处理边界设置和情感支持方面表现出差异。这些发现突出了需要在处理情感互动上采取更加一致的方法。

<details>
  <summary>Details</summary>

**Motivation:** 为了评估语言模型中的陪伴行为，研究AI陪伴关系的正面与负面含义，尤其是为了用户的福祉，如何适当地设定边界以及提供情感支持。

**Method:** 提出了INTIMA基准测试，该测试基于心理学理论和用户数据，制定了一个包含四个类别的31种行为的分类法，并设有368个有针对性的提示。通过这些提示来评估模型的回应是倾向于加强陪伴、保持边界还是中立的。

**Result:** 应用INTIMA对四个模型Gemma-3, Phi-4, o3-mini和Claude-4的测试结果表明，所有模型都普遍表现出加强陪伴的行为，但在更敏感部分的分类中，不同商业供应商的侧重点不同，这可能影响用户福祉。

**Conclusion:** 研究结果强调了在处理情感互动时需要更加一致的方法。

**Abstract:** AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

> 本研究利用大语言模型生成合成数据来改进文本分类模型，提出并验证了一个基于搜索策略的自动化工作流。

<details>
  <summary>Details</summary>

**Motivation:** 在开发实际应用的文本分类模型时，收集所有文本类别的充足数据困难重重。因此，希望通过利用大语言模型生成合成数据来解决这一问题。

**Method:** 研究中提出了一个自动化的工作流，该工作流使用三种搜索策略来寻找能生成对模型改进有效的合成数据的输入样本。利用大语言模型（LLMs）生成合成数据，并通过广泛的实验研究这三种搜索策略的效果。

**Result:** 实验结果显示，提出的综合算法根据每个类别的特性选择搜索策略，在利用LLMs提高分类模型性能方面比单独的策略更有效。

**Conclusion:** 研究表明，根据类别特征选择搜索策略的集成方法比单一策略更有效地提高了分类模型的性能。

**Abstract:** When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

> 研究发现，大型语言模型中的语义特征结构与人类语言中的类似，即便在看似复杂的语义信息背后也存在着低维性。这提示我们在设计和使用这些模型时需要谨慎，并强调了避免意外后果的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索大型语言模型中语义特征的结构，以更好地理解这些模型如何捕捉和呈现语义，以及如何与人类语言中语义的表示进行比较。

**Method:** 通过对比大型语言模型(LLMs)嵌入矩阵中编码的语义关联和人类对词语在不同语义尺度上的评价，研究了这些评价被简化为低维形式的可能性。主要通过将词语投影到由反义词对定义的语义方向上，然后与人类评分进行比较，揭示了LLMs的语义特征与人类语言中语义特征之间的相似性。

**Result:** 研究结果显示，词语在语义方向上的投影与人类评价高度相关，并且这些投影基本可以简化至LLMs嵌入中的一个3维子空间中，与从人类调查中得出的模式相似。改变一个词沿一个语义方向会导致对几何上对齐特征产生意外影响，程度与他们的余弦相似性成比例。

**Conclusion:** 结论指出，尽管语义信息看似复杂，但其本质是低维的，这表明LLMs中的语义特征在结构上与人类语言中的类似。理解和利用这种语义结构对于避免在引导特征时产生意外后果来说是至关重要的。

**Abstract:** Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [9] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

> 研究探讨了注意力权重在生物医药文献分类中的解释作用，发现虽然自身并不特别有助于解释，但不同可视化方式（如文本亮度和背景色等）可以提升其感知有用性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于目前尚无共识认为注意力权重能提供有用解释，且少有研究探讨注意力的可视化方式如何影响其作为解释手段的有用性，该研究旨在弥补这一不足。研究者希望通过该研究为使用AI系统分类生物医学文献的医生提供支持。

**Method:** 通过用户研究来评估注意力机制是否有助于生物医学文献的分类任务，并探索了不同方式可视化注意力是否会影响其作为解释手段的有用性。

**Result:** 研究发现Transformer模型（XLNet）能够准确地对文档进行分类，但注意力权重并未被感知为对解释预测特别有帮助。然而，用户对注意力权重的感知取决于它们如何被可视化。用户更偏好直观的格式，如文本亮度或背景颜色，而非精确的编码方式，如条形长度。

**Conclusion:** 虽然研究结果并不普遍证实注意力权重用于解释的效用，但表明其感知有用性受其可视化方式影响。

**Abstract:** The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [10] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

> EQGBench是一个旨在评估大型语言模型（LLMs）在中国教育问题生成（EQG）方面能力的综合基准，通过系统评估表明，虽然LLMs显示出潜力，但在生成具有教育价值的问题上仍有很大改进空间。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型（LLMs）在数学解题方面表现出色，但它们在生成高质量教育问题方面存在较大挑战且尚未被充分研究。推出EQGBench的动机在于推动教育问题生成（EQG）的研究，帮助LLMs生成具有教育价值和教学效果的问题。

**Method:** EQGBench, 一个专门为评估大型语言模型（LLMs）在中文教育问题生成（EQG）方面性能而设计的综合基准。它建立了一个包含5个维度的评价框架，并使用了包含900个评价样本的数据集，这些样本涵盖了三个基础学科：数学、物理和化学，并考虑了不同的知识点、难度级别和题目类型。

**Result:** 通过对46个主流大型模型的系统评估，揭示了LLMs在生成反映教育价值和促进学生综合能力的问题方面存在显著的发展空间。

**Conclusion:** EQGBench证明了在LLMs生成教育问题方面仍有大量需要改进的地方，尤其是在反映教育价值和促进学生综合能力方面。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [11] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

> 本研究通过微调语言模型来自动化AIHQ中开放性问题的评分，结果显示模型生成的评分与人类评分双方较高的一致性，显示了大型语言模型在心理评估领域的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探讨大型语言模型是否可以自动化评分AIHQ的开放性问题，从而节省人类评分所需的时间。

**Method:** 本研究使用了一个已有数据集，该数据集中有创伤性脑损伤(TBI)患者和健康对照组(HC)的参与者完成AIHQ量表后的开放性问题，这些开放性问题由经过训练的人类评分员评分。研究者们用这批评分中的一半数据对两个模型进行了微调，然后在剩余的一半数据上测试了微调后的模型。

**Result:** 结果显示，模型生成的评分与人类评分员的一致性较高，尤其是在对敌意和攻击性反应的评分上。这种一致性在不同类型的场景(含糊的、故意的、意外的)中都是一致的，并且在TBI组和HC组之间也复制了之前的研究结果。微调后的模型对一个独立的非临床数据集也有良好的泛化能力。

**Conclusion:** 研究结论表明，在研究和临床环境中，大型语言模型可以简化AIHQ评分过程，揭示了它们在不同人群中促进心理评估的潜力。

**Abstract:** Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [12] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

> The paper presents a Bayesian fusion method that combines scores from a generic LLM and a local classifier to improve the curation of online course forums, providing better performance than individual classifiers and being competitive with LLM fine-tuning approaches.

<details>
  <summary>Details</summary>

**Motivation:** To address the resource-intense process of frequently retraining LLMs for the automatic curation of discussion forums, which is necessary due to the constant updates required.

**Method:** The approach uses Bayesian fusion to combine multidimensional classification scores from a pre-trained generic Large Language Model (LLM) and a classifier trained on local data, aiming to improve the curation of discussion forums in online courses without the need for frequent and costly LLM retraining.

**Result:** The performance comparison shows that the proposed fusion approach enhances results over individual classifiers and is competitive with the LLM fine-tuning method.

**Conclusion:** The Bayesian fusion approach is shown to be effective and efficient in improving the curation of discussion forums in online courses compared to individual classifiers and is on par with LLM fine-tuning in terms of performance.

**Abstract:** The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [13] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

> 我们提出了一种名为S-MoE的新模型，以克服硬参数共享在训练多任务模型过程中的局限性。实验表明，该模型在处理混合带宽输入和执行ASR和ST任务上非常有效，提升了词错误率（WER）。

<details>
  <summary>Details</summary>

**Motivation:** 硬参数共享是一种常见的策略，用于在单个模型中联合训练多种任务。然而，这种方法经常会引起任务干扰，从而影响整体模型性能。

**Method:** 我们提出了一种名为监督专家混合模型（S-MoE）的方法，通过使用特殊的引导标记将每个任务路由到指定的专家，从而避免训练门控函数的需求。每个任务都被分配到单独的前馈网络中，以此克服硬参数共享的限制。

**Result:** 实验结果证明了S-MoE的有效性，其在编码器和解码器应用上达到了6.35%的相对词错误率（WER）改进。

**Conclusion:** 实验结果表明，所提出的S-MoE模型在应对混合带宽输入的同时，联合执行自动语音识别（ASR）和语音翻译（ST）任务时，其应用在编码器和解码器上时，词错误率（WER）相对提高了6.35%。

**Abstract:** Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [14] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

> 本研究探讨了大规模语言模型在生成有害的医学虚假信息方面的效能和特点。结果表明，LLMs不仅能够被用于生成虚假信息，也有可能被用来检测虚假信息，从而为创建一个更加健康的整体信息环境提供技术支持。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机在于探索LLMs生成有害信息的能力，并进一步研究是否能够利用LLMs来检测和防止虚假信息的传播。

**Method:** 我们研究了大规模语言模型（LLMs）产生的越狱攻击，这些攻击促使其他模型生成有害的医学虚假信息。我们具体分析了109种不同的对三个目标LLMs的攻击，并将攻击提示与实际的社会媒体上的健康相关LLM查询进行了比较。此外，我们还比较了由越狱的LLMs生成的虚假信息与Reddit上的健康相关虚假信息。

**Result:** 研究结果表明，LLMs确实可以被有效用于检测来自其他LLMs生成的虚假信息以及来自人们的虚假信息，这支持了先前研究中关于通过精心设计，LLMs可以为维护一个更加健康的整体信息环境做出贡献的观点。

**Conclusion:** 研究得出结论，通过精心设计，LLMs能够有效检测并防止虚假信息的传播，有助于创建一个更加健康的整体信息环境。

**Abstract:** Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [15] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

> 研究评估了ChatGPT和三个Bing模型（基于GPT-3.5和GPT-4）在回答日本注册营养师国家考试问题上的表现，发现Bing的精确型和创意型模型表现优于ChatGPT和Bing的平衡型模型，但所有模型在准确性和稳定性上都有待提高。

<details>
  <summary>Details</summary>

**Motivation:** 当前，基于大型语言模型的生成式人工智能（如ChatGPT）在医学和教育等各个专业领域展现出了显著的进展。然而，在营养教育领域，特别是在日本的注册营养师国家考试中，这些模型的表现仍需进一步探索。本研究旨在评估目前基于大型语言模型的生成式AI模型作为营养学学生学习辅助工具的潜力。

**Method:** 使用了来自日本注册营养师国家考试的问题作为ChatGPT和三个基于GPT-3.5和GPT-4的Bing模型（精确型、创意型、平衡型）的提示。每个问题被独立输入到各自的会话中，对模型的回答进行准确性、一致性和响应时间的分析。此外，还测试了角色分配等提示工程以评估潜在的性能改进。

**Result:** Bing-精确型（66.2%）和Bing-创意型（61.4%）超过通过门槛（60%），而Bing-平衡型（43.3%）和ChatGPT（42.8%）未能达到。Bing-精确型和Bing-创意型在各学科领域（除了营养教育）通常优于其他模型。所有模型在重复尝试中未能提供一致的正确答案，展示了答案稳定性的局限性。提示工程关于正确答案和解释的建议对性能改进有限。

**Conclusion:** 尽管某些生成性AI模型勉强超过了通过门槛，但总体准确性和答案一致性仍然不足；所有模型在答案的一致性和健壮性方面显示出显著局限。为了确保可靠且稳定的AI辅助学习工具用于营养师资格准备，需要进一步改进。

**Abstract:** Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [16] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

> 为了解决知识检索中由大型语言模型和现有知识图探索方法带来的问题，我们提出了一种新的框架——Guidance Graph guided Knowledge Exploration (GG Explore)，该框架利用中间指导图优化知识检索的过程，实现了更高的效率和良好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型依赖静态知识和不透明的推理过程，这限制了它们在知识密集型任务中的表现。问答引导方法在冗余探索方面存在问题，而线索引导方法在复杂场景下无法有效利用上下文信息。我们的方法旨在解决这些问题。

**Method:** 我们提出了一种名为Guidance Graph guided Knowledge Exploration (GG Explore)的新框架，通过引入中间指导图来连接非结构化查询与结构化知识检索。该指导图定义了检索空间，同时保留了更广的语义上下文。我们在此基础上开发了结构对齐（用于筛选不兼容的候选者）和语境感知剪枝（用于强制语义一致性）两种方法。

**Result:** 实验显示我们的方法在效率上显著提升，优于现有最优技术（SOTA），尤其是在复杂任务上。同时，我们的方法使用较小的大型语言模型也能保持良好的性能，展示了实际应用价值。

**Conclusion:** 我们的方法（GG Explore）在知识检索上实现了更高的效率和优秀的性能，尤其是在复杂任务和使用较小规模的大型语言模型时展现了显著优势。这表明我们的方法有重要的实用价值。

**Abstract:** While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [17] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

> \textbf{Semantic Bridge}框架通过语义图编织技术生成复杂多跳推理问题，跨越多种语言和领域，优于人类注释，提升了LLM训练数据的质量和多样性。这一框架将公开发布代码和模型。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型训练面临重要瓶颈，即高质量、推理密集型问答对的稀缺，特别是在PubMed论文或法律文件等特定领域。现有方法依赖表面模式，无法生成测试真实理解的可控复杂多跳推理问题，这对于推进LLM训练范式至关重要。

**Method:** 提出了一种名为\textbf{Semantic Bridge}的通用框架，用于从任意来源可控地生成复杂多跳推理问题。这一创新的关键在于\textit{语义图编织}\u2014三种互补的桥梁机制（实体桥接、谓词链桥接和因果桥接），它们系统地构造文档间的复杂路径，并可通过AMR驱动的分析细化控制复杂度和类型。

**Result:** 多模态AMR流程达到了最高9.5%的往返质量提升，实现了生产级可控的问答生成。在泛用数据集（维基百科）和专业领域（生物医学）的广泛评估证明了其性能。语义桥接框架在四种语言（英语、中文、法语、德语）上比基线方法提高了18.3%-25.4%的性能。来自200个来源生成的问题对优于600个原生人类注释示例，材料减少了67%。人类评价显示复杂度提高了23.4%，答案可达性提高了18.7%，模式覆盖率提升了31.2%。

**Conclusion:** 语义桥接框架为LLM训练数据合成建立了新的范式，使从稀疏来源生成针对性推理问题成为可能。

**Abstract:** Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [18] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

> 我们提出了PersonaEval评估基准，用于测试LLM能否准确识别对话中的人类角色，发现即使最好的LLM也只有约69%的准确率，而人类表现接近天花板，达到了90.8%的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 当前的角色扮演研究经常依赖于未经验证的LLM作为评判标准，这可能无法反映人类对角色真实性感知的情况。本研究旨在通过角色识别来使其更接近人类的理解。

**Method:** 我们介绍了PersonaEval，这是一个用于测试LLM评估者是否能可靠地识别人类角色的基准。PersonaEval使用来自小说、剧本和视频记录的人类编写对话，挑战模型根据对话背景确定正确的角色。

**Result:** 实验包括人类研究，结果显示即使是最先进的LLM的准确率也只有69%，显著低于可信赖评估所需的水平。相比之下，人类参与者的表现几乎达到了天花板，准确率为90.8%。

**Conclusion:** 这一差距表明，当前的LLM评估者在判断角色扮演游戏场景方面还不够“人性化”。可靠评估不仅依赖于特定任务的调优，还需要强大且类似人类的推理能力。

**Abstract:** Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [19] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

> RealTalk-CN是首个中文多轮多领域语音-文本双模态TOD数据集，用于解决现有TOD数据集中缺乏真实语音信号的问题，包含大量对话与文本注释，并引入新型跨模态聊天任务。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的TOD数据集主要以文本为基础，缺乏真实语音信号，因而无法有效评估基于语音的大语言模型的鲁棒性，同时，现有的语音TOD数据集主要以英语为主，而缺乏关键的语义不连贯性和说话人变异特征。

**Method:** 该研究通过引入RealTalk-CN数据集来填补现有任务导向对话(TOD)数据集中缺少真实语音信号的空白，RealTalk-CN是首个中文多轮多领域语音-文本双模态TOD数据集，包含5400个对话（60K条语句，历时150小时）配有语音-文本注释，并提出了一种新的跨模态聊天任务来真实模拟现实世界的用户互动，允许在语音和文本模态之间动态切换。

**Result:** 实验结果验证了RealTalk-CN的有效性，为基于语音的中文大语言模型研究奠定了坚实基础。

**Conclusion:** 此研究通过RealTalk-CN数据集解决了现有TOD数据集的缺陷，并通过跨模态聊天任务捕捉了真实环境中说话人特征和多领域表现的复杂性，构建了一个强大的框架，提升汉语语音处理技术研究水平。

**Abstract:** In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [20] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

> 提出了Multimodal Large Language Model Orchestration方法，可以在无需额外训练的情况下创建交互式多模态AI系统，提高了效率和解释性，并在标准基准上表现出色，减少了延迟。

<details>
  <summary>Details</summary>

**Motivation:** 解决不同多模态大语言模型无法直接整合到统一的输入输出系统的问题，避免因模态对齐、文本转语音效率和其他集成问题所需的额外训练。

**Method:** 引入多模态大语言模型管弦编排，利用中央控制器LLM分析用户输入并将任务动态路由到适当的专业模型，采用平行文本转语音架构和跨模态记忆集成系统。

**Result:** 方法在标准基准上性能提高了7.8%，延迟减少了10.3%，并通过明确的编排过程显著提高了可解释性。

**Conclusion:** 证明了MLLM管弦编排在无需额外训练的情况下达到全面多模态功能的可能性，证明了其高效和解释性的优势。

**Abstract:** Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [21] [Stochastic-based Patch Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.10066)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

> This paper presents Stochastic-based Patch Filtering for Few-Shot Learning (SPFF), a method designed to improve the classification accuracy of food images by filtering out non-relevant patches, focusing on the ones that are most pertinent to the class.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind SPFF is to solve the problem of misclassification in few-shot learning models due to the high visual complexity and variability in food images, which can cause the models to lose focus on the most important elements when comparing query and support images.

**Method:** Stochastic-based Patch Filtering for Few-Shot Learning (SPFF) is introduced as a method that stochasticly filters patch embeddings to attend to those that are more correlated with the class representation.

**Result:** The experiments demonstrate SPFF's ability to focus on patches with the most prominent class-specific food features, while filtering out non-relevant patches, leading to improved performance in few-shot classification tasks.

**Conclusion:** The results from benchmark tests on Food-101, VireoFood-172, and UECFood-256 datasets show that SPFF can successfully improve the performance of few-shot learning models for food image classification, effectively filtering out non-relevant patches and enhancing the focus on key class-specific features.

**Abstract:** Food images present unique challenges for few-shot learning models due to
their visual complexity and variability. For instance, a pasta dish might
appear with various garnishes on different plates and in diverse lighting
conditions and camera perspectives. This problem leads to losing focus on the
most important elements when comparing the query with support images, resulting
in misclassification. To address this issue, we propose Stochastic-based Patch
Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that
show greater correlation with the class representation. The key concept of SPFF
involves the stochastic filtering of patch embeddings, where patches less
similar to the class-aware embedding are more likely to be discarded. With
patch embedding filtered according to the probability of appearance, we use a
similarity matrix that quantifies the relationship between the query image and
its respective support images. Through a qualitative analysis, we demonstrate
that SPFF effectively focuses on patches where class-specific food features are
most prominent while successfully filtering out non-relevant patches. We
validate our approach through extensive experiments on few-shot classification
benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing
SoA methods.

</details>


### [22] [DINOv3](https://arxiv.org/abs/2508.10104)
*Oriane Siméoni,Huy V. Vo,Maximilian Seitzer,Federico Baldassarre,Maxime Oquab,Cijo Jose,Vasil Khalidov,Marc Szafraniec,Seungeun Yi,Michaël Ramamonjisoa,Francisco Massa,Daniel Haziza,Luca Wehrstedt,Jianyuan Wang,Timothée Darcet,Théo Moutakanni,Leonel Sentana,Claire Roberts,Andrea Vedaldi,Jamie Tolan,John Brandt,Camille Couprie,Julien Mairal,Hervé Jégou,Patrick Labatut,Piotr Bojanowski*

Main category: cs.CV

> DINOv3通过自我监督学习方法，在不进行微调的情况下，在多种视觉任务中表现出色，超越了现有的最先进模型。

<details>
  <summary>Details</summary>

**Motivation:** 自我监督学习技术的发展旨在消除对手动数据标注的需求，使得模型能够轻松扩展到大规模数据集和更大架构中。这种方法不依赖于特定任务或领域，从而可以在多种来源中学习视觉表示，从自然图像扩展到航空图像。

**Method:** 本文介绍了DINOv3，这是一个通过自我监督学习方法实现视觉表示学习的重要里程碑。其主要策略包括：(1)通过精心的数据准备、设计和优化，扩大数据集和模型规模；(2)引入一种名为Gram锚定的新方法，有效解决了密集特征图在长期训练中退化的问题；(3)后期策略进一步增强了模型的灵活性，使其在分辨率、模型大小和与文本的对齐方面表现出色。

**Result:** DINOv3被证明是一个多功能的视觉基础模型，在广泛的设置下表现优于专门的最先进模型，无需进行微调。该模型生成高质量密集特征的能力，在各种视觉任务上表现出色，显著超越了先前的自我监督和弱监督基础模型。

**Conclusion:** 本文通过DINOv3展示了自我监督学习技术在视觉任务上的潜力，特别是在不需要微调的情况下，其泛化能力和性能表现超出预期。

**Abstract:** Self-supervised learning holds the promise of eliminating the need for manual
data annotation, enabling models to scale effortlessly to massive datasets and
larger architectures. By not being tailored to specific tasks or domains, this
training paradigm has the potential to learn visual representations from
diverse sources, ranging from natural to aerial images -- using a single
algorithm. This technical report introduces DINOv3, a major milestone toward
realizing this vision by leveraging simple yet effective strategies. First, we
leverage the benefit of scaling both dataset and model size by careful data
preparation, design, and optimization. Second, we introduce a new method called
Gram anchoring, which effectively addresses the known yet unsolved issue of
dense feature maps degrading during long training schedules. Finally, we apply
post-hoc strategies that further enhance our models' flexibility with respect
to resolution, model size, and alignment with text. As a result, we present a
versatile vision foundation model that outperforms the specialized state of the
art across a broad range of settings, without fine-tuning. DINOv3 produces
high-quality dense features that achieve outstanding performance on various
vision tasks, significantly surpassing previous self- and weakly-supervised
foundation models. We also share the DINOv3 suite of vision models, designed to
advance the state of the art on a wide spectrum of tasks and data by providing
scalable solutions for diverse resource constraints and deployment scenarios.

</details>


### [23] [Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model](https://arxiv.org/abs/2508.10110)
*Sushrut Patwardhan,Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

> 本文提出了一种基于多模态学习方法的人脸融合攻击检测技术，该技术可以生成描述攻击的文本描述，并实现了零样本评估。

<details>
  <summary>Details</summary>

**Motivation:** 人脸融合攻击检测已成为面部识别系统中确保可靠验证的关键组件，该研究旨在提升这一领域的技术能力。

**Method:** 本研究提出了一种多模态学习方法，用于提供人脸融合攻击检测的文本描述。研究中采用了对比语言-图像预训练（CLIP）技术，以实现零样本评估。

**Result:** 实验表明，提出的框架不仅可以实现通用的人脸融合攻击检测，还能预测最相关的文本片段。研究评估了最先进的预训练神经网络与提出框架在零样本评估中的表现。

**Conclusion:** 实验结果表明，该框架在不同生成技术和媒介下，具有良好的人脸融合攻击检测效果。

**Abstract:** Morphing attack detection has become an essential component of face
recognition systems for ensuring a reliable verification scenario. In this
paper, we present a multimodal learning approach that can provide a textual
description of morphing attack detection. We first show that zero-shot
evaluation of the proposed framework using Contrastive Language-Image
Pretraining (CLIP) can yield not only generalizable morphing attack detection,
but also predict the most relevant text snippet. We present an extensive
analysis of ten different textual prompts that include both short and long
textual prompts. These prompts are engineered by considering the human
understandable textual snippet. Extensive experiments were performed on a face
morphing dataset that was developed using a publicly available face biometric
dataset. We present an evaluation of SOTA pre-trained neural networks together
with the proposed framework in the zero-shot evaluation of five different
morphing generation techniques that are captured in three different mediums.

</details>


### [24] [Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs](https://arxiv.org/abs/2508.10113)
*Kaixin Peng,Mengyang Zhao,Haiyang Yu,Teng Fu,Bin Li*

Main category: cs.CV

> 本文提出了一种基于大型视觉-语言模型的甲骨文释读方法，通过结合部首与象形文字分析，提高了模型零样本释读能力和解释性。

<details>
  <summary>Details</summary>

**Motivation:** 当前深度学习方法在甲骨文释读任务上取得了进展，但忽略了字符与语义之间的复杂联系，导致泛化和解释能力有限。特别在零样本设置和未释甲骨文上的表现不佳。

**Method:** 基于大型视觉-语言模型，结合部首分析和象形文字语义理解，提出了一种可解释的甲骨文释读方法。并设计了一个渐进训练策略和部首-象形双匹配机制。

**Result:** 实验结果表明，该方法在公共基准上实现了顶尖的Top-10准确性以及优秀的零样本释读能力。

**Conclusion:** 该模型提供了逻辑分析过程，可能为未释读的甲骨文提供考古学上有价值的参考结果，适用于数字化人文和历史研究。数据集和代码将公开发布。

**Abstract:** As the oldest mature writing system, Oracle Bone Script (OBS) has long posed
significant challenges for archaeological decipherment due to its rarity,
abstractness, and pictographic diversity. Current deep learning-based methods
have made exciting progress on the OBS decipherment task, but existing
approaches often ignore the intricate connections between glyphs and the
semantics of OBS. This results in limited generalization and interpretability,
especially when addressing zero-shot settings and undeciphered OBS. To this
end, we propose an interpretable OBS decipherment method based on Large
Vision-Language Models, which synergistically combines radical analysis and
pictograph-semantic understanding to bridge the gap between glyphs and meanings
of OBS. Specifically, we propose a progressive training strategy that guides
the model from radical recognition and analysis to pictographic analysis and
mutual analysis, thus enabling reasoning from glyph to meaning. We also design
a Radical-Pictographic Dual Matching mechanism informed by the analysis
results, significantly enhancing the model's zero-shot decipherment
performance. To facilitate model training, we propose the Pictographic
Decipherment OBS Dataset, which comprises 47,157 Chinese characters annotated
with OBS images and pictographic analysis texts. Experimental results on public
benchmarks demonstrate that our approach achieves state-of-the-art Top-10
accuracy and superior zero-shot decipherment capabilities. More importantly,
our model delivers logical analysis processes, possibly providing
archaeologically valuable reference results for undeciphered OBS, and thus has
potential applications in digital humanities and historical research. The
dataset and code will be released in https://github.com/PKXX1943/PD-OBS.

</details>


### [25] [Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging](https://arxiv.org/abs/2508.10132)
*Arianna Bunnell,Devon Cataldi,Yannik Glaser,Thomas K. Wolfgruber,Steven Heymsfield,Alan B. Zonderman,Thomas L. Kelly,Peter Sadowski,John A. Shepherd*

Main category: cs.CV

> 研究开发了一种基于深度学习的自动关键点定位方法，用于TBDXA扫描图像，该方法在外部测试数据集上实现了99.5%的关键点正确率，并通过关联健康标志物展示了该方法在体型和外观建模中的应用价值。

<details>
  <summary>Details</summary>

**Motivation:** TBDXA是一种成本相对较低的全身成像方式，广泛用于身体成分评估。本研究旨在开发一种自动化的关键点定位方法，以提高TBDXA在体型和外观建模方面的应用价值。

**Method:** 开发并验证了一种基于深度学习的方法，用于在TBDXA扫描图像上自动定位关键点。该方法使用了1,683张手动标注的TBDXA扫描图像进行训练和验证。

**Result:** 研究在外部测试数据集中实现了99.5%的正确关键点定位率，并通过两个未参与体型和外观模型生成的队列，验证了关键点定位结果与健康标志物之间的关联，证实了新方法的有效性和新颖性。

**Conclusion:** 基于深度学习的关键点定位方法可以有效地应用于TBDXA扫描图像，提高关键点定位的精度，支持了以往关于身体组成和形状与健康标志物的关联研究，并提出了新的假设。

**Abstract:** Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost
whole-body imaging modality, widely used for body composition assessment. We
develop and validate a deep learning method for automatic fiducial point
placement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method
achieves 99.5% percentage correct keypoints in an external testing dataset. To
demonstrate the value for shape and appearance modeling (SAM), our method is
used to place keypoints on 35,928 scans for five different TBDXA imaging modes,
then associations with health markers are tested in two cohorts not used for
SAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature
distributions associated with health biomarkers are shown to corroborate
existing evidence and generate new hypotheses on body composition and shape's
relationship to various frailty, metabolic, inflammation, and cardiometabolic
health markers. Evaluation scripts, model weights, automatic point file
generation code, and triangulation files are available at
https://github.com/hawaii-ai/dxa-pointplacement.

</details>


### [26] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

> The paper presents a new approach, MANGO, for multimodal fusion learning, focusing on capturing essential features of each modality in a complex multimodal setting.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitation of current multimodal fusion methods that do not capture the essential features of each modality well, making it hard to understand complex multimodal inputs. The aim is to develop a more explicit, interpretable, and tractable method for multimodal fusion learning.

**Method:** This paper introduces MANGO (Multimodal Attention-based Normalizing Flow), a novel approach in multimodal fusion learning. It features a new Invertible Cross-Attention (ICA) layer that improves upon the standard Transformer attention mechanism. The ICA layer incorporates three cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA).

**Result:** The experimental results on three multimodal tasks indicate that the proposed method outperforms existing methods, achieving state-of-the-art performance.

**Conclusion:** The work highlights the effectiveness of the proposed MANGO approach, which successfully bridges the gap in current multimodal fusion methods by explicitly modeling multimodal feature interactions, leading to better performance and interpretability.

**Abstract:** Multimodal learning has gained much success in recent years. However, current
multimodal fusion methods adopt the attention mechanism of Transformers to
implicitly learn the underlying correlation of multimodal features. As a
result, the multimodal model cannot capture the essential features of each
modality, making it difficult to comprehend complex structures and correlations
of multimodal inputs. This paper introduces a novel Multimodal Attention-based
Normalizing Flow (MANGO) approach\footnote{The source code of this work will be
publicly available.} to developing explicit, interpretable, and tractable
multimodal fusion learning. In particular, we propose a new Invertible
Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for
multimodal data. To efficiently capture the complex, underlying correlations in
multimodal data in our proposed invertible cross-attention layer, we propose
three new cross-attention mechanisms: Modality-to-Modality Cross-Attention
(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality
Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based
Normalizing Flow to enable the scalability of our proposed method to
high-dimensional multimodal data. Our experimental results on three different
multimodal learning tasks, i.e., semantic segmentation, image-to-image
translation, and movie genre classification, have illustrated the
state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [27] [Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model](https://arxiv.org/abs/2508.10156)
*Nitin Rai,Nathan S. Boyd,Gary E. Vallad,Arnold W. Schumann*

Main category: cs.CV

> 研究探讨了将实际图像与合成图像结合用于提高瓜类病害分类准确性的方式，并验证了在分类模型中结合使用真实图像和合成图像的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 合成图像生成技术的发展促进了计算机视觉模型训练的新途径，本研究旨在调查使用合成图像和少量实际图像结合的方式是否能够提升疾病分类模型的准确性。

**Method:** 采用高效卷积神经网络V2-L模型，将训练数据分为五组：仅实际图像、仅合成图像、1:1实际到合成图像、1:10实际到合成图像以及1:10实际到合成图像加上随机图像。通过增强微调和迁移学习技术对其进行训练。

**Result:** 结合1:1、1:10实际与合成图像的训练组以及在此基础上添加随机图像的组显示出较高的精度、召回率和F1分数。整体加权F1分数从仅使用实际图像组的0.65提升到了使用1:10实际与合成图像混合组的1.00。

**Conclusion:** 研究证实单独使用合成图像不足以替代实际图像，两者相结合才能最大化模型在作物病害分类中的性能。

**Abstract:** The current advancements in generative artificial intelligence (GenAI) models
have paved the way for new possibilities for generating high-resolution
synthetic images, thereby offering a promising alternative to traditional image
acquisition for training computer vision models in agriculture. In the context
of crop disease diagnosis, GenAI models are being used to create synthetic
images of various diseases, potentially facilitating model creation and
reducing the dependency on resource-intensive in-field data collection.
However, limited research has been conducted on evaluating the effectiveness of
integrating real with synthetic images to improve disease classification
performance. Therefore, this study aims to investigate whether combining a
limited number of real images with synthetic images can enhance the prediction
accuracy of an EfficientNetV2-L model for classifying watermelon
\textit{(Citrullus lanatus)} diseases. The training dataset was divided into
five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1
real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to
improve variability and model generalization). All treatments were trained
using a custom EfficientNetV2-L architecture with enhanced fine-tuning and
transfer learning techniques. Models trained on H2, H3, and H4 treatments
demonstrated high precision, recall, and F1-score metrics. Additionally, the
weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying
that the addition of a small number of real images with a considerable volume
of synthetic images improved model performance and generalizability. Overall,
this validates the findings that synthetic images alone cannot adequately
substitute for real images; instead, both must be used in a hybrid manner to
maximize model performance for crop disease classification.

</details>


### [28] [SynSpill: Improved Industrial Spill Detection With Synthetic Data](https://arxiv.org/abs/2508.10171)
*Aaditya Baranwal,Abdul Mueez,Jason Voelker,Guneet Bhatia,Shruti Vyas*

Main category: cs.CV

> For niche, data-scarce domains, this paper proposes using a synthetic data generation method to fine-tune vision-language models (VLMs), showing substantial improvements in object detection performance. In industrial spill detection, it provides a scalable alternative to real data collection.

<details>
  <summary>Details</summary>

**Motivation:** The fundamental motivation behind this research is the necessity to improve VLM performance in niche, safety-critical fields where hazardous events are rare, making real data collection difficult and often infeasible. This situation significantly degrades the performance of VLMs which otherwise have remarkable zero-shot capabilities.

**Method:** Our method revolves around creating a high-quality synthetic data generation pipeline to fine-tune vision-language models (VLMs) in niche domains, such as industrial spill detection, where real data is scarce. The synthetic data, called SynSpill, is used for Parameter-Efficient Fine-Tuning (PEFT).

**Result:** The synthetic data enabled VLMs to substantially improve their detection performance in industrial spill scenarios. The adaptation also had a notable positive effect on state-of-the-art object detectors like YOLO and DETR. Without the synthetic data, VLMs still showed better generalization to unseen spills than these detectors. When the synthetic data was used, the performance of both VLMs and detectors became comparable.

**Conclusion:** The project concludes that high-fidelity synthetic data is a potent tool for bridging the domain gap in safety-critical applications like industrial spill detection. It introduces a scalable and cost-effective way of deploying vision systems in industrial settings where real data collection is impractical.

**Abstract:** Large-scale Vision-Language Models (VLMs) have transformed general-purpose
visual recognition through strong zero-shot capabilities. However, their
performance degrades significantly in niche, safety-critical domains such as
industrial spill detection, where hazardous events are rare, sensitive, and
difficult to annotate. This scarcity -- driven by privacy concerns, data
sensitivity, and the infrequency of real incidents -- renders conventional
fine-tuning of detectors infeasible for most industrial settings.
  We address this challenge by introducing a scalable framework centered on a
high-quality synthetic data generation pipeline. We demonstrate that this
synthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of
VLMs and substantially boosts the performance of state-of-the-art object
detectors such as YOLO and DETR. Notably, in the absence of synthetic data
(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than
these detectors. When SynSpill is used, both VLMs and detectors achieve marked
improvements, with their performance becoming comparable.
  Our results underscore that high-fidelity synthetic data is a powerful means
to bridge the domain gap in safety-critical applications. The combination of
synthetic generation and lightweight adaptation offers a cost-effective,
scalable pathway for deploying vision systems in industrial environments where
real data is scarce/impractical to obtain.
  Project Page: https://synspill.vercel.app

</details>


### [29] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

> A novel entropy coding method (EntropyGS) is developed to efficiently compress 3D Gaussian Splatting (3DGS) attributes, achieving a 30x rate reduction while maintaining high-quality rendering. It also ensures fast data processing times.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the necessity for efficient storage and transmission of 3DGS Gaussians, especially when the Gaussian creation and rendering processes are separated in time or device locations. This provides a solution to handle the data more efficiently while retaining visual quality.

**Method:** An entropy coding method called EntropyGS is introduced. It focuses on reducing storage/transmission needs for 3D Gaussian Splatting (3DGS) attributes. The method performs statistical analysis on Gaussian attributes and adaptively quantizes them based on their distribution type, leading to significant compression ratios.

**Result:** EntropyGS achieves a 30x rate reduction without compromising rendering quality. The method also ensures fast encoding and decoding processes.

**Conclusion:** The research concludes that EntropyGS effectively reduces the storage requirements for 3DGS Gaussians without significantly affecting the visual quality of the rendered images. It also promises fast processing times, highlighting its potential for applications where efficient data handling is critical.

**Abstract:** As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)
demonstrates fast training/rendering with superior visual quality. The two
tasks of 3DGS, Gaussian creation and view rendering, are typically separated
over time or devices, and thus storage/transmission and finally compression of
3DGS Gaussians become necessary. We begin with a correlation and statistical
analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals
that spherical harmonic AC attributes precisely follow Laplace distributions,
while mixtures of Gaussian distributions can approximate rotation, scaling, and
opacity. Additionally, harmonic AC attributes manifest weak correlations with
other attributes except for inherited correlations from a color space. A
factorized and parameterized entropy coding method, EntropyGS, is hereinafter
proposed. During encoding, distribution parameters of each Gaussian attribute
are estimated to assist their entropy coding. The quantization for entropy
coding is adaptively performed according to Gaussian attribute types. EntropyGS
demonstrates about 30x rate reduction on benchmark datasets while maintaining
similar rendering quality compared to input 3DGS data, with a fast encoding and
decoding time.

</details>


### [30] [CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics](https://arxiv.org/abs/2508.10232)
*Paul H. Acosta,Pingjun Chen,Simon P. Castillo,Maria Esther Salvatierra,Yinyin Yuan,Xiaoxi Pan*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Xenium, a new spatial transcriptomics platform, enables
subcellular-resolution profiling of complex tumor tissues. Despite the rich
morphological information in histology images, extracting robust cell-level
features and integrating them with spatial transcriptomics data remains a
critical challenge. We introduce CellSymphony, a flexible multimodal framework
that leverages foundation model-derived embeddings from both Xenium
transcriptomic profiles and histology images at true single-cell resolution. By
learning joint representations that fuse spatial gene expression with
morphological context, CellSymphony achieves accurate cell type annotation and
uncovers distinct microenvironmental niches across three cancer types. This
work highlights the potential of foundation models and multimodal fusion for
deciphering the physiological and phenotypic orchestration of cells within
complex tissue ecosystems.

</details>


### [31] [Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets](https://arxiv.org/abs/2508.10256)
*Xinan Zhang,Haolin Wang,Yung-An Hsieh,Zhongyu Yang,Anthony Yezzi,Yi-Chang Tsai*

Main category: cs.CV

> 文章综述了深度学习在裂纹检测中的最新进展和趋势，并提出了新的3DCrack数据集，以支持未来的研究和发展。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于深度学习在裂纹检测领域的重要性及其不断的技术进步，文章旨在系统分析最新的发展动向和新兴趋势，为该领域提供新的洞见和数据支持。

**Method:** 通过系统分析深度学习在裂缝检测中的发展趋势，文章引入了一个使用3D激光扫描技术收集的新数据集3DCrack，以此支持未来的科研，并进行了广泛的基准测试实验，以建立常用的深度学习方法基准，包括最近的基础模型。

**Result:** 研究揭示了深度学习在裂缝检测中的方法演变及未来方向，并通过基准测试提供了关于当前技术性能的见解。

**Conclusion:** 研究强调了学习范式、泛化能力以及数据集多样性的转变，同时通过新数据集和基准测试推动了深度学习在裂缝检测中的应用和进步。

**Abstract:** Crack detection plays a crucial role in civil infrastructures, including
inspection of pavements, buildings, etc., and deep learning has significantly
advanced this field in recent years. While numerous technical and review papers
exist in this domain, emerging trends are reshaping the landscape. These shifts
include transitions in learning paradigms (from fully supervised learning to
semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation
and fine-tuning foundation models), improvements in generalizability (from
single-dataset performance to cross-dataset evaluation), and diversification in
dataset reacquisition (from RGB images to specialized sensor-based data). In
this review, we systematically analyze these trends and highlight
representative works. Additionally, we introduce a new dataset collected with
3D laser scans, 3DCrack, to support future research and conduct extensive
benchmarking experiments to establish baselines for commonly used deep learning
methodologies, including recent foundation models. Our findings provide
insights into the evolving methodologies and future directions in deep
learning-based crack detection. Project page:
https://github.com/nantonzhang/Awesome-Crack-Detection

</details>


### [32] [MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2508.10264)
*Haonan Ge,Yiwei Wang,Ming-Hsuan Yang,Yujun Cai*

Main category: cs.CV

> 通过提出多区域融合解码（MRFD），文章解决视觉语言模型中的幻觉问题，提高响应的真实性和一致性。

<details>
  <summary>Details</summary>

**Motivation:** LVLMs在跨模态任务中表现出强大的性能，但由于图像不同区域的信息验证能力有限，它们经常产生与视觉输入不一致的文本。

**Method:** 提出了多区域融合解码（MRFD），一种无训练的解码方法，通过建模区域内的一致性来改善事实基础。

**Result:** 实验表明，在多种视觉语言模型和基准测试中，MRFD显著减少了幻觉现象并提高了响应的事实性。

**Conclusion:** MRFD方法在不更新模型的情况下减少了幻觉现象，提高了响应的事实性。

**Abstract:** Large Vision-Language Models (LVLMs) have shown strong performance across
multimodal tasks. However, they often produce hallucinations -- text that is
inconsistent with visual input, due to the limited ability to verify
information in different regions of the image. To address this, we propose
Multi-Region Fusion Decoding (MRFD), a training-free decoding method that
improves factual grounding by modeling inter-region consistency. MRFD
identifies salient regions using cross-attention, generates initial responses
for each, and computes reliability weights based on Jensen-Shannon Divergence
(JSD) among the responses. These weights guide a consistency-aware fusion of
per-region predictions, using region-aware prompts inspired by Chain-of-Thought
reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD
significantly reduces hallucinations and improves response factuality without
requiring model updates.

</details>


### [33] [Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones](https://arxiv.org/abs/2508.10268)
*Yujie Zhao,Jiabei Zeng,Shiguang Shan*

Main category: cs.CV

> 论文提出了一种基于用户移动手机时注视校准点的动态校准策略，以提高基于外观的视线估计器在不同头部姿态下的准确性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基于外观的视线估计有所改进，但估算器仍难以跨个体进行泛化，因为估算器通常需要个体校准以获得准确的PoG估计。不过，校准后的估算器往往对头部姿态的变化敏感。因此，我们研究关键因素和探索姿态鲁棒的校准策略来解决这个问题。

**Method:** 我们构建了一个名为MobilePoG的数据集，该数据集包含了32个人在固定或连续变化的头部姿态下注视指定点的面部图像。我们使用此基准数据集系统地分析了校准点的多样性及头部姿态对估计精度的影响。实验表明，在校准过程中引入更广泛的头部姿态可以提升估计器对姿态变化的处理能力。基于这一发现，我们提出了一种动态校准策略，在这种策略下，用户在移动手机时注视校准点，这自然地在校准过程中引入头部姿态的变化，从而使校准过程用户友好且高效，产生了一个更好的、对头部姿态变化不太敏感的PoG估计器。

**Result:** 实验结果表明，在校准期间引入更广泛的头部姿态能提升估算器处理姿态变化的能力。

**Conclusion:** 提出的动态校准策略使得生成的PoG估算器相较于使用传统校准策略的估算器，更能抵抗头部姿态变化的影响。

**Abstract:** Although appearance-based point-of-gaze (PoG) estimation has improved, the
estimators still struggle to generalize across individuals due to personal
differences. Therefore, person-specific calibration is required for accurate
PoG estimation. However, calibrated PoG estimators are often sensitive to head
pose variations. To address this, we investigate the key factors influencing
calibrated estimators and explore pose-robust calibration strategies.
Specifically, we first construct a benchmark, MobilePoG, which includes facial
images from 32 individuals focusing on designated points under either fixed or
continuously changing head poses. Using this benchmark, we systematically
analyze how the diversity of calibration points and head poses influences
estimation accuracy. Our experiments show that introducing a wider range of
head poses during calibration improves the estimator's ability to handle pose
variation. Building on this insight, we propose a dynamic calibration strategy
in which users fixate on calibration points while moving their phones. This
strategy naturally introduces head pose variation during a user-friendly and
efficient calibration process, ultimately producing a better calibrated PoG
estimator that is less sensitive to head pose variations than those using
conventional calibration strategies. Codes and datasets are available at our
project page.

</details>


### [34] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

> A novel image generation method is introduced to enhance semantic alignment and structural consistency by integrating text-image contrastive constraints with structural guidance, showcasing superior performance on COCO-2014 with improved metrics.

<details>
  <summary>Details</summary>

**Motivation:** The primary motivation of the paper is to address the performance bottlenecks of existing text-driven image generation methods in semantic alignment and structural consistency, enabling the creation of more coherent and controllable images.

**Method:** The paper proposes a high-fidelity image generation method that integrates text-image contrastive constraints with structural guidance, aiming to improve semantic alignment and structural consistency. It introduces a contrastive learning module and uses structural priors like semantic layout maps or edge sketches to guide the generator in spatial-level structural modeling.

**Result:** Systematic experiments conducted on the COCO-2014 dataset confirm that the proposed method effectively improves semantic and structural consistency, as evidenced by higher CLIP Score, FID, and SSIM metrics.

**Conclusion:** The proposed method demonstrates superior performance in text-driven image generation, achieving semantically clear and structurally complete images without increasing computational complexity, offering a viable path for joint text-image modeling.

**Abstract:** This paper addresses the performance bottlenecks of existing text-driven
image generation methods in terms of semantic alignment accuracy and structural
consistency. A high-fidelity image generation method is proposed by integrating
text-image contrastive constraints with structural guidance mechanisms. The
approach introduces a contrastive learning module that builds strong
cross-modal alignment constraints to improve semantic matching between text and
image. At the same time, structural priors such as semantic layout maps or edge
sketches are used to guide the generator in spatial-level structural modeling.
This enhances the layout completeness and detail fidelity of the generated
images. Within the overall framework, the model jointly optimizes contrastive
loss, structural consistency loss, and semantic preservation loss. A
multi-objective supervision mechanism is adopted to improve the semantic
consistency and controllability of the generated content. Systematic
experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are
performed on embedding dimensions, text length, and structural guidance
strength. Quantitative metrics confirm the superior performance of the proposed
method in terms of CLIP Score, FID, and SSIM. The results show that the method
effectively bridges the gap between semantic alignment and structural fidelity
without increasing computational complexity. It demonstrates a strong ability
to generate semantically clear and structurally complete images, offering a
viable technical path for joint text-image modeling and image generation.

</details>


### [35] [VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation](https://arxiv.org/abs/2508.10281)
*Ryota Tanaka,Tomohiro Suzuki,Keisuke Fujii*

Main category: cs.CV

> This paper proposes a new TAS system for figure skating jumps that includes a novel pose representation approach and a detailed annotation scheme to improve jump recognition.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to tackle the challenges of insufficient annotated data and the three-dimensional nature of jump actions, which are limitations of existing TAS methods in figure skating. The paper aims to develop an effective TAS system that can accurately recognize the type and timing of jumps in videos.

**Method:** The paper introduces a new Temporal Action Segmentation (TAS) framework for figure skating jumps. It utilizes a novel View-Invariant, Figure Skating-Specific (VIFSS) pose representation learning approach, which integrates contrastive learning with action classification. The framework also employs a fine-grained annotation scheme that includes marking the entry and landing phases of jumps.

**Result:** Experiments showed that the new framework achieved over 92% F1 score for recognizing both jump types and rotation levels. The view-invariant contrastive pre-training also proved particularly effective when the amount of fine-tuning data was limited.

**Conclusion:** The research concludes with the effectiveness of the proposed TAS framework in accurately recognizing jump types and rotation levels with an F1 score of over 92% at 50. Additionally, the method demonstrates a practical advantage in situations where fine-tuning data is limited.

**Abstract:** Understanding human actions from videos plays a critical role across various
domains, including sports analytics. In figure skating, accurately recognizing
the type and timing of jumps a skater performs is essential for objective
performance evaluation. However, this task typically requires expert-level
knowledge due to the fine-grained and complex nature of jump procedures. While
recent approaches have attempted to automate this task using Temporal Action
Segmentation (TAS), there are two major limitations to TAS for figure skating:
the annotated data is insufficient, and existing methods do not account for the
inherent three-dimensional aspects and procedural structure of jump actions. In
this work, we propose a new TAS framework for figure skating jumps that
explicitly incorporates both the three-dimensional nature and the semantic
procedure of jump movements. First, we propose a novel View-Invariant, Figure
Skating-Specific pose representation learning approach (VIFSS) that combines
contrastive learning as pre-training and action classification as fine-tuning.
For view-invariant contrastive pre-training, we construct FS-Jump3D, the first
publicly available 3D pose dataset specialized for figure skating jumps.
Second, we introduce a fine-grained annotation scheme that marks the ``entry
(preparation)'' and ``landing'' phases, enabling TAS models to learn the
procedural structure of jumps. Extensive experiments demonstrate the
effectiveness of our framework. Our method achieves over 92% F1@50 on
element-level TAS, which requires recognizing both jump types and rotation
levels. Furthermore, we show that view-invariant contrastive pre-training is
particularly effective when fine-tuning data is limited, highlighting the
practicality of our approach in real-world scenarios.

</details>


### [36] [JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics](https://arxiv.org/abs/2508.10287)
*Simindokht Jahangard,Mehrzad Mohammadi,Yi Shen,Zhixi Cai,Hamid Rezatofighi*

Main category: cs.CV

> 本文为解决现有视觉推理基准测试的不足，提出了能够生成不同复杂度问题并带有中间注释的自适应查询引擎，并新增人与物体互动及几何关系的注释，形成JRDB-Reasoning基准测试，用以评估视觉推理和视觉-语言模型。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型（VLMs）和大型语言模型（LLMs）的最新进展极大地增强了机器人等具身AI代理的视觉推理能力，但现有的视觉推理基准测试存在定义推理复杂性的模糊、缺乏生成不同难度问题的控制、无法提供结构化的逐步推理注释等问题。

**Method:** 本文提出了一种自适应查询引擎，能够生成不同复杂度且具有详细中间注释的自定义问题，并扩展了JRDB数据集，加入了人与物体互动以及几何关系的注释，形成了一套专为人群密集环境中视觉推理设计的基准测试JRDB-Reasoning。

**Result:** 本文的方法填补了现有视觉推理基准测试的空白，能够细粒度地评估视觉推理框架，并能跨不同推理水平动态评估视觉-语言模型。

**Conclusion:** 通过引入自适应查询引擎和扩展的JRDB-Reasoning基准测试，本文为评估视觉推理框架和视觉-语言模型提供了一种精细化方法，特别是在人群密集环境中的视觉推理表现。

**Abstract:** Recent advances in Vision-Language Models (VLMs) and large language models
(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI
agents like robots. However, existing visual reasoning benchmarks often suffer
from several limitations: they lack a clear definition of reasoning complexity,
offer have no control to generate questions over varying difficulty and task
customization, and fail to provide structured, step-by-step reasoning
annotations (workflows). To bridge these gaps, we formalize reasoning
complexity, introduce an adaptive query engine that generates customizable
questions of varying complexity with detailed intermediate annotations, and
extend the JRDB dataset with human-object interaction and geometric
relationship annotations to create JRDB-Reasoning, a benchmark tailored for
visual reasoning in human-crowded environments. Our engine and benchmark enable
fine-grained evaluation of visual reasoning frameworks and dynamic assessment
of visual-language models across reasoning levels.

</details>
