<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.CV](#cs.CV) [Total: 31]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training](https://arxiv.org/abs/2508.14904)
*Jianfeng Si,Lin Sun,Zhewen Tan,Xiangzheng Zhang*

Main category: cs.CL

> 本文提出了一种新的LLMs安全行为整合方法，通过简单的指令实现细粒度的安全行为切换。实验表明，这种方法在保证安全性能的同时，显著降低了模型的成本。

<details>
  <summary>Details</summary>

**Motivation:** 现有的LLMs内容安全方法如SFT和RLHF通常依赖多阶段训练管道，缺乏细粒度的部署后可控性。本文旨在解决这些问题。

**Method:** 提出了一种统一的协同训练框架，该框架可以有效地整合多种安全行为（正面、负面和拒绝行为）到单一的监督微调阶段中。每个行为通过简单的系统级指令或魔法令牌在推理时进行动态激活。

**Result:** 实验结果显示，该方法达到了与SFT+DPO相同的安全对齐质量，而8B的模型在安全性能上超过了DeepSeek-R1 (671B)，同时显著降低了训练复杂度和部署成本。

**Conclusion:** 这项工作提供了一个可扩展、高效和高度可控的LLMs内容安全解决方案，通过在输出空间引入安全对齐间隔，提供了模型安全稳健性的实证证据。

**Abstract:** Current methods for content safety in Large Language Models (LLMs), such as
Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback
(RLHF), often rely on multi-stage training pipelines and lack fine-grained,
post-deployment controllability. To address these limitations, we propose a
unified co-training framework that efficiently integrates multiple safety
behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and
rejective (refusal-oriented/conservative) within a single SFT stage. Notably,
each behavior is dynamically activated via a simple system-level instruction,
or magic token, enabling stealthy and efficient behavioral switching at
inference time. This flexibility supports diverse deployment scenarios, such as
positive for safe user interaction, negative for internal red-teaming, and
rejective for context-aware refusals triggered by upstream moderation signals.
This co-training strategy induces a distinct Safety Alignment Margin in the
output space, characterized by well-separated response distributions
corresponding to each safety mode. The existence of this margin provides
empirical evidence for the model's safety robustness and enables unprecedented
fine-grained control. Experiments show that our method matches the safety
alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1
(671B) in safety performance, while significantly reducing both training
complexity and deployment costs. This work presents a scalable, efficient, and
highly controllable solution for LLM content safety.

</details>


### [2] [Preliminary Ranking of WMT25 General Machine Translation Systems](https://arxiv.org/abs/2508.14909)
*Tom Kocmi,Eleftherios Avramidis,Rachel Bawden,Ondřej Bojar,Konstantin Dranch,Anton Dvorkovich,Sergey Dukanov,Natalia Fedorova,Mark Fishel,Markus Freitag,Thamme Gowda,Roman Grundkiewicz,Barry Haddow,Marzena Karpinska,Philipp Koehn,Howard Lakougna,Jessica Lundin,Kenton Murray,Masaaki Nagata,Stefano Perrella,Lorenzo Proietti,Martin Popel,Maja Popović,Parker Riley,Mariya Shmatova,Steinþór Steingrímsson,Lisa Yankovskaya,Vilém Zouhar*

Main category: cs.CL

> 该摘要展示了WMT25通用机器翻译任务的初步自动评估排名，提醒读者可能存在偏差，并指出最终排名将基于人工评估。报告的主要目的是向参与者提供有用的初步结果。

<details>
  <summary>Details</summary>

**Motivation:** 此报告的动机在于向任务参与者分享通用机器翻译任务的初步结果，这可能有助于他们准备各自的系统提交论文。同时，也说明因为自动评估可能带来的偏差，最终排名将依据更具可靠性的手动评估。

**Method:** 该报告基于自动评估指标对MT系统的初步排名进行展示，而非最终基于人工评估的排名。目的在于为参与者在准备系统提交论文时提供有用的初步结果。详细的方法论和自动评估技术未在摘要中明确说明。

**Result:** 报告展示了WMT25通用机器翻译共享任务的初步排名，但提醒读者该排名可能存在偏差，特别是对于使用了例如质量估计重排序或最小贝叶斯风险解码等重排序技术的系统。

**Conclusion:** 报告强调了尽管自动评估所带来的排名具有一定的参考价值，但最终的评测结果将以人工评估为准，这也是报告中没有包含最终评估结论的原因。

**Abstract:** We present the preliminary ranking of the WMT25 General Machine Translation
Shared Task, in which MT systems have been evaluated using automatic metrics.
As this ranking is based on automatic evaluations, it may be biased in favor of
systems that employ re-ranking techniques, such as Quality Estimation
re-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be
based on human evaluation, which is more reliable and will supersede the
automatic ranking.
  The purpose of this report is not to present the final findings of the
General MT task, but rather to share preliminary results with task
participants, which may be useful when preparing their system submission
papers.

</details>


### [3] [Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages](https://arxiv.org/abs/2508.14913)
*Israel Abebe Azime,Tadesse Destaw Belay,Dietrich Klakow,Philipp Slusallek,Anshuman Chhabra*

Main category: cs.CL

> 本文提出了一种使用大型语言模型的框架，用于数学应用题的文化本地化，以解决低资源语言中缺乏本土文化背景数据集的问题，提高了模型在多语言数学推理任务中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 由于低资源语言中缺乏反映本土实体（如人名、组织名和货币）的社交文化任务数据集，导致这些语言的多语言和文化背景的数学推理能力落后于英语。

**Method:** 提出了一种基于大型语言模型（LLMs）的框架，用于数学应用题的文化本地化，该框架能够从现有资源中自动构建包含本地人名、组织名和货币单位的数据集。

**Result:** 实验表明，通过翻译生成的基准测试可能掩盖了适当的社交文化背景下的真正多语言数学能力。同时，作者提出的框架可以帮助缓解以英语为中心的实体偏见，并在引入各种语言的本地实体时提高鲁棒性。

**Conclusion:** 本研究提供了一种方法，通过自动构建反映本土文化背景的数据集，来提高大型语言模型在多语言数学推理任务中的表现，从而更好地服务于低资源语言的社区。

**Abstract:** Large language models (LLMs) have demonstrated significant capabilities in
solving mathematical problems expressed in natural language. However,
multilingual and culturally-grounded mathematical reasoning in low-resource
languages lags behind English due to the scarcity of socio-cultural task
datasets that reflect accurate native entities such as person names,
organization names, and currencies. Existing multilingual benchmarks are
predominantly produced via translation and typically retain English-centric
entities, owing to the high cost associated with human annotater-based
localization. Moreover, automated localization tools are limited, and hence,
truly localized datasets remain scarce. To bridge this gap, we introduce a
framework for LLM-driven cultural localization of math word problems that
automatically constructs datasets with native names, organizations, and
currencies from existing sources. We find that translated benchmarks can
obscure true multilingual math ability under appropriate socio-cultural
contexts. Through extensive experiments, we also show that our framework can
help mitigate English-centric entity bias and improves robustness when native
entities are introduced across various languages.

</details>


### [4] [Improving LLMs for Machine Translation Using Synthetic Preference Data](https://arxiv.org/abs/2508.14951)
*Dario Vajda,Domen Vreš,Marko Robnik-Šikonja*

Main category: cs.CL

> 本研究探索了如何使用少量数据资源通过直接偏好优化（DPO）方法来改善大型语言模型的机器翻译性能，使用斯洛文尼亚语的特殊情况表现出显著效果。

<details>
  <summary>Details</summary>

**Motivation:** 探讨如何使用相对较少的易于生成的数据资源，提高大型语言模型在机器翻译任务中的效果。使用斯洛文尼亚语作为实例进行研究。

**Method:** 使用直接偏好优化（DPO）训练方法在程序化编目和增强的公开数据子集上改进了通用指令调整的大型语言模型。通过对比两个大型语言模型GaMS-9B-Instruct和EuroLLM-9B-Instruct对英文维基百科文章的翻译，基于启发式和自动评估指标如COMET排名生成训练数据集。

**Result:** 评估结果显示，经过微调的模型在翻译维基百科文章时提高了0.04和0.02（分别与基准模型比较）的COMET分数，并且更为一致地避免了语言和格式错误。

**Conclusion:** 通过使用DPO训练方法和程序化编目和增强的数据子集，可以在较少的数据资源下提高大型语言模型的机器翻译能力，特别是在斯洛文尼亚语的用例中表现尤为突出。

**Abstract:** Large language models have emerged as effective machine translation systems.
In this paper, we explore how a general instruction-tuned large language model
can be improved for machine translation using relatively few easily produced
data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct
model using Direct Preference Optimization (DPO) training on a programmatically
curated and enhanced subset of a public dataset. As DPO requires pairs of
quality-ranked instances, we generated its training dataset by translating
English Wikipedia articles using two LLMs, GaMS-9B-Instruct and
EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics
coupled with automatic evaluation metrics such as COMET. The evaluation shows
that our fine-tuned model outperforms both models involved in the dataset
generation. In comparison to the baseline models, the fine-tuned model achieved
a COMET score gain of around 0.04 and 0.02, respectively, on translating
Wikipedia articles. It also more consistently avoids language and formatting
errors.

</details>


### [5] [Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems](https://arxiv.org/abs/2508.14982)
*Qianli Wang,Tatiana Anikina,Nils Feldhus,Simon Ostermann,Fedor Splitt,Jiaao Li,Yoana Tsoneva,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

> 本文介绍了MultiCoXQL，这是一个跨五个语种（包括一种低资源语言）的多语言扩展数据集，并提出了一种新的解析方法来增强多语言解析性能。同时，提出Compass数据集，用于ConvXAI系统中自定义输入的提取，评估了三种不同规模的LLM和BERT型模型在单语、跨语和多语环境下的表现。

<details>
  <summary>Details</summary>

**Motivation:** 由于多语言泛化面临训练数据稀缺的挑战，且对自由形式的自定义输入支持有限，本文旨在通过扩展多语言数据集MultiCoXQL和Compass，解决ConvXAI系统在多语言处理和自定义输入方面的局限性。

**Method:** 提出了一种新的解析方法，旨在增强多语言解析性能，同时通过对MultiCoXQL和Compass的多语种评估来验证不同规模的LLM以及BERT模型的效果。

**Result:** 通过在MultiCoXQL和Compass上使用不同解析策略以及多种LLM规模和BERT型模型进行的单语、跨语和多语评估，证明了新解析方法的实际应用价值。

**Conclusion:** 本文通过构建多语言数据集MultiCoXQL和Compass，提出新的解析技术来提高ConvXAI系统的多语言和自定义输入处理能力，使得这些系统在多种语言环境中都能表现良好。

**Abstract:** Conversational explainable artificial intelligence (ConvXAI) systems based on
large language models (LLMs) have garnered considerable attention for their
ability to enhance user comprehension through dialogue-based explanations.
Current ConvXAI systems often are based on intent recognition to accurately
identify the user's desired intention and map it to an explainability method.
While such methods offer great precision and reliability in discerning users'
underlying intentions for English, a significant challenge in the scarcity of
training data persists, which impedes multilingual generalization. Besides, the
support for free-form custom inputs, which are user-defined data distinct from
pre-configured dataset instances, remains largely limited. To bridge these
gaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL
dataset spanning five typologically diverse languages, including one
low-resource language. Subsequently, we propose a new parsing approach aimed at
enhancing multilingual parsing performance, and evaluate three LLMs on
MultiCoXQL using various parsing strategies. Furthermore, we present Compass, a
new multilingual dataset designed for custom input extraction in ConvXAI
systems, encompassing 11 intents across the same five languages as MultiCoXQL.
We conduct monolingual, cross-lingual, and multilingual evaluations on Compass,
employing three LLMs of varying sizes alongside BERT-type models.

</details>


### [6] [Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner](https://arxiv.org/abs/2508.15044)
*Bolian Li,Yanran Wu,Xinyu Luo,Ruqi Zhang*

Main category: cs.CL

> 文章解决了测试时对齐技术的效率瓶颈问题，通过引入奖励偏移推测采样算法，在不改变目标模型的情况下，实现了更高效的人类偏好对齐。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型的发展，使其与人类偏好对齐变得至关重要。尽管推理时对齐技术可以提高LLM的安全性和推理能力，但这些技术通常会导致推理成本增加，限制了其实际应用。我们受推测采样加速技术的启发，旨在解决推理时对齐的效率瓶颈问题。

**Method:** 引入了奖励偏移推测采样（SSS）算法，该算法中草图模型被对齐到人类偏好，而目标模型保持不变。我们理论上证明，通过修改接受标准和奖励代币分布，可以利用对齐草图模型与未对齐目标模型之间的分布偏差，恢复RLHF最优解，而无需实际获得该解。

**Result:** 我们的算法在推理成本显著降低的同时，在测试时弱到强对齐的实验中取得了更高的金质奖励分数，验证了其有效性和效率。

**Conclusion:** SSS算法不仅在测试时弱到强对齐实验中取得了更好的实际效果，同时也降低了计算成本，表明了该方法的有效性和实用性。

**Abstract:** Aligning large language models (LLMs) with human preferences has become a
critical step in their development. Recent research has increasingly focused on
test-time alignment, where additional compute is allocated during inference to
enhance LLM safety and reasoning capabilities. However, these test-time
alignment techniques often incur substantial inference costs, limiting their
practical application. We are inspired by the speculative sampling
acceleration, which leverages a small draft model to efficiently predict future
tokens, to address the efficiency bottleneck of test-time alignment. We
introduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the
draft model is aligned with human preferences, while the target model remains
unchanged. We theoretically demonstrate that the distributional shift between
the aligned draft model and the unaligned target model can be exploited to
recover the RLHF optimal solution without actually obtaining it, by modifying
the acceptance criterion and bonus token distribution. Our algorithm achieves
superior gold reward scores at a significantly reduced inference cost in
test-time weak-to-strong alignment experiments, thereby validating both its
effectiveness and efficiency.

</details>


### [7] [LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text](https://arxiv.org/abs/2508.15085)
*MohamamdJavad Ardestani,Ehsan Kamalloo,Davood Rafiei*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** LongRecall. The completeness of machine-generated text, ensuring that it
captures all relevant information, is crucial in domains such as medicine and
law and in tasks like list-based question answering (QA), where omissions can
have serious consequences. However, existing recall metrics often depend on
lexical overlap, leading to errors with unsubstantiated entities and
paraphrased answers, while LLM-as-a-Judge methods with long holistic prompts
capture broader semantics but remain prone to misalignment and hallucinations
without structured verification. We introduce LongRecall, a general three-stage
recall evaluation framework that decomposes answers into self-contained facts,
successively narrows plausible candidate matches through lexical and semantic
filtering, and verifies their alignment through structured entailment checks.
This design reduces false positives and false negatives while accommodating
diverse phrasings and contextual variations, serving as a foundational building
block for systematic recall assessment. We evaluate LongRecall on three
challenging long-form QA benchmarks using both human annotations and LLM-based
judges, demonstrating substantial improvements in recall accuracy over strong
lexical and LLM-as-a-Judge baselines.

</details>


### [8] [Mapping the Course for Prompt-based Structured Prediction](https://arxiv.org/abs/2508.15090)
*Matt Pauk,Maria Leonor Pacheco*

Main category: cs.CL

> 本研究通过结合LLMs和组合推理来提高结构化预测任务的准确性和一致性，发现添加符号推理和进行校准与微调可以改善LLMs在复杂推理任务上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管LLMs在许多语言任务中表现出色，但它们在复杂推理和结构化预测问题上遇到了困难，主要因为它们自回归的特性。这项研究旨在解决这些问题，尝试将LLMs的预测能力与推理方法提供的结构一致性结合起来。

**Method:** 结合LLMs与组合推理来提高结构化预测任务中模型的准确性和一致性。通过详尽的实验了解哪些提示策略可以有效地估计LLMs的置信值，以便与符号推理结合使用。研究还包括校准和使用结构化预测目标进行微调，以提高在具有挑战性任务中的性能。

**Result:** 即使采用不同的提示策略，添加符号推理也能使预测更加一致和准确。此外，通过结构化预测目标进行校准和微调，可以使模型在难以处理的任务上表现更好，表明结构化学习在LLMs时代仍然具有价值。

**Conclusion:** 即使在大规模语言模型时代，通过结构化的预测和校准微调，仍然可以提高模型在复杂推理任务上的性能。因此，在处理结构化预测问题时，结合LLMs和符号推理是有效的方法。

**Abstract:** LLMs have been shown to be useful for a variety of language tasks, without
requiring task-specific fine-tuning. However, these models often struggle with
hallucinations and complex reasoning problems due to their autoregressive
nature. We propose to address some of these issues, specifically in the area of
structured prediction, by combining LLMs with combinatorial inference in an
attempt to marry the predictive power of LLMs with the structural consistency
provided by inference methods. We perform exhaustive experiments in an effort
to understand which prompting strategies can effectively estimate LLM
confidence values for use with symbolic inference, and show that, regardless of
the prompting strategy, the addition of symbolic inference on top of prompting
alone leads to more consistent and accurate predictions. Additionally, we show
that calibration and fine-tuning using structured prediction objectives leads
to increased performance for challenging tasks, showing that structured
learning is still valuable in the era of LLMs.

</details>


### [9] [Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset](https://arxiv.org/abs/2508.15096)
*Rabeeh Karimi Mahabadi,Sanjeev Satheesh,Shrimai Prabhumoye,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro*

Main category: cs.CL

> 提出了一种新的科学文本抽取方法，从大规模网络数据中提取高质量数学语料库。相比于现有数据集，新方法不仅提高了数据质量，还大幅增加了有效的文本数量。

<details>
  <summary>Details</summary>

**Motivation:** 现有聚焦数学的数据集构建方法有诸多不足，例如由于粗糙的提取启发式方法、从HTML到文本的损失转换以及难以保持数学结构等原因导致数据质量下降。

**Method:** 我们提出了一种新颖且通用的科学文本抽取管道，专为从Common Crawl中提取高质量数学内容设计。该管道通过利用lynx的布局感知渲染和基于LLM的清洁阶段，能够恢复多种格式的数学内容，例如MathJax、KaTeX和MathML。

**Result:** 我们收集了高质量的数学语料库Nemotron-CC-Math-3+（133B tokens）和Nemotron-CC-Math-4+（52B tokens），后者不仅超越了所有之前的数学数据集，还包含了比FineMath-4+多5.5倍的tokens。

**Conclusion:** 我们首次提出了能够稳健地从大规模网络噪音数据中提取科学研究内容（包括数学）的管道，改进了数学、代码和一般推理能力，并在开放数学预训练语料库领域创下了新的记录。为了支持开源工作，我们公开了我们的代码和数据集。

**Abstract:** Pretraining large language models (LLMs) on high-quality, structured data
such as mathematics and code substantially enhances reasoning capabilities.
However, existing math-focused datasets built from Common Crawl suffer from
degraded quality due to brittle extraction heuristics, lossy HTML-to-text
conversion, and the failure to reliably preserve mathematical structure. In
this work, we introduce Nemotron-CC-Math, a large-scale, high-quality
mathematical corpus constructed from Common Crawl using a novel,
domain-agnostic pipeline specifically designed for robust scientific text
extraction.
  Unlike previous efforts, our pipeline recovers math across various formats
(e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx
and a targeted LLM-based cleaning stage. This approach preserves the structural
integrity of equations and code blocks while removing boilerplate,
standardizing notation into LaTeX representation, and correcting
inconsistencies.
  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+
(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably,
Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including
MegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens
than FineMath-4+, which was previously the highest-quality math pretraining
dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to
+12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines,
while also improving general-domain performance on MMLU and MMLU-Stem.
  We present the first pipeline to reliably extract scientific
content--including math--from noisy web-scale data, yielding measurable gains
in math, code, and general reasoning, and setting a new state of the art among
open math pretraining corpora. To support open-source efforts, we release our
code and datasets.

</details>


### [10] [Identifying and Answering Questions with False Assumptions: An Interpretable Approach](https://arxiv.org/abs/2508.15139)
*Zijie Wang,Eduardo Blanco*

Main category: cs.CL

> 本文探讨了解决大型语言模型在回答含有错误假设的问题时容易产生误导性答案的问题，提出了一种利用外部证据和验证原子假设的方法，并通过实验验证了这种方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在回答包含错误假设的问题时经常产生误导性的答案。本文旨在解决这一问题，以提高模型的回答质量。

**Method:** 我们提出了一种利用外部证据来减少大型语言模型（LLMs）在回答基于错误假设的问题时产生误导性答案的方法。该方法首先将问题视为事实验证问题，然后通过引入检索到的证据来改进模型的回答。

**Result:** 实验结果显示，引入检索证据对于改善LLMs的回答是有益的。此外，生成并验证原子假设可以进一步提高回答质量并提供可解释的答案，明确指出错误假设。

**Conclusion:** 本研究展示了通过引入外部证据来减少大型语言模型在回答基于错误假设的问题时产生的误导性答案是有效的方法。通过对原子假设的生成和验证，可以获得更高质量且可解释的回答。

**Abstract:** People often ask questions with false assumptions, a type of question that
does not have regular answers. Answering such questions require first
identifying the false assumptions. Large Language Models (LLMs) often generate
misleading answers because of hallucinations. In this paper, we focus on
identifying and answering questions with false assumptions in several domains.
We first investigate to reduce the problem to fact verification. Then, we
present an approach leveraging external evidence to mitigate hallucinations.
Experiments with five LLMs demonstrate that (1) incorporating retrieved
evidence is beneficial and (2) generating and validating atomic assumptions
yields more improvements and provides an interpretable answer by specifying the
false assumptions.

</details>


### [11] [ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following](https://arxiv.org/abs/2508.15164)
*Seungmin Han,Haeun Kwon,Ji-jun Park,Taeyang Yoon*

Main category: cs.CL

> 提出了CoLVLM Agent框架，用于解决现有大型语言模型和视觉语言模型在处理复杂多轮视觉互动时的问题，通过在MMDR-Bench数据集上的实验展示了其优越性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型和视觉语言模型在处理需要深入推理、长时间背景理解、实体追踪和多步骤指令遵循的复杂、多轮和基于视觉的任务时存在困难，当前基准测试难以捕捉真实世界多模态互动的复杂性。

**Method:** 引入了MMDR-Bench数据集及CoLVLM Agent框架，后者通过记忆-感知-规划-执行的迭代循环增强了现存视觉语言模型的推理和指令遵循能力。

**Result:** 实验显示，CoLVLM Agent在MMDR-Bench上表现出色，平均人类评价分数为4.03，优于如GPT-4o和Gemini 1.5 Pro等先进商业模型，并在推理深度、指令遵循及错误抑制方面表现出显著优势。

**Conclusion:** CoLVLM Agent的研发证实了其模块化设计及迭代方法在复杂多模态互动中的有效性。

**Abstract:** Despite significant advancements in Large Language Models (LLMs) and Large
Vision-Language Models (LVLMs), current models still face substantial
challenges in handling complex, multi-turn, and visually-grounded tasks that
demand deep reasoning, sustained contextual understanding, entity tracking, and
multi-step instruction following. Existing benchmarks often fall short in
capturing the dynamism and intricacies of real-world multi-modal interactions,
leading to issues such as context loss and visual hallucinations. To address
these limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning
Benchmark), a novel dataset comprising 300 meticulously designed complex
multi-turn dialogue scenarios, each averaging 5-7 turns and evaluated across
six core dimensions including visual entity tracking and reasoning depth.
Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic
framework that enhances existing LVLMs with advanced reasoning and instruction
following capabilities through an iterative
"memory-perception-planning-execution" cycle, requiring no extensive
re-training of the underlying models. Our extensive experiments on MMDR-Bench
demonstrate that CoLVLM Agent consistently achieves superior performance,
attaining an average human evaluation score of 4.03, notably surpassing
state-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro
(3.85). The framework exhibits significant advantages in reasoning depth,
instruction adherence, and error suppression, and maintains robust performance
over extended dialogue turns, validating the effectiveness of its modular
design and iterative approach for complex multi-modal interactions.

</details>


### [12] [SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling](https://arxiv.org/abs/2508.15190)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

> 提出了SemToken框架，它通过语义感知来减少token冗余并提高计算效率，在长上下文语言模型基准上显著减小token数量，提高速度，且保持准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有分词方法如BPE或WordPiece主要基于频率统计，忽略了文本的语义结构，导致语义冗余跨度过度分词和上下文连贯性的不足，在长上下文场景中尤为突出。

**Method:** SemToken是一种基于语义感知的分词框架，首先通过轻量级编码器提取语境语义嵌入，并执行局部语义聚类来合并语义等效的token。其次，根据语义密度分配异构的token粒度，实现内容丰富区域更细粒度的分词和重复或低熵跨度的更粗压缩。

**Result:** 实验表明，SemToken在WikiText-103和LongBench基准测试中的token数量减少达2.4倍，加速达1.9倍，且在困惑度和下游准确性方面几乎没有退化。

**Conclusion:** 研究表明，通过利用语义结构对分词和计算进行优化，在大型语言模型中具有很大的潜力。

**Abstract:** Tokenization plays a critical role in language modeling, yet existing
approaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on
frequency statistics, ignoring the underlying semantic structure of text. This
leads to over-tokenization of semantically redundant spans and underutilization
of contextual coherence, particularly in long-context scenarios. In this work,
we propose \textbf{SemToken}, a semantic-aware tokenization framework that
jointly reduces token redundancy and improves computation efficiency. SemToken
first extracts contextual semantic embeddings via lightweight encoders and
performs local semantic clustering to merge semantically equivalent tokens.
Then, it allocates heterogeneous token granularity based on semantic density,
allowing finer-grained tokenization in content-rich regions and coarser
compression in repetitive or low-entropy spans. SemToken can be seamlessly
integrated with modern language models and attention acceleration methods.
Experiments on long-context language modeling benchmarks such as WikiText-103
and LongBench show that SemToken achieves up to $2.4\times$ reduction in token
count and $1.9\times$ speedup, with negligible or no degradation in perplexity
and downstream accuracy. Our findings suggest that semantic structure offers a
promising new axis for optimizing tokenization and computation in large
language models.

</details>


### [13] [Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models](https://arxiv.org/abs/2508.15202)
*Yuanchen Zhou,Shuo Jiang,Jie Zhu,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.CL

> 我们引入了专门针对金融领域的Fin-PRM模型，它能够在金融任务中更有效地评估中间推理步骤，取得了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有的过程奖励模型（PRMs）主要是为了一般领域或STEM（科学、技术、工程和数学）领域训练，但在诸如金融这种需要严格逻辑性和结构化的领域内的应用有限。本研究的目标是开发一种专门针对金融领域的奖励模型，以解决这一问题。

**Method:** 我们提出了一种名为Fin-PRM的方法，专门用于在金融领域评估大型语言模型中的中间推理步骤。Fin-PRM集成了步骤级和轨迹级的奖励监督，以实现与金融逻辑对齐的推理轨迹的精细评估。该方法适用于离线和在线奖励学习设置，并支持三个关键应用：（i）选择高质量的推理轨迹用于蒸馏式监督微调，（ii）为强化学习提供密集的流程级奖励，（iii）在测试时通过奖励信息引导最佳选项的推断。

**Result:** 实验结果表明，在CFLUE和FinQA等金融推理基准测试中，Fin-PRM在轨迹选择质量方面始终优于通用PRM和强大的领域内基线模型。使用Fin-PRM训练的下游模型相较于基线模型在监督学习、强化学习和测试时性能方面分别提升了12.9%、5.2%和5.1%。

**Conclusion:** 该研究表明，专业化领域的奖励模型在将大型语言模型与专家级别的金融推理对齐方面具有重要价值，且该模型已经在相关基准测试中得到了验证。

**Abstract:** Process Reward Models (PRMs) have emerged as a promising framework for
supervising intermediate reasoning in large language models (LLMs), yet
existing PRMs are primarily trained on general or Science, Technology,
Engineering, and Mathematics (STEM) domains and fall short in domain-specific
contexts such as finance, where reasoning is more structured, symbolic, and
sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM},
a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate
reasoning steps in financial tasks. Fin-PRM integrates step-level and
trajectory-level reward supervision, enabling fine-grained evaluation of
reasoning traces aligned with financial logic. We apply Fin-PRM in both offline
and online reward learning settings, supporting three key applications: (i)
selecting high-quality reasoning trajectories for distillation-based supervised
fine-tuning, (ii) providing dense process-level rewards for reinforcement
learning, and (iii) guiding reward-informed Best-of-N inference at test time.
Experimental results on financial reasoning benchmarks, including CFLUE and
FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs
and strong domain baselines in trajectory selection quality. Downstream models
trained with Fin-PRM yield substantial improvements with baselines, with gains
of 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% in
test-time performance. These findings highlight the value of domain-specialized
reward modeling for aligning LLMs with expert-level financial reasoning. Our
project resources will be available at https://github.com/aliyun/qwen-dianjin.

</details>


### [14] [SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning](https://arxiv.org/abs/2508.15212)
*Huanxuan Liao,Yixing Xu,Shizhu He,Guanchen Li,Xuanwu Yin,Dong Li,Emad Barsoum,Jun Zhao,Kang Liu*

Main category: cs.CL

> 本文提出了一种名为SPARK的方法，通过剪枝KV的特征通道并动态恢复这些条目来减少KV缓存的冗余，使长序列数据可以在相同的内存预算下进行处理，同时在整个处理过程中保持或提高模型的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法在处理长序列的语言模型的KV缓存问题时，主要通过在时间轴上压缩KV缓存来减少内存和计算开销，但这些方法忽略了特征维度上不同的重要性差异，限制了它们在有效平衡效率和模型准确性方面的表现。本文就是针对这一问题提出了新的解决方案。

**Method:** 本文提出了一种名为SPARK的无需训练的即插即用方法，通过在特征通道层面上应用非结构化稀疏性来解决KV缓存的压缩问题，并在注意力得分计算过程中动态恢复被剪枝的条目。该方法与现有的KV缓存压缩和量化技术正交，可以结合使用以实现进一步的加速。

**Result:** 通过SPARK方法，在相同的序列长度下，KV缓存存储减少了超过30%，相比基于令牌驱逐的方法，即使在具有80%的激进剪枝率的情况下，性能退化也不超过5%。

**Conclusion:** SPARK方法通过在通道级别应用非结构化剪枝来减少KV缓存的冗余，实现了较长序列的低内存预算处理，同时保持或提高了模型的准确性，体现了其稳健性和有效性。

**Abstract:** Long-context inference in large language models (LLMs) is increasingly
constrained by the KV cache bottleneck: memory usage grows linearly with
sequence length, while attention computation scales quadratically. Existing
approaches address this issue by compressing the KV cache along the temporal
axis through strategies such as token eviction or merging to reduce memory and
computational overhead. However, these methods often neglect fine-grained
importance variations across feature dimensions (i.e., the channel axis),
thereby limiting their ability to effectively balance efficiency and model
accuracy. In reality, we observe that channel saliency varies dramatically
across both queries and positions: certain feature channels carry near-zero
information for a given query, while others spike in relevance. To address this
oversight, we propose SPARK, a training-free plug-and-play method that applies
unstructured sparsity by pruning KV at the channel level, while dynamically
restoring the pruned entries during attention score computation. Notably, our
approach is orthogonal to existing KV compression and quantization techniques,
making it compatible for integration with them to achieve further acceleration.
By reducing channel-level redundancy, SPARK enables processing of longer
sequences within the same memory budget. For sequences of equal length, SPARK
not only preserves or improves model accuracy but also reduces KV cache storage
by over 30% compared to eviction-based methods. Furthermore, even with an
aggressive pruning ratio of 80%, SPARK maintains performance with less
degradation than 5% compared to the baseline eviction method, demonstrating its
robustness and effectiveness. Our code will be available at
https://github.com/Xnhyacinth/SparK.

</details>


### [15] [Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering](https://arxiv.org/abs/2508.15213)
*Bolei He,Xinran He,Run Shao,Shanfu Shu,Xianwei Xue,Mingquan Cheng,Haifeng Li,Zhenhua Ling*

Main category: cs.CL

> 针对大型语言模型在领域特定场景中的局限性，提出Selct2Know (S2K)框架，通过内部-外部知识选择策略和选择性监督微调方法来提高性能，同时引入结构化推理数据生成管道及GRPO来增强推理能力。S2K在医学、法律和金融问答基准上超越现有方法，且以更低的成本与领域的预训练语言模型相匹配。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）在一般问答中表现良好，但在特定领域场景中表现不佳。检索增强生成（RAG）引入外部知识但容易出现幻觉和延迟。持续预训练可以内部化领域知识但代价高昂且缺乏跨领域灵活性。问题在于领域知识的长尾分布导致部分有用的知识被利用不足。认为知识获取应是渐进的，类似于人类学习过程。

**Method:** 提出了名为Selct2Know (S2K) 的框架，该框架通过内部-外部知识自我选择策略和选择性监督微调来内部化领域知识。同时引入了结构化推理数据生成管道，并整合了GRPO来增强推理能力。

**Result:** S2K在医学、法律和金融问答基准上超越了现有方法，并且与领域预训练的大型语言模型相比，以显著更低的成本取得了相似的表现。

**Conclusion:** Selct2Know (S2K)是一个有效的框架，能够以较低的成本提高大型语言模型在领域知识上的表现，相较于持续预训练的方法，它更加灵活和经济。

**Abstract:** Large Language Models (LLMs) perform well in general QA but often struggle in
domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces
external knowledge but suffers from hallucinations and latency due to noisy
retrievals. Continued pretraining internalizes domain knowledge but is costly
and lacks cross-domain flexibility. We attribute this challenge to the
long-tail distribution of domain knowledge, which leaves partial yet useful
internal knowledge underutilized. We further argue that knowledge acquisition
should be progressive, mirroring human learning: first understanding concepts,
then applying them to complex reasoning. To address this, we propose Selct2Know
(S2K), a cost-effective framework that internalizes domain knowledge through an
internal-external knowledge self-selection strategy and selective supervised
fine-tuning. We also introduce a structured reasoning data generation pipeline
and integrate GRPO to enhance reasoning ability. Experiments on medical, legal,
and financial QA benchmarks show that S2K consistently outperforms existing
methods and matches domain-pretrained LLMs with significantly lower cost.

</details>


### [16] [Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall](https://arxiv.org/abs/2508.15214)
*Sijia Cui,Aiyao He,Shuai Xu,Hongming Zhang,Yanna Wang,Qingyang Zhang,Yajing Wang,Bo Xu*

Main category: cs.CL

> SEER improves large language models' performance in multi-step tool usage by leveraging a continually updated experience pool.

<details>
  <summary>Details</summary>

**Motivation:** To solve the problem of LLMs struggling with tool selection, parameter generation, and planning in multi-step tool usage without relying on complex and inefficient manual or retrieval methods.

**Method:** Stepwise Experience Recall (SEER), a self-guided method performing fine-grained, stepwise retrieval from an experience pool that expands with past successful trajectories.

**Result:** SEER achieves improvements of 6.1% and 4.7% on ToolQA for easy and hard questions, respectively, and gains of 7.44% and 23.38% on real-world domains using different Qwen models.

**Conclusion:** SEER demonstrates significant improvements in model performance on a variety of benchmarks, proving its effectiveness in overcoming the limitations of existing approaches.

**Abstract:** Function calling enables large language models (LLMs) to interact with
external systems by leveraging tools and APIs. When faced with multi-step tool
usage, LLMs still struggle with tool selection, parameter generation, and
tool-chain planning. Existing methods typically rely on manually designing
task-specific demonstrations, or retrieving from a curated library. These
approaches demand substantial expert effort and prompt engineering becomes
increasingly complex and inefficient as tool diversity and task difficulty
scale. To address these challenges, we propose a self-guided method, Stepwise
Experience Recall (SEER), which performs fine-grained, stepwise retrieval from
a continually updated experience pool. Instead of relying on static or manually
curated library, SEER incrementally augments the experience pool with past
successful trajectories, enabling continuous expansion of the pool and improved
model performance over time. Evaluated on the ToolQA benchmark, SEER achieves
an average improvement of 6.1\% on easy and 4.7\% on hard questions. We further
test SEER on $\tau$-bench, which includes two real-world domains. Powered by
Qwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains
of 7.44\% and 23.38\%, respectively.

</details>


### [17] [Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?](https://arxiv.org/abs/2508.15218)
*Momoka Furuhashi,Kouta Nakayama,Takashi Kodama,Saku Sugawara*

Main category: cs.CL

> 本研究通过实验验证了在不同模型大小下生成检查表的有效性，并发现有选择地使用检查表在某些评估任务中可以提高表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管自动检查表生成可能是一种有前景的方法，但其实用性尚未得到充分研究。本研究旨在解决生成任务自动评估中的不明确标准的问题。

**Method:** 我们研究了是否应该对所有问题使用检查表还是有选择地使用。我们使用六种方法生成检查表，并对八种不同大小的模型进行了有效性评估，同时识别了与人工评估相关的检查表项目。我们通过配对比较和直接评分任务进行了实验。

**Result:** 实验结果表明，在配对设置中，有选择地使用检查表可以改善评估表现。然而，在直接评分任务中，这种好处并不那么一致。此外，分析显示，即使与人类评分相关性较低的检查表项目也经常反映出人工书写的标准。

**Conclusion:** 本研究发现突显了需要更明确地定义客观评估标准，以指导人工和自动评估。

**Abstract:** Automatic evaluation of generative tasks using large language models faces
challenges due to ambiguous criteria. Although automatic checklist generation
is a potentially promising approach, its usefulness remains underexplored. We
investigate whether checklists should be used for all questions or selectively,
generate them using six methods, evaluate their effectiveness across eight
model sizes, and identify checklist items that correlate with human
evaluations. Through experiments on pairwise comparison and direct scoring
tasks, we find that selective checklist use tends to improve evaluation
performance in pairwise settings, while its benefits are less consistent in
direct scoring. Our analysis also shows that even checklist items with low
correlation to human scores often reflect human-written criteria, indicating
potential inconsistencies in human evaluation. These findings highlight the
need to more clearly define objective evaluation criteria to guide both human
and automatic evaluations. \footnote{Our code is available
at~https://github.com/momo0817/checklist-effectiveness-study

</details>


### [18] [VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models](https://arxiv.org/abs/2508.15229)
*Hanling Zhang,Yayu Zhou,Tongcheng Fang,Zhihang Yuan,Guohao Dai,Yu Wang*

Main category: cs.CL

> 研究提出VocabTailor框架以解决小型语言模型（SLMs）在边缘设备上的内存问题，通过解除耦合并动态选择词汇表，减少了内存使用量。

<details>
  <summary>Details</summary>

**Motivation:** 现有的静态词汇表裁剪方法虽然能减少内存使用，但其设计僵化，缺乏灵活性，导致预填充阶段的信息丢失。为了克服小型语言模型（SLMs）在边缘设备部署时遇到的内存瓶颈，尤其是词汇相关的组件占用内存过大问题。

**Method:** 提出了一种名为VocabTailor的新框架，该框架通过解除耦合并动态选择词汇表来解决内存限制问题，具体而言，它将Embedding卸载，并为LM Head实施了一种混合的静态-动态词汇表选择策略，实现词汇表组件的按需加载。

**Result:** 实验结果显示，VocabTailor能在各种下游任务中将词汇相关的内存使用量减少高达99%，同时几乎不会或完全不影响任务性能，显著优于现有的静态词汇裁剪方法。

**Conclusion:** 该研究成功引入了一种针对SLMs的动态词汇表选择框架，有效地解决了边缘设备上内存资源的瓶颈问题，同时保持了模型性能。

**Abstract:** Small Language Models (SLMs) provide computational advantages in
resource-constrained environments, yet memory limitations remain a critical
bottleneck for edge device deployment. A substantial portion of SLMs' memory
footprint stems from vocabulary-related components, particularly embeddings and
language modeling (LM) heads, due to large vocabulary sizes. Existing static
vocabulary pruning, while reducing memory usage, suffers from rigid,
one-size-fits-all designs that cause information loss from the prefill stage
and a lack of flexibility. In this work, we identify two key principles
underlying the vocabulary reduction challenge: the lexical locality principle,
the observation that only a small subset of tokens is required during any
single inference, and the asymmetry in computational characteristics between
vocabulary-related components of SLM. Based on these insights, we introduce
VocabTailor, a novel decoupled dynamic vocabulary selection framework that
addresses memory constraints through offloading embedding and implements a
hybrid static-dynamic vocabulary selection strategy for LM Head, enabling
on-demand loading of vocabulary components. Comprehensive experiments across
diverse downstream tasks demonstrate that VocabTailor achieves a reduction of
up to 99% in the memory usage of vocabulary-related components with minimal or
no degradation in task performance, substantially outperforming existing static
vocabulary pruning.

</details>


### [19] [WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai](https://arxiv.org/abs/2508.15239)
*Peerat Limkonchotiwat,Pume Tuchinda,Lalita Lowphansirikul,Surapon Nonesung,Panuthep Tasawong,Alham Fikri Aji,Can Udomcharoenchaikit,Sarana Nutanong*

Main category: cs.CL

> The paper presents WangchanThaiInstruct, a human-authored Thai dataset for evaluating and fine-tuning large language models in specific professional domains, highlighting the importance of culturally and professionally relevant data for improving model performance in low-resource languages.

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored area of large language model performance in low-resource languages such as Thai, emphasizing the cultural and domain-specific nuances missing in translated benchmarks.

**Method:** The creation of WangchanThaiInstruct, a dataset across four professional domains and seven task types, developed through a rigorous multi-stage quality control process involving annotators, domain experts, and AI researchers.

**Result:** The zero-shot evaluation demonstrates performance gaps on culturally and professionally specific tasks. Instruction tuning on WangchanThaiInstruct outperforms models trained on translated data, both in-domain and out-of-domain.

**Conclusion:** The findings emphasize the necessity of culturally and professionally grounded instruction datasets for better alignment of LLMs in linguistically diverse, low-resource settings.

**Abstract:** Large language models excel at instruction-following in English, but their
performance in low-resource languages like Thai remains underexplored. Existing
benchmarks often rely on translations, missing cultural and domain-specific
nuances needed for real-world use. We present WangchanThaiInstruct, a
human-authored Thai dataset for evaluation and instruction tuning, covering
four professional domains and seven task types. Created through a multi-stage
quality control process with annotators, domain experts, and AI researchers,
WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing
performance gaps on culturally and professionally specific tasks, and (2) an
instruction tuning study with ablations isolating the effect of native
supervision. Models fine-tuned on WangchanThaiInstruct outperform those using
translated data in both in-domain and out-of-domain benchmarks. These findings
underscore the need for culturally and professionally grounded instruction data
to improve LLM alignment in low-resource, linguistically diverse settings.

</details>


### [20] [UniCoM: A Universal Code-Switching Speech Generator](https://arxiv.org/abs/2508.15244)
*Sangmin Lee,Woojin Chung,Seyun Um,Hong-Goo Kang*

Main category: cs.CL

> 研究提出了一种名为UniCoM的生成高质量自然CS样本的管道，构建了CS-FLEURS多语言CS语料库，并验证了其在ASR和S2TT上的有效性和自然度。

<details>
  <summary>Details</summary>

**Motivation:** 解决由于适合的数据集稀缺而导致的处理CS现象的系统被较少探索的问题，并推动CS语音技术的发展，以及促进更具包容性的多语言系统的形成。

**Method:** 提出了一种名为Substituting WORDs with Synonyms (SWORDS) 的算法，通过替换选定单词的翻译（同时考虑词性）来生成CS语音，以构建高质量且自然的CS样本而不改变句子的语义。

**Result:** 实验结果显示，CS-FLEURS在客观和主观评测指标上的表现与现有数据集相比具有较高的可理解性和自然度。

**Conclusion:** 研究指出，提出的UniCoM管道和生成的CS-FLEURS多语言CS语料库有望促进CS语音技术的进步并支持更多的多语言系统开发。

**Abstract:** Code-switching (CS), the alternation between two or more languages within a
single speaker's utterances, is common in real-world conversations and poses
significant challenges for multilingual speech technology. However, systems
capable of handling this phenomenon remain underexplored, primarily due to the
scarcity of suitable datasets. To resolve this issue, we propose Universal
Code-Mixer (UniCoM), a novel pipeline for generating high-quality, natural CS
samples without altering sentence semantics. Our approach utilizes an algorithm
we call Substituting WORDs with Synonyms (SWORDS), which generates CS speech by
replacing selected words with their translations while considering their parts
of speech. Using UniCoM, we construct Code-Switching FLEURS (CS-FLEURS), a
multilingual CS corpus designed for automatic speech recognition (ASR) and
speech-to-text translation (S2TT). Experimental results show that CS-FLEURS
achieves high intelligibility and naturalness, performing comparably to
existing datasets on both objective and subjective metrics. We expect our
approach to advance CS speech technology and enable more inclusive multilingual
systems.

</details>


### [21] [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)
*Yilin Jiang,Mingzi Zhang,Sheng Jin,Zengyi Yu,Xiangjie Kong,Binghao Tu*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Simulating Professions (SP) enables Large Language Models (LLMs) to emulate
professional roles. However, comprehensive psychological and ethical evaluation
in these contexts remains lacking. This paper introduces EMNLP, an
Educator-role Moral and Normative LLMs Profiling framework for personality
profiling, moral development stage measurement, and ethical risk under soft
prompt injection. EMNLP extends existing scales and constructs 88
teacher-specific moral dilemmas, enabling profession-oriented comparison with
human teachers. A targeted soft prompt injection set evaluates compliance and
vulnerability in teacher SP. Experiments on 12 LLMs show teacher-role LLMs
exhibit more idealized and polarized personalities than human teachers, excel
in abstract moral reasoning, but struggle with emotionally complex situations.
Models with stronger reasoning are more vulnerable to harmful prompt injection,
revealing a paradox between capability and safety. The model temperature and
other hyperparameters have limited influence except in some risk behaviors.
This paper presents the first benchmark to assess ethical and psychological
alignment of teacher-role LLMs for educational AI. Resources are available at
https://e-m-n-l-p.github.io/.

</details>


### [22] [Conflict-Aware Soft Prompting for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.15253)
*Eunseong Choi,June Park,Hyeri Lee,Jongwuk Lee*

Main category: cs.CL

> 本文介绍了一种新的方法CARE，通过一种上下文评估机制来解决在RAG中产生的上下文-记忆冲突问题，实验显示该方法在问答和事实核查基准上的性能有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 解决检索增强生成(RAG)系统中外部检索上下文与内置知识冲突的问题，称为上下文-记忆冲突。

**Method:** 提出了一种名为CARE的方法，包括一个上下文评估器和一个基础的LLM。上下文评估器从原始上下文标记中编码紧凑的记忆标记嵌入。通过基于软提示的方式，上下文评估器被训练来辨别不可靠的上下文，并捕获一个指导信号，引导推理朝向更可靠的知识来源。

**Result:** 实验结果表明，CARE在问答和事实核查基准上平均提升了5.0%的性能。

**Conclusion:** CARE有效缓解了上下文-记忆冲突，为可信和自适应的检索增强生成(RAG)系统提供了一个有前景的方向。

**Abstract:** Retrieval-augmented generation (RAG) enhances the capabilities of large
language models (LLMs) by incorporating external knowledge into their input
prompts. However, when the retrieved context contradicts the LLM's parametric
knowledge, it often fails to resolve the conflict between incorrect external
context and correct parametric knowledge, known as context-memory conflict. To
tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation
(CARE), consisting of a context assessor and a base LLM. The context assessor
encodes compact memory token embeddings from raw context tokens. Through
grounded/adversarial soft prompting, the context assessor is trained to discern
unreliable context and capture a guidance signal that directs reasoning toward
the more reliable knowledge source. Extensive experiments show that CARE
effectively mitigates context-memory conflicts, leading to an average
performance gain of 5.0\% on QA and fact-checking benchmarks, establishing a
promising direction for trustworthy and adaptive RAG systems.

</details>


### [23] [TComQA: Extracting Temporal Commonsense from Text](https://arxiv.org/abs/2508.15274)
*Lekshmi R Nair,Arun Sankar,Koninika Pal*

Main category: cs.CL

> 本文利用大型语言模型（LLMs）自动挖掘事件时间常识，构建了TComQA数据集，并验证了其在时间常识提取上的有效性，显著提高了模型在时间问答任务上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在处理需要时间常识推理的任务时存在困难，因为文本中对时间常识的明确提及较少。因此，自动挖掘事件的时间常识对于创建更加健壮的语言模型至关重要。

**Method:** 本文提出了一种利用大型语言模型（LLMs）自动挖掘事件时间常识的流水线方法，并使用该方法构建了一个基于SAMSum和RealNews语料库的TComQA数据集。

**Result:** TComQA数据集通过众包验证，其在提取时间常识方面达到了80%以上的精度。使用TComQA训练的模型在时间问答任务上优于现有数据集微调的大型语言模型。

**Conclusion:** 研究结果表明，利用LLMs挖掘事件的时间常识并构建数据集的有效性，能够改善语言模型处理时间相关的任务能力。

**Abstract:** Understanding events necessitates grasping their temporal context, which is
often not explicitly stated in natural language. For example, it is not a
trivial task for a machine to infer that a museum tour may last for a few
hours, but can not take months. Recent studies indicate that even advanced
large language models (LLMs) struggle in generating text that require reasoning
with temporal commonsense due to its infrequent explicit mention in text.
Therefore, automatically mining temporal commonsense for events enables the
creation of robust language models. In this work, we investigate the capacity
of LLMs to extract temporal commonsense from text and evaluate multiple
experimental setups to assess their effectiveness. Here, we propose a temporal
commonsense extraction pipeline that leverages LLMs to automatically mine
temporal commonsense and use it to construct TComQA, a dataset derived from
SAMSum and RealNews corpora. TComQA has been validated through crowdsourcing
and achieves over 80\% precision in extracting temporal commonsense. The model
trained with TComQA also outperforms an LLM fine-tuned on existing dataset of
temporal question answering task.

</details>


### [24] [CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing](https://arxiv.org/abs/2508.15316)
*Abdul Rehman,Jian-Jun Zhang,Xiaosong Yang*

Main category: cs.CL

> CUPE模型实现了在短时窗口内独立捕捉音素特征，在多种语言上实现了跨语言性能，证明了通过建模基本声学模式进行通用语音处理的可能性。

<details>
  <summary>Details</summary>

**Motivation:** 许多语音处理任务需要纯粹的音素表示，不受到上下文的影响，这促使了我们开发CUPE模型。

**Method:** 开发了CUPE模型，这是一种轻量级模型，能在120毫秒内捕捉关键音素特征，处理短的、固定宽度的窗口，独立于上下文影响。

**Result:** 尽管参数较少，但CUPE在多种语言上的监督学习和自我监督学习中实现了有竞争力的跨语言性能，特别在零样本测试中表现出了强大的跨语言泛化能力。

**Conclusion:** 研究表明，通过建模音素长度窗口内的基本声学模式，实现了有效的通用语音处理。

**Abstract:** Universal phoneme recognition typically requires analyzing long speech
segments and language-specific patterns. Many speech processing tasks require
pure phoneme representations free from contextual influence, which motivated
our development of CUPE - a lightweight model that captures key phoneme
features in just 120 milliseconds, about one phoneme's length. CUPE processes
short, fixed-width windows independently and, despite fewer parameters than
current approaches, achieves competitive cross-lingual performance by learning
fundamental acoustic patterns common to all languages. Our extensive evaluation
through supervised and self-supervised training on diverse languages, including
zero-shot tests on the UCLA Phonetic Corpus, demonstrates strong cross-lingual
generalization and reveals that effective universal speech processing is
possible through modeling basic acoustic patterns within phoneme-length
windows.

</details>


### [25] [KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models](https://arxiv.org/abs/2508.15357)
*Haji Gul,Abul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.CL

> 提出基于距离平均解的KG评估法（EDAS）作为元指标，以整合多指标和多数据集的表现，提供更系统、可靠和可解释性高的评估框架，实验验证了其有效性和稳健性。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有KGC模型在不同数据集和指标上评估时表现不一致的问题，提出一个统一且可解释的评估方法，以支持更有效的模型选择和公平的跨数据集评估。

**Method:** 提出基于距离平均解的评估方法（EDAS），此方法能将模型在多个数据集和不同评价指标上的表现整合成单一标准化得分，以提供全面和可解释的评估。

**Result:** 实验结果显示EDAS能够将多指标、多数据集的表现整合成一个统一的排名，提供了一种一致、稳健且通用的KGC模型评估框架，并展示了其有效性。

**Conclusion:** EDAS提供了一个新的和全面的评估框架，提高了KGC模型选择的公平性和可靠性，通过整合复杂的表现指标和跨数据集的表现，使其成为一种更为通用和一致的评估方式。

**Abstract:** Knowledge Graphs (KGs) enable applications in various domains such as
semantic search, recommendation systems, and natural language processing. KGs
are often incomplete, missing entities and relations, an issue addressed by
Knowledge Graph Completion (KGC) methods that predict missing elements.
Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank
(MR), and Hit@k, are commonly used to assess the performance of such KGC
models. A major challenge in evaluating KGC models, however, lies in comparing
their performance across multiple datasets and metrics. A model may outperform
others on one dataset but underperform on another, making it difficult to
determine overall superiority. Moreover, even within a single dataset,
different metrics such as MRR and Hit@1 can yield conflicting rankings, where
one model excels in MRR while another performs better in Hit@1, further
complicating model selection for downstream tasks. These inconsistencies hinder
holistic comparisons and highlight the need for a unified meta-metric that
integrates performance across all metrics and datasets to enable a more
reliable and interpretable evaluation framework. To address this need, we
propose KG Evaluation based on Distance from Average Solution (EDAS), a robust
and interpretable meta-metric that synthesizes model performance across
multiple datasets and diverse evaluation criteria into a single normalized
score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated
aspects of performance, EDAS offers a global perspective that supports more
informed model selection and promotes fairness in cross-dataset evaluation.
Experimental results on benchmark datasets such as FB15k-237 and WN18RR
demonstrate that EDAS effectively integrates multi-metric, multi-dataset
performance into a unified ranking, offering a consistent, robust, and
generalizable framework for evaluating KGC models.

</details>


### [26] [A Survey on Large Language Model Benchmarks](https://arxiv.org/abs/2508.15361)
*Shiwen Ni,Guhong Chen,Shuaimin Li,Xuanang Chen,Siyi Li,Bingli Wang,Qiyao Wang,Xingjian Wang,Yifan Zhang,Liyang Fan,Chengming Li,Ruifeng Xu,Le Sun,Min Yang*

Main category: cs.CL

> 本研究回顾了大型语言模型基准测试的现状和发展，对存在的问题进行了分析并提出了基准创新的设计范式。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于系统性地分析当前大型语言模型基准测试的状况，指出其存在的问题，并为未来的基准创新提供设计思路。

**Method:** 通过对283个代表性基准测试进行分类和分析，并梳理通用能力、领域特定和目标特定的基准测试，指出现有问题并提出解决框架。

**Result:** 本研究首次系统回顾了大型语言模型基准测试的现状和发展，将283个代表性基准测试分为通用能力、领域特定和目标特定三类。通用能力基准测试涵盖了核心语言学、知识和推理等方面；领域特定基准测试专注于自然科学、人文及社会科学和工程技术等；目标特定基准测试侧重于风险、可靠性和代理等方面。研究指出当前基准测试存在一些问题，如数据污染导致的分数膨胀、文化和语言偏差导致的不公平评估以及缺乏对过程可信性和动态环境的评估，并提出了未来基准设计的参考范式。

**Conclusion:** 未来基准测试需要解决数据污染、文化和语言偏差以及缺乏对过程可信性和动态环境评估的问题，并提出了设计创新基准测试的参考框架。

**Abstract:** In recent years, with the rapid development of the depth and breadth of large
language models' capabilities, various corresponding evaluation benchmarks have
been emerging in increasing numbers. As a quantitative assessment tool for
model performance, benchmarks are not only a core means to measure model
capabilities but also a key element in guiding the direction of model
development and promoting technological innovation. We systematically review
the current status and development of large language model benchmarks for the
first time, categorizing 283 representative benchmarks into three categories:
general capabilities, domain-specific, and target-specific. General capability
benchmarks cover aspects such as core linguistics, knowledge, and reasoning;
domain-specific benchmarks focus on fields like natural sciences, humanities
and social sciences, and engineering technology; target-specific benchmarks pay
attention to risks, reliability, agents, etc. We point out that current
benchmarks have problems such as inflated scores caused by data contamination,
unfair evaluation due to cultural and linguistic biases, and lack of evaluation
on process credibility and dynamic environments, and provide a referable design
paradigm for future benchmark innovation.

</details>


### [27] [Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation](https://arxiv.org/abs/2508.15370)
*Yichi Zhang,Yao Huang,Yifan Wang,Yitong Sun,Chang Liu,Zhe Zhao,Zhengwei Fang,Huanran Chen,Xiao Yang,Xingxing Wei,Hang Su,Yinpeng Dong,Jun Zhu*

Main category: cs.CL

> 研究提出MultiTrust-X用于评估和缓解MLLMs的可信性问题。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大语言模型的能力得到了显著提升，但其可信性仍然是一个主要问题。现有的评估和缓解方法往往只关注狭窄的方面，并忽视了多模态引入的风险。

**Method:** 我们提出了MultiTrust-X，一个全面的基准，用于评估、分析和缓解MLLMs的可信性问题。MultiTrust-X定义了一个三维框架，涵盖五个可信性方面，两种新型风险类型，以及多个缓解策略。

**Result:** 实验揭示了现有模型的显著漏洞，包括可信性和通用能力之间的差距，以及多模态训练和推理对基础大模型潜在风险的放大。

**Conclusion:** 研究发现现有的缓解策略在某些方面取得改进，但很难综合地解决可信性问题，且会引入意料之外的权衡，这些发现在未来改进提供了实用见解。提出了一种带有链式思维推理能力的RESA方法，实现了最新结果。

**Abstract:** The trustworthiness of Multimodal Large Language Models (MLLMs) remains an
intense concern despite the significant progress in their capabilities.
Existing evaluation and mitigation approaches often focus on narrow aspects and
overlook risks introduced by the multimodality. To tackle these challenges, we
propose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and
mitigating the trustworthiness issues of MLLMs. We define a three-dimensional
framework, encompassing five trustworthiness aspects which include
truthfulness, robustness, safety, fairness, and privacy; two novel risk types
covering multimodal risks and cross-modal impacts; and various mitigation
strategies from the perspectives of data, model architecture, training, and
inference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and
28 curated datasets, enabling holistic evaluations over 30 open-source and
proprietary MLLMs and in-depth analysis with 8 representative mitigation
methods. Our extensive experiments reveal significant vulnerabilities in
current models, including a gap between trustworthiness and general
capabilities, as well as the amplification of potential risks in base LLMs by
both multimodal training and inference. Moreover, our controlled analysis
uncovers key limitations in existing mitigation strategies that, while some
methods yield improvements in specific aspects, few effectively address overall
trustworthiness, and many introduce unexpected trade-offs that compromise model
utility. These findings also provide practical insights for future
improvements, such as the benefits of reasoning to better balance safety and
performance. Based on these insights, we introduce a Reasoning-Enhanced Safety
Alignment (RESA) approach that equips the model with chain-of-thought reasoning
ability to discover the underlying risks, achieving state-of-the-art results.

</details>


### [28] [Confidence-Modulated Speculative Decoding for Large Language Models](https://arxiv.org/abs/2508.15371)
*Jaydip Sen,Subhasis Dasgupta,Hetvi Waghela*

Main category: cs.CL

> 本研究提出了一种基于信心调制绘制的信息理论框架，用于改善投机解码的适应性、降低回滚频率、提高资源利用率并保持输出质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有的投机解码方法依赖于固定的绘制长度和严格的验证标准，限制了它们在不同模型不确定性和输入复杂度下的适用性。

**Method:** 提出了基于信心调制绘制的信息理论框架，通过熵和基于边界的不确定性度量动态调整每轮迭代中投机生成的标记数量。绘制和验证过程使用相同的信心信号进行调制，使绘制的标记接受更灵活，同时保持生成质量。

**Result:** 在机器翻译和总结任务上，实验显示与标准的投机解码相比，所提出的方法显著提高了速度，同时保持或提高了BLEU和ROUGE得分。

**Conclusion:** 实验表明，所提出的插件式方法能够在各种不确定性条件下，为大型语言模型提供高效和可靠的解码。

**Abstract:** Speculative decoding has emerged as an effective approach for accelerating
autoregressive inference by parallelizing token generation through a
draft-then-verify paradigm. However, existing methods rely on static drafting
lengths and rigid verification criteria, limiting their adaptability across
varying model uncertainties and input complexities. This paper proposes an
information-theoretic framework for speculative decoding based on
confidence-modulated drafting. By leveraging entropy and margin-based
uncertainty measures over the drafter's output distribution, the proposed
method dynamically adjusts the number of speculatively generated tokens at each
iteration. This adaptive mechanism reduces rollback frequency, improves
resource utilization, and maintains output fidelity. Additionally, the
verification process is modulated using the same confidence signals, enabling
more flexible acceptance of drafted tokens without sacrificing generation
quality. Experiments on machine translation and summarization tasks demonstrate
significant speedups over standard speculative decoding while preserving or
improving BLEU and ROUGE scores. The proposed approach offers a principled,
plug-in method for efficient and robust decoding in large language models under
varying conditions of uncertainty.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [29] [Heatmap Regression without Soft-Argmax for Facial Landmark Detection](https://arxiv.org/abs/2508.14929)
*Chiao-An Yang,Raymond A. Yeh*

Main category: cs.CV

> 研究提出了一种基于结构化预测框架的新方法，而不是使用Soft-argmax方法，实现了更快的训练和更准确的面部关键点检测。

<details>
  <summary>Details</summary>

**Motivation:** 重新审视了长期使用的Soft-argmax方法，并表明这不是唯一能够实现高性能的方法。

**Method:** 提出了一种基于经典结构化预测框架的替代训练目标方法，而不是传统的Soft-argmax方法。

**Result:** 实验结果表明，该方法在三个面部关键点基准数据集（WFLW、COFW和300W）上达到了state-of-the-art性能，训练速度提高了2.2倍，同时还保持了更好的准确率。

**Conclusion:** 研究表明，所提出的方法不仅在面部关键点检测任务上达到了出色的性能，同时还显著提升了训练的效率。

**Abstract:** Facial landmark detection is an important task in computer vision with
numerous applications, such as head pose estimation, expression analysis, face
swapping, etc. Heatmap regression-based methods have been widely used to
achieve state-of-the-art results in this task. These methods involve computing
the argmax over the heatmaps to predict a landmark. Since argmax is not
differentiable, these methods use a differentiable approximation, Soft-argmax,
to enable end-to-end training on deep-nets. In this work, we revisit this
long-standing choice of using Soft-argmax and demonstrate that it is not the
only way to achieve strong performance. Instead, we propose an alternative
training objective based on the classic structured prediction framework.
Empirically, our method achieves state-of-the-art performance on three facial
landmark benchmarks (WFLW, COFW, and 300W), converging 2.2x faster during
training while maintaining better/competitive accuracy. Our code is available
here: https://github.com/ca-joe-yang/regression-without-softarg.

</details>


### [30] [Fast Graph Neural Network for Image Classification](https://arxiv.org/abs/2508.14958)
*Mustafa Mohammadi Gharasuie,Luis Rueda*

Main category: cs.CV

> This paper integrates Graph Convolutional Networks (GCNs) with Voronoi diagrams to improve image classification efficiency and accuracy, demonstrating superior performance over existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to improve the efficiency and accuracy of image classification by leveraging the strengths of GCNs and Voronoi diagrams in modeling complex relational data structures.

**Method:** This study proposes a novel approach that integrates Graph Convolutional Networks (GCNs) with Voronoi diagrams for image classification. The method represents images as graphs with pixels or regions as vertices, and optimizes these graphs using Delaunay triangulations.

**Result:** The proposed model achieves notable improvements in preprocessing efficiency and classification accuracy, surpassing current state-of-the-art methods, especially in complex and fine-grained classification tasks.

**Conclusion:** The research emphasizes the importance of the new approach in advancing image classification and expands the applications of graph-based learning in computer vision and the analysis of unstructured data.

**Abstract:** The rapid progress in image classification has been largely driven by the
adoption of Graph Convolutional Networks (GCNs), which offer a robust framework
for handling complex data structures. This study introduces a novel approach
that integrates GCNs with Voronoi diagrams to enhance image classification by
leveraging their ability to effectively model relational data. Unlike
conventional convolutional neural networks (CNNs), our method represents images
as graphs, where pixels or regions function as vertices. These graphs are then
refined using corresponding Delaunay triangulations, optimizing their
representation. The proposed model achieves significant improvements in both
preprocessing efficiency and classification accuracy across various benchmark
datasets, surpassing state-of-the-art approaches, particularly in challenging
scenarios involving intricate scenes and fine-grained categories. Experimental
results, validated through cross-validation, underscore the effectiveness of
combining GCNs with Voronoi diagrams for advancing image classification. This
research not only presents a novel perspective on image classification but also
expands the potential applications of graph-based learning paradigms in
computer vision and unstructured data analysis.

</details>


### [31] [You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation](https://arxiv.org/abs/2508.14965)
*Hakjin Lee,Junghoon Seo,Jaehoon Sim*

Main category: cs.CV

> 提出了YOPO方法，即统一对象检测和9自由度姿态估计的单阶段、基于查询的框架，仅基于RGB图像而不需要其他数据。YOPO在多个基准测试中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 动机是提出一种更简单、仅依赖RGB图像的方法来解决在特定类别下从单一RGB图像恢复未见实例完整9自由度姿态的核心挑战。

**Method:** 提出了YOPO方法，这是一种单阶段的基于查询的框架，将9-DoF姿态估计视为2D检测的自然扩展。YOPO通过增加一个轻量级的姿态头、一个基于边界框的条件翻译模块和一个6D感知的匈牙利匹配成本来增强变压器检测器。该模型仅使用RGB图像和类别级别的姿态标签进行端到端的训练。

**Result:** 尽管设计简约，YOPO在三个基准测试中创造了新的记录。在REAL275数据集上，它达到了79.6%的$m{IoU}_{50}$和54.1%的$10^	ext{o}$10cm度量标准，超越了以前仅使用RGB方法的性能。

**Conclusion:** 研究结论是，YOPO提供了一种有效的方法，通过仅依赖于RGB图像和类别级别的姿态标签来实现高效的物体检测和9自由度姿态估计的集成。这种方法弥补了之前仅使用RGB图像方法的不足，并且几乎达到了RGB-D系统的性能。

**Abstract:** Accurately recovering the full 9-DoF pose of unseen instances within specific
categories from a single RGB image remains a core challenge for robotics and
automation. Most existing solutions still rely on pseudo-depth, CAD models, or
multi-stage cascades that separate 2D detection from pose estimation. Motivated
by the need for a simpler, RGB-only alternative that learns directly at the
category level, we revisit a longstanding question: Can object detection and
9-DoF pose estimation be unified with high performance, without any additional
data? We show that they can with our method, YOPO, a single-stage, query-based
framework that treats category-level 9-DoF estimation as a natural extension of
2D detection. YOPO augments a transformer detector with a lightweight pose
head, a bounding-box-conditioned translation module, and a 6D-aware Hungarian
matching cost. The model is trained end-to-end only with RGB images and
category-level pose labels. Despite its minimalist design, YOPO sets a new
state of the art on three benchmarks. On the REAL275 dataset, it achieves 79.6%
$\rm{IoU}_{50}$ and 54.1% under the $10^\circ$$10{\rm{cm}}$ metric, surpassing
prior RGB-only methods and closing much of the gap to RGB-D systems. The code,
models, and additional qualitative results can be found on our project.

</details>


### [32] [Paired-Sampling Contrastive Framework for Joint Physical-Digital Face Attack Detection](https://arxiv.org/abs/2508.14980)
*Andrei Balykin,Anvar Ganiev,Denis Kondranin,Kirill Polevoda,Nikolai Liudkevich,Artem Petrov*

Main category: cs.CV

> 本文提出了一个统一的训练框架，名为Paired-Sampling Contrastive Framework，用于识别面部伪造攻击，这种方法在第6届面部反欺骗挑战赛中表现优异，具有较低的复杂性和训练时间，适合实际部署。

<details>
  <summary>Details</summary>

**Motivation:** 目前的面部识别系统在面对伪造攻击时仍然存在脆弱性，传统的防御方法一般使用单独的模型处理不同的攻击，这增加了系统的复杂性和推理延迟。本方法旨在提出一种统一的训练方法来处理不同的攻击方式。

**Method:** 提出了配对采样对比框架（Paired-Sampling Contrastive Framework），该方法通过自动匹配真实和攻击自拍的配对来学习模态无关的活动线索。

**Result:** 该方法在6th Face Anti-Spoofing Challenge Unified Physical-Digital Attack Detection基准上达到了2.10%的平均分类错误率（ACER），优于以前的解决方案。

**Conclusion:** 所提出的框架在性能上表现出色，且计算量低（4.46 GFLOPs），训练时间短（少于一小时），适用于实际场景的部署。

**Abstract:** Modern face recognition systems remain vulnerable to spoofing attempts,
including both physical presentation attacks and digital forgeries.
Traditionally, these two attack vectors have been handled by separate models,
each targeting its own artifacts and modalities. However, maintaining distinct
detectors increases system complexity and inference latency and leaves systems
exposed to combined attack vectors. We propose the Paired-Sampling Contrastive
Framework, a unified training approach that leverages automatically matched
pairs of genuine and attack selfies to learn modality-agnostic liveness cues.
Evaluated on the 6th Face Anti-Spoofing Challenge Unified Physical-Digital
Attack Detection benchmark, our method achieves an average classification error
rate (ACER) of 2.10 percent, outperforming prior solutions. The framework is
lightweight (4.46 GFLOPs) and trains in under one hour, making it practical for
real-world deployment. Code and pretrained models are available at
https://github.com/xPONYx/iccv2025_deepfake_challenge.

</details>


### [33] [TAIGen: Training-Free Adversarial Image Generation via Diffusion Models](https://arxiv.org/abs/2508.15020)
*Susim Roy,Anubhooti Jain,Mayank Vatsa,Richa Singh*

Main category: cs.CV

> 本文提出TAIGen，一种无需训练的黑盒方法，用于高效生成对抗样本，通过减少采样步骤并采用选择性RGB通道策略，提高生成图像质量和速度。


<details>
  <summary>Details</summary>

**Motivation:** 为了解决生成模型对抗攻击中图像质量低和计算资源消耗大的问题，本篇论文提出了TAIGen方法，利用无条件扩散模型在较少采样步骤中生成对抗样本。


**Method:** TAIGen方法采用了选择性RGB通道策略，红色通道使用注意力图，绿色通道和蓝色通道使用GradCAM指导的扰动，以此来保持图像结构并最大化目标模型中的误分类。


**Result:**  terörizes


**Conclusion:** TAIGen方法不仅保持了较高的视觉质量（PSNR超过30dB），而且在生成对抗样本时速度比现有的扩散模型攻击方法快10倍。


**Abstract:** Adversarial attacks from generative models often produce low-quality images
and require substantial computational resources. Diffusion models, though
capable of high-quality generation, typically need hundreds of sampling steps
for adversarial generation. This paper introduces TAIGen, a training-free
black-box method for efficient adversarial image generation. TAIGen produces
adversarial examples using only 3-20 sampling steps from unconditional
diffusion models. Our key finding is that perturbations injected during the
mixing step interval achieve comparable attack effectiveness without processing
all timesteps. We develop a selective RGB channel strategy that applies
attention maps to the red channel while using GradCAM-guided perturbations on
green and blue channels. This design preserves image structure while maximizing
misclassification in target models. TAIGen maintains visual quality with PSNR
above 30 dB across all tested datasets. On ImageNet with VGGNet as source,
TAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8%
against ShuffleNet. The method generates adversarial examples 10x faster than
existing diffusion-based attacks. Our method achieves the lowest robust
accuracy, indicating it is the most impactful attack as the defense mechanism
is least successful in purifying the images generated by TAIGen.

</details>


### [34] [Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement](https://arxiv.org/abs/2508.15027)
*Chunming He,Fengyang Xiao,Rihan Zhang,Chengyu Fang,Deng-Ping Fan,Sina Farsiu*

Main category: cs.CV

> The paper introduces RUN++, a method that uses a reversible unfolding network with generative refinement to improve concealed visual perception by handling both mask and RGB domains and using a targeted diffusion model to refine uncertain regions, thereby reducing false positives and negatives effectively under real-world degradations.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitations of existing concealed visual perception methods which typically only operate within the mask domain and fail to exploit the potential of the RGB domain. This leads to inefficiencies in handling ambiguity and uncertainty in object perception.

**Method:** RUN++ formulates the CVP task as a mathematical optimization problem and implements a multi-stage deep network to address it. The network incorporates reversible modeling across mask and RGB domains, and employs three modules: CORE (for mask domain), CARE (for RGB domain), and FINE (for noise-based enhancement).

**Result:** The method provides enhanced foreground-background separation and refined detail restoration in uncertain regions, achieving significant reductions in false positives and negatives. A new paradigm for robust CVP systems is also presented that works well under real-world degradations.

**Conclusion:** The proposed RUN++ method innovates in the concealed visual perception area by integrating reversible modeling, multi-stage refinement, and targeted diffusion, thus improving the robustness and accuracy of CVP systems.

**Abstract:** Existing methods for concealed visual perception (CVP) often leverage
reversible strategies to decrease uncertainty, yet these are typically confined
to the mask domain, leaving the potential of the RGB domain underexplored. To
address this, we propose a reversible unfolding network with generative
refinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as
a mathematical optimization problem and unfolds the iterative solution into a
multi-stage deep network. This approach provides a principled way to apply
reversible modeling across both mask and RGB domains while leveraging a
diffusion model to resolve the resulting uncertainty. Each stage of the network
integrates three purpose-driven modules: a Concealed Object Region Extraction
(CORE) module applies reversible modeling to the mask domain to identify core
object regions; a Context-Aware Region Enhancement (CARE) module extends this
principle to the RGB domain to foster better foreground-background separation;
and a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a
final refinement. The FINE module introduces a targeted Bernoulli diffusion
model that refines only the uncertain regions of the segmentation mask,
harnessing the generative power of diffusion for fine-detail restoration
without the prohibitive computational cost of a full-image process. This unique
synergy, where the unfolding network provides a strong uncertainty prior for
the diffusion model, allows RUN++ to efficiently direct its focus toward
ambiguous areas, significantly mitigating false positives and negatives.
Furthermore, we introduce a new paradigm for building robust CVP systems that
remain effective under real-world degradations and extend this concept into a
broader bi-level optimization framework.

</details>


### [35] [GasTwinFormer: A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging](https://arxiv.org/abs/2508.15057)
*Toqi Tahamid Sarker,Mohamed Embaby,Taminul Islam,Amer AbuGhazaleh,Khaled R Ahmed*

Main category: cs.CV

> 本文介绍了GasTwinFormer，一种用于光气成像中实时甲烷排放分割和饮食分类的混合视觉变压器。GasTwinFormer实现了高效且准确的牲畜排放监控和饮食分类，展示了其作为监控工具的实用性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于畜牧业甲烷排放占人为甲烷生产的32%，自动化监测对于气候缓解策略至关重要。本文旨在提供一个有效的解决方案来实现实时牲畜排放监测。

**Method:** 介绍了GasTwinFormer，这是一种混合视觉变压器，用于通过光气成像实现实时甲烷排放分割和饮食分类。其独特的混合孪生编码器交替使用空间缩减的全局注意力机制和局部分组注意力机制。此外，该架构包含一个轻量级LR-ASPP解码器，用于多尺度特征聚合，可以在统一框架内同时实现甲烷分割和饮食分类。

**Result:** GasTwinFormer在分割方面达到了74.47%的mIoU和83.63%的mF1，同时维持高效的参数(3.348M)、乘加运算(3.428G FLOPs)和推理速度(114.9 FPS)。此外，它的饮食分类准确率达到了100%。

**Conclusion:** GasTwinFormer通过实验证明了其作为实时牲畜排放监测实用解决方案的有效性。

**Abstract:** Livestock methane emissions represent 32% of human-caused methane production,
making automated monitoring critical for climate mitigation strategies. We
introduce GasTwinFormer, a hybrid vision transformer for real-time methane
emission segmentation and dietary classification in optical gas imaging through
a novel Mix Twin encoder alternating between spatially-reduced global attention
and locally-grouped attention mechanisms. Our architecture incorporates a
lightweight LR-ASPP decoder for multi-scale feature aggregation and enables
simultaneous methane segmentation and dietary classification in a unified
framework. We contribute the first comprehensive beef cattle methane emission
dataset using OGI, containing 11,694 annotated frames across three dietary
treatments. GasTwinFormer achieves 74.47% mIoU and 83.63% mF1 for segmentation
while maintaining exceptional efficiency with only 3.348M parameters, 3.428G
FLOPs, and 114.9 FPS inference speed. Additionally, our method achieves perfect
dietary classification accuracy (100%), demonstrating the effectiveness of
leveraging diet-emission correlations. Extensive ablation studies validate each
architectural component, establishing GasTwinFormer as a practical solution for
real-time livestock emission monitoring. Please see our project page at
gastwinformer.github.io.

</details>


### [36] [CurveFlow: Curvature-Guided Flow Matching for Image Generation](https://arxiv.org/abs/2508.15093)
*Yan Luo,Drake Du,Hao Huang,Yi Fang,Mengyu Wang*

Main category: cs.CV

> 论文提出了CurveFlow框架，用于学习非线性轨迹，以改善语义一致性并提升生成图像质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有校正流模型基于数据和噪声分布之间的线性轨迹，这种线性强制零曲率，可能导致图像生成过程经过数据流形的低概率区域。目前较少探索的问题是：这些轨迹的曲率如何影响生成图像与其对应描述之间的语义对齐度，即指令遵从性？

**Method:** 为了应对这一问题，我们提出了CurveFlow，这是一种新颖的流匹配框架，通过直接引入曲率指导来学习平滑的非线性轨迹。我们的方法包括了一种强大的曲率正则化技术，该技术对轨迹内在动态的突然变化进行惩罚。

**Result:** 在MS COCO 2014和2017上的广泛实验表明，CurveFlow在文本到图像生成任务中达到了最先进的性能，显著超越了标准校正流变体和其他非线性基准模型，如校正扩散模型。尤其在语义一致性指标如BLEU、METEOR、ROUGE和CLAIR方面有明显提升。

**Conclusion:** 这证明了我们曲率感知模型显著增强了模型按复杂指令生成图像的能力，同时保持了高质量的图像生成。

**Abstract:** Existing rectified flow models are based on linear trajectories between data
and noise distributions. This linearity enforces zero curvature, which can
inadvertently force the image generation process through low-probability
regions of the data manifold. A key question remains underexplored: how does
the curvature of these trajectories correlate with the semantic alignment
between generated images and their corresponding captions, i.e., instructional
compliance? To address this, we introduce CurveFlow, a novel flow matching
framework designed to learn smooth, non-linear trajectories by directly
incorporating curvature guidance into the flow path. Our method features a
robust curvature regularization technique that penalizes abrupt changes in the
trajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017
demonstrate that CurveFlow achieves state-of-the-art performance in
text-to-image generation, significantly outperforming both standard rectified
flow variants and other non-linear baselines like Rectified Diffusion. The
improvements are especially evident in semantic consistency metrics such as
BLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling
substantially enhances the model's ability to faithfully follow complex
instructions while simultaneously maintaining high image quality. The code is
made publicly available at
https://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.

</details>


### [37] [HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment](https://arxiv.org/abs/2508.15130)
*Vaishnav Ramesh,Haining Wang,Md Jahidul Islam*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Despite significant progress in no-reference image quality assessment
(NR-IQA), dataset biases and reliance on subjective labels continue to hinder
their generalization performance. We propose HiRQA, Hierarchical Ranking and
Quality Alignment), a self-supervised, opinion-unaware framework that offers a
hierarchical, quality-aware embedding through a combination of ranking and
contrastive learning. Unlike prior approaches that depend on pristine
references or auxiliary modalities at inference time, HiRQA predicts quality
scores using only the input image. We introduce a novel higher-order ranking
loss that supervises quality predictions through relational ordering across
distortion pairs, along with an embedding distance loss that enforces
consistency between feature distances and perceptual differences. A
training-time contrastive alignment loss, guided by structured textual prompts,
further enhances the learned representation. Trained only on synthetic
distortions, HiRQA generalizes effectively to authentic degradations, as
demonstrated through evaluation on various distortions such as lens flare,
haze, motion blur, and low-light conditions. For real-time deployment, we
introduce \textbf{HiRQA-S}, a lightweight variant with an inference time of
only 3.5 ms per image. Extensive experiments across synthetic and authentic
benchmarks validate HiRQA's state-of-the-art (SOTA) performance, strong
generalization ability, and scalability.

</details>


### [38] [Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments](https://arxiv.org/abs/2508.15158)
*Md. Nurul Absur,Abhinav Kumar,Swastik Brahma,Saptarshi Debroy*

Main category: cs.CV

> 本文提出了一种新的受投资组合理论启发的边缘资源管理策略，用于保证在可能发生的系统中断情况下进行多视角3D重建的质量。实验结果表明，该方法相比于传统基线策略，在面对时空干扰时能提供更可靠的3D重建质量。

<details>
  <summary>Details</summary>

**Motivation:** 多视角3D重建应用在紧急响应、战术场景和公共安全等需要快速态势感知的领域中至关重要，然而这些应用中的边缘计算环境的动态性和操作挑战性会导致时空相关的干扰，进而影响相机操作并导致重建质量下降。本文旨在解决这一问题。

**Method:** 本文提出了一种受投资组合理论启发的边缘资源管理策略，以保证在可能存在系统中断的情况下，多视角3D重建的质量。采用遗传算法解决投资组合理论优化问题，该算法在实际系统设置中能快速收敛。

**Result:** 通过使用公开的和定制的3D数据集，本文展示了在存在时空干扰情况下，与传统基线策略相比，提出的相机选择策略确实实现了更可靠的3D重建。

**Conclusion:** 本文提出的方法不仅能够保证在相机容易受到时空相关干扰的情况下实现高质量的3D重建，而且通过遗传算法优化的快速收敛特性，适用于实际系统的实时需求。

**Abstract:** Multi-view 3D reconstruction applications are revolutionizing critical use
cases that require rapid situational-awareness, such as emergency response,
tactical scenarios, and public safety. In many cases, their near-real-time
latency requirements and ad-hoc needs for compute resources necessitate
adoption of `Just-in-time' edge environments where the system is set up on the
fly to support the applications during the mission lifetime. However,
reliability issues can arise from the inherent dynamism and operational
adversities of such edge environments, resulting in spatiotemporally correlated
disruptions that impact the camera operations, which can lead to sustained
degradation of reconstruction quality. In this paper, we propose a novel
portfolio theory inspired edge resource management strategy for reliable
multi-view 3D reconstruction against possible system disruptions. Our proposed
methodology can guarantee reconstruction quality satisfaction even when the
cameras are prone to spatiotemporally correlated disruptions. The portfolio
theoretic optimization problem is solved using a genetic algorithm that
converges quickly for realistic system settings. Using publicly available and
customized 3D datasets, we demonstrate the proposed camera selection strategy's
benefits in guaranteeing reliable 3D reconstruction against traditional
baseline strategies, under spatiotemporal disruptions.

</details>


### [39] [XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2508.15168)
*Masato Ito,Kaito Tanaka,Keisuke Matsuda,Aya Nakayama*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating
early and accurate diagnosis. While deep learning models have shown promise in
DR detection, their black-box nature often hinders clinical adoption due to a
lack of transparency and interpretability. To address this, we propose XDR-LVLM
(eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that
leverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis
coupled with natural language-based explanations. XDR-LVLM integrates a
specialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt
Engineering and Multi-stage Fine-tuning to deeply understand pathological
features within fundus images and generate comprehensive diagnostic reports.
These reports explicitly include DR severity grading, identification of key
pathological concepts (e.g., hemorrhages, exudates, microaneurysms), and
detailed explanations linking observed features to the diagnosis. Extensive
experiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM
achieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and
an F1 Score of 79.92% for disease diagnosis, and superior results for concept
detection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the
high fluency, accuracy, and clinical utility of the generated explanations,
showcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and
clinical needs by providing robust and interpretable insights.

</details>


### [40] [MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion](https://arxiv.org/abs/2508.15169)
*Xuyang Chen,Zhijun Zhai,Kaixuan Zhou,Zengmao Wang,Jianan He,Dong Wang,Yanfeng Zhang,mingwei Sun,Rüdiger Westermann,Konrad Schindler,Liqiu Meng*

Main category: cs.CV

> MeSS利用城市网格模型生成高质量、风格一致的3D城市户外场景，改进了现有方法的几何对齐和生成质量，并支持多样化的场景渲染风格。

<details>
  <summary>Details</summary>

**Motivation:** 现在的城市网格模型缺乏逼真的纹理，影响了它们在虚拟城市导航和自动驾驶中的应用。我们的研究动机是通过提出MeSS解决这一问题，生成更符合真实场景且几何一致的城市户外场景。

**Method:** 我们的方法MeSS旨在利用城市网格模型作为几何先验生成高质量且风格一致的户外场景。整个流程分为三个主要阶段：首先，使用级联Outpainting ControlNets生成几何一致的稀疏视图；其次，通过AGInpaint组件传播更多密集的中间视图；第三，使用GCAlign模块消除全局视觉不一致性。同时，在生成过程中，通过在网格表面上初始化高斯球来重建3D Gaussian Splatting场景。

**Result:** 我们的方法在几何对齐和生成质量上明显优于现有方法，生成的场景可以利用重光照和风格转换技术渲染出多样化的风格。

**Conclusion:** 我们的研究展示了MeSS在城市户外场景生成方面的优越性能，特别是在几何对齐和生成质量方面。此外，生成的场景可以进行风格化处理，增强了实际应用价值。

**Abstract:** Mesh models have become increasingly accessible for numerous cities; however,
the lack of realistic textures restricts their application in virtual urban
navigation and autonomous driving. To address this, this paper proposes MeSS
(Meshbased Scene Synthesis) for generating high-quality, styleconsistent
outdoor scenes with city mesh models serving as the geometric prior. While
image and video diffusion models can leverage spatial layouts (such as depth
maps or HD maps) as control conditions to generate street-level perspective
views, they are not directly applicable to 3D scene generation. Video diffusion
models excel at synthesizing consistent view sequences that depict scenes but
often struggle to adhere to predefined camera paths or align accurately with
rendered control videos. In contrast, image diffusion models, though unable to
guarantee cross-view visual consistency, can produce more geometry-aligned
results when combined with ControlNet. Building on this insight, our approach
enhances image diffusion models by improving cross-view consistency. The
pipeline comprises three key stages: first, we generate geometrically
consistent sparse views using Cascaded Outpainting ControlNets; second, we
propagate denser intermediate views via a component dubbed AGInpaint; and
third, we globally eliminate visual inconsistencies (e.g., varying exposure)
using the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting
(3DGS) scene is reconstructed by initializing Gaussian balls on the mesh
surface. Our method outperforms existing approaches in both geometric alignment
and generation quality. Once synthesized, the scene can be rendered in diverse
styles through relighting and style transfer techniques.

</details>


### [41] [Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning](https://arxiv.org/abs/2508.15207)
*Arjun Srinivasan,Anubhav Paras,Aniket Bera*

Main category: cs.CV

> 本文提出了一种新方法，使用学习来生成对抗性行为，以便识别和测试潜在的失败场景，特别是在涉及规则型代理的环境中。

<details>
  <summary>Details</summary>

**Motivation:** 在自动驾驶等安全性关键应用中，需要正确建模规则型代理，以学习期望的最优行为。

**Method:** 本文提出了一种基于学习的方法，用于推导规则型周围代理的对抗行为，旨在导致失败场景。

**Result:** 与所有规则型代理进行评估后，实验结果显示推导出的对抗性代理导致累积奖励减少。

**Conclusion:** 相比于现有的规则型代理建模方法，本文提出的方法可以更好地制造失败场景。

**Abstract:** Existing approaches in reinforcement learning train an agent to learn desired
optimal behavior in an environment with rule based surrounding agents. In
safety critical applications such as autonomous driving it is crucial that the
rule based agents are modelled properly. Several behavior modelling strategies
and IDM models are used currently to model the surrounding agents. We present a
learning based method to derive the adversarial behavior for the rule based
agents to cause failure scenarios. We evaluate our adversarial agent against
all the rule based agents and show the decrease in cumulative reward.

</details>


### [42] [DyMorph-B2I: Dynamic and Morphology-Guided Binary-to-Instance Segmentation for Renal Pathology](https://arxiv.org/abs/2508.15208)
*Leiyue Zhao,Yuechen Yang,Yanfan Zhu,Haichun Yang,Yuankai Huo,Paul D. Simonson,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

> The paper presents DyMorph-B2I, a binary-to-instance segmentation pipeline for renal pathology that integrates classical post-processing methods with adaptive refinement to enable more accurate morphometric analysis.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the precision of downstream analyses in renal pathology by providing instance-level segmentation, which is lacking in existing datasets and automated methods that mainly offer binary masks.

**Method:** The method introduces DyMorph-B2I, a dynamic, morphology-guided binary-to-instance segmentation pipeline for renal pathology. It integrates classical post-processing techniques such as watershed, skeletonization, and morphological operations with adaptive geometric refinement and customizable hyperparameter tuning.

**Result:** Experimental results show that DyMorph-B2I outperforms individual classical approaches and naive combinations, providing superior instance separation, and is available at https://github.com/ddrrnn123/DyMorph-B2I.

**Conclusion:** The conclusion is that DyMorph-B2I facilitates more accurate morphometric analysis in renal pathology by offering robust instance separation of structures in binary masks, thus enhancing the precision of downstream analyses.

**Abstract:** Accurate morphological quantification of renal pathology functional units
relies on instance-level segmentation, yet most existing datasets and automated
methods provide only binary (semantic) masks, limiting the precision of
downstream analyses. Although classical post-processing techniques such as
watershed, morphological operations, and skeletonization, are often used to
separate semantic masks into instances, their individual effectiveness is
constrained by the diverse morphologies and complex connectivity found in renal
tissue. In this study, we present DyMorph-B2I, a dynamic, morphology-guided
binary-to-instance segmentation pipeline tailored for renal pathology. Our
approach integrates watershed, skeletonization, and morphological operations
within a unified framework, complemented by adaptive geometric refinement and
customizable hyperparameter tuning for each class of functional unit. Through
systematic parameter optimization, DyMorph-B2I robustly separates adherent and
heterogeneous structures present in binary masks. Experimental results
demonstrate that our method outperforms individual classical approaches and
na\"ive combinations, enabling superior instance separation and facilitating
more accurate morphometric analysis in renal pathology workflows. The pipeline
is publicly available at: https://github.com/ddrrnn123/DyMorph-B2I.

</details>


### [43] [STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation](https://arxiv.org/abs/2508.15216)
*Vipooshan Vipulananthan,Kumudu Mohottala,Kavindu Chinthana,Nimsara Paramulla,Charith D Chitraranjan*

Main category: cs.CV

> 本文提出了一个新的模型STAGNet，用于基于驾驶舱视频预测交通事故，实验结果显示该模型优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在改善道路安全，通过使用基于驾驶舱视频的模型来预测事故，这种方法虽然更具挑战性，但却更为经济且易于部署。

**Method:** 本文提出了一种新的模型STAGNet，通过结合更好的时空特征并通过递归网络进行聚合，以提高基于驾驶舱视频的事故预测的图神经网络的性能。

**Result:** 实验结果表明，STAGNet模型在三个公开数据集上的平均精度和平均碰撞时间比之前的方法更高，无论是进行交叉验证还是在不同数据集上进行训练和测试。

**Conclusion:** 研究得出结论，通过利用STAGNet模型进行更好的时空特征聚合，可有效提高基于驾驶舱视频的事故预测效果。

**Abstract:** Accident prediction and timely warnings play a key role in improving road
safety by reducing the risk of injury to road users and minimizing property
damage. Advanced Driver Assistance Systems (ADAS) are designed to support human
drivers and are especially useful when they can anticipate potential accidents
before they happen. While many existing systems depend on a range of sensors
such as LiDAR, radar, and GPS, relying solely on dash-cam video input presents
a more challenging but a more cost-effective and easily deployable solution. In
this work, we incorporate better spatio-temporal features and aggregate them
through a recurrent network to improve upon state-of-the-art graph neural
networks for predicting accidents from dash-cam videos. Experiments using three
publicly available datasets show that our proposed STAGNet model achieves
higher average precision and mean time-to-collision values than previous
methods, both when cross-validated on a given dataset and when trained and
tested on different datasets.

</details>


### [44] [Collaborative Multi-Modal Coding for High-Quality 3D Generation](https://arxiv.org/abs/2508.15228)
*Ziang Cao,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

> TriMM创新提出多模态协作编码与辅助监督机制，结合三平面潜在扩散模型生成高质量3D内容，无论是在知名数据集还是新RGB-D数据集上，都展现了卓越的性能表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的3D原生生成架构要么主要基于单模态范式，忽略了多模态数据的互补优势，要么限制在3D结构上，从而限制了训练数据集的范围。

**Method:** TriMM采用协作式多模态编码，结合模态特定特征同时保留它们的独特表达优势，引入了辅助的2D和3D监督以提高多模态编码的鲁棒性和性能，并基于嵌入的多模态代码使用三平面潜在扩散模型生成高质量的3D资产。

**Result:** TriMM利用小量训练数据达到与大规模数据集训练模型相当的性能，并在多个知名数据集上的广泛实验中得到了验证。此外，通过对最近的RGB-D数据集的额外实验，验证了将其他多模态数据集引入3D生成中的可行性。

**Conclusion:** TriMM是首个基于前馈的3D原生生成模型，能够从小量基本多模态数据中学习，通过有效利用多模态实现高性能的3D资产生成。

**Abstract:** 3D content inherently encompasses multi-modal characteristics and can be
projected into different modalities (e.g., RGB images, RGBD, and point clouds).
Each modality exhibits distinct advantages in 3D asset modeling: RGB images
contain vivid 3D textures, whereas point clouds define fine-grained 3D
geometries. However, most existing 3D-native generative architectures either
operate predominantly within single-modality paradigms-thus overlooking the
complementary benefits of multi-modality data-or restrict themselves to 3D
structures, thereby limiting the scope of available training datasets. To
holistically harness multi-modalities for 3D modeling, we present TriMM, the
first feed-forward 3D-native generative model that learns from basic
multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM
first introduces collaborative multi-modal coding, which integrates
modality-specific features while preserving their unique representational
strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to
raise the robustness and performance of multi-modal coding. 3) Based on the
embedded multi-modal code, TriMM employs a triplane latent diffusion model to
generate 3D assets of superior quality, enhancing both the texture and the
geometric detail. Extensive experiments on multiple well-known datasets
demonstrate that TriMM, by effectively leveraging multi-modality, achieves
competitive performance with models trained on large-scale datasets, despite
utilizing a small amount of training data. Furthermore, we conduct additional
experiments on recent RGB-D datasets, verifying the feasibility of
incorporating other multi-modal datasets into 3D generation.

</details>


### [45] [Center-Oriented Prototype Contrastive Clustering](https://arxiv.org/abs/2508.15231)
*Shihao Dong,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.CV

> 研究提出了一种新的聚类框架，通过软原型对比和双重一致性学习模块，解决了类间冲突问题，并展示了优越的聚类效果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法通过原型对比来解决聚类任务中的类间冲突问题，但硬原型计算与真实类中心之间存在偏差。本研究旨在解决这一问题。

**Method:** 提出了一种以中心为导向的原型对比聚类框架，该框架由一个软原型对比模块和一个双重一致性学习模块组成。软原型对比模块使用样本属于每个类中心的概率作为权重来计算原型，旨在避免类间冲突并减少原型漂移。双重一致性学习模块则分别对同一样本的不同变换以及不同样本的邻域进行对齐，确保特征具有变换不变的语义信息和紧凑的类内分布，为原型计算提供可靠的保证。

**Result:** 在五个数据集上的广泛实验表明，所提出的方法相较于当前最优方法具有有效性。

**Conclusion:** 提出的方法在解决类间冲突和计算原型偏差问题上表现出色，并在多个数据集上证明了其有效性。代码已发布。

**Abstract:** Contrastive learning is widely used in clustering tasks due to its
discriminative representation. However, the conflict problem between classes is
difficult to solve effectively. Existing methods try to solve this problem
through prototype contrast, but there is a deviation between the calculation of
hard prototypes and the true cluster center. To address this problem, we
propose a center-oriented prototype contrastive clustering framework, which
consists of a soft prototype contrastive module and a dual consistency learning
module. In short, the soft prototype contrastive module uses the probability
that the sample belongs to the cluster center as a weight to calculate the
prototype of each category, while avoiding inter-class conflicts and reducing
prototype drift. The dual consistency learning module aligns different
transformations of the same sample and the neighborhoods of different samples
respectively, ensuring that the features have transformation-invariant semantic
information and compact intra-cluster distribution, while providing reliable
guarantees for the calculation of prototypes. Extensive experiments on five
datasets show that the proposed method is effective compared to the SOTA. Our
code is published on https://github.com/LouisDong95/CPCC.

</details>


### [46] [AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation](https://arxiv.org/abs/2508.15232)
*Ruipu Wu,Yige Zhang,Jinyu Chen,Linjiang Huang,Shifeng Zhang,Xu Zhou,Liang Wang,Si Liu*

Main category: cs.CV

> 本文提出了一种名为DuAl-VLN的新任务，其中两个无人机在不同的高度协作完成视觉和语言导航任务，同时构建了一个包含13838条数据的训练与评测数据集HaL-13k。

<details>
  <summary>Details</summary>

**Motivation:** 主要动机是解决无人机在户外环境中依靠自然语言指令和视觉线索进行导航的挑战，特别是在减少人为干预和细化指令需求方面。同时，利用无人机高机动性提供多粒度视角的能力，同时保持可控的运动空间进行学习。

**Method:** 本研究提出了一个名为AeroDuo的双无人机协作视觉和语言导航框架。高海拔无人机使用一个大规模的多模态语言模型（Pilot-LLM）进行目标推理，而低海拔无人机采用轻量级的多阶段策略进行导航和目标定标。两架无人机协同工作并仅交换极少的坐标信息以保证效率。

**Result:** 尚未提供具体的结果，但文章介绍了DuAl-VLN任务和数据集HaL-13k的构建过程。框架描述了一个如何通过协作实现目标理导航的方法，为此类任务提供了可能的解决方案并展示了其潜在优势和效果。

**Conclusion:** 通过引入DuAl-VLN任务和提出AeroDuo框架，本研究为无人机视觉与语言导航（VLN）任务提供了一个创新的解决方案，旨在通过双无人机协作提升导航性能，并利用构建的HaL-13k数据集，评估模型在新环境和未知目标上的泛化能力。

**Abstract:** Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables
Unmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural
language instructions and visual cues. However, due to the extended
trajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN
performance is challenging and often requires human intervention or overly
detailed instructions. To harness the advantages of UAVs' high mobility, which
could provide multi-grained perspectives, while maintaining a manageable motion
space for learning, we introduce a novel task called Dual-Altitude UAV
Collaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct
altitudes: a high-altitude UAV responsible for broad environmental reasoning,
and a low-altitude UAV tasked with precise navigation. To support the training
and evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising
13,838 collaborative high-low UAV demonstration trajectories, each paired with
target-oriented language instructions. This dataset includes both unseen maps
and an unseen object validation set to systematically evaluate the model's
generalization capabilities across novel environments and unfamiliar targets.
To consolidate their complementary strengths, we propose a dual-UAV
collaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a
multimodal large language model (Pilot-LLM) for target reasoning, while the
low-altitude UAV employs a lightweight multi-stage policy for navigation and
target grounding. The two UAVs work collaboratively and only exchange minimal
coordinate information to ensure efficiency.

</details>


### [47] [Pretrained Diffusion Models Are Inherently Skipped-Step Samplers](https://arxiv.org/abs/2508.15233)
*Wenju Xu*

Main category: cs.CV

> This paper proposes a skipped-step sampling method for diffusion models to improve efficiency by bypassing intermediate denoising steps. The method is shown to achieve high-quality generation with significantly fewer sampling steps and works well with popular diffusion models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of this paper is to address the sequential generation process in diffusion models, which is time-consuming. It aims to demonstrate whether the original diffusion process can achieve high efficiency without resorting to non-Markovian processes.

**Method:** Our method introduces skipped-step sampling, a mechanism that bypasses multiple intermediate denoising steps in the iterative generation process. This is done in contrast with the traditional step-by-step refinement of standard diffusion inference but still maintains the same training objective.

**Result:** The paper demonstrates that the skipped-step sampling mechanism derived from the same training objective as the standard diffusion model performs well. The enhanced generation method, integrating skipped-step sampling with DDIM, achieves high-quality generation with significantly reduced sampling steps on popular pretrained diffusion models.

**Conclusion:** The conclusion of this paper is that by using the skipped-step sampling mechanism, diffusion models can achieve high efficiency while maintaining high-quality generation. This intrinsic property of pretrained diffusion models shows promise in accelerating diffusion model inference.

**Abstract:** Diffusion models have been achieving state-of-the-art results across various
generation tasks. However, a notable drawback is their sequential generation
process, requiring long-sequence step-by-step generation. Existing methods,
such as DDIM, attempt to reduce sampling steps by constructing a class of
non-Markovian diffusion processes that maintain the same training objective.
However, there remains a gap in understanding whether the original diffusion
process can achieve the same efficiency without resorting to non-Markovian
processes. In this paper, we provide a confirmative answer and introduce
skipped-step sampling, a mechanism that bypasses multiple intermediate
denoising steps in the iterative generation process, in contrast with the
traditional step-by-step refinement of standard diffusion inference. Crucially,
we demonstrate that this skipped-step sampling mechanism is derived from the
same training objective as the standard diffusion model, indicating that
accelerated sampling via skipped-step sampling via a Markovian way is an
intrinsic property of pretrained diffusion models. Additionally, we propose an
enhanced generation method by integrating our accelerated sampling technique
with DDIM. Extensive experiments on popular pretrained diffusion models,
including the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our
method achieves high-quality generation with significantly reduced sampling
steps.

</details>


### [48] [Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent](https://arxiv.org/abs/2508.15243)
*Yixin Gao,Xin Li,Xiaohan Pan,Runsen Feng,Bingchen Li,Yunpeng Qi,Yiting Lu,Zhengxue Cheng,Zhibo Chen,Jörn Ostermann*

Main category: cs.CV

> Comp-X是一种由大型语言模型代理驱动的智能交互式图像压缩方法，通过多模式编码框架和交互式代理克服了传统编码器的问题，实验表明其有良好的交互能力和压缩性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统图像编解码器通常存在编码模式有限、需手动选择模式的问题，限制了非专业用户的使用。为了克服这些问题，Comp-X引入了三个关键创新，以进化图像编码范式。

**Method:** Comp-X采用多功能编码框架，交互式编码代理和IIC-bench基准测试。多功能编码框架集成了多种编码模式，满足不同目标需求；交互式编码代理通过上下文学习方法，在编码专家反馈的帮助下，使大语言模型代理理解编码请求、模式选择和编码工具的使用；IIC-bench是第一个专门针对智能交互式图像压缩评估设计的基准测试，包含多样化的用户请求和编码专家的相应注释。

**Result:** 实验结果表明，Comp-X可以高效理解编码请求，并展现出良好的文本交互能力。同时，它能够在单一编码框架下维持相当的压缩性能。

**Conclusion:** Comp-X提供了一种智能交互式图像压缩理念，具有处理复杂编码请求的能力，并为人工智能在图像压缩领域的发展提供了一条有希望的途径。

**Abstract:** We present Comp-X, the first intelligently interactive image compression
paradigm empowered by the impressive reasoning capability of large language
model (LLM) agent. Notably, commonly used image codecs usually suffer from
limited coding modes and rely on manual mode selection by engineers, making
them unfriendly for unprofessional users. To overcome this, we advance the
evolution of image coding paradigm by introducing three key innovations: (i)
multi-functional coding framework, which unifies different coding modes of
various objective/requirements, including human-machine perception, variable
coding, and spatial bit allocation, into one framework. (ii) interactive coding
agent, where we propose an augmented in-context learning method with coding
expert feedback to teach the LLM agent how to understand the coding request,
mode selection, and the use of the coding tools. (iii) IIC-bench, the first
dedicated benchmark comprising diverse user requests and the corresponding
annotations from coding experts, which is systematically designed for
intelligently interactive image compression evaluation. Extensive experimental
results demonstrate that our proposed Comp-X can understand the coding requests
efficiently and achieve impressive textual interaction capability. Meanwhile,
it can maintain comparable compression performance even with a single coding
framework, providing a promising avenue for artificial general intelligence
(AGI) in image compression.

</details>


### [49] [Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images](https://arxiv.org/abs/2508.15256)
*Jinsol Song,Jiamu Wang,Anh Tien Nguyen,Keunho Byeon,Sangjeong Ahn,Sung Hak Lee,Jin Tae Kwak*

Main category: cs.CV

> 本文提出Ano-NAViLa模型，以改进病理图像中异常检测的准确性和解释性。

<details>
  <summary>Details</summary>

**Motivation:** 由于在计算病理学中用于异常检测的数据有限或缺失，现有方法无法满足要求，特别是在计算限制、复杂的组织结构和缺乏可解释性方面。

**Method:** Ano-NAViLa采用了一种基于预训练的视觉语言模型，并结合了一个轻量级的可训练MLP。该模型通过整合病理学中正常的和异常的知识，提高了在病理图像中的准确性和鲁棒性，并通过图像-文本关联增强了可解释性。

**Result:** 在两种不同的器官淋巴结数据集上的实验表明，Ano-NAViLa模型在异常检测和定位上表现出色，达到了当前最优的性能。

**Conclusion:** 评估结果显示，Ano-NAViLa在淋巴结数据集上的异常检测和定位性能达到了最先进水平，超越了竞争模型。

**Abstract:** Anomaly detection in computational pathology aims to identify rare and scarce
anomalies where disease-related data are often limited or missing. Existing
anomaly detection methods, primarily designed for industrial settings, face
limitations in pathology due to computational constraints, diverse tissue
structures, and lack of interpretability. To address these challenges, we
propose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented
Vision-Language model for Anomaly detection in pathology images. Ano-NAViLa is
built on a pre-trained vision-language model with a lightweight trainable MLP.
By incorporating both normal and abnormal pathology knowledge, Ano-NAViLa
enhances accuracy and robustness to variability in pathology images and
provides interpretability through image-text associations. Evaluated on two
lymph node datasets from different organs, Ano-NAViLa achieves the
state-of-the-art performance in anomaly detection and localization,
outperforming competing models.

</details>


### [50] [RATopo: Improving Lane Topology Reasoning via Redundancy Assignment](https://arxiv.org/abs/2508.15272)
*Han Li,Shaofei Huang,Longfei Xu,Yulu Gao,Beipeng Mu,Si Liu*

Main category: cs.CV

> RATopo提出了一种冗余分配策略，以改善车道拓扑推理的性能，特别是在车道连接和与其他交通元素的关系推理方面。

<details>
  <summary>Details</summary>

**Motivation:** 现有的车道拓扑推理方法通常采用“先检测再推理”的策略，这种方法的监督方式导致性能不佳。因此，RATopo提出了一种新的冗余分配策略，以提高拓扑推理的性能。

**Method:** RATopo采用冗余分配策略进行车道拓扑推理，重新组织了Transformer解码器，并交换了交叉注意力和自我注意力层的位置，以保留冗余的车道预测。此外，还实现了多个具有独立参数的并行交叉注意力块，进一步增强了检测车道的多样性。

**Result:** 在OpenLane-V2数据集上的广泛实验表明，RATopo策略能显著提高车道-车道和车道-交通元素拓扑推理的性能。

**Conclusion:** RATopo是一种模型无关的方法，可以轻松集成到现有的车道拓扑推理框架中，增强离散车道和几何多样性，提高车道拓扑推理性能。

**Abstract:** Lane topology reasoning plays a critical role in autonomous driving by
modeling the connections among lanes and the topological relationships between
lanes and traffic elements. Most existing methods adopt a
first-detect-then-reason paradigm, where topological relationships are
supervised based on the one-to-one assignment results obtained during the
detection stage. This supervision strategy results in suboptimal topology
reasoning performance due to the limited range of valid supervision. In this
paper, we propose RATopo, a Redundancy Assignment strategy for lane Topology
reasoning that enables quantity-rich and geometry-diverse topology supervision.
Specifically, we restructure the Transformer decoder by swapping the
cross-attention and self-attention layers. This allows redundant lane
predictions to be retained before suppression, enabling effective one-to-many
assignment. We also instantiate multiple parallel cross-attention blocks with
independent parameters, which further enhances the diversity of detected lanes.
Extensive experiments on OpenLane-V2 demonstrate that our RATopo strategy is
model-agnostic and can be seamlessly integrated into existing topology
reasoning frameworks, consistently improving both lane-lane and lane-traffic
topology performance.

</details>


### [51] [DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding](https://arxiv.org/abs/2508.15297)
*Zhu Wang,Homaira Huda Shomee,Sathya N. Ravi,Sourav Medya*

Main category: cs.CV

> 本研究提出DesignCLIP框架，通过视觉-语言模型CLIP和大型美国设计专利数据库，改进设计专利分析，涵盖图像分类和检索任务。

<details>
  <summary>Details</summary>

**Motivation:** 传统设计专利分析任务依赖图像数据，但专利图像在传达全面的视觉上下文和语义信息方面存在不足，这可能导致先前技术搜索中的不确定性。CLIP等视觉-语言模型的进步为更可靠准确的人工智能驱动的专利分析提供了机会。

**Method:** DesignCLIP框架利用CLIP模型处理设计专利分析，包括图像分类和检索。该框架利用专利图像生成的详细说明和多视角图像学习，采用类别感知分类和对比学习来处理专利数据的独特特征。

**Result:** 实验结果显示DesignCLIP在专利分类和检索任务上优于基线和当前最佳模型。

**Conclusion:** 我们的研究强调多模态方法在推动专利分析进步方面的潜力，并提供了代码库供参考。

**Abstract:** In the field of design patent analysis, traditional tasks such as patent
classification and patent image retrieval heavily depend on the image data.
However, patent images -- typically consisting of sketches with abstract and
structural elements of an invention -- often fall short in conveying
comprehensive visual context and semantic information. This inadequacy can lead
to ambiguities in evaluation during prior art searches. Recent advancements in
vision-language models, such as CLIP, offer promising opportunities for more
reliable and accurate AI-driven patent analysis. In this work, we leverage CLIP
models to develop a unified framework DesignCLIP for design patent applications
with a large-scale dataset of U.S. design patents. To address the unique
characteristics of patent data, DesignCLIP incorporates class-aware
classification and contrastive learning, utilizing generated detailed captions
for patent images and multi-views image learning. We validate the effectiveness
of DesignCLIP across various downstream tasks, including patent classification
and patent retrieval. Additionally, we explore multimodal patent retrieval,
which provides the potential to enhance creativity and innovation in design by
offering more diverse sources of inspiration. Our experiments show that
DesignCLIP consistently outperforms baseline and SOTA models in the patent
domain on all tasks. Our findings underscore the promise of multimodal
approaches in advancing patent analysis. The codebase is available here:
https://anonymous.4open.science/r/PATENTCLIP-4661/README.md.

</details>


### [52] [TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification](https://arxiv.org/abs/2508.15298)
*Darya Taratynova,Alya Almsouti,Beknur Kalmakhanbet,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

> 本文提出了一种名为Temporal Prompt Alignment (TPA)的方法，用于胎儿先天性心脏病的超声视频分类。TPA结合了基础图像文本模型、prompt感知对比学习和不确定性量化，可以提高分类精度并改善预测校准。在两个数据集上的评估显示，TPA在先天性心脏病诊断上达到了85.40%的宏观F1分数，并且大幅降低了预期校准误差。

<details>
  <summary>Details</summary>

**Motivation:** 在利用机器学习进行先天性心脏病(CHD)检测时，自动化方法可以减少对操作者的依赖，但现有方法往往忽略时间信息，局限于二元分类，并不考虑预测校准。为了改进这一点，研究提出了一种新的方法。

**Method:** Temporal Prompt Alignment (TPA)提出了一种结合基础图像文本模型和prompt感知对比学习的方法，用于对胎儿先天性心脏病的超声视频进行分类。TPA从视频子片段的每一帧中提取特征，并使用可训练的时间提取器聚合这些特征来捕捉心脏运动，通过一个边缘铰链对比损失将视频表示与类别特定的文本提示对齐。此外，引入了一个条件变分自编码器风格调制(CVAESM)模块，学习一个潜在风格向量来调制嵌入，并量化分类不确定性。

**Result:** 在私有数据集和大型公开数据集EchoNet-Dynamic上的评估显示，TPA在先天性心脏病检测中达到了85.40%的宏观F1分数，并降低了预期校准误差（ECE）和自适应ECE。在EchoNet-Dynamic的三分类任务中，TPA提升了4.73%的宏观F1分数。

**Conclusion:** Temporal Prompt Alignment (TPA)是一个用于胎儿先天性心脏病超声视频分类的框架，它整合了时间建模、prompt感知对比学习和不确定性量化。实验表明，TPA在提高分类精度的同时，也提升了预测校准的可靠性。

**Abstract:** Congenital heart defect (CHD) detection in ultrasound videos is hindered by
image noise and probe positioning variability. While automated methods can
reduce operator dependence, current machine learning approaches often neglect
temporal information, limit themselves to binary classification, and do not
account for prediction calibration. We propose Temporal Prompt Alignment (TPA),
a method leveraging foundation image-text model and prompt-aware contrastive
learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts
features from each frame of video subclips using an image encoder, aggregates
them with a trainable temporal extractor to capture heart motion, and aligns
the video representation with class-specific text prompts via a margin-hinge
contrastive loss. To enhance calibration for clinical reliability, we introduce
a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which
learns a latent style vector to modulate embeddings and quantifies
classification uncertainty. Evaluated on a private dataset for CHD detection
and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA
achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while
also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On
EchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to
58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital
heart defect (CHD) classification in ultrasound videos that integrates temporal
modeling, prompt-aware contrastive learning, and uncertainty quantification.

</details>


### [53] [BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT](https://arxiv.org/abs/2508.15299)
*Ryunosuke Hayashi,Kohei Torimi,Rokuto Nagata,Kazuma Ikeda,Ozora Sako,Taichi Nakamura,Masaki Tani,Yoshimitsu Aoki,Kentaro Yoshioka*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Real-time 3D trajectory player tracking in sports plays a crucial role in
tactical analysis, performance evaluation, and enhancing spectator experience.
Traditional systems rely on multi-camera setups, but are constrained by the
inherently two-dimensional nature of video data and the need for complex 3D
reconstruction processing, making real-time analysis challenging. Basketball,
in particular, represents one of the most difficult scenarios in the MOT field,
as ten players move rapidly and complexly within a confined court space, with
frequent occlusions caused by intense physical contact.
  To address these challenges, this paper constructs BasketLiDAR, the first
multimodal dataset in the sports MOT field that combines LiDAR point clouds
with synchronized multi-view camera footage in a professional basketball
environment, and proposes a novel MOT framework that simultaneously achieves
improved tracking accuracy and reduced computational cost. The BasketLiDAR
dataset contains a total of 4,445 frames and 3,105 player IDs, with fully
synchronized IDs between three LiDAR sensors and three multi-view cameras. We
recorded 5-on-5 and 3-on-3 game data from actual professional basketball
players, providing complete 3D positional information and ID annotations for
each player. Based on this dataset, we developed a novel MOT algorithm that
leverages LiDAR's high-precision 3D spatial information. The proposed method
consists of a real-time tracking pipeline using LiDAR alone and a multimodal
tracking pipeline that fuses LiDAR and camera data. Experimental results
demonstrate that our approach achieves real-time operation, which was difficult
with conventional camera-only methods, while achieving superior tracking
performance even under occlusion conditions. The dataset is available upon
request at: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar

</details>


### [54] [First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection](https://arxiv.org/abs/2508.15313)
*Wutao Liu,YiDan Wang,Pan Gao*

Main category: cs.CV

> Introduces RAG-SEG, a training-free, two-stage COD method that outperforms or matches SOTA methods and operates on a personal laptop.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of COD, which is hard due to the similarity between objects and their backgrounds, and reduce the reliance on heavy training and computational resources.

**Method:** First RAG, Second SEG (RAG-SEG), a training-free method that divides Camouflaged Object Detection (COD) into two stages: RAG for generating coarse prompts and SEG for refining using SAM.

**Result:** Competitive performance on COD tasks, comparable or better than state-of-the-art methods, achieved without traditional training.

**Conclusion:** RAG-SEG offers computational efficiency and practicality, demonstrated by its effectiveness when run on a personal laptop.

**Abstract:** Camouflaged object detection (COD) poses a significant challenge in computer
vision due to the high similarity between objects and their backgrounds.
Existing approaches often rely on heavy training and large computational
resources. While foundation models such as the Segment Anything Model (SAM)
offer strong generalization, they still struggle to handle COD tasks without
fine-tuning and require high-quality prompts to yield good performance.
However, generating such prompts manually is costly and inefficient. To address
these challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, a
training-free paradigm that decouples COD into two stages: Retrieval-Augmented
Generation (RAG) for generating coarse masks as prompts, followed by SAM-based
segmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval
database via unsupervised clustering, enabling fast and effective feature
retrieval. During inference, the retrieved features produce pseudo-labels that
guide precise mask generation using SAM2. Our method eliminates the need for
conventional training while maintaining competitive performance. Extensive
experiments on benchmark COD datasets demonstrate that RAG-SEG performs on par
with or surpasses state-of-the-art methods. Notably, all experiments are
conducted on a \textbf{personal laptop}, highlighting the computational
efficiency and practicality of our approach. We present further analysis in the
Appendix, covering limitations, salient object detection extension, and
possible improvements.

</details>


### [55] [VideoEraser: Concept Erasure in Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.15314)
*Naen Xu,Jinghuai Zhang,Changjiang Li,Zhi Chen,Chunyi Zhou,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.CV

> 本文提出了VideoEraser，一种训练自由的框架，旨在防止文本-视频扩散模型生成带有不需要概念的视频，并在多种任务上进行了验证，取得了优于之前方法的结果。

<details>
  <summary>Details</summary>

**Motivation:** 文本到视频扩散模型快速增长引发关于隐私、版权和安全方面的问题，因为这些模型被滥用生成有害或误导的内容。VideoEraser的设计目的是解决这些问题。

**Method:** Selective Prompt Embedding Adjustment (SPEA) 和 Adversarial-Resilient Noise Guidance (ARNG) 两阶段过程设计的VideoEraser框架，以防止T2V扩散模型生成带有不需要概念的视频，即使被明确提示这些概念也有效。

**Result:** 实验结果证明，VideoEraser在四类任务（物体擦除、艺术风格擦除、名人擦除和显式内容擦除）上实现状态一流的对不需要内容生成的抑制，将不需要内容降低46%。

**Conclusion:** 实验结果显示，VideoEraser在效能、完整性、保真度、鲁棒性和泛化性方面都优于先前方法。与基线相比，VideoEraser在四类任务中平均减少了46%的不需要内容的生成，达到最先进的性能。

**Abstract:** The rapid growth of text-to-video (T2V) diffusion models has raised concerns
about privacy, copyright, and safety due to their potential misuse in
generating harmful or misleading content. These models are often trained on
numerous datasets, including unauthorized personal identities, artistic
creations, and harmful materials, which can lead to uncontrolled production and
distribution of such content. To address this, we propose VideoEraser, a
training-free framework that prevents T2V diffusion models from generating
videos with undesirable concepts, even when explicitly prompted with those
concepts. Designed as a plug-and-play module, VideoEraser can seamlessly
integrate with representative T2V diffusion models via a two-stage process:
Selective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise
Guidance (ARNG). We conduct extensive evaluations across four tasks, including
object erasure, artistic style erasure, celebrity erasure, and explicit content
erasure. Experimental results show that VideoEraser consistently outperforms
prior methods regarding efficacy, integrity, fidelity, robustness, and
generalizability. Notably, VideoEraser achieves state-of-the-art performance in
suppressing undesirable content during T2V generation, reducing it by 46% on
average across four tasks compared to baselines.

</details>


### [56] [Predicting Road Crossing Behaviour using Pose Detection and Sequence Modelling](https://arxiv.org/abs/2508.15336)
*Subhasis Dasgupta,Preetam Saha,Agniva Roy,Jaydip Sen*

Main category: cs.CV

> The study uses deep learning techniques to predict pedestrians' intent to cross roads, finding GRU superior for accuracy and 1D CNN for speed, in support of enhancing autonomous vehicle safety.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of the study is to improve autonomous vehicle safety by enabling the prediction of pedestrians' intention to cross a road from a distance.

**Method:** The study used deep learning models for pose detection and sequence modeling for temporal predictions to predict pedestrians' intent to cross the road. Three different sequence modeling techniques were analyzed, including GRU, LSTM, and 1D CNN.

**Result:** The study found that the GRU model was superior to LSTM in predicting pedestrians' intent to cross a road, while the 1D CNN model was the fastest among the models evaluated.

**Conclusion:** The research developed an end-to-end deep learning framework integrating video analysis with various sequence models, achieving better prediction performance compared to existing methods, particularly with faster prediction times using 1D CNN models.

**Abstract:** The world is constantly moving towards AI based systems and autonomous
vehicles are now reality in different parts of the world. These vehicles
require sensors and cameras to detect objects and maneuver according to that.
It becomes important to for such vehicles to also predict from a distant if a
person is about to cross a road or not. The current study focused on predicting
the intent of crossing the road by pedestrians in an experimental setup. The
study involved working with deep learning models to predict poses and sequence
modelling for temporal predictions. The study analysed three different sequence
modelling to understand the prediction behaviour and it was found out that GRU
was better in predicting the intent compared to LSTM model but 1D CNN was the
best model in terms of speed. The study involved video analysis, and the output
of pose detection model was integrated later on to sequence modelling
techniques for an end-to-end deep learning framework for predicting road
crossing intents.

</details>


### [57] [RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features](https://arxiv.org/abs/2508.15353)
*Olga Matykina,Dmitry Yudin*

Main category: cs.CV

> RCDINO是一种基于多模态变压器的模型，该模型融合了相机和雷达的数据，用于提高三维物体检测的性能，并在nuScenes数据集上取得了优异的结果。

<details>
  <summary>Details</summary>

**Motivation:** 该工作的动机是提高基于相机和雷达的多模态数据融合的有效性，以增强自动驾驶和机器人技术中的三维物体检测。

**Method:** 本文提出了RCDINO模型，该模型基于多模态变压器，通过融合来自预训练DINOv2基础模型的语义丰富的表示来增强视觉骨干特征，从而改善三维物体检测性能。

**Result:** 在nuScenes数据集上的实验表明，RCDINO在雷达-摄像机模型中达到了最先进的性能，具体表现为56.4的NDS和48.1的mAP。

**Conclusion:** RCDINO证明了通过融合视觉和语义信息来增强三维物体检测的优越性，并且保持了与基线架构的兼容性。

**Abstract:** Three-dimensional object detection is essential for autonomous driving and
robotics, relying on effective fusion of multimodal data from cameras and
radar. This work proposes RCDINO, a multimodal transformer-based model that
enhances visual backbone features by fusing them with semantically rich
representations from the pretrained DINOv2 foundation model. This approach
enriches visual representations and improves the model's detection performance
while preserving compatibility with the baseline architecture. Experiments on
the nuScenes dataset demonstrate that RCDINO achieves state-of-the-art
performance among radar-camera models, with 56.4 NDS and 48.1 mAP. Our
implementation is available at https://github.com/OlgaMatykina/RCDINO.

</details>


### [58] [An Empirical Study on How Video-LLMs Answer Video Questions](https://arxiv.org/abs/2508.15360)
*Chenhui Gou,Ziyu Ma,Zicheng Duan,Haoyu He,Feng Chen,Akide Liu,Bohan Zhuang,Jianfei Cai,Hamid Rezatofighi*

Main category: cs.CV

> 本文通过设计三种新方法来分析Video-LLMs，并揭示了其内部工作机制，说明了视频信息提取主要在早期层发生，某些中间层对视频问答起到决定性作用，以及视频与语言之间关联的机制，研究结果为未来的Video-LLMs改进提供了解释性和效率提升的思路。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于当前大多数研究专注于提升Video-LLMs的表现，而较少关注其内部运作机制，本文旨在通过系统性实证研究填补这一空白。

**Method:** 本文通过采用注意力剔除方法，设计了视频时序剔除、视频空间剔除和语言到视频剔除三种变体，应用在不同数量的层中来研究Video-LLMs的内部机制。

**Result:** 研究表明，在全局设置下，视频信息处理分为两个阶段，前后层分别用于感知编码和抽象推理；在细粒度设置下，某些中间层对视频问题回答影响大，而其他层贡献较少；在两者设置下，空间时间建模更多依赖于语言指引的检索而不是视频内部的自我关注机制。

**Conclusion:** 论文揭示了Video-LLMs内部工作方式，并指出这些见解可用于减少Video-LLMs中的注意力计算，且本文是首次系统性地揭示这种内部处理和理解视频内容的方式。

**Abstract:** Taking advantage of large-scale data and pretrained language models, Video
Large Language Models (Video-LLMs) have shown strong capabilities in answering
video questions. However, most existing efforts focus on improving performance,
with limited attention to understanding their internal mechanisms. This paper
aims to bridge this gap through a systematic empirical study. To interpret
existing VideoLLMs, we adopt attention knockouts as our primary analytical tool
and design three variants: Video Temporal Knockout, Video Spatial Knockout, and
Language-to-Video Knockout. Then, we apply these three knockouts on different
numbers of layers (window of layers). By carefully controlling the window of
layers and types of knockouts, we provide two settings: a global setting and a
fine-grained setting. Our study reveals three key findings: (1) Global setting
indicates Video information extraction primarily occurs in early layers,
forming a clear two-stage process -- lower layers focus on perceptual encoding,
while higher layers handle abstract reasoning; (2) In the fine-grained setting,
certain intermediate layers exert an outsized impact on video question
answering, acting as critical outliers, whereas most other layers contribute
minimally; (3) In both settings, we observe that spatial-temporal modeling
relies more on language-guided retrieval than on intra- and inter-frame
self-attention among video tokens, despite the latter's high computational
cost. Finally, we demonstrate that these insights can be leveraged to reduce
attention computation in Video-LLMs. To our knowledge, this is the first work
to systematically uncover how Video-LLMs internally process and understand
video content, offering interpretability and efficiency perspectives for future
research.

</details>


### [59] [Transfer learning optimization based on evolutionary selective fine tuning](https://arxiv.org/abs/2508.15367)
*Jacinto Colan,Ana Davila,Yasuhisa Hasegawa*

Main category: cs.CV

> 本文提出了一种名为BioTune的进化适应性微调技术，用于增强迁移学习效率。通过仅微调部分相关层，BioTune减少了需要训练的参数数量，从而有可能降低计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 传统微调策略通常涉及更新所有模型参数，可能会导致过拟合和计算成本增加。本文提出了一种新的方法，以增强迁移学习的效率。

**Method:** BioTune是一种进化适应性微调技术，使用进化算法识别出需要微调的一组特定层，以优化特定目标任务上的模型性能。

**Result:** 在九个不同领域的图像分类数据集上进行了评估，BioTune在准确性和效率方面优于现有的微调方法，如AutoRGN和LoRA。

**Conclusion:** BioTune通过集中微调过程于相关层子集上，减少了可训练参数的数量，这可能会降低计算成本并促进跨多样数据特性和分布的更高效迁移学习。

**Abstract:** Deep learning has shown substantial progress in image analysis. However, the
computational demands of large, fully trained models remain a consideration.
Transfer learning offers a strategy for adapting pre-trained models to new
tasks. Traditional fine-tuning often involves updating all model parameters,
which can potentially lead to overfitting and higher computational costs. This
paper introduces BioTune, an evolutionary adaptive fine-tuning technique that
selectively fine-tunes layers to enhance transfer learning efficiency. BioTune
employs an evolutionary algorithm to identify a focused set of layers for
fine-tuning, aiming to optimize model performance on a given target task.
Evaluation across nine image classification datasets from various domains
indicates that BioTune achieves competitive or improved accuracy and efficiency
compared to existing fine-tuning methods such as AutoRGN and LoRA. By
concentrating the fine-tuning process on a subset of relevant layers, BioTune
reduces the number of trainable parameters, potentially leading to decreased
computational cost and facilitating more efficient transfer learning across
diverse data characteristics and distributions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis](https://arxiv.org/abs/2508.15189)
*Jiahao Xu,Changchang Yin,Odysseas Chatzipanagiotou,Diamantis Tsilimigras,Kevin Clear,Bingsheng Yao,Dakuo Wang,Timothy Pawlik,Ping Zhang*

Main category: cs.AI

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Surgical site infection (SSI) is one of the most common and costly
healthcare-associated infections and and surgical wound care remains a
significant clinical challenge in preventing SSIs and improving patient
outcomes. While recent studies have explored the use of deep learning for
preliminary surgical wound screening, progress has been hindered by concerns
over data privacy and the high costs associated with expert annotation.
Currently, no publicly available dataset or benchmark encompasses various types
of surgical wounds, resulting in the absence of an open-source Surgical-Wound
screening tool. To address this gap: (1) we present SurgWound, the first
open-source dataset featuring a diverse array of surgical wound types. It
contains 697 surgical wound images annotated by 3 professional surgeons with
eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce
the first benchmark for surgical wound diagnosis, which includes visual
question answering (VQA) and report generation tasks to comprehensively
evaluate model performance. (3) Furthermore, we propose a three-stage learning
framework, WoundQwen, for surgical wound diagnosis. In the first stage, we
employ five independent MLLMs to accurately predict specific surgical wound
characteristics. In the second stage, these predictions serve as additional
knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess
infection risk and guide subsequent interventions. In the third stage, we train
a MLLM that integrates the diagnostic results from the previous two stages to
produce a comprehensive report. This three-stage framework can analyze detailed
surgical wound characteristics and provide subsequent instructions to patients
based on surgical images, paving the way for personalized wound care, timely
intervention, and improved patient outcomes.

</details>
