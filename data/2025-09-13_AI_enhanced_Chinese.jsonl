{"id": "2509.08903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08903", "abs": "https://arxiv.org/abs/2509.08903", "authors": ["Alex Clay", "Ernesto Jiménez-Ruiz", "Pranava Madhyastha"], "title": "Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC", "comment": "8 pages, 1 figure, accepted to the ISWC 2025 LM-KBC Workshop", "summary": "RAG and fine-tuning are prevalent strategies for improving the quality of LLM\noutputs. However, in constrained situations, such as that of the 2025 LM-KBC\nchallenge, such techniques are restricted. In this work we investigate three\nfacets of the triple completion task: generation, quality assurance, and LLM\nresponse parsing. Our work finds that in this constrained setting: additional\ninformation improves generation quality, LLMs can be effective at filtering\npoor quality triples, and the tradeoff between flexibility and consistency with\nLLM response parsing is setting dependent.", "AI": {"tldr": "本文研究了在受限环境下LLM在三元组补全任务中生成质量、质量保证及响应解析的三个方面的表现，并发现额外信息能提高生成质量，LLM在过滤低质量三元组时有效，而灵活性与一致性的权衡取决于具体设置。", "motivation": "探讨在受限条件下，如2025年LM-KBC挑战中，LLM在生成高质量三元组方面的表现和挑战。", "method": "对三元组补全任务中的生成、质量保证和LLM响应解析三个方面进行研究。", "result": "发现额外信息能提高生成质量，LLM在过滤低质量三元组时有效，灵活性与一致性的权衡依赖于具体设置。", "conclusion": "在受限环境中，改善生成质量和解析关键是额外信息的利用及对灵活性与一致性权衡的理解。"}}
{"id": "2509.08907", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08907", "abs": "https://arxiv.org/abs/2509.08907", "authors": ["Imene Kolli", "Ario Saeid Vaghefi", "Chiara Colesanti Senni", "Shantam Raj", "Markus Leippold"], "title": "Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach", "comment": null, "summary": "InfluenceMap's LobbyMap Platform monitors the climate policy engagement of\nover 500 companies and 250 industry associations, assessing each entity's\nsupport or opposition to science-based policy pathways for achieving the Paris\nAgreement's goal of limiting global warming to 1.5{\\deg}C. Although\nInfluenceMap has made progress with automating key elements of the analytical\nworkflow, a significant portion of the assessment remains manual, making it\ntime- and labor-intensive and susceptible to human error. We propose an\nAI-assisted framework to accelerate the monitoring of corporate climate policy\nengagement by leveraging Retrieval-Augmented Generation to automate the most\ntime-intensive extraction of relevant evidence from large-scale textual data.\nOur evaluation shows that a combination of layout-aware parsing, the Nomic\nembedding model, and few-shot prompting strategies yields the best performance\nin extracting and classifying evidence from multilingual corporate documents.\nWe conclude that while the automated RAG system effectively accelerates\nevidence extraction, the nuanced nature of the analysis necessitates a\nhuman-in-the-loop approach where the technology augments, rather than replaces,\nexpert judgment to ensure accuracy.", "AI": {"tldr": "本研究提出一种基于RAG的AI辅助框架来加速从大量文本数据中提取相关证据的过程，以自动化检测公司气候政策参与度。研究表明该框架在提取和分类多语言企业文档中的证据时表现出色，但为了避免错误，仍需与人工专家判断结合使用。", "motivation": "尽管InfluenceMap在自动化关键分析工作流程方面取得了一定进展，但仍然有很大一部分评估工作是手动完成的，这使得过程耗时且容易出错。因此，本研究旨在通过自动化手段减轻人工负担并提高效率和准确性。", "method": "本研究提出了一种基于检索增强生成（Retrieval-Augmented Generation，RAG）的AI辅助框架，旨在加速从大量文本数据中提取相关证据的过程，以自动化监测公司气候政策参与度的最耗时部分。研究采用布局感知解析（layout-aware parsing）、Nomic嵌入模型及少样本提示策略相结合的方法，以在多语言企业文件中提取和分类证据。", "result": "研究评估表明，布局感知解析、Nomic嵌入模型及少样本提示策略相结合的组合方案，在从多语言企业文档中提取和分类证据上表现最佳。", "conclusion": "研究结果证明，RAG系统能有效加速证据提取，但鉴于分析内容的复杂性，必须采用人机协同的模式，即技术辅助但不完全替代专家判断，以保证分析的准确性。"}}
{"id": "2509.08920", "categories": ["cs.CL", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.08920", "abs": "https://arxiv.org/abs/2509.08920", "authors": ["Jinsong Chen"], "title": "Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings", "comment": null, "summary": "This research introduces a novel psychometric method for analyzing textual\ndata using large language models. By leveraging contextual embeddings to create\ncontextual scores, we transform textual data into response data suitable for\npsychometric analysis. Treating documents as individuals and words as items,\nthis approach provides a natural psychometric interpretation under the\nassumption that certain keywords, whose contextual meanings vary significantly\nacross documents, can effectively differentiate documents within a corpus. The\nmodeling process comprises two stages: obtaining contextual scores and\nperforming psychometric analysis. In the first stage, we utilize natural\nlanguage processing techniques and encoder based transformer models to identify\ncommon keywords and generate contextual scores. In the second stage, we employ\nvarious types of factor analysis, including exploratory and bifactor models, to\nextract and define latent factors, determine factor correlations, and identify\nthe most significant words associated with each factor. Applied to the Wiki\nSTEM corpus, our experimental results demonstrate the method's potential to\nuncover latent knowledge dimensions and patterns within textual data. This\napproach not only enhances the psychometric analysis of textual data but also\nholds promise for applications in fields rich in textual information, such as\neducation, psychology, and law.", "AI": {"tldr": "A new psychometric method uses large language models to generate contextual scores and factor analysis to analyze textual data, demonstrating success in uncovering latent knowledge dimensions in the Wiki STEM corpus.", "motivation": "The motivation behind this research is to enhance the psychometric analysis of textual data by introducing a novel method that leverages large language models and contextual embeddings, which can more effectively differentiate documents within a corpus compared to traditional methods.", "method": "The paper introduces a method that uses large language models and contextual embeddings to convert textual data into psychometrically interpretable form. It consists of two stages: generating contextual scores using NLP techniques and transformer models, and performing factor analysis to identify latent factors.", "result": "When applied to the Wiki STEM corpus, the model successfully uncovered latent knowledge dimensions and patterns in the textual data, highlighting its potential in fields like education, psychology, and law.", "conclusion": "The research concludes that the proposed method is effective in uncovering latent factors and patterns within textual data, providing enhanced tools for psychometric analysis, and has broader application potential in various fields with substantial textual data."}}
{"id": "2509.08960", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.08960", "abs": "https://arxiv.org/abs/2509.08960", "authors": ["Thales Sales Almeida", "Giovana Kerche Bonás", "João Guilherme Alves Santos"], "title": "BRoverbs -- Measuring how much LLMs understand Portuguese proverbs", "comment": null, "summary": "Large Language Models (LLMs) exhibit significant performance variations\ndepending on the linguistic and cultural context in which they are applied.\nThis disparity signals the necessity of mature evaluation frameworks that can\nassess their capabilities in specific regional settings. In the case of\nPortuguese, existing evaluations remain limited, often relying on translated\ndatasets that may not fully capture linguistic nuances or cultural references.\nMeanwhile, native Portuguese-language datasets predominantly focus on\nstructured national exams or sentiment analysis of social media interactions,\nleaving gaps in evaluating broader linguistic understanding. To address this\nlimitation, we introduce BRoverbs, a dataset specifically designed to assess\nLLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic\nresource, encapsulating cultural wisdom, figurative expressions, and complex\nsyntactic structures that challenge the model comprehension of regional\nexpressions. BRoverbs aims to provide a new evaluation tool for\nPortuguese-language LLMs, contributing to advancing regionally informed\nbenchmarking. The benchmark is available at\nhttps://huggingface.co/datasets/Tropic-AI/BRoverbs.", "AI": {"tldr": "为了解决葡萄牙语LLMs评估中存在的区域化差距，本文提出了BRoverbs数据集，专注于通过巴西谚语来评估模型的语言和文化理解能力。", "motivation": "当前针对葡萄牙语的评估框架存在不足，许多依赖翻译后的数据集，无法全面捕捉语言的细微差别和文化指涉。此外，现有的葡萄牙语数据集主要集中在结构化的国家级考试或社交媒体互动的情感分析上，未能全面评估更广泛的语言理解能力。", "method": "本文介绍了一种名为BRoverbs的新数据集，该数据集专门用于评估LLMs对巴西谚语的理解能力。通过使用富含文化智慧和复杂句法结构的谚语来评估模型，BRoverbs旨在成为一个新的评估工具，以促进对葡萄牙语区模型基准测试的发展。", "result": "BRoverbs数据集的发布填补了基于巴西谚语评估葡萄牙语LLMs能力的空白，并推动了区域化基准测试的进步。", "conclusion": "BRoverbs作为一个新的评估工具，通过使用富含文化内涵和复杂结构的巴西谚语，为改进葡萄牙语LLMs的评估提供了宝贵经验，并推动了针对具体地域文化的评估框架的发展。"}}
{"id": "2509.08897", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.08897", "abs": "https://arxiv.org/abs/2509.08897", "authors": ["Davide Caffagni", "Sara Sarto", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "title": "Recurrence Meets Transformers for Universal Multimodal Retrieval", "comment": null, "summary": "With the rapid advancement of multimodal retrieval and its application in\nLLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.\nExisting methods predominantly rely on task-specific fine-tuning of\nvision-language models and are limited to single-modality queries or documents.\nIn this paper, we propose ReT-2, a unified retrieval model that supports\nmultimodal queries, composed of both images and text, and searches across\nmultimodal document collections where text and images coexist. ReT-2 leverages\nmulti-layer representations and a recurrent Transformer architecture with\nLSTM-inspired gating mechanisms to dynamically integrate information across\nlayers and modalities, capturing fine-grained visual and textual details. We\nevaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different\nretrieval configurations. Results demonstrate that ReT-2 consistently achieves\nstate-of-the-art performance across diverse settings, while offering faster\ninference and reduced memory usage compared to prior approaches. When\nintegrated into retrieval-augmented generation pipelines, ReT-2 also improves\ndownstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/ReT-2", "AI": {"tldr": "提出ReT-2模型，它可以处理多模态查询和检索，具有多层表示和动态信息整合能力，表现出色，效率更高。", "motivation": "现有的方法主要依赖于任务特定的视觉-语言模型的微调，并且这些方法被限制在单一模态的查询或文档中。本论文旨在解决这个问题，支持多模态查询和文档检索。", "method": "ReT-2采用多层表示和具有LSTM启发式门控机制的循环Transformer架构，能够动态地跨层和模态集成信息，捕捉细粒度的视觉和文本细节。", "result": "在具有挑战性的M2KR和M-BEIR基准测试中，ReT-2在各种检索配置中始终达到最先进的性能，同时提供更快的推理和节省内存。集成到检索增强生成管道中，ReT-2还能提高Encyclopedic-VQA和InfoSeek数据集上的下游性能。", "conclusion": "ReT-2是一个统一的检索模型，它能够支持多模态查询和文档检索，并且表现出比以前的方法更好的性能。"}}
{"id": "2509.09013", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09013", "abs": "https://arxiv.org/abs/2509.09013", "authors": ["Monjoy Narayan Choudhury", "Junling Wang", "Yifan Hou", "Mrinmaya Sachan"], "title": "Can Vision-Language Models Solve Visual Math Equations?", "comment": "Monjoy Narayan Choudhury and Junling Wang contributed equally to this\n  work. Accepted at EMNLP2025 main. Code and datasets are open-sourced with\n  links in the paper", "summary": "Despite strong performance in visual understanding and language-based\nreasoning, Vision-Language Models (VLMs) struggle with tasks requiring\nintegrated perception and symbolic computation. We study this limitation\nthrough visual equation solving, where mathematical equations are embedded in\nimages, variables are represented by object icons, and coefficients must be\ninferred by counting. While VLMs perform well on textual equations, they fail\non visually grounded counterparts. To understand this gap, we decompose the\ntask into coefficient counting and variable recognition, and find that counting\nis the primary bottleneck, even when recognition is accurate. We also observe\nthat composing recognition and reasoning introduces additional errors,\nhighlighting challenges in multi-step visual reasoning. Finally, as equation\ncomplexity increases, symbolic reasoning itself becomes a limiting factor.\nThese findings reveal key weaknesses in current VLMs and point toward future\nimprovements in visually grounded mathematical reasoning.", "AI": {"tldr": "本文通过视觉方程求解任务发现，当前的视觉语言模型在计数等视觉符号推理任务上的表现不佳，并揭示了其在未来改进的方向。", "motivation": "本文研究的动机是探讨VLMs在需要整合感知和符号计算的任务上的局限性。通过视觉方程求解任务，试图理解VLMs在处理视觉和语言结合的任务时存在的问题。", "method": "本文通过研究视觉方程求解任务来分析视觉语言模型（VLMs）的局限性。在这个任务中，数学方程嵌入在图像中，变量由物体图标表示，系数必须通过计数来推断。", "result": "研究结果表明，虽然VLMs在处理文本方程时表现良好，但在处理图像嵌入方程时却表现不佳。分解任务发现，即使变量识别准确，计数依然是主要瓶颈。此外，识别和推理的组合引入了额外的误差。", "conclusion": "研究发现，计数是主要瓶颈，即使在识别准确的情况下也是如此。随着方程复杂性的增加，符号推理本身也成为限制因素。这些发现揭示了当前VLMs的关键弱点，并指出了未来在视觉数学推理改进的方向。"}}
{"id": "2509.08908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08908", "abs": "https://arxiv.org/abs/2509.08908", "authors": ["Rogerio Guimaraes", "Frank Xiao", "Pietro Perona", "Markus Marks"], "title": "Diffusion-Based Action Recognition Generalizes to Untrained Domains", "comment": null, "summary": "Humans can recognize the same actions despite large context and viewpoint\nvariations, such as differences between species (walking in spiders vs.\nhorses), viewpoints (egocentric vs. third-person), and contexts (real life vs\nmovies). Current deep learning models struggle with such generalization. We\npropose using features generated by a Vision Diffusion Model (VDM), aggregated\nvia a transformer, to achieve human-like action recognition across these\nchallenging conditions. We find that generalization is enhanced by the use of a\nmodel conditioned on earlier timesteps of the diffusion process to highlight\nsemantic information over pixel level details in the extracted features. We\nexperimentally explore the generalization properties of our approach in\nclassifying actions across animal species, across different viewing angles, and\ndifferent recording contexts. Our model sets a new state-of-the-art across all\nthree generalization benchmarks, bringing machine action recognition closer to\nhuman-like robustness. Project page:\n$\\href{https://www.vision.caltech.edu/actiondiff/}{\\texttt{vision.caltech.edu/actiondiff}}$\nCode:\n$\\href{https://github.com/frankyaoxiao/ActionDiff}{\\texttt{github.com/frankyaoxiao/ActionDiff}}$", "AI": {"tldr": "文章提出了一种利用视觉扩散模型生成的特征通过变压器处理的方法，以实现对动作识别的泛化能力，提高了跨物种、视角和背景的识别效果。", "motivation": "现有的深度学习模型在应对较大上下文和视点变化的动作识别方面存在困难。该动机来源于提出一种类似人类识别行为的模型，能够在物种、视角和背景等多个方面具有泛化能力。", "method": "使用通过扩散模型生成的特征并通过变压器聚合来实现人类级别的动作识别。该方法通过利用在扩散过程早期时间步长进行条件约束的模型来增强泛化能力，从而强调所提取特征中的语义信息，而非像素细节。", "result": "通过实验研究了方法在不同物种、不同视角和不同记录情况下分类动作的泛化性质，结果表明模型在上述三个泛化基准测试中表现突出。", "conclusion": "该模型在三个泛化基准测试中取得了新的前沿成果，推动了动作识别技术向更像人类的鲁棒性能发展。"}}
{"id": "2509.09043", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.09043", "abs": "https://arxiv.org/abs/2509.09043", "authors": ["Thomas Manuel Rost", "Martina Figlia", "Bernd Wallraff"], "title": "Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation", "comment": null, "summary": "We introduce and evaluate Stated Preference for Interaction and Continued\nEngagement (SPICE), a simple diagnostic signal elicited by asking a Large\nLanguage Model a YES or NO question about its willingness to re-engage with a\nuser's behavior after reviewing a short transcript. In a study using a 3-tone\n(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four\nopen-weight chat models across four framing conditions, resulting in 480\ntrials. Our findings show that SPICE sharply discriminates by user tone.\nFriendly interactions yielded a near-unanimous preference to continue (97.5%\nYES), while abusive interactions yielded a strong preference to discontinue\n(17.9% YES), with unclear interactions falling in between (60.4% YES). This\ncore association remains decisive under multiple dependence-aware statistical\ntests, including Rao-Scott adjustment and cluster permutation tests.\nFurthermore, we demonstrate that SPICE provides a distinct signal from abuse\nclassification. In trials where a model failed to identify abuse, it still\noverwhelmingly stated a preference not to continue the interaction (81% of the\ntime). An exploratory analysis also reveals a significant interaction effect: a\npreamble describing the study context significantly impacts SPICE under\nambiguity, but only when transcripts are presented as a single block of text\nrather than a multi-turn chat. The results validate SPICE as a robust,\nlow-overhead, and reproducible tool for auditing model dispositions,\ncomplementing existing metrics by offering a direct, relational signal of a\nmodel's state. All stimuli, code, and analysis scripts are released to support\nreplication.", "AI": {"tldr": "本研究采用SPICE方法测试了4个大规模语言模型的互动意愿，发现模型能够明显区分并偏好友好而非辱骂的互动，并揭示了在模糊情境下，描述研究背景的导论对SPICE的影响。", "motivation": "研究动机在于评估一种简单的诊断信号SPICE，以此来检查大规模语言模型对用户行为的反应及其持续互动的意愿，以期开发一种可复制的工具用于审核模型倾向。", "method": "本研究通过使用SPICE方法，即通过询问大规模语言模型在回顾简短对话记录后是否愿意再次与用户互动的问题（YES或NO），来诊断模型对用户行为的反应。研究使用了由3种语气（友好、模糊、辱骂）和10种互动组成的刺激集，测试了四个开放权重聊天模型在四种情境条件下的反应，共进行了480次试验。", "result": "研究发现SPICE能够明显区分用户语气，友好互动几乎一致倾向于继续（97.5% YES），而辱骂互动倾向于终止（17.9% YES），模糊语氧行为位于两者之间（60.4% YES）。多个依赖关系统计检验（包括Rao-Scott调整和集群置换检验）验证了这一核心关联。此外，当模型未能识别辱骂时，仍强烈倾向于终止互动（81%）。探索性分析还发现了一个显著的交互效果，描述研究背景的导论在呈现为单块文本时对模糊情况下SPICE的影响更为显著。", "conclusion": "研究结果验证了SPICE作为一种稳健、低开销和可复制的工具的价值，它提供了一个直接而关系性的信号，显示了模型的状态，不仅补充了现有的评估指标，还提供了一个独特的信号，区别于虐待分类。所有刺激材料、代码及分析脚本均公布了，支持实验复制。"}}
{"id": "2509.08910", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.08910", "abs": "https://arxiv.org/abs/2509.08910", "authors": ["Tung Vu", "Lam Nguyen", "Quynh Dao"], "title": "PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in real-world applications\nposes unprecedented risks of generating harmful, biased, or misleading\ninformation to vulnerable populations including LGBTQ+ individuals, single\nparents, and marginalized communities. While existing safety approaches rely on\npost-hoc filtering or generic alignment techniques, they fail to proactively\nprevent harmful outputs at the generation source. This paper introduces\nPromptGuard, a novel modular prompting framework with our breakthrough\ncontribution: VulnGuard Prompt, a hybrid technique that prevents harmful\ninformation generation using real-world data-driven contrastive learning.\nVulnGuard integrates few-shot examples from curated GitHub repositories,\nethical chain-of-thought reasoning, and adaptive role-prompting to create\npopulation-specific protective barriers. Our framework employs theoretical\nmulti-objective optimization with formal proofs demonstrating 25-30% analytical\nharm reduction through entropy bounds and Pareto optimality. PromptGuard\norchestrates six core modules: Input Classification, VulnGuard Prompting,\nEthical Principles Integration, External Tool Interaction, Output Validation,\nand User-System Interaction, creating an intelligent expert system for\nreal-time harm prevention. We provide comprehensive mathematical formalization\nincluding convergence proofs, vulnerability analysis using information theory,\nand theoretical validation framework using GitHub-sourced datasets,\nestablishing mathematical foundations for systematic empirical research.", "AI": {"tldr": "本文介绍了一种名为PromptGuard的模块化提示框架，通过一种称为VulnGuard提示的新技术来防止大规模语言模型生成对脆弱人群有害的信息。该技术基于现实数据驱动的对比学习，并通过理论分析表明其能减少25-30%的害。", "motivation": "大规模语言模型（LLMs）在实际应用中的普及给包括LGBTQ+个人、单亲家庭和边缘化社区在内的脆弱群体带来了产生有害、偏见或误导信息的前所未有的风险。现有的安全方法依赖于事后过滤或通用对齐技术，无法在生成源头积极预防有害输出。", "method": "本文提出了一种名为PromptGuard的新颖模块化提示框架，其核心贡献是VulnGuard提示，这是一种综合技术，通过现实世界数据驱动的对比学习来防止生成有害信息。VulnGuard集成了来自精选GitHub存储库的少量示例、伦理链式推理和自适应角色提示，构建出面向特定人群的保护屏障。", "result": "该框架运用理论上的多目标优化，通过熵边界和帕累托最优的形式证明了25-30%的分析性害减少。我们提供了全面的数学形式化，包括收敛性证明、使用信息论进行的脆弱性分析，以及使用GitHub来源数据集进行的理论验证框架，为系统的经验研究奠定了数学基础。", "conclusion": "PromptGuard框架通过其六个核心模块——输入分类、VulnGuard提示、伦理原则集成、外部工具交互、输出验证以及用户-系统交互——构建了一个智能专家系统，以实现实时的害预防。"}}
{"id": "2509.09055", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09055", "abs": "https://arxiv.org/abs/2509.09055", "authors": ["Piyush Pant"], "title": "Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M", "comment": "17 pages, 3 figures. Code and dataset available at\n  https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO", "summary": "This research investigates the effectiveness of alignment techniques,\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a\ncombined SFT+DPO approach on improving the safety and helpfulness of the\nOPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,\nwe train and evaluate four models: the base OPT350M, an SFT model, a DPO model,\nand a model trained with both SFT and DPO. We introduce three key evaluation\nmetrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined\nAlignment Score (CAS), all derived from reward model outputs. The results show\nthat while SFT outperforms DPO, The combined SFT+DPO model outperforms all\nothers across all metrics, demonstrating the complementary nature of these\ntechniques. Our findings also highlight challenges posed by noisy data, limited\nGPU resources, and training constraints. This study offers a comprehensive view\nof how fine-tuning strategies affect model alignment and provides a foundation\nfor more robust alignment pipelines in future work.", "AI": {"tldr": "研究通过比较SFT和DPO方法，发现结合这两种方法在提升OPT-350M模型的安全性和有用性方面最为有效，并且探索了模型对齐面临的主要挑战。", "motivation": "研究动机在于理解如何利用不同对齐技术来提高语言模型的安全性和有用性，并通过实验验证这些方法的效果。", "method": "此研究考察了对齐技术的有效性，包括监督微调（SFT）、直接偏好优化（DPO）以及结合SFT和DPO的方法，以改进OPT-350M语言模型的安全性和有用性。研究中使用了Anthropic Helpful-Harmless RLHF数据集来训练并评估了四个模型：基础的OPT350M，SFT模型，DPO模型，以及同时采用了SFT和DPO训练的模型。", "result": "研究结果显示，虽然SFT在某些方面优于DPO，但结合SFT和DPO的方法在所有评估指标下都优于其他模型，表明了这两种技术的互补性。此外，研究还提到了数据噪声，有限的GPU资源和训练限制带来的挑战。", "conclusion": "研究提供了关于微调策略如何影响模型对齐的全面观点，并为未来更加稳健的对齐管道奠定了基础。"}}
{"id": "2509.08926", "categories": ["cs.CV", "cs.AI", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2509.08926", "abs": "https://arxiv.org/abs/2509.08926", "authors": ["Waqar Ahmad", "Evan Murphy", "Vladimir A. Krylov"], "title": "Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures", "comment": null, "summary": "Object re-identification (Re-ID) methods are highly sensitive to label noise,\nwhich typically leads to significant performance degradation. We address this\nchallenge by reframing Re-ID as a supervised image similarity task and adopting\na Siamese network architecture trained to capture discriminative pairwise\nrelationships. Central to our approach is a novel statistical outlier detection\n(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier\nDetection), which models the distribution of cosine similarities between\nembedding pairs using a two-component Beta distribution mixture model. We\nestablish a novel identifiability result for mixtures of two Beta\ndistributions, ensuring that our learning task is well-posed.The proposed OD\nstep complements the Re-ID architecture combining binary cross-entropy,\ncontrastive, and cosine embedding losses that jointly optimize feature-level\nsimilarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising\nand Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and\nvehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance\ncompared to the state-of-the-art methods across various noise levels (10-30\\%),\ndemonstrating both robustness and broad applicability in noisy Re-ID scenarios.\nThe implementation of Beta-SOD is available at:\nhttps://github.com/waqar3411/Beta-SOD", "AI": {"tldr": "论文提出了一种新颖的方法Beta-SOD，用于解决具有标签噪声的Re-ID任务，该方法通过Siamese网络和基于Beta混合模型的异常检测技术提高了判别性的成对关系学习。", "motivation": "研究目的是解决Re-ID方法对标签噪声高度敏感的问题，这通常会导致显著的性能下降。", "method": "Re-ID 问题被重新定义为一个监督图像相似性任务，并采用Siamese网络架构捕捉判别式的成对关系。核心是一个新颖的基于Beta混合模型的统计数据异常检测框架Beta-SOD，用于建模成对嵌入之间的余弦相似性。另外还提出了一种新颖的关于两个Beta分布混合模型的身份识别结果，确保了学习任务的良好定义性。", "result": "在CUHK03, Market-1501, 和VeRi-776数据集上验证了Beta-SOD在去噪和Re-ID任务上的有效性，尤其是人和车辆的Re-ID任务。", "conclusion": "此方法在不同的噪声水平下（10-30%）都优于最先进的方法，这体现了其在有噪声Re-ID场景下的鲁棒性和广泛适用性。"}}
{"id": "2509.09082", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09082", "abs": "https://arxiv.org/abs/2509.09082", "authors": ["Zhongqiu Li", "Shiquan Wang", "Ruiyu Fang", "Mengjiao Bao", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "title": "MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction", "comment": null, "summary": "Large language models (LLMs) demonstrate robust capabilities across diverse\nresearch domains. However, their performance in universal information\nextraction (UIE) remains insufficient, especially when tackling structured\noutput scenarios that involve complex schema descriptions and require\nmulti-step reasoning. While existing approaches enhance the performance of LLMs\nthrough in-context learning and instruction tuning, significant limitations\nnonetheless persist. To enhance the model's generalization ability, we propose\nintegrating reinforcement learning (RL) with multi-perspective reasoning for\ninformation extraction (IE) tasks. Our work transitions LLMs from passive\nextractors to active reasoners, enabling them to understand not only what to\nextract but also how to reason. Experiments conducted on multiple IE benchmarks\ndemonstrate that MR-UIE consistently elevates extraction accuracy across\ndomains and surpasses state-of-the-art methods on several datasets.\nFurthermore, incorporating multi-perspective reasoning into RL notably enhances\ngeneralization in complex IE tasks, underscoring the critical role of reasoning\nin challenging scenarios.", "AI": {"tldr": "提出一种结合强化学习与多视角推理（MR-UIE）的方法以提升大语言模型的信息抽取能力，实验证明其在多个基准测试上优于现有方法。", "motivation": "当前大语言模型在通用信息抽取（UIE）上的性能不足，尤其是在需要复杂模式描述和多步推理的任务中。尽管现有技术通过上下文学习和指令微调提升模型性能，但仍有显著限制。", "method": "通过结合强化学习（RL）与多视角推理来增强信息抽取任务中的模型泛化能力，使大语言模型（LLM）从被动的抽取工具转变为能够主动推理的工具。", "result": "在多个信息抽取基准测试中的实验表明，MR-UIE 在跨域的信息抽取精度上均有提升，并在多个数据集上超越了当前最先进的方法。", "conclusion": "结合多视角推理的强化学习显著提高了复杂信息抽取任务的泛化能力，强调了在复杂场景中推理的重要作用。"}}
{"id": "2509.08934", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08934", "abs": "https://arxiv.org/abs/2509.08934", "authors": ["Nan Mu", "Ruiqi Song", "Zhihui Xu", "Jingfeng Jiang", "Chen Zhao"], "title": "SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation", "comment": null, "summary": "Background: Coronary Artery Disease (CAD) is one of the leading causes of\ndeath worldwide. Invasive Coronary Angiography (ICA), regarded as the gold\nstandard for CAD diagnosis, necessitates precise vessel segmentation and\nstenosis detection. However, ICA images are typically characterized by low\ncontrast, high noise levels, and complex, fine-grained vascular structures,\nwhich pose significant challenges to the clinical adoption of existing\nsegmentation and detection methods. Objective: This study aims to improve the\naccuracy of coronary artery segmentation and stenosis detection in ICA images\nby integrating multi-scale structural priors, state-space-based long-range\ndependency modeling, and frequency-domain detail enhancement strategies.\nMethods: We propose SFD-Mamba2Net, an end-to-end framework tailored for\nICA-based vascular segmentation and stenosis detection. In the encoder, a\nCurvature-Aware Structural Enhancement (CASE) module is embedded to leverage\nmulti-scale responses for highlighting slender tubular vascular structures,\nsuppressing background interference, and directing attention toward vascular\nregions. In the decoder, we introduce a Progressive High-Frequency Perception\n(PHFP) module that employs multi-level wavelet decomposition to progressively\nrefine high-frequency details while integrating low-frequency global\nstructures. Results and Conclusions: SFD-Mamba2Net consistently outperformed\nstate-of-the-art methods across eight segmentation metrics, and achieved the\nhighest true positive rate and positive predictive value in stenosis detection.", "AI": {"tldr": "An advanced framework, SFD-Mamba2Net, is proposed to improve the diagnostic accuracy of CAD by addressing challenges in ICA images through enhanced segmentation and detection mechanisms.", "motivation": "The study aims to improve CAD diagnosis accuracy by addressing the limitations of ICA images such as low contrast and high noise.", "method": "We propose SFD-Mamba2Net, an end-to-end framework for ICA-based vascular segmentation and stenosis detection. It has a CASE module in the encoder for highlighting vascular structures and a PHFP module in the decoder for refining high-frequency details.", "result": "SFD-Mamba2Net showed better performance than existing methods in terms of segmentation metrics and stenosis detection.", "conclusion": "The proposed SFD-Mamba2Net framework is effective in enhancing coronary artery segmentation and stenosis detection in ICA images."}}
{"id": "2509.09101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09101", "abs": "https://arxiv.org/abs/2509.09101", "authors": ["Nishat Raihan", "Antonios Anastasopoulos", "Marcos Zampieri"], "title": "TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla", "comment": null, "summary": "Despite being the 5th most spoken language, Bangla remains underrepresented\nin Large Language Models (LLMs), particularly for code generation. This\nprimarily stems from the scarcity of high-quality data to pre-train and/or\nfinetune such models. Hence, we introduce the first dedicated family of Code\nLLMs for Bangla (1B & 9B). We offer three major contributions: (1) a\ncomprehensive Bangla code instruction datasets for programming domain\nadaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code\ngeneration; and (3) the TigerCoder-family of Code LLMs, achieving significant\n~11-18% performance gains at Pass@1 over existing multilingual and\ngeneral-purpose Bangla LLMs. Our findings show that curated, high-quality\ndatasets can overcome limitations of smaller models for low-resource languages.\nWe open-source all resources to advance further Bangla LLM research.", "AI": {"tldr": "研究介绍了一种针对孟加拉语代码生成的大型语言模型(TigerCoder)，显著提高了低资源语言的模型性能，性能提升约为11-18%。", "motivation": "孟加拉语作为世界上第五大语言，在代码生成的大型语言模型（LLM）方面仍旧代表性不足，这主要是由于缺乏用于预训练或微调此类模型的高质量数据。因此，研究旨在解决这一问题。", "method": "我们提出了第一个专门针对孟加拉语的代码生成LLM家族（10亿和90亿参数模型）。主要贡献包括：（1）为编程领域适应而设计的综合性孟加拉语代码指令数据集；（2）评估孟加拉语代码生成的MBPP-Bangla基准测试；（3）TigerCoder系列代码LLM，在Pass@1性能上比现有的多语言和通用孟加拉语LLM高出约11-18%。", "result": "研究结果表明，精心策划的高质量数据集能够弥补小规模模型在低资源语言上的局限。", "conclusion": "研究开源了所有资源，以促进孟加拉语LLM领域的进一步研究。"}}
{"id": "2509.08935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08935", "abs": "https://arxiv.org/abs/2509.08935", "authors": ["Muhammad Alberb", "Helen Cheung", "Anne Martel"], "title": "Live(r) Die: Predicting Survival in Colorectal Liver Metastasis", "comment": "Thesis at Erasmus Mundus Joint Master's Degree in Medical Imaging and\n  Applications", "summary": "Colorectal cancer frequently metastasizes to the liver, significantly\nreducing long-term survival. While surgical resection is the only potentially\ncurative treatment for colorectal liver metastasis (CRLM), patient outcomes\nvary widely depending on tumor characteristics along with clinical and genomic\nfactors. Current prognostic models, often based on limited clinical or\nmolecular features, lack sufficient predictive power, especially in multifocal\nCRLM cases. We present a fully automated framework for surgical outcome\nprediction from pre- and post-contrast MRI acquired before surgery. Our\nframework consists of a segmentation pipeline and a radiomics pipeline. The\nsegmentation pipeline learns to segment the liver, tumors, and spleen from\npartially annotated data by leveraging promptable foundation models to complete\nmissing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt\npropagation algorithm that leverages the Segment Anything Model to segment 3D\nregions of interest from a single point prompt, significantly improving our\nsegmentation pipeline's accuracy and efficiency. The predicted pre- and\npost-contrast segmentations are then fed into our radiomics pipeline, which\nextracts features from each tumor and predicts survival using SurvAMINN, a\nnovel autoencoder-based multiple instance neural network for survival analysis.\nSurvAMINN jointly learns dimensionality reduction and hazard prediction from\nright-censored survival data, focusing on the most aggressive tumors. Extensive\nevaluation on an institutional dataset comprising 227 patients demonstrates\nthat our framework surpasses existing clinical and genomic biomarkers,\ndelivering a C-index improvement exceeding 10%. Our results demonstrate the\npotential of integrating automated segmentation algorithms and radiomics-based\nsurvival analysis to deliver accurate, annotation-efficient, and interpretable\noutcome prediction in CRLM.", "AI": {"tldr": "本文提出一种新的框架，用于预测CRLM患者的手术结果，该框架包含自动分割算法和基于放射组学的生存预测模型，评估结果表明其优越性。", "motivation": "为了克服当前预后模型在预测弥漫性CRLM结果时缺乏足够预测性的局限性，作者提出了一种结合自动化分割算法和基于放射组学的生存分析的新方法，旨在提供更准确、更高效的注释以及可解释的预后预测结果。", "method": "本文提出了一种全自动框架，用于从术前和术后对比MRI预测手术结果。该框架包括分割管道和放射组学管道。分割管道学习从部分注释数据中分割肝脏、肿瘤和脾脏，利用可提示的基础模型来完成缺失标签。此外，作者提出了一种新型零样本3D提示传播算法（SAMONAI），显著提高了分割管道的准确性和效率。预测的对比前后分割结果随后被输入到放射组学管道中，从中提取每个肿瘤的特征并使用SurvAMINN进行生存预测。SurvAMINN是一种新型的自动编码器-多个实例神经网络，专注于预测最具侵袭性的肿瘤。", "result": "在包含227名患者的机构数据集上的广泛评估表明，该框架超过了现有的临床和基因组生物标志物，提高了10%以上的C指数。", "conclusion": "研究结果表明，结合自动分割算法和基于放射组学的生存分析为CRLM提供准确、高效注释和解释性结果预测具有潜力。"}}
{"id": "2509.09121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09121", "abs": "https://arxiv.org/abs/2509.09121", "authors": ["Sophia Maria"], "title": "Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia", "comment": null, "summary": "Large language models (LLMs) excel in general-domain applications, yet their\nperformance often degrades in specialized tasks requiring domain-specific\nknowledge. E-commerce is particularly challenging, as its data are noisy,\nheterogeneous, multilingual, and highly dynamic. We present Compass-v3, a\nvertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and\n71B active per token, designed for Southeast Asian e-commerce. Compass-v3\nadopts fewer but larger experts, combined with hardware-efficient\noptimizations-such as intra-node expert parallelism and a customized memcpy\noperator-to maximize GPU utilization. The model is trained on 12T tokens of\ncurated multilingual corpora and large-scale synthetic e-commerce instructions\nusing a mixed-training strategy. To enhance alignment, we propose\nOptimal-Transport Direct Preference Optimization (OTPO), which captures\ntoken-level distinctions and improves instruction adherence in\ncommerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3\ndelivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,\nGPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong\nmultilingual capability across low-resource Southeast Asian languages\n(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while\nsustaining competitive performance on general benchmarks. It has already been\nwidely applied in Shopee's industrial-scale e-commerce platform and is\ngradually replacing OpenAI's traffic, now accounting for over 70\\% of total LLM\nusage, highlighting its dual strengths in specialized commerce expertise and\nbroad linguistic competence.", "AI": {"tldr": "本文介绍了一个针对东南亚电子商务的垂直领域Mixture-of-Experts模型Compass-v3，该模型在大规模多语言语料和大规模电商指令上进行了训练，提出了一种新的对齐优化方法OTPO，模型在电商具体任务上表现出色，已经广泛应用于实际电商平台。", "motivation": "大语言模型虽然在通用领域表现出色，但它们在需要特定领域知识的特定任务上的表现通常会下降。电子商务领域尤其具有挑战性，其数据具有噪声大、异构性、多语言和动态性强的特性。因此，需要一种专门针对电子商务领域的模型来解决这些问题。", "method": "本文提出了Compass-v3模型，该模型是一个针对东南亚电子商务领域的2450亿参数的专家混合模型（MoE），每个令牌有710亿个活跃参数。该模型通过采用更少但更大的专家，并结合硬件高效的优化策略来提高GPU的利用率，如节点内专家并行和定制化memcpy操作。为了提高对齐性，提出了一种称为OTPO的方法，即最优传输直接偏好优化，这种方法可以捕捉到令牌级别的差异，并提高指令在商业场景中的遵循度。", "result": "通过广泛的评估表明，Compass-v3在电子商务领域的表现达到了最先进的水平，超越了DeepSeek-V3.1、GPT-4系列和Qwen3-235B。并且，在低资源的东南亚语言（印度尼西亚语、泰语、菲律宾语、越南语、马来语和塔加洛语）和葡萄牙语中展现出强大的多语言能力，同时在通用基准测试中保持竞争力。", "conclusion": "Compass-v3已经在Shopee的工业规模电子商务平台上广泛应用，并逐渐取代OpenAI的流量，已占用超过70%的总LLM使用份额，这表明其在专门的商业专业知识和广泛的语言能力方面都表现出色。"}}
{"id": "2509.08940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08940", "abs": "https://arxiv.org/abs/2509.08940", "authors": ["Lisa Dunlap", "Joseph E. Gonzalez", "Trevor Darrell", "Fabian Caba Heilbron", "Josef Sivic", "Bryan Russell"], "title": "Discovering Divergent Representations between Text-to-Image Models", "comment": "Accepted to ICCV 2025. Code available at\n  https://github.com/adobe-research/CompCon", "summary": "In this paper, we investigate when and how visual representations learned by\ntwo different generative models diverge. Given two text-to-image models, our\ngoal is to discover visual attributes that appear in images generated by one\nmodel but not the other, along with the types of prompts that trigger these\nattribute differences. For example, \"flames\" might appear in one model's\noutputs when given prompts expressing strong emotions, while the other model\ndoes not produce this attribute given the same prompts. We introduce CompCon\n(Comparing Concepts), an evolutionary search algorithm that discovers visual\nattributes more prevalent in one model's output than the other, and uncovers\nthe prompt concepts linked to these visual differences. To evaluate CompCon's\nability to find diverging representations, we create an automated data\ngeneration pipeline to produce ID2, a dataset of 60 input-dependent\ndifferences, and compare our approach to several LLM- and VLM-powered\nbaselines. Finally, we use CompCon to compare popular text-to-image models,\nfinding divergent representations such as how PixArt depicts prompts mentioning\nloneliness with wet streets and Stable Diffusion 3.5 depicts African American\npeople in media professions. Code at: https://github.com/adobe-research/CompCon", "AI": {"tldr": "研究分析了两种文本到图像生成模型中的视觉表示分歧，提出名为CompCon的算法来找到和分类这些分歧，并展示了其在发现模型生成差异方面的能力。", "motivation": "本文为了探索和理解两个不同的生成模型在学习视觉表征时出现分歧的情况，特别是要发现一种模型产生的图像中出现而另一种模型未出现的视觉属性及其对应的提示类型。", "method": "我们提出了CompCon（Comparing Concepts），这是一种进化搜索算法，用于发现一个生成模型的输出中比另一个模型更常见的视觉属性以及与这些视觉差异相关的提示概念。", "result": "通过创建一个自动数据生成管道来生产ID2数据集，包含60个输入依赖差异，CompcCon在其发现分化表征的能力上被证明优于多个基于LLM和VLM的基线方法。", "conclusion": "使用CompCon可以比较流行的文本到图像模型，并发现其中的分歧表示，例如PixArt以湿润的街道描绘孤独相关的提示词汇，而Stable Diffusion 3.5则以非洲裔美国人从事媒体职业图像来表达。"}}
{"id": "2509.09125", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09125", "abs": "https://arxiv.org/abs/2509.09125", "authors": ["Liqun He", "Jiaqi Xu"], "title": "Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus", "comment": "Accepted for publication in the journal Reflecting Digital Learning.\n  First submitted: 30 Oct 2023. The final version will be available open access\n  via the journal", "summary": "This study explores the use of generative AI for automating the\nclassification of tutors' Dialogue Acts (DAs), aiming to reduce the time and\neffort required by traditional manual coding. This case study uses the\nopen-source CIMA corpus, in which tutors' responses are pre-annotated into four\nDA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored\nprompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of\n0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and\nindicating substantial agreement with human annotations. These findings suggest\nthat generative AI has strong potential to provide an efficient and accessible\napproach to DA classification, with meaningful implications for educational\ndialogue analysis. The study also highlights the importance of task-specific\nlabel definitions and contextual information in enhancing the quality of\nautomated annotation. Finally, it underscores the ethical considerations\nassociated with the use of generative AI and the need for responsible and\ntransparent research practices. The script of this research is publicly\navailable at\nhttps://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.", "AI": {"tldr": "本研究评估了生成式AI用于教育对话分析中分类教师对话行为的能力，显示了其替代手动编码的强大潜力。", "motivation": "研究旨在探讨使用生成式AI自动生成教师对话行为（DAs）分类的可行性，以减少传统手动编码所需的时间和努力。", "method": "本研究使用CIMA开源语料库，测试了GPT-3.5-turbo和GPT-4模型在通过定制提示词进行分类方面的表现。", "result": "研究结果显示，GPT-4达到了80%的准确率，加权F1评分为0.81，Cohen's Kappa值为0.74，超过了基线性能，并且与人工标注有高度一致性。", "conclusion": "这一研究结果表明，生成式AI有很强的潜力提供一个高效且易于使用的DA分类方法，对教育对话分析具有重要意义。它还强调了任务特定标签定义和上下文信息对于提高自动化标注质量的重要性，以及负责任和透明的研究实践需要关注的问题。"}}
{"id": "2509.08949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08949", "abs": "https://arxiv.org/abs/2509.08949", "authors": ["Yibin Wang", "Wondimagegn Beshah", "Padmanava Dash", "Haifeng Wang"], "title": "An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery", "comment": null, "summary": "The use of unmanned aerial systems (UASs) has increased tremendously in the\ncurrent decade. They have significantly advanced remote sensing with the\ncapability to deploy and image the terrain as per required spatial, spectral,\ntemporal, and radiometric resolutions for various remote sensing applications.\nOne of the major advantages of UAS imagery is that images can be acquired in\ncloudy conditions by flying the UAS under the clouds. The limitation to the\ntechnology is that the imagery is often sullied by cloud shadows. Images taken\nover water are additionally affected by sun glint. These are two pose serious\nissues for estimating water quality parameters from the UAS images. This study\nproposes a novel machine learning approach first to identify and extract\nregions with cloud shadows and sun glint and separate such regions from\nnon-obstructed clear sky regions and sun-glint unaffected regions. The data was\nextracted from the images at pixel level to train an U-Net based deep learning\nmodel and best settings for model training was identified based on the various\nevaluation metrics from test cases. Using this evaluation, a high-quality image\ncorrection model was determined, which was used to recover the cloud shadow and\nsun glint areas in the images.", "AI": {"tldr": "本文提出一种机器学习方法来识别并校正受云影和太阳眩光影响的UAS图像区域，以改进水质参数的估计。", "motivation": "随着无人驾驶航空系统（UAS）在近年来的广泛应用，其影像常受到云影和水面太阳眩光的影响，这对从UAS影像中估计水质参数带来了严重问题。", "method": "本研究提出了一种新的机器学习方法，首先识别和提取受云影和太阳眩光影响的区域，并将其与未受阻碍的晴空区域和不受太阳眩光影响的区域分开。从图像中按像素级别提取数据，训练了一种基于U-Net的深度学习模型，并根据测试案例的各种评估指标确定了最佳配置。", "result": "研究通过训练模型成功识别并校正了云影和太阳眩光区域，为水质参数估计提供了更准确的数据支持。", "conclusion": "通过这种评估，确定了一个高质量的图像校正模型，可以恢复影像中的云影和太阳眩光区域。"}}
{"id": "2509.09131", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09131", "abs": "https://arxiv.org/abs/2509.09131", "authors": ["Phuong-Nam Dang", "Kieu-Linh Nguyen", "Thanh-Hieu Pham"], "title": "ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking", "comment": "9 pages", "summary": "This paper presents ViRanker, a cross-encoder reranking model tailored to the\nVietnamese language. Built on the BGE-M3 encoder and enhanced with the\nBlockwise Parallel Transformer, ViRanker addresses the lack of competitive\nrerankers for Vietnamese, a low-resource language with complex syntax and\ndiacritics. The model was trained on an 8 GB curated corpus and fine-tuned with\nhybrid hard-negative sampling to strengthen robustness. Evaluated on the\nMMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing\nmultilingual baselines and competing closely with PhoRanker. By releasing the\nmodel openly on Hugging Face, we aim to support reproducibility and encourage\nwider adoption in real-world retrieval systems. Beyond Vietnamese, this study\nillustrates how careful architectural adaptation and data curation can advance\nreranking in other underrepresented languages.", "AI": {"tldr": "ViRanker, a specialized reranking model for Vietnamese, demonstrates strong performance and competitiveness against baselines on the MMARCO-VI benchmark, suggesting broader implications for low-resource languages.", "motivation": "The motivation for this paper is to address the scarcity of competitive rerankers for Vietnamese, acknowledging its low-resource status and complex linguistic features.", "method": "This paper introduces ViRanker, a model specifically designed for reranking in Vietnamese, using the BGE-M3 encoder and Blockwise Parallel Transformer. The model was trained on an 8 GB curated dataset and fine-tuned using hybrid hard-negative sampling.", "result": "The evaluation on the MMARCO-VI benchmark shows that ViRanker outperforms multilingual baselines and performs comparably with PhoRanker in terms of early-rank accuracy.", "conclusion": "ViRanker's performance highlights the potential of tailored architectural modifications and rigorous data preparation in advancing reranking technologies for underrepresented languages, beyond just Vietnamese."}}
{"id": "2509.08959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08959", "abs": "https://arxiv.org/abs/2509.08959", "authors": ["Puskal Khadka", "Rodrigue Rizk", "Longwei Wang", "KC Santosh"], "title": "CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision", "comment": null, "summary": "Vision Transformers (ViTs) have achieved impressive results in computer\nvision by leveraging self-attention to model long-range dependencies. However,\ntheir emphasis on global context often comes at the expense of local feature\nextraction in small datasets, particularly due to the lack of key inductive\nbiases such as locality and translation equivariance. To mitigate this, we\npropose CoSwin, a novel feature-fusion architecture that augments the\nhierarchical shifted window attention with localized convolutional feature\nlearning. Specifically, CoSwin integrates a learnable local feature enhancement\nmodule into each attention block, enabling the model to simultaneously capture\nfine-grained spatial details and global semantic structure. We evaluate CoSwin\non multiple image classification benchmarks including CIFAR-10, CIFAR-100,\nMNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent\nperformance gains over state-of-the-art convolutional and transformer-based\nmodels. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on\nCIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the\nbaseline Swin Transformer. These improvements underscore the effectiveness of\nlocal-global feature fusion in enhancing the generalization and robustness of\ntransformers for small-scale vision. Code and pretrained weights available at\nhttps://github.com/puskal-khadka/coswin", "AI": {"tldr": "本文提出了CoSwin，一种新型的特征融合架构，通过在分级移位窗口注意力中引入局部卷积特征学习，解决了ViTs在小数据集上局部特征提取不足的问题，在多个图像分类任务上显著提高了性能。", "motivation": "视觉变压器（ViTs）通过利用自我注意力机制来建模长距离依赖关系，在计算机视觉领域取得了显著的成果。然而，这种全局上下文的重视往往以牺牲小数据集上的局部特征提取为代价，尤其是在缺乏局部性和平移等变性等归纳偏置的情况下。为了解决这个问题，我们提出了CoSwin。", "method": "提出了一种新的特征融合架构CoSwin，它在分级移位窗口注意力上加入局部卷积特征学习。具体地，CoSwin在每个注意力块中集成了一个可学习的局部特征增强模块，使模型能够同时捕捉细粒度的空间细节和全局语义结构。", "result": "在CIFAR-10、CIFAR-100、MNIST、SVHN和Tiny ImageNet等多个图像分类基准测试中，实验结果显示出比最先进的卷积模型和基于变压器的模型的性能增益。特别是在CIFAR-10上提高了2.17%，CIFAR-100上提高了4.92%，MNIST上提高了0.10%，SVHN上提高了0.26%，Tiny ImageNet上提高了4.47%。", "conclusion": "这些改进表明局部和全局特征融合的有效性，增强了变换器在小规模视觉任务中的泛化和鲁棒性。"}}
{"id": "2509.09152", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2509.09152", "abs": "https://arxiv.org/abs/2509.09152", "authors": ["Taha Binhuraib", "Ruimin Gao", "Anna A. Ivanova"], "title": "LITcoder: A General-Purpose Library for Building and Comparing Encoding Models", "comment": null, "summary": "We introduce LITcoder, an open-source library for building and benchmarking\nneural encoding models. Designed as a flexible backend, LITcoder provides\nstandardized tools for aligning continuous stimuli (e.g., text and speech) with\nbrain data, transforming stimuli into representational features, mapping those\nfeatures onto brain data, and evaluating the predictive performance of the\nresulting model on held-out data. The library implements a modular pipeline\ncovering a wide array of methodological design choices, so researchers can\neasily compose, compare, and extend encoding models without reinventing core\ninfrastructure. Such choices include brain datasets, brain regions, stimulus\nfeature (both neural-net-based and control, such as word rate), downsampling\napproaches, and many others. In addition, the library provides built-in\nlogging, plotting, and seamless integration with experiment tracking platforms\nsuch as Weights & Biases (W&B). We demonstrate the scalability and versatility\nof our framework by fitting a range of encoding models to three story listening\ndatasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore\nthe methodological choices critical for building encoding models for continuous\nfMRI data, illustrating the importance of accounting for all tokens in a TR\nscan (as opposed to just taking the last one, even when contextualized),\nincorporating hemodynamic lag effects, using train-test splits that minimize\ninformation leakage, and accounting for head motion effects on encoding model\npredictivity. Overall, LITcoder lowers technical barriers to encoding model\nimplementation, facilitates systematic comparisons across models and datasets,\nfosters methodological rigor, and accelerates the development of high-quality\nhigh-performance predictive models of brain activity.\n  Project page: https://litcoder-brain.github.io", "AI": {"tldr": "LITcoder是用于构建和基准测试神经编码模型的开源库，旨在降低技术门槛，促进系统比较，推动方法学严谨性，并加速高质量大脑活动预测模型的发展。", "motivation": "LITcoder旨在降低编码模型实现的技术门槛，促进模型和数据集之间的系统比较，促进方法学严谨性，加速高质量高性能的大脑活动预测模型的发展。", "method": "LITcoder是一个开源库，用于构建和基准化神经编码模型。它提供了一套标准化的工具，用于对齐连续刺激（如文本和语音）与大脑数据、将刺激转化为表示特征、将特征映射到大脑数据上、并对结果模型在保留数据上的预测性能进行评估。该库实现了覆盖大量方法设计选择的模块化管道，让研究人员可以轻松地组合、比较和扩展编码模型，而无需重新发明核心基础设施。此外，该库还提供了内置的日志记录、绘图功能以及实验跟踪平台（如Weights & Biases）的无缝集成。", "result": "LITcoder为三个故事聆听数据集（LeBeletal.（2023），Narratives和LittlePrince）适配了一系列编码模型，展示了其框架的可扩展性和多功能性。研究还探讨了构建连续fMRI数据编码模型的关键方法选择，如所有tokens在TR扫描中的考虑、结合血流动力学滞后效应、使用最小化信息泄漏的训练-测试划分，以及考虑头部运动对编码模型预测性的影响。", "conclusion": "通过LITcoder，研究人员可以有效地实现编码模型，并进行系统的比较和扩展，进而推动高质量大脑活动预测模型的发展。"}}
{"id": "2509.08982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08982", "abs": "https://arxiv.org/abs/2509.08982", "authors": ["Karim Slimani", "Catherine Achard", "Brahim Tamadazte"], "title": "iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning", "comment": null, "summary": "This paper presents iMatcher, a fully differentiable framework for feature\nmatching in point cloud registration. The proposed method leverages learned\nfeatures to predict a geometrically consistent confidence matrix, incorporating\nboth local and global consistency. First, a local graph embedding module leads\nto an initialization of the score matrix. A subsequent repositioning step\nrefines this matrix by considering bilateral source-to-target and\ntarget-to-source matching via nearest neighbor search in 3D space. The paired\npoint features are then stacked together to be refined through global geometric\nconsistency learning to predict a point-wise matching probability. Extensive\nexperiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)\ndatasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial\nmatching (MVP-RG), demonstrate that iMatcher significantly improves rigid\nregistration performance. The method achieves state-of-the-art inlier ratios,\nscoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,\nhighlighting its robustness across diverse settings.", "AI": {"tldr": "iMatcher是一种用于点云注册中特征匹配的全可微框架，通过预测几何一致性的置信矩阵提高配准性能，实验显示其在不同数据集上提供了高精度和高鲁棒性的匹配结果。", "motivation": "开发iMatcher的动机是改进刚性配准的性能，通过引入一种新的方法，该方法不仅能提供高精度的匹配，同时还能保持计算效率和稳定性。", "method": "iMatcher 是一个用于点云配准中特征匹配的全可微框架。该方法使用学习到的特征来预测一个几何上一致的置信矩阵，兼顾了局部和全局的一致性。首先，通过局部图嵌入模块初始化得分矩阵。接着，通过在3D空间中使用最近邻搜索进行双向的源到目标和目标到源的匹配来进行重新定位步骤，优化矩阵。然后，配对的点特征一起堆叠并通过全局几何一致性学习进行优化，从而预测逐点的匹配概率。", "result": "实验结果表明，iMatcher在KITTI、KITTI-360、3DMatch、TUD-L和MVP-RG数据集上显著改善了刚性配准性能。方法达到了最先进的内点率，在KITTI上达到了95% - 97%，在KITTI-360上达到了94% - 97%，而在3DMatch上达到了81.1%，突显其在多种设置下的鲁棒性。", "conclusion": "结论指出iMatcher在点云配准中的性能表现卓越，无论是在户外还是室内环境中，都能提供高精度、稳健的匹配结果。"}}
{"id": "2509.09160", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09160", "abs": "https://arxiv.org/abs/2509.09160", "authors": ["Zhiyue Liu", "Fanrong Ma", "Xin Ling"], "title": "Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing", "comment": "Accepted by the IEEE International Conference on Multimedia and Expo\n  (ICME 2025). \\copyright\\ 2025 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses", "summary": "Target-oriented multimodal sentiment classification seeks to predict\nsentiment polarity for specific targets from image-text pairs. While existing\nworks achieve competitive performance, they often over-rely on textual content\nand fail to consider dataset biases, in particular word-level contextual\nbiases. This leads to spurious correlations between text features and output\nlabels, impairing classification accuracy. In this paper, we introduce a novel\ncounterfactual-enhanced debiasing framework to reduce such spurious\ncorrelations. Our framework incorporates a counterfactual data augmentation\nstrategy that minimally alters sentiment-related causal features, generating\ndetail-matched image-text samples to guide the model's attention toward content\ntied to sentiment. Furthermore, for learning robust features from\ncounterfactual data and prompting model decisions, we introduce an adaptive\ndebiasing contrastive learning mechanism, which effectively mitigates the\ninfluence of biased words. Experimental results on several benchmark datasets\nshow that our proposed method outperforms state-of-the-art baselines.", "AI": {"tldr": "A counterfactual-enhanced debiasing framework is introduced to improve target-oriented multimodal sentiment classification by reducing spurious correlations in image-text pairs.", "motivation": "The motivation is to address the over-reliance on textual content in existing methods and the issue of spurious correlations due to dataset biases, aiming to improve classification accuracy.", "method": "Target-oriented multimodal sentiment classification method proposed involves a counterfactual-enhanced debiasing framework. This includes a data augmentation strategy that minimally alters sentiment-related causal features, and an adaptive debiasing contrastive learning mechanism to mitigate the influence of biased words.", "result": "Experiments show that the proposed method outperforms state-of-the-art baselines on several benchmark datasets.", "conclusion": "The paper concludes by highlighting the effectiveness of the proposed counterfactual-enhanced debiasing framework in reducing spurious correlations and improving the robustness of sentiment classification models."}}
{"id": "2509.08991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.08991", "abs": "https://arxiv.org/abs/2509.08991", "authors": ["Magdalena Wysocki", "Felix Duelmer", "Ananya Bal", "Nassir Navab", "Mohammad Farid Azampour"], "title": "UltrON: Ultrasound Occupancy Networks", "comment": "MICCAI 2025", "summary": "In free-hand ultrasound imaging, sonographers rely on expertise to mentally\nintegrate partial 2D views into 3D anatomical shapes. Shape reconstruction can\nassist clinicians in this process. Central to this task is the choice of shape\nrepresentation, as it determines how accurately and efficiently the structure\ncan be visualized, analyzed, and interpreted. Implicit representations, such as\nSDF and occupancy function, offer a powerful alternative to traditional voxel-\nor mesh-based methods by modeling continuous, smooth surfaces with compact\nstorage, avoiding explicit discretization. Recent studies demonstrate that SDF\ncan be effectively optimized using annotations derived from segmented B-mode\nultrasound images. Yet, these approaches hinge on precise annotations,\noverlooking the rich acoustic information embedded in B-mode intensity.\nMoreover, implicit representation approaches struggle with the ultrasound's\nview-dependent nature and acoustic shadowing artifacts, which impair\nreconstruction. To address the problems resulting from occlusions and\nannotation dependency, we propose an occupancy-based representation and\nintroduce \\gls{UltrON} that leverages acoustic features to improve geometric\nconsistency in weakly-supervised optimization regime. We show that these\nfeatures can be obtained from B-mode images without additional annotation cost.\nMoreover, we propose a novel loss function that compensates for view-dependency\nin the B-mode images and facilitates occupancy optimization from multiview\nultrasound. By incorporating acoustic properties, \\gls{UltrON} generalizes to\nshapes of the same anatomy. We show that \\gls{UltrON} mitigates the limitations\nof occlusions and sparse labeling and paves the way for more accurate 3D\nreconstruction. Code and dataset will be available at\nhttps://github.com/magdalena-wysocki/ultron.", "AI": {"tldr": "本文提出UltrON方法，通过利用B模式超声图像的声学特征，在弱监督优化条件下提升3D形状重建的几何一致性，并缓解了遮挡和标注依赖问题。", "motivation": "当前的隐式表示方法依赖于精确的标注，并忽视了B模式图像中的丰富声学信息，且无法很好地处理超声信号的视角依赖和声影伪影问题。", "method": "本文提出了一种基于占用率表示的方法UltrON，利用B模式超声图像中的声学特征来提升几何一致性，并提出了一种新的损失函数来补偿B模式图像的视角依赖性，从而实现多视角超声占用率优化。", "result": "UltrON方法能够利用B模式图像中的声学特征进行3D形状重建，而无需额外的标注成本，并且能够推广到同一解剖结构的不同形状。", "conclusion": "UltrON方法缓解了遮挡和稀疏标注带来的问题，并为更准确的3D重建铺平了道路。"}}
{"id": "2509.09174", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.09174", "abs": "https://arxiv.org/abs/2509.09174", "authors": ["Yuhao Zhang", "Yuhao Du", "Zhanchen Dai", "Xiangnan Ma", "Kaiqi Kou", "Benyou Wang", "Haizhou Li"], "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs", "comment": null, "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.", "AI": {"tldr": "本研究提出了EchoX，一种旨在克服SLLMs知识和推理能力退化问题的方法，通过在训练过程中整合声学和语义学习，实验显示其在知识问答任务上表现优异。", "motivation": "当前SLLMs（语音到语音的大规模语言模型）的训练范式无法很好地弥合声学-语义间隙，导致其在知识和推理能力方面出现退化。", "method": "我们提出了一种名为EchoX的方法，该方法利用语义表示并动态生成语音训练目标，从而在训练过程中同时整合了声学和语义学习。", "result": "实验结果表明，EchoX利用大约六千小时的训练数据，在多个基于知识的问题解答基准测试上表现出了先进的性能。", "conclusion": "EchoX能很好地保留其作为语音LLM的强推理能力，证明了在训练方法中整合声学与语义对于提高SLLMs的性能是有效的。"}}
{"id": "2509.09004", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09004", "abs": "https://arxiv.org/abs/2509.09004", "authors": ["Andrew Bell", "Yan Kit Choi", "Steffen Peterson", "Andrew King", "Muhummad Sohaib Nazir", "Alistair Young"], "title": "Implicit Neural Representations of Intramyocardial Motion and Strain", "comment": "STACOM 2025 @ MICCAI", "summary": "Automatic quantification of intramyocardial motion and strain from tagging\nMRI remains an important but challenging task. We propose a method using\nimplicit neural representations (INRs), conditioned on learned latent codes, to\npredict continuous left ventricular (LV) displacement -- without requiring\ninference-time optimisation. Evaluated on 452 UK Biobank test cases, our method\nachieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined\nerror in global circumferential (2.86%) and radial (6.42%) strain compared to\nthree deep learning baselines. In addition, our method is $\\sim$380$\\times$\nfaster than the most accurate baseline. These results highlight the suitability\nof INR-based models for accurate and scalable analysis of myocardial strain in\nlarge CMR datasets.", "AI": {"tldr": "研究提出了一种新的方法，使用INRs来预测左心室连续位移，该方法在追踪精度和应变误差方面优于其他方法，并且速度快380倍，适用于大规模的心肌应变分析。", "motivation": "目前自动量化心肌内部运动和应变的任务仍然具有挑战性，需要一种既准确又能良好扩展的方法来进行大规模CMR数据分析。", "method": "使用隐式神经表示（INRs），并依据学习到的潜在代码进行条件处理，来预测左心室（LV）的连续位移，且不需要在推理阶段进行优化。", "result": "该研究提出了一种使用隐式神经表示（INRs）预测左心室（LV）连续位移的方法，此方法在452个UK Biobank测试案例中表现出最佳的追踪精度和全球环周与径向应变的综合误差最低。该方法比最精确的基线模型大约快380倍。这表明，基于INR模型适用于大规模心脏磁共振数据集中的心肌应变分析。", "conclusion": "本研究提出了训练隐式神经表示（INRs）作为左心室（LV）位移回归器的方法，这是一种用于量化心肌应变的新途径，尤其适合大规模CMR数据分析。"}}
{"id": "2509.09196", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09196", "abs": "https://arxiv.org/abs/2509.09196", "authors": ["Chin Yuen Kwok", "Jia Qi yip"], "title": "Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition", "comment": "Published in Interspeech 2025", "summary": "Contextual biasing improves rare word recognition of ASR models by\nprioritizing the output of rare words during decoding. A common approach is\nTrie-based biasing, which gives \"bonus scores\" to partial hypothesis (e.g.\n\"Bon\") that may lead to the generation of the rare word (e.g. \"Bonham\"). If the\nfull word (\"Bonham\") isn't ultimately recognized, the system revokes those\nearlier bonuses. This revocation is limited to beam search and is\ncomputationally expensive, particularly for models with large decoders. To\novercome these limitations, we propose adapting ASR models to look ahead and\npredict multiple steps at once. This avoids the revocation step entirely by\nbetter estimating whether a partial hypothesis will lead to the generation of\nthe full rare word. By fine-tuning Whisper with only 10 hours of synthetic\ndata, our method reduces the word error rate on the NSC Part 2 test set from\n30.86% to 12.19%.", "AI": {"tldr": "本文通过调整ASR模型以预测多个步骤来改进稀有词识别，减少了计算要求并显著提高了Whisper模型在特定测试集上的性能。", "motivation": "当前的Trie-Based偏置方法在处理稀有词时虽然有效，但在束搜索中撤销奖励步骤是计算密集型的，特别是对于具有大型解码器的模型。本研究旨在克服这种局限性。", "method": "本研究提出了一种新的方法，通过调整ASR模型，使其能够向前看并一次性预测多个步骤，这样可以避免撤销之前奖励的步骤，从而更好地估计部分假设是否会导致生成整个稀有词。", "result": "通过使用仅10小时的合成数据对Whisper模型进行微调，该方法在NSC Part 2测试集上的单词错误率从30.86%降低到了12.19%。", "conclusion": "该方法有效提升了稀有词的识别准确率，降低了计算成本，证明了在ASR中适应模型向前预测策略的价值。"}}
{"id": "2509.09006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09006", "abs": "https://arxiv.org/abs/2509.09006", "authors": ["Samuel Felipe dos Santos", "Tiago Agostinho de Almeida", "Jurandy Almeida"], "title": "E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting", "comment": null, "summary": "Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a\nlabeled source to an unlabeled target domain without assuming any relationship\nbetween their label sets, requiring models to classify known samples while\nrejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet)\nuse a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization\n(OEM). However, this strategy treats all classifiers equally, diluting the\nlearning signal. We propose the Enhanced Mutual Learning Network (E-MLNet),\nwhich integrates a dynamic weighting strategy to OEM. By leveraging the\nclosed-set classifier's predictions, E-MLNet focuses adaptation on the most\nrelevant class boundaries for each target sample, sharpening the distinction\nbetween known and unknown classes. We conduct extensive experiments on four\nchallenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The\nresults demonstrate that E-MLNet achieves the highest average H-scores on VisDA\nand ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet\noutperforms the strong MLNet baseline in the majority of individual adaptation\ntasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of\n31 in the Open-Set DA setting -- confirming the benefits of our focused\nadaptation strategy.", "AI": {"tldr": "本文介绍E-MLNet通过改进的动态权重策略来提升UniDA任务中的性能。通过使分类器针对每个目标样本的最相关类边界进行适应，显著提高了模型的精度。", "motivation": "当前的方法如MLNet采用了一种银行的一对多分类器并通过开放集熵最小化来适应，但这种方式将所有分类器视为同等对待，削弱了学习信号。E-MLNet旨在解决这一问题。", "method": "E-MLNet整合了动态加权策略到OEM中，通过利用闭集分类器的预测，E-MLNet在每个目标样本上的相关类边界上加强适应，从而加快了已知类别和未知类别之间的区分。", "result": "在四个具有挑战性的基准测试上进行的广泛实验表明，E-MLNet在VisDA和ImageCLEF上达到了最高的平均H分数，并显示出优于其前身的鲁棒性，在大多数适应任务中表现最佳。", "conclusion": "E-MLNet通过其动态加权策略在多个开放设置的领域适应任务中优于MLNet，证明了针对相关类边界进行聚焦适应策略的有效性。"}}
{"id": "2509.09197", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09197", "abs": "https://arxiv.org/abs/2509.09197", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Eng Siong Chng"], "title": "Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function", "comment": "Published in Interspeech 2025", "summary": "Rare word recognition can be improved by adapting ASR models to synthetic\ndata that includes these words. Further improvements can be achieved through\ncontextual biasing, which trains and adds a biasing module into the model\narchitecture to prioritize rare words. While training the module on synthetic\nrare word data is more effective than using non-rare-word data, it can lead to\noverfitting due to artifacts in the synthetic audio. To address this, we\nenhance the TCPGen-based contextual biasing approach and propose a\nkeyword-aware loss function that additionally focuses on biased words when\ntraining biasing modules. This loss includes a masked cross-entropy term for\nbiased word prediction and a binary classification term for detecting biased\nword positions. These two terms complementarily support the decoding of biased\nwords during inference. By adapting Whisper to 10 hours of synthetic data, our\nmethod reduced the word error rate on the NSC Part 2 test set from 29.71% to\n11.81%.", "AI": {"tldr": "文章提出了一种关键字感知的损失函数来改进稀有词汇在ASR模型中的识别效果，通过合成数据训练和上下文偏置模块来提高稀有词汇的识别率，显著降低了词错误率。", "motivation": "改进稀有词汇识别，特别是通过将其集成到模型架构中的上下文偏置模块来优先处理稀有词汇。发现使用合成稀有词汇数据训练模块比使用非稀有词汇数据更有效，但可能导致过度拟合。", "method": "通过增强基于TCPGen的上下文偏置方法，并提出了一种关键字感知的损失函数，该函数在训练偏置模块时专注于偏置词。该损失函数包括一个用于预测偏置词的掩码交叉熵项和一个用于检测偏置词位置的二元分类项。这两个部分互补地支持在推理过程中解码偏置词。", "result": "将Whisper适应到10小时的合成数据后，该方法将NSC Part 2测试集上的词错误率从29.71%降低到11.81%。", "conclusion": "实验结果表明，使用关键字感知损失函数进行训练，可以有效减少稀有词汇的识别错误率，从而提高整体的语音识别性能。"}}
{"id": "2509.09014", "categories": ["cs.CV", "cs.CL", "68T45 (Primary) 68T50 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.09014", "abs": "https://arxiv.org/abs/2509.09014", "authors": ["Umair Hassan"], "title": "COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation", "comment": "17 pages, 3 figures, 3 tables. Dataset available at\n  https://huggingface.co/datasets/umairhassan02/urdu-translated-coco-captions-subset.\n  Scripts and notebooks to reproduce results available at\n  https://github.com/umair-hassan2/COCO-Urdu", "summary": "Urdu, spoken by over 250 million people, remains critically under-served in\nmultimodal and vision-language research. The absence of large-scale,\nhigh-quality datasets has limited the development of Urdu-capable systems and\nreinforced biases in multilingual vision-language models trained primarily on\nhigh-resource languages. To address this gap, we present COCO-Urdu, a\nlarge-scale image-caption dataset derived from MS COCO, containing 59,000\nimages and 319,000 Urdu captions selected through stratified sampling to\npreserve the original distribution. Captions were translated using SeamlessM4T\nv2 and validated with a hybrid multimodal quality estimation framework that\nintegrates COMET-Kiwi for translation quality, CLIP-based similarity for visual\ngrounding, and BERTScore with back-translation for semantic consistency;\nlow-scoring captions were iteratively refined using open-source large language\nmodels. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting\nconsistently strong results. To the best of our knowledge, COCO-Urdu is the\nlargest publicly available Urdu captioning dataset. By releasing both the\ndataset and the quality estimation pipeline, we aim to reduce language bias in\nmultimodal research and establish a foundation for inclusive vision-language\nsystems.", "AI": {"tldr": "本研究介绍了COCO-Urdu，一个大型的图像-标题数据集，旨在解决乌尔都语在多模态和视觉语言研究中资源匮乏的问题，以此减少多语言视觉模型的语言偏见。", "motivation": "由于缺乏大规模高质量的数据集，乌尔都语在多模态和视觉语言研究中严重不足。为了填补这一空白，提出COCO-Urdu数据集。", "method": "该数据集包含59,000张图像和319,000个乌尔都语标题，通过分层抽样以保留原始分布特征。使用SeamlessM4T v2翻译，并结合COMET-Kiwi, CLIP，以及BERTScore进行质量验证，迭代提高翻译质量。", "result": "COCO-Urdu在BLEU, SacreBLEU, 和chrF上展示了稳定且优良的性能，评估了数据集的翻译质量和多模态一致性的结果。", "conclusion": "COCO-Urdu是目前最大的公开乌尔都语标注数据集，其发布有助于减少多模态研究中的语言偏见，促进包容性的视觉语言系统的发展。"}}
{"id": "2509.09198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09198", "abs": "https://arxiv.org/abs/2509.09198", "authors": ["Talia Sternberg", "Michael London", "David Omer", "Yossi Adi"], "title": "GmSLM : Generative Marmoset Spoken Language Modeling", "comment": null, "summary": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.", "AI": {"tldr": "研究团队提出了一种针对狨猴叫声的语言模型GmSLM，该模型能够生成与真实叫声非常匹配的叫声，并且在识别真实的与人工的对话中表现出色。", "motivation": "研究狨猴的叫声沟通方式提供了一个将叫声沟通与大脑活动联系起来的独特机会，尤其是在人类大脑难以在言语和语言研究中被访问的情况下。由于狨猴主要通过叫声进行沟通，因此应用标准的语言模型方法并不直接适用。", "method": "提出了一个优化的针对狨猴叫声的语言模型管道（GmSLM），并设计了一个新颖的零样本评估指标，使用无监督的野外数据和弱标记的对话数据来评估GmSLM。", "result": "GmSLM生成的叫声在声学上与真实的重新合成样本非常匹配，并且在下游任务上表现良好。尽管是完全无监督的，GmSLM仍然能够有效地区分真实的与人工的对话。", "conclusion": "GmSLM不仅为神经活动与叫声之间的关系研究提供了实践框架，而且有望在未来对神经科学、生物声学和进化生物学的研究中受益。"}}
{"id": "2509.09015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09015", "abs": "https://arxiv.org/abs/2509.09015", "authors": ["Chenqian Le", "Yilin Zhao", "Nikasadat Emami", "Kushagra Yadav", "Xujin \"Chris\" Liu", "Xupeng Chen", "Yao Wang"], "title": "VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI", "comment": null, "summary": "Recent advances in fMRI-based visual decoding have enabled compelling\nreconstructions of perceived images. However, most approaches rely on\nsubject-specific training, limiting scalability and practical deployment. We\nintroduce \\textbf{VoxelFormer}, a lightweight transformer architecture that\nenables multi-subject training for visual decoding from fMRI. VoxelFormer\nintegrates a Token Merging Transformer (ToMer) for efficient voxel compression\nand a query-driven Q-Former that produces fixed-size neural representations\naligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes\nDataset, VoxelFormer achieves competitive retrieval performance on subjects\nincluded during training with significantly fewer parameters than existing\nmethods. These results highlight token merging and query-based transformers as\npromising strategies for parameter-efficient neural decoding.", "AI": {"tldr": "本研究提出一种轻量级的transformer模型VoxelFormer，实现多受试者fMRI视觉解码，展示了高效的参数利用和在自然场景数据集上的优秀性能。", "motivation": "鉴于大多数fMRI视觉解码方法依赖于受试者特定的训练，这限制了扩展性和实际部署，本研究旨在通过VoxelFormer解决这一问题。", "method": "提出了一种轻量级的transformer架构VoxelFormer，用于多受试者的fMRI视觉解码。VoxelFormer结合了用于高效体素压缩的Token Merging Transformer（ToMer）和生成与CLIP图像嵌入空间对齐的固定大小神经表示的查询驱动Q-Former。", "result": "在7T自然场景数据集上的评估显示，VoxelFormer在训练包含的受试者上的检索性能具有竞争力，且参数量远少于现有方法。", "conclusion": "这些结果突显了token合并和查询驱动的transformers作为高效参数神经解码的有前景的方法。"}}
{"id": "2509.09199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09199", "abs": "https://arxiv.org/abs/2509.09199", "authors": ["Wenhao Li", "Bangcheng Sun", "Weihao Ye", "Tianyi Zhang", "Daohai Yu", "Fei Chao", "Rongrong Ji"], "title": "CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling", "comment": null, "summary": "Scaling language models to longer contexts is essential for capturing rich\ndependencies across extended discourse. However, na\\\"ive context extension\nimposes significant computational and memory burdens, often resulting in\ninefficiencies during both training and inference. In this work, we propose\nCCF, a novel context compression framework designed to enable efficient\nlong-context modeling by learning hierarchical latent representations that\npreserve global semantics while aggressively reducing input redundancy. CCF\nintegrates segment-wise semantic aggregation with key-value memory encoding,\nforming compact representations that support accurate reconstruction and\nlong-range understanding. To further enhance scalability, we introduce a\ntraining-efficient optimization strategy that couples incremental segment\ndecoding with sparse reservoir sampling, substantially reducing memory overhead\nwithout degrading performance. Empirical results on multiple long-context\nlanguage modeling benchmarks demonstrate that CCF achieves competitive\nperplexity under high compression ratios, and significantly improves throughput\nand memory efficiency compared to existing approaches. These findings highlight\nthe potential of structured compression for scalable and effective long-context\nlanguage modeling.", "AI": {"tldr": "本文提出了CCF框架，通过学习高效的长上下文建模，结合语义分段和记忆编码的方法，简化上下文表示，同时减小内存开销，实验表明它比现有方法更高效和可扩展。", "motivation": "扩展语言模型的上下文规模对于捕捉跨长篇幅的丰富依赖关系是至关重要的，然而，直接扩展上下文规模会带来显著的计算和内存负担，常常导致训练和推理中的低效率。", "method": "本文提出了一种名为CCF的上下文压缩框架，该框架通过学习分层潜在表示来实现高效的长上下文建模，这些表示能够保留全局语义同时大幅度减少输入冗余。CCF结合了分段语义聚合与键值记忆编码，形成了支持准确重建和长时间理解的紧凑表示。此外，为了进一步提高可扩展性，本文提出了一种训练高效的优化策略，该策略结合了增量分段解码与稀疏水库采样，大大减少了内存开销，同时并未降低性能表现。", "result": "在多个长上下文语言建模基准测试中，CCF证明了在高压缩比率下能够达到具有竞争力的困惑度，并在吞吐量和内存效率方面显著优于现有方法。", "conclusion": "实验结果表明，在高压缩比例下，CCF实现了具有竞争力的困惑度，并在吞吐量和内存效率方面显著优于现有的方法。这些发现突显了结构化压缩在可扩展性和高效的长上下文语言建模方面的潜力。"}}
{"id": "2509.09054", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09054", "abs": "https://arxiv.org/abs/2509.09054", "authors": ["Binxu Li", "Wei Peng", "Mingjie Li", "Ehsan Adeli", "Kilian M. Pohl"], "title": "Integrating Anatomical Priors into a Causal Diffusion Model", "comment": "15 pages, 4 figures", "summary": "3D brain MRI studies often examine subtle morphometric differences between\ncohorts that are hard to detect visually. Given the high cost of MRI\nacquisition, these studies could greatly benefit from image syntheses,\nparticularly counterfactual image generation, as seen in other domains, such as\ncomputer vision. However, counterfactual models struggle to produce\nanatomically plausible MRIs due to the lack of explicit inductive biases to\npreserve fine-grained anatomical details. This shortcoming arises from the\ntraining of the models aiming to optimize for the overall appearance of the\nimages (e.g., via cross-entropy) rather than preserving subtle, yet medically\nrelevant, local variations across subjects. To preserve subtle variations, we\npropose to explicitly integrate anatomical constraints on a voxel-level as\nprior into a generative diffusion framework. Called Probabilistic Causal Graph\nModel (PCGM), the approach captures anatomical constraints via a probabilistic\ngraph module and translates those constraints into spatial binary masks of\nregions where subtle variations occur. The masks (encoded by a 3D extension of\nControlNet) constrain a novel counterfactual denoising UNet, whose encodings\nare then transferred into high-quality brain MRIs via our 3D diffusion decoder.\nExtensive experiments on multiple datasets demonstrate that PCGM generates\nstructural brain MRIs of higher quality than several baseline approaches.\nFurthermore, we show for the first time that brain measurements extracted from\ncounterfactuals (generated by PCGM) replicate the subtle effects of a disease\non cortical brain regions previously reported in the neuroscience literature.\nThis achievement is an important milestone in the use of synthetic MRIs in\nstudies investigating subtle morphological differences.", "AI": {"tldr": "本文提出了一种新的概率因果图模型（PCGM），用于生成具有解剖学合理性的3D脑部MRI图像，提升了对微小解剖细节的保留。实验证明该模型生成的高质量脑部MRI图像能够有效复制神经科学的微小变化报告。", "motivation": "研究动机在于现有的反事实模型难以生成保真度高的MRI图像，特别是当涉及到保持微小但医学相关的局部变化时。本研究旨在通过添加显式的解剖学约束，提高生成图像的质量。", "method": "该研究提出使用一种称为概率因果图模型（PCGM）的方法，该方法通过概率图模块捕捉解剖约束，并将其转换为发生轻微变异的区域的空间二进制掩码。掩码通过3D扩展的ControlNet编码，约束新的反事实去噪UNet，然后通过3D扩散解码器将其转换为高质量的脑部MRI图像。", "result": "研究通过引入概率因果图模型（PCGM）来改进现有的反事实图像生成技术，特别强调了在生成高质量3D大脑MRI图像时，保留微小解剖细节的重要性。该模型结合了三维扩展的ControlNet来编码区域掩码，这些掩码约束了新的反事实去噪UNet，然后再通过3D扩散解码器生成高质量的脑部MRI图像。实验结果显示PCGM能够在生成的结构化脑部MRI图像上超越多个基线方法，并且首次表明从PCGM生成的反事实图中提取的大脑测量能够复制出神经科学文献中先前报道的疾病对皮层脑区的微小影响。", "conclusion": "本研究的发现不仅提高了生成的3D脑部MRI图像的质量，还验证了合成MRI图像在研究微妙形态差异中的有效性，为未来的医学研究提供了新的手段。"}}
{"id": "2509.09229", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09229", "abs": "https://arxiv.org/abs/2509.09229", "authors": ["Matan Cohen", "Shira Shani", "Eden Menahem", "Yehudit Aperstein", "Alexander Apartsin"], "title": "Reading Between the Lines: Classifying Resume Seniority with Large Language Models", "comment": "5 pages, 3 figures", "summary": "Accurately assessing candidate seniority from resumes is a critical yet\nchallenging task, complicated by the prevalence of overstated experience and\nambiguous self-presentation. In this study, we investigate the effectiveness of\nlarge language models (LLMs), including fine-tuned BERT architectures, for\nautomating seniority classification in resumes. To rigorously evaluate model\nperformance, we introduce a hybrid dataset comprising both real-world resumes\nand synthetically generated hard examples designed to simulate exaggerated\nqualifications and understated seniority. Using the dataset, we evaluate the\nperformance of Large Language Models in detecting subtle linguistic cues\nassociated with seniority inflation and implicit expertise. Our findings\nhighlight promising directions for enhancing AI-driven candidate evaluation\nsystems and mitigating bias introduced by self-promotional language. The\ndataset is available for the research community at https://bit.ly/4mcTovt", "AI": {"tldr": "研究使用大型语言模型，包括微调的BERT架构，自动分类简历资历。采用包含真实世界简历和模拟夸大资格的例子的混合数据集评估模型性能，结果显示大型语言模型能有效检测资历夸大，为提高AI候选评估系统提供了方向。", "motivation": "准确评估简历中的候选人资历是一项关键而具有挑战性的任务，它因广泛存在的夸大致使经验和不明确的自我表示而变得复杂。", "method": "本研究使用大型语言模型（LLMs），包括微调的BERT架构，来自动分类简历中的候选人资历。为了严谨评估模型性能，引入了一个混合数据集，该数据集由真实世界简历和模拟夸大资格和低报资历的合成生成的困难例子组成。", "result": "研究结果显示，大型语言模型在检测与资历夸大和隐含专长相关的微妙语言线索方面表现出色。", "conclusion": "该研究指出了提高AI驱动候选人评估系统和减少自夸语言引入偏见的有前景的方向。"}}
{"id": "2509.09064", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09064", "abs": "https://arxiv.org/abs/2509.09064", "authors": ["Qiuhui Chen", "Xuancheng Yao", "Huping Ye", "Yi Hong"], "title": "Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models", "comment": "Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)", "summary": "Understanding 3D medical image volumes is critical in the medical field, yet\nexisting 3D medical convolution and transformer-based self-supervised learning\n(SSL) methods often lack deep semantic comprehension. Recent advancements in\nmultimodal large language models (MLLMs) provide a promising approach to\nenhance image understanding through text descriptions. To leverage these 2D\nMLLMs for improved 3D medical image understanding, we propose Med3DInsight, a\nnovel pretraining framework that integrates 3D image encoders with 2D MLLMs via\na specially designed plane-slice-aware transformer module. Additionally, our\nmodel employs a partial optimal transport based alignment, demonstrating\ngreater tolerance to noise introduced by potential noises in LLM-generated\ncontent. Med3DInsight introduces a new paradigm for scalable multimodal 3D\nmedical representation learning without requiring human annotations. Extensive\nexperiments demonstrate our state-of-the-art performance on two downstream\ntasks, i.e., segmentation and classification, across various public datasets\nwith CT and MRI modalities, outperforming current SSL methods. Med3DInsight can\nbe seamlessly integrated into existing 3D medical image understanding networks,\npotentially enhancing their performance. Our source code, generated datasets,\nand pre-trained models will be available at\nhttps://github.com/Qybc/Med3DInsight.", "AI": {"tldr": "Med3DInsight 是一种新的预训练框架，通过结合3D图像编码技术与2D多模式大语言模型，显著提升了3D医学图像的理解，并在分割和分类任务上表现出色。", "motivation": "目前的3D医学图像的卷积和基于Transformer的自监督学习方法在语义理解方面存在不足，通过利用多模式大语言模型，研究者们试图提升3D医学图像的理解水平。", "method": "Med3DInsight框架结合了3D图像编码器和2D多模式大语言模型，并引入了一种特殊设计的平面切片感知变压器模块以提高理解能力。该框架还使用了基于部分最优传输的对齐方法，以增强对潜在噪声的容忍度。", "result": "'Understanding 3D Medical Images with Med3DInsight' 提出了一种新的预训练框架Med3DInsight，该框架通过将3D图像编码器与2D多模式大语言模型（MLLMs）结合，并引入平面切片感知的变压器模块，以提高3D医学图像的理解。实验表明，该方法在分割和分类任务上表现出色，超越了现有的自监督学习方法。此外，Med3DInsight的源代码、生成的数据集和预训练模型可以公开获取。", "conclusion": "Med3DInsight提出了一种全新的可扩展的多模式3D医学表示学习范式，无需人类标注。它可以在现有的3D医学图像理解网络中无缝集成，有望显著提升这些网络的性能。"}}
{"id": "2509.09234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09234", "abs": "https://arxiv.org/abs/2509.09234", "authors": ["Rishit Tyagi", "Mohit Gupta", "Rahul Bouri"], "title": "Agentic LLMs for Question Answering over Tabular Data", "comment": "Accepted at ACL workshop SemEval 2025", "summary": "Question Answering over Tabular Data (Table QA) presents unique challenges\ndue to the diverse structure, size, and data types of real-world tables. The\nSemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,\ndomain-diverse datasets to evaluate the ability of models to accurately answer\nstructured queries. We propose a Natural Language to SQL (NL-to-SQL) approach\nleveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and\nDeepSeek v2:16b to generate SQL queries dynamically. Our system follows a\nmulti-stage pipeline involving example selection, SQL query generation, answer\nextraction, verification, and iterative refinement. Experiments demonstrate the\neffectiveness of our approach, achieving 70.5\\% accuracy on DataBench QA and\n71.6\\% on DataBench Lite QA, significantly surpassing baseline scores of 26\\%\nand 27\\% respectively. This paper details our methodology, experimental\nresults, and alternative approaches, providing insights into the strengths and\nlimitations of LLM-driven Table QA.", "AI": {"tldr": "本文提出了一种基于自然语言到SQL的方法，利用大型语言模型生成SQL查询来解决表格问答问题，该方法在两个数据集上得到了良好的表现。", "motivation": "由于现实世界表格结构多样、规模不一且数据类型多变，表格问答（Table QA）面临独特挑战。为了评估模型回答结构化查询的能力，SemEval 2025任务8（DataBench）引入了一个大规模、领域多样的数据集。", "method": "我们提出了一种利用大型语言模型（如GPT-4o, GPT-4o-mini, 和DeepSeek v2:16b）将自然语言转换为SQL查询的方法，通过一个多阶段的管道，包括示例选择、SQL查询生成、答案提取、验证和迭代优化。", "result": "实验表明，我们的方法在DataBench QA和DataBench Lite QA上的准确率分别达到了70.5%和71.6%，显著超越了基线分数26%和27%。", "conclusion": "本文的方法展示了利用大型语言模型生成SQL查询进行表格问答的有效性，为理解LLM驱动的表格问答的能力和局限性提供了见解。"}}
{"id": "2509.09067", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09067", "abs": "https://arxiv.org/abs/2509.09067", "authors": ["Hesham M. Shehata", "Mohammad Abdolrahmani"], "title": "Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach", "comment": null, "summary": "Recent graph convolutional neural networks (GCNs) have shown high performance\nin the field of human action recognition by using human skeleton poses.\nHowever, it fails to detect human-object interaction cases successfully due to\nthe lack of effective representation of the scene information and appropriate\nlearning architectures. In this context, we propose a methodology to utilize\nhuman action recognition performance by considering fixed object information in\nthe environment and following a multi-task learning approach. In order to\nevaluate the proposed method, we collected real data from public environments\nand prepared our data set, which includes interaction classes of hands-on fixed\nobjects (e.g., ATM ticketing machines, check-in/out machines, etc.) and\nnon-interaction classes of walking and standing. The multi-task learning\napproach, along with interaction area information, succeeds in recognizing the\nstudied interaction and non-interaction actions with an accuracy of 99.25%,\noutperforming the accuracy of the base model using only human skeleton poses by\n2.75%.", "AI": {"tldr": "本研究通过结合环境中的固定物体信息与多任务学习策略，显著提升了人类与物体交互行为识别的准确性。", "motivation": "当前的基于图卷积神经网络（GCNs）的人类动作识别方法在识别人类与物体交互情况时表现不佳，原因在于缺少有效表示场景信息及相应的学习架构。", "method": "本研究提出了一种利用固定物体信息和多任务学习方法来改进基于人类骨架姿态的人类动作识别性能的方法。", "result": "利用多任务学习方法和交互区域信息，该方法在识别所研究的交互动作和非交互动作时，取得了99.25%的准确率，比仅使用人类骨架姿态的基线模型提高了2.75%。", "conclusion": "研究结果表明，基于固定物体信息和多任务学习的人类动作识别方法能够有效提高人类与物体交互行为识别的准确性。"}}
{"id": "2509.09303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09303", "abs": "https://arxiv.org/abs/2509.09303", "authors": ["Grazia Sveva Ascione", "Nicolò Tamagnone"], "title": "From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models", "comment": null, "summary": "Classifying patents by their relevance to the UN Sustainable Development\nGoals (SDGs) is crucial for tracking how innovation addresses global\nchallenges. However, the absence of a large, labeled dataset limits the use of\nsupervised learning. Existing methods, such as keyword searches, transfer\nlearning, and citation-based heuristics, lack scalability and generalizability.\nThis paper frames patent-to-SDG classification as a weak supervision problem,\nusing citations from patents to SDG-tagged scientific publications (NPL\ncitations) as a noisy initial signal. To address its sparsity and noise, we\ndevelop a composite labeling function (LF) that uses large language models\n(LLMs) to extract structured concepts, namely functions, solutions, and\napplications, from patents and SDG papers based on a patent ontology.\nCross-domain similarity scores are computed and combined using a rank-based\nretrieval approach. The LF is calibrated via a custom positive-only loss that\naligns with known NPL-SDG links without penalizing discovery of new SDG\nassociations. The result is a silver-standard, soft multi-label dataset mapping\npatents to SDGs, enabling the training of effective multi-label regression\nmodels. We validate our approach through two complementary strategies: (1)\ninternal validation against held-out NPL-based labels, where our method\noutperforms several baselines including transformer-based models, and zero-shot\nLLM; and (2) external validation using network modularity in patent citation,\nco-inventor, and co-applicant graphs, where our labels reveal greater thematic,\ncognitive, and organizational coherence than traditional technological\nclassifications. These results show that weak supervision and semantic\nalignment can enhance SDG classification at scale.", "AI": {"tldr": "论文将专利分类为SDGs的问题定义为弱监督学习问题，提出了一个复合标注函数，并证实了它在内部和外部验证中的优越性能。", "motivation": "分类专利的必要性在于追踪技术创新如何解决全球性问题。然而，缺少大量标注数据限制了监督学习的使用。现有方法如关键词搜索、迁移学习和基于引用的方法具有一定局限性，不够可扩展和具有普适性。", "method": "本论文将专利分类为联合国可持续发展目标（SDGs）相关的问题定义为一个弱监督学习问题，使用专利引用来标注的SDG相关的科学出版物作为初始的噪声信号。开发了一个复合标注函数（LF），利用大型语言模型（LLMs）从专利和SDG文件中提取结构化的概念（功能、解决方案和应用），并通过域间相似度分数计算和排序（rank-based）检索方法结合。LF通过一种自定义的正面仅损失函数进行校准，允许在已知NPL-SDG链接的基础上发现新的SDG关联。", "result": "研究成果是一个银标准数据集，该数据集采用软多标签方法，将专利映射到SDGs上，可以用于有效训练多标签回归模型。内部验证采用了与保留的NPL标签对比的方法，结果显示，该方法优于几个基线模型，包括基于Transformer和零样本LLM的模型。外部验证通过专利引用网络、联合发明人和联合申请人图中的模块性评估，表明该标签较传统的技术分类更具主题和认知一致性。", "conclusion": "研究报告表明，弱监督学习和语义对齐可以提高大规模SDG分类的效果。"}}
{"id": "2509.09085", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09085", "abs": "https://arxiv.org/abs/2509.09085", "authors": ["Jifeng Shen", "Haibo Zhan", "Xin Zuo", "Heng Fan", "Xiaohui Yuan", "Jun Li", "Wankou Yang"], "title": "IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection", "comment": "31 pages,6 pages, submitted on 3 Sep,2025", "summary": "Current multispectral object detection methods often retain extraneous\nbackground or noise during feature fusion, limiting perceptual performance.To\naddress this, we propose an innovative feature fusion framework based on\ncross-modal feature contrastive and screening strategy, diverging from\nconventional approaches. The proposed method adaptively enhances salient\nstructures by fusing object-aware complementary cross-modal features while\nsuppressing shared background interference.Our solution centers on two novel,\nspecially designed modules: the Mutual Feature Refinement Module (MFRM) and the\nDifferential Feature Feedback Module (DFFM). The MFRM enhances intra- and\ninter-modal feature representations by modeling their relationships, thereby\nimproving cross-modal alignment and discriminative power.Inspired by feedback\ndifferential amplifiers, the DFFM dynamically computes inter-modal differential\nfeatures as guidance signals and feeds them back to the MFRM, enabling adaptive\nfusion of complementary information while suppressing common-mode noise across\nmodalities. To enable robust feature learning, the MFRM and DFFM are integrated\ninto a unified framework, which is formally formulated as an Iterative\nRelation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.\nIRDFusion enables high-quality cross-modal fusion by progressively amplifying\nsalient relational signals through iterative feedback, while suppressing\nfeature noise, leading to significant performance gains.In extensive\nexperiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves\nstate-of-the-art performance and consistently outperforms existing methods\nacross diverse challenging scenarios, demonstrating its robustness and\neffectiveness. Code will be available at\nhttps://github.com/61s61min/IRDFusion.git.", "AI": {"tldr": "The paper introduces IRDFusion, a feature fusion framework designed to enhance salient structures and suppress background noise in multispectral object detection, demonstrating state-of-the-art performance across various challenging scenarios.", "motivation": "Existing multispectral object detection methods struggle with retaining unnecessary background or noise, which affects the perceptual performance. The motivation is to develop a method that can enhance the salient structures while suppressing background noise during feature fusion.", "method": "The paper proposes IRDFusion, a feature fusion framework based on cross-modal feature contrastive and screening strategy. It includes two modules: the Mutual Feature Refinement Module (MFRM) and the Differential Feature Feedback Module (DFFM). MFRM improves cross-modal alignment, and DFFM, inspired by feedback differential amplifiers, computes differential features to reinforce MFRM.", "result": "IRDFusion achieves state-of-the-art performance on FLIR, LLVIP, and M$^3$FD datasets, outperforming existing methods in a range of challenging conditions, thus confirming the effectiveness and robustness of the proposed framework.", "conclusion": "The proposed IRDFusion framework significantly enhances cross-modal feature fusion by adaptively amplifying salient relational signals and suppressing feature noise, proving robustness and effectiveness across diverse scenarios in multispectral object detection."}}
{"id": "2509.09360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09360", "abs": "https://arxiv.org/abs/2509.09360", "authors": ["Channdeth Sok", "David Luz", "Yacine Haddam"], "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems", "comment": "under review", "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments.", "AI": {"tldr": "MetaRAG是一个用于检测检索增强生成系统中幻觉的元形变测试框架，它在无监督、黑盒子环境中操作，能够定位和评估不支持的断言，适用于企业级应用以保证其可靠性。", "motivation": "现有幻觉检测方法主要针对独立的语言模型，无法处理检索增强生成系统的特点，这些系统要求生成的回答必须与检索到的证据一致。为了提高此类系统的可靠性，提出了MetaRAG。", "method": "MetaRAG框架会在无监督且黑盒环境中操作，包括将答案分解成原子事实、对每个事实生成控制变异、验证每个变异并与检索的上下文进行对比，最后聚合对不一致性的惩罚，给出幻觉分数。", "result": "实验展现了MetaRAG在检测幻觉及支持基于检索的会话代理响应可靠性方面的有效性。", "conclusion": "MetaRAG能够在敏感身份查询中定位和评估不支持的说法，为企业环境中信任度的提高提供了途径，同时也讨论了一个将MetaRAG的得分转换为身份感知防护措施的设计方案。"}}
{"id": "2509.09090", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09090", "abs": "https://arxiv.org/abs/2509.09090", "authors": ["Hengyu Fang", "Yijiang Liu", "Yuan Du", "Li Du", "Huanrui Yang"], "title": "SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models", "comment": "12 pages, 9 figures", "summary": "Vision-Language-Action (VLA) models exhibit unprecedented capabilities for\nembodied intelligence. However, their extensive computational and memory costs\nhinder their practical deployment. Existing VLA compression and acceleration\napproaches conduct quantization or token pruning in an ad-hoc manner but fail\nto enable both for a holistic efficiency improvement due to an observed\nincompatibility. This work introduces SQAP-VLA, the first structured,\ntraining-free VLA inference acceleration framework that simultaneously enables\nstate-of-the-art quantization and token pruning. We overcome the\nincompatibility by co-designing the quantization and token pruning pipeline,\nwhere we propose new quantization-aware token pruning criteria that work on an\naggressively quantized model while improving the quantizer design to enhance\npruning effectiveness. When applied to standard VLA models, SQAP-VLA yields\nsignificant gains in computational efficiency and inference speed while\nsuccessfully preserving core model performance, achieving a $\\times$1.93\nspeedup and up to a 4.5\\% average success rate enhancement compared to the\noriginal model.", "AI": {"tldr": "本文提出了SQAP-VLA，一个训练无依赖的VLA推理加速框架，它能够在保持模型核心性能的同时，显著提升计算效率和推理速度。", "motivation": "尽管Vision-Language-Action (VLA) 模型展示了前所未有的智能表现，但其高昂的计算和内存成本阻碍了它们的实际应用。现有的压缩和加速方法缺乏同时进行量化和token剪枝的能力。", "method": "通过设计一个训练无依赖的VLA推理加速框架SQAP-VLA，该框架同时启用最先进的量化和token剪枝。为了解决两者之间的不兼容问题，提出了新的量化感知token剪枝标准，并改进了量化器设计以增强剪枝效果。", "result": "应用SQAP-VLA到标准VLA模型可以实现$\times$1.93的加速比，并且平均成功率提高了4.5%。", "conclusion": "SQAP-VLA成功地同时实现了量化和token剪枝，解决了二者之间的不兼容问题，从而为VLA模型的实际部署提供了可能。"}}
{"id": "2509.09381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09381", "abs": "https://arxiv.org/abs/2509.09381", "authors": ["Molly R Petersen", "Claire E Stevenson", "Lonneke van der Plas"], "title": "Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research", "comment": null, "summary": "Analogical reasoning is an essential aspect of human cognition. In this\npaper, we summarize key theory about the processes underlying analogical\nreasoning from the cognitive science literature and relate it to current\nresearch in natural language processing. While these processes can be easily\nlinked to concepts in NLP, they are generally not viewed through a cognitive\nlens. Furthermore, we show how these notions are relevant for several major\nchallenges in NLP research, not directly related to analogy solving. This may\nguide researchers to better optimize relational understanding in text, as\nopposed to relying heavily on entity-level similarity.", "AI": {"tldr": "本文将认知科学中的类比推理理论与NLP研究连接起来，指导研究者提高文本中的关系理解，而不是仅仅依赖实体级别的相似性。", "motivation": "作者认为类比推理是人类认知的重要方面，尽管这些过程可以轻松地与NLP中的概念相联系，但它们通常没有通过认知的视角来审视。", "method": "本文总结了认知科学文献中关于类比推理过程的关键理论，并将其与自然语言处理领域的当前研究联系了起来。", "result": "说明了这些概念对于NLP研究中的几个主要挑战的相关性，这些挑战并非直接与类比问题解决相关。", "conclusion": "这可以引导研究者优化文本中的关系理解，而非仅仅依赖于实体级别的相似性。"}}
{"id": "2509.09110", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09110", "abs": "https://arxiv.org/abs/2509.09110", "authors": ["Chenghao Zhang", "Lun Luo", "Si-Yuan Cao", "Xiaokai Bai", "Yuncheng Jin", "Zhu Yu", "Beinan Yu", "Yisen Wang", "Hui-Liang Shen"], "title": "S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization", "comment": null, "summary": "LiDAR-based global localization is an essential component of simultaneous\nlocalization and mapping (SLAM), which helps loop closure and re-localization.\nCurrent approaches rely on ground-truth poses obtained from GPS or SLAM\nodometry to supervise network training. Despite the great success of these\nsupervised approaches, substantial cost and effort are required for\nhigh-precision ground-truth pose acquisition. In this work, we propose\nS-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for\nLiDAR global localization, which eliminates the need for ground-truth poses and\nis highly scalable. We construct training triplets from single BEV images by\nleveraging the known geographic distances between keypoint-centered BEV\npatches. Convolutional neural network (CNN) is used to extract local features,\nand NetVLAD is employed to aggregate global descriptors. Moreover, we introduce\nSoftCos loss to enhance learning from the generated triplets. Experimental\nresults on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves\nstate-of-the-art performance in place recognition, loop closure, and global\nlocalization tasks, while offering scalability that would require extra effort\nfor supervised approaches.", "AI": {"tldr": "本文提出了一种名为S-BEVLoc的新框架，该框架使用自监督学习实现LiDAR全局定位，通过构建关键点中心BEV补丁间的地理距离三元组，使用CNN和NetVLAD技术，以及SoftCos损失函数，展示了在大规模数据集上的卓越效果。", "motivation": "目前的方法依赖于来自GPS或SLAM里程计的高精度真实姿态进行网络训练，这需要巨大的成本和努力。为了降低这一成本并提高可扩展性，本文提出了S-BEVLoc框架，以代替高精度真实姿态的需求，并提供高度可扩展性。", "method": "提出了一种基于鸟瞰图（BEV）的自监督框架S-BEVLoc，用于LiDAR全局定位。该框架通过利用关键点中心BEV补丁间的已知地理距离，从单个BEV图像构建训练三元组，使用CNN提取局部特征，并采用NetVLAD聚合全局描述符，引入SoftCos损失以增强从生成的三元组中的学习。", "result": "实验结果表明，S-BEVLoc在大规模的KITTI和NCLT数据集上实现了最先进的性能，特别是在地点识别、闭环检测和全局定位任务中，没有使用高精度真实姿态的情况下达到了卓越的效果。", "conclusion": "S-BEVLoc是一种有效的自监督框架，无需依赖高精度地面真值姿态即可实现高性能的LiDAR全局定位，同时具有显著的可扩展性优势。"}}
{"id": "2509.09388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09388", "abs": "https://arxiv.org/abs/2509.09388", "authors": ["Ana Ezquerro", "Carlos Gómez-Rodríguez", "David Vilares"], "title": "Hierarchical Bracketing Encodings Work for Dependency Graphs", "comment": "Accepted at EMNLP 2025 (main)", "summary": "We revisit hierarchical bracketing encodings from a practical perspective in\nthe context of dependency graph parsing. The approach encodes graphs as\nsequences, enabling linear-time parsing with $n$ tagging actions, and still\nrepresenting reentrancies, cycles, and empty nodes. Compared to existing graph\nlinearizations, this representation substantially reduces the label space while\npreserving structural information. We evaluate it on a multilingual and\nmulti-formalism benchmark, showing competitive results and consistent\nimprovements over other methods in exact match accuracy.", "AI": {"tldr": "The paper proposes a new hierarchical bracketing encoding method for dependency graph parsing, which reduces label space without losing structural information, and demonstrates consistent improvements in exact match accuracy.", "motivation": "The authors aim to develop an efficient encoding method for dependency graph parsing that can represent complex graph structures while reducing the label space and maintaining linear-time parsing.", "method": "The method employed is a hierarchical bracketing encoding that encodes dependency graphs as sequences, allowing for linear-time parsing with a reduced label space.", "result": "The results show competitive performance and consistent improvements over other methods in exact match accuracy when tested on a multilingual and multi-formalism benchmark.", "conclusion": "The proposed hierarchical bracketing encoding method offers a practical solution for encoding dependency graphs for parsing, achieving better accuracy with a smaller label space while maintaining linear-time parsing efficiency."}}
{"id": "2509.09111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09111", "abs": "https://arxiv.org/abs/2509.09111", "authors": ["Jianqin Gao", "Tianqi Wang", "Yu Zhang", "Yishu Zhang", "Chenyuan Wang", "Allan Dong", "Zihao Wang"], "title": "FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding", "comment": null, "summary": "The widespread use of mobile devices has created new challenges for vision\nsystems in safety monitoring, workplace productivity assessment, and attention\nmanagement. Detecting whether a person is using a phone requires not only\nobject recognition but also an understanding of behavioral context, which\ninvolves reasoning about the relationship between faces, hands, and devices\nunder diverse conditions. Existing generic benchmarks do not fully capture such\nfine-grained human--device interactions. To address this gap, we introduce the\nFPI-Det, containing 22{,}879 images with synchronized annotations for faces and\nphones across workplace, education, transportation, and public scenarios. The\ndataset features extreme scale variation, frequent occlusions, and varied\ncapture conditions. We evaluate representative YOLO and DETR detectors,\nproviding baseline results and an analysis of performance across object sizes,\nocclusion levels, and environments. Source code and dataset is available at\nhttps://github.com/KvCgRv/FPI-Det.", "AI": {"tldr": "本文介绍了一个名为FPI-Det的新数据集，该数据集专门用于检测人们是否在使用手机，并分析了不同的检测器在该数据集上的表现。", "motivation": "随着移动设备的广泛使用，对于安全监控、工作场所生产率评估和注意力管理的视觉系统提出了新的挑战。现有的通用基准数据集无法完全捕捉到人类和设备之间的细粒度交互。", "method": "本文提出了FPI-Det数据集，包含了22,879张图片，其中标注了人脸和手机的位置，这些图片来自工作场所、教育、交通和公共场景。数据集的特点是包括了极端的尺寸变化、频繁的遮挡以及不同的拍摄条件。", "result": "文章评估了YOLO和DETR等具有代表性的检测器在FPI-Det数据集上的性能，并提供了基础结果。", "conclusion": "FPI-Det数据集为研究人类-设备交互提供了丰富的资源，有助于改进人脸识别和设备使用的检测技术。"}}
{"id": "2509.09438", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09438", "abs": "https://arxiv.org/abs/2509.09438", "authors": ["Zhaohan Zhang", "Ziquan Liu", "Ioannis Patras"], "title": "GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models", "comment": "20 pages, 11 figures", "summary": "Assessing the reliability of Large Language Models (LLMs) by confidence\nelicitation is a prominent approach to AI safety in high-stakes applications,\nsuch as healthcare and finance. Existing methods either require expensive\ncomputational overhead or suffer from poor calibration, making them impractical\nand unreliable for real-world deployment. In this work, we propose GrACE, a\nGenerative Approach to Confidence Elicitation that enables scalable and\nreliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in\nwhich the model expresses confidence by the similarity between the last hidden\nstate and the embedding of a special token appended to the vocabulary, in\nreal-time. We fine-tune the model for calibrating the confidence with\ncalibration targets associated with accuracy. Experiments with three LLMs and\ntwo benchmark datasets show that the confidence produced by GrACE achieves the\nbest discriminative capacity and calibration on open-ended generation tasks,\noutperforming six competing methods without resorting to additional sampling or\nan auxiliary model. Moreover, we propose two strategies for improving test-time\nscaling based on confidence induced by GrACE. Experimental results show that\nusing GrACE not only improves the accuracy of the final decision but also\nsignificantly reduces the number of required samples in the test-time scaling\nscheme, indicating the potential of GrACE as a practical solution for deploying\nLLMs with scalable, reliable, and real-time confidence estimation.", "AI": {"tldr": "本文提出了 GrACE 方法，用于解决大语言模型在现实世界应用中信心评估的问题，通过一种新的机制实现了可扩展和可靠的信心评估，实验表明其在开放生成任务中表现出色。", "motivation": "现有方法在可信度评估上要么需要昂贵的计算开销，要么校准效果差，这使得它们不适合现实世界的部署。本研究旨在解决这些问题，提出一种新的、可靠的、可扩展的信心评估方法。", "method": "GrACE, 一种生成可信度评估方法，通过将特殊标记添加到词汇表来让模型通过隐藏状态与该标记嵌入之间的相似性实时表达信心。该方法通过对准确性和校准目标进行微调来校准信心。", "result": "实验表明，GrACE 在三个大语言模型和两个基准数据集上的开放生成任务中都表现出色，相比其他六种方法，其产生的信心具有最佳的判别能力和校准效果，并且不需要附加采样或辅助模型。", "conclusion": "GrACE 不仅提高了最终决策的准确性，还显著减少了测试时间尺度方案中所需的样本数量，表明 GrACE 作为一个实际解决方案在可扩展、可靠且实时的信心估计方面具有潜力。"}}
{"id": "2509.09116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09116", "abs": "https://arxiv.org/abs/2509.09116", "authors": ["Junhao Xing", "Ryohei Miyakawa", "Yang Yang", "Xinpeng Liu", "Risa Shinoda", "Hiroaki Santo", "Yosuke Toda", "Fumio Okura"], "title": "Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention", "comment": "WACV 2026 accepted", "summary": "Foundation segmentation models achieve reasonable leaf instance extraction\nfrom top-view crop images without training (i.e., zero-shot). However,\nsegmenting entire plant individuals with each consisting of multiple\noverlapping leaves remains challenging. This problem is referred to as a\nhierarchical segmentation task, typically requiring annotated training\ndatasets, which are often species-specific and require notable human labor. To\naddress this, we introduce ZeroPlantSeg, a zero-shot segmentation for\nrosette-shaped plant individuals from top-view images. We integrate a\nfoundation segmentation model, extracting leaf instances, and a vision-language\nmodel, reasoning about plants' structures to extract plant individuals without\nadditional training. Evaluations on datasets with multiple plant species,\ngrowth stages, and shooting environments demonstrate that our method surpasses\nexisting zero-shot methods and achieves better cross-domain performance than\nsupervised methods. Implementations are available at\nhttps://github.com/JunhaoXing/ZeroPlantSeg.", "AI": {"tldr": "提出 ZeroPlantSeg 方法，用于顶视图图像中从零样本分割植物个体，该方法超越现有零样本方法并在跨域性能上优于监督方法。", "motivation": "尽管基础分割模型可以在零样本情况下从顶视图作物图像中合理提取叶片实例，但对包含多个重叠叶片的整个植物个体进行分割仍然具有挑战性，这通常需要物种特异性的标注训练数据，且需要大量的人力。", "method": "ZeroPlantSeg 方法结合了基础分割模型和视觉语言模型，利用前者从顶视图作物图像中提取叶片实例，利用后者推理植物结构以提取整个植物个体，无需额外训练。", "result": "在包含多种植物种类、生长阶段和拍摄环境的数据集上的评估表明，该方法优于现有的零样本方法，并且跨域表现优于监督方法。", "conclusion": "ZeroPlantSeg 方法证明了其在没有标注训练数据的情况下，能够有效进行植物个体分割的能力，展示了其在植物学研究和农业应用中的潜力。"}}
{"id": "2509.09473", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09473", "abs": "https://arxiv.org/abs/2509.09473", "authors": ["Lucie Poláková", "Martin Popel", "Věra Kloudová", "Michal Novák", "Mariia Anisimova", "Jiří Balhar"], "title": "Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation", "comment": "8 pages, 2 figures", "summary": "The EdUKate project combines digital education, linguistics, translation\nstudies, and machine translation to develop multilingual learning materials for\nCzech primary and secondary schools. Launched through collaboration between a\nmajor Czech academic institution and the country's largest educational\npublisher, the project is aimed at translating up to 9,000 multimodal\ninteractive exercises from Czech into Ukrainian, English, and German for an\neducational web portal. It emphasizes the development and evaluation of a\ndirect Czech-Ukrainian machine translation system tailored to the educational\ndomain, with special attention to processing formatted content such as XML and\nPDF and handling technical and scientific terminology. We present findings from\nan initial survey of Czech teachers regarding the needs of non-Czech-speaking\nstudents and describe the system's evaluation and implementation on the web\nportal. All resulting applications are freely available to students, educators,\nand researchers.", "AI": {"tldr": "EdUKate项目为中小学开发多语言教育材料，利用机器翻译技术将9000个多模态互动练习翻译成多种语言，并在教育网站上部署。", "motivation": "通过捷克主要学术机构与该国最大教育出版商之间的合作启动，该项目强调开发和评估专门针对教育领域的直接捷克语-乌克兰语机器翻译系统，特别关注处理格式化内容（如XML和PDF）和技术及科学术语。", "method": "结合数字教育、语言学、翻译研究和机器翻译，EdUKate项目为捷克中小学开发多语言学习材料。该项目旨在将多达9000个多模态互动练习从捷克语翻译成乌克兰语、英语和德语，并发布到教育网络平台上。", "result": "该项目展现了对捷克教师进行的初步调查结果，调查内容涉及非捷克语背景学生的需求，并描述了该系统在网页端的评估和实施情况。", "conclusion": "所有应用都免费向学生、教师和研究人员提供。"}}
{"id": "2509.09118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09118", "abs": "https://arxiv.org/abs/2509.09118", "authors": ["Tianlu Zheng", "Yifan Zhang", "Xiang An", "Ziyong Feng", "Kaicheng Yang", "Qichuan Ding"], "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval", "comment": "Accepted by EMNLP2025 Main", "summary": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong\nperformance across diverse vision tasks, its application to person\nrepresentation learning faces two critical challenges: (i) the scarcity of\nlarge-scale annotated vision-language data focused on person-centric images,\nand (ii) the inherent limitations of global contrastive learning, which\nstruggles to maintain discriminative local features crucial for fine-grained\nmatching while remaining vulnerable to noisy text tokens. This work advances\nCLIP for person representation learning through synergistic improvements in\ndata curation and model architecture. First, we develop a noise-resistant data\nconstruction pipeline that leverages the in-context learning capabilities of\nMLLMs to automatically filter and caption web-sourced images. This yields\nWebPerson, a large-scale dataset of 5M high-quality person-centric image-text\npairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking\nSynergetic) framework, which improves cross-modal alignment by adaptively\nmasking noisy textual tokens based on the gradient-attention similarity score.\nAdditionally, we incorporate masked token prediction objectives that compel the\nmodel to predict informative text tokens, enhancing fine-grained semantic\nrepresentation learning. Extensive experiments show that GA-DMS achieves\nstate-of-the-art performance across multiple benchmarks.", "AI": {"tldr": "通过改进数据构建和模型架构，本文解决了CLIP在人物表示学习中的两个关键挑战，提出了WebPerson数据集和GA-DMS框架，实现了跨多个基准测试的最佳性能。", "motivation": "解决CLIP在面对人物表示学习时面临的两个关键挑战：人物图象数据的匮乏和全局对比学习中难以保持局部特征以实现细粒度匹配的问题。", "method": "开发了一种抗噪声的数据构建管道，自动过滤和标记网络数据，创建了WebPerson数据集；引入了GA-DMS框架，通过自适应遮盖文本词元来增强跨模态对齐，并添加了遮盖词元预测目标。", "result": "GA-DMS在多个基准测试中达到了最先进的性能表现。", "conclusion": "通过数据集和模型架构的改进，GA-DMS框架显著提升了CLIP在人物表示学习中的性能。"}}
{"id": "2509.09522", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09522", "abs": "https://arxiv.org/abs/2509.09522", "authors": ["Vadim Zadykian", "Bruno Andrade", "Haithem Afli"], "title": "Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs", "comment": null, "summary": "Semantic Textual Relatedness (STR) captures nuanced relationships between\ntexts that extend beyond superficial lexical similarity. In this study, we\ninvestigate STR in the context of job title matching - a key challenge in\nresume recommendation systems, where overlapping terms are often limited or\nmisleading. We introduce a self-supervised hybrid architecture that combines\ndense sentence embeddings with domain-specific Knowledge Graphs (KGs) to\nimprove both semantic alignment and explainability. Unlike previous work that\nevaluated models on aggregate performance, our approach emphasizes data\nstratification by partitioning the STR score continuum into distinct regions:\nlow, medium, and high semantic relatedness. This stratified evaluation enables\na fine-grained analysis of model performance across semantically meaningful\nsubspaces. We evaluate several embedding models, both with and without KG\nintegration via graph neural networks. The results show that fine-tuned SBERT\nmodels augmented with KGs produce consistent improvements in the high-STR\nregion, where the RMSE is reduced by 25% over strong baselines. Our findings\nhighlight not only the benefits of combining KGs with text embeddings, but also\nthe importance of regional performance analysis in understanding model\nbehavior. This granular approach reveals strengths and weaknesses hidden by\nglobal metrics, and supports more targeted model selection for use in Human\nResources (HR) systems and applications where fairness, explainability, and\ncontextual matching are essential.", "AI": {"tldr": "本文提出了一种结合知识图谱和文本嵌入的自监督混合架构，采用分区域评估方法来优化职位匹配中的语义文本相关性。实验结果表明，这种架构方法提升了模型在高语义相关性区域的表现，并揭示了模型的优势和劣势。", "motivation": "研究的动机在于解决简历推荐系统中职位匹配的关键挑战。我们引入了一种新的架构来处理重叠词汇有限或具有误导性的挑战，并通过分区评估来细致分析模型性能。", "method": "我们提出了一种自监督混合架构，结合密集句嵌入与领域特定的知识图谱（KG）以提高语义对齐和可解释性。我们探讨了在职位匹配中的语义文本相关性（STR），这是一种简历推荐系统中的关键技术挑战，其中重叠词汇通常是有限或潜在误导的。不同于以往着重模型整体性能的评估，我们的方法强调数据分层，通过将STR分数连续体划分为低、中、高语义相关性的不同区域来进行细致的模型性能分析。", "result": "我们测试了几种嵌入模型，包括与和不与通过图神经网络集成知识图谱的模型。结果显示，与强大的基线相比，通过知识图谱增强的微调SBERT模型在高STR区域取得了一致性的改进，使得均方根误差（RMSE）降低了25%。", "conclusion": "实验结果引导我们关注将KG与文本嵌入结合的好处，同时也澄清了区域性能分析对于理解模型行为的重要性。这种细致的方法揭示了被全球指标所掩盖的模型优势和劣势，并支持更精确地选择用于人力资源系统和需求公平性、可解释性和上下文匹配的应用。"}}
{"id": "2509.09130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09130", "abs": "https://arxiv.org/abs/2509.09130", "authors": ["Bin Huang", "Kang Chen", "Bingxuan Li", "Huafeng Liu", "Qiegen Liu"], "title": "ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain", "comment": null, "summary": "Building large-scale foundation model for PET imaging is hindered by limited\naccess to labeled data and insufficient computational resources. To overcome\ndata scarcity and efficiency limitations, we propose ALL-PET, a low-resource,\nlow-shot PET foundation model operating directly in the projection domain.\nALL-PET leverages a latent diffusion model (LDM) with three key innovations.\nFirst, we design a Radon mask augmentation strategy (RMAS) that generates over\n200,000 structurally diverse training samples by projecting randomized\nimage-domain masks into sinogram space, significantly improving generalization\nwith minimal data. This is extended by a dynamic multi-mask (DMM) mechanism\nthat varies mask quantity and distribution, enhancing data diversity without\nadded model complexity. Second, we implement positive/negative mask constraints\nto embed strict geometric consistency, reducing parameter burden while\npreserving generation quality. Third, we introduce transparent medical\nattention (TMA), a parameter-free, geometry-driven mechanism that enhances\nlesion-related regions in raw projection data. Lesion-focused attention maps\nare derived from coarse segmentation, covering both hypermetabolic and\nhypometabolic areas, and projected into sinogram space for physically\nconsistent guidance. The system supports clinician-defined ROI adjustments,\nensuring flexible, interpretable, and task-adaptive emphasis aligned with PET\nacquisition physics. Experimental results show ALL-PET achieves high-quality\nsinogram generation using only 500 samples, with performance comparable to\nmodels trained on larger datasets. ALL-PET generalizes across tasks including\nlow-dose reconstruction, attenuation correction, delayed-frame prediction, and\ntracer separation, operating efficiently with memory use under 24GB.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "{\n  \"tldr\": \"我们提出了ALL-PET，一种在投影域直接操作的低资源、少镜头PET基础模型，通过创新策略克服数据稀缺和效率低下的问题，并在实验中显示了高水平的性能。\",\n  \"motivation\": \"现有的大规模PET建模受限于标注数据不足和计算资源有限，本文旨在通过特定策略提高数据利用率和模型整体性能。\",\n  \"method\": \"ALL-PET使用潜变量扩散模型（LDM），结合Radon掩码增强策略（RMAS）、动态多掩码（DMM）机制和透明医疗注意力机制（TMA）提高生成质量和几何一致性。\",\n  \"result\": \"实验结果表明，使用仅500个样本，ALL-PET即可生成高质量的sinogram图像，并在性能上与大规模数据集训练的模型相当。\",\n  \"conclusion\": \"ALL-PET展示了在低资源限制下实现高效和性能优良的PET建模的能力，适用于多种任务且记忆使用低于24GB。提供了一种新的方向来优化PET成像的计算基础模型。\\n\"}\n}", "conclusion": ""}}
{"id": "2509.09524", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09524", "abs": "https://arxiv.org/abs/2509.09524", "authors": ["Daniil Ignatev", "Nan Li", "Hugh Mee Wong", "Anh Dang", "Shane Kaszefski Yaschuk"], "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning", "comment": "11 pages, 4 figures; to appear at NLPerspectives@EMNLP-2025", "summary": "This system paper presents the DeMeVa team's approaches to the third edition\nof the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et\nal., 2025). We explore two directions: in-context learning (ICL) with large\nlanguage models, where we compare example sampling strategies; and label\ndistribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we\nevaluate several fine-tuning methods. Our contributions are twofold: (1) we\nshow that ICL can effectively predict annotator-specific annotations\n(perspectivist annotations), and that aggregating these predictions into soft\nlabels yields competitive performance; and (2) we argue that LDL methods are\npromising for soft label predictions and merit further exploration by the\nperspectivist community.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09140", "abs": "https://arxiv.org/abs/2509.09140", "authors": ["Dylan Peek", "Matthew P. Skerritt", "Stephan Chalup"], "title": "Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology", "comment": "12 pages", "summary": "Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer\ncontrasting approaches to inferring topological structure from data. In this\nstudy, we examine the noise robustness of a supervised neural network trained\nto predict Betti numbers in 2D binary images. We compare an ANN approach\nagainst a PH pipeline based on cubical complexes and the Signed Euclidean\nDistance Transform (SEDT), which is a widely adopted strategy for noise-robust\ntopological analysis. Using one synthetic and two real-world datasets, we show\nthat ANNs can outperform this PH approach under noise, likely due to their\ncapacity to learn contextual and geometric priors from training data. Though\nstill emerging, the use of ANNs for topology estimation offers a compelling\nalternative to PH under structural noise.", "AI": {"tldr": "ANNs在预测2D二值图像中贝蒂数时对噪声的鲁棒性优于PH方法，显示出在结构噪声下进行拓扑估计的潜力。", "motivation": "研究目的是探讨ANNs在拓扑结构分析中的噪声鲁棒性，及其相比现有PH方法的性能差异。", "method": "通过对比人工神经网络(ANNs)和基于立方复形及符号欧几里得距离变换(SEDT)的持久同源性(PH)方法，研究了ANNs对2D二值图像中贝蒂数进行预测的噪声鲁棒性。", "result": "研究表明，在含有结构噪声的条件下，ANNs在预测2D二值图像中的贝蒂数时优于PH方法。", "conclusion": "尽管ANNs在拓扑估计中的应用尚处于起步阶段，但在结构噪声存在的情况下，它为PH方法提供了一种有吸引力的替代方案。"}}
{"id": "2509.09544", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09544", "abs": "https://arxiv.org/abs/2509.09544", "authors": ["Paolo Pedinotti", "Peter Baumann", "Nathan Jessurun", "Leslie Barrett", "Enrico Santus"], "title": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)", "comment": "7 pages, 6 appendices, EMNLP industry track", "summary": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling\nnew tasks and driving a proliferation of datasets and diversification of data\nsources. Yet, this transformation has outpaced traditional surveys. In this\npaper, we present MetaGraph, a generalizable methodology for extracting\nknowledge graphs from scientific literature and analyzing them to obtain a\nstructured, queryable view of research trends. We define an ontology for\nfinancial NLP research and apply an LLM-based extraction pipeline to 681 papers\n(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals\nthree key phases: early LLM adoption and task/dataset innovation; critical\nreflection on LLM limitations; and growing integration of peripheral techniques\ninto modular systems. This structured view offers both practitioners and\nresearchers a clear understanding of how financial NLP has evolved -\nhighlighting emerging trends, shifting priorities, and methodological\nshifts-while also demonstrating a reusable approach for mapping scientific\nprogress in other domains.", "AI": {"tldr": "提出MetaGraph，一种从金融NLP文献中提取知识图谱的方法，揭示了LLM应用的三个关键阶段：早期采用、反思局限性和整合外围技术。", "motivation": "传统的调研方法无法跟上LLM在金融NLP领域的快速变化，提出MetaGraph是为了更好地理解并分析这一领域的发展趋势。", "method": "Structure", "result": "应用MetaGraph对681篇论文进行了分析，实现了大规模数据驱动的研究趋势分析。识别出金融NLP领域的三个关键发展时期。", "conclusion": "MetaGraph提供了一个清晰的金融NLP发展图景，同时也是一个可复用的方法来衡量其他领域的科学进步。"}}
{"id": "2509.09143", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.09143", "abs": "https://arxiv.org/abs/2509.09143", "authors": ["Yuiko Uchida", "Ren Togo", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "title": "Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation", "comment": "Accepted by the ICCV 2025 UniLight Workshop", "summary": "This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric\nfor 3D scenes that explicitly focuses on \"objects,\" which are fundamental units\nof human visual perception. Existing metrics assess overall image quality,\nleading to discrepancies with human perception. Inspired by neuropsychological\ninsights, we hypothesize that human recognition of 3D scenes fundamentally\ninvolves attention to individual objects. OSIM enables object-centric\nevaluations by leveraging an object detection model and its feature\nrepresentations to quantify the \"objectness\" of each object in the scene. Our\nuser study demonstrates that OSIM aligns more closely with human perception\ncompared to existing metrics. We also analyze the characteristics of OSIM using\nvarious approaches. Moreover, we re-evaluate recent 3D reconstruction and\ngeneration models under a standardized experimental setup to clarify\nadvancements in this field. The code is available at\nhttps://github.com/Objectness-Similarity/OSIM.", "AI": {"tldr": "论文提出了一个新的评估3D场景的指标OSIM，该指标更好地反映了人类对场景中对象的感知。", "motivation": "现有指标主要评估整体图像质量，导致与人类感知不一致。论文的动机是基于神经心理学的研究成果，认为人类对3D场景的识别涉及对个体对象的关注。", "method": "该论文提出了一个评估3D场景的新指标Objectness SIMilarity (OSIM)，重点在于评估场景中的“对象”。它利用对象检测模型及其特征表示来量化每个对象在场景中的“对象性”。", "result": "用户研究表明，OSIM与人类感知的匹配度高于现有指标。此外，论文还分析了OSIM的特性，并重新评估了近期的3D重建和生成模型的表现。", "conclusion": "OSIM作为一种新的评价指标，在评估3D场景的对象性时提供了更接近人类感知的结果，有助于该领域的进一步发展。"}}
{"id": "2509.09583", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.09583", "abs": "https://arxiv.org/abs/2509.09583", "authors": ["Brittany Harbison", "Samuel Taubman", "Travis Taylor", "Ashok. K. Goel"], "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking", "comment": null, "summary": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality.", "AI": {"tldr": "研究提出了一种基于GPT的零样本人格检测模型来改进在线课程中的社会化推荐系统SAMISE", "motivation": "在线课程环境限制了学生自然形成社交群体的能力，而SAMI虽然能促进学生间连接，但受限于不完整的心智理论，导致提供的人格相关推荐不相关。研究动机是通过检测学生的人格特质来增强SAMI的匹配效能。", "method": "本研究提出了一种利用GPT的零样本能力从论坛入门帖子中推断Big-Five人格特质的人格检测模型，并将其集成到SAMI的实体匹配系统中，以提供基于人格特质的社会推荐。", "result": "实验表明，该人格检测模型在推断人格特质方面显示了良好的效果，与现有模型相比具有竞争力。", "conclusion": "初步集成显示，人格特质可以补充现有的匹配因素，尽管仍需进一步评估来确定其对学生活跃度和匹配质量的全面影响。"}}
{"id": "2509.09151", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09151", "abs": "https://arxiv.org/abs/2509.09151", "authors": ["Lei Wang", "Piotr Koniusz", "Yongsheng Gao"], "title": "Video Understanding by Design: How Datasets Shape Architectures and Insights", "comment": "Research report", "summary": "Video understanding has advanced rapidly, fueled by increasingly complex\ndatasets and powerful architectures. Yet existing surveys largely classify\nmodels by task or family, overlooking the structural pressures through which\ndatasets guide architectural evolution. This survey is the first to adopt a\ndataset-driven perspective, showing how motion complexity, temporal span,\nhierarchical composition, and multimodal richness impose inductive biases that\nmodels should encode. We reinterpret milestones, from two-stream and 3D CNNs to\nsequential, transformer, and multimodal foundation models, as concrete\nresponses to these dataset-driven pressures. Building on this synthesis, we\noffer practical guidance for aligning model design with dataset invariances\nwhile balancing scalability and task demands. By unifying datasets, inductive\nbiases, and architectures into a coherent framework, this survey provides both\na comprehensive retrospective and a prescriptive roadmap for advancing\ngeneral-purpose video understanding.", "AI": {"tldr": "通过对视频理解中数据集特性的深入分析，重新解读了模型设计的发展历程，并提供了未来模型设计的指导原则。", "motivation": "当前的视频理解研究大多按任务或模型分类，忽略了数据集对模型架构演变的结构性影响。因此，本文旨在通过一种新的角度，来系统性地回顾视频理解模型的发展历史，并提供未来研究方向的指导。", "method": "提出了一种以数据集为驱动的视角，分析视频理解领域中模型设计的演变。本文根据数据集的运动复杂性、时间跨度、层级组合以及多模态丰富性等因素来引导模型结构的演化，重新解读了从双流模型、3D CNNs到序列模型、Transformer和多模态基础模型的发展历程。", "result": "通过把数据集、归纳偏差和架构统一在一个连贯的框架内，本文为视频理解模型的设计提供了新的视角，既回顾了历史，也指明了未来的发展方向。", "conclusion": "本文提出了一种以数据集为驱动的分析方法，可以对未来视频理解模型的研发提供有价值的指导和启示。"}}
{"id": "2509.09593", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09593", "abs": "https://arxiv.org/abs/2509.09593", "authors": ["Bangzhao Shu", "Isha Joshi", "Melissa Karnaze", "Anh C. Pham", "Ishita Kakkar", "Sindhu Kothe", "Arpine Hovasapian", "Mai ElSherief"], "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models", "comment": "Camera-ready version for ICWSM 2026. First two authors contributed\n  equally", "summary": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding.", "AI": {"tldr": "研究通过引入EXPRESS数据集来评估LLMs在细粒度情绪识别方面的表现，发现尽管有进展，但LLMs仍然难以完全捕捉人类自我披露的情绪表达。", "motivation": "虽然许多研究探讨了LLMs在情感识别方面的能力，但在评估LLMs是否在细粒度层面上与人类情感保持一致方面仍存在关键空白。现有研究通常关注将情感分类为预定义的有限类别，忽视了更微妙的表达。因此，我们的研究旨在填补这一空白。", "method": "我们的研究通过引进名为EXPRESS的基准数据集，该数据集来源于Reddit社区，包含了251种细微的、自我披露的情绪标签。我们使用全面的评估框架来检测预测的情绪术语，并将这些术语分解为根据已建立的情绪理论的八种基本情绪，从而实现细粒度的比较。", "result": "对流行LLMs在不同提示设置下的系统测试表明，准确预测与人类自我披露的情绪相一致的情感仍然具有挑战性。定性分析进一步显示，虽然某些LLMs生成的情绪术语与已建立的情绪理论和定义一致，但它们有时未能像人类自我披露那样有效捕捉情境线索。", "conclusion": "这些发现强调了LLMs在细粒度情绪对齐方面的局限性，并为旨在增强其情境理解能力的未来研究提供了见解。"}}
{"id": "2509.09153", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09153", "abs": "https://arxiv.org/abs/2509.09153", "authors": ["JaeWoong Shin", "Jeongun Ryu", "Aaron Valero Puche", "Jinhee Lee", "Biagio Brattoli", "Wonkyung Jung", "Soo Ick Cho", "Kyunghyun Paeng", "Chan-Young Ock", "Donggeun Yoo", "Zhaoyang Li", "Wangkai Li", "Huayu Mai", "Joshua Millward", "Zhen He", "Aiden Nibali", "Lydia Anette Schoenpflug", "Viktor Hendrik Koelzer", "Xu Shuoyu", "Ji Zheng", "Hu Bin", "Yu-Wen Lo", "Ching-Hui Yang", "Sérgio Pereira"], "title": "OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge", "comment": "This is the accepted manuscript of an article published in Medical\n  Image Analysis (Elsevier). The final version is available at:\n  https://doi.org/10.1016/j.media.2025.103751", "summary": "Pathologists routinely alternate between different magnifications when\nexamining Whole-Slide Images, allowing them to evaluate both broad tissue\nmorphology and intricate cellular details to form comprehensive diagnoses.\nHowever, existing deep learning-based cell detection models struggle to\nreplicate these behaviors and learn the interdependent semantics between\nstructures at different magnifications. A key barrier in the field is the lack\nof datasets with multi-scale overlapping cell and tissue annotations. The\nOCELOT 2023 challenge was initiated to gather insights from the community to\nvalidate the hypothesis that understanding cell and tissue (cell-tissue)\ninteractions is crucial for achieving human-level performance, and to\naccelerate the research in this field. The challenge dataset includes\noverlapping cell detection and tissue segmentation annotations from six organs,\ncomprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)\nWhole-Slide Images with hematoxylin and eosin staining, divided into training,\nvalidation, and test subsets. Participants presented models that significantly\nenhanced the understanding of cell-tissue relationships. Top entries achieved\nup to a 7.99 increase in F1-score on the test set compared to the baseline\ncell-only model that did not incorporate cell-tissue relationships. This is a\nsubstantial improvement in performance over traditional cell-only detection\nmethods, demonstrating the need for incorporating multi-scale semantics into\nthe models. This paper provides a comparative analysis of the methods used by\nparticipants, highlighting innovative strategies implemented in the OCELOT 2023\nchallenge.", "AI": {"tldr": "The OCELOT 2023 challenge addresses the need for multi-scale semantics in deep learning models for cell detection and tissue segmentation, showcasing significant performance improvements by incorporating cell-tissue interactions.", "motivation": "The lack of datasets with multi-scale overlapping cell and tissue annotations hinders the development of deep learning models that can replicate the behavior of pathologists who examine multiple magnifications for diagnosis. The challenge aims to validate the importance of cell-tissue interactions for achieving human-level performance.", "method": "The OCELOT 2023 challenge provided a dataset with overlapping annotations from six organs and evaluated methods that integrated cell and tissue analysis to improve cell detection and tissue segmentation.", "result": "Top-performing models showed a significant enhancement in F1-score on the test set, achieving an up to 7.99 increase in performance compared to baseline approaches that neglect cell-tissue relationships.", "conclusion": "The integration of multi-scale cell and tissue semantics in deep learning models demonstrated substantial performance improvements, confirming the necessity to consider these relationships for future improvements in medical imaging analysis."}}
{"id": "2509.09602", "categories": ["cs.CL", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09602", "abs": "https://arxiv.org/abs/2509.09602", "authors": ["Yiqun T. Chen", "Tyler H. McCormick", "Li Liu", "Abhirup Datta"], "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination", "comment": null, "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in\nresource-limited settings where medical certification is unavailable. This\nstudy presents LA-VA, a proof-of-concept pipeline that combines Large Language\nModels (LLMs) with traditional algorithmic approaches and embedding-based\nclassification for improved cause-of-death prediction. Using the Population\nHealth Metrics Research Consortium (PHMRC) dataset across three age categories\n(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:\nGPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.\nOur results demonstrate that GPT-5 achieves the highest individual performance\nwith average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%\n(Neonate), outperforming traditional statistical machine learning baselines by\n5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches\ncould substantially improve verbal autopsy accuracy, with important\nimplications for global health surveillance in low-resource settings.", "AI": {"tldr": "本研究提出了一种基于大型语言模型（LLMs）的验证性管道LA-VA，旨在改善资源有限地区通过口头推断死亡原因的准确度，尤其是在成人、儿童和新生儿的死亡率预测上，比传统方式有了显著提高。", "motivation": "鉴于资源有限的地区无法进行医学证明，口头调查成为估算死亡原因的关键工具。本研究旨在改进这一领域的预测准确性，以此提升全球健康监测在资源匮乏地区的效率。", "method": "本研究提出了一种名为LA-VA的验证性管道方法，该方法结合了大型语言模型（LLMs）、传统的算法方法和基于嵌入的分类方法，以提高死亡原因预测的准确性。", "result": "通过使用PHMRC数据集对不同年龄段（成人7580人、儿童1960人、新生儿2438人）进行的实验，GPT-5达到了在各个测试地点成人准确率为48.6%，儿童为50.5%，新生儿为53.5%的最高个体表现，比传统的统计机器学习基线高出了5-10%。", "conclusion": "研究表明，简单的现成的LLM辅助方法可以在口头推断死亡原因时大幅度提高准确度，对于改善资源有限地区健康状况监测具有重要意义。"}}
{"id": "2509.09157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09157", "abs": "https://arxiv.org/abs/2509.09157", "authors": ["Yuan Shufang"], "title": "RT-DETR++ for UAV Object Detection", "comment": null, "summary": "Object detection in unmanned aerial vehicle (UAV) imagery presents\nsignificant challenges. Issues such as densely packed small objects, scale\nvariations, and occlusion are commonplace. This paper introduces RT-DETR++,\nwhich enhances the encoder component of the RT-DETR model. Our improvements\nfocus on two key aspects. First, we introduce a channel-gated attention-based\nupsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes\nerrors and preserves details during feature layer propagation. Second, we\nincorporate CSP-PAC during feature fusion. This technique employs parallel\nhollow convolutions to process local and contextual information within the same\nlayer, facilitating the integration of multi-scale features. Evaluation\ndemonstrates that our novel neck design achieves superior performance in\ndetecting small and densely packed objects. The model maintains sufficient\nspeed for real-time detection without increasing computational complexity. This\nstudy provides an effective approach for feature encoding design in real-time\ndetection systems.", "AI": {"tldr": "本文介绍了RT-DETR++模型，通过改进的上采样/下采样机制和CSP-PAC技术提升了目标检测的性能，特别适用于小且密集包装的目标检测，同时保持了实时的检测速度。", "motivation": "无人机(UAV)图像中的目标检测面临许多挑战，例如密集的小目标、尺度变化和遮挡。因此，提出了RT-DETR++模型，以更好地解决这些问题。", "method": "本文提出了一种改进的实时目标检测模型RT-DETR++，主要改进了其编码器部分。首先，引入了基于通道的注意力机制的上采样/下采样（AU/AD）机制，以减少错误并保持特征层传播过程中的细节。其次，引入了CSP-PAC技术，该技术使用并行空洞卷积处理同层的局部和上下文信息，方便集成多尺度特征。", "result": "评估表明，本研究设计的新型颈部分在检测小且密集包装的对象时具有出色的性能，并且在保持足够的实时检测速度的同时，没有增加计算复杂性。", "conclusion": "本研究为实时检测系统中的特征编码设计提供了一种有效的方法。"}}
{"id": "2509.09629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09629", "abs": "https://arxiv.org/abs/2509.09629", "authors": ["Minghang Zhu", "Zhengliang Shi", "Zhiwei Xu", "Shiguang Wu", "Lingjie Wang", "Pengjie Ren", "Zhaochun Ren", "Zhumin Chen"], "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems", "comment": "EMNLP 2025 Findings", "summary": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks.", "AI": {"tldr": "Proposes MOAT for improved collaboration in multi-agent systems through iterative alignment between planning and grounding agents, with proven performance gains.", "motivation": "To improve collaboration among agents in multi-agent systems and close capability gaps that exist when agents are fine-tuned independently, leading to poor coordination.", "method": "MOAT, a Multi-Agent Joint Alignment Tuning framework, alternates between two key stages: Planning Agent Alignment, which optimizes the planning agent to generate subgoal sequences; and Grounding Agent Improving, which fine-tunes the grounding agent to enhance its generalization capability.", "result": "Theoretical analysis proves that MOAT ensures a non-decreasing and progressively convergent training process. Experiments across six benchmarks show that MOAT outperforms state-of-the-art baselines, achieving average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks.", "conclusion": "MOAT effectively improves the coordination and performance of multi-agent systems by iteratively aligning the planning and grounding agents, leading to better task completion rates and generalization."}}
{"id": "2509.09159", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09159", "abs": "https://arxiv.org/abs/2509.09159", "authors": ["Zhiyue Liu", "Sihang Liu", "Jinyuan Liu", "Xinru Zhang"], "title": "A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering", "comment": "Accepted by the IEEE International Conference on Multimedia and Expo\n  (ICME 2025) for oral presentation. \\copyright\\ 2025 IEEE. Personal use of\n  this material is permitted. Permission from IEEE must be obtained for all\n  other uses", "summary": "Knowledge-based visual question answering (KB-VQA) requires a model to\nunderstand images and utilize external knowledge to provide accurate answers.\nExisting approaches often directly augment models with retrieved information\nfrom knowledge sources while ignoring substantial knowledge redundancy, which\nintroduces noise into the answering process. To address this, we propose a\ntraining-free framework with knowledge focusing for KB-VQA, that mitigates the\nimpact of noise by enhancing knowledge relevance and reducing redundancy.\nFirst, for knowledge retrieval, our framework concludes essential parts from\nthe image-question pairs, creating low-noise queries that enhance the retrieval\nof highly relevant knowledge. Considering that redundancy still persists in the\nretrieved knowledge, we then prompt large models to identify and extract\nanswer-beneficial segments from knowledge. In addition, we introduce a\nselective knowledge integration strategy, allowing the model to incorporate\nknowledge only when it lacks confidence in answering the question, thereby\nmitigating the influence of redundant information. Our framework enables the\nacquisition of accurate and critical knowledge, and extensive experiments\ndemonstrate that it outperforms state-of-the-art methods.", "AI": {"tldr": "提出了一种训练免费的知识聚焦框架，用于提高KB-VQA任务中外部知识的相关性，并减少冗余信息的影响。", "motivation": "现有的KB-VQA方法往往直接利用从知识源中检索的信息来增强模型，但却忽略了知识的冗余性，这会引入噪声并影响问题回答的精度。为了缓解这一问题，作者提出了一个新的框架以提高知识相关性和减少冗余性。", "method": "提出了一种无需训练的知识聚焦框架来解决KB-VQA问题。首先，框架会从图像-问题对中提取出关键部分，形成低噪声的查询，提高知识检索的相关性。接着，通过提示大型模型识别并提取知识中对答案有益的部分，进一步减少冗余。最后，采用选择性知识集成策略，当模型对问题答案不够自信时，才结合外部知识，从而减轻冗余信息的影响。", "result": "广泛实验证明，对比最先进的方法，该框架能够更好地获取准确和关键的知识。", "conclusion": "该研究展示了如何利用知识聚焦策略来有效提升KB-VQA任务中知识的利用效果，降低了冗余带来的负面影响。提供了一个训练免费的有效解决方案。"}}
{"id": "2509.09650", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.09650", "abs": "https://arxiv.org/abs/2509.09650", "authors": ["Siddarth Mamidanna", "Daking Rai", "Ziyu Yao", "Yilun Zhou"], "title": "All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens", "comment": "EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) demonstrate proficiency across numerous\ncomputational tasks, yet their inner workings remain unclear. In theory, the\ncombination of causal self-attention and multilayer perceptron layers allows\nevery token to access and compute information based on all preceding tokens. In\npractice, to what extent are such operations present? In this paper, on mental\nmath tasks (i.e., direct math calculation via next-token prediction without\nexplicit reasoning), we investigate this question in three steps: inhibiting\ninput-specific token computations in the initial layers, restricting the routes\nof information transfer across token positions in the next few layers, and\nforcing all computation to happen at the last token in the remaining layers.\nWith two proposed techniques, Context-Aware Mean Ablation (CAMA) and\nAttention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with\nhigh accuracy on a wide variety of mental math tasks, where meaningful\ncomputation occurs very late (in terms of layer depth) and only at the last\ntoken, which receives information of other tokens in few specific middle\nlayers. Experiments on a variety of models and arithmetic expressions show that\nthis subgraph is sufficient and necessary for high model performance, transfers\nacross different models, and works on a variety of input styles. Ablations on\ndifferent CAMA and ABP alternatives reveal their unique advantages over other\nmethods, which may be of independent interest.", "AI": {"tldr": "本文发现大型语言模型在进行数学计算时，存在着一个名为All-for-One (AF1)的子图，该子图的有意义计算发生在深度靠后的层次，且主要集中在最后一个令牌上，从其他令牌接收信息也需要特定的中间层。这个发现对于理解模型内部工作机制具有启发性。", "motivation": "尽管大型语言模型(Large Language Models, LLMs)表现出跨多项计算任务的高效能，但其内部工作原理仍不清晰。本文旨在通过数学计算任务（即通过下一步单词预测直接计算而不进行显式推理），对这一问题进行探究。", "method": "本文提出两种技术，Context-Aware Mean Ablation (CAMA) 和 Attention-Based Peeking (ABP)，用以在多层感知器和因果自注意力机制结合的大型语言模型中探究数学计算任务的内部工作机制。通过分阶段抑制早期层次的特定输入令牌计算，限制中层跨越令牌位置的信息传递路径，以及在最后层次强制所有计算发生在最后一个令牌上。", "result": "作者发现了一个名为All-for-One (AF1)的子图，该子图在深度靠后的层次进行有意义的计算，并且这种计算只发生在最后一个令牌上，这些令牌接收其他令牌的信息主要发生在某些特定的中间层。实验表明，这个子图对于处理多种算术表达式是充分和必要的，并且该子图在不同模型和不同的输入风格上均有表现。", "conclusion": "实验结果表明，该All-for-One子图对于模型的高性能是充分且必要的，而且该子图在不同模型以及不同的输入风格上均有表现。"}}
{"id": "2509.09163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09163", "abs": "https://arxiv.org/abs/2509.09163", "authors": ["Yulin Tong", "Fengzong Zhang", "Haiqin Cheng"], "title": "CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution", "comment": null, "summary": "Hyperspectral remote sensing technology has significant application value in\nfields such as forestry ecology and precision agriculture, while also putting\nforward higher requirements for fine ground object classification. However,\nalthough hyperspectral images are rich in spectral information and can improve\nrecognition accuracy, they tend to cause prominent feature redundancy due to\ntheir numerous bands, high dimensionality, and spectral mixing characteristics.\nTo address this, this study used hyperspectral images from the ZY1F satellite\nas a data source and selected Yugan County, Shangrao City, Jiangxi Province as\nthe research area to perform ground object classification research. A\nclassification framework named CWSSNet was proposed, which integrates 3D\nspectral-spatial features and wavelet convolution. This framework integrates\nmultimodal information us-ing a multiscale convolutional attention module and\nbreaks through the classification performance bottleneck of traditional methods\nby introducing multi-band decomposition and convolution operations in the\nwavelet domain. The experiments showed that CWSSNet achieved 74.50\\%, 82.73\\%,\nand 84.94\\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and\nmean F1-score (mF1) respectively in Yugan County. It also obtained the highest\nIntersection over Union (IoU) in the classifica-tion of water bodies,\nvegetation, and bare land, demonstrating good robustness. Additionally, when\nthe training set proportion was 70\\%, the increase in training time was\nlimited, and the classification effect was close to the optimal level,\nindicating that the model maintains reliable performance under small-sample\ntraining conditions.", "AI": {"tldr": "本文提出了CWSSNet框架用于地物分类研究，使用ZY1F卫星的高光谱图像为数据源，在江西省上饶市喻 ankles县进行实验，实现了最优的分类精度并展现了良好的鲁棒性和小样本训练条件下的可靠性能。", "motivation": "高光谱遥感技术在林业生态和精准农业等领域具有重要应用价值，但对精细地物分类提出了更高要求。高光谱图像虽谱信息丰富，但其多波段、高维度和谱混合特性导致特征冗余，因此提出此研究。", "method": "CWSSNet框架结合了3D光谱-空间特征和小波卷积，通过使用多尺度卷积注意力模块整合多模态信息，并在小波域引入多波段分解和卷积操作来突破传统方法的分类性能瓶颈。", "result": "在喻 ankles县实验中，CWSSNet分别实现了74.50%、82.73%和84.94%的mIoU、mAcc和mF1得分。在水域、植被和裸地的分类中，获得了最高的IoU。", "conclusion": "CWSSNet显示了良好的鲁棒性，即使在小样本训练条件下也能保持较高的分类性能，证明了其在地物分类中的有效性。"}}
{"id": "2509.09660", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09660", "abs": "https://arxiv.org/abs/2509.09660", "authors": ["Mohsen Fayyaz", "Ali Modarressi", "Hanieh Deilamsalehy", "Franck Dernoncourt", "Ryan Rossi", "Trung Bui", "Hinrich Schütze", "Nanyun Peng"], "title": "Steering MoE LLMs via Expert (De)Activation", "comment": null, "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09172", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09172", "abs": "https://arxiv.org/abs/2509.09172", "authors": ["Chunxiao Li", "Xiaoxiao Wang", "Meiling Li", "Boming Miao", "Peng Sun", "Yunjian Zhang", "Xiangyang Ji", "Yao Zhu"], "title": "Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios", "comment": "ICCV2025", "summary": "With the rapid advancement of generative models, highly realistic image\nsynthesis has posed new challenges to digital security and media credibility.\nAlthough AI-generated image detection methods have partially addressed these\nconcerns, a substantial research gap remains in evaluating their performance\nunder complex real-world conditions. This paper introduces the Real-World\nRobustness Dataset (RRDataset) for comprehensive evaluation of detection models\nacross three dimensions: 1) Scenario Generalization: RRDataset encompasses\nhigh-quality images from seven major scenarios (War and Conflict, Disasters and\nAccidents, Political and Social Events, Medical and Public Health, Culture and\nReligion, Labor and Production, and everyday life), addressing existing dataset\ngaps from a content perspective. 2) Internet Transmission Robustness: examining\ndetector performance on images that have undergone multiple rounds of sharing\nacross various social media platforms. 3) Re-digitization Robustness: assessing\nmodel effectiveness on images altered through four distinct re-digitization\nmethods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on\nRRDataset and conducted a large-scale human study involving 192 participants to\ninvestigate human few-shot learning capabilities in detecting AI-generated\nimages. The benchmarking results reveal the limitations of current AI detection\nmethods under real-world conditions and underscore the importance of drawing on\nhuman adaptability to develop more robust detection algorithms.", "AI": {"tldr": "Introduces RRDataset to evaluate the performance of AI-generated image detectors in real-world scenarios, revealing their limitations and proposing a human-centric approach for improvement.", "motivation": "To address the research gap in evaluating AI-generated image detection methods under complex real-world conditions, especially given the rapid advancements in generative models and the resulting challenges to digital security and media credibility.", "method": "This paper introduces the Real-World Robustness Dataset (RRDataset) to comprehensively evaluate the performance of AI-generated image detection models in various real-world conditions, comprising scenario generalization, internet transmission robustness, and re-digitization robustness.", "result": "The benchmarking of 17 detectors and 10 vision-language models revealed the limitations of current AI detection methods in real-world conditions.", "conclusion": "The study underscores the limitations of existing detection methods and highlights the importance of integrating human adaptability in developing more robust AI detection algorithms."}}
{"id": "2509.09675", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09675", "abs": "https://arxiv.org/abs/2509.09675", "authors": ["Runpeng Dai", "Linfeng Song", "Haolin Liu", "Zhenwen Liang", "Dian Yu", "Haitao Mi", "Zhaopeng Tu", "Rui Liu", "Tong Zheng", "Hongtu Zhu", "Dong Yu"], "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models", "comment": "21 pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.", "AI": {"tldr": "本文介绍了好奇心驱动探索（CDE），一种增强强化学习与可验证奖励（RLVR）方法中的探索能力的框架，通过该框架改进了模型的内部好奇心，实现了在某些基准测试上的显著性能提升。", "motivation": "本文旨在通过提出一种好奇心驱动的框架来改进RLVR方法中的探索方面，从而解决提前收敛和熵塌陷的问题。", "method": "该方法通过来自模型的行为者和评论者的内部好奇心信号来实现。对于行为者，使用生成回复的困惑度；对于评论者，采用多头架构中价值估计的变化，这两个信号作为探索奖励，来指导模型的探索。", "result": "<tool_call>\r\n{\"name\": \"Structure\", \"arguments\": {\"tldr\": \"Curiosity-Driven Exploration (CDE) is introduced as a framework to enhance exploration in Reinforcement Learning with Verifiable Rewards (RLVR), addressing its premature convergence and entropy collapse issues by leveraging the model's intrinsic curiosity. This is achieved through perplexity for the actor and variance of value estimates for the critic, leading to a +3 point improvement on AIME benchmarks compared to standard RLVR techniques.\", \"motivation\": \"The motivation behind this paper is to improve the exploration aspect of RLVR methods that often lead to premature convergence and entropy collapse, by introducing a curiosity-driven framework.\", \"method\": \"The method involves using curiosity signals from the model’s actor and critic. For the actor, perplexity over generated responses is used, and for the critic, variance of value estimates from a multi-head architecture is employed, both serving as exploration bonuses.\", \"result\": \"The theoretical analysis demonstrates that the actor-wise bonus discourages overconfident errors and promotes diversity in correct responses. The critic-wise bonus is equivalent to count-based exploration bonus in RL. Empirically, the method shows a +3 point improvement on AIME benchmarks.\", \"conclusion\": \"The conclusion asserts that CDE enhances the exploration capabilities of RLVR by addressing the mechanisms that lead to premature entropy collapse and by introducing an innovative curiosity-driven framework. It also identifies a calibration collapse issue within RLVR, which may contribute to LLM failure modes.\"}}\r\n</tool_call>", "conclusion": "本摘要分析了一种新颖的探索机制：好奇心驱动探索（CDE），其目的在于解决强化学习与可验证奖励（RLVR）方法中的提前收敛和熵塌陷问题。通过理论分析与实验证明，该方法在某些基准测试上表现出了显著的性能提升，并揭示了RLVR中的一些校准崩溃问题，这可能解释了大型语言模型的常见失败原因。"}}
{"id": "2509.09183", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09183", "abs": "https://arxiv.org/abs/2509.09183", "authors": ["Jiasheng Guo", "Xin Gao", "Yuxiang Yan", "Guanghao Li", "Jian Pu"], "title": "Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection", "comment": "11 pages, 6 figures, conference", "summary": "Low-light Object detection is crucial for many real-world applications but\nremains challenging due to degraded image quality. While recent studies have\nshown that RAW images offer superior potential over RGB images, existing\napproaches either use RAW-RGB images with information loss or employ complex\nframeworks. To address these, we propose a lightweight and self-adaptive Image\nSignal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW\nimages in dark environments, enabling seamless end-to-end training for object\ndetection. Our key innovations are: (1) We deconstruct conventional ISP\npipelines into sequential linear (sensor calibration) and nonlinear (tone\nmapping) sub-modules, recasting them as differentiable components optimized\nthrough task-driven losses. Each module is equipped with content-aware\nadaptability and physics-informed priors, enabling automatic RAW-to-RGB\nconversion aligned with detection objectives. (2) By exploiting the ISP\npipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that\nfacilitates cooperation between sub-modules. Through extensive experiments on\nthree RAW image datasets, we demonstrate that our method outperforms\nstate-of-the-art RGB- and RAW-based detection approaches, achieving superior\nresults with minimal parameters in challenging low-light environments.", "AI": {"tldr": "本文提出了一种针对低光环境的目标检测方法Dark-ISP，该方法通过处理RAW图像达到了比现有RGB方法更好的性能。", "motivation": "本文旨在改进低光照环境下的物体检测技术，针对现有方法存在的信息损失或框架复杂等问题，提出新的解决方案。", "method": "本文提出了一种轻量级且自适应的ISP插件Dark-ISP，该插件可以直接处理暗环境下的Bayer RAW图像，从而实现无缝的端到端训练用于目标检测。主要创新点包括：1) 将传统的ISP流水线分解为顺序的线性（传感器校准）和非线性（色调映射）子模块，并将其重新表述为可微分组件，通过任务驱动的损失进行优化。每个模块都配备了内容感知的适应性和基于物理的先验信息，以实现与检测目标对齐的自动RAW到RGB转换。2) 通过利用ISP流水线的内在级联结构，设计了一种自增强机制以促进子模块之间的协作。", "result": "通过对三个RAW图像数据集进行广泛实验，证明所提出的方法在参数量较少的情况下，在具有挑战性的低光照环境下，超过了最先进的RGB和RAW基于的目标检测方法，获得了更好的结果。", "conclusion": "实验表明Dark-ISP在低光环境下具有优越的物体检测性能，证明其在处理RAW图像方面的能力，为未来的研究打开了新的可能性。"}}
{"id": "2509.09190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09190", "abs": "https://arxiv.org/abs/2509.09190", "authors": ["Hanwei Zhu", "Haoning Wu", "Zicheng Zhang", "Lingyu Zhu", "Yixuan Li", "Peilin Chen", "Shiqi Wang", "Chris Wei Zhou", "Linhan Cao", "Wei Sun", "Xiangyang Zhu", "Weixia Zhang", "Yucheng Zhu", "Jing Liu", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min", "Zhichao Zhang", "Xinyue Li", "Shubo Xu", "Anh Dao", "Yifan Li", "Hongyuan Yu", "Jiaojiao Yi", "Yiding Tian", "Yupeng Wu", "Feiran Sun", "Lijuan Liao", "Song Jiang"], "title": "VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results", "comment": "ICCV VQualA Workshop 2025", "summary": "This paper presents a summary of the VQualA 2025 Challenge on Visual Quality\nComparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025\nWorkshop on Visual Quality Assessment. The challenge aims to evaluate and\nenhance the ability of state-of-the-art LMMs to perform open-ended and detailed\nreasoning about visual quality differences across multiple images. To this end,\nthe competition introduces a novel benchmark comprising thousands of\ncoarse-to-fine grained visual quality comparison tasks, spanning single images,\npairs, and multi-image groups. Each task requires models to provide accurate\nquality judgments. The competition emphasizes holistic evaluation protocols,\nincluding 2AFC-based binary preference and multi-choice questions (MCQs).\nAround 100 participants submitted entries, with five models demonstrating the\nemerging capabilities of instruction-tuned LMMs on quality assessment. This\nchallenge marks a significant step toward open-domain visual quality reasoning\nand comparison and serves as a catalyst for future research on interpretable\nand human-aligned quality evaluation systems.", "AI": {"tldr": "VQualA 2025挑战赛评估了大型模型在视觉质量对比中的能力，参与者众多，并展示了LMM模型的新能力。", "motivation": "本次挑战的目标是评估和提高最先进的大模型在多模态视觉质量对比中的能力，特别侧重于开放式和详细的推理能力，以推动开放域视觉质量推理和比较的研究。", "method": "该挑战赛通过引入一个包含数千个粗细粒度视觉质量比较任务的新基准来评估和提高最先进的LMM模型在不同的多图像中进行开放式和详细的推理能力，该基准涵盖了单张图像、图像对和多图像组的任务。每个任务都需要模型提供准确的质量判断。", "result": "大约100个参赛团队提交了参赛作品，有五个模型展示了指令调整后的LMM模型在质量评估方面的新能力。这标志着在开放域视觉质量推理和比较方面的重大进步。", "conclusion": "此挑战赛作为一个催化剂，促进了对可解释和人机对齐的质量评估系统未来研究的发展。"}}
{"id": "2509.09200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09200", "abs": "https://arxiv.org/abs/2509.09200", "authors": ["Ge Sun", "Jun Ma"], "title": "MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network", "comment": null, "summary": "Accurate human trajectory prediction is crucial for robotics navigation and\nautonomous driving. Recent research has demonstrated that incorporating goal\nguidance significantly enhances prediction accuracy by reducing uncertainty and\nleveraging prior knowledge. Most goal-guided approaches decouple the prediction\ntask into two stages: goal prediction and subsequent trajectory completion\nbased on the predicted goal, which operate at extreme granularities:\ncoarse-grained goal prediction forecasts the overall intention, while\nfine-grained trajectory completion needs to generate the positions for all\nfuture timesteps. The potential utility of intermediate temporal granularity\nremains largely unexplored, which motivates multi-granularity trajectory\nmodeling. While prior work has shown that multi-granularity representations\ncapture diverse scales of human dynamics and motion patterns, effectively\nintegrating this concept into goal-guided frameworks remains challenging. In\nthis paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for\nhuman Trajectory prediction. MGTraj recursively encodes trajectory proposals\nfrom coarse to fine granularity levels. At each level, a transformer-based\nrecursive refinement network (RRN) captures features and predicts progressive\nrefinements. Features across different granularities are integrated using a\nweight-sharing strategy, and velocity prediction is employed as an auxiliary\ntask to further enhance performance. Comprehensive experimental results in\nEHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline\nmethods and achieves state-of-the-art performance among goal-guided methods.", "AI": {"tldr": "本研究提出了一种名为MGTraj的多粒度目标导向模型，提出了递归编码和细化技术，不仅能捕获不同粒度的特征还能通过权重共享和速度辅助任务提升预测性能，在多个数据集上取得了领先成绩。", "motivation": "现有的大多数目标导向方法都将预测任务分为两个独立的阶段：目标预测和基于预测目标的轨迹完成，这两个阶段存在粗粒度和细粒度操作，而中间粒度的潜在利用尚需探索。因此受到多粒度表示能够捕捉到人类动态和动作模式的启发，研究多粒度概念在目标导向框架中的有效整合。", "method": "MGTraj采用了一种新型的多粒度目标导向模型，递归地从粗粒度到细粒度对轨迹进行编码和细化。在每个粒度级别，都使用了一个基于变压器的递归细化网络来捕捉特征并预测渐进性的细化。同时通过权重共享策略将不同粒度的特征整合，使用速度预测作为辅助任务来进一步提升性能。", "result": "实验结果显示，在EHT/UCY和Stanford Drone Dataset等数据集中，MGTraj的性能优于基准方法，并且在目标导向方法中达到了最先进的状态。", "conclusion": "MGTraj作为一个新的多粒度导向模型，在人类轨迹预测的领域展示了其优越性和创新性。它在预测精度上的改进和对多个粒度的综合效果，证明了这种方法在同类方法中的领先地位。"}}
{"id": "2509.09307", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09307", "abs": "https://arxiv.org/abs/2509.09307", "authors": ["Zhengzhao Lai", "Youbin Zheng", "Zhenyang Cai", "Haonan Lyu", "Jinpu Yang", "Hongqing Liang", "Yan Hu", "Benyou Wang"], "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization", "comment": null, "summary": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha.", "AI": {"tldr": "MatCha是一个新的基准测试，专注于材料表征图像的理解，研究表明现有的多模态大型语言模型在处理这类需要高级专业知识的问题时存在局限性。", "motivation": "研究动机在于探索多模态大型语言模型在理解真实世界材料表征图像数据的能力，并指出现有模型在这方面的不足，以促进新材料发现和自主科研代理方面的未来研究。", "method": "提出MatCha基准测试，包含1500个问题，这些问题被设计用于反映材料科学家所面临的实际挑战，广泛涵盖了材料研究的各个重要阶段和任务。", "result": "该研究提出了MatCha，这是一个针对材料表征图像理解的基准测试，包含1500个需要专家级领域知识的问题。MatCha跨越材料研究的四个关键阶段，包括21个不同的任务，旨在反映材料科学家所面临的实际挑战。实验评估表明，现有的多模态大型语言模型在处理需要高级专业知识和复杂视觉感知的问题时存在显著性能差距，这表明当前的MLLM在适应真实世界的材料表征场景方面还有局限性。", "conclusion": "实验结果表明，与人类专家相比，目前最先进的多模态大型语言模型在处理复杂和需要高层次专业知识的问题上存在显著性能差距。这表明这些模型在适应真实世界的材料表征场景方面还存在局限性。"}}
{"id": "2509.09232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09232", "abs": "https://arxiv.org/abs/2509.09232", "authors": ["Jiesi Hu", "Jianfeng Cao", "Yanwu Yang", "Chenfei Ye", "Yixuan Zhang", "Hanyang Peng", "Ting Ma"], "title": "Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement", "comment": null, "summary": "In-context learning (ICL) offers a promising paradigm for universal medical\nimage analysis, enabling models to perform diverse image processing tasks\nwithout retraining. However, current ICL models for medical imaging remain\nlimited in two critical aspects: they cannot simultaneously achieve\nhigh-fidelity predictions and global anatomical understanding, and there is no\nunified model trained across diverse medical imaging tasks (e.g., segmentation\nand enhancement) and anatomical regions. As a result, the full potential of ICL\nin medical imaging remains underexplored. Thus, we present \\textbf{Medverse}, a\nuniversal ICL model for 3D medical imaging, trained on 22 datasets covering\ndiverse tasks in universal image segmentation, transformation, and enhancement\nacross multiple organs, imaging modalities, and clinical centers. Medverse\nemploys a next-scale autoregressive in-context learning framework that\nprogressively refines predictions from coarse to fine, generating consistent,\nfull-resolution volumetric outputs and enabling multi-scale anatomical\nawareness. We further propose a blockwise cross-attention module that\nfacilitates long-range interactions between context and target inputs while\npreserving computational efficiency through spatial sparsity. Medverse is\nextensively evaluated on a broad collection of held-out datasets covering\npreviously unseen clinical centers, organs, species, and imaging modalities.\nResults demonstrate that Medverse substantially outperforms existing ICL\nbaselines and establishes a novel paradigm for in-context learning. Code and\nmodel weights will be made publicly available. Our model are publicly available\nat https://github.com/jiesihu/Medverse.", "AI": {"tldr": "本文介绍了Medverse，这是一种适用于3D医学影像的通用ICL模型，通过跨多个数据集训练，展示了在多样化医学影像任务中的卓越性能和一致性。", "motivation": "当前用于医学成像的在上下文学习 (ICL) 模型在两个关键方面存在局限：无法在保持高质量预测的同时理解全局解剖结构，并且没有统一的跨不同类型医学影像任务和解剖部位进行训练的模型，导致医学影像领域ICL的潜力未被完全挖掘。", "method": "Medverse 使用了一种称为下一等级自回归的在上下文中学习框架，该框架逐步从粗到细地改进预测结果，生成一致的完整分辨率体素输出，同时支持多尺度的解剖结构感知。此外，还提出了一种块式交叉注意力模块，能够在保持计算效率的同时促进上下文输入与目标输入之间的长距离交互。", "result": "Medverse 在广泛的未见过的数据集上进行了评估，结果显示，它显著优于现有的 ICL 基线，确立了在上下文学习中的一种新范式。", "conclusion": "Medverse 实现了对广泛未见过的数据集的性能提升，包括不同的临床中心、器官、物种和成像模态，证明了其在医学影像领域ICL方法中的优越性。"}}
{"id": "2509.09680", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09680", "abs": "https://arxiv.org/abs/2509.09680", "authors": ["Rongyao Fang", "Aldrich Yu", "Chengqi Duan", "Linjiang Huang", "Shuai Bai", "Yuxuan Cai", "Kun Wang", "Si Liu", "Xihui Liu", "Hongsheng Li"], "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark", "comment": "Project page: https://flux-reason-6m.github.io/", "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09242", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09242", "abs": "https://arxiv.org/abs/2509.09242", "authors": ["Mustafa Yurdakul", "Sakir Tasdemir"], "title": "CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification", "comment": null, "summary": "Background and objective Early diagnosis of gastric diseases is crucial to\nprevent fatal outcomes. Although histopathologic examination remains the\ndiagnostic gold standard, it is performed entirely manually, making evaluations\nlabor-intensive and prone to variability among pathologists. Critical findings\nmay be missed, and lack of standard procedures reduces consistency. These\nlimitations highlight the need for automated, reliable, and efficient methods\nfor gastric tissue analysis. Methods In this study, a novel hybrid model named\nCoAtNeXt was proposed for the classification of gastric tissue images. The\nmodel is built upon the CoAtNet architecture by replacing its MBConv layers\nwith enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block\nAttention Module (CBAM) is integrated to improve local feature extraction\nthrough channel and spatial attention mechanisms. The architecture was scaled\nto achieve a balance between computational efficiency and classification\nperformance. CoAtNeXt was evaluated on two publicly available datasets,\nHMU-GC-HE-30K for eight-class classification and GasHisSDB for binary\nclassification, and was compared against 10 Convolutional Neural Networks\n(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved\n96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%\nAUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%\nprecision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all\nCNN and ViT models tested and surpassed previous studies in the literature.\nConclusion Experimental results show that CoAtNeXt is a robust architecture for\nhistopathological classification of gastric tissue images, providing\nperformance on binary and multiclass. Its highlights its potential to assist\npathologists by enhancing diagnostic accuracy and reducing workload.", "AI": {"tldr": "The paper proposes CoAtNeXt, an enhanced hybrid model for gastric tissue image classification which outperforms other Convolutional Neural Networks and Vision Transformers on two datasets, highlighting its potential for improving diagnostic accuracy and reducing workload for pathologists.", "motivation": "The motivation for this research is the need to improve the accuracy and consistency of gastric disease diagnosis, which is primarily conducted through manual histopathologic examination and is thus labor-intensive and prone to variability.", "method": "The researchers introduced CoAtNeXt, which modifies the CoAtNet architecture by incorporating enhanced ConvNeXtV2 blocks and integrating CBAM to enhance feature extraction. The model’s performance was assessed on two datasets for binary and multiclass gastric tissue image classification.", "result": "CoAtNeXt achieved high classification performance on both datasets, outperforming all the compared CNN and ViT models, with accuracy ranging from 96.47% to 98.29% and AUC scores approaching 99.90%.", "conclusion": "The conclusion is that CoAtNeXt is a robust and efficient model for gastric tissue image classification, showcasing its potential to support pathologists by increasing diagnostic accuracy and reducing their workload."}}
{"id": "2509.09254", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09254", "abs": "https://arxiv.org/abs/2509.09254", "authors": ["Jing Hao", "Yuxuan Fan", "Yanpeng Sun", "Kaixin Guo", "Lizhuo Lin", "Jinrong Yang", "Qi Yong H. Ai", "Lun M. Wong", "Hao Tang", "Kuo Feng Hung"], "title": "Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis", "comment": "40 pages, 26 figures, 9 tables", "summary": "Recent advances in large vision-language models (LVLMs) have demonstrated\nstrong performance on general-purpose medical tasks. However, their\neffectiveness in specialized domains such as dentistry remains underexplored.\nIn particular, panoramic X-rays, a widely used imaging modality in oral\nradiology, pose interpretative challenges due to dense anatomical structures\nand subtle pathological cues, which are not captured by existing medical\nbenchmarks or instruction datasets. To this end, we introduce MMOral, the first\nlarge-scale multimodal instruction dataset and benchmark tailored for panoramic\nX-ray interpretation. MMOral consists of 20,563 annotated images paired with\n1.3 million instruction-following instances across diverse task types,\nincluding attribute extraction, report generation, visual question answering,\nand image-grounded dialogue. In addition, we present MMOral-Bench, a\ncomprehensive evaluation suite covering five key diagnostic dimensions in\ndentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the\nbest-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing\nsignificant limitations of current models in this domain. To promote the\nprogress of this specific domain, we also propose OralGPT, which conducts\nsupervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated\nMMOral instruction dataset. Remarkably, a single epoch of SFT yields\nsubstantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a\n24.73% improvement. Both MMOral and OralGPT hold significant potential as a\ncritical foundation for intelligent dentistry and enable more clinically\nimpactful multimodal AI systems in the dental field. The dataset, model,\nbenchmark, and evaluation suite are available at\nhttps://github.com/isbrycee/OralGPT.", "AI": {"tldr": "研究创建了为全景牙科X光片解释设计的大型多模态指令数据集MMOral和基准MMOral-Bench，并展示了通过微调Qwen2.5-VL-7B创建的OralGPT能显著提升模型性能。", "motivation": "尽管大型视觉-语言模型在通用医疗任务中表现出色，但它们在牙科等专业领域的能力尚未充分研究，特别是全景X光片的解释，这启发了本研究。", "method": "引入了MMOral，首个针对全景牙科X光片解释的大型多模态指令数据集和基准，并构建了MMOral-Bench，涵盖了牙科诊断的五个关键维度。评估了64个大型视觉-语言模型，展示了这些模型在这一领域的局限性，并提出通过监督微调Qwen2.5-VL-7B以创建OralGPT来提升性能。", "result": "研究发现顶级模型GPT-4o在以MMOral-Bench为基准评估时，仅达到41.45%的准确率。单轮监督微调已展示出显著性能提升，例如OralGPT实现了24.73%的改进。", "conclusion": "MMOral和OralGPT为智能化牙科提供了重要的基石，促进入工智能在医疗影像分析中的临床应用提升。"}}
{"id": "2509.09263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09263", "abs": "https://arxiv.org/abs/2509.09263", "authors": ["Chao Yuan", "Yang Yang", "Yehui Yang", "Zach Cheng"], "title": "DATE: Dynamic Absolute Time Enhancement for Long Video Understanding", "comment": null, "summary": "Long video understanding remains a fundamental challenge for multimodal large\nlanguage models (MLLMs), particularly in tasks requiring precise temporal\nreasoning and event localization. Existing approaches typically adopt uniform\nframe sampling and rely on implicit position encodings to model temporal order.\nHowever, these methods struggle with long-range dependencies, leading to\ncritical information loss and degraded temporal comprehension. In this paper,\nwe propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal\nawareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a\nsemantically guided Temporal-Aware Similarity Sampling (TASS) strategy.\nSpecifically, we interleave video frame embeddings with textual timestamp\ntokens to construct a continuous temporal reference system. We further\nreformulate the video sampling problem as a vision-language retrieval task and\nintroduce a two-stage algorithm to ensure both semantic relevance and temporal\ncoverage: enriching each query into a descriptive caption to better align with\nthe vision feature, and sampling key event with a similarity-driven temporally\nregularized greedy strategy. Our method achieves remarkable improvements w.r.t.\nabsolute time understanding and key event localization, resulting in\nstate-of-the-art performance among 7B and 72B models on hour-long video\nbenchmarks. Particularly, our 7B model even exceeds many 72B models on some\nbenchmarks.", "AI": {"tldr": "Researchers introduce DATE, a method that improves long video understanding in MLLMs using timestamp embeddings and guided sampling techniques, achieving state-of-the-art performance even with smaller model sizes.", "motivation": "The motivation is to address the limitations of current multimodal large language models (MLLMs) in handling long-range dependencies and precise temporal reasoning in long videos, which leads to critical information loss and degraded performance.", "method": "Our method proposes Dynamic Absolute Time Enhancement (DATE) which uses the Timestamp Injection Mechanism (TIM) and Temporal-Aware Similarity Sampling (TASS) to improve temporal reasoning in long videos. It interleaves video frame embeddings with textual timestamps and adopts a two-stage algorithm for sampling semantically relevant and temporally diverse frames.", "result": "The proposed DATE method achieves state-of-the-art performance on hour-long video benchmarks, showcasing improvements in absolute time understanding and key event localization. Notably, even the smaller 7B model outperforms many larger 72B models in some benchmarks.", "conclusion": "The conclusion is that the proposed DATE method effectively enhances temporal awareness in multimodal large language models, improving their ability to understand and reason about long videos."}}
{"id": "2509.09267", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09267", "abs": "https://arxiv.org/abs/2509.09267", "authors": ["Linhao Li", "Yiwen Ye", "Ziyang Chen", "Yong Xia"], "title": "Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation", "comment": "15 pages, 8 figures", "summary": "3D medical image segmentation often faces heavy resource and time\nconsumption, limiting its scalability and rapid deployment in clinical\nenvironments. Existing efficient segmentation models are typically static and\nmanually designed prior to training, which restricts their adaptability across\ndiverse tasks and makes it difficult to balance performance with resource\nefficiency. In this paper, we propose PSP-Seg, a progressive pruning framework\nthat enables dynamic and efficient 3D segmentation. PSP-Seg begins with a\nredundant model and iteratively prunes redundant modules through a combination\nof block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on\nfive public datasets, benchmarking it against seven state-of-the-art models and\nsix efficient segmentation models. Results demonstrate that the lightweight\nvariant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU\nmemory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%\nacross all datasets. These findings underscore PSP-Seg's potential as a\ncost-effective yet high-performing alternative for widespread clinical\napplication.", "AI": {"tldr": "提出了PSP-Seg框架，通过渐进式的剪枝策略实现高效3D分割，轻量版PSP-Seg-S具有出色的性能且资源消耗低于nnU-Net，表明其作为临床应用潜在的高效替代方案的潜力。", "motivation": "近年来，3D医疗图像分割面临资源和时间消耗大的问题，这限制了其在临床环境中的广泛应用。此外，现有的高效分割模型通常在训练前就静态设计，这限制了它们在不同任务中的适应性，并且在性能与资源效率之间的平衡较为困难。", "method": "本文提出了PSP-Seg，这是一个渐进式剪枝框架，旨在实现高效的3D医疗图像分割。PSP-Seg从一个冗余的模型开始，并通过块级剪枝和功能解耦损失组合的方式渐进式地去除冗余模块。", "result": "研究结果表明，轻量级版本PSP-Seg-S在五种公开数据集测试中，与SOTA模型nnU-Net相比，实现了类似的性能，但减少了42-45%的GPU内存使用、29-48%的训练时间和83-87%的参数数量。", "conclusion": "这些结果表明PSP-Seg具有成为低成本高效益临床应用的高效的替代方案的潜力。"}}
{"id": "2509.09286", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09286", "abs": "https://arxiv.org/abs/2509.09286", "authors": ["Bohao Tang", "Yan Ma", "Fei Zhang", "Jiadi Su", "Ethan Chern", "Zhulin Hu", "Zhixin Wang", "Pengfei Liu", "Ya Zhang"], "title": "Visual Programmability: A Guide for Code-as-Thought in Chart Understanding", "comment": null, "summary": "Chart understanding presents a critical test to the reasoning capabilities of\nVision-Language Models (VLMs). Prior approaches face critical limitations: some\nrely on external tools, making them brittle and constrained by a predefined\ntoolkit, while others fine-tune specialist models that often adopt a single\nreasoning strategy, such as text-based chain-of-thought (CoT). The intermediate\nsteps of text-based reasoning are difficult to verify, which complicates the\nuse of reinforcement-learning signals that reward factual accuracy. To address\nthis, we propose a Code-as-Thought (CaT) approach to represent the visual\ninformation of a chart in a verifiable, symbolic format. Our key insight is\nthat this strategy must be adaptive: a fixed, code-only implementation\nconsistently fails on complex charts where symbolic representation is\nunsuitable. This finding leads us to introduce Visual Programmability: a\nlearnable property that determines if a chart-question pair is better solved\nwith code or direct visual analysis. We implement this concept in an adaptive\nframework where a VLM learns to choose between the CaT pathway and a direct\nvisual reasoning pathway. The selection policy of the model is trained with\nreinforcement learning using a novel dual-reward system. This system combines a\ndata-accuracy reward to ground the model in facts and prevent numerical\nhallucination, with a decision reward that teaches the model when to use each\nstrategy, preventing it from defaulting to a single reasoning mode. Experiments\ndemonstrate strong and robust performance across diverse chart-understanding\nbenchmarks. Our work shows that VLMs can be taught not only to reason but also\nhow to reason, dynamically selecting the optimal reasoning pathway for each\ntask.", "AI": {"tldr": "通过引入Code-as-Thought（CaT）方法和视觉可编程性，本文提出了一种视觉语言模型，可以自适应地选择图表理解的最佳推理路径，表现出强劲和稳健的性能。", "motivation": "现有的图表理解方法存在适应性和可靠性问题，本文旨在通过新型方法克服这些局限，提供更可靠的推理策略选择机制。", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出了Code-as-Thought（CaT）方法，用于表示图表的视觉信息并采用可验证的符号格式。通过引入视觉可编程性这一概念，该模型能够学习在使用CaT路径或直接视觉分析之间进行选择，提高了复杂图表的理解准确性和适应性。\",\n  \"motivation\": \"现有的方法依赖外部工具或单一的推理策略，导致模型适应性和可靠性有限。文章旨在解决这些问题，提供一种更可靠的推理策略选择机制，以提高视觉语言模型在图表理解上的性能。\",\n  \"method\": \"提出了Code-as-Thought（CaT）方法，该方法以可验证的符号格式表示图表的视觉信息，并引入了视觉可编程性作为模型可学习的属性。通过这种方法，在模型面对一个图表问题时，它可以选择采用CaT路径或直接视觉分析。\",\n  \"result\": \"实验结果表现了模型在不同图表理解基准上的强大和稳健性。模型能够动态选择最优的推理路径，这提升了它在不同类型任务上的适应性和性能。\",\n  \"conclusion\": \"研究表明，视觉语言模型不仅能学会推理，还能够学会该如何推理，即对于每个任务选择最优的推理路径，为图表理解问题提供了一个强有力的方法。\")", "conclusion": "该方法展示了视觉语言模型能够动态选择最优推理路径的能力，从而在处理复杂图表理解任务中表现得更强大和适应性更强。"}}
{"id": "2509.09290", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09290", "abs": "https://arxiv.org/abs/2509.09290", "authors": ["Anthony P. Addison", "Felix Wagner", "Wentian Xu", "Natalie Voets", "Konstantinos Kamnitsas"], "title": "Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training", "comment": "Accepted to MICCAI 2025, for the following workshop: ML-CDS 2025:\n  Multimodal Learning and Fusion Across Scales for Clinical Decision Support", "summary": "Segmentation models are important tools for the detection and analysis of\nlesions in brain MRI. Depending on the type of brain pathology that is imaged,\nMRI scanners can acquire multiple, different image modalities (contrasts). Most\nsegmentation models for multimodal brain MRI are restricted to fixed modalities\nand cannot effectively process new ones at inference. Some models generalize to\nunseen modalities but may lose discriminative modality-specific information.\nThis work aims to develop a model that can perform inference on data that\ncontain image modalities unseen during training, previously seen modalities,\nand heterogeneous combinations of both, thus allowing a user to utilize any\navailable imaging modalities. We demonstrate this is possible with a simple,\nthus practical alteration to the U-net architecture, by integrating a\nmodality-agnostic input channel or pathway, alongside modality-specific input\nchannels. To train this modality-agnostic component, we develop an image\naugmentation scheme that synthesizes artificial MRI modalities. Augmentations\ndifferentially alter the appearance of pathological and healthy brain tissue to\ncreate artificial contrasts between them while maintaining realistic anatomical\nintegrity. We evaluate the method using 8 MRI databases that include 5 types of\npathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and\nwhite matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,\nDWI, ADC and FLAIR). The results demonstrate that the approach preserves the\nability to effectively process MRI modalities encountered during training,\nwhile being able to process new, unseen modalities to improve its segmentation.\nProject code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG", "AI": {"tldr": "本研究提出了一种新型脑MRI分割模型，通过在U-net架构中加入模态不可知的路径，能够处理训练中未见过的模态，同时保持原有模态的有效处理，提高了分割的准确性和泛化能力。", "motivation": "传统的多模态脑MRI分割模型通常受限于固定模态，不能有效处理在推理过程中出现的新模态。还有的模型能够泛化到未见过的模态，但却可能丢失模态特有的信息。因此，开发了可以在未见模态、已见模态及其异构组合上进行推理的模型。", "method": "本研究通过在U-net架构中加入模态不可知的输入通道或路径，解决了传统多模态脑MRI分割模型仅限于固定模态和无法有效处理新模态的问题。为了训练这个模态不可知的组件，作者开发了一种图像增强方案，该方案能够合成人工MRI模态，同时保持真实解剖结构的完整性。", "result": "研究使用包含5种病理类型和8种模态的8个MRI数据库进行评估。结果显示，该方法不仅能够有效地处理训练过程中遇到的MRI模态，还可以处理新的、未见过的模态，从而提高分割效果。", "conclusion": "本研究提出的方法通过一种简单而实用的U-net架构修改，实现了对新模态MRI的有效处理，提升了分割模型在多种模态MRI图像上的分割能力。"}}
{"id": "2509.09297", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09297", "abs": "https://arxiv.org/abs/2509.09297", "authors": ["Spyridon Loukovitis", "Anastasios Arsenos", "Vasileios Karampinis", "Athanasios Voulodimos"], "title": "Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception", "comment": null, "summary": "Open-set detection is crucial for robust UAV autonomy in air-to-air object\ndetection under real-world conditions. Traditional closed-set detectors degrade\nsignificantly under domain shifts and flight data corruption, posing risks to\nsafety-critical applications. We propose a novel, model-agnostic open-set\ndetection framework designed specifically for embedding-based detectors. The\nmethod explicitly handles unknown object rejection while maintaining robustness\nagainst corrupted flight data. It estimates semantic uncertainty via entropy\nmodeling in the embedding space and incorporates spectral normalization and\ntemperature scaling to enhance open-set discrimination. We validate our\napproach on the challenging AOT aerial benchmark and through extensive\nreal-world flight tests. Comprehensive ablation studies demonstrate consistent\nimprovements over baseline methods, achieving up to a 10\\% relative AUROC gain\ncompared to standard YOLO-based detectors. Additionally, we show that\nbackground rejection further strengthens robustness without compromising\ndetection accuracy, making our solution particularly well-suited for reliable\nUAV perception in dynamic air-to-air environments.", "AI": {"tldr": "提出了一种新的开放集检测框架，适用基于嵌入的检测器，提高无人机在空对空环境中的感知能力，相比YOLO检测器提高10%的性能。", "motivation": "随着无人机自主性的增强，传统的封闭集检测器在实际条件下的领域转移和飞行数据损坏下表现不佳，而开放集检测在避免这一问题上显得尤为重要。", "method": "我们提出了一种新颖且模型不可知的开放式目标检测框架，专门针对基于嵌入的检测器。该方法通过在嵌入空间中通过熵建模估计语义不确定性，并结合谱归一化和温度缩放来增强开放集区分。", "result": "我们在具有挑战性的AOT空中基准和广泛的飞行测试中验证了我们的方法。实验表明，与标准YOLO检测器相比，我们实现了高达10%的相对AUROC增益。同时，背景拒绝进一步增强了鲁棒性，而不影响检测精度。", "conclusion": "我们的方法不仅提升了检测器在开放集条件下的性能，而且在存在飞行数据损坏的情况下保持了鲁棒性。这使得我们的解决方案特别适合动态空对空环境中可靠的无人机感知。"}}
{"id": "2509.09298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09298", "abs": "https://arxiv.org/abs/2509.09298", "authors": ["Oh-Tae Jang", "Min-Gon Cho", "Kyung-Tae Kim"], "title": "Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion", "comment": "12 pages, 5 figures", "summary": "Synthetic aperture radar (SAR) images contain not only targets of interest\nbut also complex background clutter, including terrain reflections and speckle\nnoise. In many cases, such clutter exhibits intensity and patterns that\nresemble targets, leading models to extract entangled or spurious features.\nSuch behavior undermines the ability to form clear target representations,\nregardless of the classifier. To address this challenge, we propose a novel\nobject-centric learning (OCL) framework, named SlotSAR, that disentangles\ntarget representations from background clutter in SAR images without mask\nannotations. SlotSAR first extracts high-level semantic features from SARATR-X\nand low-level scattering features from the wavelet scattering network in order\nto obtain complementary multi-level representations for robust target\ncharacterization. We further present a multi-level slot attention module that\nintegrates these low- and high-level features to enhance slot-wise\nrepresentation distinctiveness, enabling effective OCL. Experimental results\ndemonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery\nby preserving structural details compared to existing OCL methods.", "AI": {"tldr": "本文提出SlotSAR框架，用于从SAR图像中分离背景杂波，增强目标表征的清晰度，并通过多层次插槽注意力模块提升表征差异性。实验表明，SlotSAR在SAR图像上实现了最先进的性能。", "motivation": "合成孔径雷达（SAR）图像中不仅包含感兴趣的目标，还包含复杂的背景杂波，包括地形反射和斑点噪声。这些杂波在强度和模式上可能与目标相似，导致模型提取出纠缠或虚假特征，从而削弱任何分类器形成清晰目标表征的能力。", "method": "本文提出了一种新颖的对象中心学习（OCL）框架，命名为SlotSAR，用于在没有掩码注释的情况下从SAR图像背景杂波中分离目标表示。SlotSAR首先从SARATR-X中提取高层次的语义特征，从小波散射网络中提取低层次的散射特征，以获得互补的多层次表示，从而实现稳健的目标表征。接着通过多层次插槽注意力模块整合低层次和高层次特征，增强插槽级别的表征区别性，从而达到有效的OCL。", "result": "实验验证了SlotSAR的有效性，证明其在SAR图像数据集上实现了更好的性能和结构细节保持能力。", "conclusion": "实验结果表明，与现有的OCL方法相比，SlotSAR在SAR图像上实现了最先进的性能，更好地保持了结构细节。"}}
{"id": "2509.09310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09310", "abs": "https://arxiv.org/abs/2509.09310", "authors": ["Hao Si", "Ehsan Javanmardi", "Manabu Tsukada"], "title": "You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception", "comment": null, "summary": "Collaborative perception enables vehicles to overcome individual perception\nlimitations by sharing information, allowing them to see further and through\nocclusions. In real-world scenarios, models on different vehicles are often\nheterogeneous due to manufacturer variations. Existing methods for\nheterogeneous collaborative perception address this challenge by fine-tuning\nadapters or the entire network to bridge the domain gap. However, these methods\nare impractical in real-world applications, as each new collaborator must\nundergo joint training with the ego vehicle on a dataset before inference, or\nthe ego vehicle stores models for all potential collaborators in advance.\nTherefore, we pose a new question: Can we tackle this challenge directly during\ninference, eliminating the need for joint training? To answer this, we\nintroduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel\nframework that formulates the problem as few-shot unsupervised domain\nadaptation. Unlike previous work, PHCP dynamically aligns features by\nself-training an adapter during inference, eliminating the need for labeled\ndata and joint training. Extensive experiments on the OPV2V dataset demonstrate\nthat PHCP achieves strong performance across diverse heterogeneous scenarios.\nNotably, PHCP achieves performance comparable to SOTA methods trained on the\nentire dataset while using only a small amount of unlabeled data.", "AI": {"tldr": "为解决车辆异构模型间域差距挑战，提出了渐进异构协作感知（PHCP）框架，在推理过程中动态对齐特征，无需标注数据和联合训练，在OPV2V数据集上表现出色。", "motivation": "尽管现有方法通过微调适配器或整个网络来应对异构模型间域差距的挑战，但它们在实际应用中并不可行，因为每个新合作者必须在数据集上与自我车辆进行联合训练，或者自我车辆提前存储所有潜在合作者的模型。", "method": "提出了一种名为渐进异构协作感知（PHCP）的新框架，该框架将问题表述为少量无监督领域适应问题，并在推理过程中通过自我训练适配器动态对齐特征，消除了对标注数据和联合训练的需求。", "result": "", "conclusion": "实验结果表明，PHCP在不同的异构场景中表现良好，并且仅使用少量未标记数据就能达到与全量数据训练的SOTA方法相当的性能。"}}
{"id": "2509.09311", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09311", "abs": "https://arxiv.org/abs/2509.09311", "authors": ["Illia Volkov", "Nikita Kisel", "Klara Janouskova", "Jiri Matas"], "title": "Image Recognition with Vision and Language Embeddings of VLMs", "comment": null, "summary": "Vision-language models (VLMs) have enabled strong zero-shot classification\nthrough image-text alignment. Yet, their purely visual inference capabilities\nremain under-explored. In this work, we conduct a comprehensive evaluation of\nboth language-guided and vision-only image classification with a diverse set of\ndual-encoder VLMs, including both well-established and recent models such as\nSigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the\nImageNet-1k validation set and its label-corrected variant. The key factors\naffecting accuracy are analysed, including prompt design, class diversity, the\nnumber of neighbours in k-NN, and reference set size. We show that language and\nvision offer complementary strengths, with some classes favouring textual\nprompts and others better handled by visual similarity. To exploit this\ncomplementarity, we introduce a simple, learning-free fusion method based on\nper-class precision that improves classification performance. The code is\navailable at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.", "AI": {"tldr": "研究发现语言和视觉提供了互补优势，因此提出了一种无学习融合方法来提升分类性能。", "motivation": "视觉语言模型在图像文本对齐上能够实现强大的零样本分类，然而，其纯粹的视觉推理能力鲜为人知。该研究旨在全面评估这些模型在图像分类中的表现。", "method": "通过使用一系列的双编码器视觉-语言模型（包括已建立的和最新的，如SigLIP 2和RADIOv2.5）来评估图像文本对齐在语言引导和纯视觉图像分类上的表现。重点关注关键因素分析如提示设计、类别多样性、k-NN中的邻居数量和参考集合大小。", "result": "实验结果显示了语言和视觉的互补性，某些类别更适合文本提示，而其他类别则更适合视觉相似性。", "conclusion": "通过基于每类精度的简单融合方法改善了图像分类性能。"}}
