{"id": "2510.25776", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25776", "abs": "https://arxiv.org/abs/2510.25776", "authors": ["Chiung-Yi Tseng", "Somshubhra Roy", "Maisha Thasin", "Danyang Zhang", "Blessing Effiong"], "title": "StreetMath: Study of LLMs' Approximation Behaviors", "comment": null, "summary": "There is a substantial body of literature examining the mathematical\nreasoning capabilities of large language models (LLMs), particularly their\nperformance on precise arithmetic operations in autoregressive architectures.\nHowever, their ability to perform approximate reasoning in informal, fast-paced\nmathematical operations has received far less attention, especially among\nnon-autoregressive decoder models. Our work addresses this gap by introducing\nStreetMath, a benchmark designed to evaluate models' approximation abilities\nunder real-world approximation scenarios. We conduct extensive evaluations\nacross different LLM architectures: Qwen3-4B-Instruct-2507,\nQwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and\nMamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to\nprobe their internal computational states. Our analysis reveals that LLMs\ngenerally attempt to compute exact values or invoke external tools even in\ntasks that call for approximation. Moreover, while models sometimes reach the\ncorrect answer in early layers or steps, they still consume more tokens when\nsolving approximation tasks. Additional experiments indicate that exact and\napproximate arithmetic operations rely on largely separate neural components.\nDrawing upon research on cognitive psychology, we argue that LLMs do not\nexhibit cognitive miserliness in the same way humans do in street math\nsettings. We open source our work https://github.com/ctseng777/StreetMath", "AI": {"tldr": "研究提出了StreetMath基准测试，评估LLMs的近似推理能力，并指出这些模型在需要近似计算时倾向于进行精确计算。", "motivation": "大量文献探讨了大型语言模型的数学推理能力，特别是它们在自回归架构中的精确算术运算中的表现，但它们在非正式、快速数学运算中的近似推理能力却关注较少，尤其是在非自回归解码器模型中。", "method": "本研究提出了StreetMath基准测试，用于评估大语言模型在现实世界近似数学情景下的近似推理能力。研究在不同架构的大型语言模型中进行广泛的评估，并使用机制可解释性技术探查模型的内部计算状态。", "result": "分析显示，大型语言模型倾向于计算精确值或调用外部工具来处理近似计算任务，即使这些任务要求近似处理。虽然模型有时在早期层或步骤中得出正确答案，但在解决近似任务时仍消耗更多的标记。additional experiments indicate that exact and approximate arithmetic operations rely on largely separate neural components.", "conclusion": "基于认知心理学研究，我们提出LLMs在街头数学环境中并不表现出人类的那种认知吝啬。"}}
{"id": "2510.25778", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25778", "abs": "https://arxiv.org/abs/2510.25778", "authors": ["Pratik N. Kalamkar", "Anupama G. Phakatkar"], "title": "Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis", "comment": "10 pages, 3 figures, International Journal Of Engineering And\n  Computer Science ISSN:2319-7242", "summary": "Opinion mining, also called sentiment analysis, is the field of study that\nanalyzes people opinions, sentiments, evaluations, appraisals, attitudes, and\nemotions towards entities such as products, services, organizations,\nindividuals, issues, events, topics, and their attributes. Holistic\nlexicon-based approach does not consider the strength of each opinion, i.e.,\nwhether the opinion is very strongly negative (or positive), strongly negative\n(or positive), moderate negative (or positive), very weakly negative (or\npositive) and weakly negative (or positive). In this paper, we propose approach\nto rank entities based on orientation and strength of the entity reviews and\nuser's queries by classifying them in granularity levels (i.e. very weak, weak,\nmoderate, very strong and strong) by combining opinion words (i.e. adverb,\nadjective, noun and verb) that are related to aspect of interest of certain\nproduct. We shall use fuzzy logic algorithmic approach in order to classify\nopinion words into different category and syntactic dependency resolution to\nfind relations for desired aspect words. Opinion words related to certain\naspects of interest are considered to find the entity score for that aspect in\nthe review.", "AI": {"tldr": "本文提出了一种方法，通过结合描述感兴趣产品属性的意见词（如副词、形容词、名词和动词），并利用模糊逻辑算法和句法依赖解析，将意见词分类为不同类别（非常弱、弱、中等、非常强、强），从而根据评论的方向和强度以及用户的查询对实体进行排序。", "motivation": "整体词汇方法没有考虑每个意见的强度，仅仅关注了意见的方向。为了更准确地反映意见的强度，本文旨在改进意见挖掘的方法。", "method": "结合描述产品属性的意见词，使用模糊逻辑算法对意见词进行分类，通过句法依赖解析找到有关方面词汇的关系，以计算实体在评论中的分数。", "result": "通过结合意见词并根据其强度分类，使得对实体的评价更加细化和准确，可以帮助更精确地对实体进行排序。", "conclusion": "所提出的方法能够更细致地评估和排序实体，不仅考虑到意见的方向，也考虑到了意见的强度。"}}
{"id": "2510.25783", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.25783", "abs": "https://arxiv.org/abs/2510.25783", "authors": ["DongJae Kim", "Yaejin Lee", "Minsu Park", "Eunil Park"], "title": "LASTIST: LArge-Scale Target-Independent STance dataset", "comment": "8 pages (two columned), 1 figure", "summary": "Stance detection has emerged as an area of research in the field of\nartificial intelligence. However, most research is currently centered on the\ntarget-dependent stance detection task, which is based on a person's stance in\nfavor of or against a specific target. Furthermore, most benchmark datasets are\nbased on English, making it difficult to develop models in low-resource\nlanguages such as Korean, especially for an emerging field such as stance\ndetection. This study proposes the LArge-Scale Target-Independent STance\n(LASTIST) dataset to fill this research gap. Collected from the press releases\nof both parties on Korean political parties, the LASTIST dataset uses 563,299\nlabeled Korean sentences. We provide a detailed description of how we collected\nand constructed the dataset and trained state-of-the-art deep learning and\nstance detection models. Our LASTIST dataset is designed for various tasks in\nstance detection, including target-independent stance detection and diachronic\nevolution stance detection. We deploy our dataset on\nhttps://anonymous.4open.science/r/LASTIST-3721/.", "AI": {"tldr": "本文提供了一个名为LASTIST的大规模韩语无目标依存立场检测数据集，适用于多种立场检测任务，填补了低资源语言立场检测研究的空白。", "motivation": "当前立场检测研究大多集中在目标依存立场检测上，且大多数基准数据集基于英语，限制了在诸如韩语等低资源语言中模型的发展。为了应对这一挑战，本研究提出了一个新的数据集。", "method": "本文提出了一种名为LArge-Scale Target-Independent STance (LASTIST) 的大规模无目标依存立场数据集，填补了研究空白。该数据集是从韩国政党的新闻稿中收集的，包含563,299个标注好的韩语句子。", "result": "LASTIST 数据集适用于各种立场检测任务，包括无目标依存立场检测和历时立场检测，并且已经在 https://anonymous.4open.science/r/LASTIST-3721/ 上部署。", "conclusion": "本研究通过提供LASTIST 数据集，为立场检测领域提供了重要资源，尤其对于韩语语言的立场检测研究具有推动意义。"}}
{"id": "2510.25784", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25784", "abs": "https://arxiv.org/abs/2510.25784", "authors": ["Dhananjaya Gowda", "Seoha Song", "Harshith Goka", "Junhyun Lee"], "title": "zFLoRA: Zero-Latency Fused Low-Rank Adapters", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed with task-specific\nadapters catering to multiple downstream applications. In such a scenario, the\nadditional compute associated with these apparently insignificant number of\nadapter parameters (typically less than 1% of the base model) turns out to be\ndisproportionately significant during inference time (upto 2.5x times that of\nthe base model). In this paper, we propose a new zero-latency fused low-rank\nadapter (zFLoRA) that introduces zero or negligible latency overhead on top of\nthe base model. Experimental results on LLMs of size 1B, 3B and 7B show that\nzFLoRA compares favorably against the popular supervised fine-tuning benchmarks\nincluding low-rank adapters (LoRA) as well as full fine-tuning (FFT).\nExperiments are conducted on 18 different tasks across three different\ncategories namely commonsense reasoning, math reasoning and summary-dialogue.\nLatency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA\nH100) platforms show that the proposed zFLoRA adapters introduce zero to\nnegligible latency overhead.", "AI": {"tldr": "研究提出了一种无延迟融合低秩适配器（zFLoRA），大大降低了大语言模型在推理时的延迟开销。", "motivation": "随着大语言模型（LLMs）的应用，任务特定的适配器成为了常见配置。尽管适配器参数数量通常不足基础模型的 1%，但在推理时间上，其额外计算量可高达基础模型的 2.5 倍。本研究旨在降低这种延迟以提高推理效率。", "method": "本研究提出了一种新的无延迟融合低秩适配器（zFLoRA），旨在在基础模型之上引入零或可忽略的延迟开销。", "result": "实验结果显示，zFLoRA 在 1B、3B 和 7B 规模的大语言模型上，与流行的监督微调基准（包括低秩适配器 LoRA 以及全量微调 FFT）相比表现更优。通过在三种不同类别的 18 项任务（常识推理、数学推理和总结对话）上进行实验验证，并在 NPU（三星 Galaxy S25+）和 GPU（NVIDIA H100）平台上测量延迟，表明 zFLoRA 适配器引入几乎无延迟开销。", "conclusion": "zFLoRA 的无延迟或可忽略延迟特性表明它是一个有潜力的适配器方案，可以在不牺牲模型性能的同时提高推理效率。"}}
{"id": "2510.25797", "categories": ["cs.CV", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25797", "abs": "https://arxiv.org/abs/2510.25797", "authors": ["Sai Likhith Karri", "Ansh Saxena"], "title": "Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks", "comment": null, "summary": "This study examines the effectiveness of spatio-temporal modeling and the\nintegration of spatial attention mechanisms in deep learning models for\nunderwater object detection. Specifically, in the first phase, the performance\nof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with\nthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is\ndeveloped, through the addition of a Convolutional Block Attention Module\n(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and\nT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the\nresearch highlights how temporal modeling improves detection accuracy in\ndynamic marine environments, particularly under conditions of sudden movements,\npartial occlusions, and gradual motion. The testing results showed that YOLOv5\nachieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM\noutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,\nhighlighting their superior accuracy and generalization in detecting complex\nobjects. The findings demonstrate that T-YOLOv5 significantly enhances\ndetection reliability compared to the standard model, while T-YOLOv5 with CBAM\nfurther improves performance in challenging scenarios, although there is a loss\nof accuracy when it comes to simpler scenarios.", "AI": {"tldr": "本研究评估了时空模型和深度学习中空间注意力机制在水下目标检测中的有效性。结果表明，T-YOLOv5在动态海洋环境下的检测精度优于YOLOv5，而加入CBAM的T-YOLOv5进一步提高了在具有挑战性场景中的性能。", "motivation": "研究动机在于探索在水下环境中使用改进的时空模型和注意力机制，以提高目标检测方法的性能和鲁棒性。", "method": "方法包括，在第一个阶段评估了T-YOLOv5对比标准YOLOv5的性能；在第二阶段，通过添加CBAM构建了增强版的T-YOLOv5。", "result": "研究结果显示YOLOv5的mAP@50-95得分为0.563，而T-YOLOv5和带有CBAM的T-YOLOv5的mAP@50-95得分分别为0.813和0.811。", "conclusion": "结论指出，T-YOLOv5通过时间建模显著增强了检测可靠性，而T-YOLOv5加上CBAM在具有挑战性的场景中进一步提高了性能，尽管在简单场景中存在准确度损失。"}}
{"id": "2510.25786", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25786", "abs": "https://arxiv.org/abs/2510.25786", "authors": ["Yaniv Nikankin", "Dana Arad", "Itay Itzhak", "Anja Reusch", "Adi Simhi", "Gal Kesten-Pomeranz", "Yonatan Belinkov"], "title": "BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection", "comment": null, "summary": "One of the main challenges in mechanistic interpretability is circuit\ndiscovery, determining which parts of a model perform a given task. We build on\nthe Mechanistic Interpretability Benchmark (MIB) and propose three key\nimprovements to circuit discovery. First, we use bootstrapping to identify\nedges with consistent attribution scores. Second, we introduce a simple\nratio-based selection strategy to prioritize strong positive-scoring edges,\nbalancing performance and faithfulness. Third, we replace the standard greedy\nselection with an integer linear programming formulation. Our methods yield\nmore faithful circuits and outperform prior approaches across multiple MIB\ntasks and models. Our code is available at:\nhttps://github.com/technion-cs-nlp/MIB-Shared-Task.", "AI": {"tldr": "通过三项关键改进提高电路发现过程，以更好地理解和解释模型的工作方式。", "motivation": "解决机制可解释性的主要挑战之一即电路发现，识别模型中执行给定任务的部分。", "method": "通过使用自助法来识别具有稳定归因分数的边，引入基于比率的选择策略以及使用整数线性规划取代标准贪婪选择，改进了回路发现过程。", "result": "新的方法得到了更忠实的电路，并且在多个MIB任务和模型中优于先前的方法。", "conclusion": "提出的改进方法能够更好地识别模型中的电路，提升了模型的可解释性和性能。"}}
{"id": "2510.25897", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25897", "abs": "https://arxiv.org/abs/2510.25897", "authors": ["Nicolas Dufour", "Lucas Degeorge", "Arijit Ghosh", "Vicky Kalogeiton", "David Picard"], "title": "MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency", "comment": "Project page: https://nicolas-dufour.github.io/miro", "summary": "Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).", "AI": {"tldr": "We introduce a method called MIRO, which conditions text-to-image models on multiple reward models during training, improving the visual quality and efficiency of generated images.", "motivation": "To address the issues of reduced diversity, semantic fidelity, and efficiency that arise from using single reward models for post-hoc selection of generated images.", "method": "We propose to condition text-to-image generative models on multiple reward models during training to let the model learn user preferences directly, rather than using post-hoc selection methods.", "result": "Our proposed method, called MIRO, achieves state-of-the-art performances on GenEval compositional benchmark and high user-preference scores using different reward models.", "conclusion": "The MIRO method demonstrates superior performance in learning user preferences directly during training, leading to enhanced visual quality and faster training times compared to conventional methods."}}
{"id": "2510.25799", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25799", "abs": "https://arxiv.org/abs/2510.25799", "authors": ["Adam S. Jovine", "Tinghan Ye", "Francis Bahk", "Jingjing Wang", "David B. Shmoys", "Peter I. Frazier"], "title": "LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection", "comment": null, "summary": "Human experts often struggle to select the best option from a large set of\nitems with multiple competing objectives, a process bottlenecked by the\ndifficulty of formalizing complex, implicit preferences. To address this, we\nintroduce LISTEN, a framework that leverages a Large Language Model (LLM) as a\nzero-shot preference oracle, guided only by an expert's high-level priorities\nin natural language. To operate within LLM constraints like context windows and\ninference costs, we propose two iterative algorithms: LISTEN-U, which uses the\nLLM to refine a parametric utility function, and LISTEN-T, a non-parametric\nmethod that performs tournament-style selections over small batches of\nsolutions. Evaluated on diverse tasks including flight booking, shopping, and\nexam scheduling, our results show LISTEN-U excels when preferences are\nparametrically aligned (a property we measure with a novel concordance metric),\nwhile LISTEN-T offers more robust performance. This work explores a promising\ndirection for steering complex multi-objective decisions directly with natural\nlanguage, reducing the cognitive burden of traditional preference elicitation.", "AI": {"tldr": "本研究介绍了一种框架LISTEN，利用大型语言模型作为偏好预言机，以解决多目标决策中人类专家选择难题，提出了两种算法用于应对模型限制问题。", "motivation": "专家们常在选择一项最佳方案时遇到困难，特别是在面对多目标竞争的问题上，重点在于难以形式化复杂的、隐含的偏好。本研究旨在解决这一问题。", "method": "本研究提出了LISTEN框架，利用大型语言模型(LLM)作为零次学习偏好预言机，基于专家的高层次自然语言优先级进行指导。为了适应LLM的限制，如上下文窗口和推理成本，研究提出了两个迭代算法：LISTEN-U，该算法利用LLM来精炼一个参数化效用函数；LISTEN-T，则是一次对小批量解进行锦标赛式选择的非参数化方法。", "result": "研究在航班预订、购物和考试安排等多样化任务上进行了评估，结果显示，当偏好在参数上一致时，LISTEN-U表现更佳（这一特性通过一个新颖的符合度指标进行测量），而LISTEN-T在性能上更稳定。", "conclusion": "本研究探索了一种有前景的方向，即通过自然语言直接指导复杂的多目标决策，减少传统偏好激发的认知负担。"}}
{"id": "2510.25901", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25901", "abs": "https://arxiv.org/abs/2510.25901", "authors": ["Denniz Goren", "Holger Caesar"], "title": "BikeScenes: Online LiDAR Semantic Segmentation for Bicycles", "comment": null, "summary": "The vulnerability of cyclists, exacerbated by the rising popularity of faster\ne-bikes, motivates adapting automotive perception technologies for bicycle\nsafety. We use our multi-sensor 'SenseBike' research platform to develop and\nevaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the\nautomotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg\nDataset, comprising 3021 consecutive LiDAR scans around the university campus\nof the TU Delft, semantically annotated for 29 dynamic and static classes. By\nevaluating model performance, we demonstrate that fine-tuning on our BikeScenes\ndataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly\noutperforming the 13.8% obtained with SemanticKITTI pre-training alone. This\nresult underscores the necessity and effectiveness of domain-specific training.\nWe highlight key challenges specific to bicycle-mounted, hardware-constrained\nperception systems and contribute the BikeScenes dataset as a resource for\nadvancing research in cyclist-centric LiDAR segmentation.", "AI": {"tldr": "本文开发了一种适用于自行车的LiDAR分割技术，通过引入新的BikeScenes-lidarseg数据集来进行模型训练和评估，展示了特定领域训练的必要性和有效性。", "motivation": "由于电动自行车的流行，骑车人的脆弱性变得更加严重。这篇文章的动机是将汽车感知技术适应到自行车安全中。", "method": "我们使用多传感器'SenseBike'研究平台来开发和评估一种针对自行车的3D LiDAR分割方法。为了弥补从汽车到自行车领域的差距，我们引入了一个新的BikeScenes-lidarseg数据集，包含在代尔夫特理工大学校园周围连续的3021个LiDAR扫描，对29个动态和静态类进行了语义注释。", "result": "通过在BikeScenes数据集上进行微调，模型的表现大大提高，达到了63.6%的平均交并比（mIoU），远高于仅使用SemanticKITTI预训练所获得的13.8%。", "conclusion": "这篇文章指出自行车安装的感知系统中硬件受限的关键挑战，并建议BikeScenes数据集作为研究自行车中心LiDAR分割的一个资源。"}}
{"id": "2510.25804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25804", "abs": "https://arxiv.org/abs/2510.25804", "authors": ["Haoran Deng", "Yingyu Lin", "Zhenghao Lin", "Xiao Liu", "Yizhou Sun", "Yi-An Ma", "Yeyun Gong"], "title": "Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data", "comment": null, "summary": "Long-context language models unlock advanced capabilities in reasoning, code\ngeneration, and document summarization by leveraging dependencies across\nextended spans of text. However, a significant portion of readily available\nlong-text data lacks meaningful long-distance dependencies; most spans can be\npredicted using only local context. Training on such data is inefficient,\nmaking careful data selection crucial. Therefore, we introduce LongFilter, a\nframework for curating training data tailored to long-context pretraining.\nLongFilter measures the information gain provided by extended context by\ncontrasting model predictions under long-context versus short-context settings,\nthereby identifying samples where long-range dependencies are essential.\nExperiments with LLaMA-3-8B, extending its context length from 8K to 64K, show\nthat LongFilter efficiently selects high-quality data and yields substantial\nimprovements on benchmarks such as HELMET, LongBench, and RULER.", "AI": {"tldr": "LongFilter是一个精选长文本预训练数据的框架，通过对长距离依赖重要性的评估来挑选关键样本，从而改善模型在各种基准测试中的表现。", "motivation": "为解决长文本语言模型训练效率低下的问题，作者设计了LongFilter框架，专门筛选高质量训练数据。", "method": "LongFilter通过对比长文本和短文本模型预测结果，挑选出含有关键长距离依赖的数据样本。", "result": "{\n  \"tldr\": \"LongFilter is a framework designed to curate training data for long-context pretraining by identifying samples with essential long-range dependencies, improving model performance on various benchmarks.\", \n  \"motivation\": \"To address the inefficiency of training long-context language models on data lacking meaningful long-distance dependencies, the authors introduce LongFilter to filter and curate high-quality training data.\", \n  \"method\": \"LongFilter measures the gain of long context over short context in model predictions and selects samples where long-range dependencies are crucial for prediction.\", \n  \"result\": \"Experiments with LLaMA-3-8B showed that LongFilter efficiently selects data and improves performance on benchmarks such as HELMET, LongBench, and RULER.\", \n  \"conclusion\": \"The introduction of LongFilter for data curation leads to better and more efficient long-context language modeling training. This enhances the model's performance on reasoning, code generation, and document summarization tasks.\"]}", "conclusion": "LongFilter有效改善了长文本训练数据的选择，提升了语言模型在推理、代码生成和文档总结任务上的性能。"}}
{"id": "2510.25921", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.25921", "abs": "https://arxiv.org/abs/2510.25921", "authors": ["Nikola L. Kolev", "Tommaso Rodani", "Neil J. Curson", "Taylor J. Z. Stock", "Alberto Cazzaniga"], "title": "Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy", "comment": null, "summary": "Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and\natom manipulation, but its utility is often limited by tip degradation and slow\nserial data acquisition. Fabrication adds another layer of complexity since the\ntip is often subjected to large voltages, which may alter the shape of its\napex, requiring it to be conditioned. Here, we propose a machine learning (ML)\napproach for image repair and super-resolution to alleviate both challenges.\nUsing a dataset of only 36 pristine experimental images of Si(001):H, we\ndemonstrate that a physics-informed synthetic data generation pipeline can be\nused to train several state-of-the-art flow-matching and diffusion models.\nQuantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy\n(CMMD) score and structural similarity demonstrates that our models are able to\neffectively restore images and offer a two- to fourfold reduction in image\nacquisition time by accurately reconstructing images from sparsely sampled\ndata. Our framework has the potential to significantly increase STM\nexperimental throughput by offering a route to reducing the frequency of\ntip-conditioning procedures and to enhancing frame rates in existing high-speed\nSTM systems.", "AI": {"tldr": "论文提出了一种机器学习方法，用于STM图像的修复和超分辨率，从而提高了STM的实验效率。", "motivation": "开发一种方法，解决STM的针尖退化和低数据采集群速度问题，以改善STM的成像效果并加快实验过程。", "method": "使用机器学习方法，通过分析36张Si(001):H的实验图像进行训练，其中包含流匹配和扩散模型。", "result": "此论文提出了一种机器学习方法，用于解决扫描隧道显微镜（STM）中的图像修复和超分辨率问题。基于36张Si(001):H的实验图像，使用物理信息合成数据生成管道训练了多种最先进的流匹配和扩散模型。实验结果表明，该模型可以有效恢复图像，并通过从稀疏采样数据中准确重建图像将图像采集时间减少两到四倍，从而提高STM实验的吞吐量。", "conclusion": "该框架具有巨大潜力，可以显著提高STM实验的总吞吐量，通过减少针尖调节的频率并提高现有高速STM系统的帧率。"}}
{"id": "2510.25805", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25805", "abs": "https://arxiv.org/abs/2510.25805", "authors": ["Stefano Civelli", "Pietro Bernardelle", "Nardiena A. Pratama", "Gianluca Demartini"], "title": "Ideology-Based LLMs for Content Moderation", "comment": null, "summary": "Large language models (LLMs) are increasingly used in content moderation\nsystems, where ensuring fairness and neutrality is essential. In this study, we\nexamine how persona adoption influences the consistency and fairness of harmful\ncontent classification across different LLM architectures, model sizes, and\ncontent modalities (language vs. vision). At first glance, headline performance\nmetrics suggest that personas have little impact on overall classification\naccuracy. However, a closer analysis reveals important behavioral shifts.\nPersonas with different ideological leanings display distinct propensities to\nlabel content as harmful, showing that the lens through which a model \"views\"\ninput can subtly shape its judgments. Further agreement analyses highlight that\nmodels, particularly larger ones, tend to align more closely with personas from\nthe same political ideology, strengthening within-ideology consistency while\nwidening divergence across ideological groups. To show this effect more\ndirectly, we conducted an additional study on a politically targeted task,\nwhich confirmed that personas not only behave more coherently within their own\nideology but also exhibit a tendency to defend their perspective while\ndownplaying harmfulness in opposing views. Together, these findings highlight\nhow persona conditioning can introduce subtle ideological biases into LLM\noutputs, raising concerns about the use of AI systems that may reinforce\npartisan perspectives under the guise of neutrality.", "AI": {"tldr": "研究发现，不同的意识形态人格设定会对大型语言模型的有害内容分类产生微妙的影响，特别是意识形态一致性会随着模型的大小增加。这种影响可能会导致在中立名义下的AI系统在无意识中强化党派观点。", "motivation": "研究动机在于探讨在内容审核系统中使用大型语言模型时，保证公平性和中立性的重要性。这里特别关注不同的人格设定对LLM架构、模型大小以及内容模态（语言和视觉）的影响。", "method": "研究首先分析了不同意识形态的人格设定在有害内容分类上的倾向性。接着，通过一致性分析显示了模型（尤其是较大的模型）与相同意识形态的人格设定更加一致，导致在意识形态群体之间的差异增大。最后，通过一个政治针对性任务研究进一步验证了这些发现。", "result": "该研究通过分析不同的人格设定如何影响大型语言模型（LLMs）对有害内容分类的一致性和公平性，提出了一个新的视角。尽管表面上看来，人格设定对整体分类准确性影响不大，但深入分析却发现模型对有害内容的识别倾向随着人格设定的意识形态偏好而变化。特别是较大的模型，在与相同意识形态的人格设定下显示出更高的一致性，而与不同意识形态的人格设定下则呈现出更大的分歧。这表明，人格设定可能会引入微妙的意识形态偏见到LLM的输出中，从而引起对可能在中立名义下强化党派观点的AI系统的担忧。", "conclusion": "研究结论指出，尽管人格设定可能会对危害内容的分类准确性影响较小，但是他们从本质上给LLM带入了潜在的意识形态偏见。这一发现引发了关于在中立的名义下AIS系统可能强化党派观点的担忧。"}}
{"id": "2510.25970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25970", "abs": "https://arxiv.org/abs/2510.25970", "authors": ["Sung-Hoon Yoon", "Minghan Li", "Gaspard Beaudouin", "Congcong Wen", "Muhammad Rafay Azhar", "Mengyu Wang"], "title": "SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing", "comment": "Camera-ready version for NeurIPS 2025, 10 pages (main paper)", "summary": "Rectified flow models have become a de facto standard in image generation due\nto their stable sampling trajectories and high-fidelity outputs. Despite their\nstrong generative capabilities, they face critical limitations in image editing\ntasks: inaccurate inversion processes for mapping real images back into the\nlatent space, and gradient entanglement issues during editing often result in\noutputs that do not faithfully reflect the target prompt. Recent efforts have\nattempted to directly map source and target distributions via ODE-based\napproaches without inversion; however,these methods still yield suboptimal\nediting quality. In this work, we propose a flow decomposition-and-aggregation\nframework built upon an inversion-free formulation to address these\nlimitations. Specifically, we semantically decompose the target prompt into\nmultiple sub-prompts, compute an independent flow for each, and aggregate them\nto form a unified editing trajectory. While we empirically observe that\ndecomposing the original flow enhances diversity in the target space,\ngenerating semantically aligned outputs still requires consistent guidance\ntoward the full target prompt. To this end, we design a projection and\nsoft-aggregation mechanism for flow, inspired by gradient conflict resolution\nin multi-task learning. This approach adaptively weights the sub-target\nvelocity fields, suppressing semantic redundancy while emphasizing distinct\ndirections, thereby preserving both diversity and consistency in the final\nedited output. Experimental results demonstrate that our method outperforms\nexisting zero-shot editing approaches in terms of semantic fidelity and\nattribute disentanglement. The code is available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.", "AI": {"tldr": "研究提出了一种新的基于流分解和聚合的图像编辑方法，通过无逆映射框架解决了现有流模型在图像编辑中的局限，展现了高质量的零样本编辑能力。", "motivation": "该研究的动机在于解决现有的流模型在图像编辑任务中的局限性，特别是准确度不高和编辑过程中梯度缠结的问题。", "method": "我们提出了一种基于分解和聚合的无逆映射框架，通过将目标提示语分解为多个子提示语，并为每个子提示语计算独立的流，然后聚合以形成统一的编辑轨迹来克服现有问题。我们的方法还包括设计的投影和软聚合机制，用于解决多任务学习中的梯度冲突问题，从而自适应地加权子目标速度场，减少语义冗余并强调不同的方向。", "result": "实验结果表明，我们的方法在语义保真度和属性解耦方面优于现有的零样本编辑方法。", "conclusion": "本研究提出了一种新的流分解和聚合框架，通过无逆映射的方式解决了图像编辑的关键限制。"}}
{"id": "2510.25816", "categories": ["cs.CL", "cs.LG", "68T50, 68T07", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2510.25816", "abs": "https://arxiv.org/abs/2510.25816", "authors": ["Tarun Kumar Chawdhury", "Jon D. Duke"], "title": "Beyond Long Context: When Semantics Matter More than Tokens", "comment": "12 pages, 5 figures", "summary": "Electronic Health Records (EHR) store clinical documentation as base64\nencoded attachments in FHIR DocumentReference resources, which makes semantic\nquestion answering difficult. Traditional vector database methods often miss\nnuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)\nmethod, introduced by Lopez et al. 2025, uses entity aware retrieval and\nachieved improved performance with an F1 score of 0.90 versus 0.86 for\nembedding based retrieval, while using over 70 percent fewer tokens. We\ndeveloped a Clinical Notes QA Evaluation Platform to validate CLEAR against\nzero shot large context inference and traditional chunk based retrieval\naugmented generation. The platform was tested on 12 clinical notes ranging from\n10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a\n58.3 percent win rate, an average semantic similarity of 0.878, and used 78\npercent fewer tokens than wide context processing. The largest performance\ngains occurred on long notes, with a 75 percent win rate for documents\nexceeding 65,000 tokens. These findings confirm that entity aware retrieval\nimproves both efficiency and accuracy in clinical natural language processing.\nThe evaluation framework provides a reusable and transparent benchmark for\nassessing clinical question answering systems where semantic precision and\ncomputational efficiency are critical.", "AI": {"tldr": "Lopez等人提出的CLEAR方法在临床自然语言处理中通过实体感知检索提高了效率和准确性，优于传统方法。", "motivation": "传统向量数据库方法往往忽视细微的临床关系，而电子健康记录(EHR)将临床文档作为base64编码附件存储，使其语义问答变得困难。", "method": "CLEAR方法使用实体感知检索，相比基于嵌入的检索在F1分数上提高了0.04，同时减少了70%的令牌使用。开发了一个临床笔记QA评估平台来验证CLEAR方法。", "result": "在12份临床笔记上测试，CLEAR方法取得了58.3%的胜率，平均语义相似度为0.878，使用87%更少的令牌。对于超过65,000个令牌的文档，胜率为75%。", "conclusion": "实体感知检索改进了临床自然语言处理的效率和准确性，提供的评估框架有助于评估语义精度和计算效率关键的临床问答系统。"}}
{"id": "2510.25976", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.25976", "abs": "https://arxiv.org/abs/2510.25976", "authors": ["Roman Beliy", "Amit Zalcher", "Jonathan Kogman", "Navve Wasserman", "Michal Irani"], "title": "Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer", "comment": null, "summary": "Reconstructing images seen by people from their fMRI brain recordings\nprovides a non-invasive window into the human brain. Despite recent progress\nenabled by diffusion models, current methods often lack faithfulness to the\nactual seen images. We present \"Brain-IT\", a brain-inspired approach that\naddresses this challenge through a Brain Interaction Transformer (BIT),\nallowing effective interactions between clusters of functionally-similar\nbrain-voxels. These functional-clusters are shared by all subjects, serving as\nbuilding blocks for integrating information both within and across brains. All\nmodel components are shared by all clusters & subjects, allowing efficient\ntraining with a limited amount of data. To guide the image reconstruction, BIT\npredicts two complementary localized patch-level image features: (i)high-level\nsemantic features which steer the diffusion model toward the correct semantic\ncontent of the image; and (ii)low-level structural features which help to\ninitialize the diffusion process with the correct coarse layout of the image.\nBIT's design enables direct flow of information from brain-voxel clusters to\nlocalized image features. Through these principles, our method achieves image\nreconstructions from fMRI that faithfully reconstruct the seen images, and\nsurpass current SotA approaches both visually and by standard objective\nmetrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve\nresults comparable to current methods trained on full 40-hour recordings.", "AI": {"tldr": "The study introduces Brain-IT, a brain-inspired method using a Brain Interaction Transformer (BIT) to facilitate effective interactions between brain-voxel clusters for accurate image reconstruction from fMRI, surpassing state-of-the-art methods by visual and objective metrics, even with limited data.", "motivation": "The motivation behind this study is to tackle the challenge of producing faithful image reconstructions from fMRI brain recordings, as current methods often fall short in their accuracy, and to do so with less data than current approaches.", "method": "Brain-IT method uses a Brain Interaction Transformer (BIT) to effectively interact between clusters of functionally-similar brain-voxels, guiding the image reconstruction process through high-level semantic and low-level structural features.", "result": "The method achieves more faithful image reconstructions from fMRI data compared to state-of-the-art methods, as measured by visual fidelity and standard objective metrics. It also demonstrates comparable performance to current methods with significantly less training data.", "conclusion": "BIT's approach to image reconstruction from fMRI data, employing functional-clusters interaction and guided by both high-level semantic and low-level structural features, outperforms current top methods and does so efficiently with limited data."}}
{"id": "2510.25817", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25817", "abs": "https://arxiv.org/abs/2510.25817", "authors": ["Junyu Luo", "Bohan Wu", "Xiao Luo", "Zhiping Xiao", "Yiqiao Jin", "Rong-Cheng Tu", "Nan Yin", "Yifan Wang", "Jingyang Yuan", "Wei Ju", "Ming Zhang"], "title": "A Survey on Efficient Large Language Model Training: From Data-centric Perspectives", "comment": "ACL 2025", "summary": "Post-training of Large Language Models (LLMs) is crucial for unlocking their\ntask generalization potential and domain-specific capabilities. However, the\ncurrent LLM post-training paradigm faces significant data challenges, including\nthe high costs of manual annotation and diminishing marginal returns on data\nscales. Therefore, achieving data-efficient post-training has become a key\nresearch question. In this paper, we present the first systematic survey of\ndata-efficient LLM post-training from a data-centric perspective. We propose a\ntaxonomy of data-efficient LLM post-training methods, covering data selection,\ndata quality enhancement, synthetic data generation, data distillation and\ncompression, and self-evolving data ecosystems. We summarize representative\napproaches in each category and outline future research directions. By\nexamining the challenges in data-efficient LLM post-training, we highlight open\nproblems and propose potential research avenues. We hope our work inspires\nfurther exploration into maximizing the potential of data utilization in\nlarge-scale model training. Paper List:\nhttps://github.com/luo-junyu/Awesome-Data-Efficient-LLM", "AI": {"tldr": "本文是首个从数据角度对数据高效的LLM后训练方法进行系统性研究的工作，涵盖了多种方法类别，并提出了未来的研究方向。", "motivation": "本文旨在解决当前LLM后训练中面临的数据挑战，例如人工标注的成本高昂以及数据规模递增带来的收益递减问题，提出数据高效的后训练方法成为关键的研究问题。", "method": "本篇论文采用系统性的视角对数据高效的LLM后训练方法进行了分类，涵盖了数据选择、数据质量提升、合成数据生成、数据蒸馏与压缩和自我演化的数据生态系统。", "result": "论文提供了一个数据高效的LLM后训练方法的分类法，并总结了每个类别中代表性方法，提出了未来的研究方向。", "conclusion": "通过对数据高效LLM后训练中的挑战进行分析，本文强调了公开的问题并提出了潜在的研究途径，激发了对大规模模型训练中数据利用最大化的进一步探索。"}}
{"id": "2510.25990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.25990", "abs": "https://arxiv.org/abs/2510.25990", "authors": ["Valentin Boussot", "Cédric Hémon", "Jean-Claude Nunes", "Jean-Louis Dillenseger"], "title": "Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI", "comment": "Paper for the Trackrad2025 challenge, Team BreizhTrack", "summary": "In this work, we address the TrackRAD2025 challenge of real-time tumor\ntracking in cine-MRI sequences of the thoracic and abdominal regions under\nstrong data scarcity constraints. Two complementary strategies were explored:\n(i) unsupervised registration with the IMPACT similarity metric and (ii)\nfoundation model-based segmentation leveraging SAM 2.1 and its recent variants\nthrough prompt-based interaction. Due to the one-second runtime constraint, the\nSAM-based method was ultimately selected. The final configuration used SAM2.1\nb+ with mask-based prompts from the first annotated slice, fine-tuned solely on\nthe small labeled subset from TrackRAD2025. Training was configured to minimize\noverfitting, using 1024x1024 patches (batch size 1), standard augmentations,\nand a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was\napplied to all modules (prompt encoder, decoder, Hiera backbone) to preserve\ngeneralization while adapting to annotator-specific styles. Training lasted 300\nepochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently\napplied across all anatomical sites and MRI field strengths. Test-time\naugmentation was considered but ultimately discarded due to negligible\nperformance gains. The final model was selected based on the highest Dice\nSimilarity Coefficient achieved on the validation set after fine-tuning. On the\nhidden test set, the model reached a Dice score of 0.8794, ranking 6th overall\nin the TrackRAD2025 challenge. These results highlight the strong potential of\nfoundation models for accurate and real-time tumor tracking in MRI-guided\nradiotherapy.", "AI": {"tldr": "在强数据稀缺条件下，通过基于提示的基础模型SAM 2.1进行可靠的实时肿瘤跟踪，取得了良好的Dice相似系数结果。", "motivation": "解决在胸部和腹部区域实时MRI序列中，在数据稀缺的条件下进行肿瘤跟踪的难题。", "method": "探讨了两种策略：(i) 使用IMPACT相似度度量的无监督注册；(ii) 基于SAM 2.1及其变体的提示式基础模型分割。最终选择了SAM-based方法作为解决方案。", "result": "{BaseContext}.structure_result", "conclusion": "该方法在TrackRAD2025挑战中达到了0.8794的Dice得分，排名第六，证明了基础模型在MRI引导的放射治疗中肿瘤跟踪的应用潜力。"}}
{"id": "2510.25904", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25904", "abs": "https://arxiv.org/abs/2510.25904", "authors": ["Frederico Belcavello", "Ely Matos", "Arthur Lorenzi", "Lisandra Bonoto", "Lívia Ruiz", "Luiz Fernando Pereira", "Victor Herbst", "Yulla Navarro", "Helen de Andrade Abreu", "Lívia Dutra", "Tiago Timponi Torrent"], "title": "Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation", "comment": null, "summary": "The use of LLM-based applications as a means to accelerate and/or substitute\nhuman labor in the creation of language resources and dataset is a reality.\nNonetheless, despite the potential of such tools for linguistic research,\ncomprehensive evaluation of their performance and impact on the creation of\nannotated datasets, especially under a perspectivized approach to NLP, is still\nmissing. This paper contributes to reduction of this gap by reporting on an\nextensive evaluation of the (semi-)automatization of FrameNet-like semantic\nannotation by the use of an LLM-based semantic role labeler. The methodology\nemployed compares annotation time, coverage and diversity in three experimental\nsettings: manual, automatic and semi-automatic annotation. Results show that\nthe hybrid, semi-automatic annotation setting leads to increased frame\ndiversity and similar annotation coverage, when compared to the human-only\nsetting, while the automatic setting performs considerably worse in all\nmetrics, except for annotation time.", "AI": {"tldr": "The paper evaluates the use of a large language model for FrameNet-like semantic annotation, comparing manual, automatic, and semi-automatic methods. It finds that semi-automatic annotation is most effective, balancing diversity and coverage, with automatic annotation being quicker but less effective overall.", "motivation": "The motivation is to address the lack of comprehensive evaluation of LLM performance and impact on creating annotated datasets, particularly in the context of NLP research. The study aims to reduce the knowledge gap by providing insights into the effectiveness of (semi-)automatized semantic annotation.", "method": "The paper evaluates the performance of a large language model (LLM)-based semantic role labeler in automating FrameNet-like semantic annotation. It compares three settings: manual, automatic, and semi-automatic annotation, assessing factors such as annotation time, coverage, and diversity of semantic frames.", "result": "The results indicate that the semi-automatic annotation method increases frame diversity and maintains similar annotation coverage in comparison to purely manual annotation. The fully automatic approach, however, is less effective in most metrics except for reducing annotation time.", "conclusion": "The study concludes that semi-automated annotation using LLMs can be an effective method for enhancing the diversity of semantic annotations while maintaining coverage levels typical of human annotation. The automatic approach, while faster, is less effective overall for creating high-quality annotated datasets."}}
{"id": "2510.26001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26001", "abs": "https://arxiv.org/abs/2510.26001", "authors": ["Xinhua Wang", "Caibo Feng", "Xiangjun Fu", "Chunxiao Liu"], "title": "Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement", "comment": null, "summary": "We propose an innovative enhancement to the Mamba framework by increasing the\nHausdorff dimension of its scanning pattern through a novel Hilbert Selective\nScan mechanism. This mechanism explores the feature space more effectively,\ncapturing intricate fine-scale details and improving overall coverage. As a\nresult, it mitigates information inconsistencies while refining spatial\nlocality to better capture subtle local interactions without sacrificing the\nmodel's ability to handle long-range dependencies. Extensive experiments on\npublicly available benchmarks demonstrate that our approach significantly\nimproves both the quantitative metrics and qualitative visual fidelity of\nexisting Mamba-based low-light image enhancement methods, all while reducing\ncomputational resource consumption and shortening inference time. We believe\nthat this refined strategy not only advances the state-of-the-art in low-light\nimage enhancement but also holds promise for broader applications in fields\nthat leverage Mamba-based techniques.", "AI": {"tldr": "通过引入Hilbert Selective Scan机制提升Mamba框架的扫描模式的豪斯多夫维度，提高了细节捕捉能力和空间局部性，同时减少了计算资源消耗和推断时间，显著提升了低光图像增强的效果。", "motivation": "改进Mamba框架以更好地捕捉细节并提高空间局部性，同时优化计算资源使用和推断时间。", "method": "提出Hilbert Selective Scan机制提升扫描模式的豪斯多夫维度，改进Mamba框架。", "result": "在公开基准测试中展示了方法的有效性，提高了图像增强的质量和效率。", "conclusion": "该方法不仅增强了低光图像增强的效果，也为更广泛领域的Mamba技术应用提供了可能。"}}
{"id": "2510.25941", "categories": ["cs.CL", "I.2"], "pdf": "https://arxiv.org/pdf/2510.25941", "abs": "https://arxiv.org/abs/2510.25941", "authors": ["André V. Duarte", "Xuying li", "Bin Zeng", "Arlindo L. Oliveira", "Lei Li", "Zhuo Li"], "title": "RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline", "comment": null, "summary": "If we cannot inspect the training data of a large language model (LLM), how\ncan we ever know what it has seen? We believe the most compelling evidence\narises when the model itself freely reproduces the target content. As such, we\npropose RECAP, an agentic pipeline designed to elicit and verify memorized\ntraining data from LLM outputs. At the heart of RECAP is a feedback-driven\nloop, where an initial extraction attempt is evaluated by a secondary language\nmodel, which compares the output against a reference passage and identifies\ndiscrepancies. These are then translated into minimal correction hints, which\nare fed back into the target model to guide subsequent generations. In\naddition, to address alignment-induced refusals, RECAP includes a jailbreaking\nmodule that detects and overcomes such barriers. We evaluate RECAP on\nEchoTrace, a new benchmark spanning over 30 full books, and the results show\nthat RECAP leads to substantial gains over single-iteration approaches. For\ninstance, with GPT-4.1, the average ROUGE-L score for the copyrighted text\nextraction improved from 0.38 to 0.47 - a nearly 24% increase.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.26006", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26006", "abs": "https://arxiv.org/abs/2510.26006", "authors": ["Rishika Bhagwatkar", "Syrielle Montariol", "Angelika Romanou", "Beatriz Borges", "Irina Rish", "Antoine Bosselut"], "title": "CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments", "comment": null, "summary": "Humans can naturally identify, reason about, and explain anomalies in their\nenvironment. In computer vision, this long-standing challenge remains limited\nto industrial defects or unrealistic, synthetically generated anomalies,\nfailing to capture the richness and unpredictability of real-world anomalies.\nIn this work, we introduce CAVE, the first benchmark of real-world visual\nanomalies. CAVE supports three open-ended tasks: anomaly description,\nexplanation, and justification; with fine-grained annotations for visual\ngrounding and categorizing anomalies based on their visual manifestations,\ntheir complexity, severity, and commonness. These annotations draw inspiration\nfrom cognitive science research on how humans identify and resolve anomalies,\nproviding a comprehensive framework for evaluating Vision-Language Models\n(VLMs) in detecting and understanding anomalies. We show that state-of-the-art\nVLMs struggle with visual anomaly perception and commonsense reasoning, even\nwith advanced prompting strategies. By offering a realistic and cognitively\ngrounded benchmark, CAVE serves as a valuable resource for advancing research\nin anomaly detection and commonsense reasoning in VLMs.", "AI": {"tldr": "该论文提出了CAVE基准，专注于现实世界中视觉异常的检测与理解，揭示了现有模型在感知异常和常识推理上的不足。", "motivation": "现有的异常检测在计算机视觉领域局限于工业缺陷或合成生成的异常，无法捕捉现实世界异常的丰富性和不可预测性。这个基准测试旨在填补这一研究空白。", "method": "介绍了一个名为CAVE的新基准，专注于现实中视觉异常的描述、解释和证明三大开放式任务，并使用细粒度标注来辅助视觉定位和分类。", "result": "结果显示最先进的Vision-Language Models (VLMs)在感知视觉异常和进行常识推理方面存在问题，即便采用了高级的提示策略。", "conclusion": "通过提供一个现实且有认知基础的基准测试，CAVE作为推进VLMs在异常检测和常识推理方面研究的一个宝贵资源。"}}
{"id": "2510.25947", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25947", "abs": "https://arxiv.org/abs/2510.25947", "authors": ["Negar Foroutan", "Paul Teiletche", "Ayush Kumar Tarun", "Antoine Bosselut"], "title": "Revisiting Multilingual Data Mixtures in Language Model Pretraining", "comment": "Under Review", "summary": "The impact of different multilingual data mixtures in pretraining large\nlanguage models (LLMs) has been a topic of ongoing debate, often raising\nconcerns about potential trade-offs between language coverage and model\nperformance (i.e., the curse of multilinguality). In this work, we investigate\nthese assumptions by training 1.1B and 3B parameter LLMs on diverse\nmultilingual corpora, varying the number of languages from 25 to 400. Our study\nchallenges common beliefs surrounding multilingual training. First, we find\nthat combining English and multilingual data does not necessarily degrade the\nin-language performance of either group, provided that languages have a\nsufficient number of tokens included in the pretraining corpus. Second, we\nobserve that using English as a pivot language (i.e., a high-resource language\nthat serves as a catalyst for multilingual generalization) yields benefits\nacross language families, and contrary to expectations, selecting a pivot\nlanguage from within a specific family does not consistently improve\nperformance for languages within that family. Lastly, we do not observe a\nsignificant \"curse of multilinguality\" as the number of training languages\nincreases in models at this scale. Our findings suggest that multilingual data,\nwhen balanced appropriately, can enhance language model capabilities without\ncompromising performance, even in low-resource settings", "AI": {"tldr": "研究通过大量参数的语言模型及多种语言语料库的训练，挑战了多语言训练的常见假设，表明多语言数据在平衡得当的情况下可以提升模型性能。", "motivation": "研究旨在探讨多语言数据混合在预训练大型语言模型时语言覆盖范围与模型性能之间的潜在折衷问题，即所谓的'多语言诅咒'。", "method": "本研究通过训练参数规模为11亿和30亿的大型语言模型（LLMs），使用从25种到400种语言不同的多语言语料库，对多语言预训练的影响进行了调查。", "result": "研究结果表明，结合英语和其他多语言数据并不必然降低这些语言的性能，只要这些语言在预训练语料库中有足够的token。将英语作为中介语言可以给跨语言家族带来好处，并且选择某个特定家族的语言作为中介语言并不会一致地改善该家族语言的性能。随着训练语言数量的增加，模型并未显示出显著的'多语言诅咒'。", "conclusion": "研究发现，多语言数据，当适量平衡时，可以增强语言模型的能力而不损害性能，即使在低资源环境中也是如此。"}}
{"id": "2510.26017", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26017", "abs": "https://arxiv.org/abs/2510.26017", "authors": ["Bilal Hassan", "Areg Karapetyan", "Aaron Chung Hin Chow", "Samer Madanat"], "title": "Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning", "comment": "Submitted to Hydrology and Earth System Sciences", "summary": "Climate change and sea-level rise (SLR) pose escalating threats to coastal\ncities, intensifying the need for efficient and accurate methods to predict\npotential flood hazards. Traditional physics-based hydrodynamic simulators,\nalthough precise, are computationally expensive and impractical for city-scale\ncoastal planning applications. Deep Learning (DL) techniques offer promising\nalternatives, however, they are often constrained by challenges such as data\nscarcity and high-dimensional output requirements. Leveraging a recently\nproposed vision-based, low-resource DL framework, we develop a novel,\nlightweight Convolutional Neural Network (CNN)-based model designed to predict\ncoastal flooding under variable SLR projections and shoreline adaptation\nscenarios. Furthermore, we demonstrate the ability of the model to generalize\nacross diverse geographical contexts by utilizing datasets from two distinct\nregions: Abu Dhabi and San Francisco. Our findings demonstrate that the\nproposed model significantly outperforms state-of-the-art methods, reducing the\nmean absolute error (MAE) in predicted flood depth maps on average by nearly\n20%. These results highlight the potential of our approach to serve as a\nscalable and practical tool for coastal flood management, empowering\ndecision-makers to develop effective mitigation strategies in response to the\ngrowing impacts of climate change. Project Page: https://caspiannet.github.io/", "AI": {"tldr": "A lightweight CNN-based model for coastal flood prediction under various sea-level rise scenarios outperforms existing methods with a 20% reduction in mean absolute error.", "motivation": "Traditional methods for predicting flood hazards are too computationally expensive for practical city-scale planning. This paper aims to provide an efficient and accurate alternative using deep learning.", "method": "The paper introduces a vision-based, low-resource DL framework, leveraging CNN-based models to predict flooding under different sea-level rise and shoreline adaptation scenarios.", "result": "The model demonstrates superior performance in predicting flood depth, with a nearly 20% decrease in MAE compared to state-of-the-art methods, validated across datasets from Abu Dhabi and San Francisco.", "conclusion": "The proposed model is a scalable and practical tool for coastal flood management, offering a promising solution for decision-makers to implement effective climate change mitigation strategies."}}
{"id": "2510.25967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25967", "abs": "https://arxiv.org/abs/2510.25967", "authors": ["Mohsinul Kabir", "Tasnim Ahmed", "Md Mezbaur Rahman", "Polydoros Giannouris", "Sophia Ananiadou"], "title": "Semantic Label Drift in Cross-Cultural Translation", "comment": null, "summary": "Machine Translation (MT) is widely employed to address resource scarcity in\nlow-resource languages by generating synthetic data from high-resource\ncounterparts. While sentiment preservation in translation has long been\nstudied, a critical but underexplored factor is the role of cultural alignment\nbetween source and target languages. In this paper, we hypothesize that\nsemantic labels are drifted or altered during MT due to cultural divergence.\nThrough a series of experiments across culturally sensitive and neutral\ndomains, we establish three key findings: (1) MT systems, including modern\nLarge Language Models (LLMs), induce label drift during translation,\nparticularly in culturally sensitive domains; (2) unlike earlier statistical MT\ntools, LLMs encode cultural knowledge, and leveraging this knowledge can\namplify label drift; and (3) cultural similarity or dissimilarity between\nsource and target languages is a crucial determinant of label preservation. Our\nfindings highlight that neglecting cultural factors in MT not only undermines\nlabel fidelity but also risks misinterpretation and cultural conflict in\ndownstream applications.", "AI": {"tldr": "文章探讨了文化差异对机器翻译中语义标签漂移的影响，并指出文化因素的忽视会导致标签保真度下降和潜在的文化冲突。", "motivation": "探讨机器翻译过程中，文化差异对语义标签漂移的影响，以及这种影响对最终翻译质量和文化适应性的重要性。", "method": "通过在文化和非文化敏感领域进行一系列实验，比较MT系统（包括现代大型语言模型）在翻译过程中语义标签的漂移情况。", "result": "发现MT系统在文化和非文化敏感领域翻译时会诱导标签漂移，大型语言模型因编码了文化知识，反而加剧了这个问题。", "conclusion": "文化相似度影响标签的保真度，忽视文化因素会使得下游应用面临解释错误和文化冲突的风险。"}}
{"id": "2510.26027", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26027", "abs": "https://arxiv.org/abs/2510.26027", "authors": ["Ali Rasekh", "Erfan Bagheri Soula", "Omid Daliran", "Simon Gottschalk", "Mohsen Fayyaz"], "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders", "comment": "Accepted to NeurIPS 2025", "summary": "Despite significant advances in Multimodal Large Language Models (MLLMs),\nunderstanding complex temporal dynamics in videos remains a major challenge.\nOur experiments show that current Video Large Language Model (Video-LLM)\narchitectures have critical limitations in temporal understanding, struggling\nwith tasks that require detailed comprehension of action sequences and temporal\nprogression. In this work, we propose a Video-LLM architecture that introduces\nstacked temporal attention modules directly within the vision encoder. This\ndesign incorporates a temporal attention in vision encoder, enabling the model\nto better capture the progression of actions and the relationships between\nframes before passing visual tokens to the LLM. Our results show that this\napproach significantly improves temporal reasoning and outperforms existing\nmodels in video question answering tasks, specifically in action recognition.\nWe improve on benchmarks including VITATECS, MVBench, and Video-MME by up to\n+5.5%. By enhancing the vision encoder with temporal structure, we address a\ncritical gap in video understanding for Video-LLMs. Project page and code are\navailable at: https://alirasekh.github.io/STAVEQ2/.", "AI": {"tldr": "研究指出，新型Video-LLM通过增强时间注意力机制，在视频问答和动作识别任务上表现出色，提高了理解视频时间动态的能力。", "motivation": "当前的Video-LLM架构在理解视频中的时间动态方面存在局限性，特别是在需要详细理解动作序列和时间进展的任务上表现不佳。", "method": "本文提出了一种新的Video-LLM架构，通过在视觉编码器中引入堆叠的时间注意力模块，增强了对视频中时间动态的理解能力。", "result": "实验结果显示，本文的方法显著提升了时间推理能力，在视频问答任务上的表现优于现有模型，特别是在动作识别方面，提升了最多5.5%的性能。", "conclusion": "通过改进视觉编码器中的时间结构，本研究解决了Video-LLM在视频理解中的关键问题。"}}
{"id": "2510.25975", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.25975", "abs": "https://arxiv.org/abs/2510.25975", "authors": ["Sina Bagheri Nezhad", "Yao Li", "Ameeta Agrawal"], "title": "SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation", "comment": null, "summary": "Large Language Models (LLMs) often struggle with complex mathematical\nreasoning, where prose-based generation leads to unverified and arithmetically\nunsound solutions. Current prompting strategies like Chain of Thought still\noperate within this unreliable medium, lacking a mechanism for deterministic\nverification. To address these limitations, we introduce SymCode, a\nneurosymbolic framework that reframes mathematical problem-solving as a task of\nverifiable code generation using the SymPy library. We evaluate SymCode on\nchallenging benchmarks, including MATH-500 and OlympiadBench, demonstrating\nsignificant accuracy improvements of up to 13.6 percentage points over\nbaselines. Our analysis shows that SymCode is not only more token-efficient but\nalso fundamentally shifts model failures from opaque logical fallacies towards\ntransparent, programmatic errors. By grounding LLM reasoning in a deterministic\nsymbolic engine, SymCode represents a key step towards more accurate and\ntrustworthy AI in formal domains.", "AI": {"tldr": "SymCode框架将数学问题求解视为使用SymPy库的可验证代码生成任务，显著提高了准确率，提升了AI在正式领域的准确性和可信度。", "motivation": "大型语言模型在数学推理方面往往表现不佳，其基于文本的生成方式常常导致未验证且算术上或对不对的答案。", "method": "引入了一个名为SymCode的神经符号框架，该框架将数学问题求解重新定义为使用SymPy库的可验证代码生成任务。", "result": "在MATH-500和OlympiadBench等具有挑战性的基准测试上，SymCode将准确率提高了最多13.6个百分点，比基线模型更高效。", "conclusion": "通过将大型语言模型的推理基于一个确定性的符号引擎，SymCode代表了更准确和可信赖的AI系统的一个关键步骤，特别是在正式的领域中。"}}
{"id": "2510.26049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26049", "abs": "https://arxiv.org/abs/2510.26049", "authors": ["Yuyue Zhou", "Jessica Knight", "Shrimanti Ghosh", "Banafshe Felfeliyan", "Jacob L. Jaremko", "Abhilash R. Hareendranathan"], "title": "FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation", "comment": null, "summary": "Elbow and wrist fractures are the most common fractures in pediatric\npopulations. Automatic segmentation of musculoskeletal structures in ultrasound\n(US) can improve diagnostic accuracy and treatment planning. Fractures appear\nas cortical defects but require expert interpretation. Deep learning (DL) can\nprovide real-time feedback and highlight key structures, helping lightly\ntrained users perform exams more confidently. However, pixel-wise expert\nannotations for training remain time-consuming and costly. To address this\nchallenge, we propose FlexICL, a novel and flexible in-context learning (ICL)\nframework for segmenting bony regions in US images. We apply it to an\nintra-video segmentation setting, where experts annotate only a small subset of\nframes, and the model segments unseen frames. We systematically investigate\nvarious image concatenation techniques and training strategies for visual ICL\nand introduce novel concatenation methods that significantly enhance model\nperformance with limited labeled data. By integrating multiple augmentation\nstrategies, FlexICL achieves robust segmentation performance across four wrist\nand elbow US datasets while requiring only 5% of the training images. It\noutperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and\nconventional segmentation models like U-Net and TransUNet by 1-27% Dice\ncoefficient on 1,252 US sweeps. These initial results highlight the potential\nof FlexICL as an efficient and scalable solution for US image segmentation well\nsuited for medical imaging use cases where labeled data is scarce.", "AI": {"tldr": "研究了全新的上下文学习框架FlexICL，通过少量标注样本实现超声图像的高效分割，显著提高儿童骨折诊断的准确性，减少了手动标注的需求。", "motivation": "儿童群体中肘部和腕部骨折是最常见的骨折类型。通过自动分割肌肉骨骼结构，超声诊断的准确性及治疗规划均可得到提升。但是，获取用于训练的像素级专家标注数据耗时费力。因此，此研究旨在通过减少标注需求来提高诊断效率并降低成本。", "method": "提出了一种新的灵活的上下文学习框架FlexICL，用于在超声图像中分割骨骼区域。此框架应用于视频内分割环境中，其中专家仅标注一小部分帧，而模型能够分割未见帧。研究了多种图像拼接技术和训练策略，并引入新的拼接方法，显著提升了模型在有限标签数据下的表现。通过集成多种增强策略，FlexICL仅需5%的训练图像即可实现稳健的分割性能。", "result": "FlexICL在四个腕部和肘部超声数据集中实现了比最先进的视觉ICL模型（Painter, MAE-VQGAN）和传统分割模型（U-Net, TransUNet）更高的分割性能，Dice系数提高了1-27%，基于1,252次超声扫描结果。", "conclusion": "FlexICL作为超声图像分割的一种有效和可扩展的解决方案，特别是在标记数据稀缺的医学成像使用案例中展现出巨大的潜力。"}}
{"id": "2510.25977", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.25977", "abs": "https://arxiv.org/abs/2510.25977", "authors": ["Dinghong Song", "Jierui Xu", "Weichu Yang", "Pengfei Su", "Dong Li"], "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium", "comment": "12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)", "summary": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.", "AI": {"tldr": "本文通过优化矩阵乘法运算，提升了基于Trainium架构的大规模语言模型推理性能，获得了显著的速度提升。", "motivation": "通过优化矩阵乘法运算，降低数据搬运消耗、最大化SRAM带宽、避免矩阵转置操作，从而提高大规模语言模型在Trainium架构上的训练和推理性能。", "method": "研究设计了一种高效的矩阵乘法（matmul）算法，采用了核函数融合和创新的缓存策略，减少数据在内存层次之间的搬移，最大化SRAM带宽，并避免昂贵的矩阵转置操作。", "result": "本文通过设计适用于Trainium的高效矩阵乘法运算，显著提升了大规模语言模型（LLM）的推理性能，相较于AWS现有的方法，矩阵乘法运算的平均加速比达到1.35倍，最高可达2.22倍，最终使得整个LLM推理过程的平均加速比达到1.66倍，最高加速比为2.49倍。", "conclusion": "通过针对Trainium架构优化矩阵乘法算法，该研究显著提升了大规模语言模型推理的性能，展示了在特定硬件上优化算法的重要作用。"}}
{"id": "2510.26052", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26052", "abs": "https://arxiv.org/abs/2510.26052", "authors": ["Hoyeon Chang", "Seungjin Kim", "Yoonseok Choi"], "title": "Dynamic VLM-Guided Negative Prompting for Diffusion Models", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: The First Workshop on Generative and Protective AI for\n  Content Creation", "summary": "We propose a novel approach for dynamic negative prompting in diffusion\nmodels that leverages Vision-Language Models (VLMs) to adaptively generate\nnegative prompts during the denoising process. Unlike traditional Negative\nPrompting methods that use fixed negative prompts, our method generates\nintermediate image predictions at specific denoising steps and queries a VLM to\nproduce contextually appropriate negative prompts. We evaluate our approach on\nvarious benchmark datasets and demonstrate the trade-offs between negative\nguidance strength and text-image alignment.", "AI": {"tldr": "本文提出了一种创新的动态负提示方法，利用视觉-语言模型在扩散模型中自动生成适应性负提示，以改善图像生成的质量和文本-图像对齐。", "motivation": "为了提高扩散模型在生成图像时的灵活性和准确性，我们引入了一种新的方法来动态生成负提示，从而在去噪过程中提供更精细的控制。", "method": "我们提出了一种在扩散模型中使用视觉-语言模型（VLM）自适应地生成负提示的新型动态负提示方法。与传统的使用固定负提示的负提示方法不同，我们在特定的去噪步骤中生成中间图像预测，并查询VLM以产生与上下文相关的负提示。", "result": "我们在各种基准数据集上评估了我们的方法，并展示了负提示强度和文本-图像对齐之间的权衡。", "conclusion": "实验表明，这种方法可以在保持图像质量的同时，通过调整负提示的强度来改善文本-图像对齐的效果。"}}
{"id": "2510.25979", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25979", "abs": "https://arxiv.org/abs/2510.25979", "authors": ["Dinghong Song", "Yuan Feng", "Yiwei Wang", "Shangye Chen", "Cyril Guyot", "Filip Blagojevic", "Hyeran Jeon", "Pengfei Su", "Dong Li"], "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache", "comment": "10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)", "summary": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.", "AI": {"tldr": "AttnCache accelerates the prefill stage of LLM inference by caching and reusing attention maps, achieving notable speedups on both CPU and GPU with minimal accuracy trade-off.", "motivation": "To address the performance bottleneck caused by self-attention computation in the prefill stage of inference for large language models.", "method": "AttnCache, a framework utilizing an attention map memorization database to cache and reuse attention maps during inference.", "result": "AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.", "conclusion": "The proposed AttnCache framework effectively accelerates the prefill stage of LLM inference by reusing pre-cached attention maps, significantly improving performance without compromising accuracy."}}
{"id": "2510.26105", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26105", "abs": "https://arxiv.org/abs/2510.26105", "authors": ["Xiaosen Wang", "Zhijin Ge", "Shaokang Wang"], "title": "Security Risk of Misalignment between Text and Image in Multi-modal Model", "comment": null, "summary": "Despite the notable advancements and versatility of multi-modal diffusion\nmodels, such as text-to-image models, their susceptibility to adversarial\ninputs remains underexplored. Contrary to expectations, our investigations\nreveal that the alignment between textual and Image modalities in existing\ndiffusion models is inadequate. This misalignment presents significant risks,\nespecially in the generation of inappropriate or Not-Safe-For-Work (NSFW)\ncontent. To this end, we propose a novel attack called Prompt-Restricted\nMulti-modal Attack (PReMA) to manipulate the generated content by modifying the\ninput image in conjunction with any specified prompt, without altering the\nprompt itself. PReMA is the first attack that manipulates model outputs by\nsolely creating adversarial images, distinguishing itself from prior methods\nthat primarily generate adversarial prompts to produce NSFW content.\nConsequently, PReMA poses a novel threat to the integrity of multi-modal\ndiffusion models, particularly in image-editing applications that operate with\nfixed prompts. Comprehensive evaluations conducted on image inpainting and\nstyle transfer tasks across various models confirm the potent efficacy of\nPReMA.", "AI": {"tldr": "研究揭示了多模态扩散模型（如文本到图像模型）易受到对抗性输入的影响，并提出了一种新型攻击方法PReMA，通过修改输入图像来操纵生成内容，无需改变提示词，并证明了其在图像修复和风格转换任务中的有效性。", "motivation": "现有的多模态扩散模型在面对对抗性输入时的脆弱性未被充分探索，尤其是文本与图像模态对齐不足可能导致生成不适当或NSFW内容，威胁模型完整性。", "method": "Structure", "result": "PReMA攻击首次通过创建对抗性图像来操纵生成内容，证明了其在图像修复和风格转换任务中的高效性。", "conclusion": "PReMA作为一种新的威胁形式，对多模态扩散模型的完整性和固定提示词下的图像编辑应用构成了新的挑战。"}}
{"id": "2510.25992", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25992", "abs": "https://arxiv.org/abs/2510.25992", "authors": ["Yihe Deng", "I-Hung Hsu", "Jun Yan", "Zifeng Wang", "Rujun Han", "Gufeng Zhang", "Yanfei Chen", "Wei Wang", "Tomas Pfister", "Chen-Yu Lee"], "title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning", "comment": null, "summary": "Large Language Models (LLMs) often struggle with problems that require\nmulti-step reasoning. For small-scale open-source models, Reinforcement\nLearning with Verifiable Rewards (RLVR) fails when correct solutions are rarely\nsampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to\noverfit long demonstrations through rigid token-by-token imitation. To address\nthis gap, we propose Supervised Reinforcement Learning (SRL), a framework that\nreformulates problem solving as generating a sequence of logical \"actions\". SRL\ntrains the model to generate an internal reasoning monologue before committing\nto each action. It provides smoother rewards based on the similarity between\nthe model's actions and expert actions extracted from the SFT dataset in a\nstep-wise manner. This supervision offers richer learning signals even when all\nrollouts are incorrect, while encouraging flexible reasoning guided by expert\ndemonstrations. As a result, SRL enables small models to learn challenging\nproblems previously unlearnable by SFT or RLVR. Moreover, initializing training\nwith SRL before refining with RLVR yields the strongest overall performance.\nBeyond reasoning benchmarks, SRL generalizes effectively to agentic software\nengineering tasks, establishing it as a robust and versatile training framework\nfor reasoning-oriented LLMs.", "AI": {"tldr": "提出了SRL框架，通过创新的训练方法解决多步推理问题，适用于小型开源模型，增强了其处理复杂问题的能力。", "motivation": "解决大语言模型在多步推理问题上的困难，特别是在小型开源模型中，强化学习（RLVR）在正确解法很难被采样的情况下失败，而监督微调（SFT）倾向于过度拟合到长的演示，通过僵化的按令牌模仿来实现。", "method": "提出了监督强化学习（SRL）框架，此框架将问题解决重新定义为生成逻辑“动作”序列的任务。SRL训练模型在提交每个动作之前先生成一种内部理性独白，根据从SFT数据集中按步骤提取的专家动作与模型动作的相似度提供平滑奖励。", "result": "该框架通过将问题解决重新定义为生成一系列逻辑“动作”的方式，使得模型可以生成一段内部推理独白，然后再执行每个动作。这种训练方法提供了基于模型动作与从SFT数据集中提取的专家动作相似度的平滑奖励，即便所有尝试都是错误的情况下也能提供丰富的学习信号，同时鼓励了灵活的推理，使小模型能够学习到以前无法学习的挑战性问题。初始训练使用SRL，并用RLVR进行优化，可以获得最佳的整体性能。此外，SRL在代理软件工程任务上表现良好，证明了它作为一个强大的和多功能的训练框架的实用性，适用于侧重推理的大型语言模型。", "conclusion": "SRL框架不仅能够让小型模型学习到从前无法学到的困难问题，还展示了在代理软件工程任务中的有效推广。初步使用SRL训练，接着通过RLVR进行优化，可以获得最佳的整体性能。"}}
{"id": "2510.26113", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26113", "abs": "https://arxiv.org/abs/2510.26113", "authors": ["Minjoon Jung", "Junbin Xiao", "Junghyun Kim", "Byoung-Tak Zhang", "Angela Yao"], "title": "EgoExo-Con: Exploring View-Invariant Video Temporal Understanding", "comment": "project page:\n  \\url{https://minjoong507.github.io/projects/EgoExo-Con/}", "summary": "Can Video-LLMs achieve consistent temporal understanding when videos capture\nthe same event from different viewpoints? To study this, we introduce\nEgoExo-Con (Consistency), a benchmark of comprehensively synchronized\negocentric and exocentric video pairs with human-refined queries in natural\nlanguage. EgoExo-Con emphasizes two temporal understanding tasks: Temporal\nVerification and Temporal Grounding. It evaluates not only correctness but\nconsistency across viewpoints. Our analysis reveals two critical limitations of\nexisting Video-LLMs: (1) models often fail to maintain consistency, with\nresults far worse than their single-view performances. (2) When naively\nfinetuned with synchronized videos of both viewpoints, the models show improved\nconsistency but often underperform those trained on a single view. For\nimprovements, we propose View-GRPO, a novel reinforcement learning framework\nthat effectively strengthens view-specific temporal reasoning while encouraging\nconsistent comprehension across viewpoints. Our method demonstrates its\nsuperiority over naive SFT and GRPO, especially for improving cross-view\nconsistency. All resources will be made publicly available.", "AI": {"tldr": "研究视频大模型在不同视角视频中的一致时间理解能力，提出了一种新的强化学习框架，提高了跨视角的一致性。", "motivation": "为了研究视频大模型在从不同视角捕捉同一事件的视频时能否实现一致的时间理解，引入了EgoExo-Con这一基准。", "method": "提出了一种新的强化学习框架View-GRPO，旨在增强针对不同视角的时间推理能力，并鼓励跨视角的一致性理解。", "result": "分析揭示了现有视频大模型的两个关键限制：难以保持一致性；以端到端的方式使用同步视频进行微调时，虽然一致性有所提高，但在某些情况下性能不如仅在一个视角上进行训练。", "conclusion": "View-GRPO方法相对于简单的SFT和GRPO方法表现出了优势，特别是在改善跨视角一致性方面，并且所有资源将公开提供。"}}
{"id": "2510.26020", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26020", "abs": "https://arxiv.org/abs/2510.26020", "authors": ["Feijie Wu", "Weiwu Zhu", "Yuxiang Zhang", "Soumya Chatterjee", "Jiarong Zhu", "Fan Mo", "Rodin Luo", "Jing Gao"], "title": "PORTool: Tool-Use LLM Training with Rewarded Tree", "comment": null, "summary": "Current tool-use large language models (LLMs) are trained on static datasets,\nenabling them to interact with external tools and perform multi-step,\ntool-integrated reasoning, which produces tool-call trajectories. However,\nthese models imitate how a query is resolved in a generic tool-call routine,\nthereby failing to explore possible solutions and demonstrating limited\nperformance in an evolved, dynamic tool-call environment. In this work, we\npropose PORTool, a reinforcement learning (RL) method that encourages a\ntool-use LLM to explore various trajectories yielding the correct answer.\nSpecifically, this method starts with generating multiple rollouts for a given\nquery, and some of them share the first few tool-call steps, thereby forming a\ntree-like structure. Next, we assign rewards to each step, based on its ability\nto produce a correct answer and make successful tool calls. A shared step\nacross different trajectories receives the same reward, while different steps\nunder the same fork receive different rewards. Finally, these step-wise rewards\nare used to calculate fork-relative advantages, blended with\ntrajectory-relative advantages, to train the LLM for tool use. The experiments\nutilize 17 tools to address user queries, covering both time-sensitive and\ntime-invariant topics. We conduct ablation studies to systematically justify\nthe necessity and the design robustness of step-wise rewards. Furthermore, we\ncompare the proposed PORTool with other training approaches and demonstrate\nsignificant improvements in final accuracy and the number of tool-call steps.", "AI": {"tldr": "PORTool, a reinforcement learning method, enhances LLMs' tool-use capabilities in dynamic environments by encouraging exploration of diverse solution trajectories, leading to improved accuracy and efficiency.", "motivation": "The motivation behind this work is to address the limitations of current tool-use LLMs, which are trained on static datasets and thus fail to explore various solutions in a dynamic tool-call environment.", "method": "This paper proposes PORTool, a reinforcement learning method that aims to enhance the performance of tool-use LLMs by encouraging them to explore a variety of trajectories leading to correct answers. The method involves generating multiple rollouts for a given query, assigning step-wise rewards based on the success of tool calls and the production of correct answers, and training the LLM using these rewards.", "result": "Experiments using PORTool with 17 tools to address user queries demonstrate improvements in final accuracy and the number of tool-call steps compared to other training approaches.", "conclusion": "The study concludes that PORTool effectively improves the performance of tool-use LLMs by providing a mechanism to explore and optimize various solution paths."}}
{"id": "2510.26114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26114", "abs": "https://arxiv.org/abs/2510.26114", "authors": ["Caoshuo Li", "Zengmao Ding", "Xiaobin Hu", "Bang Li", "Donghao Luo", "Xu Peng", "Taisong Jin", "Yongge Liu", "Shengwei Han", "Jing Yang", "Xiaoping He", "Feng Gao", "AndyPian Wu", "SevenShu", "Chaoyang Wang", "Chengjie Wang"], "title": "OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research", "comment": null, "summary": "As one of the earliest writing systems, Oracle Bone Script (OBS) preserves\nthe cultural and intellectual heritage of ancient civilizations. However,\ncurrent OBS research faces two major challenges: (1) the interpretation of OBS\ninvolves a complex workflow comprising multiple serial and parallel sub-tasks,\nand (2) the efficiency of OBS information organization and retrieval remains a\ncritical bottleneck, as scholars often spend substantial effort searching for,\ncompiling, and managing relevant resources. To address these challenges, we\npresent OracleAgent, the first agent system designed for the structured\nmanagement and retrieval of OBS-related information. OracleAgent seamlessly\nintegrates multiple OBS analysis tools, empowered by large language models\n(LLMs), and can flexibly orchestrate these components. Additionally, we\nconstruct a comprehensive domain-specific multimodal knowledge base for OBS,\nwhich is built through a rigorous multi-year process of data collection,\ncleaning, and expert annotation. The knowledge base comprises over 1.4M\nsingle-character rubbing images and 80K interpretation texts. OracleAgent\nleverages this resource through its multimodal tools to assist experts in\nretrieval tasks of character, document, interpretation text, and rubbing image.\nExtensive experiments demonstrate that OracleAgent achieves superior\nperformance across a range of multimodal reasoning and generation tasks,\nsurpassing leading mainstream multimodal large language models (MLLMs) (e.g.,\nGPT-4o). Furthermore, our case study illustrates that OracleAgent can\neffectively assist domain experts, significantly reducing the time cost of OBS\nresearch. These results highlight OracleAgent as a significant step toward the\npractical deployment of OBS-assisted research and automated interpretation\nsystems.", "AI": {"tldr": "OracleAgent是首个针对甲骨文信息管理与检索设计的智能系统，集成了多种分析工具并能够利用大型语言模型来协同工作，构建了包含丰富资源的多模态知识库。实验表明其在多模态推理与生成任务中表现出色。", "motivation": "针对甲骨文释读中存在的复杂工作流程以及信息组织和检索效率低下的问题，设计了OracleAgent系统，以便更好地管理、检索与甲骨文相关的信息。", "method": "设计了一个名为OracleAgent的智能系统，该系统集成了多个甲骨文分析工具，并利用大型语言模型(如LLMs)灵活地协同这些工具。此外，构建了一个全面的专用多模态知识库，包含了超过140万个单字拓片图像和80000个释文文本，以支持OracleAgent的检索任务。", "result": "实验显示，OracleAgent在多模态推理和生成任务中表现出色，超越了现有的多模态大型语言模型如GPT-4，同时通过案例研究验证了其能显著减少甲骨文研究的时间成本。", "conclusion": "OracleAgent为辅助甲骨文研究和自动释读系统的实际部署迈出了重要一步，展示了在未来研究中的潜力。"}}
{"id": "2510.26024", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26024", "abs": "https://arxiv.org/abs/2510.26024", "authors": ["HyoJung Han", "Sweta Agrawal", "Eleftheria Briakou"], "title": "Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs", "comment": null, "summary": "Cross-lingual alignment (CLA) aims to align multilingual representations,\nenabling Large Language Models (LLMs) to seamlessly transfer knowledge across\nlanguages. While intuitive, we hypothesize, this pursuit of representational\nconvergence can inadvertently cause \"cultural erasure\", the functional loss of\nproviding culturally-situated responses that should diverge based on the query\nlanguage. In this work, we systematically analyze this trade-off by introducing\na holistic evaluation framework, the transfer-localization plane, which\nquantifies both desirable knowledge transfer and undesirable cultural erasure.\nUsing this framework, we re-evaluate recent CLA approaches and find that they\nconsistently improve factual transfer at the direct cost of cultural\nlocalization across all six languages studied. Our investigation into the\ninternal representations of these models reveals a key insight: universal\nfactual transfer and culturally-specific knowledge are optimally steerable at\ndifferent model layers. Based on this finding, we propose Surgical Steering, a\nnovel inference-time method that disentangles these two objectives. By applying\ntargeted activation steering to distinct layers, our approach achieves a better\nbalance between the two competing dimensions, effectively overcoming the\nlimitations of current alignment techniques.", "AI": {"tldr": "本研究发现跨语言对齐虽提升了事实知识的跨语言传输，却可能导致文化响应的本地化丧失。提出一种新方法——手术导向，可以在不同网络层调节事实传输和文化知识的平衡。", "motivation": "研究动机在于质疑跨语言对齐（CLA）在追求表征收敛的同时是否存在“文化擦除”的风险，即丧失提供基于查询语言的文化特定响应的能力。", "method": "本研究提出了一个全面的评估框架——传输-本地化平面，该框架量化了知识传输的理想效果和文化擦除的不良影响。通过这个框架，作者重新评估了最近的跨语言对齐方法，并发现这些方法在改善事实性知识传输的同时，会以文化本地化为代价。研究还揭示了普遍事实传输和文化特定知识在模型的不同层中是最优可调节的。基于这一发现，提出了手术导向方法，该方法通过在不同的层上应用定向激活引导，实现了两个竞争维度之间的更好平衡。", "result": "使用传输-本地化平面评估框架，作者发现所研究的六种语言中，最近的跨语言对齐方法普遍提高了事实传输知识，但以文化本地化为代价。", "conclusion": "提出了一种新颖的推理时间方法——手术导向，该方法通过在不同层应用定向激活引导，可以更好地平衡事实知识传输和文化本地化的竞争需求，克服了现有对齐技术的局限性。"}}
{"id": "2510.26117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26117", "abs": "https://arxiv.org/abs/2510.26117", "authors": ["Yuxuan Li", "Tao Wang", "Xianben Yang"], "title": "JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting", "comment": null, "summary": "Traditional novel view synthesis methods heavily rely on external camera pose\nestimation tools such as COLMAP, which often introduce computational\nbottlenecks and propagate errors. To address these challenges, we propose a\nunified framework that jointly optimizes 3D Gaussian points and camera poses\nwithout requiring pre-calibrated inputs. Our approach iteratively refines 3D\nGaussian parameters and updates camera poses through a novel co-optimization\nstrategy, ensuring simultaneous improvements in scene reconstruction fidelity\nand pose accuracy. The key innovation lies in decoupling the joint optimization\ninto two interleaved phases: first, updating 3D Gaussian parameters via\ndifferentiable rendering with fixed poses, and second, refining camera poses\nusing a customized 3D optical flow algorithm that incorporates geometric and\nphotometric constraints. This formulation progressively reduces projection\nerrors, particularly in challenging scenarios with large viewpoint variations\nand sparse feature distributions, where traditional methods struggle. Extensive\nevaluations on multiple datasets demonstrate that our approach significantly\noutperforms existing COLMAP-free techniques in reconstruction quality, and also\nsurpasses the standard COLMAP-based baseline in general.", "AI": {"tldr": "提出了一种新型框架用于改进传统视图合成方法，无需外部相机姿态估计工具，解决了计算瓶颈和错误传播。", "motivation": "传统的视图合成方法依赖于外部相机姿态估计工具，如COLMAP，这些工具往往引入计算瓶颈和错误传播。", "method": "该论文提出了一种统一的框架，该框架能够同时优化3D高斯点和相机姿态，而不需要预校准的输入。方法包括两个交替的过程：1) 使用固定姿态的可微渲染来更新3D高斯参数；2) 使用结合了几何和辐射约束的定制3D光流算法来细化相机姿态。", "result": "实验结果表明，该方法在重建质量上显著优于现有的无需COLMAP的方法，甚至超过了基于COLMAP的标准方法。", "conclusion": "该框架通过解耦联优化策略，分别对3D高斯参数进行可微渲染更新，随后使用定制的3D光流算法更新相机姿态，从而提高重建的准确性和姿态的精确性，这些问题在大的视角变化和稀疏特征分布场景中尤其突出。"}}
{"id": "2510.26032", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26032", "abs": "https://arxiv.org/abs/2510.26032", "authors": ["Felipe Larios", "Mariana Borras-Osorio", "Yuqi Wu", "Ana Gabriela Claros", "David Toro-Tobon", "Esteban Cabezas", "Ricardo Loor-Torres", "Maria Mateo Chavez", "Kerly Guevara Maldonado", "Luis Vilatuna Andrango", "Maria Lizarazo Jimenez", "Ivan Mateo Alzamora", "Misk Al Zahidy", "Marcelo Montero", "Ana Cristina Proano", "Cristian Soto Jacome", "Jungwei W. Fan", "Oscar J. Ponce-Ponte", "Megan E. Branda", "Naykky Singh Ospina", "Juan P. Brito"], "title": "Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings", "comment": null, "summary": "Importance Incidental thyroid findings (ITFs) are increasingly detected on\nimaging performed for non-thyroid indications. Their prevalence, features, and\nclinical consequences remain undefined. Objective To develop, validate, and\ndeploy a natural language processing (NLP) pipeline to identify ITFs in\nradiology reports and assess their prevalence, features, and clinical outcomes.\nDesign, Setting, and Participants Retrospective cohort of adults without prior\nthyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from\nJuly 1, 2017, to September 30, 2023. A transformer-based NLP pipeline\nidentified ITFs and extracted nodule characteristics from image reports from\nmultiple modalities and body regions. Main Outcomes and Measures Prevalence of\nITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer\ndiagnosis. Logistic regression identified demographic and imaging-related\nfactors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%\nwomen), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more\nlikely in women, older adults, those with higher BMI, and when imaging was\nordered by oncology or internal medicine. Compared with chest CT, ITFs were\nmore likely via neck CT, PET, and nuclear medicine scans. Nodule\ncharacteristics were poorly documented, with size reported in 44% and other\nfeatures in fewer than 15% (e.g. calcifications). Compared with patients\nwithout ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,\nbiopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were\npapillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were\ncommon and strongly associated with cascades leading to the detection of small,\nlow-risk cancers. These findings underscore the role of ITFs in thyroid cancer\noverdiagnosis and the need for standardized reporting and more selective\nfollow-up.", "AI": {"tldr": "基于一种NLP管道开发、验证并部署以识别放射学报告中的非预期甲状腺发现（ITFs），结果显示ITFs发生率为7.8%，更常见于特定人群和成像方式中，并且与甲状腺结节、活检、手术和癌症的诊断有关，特别是之后检测到的癌症往往较大。这突显了规范化报告和选择性随访的必要性。", "motivation": "研究旨在评估非预期甲状腺发现（ITFs）在成人放射学报告中的发生率、特征和临床后果，探索开发和应用自然语言处理（NLP）技术在提升甲状腺疾病诊断与管理中的作用。", "method": "本研究采用一种基于变压器的自然语言处理（NLP）技术来识别放射学报告中的ITFs。研究对象是在2017年7月1日至2023年9月30日期间在Mayo Clinic接受甲状腺捕获成像的成人患者。NLP管道用于提取结节特征。", "result": "本次研究通过开发并验证基于变压器的自然语言处理（NLP）管道，识别放射学报告中的非预期甲状腺发现（ITFs），并分析其发生率、特征和临床后果。研究在无既往甲状腺疾病的成人患者中进行，通过不同成像方式发现ITFs的总体发生率为7.8%。ITFs常在女性、年龄较大、BMI较高以及由肿瘤学或内科开立影像检查的患者中出现。ITFs还被发现在颈部CT、PET和核医学扫描中的可能性更高。与其他患者相比，有ITFs的患者更有可能被诊断为甲状腺结节、进行活检、甲状腺手术和甲状腺癌诊断，特别是乳头状癌。这些发现强调了ITFs在甲状腺癌过度诊断中的作用，呼吁标准化报告和更有选择性的随访。", "conclusion": "非预期甲状腺发现（ITFs）的现象普遍，并与甲状腺疾病的诊断和治疗途径紧密相关。这些发现提示了ITFs在甲状腺癌过度诊断中的角色，强调了标准化报告和更审慎的随访措施的必要性。"}}
{"id": "2510.26125", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26125", "abs": "https://arxiv.org/abs/2510.26125", "authors": ["Runsheng Xu", "Hubert Lin", "Wonseok Jeon", "Hao Feng", "Yuliang Zou", "Liting Sun", "John Gorman", "Kate Tolstaya", "Sarah Tang", "Brandyn White", "Ben Sapp", "Mingxing Tan", "Jyh-Jing Hwang", "Drago Anguelov"], "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios", "comment": null, "summary": "Vision-based end-to-end (E2E) driving has garnered significant interest in\nthe research community due to its scalability and synergy with multimodal large\nlanguage models (MLLMs). However, current E2E driving benchmarks primarily\nfeature nominal scenarios, failing to adequately test the true potential of\nthese systems. Furthermore, existing open-loop evaluation metrics often fall\nshort in capturing the multi-modal nature of driving or effectively evaluating\nperformance in long-tail scenarios. To address these gaps, we introduce the\nWaymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021\ndriving segments (approximately 12 hours), specifically curated for challenging\nlong-tail scenarios that that are rare in daily life with an occurring\nfrequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the\nhigh-level routing information, ego states, and 360-degree camera views from 8\nsurrounding cameras. To evaluate the E2E driving performance on these long-tail\nsituations, we propose a novel open-loop evaluation metric: Rater Feedback\nScore (RFS). Unlike conventional metrics that measure the distance between\npredicted way points and the logs, RFS measures how closely the predicted\ntrajectory matches rater-annotated trajectory preference labels. We have\nreleased rater preference labels for all WOD-E2E validation set segments, while\nthe held out test set labels have been used for the 2025 WOD-E2E Challenge.\nThrough our work, we aim to foster state of the art research into\ngeneralizable, robust, and safe end-to-end autonomous driving agents capable of\nhandling complex real-world situations.", "AI": {"tldr": "本文提出WOD-E2E数据集和RFS评估指标，以解决当前E2E自动驾驶基准和评估指标的问题，推动自动驾驶研究。", "motivation": "针对现有E2E驾驶基准测试主要是名义场景和开放环评估指标不完善的缺点，旨在促进能处理复杂现实情况的E2E自动驾驶研究。", "method": "介绍Waymo Open Dataset for End-to-End Driving (WOD-E2E)，包含4,021个驾驶片段，专注于罕见的长尾场景，提供高精度路由信息、车辆状态和8个周围摄像头的360度视角。提出Rater Feedback Score (RFS)用于评估E2E驾驶性能，该指标衡量预测轨迹与标注轨迹的匹配程度。", "result": "开发了WOD-E2E数据集和RFS评估指标，用于释放自动化驾驶在长尾场景中的潜力。", "conclusion": "通过WOD-E2E数据集和RFS评估指标，促进E2E自动驾驶研究的发展，使其更通用、更稳健、更安全。"}}
{"id": "2510.26101", "categories": ["cs.CL", "cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.26101", "abs": "https://arxiv.org/abs/2510.26101", "authors": ["Taku Mikuriya", "Tatsuya Ishigaki", "Masayuki Kawarada", "Shunya Minami", "Tadashi Kadowaki", "Yohichi Suzuki", "Soshun Naito", "Shunya Takata", "Takumi Kato", "Tamotsu Basseda", "Reo Yamada", "Hiroya Takamura"], "title": "QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback", "comment": null, "summary": "Large language models (LLMs) have increasingly been applied to automatic\nprogramming code generation. This task can be viewed as a language generation\ntask that bridges natural language, human knowledge, and programming logic.\nHowever, it remains underexplored in domains that require interaction with\nhardware devices, such as quantum programming, where human coders write Python\ncode that is executed on a quantum computer. To address this gap, we introduce\nQCoder Benchmark, an evaluation framework that assesses LLMs on quantum\nprogramming with feedback from simulated hardware devices. Our benchmark offers\ntwo key features. First, it supports evaluation using a quantum simulator\nenvironment beyond conventional Python execution, allowing feedback of\ndomain-specific metrics such as circuit depth, execution time, and error\nclassification, which can be used to guide better generation. Second, it\nincorporates human-written code submissions collected from real programming\ncontests, enabling both quantitative comparisons and qualitative analyses of\nLLM outputs against human-written codes. Our experiments reveal that even\nadvanced models like GPT-4o achieve only around 18.97% accuracy, highlighting\nthe difficulty of the benchmark. In contrast, reasoning-based models such as o3\nreach up to 78% accuracy, outperforming averaged success rates of human-written\ncodes (39.98%). We release the QCoder Benchmark dataset and public evaluation\nAPI to support further research.", "AI": {"tldr": "本文介绍QCoder基准测试框架，用于评估大语言模型在量子编程中的表现，并发现尽管GPT-4的准确率只有18.97%，但基于推理的模型如o3可达到78%的准确率，超过了人类编写的代码平均成功率。", "motivation": "目前，大语言模型在需要与硬件设备交互的领域，如量子编程中应用较少。本研究旨在填补这一空白，研究大语言模型在量子编程中的应用。", "method": "通过引入QCoder基准测试框架来评估大语言模型在量子编程中的表现，该框架使用量子模拟器环境进行反馈评估，并结合来自真实编程比赛的人类编写的代码样本来进行定量和定性分析。", "result": "实验结果显示，即使像GPT-4这样的先进模型也仅能达到约18.97%的准确率；而基于推理的模型如o3则能达到78%的准确率，超过了人类编写的代码平均成功率（39.98%）。", "conclusion": "研究发现量子编程仍是一个挑战性高的领域，但基于推理的模型具有更好的性能。发布的QCoder基准测试集和公共评估API将支持进一步研究。"}}
{"id": "2510.26131", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26131", "abs": "https://arxiv.org/abs/2510.26131", "authors": ["Ali Caglayan", "Nevrez Imamoglu", "Oguzhan Guclu", "Ali Osman Serhatoglu", "Ahmet Burak Can", "Ryosuke Nakamura"], "title": "Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM", "comment": "double-column 5 pages, 3 figures", "summary": "Attention models have recently emerged as a powerful approach, demonstrating\nsignificant progress in various fields. Visualization techniques, such as class\nactivation mapping, provide visual insights into the reasoning of convolutional\nneural networks (CNNs). Using network gradients, it is possible to identify\nregions where the network pays attention during image recognition tasks.\nFurthermore, these gradients can be combined with CNN features to localize more\ngeneralizable, task-specific attentive (salient) regions within scenes.\nHowever, explicit use of this gradient-based attention information integrated\ndirectly into CNN representations for semantic object understanding remains\nlimited. Such integration is particularly beneficial for visual tasks like\nsimultaneous localization and mapping (SLAM), where CNN representations\nenriched with spatially attentive object locations can enhance performance. In\nthis work, we propose utilizing task-specific network attention for RGB-D\nindoor SLAM. Specifically, we integrate layer-wise attention information\nderived from network gradients with CNN feature representations to improve\nframe association performance. Experimental results indicate improved\nperformance compared to baseline methods, particularly for large environments.", "AI": {"tldr": "本文提出了一种新的方法，通过将基于任务特定网络梯度的注意力信息集成到CNN特征表示中，以提高RGB-D室内SLAM任务的性能。", "motivation": "虽然可视化技术如类激活映射为理解CNN的推理过程提供了视觉上的洞察，但将基于梯度的注意力信息明确地集成到CNN表示中以提高语义对象理解的能力仍然有限。这种集成对于像同时定位与地图构建(SLAM)这样的视觉任务尤其有益。", "method": "本文提出了一种在RGB-D室内SLAM任务中利用任务特定网络注意力的方法。具体而言，本文将从网络梯度中提取的逐层注意力信息与CNN特征表示相结合，以提高帧关联性能。", "result": "实验结果表明，本文提出的方法在基准方法基础上提高了性能，特别是在大型环境中表现出色。", "conclusion": "这项工作展示了通过结合注意力机制和CNN特征来改善SLAM系统性能的有效性，为未来在大型环境中的视觉任务提供了新的解决方案。"}}
{"id": "2510.26122", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26122", "abs": "https://arxiv.org/abs/2510.26122", "authors": ["Feng Ju", "Zeyu Qin", "Rui Min", "Zhitao He", "Lingpeng Kong", "Yi R. Fung"], "title": "Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking", "comment": null, "summary": "While Test-Time Scaling (TTS) has proven effective in improving the reasoning\nability of large language models (LLMs), low diversity in model outputs often\nbecomes a bottleneck; this is partly caused by the common \"one problem, one\nsolution\" (1P1S) training practice, which provides a single canonical answer\nand can push models toward a narrow set of reasoning paths. To address this, we\npropose a \"one problem, multiple solutions\" (1PNS) training paradigm that\nexposes the model to a variety of valid reasoning trajectories and thus\nincreases inference diversity. A core challenge for 1PNS is reliably measuring\nsemantic differences between multi-step chains of thought, so we introduce\nReasoning Path Divergence (RPD), a step-level metric that aligns and scores\nLong Chain-of-Thought solutions to capture differences in intermediate\nreasoning. Using RPD, we curate maximally diverse solution sets per problem and\nfine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields\nmore varied outputs and higher pass@k, with an average +2.80% gain in pass@16\nover a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that\n1PNS further amplifies the effectiveness of TTS. Our code is available at\nhttps://github.com/fengjujf/Reasoning-Path-Divergence .", "AI": {"tldr": "提出了\"一个问题，多种解决方案\"（1PNS）训练范式以提高模型推理多样性，并引入了推理路径差异（RPD）来衡量多步思维链之间的语义差异。实验表明，与\"一个问题，一个解决方案\"（1P1S）基准相比，1PNS训练在通过率上有了显著提升。", "motivation": "旨在解决大语言模型推理能力提升的同时，输出多样性不足的问题，特别是由于\"一个问题，一个解决方案\"（1P1S）训练实践中提供的单一标准答案导致模式趋窄的问题。", "method": "提出了\"一个问题，多种解决方案\"（1PNS）方法，以增加有效的推理路径的多样性，并引入了推理路径差异（RPD），来评估多步链式思维之间的语义差异。利用RPD，为每个问题选择最大多样性的解决方案集合，并对基础模型Qwen3-4B-Base进行微调。", "result": "实验结果显示，使用RPD选择的训练方法能产生更多样化的输出，并在多个指标上提升了通过率，包括+2.80%的pass@16增益，以及在特定任务上+4.99%的增益。", "conclusion": "研究证明了\"一个问题，多种解决方案\"（1PNS）方法能进一步提升模型的测试时间扩展（TTS）的有效性，特别是在增加推理多样性和提高通过率方面。"}}
{"id": "2510.26140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26140", "abs": "https://arxiv.org/abs/2510.26140", "authors": ["Lihe Ding", "Shaocong Dong", "Yaokun Li", "Chenjian Gao", "Xiao Chen", "Rui Han", "Yihao Kuang", "Hong Zhang", "Bo Huang", "Zhanpeng Huang", "Zibin Wang", "Dan Xu", "Tianfan Xue"], "title": "FullPart: Generating each 3D Part at Full Resolution", "comment": "Project page: https://fullpart3d.github.io", "summary": "Part-based 3D generation holds great potential for various applications.\nPrevious part generators that represent parts using implicit vector-set tokens\noften suffer from insufficient geometric details. Another line of work adopts\nan explicit voxel representation but shares a global voxel grid among all\nparts; this often causes small parts to occupy too few voxels, leading to\ndegraded quality. In this paper, we propose FullPart, a novel framework that\ncombines both implicit and explicit paradigms. It first derives the bounding\nbox layout through an implicit box vector-set diffusion process, a task that\nimplicit diffusion handles effectively since box tokens contain little\ngeometric detail. Then, it generates detailed parts, each within its own fixed\nfull-resolution voxel grid. Instead of sharing a global low-resolution space,\neach part in our method - even small ones - is generated at full resolution,\nenabling the synthesis of intricate details. We further introduce a\ncenter-point encoding strategy to address the misalignment issue when\nexchanging information between parts of different actual sizes, thereby\nmaintaining global coherence. Moreover, to tackle the scarcity of reliable part\ndata, we present PartVerse-XL, the largest human-annotated 3D part dataset to\ndate with 40K objects and 320K parts. Extensive experiments demonstrate that\nFullPart achieves state-of-the-art results in 3D part generation. We will\nrelease all code, data, and model to benefit future research in 3D part\ngeneration.", "AI": {"tldr": "本文提出了FullPart框架，结合隐式和显式表示，解决了现有的3D部分生成器在几何细节和部分质量上的问题，通过引入中心点编码策略来保持全局一致性，并发布了PartVerse-XL数据集。", "motivation": "解决现有部分生成器在3D生成中几何细节不足的问题，以及在共享全局体素网格时，小部分因体素占用太少而导致质量下降的问题。", "method": "FullPart提出了一种结合隐式和显式表示的新框架。它首先通过隐式框向量集扩散过程推导出边界框布局，这使得即使是小部分也能在完整分辨率的固定体素网格中生成，从而实现细节丰富的合成。此外，还引入了中心点编码策略以解决不同实际大小部分之间的信息交换对齐问题，以保持全局一致性。", "result": "实验结果表明，FullPart在3D部分生成中达到了最先进的结果，同时引入了PartVerse-XL，这是迄今为止最大的人类注释3D部分数据集，包含40K个对象和320K个部分。", "conclusion": "FullPart框架有效解决了在3D部分生成中关于几何细节和部分质量的问题，其结果优于现有技术，并将发布所有代码、数据和模型以促进3D部分生成领域的未来研究。"}}
{"id": "2510.26124", "categories": ["cs.CL", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.26124", "abs": "https://arxiv.org/abs/2510.26124", "authors": ["Nawar Turk", "Sevag Kaspar", "Leila Kosseim"], "title": "On the Influence of Discourse Relations in Persuasive Texts", "comment": "Published in Proceedings of the 38th Canadian Conference on\n  Artificial Intelligence CanAI 2025 Calgary Alberta May 26-27 2025. 5 figures\n  7 tables", "summary": "This paper investigates the relationship between Persuasion Techniques (PTs)\nand Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and\nprompt engineering. Since no dataset annotated with both PTs and DRs exists, we\ntook the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point\nand developed LLM-based classifiers to label each instance of the dataset with\none of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10\ndifferent prompts, resulting in 40 unique DR classifiers. Ensemble models using\ndifferent majority-pooling strategies were used to create 5 silver datasets of\ninstances labelled with both persuasion techniques and level-2 PDTB senses. The\nsilver dataset sizes vary from 1,281 instances to 204 instances, depending on\nthe majority pooling technique used. Statistical analysis of these silver\ndatasets shows that six discourse relations (namely Cause, Purpose, Contrast,\nCause+Belief, Concession, and Condition) play a crucial role in persuasive\ntexts, especially in the use of Loaded Language, Exaggeration/Minimisation,\nRepetition and to cast Doubt. This insight can contribute to detecting online\npropaganda and misinformation, as well as to our general understanding of\neffective communication.", "AI": {"tldr": "本研究通过数据集重新标注和分类，探索了说服技巧和话语关系之间的联系，并发现某些话语关系对说服性文本有显著影响，这在检测在线宣传和错误信息方面有潜在应用。", "motivation": "研究的主要动机是探讨PTs和DRs之间的关系，并通过开发银色数据集来填补现有数据集的空白，这有助于检测在线宣传和错误信息，并提高我们对有效沟通的一般理解。", "method": "本研究利用了大语言模型（LLMs）和提示工程来探索说服技巧（PTs）和话语关系（DRs）之间的关系。由于不存在同时标注了PTs和DRs的数据集，研究者们以SemEval 2023任务3中包含19种PTs的数据集为基础，开发了基于LLMs的分类器来标注数据集中的每个实例与22种PDTB 3.0二级DRs中的一个。", "result": "研究评估了4个不同的LLMs，使用了10种不同的提示，最终产生了40个不同的DR分类器。通过使用各种多数投票策略的集成模型，创建了5个标注了说服技巧和PDTB二级意义的银色数据集，其规模从1,281个实例到204个实例不等，具体取决于使用的多数投票手段。统计分析显示，六种话语关系（即因果关系、目的、对比、因果+信念、让步、条件）在说服性文本中起着关键作用，特别是在使用夸张/淡化、重复和制造怀疑的说服手法时。", "conclusion": "该研究证明了某些话语关系在说服性文本中的重要性，特别是当这些文本使用特定说服技巧时，这些发现可以应用到在线宣传和错误信息的检测中，从而提高对有效沟通的理解。"}}
{"id": "2510.26149", "categories": ["cs.CV", "I.4.3"], "pdf": "https://arxiv.org/pdf/2510.26149", "abs": "https://arxiv.org/abs/2510.26149", "authors": ["Wei Shang", "Wanying Zhang", "Shuhang Gu", "Pengfei Zhu", "Qinghua Hu", "Dongwei Ren"], "title": "BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation", "comment": "13 pages, 10 figures, 5 tables", "summary": "Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution\nof video frames, potentially at various scaling factors, which presents several\nchallenges regarding spatial detail reproduction, temporal consistency, and\ncomputational complexity. In this paper, we propose a strong baseline BasicAVSR\nfor AVSR by integrating four key components: 1) adaptive multi-scale frequency\npriors generated from image Laplacian pyramids, 2) a flow-guided propagation\nunit to aggregate spatiotemporal information from adjacent frames, 3) a\nsecond-order motion compensation unit for more accurate spatial alignment of\nadjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and\ncontent-independent upsampling kernels. To meet diverse application demands, we\ninstantiate three propagation variants: (i) a unidirectional RNN unit for\nstrictly online inference, (ii) a unidirectional RNN unit empowered with a\nlimited lookahead that tolerates a small output delay, and (iii) a\nbidirectional RNN unit designed for offline tasks where computational resources\nare less constrained. Experimental results demonstrate the effectiveness and\nadaptability of our model across these different scenarios. Through extensive\nexperiments, we show that BasicAVSR significantly outperforms existing methods\nin terms of super-resolution quality, generalization ability, and inference\nspeed. Our work not only advances the state-of-the-art in AVSR but also extends\nits core components to multiple frameworks for diverse scenarios. The code is\navailable at https://github.com/shangwei5/BasicAVSR.", "AI": {"tldr": "本文介绍了一种新型的任意尺度视频超分辨率算法BasicAVSR，该算法在超分辨率质量、泛化能力和推理速度方面超越现有方法，并适用于不同场景。", "motivation": "任意尺度视频超分辨率技术以在不同比例因子下增强视频帧的分辨率为目标，这一过程面临着如何再现空间细节、维持时间一致性及处理计算复杂性等方面的挑战。本文旨在解决这些问题，提出一种有效的任意尺度视频超分辨率方法。", "method": "本文提出了一种新型的任意尺度视频超分辨率增强技术BasicAVSR，该技术结合了四个关键组件：1）来自图像拉普拉斯金字塔的自适应多尺度频率先验知识，2）流引导传播单元来聚合相邻帧的时空信息，3）第二阶运动补偿单元以更准确地对齐相邻帧，4）超上采样单元生成比例感知和内容无关的上采样核。此外，提供了三种传播变体，满足不同的应用场景需求，包括严格在线推理的单向RNN单元、带有限前瞻能力的单向RNN单元以及适用于计算资源较为充裕的离线任务的双向RNN单元。", "result": "实验结果表明，BasicAVSR在超分辨率质量、通用能力和推理速度方面都优于现有的方法。本文的研究不仅推动了任意尺度视频超分辨率领域的发展，也通过将核心技术扩展到多种框架中以适用于不同的场景。", "conclusion": "通过广泛的实验验证，BasicAVSR证明了其在任意尺度视频超分辨率技术上的优越性，不仅提升了超分辨率质量，也扩大了其在不同框架中的应用范围。"}}
{"id": "2510.26182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26182", "abs": "https://arxiv.org/abs/2510.26182", "authors": ["Shikhar Tuli", "James Seale Smith", "Haris Jeelani", "Chi-Heng Lin", "Abhishek Patel", "Vasili Ramanishka", "Yen-Chang Hsu", "Hongxia Jin"], "title": "MossNet: Mixture of State-Space Experts is a Multi-Head Attention", "comment": null, "summary": "Large language models (LLMs) have significantly advanced generative\napplications in natural language processing (NLP). Recent trends in model\narchitectures revolve around efficient variants of transformers or\nstate-space/gated-recurrent models (SSMs, GRMs). However, prevailing\nSSM/GRM-based methods often emulate only a single attention head, potentially\nlimiting their expressiveness. In this work, we propose MossNet, a novel\nmixture-of-state-space-experts architecture that emulates a linear multi-head\nattention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation\nnot only in channel-mixing multi-layered perceptron (MLP) blocks but also in\nthe time-mixing SSM kernels to realize multiple \"attention heads.\" Extensive\nexperiments on language modeling and downstream evaluations show that MossNet\noutperforms both transformer- and SSM-based architectures of similar model size\nand data budgets. Larger variants of MossNet, trained on trillions of tokens,\nfurther confirm its scalability and superior performance. In addition,\nreal-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU\ndemonstrate favorable runtime speed and resource usage compared to similarly\nsized baselines. Our results suggest that MossNet is a compelling new direction\nfor efficient, high-performing recurrent LLM architectures.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.26151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26151", "abs": "https://arxiv.org/abs/2510.26151", "authors": ["Shunjie-Fabian Zheng", "Hyeonjun Lee", "Thijs Kooi", "Ali Diba"], "title": "MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction", "comment": "Accepted to Computer Vision for Automated Medical Diagnosis (CVAMD)\n  Workshop at ICCV 2025", "summary": "Large annotated datasets are essential for training robust Computer-Aided\nDiagnosis (CAD) models for breast cancer detection or risk prediction. However,\nacquiring such datasets with fine-detailed annotation is both costly and\ntime-consuming. Vision-Language Models (VLMs), such as CLIP, which are\npre-trained on large image-text pairs, offer a promising solution by enhancing\nrobustness and data efficiency in medical imaging tasks. This paper introduces\na novel Multi-View Mammography and Language Model for breast cancer\nclassification and risk prediction, trained on a dataset of paired mammogram\nimages and synthetic radiology reports. Our MV-MLM leverages multi-view\nsupervision to learn rich representations from extensive radiology data by\nemploying cross-modal self-supervision across image-text pairs. This includes\nmultiple views and the corresponding pseudo-radiology reports. We propose a\nnovel joint visual-textual learning strategy to enhance generalization and\naccuracy performance over different data types and tasks to distinguish breast\ntissues or cancer characteristics(calcification, mass) and utilize these\npatterns to understand mammography images and predict cancer risk. We evaluated\nour method on both private and publicly available datasets, demonstrating that\nthe proposed model achieves state-of-the-art performance in three\nclassification tasks: (1) malignancy classification, (2) subtype\nclassification, and (3) image-based cancer risk prediction. Furthermore, the\nmodel exhibits strong data efficiency, outperforming existing fully supervised\nor VLM baselines while trained on synthetic text reports and without the need\nfor actual radiology reports.", "AI": {"tldr": "本文介绍了一种基于多视角乳腺X光图像和合成放射学报告的视觉-语言模型（MV-MLM），用于乳腺癌分类和风险预测。通过跨模态自我监督学习和新颖的视觉-文本联合学习策略，在多个分类任务中展现了卓越的数据效率和性能。", "motivation": "解决获取带有详细标注的大规模医疗图像数据集的成本高昂和耗时的问题，并利用视觉-语言模型提高在医学成像任务中的稳健性和数据效率。", "method": "引入了多视角乳腺X光图像和合成放射学报告的数据集，提出了一种多视图视觉-语言模型（MV-MLM），通过跨模态自我监督学习和视觉-文本联合学习策略增强了模型的泛化能力和准确率。", "result": "评估了该方法在私人及公开数据集上的性能，实现了三种分类任务中的最先进性能：恶性肿瘤分类、亚型分类和基于图像的癌症风险预测。", "conclusion": "所提出的MV-MLM模型不仅展示了显著的数据效率，在利用合成文本报告而非实际放射学报告的情况下仍超越现有全监督或视觉-语言模型基线。"}}
{"id": "2510.26183", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26183", "abs": "https://arxiv.org/abs/2510.26183", "authors": ["Allen Schmaltz"], "title": "Similarity-Distance-Magnitude Language Models", "comment": "8 pages, 5 tables", "summary": "We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which\nare sequence prediction models fine-tuned to maximize the proportion of\ngenerations in the well-calibrated, high-probability region partitioned by a\nfinal-layer SDM activation layer used for binary classification of\ninstruction-following. We demonstrate that existing pre-trained decoder-only\nTransformer LMs can be readily converted into SDM LMs via supervised\nfine-tuning, using the final-layer SDM activation layer during training to\nestimate a change-of-base for a supervised next-token loss over a contrastive\ninput encoding scheme, with additional hard negative examples generated online\nduring training. This results in reduced abstentions (i.e., improved\nstatistical efficiency) compared to strong supervised baselines.", "AI": {"tldr": "通过监督微调方法，将现有Transformer语言模型转换为SDM语言模型，以提高其在指令跟随任务中的表现。", "motivation": "旨在提升语言模型在遵循指令时生成高质量文本的能力，特别是在高概率区域内保持校准。", "method": "提出了相似度-距离-幅度（SDM）语言模型，为指令跟随的二元分类使用最终层的SDM激活层，通过监督微调将现有的预训练解码器Transformer语言模型转换为SDM语言模型。", "result": "相比强大的监督基线模型，SDM语言模型减少了弃权情况（即提高了统计效率）。", "conclusion": "通过引入SDM语言模型，有效提高了语言模型在生成任务中的统计效率。"}}
{"id": "2510.26154", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26154", "abs": "https://arxiv.org/abs/2510.26154", "authors": ["Sudipto Das Sukanto", "Diponker Roy", "Fahim Shakil", "Nirjhar Singha", "Abdullah Asik", "Aniket Joarder", "Mridha Md Nafis Fuad", "Muhammad Ibrahim"], "title": "Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh", "comment": "16 pages", "summary": "Modes of transportation vary across countries depending on geographical\nlocation and cultural context. In South Asian countries rickshaws are among the\nmost common means of local transport. Based on their mode of operation,\nrickshaws in cities across Bangladesh can be broadly classified into non-auto\n(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of\nauto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from\naccessing certain routes. However, existing surveillance systems make it quite\ndifficult to monitor them due to their similarity to other vehicles, especially\nnon-auto rickshaws whereas manual video analysis is too time-consuming. This\npaper presents a machine learning-based approach to automatically detect\nauto-rickshaws in traffic images. In this system, we used real-time object\ndetection using the YOLOv8 model. For training purposes, we prepared a set of\n1,730 annotated images that were captured under various traffic conditions. The\nresults show that our proposed model performs well in real-time auto-rickshaw\ndetection and offers an mAP50 of 83.447% and binary precision and recall values\nabove 78%, demonstrating its effectiveness in handling both dense and sparse\ntraffic scenarios. The dataset has been publicly released for further research.", "AI": {"tldr": "研究提出了一个基于YOLOv8模型的实时机动三轮车自动检测方法，展示了在处理不同类型交通场景的有效性，并公布了数据集供进一步研究。", "motivation": "鉴于交通规则限制了机动三轮车的行驶路线，监测其运动非常必要。然而现有的监控系统由于机动三轮车与非机动三轮车等其他车辆的相似性，很难准确监测它们，人工视频分析又费时费力。因此需要一种自动化的解决方案。", "method": "本研究采用实时目标检测方法，使用YOLOv8模型自动检测交通图像中的机动三轮车。", "result": "实验结果表明，该模型在实时检测机动三轮车上表现良好，mAP50值达到83.447%，且二元准确率和召回率均超过78%，展示了其在处理密集和稀疏交通场景中的有效性。", "conclusion": "该研究提出的基于机器学习的解决方案能够有效自动检测照片中的机动三轮车，结果表明有很大潜力进一步应用和研究。"}}
{"id": "2510.26193", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26193", "abs": "https://arxiv.org/abs/2510.26193", "authors": ["Dongjun Jang", "Youngchae Ahn", "Hyopil Shin"], "title": "RCScore: Quantifying Response Consistency in Large Language Models", "comment": null, "summary": "Current LLM evaluations often rely on a single instruction template,\noverlooking models' sensitivity to instruction style-a critical aspect for\nreal-world deployments. We present RCScore, a multi-dimensional framework\nquantifying how instruction formulation affects model responses. By\nsystematically transforming benchmark problems into multiple instruction\nstyles, RCScore reveals performance variations undetected by conventional\nmetrics. Our experiments across ten LLMs on four reasoning benchmarks\ndemonstrate that instruction style can shift accuracy by up to 16.7% points. We\nintroduce Cross-Response Similarity (CRS), a method applying RCScore metrics to\nmeasure stylistic self-consistency, and establish its strong correlation with\ntask accuracy, suggesting consistency as a valuable proxy for model\nreliability. Additional findings show that deterministic decoding produces more\nstylistically stable outputs, and model scale correlates positively with\ncross-style consistency. RCScore offers a principled approach to assess\ninstruction robustness.", "AI": {"tldr": "提出RCScore评估LLM对指令风格敏感性的框架，揭示了其影响及自洽性的重要性。", "motivation": "现有的大型语言模型（LLM）评估通常依赖单一指令模板，忽视了指令风格对模型响应的影响，这在实际部署中是一个关键因素。希望通过RCScore提供一种评估指令鲁棒性的原则方法。", "method": "提出RCScore框架，该框架通过系统地将基准问题转化为多种指令风格，来量化指令形式如何影响模型的响应。此外，引入交叉响应相似性(CRS)方法来衡量风格上的自洽性。", "result": "实验显示了指令风格可以改变精度高达16.7个百分点的结果，建立风格上的自洽性与任务准确性的强相关关系。还发现确定性解码会产生风格上更稳定的输出，模型规模与跨风格一致性成正相关。", "conclusion": "RCScore为评估LLM对指令风格的敏感性提供了一种量化框架，并通过实验展示其有效性，强调风格自洽性作为模型可靠性的一个有价值指标。"}}
{"id": "2510.26160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26160", "abs": "https://arxiv.org/abs/2510.26160", "authors": ["Jiaqi Wang", "Xiao Yang", "Kai Sun", "Parth Suresh", "Sanat Sharma", "Adam Czyzewski", "Derek Andersen", "Surya Appini", "Arkav Banerjee", "Sajal Choudhary", "Shervin Ghasemlou", "Ziqiang Guan", "Akil Iyer", "Haidar Khan", "Lingkun Kong", "Roy Luo", "Tiffany Ma", "Zhen Qiao", "David Tran", "Wenfang Xu", "Skyler Yeatman", "Chen Zhou", "Gunveer Gujral", "Yinglong Xia", "Shane Moon", "Nicolas Scheffer", "Nirav Shah", "Eun Chang", "Yue Liu", "Florian Metze", "Tammy Stark", "Zhaleh Feizollahi", "Andrea Jessee", "Mangesh Pujari", "Ahmed Aly", "Babak Damavandi", "Rakesh Wanga", "Anuj Kumar", "Rohit Patel", "Wen-tau Yih", "Xin Luna Dong"], "title": "CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark", "comment": null, "summary": "Wearable devices such as smart glasses are transforming the way people\ninteract with their surroundings, enabling users to seek information regarding\nentities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)\nplays a key role in supporting such questions, yet there is still no\ncomprehensive benchmark for this task, especially regarding wearables\nscenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG\nbenchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse\nset of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn\nconversations across 13 domains, including 6.2K egocentric images designed to\nmimic captures from wearable devices. We carefully constructed the questions to\nreflect real-world scenarios and challenges, including five types of\nimage-quality issues, six question types, varying entity popularity, differing\ninformation dynamism, and different conversation turns. We design three tasks:\nsingle-source augmentation, multi-source augmentation, and multi-turn\nconversations -- each paired with an associated retrieval corpus and APIs for\nboth image-KG retrieval and webpage retrieval. Our evaluation shows that\nstraightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM\nsingle- and multi-turn QA, respectively, whereas state-of-the-art industry\nsolutions have similar quality (32%/45%), underscoring ample room for\nimprovement. The benchmark has hosted KDD Cup 2025, attracting about 1K\nparticipants and 5K submissions, with winning solutions improving baseline\nperformance by 28%, highlighting its early impact on advancing the field.", "AI": {"tldr": "提出了CRAG-MM，一个多模态、多轮次对话的全面基准，旨在填补关于可穿戴设备场景下多模态检索增强生成（MM-RAG）研究的空白。", "motivation": "弥补现有研究中缺乏全面评估多模态多轮次对话尤其是穿戴设备场景下信息检索和生成任务的基准。", "method": "构建了一个含有6.5K(图像、问题、答案)三元组和2K视觉多轮次对话的基准，涵盖了包括来自可穿戴设备拍摄的6.2K第一人称图像等13个领域的数据集。设计了单/多数据源增强和多轮次对话三种任务。", "result": "评估显示，直接的RAG方法在单/多轮次问答上的真实度分别为32%和43%，而先进的行业解决方案效果类似（32%/45%），表明有大量改进空间；CRAG-MM在KDD Cup 2025比赛中引领了领域进步。", "conclusion": "CRAG-MM提供了一个全面的基准，能够评估和推进MM-RAG技术在穿戴设备场景下的应用能力。"}}
{"id": "2510.26200", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26200", "abs": "https://arxiv.org/abs/2510.26200", "authors": ["Woojin Kim", "Jaeyoung Do"], "title": "Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation", "comment": "Accepted in NeurIPS 2025", "summary": "While diffusion language models (DLMs) enable fine-grained refinement, their\npractical controllability remains fragile. We identify and formally\ncharacterize a central failure mode called update forgetting, in which uniform\nand context agnostic updates induce token level fluctuations across timesteps,\nerasing earlier semantic edits and disrupting the cumulative refinement\nprocess, thereby degrading fluency and coherence. As this failure originates in\nuniform and context agnostic updates, effective control demands explicit token\nordering. We propose Token Timestep Allocation (TTA), which realizes soft and\nsemantic token ordering via per token timestep schedules: critical tokens are\nfrozen early, while uncertain tokens receive continued refinement. This\ntimestep based ordering can be instantiated as either a fixed policy or an\nadaptive policy driven by task signals, thereby supporting a broad spectrum of\nrefinement strategies. Because it operates purely at inference time, it applies\nuniformly across various DLMs and naturally extends to diverse supervision\nsources. Empirically, TTA improves controllability and fluency: on sentiment\ncontrol, it yields more than 20 percent higher accuracy and nearly halves\nperplexity using less than one fifth the steps; in detoxification, it lowers\nmaximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).\nTogether, these results demonstrate that softened ordering via timestep\nallocation is the critical lever for mitigating update forgetting and achieving\nstable and controllable diffusion text generation.", "AI": {"tldr": "The paper presents TTA, a method to manage token refinement during text generation in DLMs, leading to improved control and coherence.", "motivation": "The motivation is to address the issue of \"update forgetting\" in diffusion language models (DLMs), which occurs due to uniform and context-agnostic updates that lead to token-level fluctuations and disrupt the refinement process.", "method": "Token Timestep Allocation (TTA) is proposed. This approach schedules token refinement based on their importance, freezing critical tokens early and continuing to refine uncertain ones. TTA can be implemented as a fixed policy or an adaptive policy driven by task signals.", "result": "TTA enhances controllability and fluency, achieving over 20% higher accuracy in sentiment control, reducing perplexity to less than half, and completing in less than one-fifth of the steps. In detoxification tasks, TTA reduces maximum toxicity and perplexity.", "conclusion": "The results demonstrate that softened ordering through timestep allocation is crucial for mitigating update forgetting and achieving stable, controllable diffusion text generation."}}
{"id": "2510.26173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26173", "abs": "https://arxiv.org/abs/2510.26173", "authors": ["Wontae Choi", "Jaelin Lee", "Hyung Sup Yun", "Byeungwoo Jeon", "Il Yong Chun"], "title": "MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models", "comment": "10 pages, 6 figures", "summary": "Accurate estimation of motion information is crucial in diverse computational\nimaging and computer vision applications. Researchers have investigated various\nmethods to extract motion information from a single blurred image, including\nblur kernels and optical flow. However, existing motion representations are\noften of low quality, i.e., coarse-grained and inaccurate. In this paper, we\npropose the first high-resolution (HR) Motion Trajectory estimation framework\nusing Diffusion models (MoTDiff). Different from existing motion\nrepresentations, we aim to estimate an HR motion trajectory with high-quality\nfrom a single motion-blurred image. The proposed MoTDiff consists of two key\ncomponents: 1) a new conditional diffusion framework that uses multi-scale\nfeature maps extracted from a single blurred image as a condition, and 2) a new\ntraining method that can promote precise identification of a fine-grained\nmotion trajectory, consistent estimation of overall shape and position of a\nmotion path, and pixel connectivity along a motion trajectory. Our experiments\ndemonstrate that the proposed MoTDiff can outperform state-of-the-art methods\nin both blind image deblurring and coded exposure photography applications.", "AI": {"tldr": "该论文介绍了MoTDiff，一种新的高分辨率运动轨迹估计框架，通过扩散模型从单个运动模糊图像中估计高质量的运动轨迹，实验表明该方法优于现有技术。", "motivation": "目前的运动表示方法存在质量和精度问题，难以准确捕捉单个模糊图像中的详细运动信息。为此，研究人员提出了高质量的高分辨率（HR）运动轨迹估计框架MoTDiff，以解决现有表示方法的不足。", "method": "MoTDiff 包括两个关键组成部分：1）一个使用从单个模糊图像提取的多尺度特征图作为条件的新条件扩散框架；2）一种能够促进细粒度运动轨迹精确识别、整体形状和位置一致估计以及沿着运动轨迹的像素连通性训练的新方法。", "result": "实验表明，所提出的MoTDiff在盲目图像去模糊和编码曝光摄影应用中能够超越现有的最先进技术。", "conclusion": "该研究成功实现了从单个模糊图像中估计高质量高分辨率运动轨迹的目标，为计算成像和计算机视觉领域的多种应用提供了更精确的运动信息。"}}
{"id": "2510.26202", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26202", "abs": "https://arxiv.org/abs/2510.26202", "authors": ["Rajiv Movva", "Smitha Milli", "Sewon Min", "Emma Pierson"], "title": "What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data", "comment": "Code: https://github.com/rmovva/wimhf", "summary": "Human feedback can alter language models in unpredictable and undesirable\nways, as practitioners lack a clear understanding of what feedback data\nencodes. While prior work studies preferences over certain attributes (e.g.,\nlength or sycophancy), automatically extracting relevant features without\npre-specifying hypotheses remains challenging. We introduce What's In My Human\nFeedback? (WIMHF), a method to explain feedback data using sparse autoencoders.\nWIMHF characterizes both (1) the preferences a dataset is capable of measuring\nand (2) the preferences that the annotators actually express. Across 7\ndatasets, WIMHF identifies a small number of human-interpretable features that\naccount for the majority of the preference prediction signal achieved by\nblack-box models. These features reveal a wide diversity in what humans prefer,\nand the role of dataset-level context: for example, users on Reddit prefer\ninformality and jokes, while annotators in HH-RLHF and PRISM disprefer them.\nWIMHF also surfaces potentially unsafe preferences, such as that LMArena users\ntend to vote against refusals, often in favor of toxic content. The learned\nfeatures enable effective data curation: re-labeling the harmful examples in\nArena yields large safety gains (+37%) with no cost to general performance.\nThey also allow fine-grained personalization: on the Community Alignment\ndataset, we learn annotator-specific weights over subjective features that\nimprove preference prediction. WIMHF provides a human-centered analysis method\nfor practitioners to better understand and use preference data.", "AI": {"tldr": "通过WIMHF方法分析人类反馈数据揭示了偏好的多样性和上下文，且有助于提升数据整理的安全性和个性化预测。", "motivation": "由于实践者对反馈数据所编码的内容缺乏明确的理解，人类反馈可能导致语言模型发生不可预测和不希望的变化。本研究旨在无需预设假设的情况下自动提取相关特征。", "method": "本研究提出了名为What's In My Human Feedback? (WIMHF) 的方法，利用稀疏自编码器来解释反馈数据，既表征了数据集能够测量的偏好，也揭示了数据标注者实际表达的偏好。", "result": "WIMHF 在7个数据集中确定了少数几个可解释的人类特征，这些特征解释了黑盒模型实现的一些偏好预测信号。特征揭示了人类偏好的多样性及数据集级别的上下文的角色。利用学到的特征可以有效进行数据整理以及个性化预测的提升。", "conclusion": "WIMHF 提供了一种以人类为中心的分析方法，帮助从业者更好地理解和利用偏好数据。"}}
{"id": "2510.26186", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26186", "abs": "https://arxiv.org/abs/2510.26186", "authors": ["Jinho Choi", "Hyesu Lim", "Steffen Schneider", "Jaegul Choo"], "title": "ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts", "comment": "Published in the Thirty-Ninth Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Dataset bias, where data points are skewed to certain concepts, is ubiquitous\nin machine learning datasets. Yet, systematically identifying these biases is\nchallenging without costly, fine-grained attribute annotations. We present\nConceptScope, a scalable and automated framework for analyzing visual datasets\nby discovering and quantifying human-interpretable concepts using Sparse\nAutoencoders trained on representations from vision foundation models.\nConceptScope categorizes concepts into target, context, and bias types based on\ntheir semantic relevance and statistical correlation to class labels, enabling\nclass-level dataset characterization, bias identification, and robustness\nevaluation through concept-based subgrouping. We validate that ConceptScope\ncaptures a wide range of visual concepts, including objects, textures,\nbackgrounds, facial attributes, emotions, and actions, through comparisons with\nannotated datasets. Furthermore, we show that concept activations produce\nspatial attributions that align with semantically meaningful image regions.\nConceptScope reliably detects known biases (e.g., background bias in\nWaterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects\nin ImageNet), offering a practical tool for dataset auditing and model\ndiagnostics.", "AI": {"tldr": "ConceptScope是一个分析视觉数据集的可扩展自动框架，用于发现和量化人类可解释的视觉概念。它能够有效地识别数据集偏差并可视化语义有意义的图像区域。", "motivation": "系统地识别数据集偏差具有挑战性，特别是在缺乏详细的属性标注的情况下。ConceptScope旨在解决这一挑战，为分析视觉数据集提供工具。", "method": "使用稀疏自编码器训练视觉基础模型的表示，以发现和量化人类可解释的概念。ConceptScope将这些概念分类为目标、上下文和偏见类型，基于它们对类别标签的语义相关性和统计相关性，从而使数据集级别表征、偏见识别和通过基于概念的子组进行鲁棒性评估成为可能。", "result": "验证了ConceptScope能够捕捉到广泛视觉概念，并通过空间属性标注证明了所识别概念与其在图像中对应区域的语义意义高度一致。此外，它可靠地检测出了已知偏见和未标注偏见。", "conclusion": "通过利用视觉基础模型的表示和稀疏自编码器，ConceptScope提供了一种有效的工具，用于数据集审计和模型诊断，特别是在识别视觉偏见方面有着极高的实用价值。"}}
{"id": "2510.26205", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26205", "abs": "https://arxiv.org/abs/2510.26205", "authors": ["Qi Luo", "Xiaonan Li", "Tingshuo Fan", "Xinchi Chen", "Xipeng Qiu"], "title": "Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning", "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a leading approach to\nreducing hallucinations in large language models (LLMs). Current RAG evaluation\nbenchmarks primarily focus on what we call local RAG: retrieving relevant\nchunks from a small subset of documents to answer queries that require only\nlocalized understanding within specific text chunks. However, many real-world\napplications require a fundamentally different capability -- global RAG --\nwhich involves aggregating and analyzing information across entire document\ncollections to derive corpus-level insights (for example, \"What are the top 10\nmost cited papers in 2023?\"). In this paper, we introduce GlobalQA -- the first\nbenchmark specifically designed to evaluate global RAG capabilities, covering\nfour core task types: counting, extremum queries, sorting, and top-k\nextraction. Through systematic evaluation across different models and\nbaselines, we find that existing RAG methods perform poorly on global tasks,\nwith the strongest baseline achieving only 1.51 F1 score. To address these\nchallenges, we propose GlobalRAG, a multi-tool collaborative framework that\npreserves structural coherence through chunk-level retrieval, incorporates\nLLM-driven intelligent filters to eliminate noisy documents, and integrates\naggregation modules for precise symbolic computation. On the Qwen2.5-14B model,\nGlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,\nvalidating the effectiveness of our method.", "AI": {"tldr": "提出GlobalQA，一个专为评估全局RAG能力设计的基准，涵盖四种类别的核心任务：计数、极值查询、排序和顶级提取。并提出了GlobalRAG框架，展示了显著的性能提升。", "motivation": "当前的RAG评估基准主要集中在局部RAG上，然而许多现实世界的应用需要一种完全不同的能力——全局RAG，涉及在整个文档集合中聚合和分析信息以得出语料库级别的见解。", "method": "GlobalRAG，一种多工具协作框架，它通过片段级别的检索保持结构一致性，运用大语言模型驱动的智能过滤器消除噪声文档，并集成聚合模块以实现精确的符号计算。", "result": "在Qwen2.5-14B模型上，GlobalRAG在全局任务上的F1分数达到6.63，远超最强基线的1.51。", "conclusion": "提出的GlobalRAG框架通过多工具协作，有效提高了全局RAG任务的性能。"}}
{"id": "2510.26196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26196", "abs": "https://arxiv.org/abs/2510.26196", "authors": ["Li Wang", "Yiyu Zhuang", "Yanwen Wang", "Xun Cao", "Chuan Guo", "Xinxin Zuo", "Hao Zhu"], "title": "Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction", "comment": "SIGGRAPH Asia 2025", "summary": "3D human pose estimation from sketches has broad applications in computer\nanimation and film production. Unlike traditional human pose estimation, this\ntask presents unique challenges due to the abstract and disproportionate nature\nof sketches. Previous sketch-to-pose methods, constrained by the lack of\nlarge-scale sketch-3D pose annotations, primarily relied on optimization with\nheuristic rules-an approach that is both time-consuming and limited in\ngeneralizability. To address these challenges, we propose a novel approach\nleveraging a \"learn from synthesis\" strategy. First, a diffusion model is\ntrained to synthesize sketch images from 2D poses projected from 3D human\nposes, mimicking disproportionate human structures in sketches. This process\nenables the creation of a synthetic dataset, SKEP-120K, consisting of 120k\naccurate sketch-3D pose annotation pairs across various sketch styles. Building\non this synthetic dataset, we introduce an end-to-end data-driven framework for\nestimating human poses and shapes from diverse sketch styles. Our framework\ncombines existing 2D pose detectors and generative diffusion priors for sketch\nfeature extraction with a feed-forward neural network for efficient 2D pose\nestimation. Multiple heuristic loss functions are incorporated to guarantee\ngeometric coherence between the derived 3D poses and the detected 2D poses\nwhile preserving accurate self-contacts. Qualitative, quantitative, and\nsubjective evaluations collectively show that our model substantially surpasses\nprevious ones in both estimation accuracy and speed for sketch-to-pose tasks.", "AI": {"tldr": "A novel 'learn from synthesis' approach is used to create a synthetic dataset and framework for accurately and efficiently estimating 3D human poses from sketch images, improving upon previous methods.", "motivation": "To overcome the limitations of previous methods in sketch-to-pose estimation, such as time-consumption and limited generalizability, due to the lack of large-scale sketch-3D pose annotations.", "method": "First, a diffusion model synthesizes sketch images from 2D poses projected from 3D human poses. Then, an end-to-end data-driven framework is introduced, combining 2D pose detectors, generative diffusion priors for feature extraction, and a feed-forward neural network for 2D pose estimation.", "result": "Qualitative, quantitative, and subjective evaluations show that the model surpasses previous ones in both accuracy and speed for sketch-to-pose tasks.", "conclusion": "The proposed method, which leverages synthetic datasets and advanced models, is more efficient and accurate in deriving 3D human poses from sketches."}}
{"id": "2510.26253", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26253", "abs": "https://arxiv.org/abs/2510.26253", "authors": ["Takuma Sato", "Seiya Kawano", "Koichiro Yoshino"], "title": "Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs", "comment": null, "summary": "The ability to accurately interpret implied meanings plays a crucial role in\nhuman communication and language use, and language models are also expected to\npossess this capability. This study demonstrates that providing language models\nwith pragmatic theories as prompts is an effective in-context learning approach\nfor tasks to understand implied meanings. Specifically, we propose an approach\nin which an overview of pragmatic theories, such as Gricean pragmatics and\nRelevance Theory, is presented as a prompt to the language model, guiding it\nthrough a step-by-step reasoning process to derive a final interpretation.\nExperimental results showed that, compared to the baseline, which prompts\nintermediate reasoning without presenting pragmatic theories (0-shot\nChain-of-Thought), our methods enabled language models to achieve up to 9.6\\%\nhigher scores on pragmatic reasoning tasks. Furthermore, we show that even\nwithout explaining the details of pragmatic theories, merely mentioning their\nnames in the prompt leads to a certain performance improvement (around 1-3%) in\nlarger models compared to the baseline.", "AI": {"tldr": "研究展示了为语言模型提供语用理论作为提示的在上下文中学习方法在理解暗示意义任务上的有效性，该方法较基准方法能提高性能。", "motivation": "研究动机是展示为语言模型提供语用理论作为提示是一种有效的在上下文中学习的方法，可以帮助模型理解暗示意义。", "method": "本研究提出了一种方法，即将语用理论概述（如格赖斯语用学和相关理论）作为语言模型的提示，引导其进行逐步推理以得出最终解释。", "result": "实验结果表明，与不提供语用理论仅提供中间推理提示的基准方法相比，本研究的方法能够使语言模型在语用推理任务上的得分提高最多9.6%。", "conclusion": "研究得出的结论是，即使只是在提示中提到语用理论的名字，而不解释其细节，也能使较大规模的语言模型相较于基准方法取得一定的性能提升（约1%-3%）。"}}
{"id": "2510.26203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26203", "abs": "https://arxiv.org/abs/2510.26203", "authors": ["Mehdi Khaleghi", "Nastaran Khaleghi", "Sobhan Sheykhivand", "Sebelan Danishvar"], "title": "Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management", "comment": null, "summary": "The sustainability of supply chain plays a key role in achieving optimal\nperformance in controlling the supply chain. The management of risks that occur\nin a supply chain is a fundamental problem for the purpose of developing the\nsustainability of the network and elevating the performance efficiency of the\nsupply chain. The correct classification of products is another essential\nelement in a sustainable supply chain. Acknowledging recent breakthroughs in\nthe context of deep networks, several architectural options have been deployed\nto analyze supply chain datasets. A novel geometric deep network is used to\npropose an ensemble deep network. The proposed Chebyshev ensemble geometric\nnetwork (Ch-EGN) is a hybrid convolutional and geometric deep learning. This\nnetwork is proposed to leverage the information dependencies in supply chain to\nderive invisible states of samples in the database. The functionality of the\nproposed deep network is assessed on the two different databases. The\nSupplyGraph Dataset and DataCo are considered in this research. The prediction\nof delivery status of DataCo supply chain is done for risk administration. The\nproduct classification and edge classification are performed using the\nSupplyGraph database to enhance the sustainability of the supply network. An\naverage accuracy of 98.95% is obtained for the ensemble network for risk\nmanagement. The average accuracy of 100% and 98.07% are obtained for\nsustainable supply chain in terms of 5 product group classification and 4\nproduct relation classification, respectively. The average accuracy of 92.37%\nis attained for 25 company relation classification. The results confirm an\naverage improvement and efficiency of the proposed method compared to the\nstate-of-the-art approaches.", "AI": {"tldr": "该研究提出了一种新的几何深度网络——Chebyshev集成几何网络(Ch-EGN)，用于分析供应链数据集，提高供应链的可持续性和风险管理。通过两个不同数据库验证了该方法的有效性，结果表明该方法比最先进的方法更具优势。", "motivation": "供应链中风险管理和产品的正确分类对于提高供应链的可持续性和性能至关重要。现有方法在供应链风险管理中表现并不出色，因此需要更有效的方法来提高这一领域的性能。", "method": "研究提出了一种新的几何深度网络——Chebyshev集成几何网络(Ch-EGN)，集成卷积和几何深度学习技术，旨在利用供应链中的信息依赖性，以揭示数据库中的隐形状态。", "result": "研究通过SupplyGraph数据库和DataCo数据库验证了方法的有效性。基于DataCo数据库的供应链风险管理和基于SupplyGraph数据库的产品分类与边分类实验表明，平均准确率分别为98.95%、100%和98.07%，对于25家公司关系分类准确率为92.37%，显著优于现有方法。", "conclusion": "研究证明了所提出的方法在提高供应链的可持续性和风险管理方面的有效性和优越性，能够显著提升供应链的风险管理准确率和可持续性。"}}
{"id": "2510.26254", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26254", "abs": "https://arxiv.org/abs/2510.26254", "authors": ["Mérilin Sousa Silva", "Sina Ahmadi"], "title": "Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages", "comment": "Under review", "summary": "Throughout language history, words are borrowed from one language to another\nand gradually become integrated into the recipient's lexicon. Speakers can\noften differentiate these loanwords from native vocabulary, particularly in\nbilingual communities where a dominant language continuously imposes lexical\nitems on a minority language. This paper investigates whether pretrained\nlanguage models, including large language models, possess similar capabilities\nfor loanword identification. We evaluate multiple models across 10 languages.\nDespite explicit instructions and contextual information, our results show that\nmodels perform poorly in distinguishing loanwords from native ones. These\nfindings corroborate previous evidence that modern NLP systems exhibit a bias\ntoward loanwords rather than native equivalents. Our work has implications for\ndeveloping NLP tools for minority languages and supporting language\npreservation in communities under lexical pressure from dominant languages.", "AI": {"tldr": "研究发现预训练语言模型在识别借词和本地词汇方面的能力较差，这对开发支持弱势语言的NLP工具具有重要意义。", "motivation": "研究动机在于探索预训练语言模型是否能像双语环境中的讲者一样识别借词，特别是在强势语言对弱势语言产生影响的情况下。", "method": "该研究评估了多个预训练语言模型（包括大型语言模型）在10种不同语言中识别借词的能力。", "result": "研究结果显示，即使提供明确指令和上下文信息，模型在区分借词和本地词汇方面表现不佳。", "conclusion": "该研究发现证实了现代NLP系统在借词识别上存在向借词偏倚的现象，这对开发针对弱势语言的NLP工具和支持这些语言的保存具有重要意义。"}}
{"id": "2510.26213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26213", "abs": "https://arxiv.org/abs/2510.26213", "authors": ["Hengrui Kang", "Zhuangcheng Gu", "Zhiyuan Zhao", "Zichen Wen", "Bin Wang", "Weijia Li", "Conghui He"], "title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation", "comment": "TL;DR: With OmniLayout-1M dataset and LLM-based coarse-to-fine\n  learning, we enable universal and diverse document layout generation", "summary": "Document AI has advanced rapidly and is attracting increasing attention. Yet,\nwhile most efforts have focused on document layout analysis (DLA), its\ngenerative counterpart, document layout generation, remains underexplored. A\nmajor obstacle lies in the scarcity of diverse layouts: academic papers with\nManhattan-style structures dominate existing studies, while open-world genres\nsuch as newspapers and magazines remain severely underrepresented. To address\nthis gap, we curate OmniLayout-1M, the first million-scale dataset of diverse\ndocument layouts, covering six common document types and comprising\ncontemporary layouts collected from multiple sources. Moreover, since existing\nmethods struggle in complex domains and often fail to arrange long sequences\ncoherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage\nCoarse-to-Fine learning paradigm: 1) learning universal layout principles from\nOmniLayout-1M with coarse category definitions, and 2) transferring the\nknowledge to a specific domain with fine-grained annotations. Extensive\nexperiments demonstrate that our approach achieves strong performance on\nmultiple domains in M$^{6}$Doc dataset, substantially surpassing both existing\nlayout generation experts and several latest general-purpose LLMs. Our code,\nmodels, and dataset will be publicly released.", "AI": {"tldr": "研究构建了第一个百万级别的多样化文档布局数据集OmniLayout-1M，并提出基于该数据集的两阶段学习范式的OmniLayout-LLM模型，显著提升了文档布局生成的效果，并将在多个领域中展现优越性能。", "motivation": "现有的文档布局生成研究主要集中在学术论文等具有Manhattan风格结构的文档上，而报纸和杂志等开放世界的文档类型代表性严重不足。为了解决这一问题，研究人员构建了包含六种常见文档类型的OmniLayout-1M数据集，旨在解决现有方法在复杂领域中表现不佳的问题。", "method": "OmniLayout-LLM采用两阶段粗细学习范式：首先，通过OmniLayout-1M数据集中的粗略分类定义学习普遍的布局原则；其次，将知识转移到具有细粒度标注的具体领域中。", "result": "实验结果表明，该方法在M$^{6}$Doc数据集的多个领域中表现优异，显著优于现有的布局生成专家和一些最新的通用大模型。", "conclusion": "研究开发了OmniLayout-LLM模型，通过两阶段的学习方法实现了在多样文档类型中的强大的布局生成能力，该成果将在代码、模型和数据集上公开。"}}
{"id": "2510.26271", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26271", "abs": "https://arxiv.org/abs/2510.26271", "authors": ["Sukrit Sriratanawilai", "Jhayahgrit Thongwat", "Romrawin Chumpu", "Patomporn Payoungkhamdee", "Sarana Nutanong", "Peerat Limkonchotiwat"], "title": "Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual", "comment": "Work in progress", "summary": "Vision-language models (VLMs) exhibit uneven performance across languages, a\nproblem that is often exacerbated when the model size is reduced. While\nKnowledge distillation (KD) demonstrates promising results in transferring\nknowledge from larger to smaller VLMs, applying KD in multilingualism is an\nunderexplored area. This paper presents a controlled empirical study of KD\nbehavior across five distillation approaches, isolating their effects on\ncross-lingual representation consistency and downstream performance stability\nunder model compression. We study five distillation formulations across CLIP\nand SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual\nQA. We find that some configurations preserve or even improve multilingual\nretrieval robustness despite halving model size, but others fail to maintain\ncross-task stability, exposing design-sensitive trade-offs that aggregate\naccuracy alone does not reveal.", "AI": {"tldr": "论文通过实验研究了五种知识蒸馏方法在多语言视觉语言模型中的效果，发现尽管模型大小减半，某些配置仍能改善跨语言检索的稳定性，但也存在无法维持跨任务稳定性的配置。", "motivation": "文章的动机在于探索在多语言环境下，应用知识蒸馏技术于视觉语言模型时，能否在保证模型压缩的同时提升或维持其性能，特别关注跨语言理解和任务稳定性的表现。", "method": "本文通过在一个多语言的视觉语言模型上进行知识蒸馏实验，研究了五种不同的蒸馏方法对于跨语言表示一致性以及模型压缩后下游任务性能稳定性的影响。", "result": "研究发现，某些蒸馏配置即使在模型大小减半的情况下也能保持或改善多语言检索的鲁棒性，但其他配置则未能维持跨任务的稳定性，揭示了准确率之外的设计敏感性权衡。", "conclusion": "结论指出，知识蒸馏在不同配置下对多语言视觉语言模型的影响各不相同，某些设计可以提高压缩模型的性能和稳定性，但也存在权衡选择的必要。"}}
{"id": "2510.26241", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26241", "abs": "https://arxiv.org/abs/2510.26241", "authors": ["Shiho Matta", "Lis Kanashiro Pereira", "Peitao Han", "Fei Cheng", "Shigeru Kitazawa"], "title": "Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models", "comment": "10 pages", "summary": "Modern vision-language models (VLMs) excel at many multimodal tasks, yet\ntheir grasp of temporal information in video remains weak and, crucially,\nunder-evaluated. We probe this gap with a deceptively simple but revealing\nchallenge: judging the arrow of time (AoT)-whether a short clip is played\nforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated\nbenchmark that tests whether VLMs can infer temporal direction in natural\nvideos using the same stimuli and behavioral baselines established for humans.\nOur comprehensive evaluation of open-weight and proprietary, reasoning and\nnon-reasoning VLMs reveals that most models perform near chance, and even the\nbest lag far behind human accuracy on physically irreversible processes (e.g.,\nfree fall, diffusion/explosion) and causal manual actions (division/addition)\nthat humans recognize almost instantly. These results highlight a fundamental\ngap in current multimodal systems: while they capture rich visual-semantic\ncorrelations, they lack the inductive biases required for temporal continuity\nand causal understanding. We release the code and data for AoT-PsyPhyBENCH to\nencourage further progress in the physical and temporal reasoning capabilities\nof VLMs.", "AI": {"tldr": "研究通过判断视频片段播放方向的测试揭示了视觉语言模型在处理视频时间信息方面的不足。", "motivation": "当前的VLMs虽然在许多多模态任务上表现出色，但在处理视频中的时间信息方面仍然薄弱且评估不足。这项研究旨在探讨这一不足，并通过极简但具有启发性的挑战来揭示这种能力的缺失。", "method": "通过判断视频片段的时间箭头（AoT）——即判断片段是正向播放还是反向播放——来测试多模态视觉语言模型（VLMs）在视频中的时间信息处理能力。为此，引入了AoT-PsyPhyBENCH基准测试，该测试使用经过心理物理验证的刺激和人类的行为基线来评估VLMs能否在自然视频中推断时间方向。", "result": "对开源和专有、推理和非推理VLMs的全面评估显示，大多数模型的性能接近随机水平，且明显的滞后于人类对于物理不可逆过程及因果动作的识别准确性。", "conclusion": "大多数VLMs在AoT-PsyPhyBENCH基准测试上的表现接近随机水平，即使表现最好的模型在人类几乎可以瞬间识别的物理不可逆过程和因果手动动作（例如自由落体、扩散/爆炸、分割/加法）上也远远落后于人类准确性。结果强调了当前多模态系统在时间连续性和因果理解上缺乏所需的归纳偏见。"}}
{"id": "2510.26277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26277", "abs": "https://arxiv.org/abs/2510.26277", "authors": ["Kang Chen", "Yaoning Wang", "Kai Xiong", "Zhuoka Feng", "Wenhe Sun", "Haotian Chen", "Yixin Cao"], "title": "Do LLMs Signal When They're Right? Evidence from Neuron Agreement", "comment": null, "summary": "Large language models (LLMs) commonly boost reasoning via\nsample-evaluate-ensemble decoders, achieving label free gains without ground\ntruth. However, prevailing strategies score candidates using only external\noutputs such as token probabilities, entropies, or self evaluations, and these\nsignals can be poorly calibrated after post training. We instead analyze\ninternal behavior based on neuron activations and uncover three findings: (1)\nexternal signals are low dimensional projections of richer internal dynamics;\n(2) correct responses activate substantially fewer unique neurons than\nincorrect ones throughout generation; and (3) activations from correct\nresponses exhibit stronger cross sample agreement, whereas incorrect ones\ndiverge. Motivated by these observations, we propose Neuron Agreement Decoding\n(NAD), an unsupervised best-of-N method that selects candidates using\nactivation sparsity and cross sample neuron agreement, operating solely on\ninternal signals and without requiring comparable textual outputs. NAD enables\nearly correctness prediction within the first 32 generated tokens and supports\naggressive early stopping. Across math and science benchmarks with verifiable\nanswers, NAD matches majority voting; on open ended coding benchmarks where\nmajority voting is inapplicable, NAD consistently outperforms Avg@64. By\npruning unpromising trajectories early, NAD reduces token usage by 99% with\nminimal loss in generation quality, showing that internal signals provide\nreliable, scalable, and efficient guidance for label free ensemble decoding.", "AI": {"tldr": "The paper investigates internal neuron activations in large language models for enhancing ensemble decoding and introduces Neuron Agreement Decoding (NAD), an innovative method that achieves higher accuracy and reduced token usage by focusing on internal dynamics for selection among generated responses.", "motivation": "Addresses the inadequacies of using only external signals for decoders in large language models, which can be poorly calibrated post-training. Seeks to improve label-free reasoning gains by leveraging internal dynamics.", "method": "Proposes Neuron Agreement Decoding (NAD), an unsupervised method that utilizes internal neuron activations of LLMs for selecting the best generated response among multiple candidates.", "result": "NAD matches the performance of majority voting on benchmarks with verifiable answers and outperforms Avg@64 on coding tasks, while reducing token usage by 99%.", "conclusion": "Internal signals within large language models can serve as a reliable, scalable, and efficient guide for label-free ensemble decoding, with NAD showcasing significant improvements over traditional methods."}}
{"id": "2510.26268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26268", "abs": "https://arxiv.org/abs/2510.26268", "authors": ["Lin Guo", "Xiaoqing Luo", "Wei Xie", "Zhancheng Zhang", "Hui Li", "Rui Wang", "Zhenhua Feng", "Xiaoning Song"], "title": "Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws", "comment": "NeurIPS 2025 spotlight", "summary": "Existing infrared and visible image fusion methods often face the dilemma of\nbalancing modal information. Generative fusion methods reconstruct fused images\nby learning from data distributions, but their generative capabilities remain\nlimited. Moreover, the lack of interpretability in modal information selection\nfurther affects the reliability and consistency of fusion results in complex\nscenarios. This manuscript revisits the essence of generative image fusion\nunder the inspiration of human cognitive laws and proposes a novel infrared and\nvisible image fusion method, termed HCLFuse. First, HCLFuse investigates the\nquantification theory of information mapping in unsupervised fusion networks,\nwhich leads to the design of a multi-scale mask-regulated variational\nbottleneck encoder. This encoder applies posterior probability modeling and\ninformation decomposition to extract accurate and concise low-level modal\ninformation, thereby supporting the generation of high-fidelity structural\ndetails. Furthermore, the probabilistic generative capability of the diffusion\nmodel is integrated with physical laws, forming a time-varying physical\nguidance mechanism that adaptively regulates the generation process at\ndifferent stages, thereby enhancing the ability of the model to perceive the\nintrinsic structure of data and reducing dependence on data quality.\nExperimental results show that the proposed method achieves state-of-the-art\nfusion performance in qualitative and quantitative evaluations across multiple\ndatasets and significantly improves semantic segmentation metrics. This fully\ndemonstrates the advantages of this generative image fusion method, drawing\ninspiration from human cognition, in enhancing structural consistency and\ndetail quality.", "AI": {"tldr": "HCLFuse 是一种受人类认知定律启发的红外和可见光图像融合方法，通过多尺度掩模调节变分瓶颈编码器和时间变化的物理引导机制，实现了高质量的数据结构感知和细节生成，提升了图像融合的表现。", "motivation": "现有方法在模态信息平衡、生成能力和选择解释性方面存在不足，影响了复杂场景下的可靠性和一致性。HCLFuse旨在改进这些问题，提供更加一致和清晰的图像结构细节。", "method": "HCLFuse通过多尺度掩模调节变分瓶颈编码器提取精确的低层次模态信息并生成高质量的结构细节，结合扩散模型的生成能力和物理定律形成自适应调节机制。", "result": "实验结果表明HCLFuse在多数据集的定性和定量评估中表现出优秀融合性能，并显著提高了语义分割指标。", "conclusion": "这种方法展示了人类认知启发的生成图像融合方法在提高结构一致性和细节质量方面的优势。"}}
{"id": "2510.26285", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2510.26285", "abs": "https://arxiv.org/abs/2510.26285", "authors": ["Michal Štefánik", "Timothee Mickus", "Marek Kadlčík", "Bertram Højer", "Michal Spiegel", "Raúl Vázquez", "Aman Sinha", "Josef Kuchař", "Philipp Mondorf"], "title": "Unravelling the Mechanisms of Manipulating Numbers in Language Models", "comment": null, "summary": "Recent work has shown that different large language models (LLMs) converge to\nsimilar and accurate input embedding representations for numbers. These\nfindings conflict with the documented propensity of LLMs to produce erroneous\noutputs when dealing with numeric information. In this work, we aim to explain\nthis conflict by exploring how language models manipulate numbers and quantify\nthe lower bounds of accuracy of these mechanisms. We find that despite\nsurfacing errors, different language models learn interchangeable\nrepresentations of numbers that are systematic, highly accurate and universal\nacross their hidden states and the types of input contexts. This allows us to\ncreate universal probes for each LLM and to trace information -- including the\ncauses of output errors -- to specific layers. Our results lay a fundamental\nunderstanding of how pre-trained LLMs manipulate numbers and outline the\npotential of more accurate probing techniques in addressed refinements of LLMs'\narchitectures.", "AI": {"tldr": "研究发现尽管大语言模型容易在处理数字信息时出错，但它们学习到了一致的、高度准确的数字表示，这些表示在模型的不同隐藏层和输入上下文中都是统一的。", "motivation": "解释大语言模型在输入嵌入表示中表现出的高度一致性和准确性与在处理数字信息时容易产生错误输出的现象之间的矛盾。", "method": "通过探讨语言模型如何处理数字信息并量化这些机制的准确性的下限来解释这种冲突。", "result": "发现不同语言模型学习到了彼此可交换的、系统性和高度准确的数字表示方法，允许创建针对每个模型的通用探测器并将信息追溯到特定层级。", "conclusion": "此项研究为进一步理解和改进大语言模型处理数字信息的方式提供了基础。"}}
{"id": "2510.26282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26282", "abs": "https://arxiv.org/abs/2510.26282", "authors": ["Fernando Alonso-Fernandez", "Kevin Hernandez Diaz", "Jose M. Buades", "Kiran Raja", "Josef Bigun"], "title": "Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances", "comment": "Accepted at BIOSIG 2025 conference", "summary": "We study the complementarity of different CNNs for periocular verification at\ndifferent distances on the UBIPr database. We train three architectures of\nincreasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of\neye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,\ncompare different network initialisations, and apply score-level fusion via\nlogistic regression. In addition, we use LIME heatmaps and Jensen-Shannon\ndivergence to compare attention patterns of the CNNs. While ResNet50\nconsistently performs best individually, the fusion provides substantial gains,\nespecially when combining all three networks. Heatmaps show that networks\nusually focus on distinct regions of a given image, which explains their\ncomplementarity. Our method significantly outperforms previous works on UBIPr,\nachieving a new state-of-the-art.", "AI": {"tldr": "Three CNN architectures trained on VGGFace2 eye crops are compared and fused for periocular verification on UBIPr, achieving state-of-the-art performance with significant gains especially when networks are fused.", "motivation": "The goal is to study the complementarity of different CNNs for periocular verification at varying distances using the UBIPr database.", "method": "We train three architectures of increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of eye crops from VGGFace2. We analyze performance with cosine and chi2 metrics, compare different network initializations, and apply score-level fusion via logistic regression. LIME heatmaps and Jensen-Shannon divergence are utilized to compare the attention patterns of the CNNs.", "result": "ResNet50 performs the best individually, but the fusion of all three networks provides even greater performance gains. The networks tend to focus on different regions of the image, explaining their complementarity.", "conclusion": "The proposed method outperforms previous works on the UBIPr database, achieving state-of-the-art results."}}
