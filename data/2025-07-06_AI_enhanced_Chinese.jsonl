{"id": "2507.02074", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02074", "abs": "https://arxiv.org/abs/2507.02074", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "comment": null, "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "AI": {"tldr": "本文结构化地调查并总结了利用大型语言模型进行视频数据中交通事故检测的近期方法，提供了未来研究的坚实基础。", "motivation": "由于大型语言模型（LLMs）和视觉-语言模型（VLMs）的发展，如何处理、推理和总结多模态信息得到了转变，因此本文关注于智能交通系统中超重要的问题之一：从视频流中检测交通事故。", "method": "本文通过结构化的分类策略，总结了关键的数据集，分析了模型架构，比较了性能基准，并讨论了正在进行的挑战和机遇，以调查利用大型语言模型（LLMs）进行视频数据中交通事故检测的近期方法。", "result": "本文提出了一个融合策略的结构化分类法，总结了关键的数据集，分析了模型架构，比较了性能基准，并讨论了正在进行的挑战和机遇。", "conclusion": "本文为视频理解和基础模型这一快速发展的交叉领域的未来研究提供了基础。"}}
{"id": "2507.02148", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02148", "abs": "https://arxiv.org/abs/2507.02148", "authors": ["Zijie Cai", "Christopher Metzler"], "title": "Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning", "comment": null, "summary": "Monocular depth estimation has recently advanced to provide not only relative\nbut also metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground-truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of\nstate-of-the-art models across a range of underwater conditions with different\nranges. Our results show that large-scale models trained on terrestrial (real\nor synthetic) data, while effective in in-air settings, perform poorly\nunderwater due to significant domain shifts. To address this, we fine-tune\nDepth Anything V2 with a ViT-S backbone encoder on a synthetic underwater\nvariant of the Hypersim dataset, which we generated using a physically based\nunderwater image formation model. We demonstrate our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. Our study provides\na detailed evaluation and visualization for monocular metric depth estimation\nin underwater scenes, highlighting the importance of domain adaptation and\nscale-aware supervision for achieving robust and generalizable metric depth\npredictions in challenging underwater environments for future research.", "AI": {"tldr": "本文评估了单目度量深度估计模型在水下环境中的表现，并通过对模型进行微调来改善其性能。", "motivation": "水下环境由于光衰减、散射、色彩失真和浊度等因素，使得单目度量深度预测的可靠性有限。此外，还缺乏高质量的度量真实数据。本文旨在评测和改善单目深度估计在水下环境中的性能。", "method": "本研究使用了大规模在陆地（真实或合成）数据上训练的单目度量深度估计模型，并对其中一个模型Depth Anything V2使用了ViT-S骨干编码器进行了微调，使用的数据集是通过基于物理的水下图像形成模型生成的合成水下Hypersim数据集变体。", "result": "研究结果显示，经过微调的模型在所有基准上都显示出比仅在清洁的陆地Hypersim数据集上训练的基线模型更好的性能。", "conclusion": "本文提供的详细评估和可视化结果表明，进行领域自适应和尺度感知监督对于在未来研究中在具有挑战性的水下环境中实现稳健且普遍适用的度量深度预测至关重要。"}}
{"id": "2507.02200", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02200", "abs": "https://arxiv.org/abs/2507.02200", "authors": ["Xiao Wang", "Jingtao Jiang", "Qiang Chen", "Lan Chen", "Lin Zhu", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning", "comment": "A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition", "summary": "Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT.", "AI": {"tldr": "提出了一种基于链式思考推理的事件流场景文本识别框架ESTR-CoT，用于在低光照、快速运动等极端情况下进行文本识别，解决了现有方法在可解释性和上下文逻辑推理方面存在的问题。实验结果验证了该框架的有效性和可解释性。", "motivation": "现有的基于事件流的场景文本识别方法主要采用端到端的编码器-解码器框架或大型语言模型，但这些方法在可解释性和逻辑推理方面仍然存在不足。因此，我们提出了ESTR-CoT框架以解决这些问题。", "method": "该框架首先使用EVA-CLIP把事件流转换为令牌，并使用Llama tokenizer对给定的生成提示进行编码。然后通过Q-former将视觉令牌与预训练的大型语言模型Vicuna-7B对齐，并同时输出答案和链式思考过程。", "result": "该框架在三个事件流STR基准数据集（EventSTR、WordArt*、IC15*）上进行了广泛实验，结果验证了其有效性和可解释性。", "conclusion": "ESTR-CoT框架在极端条件下表现良好，特别是在低光照、快速运动的情况下，该框架强调了其在解释能力和上下文逻辑推理方面的能力。"}}
{"id": "2507.02205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02205", "abs": "https://arxiv.org/abs/2507.02205", "authors": ["Elena Ryumina", "Maxim Markitantov", "Alexandr Axyonov", "Dmitry Ryumin", "Mikhail Dolgushin", "Alexey Karpov"], "title": "Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach", "comment": "8", "summary": "Compound Expression Recognition (CER), a subfield of affective computing,\naims to detect complex emotional states formed by combinations of basic\nemotions. In this work, we present a novel zero-shot multimodal approach for\nCER that combines six heterogeneous modalities into a single pipeline: static\nand dynamic facial expressions, scene and label matching, scene context, audio,\nand text. Unlike previous approaches relying on task-specific training data,\nour approach uses zero-shot components, including Contrastive Language-Image\nPretraining (CLIP)-based label matching and Qwen-VL for semantic scene\nunderstanding. We further introduce a Multi-Head Probability Fusion (MHPF)\nmodule that dynamically weights modality-specific predictions, followed by a\nCompound Expressions (CE) transformation module that uses Pair-Wise Probability\nAggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods\nto produce interpretable compound emotion outputs. Evaluated under multi-corpus\ntraining, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%\non Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via\nzero-shot testing, which is comparable to the results of supervised approaches\ntrained on target data. This demonstrates the effectiveness of the proposed\napproach for capturing CE without domain adaptation. The source code is\npublicly available.", "AI": {"tldr": "该研究提出了一种新颖的零样本多模态方法进行复合表情识别，结合了六种模态。在多个数据集上显示出可比较于监督方法的性能。", "motivation": "传统的复合表情识别方法通常依赖任务特定的训练数据，该研究旨在提出一种零样本的方法，以减少对大量标注数据的依赖。", "method": "该研究提出了一种新颖的零样本多模态方法进行复合表情识别，结合了六种异构模态：静态和动态面部表情、场景和标签匹配、场景上下文、音频和文本。该方法使用Contrastive Language-Image Pretraining (CLIP) 基于的标签匹配和Qwen-VL进行语义场景理解。", "result": "在多个语料库训练下，该方法在AffWild2数据集上的F1得分为46.95%，在Acted Facial Expressions in The Wild (AFEW) 数据集上的F1得分为49.02%，在C-EXPR-DB数据集上的零样本测试F1得分为34.85%。", "conclusion": "结果表明，该方法在复合情感识别中无需域适应就能获得有效结果，其性能与基于目标数据训练的监督方法相当。"}}
{"id": "2507.02088", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02088", "abs": "https://arxiv.org/abs/2507.02088", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "comment": "24 pages, 9 figures", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.", "AI": {"tldr": "创建了多任务中文偏见评估基准（McBE），用于评估语言模型中的偏见，并揭示不同模型的偏见程度。", "motivation": "鉴于现有偏见评估数据集主要基于英文和北美文化，且不适用于其他文化，本研究旨在弥补中文语言和文化背景下的偏见评估数据集的不足。", "method": "构建了多任务中文偏见评估基准（McBE），包含4,077个偏见评估实例，涵盖12类单偏见、82个子类和5个评估任务，提供广泛的类别覆盖、内容多样性和评估全面性。", "result": "对多个系列和参数规模的流行语言模型进行评估，所有模型表现出不同程度的偏见。", "conclusion": "提供了对语言模型偏见的深入分析，为未来研究提供了新的见解。"}}
{"id": "2507.02212", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02212", "abs": "https://arxiv.org/abs/2507.02212", "authors": ["Takuro Kawada", "Shunsuke Kitada", "Sota Nemoto", "Hitoshi Iyatomi"], "title": "SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers", "comment": "21 pages, 15 figures, 4 tables. Project Page:\n  https://iyatomilab.github.io/SciGA/", "summary": "Graphical Abstracts (GAs) play a crucial role in visually conveying the key\nfindings of scientific papers. While recent research has increasingly\nincorporated visual materials such as Figure 1 as de facto GAs, their potential\nto enhance scientific communication remains largely unexplored. Moreover,\ndesigning effective GAs requires advanced visualization skills, creating a\nbarrier to their widespread adoption. To tackle these challenges, we introduce\nSciGA-145k, a large-scale dataset comprising approximately 145,000 scientific\npapers and 1.14 million figures, explicitly designed for supporting GA\nselection and recommendation as well as facilitating research in automated GA\ngeneration. As a preliminary step toward GA design support, we define two\ntasks: 1) Intra-GA recommendation, which identifies figures within a given\npaper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,\nwhich retrieves GAs from other papers to inspire the creation of new GAs. We\nprovide reasonable baseline models for these tasks. Furthermore, we propose\nConfidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation\nmetric that offers a fine-grained analysis of model behavior. CAR addresses\nlimitations in traditional ranking-based metrics by considering cases where\nmultiple figures within a paper, beyond the explicitly labeled GA, may also\nserve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a\nfoundation for advancing visual scientific communication while contributing to\nthe development of AI for Science.", "AI": {"tldr": "The paper introduces SciGA-145k, a large-scale dataset aimed at supporting Graphical Abstracts (GAs) selection, recommendation and automated generation, through two defined tasks and a novel metric named CAR.", "motivation": "The motivation is to enhance scientific communication through effective GAs, despite the barrier of advanced visualization skills required to design them.", "method": "The method involves creating a large dataset named SciGA-145k with scientific papers and figures for GA support, defining tasks for GA recommendation and proposing the CAR metric.", "result": "The paper provides baseline models for GA recommendation tasks and a detailed metric analysis through CAR.", "conclusion": "The conclusion is that SciGA-145k and the proposed CAR metric lay the groundwork for improving visual scientific communication and AI in science research."}}
{"id": "2507.02145", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02145", "abs": "https://arxiv.org/abs/2507.02145", "authors": ["Keyan Jin", "Yapeng Wang", "Leonel Santos", "Tao Fang", "Xu Yang", "Sio Kei Im", "Hugo Gonçalo Oliveira"], "title": "Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization", "comment": null, "summary": "Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization.", "AI": {"tldr": "研究发现，尽管大型语言模型在总结任务上取得显著进展，但在对话总结中，基于逐步推理的模型并不一定优于非推理模型，反而可能产生冗长和不一致的总结。", "motivation": "探索逐步推理模型在对话场景中的总结能力，特别是在要求简洁和抽象能力的情况下。", "method": "首次对尖端推理大型语言模型和非推理模型进行了全面系统的评估，涵盖了通用、角色导向和查询导向的对话总结，使用了多种语言、领域和总结长度的强基准。", "result": "研究发现，尽管其他推理密集型任务中逐步推理模型表现良好，但在对话总结中它们并不一定更好，反而可能产生冗长和不一致的总结。", "conclusion": "研究揭示了当前推理语言模型在对话总结中的局限性，并强调需要针对实际对话总结开发特定的建模和评估策略。"}}
{"id": "2507.02217", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02217", "abs": "https://arxiv.org/abs/2507.02217", "authors": ["Brandon Trabucco", "Qasim Wani", "Benjamin Pikus", "Vasu Sharma"], "title": "Understanding Trade offs When Conditioning Synthetic Data", "comment": null, "summary": "Learning robust object detectors from only a handful of images is a critical\nchallenge in industrial vision systems, where collecting high quality training\ndata can take months. Synthetic data has emerged as a key solution for data\nefficient visual inspection and pick and place robotics. Current pipelines rely\non 3D engines such as Blender or Unreal, which offer fine control but still\nrequire weeks to render a small dataset, and the resulting images often suffer\nfrom a large gap between simulation and reality. Diffusion models promise a\nstep change because they can generate high quality images in minutes, yet\nprecise control, especially in low data regimes, remains difficult. Although\nmany adapters now extend diffusion beyond plain text prompts, the effect of\ndifferent conditioning schemes on synthetic data quality is poorly understood.\nWe study eighty diverse visual concepts drawn from four standard object\ndetection benchmarks and compare two conditioning strategies: prompt based and\nlayout based. When the set of conditioning cues is narrow, prompt conditioning\nyields higher quality synthetic data; as diversity grows, layout conditioning\nbecomes superior. When layout cues match the full training distribution,\nsynthetic data raises mean average precision by an average of thirty four\npercent and by as much as one hundred seventy seven percent compared with using\nreal data alone.", "AI": {"tldr": "本研究通过分析两种合成数据生成策略，发现基于布局的条件策略在提升对象检测性能方面表现更佳，尤其在数据多样性较高的情况下。使用匹配完整训练分布的布局线索生成的合成数据可显著提升检测准确率。", "motivation": "研究动机在于解决从有限图像中学习鲁棒对象检测器的挑战。当前使用3D引擎生成数据的方法耗时长且模拟与现实之间存在较大差异。扩散模型虽然可以快速生成高质量图像，但在低数据环境下实现精确控制仍具挑战性。", "method": "研究针对四个标准对象检测基准中的八十种多样化视觉概念进行了分析，比较了基于提示和基于布局两种条件策略的效果。", "result": "本研究分析了两种条件策略（基于提示和基于布局）在生成高质量合成数据方面的效果，特别是针对对象检测任务。研究发现，在条件线索较窄时，基于提示的条件策略生成的合成数据质量更高；随着多样性的增加，基于布局的条件策略则表现出更好的性能。当布局线索与完整的训练分布相匹配时，使用合成数据可以将平均精度提高34%，甚至最高可以提高177%，相比单独使用真实数据。", "conclusion": "研究证明了基于布局的条件策略在合成数据生成中的优越性，特别是在条件多样性增加时。使用合成数据可以显著提高对象检测任务的性能。"}}
{"id": "2507.02199", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02199", "abs": "https://arxiv.org/abs/2507.02199", "authors": ["Wenquan Lu", "Yuechuan Yang", "Kyle Lee", "Yanshu Li", "Enqi Liu"], "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot.", "AI": {"tldr": "本文探究了递归式变压器模型Huginn-3.5B中是否形成了可解释的内部化推理结构。结果表明，虽然存在一些证据，但是限制较大，递归深度增加对性能提升有限。", "motivation": "本文旨在研究Huginn-3.5B（一种在推理时重复使用层数而不会增加参数数量的深度递归变压器）中是否形成了推理结构，并探讨内部化推理在潜在空间中的表现。", "method": "本文使用了一系列探测技术，包括Logit Lens和Coda Lens，来探究模型在算术任务上的内部行为。", "result": "研究结果显示，通过跟踪最终和中间结果标记的排名轨迹，只有有限的证据表明存在可解释的潜在链式思维。另外，研究还揭示了递归块中的探测不一致性。", "conclusion": "实验证明，增加递归深度只能带来微不足道的收益，并远不及那些明确外部化推理步骤的模型。"}}
{"id": "2507.02222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02222", "abs": "https://arxiv.org/abs/2507.02222", "authors": ["Tian Gao", "Zhiyuan Zhang", "Kaijie Yin", "Xu-Cheng Zhong", "Hui Kong"], "title": "High-Fidelity Differential-information Driven Binary Vision Transformer", "comment": null, "summary": "The binarization of vision transformers (ViTs) offers a promising approach to\naddressing the trade-off between high computational/storage demands and the\nconstraints of edge-device deployment. However, existing binary ViT methods\noften suffer from severe performance degradation or rely heavily on\nfull-precision modules. To address these issues, we propose DIDB-ViT, a novel\nbinary ViT that is highly informative while maintaining the original ViT\narchitecture and computational efficiency. Specifically, we design an\ninformative attention module incorporating differential information to mitigate\ninformation loss caused by binarization and enhance high-frequency retention.\nTo preserve the fidelity of the similarity calculations between binary Q and K\ntensors, we apply frequency decomposition using the discrete Haar wavelet and\nintegrate similarities across different frequencies. Additionally, we introduce\nan improved RPReLU activation function to restructure the activation\ndistribution, expanding the model's representational capacity. Experimental\nresults demonstrate that our DIDB-ViT significantly outperforms\nstate-of-the-art network quantization methods in multiple ViT architectures,\nachieving superior image classification and segmentation performance.", "AI": {"tldr": "论文提出了一种名为DIDB-ViT的新二值化视觉变压器，以解决现有二值化ViT性能下降或依赖实数操作的问题，以保持信息性和计算效率。", "motivation": "论文指出，视觉变压器（ViTs）的二值化为解决高计算/存储需求与边缘设备部署之间的矛盾提供了一种有前景的方法。然而，现有的二值化ViT方法往往面临着性能显著下降或严重依赖全精度操作的问题。", "method": "该论文提出了DIDB-ViT，这是一种新颖的二值化视觉变压器（ViT），在保持原始ViT架构和计算效率的同时，其信息性强。具体来说，设计了一个包含差异信息的明智注意力模块，以减轻二值化引起的信息损失，并增强高频保留。同时，通过离散哈夫小波的频率分解和不同频率下的相似性的集成来保持二值化Q和K张量之间的相似性计算的保真度。此外，引入了改进的RPReLU激活函数来重新构造激活分布，扩大模型的表示能力。", "result": "实验结果显示，DIDB-ViT在网络量化领域超越了现有最先进的方法，同时在多种ViT架构中展现了出色的图像分类和分割性能。", "conclusion": "该论文提出的方法在保持高信息性和计算效率的同时，解决了ViT二值化面临的关键挑战，其DIDB-ViT方法在不同基准测试中表现优异。"}}
{"id": "2507.02221", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02221", "abs": "https://arxiv.org/abs/2507.02221", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "comment": "11 pages, 1 figure, 7 tables", "summary": "Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds.", "AI": {"tldr": "本文介绍了GDC Cohort Copilot，一个开源工具，用于通过自然语言描述生成GDC队列过滤器，优于商业化的GPT-4方法。", "motivation": "为了简化GDC用户，特别是新用户在数百个可能的字段和属性中寻找特定队列描述符的困难，我们开发了一种可以将用户用自然语言描述的所需的队列转化为GDC队列过滤器的工具。", "method": "我们引入了GDC Cohort Copilot，这是一个开源的辅助工具，用于从GDC中整理队列。GDC Cohort Copilot能根据用户输入的自然语言描述自动生成GDC队列过滤器，并可以将队列导回到GDC进行进一步分析。此外，我们还开发和评估了几种大型语言模型（LLMs）用于GDC Cohort Copilot，并证明了我们的本地服务的开源GDC Cohort LLM比GPT-4的提示生成GDC队列效果更好。", "result": "我们展示了本地服务的开源GDC Cohort LLM在生成GDC队列方面比GPT-4的提示表现更好。", "conclusion": "GDC Cohort Copilot及其相关模型已被证明在生成符合用户自然语言描述的GDC队列方面有效，并且其开源和本地服务特性使其对用户群体具有吸引力。"}}
{"id": "2507.02250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02250", "abs": "https://arxiv.org/abs/2507.02250", "authors": ["Jiangxia Chen", "Tongyuan Huang", "Ke Song"], "title": "FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model", "comment": null, "summary": "3D semantic occupancy prediction plays a pivotal role in autonomous driving.\nHowever, inherent limitations of fewframe images and redundancy in 3D space\ncompromise prediction accuracy for occluded and distant scenes. Existing\nmethods enhance performance by fusing historical frame data, which need\nadditional data and significant computational resources. To address these\nissues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement\noccupancy network with flow matching selective state space model for few-frame\n3D occupancy prediction. Firstly, to generate missing features, we designed a\nfeature refinement module based on a flow matching model, which is called Flow\nMatching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and\nPlane Selective SSM (PS3M), we selectively filter TPV features to reduce the\nimpact of air voxels on non-air voxels, thereby enhancing the overall\nefficiency of the model and prediction capability for distant scenes. Finally,\nwe design the Mask Training (MT) method to enhance the robustness of FMOcc and\naddress the issue of sensor data loss. Experimental results on the\nOcc3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing\nstate-of-theart methods. Our FMOcc with two frame input achieves notable scores\nof 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on\nOpenOcc with 5.4 G inference memory and 330ms inference time.", "AI": {"tldr": "论文介绍了一种新的网络FMOcc，用于提高基于少量帧数据的3D占用预测准确性。", "motivation": "现有的方法通过融合历史帧来提高性能，但这需要额外的数据和计算资源。此论文提出FMOcc来解决由于少数帧图像和3D空间冗余导致的遮挡和远距离场景预测精度低的问题。", "method": "FMOcc, 一个基于流匹配选择性状态空间模型（FMSSM）、TPV SSM层和PS3M的三视角视图（TPV）细化占有率网络，旨在解决少数帧3D占用预测的问题。", "result": "实验结果显示FMOcc在Occ3D-nuScenes和OpenOcc数据集上的表现优于现有最先进方法。使用两帧输入时，在Occ3D-nuScenes验证上的RayIoU为43.1%，mIoU为39.8%，在OpenOcc数据集上RayIoU为42.6%，推理内存为5.4G，推理时间为330毫秒。", "conclusion": "通过设计FMSSM、TPV SSM层、PS3M和Mask Training方法，FMOcc实现了对远距离场景预测能力的增强，以及模型效率与鲁棒性的提升。相比现有方法，FMOcc在性能上有所提升，同时保持较低的计算资源需求。"}}
{"id": "2507.02259", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02259", "abs": "https://arxiv.org/abs/2507.02259", "authors": ["Hongli Yu", "Tinghong Chen", "Jiangtao Feng", "Jiangjie Chen", "Weinan Dai", "Qiying Yu", "Ya-Qin Zhang", "Wei-Ying Ma", "Jingjing Liu", "Mingxuan Wang", "Hao Zhou"], "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent", "comment": "Project Page: https://memagent-sialab.github.io/", "summary": "Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.", "AI": {"tldr": "论文通过MemAgent和扩展的DAPO算法解决了长上下文处理的挑战，显示了极好的扩展能力。测试表明可以实现从8K上下文扩展到32K文本，并在3.5M QA任务中保持性能损失低于5%。", "motivation": "论文动机在于解决当前长文本处理技术面临的挑战，即在性能不下降的情况下，实现对于无限长文档的线性复杂度计算。", "method": "提出了一个新的代理工作流程MemAgent，它采用分段读取文本并使用覆盖策略更新内存的方法。同时，扩展了DAPO算法以便通过独立上下文的多对话生成助力训练过程。", "result": "尽管在长度外推、有效注意力和内存模块方面有所改进，但在处理长文本时，如何在性能不降低的前提下实现线性复杂度的无限长文档处理，仍然是一个最终挑战。我们以端到端的方式针对长文本任务进行直接优化，并引入了一种新型的代理工作流程MemAgent，该代理分段读取文本并通过覆盖策略更新内存。我们扩展了DAPO算法，通过独立上下文的多对话生成来促进训练。MemAgent展示了卓越的长上下文处理能力，能够从8K上下文训练扩展到32K文本，并成功完成3.5M QA任务，性能损失小于5%，在512K RULER测试中达到95%以上的表现。", "conclusion": "MemAgent展示了处理长上下文的强大能力，并能在从较小上下文扩展到大规模任务时维持相对稳定的性能，这证明了对长文档处理的实质性进展。"}}
{"id": "2507.02252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02252", "abs": "https://arxiv.org/abs/2507.02252", "authors": ["Zeyu Lei", "Hongyuan Yu", "Jinlin Wu", "Zhen Chen"], "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement", "comment": null, "summary": "Precise surgical interventions are vital to patient safety, and advanced\nenhancement algorithms have been developed to assist surgeons in\ndecision-making. Despite significant progress, these algorithms are typically\ndesigned for single tasks in specific scenarios, limiting their effectiveness\nin complex real-world situations. To address this limitation, we propose\nSurgVisAgent, an end-to-end intelligent surgical vision agent built on\nmultimodal large language models (MLLMs). SurgVisAgent dynamically identifies\ndistortion categories and severity levels in endoscopic images, enabling it to\nperform a variety of enhancement tasks such as low-light enhancement,\noverexposure correction, motion blur elimination, and smoke removal.\nSpecifically, to achieve superior surgical scenario understanding, we design a\nprior model that provides domain-specific knowledge. Additionally, through\nin-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent\ndelivers customized image enhancements tailored to a wide range of distortion\ntypes and severity levels, thereby addressing the diverse requirements of\nsurgeons. Furthermore, we construct a comprehensive benchmark simulating\nreal-world surgical distortions, on which extensive experiments demonstrate\nthat SurgVisAgent surpasses traditional single-task models, highlighting its\npotential as a unified solution for surgical assistance.", "AI": {"tldr": "本文提出了一种名为SurgVisAgent的端到端智能手术视觉代理，它可以识别和减轻多种类型的图像失真，适应复杂的手术环境，提供了统一的手术辅助方案。", "motivation": "尽管在手术算法方面取得了显著进步，但这些算法通常针对特定场景下的单个任务设计，这在复杂的真实世界环境中限制了它们的效果。为了克服这一局限性，需要一个能够应对多种失真类型和严重程度的综合性解决方案。", "method": "本文提出了SurgVisAgent，一种基于多模态大规模语言模型的端到端智能手术视觉代理。SurgVisAgent能够识别内窥镜图像中的畸变类别和严重程度，并实现低光增强、过曝校正、运动模糊消除和烟雾去除等增强任务。为了实现对手术场景的深入理解，设计了一个提供领域特定知识的先验模型。此外，通过上下文中的少量学习和链式思维推理，SurgVisAgent能够针对不同类型的畸变和严重程度提供定制化的图像增强。", "result": "通过构建一个模拟现实世界手术失真的全面基准，实验结果表明，SurgVisAgent超越了传统的单一任务模型，展示了它作为统一的手术辅助方案的潜力。", "conclusion": "SurgVisAgent能够应对手术过程中遇到的各种图像失真问题，提供统一的解决方案，相较于传统单一任务模型具有显著优势。这为提升手术过程中的图像质量提供了强有力的支持，进而增强手术成功率和安全性。"}}
{"id": "2507.02302", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02302", "abs": "https://arxiv.org/abs/2507.02302", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "comment": "22 pages, 5 figures, ACL 2025 Main", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX.", "AI": {"tldr": "DoMIX是一个新的算法，用于解决现有的连续领域自适应预训练方法中存在的挑战，该方法效果好且计算成本低，并能为特定任务提供定制化的预训练模型。", "motivation": "针对现有的连续领域自适应预训练方法存在的问题，如计算成本高、对增量数据的顺序敏感、以及提供单一的通用模型与领域自适应预训练的本质相矛盾，DoMIX旨在提供一种更优的解决方案。", "method": "通过利用LoRA模块（一种参数高效微调方法），DoMIX提出了一个有效且能够并行进行领域自适应预训练的新方法，这种方法对抗领域顺序的鲁棒性强，并能有效利用积累的知识，提供特定任务所需的预训练模型。", "result": "该论文展示了DoMIX方法不仅可以在领域自适应预训练中有效执行，而且可以扩展到标准的大规模语言模型微调场景中。", "conclusion": "DoMIX能够解决现有连续领域自适应预训练方法的挑战，提供了一种高效、鲁棒且可定制的预训练模型解决方案。此外，这种方法在标准的大规模语言模型微调场景中也显示出良好的适应性。"}}
{"id": "2507.02265", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02265", "abs": "https://arxiv.org/abs/2507.02265", "authors": ["Zhangding Liu", "Neda Mohammadi", "John E. Taylor"], "title": "Multi-Label Classification Framework for Hurricane Damage Assessment", "comment": "9 pages, 3 figures. Accepted at the ASCE International Conference on\n  Computing in Civil Engineering (i3CE 2025)", "summary": "Hurricanes cause widespread destruction, resulting in diverse damage types\nand severities that require timely and accurate assessment for effective\ndisaster response. While traditional single-label classification methods fall\nshort of capturing the complexity of post-hurricane damage, this study\nintroduces a novel multi-label classification framework for assessing damage\nusing aerial imagery. The proposed approach integrates a feature extraction\nmodule based on ResNet and a class-specific attention mechanism to identify\nmultiple damage types within a single image. Using the Rescuenet dataset from\nHurricane Michael, the proposed method achieves a mean average precision of\n90.23%, outperforming existing baseline methods. This framework enhances\npost-hurricane damage assessment, enabling more targeted and efficient disaster\nresponse and contributing to future strategies for disaster mitigation and\nresilience. This paper has been accepted at the ASCE International Conference\non Computing in Civil Engineering (i3CE 2025), and the camera-ready version\nwill appear in the official conference proceedings.", "AI": {"tldr": "研究提出了一种基于多标签分类框架的飓风损害评估方法，使用航空影像，达到了更好的评估准确性。", "motivation": "传统的单标签分类方法无法捕捉到飓风过后损害的复杂性。因此，需要一种新的方法来及时准确地对损害进行评估，以有效地进行灾害应对。", "method": "此研究介绍了一种新型的多标签分类框架，用于利用航空影像评估飓风后的损害。该方法结合了基于ResNet的特征提取模块和特定类别的注意机制，以识别单个图像中的多种损害类型。", "result": "使用Michael飓风的Rescuenet数据集，所提出的方法达到了90.23%的平均精度，优于现有的基线方法。", "conclusion": "该框架增强了飓风后的损害评估，使其更具针对性和效率，并为灾害预防和恢复策略提出贡献。"}}
{"id": "2507.02357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02357", "abs": "https://arxiv.org/abs/2507.02357", "authors": ["Christian Jaumann", "Annemarie Friedrich", "Rainer Lienhart"], "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models", "comment": "Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.", "AI": {"tldr": "该论文描述了一个用于SciVQA 2025共享任务——科学视觉问答的系统。系统采用两种多模态大型语言模型的组合和多种少量示例检索策略，并根据图表和问题类型选择模型和少量示例设置，依据模型的置信度选择答案。在盲测数据上，该系统排名第三，平均F1分达到85.12。代码公开可用。", "motivation": "旨在提高科学视觉问答任务的性能，利用多模型组合和少量示例检索策略来提升表现。", "method": "Structure", "result": "{\n  \"tldr\": \"该论文描述了一个用于SciVQA 2025共享任务——科学视觉问答的系统。系统采用两种多模态大型语言模型的组合和多种少量示例检索策略，并根据图表和问题类型选择模型和少量示例设置，依据模型的置信度选择答案。在盲测数据上，该系统排名第三，平均F1分达到85.12。代码公开可用。\",\n  \"motivation\": \"旨在提高科学视觉问答任务的性能，利用多模型组合和少量示例检索策略来提升表现。\",\n  \"method\": \"使用多种多模态大型语言模型组合和少量实例检索策略，根据问题类型和图表类型选择具体的模型和检索策略，依据模型的置信度水平选择最终答案。\",\n  \"result\": \"在盲测数据上，该系统的平均F1分（基于ROUGE-1, ROUGE-L, 和BERTS）达到了85.12，排名第三。\",\n  \"conclusion\": \"研究结果表明，多模型组合和少量实例检索策略在提高科学视觉问答的准确性方面是有效的，公开的代码有利于进一步的研究工作。\"}\n}\n", "conclusion": "研究结果表明，多模型组合和少量实例检索策略在提高科学视觉问答的准确性方面是有效的，公开的代码有利于进一步的研究工作。"}}
{"id": "2507.02268", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.02268", "abs": "https://arxiv.org/abs/2507.02268", "authors": ["Yuxiang Zhang", "Wei Li", "Wen Jia", "Mengmeng Zhang", "Ran Tao", "Shunlin Liang"], "title": "Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation", "comment": null, "summary": "Utilizing hyperspectral remote sensing technology enables the extraction of\nfine-grained land cover classes. Typically, satellite or airborne images used\nfor training and testing are acquired from different regions or times, where\nthe same class has significant spectral shifts in different scenes. In this\npaper, we propose a Bi-directional Domain Adaptation (BiDA) framework for\ncross-domain hyperspectral image (HSI) classification, which focuses on\nextracting both domain-invariant features and domain-specific information in\nthe independent adaptive space, thereby enhancing the adaptability and\nseparability to the target scene. In the proposed BiDA, a triple-branch\ntransformer architecture (the source branch, target branch, and coupled branch)\nwith semantic tokenizer is designed as the backbone. Specifically, the source\nbranch and target branch independently learn the adaptive space of source and\ntarget domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is\ndeveloped in coupled branch for feature interaction and inter-domain\ncorrelation mining. Furthermore, a bi-directional distillation loss is designed\nto guide adaptive space learning using inter-domain correlation. Finally, we\npropose an Adaptive Reinforcement Strategy (ARS) to encourage the model to\nfocus on specific generalized feature extraction within both source and target\nscenes in noise condition. Experimental results on cross-temporal/scene\nairborne and satellite datasets demonstrate that the proposed BiDA performs\nsignificantly better than some state-of-the-art domain adaptation approaches.\nIn the cross-temporal tree species classification task, the proposed BiDA is\nmore than 3\\%$\\sim$5\\% higher than the most advanced method. The codes will be\navailable from the website:\nhttps://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.", "AI": {"tldr": "本研究提出了一种名为BiDA的框架，用于提高跨域高光谱图像分类的适应性和可分离性，在多个实验中表现出色。", "motivation": "高光谱遥感技术可用于土地覆盖类别提取，但是不同地区和时间的图像存在显著的光谱漂移，因此本研究旨在解决跨域场景下的这一挑战。", "method": "提出了一种双向领域自适应（BiDA）框架，用于跨域高光谱图像分类，专注于提取源域和目标域的领域不变性和特定领域信息。框架包括三分支Transformer架构（源分支、目标分支和耦合分支），设计了耦合多头交叉注意力机制和双向蒸馏损失，最后提出了适应性增强策略（ARS）。", "result": "实验结果表明，该BiDA框架在跨时/场景的航空和卫星数据集上显著优于一些最先进的领域自适应方法。在跨时树木分类任务中，BiDA比最先进方法高3%到5%。", "conclusion": "研究结果表明，BiDA框架相比现有方法在跨域高光谱图像分类任务中实现了更优异的性能，尤其在跨时树木分类任务中显示出显著优势。"}}
{"id": "2507.02364", "categories": ["cs.CL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.02364", "abs": "https://arxiv.org/abs/2507.02364", "authors": ["Pilsung Kang"], "title": "QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers", "comment": null, "summary": "Parameterized quantum circuits (PQCs) have recently emerged as promising\ncomponents for enhancing the expressibility of neural architectures. In this\nwork, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the\nfeedforward network (FFN) modules of a compact BERT variant are replaced by\nPQC-based layers. This design is motivated by the dominant parameter\ncontribution of FFNs, which account for approximately two-thirds of the\nparameters within standard Transformer encoder blocks. While prior studies have\nprimarily integrated PQCs into self-attention modules, our work focuses on the\nFFN and systematically investigates the trade-offs between PQC depth,\nexpressibility, and trainability. Our final PQC architecture incorporates a\nresidual connection, both $R_Y$ and $R_Z$ rotations, and an alternating\nentanglement strategy to ensure stable training and high expressibility. Our\nexperiments, conducted on a classical simulator, on the SST-2 and DBpedia\nbenchmarks demonstrate two key findings. First, a carefully configured\nQFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its\nclassical counterpart in a full-data setting while reducing FFN-specific\nparameters by over 99%. Second, our model exhibits a consistent and competitive\nedge in few-shot learning scenarios, confirming its potential for superior data\nefficiency. These results, supported by an ablation study on a non-optimized\nPQC that failed to learn, confirm that PQCs can serve as powerful and\nparameter-efficient alternatives to classical FFNs when co-designed with\nfoundational deep learning principles.", "AI": {"tldr": "本研究提出QFFN-BERT，将BERT变体中的前馈网络替换为PQC基层，证明了PQC作为经典FFN替代方案时，可以实现参数高效和数据高效。", "motivation": "在大多数变压器编码器块中，FFN约占有总量的三分之二的参数，而以往的研究主要集中在将PQC集成于自我注意力模块中。因此，本研究选择FFN进行探索，系统地研究了PQC深度、表达能力和可训练性之间的权衡。", "method": "本研究设计了一种名为QFFN-BERT的混合量子经典变压器，将紧凑型BERT变体中的前馈网络（FFN）模块替换为基于参数化量子电路（PQC）的层。PQC架构中包含残差连接、$R_Y$和$R_Z$旋转以及交替纠缠策略，以确保稳定训练和高表达能力。", "result": "实验结果表明，优化后的QFFN-BERT可以在全数据设置下达到基线准确率的102.0%，同时将FFN相关的参数减少了超过99%。此外，该模型在少量样本学习场景中表现出色，证明了其在数据效率方面的潜在优势。", "conclusion": "综合以上结果，可以得出结论，当与基础深度学习原则协同设计时，参数化量子电路可以作为强大的经典前馈网络替代方案，并享有参数高效的优势。"}}
{"id": "2507.02270", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02270", "abs": "https://arxiv.org/abs/2507.02270", "authors": ["Fanghai Yi", "Zehong Zheng", "Zexiao Liang", "Yihang Dong", "Xiyang Fang", "Wangyu Wu", "Xuhang Chen"], "title": "MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement", "comment": "Accepted by IEEE SMC 2025", "summary": "Enhancing underwater images is crucial for exploration. These images face\nvisibility and color issues due to light changes, water turbidity, and bubbles.\nTraditional prior-based methods and pixel-based methods often fail, while deep\nlearning lacks sufficient high-quality datasets. We introduce the Multi-Axis\nConditional Lookup (MAC-Lookup) model, which enhances visual quality by\nimproving color accuracy, sharpness, and contrast. It includes Conditional 3D\nLookup Table Color Correction (CLTCC) for preliminary color and quality\ncorrection and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.\nThis model prevents over-enhancement and saturation while handling underwater\nchallenges. Extensive experiments show that MAC-Lookup excels in enhancing\nunderwater images by restoring details and colors better than existing methods.\nThe code is https://github.com/onlycatdoraemon/MAC-Lookup.", "AI": {"tldr": "文章提出了MAC-Lookup模型以解决水下图像的能见度和颜色问题，通过条件3D查找表色彩校正和多轴自适应增强技术提升了图像质量。", "motivation": "水下图像由于光照变化、水体浑浊和气泡等原因存在能见度和颜色问题，传统的基于先验和像素的方法往往失效，而深度学习则缺乏高质量的数据集。", "method": "引入了MAC-Lookup模型，该模型包含条件3D查找表色彩校正（CLTCC）和多轴自适应增强（MAAE），以提高水下图像的颜色准确性、清晰度和对比度。", "result": "实验结果表明，MAC-Lookup模型在恢复水下图像细节和颜色方面优于现有方法。", "conclusion": "MAC-Lookup模型不仅能提升水下图像质量，还能有效避免过度增强和饱和度问题。"}}
{"id": "2507.02378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02378", "abs": "https://arxiv.org/abs/2507.02378", "authors": ["Weijie Lyu", "Sheng-Jun Huang", "Xuan Xia"], "title": "Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection", "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.", "AI": {"tldr": "This paper introduces a parametric model for selecting high-quality code data to improve the training efficiency and performance of large language models.", "motivation": "The motivation behind this work is to improve the training efficiency and performance of LLMs in code generation, which current methods fail to do effectively due to a focus on data quantity over quality.", "method": "Our method utilizes a parametric model to select high-quality code data, optimizing for distribution consistency and diversity, which improves the efficiency and performance of large language models.", "result": "Experimental results show that the proposed method achieves performance gains of 2.4% on HumanEval and 2.3% on MBPP, using only 10K samples compared to a 92K full-sampled baseline.", "conclusion": "The research concludes that their method not only outperforms existing sampling approaches but also does so with significantly fewer data points, highlighting its effectiveness in enhancing model performance while reducing computational costs."}}
{"id": "2507.02271", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.02271", "abs": "https://arxiv.org/abs/2507.02271", "authors": ["Feizhen Huang", "Yu Wu", "Yutian Lin", "Bo Du"], "title": "Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation", "comment": "Accepted by IJCAI 2025", "summary": "Video-to-Audio (V2A) Generation achieves significant progress and plays a\ncrucial role in film and video post-production. However, current methods\noverlook the cinematic language, a critical component of artistic expression in\nfilmmaking. As a result, their performance deteriorates in scenarios where\nFoley targets are only partially visible. To address this challenge, we propose\na simple self-distillation approach to extend V2A models to cinematic language\nscenarios. By simulating the cinematic language variations, the student model\nlearns to align the video features of training pairs with the same audio-visual\ncorrespondences, enabling it to effectively capture the associations between\nsounds and partial visual information. Our method not only achieves impressive\nimprovements under partial visibility across all evaluation metrics, but also\nenhances performance on the large-scale V2A dataset, VGGSound.", "AI": {"tldr": "本文提出了一种自我蒸馏方法来改进视频到音频生成技术，使之能够更好地处理电影语言场景中的部分可见目标问题。", "motivation": "当前方法未能充分考虑电影语言，导致在目标部分可见的情况下表现不佳，因此需要改进。", "method": "通过模拟电影语言变化，训练模型使视频和音频特征对齐，进而捕捉声音与部分视觉信息之间的关联。", "result": "通过提出一种简单的自我蒸馏方法，该方法能够扩展V2A模型以适应电影语言场景，从而当目标仅部分可见时改进音频生成性能。实验表明该方法在所有评估指标中均取得显著改进，并提高了在大规模V2A数据集VGGSound上的性能。", "conclusion": "该研究提出了新的自我蒸馏方法，有效提升了在处理部分可见目标时的视频到音频生成质量，并在大规模数据集上表现优异。"}}
{"id": "2507.02407", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.02407", "abs": "https://arxiv.org/abs/2507.02407", "authors": ["Mark Atta Mensah", "Isaac Wiafe", "Akon Ekpezu", "Justice Kwame Appati", "Jamal-Deen Abdulai", "Akosua Nyarkoa Wiafe-Akenten", "Frank Ernest Yeboah", "Gifty Odame"], "title": "Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability", "comment": "This version has been reviewed and accepted for presentation at the\n  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025\n  in Munich, Germany. 17 pages, 4 figures, 1 table", "summary": "Most existing automatic speech recognition (ASR) research evaluate models\nusing in-domain datasets. However, they seldom evaluate how they generalize\nacross diverse speech contexts. This study addresses this gap by benchmarking\nseven Akan ASR models built on transformer architectures, such as Whisper and\nWav2Vec2, using four Akan speech corpora to determine their performance. These\ndatasets encompass various domains, including culturally relevant image\ndescriptions, informal conversations, biblical scripture readings, and\nspontaneous financial dialogues. A comparison of the word error rate and\ncharacter error rate highlighted domain dependency, with models performing\noptimally only within their training domains while showing marked accuracy\ndegradation in mismatched scenarios. This study also identified distinct error\nbehaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned\nWhisper Akan models led to more fluent but potentially misleading transcription\nerrors, Wav2Vec2 produced more obvious yet less interpretable outputs when\nencountering unfamiliar inputs. This trade-off between readability and\ntransparency in ASR errors should be considered when selecting architectures\nfor low-resource language (LRL) applications. These findings highlight the need\nfor targeted domain adaptation techniques, adaptive routing strategies, and\nmultilingual training frameworks for Akan and other LRLs.", "AI": {"tldr": "该研究详述了基于Transformer架构的不同ASR模型在处理阿肯语不同领域语音数据时的表现差异，并指出这些模型存在显著的域依赖性及错误行为的区别。", "motivation": "大多数现有的自动语音识别研究使用领域内数据集来评估模型，但很少评估它们跨多样语音环境的表现。这项研究旨在填补这一空白，探讨ASR模型在不同域上的性能表现。", "method": "本研究通过使用四种阿肯语语音语料库来评估七种基于Transformer架构的阿肯语自动语音识别（ASR）模型，包括Whisper和Wav2Vec2模型的性能。语料库涵盖了不同的领域，包括文化相关的图像描述、非正式对话、圣经经文朗读以及自发性金融对话。", "result": "研究发现，当模型面临与训练域不匹配的情境时，其准确性显著下降，这表明了ASR模型存在域依赖性。此外，Whisper和Wav2Vec2模型在面对不熟悉输入时具有不同的错误行为。", "conclusion": "针对低资源语言的ASR应用选择架构时，需要在转录错误的流畅性和透明性之间权衡。研究还提出，为了改善低资源语言的性能，应采取特定的领域适应技术、自适应路由策略以及多语言训练框架。"}}
{"id": "2507.02279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02279", "abs": "https://arxiv.org/abs/2507.02279", "authors": ["Juntao Liu", "Liqiang Niu", "Wenchao Chen", "Jie Zhou", "Fandong Meng"], "title": "LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models", "comment": null, "summary": "Existing visual token compression methods for Multimodal Large Language\nModels (MLLMs) predominantly operate as post-encoder modules, limiting their\npotential for efficiency gains. To address this limitation, we propose LaCo\n(Layer-wise Visual Token Compression), a novel framework that enables effective\ntoken compression within the intermediate layers of the vision encoder. LaCo\nintroduces two core components: 1) a layer-wise pixel-shuffle mechanism that\nsystematically merges adjacent tokens through space-to-channel transformations,\nand 2) a residual learning architecture with non-parametric shortcuts that\npreserves critical visual information during compression. Extensive experiments\nindicate that our LaCo outperforms all existing methods when compressing tokens\nin the intermediate layers of the vision encoder, demonstrating superior\neffectiveness. In addition, compared to external compression, our method\nimproves training efficiency beyond 20% and inference throughput over 15% while\nmaintaining strong performance.", "AI": {"tldr": "LaCo is a new framework allowing token compression in intermediate layers of vision encoders, offering better performance and efficiency compared to existing methods.", "motivation": "The motivation behind LaCo is to overcome the limitation of existing visual token compression methods, which predominantly operate as post-encoder modules. The objective is to enable more efficient token compression that can lead to significant gains in training and inference efficiency.", "method": "LaCo (Layer-wise Visual Token Compression) proposes a novel framework for performing token compression within the intermediate layers of a vision encoder. It introduces two core components: a layer-wise pixel-shuffle mechanism and a residual learning architecture with non-parametric shortcuts.", "result": "LaCo outperforms existing methods in token compression within the intermediate layers of vision encoders, achieving superior compression efficiency and preserving strong performance in both training and inference.", "conclusion": "The study concludes that LaCo provides a significant improvement in visual token compression, leading to better efficiency gains and performance in MLLMs (Multimodal Large Language Models)."}}
{"id": "2507.02428", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.02428", "abs": "https://arxiv.org/abs/2507.02428", "authors": ["Sumaya Ahmed Salihs", "Isaac Wiafe", "Jamal-Deen Abdulai", "Elikem Doe Atsakpo", "Gifty Ayoka", "Richard Cave", "Akon Obu Ekpezu", "Catherine Holloway", "Katrin Tomanek", "Fiifi Baffoe Payin Winful"], "title": "A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages", "comment": "This version has been reviewed and accepted for presentation at the\n  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5\n  pages and 3 tables", "summary": "This study presents an approach for collecting speech samples to build\nAutomatic Speech Recognition (ASR) models for impaired speech, particularly,\nlow-resource languages. It aims to democratize ASR technology and data\ncollection by developing a \"cookbook\" of best practices and training for\ncommunity-driven data collection and ASR model building. As a proof-of-concept,\nthis study curated the first open-source dataset of impaired speech in Akan: a\nwidely spoken indigenous language in Ghana. The study involved participants\nfrom diverse backgrounds with speech impairments. The resulting dataset, along\nwith the cookbook and open-source tools, are publicly available to enable\nresearchers and practitioners to create inclusive ASR technologies tailored to\nthe unique needs of speech impaired individuals. In addition, this study\npresents the initial results of fine-tuning open-source ASR models to better\nrecognize impaired speech in Akan.", "AI": {"tldr": "研究开发了一套社区驱动的ASR模型建构方法及数据收集最佳实践，创建了开放资源的阿坎语障碍语音数据集，并展示了针对阿坎语障碍语音的ASR模型初步微调结果。", "motivation": "该研究的目的是通过建立ASR技术和数据收集的最佳实践，推动ASR技术的普及化和数据收集的民主化。", "method": "本研究提出了一种收集语音样本的方法，以建立针对有语音障碍者的自动语音识别（ASR）模型，尤其针对资源匮乏的语言。研究通过制定一套“烹饪手册”来推动社区驱动的数据收集和ASR模型构建的技术，并以阿坎语为例，这是加纳广泛使用的土著语言之一。", "result": "研究收集了来自不同背景的有语音障碍的参与者的语音数据，形成了第一个开放来源的阿坎语障碍语音数据集，并且这个数据集、手册以及开源工具都是公开可用的。此外，本研究还展示了将开源ASR模型微调以更好地识别阿坎语障碍语音的初步结果。", "conclusion": "本研究通过提供开放的数据集、最佳实践指南和开源工具，为研究者和从业者创造包容性ASR技术，以满足有语音障碍者的需求，初步展示了针对阿坎语障碍语音的ASR模型微调的可行性。"}}
{"id": "2507.02288", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02288", "abs": "https://arxiv.org/abs/2507.02288", "authors": ["De Cheng", "Zhipeng Xu", "Xinyang Jiang", "Dongsheng Li", "Nannan Wang", "Xinbo Gao"], "title": "Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization", "comment": null, "summary": "Domain Generalization (DG) seeks to develop a versatile model capable of\nperforming effectively on unseen target domains. Notably, recent advances in\npre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated\nconsiderable potential in enhancing the generalization capabilities of deep\nlearning models. Despite the increasing attention toward VFM-based domain\nprompt tuning within DG, the effective design of prompts capable of\ndisentangling invariant features across diverse domains remains a critical\nchallenge. In this paper, we propose addressing this challenge by leveraging\nthe controllable and flexible language prompt of the VFM. Noting that the text\nmodality of VFMs is naturally easier to disentangle, we introduce a novel\nframework for text feature-guided visual prompt tuning. This framework first\nautomatically disentangles the text prompt using a large language model (LLM)\nand then learns domain-invariant visual representation guided by the\ndisentangled text feature. However, relying solely on language to guide visual\nfeature disentanglement has limitations, as visual features can sometimes be\ntoo complex or nuanced to be fully captured by descriptive text. To address\nthis, we introduce Worst Explicit Representation Alignment (WERA), which\nextends text-guided visual prompts by incorporating an additional set of\nabstract prompts. These prompts enhance source domain diversity through\nstylized image augmentations, while alignment constraints ensure that visual\nrepresentations remain consistent across both the original and augmented\ndistributions. Experiments conducted on major DG datasets, including PACS,\nVLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method\noutperforms state-of-the-art DG methods.", "AI": {"tldr": "我们提出了一种基于文本引导的视觉领域适应方法，通过解耦文本特征来指导视觉特征的学习，并引入WERA方法来增强视觉表征的一致性，实验结果表明我们的方法优于现有的领域泛化方法。", "motivation": "由于现有的视觉基础模型在领域泛化中表现出色，但设计能够解耦不同领域中不变特征的提示仍旧是一个关键挑战，因此我们提出了基于文本特征指导的视觉提示调优框架来解决这一问题。", "method": "我们提出了一种基于文本特征引导的视觉提示调优框架，通过大语言模型自动解耦文本提示，并以解耦的文本特征为引导学习领域不变的视觉表示。为克服仅依赖语言指导视觉特征解耦的局限性，我们还提出了WERA方法，通过使用抽象提示和风格化图像增强来提高原领域和增强后的分布之间的一致性。", "result": "实验结果表明，我们在PACS、VLCS、OfficeHome、DomainNet和TerraInc等主要领域泛化数据集上的表现优于现有的领域泛化方法。", "conclusion": "通过引入基于大语言模型的解耦文本特征引导的方法和WERA机制，我们的方法在领域泛化任务中展示了超越现有方法的表现，为设计领域不变的视觉模型提供了新的视角。"}}
{"id": "2507.02506", "categories": ["cs.CL", "cs.AI", "cs.LG", "91B14, 68T50", "I.2.7; K.4.1; K.5.2"], "pdf": "https://arxiv.org/pdf/2507.02506", "abs": "https://arxiv.org/abs/2507.02506", "authors": ["Sneha Deshmukh", "Prathmesh Kamble"], "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders", "comment": "9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and\n  GitHub. Submitted to arXiv for open access", "summary": "Legal NLP remains underdeveloped in regions like India due to the scarcity of\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\npipeline and verified for consistency. This resource supports a wide range of\nlegal NLP tasks such as outcome prediction, summarization, and fairness\nanalysis, and is the first publicly available dataset focused specifically on\nIndian bail jurisprudence.", "AI": {"tldr": "我们介绍了一个新的基准数据集IndianBailJudgments-1200，包含1200份印度法院判决书，注释了20多个属性，如保释结果、相关刑法节、犯罪类型和法律推理。此数据集支持法律NLP任务，如结果预测、摘要和公正性分析。", "motivation": "印度等地区法律NLP发展滞后，主要是因为缺乏结构化数据集。我们的目标是填补这一空白，提供一个丰富的法律文本数据集。", "method": "我们使用了经过提示工程优化的GPT-4管道生成注释，并验证了注释的一致性，创建了IndianBailJudgments-1200数据集。", "result": "创建了包含1200份印度法院保释决定判决书的数据集，注释了超过20个属性，首次公开发布专门针对印度保释法理学的数据集。", "conclusion": "该数据集支持广泛的法律NLP任务，并首次公开发布，专门针对印度保释法理学，填补了印度地区该领域的空白。"}}
{"id": "2507.02294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.02294", "abs": "https://arxiv.org/abs/2507.02294", "authors": ["Hanbo Bi", "Yulong Xu", "Ya Li", "Yongqiang Mao", "Boyuan Tong", "Chongyang Li", "Chunbo Lang", "Wenhui Diao", "Hongqi Wang", "Yingchao Feng", "Xian Sun"], "title": "ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation", "comment": null, "summary": "The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits\nstrong generalization in generic segmentation tasks. However, applying SAM to\nremote sensing (RS) images still faces two major challenges. First, manually\nconstructing precise prompts for each image (e.g., points or boxes) is\nlabor-intensive and inefficient, especially in RS scenarios with dense small\nobjects or spatially fragmented distributions. Second, SAM lacks domain\nadaptability, as it is pre-trained primarily on natural images and struggles to\ncapture RS-specific semantics and spatial characteristics, especially when\nsegmenting novel or unseen classes. To address these issues, inspired by\nfew-shot learning, we propose ViRefSAM, a novel framework that guides SAM\nutilizing only a few annotated reference images that contain class-specific\nobjects. Without requiring manual prompts, ViRefSAM enables automatic\nsegmentation of class-consistent objects across RS images. Specifically,\nViRefSAM introduces two key components while keeping SAM's original\narchitecture intact: (1) a Visual Contextual Prompt Encoder that extracts\nclass-specific semantic clues from reference images and generates object-aware\nprompts via contextual interaction with target images; and (2) a Dynamic Target\nAlignment Adapter, integrated into SAM's image encoder, which mitigates the\ndomain gap by injecting class-specific semantics into target image features,\nenabling SAM to dynamically focus on task-relevant regions. Extensive\nexperiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,\nLoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and\nautomatic segmentation of unseen classes by leveraging only a few reference\nimages and consistently outperforms existing few-shot segmentation methods\nacross diverse datasets.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.02592", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.02592", "abs": "https://arxiv.org/abs/2507.02592", "authors": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Liwen Zhang", "Litu Ou", "Jialong Wu", "Wenbiao Yin", "Baixuan Li", "Zhengwei Tao", "Xinyu Wang", "Weizhou Shen", "Junkai Zhang", "Dingchu Zhang", "Xixi Wu", "Yong Jiang", "Ming Yan", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebSailor: Navigating Super-human Reasoning for Web Agent", "comment": null, "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.", "AI": {"tldr": "提出WebSailor方法，通过生成高不确定性的任务和应用DUPO算法提升开源模型在复杂信息搜索任务中的表现，以缩小与专有模型之间的性能差距。", "motivation": "在复杂信息搜索任务中超越人类认知限制，提升开源语言模型的推理能力，实现与专有模型相匹配的表现。", "method": "采用生成高不确定性任务、结构化采样和信息混淆等策略，并通过DUPO算法进行高效的基于代理的强化学习训练。", "result": "WebSailor在复杂信息搜索任务上显著超越了所有开源模型，并达到了与专有模型相当的性能。", "conclusion": "证明了WebSailor作为一种完整的方法论，在培养大规模语言模型处理高不确定环境下的系统化能力方面是有效的。"}}
