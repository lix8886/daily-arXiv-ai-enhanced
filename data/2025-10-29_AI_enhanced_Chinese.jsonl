{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "本研究开发了一个用于检测图像真实性的解释系统，结合了轻量级卷积分类器和视觉语言模型，实现了快速准确的伪迹检测与定位。", "motivation": "鉴于AI生成图像现实性提高所带来的验证图像真实性挑战，本研究旨在构建一个可解释的真实度检测系统。", "method": "使用轻量级卷积分类器“Faster-Than-Lies”结合视觉语言模型Qwen2-VL-7B来对32x32图像中的伪迹进行分类、定位和解释。使用自编码器重构错误图生成伪迹定位热图以增强可解释性，并将70种视觉伪迹类型分为八大类，为每个检测到的异常生成可解释的文本描述。", "result": "该模型在扩展的CiFAKE数据集上获得了96.5%的准确率，此数据集还包括对抗性扰动，并且在8核CPU上的推理时间为175毫秒，支持部署在本地或边缘设备上。", "conclusion": "这项工作展示了通过结合视觉和语言推理技术来实现低分辨率图像的真实度检测，并展示了其在法医鉴定、工业检查以及社交媒体监管等跨域应用的潜力。"}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "本研究提出了CountFormer模型，采用DINOv2基础模型来改进物体计数，实现了性能卓越的类无关物体计数，尤其在复杂结构场景中表现好。", "motivation": "人类能够通过识别视觉重复和结构关系而非依靠类别身份来轻松计数不同的物体，然而现有的大多数计数模型在面对复杂形状、内部对称性或组件重叠的物体时会计数错误。", "method": "引入了CountFormer，这是一个基于Transformer的框架，用于学习识别重复和结构一致性来进行类无关的对象计数。该模型基于CounTR架构，将原视觉编码器替换为自监督基础模型DINOv2，该基础模型可以生成更丰富且空间一致的特征表示。此外，模型还引入位置嵌入融合，在解码为密度图之前保留几何关系，解码采用轻量级卷积解码器。", "result": "在FSC-147数据集上进行评估，该模型的表现与当前最先进的方法相当，并且在结构复杂或密集的场景中表现出更高的准确性。", "conclusion": "研究表明，使用DINOv2等基础模型的集成可以让计数系统接近人类的结构感知能力，迈向真正普遍且无示例的计数范式。"}}
{"id": "2510.23798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23798", "abs": "https://arxiv.org/abs/2510.23798", "authors": ["Gauthier Grimmer", "Romain Wenger", "Clément Flint", "Germain Forestier", "Gilles Rixhon", "Valentin Chardon"], "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras", "comment": null, "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.", "AI": {"tldr": "本文提出了一种基于固定摄像头和深度学习的河中漂浮垃圾持续监测框架，并强调了数据集创建的重要性，最终证明了该方法在城市水环境中的可行性。", "motivation": "由于河中漂浮的人为垃圾泛滥成为了一个迫在眉睫的环境问题，对生物多样性、水质和人类活动如航行和娱乐产生了负面影响，因此研究旨在解决这一问题。", "method": "本研究提出了一种利用固定在位摄像头和深度学习技术对河水中漂浮垃圾进行持续量化和监测的新框架。同时，本方法还识别出最适合复杂环境的深度学习模型，在准确性和推断速度方面进行了测试。此外，还实现了一个几何模型，从2D图像中估算检测对象的实际大小，并考虑了相机的内在和外在特性。", "result": "研究结果强调了数据集构成协议的重要性，特别是关于负图像的集成和时间泄漏的考虑。实验发现了一种结合投影几何和回归修正的可量化的方法用于估算物体的度量。", "conclusion": "研究展示了结合投影几何和回归校正进行度量对象估算的可行性，为城市水环境的坚固、低成本和自动化监测系统的发展铺平了道路。"}}
{"id": "2510.23816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23816", "abs": "https://arxiv.org/abs/2510.23816", "authors": ["Forouzan Fallah", "Wenwen Li", "Chia-Yu Hsu", "Hyunho Lee", "Yezhou Yang"], "title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "comment": null, "summary": "Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.", "AI": {"tldr": "RareFlow是一个物理感知的遥感图像超级分辨率框架，特别针对分布外数据进行了优化。该框架通过门控控制网络保留几何细节，并通过文本提示合成复杂特征。实验结果表明，RareFlow输出的图像保真度接近真值图像，且优于现有方法。", "motivation": "超级分辨率（SR）在遥感图像下通常在分布外（OOD）条件下表现不佳，尤其是在罕见的地质特征和多样化的传感器数据中，导致生成的图像在视觉上看起来合理但在物理上不准确。", "method": "RareFlow采用了一种双条件架构，包含一个门控控制网络（Gated ControlNet），用于保留低分辨率输入中的细粒度几何细节，并通过文本提示提供语义引导来合成复杂特征。为了确保物理上的合理性，RareFlow引入了一种多方面的损失函数，保证输出在光谱和辐射度方面与传感器特性保持一致。此外，该框架通过采用随机前向传播方法量化其自身的预测不确定性，输出的方差可以识别不熟悉的输入，减少特征幻觉。", "result": "在一项针对多传感器卫星图像的新建立的基准数据集上的验证显示，在盲测中，地球物理专家认为RareFlow生成的输出接近地面真值图像的保真度，明显优于现有的最先进的基线模型。", "conclusion": "RareFlow为数据稀缺的科学领域提供了一个高保真合成的稳健框架，并为在严重领域偏移下的可控生成提供了新的范式。"}}
{"id": "2510.23730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23730", "abs": "https://arxiv.org/abs/2510.23730", "authors": ["Alessandra Terranova", "Björn Ross", "Alexandra Birch"], "title": "Evaluating Long-Term Memory for Long-Context Question Answering", "comment": "14 pages including appendix, 3 figures. Submitted to October ARR and\n  to Metacognition in Generative AI EurIPS workshop (under review for both)", "summary": "In order for large language models to achieve true conversational continuity\nand benefit from experiential learning, they need memory. While research has\nfocused on the development of complex memory systems, it remains unclear which\ntypes of memory are most effective for long-context conversational tasks. We\npresent a systematic evaluation of memory-augmented methods using LoCoMo, a\nbenchmark of synthetic long-context dialogues annotated for question-answering\ntasks that require diverse reasoning strategies. We analyse full-context\nprompting, semantic memory through retrieval-augmented generation and agentic\nmemory, episodic memory through in-context learning, and procedural memory\nthrough prompt optimization. Our findings show that memory-augmented approaches\nreduce token usage by over 90% while maintaining competitive accuracy. Memory\narchitecture complexity should scale with model capability, with small\nfoundation models benefitting most from RAG, and strong instruction-tuned\nreasoning model gaining from episodic learning through reflections and more\ncomplex agentic semantic memory. In particular, episodic memory can help LLMs\nrecognise the limits of their own knowledge.", "AI": {"tldr": "研究通过LoCoMo基准测试了不同类型的记忆系统在长时间对话任务中的有效性，发现带有记忆增强的方法可以大幅减少token使用同时保持准确性。小型基础模型从检索增强生成中受益最多，而强大的指令调优推理模型则从回溯和更复杂代理语义记忆中获益。特别是，情景记忆有助于LLM意识到自身知识的局限性。", "motivation": "探索哪种类型的记忆系统能够最有效地提升大语言模型在长时间对话中的表现。", "method": "使用LoCoMo基准测试全上下文提示、检索增强生成等几种记忆增强方法。", "result": "记忆增强的方法可以减少90%的token使用量，并保持准确性。小型基础模型从检索增强生成中收益最大，而强大的指令调优推理模型则从情景学习与更复杂的代理语义记忆中收益。", "conclusion": "记忆增强的方法对于提高大语言模型的效率和准确性是必要的，模型能力越强则应该采用越复杂的记忆架构。情景记忆可以提升LLM自我意识，认识到其知识的局限性。"}}
{"id": "2510.23880", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.23880", "abs": "https://arxiv.org/abs/2510.23880", "authors": ["Hanke Chen", "Yuan Liu", "Minchen Li"], "title": "TRELLISWorld: Training-Free World Generation from Object Generators", "comment": null, "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.", "AI": {"tldr": "本文提出了一种基于文本驱动的3D场景生成方法，使用扩散模型作为模块生成器，解决了现有方法的局限性，使得场景可以大规模合成并支持全方位视图。", "motivation": "现有的3D场景生成方法通常受到单一对象生成的限制，需要特定领域的训练，或者不支持360度全方位视图。本研究旨在提出一种无需训练的方法，以改善这些问题。", "method": "通过重新定位通用的文本到3D对象扩散模型作为模块化瓷砖生成器，将场景生成重新定义为多瓷砖去噪问题。这种方法可以独立生成和无缝融合重叠的3D区域，从而能够合成大型连贯场景，同时保持局部语义控制。", "result": "研究展示了一种支持多样化场景布局、高效生成和灵活编辑的方法，为通用的、语言驱动的3D场景构建奠定了简单而强大的基础。", "conclusion": "该方法无需场景级数据集或重新训练，依赖极小的启发式规则，继承了对象级先验的知识，为3D场景构建提供了一种高效灵活的方法。"}}
{"id": "2510.23766", "categories": ["cs.CL", "68T05", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.23766", "abs": "https://arxiv.org/abs/2510.23766", "authors": ["Ramshankar Bhuvaneswaran", "Handan Liu"], "title": "BitSkip: An Empirical Analysis of Quantization and Early Exit Composition", "comment": "Submitted to JMLR", "summary": "The pursuit of efficient Large Language Models (LLMs) has led to increasingly\ncomplex techniques like extreme quantization and dynamic routing. While\nindividual benefits of these methods are well-documented, their compositional\neffects remain poorly understood. This paper introduces BitSkip, a hybrid\narchitectural framework for systematically exploring these interactions.\nCounter-intuitively, our findings reveal that a simple 8-bit quantized model\nwithout Hadamard transform (BitSkip-V1) not only outperforms its more complex\n4-bit and Hadamard-enhanced counterparts but also competes the full-precision\nbaseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard\ntransforms, even at 8-bit precision, catastrophically degraded performance by\nover 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe\ndemonstrates superior early-exit characteristics, with layer 18 providing\noptimal 32.5% speed gain for minimal 4% quality loss.", "AI": {"tldr": "BitSkip-V1，一个简单的8位量化模型，没有Hadamard变换，展示出超过复杂模型的性能，甚至能在质量上与全精度基线竞争，同时带来显著的速度提升。", "motivation": "鉴于个体技术如极限量化和动态路由的益处已经得到了充分的文档记录，但它们组合效果的理解依然不足。通过探索这些问题，作者希望增进对该领域的理解。", "method": "此论文介绍了BitSkip，一个用于系统性探索这些技术间交互的混合架构框架。它探讨了一个简单的8位量化的模型而没有Hadamard变换（BitSkip-V1）的表现。", "result": "研究发现，BitSkip-V1不仅胜过更为复杂的4位和通过Hadamard变换增强的模型，还在质量（困惑度为1.13对1.19）上与全精度基线竞争。引入Hadamard变换，即使是8位精度，也会使性能恶化超过37000%。BitSkip-V1配方展示了出色的早期退出特性，第18层提供最佳的32.5%速度提升，只需4%的质量损失。", "conclusion": "论文得出结论，简单8位量化的BitSkip-V1模型在性能上优于更复杂的模型，并展现出优秀的早期退出特性和速度优势，提供了对大型语言模型架构的有趣见解。"}}
{"id": "2510.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23894", "abs": "https://arxiv.org/abs/2510.23894", "authors": ["Jinxin Zhou", "Jiachen Jiang", "Zhihui Zhu"], "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "23 pages, 10 figures, 14 tables", "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.", "AI": {"tldr": "LHT-CLIP 通过分析CLIP模型的层、头部和标记级别视觉区分度，提出了无训练方法改善语义分割性能，达到新标杆。", "motivation": "解决CLIP模型在语义分割中的表现欠佳，这归因于其在图像级别预训练目标和像素级别视觉理解之间的错位。旨在克服现有方法由于继承了之前层的全局对齐偏差而导致的分割性能不佳的问题。", "method": "本文提出了一种名为LHT-CLIP的无训练框架，该框架通过综合分析揭示了三个关键见解：(i) 最终层加强了图像-文本对齐但牺牲了视觉区分度；(ii) 注意力头的子集表现出一致的强视觉区分度；(iii) 异常标记表现出稀疏且一致的激活模式。基于这些发现，提出了三种互补技术：语义空间重加权，选择性头部增强和异常标记替换，以有效恢复视觉区分度并提高分割性能。", "result": "通过无需额外训练或调整大量超参数的策略，LHT-CLIP系统性地提升了视觉区分度，并显著提高分割效果，在多样化的情况下取得了最佳性能。", "conclusion": "实验表明，LHT-CLIP在8个常见语义分割基准测试中取得了最先进的性能，展示其实用性和有效性，具有实际部署潜力。"}}
{"id": "2510.23828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23828", "abs": "https://arxiv.org/abs/2510.23828", "authors": ["Mena Attia", "Aashiq Muhamed", "Mai Alkhamissi", "Thamar Solorio", "Mona Diab"], "title": "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language", "comment": null, "summary": "We present a comprehensive evaluation of the ability of large language models\n(LLMs) to process culturally grounded language, specifically to understand and\npragmatically use figurative expressions that encode local knowledge and\ncultural nuance. Using figurative language as a proxy for cultural nuance and\nlocal knowledge, we design evaluation tasks for contextual understanding,\npragmatic use, and connotation interpretation in Arabic and English. We\nevaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,\nmultidialectal Arabic proverbs, and English proverbs. Our results show a\nconsistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower\nthan for English proverbs, and performance for Egyptian idioms is 10.28% lower\nthan for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%\nrelative to understanding, though providing contextual idiomatic sentences\nimproves accuracy by 10.66%. Models also struggle with connotative meaning,\nreaching at most 85.58% agreement with human annotators on idioms with 100%\ninter-annotator agreement. These findings demonstrate that figurative language\nserves as an effective diagnostic for cultural reasoning: while LLMs can often\ninterpret figurative meaning, they face challenges in using it appropriately.\nTo support future research, we release Kinayat, the first dataset of Egyptian\nArabic idioms designed for both figurative understanding and pragmatic use\nevaluation.", "AI": {"tldr": "该研究评估了大型语言模型（LLMs）处理具有文化背景的语言的能力，特别是理解和实用寓意表达的能力。在阿拉伯语和英语中设计了上下文理解、实用和象征意义解释的评估任务，并评估了22个开源和闭源LLMs。结果表明LLMs在处理包含文化细微差别的寓意语言方面存在挑战。", "motivation": "研究动机是了解大型语言模型在处理包含文化细微差别的语言方面的表现，特别是理解和实用寓意表达的能力。", "method": "研究设计了包括上下文理解、实用和象征意义解释在内的评估任务，针对埃及阿拉伯语的成语、多方言阿拉伯语的谚语和英语谚语进行了评估。", "result": "结果显示，对于阿拉伯语谚语，准确率比英语谚语低4.29%；对于埃及成语，准确率比阿拉伯谚语低10.28%。在实用任务中，准确率比理解任务低14.07%，但提供上下文成语句子会提高准确率。模型在象征意义理解上的表现有限，与人工注释者最高只能达到85.58%的一致性。", "conclusion": "研究发现寓意语言可以用来诊断文化推理能力，尽管LLMs能解释某些寓意的意义，但在使用这些意义上仍面临挑战。研究还发布了Kinayat数据集，为评估埃及阿拉伯语成语的寓意理解与实用提供了支持。"}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "DynaStride是用于生成连贯场景级别字幕的流水线，无需手动场景分割。它可以自适应地捕捉关键过渡，并生成更有时间连贯性和信息量的字幕，从而提升教学内容的生成效果。", "motivation": "场景级别的字幕生成可以增强教学视频的学习效果，因为这需要理解视觉线索和时间结构。然而，如果字幕不能够捕获这些结构，可能会导致缺乏连贯性和质量，进而产生困惑并削弱视频的教育意图。为了填补这一空白，提出了DynaStride。", "method": "通过使用YouCookII数据集的场景注释，DynaStride执行自适应帧采样和多模态窗口化，以捕捉每个场景内的关键过渡。接着，采用多模态思维链过程生成多个动作-对象对，然后通过动态步幅窗口选择算法对这些对进行优化和融合，从而实现视觉语义和时间推理的一体化说明性字幕生成。", "result": "实证评估结果显示，与强大的基准模型相比，DynaStride在N-gram指标和语义相似性度量上均有显著提高。定性分析表明，DynaStride生成的字幕在时间连贯性和信息量方面都有优势。", "conclusion": "DynaStride提供了一种提高AI生成的教学内容质量的新方向，为未来的研究指出了一个有希望的路径。"}}
{"id": "2510.23842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23842", "abs": "https://arxiv.org/abs/2510.23842", "authors": ["Saki Imai", "Lee Kezar", "Laurel Aichler", "Mert Inan", "Erin Walker", "Alicia Wooten", "Lorna Quandt", "Malihe Alikhani"], "title": "How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse", "comment": null, "summary": "Most state-of-the-art sign language models are trained on interpreter or\nisolated vocabulary data, which overlooks the variability that characterizes\nnatural dialogue. However, human communication dynamically adapts to contexts\nand interlocutors through spatiotemporal changes and articulation style. This\nspecifically manifests itself in educational settings, where novel vocabularies\nare used by teachers, and students. To address this gap, we collect a motion\ncapture dataset of American Sign Language (ASL) STEM (Science, Technology,\nEngineering, and Mathematics) dialogue that enables quantitative comparison\nbetween dyadic interactive signing, solo signed lecture, and interpreted\narticles. Using continuous kinematic features, we disentangle dialogue-specific\nentrainment from individual effort reduction and show spatiotemporal changes\nacross repeated mentions of STEM terms. On average, dialogue signs are\n24.6%-44.6% shorter in duration than the isolated signs, and show significant\nreductions absent in monologue contexts. Finally, we evaluate sign embedding\nmodels on their ability to recognize STEM signs and approximate how entrained\nthe participants become over time. Our study bridges linguistic analysis and\ncomputational modeling to understand how pragmatics shape sign articulation and\nits representation in sign language technologies.", "AI": {"tldr": "我们针对ASL中STEM话题对话收集数据，分析了双人互动与独白中手势的时空变化，发现互动手势比独立手势更短，模型在识别STEM词汇时，参与者之间的协调程度有显著变化。", "motivation": "大多数最先进的手语模型都是使用口译员数据或孤立词汇进行训练的，这忽视了自然对话中所具有的变化性。而人类交流会根据上下文和交流者通过时空变化和发音风格进行动态调整。特别是在教育环境中，教师和学生会使用新的词汇，这方面的研究有所欠缺。", "method": "我们收集了一个美国手语（ASL）STEM对话的动态捕捉数据集，以实现双人互动性手语、单人讲座和翻译文章之间的定量比较。通过连续的运动特征，我们在重复出现STEM词汇时，将对话特定的反复动作与个人努力的减少分开。", "result": "我们发现，平均而言，对话中的手势比孤立的手势短24.6%-44.6%，在独白情景中没有这种显著的减少。评估了手势嵌入模型在识别STEM手势方面的能力，并模拟参与者在其间随着时间变得协调的程度。", "conclusion": "这项研究将语言分析和计算建模结合起来，探讨了实际语用如何影响手语的表达以及其在手语技术中的表现形式。"}}
{"id": "2510.23929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23929", "abs": "https://arxiv.org/abs/2510.23929", "authors": ["Emily Kim", "Julieta Martinez", "Timur Bagautdinov", "Jessica Hodgins"], "title": "TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis", "comment": null, "summary": "We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.", "AI": {"tldr": "本文介绍了TurboPortrait3D，一种用于人像3D新视角合成的方法，结合图像扩散模型，以提升生成的3D表示的质量同时保持低延迟。", "motivation": "现有的图像到3D模型虽然可以生成可渲染的3D表示，但是往往存在视觉伪影、细节不足和难以完全保留主体身份的缺陷。而图像扩散模型虽然可以生成高质量的图像，但由于计算成本高，且不具备3D基础，不能直接产生多视角一致的输出。", "method": "我们的方法名为TurboPortrait3D，用于生成3D人像的新视角合成。该方法利用图像扩散模型提升现有的图像到3D模型质量，同时保持低延迟和三维感知能力。此方法首先通过单个正脸图像输入，应用图像至3D头像生成管道获取初始的3D表示及相应的噪点渲染结果。接着，我们将这些噪点渲染输入给一个扩散模型，该模型以输入图像为条件，专门训练用于以多视角一致的方式修正渲染结果。此外，我们提出了一种新的训练策略，包括在大量合成多视角数据上预训练，然后在高质量真实图像上进行微调。", "result": "实验结果表明，我们的方法在人像新视角合成中，无论是在定性分析还是定量分析上，都有优异的表现，并且在效率上也有提升。", "conclusion": "结论部分指出，通过此方法，我们能够以高质量和低延迟的方式生成高质量的3D人像多视角合成，并显示出优于当前技术水平的结果。"}}
{"id": "2510.23845", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23845", "abs": "https://arxiv.org/abs/2510.23845", "authors": ["Grace Byun", "Rebecca Lipschutz", "Sean T. Minton", "Abigail Lott", "Jinho D. Choi"], "title": "CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection", "comment": null, "summary": "Detecting mental health crisis situations such as suicide ideation, rape,\ndomestic violence, child abuse, and sexual harassment is a critical yet\nunderexplored challenge for language models. When such situations arise during\nuser--model interactions, models must reliably flag them, as failure to do so\ncan have serious consequences. In this work, we introduce CRADLE BENCH, a\nbenchmark for multi-faceted crisis detection. Unlike previous efforts that\nfocus on a limited set of crisis types, our benchmark covers seven types\ndefined in line with clinical standards and is the first to incorporate\ntemporal labels. Our benchmark provides 600 clinician-annotated evaluation\nexamples and 420 development examples, together with a training corpus of\naround 4K examples automatically labeled using a majority-vote ensemble of\nmultiple language models, which significantly outperforms single-model\nannotation. We further fine-tune six crisis detection models on subsets defined\nby consensus and unanimous ensemble agreement, providing complementary models\ntrained under different agreement criteria.", "AI": {"tldr": "本文提出了CRADLE BENCH，一个涵盖多种危机类型及时间标签的基准测试，通过自动标注的语料库训练模型，并通过微调提供了六种互补的危机检测模型。", "motivation": "本文的动机是解决语言模型在检测如自杀倾向、性侵、家庭暴力、儿童虐待和性骚扰等精神健康危机情况下可靠性不足的问题。如果不能可靠地识别这些危机情况，将会产生严重的后果。", "method": "本文引入了CRADLE BENCH，这是一个多方面的危机检测基准，涵盖了七种类型的危机情况，并首次结合了时间标签。为了训练该模型，使用了大约4K个用多种语言模型的多数票集成自动标注的训练语料库，这比单一模型标注的效果更好。", "result": "通过对共识和一致同意的子集进行微调，本文提供了六种危机检测模型，它们在不同的同意标准下进行了互补训练，从而提高了危机检测的准确性和可靠性。", "conclusion": "CRADLE BENCH作为多方面的危机检测基准，通过共识和一致同意机制提供了多种互补的危机检测模型，不仅提升了危机检测的可靠性，还丰富了对危机识别的研究。"}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "提出了一种新的室内场景三维表面重建方法PlanarGS，该方法基于3DGS，通过引入平面和几何先验监督，解决了大量低纹理区域导致的几何重建不稳定问题，实验表明方法优于现有最佳方法。", "motivation": "三维高斯喷点（3DGS）作为新颖视图合成的有效表示方法，已经实现了令人印象深刻的视觉质量，但在以大面积低纹理区域为主导的场景中，如室内环境，常用的光度损失用于优化3DGS时会导致几何模糊并妨碍高质量3D表面的恢复。", "method": "我们的方法PlanarGS是一个基于3D高斯喷点（3DGS）的框架，专为室内场景重建设计。具体来说，我们设计了一个用于语言提示平面先验（LP3）的流程，该流程利用预训练的视觉-语言分割模型，并通过多视角融合和几何先验来精炼区域建议。框架中的3D高斯点通过额外的两项进行优化：一项是平面先验监督，以确保平面一致性；另一项是几何先验监督，引导高斯点朝向深度和法线线索调整。", "result": "我们在标准的室内数据集上进行了广泛实验，结果显示PlanarGS能够重建精确且详细的3D表面，相较于现有方法，整体性能有显著的提高。", "conclusion": "我们的工作引入了一种新的3DGS框架（PlanarGS），该框架解决了室内环境中大面积低纹理区域难以精确重建的问题。通过引入平面和几何先验监督，我们的方法在重建3D表面时表现出更高的精度和细节度，达标记现时最佳方法的性能。"}}
{"id": "2510.23853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23853", "abs": "https://arxiv.org/abs/2510.23853", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Kazem Faghih", "Parsa Hosseini", "Wenxiao Wang", "Soheil Feizi"], "title": "Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception", "comment": "preliminary work in progress", "summary": "Large language model agents are increasingly used in multi-turn\nconversational settings to interact with and execute tasks in dynamic\nenvironments. However, a key limitation is their temporal blindness: they, by\ndefault, operate with a stationary context, failing to account for the\nreal-world time elapsed between messages. This becomes a critical liability\nwhen an agent must decide whether to invoke a tool based on how much time has\npassed since the last observation. Without temporal awareness, agents often\neither over-rely on previous context (skipping necessary tool calls), or\nunder-rely on it (unnecessarily repeating tool calls). To study this challenge,\nwe introduce TicToc-v1, a test set of multi-turn user-agent trajectories across\n34 scenarios with varying time sensitivity. Each trajectory ends with a user\nquestion, where the need for a tool call depends on the amount of time elapsed\nsince the last message. To give LLMs temporal context, we augment dialogue\nmessages with explicit timestamps, bridging the gap between static dialogue and\nevolving environments. We then collected human preferences for these samples,\ncreating two subsets: one where humans preferred relying on the previous\nobservation (prefer-noTool), and another where they preferred a new tool call\n(prefer-Tool). We evaluated how well LLM tool-calling decisions align with\nhuman preferences under varying time intervals on TicToc-v1. Our analysis show\nthat without time information, most models perform only slightly better than\nrandom, with the top alignment rate being just over 60%. While adding\ntimestamps leads to a slight improvement, particularly for larger models, the\nimprovement is modest, peaking at around 65%. We also show that naive,\nprompt-based alignment have limited effectiveness. Our findings highlight the\nneed for specific post-training alignment to align multi-turn LLM tool use with\nhuman temporal perception.", "AI": {"tldr": "文章通过设计TicToc-v1测试集研究大型语言模型的时间盲视问题，并发现即使加入了时间戳，多数模型的表现也远未达到理想水平，强调了针对人类时间感知进行特定后训练对齐的需求。", "motivation": "文章探讨了大型语言模型代理在多轮对话中处理时间敏感任务时存在的关键问题——时间盲视，即模型默认操作基于静态上下文，无法根据实际时间流逝做出决策。为研究这一挑战，提出了TicToc-v1测试集，以考察时间信息对于决策的影响。", "method": "引入TicToc-v1测试集，涵盖34个对时间敏感的不同场景的多轮对话轨迹。向对话消息添加时间戳，以此提供时间上下文，并根据人类偏好将样本分为两个子集：一个子集表明人类倾向于依赖之前的观察结果，另一个子集则倾向于需要新的工具调用。通过人机偏好对比分析LLM在不同时间间隔下的工具调用决策。", "result": "分析结果表明，缺乏时间信息时，多数模型的表现仅略高于随机水平，最佳对齐率仅为60%以上。而增加时间戳虽有助于性能稍有提升，特别是对较大模型而言，但仍达不到显著改善，最佳表现仅能达到65%。简单的基于提示的对齐方式效果有限。", "conclusion": "研究结论指出现有模型在动态环境中的工具调用与人类时间感知之间的对齐问题，并强调在多轮对话语言模型训练后进行特定的后训练对齐的必要性。"}}
{"id": "2510.23943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23943", "abs": "https://arxiv.org/abs/2510.23943", "authors": ["Diana Aldana", "João Paulo Lima", "Daniel Csillag", "Daniel Perazzo", "Haoan Feng", "Luiz Velho", "Tiago Novello"], "title": "Adaptive Training of INRs via Pruning and Densification", "comment": null, "summary": "Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.", "AI": {"tldr": "AIRe提出了一种自适应训练方案来改进隐式神经表示(INRs)，该方案通过神经元剪枝和输入频率密集化来优化模型结构，减少参数冗余并提高重建质量。", "motivation": "现有方法在选择输入频率和架构以及管理参数冗余方面依赖于启发式方法和繁重的超参数优化，本文旨在通过AIRe自适应训练方案解决这些问题。", "method": "AIRe采用自适应训练方案，通过神经元剪枝机制避免冗余，并通过输入频率密集化提升表示能力。剪枝阶段识别对模型贡献较小的神经元，并通过有针对性的权重衰减将它们的信息转移到剩余的神经元上，然后进行结构化剪枝。密集化阶段则在信号拟合不足的频谱区域增加输入频率，扩展表示基础。", "result": "实验结果表明，AIRe在图像和SDF(Signed Distance Fields)上减少模型大小的同时保持或改进了重建质量。", "conclusion": "AIRe通过改进的训练方案，不仅减少了模型大小，而且保持或提升了重建质量，因此是一种有效的隐式神经表示方法。"}}
