{"id": "2507.11549", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11549", "abs": "https://arxiv.org/abs/2507.11549", "authors": ["Wendong Mao", "Mingfan Zhao", "Jianfeng Guan", "Qiwei Dong", "Zhongfeng Wang"], "title": "An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search", "comment": null, "summary": "Deformable Attention Transformers (DAT) have shown remarkable performance in\ncomputer vision tasks by adaptively focusing on informative image regions.\nHowever, their data-dependent sampling mechanism introduces irregular memory\naccess patterns, posing significant challenges for efficient hardware\ndeployment. Existing acceleration methods either incur high hardware overhead\nor compromise model accuracy. To address these issues, this paper proposes a\nhardware-friendly optimization framework for DAT. First, a neural architecture\nsearch (NAS)-based method with a new slicing strategy is proposed to\nautomatically divide the input feature into uniform patches during the\ninference process, avoiding memory conflicts without modifying model\narchitecture. The method explores the optimal slice configuration by jointly\noptimizing hardware cost and inference accuracy. Secondly, an FPGA-based\nverification system is designed to test the performance of this framework on\nedge-side hardware. Algorithm experiments on the ImageNet-1K dataset\ndemonstrate that our hardware-friendly framework can maintain have only 0.2%\naccuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA\nshow the proposed method reduces DRAM access times to 18% compared with\nexisting DAT acceleration methods.", "AI": {"tldr": "A new slicing strategy in combination with a NAS-based approach is introduced to optimize DAT for hardware efficiency without compromising accuracy, verified through algorithm and hardware experiments.", "motivation": "To address the challenges of efficient hardware deployment of deformable attention transformers (DAT), which include significant hardware overhead or compromise in model accuracy due to irregular memory access patterns from their data-dependent sampling mechanism.", "method": "First, a neural architecture search (NAS)-based method with a new slicing strategy is proposed to automatically divide the input feature into uniform patches during the inference process, avoiding memory conflicts without modifying model architecture. Secondly, an FPGA-based verification system is designed to test the performance of this framework on edge-side hardware.", "result": "Algorithm experiments on the ImageNet-1K dataset show only a 0.2% accuracy drop compared to the baseline DAT, and hardware experiments on Xilinx FPGA demonstrate that the proposed method reduces DRAM access times to 18% compared with existing DAT acceleration methods.", "conclusion": "The proposed hardware-friendly optimization framework for DAT effectively balances hardware efficiency and model accuracy, making it a promising solution for deploying DAT on edge devices."}}
{"id": "2507.11550", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11550", "abs": "https://arxiv.org/abs/2507.11550", "authors": ["Hyeonseok Jin", "Geonmin Kim", "Kyungbaek Kim"], "title": "Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction", "comment": "7 pages", "summary": "Spatio-temporal traffic prediction plays a key role in intelligent\ntransportation systems by enabling accurate prediction in complex urban areas.\nAlthough not only accuracy but also efficiency for scalability is important,\nsome previous methods struggle to capture heterogeneity such as varying traffic\npatterns across regions and time periods. Moreover, Graph Neural Networks\n(GNNs), which are the mainstream of traffic prediction, not only require\npredefined adjacency matrix, but also limit scalability to large-scale data\ncontaining many nodes due to their inherent complexity. To overcome these\nlimitations, we propose Deformable Dynamic Convolution Network (DDCN) for\naccurate yet efficient traffic prediction. Traditional Convolutional Neural\nNetworks (CNNs) are limited in modeling non-Euclidean spatial structures and\nspatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically\napplying deformable filters based on offset. Specifically, DDCN decomposes\ntransformer-style CNN to encoder-decoder structure, and applies proposed\napproaches to the spatial and spatio-temporal attention blocks of the encoder\nto emphasize important features. The decoder, composed of feed-forward module,\ncomplements the output of the encoder. This novel structure make DDCN can\nperform accurate yet efficient traffic prediction. In comprehensive experiments\non four real-world datasets, DDCN achieves competitive performance, emphasizing\nthe potential and effectiveness of CNN-based approaches for spatio-temporal\ntraffic prediction.", "AI": {"tldr": "DDCN提出了一种基于动态可变形卷积的新型交通预测模型，能够有效处理时空异质性以及大规模数据的可扩展性问题，实验结果表明其具有很高的预测精度和效率。", "motivation": "提出DDCN是为了克服传统的时空交通预测方法在捕获时空异质性和模型复杂性方面的局限，尤其是针对大规模数据的可扩展性问题。", "method": "DDCN提出了一种基于动态可变形卷积的方法，以应对时空异质性和非欧氏空间结构建模的挑战。该方法基于卷积神经网络，并且将Transformer风格的CNN分解为编码器-解码器结构，通过应用可变形偏移滤波器来增强重要特征。解码器由前馈模块组成，用于补充编码器的输出。", "result": "DDCN在四个真实世界的数据集上进行了全面实验，展现了具有竞争力的表现，强调了CNN方法在时空交通预测中的潜力和有效性。", "conclusion": "DDCN作为一种新颖的结构，能够实现既精确又高效的交通预测，并为基于卷积的时空预测方法展现了新的可能性。"}}
{"id": "2507.11554", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11554", "abs": "https://arxiv.org/abs/2507.11554", "authors": ["Zejian Li", "Yize Li", "Chenye Meng", "Zhongni Liu", "Yang Ling", "Shengyuan Zhang", "Guang Yang", "Changyuan Yang", "Zhiyuan Yang", "Lingyun Sun"], "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models", "comment": null, "summary": "Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO", "AI": {"tldr": "本文提出了Inversion-DPO，一种新的对齐框架，它使用DDIM逆式对Direct Preference Optimization (DPO)进行了重新表述，避免了对奖励模型的需要。这提高了精度和效率，特别是在由11,140张具有复杂结构标注图像的实验数据集上验证了其在组成图像生成的复杂任务中的优势。", "motivation": "尽管近年来扩散模型的进展是由对齐方法推动的，这些方法能够更好地使模型符合人类偏好，但是这些方法通常需要大量计算资源训练基础模型和奖励模型，导致计算资源消耗高、模型准确性和训练效率低下。为了解决这些问题，本文提出了Inversion-DPO。", "method": "我们提出了Inversion-DPO，一种新的对齐框架，通过将Direct Preference Optimization (DPO)与DDIM逆式结合，避免了奖励模型的训练。该方法在DMs中的Diffusion-DPO中通过从胜者和输者样本到噪声的确定性逆式进行难以处理的后验采样，从而形成一个新的后训练范式。这种方法消除了对辅助奖励模型或不准确近似的需要，显著提高了训练的精度和效率。", "result": "我们将Inversion-DPO应用于文本到图像生成的基本任务和组成图像生成的复杂任务。大量的实验表明，与现有的后训练方法相比，Inversion-DPO实现了显著的性能提升，并强调了训练生成模型生成高保真组合一致性图像的能力。", "conclusion": "Inversion-DPO探索了快速、高精度对齐扩散模型的新途径，推动了它们在复杂现实生成任务中的应用。"}}
{"id": "2507.11558", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11558", "abs": "https://arxiv.org/abs/2507.11558", "authors": ["Changlu Chen", "Yanbin Liu", "Chaoxi Niu", "Ling Chen", "Tianqing Zhu"], "title": "Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting", "comment": null, "summary": "Foundation models have achieved remarkable success in natural language\nprocessing and computer vision, demonstrating strong capabilities in modeling\ncomplex patterns. While recent efforts have explored adapting large language\nmodels (LLMs) for time-series forecasting, LLMs primarily capture\none-dimensional sequential dependencies and struggle to model the richer\nspatio-temporal (ST) correlations essential for accurate ST forecasting. In\nthis paper, we present \\textbf{ST-VFM}, a novel framework that systematically\nreprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal\nforecasting. While VFMs offer powerful spatial priors, two key challenges arise\nwhen applying them to ST tasks: (1) the lack of inherent temporal modeling\ncapacity and (2) the modality gap between visual and ST data. To address these,\nST-VFM adopts a \\emph{dual-branch architecture} that integrates raw ST inputs\nwith auxiliary ST flow inputs, where the flow encodes lightweight temporal\ndifference signals interpretable as dynamic spatial cues. To effectively\nprocess these dual-branch inputs, ST-VFM introduces two dedicated reprogramming\nstages. The \\emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token\nAdapter to embed temporal context and align both branches into VFM-compatible\nfeature spaces. The \\emph{post-VFM reprogramming} stage introduces a Bilateral\nCross-Prompt Coordination module, enabling dynamic interaction between branches\nthrough prompt-based conditioning, thus enriching joint representation learning\nwithout modifying the frozen VFM backbone. Extensive experiments on ten\nspatio-temporal datasets show that ST-VFM outperforms state-of-the-art\nbaselines, demonstrating effectiveness and robustness across VFM backbones\n(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong\ngeneral framework for spatio-temporal forecasting.", "AI": {"tldr": "本文提出了ST-VFM框架，重新编程视觉基础模型以进行时空预测，通过双分支结构和两个专用的再编程阶段解决时空预测的挑战，实验表明其优越性。", "motivation": "尽管大型语言模型在时间序列预测方面有所探索，但它们主要捕捉一维的序列依赖关系，难以建模预测时空数据所需的丰富时空相关性，因此本文提出ST-VFM框架以解决此问题。", "method": "ST-VFM采用双分支架构，整合原始时空输入与辅助的时空流输入，其中流编码轻量级的时间差异信号作为动态空间线索。为有效处理双分支输入，ST-VFM引入两个专有的重新编程阶段：预VFMs重新编程阶段通过应用带有时间感知令牌适配器来嵌入时间上下文，对齐两个分支以满足VFMs兼容的特征空间；以及后VFMs重新编程阶段，引入双边交叉提示协调模块，通过提示条件动态交互分支，增强联合表示学习而无需修改固定的VFMs主干。", "result": "在十个时空数据集上的广泛实验表明，ST-VFM在保持时空预测的准确性的同时，还能在多种VFMs的基础上超越现有的最先进基线方法。", "conclusion": "ST-VFM框架证明了作为通用时空预测方法的有效性和鲁棒性，展示了在多种VFMs后端及其消融研究中的优越性能，确立了其作为强大通用时空预测框架的地位。"}}
{"id": "2507.11582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11582", "abs": "https://arxiv.org/abs/2507.11582", "authors": ["Kazuyoshi Otsuka"], "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "AI": {"tldr": "本研究通过将大语言模型（LLMs）视为“主观文学评论家”，探索其在文学评估中的审美偏好和评价模式。通过对十篇日本科幻短篇小说的英译版本进行六种最先进的LLMs评估，揭示了它们评价的一致性（α值从1.00到0.35不等）和五种不同的评价模式。这表明LLMs可能具有类似人类批评学派的个体评价特征，而不是作为中立的评估工具。", "motivation": "研究动机是为了探索LLMs在文学评价任务中的主观性，特别是它们的表现形式和背后的逻辑，通过这种方式来更好地理解这些模型内部可能固有的价值体系。", "method": "研究采用十篇日本科幻短篇小说的英译版本，由六种最先进的LLMs在七次独立的评估会话中进行评分。使用主成分分析和聚类技术分析评价结果的一致性和差异。", "result": "结果发现LLMs的评估存在显著的一致性差异（α值范围1.00到0.35，五种不同的评价模式）。TF-IDF分析显示每个模型的评价词汇有所不同。", "conclusion": "结论表明，LLMs可能具有类似人类批评学派的个体评价特性，这并非中立基准评估工具，而是受其训练方式（如强化学习反馈）影响的主观评价系统。"}}
{"id": "2507.11562", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11562", "abs": "https://arxiv.org/abs/2507.11562", "authors": ["Ozer Can Devecioglu", "Serkan Kiranyaz", "Mehmet Yamac", "Moncef Gabbouj"], "title": "Expert Operational GANS: Towards Real-Color Underwater Image Restoration", "comment": "6 pages", "summary": "The wide range of deformation artifacts that arise from complex light\npropagation, scattering, and depth-dependent attenuation makes the underwater\nimage restoration to remain a challenging problem. Like other single deep\nregressor networks, conventional GAN-based restoration methods struggle to\nperform well across this heterogeneous domain, since a single generator network\nis typically insufficient to capture the full range of visual degradations. In\norder to overcome this limitation, we propose xOp-GAN, a novel GAN model with\nseveral expert generator networks, each trained solely on a particular subset\nwith a certain image quality. Thus, each generator can learn to maximize its\nrestoration performance for a particular quality range. Once a xOp-GAN is\ntrained, each generator can restore the input image and the best restored image\ncan then be selected by the discriminator based on its perceptual confidence\nscore. As a result, xOP-GAN is the first GAN model with multiple generators\nwhere the discriminator is being used during the inference of the regression\ntask. Experimental results on benchmark Large Scale Underwater Image (LSUI)\ndataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,\nsurpassing all single-regressor models by a large margin even, with reduced\ncomplexity.", "AI": {"tldr": "The paper introduces xOp-GAN, a GAN model using multiple experts for underwater image restoration, leading to improved restoration quality.", "motivation": "The motivation is to improve the quality of underwater images by overcoming the limitations of traditional single deep regressor networks and conventional GAN-based approaches, which are ineffective in capturing a broad range of underwater image degradations.", "method": "The paper proposes xOp-GAN, a novel GAN model with multiple expert generator networks trained on specific subsets of image quality, aiming to address the limitation of a single generator network in dealing with diverse visual degradations in underwater images.", "result": "Experimental results show that xOp-GAN outperforms existing single-regressor methods on the LSUI dataset with PSNR levels up to 25.16 dB.", "conclusion": "xOp-GAN, with its multiple expert generator networks and discriminator selection, is a more effective model for underwater image restoration, achieving better performance than previous single-regressor models."}}
{"id": "2507.11625", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11625", "abs": "https://arxiv.org/abs/2507.11625", "authors": ["Varun Srivastava", "Fan Lei", "Srija Mukhopadhyay", "Vivek Gupta", "Ross Maciejewski"], "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering", "comment": "Published as a conference paper at COLM 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.", "AI": {"tldr": "研究介绍了一个新的数据集MapIQ，用来评估多模态大语言模型在不同地图类型上的视觉问答性能，同时探讨了改变地图设计对模型性能的影响。", "motivation": "当前的视觉问答研究主要集中在等值地图上，这仅涵盖了有限的主题类别和可视分析任务。为了填补这些空白，引入了一个新的数据集和评估方法，以便于更全面地评估模型的性能。", "method": "通过引入MapIQ数据集，该数据集包含14,706个问题答案对，涵盖三种地图类型：等值地图、变形地图和比例符号地图，跨越六个不同的主题。同时，通过改变地图设计（例如，改变颜色方案，修改图例设计，去除地图元素）来评估多种多模态大语言模型（MLLMs）的性能，并将其与人类基准进行比较。", "result": "通过对比MLLMs模型在六种视觉分析任务上的表现，提供了关于模型鲁棒性、敏感性和对内部地理知识依赖性的见解。", "conclusion": "研究表明通过MapIQ可以有效地评估多模态大语言模型在视觉问答上的性能，并且指出未来可以通过优化地图设计来提高模型的表现。"}}
{"id": "2507.11571", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11571", "abs": "https://arxiv.org/abs/2507.11571", "authors": ["Varun Velankar"], "title": "Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation", "comment": null, "summary": "Estimating a person's age from their gait has important applications in\nhealthcare, security and human-computer interaction. In this work, we review\nfifty-nine studies involving over seventy-five thousand subjects recorded with\nvideo, wearable and radar sensors. We observe that convolutional neural\nnetworks produce an average error of about 4.2 years, inertial-sensor models\nabout 4.5 years and multi-sensor fusion as low as 3.4 years, with notable\ndifferences between lab and real-world data. We then analyse sixty-three\nthousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population\ndataset to quantify correlations between age and five key metrics: stride\nlength, walking speed, step cadence, step-time variability and joint-angle\nentropy, with correlation coefficients of at least 0.27. Next, we fine-tune a\nResNet34 model and apply Grad-CAM to reveal that the network attends to the\nknee and pelvic regions, consistent with known age-related gait changes.\nFinally, on a one hundred thousand sample subset of the VersatileGait database,\nwe compare support vector machines, decision trees, random forests, multilayer\nperceptrons and convolutional neural networks, finding that deep networks\nachieve up to 96 percent accuracy while processing each sample in under 0.1\nseconds. By combining a broad meta-analysis with new large-scale experiments\nand interpretable visualizations, we establish solid performance baselines and\npractical guidelines for reducing gait-age error below three years in\nreal-world scenarios.", "AI": {"tldr": "本研究通过综合元分析和大规模实验，对步态年龄估计技术进行了评估，揭示了不同方法的表现，并提出实际指南以降低现实场景中的年龄估计误差。", "motivation": "步态年龄估算在医疗保健、安全和人机交互领域具有重要应用，本研究旨在建立坚实的性能基线和实用指南，以在现实场景中减少步态年龄估算误差。", "method": "本研究结合了广泛的元分析和大规模实验，分析了五十九项研究，涉及超过七万五千名受试者通过视频、可穿戴设备和雷达传感器记录的数据。通过分析，探讨了卷积神经网络、惯性传感器模型以及多传感器融合方法在年龄估算上的表现，并且对 OU-ISIR 大规模数据集中六万三千八百四十六个步态周期进行了分析，以量化年龄与步长、步速、步频、步距变异性和关节角度熵之间的相关性，并通过 Grad-CAM 解析了 ResNet34 模型关注的步态特征区域。最后，在 VersatileGait 数据库的一百万样本子集中，比较了支持向量机、决策树、随机森林、多层感知器和卷积神经网络的表现，评估模型的准确性和处理速度。", "result": "研究结果显示，卷积神经网络在年龄估算上的平均误差约为4.2年，惯性传感器模型约为4.5年，而多传感器融合方法的误差可低至3.4年，但在实验室数据和现实世界数据之间存在显著差异。年龄与步长、步速等五个关键指标间的相关系数至少为0.27。实验表明，深化神经网络可以达到96%的准确性，每个样本处理时间不到0.1秒。", "conclusion": "通过广泛的分析和实验，确立了步态年龄估算技术的性能基线，并提供了具体的实践指导，以期在未来研究中进一步减少步态年龄估算的误差。"}}
{"id": "2507.11634", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11634", "abs": "https://arxiv.org/abs/2507.11634", "authors": ["Farideh Majidi", "Ziaeddin Beheshtifard"], "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation", "comment": "Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems", "summary": "This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.", "AI": {"tldr": "The study fine-tunes multilingual pre-trained models using few-shot and incremental learning to conduct sentiment analysis in Persian, achieving 96% accuracy with mDeBERTa and XLM-RoBERTa.", "motivation": "The motivation is to develop a model that can perform sentiment analysis in Persian with a limited amount of data, leveraging knowledge from high-resource languages.", "method": "The paper uses few-shot and incremental learning methods to fine-tune three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, DistilBERT) on small samples of Persian datasets from various sources like X, Instagram, Digikala, Snappfood, and Taaghche.", "result": "The mDeBERTa and XLM-RoBERTa models achieved 96% accuracy in Persian sentiment analysis.", "conclusion": "Combining few-shot learning and incremental learning with multilingual pre-trained models is effective for performing sentiment analysis in Persian with limited data."}}
{"id": "2507.11575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11575", "abs": "https://arxiv.org/abs/2507.11575", "authors": ["Victor Caquilpan"], "title": "What cat is that? A re-id model for feral cats", "comment": "Master's project", "summary": "Feral cats exert a substantial and detrimental impact on Australian wildlife,\nplacing them among the most dangerous invasive species worldwide. Therefore,\nclosely monitoring these cats is essential labour in minimising their effects.\nIn this context, the potential application of Re-Identification (re-ID) emerges\nto enhance monitoring activities for these animals, utilising images captured\nby camera traps. This project explores different CV approaches to create a\nre-ID model able to identify individual feral cats in the wild. The main\napproach consists of modifying a part-pose guided network (PPGNet) model,\ninitially used in the re-ID of Amur tigers, to be applicable for feral cats.\nThis adaptation, resulting in PPGNet-Cat, which incorporates specific\nmodifications to suit the characteristics of feral cats images. Additionally,\nvarious experiments were conducted, particularly exploring contrastive learning\napproaches such as ArcFace loss. The main results indicate that PPGNet-Cat\nexcels in identifying feral cats, achieving high performance with a mean\nAverage Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes\nestablish PPGNet-Cat as a competitive model within the realm of re-ID.", "AI": {"tldr": "研究通过修改PPGNet模型为PPGNet-Cat来识别野猫个体，使用对比学习方法使其在野猫重识别中表现出色（mAP 0.86，rank-1准确率0.95），展示了其在生态系统保护中的应用潜力。", "motivation": "野猫对澳大利亚野生动物造成显著损害，使它们成为世界上最危险的入侵物种之一。因此，密切监控这些野猫对于减少其影响是至关重要的工作。", "method": "本研究主要采用了对部分姿态引导网络(PPGNet)进行修改的方法，使其适应于野猫个体识别的应用，生成了PPGNet-Cat模型。同时，研究还采用了对比学习方法如ArcFace损失函数进行了多种实验。", "result": "主要结果显示，PPGNet-Cat模型在野猫个体识别中表现出色，以平均精度均值(mAP)0.86和第一排名准确率0.95的高绩效，确立了该模型在重识别领域的竞争力。", "conclusion": "研究结果表明，通过修改和优化PPGNet模型为PPGNet-Cat，可以有效提高野猫个体识别的精度和效率，有助于更好地监控野猫活动，减少其对生态系统的影响。"}}
{"id": "2507.11661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11661", "abs": "https://arxiv.org/abs/2507.11661", "authors": ["Guimin Hu", "Yi Xin", "Lijie Hu", "Zhihong Zhu", "Hasti Seifi"], "title": "Partitioner Guided Modal Learning Framework", "comment": "acm multimedia 2025", "summary": "Multimodal learning benefits from multiple modal information, and each\nlearned modal representations can be divided into uni-modal that can be learned\nfrom uni-modal training and paired-modal features that can be learned from\ncross-modal interaction. Building on this perspective, we propose a\npartitioner-guided modal learning framework, PgM, which consists of the modal\npartitioner, uni-modal learner, paired-modal learner, and uni-paired modal\ndecoder. Modal partitioner segments the learned modal representation into\nuni-modal and paired-modal features. Modal learner incorporates two dedicated\ncomponents for uni-modal and paired-modal learning. Uni-paired modal decoder\nreconstructs modal representation based on uni-modal and paired-modal features.\nPgM offers three key benefits: 1) thorough learning of uni-modal and\npaired-modal features, 2) flexible distribution adjustment for uni-modal and\npaired-modal representations to suit diverse downstream tasks, and 3) different\nlearning rates across modalities and partitions. Extensive experiments\ndemonstrate the effectiveness of PgM across four multimodal tasks and further\nhighlight its transferability to existing models. Additionally, we visualize\nthe distribution of uni-modal and paired-modal features across modalities and\ntasks, offering insights into their respective contributions.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.11579", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11579", "abs": "https://arxiv.org/abs/2507.11579", "authors": ["Sathvik Chereddy", "John Femiani"], "title": "SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation", "comment": "17 pages, 63 figures, Proceedings of the 42nd International\n  Conference on Machine Learning (ICML2025)", "summary": "We present SketchDNN, a generative model for synthesizing CAD sketches that\njointly models both continuous parameters and discrete class labels through a\nunified continuous-discrete diffusion process. Our core innovation is\nGaussian-Softmax diffusion, where logits perturbed with Gaussian noise are\nprojected onto the probability simplex via a softmax transformation,\nfacilitating blended class labels for discrete variables. This formulation\naddresses 2 key challenges, namely, the heterogeneity of primitive\nparameterizations and the permutation invariance of primitives in CAD sketches.\nOur approach significantly improves generation quality, reducing Fr\\'echet\nInception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)\nfrom 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch\ngeneration on the SketchGraphs dataset.", "AI": {"tldr": "本文提出了SketchDNN模型，利用高斯-Softmax扩散过程，解决了CAD草图生成中的异质性和置换不变性问题，提高了生成质量，达到了新的最先进水平。", "motivation": "这种建模方式解决两个关键挑战：CAD草图中图元参数化的异质性和图元的置换不变性。", "method": "我们提出了SketchDNN，这是一种通过统一的连续-离散扩散过程来共同建模连续参数和离散类别标签的生成模型。核心创新在于使用高斯-Softmax扩散，其中通过高斯噪声扰动的 logits 经过softmax转换后投影到概率单纯形，从而为离散变量提供混合类别标签。", "result": "我们的方法在生成质量上有了显著提高，将Frechet Inception Distance（FID）从16.04降到7.80，将负对数似然（NLL）从84.8降低到81.33，在SketchGraphs数据集上建立了新的生成CAD草图的最先进水平。", "conclusion": "通过这种方法，我们验证了高斯-Softmax扩散过程在同时处理离散和连续参数时的有效性，并展示了它在CAD草图生成中的优越性能。"}}
{"id": "2507.11694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11694", "abs": "https://arxiv.org/abs/2507.11694", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Sáez", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro", "Héctor Cerezo-Costas"], "title": "ExpliCIT-QA: Explainable Code-Based Image Table Question Answering", "comment": "This work has been accepted for presentation at the 24nd Portuguese\n  Conference on Artificial Intelligence (EPIA 2025) and will be published in\n  the proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We present ExpliCIT-QA, a system that extends our previous MRT approach for\ntabular question answering into a multimodal pipeline capable of handling\ncomplex table images and providing explainable answers. ExpliCIT-QA follows a\nmodular design, consisting of: (1) Multimodal Table Understanding, which uses a\nChain-of-Thought approach to extract and transform content from table images;\n(2) Language-based Reasoning, where a step-by-step explanation in natural\nlanguage is generated to solve the problem; (3) Automatic Code Generation,\nwhere Python/Pandas scripts are created based on the reasoning steps, with\nfeedback for handling errors; (4) Code Execution to compute the final answer;\nand (5) Natural Language Explanation that describes how the answer was\ncomputed. The system is built for transparency and auditability: all\nintermediate outputs, parsed tables, reasoning steps, generated code, and final\nanswers are available for inspection. This strategy works towards closing the\nexplainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on\nthe TableVQA-Bench benchmark, comparing it with existing baselines. We\ndemonstrated improvements in interpretability and transparency, which open the\ndoor for applications in sensitive domains like finance and healthcare where\nauditing results are critical.", "AI": {"tldr": "研究提出了一种名为ExpliCIT-QA的系统，该系统通过可解释的步骤处理表格图像，并在最终回答问题时提供透明度和可审核性。", "motivation": "本研究旨在解决表格视觉问题回答（TableVQA）系统中解释性差距的问题，特别是为金融和医疗保健等关键领域的应用提供支持。", "method": "ExpliCIT-QA系统采用模块化设计，包括5个主要模块：多模态表格理解、基于语言的推理、自动代码生成、代码执行和自然语言解释。系统旨在提高透明度和可审核性，所有中间输出、解析的表格、推理步骤、生成的代码和最终答案都可以供检查。", "result": "研究在TableVQA-Bench基准测试中评估了ExpliCIT-QA系统，展示了在解释性和透明度方面的改进。", "conclusion": "通过这些改进，该系统为金融和医疗保健等对结果审核至关重要的领域打开了应用程序的大门。"}}
{"id": "2507.11638", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11638", "abs": "https://arxiv.org/abs/2507.11638", "authors": ["Benjamin Keel", "Aaron Quyn", "David Jayne", "Maryam Mohsin", "Samuel D. Relton"], "title": "Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders", "comment": "Published in Medical Image Understanding and Analysis (MIUA) 2025", "summary": "Effective treatment for rectal cancer relies on accurate lymph node\nmetastasis (LNM) staging. However, radiological criteria based on lymph node\n(LN) size, shape and texture morphology have limited diagnostic accuracy. In\nthis work, we investigate applying a Variational Autoencoder (VAE) as a feature\nencoder model to replace the large pre-trained Convolutional Neural Network\n(CNN) used in existing approaches. The motivation for using a VAE is that the\ngenerative model aims to reconstruct the images, so it directly encodes visual\nfeatures and meaningful patterns across the data. This leads to a disentangled\nand structured latent space which can be more interpretable than a CNN. Models\nare deployed on an in-house MRI dataset with 168 patients who did not undergo\nneo-adjuvant treatment. The post-operative pathological N stage was used as the\nground truth to evaluate model predictions. Our proposed model 'VAE-MLP'\nachieved state-of-the-art performance on the MRI dataset, with cross-validated\nmetrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85\n+/- 0.05. Code is available at:\nhttps://github.com/benkeel/Lymph_Node_Classification_MIUA.", "AI": {"tldr": "本文研究使用变分自编码器（VAE）替代大预训练的卷积神经网络（CNN），改进了MRI图像中淋巴结转移（LNM）的诊断准确性，达到了最先进的性能。", "motivation": "由于基于淋巴结（LN）大小、形状和纹理特征的放射学标准的诊断准确性有限，本文旨在改进现有模型的诊断性能。VAE的使用目的在于，其生成模型旨在重建图像，从而直接编码数据中的视觉特征和有意义的模式，从而得到一个更可解释的、解缠的、结构化的潜在空间。", "method": "本文使用变分自编码器（VAE）替换现有的大型预训练卷积神经网络（CNN），作为MRI图像中的淋巴结特征提取模型，旨在提高直肠癌淋巴结转移（LNM）的诊断准确性。", "result": "提出的VAE-MLP模型在使用168名未经新辅助治疗患者的内部MRI数据集中，达到了最先进的性能，其交叉验证指标为AUC 0.86 +/- 0.05，灵敏度0.79 +/- 0.06，特异性0.85 +/- 0.05。", "conclusion": "实验表明，VAE模型在淋巴结转移过程中能有效提高诊断准确性，结构化和可解释的潜在空间有助于理解数据中的复杂模式。"}}
{"id": "2507.11742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11742", "abs": "https://arxiv.org/abs/2507.11742", "authors": ["Meng Li", "Timothy M. McPhillips", "Dingmin Wang", "Shin-Rong Tsai", "Bertram Ludäscher"], "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks", "comment": "Preprint. Accepted to COLM 2025", "summary": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.", "AI": {"tldr": "本文提出了一种称为CRABS的方法，用于理解Python笔记本，它结合了浅层语法分析和大型语言模型（LLM），以消除理解过程中的歧义。实验表明该方法在理解笔记信息流和单元执行依赖关系方面非常有效。", "motivation": "理解Python笔记本中的信息流和操作对于评估、复用和适应新的任务是至关重要的。然而，通过重新执行来理解笔记本并不总是可行的，因为它涉及到解决数据和软件依赖关系的难题。尽管大型语言模型（LLMs）在理解代码方面表现出色，但它们仍然在解析一些现实中的笔记本时存在幻觉和长上下文的问题。", "method": "本文提出了一种称为CRABS的方法，该方法结合了浅层语法分析和大型语言模型（LLM），以理解Python笔记本中的信息流和单元执行依赖关系。具体来说，它使用抽象语法树（AST）来捕捉单元间输入输出的下限和上限估计，并通过单元逐个零样本学习解决剩余的歧义。", "result": "作者通过一个包含50个具有代表性的、高评价的Kaggle笔记本的注释数据集进行了评估。实验表明，LLM正确解决了1425个歧义中的1397个（98%），CRABS在50个笔记本上的平均F1分为98%，用于识别单元间信息流，而识别传递的单元执行依赖关系的平均F1分为99%。", "conclusion": "研究结果显示，结合语法分析和大型语言模型的方法有效解决了笔记本理解中的许多挑战，尤其是在处理代码解析时的幻觉和长上下文难题。CRABS方法在识别单元间信息流和传递执行依赖关系时表现出高准确度。这为更好地评估、复用和适应Python笔记本提供了途径。"}}
{"id": "2507.11642", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11642", "abs": "https://arxiv.org/abs/2507.11642", "authors": ["Abhishek Jaiswal", "Nisheeth Srivastava"], "title": "Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment", "comment": null, "summary": "Posture-based mental state inference has significant potential in diagnosing\nfatigue, preventing injury, and enhancing performance across various domains.\nSuch tools must be research-validated with large datasets before being\ntranslated into practice. Unfortunately, such vision diagnosis faces serious\nchallenges due to the sensitivity of human subject data. To address this, we\nidentify sports settings as a viable alternative for accumulating data from\nhuman subjects experiencing diverse emotional states. We test our hypothesis in\nthe game of cricket and present a posture-based solution to identify human\nintent from activity videos. Our method achieves over 75\\% F1 score and over\n80\\% AUC-ROC in discriminating aggressive and defensive shot intent through\nmotion analysis. These findings indicate that posture leaks out strong signals\nfor intent inference, even with inherent noise in the data pipeline.\nFurthermore, we utilize existing data statistics as weak supervision to\nvalidate our findings, offering a potential solution for overcoming data\nlabelling limitations. This research contributes to generalizable techniques\nfor sports analytics and also opens possibilities for applying human behavior\nanalysis across various fields.", "AI": {"tldr": "A posture-based technique achieves high accuracy in inferring shot intent in cricket, suggesting broad applications in human behavior analysis across multiple domains.", "motivation": "The research is motivated by the potential of posture-based mental state inference for diagnosing fatigue, preventing injury, and enhancing performance in various fields, while addressing challenges of acquiring and managing human subject data.", "method": "The study proposes a posture-based method for inferring human intent from activity videos in the context of cricket, aiming to differentiate between aggressive and defensive shot intent using motion analysis.", "result": "The method achieves over 75% F1 score and over 80% AUC-ROC in identifying aggressive or defensive shot intent in cricket players based on posture analysis.", "conclusion": "The research highlights that posture can reveal strong signals for intent inference, even with data noise. It further demonstrates the feasibility of using existing data statistics as weak supervision to validate findings and overcomes limitations of data labeling in sports analytics and potentially in the analysis of human behavior in other fields."}}
{"id": "2507.11764", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.11764", "abs": "https://arxiv.org/abs/2507.11764", "authors": ["Matteo Fasulo", "Luca Babboni", "Luca Tedeschini"], "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles", "comment": "14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab", "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", "AI": {"tldr": "本文介绍了AI Wizards团队在CLEF 2025 CheckThat! 实验室任务1中的方法，通过情感特征融合改进了基于transformer的分类器，从而显著提高了主观性检测的性能。", "motivation": "研究动机是为了提高在CLEF 2025 CheckThat! 实验室任务1中的主观性检测的性能，任务包括在单语、多语和零样本设置下对新闻文章中的句子进行主观/客观分类。", "method": "本文提出了一种基于transformer的分类器增强策略，通过将由辅助模型生成的.sentiment分数与句子表示结合，改进了标准的微调方法。实验中使用了mDeBERTaV3-base, ModernBERT-base（英语）和Llama3.2-1B模型。为了应对跨语言普遍存在的话题不平衡问题，采用了在开发集上优化的决策阈值校准策略。", "result": "实验结果显示，通过情感特征融合可以显著提高性能，特别是在提高主观性F1分数方面。该框架在多个语言中取得了高的排名，尤其是在希腊语中获得了1st（Macro F1 = 0.51）。", "conclusion": "通过情感特征融合改进的transformer基分类器可以显著提升主观性检测的性能，特别是在处理多语言和零样本情况下，能够应对话题不平衡问题，提高F1分数。"}}
{"id": "2507.11653", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11653", "abs": "https://arxiv.org/abs/2507.11653", "authors": ["Hannah Shafferman", "Annika Thomas", "Jouko Kinnari", "Michael Ricard", "Jose Nino", "Jonathan How"], "title": "VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization", "comment": "9 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Global localization is critical for autonomous navigation, particularly in\nscenarios where an agent must localize within a map generated in a different\nsession or by another agent, as agents often have no prior knowledge about the\ncorrelation between reference frames. However, this task remains challenging in\nunstructured environments due to appearance changes induced by viewpoint\nvariation, seasonal changes, spatial aliasing, and occlusions -- known failure\nmodes for traditional place recognition methods. To address these challenges,\nwe propose VISTA (View-Invariant Segmentation-Based Tracking for Frame\nAlignment), a novel open-set, monocular global localization framework that\ncombines: 1) a front-end, object-based, segmentation and tracking pipeline,\nfollowed by 2) a submap correspondence search, which exploits geometric\nconsistencies between environment maps to align vehicle reference frames. VISTA\nenables consistent localization across diverse camera viewpoints and seasonal\nchanges, without requiring any domain-specific training or finetuning. We\nevaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a\n69% improvement in recall over baseline methods. Furthermore, we maintain a\ncompact object-based map that is only 0.6% the size of the most\nmemory-conservative baseline, making our approach capable of real-time\nimplementation on resource-constrained platforms.", "AI": {"tldr": "提出VISTA框架，用于解决非结构化环境中的全局定位问题，特别是在面对视角和季节变化时提高定位准确性，并减少了存储需求。", "motivation": "解决由于视角变化、季节变化、空间别名和遮挡等引起的外观变化，这些是传统地点识别方法的失败模式，特别是在非结构化环境中进行自主导航时的全局定位问题。", "method": "VISTA（视图不变分割追踪框架）结合基于对象的分割和追踪前端流程，以及利用环境地图几何一致性的子地图对应搜索后端流程。", "result": "在季节性和倾斜角度的航空数据集上进行评估，VISTA取得了比基线方法高69%的召回率提升，且只占最节省内存基线方法0.6%的存储空间。", "conclusion": "VISTA提出了一种新颖的、开集的、单目全局定位框架，适用于实时处理资源受限的平台，能够在不同视角和季节变化下保持一致的定位效果。"}}
