{"id": "2509.07135", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07135", "abs": "https://arxiv.org/abs/2509.07135", "authors": ["Ruggero Marino Lazzaroni", "Alessandro Angioi", "Michelangelo Puliga", "Davide Sanna", "Roberto Marras"], "title": "MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations", "comment": "Accepted as an oral presentation at CLiC-it 2025", "summary": "Large language models (LLMs) show increasing potential in education, yet\nbenchmarks for non-English languages in specialized domains remain scarce. We\nintroduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on\nItalian medical university entrance examinations. Sourced from Edizioni Simone,\na leading preparatory materials publisher, MedBench-IT comprises 17,410\nexpert-written multiple-choice questions across six subjects (Biology,\nChemistry, Logic, General Culture, Mathematics, Physics) and three difficulty\nlevels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude\nseries) and resource-efficient open-source alternatives (<30B parameters)\nfocusing on practical deployability.\n  Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response\nconsistency, varying by subject), ordering bias analysis (minimal impact), and\nreasoning prompt evaluation. We also examined correlations between question\nreadability and model performance, finding a statistically significant but\nsmall inverse relationship. MedBench-IT provides a crucial resource for Italian\nNLP community, EdTech developers, and practitioners, offering insights into\ncurrent capabilities and standardized evaluation methodology for this critical\ndomain.", "AI": {"tldr": "本文提出了MedBench-IT，一个针对意大利医学大学入学考试的基准测试，分析了多种语言模型在该基准上的表现，并探讨了问题难度与模型性能的关系。", "motivation": "大型语言模型在教育领域的潜力日益增长，但对于特定领域中的非英语语言基准仍然是稀缺的。因此，我们引入了一个针对意大利语的医学大学入学考试的评估基准--MedBench-IT。", "method": "我们介绍了MedBench-IT，这是一个针对意大利语医学大学入学考试的全面评估基准。MedBench-IT包含了17,410个由专家编写的多选题，涵盖了六个学科和三个难度级别。在实验中，我们测试了各种模型，包括专有的大语言模型和资源高效的开源模型。", "result": "除了准确率外，我们还进行了严格的可重现性测试（88.86%的回答一致性，按学科不同）和排序偏差分析（影响较小），以及推理提示评估。我们还研究了问题的可读性和模型性能之间的相关性，发现存在一个统计学意义但较小的负相关关系。", "conclusion": "MedBench-IT为意大利NLP社区、EdTech开发者和实践者提供了宝贵的资源，它揭示了当前的语言模型在该关键领域的能力并提供了一个标准化的评估方法。"}}
{"id": "2509.07139", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.07139", "abs": "https://arxiv.org/abs/2509.07139", "authors": ["William Chen", "Chutong Meng", "Jiatong Shi", "Martijn Bartelds", "Shih-Heng Wang", "Hsiu-Hsuan Wang", "Rafael Mosquera", "Sara Hincapie", "Dan Jurafsky", "Antonis Anastasopoulos", "Hung-yi Lee", "Karen Livescu", "Shinji Watanabe"], "title": "The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties", "comment": "Interspeech 2025", "summary": "Recent improvements in multilingual ASR have not been equally distributed\nacross languages and language varieties. To advance state-of-the-art (SOTA) ASR\nmodels, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a\nnew test suite that consists of data from 200+ languages, accents, and dialects\nto evaluate SOTA multilingual speech models. The challenge also introduces an\nonline evaluation server based on DynaBench, allowing for flexibility in model\ndesign and architecture for participants. The challenge received 5 submissions\nfrom 3 teams, all of which outperformed our baselines. The best-performing\nsubmission achieved an absolute improvement in LID accuracy of 23% and a\nreduction in CER of 18% when compared to the best baseline on a general\nmultilingual test set. On accented and dialectal data, the best submission\nobtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance\nof community challenges in making speech technologies more inclusive.", "AI": {"tldr": "Interspeech 2025 ML-SUPERB 2.0挑战赛构建了包含200多种语言的数据集来评估多语种语音模型，并引入了一个在线评估服务器。参赛模型在LID和CER指标上显著超越了基线。", "motivation": "由于近期多语种语音识别（ASR）的进步在不同语言和语言变体之间的分布不均，为了推动ASR模型的前沿发展，提出了Interspeech 2025 ML-SUPERB 2.0挑战赛。", "method": "构建了一个包含超过200种语言、口音和方言的数据集来评估先进的多语种语音模型。此外，还引入了一个基于DynaBench的在线评估服务器，为参赛者提供模型设计和架构的灵活性。", "result": "挑战赛收到了3个团队的5份提交，所有提交的模型都超越了基线模型。其中，最佳提交模型在通用多语种测试集上将LID准确率提高了23%，CER降低了18%。在口音和方言数据上，最佳提交模型将CER降低了30.2%，LID准确率提高了15.7%。", "conclusion": "此次挑战赛证明了社区挑战赛在使语音技术更具包容性方面的重要性。"}}
{"id": "2509.07142", "categories": ["cs.CL", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.07142", "abs": "https://arxiv.org/abs/2509.07142", "authors": ["Zhiyin Tan", "Jennifer D'Souza"], "title": "Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models", "comment": "Accepted for publication in International Journal on Digital\n  Libraries (IJDL)", "summary": "This study presents a framework for automated evaluation of dynamically\nevolving topic models using Large Language Models (LLMs). Topic modeling is\nessential for organizing and retrieving scholarly content in digital library\nsystems, helping users navigate complex and evolving knowledge domains.\nHowever, widely used automated metrics, such as coherence and diversity, often\ncapture only narrow statistical patterns and fail to explain semantic failures\nin practice. We introduce a purpose-oriented evaluation framework that employs\nnine LLM-based metrics spanning four key dimensions of topic quality: lexical\nvalidity, intra-topic semantic soundness, inter-topic structural soundness, and\ndocument-topic alignment soundness. The framework is validated through\nadversarial and sampling-based protocols, and is applied across datasets\nspanning news articles, scholarly publications, and social media posts, as well\nas multiple topic modeling methods and open-source LLMs. Our analysis shows\nthat LLM-based metrics provide interpretable, robust, and task-relevant\nassessments, uncovering critical weaknesses in topic models such as redundancy\nand semantic drift, which are often missed by traditional metrics. These\nresults support the development of scalable, fine-grained evaluation tools for\nmaintaining topic relevance in dynamic datasets. All code and data supporting\nthis work are accessible at\nhttps://github.com/zhiyintan/topic-model-LLMjudgment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.07177", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07177", "abs": "https://arxiv.org/abs/2509.07177", "authors": ["Amal Chebbi", "Babajide Kolade"], "title": "Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector", "comment": null, "summary": "Large Language Models have demonstrated impressive capabilities across\nvarious domains. However, their general-purpose nature often limits their\neffectiveness in specialized fields such as energy, where deep technical\nexpertise and precise domain knowledge are essential. In this paper, we\nintroduce EnergyGPT, a domain-specialized language model tailored for the\nenergy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised\nFine-Tuning on a high-quality, curated corpus of energy-related texts. We\npresent a complete development pipeline, including data collection and\ncuration, model fine-tuning, benchmark design and LLM-judge choice, evaluation\nand deployment. Through this work, we demonstrate that our training strategy\nenables improvements in domain relevance and performance without the need for\nlarge-scale infrastructure. By evaluating the performance of the model using\ndomain-specific question-answering benchmarks, our results demonstrate that\nEnergyGPT outperforms the base model in most of the energy-related language\nunderstanding and generation tasks.", "AI": {"tldr": "本文介绍了EnergyGPT，一种专门针对能源领域的语言模型，通过微调LLaMA 3.1-8B模型开发，展示了其在多数能源相关任务中的优越表现。", "motivation": "大语言模型虽然在多个领域展示了惊人的能力，但其通用性质往往限制了在如能源这类需要深度技术专长和精确领域知识的专门领域的有效性。因此，有必要开发一个针对特定领域的语言模型，以提高在这些领域的表现。", "method": "通过监督微调，使用高质量整理过的能源相关文本语料库，对LLaMA 3.1-8B模型进行微调，开发出专门针对能源领域的语言模型EnergyGPT，并展示了包括数据收集与整理、模型微调、基准设计、LLM-judge选择、评估与部署在内的完整开发流程。", "result": "实验结果表明，与基础模型相比，EnergyGPT在大多数能源相关的语言理解和生成任务中表现出更好的性能。", "conclusion": "这项研究表明，在不需要大规模基础设施的情况下，通过采用适当的训练策略，可以开发出在特定领域表现优秀的语言模型。"}}
{"id": "2509.06986", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06986", "abs": "https://arxiv.org/abs/2509.06986", "authors": ["Cedric Caruzzo", "Jong Chul Ye"], "title": "CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis", "comment": "14 pages, 4 figures. Code available at:\n  https://github.com/CellPainTR/CellPainTR", "summary": "Large-scale biological discovery requires integrating massive, heterogeneous\ndatasets like those from the JUMP Cell Painting consortium, but technical batch\neffects and a lack of generalizable models remain critical roadblocks. To\naddress this, we introduce CellPainTR, a Transformer-based architecture\ndesigned to learn foundational representations of cellular morphology that are\nrobust to batch effects. Unlike traditional methods that require retraining on\nnew data, CellPainTR's design, featuring source-specific context tokens, allows\nfor effective out-of-distribution (OOD) generalization to entirely unseen\ndatasets without fine-tuning. We validate CellPainTR on the large-scale JUMP\ndataset, where it outperforms established methods like ComBat and Harmony in\nboth batch integration and biological signal preservation. Critically, we\ndemonstrate its robustness through a challenging OOD task on the unseen Bray et\nal. dataset, where it maintains high performance despite significant domain and\nfeature shifts. Our work represents a significant step towards creating truly\nfoundational models for image-based profiling, enabling more reliable and\nscalable cross-study biological analysis.", "AI": {"tldr": "研究人员提出了一种名为CellPainTR的基于Transformer的架构，旨在解决大规模生物数据集整合的挑战。这种方法在多个任务中表现出色，标志着图像基础性分析模型的一个重要进展。", "motivation": "大规模的生物发现需要整合大规模的异构数据集，如JUMP细胞绘画联盟的数据集，但技术批次效应和缺乏通用性模型仍然是关键障碍。为了应对这些问题，研究人员提出了一种新型的方法。", "method": "CellPainTR, 一种基于Transformer的架构，旨在学习细胞形态的基础表示，这些表示能够抵御批次效应。该方法通过引入源特定上下文令牌，实现了对未见数据集的有效泛化，并且无需再调优。", "result": "在大规模的JUMP数据集上验证了CellPainTR，结果表明，它在批次整合和生物学信号保持方面都超越了已建立的方法如ComBat和Harmony。此外，在具有显著领域和特征转换的未见Bray等数据集上，CellPainTR也表现出强大的性能和健壮性。", "conclusion": "这项研究表明，CellPainTR在大规模图像基生物测定分析中，成为了基础模型，实现了跨研究分析的可靠性和可扩展性。"}}
{"id": "2509.07188", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07188", "abs": "https://arxiv.org/abs/2509.07188", "authors": ["Zonghai Yao", "Michael Sun", "Won Seok Jang", "Sunjae Kwon", "Soie Kwon", "Hong Yu"], "title": "DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge", "comment": "Equal contribution for the first two authors. To appear in the\n  proceedings of the Main Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) 2025", "summary": "Discharge communication is a critical yet underexplored component of patient\ncare, where the goal shifts from diagnosis to education. While recent large\nlanguage model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they\nfail to evaluate models' ability to support patients after the visit. We\nintroduce DischargeSim, a novel benchmark that evaluates LLMs on their ability\nto act as personalized discharge educators. DischargeSim simulates post-visit,\nmulti-turn conversations between LLM-driven DoctorAgents and PatientAgents with\ndiverse psychosocial profiles (e.g., health literacy, education, emotion).\nInteractions are structured across six clinically grounded discharge topics and\nassessed along three axes: (1) dialogue quality via automatic and LLM-as-judge\nevaluation, (2) personalized document generation including free-text summaries\nand structured AHRQ checklists, and (3) patient comprehension through a\ndownstream multiple-choice exam. Experiments across 18 LLMs reveal significant\ngaps in discharge education capability, with performance varying widely across\npatient profiles. Notably, model size does not always yield better education\noutcomes, highlighting trade-offs in strategy use and content prioritization.\nDischargeSim offers a first step toward benchmarking LLMs in post-visit\nclinical education and promoting equitable, personalized patient support.", "AI": {"tldr": "研究开发了一个新的基准测试DischargeSim，用于评估语言模型在出院后教育患者的能力，结果显示模型规模与教育效果不一定成正比。", "motivation": "现有的大型语言模型对住院期间的诊断推理能力进行了测试，但对于出院后的教育能力缺乏评估。因此，需要一种新的方法来评估LLM在出院教育中的表现。", "method": "通过DischargeSim模拟出院教育中的多轮对话，评估语言模型作为个性化出院教育者的性能。对话参与者包括基于LLM的医生代理和具有不同心理社会背景的患者代理。评估标准包括对话质量，个性化文档生成能力以及患者理解程度。", "result": "实验结果显示，不同LLM在出院教育中的能力存在显著差异，且并非模型规模越大教育效果越好，模型策略使用和内容优先级的权衡导致效果不一。", "conclusion": "DischargeSim为评估LLM在出院教育中的表现提供了新的方法，有助于推动个性化、公平的患者支持措施的发展。"}}
{"id": "2509.06987", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06987", "abs": "https://arxiv.org/abs/2509.06987", "authors": ["Alexey Zhukov", "Jenny Benois-Pineau", "Amira Youssef", "Akka Zemmari", "Mohamed Mosbah", "Virginie Taillandier"], "title": "FusWay: Multimodal hybrid fusion approach. Application to Railway Defect Detection", "comment": null, "summary": "Multimodal fusion is a multimedia technique that has become popular in the\nwide range of tasks where image information is accompanied by a signal/audio.\nThe latter may not convey highly semantic information, such as speech or music,\nbut some measures such as audio signal recorded by mics in the goal to detect\nrail structure elements or defects. While classical detection approaches such\nas You Only Look Once (YOLO) family detectors can be efficiently deployed for\ndefect detection on the image modality, the single modality approaches remain\nlimited. They yield an overdetection in case of the appearance similar to\nnormal structural elements. The paper proposes a new multimodal fusion\narchitecture built on the basis of domain rules with YOLO and Vision\ntransformer backbones. It integrates YOLOv8n for rapid object detection with a\nVision Transformer (ViT) to combine feature maps extracted from multiple layers\n(7, 16, and 19) and synthesised audio representations for two defect classes:\nrail Rupture and Surface defect. Fusion is performed between audio and image.\nExperimental evaluation on a real-world railway dataset demonstrates that our\nmultimodal fusion improves precision and overall accuracy by 0.2 points\ncompared to the vision-only approach. Student's unpaired t-test also confirms\nstatistical significance of differences in the mean accuracy.", "AI": {"tldr": "A new multimodal fusion architecture, integrating YOLO and Vision Transformer with audio signals, is proposed for rail and surface defect detection. This method significantly enhances detection precision and accuracy compared to a vision-only approach.", "motivation": "The motivation is to overcome the limitations of single modality defect detection methods which often result in overdetection due to similarities with normal structural elements. The multimodal approach aims to enhance precision and accuracy.", "method": "The paper proposes a new multimodal fusion architecture that combines YOLOv8n for rapid object detection and Vision Transformer (ViT) for feature extraction from multiple layers, integrating both with audio signals to detect rail and surface defects.", "result": "The multimodal fusion method improves precision and overall accuracy by 0.2 points over the vision-only approach. Student's unpaired t-test confirms the statistical significance of the accuracy improvements on a real-world railway dataset.", "conclusion": "The experimental results demonstrate that the proposed multimodal fusion approach effectively enhances detection precision and accuracy for rail defects, supporting its superior performance over single modality methods."}}
{"id": "2509.07190", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.07190", "abs": "https://arxiv.org/abs/2509.07190", "authors": ["Zahra Atf", "Peter R Lewis"], "title": "Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation", "comment": "This paper was accepted for presentation at the 35th IEEE\n  International Conference on Collaborative Advances in Software and Computing.\n  Conference website:https://conf.researchr.org/home/cascon-2025", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings,\nwhere explaining uncertainty is both technical and ethical. Probabilistic\nmethods are often opaque and misaligned with expectations of transparency. We\npropose a framework based on rule-based moral principles for handling\nuncertainty in LLM-generated text. Using insights from moral psychology and\nvirtue ethics, we define rules such as precaution, deference, and\nresponsibility to guide responses under epistemic or aleatoric uncertainty.\nThese rules are encoded in a lightweight Prolog engine, where uncertainty\nlevels (low, medium, high) trigger aligned system actions with plain-language\nrationales. Scenario-based simulations benchmark rule coverage, fairness, and\ntrust calibration. Use cases in clinical and legal domains illustrate how moral\nreasoning can improve trust and interpretability. Our approach offers a\ntransparent, lightweight alternative to probabilistic models for socially\nresponsible natural language generation.", "AI": {"tldr": "本文提出了一个基于道德原则的框架，用于处理大型语言模型生成文本中的不确定性问题，旨在提供一个透明、轻量级的替代概率模型的方案，适用于负责任的自然语言生成。", "motivation": "大型语言模型(LLMs)越来越被用于高风险环境中，在这种环境中解释不确定性既是技术问题也是伦理问题。概率方法通常不透明并且与透明度的期望不一致。我们提出了一种基于规则的道德原则框架来处理LLMs生成文本中的不确定性。", "method": "基于道德心理和美德伦理学的见解，我们定义了如预防、谦让和责任等规则来指导在知识或随机不确定性情况下的响应。这些规则被编码在一个轻量级的Prolog引擎中，不同的不确定性水平（低、中、高）会触发带有简单语言解释的相应系统动作。", "result": "通过对场景为基础的模拟进行基准测试，我们评估了规则的覆盖率、公平性和信任校准。临床和法律领域的实例展示了道德推理如何提高信任度和解释性。", "conclusion": "我们的方法提供了一种透明、轻量级的替代概率模型的方案，尤其适用于负责任的自然语言生成。"}}
{"id": "2509.06988", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06988", "abs": "https://arxiv.org/abs/2509.06988", "authors": ["Yingsheng Wang", "Shuo Lu", "Jian Liang", "Aihua Zheng", "Ran He"], "title": "Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection", "comment": "Accepted to PRCV2025", "summary": "Out-of-distribution (OOD) detection helps models identify data outside the\ntraining categories, crucial for security applications. While feature-based\npost-hoc methods address this by evaluating data differences in the feature\nspace without changing network parameters, they often require access to\ntraining data, which may not be suitable for some data privacy scenarios. This\nmay not be suitable in scenarios where data privacy protection is a concern. In\nthis paper, we propose a simple yet effective post-hoc method, termed\nClassifier-based Feature Reconstruction (ClaFR), from the perspective of\nsubspace projection. It first performs an orthogonal decomposition of the\nclassifier's weights to extract the class-known subspace, then maps the\noriginal data features into this subspace to obtain new data representations.\nSubsequently, the OOD score is determined by calculating the feature\nreconstruction error of the data within the subspace. Compared to existing OOD\ndetection algorithms, our method does not require access to training data while\nachieving leading performance on multiple OOD benchmarks. Our code is released\nat https://github.com/Aie0923/ClaFR.", "AI": {"tldr": "提出了Classifier-based Feature Reconstruction (ClaFR)，通过子空间投影的方法，在不使用训练数据的情况下，实现高效的OOD检测。", "motivation": "现有的基于特征的后验方法虽然有效，但通常需要访问训练数据，这在需要保护数据隐私的场景下可能不适合。", "method": "ClaFR首先对分类器的权重进行正交分解以提取已知类子空间，然后将原始数据特征映射到该子空间中获得新的数据表示，最后通过计算数据在子空间内的特征重构误差来确定OOD分数。", "result": "与现有的OOD检测算法相比，本方法在多个OOD基准上实现了领先性能，且不需要访问训练数据。", "conclusion": "该方法不仅简单有效，而且易于理解和应用，对数据隐私保护具有重要意义。"}}
{"id": "2509.07274", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07274", "abs": "https://arxiv.org/abs/2509.07274", "authors": ["Aida Kostikova", "Ole Pütz", "Steffen Eger", "Olga Sabelfeld", "Benjamin Paassen"], "title": "LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade", "comment": null, "summary": "Migration has been a core topic in German political debate, from millions of\nexpellees post World War II over labor migration to refugee movements in the\nrecent past. Studying political speech regarding such wide-ranging phenomena in\ndepth traditionally required extensive manual annotations, limiting the scope\nof analysis to small subsets of the data. Large language models (LLMs) have the\npotential to partially automate even complex annotation tasks. We provide an\nextensive evaluation of a multiple LLMs in annotating (anti-)solidarity\nsubtypes in German parliamentary debates compared to a large set of thousands\nof human reference annotations (gathered over a year). We evaluate the\ninfluence of model size, prompting differences, fine-tuning, historical versus\ncontemporary data; and we investigate systematic errors. Beyond methodological\nevaluation, we also interpret the resulting annotations from a social science\nlense, gaining deeper insight into (anti-)solidarity trends towards migrants in\nthe German post-World War II period and recent past. Our data reveals a high\ndegree of migrant-directed solidarity in the postwar period, as well as a\nstrong trend towards anti-solidarity in the German parliament since 2015,\nmotivating further research. These findings highlight the promise of LLMs for\npolitical text analysis and the importance of migration debates in Germany,\nwhere demographic decline and labor shortages coexist with rising polarization.", "AI": {"tldr": "通过大规模语言模型评估德国议会辩论中关于移民的(反)团结趋势，发现二战后对移民有高度团结，但从2015年起出现强烈的反团结趋势。", "motivation": "传统的手动注释限制了分析的范围，仅限于数据的小子集。大规模语言模型有潜力部分自动化复杂的注释任务，以更广泛地研究与迁移相关的政治演讲。", "method": "使用大规模语言模型(LLMs)对政治演讲中的(反)团结子类型进行自动注释，并将其与数千个人类注释参考进行比较。评估考虑了模型大小、提示差异、微调、历史与当代数据的影响，并调查了系统性错误。", "result": "研究结果发现二战后德国议会表现出高度的移民团结，但自2015年起呈现出强烈的反团结趋势。", "conclusion": "大规模语言模型在政治文本分析中表现出前景，同时也强调了在德国进行移民辩论的重要性，因为人口减少和劳动力短缺与日益增加的分化共存。"}}
{"id": "2509.06990", "categories": ["cs.CV", "cs.LG", "I.2; I.4"], "pdf": "https://arxiv.org/pdf/2509.06990", "abs": "https://arxiv.org/abs/2509.06990", "authors": ["Bryan Rodas", "Natalie Montesino", "Jakob Ambsdorf", "David Klindt", "Randall Balestriero"], "title": "DIET-CP: Lightweight and Data Efficient Self Supervised Continued Pretraining", "comment": null, "summary": "Continued pretraining offers a promising solution for adapting foundation\nmodels to a new target domain. However, in specialized domains, available\ndatasets are often very small, limiting the applicability of SSL methods\ndeveloped for large-scale pretraining and making hyperparameter search\ninfeasible. In addition, pretrained models are usually released as\nbackbone-weights only, lacking important information to continue pretraining.\nWe propose to bridge this gap with DIET-CP, a simple continued pretraining\nstrategy, where any strong foundation model can be steered towards the new data\ndistribution of interest. DIET-CP relies on a very simple objective, requires\nno labels, and introduces no more hyperparameters than supervised finetuning.\nIt is stable across data modalities and backbone choices, while providing a\nsignificant performance boost for state-of-the-art models such as DINOv3 using\nonly 1000 images.", "AI": {"tldr": "文章提出DIET-CP，一种适合小数据集的持续预训练策略，不需要标签，并且只比监督微调多使用一个超参数，可以显著提升模型在特定数据集上的性能。", "motivation": "持续预训练作为一种将基础模型适应新目标域的有前景的方法，但在特定领域可用的数据集通常非常小，导致无法应用大规模预训练的自监督方法，也使得超参数搜索不可行。此外，预训练模型通常是仅以主干权重的形式发布，缺乏持续预训练的重要信息。", "method": "DIET-CP, 一种简单的持续预训练策略，可引导任何强大的基础模型适应新的数据分布。DIET-CP依赖于一个非常简单的目标，不需要标签，并且除了监督微调之外不引入任何超参数。", "result": "DIET-CP在不同的数据模态和主干选择上都稳定，仅使用1000张图像即可为最先进的模型，如DINOv3，提供显著的性能提升。", "conclusion": "DIET-CP提供了一种解决小规模数据集和超参数搜索挑战的方法，有助于提升模型在特定领域的适应性和性能。"}}
{"id": "2509.07301", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07301", "abs": "https://arxiv.org/abs/2509.07301", "authors": ["Zhuoqing Song", "Peng Sun", "Huizhuo Yuan", "Quanquan Gu"], "title": "Causal Attention with Lookahead Keys", "comment": null, "summary": "In standard causal attention, each token's query, key, and value (QKV) are\nstatic and encode only preceding context. We introduce CAuSal aTtention with\nLookahead kEys (CASTLE), an attention mechanism that continually updates each\ntoken's keys as the context unfolds. We term these updated keys lookahead keys\nbecause they belong to earlier positions yet integrate information from tokens\nthat appear later relative to those positions, while strictly preserving the\nautoregressive property. Although the mechanism appears sequential, we derive a\nmathematical equivalence that avoids explicitly materializing lookahead keys at\neach position and enables efficient parallel training. On language modeling\nbenchmarks, CASTLE consistently outperforms standard causal attention across\nmodel scales, reducing validation perplexity and improving performance on a\nrange of downstream tasks.", "AI": {"tldr": "本文提出了一种名为CASTLE的新注意力机制，通过不断更新每个标记的键值并保持自回归属性，提高了语言模型表现。", "motivation": "标准的因果注意力中，每个标记的查询、键、值(QKV)是静态的，仅编码之前的上下文。研究动机在于引入一种新的注意力机制（CASTLE），能够在上下文逐渐展开时更新每个标记的键，以提高模型的表现。", "method": "提出了名为CAuSal aTtention with Lookahead kEys (CASTLE) 的注意力机制，该机制在上下文展开时不断更新每个标记的关键值(k)，这些关键值被称为前瞻关键值，因为它们属于较早的位置，但包含了相对于那些位置较晚出现的标记的信息，同时严格保持自回归属性。尽管该机制看似是顺序的，但其被证明可以等效转换为可以高效并行训练的形式。", "result": "在语言建模基准测试中，CASTLE在不同的模型规模上始终优于标准因果注意力，降低了验证困惑度，并提升了多种下游任务的性能。", "conclusion": "通过更新键的方式，CASTLE改进了标准因果注意力模型的表现，展示了在语言建模上的潜在价值，并且在不同规模的模型以及多种下游任务上表现出了改进。"}}
{"id": "2509.06992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06992", "abs": "https://arxiv.org/abs/2509.06992", "authors": ["Kun Zhai", "Siheng Chen", "Xingjun Ma", "Yu-Gang Jiang"], "title": "FedAPT: Federated Adversarial Prompt Tuning for Vision-Language Models", "comment": "ACM MM25", "summary": "Federated Prompt Tuning (FPT) is an efficient method for cross-client\ncollaborative fine-tuning of large Vision-Language Models (VLMs). However,\nmodels tuned using FPT are vulnerable to adversarial attacks, leading to\nmisclassification in downstream tasks. In this work, we introduce Federated\nAdversarial Prompt Tuning (\\textbf{FedAPT}), a novel method designed to enhance\nthe adversarial robustness of FPT. We identify a key issue in FedAPT under\nnon-independent and identically distributed (non-IID) settings: a \\textit{class\ninformation gap} between clients and the global model. Clients rely solely on\nlimited local label information to generate adversarial samples for training,\nwhile the global model must defend against adversarial attacks from global\nlabels. To address this issue, we propose a \\textbf{class-aware prompt\ngenerator} that generates visual prompts from text prompts. This generator is\nguided by a \\emph{Global Label Embedding} (serving as a ``beacon\") which\nencodes cross-client label information to create more globally-aligned visual\nprompts. Additionally, we propose a \\textbf{cross-layer generator sharing}\nstrategy to enhance prompt coupling across different layers of the model,\nfurther boosting adversarial robustness. Extensive experiments on multiple\nimage classification datasets demonstrate the superiority of FedAPT in\nimproving adversarial robustness, outperforming existing methods by a large\nmargin. FedAPT also exhibits exceptional generalization in cross-domain and\ncross-dataset scenarios, indicating its effectiveness in real-world\napplications.", "AI": {"tldr": "本文提出了一种名为FedAPT的新方法，旨在提高联邦提示调整（FPT）的对抗鲁棒性。通过引入一个基于全局标签嵌入的类别感知提示生成器和跨层生成器共享策略，FedAPT解决了在非独立同分布设置下的类别信息差距。实验结果表明，在多个图像分类数据集上，FedAPT显着提升对抗鲁棒性，表现优于现有方法，并且在跨域和跨数据集应用中展现出优秀的泛化能力。", "motivation": "Federated Prompt Tuning (FPT)方法虽然在跨客户端协作微调视觉语言模型时高效，但其模型容易受到对抗攻击，导致下游任务的误分类。为解决这一问题，本文提出FedAPT，旨在增强FPT模型的对抗鲁棒性。", "method": "本文提出了一种名为FedAPT的对抗鲁棒性提升方法。FedAPT包括一个类别感知的提示生成器，它基于全局标签嵌入创建更全局一致的视觉提示，并通过跨层生成器共享策略来增强不同模型层之间的提示耦合。", "result": "实验结果表明，FedAPT在多个图像分类数据集上表现出对抗鲁棒性的显著提高，优于现有方法。此外，它在跨域和跨数据集场景中的泛化能力也非常出色。", "conclusion": "本文提出FedAPT方法在解决非独立同分布场景下的类别信息差距和提高FPT模型对抗鲁棒性方面取得了好的成效。实验结果验证了FedAPT的优越性，尤其是在跨域和跨数据集情况下的泛化性能。"}}
{"id": "2509.07308", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07308", "abs": "https://arxiv.org/abs/2509.07308", "authors": ["David Oprea", "Sam Powers"], "title": "Basis Vector Metric: A Method for Robust Open-Ended State Change Detection", "comment": "24 pages", "summary": "We test a new method, which we will abbreviate using the acronym BVM (Basis\nVectors Method), in its ability to judge the state changes in images through\nusing language embeddings. We used the MIT-States dataset, containing about\n53,000 images, to gather all of our data, which has 225 nouns and 115\nadjectives, with each noun having about 9 different adjectives, forming\napproximately 1000 noun-adjective pairs. For our first experiment, we test our\nmethod's ability to determine the state of each noun class separately against\nother metrics for comparison. These metrics are cosine similarity, dot product,\nproduct quantization, binary index, Naive Bayes, and a custom neural network.\nAmong these metrics, we found that our proposed BVM performs the best in\nclassifying the states for each noun. We then perform a second experiment where\nwe try using BVM to determine if it can differentiate adjectives from one\nanother for each adjective separately. We compared the abilities of BVM to\ndifferentiate adjectives against the proposed method the MIT-States paper\nsuggests: using a logistic regression model. In the end, we did not find\nconclusive evidence that our BVM metric could perform better than the logistic\nregression model at discerning adjectives. Yet, we were able to find evidence\nfor possible improvements to our method; this leads to the chance of increasing\nour method's accuracy through certain changes in our methodologies.", "AI": {"tldr": "研究了一种新的称为BVM的方法来判断图像中的状态变化，并通过语言嵌入进行测试，实验一表明BVM在分类名词状态上优于其他几种方法，实验二则显示BVM在区分形容词上不如逻辑回归模型，但也为方法的改进提供了可能。", "motivation": "为了测试基于语言嵌入的BVM方法在判断图像状态变化上的效能。", "method": "使用MIT-States数据集，首先用BVM和其他几种指标比较，测试其在单独名词状态分类上的表现；然后用BVM与逻辑回归模型比较，测试其在单独形容词识别上的效果。", "result": "实验一表明BVM在分类名词状态上表现最佳；实验二结果显示BVM在区分形容词上不如逻辑回归模型，但表明可以通过方法上的改进提高其准确性。", "conclusion": "BVM在名词状态分类上表现出色，但在形容词区分上还需改进，显示出改进BVM以提高整体准确性的潜力。"}}
{"id": "2509.06993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06993", "abs": "https://arxiv.org/abs/2509.06993", "authors": ["Zirui Xu", "Raphael Tang", "Mike Bianco", "Qi Zhang", "Rishi Madhok", "Nikolaos Karianakis", "Fuxun Yu"], "title": "Geospatial Foundational Embedder: Top-1 Winning Solution on EarthVision Embed2Scale Challenge (CVPR 2025)", "comment": "CVPR 2025 EarthVision Embed2Scale challenge Top-1 Winning Solution", "summary": "EarthVision Embed2Scale challenge (CVPR 2025) aims to develop foundational\ngeospatial models to embed SSL4EO-S12 hyperspectral geospatial data cubes into\nembedding vectors that faciliatetes various downstream tasks, e.g.,\nclassification, regression, etc. In this technical report, we introduce our\nproposed method for the Top-1 winning solution on the Embed2Scale Challenge.", "AI": {"tldr": "介绍了在Embed2Scale竞赛中获得第一名的基础地理空间模型解决方案的描述，竞赛目的是利用高光谱数据支持多种后端任务。", "motivation": "竞赛的动机是促进基础地理空间模型的发展，这些模型能够对高光谱地理空间数据进行有效嵌入，以支持如分类和回归等各种后端任务。", "method": "由于缺乏具体方法描述，无法提供。", "result": "由于提供的内容仅为竞赛挑战的背景描述，并没有包含具体的技术细节、方法和结果，所以在没有更多具体内容的情况下，无法生成完整的分析摘要。不过，根据提供的信息，可以初步概括：挑战旨在利用SSL4EO-S12高光谱地理空间数据立方体，建立能够支持多种下游任务（如分类、回归等）的基础地理空间模型。报告中介绍了在Embed2Scale竞赛中获得第一名的解决方案。", "conclusion": "由于缺乏结论信息，无法提供。"}}
{"id": "2509.07309", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07309", "abs": "https://arxiv.org/abs/2509.07309", "authors": ["Chi-Yang Hsu", "Alexander Braylan", "Yiheng Su", "Omar Alonso", "Matthew Lease"], "title": "Instance-level Performance Prediction for Long-form Generation Tasks", "comment": null, "summary": "We motivate and share a new benchmark for instance-level performance\nprediction of long-form generation tasks having multi-faceted, fine-grained\nquality metrics. Our task-, model- and metric-agnostic formulation predicts\ncontinuous evaluation metric scores given only black-box model inputs and\noutputs. Beyond predicting point estimates of metric scores, the benchmark also\nrequires inferring prediction intervals to quantify uncertainty around point\nestimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs,\nbaselines, and metrics per task. We show that scores can be effectively\npredicted across long-form generation tasks using as few as 16 training\nexamples. Overall, we introduce a novel and useful task, a valuable benchmark\nto drive progress, and baselines ready for practical adoption today.", "AI": {"tldr": "本文提出了一种新型的、适用于长文本生成任务的性能预测基准，该方法能够对多方面的评价标准进行预测，并适用于多种模型。实验表明该方法能够在较少样本的情况下有效预测评价分数。", "motivation": "我们引入了这一基准来提升长文本生成任务的性能预测能力，特别是在多方面、细粒度的质量度量条件下。", "method": "我们提出了一种新型的、适用于长文本生成任务的实例级性能预测基准。该基准能够预测连续的评价分数，并且能够基于模型输入和输出估计预测区间的不确定性。这一方法对任务、模型和度量标准都是通用的。", "result": "实验结果表明，使用该基准，仅需16个训练样本即可有效预测长文本生成任务的评价分数。", "conclusion": "我们介绍了一个新的且有用的预测任务，这一基准对于促进性能预测研究和实践应用具有重要价值。"}}
{"id": "2509.06994", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06994", "abs": "https://arxiv.org/abs/2509.06994", "authors": ["Srihari Bandraupalli", "Anupam Purwar"], "title": "VLMs-in-the-Wild: Bridging the Gap Between Academic Benchmarks and Enterprise Reality", "comment": null, "summary": "Open-source Vision-Language Models show immense promise for enterprise\napplications, yet a critical disconnect exists between academic evaluation and\nenterprise deployment requirements. Current benchmarks rely heavily on\nmultiple-choice questions and synthetic data, failing to capture the complexity\nof real-world business applications like social media content analysis. This\npaper introduces VLM-in-the-Wild (ViLD), a comprehensive framework to bridge\nthis gap by evaluating VLMs on operational enterprise requirements. We define\nten business-critical tasks: logo detection, OCR, object detection, human\npresence and demographic analysis, human activity and appearance analysis,\nscene detection, camera perspective and media quality assessment, dominant\ncolors, comprehensive description, and NSFW detection. To this framework, we\nbring an innovative BlockWeaver Algorithm that solves the challenging problem\nof comparing unordered, variably-grouped OCR outputs from VLMs without relying\non embeddings or LLMs, achieving remarkable speed and reliability. To\ndemonstrate efficacy of ViLD, we constructed a new benchmark dataset of 7,500\ndiverse samples, carefully stratified from a corpus of one million real-world\nimages and videos. ViLD provides actionable insights by combining semantic\nmatching (both embedding-based and LLM-as-a-judge approaches), traditional\nmetrics, and novel methods to measure the completeness and faithfulness of\ndescriptive outputs. By benchmarking leading open-source VLMs (Qwen, MIMO, and\nInternVL) against a powerful proprietary baseline as per ViLD framework, we\nprovide one of the first industry-grounded, task-driven assessment of VLMs\ncapabilities, offering actionable insights for their deployment in enterprise\nenvironments.", "AI": {"tldr": "本文介绍了VLM-in-the-Wild（ViLD）框架，该框架通过对实际企业需求的验证来评估开放语言视觉模型。通过引入创新的BlockWeaver算法，解决了无序文本比较的问题，并通过构建7500个样本的新基准数据集，为领先开源VL模型的实际应用提供了见解。", "motivation": "当前基准测试主要依赖于多项选择题和合成数据，无法捕捉到例如社交媒体内容分析等实际应用的复杂性。这导致了学术评估和企业部署需求之间的差距。", "method": "ViLD框架定义了十个业务关键任务，并引入了创新的BlockWeaver算法以解决不同输出间的比较问题。创建了一个包含7500个样本的新基准数据集，用于评估视觉语言模型的表现。", "result": "通过对领先的开源视觉语言模型进行评估，该框架提供了有关这些模型在企业环境中部署的实际能力的动作建议。", "conclusion": "ViLD框架为开放语言视觉模型的能力提供了基于行业内实操需求的评估，为这些模型在企业的部署提供了可行的见解。"}}
{"id": "2509.07311", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07311", "abs": "https://arxiv.org/abs/2509.07311", "authors": ["Sihyun Park"], "title": "Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations", "comment": null, "summary": "Recent advances in large language models (LLMs) have been driven by\npretraining, supervised fine tuning (SFT), and alignment tuning. Among these,\nSFT plays a crucial role in transforming a model 's general knowledge into\nstructured responses tailored to specific tasks. However, there is no clearly\nestablished methodology for effective training data selection. Simply\nincreasing the volume of data does not guarantee performance improvements,\nwhile preprocessing, sampling, and validation require substantial time and\ncost.\n  To address this issue, a variety of data selection methods have been\nproposed. Among them, knowledge based selection approaches identify suitable\ntraining data by analyzing the model 's responses. Nevertheless, these methods\ntypically rely on prompt engineering, making them sensitive to variations and\nincurring additional costs for prompt design.\n  In this study, we propose Knowledge Analysis via Model Internal\nRepresentations (KAMIR), a novel approach that overcomes these limitations by\nanalyzing data based on the model 's internal representations. KAMIR computes\nsimilarities between the hidden states of each layer (block) and the final\nhidden states for a given input to assess the data. Unlike prior methods that\nwere largely limited to multiple choice tasks, KAMIR can be applied to a wide\nrange of tasks such as machine reading comprehension and summarization.\nMoreover, it selects data useful for training based on the model 's familiarity\nwith the input, even with a small dataset and a simple classifier architecture.\nExperiments across diverse task datasets demonstrate that training with less\nfamiliar data leads to better generalization performance.", "AI": {"tldr": "本文提出了一种新方法KAMIR，通过分析模型内部表示来选择训练数据，适用于多种任务，实验证明选择较少熟悉的训练数据可以提高模型的泛化性能。", "motivation": "鉴于在监督微调过程中，没有明确的方法来选择有效的训练数据，简单的增加数据量并不一定提高性能，且前期处理、采样、验证需要大量的时间和成本，本文提出了KAMIR这一新方法来解决这一问题。", "method": "KAMIR通过分析模型的内部隐藏状态来选择训练数据，它计算每个层的隐藏状态与最终隐藏状态之间的相似度来评估数据，这种方法能够克服原有依赖于提示工程的方法的局限性。", "result": "本文提出了一种新颖的方法——基于模型内部表示的知识分析（KAMIR），通过分析模型的内部表示来选择合适的训练数据。KAMIR通过计算隐藏状态之间的相似性来评估数据，这种方法不仅适用于多个选择任务，而且还可以应用于机器阅读理解、摘要生成等广泛的任务。实验表明，使用较少熟悉的训练数据可以提高模型的泛化性能。", "conclusion": "KAMIR方法克服了依赖于提示工程的方法的局限性，适用于多种任务，并且即使使用小数据集和简单的分类器架构也可以选择有用的训练数据。而且，实验证实使用较少熟悉的训练数据可以提高模型的泛化性能。"}}
{"id": "2509.06995", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.06995", "abs": "https://arxiv.org/abs/2509.06995", "authors": ["Jimmy Joseph"], "title": "The Protocol Genome A Self Supervised Learning Framework from DICOM Headers", "comment": null, "summary": "In this paper, we introduce the Protocol Genome, a self-supervised learning\nsystem that learns correlations from DICOM headers and achieves AUROC 0.901 (vs\n0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.\nOur method also improves calibration and robustness across modalities (CT, MRI,\nCXR) and vendors. Clinical imaging is funneled through PACS/DICOM, where\nprocedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slice\nthickness) have consequences for contrast, noise, and artifact. These latent\nconfounders impede the generalization of image-only networks across sites. We\nconsider structured DICOM headers as a label and learn protocol-aware but\nclinically robust image representations. Protocol Genome obtains tokenized\nembeddings of de-identified header fields and models them along with image\nfeatures using: (1) protocol-image contrastive learning, (2) masked protocol\nprediction, and (3) protocol-protocol translation. With 1.26M studies (7 health\nsystems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CT\ntriage for PE, (B) brain MRI glioma grading, and (C) chest radiograph\ncardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as well\nas ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:\ncardiomegaly) is associated with higher external AUROC; 25-37% calibration\nimprovements are obtained (p < 0.01, DeLong tests). While the gains may be\ntask-dependent, they are preserved with 10-20% of labeled data. From a clinical\npoint of view, the technique reduces false positives at protocol borders and is\napplicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish a\nmodel card and deployment guide, complete with both de-identification and bias\naudits.", "AI": {"tldr": "Protocol Genome是一种通过学习DICOM标题来改进影像分析自监督学习系统，提高了临床影像任务的性能和校准，并减少了误报。", "motivation": "本文旨在解决由扫描器型号、序列、核函数等参数引起的潜在混杂因素，这些因素限制了仅基于图像网络的跨站点泛化能力。希望通过引入结构化DICOM标题作为标签来改善这种状况。", "method": "Protocol Genome通过对结构化DICOM标题的嵌入建模来学习协议感知但临床鲁棒的图像表示。技术方法包括协议-图像对比学习、屏蔽协议预测和协议-协议翻译。", "result": "该论文提出了Protocol Genome，一种自监督学习系统，通过学习DICOM标题中的关联性，实现了在完全独立的外部验证上的AUROC 0.901（对比基线0.847）和ECE 0.036（对比基线0.058）。该方法改进了跨模态（CT、MRI、CXR）和供应商的校准和鲁棒性。研究中采用了包含1.26百万项研究的数据集，涵盖7个健康系统、31个扫描仪和3个供应商。实验结果表明，相较于强大的自监督学习基线和ImageNet迁移，Protocol Genome在三种不同的临床影像任务中均显示出更高的外部AUROC、以及显著改善的校准；在协议边界处减少了误报。这项技术适用于PACS，可通过DICOM C-FIND/C-MOVE和DICOMweb QIDO/WADO来部署。该系统还需进行去标识化和偏见审核，并发布了模型卡和部署指南。", "conclusion": "Protocol Genome除了在多种临床影像任务中提供了更高的外部AUROC外，还显著改善了校准性能，并减少了误报。该技术对不同任务的增益保持颇为稳健，即使仅使用少量标注数据。"}}
{"id": "2509.07324", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07324", "abs": "https://arxiv.org/abs/2509.07324", "authors": ["Nakyung Lee", "Yeongoon Kim", "Minhae Oh", "Suhwan Kim", "Jin Woo Koo", "Hyewon Jo", "Jungwoo Lee"], "title": "Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation", "comment": "Accepted at EMNLP 2025", "summary": "Transformer-based self-attention mechanism serves as the core of modern\nlanguage models, yet it often suffers from localization, where attentions\ncollapse onto a limited subset of tokens and fail to capture long-range\ndependencies. To address this issue, we propose Self-Attention One-step Belief\nPropagation (SAOBP), a refinement framework that injects multi-hop\nrelationships through a belief propagation process. To interpret and quantify\nthese interactions, we introduce Global Token Dependency (GTD) that captures\nthe relative contribution of multihop connections within the attention graph.\nEmpirical results indicate that SAOBP helps prevent entropy collapse in deeper\nlayers and adaptively maintains GTD at task-appropriate levels, thereby\nsupporting improvements in model performance. Importantly, we observe\ncompetitive gains in small-scale models, highlighting its potential for\nimproving inference quality in resource-constrained scenarios.", "AI": {"tldr": "SAOBP通过信念传播过程引入多跳连接，解决自注意力机制在现代语言模型中遇到的局部化问题，提高模型性能，尤其是在小型模型和资源受限环境中。", "motivation": "现代语言模型中的基于Transformer的自注意力机制通常会陷入局部化问题，即注意力会在有限的令牌子集上集中，而无法捕捉长距离依赖性。", "method": "我们提出了Self-Attention One-step Belief Propagation (SAOBP)框架，通过信念传播过程来注入多跳关系，并引入全局令牌依赖性（GTD）来解释和量化这些交互。", "result": "实验结果表明，SAOBP有助于防止深层中的熵崩溃，并且能在任务适当的水平上自适应地维持GTD，从而支持模型性能的提升，特别是在小型模型中，这显示了其在资源有限的情况下提高推理质量的潜力。", "conclusion": "SAOBP框架通过增强多跳连接的能力来缓解自注意力机制的局部化问题，并显示出提高模型性能的潜力，特别是在资源受限的情况下。"}}
{"id": "2509.06996", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06996", "abs": "https://arxiv.org/abs/2509.06996", "authors": ["Jie Zhang", "Ting Xu", "Gelei Deng", "Runyi Hu", "Han Qiu", "Tianwei Zhang", "Qing Guo", "Ivor Tsang"], "title": "Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems", "comment": null, "summary": "Writing is a universal cultural technology that reuses vision for symbolic\ncommunication. Humans display striking resilience: we readily recognize words\neven when characters are fragmented, fused, or partially occluded. This paper\ninvestigates whether advanced vision language models (VLMs) share this\nresilience. We construct two psychophysics inspired benchmarks across distinct\nwriting systems, Chinese logographs and English alphabetic words, by splicing,\nrecombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli\nfor models while remaining legible to humans. Despite strong performance on\nclean text, contemporary VLMs show a severe drop under these perturbations,\nfrequently producing unrelated or incoherent outputs. The pattern suggests a\nstructural limitation: models heavily leverage generic visual invariances but\nunder rely on compositional priors needed for robust literacy. We release\nstimuli generation code, prompts, and evaluation protocols to facilitate\ntransparent replication and follow up work. Our findings motivate architectures\nand training strategies that encode symbol segmentation, composition, and\nbinding across scripts, and they delineate concrete challenges for deploying\nmultimodal systems in education, accessibility, cultural heritage, and\nsecurity.", "AI": {"tldr": "该研究表明，尽管先进视觉语言模型（VLMs）在正常条件下表现良好，但在处理经过特定干扰的文字时表现显著下降，由此指出模型在结构化先验方面的不足，并提出了改进模型鲁棒性的建议。", "motivation": "作者希望探索先进视觉语言模型（VLMs）在识别经过特定处理（例如拼接、重组、叠加）而变得难以识别的文字时的鲁棒性，从而揭示现有模型的局限性，并促进更鲁棒可靠的VLMs的发展。", "method": "作者设计了两种心理学启发的基准测试，涵盖了中文字符和英语单词，通过拼接、重组、叠加等手段制造“可见但难以阅读”的视觉刺激。", "result": "该论文探讨了先进视觉语言模型（VLMs）在识别经过人为干扰的文字时的鲁棒性。通过两类不同的文字系统（中文字符和英文单词）创建了两个心理学启发的基准数据集，其中文字通过拼接、重组或叠加等方式使得机器难于识别，但人类依然可以轻易辨认。尽管在没有干扰的文字识别上表现良好，但面对这些干扰，当前VLMs表现显著下降，经常产生无关或不连贯的输出。这表明现有模型过于依赖通用的视觉不变性，而忽略了组成文字的结构化先验。作者提供了生成刺激、提示和评估协议的代码，以推动透明的复制和后续研究。研究结果表明，需要开发能更好地捕捉符号划分、组合和绑定的架构与训练策略，以应对在教育、无障碍、文化遗产和安全领域内部署多模态系统时的具体挑战。", "conclusion": "研究表明现有先进的VLMs在面对人为干扰的文字时表现出不稳定且不准确的预测，强调了在模型设计中更全面地考虑文字组成和结构化先验的重要性。"}}
{"id": "2509.07370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07370", "abs": "https://arxiv.org/abs/2509.07370", "authors": ["Yixuan Tang", "Yi Yang", "Ahmed Abbasi"], "title": "PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) demonstrate remarkable\ncapabilities across various fields. These developments have led to more direct\ncommunication between humans and LLMs in various situations, such as social\ncompanionship and psychological support. However, LLMs often exhibit\nlimitations in emotional perception and social competence during real-world\nconversations. These limitations partly originate from their inability to adapt\ntheir communication style and emotional expression to different social and task\ncontexts. In this work, we introduce PersonaFuse, a novel LLM post-training\nframework that enables LLMs to adapt and express different personalities for\nvarying situations. Inspired by Trait Activation Theory and the Big Five\npersonality model, PersonaFuse employs a Mixture-of-Expert architecture that\ncombines persona adapters with a dynamic routing network, enabling contextual\ntrait expression. Experimental results show that PersonaFuse substantially\noutperforms baseline models across multiple dimensions of social-emotional\nintelligence. Importantly, these gains are achieved without sacrificing general\nreasoning ability or model safety, which remain common limitations of direct\nprompting and supervised fine-tuning approaches. PersonaFuse also delivers\nconsistent improvements in downstream human-centered applications, such as\nmental health counseling and review-based customer service. Finally, human\npreference evaluations against leading LLMs, including GPT-4o and DeepSeek,\ndemonstrate that PersonaFuse achieves competitive response quality despite its\ncomparatively smaller model size. These findings demonstrate that\nPersonaFuse~offers a theoretically grounded and practical approach for\ndeveloping social-emotional enhanced LLMs, marking a significant advancement\ntoward more human-centric AI systems.", "AI": {"tldr": "PersonaFuse是一种新的大规模语言模型后训练框架，通过结合人格适配器和动态路由网络，使模型能够适应并表达不同的人格特性，从而提高其在社交和情感智能方面的表现。实验结果表明，PersonaFuse在多方面表现优于基线模型，并在下游应用中显示出一致的改善，同时保持了良好的通用推理和模型安全性。", "motivation": "由于大规模语言模型在实际对话中存在情感感知和社会能力上的局限性，研究团队提出了一种新的框架，旨在增强语言模型的社会和情感智能，以提高其与人类的交流质量。", "method": "PersonaFuse采用了专家混合的架构，结合了人格适配器和动态路由网络，根据情境理论进行个性化的特质表达。该方法基于特质激活理论和大五人格模型来设计适配器，以适应不同的社会和任务环境。", "result": "实验结果表明PersonaFuse在社交和情感智能的多个维度上显著优于基线模型，并在如心理健康咨询和基于评论的客户服务等下游应用中显示出一致性改善，同时不会损害模型的安全性。", "conclusion": "PersonaFuse为开发社会情感增强的语言模型提供了一种理论基础强且实用的方法，它标志着向着更人性化的AI系统迈进的重要一步。"}}
{"id": "2509.06997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06997", "abs": "https://arxiv.org/abs/2509.06997", "authors": ["Guan Yu", "Zhang Jianhua", "Liang Dong", "Liu Qiegen"], "title": "K-Syn: K-space Data Synthesis in Ultra Low-data Regimes", "comment": null, "summary": "Owing to the inherently dynamic and complex characteristics of cardiac\nmagnetic resonance (CMR) imaging, high-quality and diverse k-space data are\nrarely available in practice, which in turn hampers robust reconstruction of\ndynamic cardiac MRI. To address this challenge, we perform feature-level\nlearning directly in the frequency domain and employ a temporal-fusion strategy\nas the generative guidance to synthesize k-space data. Specifically, leveraging\nthe global representation capacity of the Fourier transform, the frequency\ndomain can be considered a natural global feature space. Therefore, unlike\ntraditional methods that use pixel-level convolution for feature learning and\nmodeling in the image domain, this letter focuses on feature-level modeling in\nthe frequency domain, enabling stable and rich generation even with ultra\nlow-data regimes. Moreover, leveraging the advantages of feature-level modeling\nin the frequency domain, we integrate k-space data across time frames with\nmultiple fusion strategies to steer and further optimize the generative\ntrajectory. Experimental results demonstrate that the proposed method possesses\nstrong generative ability in low-data regimes, indicating practical potential\nto alleviate data scarcity in dynamic MRI reconstruction.", "AI": {"tldr": "本文采用频率域特征级学习和时间融合策略来生成心脏磁共振成像的k空间数据，在低数据量条件下展现了优秀的性能。", "motivation": "由于心脏磁共振成像的动态性和复杂性，高质且多样的k空间数据在实践中难以获得。这限制了动态心脏MRI的重建。为此，本研究提出了一个基于频率域特征级学习的方法以解决数据稀少问题。", "method": "本研究通过频率域的特征级学习来解决心脏磁共振成像数据稀少和质量波动的问题，并采用时间融合策略作为生成指导来合成k空间数据。该方法利用傅里叶变换的全局表示能力，在频率域进行特征级建模，以生成稳定且丰富的数据，即使在极低数据量的情况下也能如此。此外，该方法还融合了多个时间帧的k空间数据，优化了生成路径。", "result": "实验结果表明，该方法在低数据量的情况下具有强大的生成能力，显示出在动态MRI重建中解决数据稀缺问题的实际潜力。", "conclusion": "通过频率域的特征级建模和多个时间帧下的k空间数据融合，本研究展示了一个有效的方法来解决动态心脏MRI重建中的数据稀疏问题。"}}
{"id": "2509.07389", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07389", "abs": "https://arxiv.org/abs/2509.07389", "authors": ["Sankalp Tattwadarshi Swain", "Anshika Krishnatray", "Dhruv Kumar", "Jagat Sesh Challa"], "title": "Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents", "comment": "Under review", "summary": "Existing evaluation studies on linguistic competence of large language models\n(LLM agents) have focused primarily on vocabulary learning, morphological rule\ninduction, syntactic generalization, pragmatic inference, and cross-linguistic\ntransfer. However, none assess whether LLM agents can acquire a language\nthrough pattern recognition and interactive feedback, a central feature of\nhuman language acquisition. We propose a novel experimental framework in which\nan LLM agent is evaluated on its ability to acquire and use a newly constructed\nlanguage (Tinkatongue) in conversation with a bot that understands only\nTinkatongue. Our findings show that LLM agents fail to establish a conversation\nwithin 100 responses, yet they adopt distinct strategies that mirror human\napproaches to language learning. The results suggest a new direction for\nevaluation benchmarks and open pathways to model designs that learn more\neffectively from interactive feedback.", "AI": {"tldr": "本研究提出一个实验框架，评估大型语言模型通过模式识别和互动反馈掌握新语言的能力。发现模型虽未能成功建立对话，但学习方式类同于人类，衬托出互动反馈学习对于语言模型的重要作用。", "motivation": "现有的语言模型评估主要集中在其词汇学习、形态规则归纳、句法推广、语用推理和跨语言迁移上的表现，缺乏针对模型能否通过模式识别和互动反馈掌握新语言的研究。因此，提出这个新的实验框架，评估现有的LLM能否像人一样通过互动学习语言。", "method": "研究者创造了一个新的语言（Tinkatongue）作为评估对象，让大型语言模型（LLM代理）与只能理解这种新语言的机器人互动，以此来测试LLM代理获取并运用新语言的能力。", "result": "本研究提出了一种新的实验框架，用于评估大型语言模型（LLM代理）是否能通过模式识别和互动反馈来掌握一门语言。实验展示了一个LLM代理与仅能理解新构建语言（Tinkatongue）的机器人进行对话的能力。研究发现，尽管LLM代理在100次回应后未能建立有效对话，但它们采用了与人类语言学习相似的策略。结果暗示了评估基准的新方向，并为从互动反馈中更有效地学习的模型设计提供了路径。", "conclusion": "实验结果表明，LLM代理在直接通过互动学习并使用新语言方面存在挑战，但其学习方法反映了人类学习语言的过程。这为未来研究提供了方向，并提示了开发更有效互动学习模型的可能路径。"}}
{"id": "2509.06998", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06998", "abs": "https://arxiv.org/abs/2509.06998", "authors": ["Liviu Nicolae Fircă", "Antonio Bărbălau", "Dan Oneata", "Elena Burceanu"], "title": "Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories", "comment": null, "summary": "Can models generalize attribute knowledge across semantically and\nperceptually dissimilar categories? While prior work has addressed attribute\nprediction within narrow taxonomic or visually similar domains, it remains\nunclear whether current models can abstract attributes and apply them to\nconceptually distant categories. This work presents the first explicit\nevaluation for the robustness of the attribute prediction task under such\nconditions, testing whether models can correctly infer shared attributes\nbetween unrelated object types: e.g., identifying that the attribute \"has four\nlegs\" is common to both \"dogs\" and \"chairs\". To enable this evaluation, we\nintroduce train-test split strategies that progressively reduce correlation\nbetween training and test sets, based on: LLM-driven semantic grouping,\nembedding similarity thresholding, embedding-based clustering, and\nsupercategory-based partitioning using ground-truth labels. Results show a\nsharp drop in performance as the correlation between training and test\ncategories decreases, indicating strong sensitivity to split design. Among the\nevaluated methods, clustering yields the most effective trade-off, reducing\nhidden correlations while preserving learnability. These findings offer new\ninsights into the limitations of current representations and inform future\nbenchmark construction for attribute reasoning.", "AI": {"tldr": "本研究评估模型在不同类别间的属性预测能力，并提出几种训练-测试拆分策略。结果表明模型对类别相关性变化敏感，聚类方法效果最佳。", "motivation": "研究动机在于探索模型是否能在语义和感知不同的类别之间推广属性知识。尽管先前的工作已解决了狭窄的分类或视觉相似域内的属性预测问题，但尚不清楚当前模型能否将属性抽象化并应用到概念上距离较远的类别。", "method": "本研究提出了几种训练-测试拆分策略，以逐步减少训练集与测试集之间的相关性。这些策略包括：由LLM驱动的语义分组、嵌入相似性阈值、基于嵌入的聚类以及基于超类别的分区采用真实标签。", "result": "结果表明，随着训练集与测试集之间的类别相关性降低，模型性能显著下降，显示出对拆分设计的高度敏感性。在评估的方法中，聚类提供了最佳的折中方案，它减少了隐藏的相关性同时保留了可学习性。", "conclusion": "这些发现为当前表示方法的局限性提供了新的洞见，并为未来的属性推理基准构建提供了指导。"}}
{"id": "2509.07399", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07399", "abs": "https://arxiv.org/abs/2509.07399", "authors": ["Yi-Jie Cheng", "Oscar Chew", "Yun-Nung Chen"], "title": "The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering", "comment": "Extended from ACL 2025 SRW", "summary": "Integrating knowledge graphs (KGs) into the reasoning processes of large\nlanguage models (LLMs) has emerged as a promising approach to mitigate\nhallucination. However, existing work in this area often relies on proprietary\nor extremely large models, limiting accessibility and scalability. In this\nstudy, we investigate the capabilities of existing integration methods for\nsmall language models (SLMs) in KG-based question answering and observe that\ntheir performance is often constrained by their limited ability to traverse and\nreason over knowledge graphs. To address this limitation, we propose leveraging\nsimple and efficient exploration modules to handle knowledge graph traversal in\nplace of the language model itself. Experiment results demonstrate that these\nlightweight modules effectively improve the performance of small language\nmodels on knowledge graph question answering tasks. Source code:\nhttps://github.com/yijie-cheng/SLM-ToG/.", "AI": {"tldr": "研究探讨了小型语言模型在知识图谱问题回答中的能力，并提出使用轻量级探索模块来提升性能。", "motivation": "现有集成知识图谱的方法主要依赖于大型且专有模型，这限制了其可访问性和可扩展性。这项研究旨在探索对于小型语言模型（SLMs）来说，如何有效利用知识图谱进行问题回答。", "method": "研究提出采用简单且高效的探索模块来辅助小型语言模型进行知识图谱的遍历和推理，而不是让语言模型自身执行这些任务。", "result": "实验表明，轻量级探索模块能够有效提升小型语言模型在基于知识图谱的问题回答任务上的性能。", "conclusion": "研究表明，通过引入轻量级探索模块，能够克服小型语言模型在知识图谱推理上的性能限制，显著提升其在问题回答任务中的表现。"}}
{"id": "2509.07010", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.07010", "abs": "https://arxiv.org/abs/2509.07010", "authors": ["Ahmed R. Sadik", "Mariusz Bujny"], "title": "Human-in-the-Loop: Quantitative Evaluation of 3D Models Generation by Large Language Models", "comment": null, "summary": "Large Language Models are increasingly capable of interpreting multimodal\ninputs to generate complex 3D shapes, yet robust methods to evaluate geometric\nand structural fidelity remain underdeveloped. This paper introduces a human in\nthe loop framework for the quantitative evaluation of LLM generated 3D models,\nsupporting applications such as democratization of CAD design, reverse\nengineering of legacy designs, and rapid prototyping. We propose a\ncomprehensive suite of similarity and complexity metrics, including volumetric\naccuracy, surface alignment, dimensional fidelity, and topological intricacy,\nto benchmark generated models against ground truth CAD references. Using an L\nbracket component as a case study, we systematically compare LLM performance\nacross four input modalities: 2D orthographic views, isometric sketches,\ngeometric structure trees, and code based correction prompts. Our findings\ndemonstrate improved generation fidelity with increased semantic richness, with\ncode level prompts achieving perfect reconstruction across all metrics. A key\ncontribution of this work is demonstrating that our proposed quantitative\nevaluation approach enables significantly faster convergence toward the ground\ntruth, especially compared to traditional qualitative methods based solely on\nvisual inspection and human intuition. This work not only advances the\nunderstanding of AI assisted shape synthesis but also provides a scalable\nmethodology to validate and refine generative models for diverse CAD\napplications.", "AI": {"tldr": "The paper presents a novel human-in-the-loop evaluation framework for 3D models generated by Large Language Models, highlighting the importance of quantitative metrics over qualitative visual inspection.", "motivation": "The motivation of the paper is to develop robust methods to evaluate the geometric and structural fidelity of 3D models generated by Large Language Models, overcoming the limitations of current evaluation techniques.", "method": "This paper introduces a human-in-the-loop framework for quantitative evaluation of 3D models generated by Large Language Models, using a suite of metrics for volumetric accuracy, surface alignment, dimensional fidelity, and topological intricacy.", "result": "Results show improved generation fidelity with more semantically-rich inputs, with code-level prompts achieving perfect reconstruction across all metrics in the case study of an L-bracket component.", "conclusion": "The study concludes that an LLM can achieve high-fidelity generation, especially with code-based prompts, and demonstrates that their evaluation approach enables faster convergence to ground truth than traditional methods."}}
{"id": "2509.07403", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07403", "abs": "https://arxiv.org/abs/2509.07403", "authors": ["Weichu Liu", "Jing Xiong", "Yuxuan Hu", "Zixuan Li", "Minghuan Tan", "Ningning Mao", "Chenyang Zhao", "Zhongwei Wan", "Chaofan Tao", "Wendong Xu", "Hui Shen", "Chengming Li", "Lingpeng Kong", "Ngai Wong"], "title": "LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction", "comment": "Technical Report", "summary": "Large language models (LLMs) make significant progress in Emotional\nIntelligence (EI) and long-context understanding. However, existing benchmarks\ntend to overlook certain aspects of EI in long-context scenarios, especially\nunder realistic, practical settings where interactions are lengthy, diverse,\nand often noisy. To move towards such realistic settings, we present\nLongEmotion, a benchmark specifically designed for long-context EI tasks. It\ncovers a diverse set of tasks, including Emotion Classification, Emotion\nDetection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion\nExpression. On average, the input length for these tasks reaches 8,777 tokens,\nwith long-form generation required for Emotion Expression. To enhance\nperformance under realistic constraints, we incorporate Retrieval-Augmented\nGeneration (RAG) and Collaborative Emotional Modeling (CoEM), and compare them\nwith standard prompt-based methods. Unlike conventional approaches, our RAG\nmethod leverages both the conversation context and the large language model\nitself as retrieval sources, avoiding reliance on external knowledge bases. The\nCoEM method further improves performance by decomposing the task into five\nstages, integrating both retrieval augmentation and limited knowledge\ninjection. Experimental results show that both RAG and CoEM consistently\nenhance EI-related performance across most long-context tasks, advancing LLMs\ntoward more practical and real-world EI applications. Furthermore, we conducted\na comparative case study experiment on the GPT series to demonstrate the\ndifferences among various models in terms of EI. Code is available on GitHub at\nhttps://github.com/LongEmotion/LongEmotion, and the project page can be found\nat https://longemotion.github.io/.", "AI": {"tldr": "研究设计了一个名为LongEmotion的长上下文情感智能基准，并通过RAG和CoEM方法在情感分类、检测、问答、对话、总结和表达任务中展示了优于标准提示方法的性能，助力大型语言模型更好地应用于实际情感智能任务。", "motivation": "现有的基准在长上下文情感智能评估中有些不足，尤其是在真实环境的应用中。因此，研究旨在设计一个更贴近真实应用场景的评估基准。", "method": "研究使用了长上下文情感智能基准（LongEmotion）来评估情感分类、检测、问答、对话、总结和表达等任务，并采用了检索增强生成（RAG）和合作情感建模（CoEM）方法来提高真实场景下的性能。其中RAG方法利用对话上下文和大型语言模型自身作为检索源，而CoEM方法进一步分为五个阶段。", "result": "实验结果表明，RAG和CoEM两种方法在大多数长上下文情感任务上都能有效提升大型语言模型的情感智能性能。同时，研究表明在GPT系列模型中，不同模型在情感智能任务上的表现存在显著差异。", "conclusion": "研究提出了LongEmotion基准，用于评估长上下文情感任务的模型性能，并展示了RAG和CoEM方法在这些任务上的改进效果，推动了大型语言模型在实用情感智能应用中的进展。"}}
{"id": "2509.07021", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07021", "abs": "https://arxiv.org/abs/2509.07021", "authors": ["Jiarui Chen", "Yikeng Chen", "Yingshuang Zou", "Ye Huang", "Peng Wang", "Yuan Liu", "Yujing Sun", "Wenping Wang"], "title": "MEGS$^{2}$: Memory-Efficient Gaussian Splatting via Spherical Gaussians and Unified Pruning", "comment": "14 pages, 4 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a dominant novel-view synthesis\ntechnique, but its high memory consumption severely limits its applicability on\nedge devices. A growing number of 3DGS compression methods have been proposed\nto make 3DGS more efficient, yet most only focus on storage compression and\nfail to address the critical bottleneck of rendering memory. To address this\nproblem, we introduce MEGS$^{2}$, a novel memory-efficient framework that\ntackles this challenge by jointly optimizing two key factors: the total\nprimitive number and the parameters per primitive, achieving unprecedented\nmemory compression. Specifically, we replace the memory-intensive spherical\nharmonics with lightweight arbitrarily-oriented spherical Gaussian lobes as our\ncolor representations. More importantly, we propose a unified soft pruning\nframework that models primitive-number and lobe-number pruning as a single\nconstrained optimization problem. Experiments show that MEGS$^{2}$ achieves a\n50% static VRAM reduction and a 40% rendering VRAM reduction compared to\nexisting methods, while maintaining comparable rendering quality.", "AI": {"tldr": "本文提出了MEGS$^{2}$框架，通过优化两个关键因素实现了显著的内存压缩，并且大大减少了VRAM的使用，同时保证了渲染质量。", "motivation": "3D高斯散度技术在新兴视角合成技术上表现出色，但其高内存消耗限制了其在边缘设备上的应用。作者试图通过提出一种内存高效的方法，解决渲染内存这一关键瓶颈。", "method": "我们介绍了MEGS$^{2}$，一个内存高效的框架，该框架通过优化两个关键因素：总的原始数量和每个原始的参数，实现了前所未有的内存压缩。具体来说，我们用轻量级的任意定向球形高斯散度代替了内存密集的球谐函数作为颜色表示。更重要的是，我们提出了一种统一的软修剪框架，将原始数量和散度数量的修剪建模为一个单一的约束优化问题。", "result": "实验表明，与现有方法相比，MEGS$^{2}$实现了50%的静态VRAM减少和40%的渲染VRAM减少，同时保持了可比较的渲染质量。", "conclusion": "本文提出的新方法实现了显著的内存压缩，减少了VRAM的使用，同时确保了渲染质量不打折扣。这证明了MEGS$^{2}$在增强3DGS在边缘设备上的实用性方面的潜力。"}}
{"id": "2509.07459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07459", "abs": "https://arxiv.org/abs/2509.07459", "authors": ["Christian Rene Thelen", "Patrick Gustav Blaneck", "Tobias Bornheim", "Niklas Grieger", "Stephan Bialonski"], "title": "AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training", "comment": "6 pages, 1 figure, 2 tables", "summary": "Positive, supportive online communication in social media (candy speech) has\nthe potential to foster civility, yet automated detection of such language\nremains underexplored, limiting systematic analysis of its impact. We\ninvestigate how candy speech can be reliably detected in a 46k-comment German\nYouTube corpus by monolingual and multilingual language models, including\nGBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual\nXLM-RoBERTa-Large model trained to detect candy speech at the span level\noutperforms other approaches, ranking first in both binary positive F1: 0.8906)\nand categorized span-based detection (strict F1: 0.6307) subtasks at the\nGermEval 2025 Shared Task on Candy Speech Detection. We speculate that\nspan-based training, multilingual capabilities, and emoji-aware tokenizers\nimproved detection performance. Our results demonstrate the effectiveness of\nmultilingual models in identifying positive, supportive language.", "AI": {"tldr": "本研究调查了使用多语言和单语言语言模型在德语YouTube评论中检测积极语言（糖果言论）的方法，发现多语言的XLM-RoBERTa-Large表现最佳，展示了多语言模型在识别积极支持语言方面的有效性。", "motivation": "尽管积极的、支持性的在线交流（糖果言论）有可能培养文明，但自动检测此类语言仍鲜为人知，这限制了对其影响的系统性分析。", "method": "我们研究了如何在包含46k条评论的德语YouTube语料库中，通过单语和多语言语言模型（包括GBERT、Qwen3嵌入和XLM-RoBERTa）来可靠地检测糖果言论。", "result": "研究结果表明，用于检测糖果言论的多语言XLM-RoBERTa-Large模型在GermEval 2025共享任务中的二元正F1（0.8906，严格F1为0.6307）方面优于其他方法。", "conclusion": "我们的研究结果表明，多语言模型在识别积极、支持性语言方面是有效的。"}}
{"id": "2509.07027", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07027", "abs": "https://arxiv.org/abs/2509.07027", "authors": ["Jisung Hwang", "Jaihoon Kim", "Minhyuk Sung"], "title": "Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models", "comment": "Submitted to NeurIPS 2025", "summary": "We propose a novel regularization loss that enforces standard Gaussianity,\nencouraging samples to align with a standard Gaussian distribution. This\nfacilitates a range of downstream tasks involving optimization in the latent\nspace of text-to-image models. We treat elements of a high-dimensional sample\nas one-dimensional standard Gaussian variables and define a composite loss that\ncombines moment-based regularization in the spatial domain with power\nspectrum-based regularization in the spectral domain. Since the expected values\nof moments and power spectrum distributions are analytically known, the loss\npromotes conformity to these properties. To ensure permutation invariance, the\nlosses are applied to randomly permuted inputs. Notably, existing\nGaussianity-based regularizations fall within our unified framework: some\ncorrespond to moment losses of specific orders, while the previous\ncovariance-matching loss is equivalent to our spectral loss but incurs higher\ntime complexity due to its spatial-domain computation. We showcase the\napplication of our regularization in generative modeling for test-time reward\nalignment with a text-to-image model, specifically to enhance aesthetics and\ntext alignment. Our regularization outperforms previous Gaussianity\nregularization, effectively prevents reward hacking and accelerates\nconvergence.", "AI": {"tldr": "A novel regularization loss that promotes standard Gaussianity in high-dimensional samples for optimization in text-to-image models, surpassing existing methods in efficiency and effectiveness.", "motivation": "The motivation is to facilitate a range of downstream tasks involving optimization in the latent space of text-to-image models and to ensure permutation invariance by applying losses to randomly permuted inputs.", "method": "We propose a novel regularization loss that enforces standard Gaussianity, encouraging samples to align with a standard Gaussian distribution. The loss combines moment-based regularization in the spatial domain with power spectrum-based regularization in the spectral domain.", "result": "The regularization outperforms previous Gaussianity regularization, effectively prevents reward hacking, and accelerates convergence.", "conclusion": "This regularization framework is effective for enhancing the aesthetics and text alignment in generative modeling for text-to-image models, demonstrating superiority over existing methods by reducing time complexity and improving performance."}}
{"id": "2509.07462", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07462", "abs": "https://arxiv.org/abs/2509.07462", "authors": ["Yiliang Zhou", "Di Hu", "Tianchu Lyu", "Jasmine Dhillon", "Alexandra L. Beck", "Gelareh Sadigh", "Kai Zheng"], "title": "Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts", "comment": null, "summary": "Stigmatizing language results in healthcare inequities, yet there is no\nuniversally accepted or standardized lexicon defining which words, terms, or\nphrases constitute stigmatizing language in healthcare. We conducted a\nsystematic search of the literature to identify existing stigmatizing language\nlexicons and then analyzed them comparatively to examine: 1) similarities and\ndiscrepancies between these lexicons, and 2) the distribution of positive,\nnegative, or neutral terms based on an established sentiment dataset. Our\nsearch identified four lexicons. The analysis results revealed moderate\nsemantic similarity among them, and that most stigmatizing terms are related to\njudgmental expressions by clinicians to describe perceived negative behaviors.\nSentiment analysis showed a predominant proportion of negatively classified\nterms, though variations exist across lexicons. Our findings underscore the\nneed for a standardized lexicon and highlight challenges in defining\nstigmatizing language in clinical texts.", "AI": {"tldr": "本研究系统地检索和比较了四种现有的医疗领域污名化语言词汇表，发现这些词汇表之间有中等程度的语义相似性，而多数污名化术语涉及医生的判断性表达。结果强调了建立标准化污名化语言词汇表的必要性。", "motivation": "污名化语言导致医疗不公，然而目前尚无普遍接受或标准化的词汇表来定义医疗领域的污名化语言，因此本研究旨在识别现有污名化语言词汇表并进行比较分析。", "method": "本研究通过系统的文献检索来识别存在的污名化语言词汇表，并对这些词汇表进行了比较分析，考察它们之间的相似性和差异性，以及基于现有情感数据集的情感分类分布。", "result": "研究发现共识别出四个词汇表，并分析结果显示它们之间存在中等程度的语义相似性。大多数污名化术语与临床医生用来描述感知到的负面行为的判断性表达有关。情感分析显示大部分词汇被归类为负面情绪的词汇，不过不同词汇表间存在差异。", "conclusion": "研究结果强调需要建立一个标准化的污名化语言词汇表，并强调了在临床文本中识别污名化语言的挑战。"}}
{"id": "2509.07047", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07047", "abs": "https://arxiv.org/abs/2509.07047", "authors": ["Kamyar Barakati", "Utkarsh Pratiush", "Sheryl L. Sanchez", "Aditya Raghavan", "Delia J. Milliron", "Mahshid Ahmadi", "Philip D. Rack", "Sergei V. Kalinin"], "title": "SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards", "comment": "19 pages, 8 figures", "summary": "Image segmentation is a critical task in microscopy, essential for accurately\nanalyzing and interpreting complex visual data. This task can be performed\nusing custom models trained on domain-specific datasets, transfer learning from\npre-trained models, or foundational models that offer broad applicability.\nHowever, foundational models often present a considerable number of\nnon-transparent tuning parameters that require extensive manual optimization,\nlimiting their usability for real-time streaming data analysis. Here, we\nintroduce a reward function-based optimization to fine-tune foundational models\nand illustrate this approach for SAM (Segment Anything Model) framework by\nMeta. The reward functions can be constructed to represent the physics of the\nimaged system, including particle size distributions, geometries, and other\ncriteria. By integrating a reward-driven optimization framework, we enhance\nSAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$,\nthat better aligns with the requirements of diverse segmentation tasks and\nparticularly allows for real-time streaming data segmentation. We demonstrate\nthe effectiveness of this approach in microscopy imaging, where precise\nsegmentation is crucial for analyzing cellular structures, material interfaces,\nand nanoscale features.", "AI": {"tldr": "本文提出了一种改善基础模型，特别是SAM框架的适应性和性能的新方法，以更好地满足不同分割任务的要求。", "motivation": "本文旨在解决基础模型在实际应用中参数不透明和需要大量手动调整的问题，特别是在实时流数据分析中的应用限制。", "method": "本文提出了一种基于奖励函数的优化方法来微调基础模型，并以Meta的SAM（Segment Anything Model）框架为例说明这一方法。通过构建反映成像系统物理属性的奖励函数来优化模型。", "result": "采用奖励驱动的优化框架，文章提升了SAM的适应性和性能，从而得到了一个更适合各种分割任务优化版本SAM$^{*}$，尤其对于实时流数据的分割具有重要意义。", "conclusion": "文章通过引入基于奖励函数的优化方法，证明了其在显微成像中的有效性，特别是在精确的细胞结构、材料界面和纳米尺度特征分析方面。"}}
{"id": "2509.07471", "categories": ["cs.CL", "68T50", "I.7"], "pdf": "https://arxiv.org/pdf/2509.07471", "abs": "https://arxiv.org/abs/2509.07471", "authors": ["Mardiyyah Oduwole", "Oluwatosin Olajide", "Jamiu Suleiman", "Faith Hunja", "Busayo Awobade", "Fatimo Adebanjo", "Comfort Akanni", "Chinonyelum Igwe", "Peace Ododo", "Promise Omoigui", "Steven Kolawole", "Abraham Owodunni"], "title": "From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation", "comment": "8 pages, 3 tables. Exploratory work on Data Augmentation for African\n  Machine Translation", "summary": "The linguistic diversity across the African continent presents different\nchallenges and opportunities for machine translation. This study explores the\neffects of data augmentation techniques in improving translation systems in\nlow-resource African languages. We focus on two data augmentation techniques:\nsentence concatenation with back translation and switch-out, applying them\nacross six African languages. Our experiments show significant improvements in\nmachine translation performance, with a minimum increase of 25\\% in BLEU score\nacross all six languages.We provide a comprehensive analysis and highlight the\npotential of these techniques to improve machine translation systems for\nlow-resource languages, contributing to the development of more robust\ntranslation systems for under-resourced languages.", "AI": {"tldr": "本研究采用数据增强技术改善六种非洲低资源语言的机器翻译系统，显著提高了这些系统的性能。", "motivation": "研究目的是探索数据增强技术在非洲大陆语言多样性下，改善机器翻译系统的挑战与机遇。", "method": "本研究探讨了数据增强技术在改善非洲低资源语言的机器翻译系统中的效果，特别关注了句子拼接加回译和切换两种技术。", "result": "实验表明，机器翻译性能显著提升，六种语言的BLEU分数至少提高了25%。", "conclusion": "研究结果揭示了这些技术提高低资源语言机器翻译系统性能的潜力，有助于开发针对资源匮乏语言的更强大翻译系统。"}}
{"id": "2509.07049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07049", "abs": "https://arxiv.org/abs/2509.07049", "authors": ["Rwad Khatib", "Yehudit Aperstein"], "title": "Enhancing Classification of Streaming Data with Image Distillation", "comment": "11 pages", "summary": "This study tackles the challenge of efficiently classifying streaming data in\nenvi-ronments with limited memory and computational resources. It delves into\nthe application of data distillation as an innovative approach to improve the\nprecision of streaming image data classification. By focusing on distilling\nessential features from data streams, our method aims to minimize computational\ndemands while preserving crucial information for accurate classification. Our\ninvestigation com-pares this approach against traditional algorithms like\nHoeffding Trees and Adap-tive Random Forest, adapted through embeddings for\nimage data. The Distillation Based Classification (DBC) demonstrated superior\nperformance, achieving a 73.1% accuracy rate, surpassing both traditional\nmethods and Reservoir Sam-pling Based Classification (RBC) technique. This\nmarks a significant advance-ment in streaming data classification, showcasing\nthe effectiveness of our method in processing complex data streams and setting\na new standard for accuracy and efficiency.", "AI": {"tldr": "The study uses a Distillation Based Classification (DBC) method to improve the precision of streaming image data classification, achieving better accuracy than traditional methods with limited resources.", "motivation": "To efficiently classify streaming data with limited memory and computational resources using an innovative approach.", "method": "Distillation Based Classification (DBC) method that focuses on distilling essential features from data streams to classify streaming image data.", "result": "DBC achieved a 73.1% accuracy rate, surpassing traditional methods such as Hoeffding Trees and Adaptive Random Forest, and the Reservoir Sampling Based Classification (RBC) technique.", "conclusion": "The study demonstrates significant advancements in streaming data classification, highlighting the effectiveness of the DBC method in both accuracy and efficiency."}}
{"id": "2509.07475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07475", "abs": "https://arxiv.org/abs/2509.07475", "authors": ["Saumya Goswami", "Siddharth Kurra"], "title": "HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention", "comment": null, "summary": "Detecting content that contradicts or is unsupported by a given source text\nis a critical challenge for the safe deployment of generative language models.\nWe introduce HALT-RAG, a post-hoc verification system designed to identify\nhallucinations in the outputs of Retrieval-Augmented Generation (RAG)\npipelines. Our flexible and task-adaptable framework uses a universal feature\nset derived from an ensemble of two frozen, off-the-shelf Natural Language\nInference (NLI) models and lightweight lexical signals. These features are used\nto train a simple, calibrated, and task-adapted meta-classifier. Using a\nrigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and\nproduce unbiased estimates, we evaluate our system on the HaluEval benchmark.\nBy pairing our universal feature set with a lightweight, task-adapted\nclassifier and a precision-constrained decision policy, HALT-RAG achieves\nstrong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA,\nand dialogue tasks, respectively. The system's well-calibrated probabilities\nenable a practical abstention mechanism, providing a reliable tool for\nbalancing model performance with safety requirements.", "AI": {"tldr": "研究团队推出了HALT-RAG，这是一种用于检测RAG生成输出中幻觉现象的系统，通过使用NLI模型集合和轻量级词汇信号，并经过严格的训练和验证，系统在多个任务上均展现出了良好的性能，并具备灵活调整以满足安全要求的能力。", "motivation": "随着生成语言模型的安全部署变得至关重要，检测生成内容是否与给定源头文本相矛盾或没有支持成为了一个严重的挑战。HALT-RAG被设计用于解决这一问题，以确保生成内容的可靠性和安全性。", "method": "我们的研究引入了HALT-RAG，一种用于检测基于检索增强生成（RAG）管道输出中的幻觉（hallucinations）的后处理验证系统。该系统采用两种预先训练好的自然语言推理（NLI）模型的集合以及轻量级的词汇信号来创建一个通用特征集，并利用这些特征训练一个简单的、可校准的、针对特定任务的元分类器。", "result": "通过严谨的5折交叉验证协议，我们评估了HALT-RAG在HaluEval基准上的表现，该系统在摘要、问答和对话任务上分别实现了0.7756、0.9786和0.7391的较强的OOF F1得分。系统的良好校准概率使得可以实现实践中的弃权机制，保证达到模型性能与安全性要求之间的平衡。", "conclusion": "HALT-RAG通过采用一组通用特征以及特定任务的分类器，并采取了精密约束决策策略，实现了对幻觉内容的有效检测，其良好的校准概率为平衡模型性能和安全需求提供了一种可靠的工具。"}}
{"id": "2509.07050", "categories": ["cs.CV", "cs.AI", "cs.CY", "I.2.7; F.2.2"], "pdf": "https://arxiv.org/pdf/2509.07050", "abs": "https://arxiv.org/abs/2509.07050", "authors": ["Juan Manuel Contreras"], "title": "Automated Evaluation of Gender Bias Across 13 Large Multimodal Models", "comment": null, "summary": "Large multimodal models (LMMs) have revolutionized text-to-image generation,\nbut they risk perpetuating the harmful social biases in their training data.\nPrior work has identified gender bias in these models, but methodological\nlimitations prevented large-scale, comparable, cross-model analysis. To address\nthis gap, we introduce the Aymara Image Fairness Evaluation, a benchmark for\nassessing social bias in AI-generated images. We test 13 commercially available\nLMMs using 75 procedurally-generated, gender-neutral prompts to generate people\nin stereotypically-male, stereotypically-female, and non-stereotypical\nprofessions. We then use a validated LLM-as-a-judge system to score the 965\nresulting images for gender representation. Our results reveal (p < .001 for\nall): 1) LMMs systematically not only reproduce but actually amplify\noccupational gender stereotypes relative to real-world labor data, generating\nmen in 93.0% of images for male-stereotyped professions but only 22.5% for\nfemale-stereotyped professions; 2) Models exhibit a strong default-male bias,\ngenerating men in 68.3% of the time for non-stereotyped professions; and 3) The\nextent of bias varies dramatically across models, with overall male\nrepresentation ranging from 46.7% to 73.3%. Notably, the top-performing model\nde-amplified gender stereotypes and approached gender parity, achieving the\nhighest fairness scores. This variation suggests high bias is not an inevitable\noutcome but a consequence of design choices. Our work provides the most\ncomprehensive cross-model benchmark of gender bias to date and underscores the\nnecessity of standardized, automated evaluation tools for promoting\naccountability and fairness in AI development.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.07512", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07512", "abs": "https://arxiv.org/abs/2509.07512", "authors": ["Zihan Chen", "Lei Shi", "Weize Wu", "Qiji Zhou", "Yue Zhang"], "title": "ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval", "comment": null, "summary": "Many contemporary data-driven research efforts in the natural sciences, such\nas chemistry and materials science, require large-scale, high-performance\nentity recognition from scientific datasets. Large language models (LLMs) have\nincreasingly been adopted to solve the entity recognition task, with the same\ntrend being observed on all-spectrum NLP tasks. The prevailing entity\nrecognition LLMs rely on fine-tuned technology, yet the fine-tuning process\noften incurs significant cost. To achieve a best performance-cost trade-off, we\npropose ALLabel, a three-stage framework designed to select the most\ninformative and representative samples in preparing the demonstrations for LLM\nmodeling. The annotated examples are used to construct a ground-truth retrieval\ncorpus for LLM in-context learning. By sequentially employing three distinct\nactive learning strategies, ALLabel consistently outperforms all baselines\nunder the same annotation budget across three specialized domain datasets.\nExperimental results also demonstrate that selectively annotating only 5\\%-10\\%\nof the dataset with ALLabel can achieve performance comparable to the method\nannotating the entire dataset. Further analyses and ablation studies verify the\neffectiveness and generalizability of our proposal.", "AI": {"tldr": "提出了ALLabel，一种用于优化实体识别任务中样本选择的三阶段主动学习框架，显著降低成本并保持高水平的性能。", "motivation": "提高实体识别任务的性能-成本平衡，尤其是针对大规模高性能实体识别任务，而现有方法通常成本高。", "method": "ALLabel是一种三阶段框架，设计用于在准备LLM建模演示时选择最具信息量和代表性的样本。通过顺序使用三种不同的主动学习策略，ALLabel能够超越所有基线方法，尤其是在专业化领域数据集上表现出色。", "result": "实验结果显示，使用ALLabel对数据集的5%-10%进行选择性标注，可以达到与对整个数据集标注的方法相媲美的性能。", "conclusion": "ALLabel证明了其在不同专业领域数据集上能够有效降低标注成本同时保持高性能，进一步分析和消融实验也验证了其有效性和泛化能力。"}}
{"id": "2509.07120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07120", "abs": "https://arxiv.org/abs/2509.07120", "authors": ["Chung-Shien Brian Wang", "Christian Schmidt", "Jens Piekenbrinck", "Bastian Leibe"], "title": "Faster VGGT with Block-Sparse Global Attention", "comment": "Project page at https://vision.rwth-aachen.de/sparse-vggt", "summary": "Efficient and accurate feed-forward multi-view reconstruction has long been\nan important task in computer vision. Recent transformer-based models like VGGT\nand $\\pi^3$ have achieved impressive results with simple architectures, yet\nthey face an inherent runtime bottleneck, due to the quadratic complexity of\nthe global attention layers, that limits the scalability to large image sets.\nIn this paper, we empirically analyze the global attention matrix of these\nmodels and observe that probability mass concentrates on a small subset of\npatch-patch interactions that correspond to cross-view geometric matches.\nMotivated by the structured attention and inspired by recent advancement in\nlarge language models, we propose a replacement for the dense global attention\noperation based on highly optimized block-sparse kernels, yielding up to\n$4\\times$ faster inference with comparable task performance. Our retrofit\nrequires no retraining of the backbone, extends to both VGGT and $\\pi^3$, and\nsupports large image collections. Evaluations on a comprehensive suite of\nmulti-view benchmarks demonstrate the effectiveness of our approach.", "AI": {"tldr": "为了解决全局注意力层中的运行时间瓶颈问题，我们提出了一种基于块稀疏核的方法，极大提升了推理速度，同时保持了与之前模型相当的任务性能。", "motivation": "受到大型语言模型的进步启发，我们旨在解决transformer-based模型中的全局注意力层的二次复杂度运行时间瓶颈问题，该问题限制了这些模型在大规模图像集合上的可扩展性。", "method": "我们提出了一种基于高度优化的块稀疏核替代密集全局注意力操作的方法。这种方法能够实现高达4倍的加速推理，并且任务性能相当。", "result": "在多视图基准测试的全面套件上进行的评估表明了我们方法的有效性。并且这种方法无需重新训练骨干网络，适用于VGGT和$\\pi^3$模型，支持大规模图像集合。", "conclusion": "实验结果证明了这种方法的有效性，不仅加速了推理过程，还支持大规模图像集合，且不需要重新训练模型。"}}
{"id": "2509.07553", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07553", "abs": "https://arxiv.org/abs/2509.07553", "authors": ["Zheng Wu", "Heyuan Huang", "Xingyu Lou", "Xiangmou Qu", "Pengzhou Cheng", "Zongru Wu", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhaoxiang Wang", "Zhuosheng Zhang"], "title": "VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents", "comment": null, "summary": "With the rapid progress of multimodal large language models, operating system\n(OS) agents become increasingly capable of automating tasks through on-device\ngraphical user interfaces (GUIs). However, most existing OS agents are designed\nfor idealized settings, whereas real-world environments often present\nuntrustworthy conditions. To mitigate risks of over-execution in such\nscenarios, we propose a query-driven human-agent-GUI interaction framework that\nenables OS agents to decide when to query humans for more reliable task\ncompletion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy\nOS agent trained with a two-stage learning paradigm that falicitate the\ndecoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent\nautonomously executes actions in normal conditions while proactively querying\nhumans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves\nthe average step-wise success rate by 20.64\\% in untrustworthy scenarios over\nthe state-of-the-art, without compromising normal performance. Analysis\nhighlights VeriOS-Agent's rationality, generalizability, and scalability. The\ncodes, datasets and models are available at\nhttps://github.com/Wuzheng02/VeriOS.", "AI": {"tldr": "研究提出了VeriOS-Agent，一个基于查询驱动的人机GUI交互框架训练的操作系统代理，与最先进的方法相比，VeriOS-Agent在不可信场景下将平均每步成功概率提高了20.64%。", "motivation": "随着多模态大型语言模型的快速发展，操作系统代理通过设备上的图形用户界面（GUI）自动化任务的能力越来越强。然而，大多数现有的操作系统代理设计是为理想化的环境，而现实世界中经常存在不可信的条件。为了减轻这种场景下过执行的风险，本研究提出了上述方法。", "method": "提出了一种基于查询驱动的人机GUI交互框架，该框架允许操作系统代理在不可信场景中主动查询人类以获得更可靠的完成任务的方法。具体来说，VeriOS-Agent代理在正常条件下自主执行动作，而在不可信场景中主动向人类查询。这种方法采用了两阶段学习范式，用以解耦和利用元知识。", "result": "实验表明，与现有的方法相比，VeriOS-Agent在不降低正常场景下性能的同时，显著提高了在不可信条件下的任务完成成功率。", "conclusion": "实验结果显示，与最先进的方法相比，在不可信场景下，VeriOS-Agent将平均每步成功概率提高了20.64%，同时不影响正常的表现。分析表明，VeriOS-Agent在合理性、泛化和可扩展性方面表现出色。"}}
{"id": "2509.07130", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.07130", "abs": "https://arxiv.org/abs/2509.07130", "authors": ["Soruya Saha", "Md Nurul Absur", "Saptarshi Debroy"], "title": "Detection and Recovery of Adversarial Slow-Pose Drift in Offloaded Visual-Inertial Odometry", "comment": "12 Pages, 8 Figures", "summary": "Visual-Inertial Odometry (VIO) supports immersive Virtual Reality (VR) by\nfusing camera and Inertial Measurement Unit (IMU) data for real-time pose.\nHowever, current trend of offloading VIO to edge servers can lead server-side\nthreat surface where subtle pose spoofing can accumulate into substantial\ndrift, while evading heuristic checks. In this paper, we study this threat and\npresent an unsupervised, label-free detection and recovery mechanism. The\nproposed model is trained on attack-free sessions to learn temporal\nregularities of motion to detect runtime deviations and initiate recovery to\nrestore pose consistency. We evaluate the approach in a realistic offloaded-VIO\nenvironment using ILLIXR testbed across multiple spoofing intensities.\nExperimental results in terms of well-known performance metrics show\nsubstantial reductions in trajectory and pose error compared to a no-defense\nbaseline.", "AI": {"tldr": "本文提出了一种新的检测和恢复机制，能在边缘服务器环境中的VIO遭遇姿态欺骗攻击时，恢复其准确性。", "motivation": "论文研究了将VIO卸载到边缘服务器时存在的一种安全威胁，即攻击者可能通过细微的姿态欺骗造成显著的漂移，并且这些攻击可以避开传统的检测机制。", "method": "该论文提出了一种无监督且无需标签的检测和恢复机制，用于识别VIO（视觉惯性里程计）中的姿态欺骗，并恢复姿态的一致性。该模型通过学习无攻击会话中的运动时间规律来检测运行时的偏差并启动恢复。", "result": "实验结果显示，与无防御基准相比，该方法在现实的边缘卸载VIO环境中，在多种欺骗强度下，显著减少了轨迹和姿态误差。", "conclusion": "论文实验结果表明，提出的模型能够在各种姿态欺骗强度下有效降低轨迹和姿态误差，有效保护VIO在卸载到边缘服务器时的安全性和准确性。"}}
{"id": "2509.07555", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07555", "abs": "https://arxiv.org/abs/2509.07555", "authors": ["Yi Liu", "Xiangrong Zhu", "Xiangyu Liu", "Wei Wei", "Wei Hu"], "title": "Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition", "comment": "Accepted in EMNLP Findings 2025", "summary": "In a rapidly evolving world where information updates swiftly, knowledge in\nlarge language models (LLMs) becomes outdated quickly. Retraining LLMs is not a\ncost-effective option, making knowledge editing (KE) without modifying\nparameters particularly necessary. We find that although existing\nretrieval-augmented generation (RAG)-based KE methods excel at editing simple\nknowledge, they struggle with KE in multi-hop question answering due to the\nissue of \"edit skipping\", which refers to skipping the relevant edited fact in\ninference. In addition to the diversity of natural language expressions of\nknowledge, edit skipping also arises from the mismatch between the granularity\nof LLMs in problem-solving and the facts in the edited memory. To address this\nissue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing\nmethod with guided decomposition (IRAKE) through the guidance from single\nedited facts and entire edited cases. Experimental results demonstrate that\nIRAKE mitigates the failure of editing caused by edit skipping and outperforms\nstate-of-the-art methods for KE in multi-hop question answering.", "AI": {"tldr": "The paper introduces IRAKE, an iterative and guided method to improve knowledge editing in large language models for multi-hop question answering, addressing the challenge of edit skipping.", "motivation": "The motivation behind this paper is to improve knowledge editing (KE) efficiency without retraining large language models (LLMs). It addresses the limitation of existing retrieval-augmented generation (RAG)-based methods that excel in editing simple knowledge but fail in complex, multi-hop question answering due to the issue of 'edit skipping'.", "method": "We propose a novel Iterative Retrieval-Augmented Knowledge Editing method with guided decomposition (IRAKE) which uses guidance from single edited facts and entire edited cases to address the issue of 'edit skipping' in multi-hop question answering.", "result": "Experimental results show that IRAKE successfully mitigates the failure of editing caused by edit skipping, improving performance in multi-hop question answering tasks over existing state-of-the-art methods.", "conclusion": "The paper concludes that IRAKE, by leveraging guided decomposition and iterative retrieval-augmentation, significantly enhances knowledge editing capabilities in complex multi-hop scenarios, overcoming the 'edit skipping' issue present in current techniques."}}
{"id": "2509.07178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07178", "abs": "https://arxiv.org/abs/2509.07178", "authors": ["Muhammad Saad Saeed", "Ijaz Ul Haq", "Khalid Malik"], "title": "Realism to Deception: Investigating Deepfake Detectors Against Face Enhancement", "comment": null, "summary": "Face enhancement techniques are widely used to enhance facial appearance.\nHowever, they can inadvertently distort biometric features, leading to\nsignificant decrease in the accuracy of deepfake detectors. This study\nhypothesizes that these techniques, while improving perceptual quality, can\ndegrade the performance of deepfake detectors. To investigate this, we\nsystematically evaluate whether commonly used face enhancement methods can\nserve an anti-forensic role by reducing detection accuracy. We use both\ntraditional image processing methods and advanced GAN-based enhancements to\nevaluate the robustness of deepfake detectors. We provide a comprehensive\nanalysis of the effectiveness of these enhancement techniques, focusing on\ntheir impact on Na\\\"ive, Spatial, and Frequency-based detection methods.\nFurthermore, we conduct adversarial training experiments to assess whether\nexposure to face enhancement transformations improves model robustness.\nExperiments conducted on the FaceForensics++, DeepFakeDetection, and CelebDF-v2\ndatasets indicate that even basic enhancement filters can significantly reduce\ndetection accuracy achieving ASR up to 64.63\\%. In contrast, GAN-based\ntechniques further exploit these vulnerabilities, achieving ASR up to 75.12\\%.\nOur results demonstrate that face enhancement methods can effectively function\nas anti-forensic tools, emphasizing the need for more resilient and adaptive\nforensic methods.", "AI": {"tldr": "This paper investigates whether face enhancement techniques can significantly reduce the accuracy of deepfake detectors, demonstrating that even basic filters and GAN-based methods can function as effective anti-forensic tools.", "motivation": "To understand if face enhancement methods inadvertently degrade the performance of deepfake detection systems through unintentional biometric feature distortion.", "method": "Structure", "result": "Experiments show that both basic filters and GAN-based techniques can reduce the detection accuracy of deepfake detectors with ASR reaching up to 75.12% in some cases.", "conclusion": "The study underscores the need for improved and more resilient forensic methods to counteract the anti-forensic capabilities of face enhancement techniques."}}
{"id": "2509.07588", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3; J.3"], "pdf": "https://arxiv.org/pdf/2509.07588", "abs": "https://arxiv.org/abs/2509.07588", "authors": ["Andrey Sakhovskiy", "Elena Tutubalina"], "title": "BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment", "comment": "9 pages, 1 figure, published in \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR 2025)\"", "summary": "In recent years, there has been substantial progress in using pretrained\nLanguage Models (LMs) on a range of tasks aimed at improving the understanding\nof biomedical texts. Nonetheless, existing biomedical LLMs show limited\ncomprehension of complex, domain-specific concept structures and the factual\ninformation encoded in biomedical Knowledge Graphs (KGs). In this work, we\npropose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel\njoint LM and KG pre-training method that augments an LM with external knowledge\nby the simultaneous learning of a dedicated KG encoder and aligning the\nrepresentations of both the LM and the graph. For a given textual sequence, we\nlink biomedical concept mentions to the Unified Medical Language System (UMLS)\nKG and utilize local KG subgraphs as cross-modal positive samples for these\nmentions. Our empirical findings indicate that implementing our method on\nseveral leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves\ntheir performance on a range of language understanding tasks and the quality of\nentity representations, even with minimal pre-training on a small alignment\ndataset sourced from PubMed scientific abstracts.", "AI": {"tldr": "The paper proposes BALI, a joint pre-training method for biomedical LMs and KGs, which improves the performance on language understanding tasks and the quality of entity representations by aligning LM and KG representations.", "motivation": "To improve the comprehension of complex, domain-specific concept structures and factual information encoded in biomedical Knowledge Graphs (KGs) by existing biomedical LMs.", "method": "BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph.", "result": "Implementing the method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations.", "conclusion": "The proposed BALI method enhances the understanding of complex, domain-specific concept structures and factual information encoded in biomedical Knowledge Graphs (KGs) for biomedical LMs."}}
{"id": "2509.07184", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07184", "abs": "https://arxiv.org/abs/2509.07184", "authors": ["Erencem Ozbey", "Dimitrios I. Diochnos"], "title": "Dimensionally Reduced Open-World Clustering: DROWCULA", "comment": "16 pages, 12 Figures, 12 Tables", "summary": "Working with annotated data is the cornerstone of supervised learning.\nNevertheless, providing labels to instances is a task that requires significant\nhuman effort. Several critical real-world applications make things more\ncomplicated because no matter how many labels may have been identified in a\ntask of interest, it could be the case that examples corresponding to novel\nclasses may appear in the future. Not unsurprisingly, prior work in this,\nso-called, `open-world' context has focused a lot on semi-supervised\napproaches.\n  Focusing on image classification, somehow paradoxically, we propose a fully\nunsupervised approach to the problem of determining the novel categories in a\nparticular dataset. Our approach relies on estimating the number of clusters\nusing Vision Transformers, which utilize attention mechanisms to generate\nvector embeddings. Furthermore, we incorporate manifold learning techniques to\nrefine these embeddings by exploiting the intrinsic geometry of the data,\nthereby enhancing the overall image clustering performance. Overall, we\nestablish new State-of-the-Art results on single-modal clustering and Novel\nClass Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do\nso, both when the number of clusters is known or unknown ahead of time. The\ncode is available at: https://github.com/DROWCULA/DROWCULA.", "AI": {"tldr": "论文提出了一种基于视觉变压器和流形学习的无监督方法，用于识别图像数据集中的新类别，并在多个基准数据集中取得了优异结果。", "motivation": "尽管标注数据是监督学习的基础，但在“开放世界”背景下，由于未来可能会出现新的类别，标注工作变得复杂且耗时。该论文旨在解决这一问题。", "method": "该研究提出了一种完全无监督的方法来确定特定数据集中的新类别。方法基于视觉变压器，利用注意力机制生成向量嵌入，并结合流形学习技术来优化这些嵌入，以提升图像聚类的整体性能。", "result": "研究在CIFAR-10, CIFAR-100, ImageNet-100, 和 Tiny ImageNet 上建立了单模态聚类和新类别发现的新SOTA结果，不论是已知还是未知类别数量的情况。", "conclusion": "该方法使用无监督的方式在图像分类任务中实现了新的类别识别，特别是在开放世界情况下，当数据集中出现前所未见的类别时，能够有效地对其进行识别和分类。"}}
{"id": "2509.07622", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07622", "abs": "https://arxiv.org/abs/2509.07622", "authors": ["Libo Ren", "Yee Man Ng", "Lifeng Han"], "title": "MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs", "comment": "system paper at CLEF 2025", "summary": "Efficient communication between patients and clinicians plays an important\nrole in shared decision-making. However, clinical reports are often lengthy and\nfilled with clinical jargon, making it difficult for domain experts to identify\nimportant aspects in the document efficiently. This paper presents the\nmethodology we applied in the MultiClinSUM shared task for summarising clinical\ncase documents. We used an Iterative Self-Prompting technique on large language\nmodels (LLMs) by asking LLMs to generate task-specific prompts and refine them\nvia example-based few-shot learning. Furthermore, we used lexical and embedding\nspace metrics, ROUGE and BERT-score, to guide the model fine-tuning with\nepochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved\nROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P,\nR, F1) from the official evaluation on 3,396 clinical case reports from various\nspecialties extracted from open journals. The high BERTscore indicates that the\nmodel produced semantically equivalent output summaries compared to the\nreferences, even though the overlap at the exact lexicon level is lower, as\nreflected in the lower ROUGE scores. This work sheds some light on how\nperspective-aware ISP (PA-ISP) can be deployed for clinical report\nsummarisation and support better communication between patients and clinicians.", "AI": {"tldr": "本研究提出了一种使用迭代自提示技术的临床报告总结方法，通过细粒度的模型微调，所得总结与参考摘要在语义上的等价性高，有助于提升医患间的沟通效率。", "motivation": "临床报告内容详尽且充满专业术语，导致领域专家难以高效识别报告中的关键内容。本文旨在通过改进的总结方法提高临床报告的可读性和重要信息的提取效率，以促进医生和患者之间的沟通。", "method": "采用迭代自提示技术在大型语言模型上生成特定任务的提示，并通过基于实例的少量示例学习来优化这些提示。另外，利用词法和嵌入空间指标，如ROUGE和BERT评分，来指导模型的微调过程。", "result": "使用基于GPT-4和GPT-4o的视角感知ISP方法的提交版本，获得了ROUGE分数（46.53, 24.68, 30.77）及BERT得分（87.84, 83.25, 85.46）（分别为P值、R值和F1值），这些得分来源于针对3,396份从开放期刊中提取的临床病例报告的官方评估。", "conclusion": "本次研究展示了视角感知ISP技术在临床报告总结中的应用前景，它可以生成与参考摘要在语义上等效的总结，尽管词汇层面的重叠较低，从而支持更好的医患沟通。"}}
{"id": "2509.07213", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07213", "abs": "https://arxiv.org/abs/2509.07213", "authors": ["Raja Mallina", "Bryar Shareef"], "title": "XBusNet: Text-Guided Breast Ultrasound Segmentation via Multimodal Vision-Language Learning", "comment": "15 pages, 3 figures, 4 tables", "summary": "Background: Precise breast ultrasound (BUS) segmentation supports reliable\nmeasurement, quantitative analysis, and downstream classification, yet remains\ndifficult for small or low-contrast lesions with fuzzy margins and speckle\nnoise. Text prompts can add clinical context, but directly applying weakly\nlocalized text-image cues (e.g., CAM/CLIP-derived signals) tends to produce\ncoarse, blob-like responses that smear boundaries unless additional mechanisms\nrecover fine edges. Methods: We propose XBusNet, a novel dual-prompt,\ndual-branch multimodal model that combines image features with clinically\ngrounded text. A global pathway based on a CLIP Vision Transformer encodes\nwhole-image semantics conditioned on lesion size and location, while a local\nU-Net pathway emphasizes precise boundaries and is modulated by prompts that\ndescribe shape, margin, and Breast Imaging Reporting and Data System (BI-RADS)\nterms. Prompts are assembled automatically from structured metadata, requiring\nno manual clicks. We evaluate on the Breast Lesions USG (BLU) dataset using\nfive-fold cross-validation. Primary metrics are Dice and Intersection over\nUnion (IoU); we also conduct size-stratified analyses and ablations to assess\nthe roles of the global and local paths and the text-driven modulation.\nResults: XBusNet achieves state-of-the-art performance on BLU, with mean Dice\nof 0.8765 and IoU of 0.8149, outperforming six strong baselines. Small lesions\nshow the largest gains, with fewer missed regions and fewer spurious\nactivations. Ablation studies show complementary contributions of global\ncontext, local boundary modeling, and prompt-based modulation. Conclusions: A\ndual-prompt, dual-branch multimodal design that merges global semantics with\nlocal precision yields accurate BUS segmentation masks and improves robustness\nfor small, low-contrast lesions.", "AI": {"tldr": "This paper introduces XBusNet, a multimodal model that combines visual and text-based information to improve BUS segmentation accuracy, particularly for small or less contrast lesions, demonstrating superior performance over existing approaches.", "motivation": "The motivation behind this paper is to improve the accuracy of BUS segmentation for small or low contrast lesions, which are difficult to segment due to fuzzy boundaries and speckle noise, by effectively using text prompts to add clinical context.", "method": "We propose XBusNet, a novel dual-prompt, dual-branch multimodal model that integrates image features with clinically grounded text descriptions to achieve precise breast ultrasound (BUS) segmentation.", "result": "XBusNet achieves state-of-the-art performance with a mean Dice of 0.8765 and IoU of 0.8149 on the BLU dataset, outperforming six strong baselines, with significant improvements in segmenting small lesions.", "conclusion": "A dual-prompt, dual-branch multimodal design that integrates global semantic understanding with local precision enhances the accuracy and robustness of BUS segmentation, especially for small, low-contrast lesions."}}
{"id": "2509.07666", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07666", "abs": "https://arxiv.org/abs/2509.07666", "authors": ["Xixi Wu", "Yanchao Tan", "Nan Hou", "Ruiyang Zhang", "Hong Cheng"], "title": "MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval", "comment": "EMNLP Main 2025", "summary": "Document Understanding is a foundational AI capability with broad\napplications, and Document Question Answering (DocQA) is a key evaluation task.\nTraditional methods convert the document into text for processing by Large\nLanguage Models (LLMs), but this process strips away critical multi-modal\ninformation like figures. While Large Vision-Language Models (LVLMs) address\nthis limitation, their constrained input size makes multi-page document\ncomprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate\nthis by selecting relevant pages, but they rely solely on semantic relevance,\nignoring logical connections between pages and the query, which is essential\nfor reasoning.\n  To this end, we propose MoLoRAG, a logic-aware retrieval framework for\nmulti-modal, multi-page document understanding. By constructing a page graph\nthat captures contextual relationships between pages, a lightweight VLM\nperforms graph traversal to retrieve relevant pages, including those with\nlogical connections often overlooked. This approach combines semantic and\nlogical relevance to deliver more accurate retrieval. After retrieval, the\ntop-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance\nflexibility, MoLoRAG offers two variants: a training-free solution for easy\ndeployment and a fine-tuned version to improve logical relevance checking.\nExperiments on four DocQA datasets demonstrate average improvements of 9.68% in\naccuracy over LVLM direct inference and 7.44% in retrieval precision over\nbaselines. Codes and datasets are released at\nhttps://github.com/WxxShirley/MoLoRAG.", "AI": {"tldr": "提出MoLoRAG框架解决传统方法在多模态多页文档理解中忽略逻辑联系的问题，提升了文档问答任务的准确性和检索精度。", "motivation": "现有的大型语言模型和大型视觉语言模型在处理多模态多页文档理解时存在局限性，特别是对于页面间的逻辑联系和查询的关系考虑不足，这影响了推理能力。", "method": "提出了一种名为MoLoRAG的逻辑感知检索框架，通过构建页面图来捕捉页面间的关系，采用轻量级视觉语言模型进行图遍历来检索相关页面，强调结合语义和逻辑相关性，提高检索准确性。", "result": "实验结果表明，在四个文档问答数据集上的准确率较直接使用大型视觉语言模型提升了平均9.68%，检索精度也提高了平均7.44%。", "conclusion": "MoLoRAG框架通过融合语义和逻辑的相关性，显著提高了多模态多页文档理解中相关页面的检索质量，增强了文档问答任务的表现。"}}
{"id": "2509.07277", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07277", "abs": "https://arxiv.org/abs/2509.07277", "authors": ["Sepehr Salem", "M. Moein Esfahani", "Jingyu Liu", "Vince Calhoun"], "title": "Breast Cancer Detection in Thermographic Images via Diffusion-Based Augmentation and Nonlinear Feature Fusion", "comment": "Accepted to IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI 2025)", "summary": "Data scarcity hinders deep learning for medical imaging. We propose a\nframework for breast cancer classification in thermograms that addresses this\nusing a Diffusion Probabilistic Model (DPM) for data augmentation. Our\nDPM-based augmentation is shown to be superior to both traditional methods and\na ProGAN baseline. The framework fuses deep features from a pre-trained\nResNet-50 with handcrafted nonlinear features (e.g., Fractal Dimension) derived\nfrom U-Net segmented tumors. An XGBoost classifier trained on these fused\nfeatures achieves 98.0\\% accuracy and 98.1\\% sensitivity. Ablation studies and\nstatistical tests confirm that both the DPM augmentation and the nonlinear\nfeature fusion are critical, statistically significant components of this\nsuccess. This work validates the synergy between advanced generative models and\ninterpretable features for creating highly accurate medical diagnostic tools.", "AI": {"tldr": "A study uses a DPM for data augmentation to improve breast cancer classification in thermograms, combining deep and handcrafted features for high accuracy.", "motivation": "The motivation for this research is to address the challenge of data scarcity in deep learning applications for medical imaging, particularly for breast cancer classification in thermograms.", "method": "The method utilizes a Diffusion Probabilistic Model (DPM) for enhancing breast cancer classification in thermograms through data augmentation. It fuses deep features from a pre-trained ResNet-50 with handcrafted nonlinear features, such as Fractal Dimension, obtained from U-Net segmented tumors, and employs an XGBoost classifier.", "result": "The framework achieves 98.0% accuracy and 98.1% sensitivity in classifying breast cancer in thermograms. Ablation studies and statistical tests support the importance of DPM augmentation and nonlinear feature fusion.", "conclusion": "The study concludes that combining advanced generative models with interpretable features can significantly improve the accuracy of medical diagnostic tools, in this case for breast cancer classification in thermograms."}}
{"id": "2509.07730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07730", "abs": "https://arxiv.org/abs/2509.07730", "authors": ["Zexuan Li", "Hongliang Dai", "Piji Li"], "title": "M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models", "comment": "Accepted by EMNLP2025 Main Conference", "summary": "For Relation Extraction (RE), the manual annotation of training data may be\nprohibitively expensive, since the sentences that contain the target relations\nin texts can be very scarce and difficult to find. It is therefore beneficial\nto develop an efficient method that can automatically extract training\ninstances from unlabeled texts for training RE models. Recently, large language\nmodels (LLMs) have been adopted in various natural language processing tasks,\nwith RE also benefiting from their advances. However, when leveraging LLMs for\nRE with predefined relation categories, two key challenges arise. First, in a\nmulti-class classification setting, LLMs often struggle to comprehensively\ncapture the semantics of every relation, leading to suboptimal results. Second,\nalthough employing binary classification for each relation individually can\nmitigate this issue, it introduces significant computational overhead,\nresulting in impractical time complexity for real-world applications.\nTherefore, this paper proposes a framework called M-BRe to extract training\ninstances from unlabeled texts for RE. It utilizes three modules to combine the\nadvantages of both of the above classification approaches: Relation Grouping,\nRelation Extraction, and Label Decision. Extensive experiments confirm its\nsuperior capability in discovering high-quality training samples from unlabeled\ntexts for RE.", "AI": {"tldr": "A framework named M-BRe is proposed for efficient and effective extraction of training instances from unlabeled texts for Relation Extraction, addressing the challenges of using large language models (LLMs) for RE.", "motivation": "The manual annotation of training data for Relation Extraction may be prohibitively expensive, so an efficient automatic method to extract training instances from unlabeled texts is developed. The challenges of using large language models (LLMs) for RE with predefined relation categories are addressed, aiming to reduce computational overhead and improve semantic capture of relations.", "method": "This paper proposes a framework called M-BRe, which consists of three modules: Relation Grouping, Relation Extraction, and Label Decision, aiming to combine the advantages of multi-class and binary classification approaches to improve the efficiency and effectiveness of extracting training instances from unlabeled texts for Relation Extraction (RE).", "result": "Experiments confirm the framework's superior capability in discovering high-quality training samples from unlabeled texts for RE, thus improving the efficiency and effectiveness of Relation Extraction.", "conclusion": "The proposed framework M-BRe successfully offers a more efficient and effective way of extracting training instances for Relation Extraction by utilizing a combination of relation grouping, relation extraction, and label decision, thus mitigating the issues of large language models in capturing relation semantics and reducing computational overhead."}}
{"id": "2509.07295", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07295", "abs": "https://arxiv.org/abs/2509.07295", "authors": ["Ji Xie", "Trevor Darrell", "Luke Zettlemoyer", "XuDong Wang"], "title": "Reconstruction Alignment Improves Unified Multimodal Models", "comment": "28 pages, 24 figures and 10 tables", "summary": "Unified multimodal models (UMMs) unify visual understanding and generation\nwithin a single architecture. However, conventional training relies on\nimage-text pairs (or sequences) whose captions are typically sparse and miss\nfine-grained visual details--even when they use hundreds of words to describe a\nsimple image. We introduce Reconstruction Alignment (RecA), a\nresource-efficient post-training method that leverages visual understanding\nencoder embeddings as dense \"text prompts,\" providing rich supervision without\ncaptions. Concretely, RecA conditions a UMM on its own visual understanding\nembeddings and optimizes it to reconstruct the input image with a\nself-supervised reconstruction loss, thereby realigning understanding and\ngeneration. Despite its simplicity, RecA is broadly applicable: across\nautoregressive, masked-autoregressive, and diffusion-based UMMs, it\nconsistently improves generation and editing fidelity. With only 27 GPU-hours,\npost-training with RecA substantially improves image generation performance on\nGenEval (0.73$\\rightarrow$0.90) and DPGBench (80.93$\\rightarrow$88.15), while\nalso boosting editing benchmarks (ImgEdit 3.38$\\rightarrow$3.75, GEdit\n6.94$\\rightarrow$7.25). Notably, RecA surpasses much larger open-source models\nand applies broadly across diverse UMM architectures, establishing it as an\nefficient and general post-training alignment strategy for UMMs", "AI": {"tldr": "论文提出Reconstruction Alignment (RecA)策略，通过UMMs自身的视觉理解嵌入作为“文本提示”，改善了多种UMMs架构的生成和编辑效果，且计算资源需求低。", "motivation": "传统训练方法依赖的图像-文本对的描述通常缺乏细粒度的视觉细节。为了改善UMMs的图像生成和编辑保真度，降低成本。", "method": "引入Reconstruction Alignment (RecA)，利用UMMs的视觉理解嵌入作为密集的“文本提示”，并通过自监督重构损失优化输入图像的生成。", "result": "UMMs统一了视觉理解和生成。传统训练依赖图像-文本对，但描述中的细粒度视觉细节不足。研究引入Reconstruction Alignment (RecA)，该方法在没有丰富词汇描述的条件下，使用UMMs自己的视觉理解嵌入作为密集的“文本提示”，通过自监督重构损失优化输入图像的生成。实验表明，RecA在多种UMMs架构中改善了图像生成和编辑的保真度，使用很少的计算资源（27个GPU小时）显著提高了性能。", "conclusion": "RecA作为一个高效的后训练策略，适用于多种UMMs架构，其效果超越了更大的开源模型。"}}
{"id": "2509.07755", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.07755", "abs": "https://arxiv.org/abs/2509.07755", "authors": ["Rochana Prih Hastuti", "Rian Adam Rajagede", "Mansour Al Ghanim", "Mengxin Zheng", "Qian Lou"], "title": "Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts", "comment": "Accepted at EMNLP 2025 Findings", "summary": "As large language models (LLMs) adapted to sensitive domains such as\nmedicine, their fluency raises safety risks, particularly regarding provenance\nand accountability. Watermarking embeds detectable patterns to mitigate these\nrisks, yet its reliability in medical contexts remains untested. Existing\nbenchmarks focus on detection-quality tradeoffs, overlooking factual risks\nunder low-entropy settings often exploited by watermarking's reweighting\nstrategy. We propose a medical-focused evaluation workflow that jointly\nassesses factual accuracy and coherence. Using GPT-Judger and further human\nvalidation, we introduce the Factuality-Weighted Score (FWS), a composite\nmetric prioritizing factual accuracy beyond coherence to guide watermarking\ndeployment in medical domains. Our evaluation shows current watermarking\nmethods substantially compromise medical factuality, with entropy shifts\ndegrading medical entity representation. These findings underscore the need for\ndomain-aware watermarking approaches that preserve the integrity of medical\ncontent.", "AI": {"tldr": "我们对现有水印方法在医学领域的有效性进行了评估，提出了FWS度量标准，发现当前水印方法极大损害了医学内容的事实准确性，强调了开发领域感知的水印策略的需求。", "motivation": "随着大型语言模型在医学等敏感领域的应用，其流畅性带来了安全风险，尤其是在来源和责任方面。现有的基准测试侧重于检测质量和权衡，忽视了在低熵环境下因水印策略的重新加权策略而出现的事实风险。该研究旨在填补这一空白。", "method": "我们提出了一种以医学为重点的评估工作流程，该流程同时评估事实准确性与连贯性。使用GPT-Judger和进一步的人类验证，引入了事实加权得分（FWS），这是一个优先考虑事实准确性而非连贯性的复合度量，以指导医学领域的水印策略部署。", "result": "评估结果显示，目前的水印方法严重损害了医学的事实准确性，并且熵的变化影响了医学实体的表示。", "conclusion": "这些发现强调了开发领域感知的水印方法的需求，这些方法可以保持医学内容的完整性。"}}
{"id": "2509.07327", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07327", "abs": "https://arxiv.org/abs/2509.07327", "authors": ["Shucong Li", "Zhenyu Liu", "Zijie Hong", "Zhiheng Zhou", "Xianghai Cao"], "title": "DEPF: A UAV Multispectral Object Detector with Dual-Domain Enhancement and Priority-Guided Mamba Fusion", "comment": null, "summary": "Multispectral remote sensing object detection is one of the important\napplication of unmanned aerial vehicle (UAV). However, it faces three\nchallenges. Firstly, the low-light remote sensing images reduce the\ncomplementarity during multi-modality fusion. Secondly, the local small target\nmodeling is interfered with redundant information in the fusion stage easily.\nThirdly, due to the quadratic computational complexity, it is hard to apply the\ntransformer-based methods on the UAV platform. To address these limitations,\nmotivated by Mamba with linear complexity, a UAV multispectral object detector\nwith dual-domain enhancement and priority-guided mamba fusion (DEPF) is\nproposed. Firstly, to enhance low-light remote sensing images, Dual-Domain\nEnhancement Module (DDE) is designed, which contains Cross-Scale Wavelet Mamba\n(CSWM) and Fourier Details Recovery block (FDR). CSWM applies cross-scale mamba\nscanning for the low-frequency components to enhance the global brightness of\nimages, while FDR constructs spectrum recovery network to enhance the frequency\nspectra features for recovering the texture-details. Secondly, to enhance local\ntarget modeling and reduce the impact of redundant information during fusion,\nPriority-Guided Mamba Fusion Module (PGMF) is designed. PGMF introduces the\nconcept of priority scanning, which starts from local targets features\naccording to the priority scores obtained from modality difference. Experiments\non DroneVehicle dataset and VEDAI dataset reports that, DEPF performs well on\nobject detection, comparing with state-of-the-art methods. Our code is\navailable in the supplementary material.", "AI": {"tldr": "本文提出了DEPF检测器，结合双域增强和优先级引导曼巴融合，有效解决了多光谱遥感目标检测中的几项技术挑战。", "motivation": "本文旨在解决多光谱遥感目标检测中的三个挑战：低光遥感影像中的多模态融合互补性降低、局部小目标建模受到融合阶段冗余信息干扰、基于变换的方法在无人机平台上的二次计算复杂性问题。", "method": "本文提出了一种名为DEPF的无人机多光谱目标检测器，采用双域增强模块（DDE）和优先级引导曼巴融合模块（PGMF）。DDE模块包括跨尺度小波曼巴（CSWM）和傅里叶细节恢复块（FDR），用于增强低光遥感图像。PGMF模块则引入了优先级扫描的概念，以减轻冗余信息对局部目标建模的影响。", "result": "实验结果显示，在DroneVehicle和VEDAI数据集上，DEPF在目标检测方面表现出色，优于最先进的方法。", "conclusion": "DEPF通过其创新的设计在无人机多光谱目标检测的低光增强和局部目标建模方面取得了良好效果。"}}
{"id": "2509.07768", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07768", "abs": "https://arxiv.org/abs/2509.07768", "authors": ["Michele Joshua Maggini", "Dhia Merzougui", "Rabiraj Bandyopadhyay", "Gaël Dias", "Fabrice Maurel", "Pablo Gamallo"], "title": "Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning", "comment": null, "summary": "The spread of fake news, polarizing, politically biased, and harmful content\non online platforms has been a serious concern. With large language models\nbecoming a promising approach, however, no study has properly benchmarked their\nperformance across different models, usage methods, and languages. This study\npresents a comprehensive overview of different Large Language Models adaptation\nparadigms for the detection of hyperpartisan and fake news, harmful tweets, and\npolitical bias. Our experiments spanned 10 datasets and 5 different languages\n(English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and\nmulticlass classification scenarios. We tested different strategies ranging\nfrom parameter efficient Fine-Tuning of language models to a variety of\ndifferent In-Context Learning strategies and prompts. These included zero-shot\nprompts, codebooks, few-shot (with both randomly-selected and\ndiversely-selected examples using Determinantal Point Processes), and\nChain-of-Thought. We discovered that In-Context Learning often underperforms\nwhen compared to Fine-Tuning a model. This main finding highlights the\nimportance of Fine-Tuning even smaller models on task-specific settings even\nwhen compared to the largest models evaluated in an In-Context Learning setup -\nin our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and\nQwen2.5-7B-Instruct.", "AI": {"tldr": "研究对比了大型语言模型检测假新闻和偏见内容的各种适应策略，发现微调模型优于In-Context Learning。", "motivation": "在线平台上假新闻和有害内容传播的担忧加剧，LLM在该领域尚未得到全面评估。", "method": "通过参数效率的微调和多种In-Context Learning策略（包括零样本提示、编码本、少样本学习以及推理链）来检测假新闻、极化内容和政治偏见的性能对比。", "result": "研究发现In-Context Learning通常不如微调模型表现好，即使是最小的微调模型也优于最大的In-Context Learning模型。", "conclusion": "强调了在特定任务设置下微调模型的重要性，即便对于较小模型也是如此。"}}
{"id": "2509.07335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07335", "abs": "https://arxiv.org/abs/2509.07335", "authors": ["Haiqing Ren", "Zhongkai Luo", "Heng Fan", "Xiaohui Yuan", "Guanchen Wang", "Libo Zhang"], "title": "G3CN: Gaussian Topology Refinement Gated Graph Convolutional Network for Skeleton-Based Action Recognition", "comment": "8 pages, 5 figures, IROS", "summary": "Graph Convolutional Networks (GCNs) have proven to be highly effective for\nskeleton-based action recognition, primarily due to their ability to leverage\ngraph topology for feature aggregation, a key factor in extracting meaningful\nrepresentations. However, despite their success, GCNs often struggle to\neffectively distinguish between ambiguous actions, revealing limitations in the\nrepresentation of learned topological and spatial features. To address this\nchallenge, we propose a novel approach, Gaussian Topology Refinement Gated\nGraph Convolution (G$^{3}$CN), to address the challenge of distinguishing\nambiguous actions in skeleton-based action recognition. G$^{3}$CN incorporates\na Gaussian filter to refine the skeleton topology graph, improving the\nrepresentation of ambiguous actions. Additionally, Gated Recurrent Units (GRUs)\nare integrated into the GCN framework to enhance information propagation\nbetween skeleton points. Our method shows strong generalization across various\nGCN backbones. Extensive experiments on NTU RGB+D, NTU RGB+D 120, and NW-UCLA\nbenchmarks demonstrate that G$^{3}$CN effectively improves action recognition,\nparticularly for ambiguous samples.", "AI": {"tldr": "本论文提出了G$^{3}$CN，通过高斯滤波和GRUs改良GCN，提升了基于骨架动作识别对于模糊动作的分辨精确度。", "motivation": "尽管图卷积网络（GCN）在基于骨架的动作识别方面取得了显著成功，但它们在区分模糊动作方面存在不足。为了解决这一问题，本研究提出了G$^{3}$CN方法。", "method": "本研究提出了一种新颖的方法——高斯拓扑细化门控图卷积网络（G$^{3}$CN），用于区分基于骨架的动作识别中的模糊动作。该方法结合了高斯滤波器来优化骨架拓扑图，增强对模糊动作的表示能力，并集成了门控循环单元（GRUs）以增强骨架点之间的信息传播。", "result": "在NTU RGB+D、NTU RGB+D 120 和 NW-UCLA 数据集上的广泛实验证明，G$^{3}$CN 有效提高了动作识别的准确性，尤其是对于模糊样本。", "conclusion": "G$^{3}$CN 通过高斯滤波和门控循环单元的结合，能够增强基于骨架的动作识别中模糊动作的区分能力，展示了在各种GCN模型中的广泛适用性。"}}
{"id": "2509.07801", "categories": ["cs.CL", "cs.DL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.07801", "abs": "https://arxiv.org/abs/2509.07801", "authors": ["Decheng Duan", "Yingyi Zhang", "Jitong Peng", "Chengzhi Zhang"], "title": "SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP", "comment": "EMNLP 2025 Main", "summary": "Structured information extraction from scientific literature is crucial for\ncapturing core concepts and emerging trends in specialized fields. While\nexisting datasets aid model development, most focus on specific publication\nsections due to domain complexity and the high cost of annotating scientific\ntexts. To address this limitation, we introduce SciNLP - a specialized\nbenchmark for full-text entity and relation extraction in the Natural Language\nProcessing (NLP) domain. The dataset comprises 60 manually annotated full-text\nNLP publications, covering 7,072 entities and 1,826 relations. Compared to\nexisting research, SciNLP is the first dataset providing full-text annotations\nof entities and their relationships in the NLP domain. To validate the\neffectiveness of SciNLP, we conducted comparative experiments with similar\ndatasets and evaluated the performance of state-of-the-art supervised models on\nthis dataset. Results reveal varying extraction capabilities of existing models\nacross academic texts of different lengths. Cross-comparisons with existing\ndatasets show that SciNLP achieves significant performance improvements on\ncertain baseline models. Using models trained on SciNLP, we implemented\nautomatic construction of a fine-grained knowledge graph for the NLP domain.\nOur KG has an average node degree of 3.2 per entity, indicating rich semantic\ntopological information that enhances downstream applications. The dataset is\npublicly available at https://github.com/AKADDC/SciNLP.", "AI": {"tldr": "该研究提出了一个用于NLP领域的全文实体和关系抽取数据集SciNLP，并验证了其在现有模型上的有效性，展示了更高的性能。", "motivation": "由于现有数据集的局限性，本研究旨在开发一个全文本实体和关系抽取的数据集SciNLP，以提升NLP领域内的信息提取能力。", "method": "从科学文献中提取结构化信息对捕捉各个专业领域中的核心概念和新兴趋势至关重要。尽管现有的数据集有助于模型的开发，但由于专业知识的复杂性和注释科学文本的高成本，大多数数据集关注特定的出版物部分。为了解决这个问题，我们介绍了 SciNLP - 一个专门用于NLP领域全文实体和关系提取的基准。该数据集包括60篇手动注释的NLP全文出版物，涵盖了7,072个实体和1,826种关系。与现有的研究相比，SciNLP 是第一个提供 NLP 领域全文实体及其关系注释的数据集。为了验证 SciNLP 的有效性，我们进行了与类似数据集的对比实验，并评估了最先进的监督模型在这个数据集上的表现。结果揭示了现有模型在不同长度的学术文本中实体提取能力的变化。", "result": "与现有数据集的交叉比较表明，SciNLP 对于一些基础模型实现了显著的性能改进。使用在 SciNLP 上训练的模型实现了NLP领域的细粒度知识图谱的自动构建。我们的KG每个实体的平均节点度为3.2，表明富含语义拓扑信息，能够增强下游应用。", "conclusion": "本文介绍了一个新型数据集 SciNLP，用于NLP领域的全文实体和关系提取，展示了其在模型性能上的改进，并用于构建了一个细粒度的知识图谱。该数据集可公开获取。"}}
{"id": "2509.07385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07385", "abs": "https://arxiv.org/abs/2509.07385", "authors": ["Shibang Liu", "Xuemei Xie", "Guangming Shi"], "title": "Parse Graph-Based Visual-Language Interaction for Human Pose Estimation", "comment": null, "summary": "Parse graphs boost human pose estimation (HPE) by integrating context and\nhierarchies, yet prior work mostly focuses on single modality modeling,\nignoring the potential of multimodal fusion. Notably, language offers rich HPE\npriors like spatial relations for occluded scenes, but existing visual-language\nfusion via global feature integration weakens occluded region responses and\ncauses alignment and location failures. To address this issue, we propose Parse\nGraph-based Visual-Language interaction (PGVL) with a core novel Guided Module\n(GM). In PGVL, low-level nodes focus on local features, maximizing the\nmaintenance of responses in occluded areas and high-level nodes integrate\nglobal features to infer occluded or invisible parts. GM enables high semantic\nnodes to guide the feature update of low semantic nodes that have undergone\ncross attention. It ensuring effective fusion of diverse information. PGVL\nincludes top-down decomposition and bottom-up composition. In the first stage,\nmodality specific parse graphs are constructed. Next stage. recursive\nbidirectional cross-attention is used, purified by GM. We also design network\nbased on PGVL. The PGVL and our network is validated on major pose estimation\ndatasets. We will release the code soon.", "AI": {"tldr": "本研究提出PGVL方法，通过设计Guided Module改善视觉-语言融合在人体姿态估计中的效果，尤其在处理遮挡区域时具有优势。", "motivation": "以往的工作大多集中在单模态建模上，忽略了多模态融合的潜力。本研究旨在解决现有视觉-语言融合方法在处理遮挡区域时响应减弱以及对齐和位置失败的问题。通过融合语言提供的空间关系等丰富的HPE先验知识，增强人体姿态估计的效果。", "method": "本研究提出了Parse Graph-based Visual-Language交互(PGVL)方法，并在其中设计了一个核心的新模块——Guided Module (GM)。在PGVL中，低级节点专注于局部特征，以最大限度地维持遮挡区域的响应，而高级节点则融合全局特征以推断遮挡或不可见的部分。GM允许高级节点指导已经经过交叉注意力机制的低级节点的特征更新，从而确保不同信息的有效融合。PGVL包括自上而下的分解和自下而上的组合两阶段。首先构建模式特定的解析图，然后使用递归双向交叉注意力机制，通过GM进行净化。", "result": "尚未具体提及实验结果，但表明了方法在主要的人体姿态估计数据集上的有效性，并计划发布代码以供验证。", "conclusion": "研究表明，通过PGVL及其Guided Module，可以实现有效的多模态信息融合，提高人体姿态估计精度，尤其是处理遮挡和不可见区域的场景。"}}
{"id": "2509.07817", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.07817", "abs": "https://arxiv.org/abs/2509.07817", "authors": ["Xiaolin Chen", "Xuemeng Song", "Haokun Wen", "Weili Guan", "Xiangyu Zhao", "Liqiang Nie"], "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems", "comment": null, "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters.", "AI": {"tldr": "本文提出了一种新型的双知识增强两阶段推理器DK2R，旨在利用结构化属性和非结构化评论知识以及大型语言模型(LLMs)来改进多模态任务导向对话系统中的文本响应生成。", "motivation": "通过利用大型语言模型(LLMs)与结构化和非结构化知识相结合的技术来改进多模态任务导向对话系统中的文本响应生成，应对动态知识类型选择和意图-响应解耦两大挑战。", "method": "DK2R方法首先从外部知识库中提取结构化属性和非结构化评论知识，随后使用LLMs评估每种知识的效用，并通过专门的推理总结出意图导向的关键线索来加强基于LLMs的文本响应生成。", "result": "在公开数据集上进行的大量实验验证了DK2R的优越性。", "conclusion": "本文通过DK2R方法证实了在多模态任务导向对话系统中利用双知识和大型语言模型来改进文本响应生成的有效性。"}}
{"id": "2509.07435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07435", "abs": "https://arxiv.org/abs/2509.07435", "authors": ["Ze-Xin Yin", "Jiaxiong Qiu", "Liu Liu", "Xinjie Wang", "Wei Sui", "Zhizhong Su", "Jian Yang", "Jin Xie"], "title": "DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation", "comment": "14 pages, 7 figures, project page:\n  https://zx-yin.github.io/dreamlifting/", "summary": "The labor- and experience-intensive creation of 3D assets with physically\nbased rendering (PBR) materials demands an autonomous 3D asset creation\npipeline. However, most existing 3D generation methods focus on geometry\nmodeling, either baking textures into simple vertex colors or leaving texture\nsynthesis to post-processing with image diffusion models. To achieve end-to-end\nPBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter\n(LGAA), a novel framework that unifies the modeling of geometry and PBR\nmaterials by exploiting multi-view (MV) diffusion priors from a novel\nperspective. The LGAA features a modular design with three components.\nSpecifically, the LGAA Wrapper reuses and adapts network layers from MV\ndiffusion models, which encapsulate knowledge acquired from billions of images,\nenabling better convergence in a data-efficient manner. To incorporate multiple\ndiffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns\nmultiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed\nvariational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D\nGaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated\npost-processing procedure to effectively extract high-quality, relightable mesh\nassets from the resulting 2DGS. Extensive quantitative and qualitative\nexperiments demonstrate the superior performance of LGAA with both text-and\nimage-conditioned MV diffusion models. Additionally, the modular design enables\nflexible incorporation of multiple diffusion priors, and the\nknowledge-preserving scheme leads to efficient convergence trained on merely\n69k multi-view instances. Our code, pre-trained weights, and the dataset used\nwill be publicly available via our project page:\nhttps://zx-yin.github.io/dreamlifting/.", "AI": {"tldr": "本文介绍了LGAA框架，一个用于生成PBR-ready 3D资产的全新方法，该框架利用了多视角扩散先验知识，通过模块化设计实现了高效训练和灵活先验整合。", "motivation": "由于基于物理渲染（PBR）的3D资产创建工作既耗时又耗力，该研究的目的是为了实现一个从头到尾的PBR/assets生成流水线，以解决现有3D生成方法中的局限性。", "method": "文章提出了一个名为轻量级高斯资产适配器（Lightweight Gaussian Asset Adapter, LGAA）的新框架，该框架通过利用多视角扩散先验知识来统一几何建模和基于物理渲染材料的建模。该框架分为三个模块：LGAA Wrapper、LGAA Switcher和LGAA Decoder。这个框架能够高效地训练，并集成多个扩散先验知识。", "result": "大量的定量和定性实验表明，LGAA在多视角扩散模型的文本和图像条件生成上都表现出优越的性能。同时，其模块化设计使得整合多种扩散先验知识更加灵活高效。", "conclusion": "LGAA通过模块化设计有效地实现了几何模型和PBR材料的统一建模，且能够高效训练、快速收敛。"}}
{"id": "2509.07829", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07829", "abs": "https://arxiv.org/abs/2509.07829", "authors": ["Mihai Nadas", "Laura Diosan", "Andreea Tomescu", "Andrei Piscoran"], "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost", "comment": "25 pages, 8 figures, includes datasets and models released on Hugging\n  Face", "summary": "Literary translation has recently gained attention as a distinct and complex\ntask in machine translation research. However, the translation by small open\nmodels remains an open problem. We contribute to this ongoing research by\nintroducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for\ndataset creation, fine tuning, and evaluation in English-Romanian literary\ntranslations, centred on the creation and open release of both a compact, fine\ntuned language model (TF2-12B) and large scale synthetic parallel datasets\n(DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the\nlargest collection of synthetic English fables to date, we address the need for\nrich, high quality literary datasets in low resource languages such as\nRomanian. Our pipeline first generates 15k high quality Romanian references\nfrom the TF1 pool using a high performing LLM. We then apply a two stage fine\ntuning process to a 12B parameter open weight model: (i) instruction tuning to\ncapture genre specific narrative style, and (ii) adapter compression for\nefficient deployment. Evaluation combines corpus level BLEU and a five\ndimension LLM based rubric (accuracy, fluency, coherence, style, cultural\nadaptation) to provide a nuanced assessment of translation quality. Results\nshow that our fine tuned model achieves fluency and adequacy competitive with\ntop performing large proprietary models, while being open, accessible, and\nsignificantly more cost effective. Alongside the fine tuned model and both\ndatasets, we publicly release all scripts and evaluation prompts. TF2 thus\nprovides an end-to-end, reproducible pipeline for research on cost efficient\ntranslation, cross lingual narrative generation, and the broad adoption of open\nmodels for culturally significant literary content in low resource settings.", "AI": {"tldr": "TF2 是一个专注于英罗文学翻译的统一框架，包含小规模模型微调和大规模合成平行语料库的创建、评估。它通过两阶段微调过程（指令牌调整和适配器压缩）来实现高效部署，并且在成本效率、开放性和可访问性方面具有优势，同时确保了翻译质量。", "motivation": "鉴于文学翻译在机器翻译研究中的独特性和复杂性，特别是在小型开放模型中的翻译问题尚未解决，该研究旨在提供一种解决方案，同时满足罗语这种低资源语言高质量文学数据的需求", "method": "TF2 框架通过两个阶段微调过程建立：第一步，使用高性能LLM从TF1生成高质量罗语参考文献；第二步，对12B参数开放权重模型进行指令调微和适配器压缩，以捕捉特定体裁的叙事风格，并实现高效部署", "result": "研究结果表明，经过微调的TF2-12B模型在流畅性和准确性方面可与顶尖的大规模专有模型相媲美，并且保持了开放性和成本效益", "conclusion": "TF2 提供了高效翻译研究及跨语言叙事生成的端到端可重复管线。此研究推动了开放模型在低资源情境下对文化意义重大的文学内容的广泛采用"}}
{"id": "2509.07447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07447", "abs": "https://arxiv.org/abs/2509.07447", "authors": ["Taiying Peng", "Jiacheng Hua", "Miao Liu", "Feng Lu"], "title": "In the Eye of MLLM: Benchmarking Egocentric Video Intent Understanding with Gaze-Guided Prompting", "comment": null, "summary": "The emergence of advanced multimodal large language models (MLLMs) has\nsignificantly enhanced AI assistants' ability to process complex information\nacross modalities. Recently, egocentric videos, by directly capturing user\nfocus, actions, and context in an unified coordinate, offer an exciting\nopportunity to enable proactive and personalized AI user experiences with\nMLLMs. However, existing benchmarks overlook the crucial role of gaze as an\nindicator of user intent. To address this gap, we introduce EgoGazeVQA, an\negocentric gaze-guided video question answering benchmark that leverages gaze\ninformation to improve the understanding of longer daily-life videos.\nEgoGazeVQA consists of gaze-based QA pairs generated by MLLMs and refined by\nhuman annotators. Our experiments reveal that existing MLLMs struggle to\naccurately interpret user intentions. In contrast, our gaze-guided intent\nprompting methods significantly enhance performance by integrating spatial,\ntemporal, and intent-related cues. We further conduct experiments on\ngaze-related fine-tuning and analyze how gaze estimation accuracy impacts\nprompting effectiveness. These results underscore the value of gaze for more\npersonalized and effective AI assistants in egocentric settings.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.07869", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.07869", "abs": "https://arxiv.org/abs/2509.07869", "authors": ["Jiahui Li", "Sean Papay", "Roman Klinger"], "title": "Are Humans as Brittle as Large Language Models?", "comment": null, "summary": "The output of large language models (LLM) is unstable, due to both\nnon-determinism of the decoding process as well as to prompt brittleness. While\nthe intrinsic non-determinism of LLM generation may mimic existing uncertainty\nin human annotations through distributional shifts in outputs, it is largely\nassumed, yet unexplored, that the prompt brittleness effect is unique to LLMs.\nThis raises the question: do human annotators show similar sensitivity to\ninstruction changes? If so, should prompt brittleness in LLMs be considered\nproblematic? One may alternatively hypothesize that prompt brittleness\ncorrectly reflects human annotation variances. To fill this research gap, we\nsystematically compare the effects of prompt modifications on LLMs and\nidentical instruction modifications for human annotators, focusing on the\nquestion of whether humans are similarly sensitive to prompt perturbations. To\nstudy this, we prompt both humans and LLMs for a set of text classification\ntasks conditioned on prompt variations. Our findings indicate that both humans\nand LLMs exhibit increased brittleness in response to specific types of prompt\nmodifications, particularly those involving the substitution of alternative\nlabel sets or label formats. However, the distribution of human judgments is\nless affected by typographical errors and reversed label order than that of\nLLMs.", "AI": {"tldr": "研究对比了人类注释员和大语言模型(LLM)在不同提示词变化下的表现，发现在某些提示词修改下(如标签集或标签格式的替换)，两者都表现出较高的脆弱性，但在打字错误和标签顺序相反方面，人类的表现受到的影响较小。", "motivation": "研究旨在解决LLM因提示词的微妙变化而导致输出不稳定的问题，探讨这种脆弱性是否独有于LLM，还是也为人类注释员所共有。", "method": "本研究通过系统对比LLM和人类注释员在面对提示词变化时的反应，尤其是在文本分类任务中，来探讨LLM所表现出的提示词脆弱性是否也存在于人类注释员中。", "result": "研究发现，对于特定类型的提示词修改，如标签集或标签格式的替换，人类和LLM都展示出较高的脆弱性。但在面对打字错误和标签顺序反转时，人类注释员受到的影响比LLM小。", "conclusion": "人类和LLM在面对某些提示词修改时表现出相似的敏感性，但在其他类型的修改上则存在差异。这表明LLM的提示词脆弱性并不必然问题化，可能在某些情况下反映了人类注释的变异性。"}}
{"id": "2509.07450", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07450", "abs": "https://arxiv.org/abs/2509.07450", "authors": ["Xudong Lu", "Zhi Zheng", "Yi Wan", "Yongxiang Yao", "Annan Wang", "Renrui Zhang", "Panwang Xia", "Qiong Wu", "Qingyun Li", "Weifeng Lin", "Xiangyu Zhao", "Xue Yang", "Hongsheng Li"], "title": "GLEAM: Learning to Match and Explain in Cross-View Geo-Localization", "comment": "18 pages", "summary": "Cross-View Geo-Localization (CVGL) focuses on identifying correspondences\nbetween images captured from distinct perspectives of the same geographical\nlocation. However, existing CVGL approaches are typically restricted to a\nsingle view or modality, and their direct visual matching strategy lacks\ninterpretability: they merely predict whether two images correspond, without\nexplaining the rationale behind the match. In this paper, we present GLEAM-C, a\nfoundational CVGL model that unifies multiple views and modalities-including\nUAV imagery, street maps, panoramic views, and ground photographs-by aligning\nthem exclusively with satellite imagery. Our framework enhances training\nefficiency through optimized implementation while achieving accuracy comparable\nto prior modality-specific CVGL models through a two-phase training strategy.\nMoreover, to address the lack of interpretability in traditional CVGL methods,\nwe leverage the reasoning capabilities of multimodal large language models\n(MLLMs) to propose a new task, GLEAM-X, which combines cross-view\ncorrespondence prediction with explainable reasoning. To support this task, we\nconstruct a bilingual benchmark using GPT-4o and Doubao-1.5-Thinking-Vision-Pro\nto generate training and testing data. The test set is further refined through\ndetailed human revision, enabling systematic evaluation of explainable\ncross-view reasoning and advancing transparency and scalability in\ngeo-localization. Together, GLEAM-C and GLEAM-X form a comprehensive CVGL\npipeline that integrates multi-modal, multi-view alignment with interpretable\ncorrespondence analysis, unifying accurate cross-view matching with explainable\nreasoning and advancing Geo-Localization by enabling models to better Explain\nAnd Match. Code and datasets used in this work will be made publicly accessible\nat https://github.com/Lucky-Lance/GLEAM.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.07889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07889", "abs": "https://arxiv.org/abs/2509.07889", "authors": ["Chengyan Wu", "Yiqiang Cai", "Yufei Cheng", "Yun Xue"], "title": "From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing", "comment": "NLPCC 2025", "summary": "This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which\nfocuses on sentence-level gender bias detection and mitigation in Chinese. The\ntask aims to promote fairness and controllability in natural language\ngeneration by automatically detecting, classifying, and mitigating gender bias.\nTo address this challenge, we adopt a fine-tuning approach based on large\nlanguage models (LLMs), efficiently adapt to the bias detection task via\nLow-Rank Adaptation (LoRA). In terms of data processing, we construct a more\nbalanced training set to alleviate class imbalance and introduce heterogeneous\nsamples from multiple sources to enhance model generalization. For the\ndetection and classification sub-tasks, we employ a majority voting strategy\nthat integrates outputs from multiple expert models to boost performance.\nAdditionally, to improve bias generation detection and mitigation, we design a\nmulti-temperature sampling mechanism to capture potential variations in bias\nexpression styles. Experimental results demonstrate the effectiveness of our\napproach in bias detection, classification, and mitigation. Our method\nultimately achieves an average score of 47.90%, ranking fourth in the shared\ntask.", "AI": {"tldr": "这篇论文提出了针对汉语句子层面性别偏见检测和缓解的任务解决方案，使用大型语言模型的微调技术，并取得了较佳的实验结果。", "motivation": "该研究旨在通过自动检测、分类和缓解性别偏见来促进自然语言生成中的公平性和可控性。", "method": "该论文采用基于大型语言模型的微调方法，并使用低秩适配（LoRA）来高效适应偏见检测任务。在数据处理方面，构建了一个更平衡的训练集，并引入了来自多个来源的异构样本。针对检测和分类子任务，采用多数投票策略结合多个专家模型的输出以提升性能。为了改进偏见生成检测和缓解，设计了一种多温度采样机制。", "result": "实验结果显示该方法在偏见检测、分类和缓解方面有效，最终获得47.90%的平均分数，在共享任务中排名第四。", "conclusion": "实验结果证明了所提方法的有效性，尽管排名第四，但仍为性别偏见检测和缓解领域的进一步研究提供了有价值的参考。"}}
{"id": "2509.07455", "categories": ["cs.CV", "J.3"], "pdf": "https://arxiv.org/pdf/2509.07455", "abs": "https://arxiv.org/abs/2509.07455", "authors": ["Pooya Khosravi", "Kun Han", "Anthony T. Wu", "Arghavan Rezvani", "Zexin Feng", "Xiaohui Xie"], "title": "XOCT: Enhancing OCT to OCTA Translation via Cross-Dimensional Supervised Multi-Scale Feature Learning", "comment": "11 pages, 3 figures, Accepted to MICCAI 2025", "summary": "Optical Coherence Tomography Angiography (OCTA) and its derived en-face\nprojections provide high-resolution visualization of the retinal and choroidal\nvasculature, which is critical for the rapid and accurate diagnosis of retinal\ndiseases. However, acquiring high-quality OCTA images is challenging due to\nmotion sensitivity and the high costs associated with software modifications\nfor conventional OCT devices. Moreover, current deep learning methods for\nOCT-to-OCTA translation often overlook the vascular differences across retinal\nlayers and struggle to reconstruct the intricate, dense vascular details\nnecessary for reliable diagnosis. To overcome these limitations, we propose\nXOCT, a novel deep learning framework that integrates Cross-Dimensional\nSupervision (CDS) with a Multi-Scale Feature Fusion (MSFF) network for\nlayer-aware vascular reconstruction. Our CDS module leverages 2D layer-wise\nen-face projections, generated via segmentation-weighted z-axis averaging, as\nsupervisory signals to compel the network to learn distinct representations for\neach retinal layer through fine-grained, targeted guidance. Meanwhile, the MSFF\nmodule enhances vessel delineation through multi-scale feature extraction\ncombined with a channel reweighting strategy, effectively capturing vascular\ndetails at multiple spatial scales. Our experiments on the OCTA-500 dataset\ndemonstrate XOCT's improvements, especially for the en-face projections which\nare significant for clinical evaluation of retinal pathologies, underscoring\nits potential to enhance OCTA accessibility, reliability, and diagnostic value\nfor ophthalmic disease detection and monitoring. The code is available at\nhttps://github.com/uci-cbcl/XOCT.", "AI": {"tldr": "本文提出了XOCT框架，结合CDS和MSFF技术，显著提升了OCT到OCTA转换中的血管重建质量，增强了OCTA图像的临床应用价值。", "motivation": "为了克服当前光学相干断层扫描血管成像（OCTA）技术中图像质量获取难题，以及深度学习方法在OCT到OCTA转换过程中的局限性，尤其是血管重建中的细节捕捉问题。", "method": "本文提出了一种新颖的深度学习框架XOCT，它结合了跨维度监督（CDS）和多尺度特征融合（MSFF）网络，以实现层感知的血管重建。CDS模块利用通过分割加权Z轴平均生成的2D层面投影作为监督信号，旨在引导网络学习每层视网膜的特定表示。MSFF模块则通过多尺度特征提取结合通道重加权策略增强血管勾勒效果。", "result": "实验结果表明，XOCT在OCTA-500数据集上的表现优于现有技术，尤其在面向临床评估视网膜病理性变化的en-face投影重建方面，显著提升了OCTA图像的访问性、可靠性和诊断价值。", "conclusion": "XOCT框架提出的改进方法增强了OCTA在眼科疾病检测和监测中的实用性，提升了对视网膜血管结构在多个空间尺度上的细节表现。"}}
{"id": "2509.07908", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07908", "abs": "https://arxiv.org/abs/2509.07908", "authors": ["Donya Rooein", "Vilém Zouhar", "Debora Nozza", "Dirk Hovy"], "title": "Biased Tales: Cultural and Topic Bias in Generating Children's Stories", "comment": null, "summary": "Stories play a pivotal role in human communication, shaping beliefs and\nmorals, particularly in children. As parents increasingly rely on large\nlanguage models (LLMs) to craft bedtime stories, the presence of cultural and\ngender stereotypes in these narratives raises significant concerns. To address\nthis issue, we present Biased Tales, a comprehensive dataset designed to\nanalyze how biases influence protagonists' attributes and story elements in\nLLM-generated stories. Our analysis uncovers striking disparities. When the\nprotagonist is described as a girl (as compared to a boy), appearance-related\nattributes increase by 55.26%. Stories featuring non-Western children\ndisproportionately emphasize cultural heritage, tradition, and family themes\nfar more than those for Western children. Our findings highlight the role of\nsociocultural bias in making creative AI use more equitable and diverse.", "AI": {"tldr": "本研究建立了一个名为Biased Tales的数据集，用于分析大型语言模型生成的故事中的偏见对主人公属性和故事元素的影响，揭示了性别和文化背景对故事叙述的显著影响。", "motivation": "随着家长越来越多地依靠大型语言模型来创作睡前故事，故事中文化及性别刻板印象的存在引发了重要关切。", "method": "创建名为Biased Tales的数据集，用于分析讲大型语言模型生成的故事中，性别和文化背景对主人公属性和故事元素的影响。", "result": "分析发现，当主人公是女孩时，与其外貌相关的属性增加了55.26%；故事中涉及非西方儿童时，文化传承、传统和家庭主题的比例远高于西方儿童。", "conclusion": "研究结果强调，要使创意AI的使用更为公平和多样性，必须认识到社会文化偏见的作用。"}}
{"id": "2509.07456", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.07456", "abs": "https://arxiv.org/abs/2509.07456", "authors": ["Sai Siddhartha Chary Aylapuram", "Veeraraju Elluru", "Shivang Agarwal"], "title": "Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting", "comment": "Accepted for publication at ICCV 2025 UnMe workshop", "summary": "Deep neural networks often rely on spurious correlations in training data,\nleading to biased or unfair predictions in safety-critical domains such as\nmedicine and autonomous driving. While conventional bias mitigation typically\nrequires retraining from scratch or redesigning data pipelines, recent advances\nin machine unlearning provide a promising alternative for post-hoc model\ncorrection. In this work, we investigate \\textit{Bias-Aware Machine\nUnlearning}, a paradigm that selectively removes biased samples or feature\nrepresentations to mitigate diverse forms of bias in vision models. Building on\nprivacy-preserving unlearning techniques, we evaluate various strategies\nincluding Gradient Ascent, LoRA, and Teacher-Student distillation. Through\nempirical analysis on three benchmark datasets, CUB-200-2011 (pose bias),\nCIFAR-10 (synthetic patch bias), and CelebA (gender bias in smile detection),\nwe demonstrate that post-hoc unlearning can substantially reduce subgroup\ndisparities, with improvements in demographic parity of up to \\textbf{94.86\\%}\non CUB-200, \\textbf{30.28\\%} on CIFAR-10, and \\textbf{97.37\\%} on CelebA. These\ngains are achieved with minimal accuracy loss and with methods scoring an\naverage of 0.62 across the 3 settings on the joint evaluation of utility,\nfairness, quality, and privacy. Our findings establish machine unlearning as a\npractical framework for enhancing fairness in deployed vision systems without\nnecessitating full retraining.", "AI": {"tldr": "Investigates methods to mitigate bias in deep learning models after training by selectively removing biased samples or features. Demonstrates significant improvements in fairness across various datasets.", "motivation": "Deep neural networks may utilize spurious correlations from training data leading to biased and unfair predictions, especially in critical fields like medicine and autonomous driving.", "method": "Bias-Aware Machine Unlearning is explored, which removes biased samples or representations post-hoc using methods like Gradient Ascent, LoRA, and Teacher-Student distillation.", "result": "The method improves demographic parity significantly on CUB-200-2011, CIFAR-10, and CelebA datasets, with minimal accuracy loss.", "conclusion": "Bias-Aware Machine Unlearning is a practical approach to mitigate bias in deployed vision systems without the need for full retraining."}}
{"id": "2509.07925", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07925", "abs": "https://arxiv.org/abs/2509.07925", "authors": ["Tuo Wang", "Adithya Kulkarni", "Tyler Cody", "Peter A. Beling", "Yujun Yan", "Dawei Zhou"], "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models", "comment": "Accepted by EMNLP 2025", "summary": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ.", "AI": {"tldr": "本文提出了GENUINE，一种增强的多级不确定性估计框架，用于提高大型语言模型的可靠性，特别是在高风险应用中。通过使用依赖解析树和层次图汇聚，该方法显著提高了不确定性评估的效果。", "motivation": "现有的不确定性估计方法往往忽视了语义依赖关系，仅依赖于无法捕捉文本生成中结构关系的token级概率测量。为了增强高风险领域中大型语言模型的可靠性，提出了一种结构感知框架。", "method": "GENUINE: 使用依赖解析树和层次图汇聚来提高大型语言模型的不确定性量化，通过监督学习建模语义和结构关系，以改进置信度评估。", "result": "实验结果显示，与基于语义熵的方法相比，GENUINE的AUROC提高了最多29%，并降低了超过15%的校准误差，证明了基于图的不确定性建模的有效性。", "conclusion": "实验证明，GENUINE通过改进的不确定性量化方法，能够有效提升大型语言模型在自然语言处理任务中的表现，特别是在可靠性和校准准确性方面。"}}
{"id": "2509.07472", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.07472", "abs": "https://arxiv.org/abs/2509.07472", "authors": ["Wenshuo Gao", "Xicheng Lan", "Shuai Yang"], "title": "ANYPORTAL: Zero-Shot Consistent Video Background Replacement", "comment": "8 pages, ICCV 2025, Website: https://gaowenshuo.github.io/AnyPortal/", "summary": "Despite the rapid advancements in video generation technology, creating\nhigh-quality videos that precisely align with user intentions remains a\nsignificant challenge. Existing methods often fail to achieve fine-grained\ncontrol over video details, limiting their practical applicability. We\nintroduce ANYPORTAL, a novel zero-shot framework for video background\nreplacement that leverages pre-trained diffusion models. Our framework\ncollaboratively integrates the temporal prior of video diffusion models with\nthe relighting capabilities of image diffusion models in a zero-shot setting.\nTo address the critical challenge of foreground consistency, we propose a\nRefinement Projection Algorithm, which enables pixel-level detail manipulation\nto ensure precise foreground preservation. ANYPORTAL is training-free and\novercomes the challenges of achieving foreground consistency and temporally\ncoherent relighting. Experimental results demonstrate that ANYPORTAL achieves\nhigh-quality results on consumer-grade GPUs, offering a practical and efficient\nsolution for video content creation and editing.", "AI": {"tldr": "The paper presents ANYPORTAL, a zero-shot video background replacement framework using pre-trained diffusion models with a focus on improving foreground consistency and achieving fine-grained control over video details.", "motivation": "The motivation behind this paper is to address the challenge of generating high-quality videos that precisely align with user intentions, particularly focusing on achieving fine-grained control over video details, which current methods struggle to accomplish.", "method": "The paper introduces ANYPORTAL, a zero-shot framework that utilizes pre-trained diffusion models for video background replacement, integrating temporal and relighting capabilities to ensure high-quality video generation. It includes a Refinement Projection Algorithm for foreground consistency and pixel-level detail manipulation.", "result": "The paper reports that ANYPORTAL achieves high-quality results on consumer-grade GPUs, providing a practical and efficient solution for video content creation and editing.", "conclusion": "The conclusion of the paper is that ANYPORTAL, being training-free, effectively solves the problems of foreground consistency and temporally coherent relighting in video generation, offering significant practical advantages for video editing and content creation tasks."}}
{"id": "2509.07968", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.07968", "abs": "https://arxiv.org/abs/2509.07968", "authors": ["Lukas Haas", "Gal Yona", "Giovanni D'Antonio", "Sasha Goldshtein", "Dipanjan Das"], "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge", "comment": null, "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.", "AI": {"tldr": "提出了SimpleQA Verified，一个更准确的基准用于评估LLM的短格式准确性，解决了以往基准中的问题，并由Gemini 2.5 Pro实现了新的SOTA结果。", "motivation": "该研究旨在提供一个更加精确和无偏差的基准测试工具，用于衡量大型语言模型的短格式事实准确性。", "method": "通过严格的多阶段过程创建了SimpleQA Verified，包括去重、主题平衡和来源整合。此外，还改进了自动评分提示。", "result": "该论文介绍了SimpleQA Verified，这是一个用于评估大规模语言模型（LLM）在短格式准确性方面的基准，包含1000个提示。它解决了OpenAI SimpleQA基准中存在的问题，如标签噪声、不正确的标签、主题偏差和问题冗余。SimpleQA Verified是通过严格的多阶段过滤过程创建的，包括去重、主题平衡和来源整合，以产生更可靠和更具挑战性的评估集，同时改进了自动评分提示。在这个新基准上，Gemini 2.5 Pro实现了55.6的F1分数，超过了其他前沿模型，包括GPT-5。这项工作为研究人员提供了一个更精确的工具，用于跟踪参数模型准确性的真正进展并减轻幻觉问题。基准数据集、评估代码和排行榜可以在https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified找到。", "conclusion": "Gemini 2.5 Pro在SimpleQA Verified基准上获得了优秀的F1分数，这表明SimpleQA Verified能提供一个更高保真度的工具，用以追踪参数模型事实准确性的真正进步，并减少幻觉问题。"}}
{"id": "2509.07477", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07477", "abs": "https://arxiv.org/abs/2509.07477", "authors": ["Patrick Wienholt", "Christiane Kuhl", "Jakob Nikolas Kather", "Sven Nebelung", "Daniel Truhn"], "title": "MedicalPatchNet: A Patch-Based Self-Explainable AI Architecture for Chest X-ray Classification", "comment": null, "summary": "Deep neural networks excel in radiological image classification but\nfrequently suffer from poor interpretability, limiting clinical acceptance. We\npresent MedicalPatchNet, an inherently self-explainable architecture for chest\nX-ray classification that transparently attributes decisions to distinct image\nregions. MedicalPatchNet splits images into non-overlapping patches,\nindependently classifies each patch, and aggregates predictions, enabling\nintuitive visualization of each patch's diagnostic contribution without\npost-hoc techniques. Trained on the CheXpert dataset (223,414 images),\nMedicalPatchNet matches the classification performance (AUROC 0.907 vs. 0.908)\nof EfficientNet-B0, while substantially improving interpretability:\nMedicalPatchNet demonstrates substantially improved interpretability with\nhigher pathology localization accuracy (mean hit-rate 0.485 vs. 0.376 with\nGrad-CAM) on the CheXlocalize dataset. By providing explicit, reliable\nexplanations accessible even to non-AI experts, MedicalPatchNet mitigates risks\nassociated with shortcut learning, thus improving clinical trust. Our model is\npublicly available with reproducible training and inference scripts and\ncontributes to safer, explainable AI-assisted diagnostics across medical\nimaging domains. We make the code publicly available:\nhttps://github.com/TruhnLab/MedicalPatchNet", "AI": {"tldr": "MedicalPatchNet是一个针对胸X光片分类的自解释深度学习架构，它分割图像成Patch进行独立分类，并聚合结果，从而提供了透明的决策解释能力和卓越的病灶定位准确性。", "motivation": "传统的深度神经网络在医学影像分类方面表现出色，但可解释性差。作者旨在开发一种自解释架构，提高临床接受度和交互性，特别是在胸X光片分类中。", "method": "MedicalPatchNet将医学影像分割为不重叠的Patch，分别对每个Patch进行分类，并将结果聚合以进行预测。此方法使模型能够直观地展示每个Patch对诊断的贡献，无需依赖后训练的解释技术。", "result": "MedicalPatchNet在CheXpert数据集上达到了与EfficientNet-B0相当的分类性能(AUROC 0.907 vs. 0.908)，同时在CheXlocalize数据集上表现出更高的病灶定位准确性(平均命中率0.485 vs. 0.376)，展示了显著的可解释性提升。", "conclusion": "论文结论表明，MedicalPatchNet通过提供易于理解和验证的解释，提高了临床诊断中的信任度，为医学影像领域的可解释AI辅助诊断做出了贡献，并公开了代码和可复现的实验脚本。"}}
