<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications](https://arxiv.org/abs/2510.21762)
*Eric Jeangirard*

Main category: cs.CL

> 基于833k段落构建了一个分类科学文献数据集，并公开发布于HuggingFace，有利于文本分类和命名实体识别系统的开发。

<details>
  <summary>Details</summary>

**Motivation:** 此数据集的目的是提供一种资源，以促进有关科学文献的数据分类和命名实体识别的研究。

**Method:** 我们通过从CC-BY许可的科学出版物中提取833K段落，并使用fastText进行语言识别以及使用OpenAlex进行科学领域标注，构建了一个数据集。该数据集主要用于训练文本分类模型和开发针对科学文献的命名实体识别系统。

**Result:** 构建了一个包含833k段落的多语言、多领域的科学文献数据集，为后续研究提供了基础资源。

**Conclusion:** 该数据集公开发布在HuggingFace上，数据集中段落的分类和标注有利于后续的研究工作，特别是在文本分类和命名实体识别方面。

**Abstract:** We present a dataset of 833k paragraphs extracted from CC-BY licensed
scientific publications, classified into four categories: acknowledgments, data
mentions, software/code mentions, and clinical trial mentions. The paragraphs
are primarily in English and French, with additional European languages
represented. Each paragraph is annotated with language identification (using
fastText) and scientific domain (from OpenAlex). This dataset, derived from the
French Open Science Monitor corpus and processed using GROBID, enables training
of text classification models and development of named entity recognition
systems for scientific literature mining. The dataset is publicly available on
HuggingFace https://doi.org/10.57967/hf/6679 under a CC-BY license.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models](https://arxiv.org/abs/2510.21740)
*Alexa R. Tartaglini,Satchel Grant,Daniel Wurgaft,Christopher Potts,Judith E. Fan*

Main category: cs.CV

> 研究发现，一些模型在生成单个数据点的坐标时存在问题，这些问题常常导致最终错误响应。虽然模型在生成错误响应时，可以从视觉编码器的隐藏表示中成功读取出正确的坐标，但这些错误通常源于视觉和语言模块间的交接问题。精调模型以解决统计关系的性能提升有限，表明现有的VLM架构限制了对数据可视化的可靠理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前视觉语言模型在处理基础的数据可视化理解任务时表现不佳，但其失败的原因尚不明确，目的是确定失败是否来自于数据可视化中视觉信息编码的局限，视觉模块与自然语言模块间信息传输的问题，还是自然语言模块内的信息处理问题。

**Method:** 提出了FUGU，一套用于精确表征数据可视化理解任务潜在难度的测试套件，包括数据点的位置、间距和总结统计数据等任务。通过多种提示策略，采用激活修补技术和线性探针技术诊断现有VLM模型错误的来源。

**Result:** 研究发现在模型中正确地提供坐标可以显著提高性能。但是当任务需要提取大量数据点间的统计关系时，提供正确的坐标反而会降低性能。即使是通过对FUGU进行微调，模型也没有达到理想的性能表现。

**Conclusion:** 这些发现表明现有的视觉语言模型在处理数据可视化时存在固有的架构限制，这对可靠的数据可视化理解构成了重大挑战。

**Abstract:** Data visualizations are vital components of many scientific articles and news
stories. Current vision-language models (VLMs) still struggle on basic data
visualization understanding tasks, but the causes of failure remain unclear.
Are VLM failures attributable to limitations in how visual information in the
data visualization is encoded, how information is transferred between the
vision and language modules, or how information is processed within the
language module? We developed FUGU, a suite of data visualization understanding
tasks, to precisely characterize potential sources of difficulty (e.g.,
extracting the position of data points, distances between them, and other
summary statistics). We used FUGU to investigate three widely used VLMs. To
diagnose the sources of errors produced by these models, we used activation
patching and linear probes to trace information flow through models across a
variety of prompting strategies. We found that some models fail to generate the
coordinates of individual data points correctly, and these initial errors often
lead to erroneous final responses. When these models are provided with the
correct coordinates, performance improves substantially. Moreover, even when
the model generates an incorrect response, the correct coordinates can be
successfully read out from the latent representations in the vision encoder,
suggesting that the source of these errors lies in the vision-language handoff.
We further found that while providing correct coordinates helps with tasks
involving one or a small number of data points, it generally worsens
performance for tasks that require extracting statistical relationships across
many data points. Fine-tuning models on FUGU also fails to yield ceiling
performance. These findings point to architectural constraints in current VLMs
that might pose significant challenges for reliable data visualization
understanding.

</details>


### [3] [Agro-Consensus: Semantic Self-Consistency in Vision-Language Models for Crop Disease Management in Developing Countries](https://arxiv.org/abs/2510.21757)
*Mihir Gupta,Pratik Desai,Ross Greer*

Main category: cs.CV

> 本文提出了一种成本效益高的自我一致性框架，用于改善农业图像字幕的视觉语言模型(VLM)的可靠性，特别针对发展中国家的农业疾病管理问题。

<details>
  <summary>Details</summary>

**Motivation:** 针对发展中国家由于缺乏植物病理专家、不可靠的互联网连接和成本限制，难以部署大规模AI系统，本文旨在提供一种经济有效的解决方案，以提高农业病害图像字幕的准确性。

**Method:** 该方法利用语义聚类，通过一个轻量级的预训练嵌入模型（80MB）对多个候选答案进行分组，并通过基于余弦相似性的共识机制选择最连贯的诊断报告。同时还结合了人类介入的组件，以提高输入质量。

**Result:** 使用3B参数的PaliGemma模型微调并在PlantVillage公开数据集上应用该框架，结果显示单簇共识方法的峰值准确率为83.1%，而采用多簇共识机制时，准确率进一步提高到94.0%。

**Conclusion:** 这种方法展示了在成本效益和可靠性上优于传统解码方法，对于提高发展中国家农业病害管理的水平具有重要意义。

**Abstract:** Agricultural disease management in developing countries such as India, Kenya,
and Nigeria faces significant challenges due to limited access to expert plant
pathologists, unreliable internet connectivity, and cost constraints that
hinder the deployment of large-scale AI systems. This work introduces a
cost-effective self-consistency framework to improve vision-language model
(VLM) reliability for agricultural image captioning. The proposed method
employs semantic clustering, using a lightweight (80MB) pre-trained embedding
model to group multiple candidate responses. It then selects the most coherent
caption -- containing a diagnosis, symptoms, analysis, treatment, and
prevention recommendations -- through a cosine similarity-based consensus. A
practical human-in-the-loop (HITL) component is incorporated, wherein user
confirmation of the crop type filters erroneous generations, ensuring
higher-quality input for the consensus mechanism. Applied to the publicly
available PlantVillage dataset using a fine-tuned 3B-parameter PaliGemma model,
our framework demonstrates improvements over standard decoding methods.
Evaluated on 800 crop disease images with up to 21 generations per image, our
single-cluster consensus method achieves a peak accuracy of 83.1% with 10
candidate generations, compared to the 77.5% baseline accuracy of greedy
decoding. The framework's effectiveness is further demonstrated when
considering multiple clusters; accuracy rises to 94.0% when a correct response
is found within any of the top four candidate clusters, outperforming the 88.5%
achieved by a top-4 selection from the baseline.

</details>


### [4] [Proportion and Perspective Control for Flow-Based Image Generation](https://arxiv.org/abs/2510.21763)
*Julien Boudier,Hugo Caselles-Dupré*

Main category: cs.CV

> 研究专注于改善文本到图像生成模型在图像的空间与几何结构控制效果，通过引入比例和透视ControlNets模块实现更精确的图像生成控制。

<details>
  <summary>Details</summary>

**Motivation:** 现代文本到图像的扩散模型虽然生成了高保真度的图像，但是对生成图像的空间和几何结构的控制有限。

**Method:** 引入并评估了两种专门用于艺术控制的ControlNets：（1）比例ControlNet，它使用边界框来控制对象的位置和大小；（2）透视ControlNet，它利用消失线来控制场景的三维几何结构。

**Result:** 实验表明，这两种模块提供了有效的控制手段，但对复杂条件的处理能力有限。

**Conclusion:** 两种模块都提供了有效控制，但在处理复杂约束方面表现出局限性。

**Abstract:** While modern text-to-image diffusion models generate high-fidelity images,
they offer limited control over the spatial and geometric structure of the
output. To address this, we introduce and evaluate two ControlNets specialized
for artistic control: (1) a proportion ControlNet that uses bounding boxes to
dictate the position and scale of objects, and (2) a perspective ControlNet
that employs vanishing lines to control the 3D geometry of the scene. We
support the training of these modules with data pipelines that leverage
vision-language models for annotation and specialized algorithms for
conditioning image synthesis. Our experiments demonstrate that both modules
provide effective control but exhibit limitations with complex constraints.
Both models are released on HuggingFace:
https://huggingface.co/obvious-research

</details>


### [5] [H2OFlow: Grounding Human-Object Affordances with 3D Generative Models and Dense Diffused Flows](https://arxiv.org/abs/2510.21769)
*Harry Zhang,Luca Carlone*

Main category: cs.CV

> H2OFlow通过合成数据学习全面的3D人-物互动属性，包括接触、方向和空间占用。这种方法避免了手工标注数据的需求，并且在真实物体上的表现优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前的方法依赖于耗时且成本高昂的手工标记数据集来捕捉真实世界或模拟的人-物互动任务，而且大多数现有的3D人-物互动理解方法仅限于基于接触的分析，忽略了一些重要方面，例如人体对物体的偏好方位和空间占用。

**Method:** H2OFlow, 一种新的框架，该框架使用仅由3D生成模型生成的合成数据全面学习3D人-物互动属性，包括接触、方向和空间占用。H2OFlow使用一种基于密集3D流的表示方法，通过在点云上操作的密集扩散过程学习此流。

**Result:** 通过广泛的定量和定性评估，展示了H2OFlow在学习3D人-物互动属性方面不仅有效泛化到现实世界的物体，而且在建模方面超越了依赖于手动注释或网格表示的方法。

**Conclusion:** 该研究提出了一种新的3D人-物互动理解框架H2OFlow，它通过使用仅由3D生成模型生成的合成数据，解决了现有方法对数据的过度依赖及在互动属性上的局限性，并展示了其在真实物体上的有效性和优越性。

**Abstract:** Understanding how humans interact with the surrounding environment, and
specifically reasoning about object interactions and affordances, is a critical
challenge in computer vision, robotics, and AI. Current approaches often depend
on labor-intensive, hand-labeled datasets capturing real-world or simulated
human-object interaction (HOI) tasks, which are costly and time-consuming to
produce. Furthermore, most existing methods for 3D affordance understanding are
limited to contact-based analysis, neglecting other essential aspects of
human-object interactions, such as orientation (\eg, humans might have a
preferential orientation with respect certain objects, such as a TV) and
spatial occupancy (\eg, humans are more likely to occupy certain regions around
an object, like the front of a microwave rather than its back). To address
these limitations, we introduce \emph{H2OFlow}, a novel framework that
comprehensively learns 3D HOI affordances -- encompassing contact, orientation,
and spatial occupancy -- using only synthetic data generated from 3D generative
models. H2OFlow employs a dense 3D-flow-based representation, learned through a
dense diffusion process operating on point clouds. This learned flow enables
the discovery of rich 3D affordances without the need for human annotations.
Through extensive quantitative and qualitative evaluations, we demonstrate that
H2OFlow generalizes effectively to real-world objects and surpasses prior
methods that rely on manual annotations or mesh-based representations in
modeling 3D affordance.

</details>
