<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

> 尽管大型语言模型（LLMs）在许多任务上取得了进展，但这项研究表明，对于对话摘要任务，明确的分步推理并不总是带来更好的效果。研究对多种语言的对话进行了评估，指出分步推理模型容易出现冗长和不够简洁的摘要。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型（LLMs）在摘要任务上取得了显著进展，但专门针对对话场景所需的同时抽象性和简洁性之间的平衡，长期链式思考（CoT）实现的分步推理架构表现如何，尚未得到充分探索。因此，本研究旨在探讨这一方面的局限性。

**Method:** 该研究对现有推理LLM和非推理LLM在通用、角色导向和查询导向的对话摘要范式进行了全面和系统的评估，涵盖了多种语言、领域和摘要长度。研究使用了强基准测试（如SAMSum, DialogSum, CSDS, 和QMSum）和高级评估协议，包括基于LLM的自动指标和人类启发的标准。

**Result:** 该研究发现，与其他需要大量推理的任务趋势相反，明确的分步推理并未一致提高对话摘要的质量。相反，推理LLM往往存在冗长、事实不一致和非简洁的问题。

**Conclusion:** 这项工作揭示了当前推理LLM在复杂对话上下文中的局限性，并强调了需要有针对性地进行建模和评估策略来提高现实世界对话摘要的准确性和简洁性。

**Abstract:** Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

> 研究Huginn-3.5B模型在算术任务中的内在链式思考能力，发现有但证据有限。

<details>
  <summary>Details</summary>

**Motivation:** 探索在不增加参数量的情况下,通过推理时重复使用层级能否在Huginn-3.5B中生成可解释的内在链式思考。

**Method:** 通过探针技术套件如Logit Lens和Coda Lens探究Huginn-3.5B在算术任务上的内部行为。

**Result:** 研究表明，在递归块之间存在探测不一致性，隐含状态的可解释性依赖于层级索引和解码方法。此外，追踪最终和中间结果标记的排名轨迹显示了内部可解释链式思考的有限证据。

**Conclusion:** 发现深度递增对内在链式思考能力的提升有限，仍未达到显式外部化推理步骤模型的表现。

**Abstract:** Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

> GDC Cohort Copilot is a tool that automatically generates GDC cohort filters based on natural language input, offering better outcomes with its locally-served LLM than with GPT-4.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind GDC Cohort Copilot is to address the difficulty users face in creating specific cohorts through GDC's Cohort Builder by allowing them to describe desired cohorts in natural language.

**Method:** The paper develops and evaluates multiple large language models (LLMs) to generate GDC cohort filters from user-provided natural language descriptions, focusing on a locally-served, open-source GDC Cohort LLM.

**Result:** The locally-served, open-source GDC Cohort LLM demonstrates superior performance in generating GDC cohorts compared to GPT-4 prompting.

**Conclusion:** GDC Cohort Copilot is made available as an open-source tool, demonstrating the effectiveness of local LLMs over generic models like GPT-4 for generating GDC cohorts.

**Abstract:** Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

> 我们通过MemAgent解决了长文本处理中的性能瓶颈问题，并在大规模测试中表现良好。

<details>
  <summary>Details</summary>

**Motivation:** 尽管长度外推、高效注意和记忆模块有所改进，但在长文本处理中，如何以线性复杂度处理无限长度文档而不降低外推性能的挑战依然存在。我们的动机是解决这一问题。

**Method:** 我们提出了名为MemAgent的新代理工作流，该工作流以段落为单位读取文本并通过覆盖策略更新记忆。同时，我们扩展了DAPO算法，以通过独立上下文多对话生成来促进训练。

**Result:** MemAgent展示了卓越的长上下文能力，能够从8K上下文扩展到32K文本训练，并在3.5M QA任务中表现性能损失<5%，同时在512K RULER测试中达到95%+。

**Conclusion:** MemAgent能够有效地处理长文本，并在保持高性能的同时实现了良好的扩展性。这为长文本任务中的高效处理开辟了新的途径。

**Abstract:** Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

> 本文提出了一种名为DoMIX的新方法，该方法利用LoRA模块解决了现有连续领域自适应预训练方法中存在的问题。DoMIX方法能够实现对不同领域数据的高效并行预训练，适用于特定任务并可扩展到标准的大型语言模型微调场景。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现有的连续领域自适应预训练方法（continual DAP）面临的问题，包括高昂的计算成本和GPU内存使用、对增量数据顺序的敏感性、以及未能为最终任务提供特定模型的普遍性模型，作者提出了DoMIX方法。

**Method:** 通过利用LoRA模块，一种代表性的参数高效微调(PEFT)方法，DoMIX能够实现对不同领域数据的高效并行预训练，并且对领域顺序具有鲁棒性，同时有效利用积累的知识为特定任务提供定制化的预训练模型。

**Result:** 该方法不仅解决了现有问题，还证明了其可以扩展到标准的LLM微调场景。

**Conclusion:** DoMIX通过利用LoRA模块在有效管理计算成本和内存的同时，解决了现有连续领域自适应预训练方法中的问题，提供了一种高效并行预训练方法，适用于特定任务。该方法还证明了其扩展性，不仅限于DAP设置，还可应用于标准的大型语言模型微调场景。

**Abstract:** Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

> 本文描述了一个用于SciVQA 2025共享任务的系统，该任务涉及科学视觉问答。该系统利用双多模态大模型组合和少量样本检索策略。在盲测中排名第三，平均F1评分为85.12。

<details>
  <summary>Details</summary>

**Motivation:** 旨在参与SciVQA 2025共享任务的挑战，特别是在科学视觉问答领域，希望通过结合多种技术和策略来提高回答的准确性和可靠性。

**Method:** 该系统采用两个多模态大型语言模型的组合，并结合不同的少量样本检索策略。模型和少量样本设置的选择基于图形和问题类型。同时，答案的选择基于模型的信心水平。

**Result:** 该系统在盲测中排名第三，平均F1评分为85.12，此评分涵盖了ROUGE-1、ROUGE-L和BERTS三项评估指标。

**Conclusion:** 系统展示了多模态大型语言模型结合适当策略处理科学视觉问题的有效性。源代码已公开。

**Abstract:** This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

> 本文提出在BERT中用PQC代替FFN模块，降低了参数量且提高了数据效率，实验结果表明其能够在少量样本学习场景中表现出优越性。

<details>
  <summary>Details</summary>

**Motivation:** 考虑到BERT中的FFN模块贡献了标准Transformer编码器块中大约三分之二的参数，本文旨在通过替换FFN模块来降低参数量，并探讨PQC深度、表达能力与可训练性的权衡。

**Method:** 本文介绍了一种量子-经典混合的变压器模型QFFN-BERT，该模型用参数化量子电路（PQC）层替换了紧凑型BERT变体中的前馈神经网络（FFN）模块。PQC层包括剩余连接、$R_Y$和$R_Z$旋转以及交替纠缠策略，以确保稳定训练和高表达能力。

**Result:** 实验结果：1) 在全量数据集上，优化配置的QFFN-BERT可以达到基线准确率的102.0%，同时减少了超过99%的FFN特定参数。2) 在少量样本学习场景中，模型表现出了持续和竞争性的优势，证实了其在数据效率方面的潜力。

**Conclusion:** 实验及消融研究确认，当与基础深度学习原则协同设计时，PQC可以作为功能强大且参数高效的经典FFN的替代品。

**Abstract:** Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

> A parametric model for selecting high-quality code data is introduced to improve training efficiency and model performance, achieving better results with fewer samples.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of current methods focusing on data quantity while often overlooking data quality, thereby reducing training efficiency.

**Method:** An approach that utilizes a parametric model for code data selection, aimed at improving both training efficiency and model performance.

**Result:** Experimental results demonstrate that using only 10K samples, our method achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled baseline, outperforming other sampling approaches in both performance and efficiency.

**Conclusion:** This underscores that our method effectively boosts model performance while significantly reducing computational costs.

**Abstract:** Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

> 本研究评估了基于变压器架构的七种阿坎语ASR模型在不同类型的语音数据上的表现，发现模型在与训练域不匹配的情况下准确率显著下降。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有的自动语音识别（ASR）研究在使用域内数据集评估模型性能时，很少考虑这些模型如何泛化到不同的语音环境中。这项研究填补了这一知识空白。

**Method:** 本研究通过使用四个阿坎语语料库（包含文化相关的图像描述、非正式对话、圣经经文朗诵和即兴金融对话）来评估七种基于变压器架构的ASR模型（如Whisper和Wav2Vec2）的性能。

**Result:** 研究结果表明ASR模型在不同领域的性能存在很大差异，它们仅在训练领域表现最佳，而在不匹配的场景中准确率显著下降。

**Conclusion:** 研究结果揭示了为LRLs（包括阿坎语）研发针对域适应技术、自适应路由策略和多语言训练框架的需求。此外，该研究指出在选择ASR模型架构时需要权衡错误输出的可读性和透明度。

**Abstract:** Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

> 该研究通过开发社区驱动的数据收集最佳实践和培训，构建了针对低资源语言阿肯语的首份公开源代码的失常语音数据集，并展示了用这些数据训练的ASR模型初版结果。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在使ASR技术及数据收集民主化，并特别针对资源较少的语言构建ASR模型。

**Method:** 该研究开发了一套“烹饪书”，包含最佳实践和培训，以实现由社区驱动的数据收集和ASR模型构建。作为概念验证，研究整理了第一份公开源代码的阿肯语（加纳广泛使用的土著语言）的失常语音数据集。

**Result:** 研究整理的数据集，连同烹饪书和开源工具，均公开可用，以促进研究人员和实践者开发适用于失常语音个体的独特需求的包容性ASR技术。

**Conclusion:** 初步结果显示，基于开源ASR模型对阿肯语失常语音的微调改善了识别性能。

**Abstract:** This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

> A new dataset called IndianBailJudgments-1200 has been developed to address the lack of structured legal data in India. It includes 1200 bail judgments annotated with over 20 attributes, and it supports several legal NLP tasks.

<details>
  <summary>Details</summary>

**Motivation:** Legal NLP is underdeveloped in regions like India due to the scarcity of structured datasets. This paper aims to address this issue by creating a new benchmark dataset focused on Indian bail jurisprudence.

**Method:** We introduce IndianBailJudgments-1200, a new benchmark dataset comprising 1200 Indian court judgments on bail decisions, annotated across 20+ attributes. Annotations were generated using a prompt-engineered GPT-4 pipeline and verified for consistency.

**Result:** The resource supports a wide range of legal NLP tasks such as outcome prediction, summarization, and fairness analysis.

**Conclusion:** This dataset is the first publicly available resource focused specifically on Indian bail jurisprudence.

**Abstract:** Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

> 本文综述了使用LLMs从视频中检测碰撞的方法，提出了融合策略的分类，总结了关键数据集和模型架构，比较了性能基准，并讨论了挑战和机会，为这一快速发展的领域奠定了基础。

<details>
  <summary>Details</summary>

**Motivation:** 随着大语言模型(LLMs)和视觉语言模型(VLMs)的发展，处理、推理和总结多模态信息的方式发生了转变。本文旨在解决智能交通系统中的碰撞检测问题。

**Method:** 本文综述了利用大语言模型(LLMs)从视频数据中进行碰撞检测的方法，提出了融合策略的结构化分类，总结了关键数据集，分析了模型架构，并比较了性能基准。

**Result:** 本文比较了性能基准，讨论了正在进行的挑战和机会，但没有提供具体的实验结果。

**Conclusion:** 本文综述了视频理解和基础模型的交叉领域中的关键进展，并指出了未来的研究方向。

**Abstract:** Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [14] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

> 在水下环境中测试了几种单目深度估计模型后，该研究指出那些在空气环境中有效的大型模型在水下表现不佳。研究中介绍了一种细调后的深度模型，它在水下环境中表现优异，改进了在水下障碍环境中的深度估计性能。

<details>
  <summary>Details</summary>

**Motivation:** 单目深度估计在空中环境中的有效性和在水下环境中由于光衰减、散射、色偏差和浑浊等因素而受到限制，特别是缺乏高质量的地面真实数据，使得评估水下环境中的单目深度估计的性能至关重要。

**Method:** 研究中采用全面的基准测试方法，评估了零样本和微调的单目度量深度估计模型在带有度量深度标注的真实世界水下数据集上的性能。特别是使用了经过物理基础模型生成的合成水下版本的Hypersim数据集对Depth Anything V2模型进行微调。

**Result:** 该论文通过在真实的水下数据集FLSea和SQUID上评估几种先进的单目深度估计模型，指出了在空气环境下有效的大型模型在水下表现不佳的问题。为了改善这一情况，作者利用基于物理的水下图像生成模型对Hypersim数据集生成了合成水下版本，并用此对Depth Anything V2进行了微调。实验表明，经过微调的模型在各种基准上均表现出色，优于仅在无污染的Hypersim数据集上训练的模型。这项研究表明领域适应和尺度感知监督对于在挑战性的水下环境中实现稳健和通用的深度预测至关重要。

**Conclusion:** 研究表明，针对水下环境进行领域适应和采用尺度感知监督的重要性，这对实现未来研究中更为稳健和具有广泛适用性的度量深度预测至关重要。

**Abstract:** Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [15] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

> 提出了一种基于chain-of-thought推理的事件流场景文本识别框架（ESTR-CoT），具有更好的解释性和逻辑推理能力，并在三个基准数据集上进行了验证。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法在极低光照和快速运动等极端条件下性能有限，主要基于端到端的编码器-解码器框架或大规模语言模型以增强识别，但仍然受限于解释性和逻辑推理能力弱的问题。

**Method:** 采用视觉编码器EVA-CLIP（ViT-G/14）将输入事件流转换为tokens，并使用Llama tokenizer对给定的生成提示进行编码。通过Q-former将视觉tokens与预训练的大规模语言模型Vicuna-7B进行对齐，同时输出答案和推理过程（CoT）。

**Result:** 在三个事件流STR基准数据集（EventSTR，WordArt*，IC15*）上进行了广泛的实验，证实了所提出框架的有效性和可解释性。

**Conclusion:** 开发了大规模的CoT数据集，通过三阶段处理（生成、优化和专家验证）训练框架，为后续推理型大规模模型的发展奠定了坚实的数据基础。

**Abstract:** Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [16] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

> 提出一种新的零样本多模态方法，结合多种模态检测复合情感状态，结果表明该方法在检测复合情感时无需领域适应，效果与监督学习相当。

<details>
  <summary>Details</summary>

**Motivation:** 旨在检测由基本情感组合形成的复杂情感状态，与依赖特定任务训练数据的传统方法不同，这种方法采用零样本组件，无需领域适应即可捕获复合情感。

**Method:** 提出了一种结合六种异构模态的零样本多模态方法，这些模态包括静态和动态面部表情、场景和标签匹配、场景上下文、音频和文字。该方法使用CLIP为基础的标签匹配和Qwen-VL进行语义场景理解，并引入了多头概率融合（MHPF）模块以及复合表情变换模块，后者使用成对概率聚合（PPA）和特征相似性聚合（PFSA）方法生成可解释的复合情感输出。

**Result:** 该方法在AffWild2上实现了46.95%的F1值，在AFEW上为49.02%，在C-EXPR-DB上为34.85%，其效果与监督训练方法相当，表明了该方法的有效性。

**Conclusion:** 本研究开发的零样本复合情感识别方法被证明能够有效识别复合表情，为情感计算领域提供了一种新的有效方法。

**Abstract:** Compound Expression Recognition (CER), a subfield of affective computing,
aims to detect complex emotional states formed by combinations of basic
emotions. In this work, we present a novel zero-shot multimodal approach for
CER that combines six heterogeneous modalities into a single pipeline: static
and dynamic facial expressions, scene and label matching, scene context, audio,
and text. Unlike previous approaches relying on task-specific training data,
our approach uses zero-shot components, including Contrastive Language-Image
Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene
understanding. We further introduce a Multi-Head Probability Fusion (MHPF)
module that dynamically weights modality-specific predictions, followed by a
Compound Expressions (CE) transformation module that uses Pair-Wise Probability
Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods
to produce interpretable compound emotion outputs. Evaluated under multi-corpus
training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%
on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via
zero-shot testing, which is comparable to the results of supervised approaches
trained on target data. This demonstrates the effectiveness of the proposed
approach for capturing CE without domain adaptation. The source code is
publicly available.

</details>


### [17] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

> 引入由约145,000篇科学论文和1.14百万张图像组成的数据集SciGA-145k，用于GA的推荐和生成研究，同时提出两个GA任务及新的评估指标CAR。

<details>
  <summary>Details</summary>

**Motivation:** 图形摘要在科学论文中起着重要视觉传达作用。然而，目前对图形摘要的潜力提升方式仍不清楚，设计有效的图形摘要需要高级可视化技能，这也阻碍了其广泛应用。因此，构建了这个数据集来支持研究在自动化图形摘要生成方面。

**Method:** 通过构建名为SciGA-145k的大规模数据集来解决图形摘要(GA)选择和推荐的问题，并为自动化GA生成研究提供支持。数据集覆盖约145,000篇科学论文以及1.14百万张图片。同时，为GA设计支持制定了两个任务：1) 通过在给定论文中识别适合用作GA的图片，来实现GA内部推荐。2) 从其它论文中检索GA，以启发新的GA设计。还提供这些任务的基本模型，并提出一个新的推荐指标Confidence Adjusted top-1 ground truth Ratio (CAR)，来弥补传统排名度量指标的不足。

**Result:** 成功构造了包含约145,000篇科学论文和1.14百万张图像的SciGA-145k数据集，并提供了一些任务和推荐指标，为GA生成的自动化研究提供了支持。通过提出的评估指标CAR，可以更细致地分析模型行为。

**Conclusion:** 通过统一这些任务和度量，SciGA-145k奠定了推动可视化科学交流的基础，同时为科学领域AI的发展做出了贡献。

**Abstract:** Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [18] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

> 研究了不同条件策略对合成数据质量的影响，发现基于提示和基于布局的条件策略各有优势，基于布局的条件策略在某些情况下可以使合成数据的性能比使用真实数据提高很多。

<details>
  <summary>Details</summary>

**Motivation:** 学习仅从少量图像中获得稳健的对象检测器是工业视觉系统中的一个关键挑战，因为高质量训练数据的收集可能需要数月时间。目前的方法依赖于3D引擎，渲染数据仍然需要数周时间，并且合成图像与真实图像之间存在较大的差距。

**Method:** 研究了从四个标准对象检测基准中抽取的八十种不同视觉概念，并比较了两种条件策略：基于提示的和基于布局的。当条件线索集较窄时，基于提示的条件生成更高质量的合成数据；当多样性增加，基于布局的条件效果更好。

**Result:** 当布局线索与完整的训练分布相匹配时，合成数据将平均精度提高了34%，在某些情况下甚至提高了177%，相比仅使用真实数据。

**Conclusion:** 在不同的条件策略下，合成数据的质量会有所不同。基于提示和基于布局的条件策略在合成数据生成中各有优势，可以根据条件线索的多样性选择合适的策略。

**Abstract:** Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulation and reality. Diffusion models promise a
step change because they can generate high quality images in minutes, yet
precise control, especially in low data regimes, remains difficult. Although
many adapters now extend diffusion beyond plain text prompts, the effect of
different conditioning schemes on synthetic data quality is poorly understood.
We study eighty diverse visual concepts drawn from four standard object
detection benchmarks and compare two conditioning strategies: prompt based and
layout based. When the set of conditioning cues is narrow, prompt conditioning
yields higher quality synthetic data; as diversity grows, layout conditioning
becomes superior. When layout cues match the full training distribution,
synthetic data raises mean average precision by an average of thirty four
percent and by as much as one hundred seventy seven percent compared with using
real data alone.

</details>


### [19] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

> DIDB-ViT是一个新型的二值视觉变压器，通过引入信息注意力模块、频率分解和改进的RPReLU激活函数，解决了二值化过程中性能下降和信息丢失的问题，并在多种ViT架构上达到了最优的图像分类和分割效果。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服现有的二值ViT方法性能下降严重或依赖全精度模块的问题，我们提出了一个新的二值ViT模型——DIDB-ViT。

**Method:** 我们设计了一个包含差异信息的信息注意力模块来缓解由二值化引起的性能下降，进一步引入改进的RPReLU激活函数来扩展模型的表达能力。此外，我们通过离散哈氏小波进行频率分解，以保留二值Q和K张量之间相似性计算的保真度。

**Result:** 实验结果显示，我们的DIDB-ViT在多种ViT架构中显著超越了当前最先进的网络量化方法，在图像分类和分割性能上表现出色。

**Conclusion:** DIDB-ViT通过设计信息注意力模块、频率分解和改进的RPReLU激活函数，有效提升了高频率信息的保留和计算精确度，从而在轻量和性能之间找到更好的平衡。

**Abstract:** The binarization of vision transformers (ViTs) offers a promising approach to
addressing the trade-off between high computational/storage demands and the
constraints of edge-device deployment. However, existing binary ViT methods
often suffer from severe performance degradation or rely heavily on
full-precision modules. To address these issues, we propose DIDB-ViT, a novel
binary ViT that is highly informative while maintaining the original ViT
architecture and computational efficiency. Specifically, we design an
informative attention module incorporating differential information to mitigate
information loss caused by binarization and enhance high-frequency retention.
To preserve the fidelity of the similarity calculations between binary Q and K
tensors, we apply frequency decomposition using the discrete Haar wavelet and
integrate similarities across different frequencies. Additionally, we introduce
an improved RPReLU activation function to restructure the activation
distribution, expanding the model's representational capacity. Experimental
results demonstrate that our DIDB-ViT significantly outperforms
state-of-the-art network quantization methods in multiple ViT architectures,
achieving superior image classification and segmentation performance.

</details>


### [20] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

> 该论文提出FMOcc方法，用于解决基于少数帧图像的3D占用预测问题，尤其改善了遮挡和远距离场景的预测。实验表明，该方法在Occ3D-nuScenes和OpenOcc数据集上优于现有技术。

<details>
  <summary>Details</summary>

**Motivation:** 当前3D占用预测方法依赖于融合历史帧数据以提高性能，这需要额外的数据和显著的计算资源。论文旨在解决使用少数帧图像进行预测时遇到的准确性问题，特别是在遮挡和远处场景的预测时。

**Method:** 该论文提出了一种名为FMOcc的新方法，该方法结合了Tri-perspective View (TPV) refinement occupancy网络与flow matching selective state space模型（FMSSM），专门用于解决基于少数帧图像的3D占用预测问题。FMOcc设计了一个基于流匹配模型的特性细化模块，用于产生丢失的特征，并引入了TPV SSM层和选择性平面SSM（PS3M）来提高整体效率和对远处场景的预测能力。此外，还设计了一种掩模训练（MT）方法以增强FMOcc的鲁棒性并应对传感器数据分析的不足。

**Result:** 在Occ3D-nuScenes和OpenOcc数据集上的实验结果表明，FMOcc优于现有的最先进技术。使用两帧输入时，在Occ3D-nuScenes验证集上获得43.1%的RayIoU和39.8%的mIoU，在OpenOcc上得到42.6%的RayIoU，使用5.4 G推理内存和330ms的推理时间。

**Conclusion:** 该研究成功展示了FMOcc在网络效率和远处场景预测能力方面的改进，给自动驾驶中的3D语义占用预测问题提供了一个新颖且有效的解决方案。

**Abstract:** 3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state space model for few-frame
3D occupancy prediction. Firstly, to generate missing features, we designed a
feature refinement module based on a flow matching model, which is called Flow
Matching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and
Plane Selective SSM (PS3M), we selectively filter TPV features to reduce the
impact of air voxels on non-air voxels, thereby enhancing the overall
efficiency of the model and prediction capability for distant scenes. Finally,
we design the Mask Training (MT) method to enhance the robustness of FMOcc and
address the issue of sensor data loss. Experimental results on the
Occ3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing
state-of-theart methods. Our FMOcc with two frame input achieves notable scores
of 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on
OpenOcc with 5.4 G inference memory and 330ms inference time.

</details>
