<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off](https://arxiv.org/abs/2509.02785)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Zimeng Huang,Xiaofei Sun,Jian Wang,Chengpei Tang,Keze Wang*

Main category: cs.CL

> DrDiff is a novel, efficient long-text generation framework overcoming existing efficiency-quality trade-offs with advanced scheduling, attention, and optimization mechanisms.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is to overcome the efficiency-quality trade-off in long-text generation by improving both computational efficiency and model performance.

**Method:** This paper introduces DrDiff, which incorporates three core technologies: a dynamic expert scheduling mechanism, a Hierarchical Sparse Attention (HSA) mechanism, and a soft absorption guidance optimization strategy combined with DPM-solver++.

**Result:** Comprehensive experiments on various long-text generation benchmarks show that DrDiff outperforms existing state-of-the-art methods.

**Conclusion:** DrDiff, with its innovative technologies, sets a new standard for efficiency and quality in long-text generation, outpacing current state-of-the-art techniques.

**Abstract:** This paper introduces DrDiff, a novel framework for long-text generation that
overcomes the efficiency-quality trade-off through three core technologies.
First, we design a dynamic expert scheduling mechanism that intelligently
allocates computational resources during the diffusion process based on text
complexity, enabling more efficient handling of text generation tasks of
varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)
mechanism that adaptively adjusts attention patterns according to a variety of
input lengths, reducing computational complexity from O($n^2$) to O($n$) while
maintaining model performance. Finally, we propose a soft absorption guidance
optimization strategy that combines with DPM-solver++ to reduce diffusion
steps, significantly improving generation speed. Comprehensive experiments on
various long-text generation benchmarks demonstrate the superiority of our
DrDiff over the existing SOTA methods.

</details>


### [2] [SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR](https://arxiv.org/abs/2509.02830)
*Pu Wang,Shinji Watanabe,Hugo Van hamme*

Main category: cs.CL

> 本研究探索了低秩适应的各种变体在语音任务中的应用，并引入了一种新的SSVD微调方法，实现了高效的领域适应。

<details>
  <summary>Details</summary>

**Motivation:** 虽然低秩适应在语音应用程序中被广泛应用，但其先进变体主要为语言和视觉任务开发，且在语音任务上有验证不足的问题。本研究动机是解决该问题，推动语音领域高效适应技术的发展。

**Method:** 本研究全面整合并评估了低秩适应（LoRA）的各种先进变体，如VeRA、DoRA、PiSSA和SVFT，并首次将其应用于ESPnet语音任务中。此外，研究中引入了一种基于结构化奇异值分解（SSVD）的精细微调方法，该方法通过选择性地旋转与输入相关的奇异向量，同时固定与输出相关的奇异向量，以最小的可训练参数实现稳健的领域适应，有效提高效率。

**Result:** 所有方法均在领域转移的语音识别任务上进行了评估，包括儿童语音和方言变化，涵盖模型规模从0.1B到2B。

**Conclusion:** 所有实现都已发布在ESPnet中，以支持可重复研究和未来的工作。研究成果展示了SSVD方法与其他PEFT方法在语音领域适应方面的有效性和优势。

**Abstract:** Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for
adapting large foundation models. While low-rank adaptation (LoRA) is widely
used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,
PiSSA, and SVFT, are developed mainly for language and vision tasks, with
limited validation in speech. This work presents the first comprehensive
integration and benchmarking of these PEFT methods within ESPnet. We further
introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates
input-associated right singular vectors while keeping output-associated vectors
fixed to preserve semantic mappings. This design enables robust domain
adaptation with minimal trainable parameters and improved efficiency. We
evaluate all methods on domain-shifted speech recognition tasks, including
child speech and dialectal variation, across model scales from 0.1B to 2B. All
implementations are released in ESPnet to support reproducibility and future
work.

</details>


### [3] [Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models](https://arxiv.org/abs/2509.02834)
*Gustavo Bonil,João Gondim,Marina dos Santos,Simone Hashiguti,Helena Maia,Nadia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

> 本研究使用LLaMA 3.2-3B模型生成的葡萄牙语短篇小说，通过计算方法分析文本，并得出三种主要的叙事模式，揭示了文本中的隐性殖民结构和性别不平等，提出了一种新的综合分析方法。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探究大型语言模型在葡萄牙语短篇小说中对黑人和白人女性构建叙事的方式，并从中发现可能存在的隐性偏见和不平等。

**Method:** 本研究采用计算方法对2100篇文本进行分组，以选出适合定性分析的短故事。研究中使用了大型语言模型LLaMA 3.2-3B。

**Result:** 研究结果揭示了三种主要的叙事表现形式：社会克服、祖先神话化和个人自我实现。此外，结果显示，尽管文本在语法上连贯且看似中立，但它们实际上强化了对女性身体的殖民化结构描述，体现了历史上的不平等。

**Conclusion:** 本研究提出了一种结合机器学习技术和定性的人工话语分析的综合方法，以更好地理解文本中复杂的性别和种族叙事。

**Abstract:** This study investigates how large language models, in particular LLaMA
3.2-3B, construct narratives about Black and white women in short stories
generated in Portuguese. From 2100 texts, we applied computational methods to
group semantically similar stories, allowing a selection for qualitative
analysis. Three main discursive representations emerge: social overcoming,
ancestral mythification and subjective self-realization. The analysis uncovers
how grammatically coherent, seemingly neutral texts materialize a crystallized,
colonially structured framing of the female body, reinforcing historical
inequalities. The study proposes an integrated approach, that combines machine
learning techniques with qualitative, manual discourse analysis.

</details>


### [4] [IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations](https://arxiv.org/abs/2509.02855)
*Hyunji Nam,Lucia Langlois,James Malamut,Mei Tan,Dorottya Demszky*

Main category: cs.CL

> 研究提出并评估了IDEAlgin，这是一个用于评估LLM生成注释与人类专家生成注释之间相似性的基准测试范式。经验证明，与传统的度量方法相比，IDEAlgin能够更有效地评估LLM与专家的一致性，并提供了提升性能的方法。

<details>
  <summary>Details</summary>

**Motivation:** 大规模语言模型（LLMs）在开放式、解释性注释任务中的应用越来越广泛，这类任务包括研究人员的主题分析和教师对学生作业的反馈等。这些任务涉及需要基于特定目标（例如研究问题或教学目标）进行专家判断的自由文本注释。目前尚无有效、可扩展的方法来衡量LLM生成的注释与人类专家生成的注释之间的相似性。因此，研究提出了IDEAlgin作为衡量LLM生成注释与专家注释之间相似性的新基准。

**Method:** 提出了IDEAlgin这一基准测试范式，采用“三元组判断任务”的方式来捕捉专家的人类相似性评分，并评估了包括基于向量的方法（如主题模型和嵌入式方法）和利用LLM作为判断者的相似性度量。

**Result:** 实验结果表明，传统的词汇和基于向量的度量方法难以捕捉专家对相似性的微妙理解和评估，而经由IDEAlgin设计的LLM判断方式能够显著提高与专家判断的一致性，在两个实际的教育数据集上提高了一致性，提升了9-30个百分点。

**Conclusion:** 结果表明IDEAlgin作为一个评价大规模语言模型与开放式专家注释之间相似性的基准范式是切实可行的，其意在指导LLM在教育及其他领域的负责任使用。

**Abstract:** Large language models (LLMs) are increasingly applied to open-ended,
interpretive annotation tasks, such as thematic analysis by researchers or
generating feedback on student work by teachers. These tasks involve free-text
annotations requiring expert-level judgments grounded in specific objectives
(e.g., research questions or instructional goals). Evaluating whether
LLM-generated annotations align with those generated by expert humans is
challenging to do at scale, and currently, no validated, scalable measure of
similarity in ideas exists. In this paper, we (i) introduce the scalable
evaluation of interpretive annotation by LLMs as a critical and understudied
task, (ii) propose IDEAlgin, an intuitive benchmarking paradigm for capturing
expert similarity ratings via a "pick-the-odd-one-out" triplet judgment task,
and (iii) evaluate various similarity metrics, including vector-based ones
(topic models, embeddings) and LLM-as-a-judge via IDEAlgin, against these human
benchmarks. Applying this approach to two real-world educational datasets
(interpretive analysis and feedback generation), we find that vector-based
metrics largely fail to capture the nuanced dimensions of similarity meaningful
to experts. Prompting LLMs via IDEAlgin significantly improves alignment with
expert judgments (9-30% increase) compared to traditional lexical and
vector-based metrics. These results establish IDEAlgin as a promising paradigm
for evaluating LLMs against open-ended expert annotations at scale, informing
responsible deployment of LLMs in education and beyond.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model](https://arxiv.org/abs/2509.02659)
*Zilong Guo,Yi Luo,Long Sha,Dongxu Wang,Panqu Wang,Chenyang Xu,Yi Yang*

Main category: cs.CV

> 本研究展示将端到端架构设计与知识丰富的多模态视觉语言模型相结合，在仅使用单目摄像头的情况下，实现了驾驶任务上的出色表现，证明了基于视觉的驾驶方法的有效性和端到端驾驶任务的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管许多研究集中于使用模块化的深度神经网络构建端到端架构，但是否可以利用强大的大语言模型，尤其是多模态视觉语言模型，来提升驾驶任务的端到端性能，仍然是一个待解答的问题。

**Method:** 研究结合了端到端架构设计和知识丰富的多模态视觉语言模型以提升驾驶任务的表现。

**Result:** 该研究显示，仅使用单目摄像头的情况下，所提出的方法在排行榜上成为了最优的单摄像头解决方案。

**Conclusion:** 实验结果证明了基于视觉的驾驶方法的有效性以及多模态视觉语言模型应用于端到端驾驶任务的潜力。

**Abstract:** End-to-end autonomous driving has drawn tremendous attention recently. Many
works focus on using modular deep neural networks to construct the end-to-end
archi-tecture. However, whether using powerful large language models (LLM),
especially multi-modality Vision Language Models (VLM) could benefit the
end-to-end driving tasks remain a question. In our work, we demonstrate that
combining end-to-end architectural design and knowledgeable VLMs yield
impressive performance on the driving tasks. It is worth noting that our method
only uses a single camera and is the best camera-only solution across the
leaderboard, demonstrating the effectiveness of vision-based driving approach
and the potential for end-to-end driving tasks.

</details>


### [6] [PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?](https://arxiv.org/abs/2509.02807)
*Mennatullah Siam*

Main category: cs.CV

> 研究提出了一种专门针对视觉定位任务的运动中心探测技术，设计了运动中心基准测试MoCentric-Bench，并展示运动预测技术在基准测试中的高级性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前的基准测试存在不足，单一帧可以足够捕获运动描述表达，而不进行任何时间推理。为了解决这个问题，该研究提出了一种专门针对视觉定位任务的运动中心探测技术。

**Method:** 研究识别了现有基准测试的不足，引入了四种运动中心探测技术，设计了运动中心基准测试MoCentric-Bench，为此，还建立了强大的单图像基线，并探索了简单的运动中心适应技术。

**Result:** 基准测试MoCentric-Bench确保视觉多模态大语言模型评估侧重于利用运动和语言的交互作用，而不是受现有视觉定位数据集中强调的静态外观线索的影响。简单运动中心适应技术在MoCentric-Bench上提供了最先进的性能。

**Conclusion:** 该研究提出了一个运动中心基准测试MoCentric-Bench，挑战未来模型提高视频中的密集时空定位和像素级理解能力，并展示了在该基准上的顶级性能。

**Abstract:** Multi-modal large language models (MLLMs) have shown impressive
generalization across tasks using images and text modalities. While their
extension to video has enabled tasks such as video question answering and video
captioning, their pixel-level visual grounding abilities are less studied. In
this work, we raise the pertinent question of whether motion is used in
pixel-level visual grounding and whether video MLLMs can segment objects based
on natural language expressions describing their motion patterns. We identify
the shortcomings in the current benchmarks, where we show that a single frame
can often suffice for capturing the motion referring expression without any
temporal reasoning. To address this, we introduce four motion-centric probing
techniques, particularly designed for the visual grounding task, to study video
MLLMs' ability to identify true motion from a fake one and their ability to
grasp the motion order. Consequently, we provide a motion-centric benchmark,
MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging
the interaction between motion and language rather than being dominated by
static appearance cues emphasized in existing visual grounding datasets. We
further establish strong single-image baselines that are on par with or
outperform prior methods. Finally, we explore simple motion-centric adaptation
techniques that provide state-of-the-art performance on our MoCentric-Bench.
Our motion-centric benchmark, evaluation and findings challenge future models
to improve dense spatiotemporal grounding and pixel-level understanding within
videos. Code and datasets will be made publicly available at
https://github.com/MSiam/PixFoundation-2.0.git.

</details>


### [7] [Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach](https://arxiv.org/abs/2509.02851)
*Sadra Saremi,Amirhossein Ahmadkhan Kordbacheh*

Main category: cs.CV

> 研究提出了一种结合胶囊网络、图注意力机制、变压器模块和残差学习的多尺度深度学习架构，以提高LC25000数据集上结肠癌的分类性能，方法中的HG-TNet模型展示了超越标准架构的表现。

<details>
  <summary>Details</summary>

**Motivation:** 结肠癌是一种全球性的恶性癌症，早期检测对于预防其恶化至关重要。这项研究是为了开发一种新的架构来提高结肠癌分类的性能，以尽早准确检测结肠癌。

**Method:** 论文中提出的方法使用了称为HG-TNet的混合架构，该架构结合了变压器和卷积神经网络的优点，以捕获组织学图像中的多尺度特征。变压器分支通过卷积基础的补丁嵌入成块地分割图像，而CNN分支捕获细粒度的局部细节。这些不同的特征通过自监督旋转预测目标进行组合。

**Result:** 研究结果显示，该模型不仅在准确度或损失函数方面表现出色，而且通过利用胶囊网络来保持空间顺序，并识别每个元素是如何个别组合并形成整体结构的，也优于传统的架构。

**Conclusion:** 研究表明，提出的基于HG-TNet的新型混合架构在结肠病理图像分类任务上表现出卓越的性能，特别是在维持空间顺序和识别整体结构方面优于传统方法。

**Abstract:** Colon cancer also known as Colorectal cancer, is one of the most malignant
types of cancer worldwide. Early-stage detection of colon cancer is highly
crucial to prevent its deterioration. This research presents a hybrid
multi-scale deep learning architecture that synergizes capsule networks, graph
attention mechanisms, transformer modules, and residual learning to advance
colon cancer classification on the Lung and Colon Cancer Histopathological
Image Dataset (LC25000) dataset. The proposed model in this paper utilizes the
HG-TNet model that introduces a hybrid architecture that joins strength points
in transformers and convolutional neural networks to capture multi-scale
features in histopathological images. Mainly, a transformer branch extracts
global contextual bonds by partitioning the image into patches by
convolution-based patch embedding and then processing these patches through a
transformer encoder. Analogously, a dedicated CNN branch captures fine-grained,
local details through successive Incorporation these diverse features, combined
with a self-supervised rotation prediction objective, produce a robust
diagnostic representation that surpasses standard architectures in performance.
Results show better performance not only in accuracy or loss function but also
in these algorithms by utilizing capsule networks to preserve spatial orders
and realize how each element individually combines and forms whole structures.

</details>


### [8] [PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis](https://arxiv.org/abs/2509.02898)
*Armin Saadat,Nima Hashemi,Hooman Vaseli,Michael Y. Tsang,Christina Luong,Michiel Van de Panne,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

> 本研究开发了一种强化学习驱动的主动视频采集框架，用于主动脉瓣狭窄的诊断，实现了高效、可扩展和个性化的超声心动图评估，试验结果表明可在使用更少视频片段的情况下达到80.6%的分类准确率。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于超声心动图诊断在资源受限地区的可获得性受限以及即时护理超声（POCUS）受限于操作者经验和难以选择最相关的成像视图，本研究旨在提高主动特征采集在主动脉瓣狭窄诊断中的效率和可扩展性，使之更加个性化。

**Method:** 本研究提出了一种基于强化学习（RL）的主动视频采集框架，用于动态选择每个患者的最有信息量的超声心动图视图，从而优化准确性和效率，不同于依赖固定视频集的传统方法。

**Result:** 该方法在2,572名患者的数据上测试时，实现了80.6%的分类准确率，且只需使用常规采集47%的超声心动图视频，证明了主动特征采集在主动脉瓣狭窄诊断中的有效性。

**Conclusion:** 研究证明了主动特征采集方法在提高主动脉瓣狭窄诊断效率、可扩展性和个性化方面的潜力，实现了高效的超声心动图评估。

**Abstract:** Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of
the aortic valve, leading to impaired blood flow. Despite its high prevalence,
access to echocardiography (echo), the gold-standard diagnostic tool, is often
limited due to resource constraints, particularly in rural and underserved
areas. Point-of-care ultrasound (POCUS) offers a more accessible alternative
but is restricted by operator expertise and the challenge of selecting the most
relevant imaging views. To address this, we propose a reinforcement learning
(RL)-driven active video acquisition framework that dynamically selects each
patient's most informative echo videos. Unlike traditional methods that rely on
a fixed set of videos, our approach continuously evaluates whether additional
imaging is needed, optimizing both accuracy and efficiency. Tested on data from
2,572 patients, our method achieves 80.6% classification accuracy while using
only 47% of the echo videos compared to a full acquisition. These results
demonstrate the potential of active feature acquisition to enhance AS
diagnosis, making echocardiographic assessments more efficient, scalable, and
personalized. Our source code is available at:
https://github.com/Armin-Saadat/PRECISE-AS.

</details>


### [9] [LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research](https://arxiv.org/abs/2509.02902)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

> 文章介绍了一种名为LiGuard的开源软件框架，旨在为激光雷达数据的研究和处理提供标准化支持，减少重复工作并促进代码复用。

<details>
  <summary>Details</summary>

**Motivation:** 激光雷达自主移动和智能交通系统（ITS）的发展引起了越来越多的兴趣。然而，在研究激光雷达数据时，研究人员经常为特定的应用领域开发专门的代码，这导致了重复工作，并且数据、算法或研究重点的微小变化可能会迫使重大代码修订。因此，开发LiGuard框架来解决这些问题。

**Method:** 该论文提出了一种名为LiGuard的开源软件框架，用于支持激光雷达数据的处理。该框架提供了数据输入/输出（I/O）、预处理/后处理和常用算法的支持，允许用户以交互方式添加/移除/重新排序自定义算法，调整其参数，并可视化分类、检测、分割和跟踪任务的结果。此外，该框架将所有代码文件创建在结构化的目录中，便于研究人员分享整个项目或复用单个组件。

**Result:** 通过案例研究验证了LiGuard的有效性。

**Conclusion:** LiGuard软件框架通过提供标准化的支持以及便于代码复用的结构化目录系统，促进了激光雷达相关研究的效率和协作。

**Abstract:** There is a growing interest in the development of lidar-based autonomous
mobility and Intelligent Transportation Systems (ITS). To operate and research
on lidar data, researchers often develop code specific to application niche.
This approach leads to duplication of efforts across studies that, in many
cases, share multiple methodological steps such as data input/output (I/O),
pre/post processing, and common algorithms in multi-stage solutions. Moreover,
slight changes in data, algorithms, and/or research focus may force major
revisions in the code. To address these challenges, we present LiGuard, an
open-source software framework that allows researchers to: 1) rapidly develop
code for their lidar-based projects by providing built-in support for data I/O,
pre/post processing, and commonly used algorithms, 2) interactively
add/remove/reorder custom algorithms and adjust their parameters, and 3)
visualize results for classification, detection, segmentation, and tracking
tasks. Moreover, because it creates all the code files in structured
directories, it allows easy sharing of entire projects or even the individual
components to be reused by other researchers. The effectiveness of LiGuard is
demonstrated via case studies.

</details>


### [10] [PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems](https://arxiv.org/abs/2509.02903)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

> This paper presents a methodology for creating large-scale, high-quality synthetic datasets with High-Fidelity Digital Twins for LiDAR-based perception in ITS, providing a cost-effective solution to overcome the limitations in dataset creation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to circumvent the costly and time-consuming process of creating large-scale labeled real-world datasets for training deep learning models, which is necessary for LiDAR-based perception tasks, and to leverage the potential of Sim2Real learning.

**Method:** The paper introduces a methodology for creating scalable and high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs) for LiDAR-based perception systems in ITS.

**Result:** The result is a workflow that outlines the steps, tools, and best practices for generating synthetic environments with high fidelity to the real-world, enabling diverse and cost-effective dataset generation.

**Conclusion:** The conclusion is that the proposed methodology can create a reliable foundation for robust Sim2Real learning, allowing for scalable and efficient deployment of LiDAR-based perceptual systems in ITS.

**Abstract:** LiDAR-based perception in intelligent transportation systems (ITS), for tasks
such as object detection, tracking, and semantic and instance segmentation, is
predominantly solved by deep neural network models which often require
large-scale labeled datasets during training to achieve generalization.
However, creating these datasets is costly. time consuming and require human
labor before the datasets are ready for training models. This hinders
scalability of the LiDAR-based perception systems in ITS. Sim2Real learning
offers scalable alternative, however, its effectiveness is dependent on the
fidelity of the source simulation(s) to real-world, in terms of environment
structure, actor dynamics, and sensor emulations. In response, this paper
introduces a rigorous and reproducible methodology for creating large-scale,
high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs).
The proposed workflow outlines the steps, tools, and best practices for
digitally replicating real-world environments, encompassing static geometry
modeling, road infrastructure replication, and dynamic traffic scenario
generation. Leveraging open-source and readily available resources such as
satellite imagery and OpenStreetMap data, alongside specific sensor
configurations, this paper provides practical, detailed guidance for
constructing robust synthetic environments. These environments subsequently
facilitate scalable, cost-effective, and diverse dataset generation, forming a
reliable foundation for robust Sim2Real learning.

</details>


### [11] [High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception](https://arxiv.org/abs/2509.02904)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

> 本文提出了一种高保真数字孪生框架，来解决LiDAR感知中的仿真到现实世界性能差距问题。实验显示，模型在合成数据上的表现优于真实数据，性能提升显著。

<details>
  <summary>Details</summary>

**Motivation:** 由于在仿真中训练的感知模型通常在实际数据上表现不佳，因此本文旨在通过提出HiFi DT框架来解决Sim2Real间隙问题，从而提高基于LiDAR的感知（例如对象检测、跟踪、分割）在智能运输系统（ITS）中的性能。

**Method:** 本文提出了一个高保真数字孪生（HiFi DT）框架，它结合了现实世界的背景几何、车道级道路拓扑结构以及特定传感器的规格和放置位置。该框架旨在解决仿真到现实世界（Sim2Real）学习中的领域适应挑战，并提供了一种系统的方法来构建能够生成符合领域数据的仿真环境。

**Result:** 实验表明，在HiFi DT生成的合成数据上训练的模型比在真实数据上训练的等效模型表现更好，性能提高了4.8%。此外，通过使用包括Chamfer Distance、Maximum Mean Discrepancy、Earth Mover's Distance和Fr'echet Distance在内的多种度量方法，验证了HiFi DT显著降低了领域偏移，并提升了泛化能力。

**Conclusion:** 研究结果强调了数字孪生在使基于仿真的LiDAR感知在现实世界的ITS应用中更为可靠方面的重要作用。这些发现为通过仿真提升基于LiDAR的ITS感知性能提供了新的视角。

**Abstract:** Sim2Real domain transfer offers a cost-effective and scalable approach for
developing LiDAR-based perception (e.g., object detection, tracking,
segmentation) in Intelligent Transportation Systems (ITS). However, perception
models trained in simulation often under perform on real-world data due to
distributional shifts. To address this Sim2Real gap, this paper proposes a
high-fidelity digital twin (HiFi DT) framework that incorporates real-world
background geometry, lane-level road topology, and sensor-specific
specifications and placement. We formalize the domain adaptation challenge
underlying Sim2Real learning and present a systematic method for constructing
simulation environments that yield in-domain synthetic data. An off-the-shelf
3D object detector is trained on HiFi DT-generated synthetic data and evaluated
on real data. Our experiments show that the DT-trained model outperforms the
equivalent model trained on real data by 4.8%. To understand this gain, we
quantify distributional alignment between synthetic and real data using
multiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy
(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both
raw-input and latent-feature levels. Results demonstrate that HiFi DTs
substantially reduce domain shift and improve generalization across diverse
evaluation scenarios. These findings underscore the significant role of digital
twins in enabling reliable, simulation-based LiDAR perception for real-world
ITS applications.

</details>


### [12] [Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach](https://arxiv.org/abs/2509.02918)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.CV

> 本文提出了KG-DG，一种用于糖尿病视网膜病变分类的神经符号框架，通过融合视觉变换器和专家引导的符号推理以增强在未见领域上的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 在医疗影像领域，模型往往在数据分布变化时表现不佳。目的是通过结合视觉和符号处理，来提高模型的跨领域泛化能力。

**Method:** 框架使用临床病变本体和视网膜血管分割的结构化、基于规则的特征，并通过置信度加权集成策略与深度视觉表示相结合。通过最小化域嵌入之间的KL散度来实现跨单领域和多领域的泛化。

**Result:** 实验显示，在跨域设置下，准确率提高了5.2%，对比基线ViT模型提高了6%。通过融合全部神经符号达到最高准确率。

**Conclusion:** 研究表明，神经符号集成是构建临床鲁棒和领域不变的医疗AI系统的一个有前景的方法。

**Abstract:** Domain generalization remains a critical challenge in medical imaging, where
models trained on single sources often fail under real-world distribution
shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy
(DR) classification that integrates vision transformers with expert-guided
symbolic reasoning to enable robust generalization across unseen domains. Our
approach leverages clinical lesion ontologies through structured, rule-based
features and retinal vessel segmentation, fusing them with deep visual
representations via a confidence-weighted integration strategy. The framework
addresses both single-domain generalization (SDG) and multi-domain
generalization (MDG) by minimizing the KL divergence between domain embeddings,
thereby enforcing alignment of high-level clinical semantics. Extensive
experiments across four public datasets (APTOS, EyePACS, Messidor-1,
Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in
cross-domain settings and a 6% improvement over baseline ViT models. Notably,
our symbolic-only model achieves a 63.67% average accuracy in MDG, while the
complete neuro-symbolic integration achieves the highest accuracy compared to
existing published baselines and benchmarks in challenging SDG scenarios.
Ablation studies reveal that lesion-based features (84.65% accuracy)
substantially outperform purely neural approaches, confirming that symbolic
components act as effective regularizers beyond merely enhancing
interpretability. Our findings establish neuro-symbolic integration as a
promising paradigm for building clinically robust, and domain-invariant medical
AI systems.

</details>
