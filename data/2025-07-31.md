<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 30]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian](https://arxiv.org/abs/2507.22159)
*Vanessa Rebecca Wiyono,David Anugraha,Ayu Purwarianti,Genta Indra Winata*

Main category: cs.CL

> 本研究提出了IndoPref数据集，解决了印尼语在大规模语言模型偏好研究中的代表性不足问题，它是一个完全由人类撰写的、多领域的印尼语偏好数据集。

<details>
  <summary>Details</summary>

**Motivation:** 印尼语使用者超过2亿人，但在大型语言模型（LLM）的偏好研究中，该语言严重不足。现有的多语种数据集大多来源于英语翻译，导致缺乏文化与语言的真实性。本研究旨在填补这一空白。

**Method:** 本研究介绍了IndoPref，这是首个完全由人类撰写且多领域的印尼语偏好数据集，专门用于评估大型语言模型（LLM）生成文本的自然度和质量。所有标注均为原生印尼语，并使用Krippendorff's alpha评估了强的注释者间一致性。

**Result:** 研究对多个大型语言模型进行了基准测试，并评估了每个模型的输出质量。

**Conclusion:** IndoPref数据集为评估LLM在印尼语生成文本的自然度和质量提供了一个新的基准，有助于提升模型在印尼语环境中的表现。

**Abstract:** Over 200 million people speak Indonesian, yet the language remains
significantly underrepresented in preference-based research for large language
models (LLMs). Most existing multilingual datasets are derived from English
translations, often resulting in content that lacks cultural and linguistic
authenticity. To address this gap, we introduce IndoPref, the first fully
human-authored and multi-domain Indonesian preference dataset specifically
designed to evaluate the naturalness and quality of LLM-generated text. All
annotations are natively written in Indonesian and evaluated using
Krippendorff's alpha, demonstrating strong inter-annotator agreement.
Additionally, we benchmark the dataset across multiple LLMs and assess the
output quality of each model.

</details>


### [2] [Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles](https://arxiv.org/abs/2507.22168)
*Kimberly Le Truong,Riccardo Fogliato,Hoda Heidari,Zhiwei Steven Wu*

Main category: cs.CL

> 研究表明，写作风格变化显著影响LLMs的表现，提供了一种扩展现有基准测试的可扩展方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前评估大型语言模型（LLMs）的基准测试往往缺乏写作风格的多样性，这可能导致这些模型在面对非标准输入时表现不佳。

**Method:** 通过使用基于角色的LLM提示重写评估提示，以模仿多样的写作风格，从而测试大型语言模型（LLMs）在非标准输入下的表现是否薄弱。

**Result:** 即使语义内容相同，写作风格和提示格式的变化也会显著影响被评估的LLM的表现。我们发现某些写作风格会一致地触发低或高表现，无论模型的家族、大小和新旧。

**Conclusion:** 这项工作提出了一种扩展现有基准测试的方法，可以提高评估LLMs在语言变化中表现的外部有效性。

**Abstract:** Current benchmarks for evaluating Large Language Models (LLMs) often do not
exhibit enough writing style diversity, with many adhering primarily to
standardized conventions. Such benchmarks do not fully capture the rich variety
of communication patterns exhibited by humans. Thus, it is possible that LLMs,
which are optimized on these benchmarks, may demonstrate brittle performance
when faced with "non-standard" input. In this work, we test this hypothesis by
rewriting evaluation prompts using persona-based LLM prompting, a low-cost
method to emulate diverse writing styles. Our results show that, even with
identical semantic content, variations in writing style and prompt formatting
significantly impact the estimated performance of the LLM under evaluation.
Notably, we identify distinct writing styles that consistently trigger either
low or high performance across a range of models and tasks, irrespective of
model family, size, and recency. Our work offers a scalable approach to augment
existing benchmarks, improving the external validity of the assessments they
provide for measuring LLM performance across linguistic variations.

</details>


### [3] [A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models](https://arxiv.org/abs/2507.22187)
*Adam M. Morgan,Adeen Flinker*

Main category: cs.CL

> 研究介绍了一种估算动词框架频率（VFFs）的自动化管道，该管道利用大型语言模型（LLMs）生成语料库并分析句法结构，比传统的句法分析器表现更好，且资源消耗较少，能够进行快速大规模的VFF估算。

<details>
  <summary>Details</summary>

**Motivation:** 传统的VFF估算工具在规模、准确性和可访问性方面存在局限性。

**Method:** 通过大型语言模型生成包含476个英文动词的句子语料库，并指导模型像专家语言学家一样分析这些句子的句法结构。

**Result:** 该管道在多个评估数据集上优于广泛使用的句法分析器，且所需资源远少于手动解析。

**Conclusion:** 研究展示了自动帧频率估算的可行性，并发布了所有代码和数据以支持未来研究。

**Abstract:** We present an automated pipeline for estimating Verb Frame Frequencies
(VFFs), the frequency with which a verb appears in particular syntactic frames.
VFFs provide a powerful window into syntax in both human and machine language
systems, but existing tools for calculating them are limited in scale,
accuracy, or accessibility. We use large language models (LLMs) to generate a
corpus of sentences containing 476 English verbs. Next, by instructing an LLM
to behave like an expert linguist, we had it analyze the syntactic structure of
the sentences in this corpus. This pipeline outperforms two widely used
syntactic parsers across multiple evaluation datasets. Furthermore, it requires
far fewer resources than manual parsing (the gold-standard), thereby enabling
rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF
database with broader verb coverage, finer-grained syntactic distinctions, and
explicit estimates of the relative frequencies of structural alternates
commonly studied in psycholinguistics. The pipeline is easily customizable and
extensible to new verbs, syntactic frames, and even other languages. We present
this work as a proof of concept for automated frame frequency estimation, and
release all code and data to support future research.

</details>


### [4] [The role of media memorability in facilitating startups' access to venture capital funding](https://arxiv.org/abs/2507.22201)
*L. Toschi,S. Torrisi,A. Fronzetti Colladon*

Main category: cs.CL

> 研究揭示了一种新的媒体影响力指标，即“媒体记忆力”，通过分析197家英国微技术和纳米技术领域的初创企业数据，发现媒体记忆力对融资结果有重大影响。

<details>
  <summary>Details</summary>

**Motivation:** 过去的媒介曝光研究过于狭隘地集中在一般性媒介曝光上，限制了我们对媒体如何真正影响融资决策的理解。本研究的动机是深入探索媒体在投资决策中的更细微影响，尤其是通过引入“媒体记忆力”的概念。

**Method:** 使用了来自197家英国微技术和纳米技术领域的初创企业（1995年至2004年间获得融资）的数据，分析媒体的记忆力如何影响投资结果。

**Result:** 研究结果显示，媒体记忆力对投资结果有显著影响。风险投资家依赖于详细的线索，如企业的独特性及其在新闻语义网络中的连通性。

**Conclusion:** 此研究为创业金融和媒体合法化领域的研究做出了贡献，并建议企业家不仅要频繁地被媒体报道，还应通过更有针对性、更有意义的报道来强化品牌的认知度。

**Abstract:** Media reputation plays an important role in attracting venture capital
investment. However, prior research has focused too narrowly on general media
exposure, limiting our understanding of how media truly influences funding
decisions. As informed decision-makers, venture capitalists respond to more
nuanced aspects of media content. We introduce the concept of media
memorability - the media's ability to imprint a startup's name in the memory of
relevant investors. Using data from 197 UK startups in the micro and
nanotechnology sector (funded between 1995 and 2004), we show that media
memorability significantly influences investment outcomes. Our findings suggest
that venture capitalists rely on detailed cues such as a startup's
distinctiveness and connectivity within news semantic networks. This
contributes to research on entrepreneurial finance and media legitimation. In
practice, startups should go beyond frequent media mentions to strengthen brand
memorability through more targeted, meaningful coverage highlighting their
uniqueness and relevance within the broader industry conversation.

</details>


### [5] [How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?](https://arxiv.org/abs/2507.22209)
*Christian Clark,Byung-Doh Oh,William Schuler*

Main category: cs.CL

> 研究通过蒙特卡洛方法评估单词的上下文熵，发现其与仅基于单词首标记的估计方法之间存在显著差异，提示应谨慎使用首标记近似法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的通过语言模型概率分布仅基于单词首子词标记来估计熵的方法存在低估和潜在失真的问题，本研究试图解决这一问题。

**Method:** 本研究使用蒙特卡洛（MC）方法估计单词熵，允许单词跨越可变数量的标记，以解决基于首标记近似单词熵导致的低估和潜在失真问题。

**Result:** 回归实验表明，基于首标记和MC方法的单词熵之间的阅读时间结果存在分歧，表明对首标记法近似上下文熵应谨慎对待。

**Conclusion:** 研究结果建议在估计上下文熵时应避免使用首标记近似法，转而采用更精确的方法如蒙特卡洛估计。

**Abstract:** Contextual entropy is a psycholinguistic measure capturing the anticipated
difficulty of processing a word just before it is encountered. Recent studies
have tested for entropy-related effects as a potential complement to well-known
effects from surprisal. For convenience, entropy is typically estimated based
on a language model's probability distribution over a word's first subword
token. However, this approximation results in underestimation and potential
distortion of true word entropy. To address this, we generate Monte Carlo (MC)
estimates of word entropy that allow words to span a variable number of tokens.
Regression experiments on reading times show divergent results between
first-token and MC word entropy, suggesting a need for caution in using
first-token approximations of contextual entropy.

</details>


### [6] [RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation](https://arxiv.org/abs/2507.22219)
*Dongyub Jude Lee,Zhenyi Ye,Pengcheng He*

Main category: cs.CL

> 文章提出了一种更依赖于教师模型连续反馈的新框架--RLfR，它提高了机器翻译的质量和泛化能力，特别在语义恰当性和实体保持性方面表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 现有的偏好学习方法虽然在机器翻译（MT）领域取得了显著的成绩，但严重依赖于大量精心整理的三元组数据集，并且常常无法推广到其调优领域之外。因此，提出了RLfR框架希望能够克服这些限制，提高翻译的质量和泛化能力。

**Method:** RLfR是一个新的框架，它通过利用外部教师模型（如GPT-4o）提供的连续、高质量的反馈来减少对静态三元组数据集的依赖。每个翻译步骤都被视为微型教程：演员生成一个假设，教师对其进行改进，演员的奖励基于它与教师改进的接近程度。主要使用两种互补信号：(i) 负编辑距离，促进词汇和结构上的保真度；(ii) COMET得分，确保语义上的准确性。

**Result:** 在多语言FLORES-200基准测试（包括英语到和自德语、西班牙语、中文、韩语、日语）上，RLfR框架在多个指标上都优于MT-SFT和偏好模型基线，显著提升了COMET（语义恰当性）和M-ETA（实体保持性）得分。

**Conclusion:** RLfR框架通过连续反馈和奖励机制，模拟了人类学习过程中的逐步和迭代改进，从而提高了翻译质量和泛化能力。

**Abstract:** Preference-learning methods for machine translation (MT)--such as Direct
Preference Optimization (DPO)--have achieved impressive gains but depend
heavily on large, carefully curated triplet datasets and often struggle to
generalize beyond their tuning domains. We propose Reinforcement Learning from
Teacher-Model Refinement (RLfR), a novel framework that removes reliance on
static triplets by leveraging continuous, high-quality feedback from an
external teacher model (GPT-4o). RLfR frames each translation step as a
micro-tutorial: the actor generates a hypothesis, the teacher refines it, and
the actor is rewarded based on how closely it aligns with the teacher's
refinement. Guided by two complementary signals--(i) negative edit distance,
promoting lexical and structural fidelity, and (ii) COMET score, ensuring
semantic adequacy--the actor progressively learns to emulate the teacher,
mirroring a human learning process through incremental, iterative improvement.
On the FLORES-200 benchmark (English to and from German, Spanish, Chinese,
Korean, and Japanese), RLfR consistently outperforms both MT-SFT and
preference-based baselines, significantly improving COMET (semantic adequacy)
and M-ETA (entity preservation) scores.

</details>


### [7] [Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs](https://arxiv.org/abs/2507.22286)
*Supantho Rakshit,Adele Goldberg*

Main category: cs.CL

> 研究通过分析大型语言模型（LLMs）中英语施为结构（双宾结构和介词宾语结构）的神经表示，发现这些表示反映了用法构建主义提出的基于功能的渐变性，表明LLMs学习了富含意义的渐变结构表示。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在探索大型语言模型的内部表示是否反映出用法构建主义所主张的功能渐变性。

**Method:** 研究分析了Pythia-1.4B中英语施为结构的神经表示，使用了一个含有5000个系统变化的句子对的数据集，根据人类评估的偏好强度进行分析。

**Result:** 宏观几何分析显示，构造表示的可分离性（通过能量距离或杰森-香农散度测量）受到渐变偏好强度的系统性调节。更具代表性的结构实例在LLMs的激活空间中占据更独立的区域。

**Conclusion:** 研究结果强有力地证明了LLMs学习了丰富的、富含意义的、渐变的构造表示，并支持了LLMs中基本构建主义原则的几何测量。

**Abstract:** The usage-based constructionist (UCx) approach posits that language comprises
a network of learned form-meaning pairings (constructions) whose use is largely
determined by their meanings or functions, requiring them to be graded and
probabilistic. This study investigates whether the internal representations in
Large Language Models (LLMs) reflect the proposed function-infused gradience.
We analyze the neural representations of the English dative constructions
(Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of
$5000$ sentence pairs systematically varied for human-rated preference
strength. A macro-level geometric analysis finds that the separability between
construction representations, as measured by Energy Distance or Jensen-Shannon
Divergence, is systematically modulated by gradient preference strength. More
prototypical exemplars of each construction occupy more distinct regions in the
activation space of LLMs. These results provide strong evidence that LLMs learn
rich, meaning-infused, graded representations of constructions and offer
support for geometric measures of basic constructionist principles in LLMs.

</details>


### [8] [Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations](https://arxiv.org/abs/2507.22289)
*Galo Castillo-López,Gaël de Chalendar,Nasredine Semmar*

Main category: cs.CL

> 本研究提出了一种结合BERT和大语言模型的方法，在零样本和少样本设置下提高任务导向对话系统识别用户意图和检测出界话语的性能。

<details>
  <summary>Details</summary>

**Motivation:** 任务导向对话系统(TODS)需要大量的标注数据，而在零样本和少样本设置下，识别用户意图和检测OOS话语是保持系统可靠性的关键。

**Method:** 本研究提出了一种结合BERT和LLMs的方法，在零样本和少样本设置下识别用户的意图并检测出界(OOS)的话语。这种方法利用了LLMs的一般化能力和BERT的计算效率。

**Result:** 实验结果表明，在多党对话语料库上，这种方法能通过在BERT输出和LLMs之间的信息共享来提高系统性能。

**Conclusion:** 通过结合BERT和LLMs，可以在零样本和少样本设置下有效提高任务导向对话系统的性能。

**Abstract:** Intent recognition is a fundamental component in task-oriented dialogue
systems (TODS). Determining user intents and detecting whether an intent is
Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However,
traditional TODS require large amount of annotated data. In this work we
propose a hybrid approach to combine BERT and LLMs in zero and few-shot
settings to recognize intents and detect OOS utterances. Our approach leverages
LLMs generalization power and BERT's computational efficiency in such
scenarios. We evaluate our method on multi-party conversation corpora and
observe that sharing information from BERT outputs to LLMs leads to system
performance improvement.

</details>


### [9] [A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers](https://arxiv.org/abs/2507.22337)
*Roxana Petcu,Samarth Bhargav,Maarten de Rijke,Evangelos Kanoulas*

Main category: cs.CL

> 论文通过分类法和数据集的创建，提高神经信息检索模型在处理否定信息时的性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决神经模型在处理包含否定信息的任务时表现不佳的问题。

**Method:** 1) 提出否定句的分类法；2) 创建两个基准数据集；3) 提出基于逻辑的分类机制。

**Result:** 该论文探讨了传统神经信息检索模型和基于LLM（大规模语言模型）的模型在处理包含否定句的信息搜索任务中的表现，并揭示了这些模型在处理否定句时存在的问题。研究通过引入一种基于哲学、语言学和逻辑学定义的否定分类法，创建了两个用于评估模型性能和增强性能的基准数据集，并提出了一种逻辑分类机制来分析现有数据集上的检索模型表现。该分类法能产生各否定类型分布均衡的数据集，有助于神经模型在NevIR数据集上更快收敛。此外，论文提出了一个分类方案，以揭示现有数据集中否定类型的覆盖率，为改进模型泛化能力提供了见解。

**Conclusion:** 论文提出的方法和数据集有助于神经信息检索模型更好地处理否定信息，提高模型性能。

**Abstract:** Understanding and solving complex reasoning tasks is vital for addressing the
information needs of a user. Although dense neural models learn contextualised
embeddings, they still underperform on queries containing negation. To
understand this phenomenon, we study negation in both traditional neural
information retrieval and LLM-based models. We (1) introduce a taxonomy of
negation that derives from philosophical, linguistic, and logical definitions;
(2) generate two benchmark datasets that can be used to evaluate the
performance of neural information retrieval models and to fine-tune models for
a more robust performance on negation; and (3) propose a logic-based
classification mechanism that can be used to analyze the performance of
retrieval models on existing datasets. Our taxonomy produces a balanced data
distribution over negation types, providing a better training setup that leads
to faster convergence on the NevIR dataset. Moreover, we propose a
classification schema that reveals the coverage of negation types in existing
datasets, offering insights into the factors that might affect the
generalization of fine-tuned models on negation.

</details>


### [10] [Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors](https://arxiv.org/abs/2507.22367)
*Jia Li,Yichao He,Jiacheng Xu,Tianhao Luo,Zhenzhen Hu,Richang Hong,Meng Wang*

Main category: cs.CL

> 本文提出了一种称为Traits Run Deep的新性格评估框架，通过心理学导向提示来提取高层次性格相关信息，并整合异步多模态数据，显著提高了性格评估的准确度。在AVI挑战2025上，该方法在性格评估中排名第一。源代码将在指定链接提供。

<details>
  <summary>Details</summary>

**Motivation:** 准确可靠的性格评估在情感智能、心理健康诊断和个人化教育等领域至关重要。与短暂的情绪不同，性格特征是稳定的，并且往往通过语言、面部表情和身体行为等渠道无意识地泄露出来，每个模态之间的模式通常是异步的。传统的浅层特征难以建模性格语义，实现有效的跨模态理解似乎是不可能的。

**Method:** 提出了一种名为Traits Run Deep的新性格评估框架。该框架使用心理学启发的提示来获取高层次的性格相关语义表示。还设计了一个以文本为中心的性格融合网络，用于降维、跨模态信号对齐和融合，并提高数据稀缺情况下的泛化能力。此外，通过提取和融合视听表观行为特征进一步提高了准确性。

**Result:** 实验结果表明，AVI验证集上所述组件的有效性，即均方误差（MSE）大约降低了45%。在AVI挑战2025测试集上的最终评估证实了我们方法的优越性，我们的方法在性格评估赛道中排名第一。

**Conclusion:** 本研究首次将性格特定的提示应用于引导大型语言模型（LLMs）从文本、视觉和语音等多模态信号中提取性格感知的语义表示，从而提高了性格评估的准确性。

**Abstract:** Accurate and reliable personality assessment plays a vital role in many
fields, such as emotional intelligence, mental health diagnostics, and
personalized education. Unlike fleeting emotions, personality traits are
stable, often subconsciously leaked through language, facial expressions, and
body behaviors, with asynchronous patterns across modalities. It was hard to
model personality semantics with traditional superficial features and seemed
impossible to achieve effective cross-modal understanding. To address these
challenges, we propose a novel personality assessment framework called
\textit{\textbf{Traits Run Deep}}. It employs
\textit{\textbf{psychology-informed prompts}} to elicit high-level
personality-relevant semantic representations. Besides, it devises a
\textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text
semantics to align and integrate asynchronous signals from other modalities. To
be specific, such fusion module includes a Chunk-Wise Projector to decrease
dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for
effective modality fusion and an ensemble regression head to improve
generalization in data-scarce situations. To our knowledge, we are the first to
apply personality-specific prompts to guide large language models (LLMs) in
extracting personality-aware semantics for improved representation quality.
Furthermore, extracting and fusing audio-visual apparent behavior features
further improves the accuracy. Experimental results on the AVI validation set
have demonstrated the effectiveness of the proposed components, i.e.,
approximately a 45\% reduction in mean squared error (MSE). Final evaluations
on the test set of the AVI Challenge 2025 confirm our method's superiority,
ranking first in the Personality Assessment track. The source code will be made
available at https://github.com/MSA-LMC/TraitsRunDeep.

</details>


### [11] [PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs](https://arxiv.org/abs/2507.22387)
*Homaira Huda Shomee,Suman Kalyan Maity,Sourav Medya*

Main category: cs.CL

> 本文介绍了一种新的方法，使用大型语言模型(Large Language Models, LLMs)来革新专利写作流程，特别是在专利摘要生成方面。通过对比六种主流LLMs在不同条件下的表现，提出了一套综合性的评价框架PATENTWRITER。实验结果显示，现代LLMs能够生成高保真、风格恰当的专利摘要文本，整体性能优于特定领域的基准模型。

<details>
  <summary>Details</summary>

**Motivation:** 专利撰写的流程通常是繁琐且耗时的。通过使用LLMs，论文希望能加快并优化这一流程，特别是通过自动化生成专利摘要来实现。

**Method:** 论文开发了初步的综合评估框架PATENTWRITER，用来评测六种不同的LLMs生成专利摘要的能力。评估框架涵盖了零样本学习、少样本学习和链式思考方法在内的多种评测策略。此外，还考量了生成质量的多个维度，如文本长度、易读性和语气。

**Result:** 实验证明，现代LLMs在生成专利摘要方面表现出色，普遍优于专门领域的基线模型。

**Conclusion:** 研究结果表明LLMs可以有效地贯通于专利撰写，特别是在生成高质量专利摘要上有巨大潜力。论文提供了代码和数据集支持，为进一步研究打开了大门。

**Abstract:** Large language models (LLMs) have emerged as transformative approaches in
several important fields. This paper aims for a paradigm shift for patent
writing by leveraging LLMs to overcome the tedious patent-filing process. In
this work, we present PATENTWRITER, the first unified benchmarking framework
for evaluating LLMs in patent abstract generation. Given the first claim of a
patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a
consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting
strategies to generate the abstract of the patent. Our benchmark PATENTWRITER
goes beyond surface-level evaluation: we systematically assess the output
quality using a comprehensive suite of metrics -- standard NLP measures (e.g.,
BLEU, ROUGE, BERTScore), robustness under three types of input perturbations,
and applicability in two downstream patent classification and retrieval tasks.
We also conduct stylistic analysis to assess length, readability, and tone.
Experimental results show that modern LLMs can generate high-fidelity and
stylistically appropriate patent abstracts, often surpassing domain-specific
baselines. Our code and dataset are open-sourced to support reproducibility and
future research.

</details>


### [12] [Question Generation for Assessing Early Literacy Reading Comprehension](https://arxiv.org/abs/2507.22410)
*Xiaocheng Yang,Sumuk Shashidhar,Dilek Hakkani-Tur*

Main category: cs.CL

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** Structure

**Result:** {"tldr": "本文提出了一种针对K-2年级英语学习者的阅读理解题目生成的新方法，能全面覆盖学习材料并适应学习者特定的技能水平。使用FairytaleQA数据集进行了评估，有潜力成为自动AI驱动英语教学的一部分。", "motivation": "评估通过基于内容的互动进行的阅读理解在阅读能力培养过程中扮演重要角色。", "method": "提出的新方法确保了对底层材料的完全覆盖，并且能够适应学习者的特定技能，生成不同类型的题目，以不同难度水平进行全面评估。", "result": "使用FairytaleQA数据集评估了各种语言模型的表现。", "conclusion": "该提议的方法有可能成为自动AI驱动英语教学的重要组成部分。"}

**Conclusion:** 

**Abstract:** Assessment of reading comprehension through content-based interactions plays
an important role in the reading acquisition process. In this paper, we propose
a novel approach for generating comprehension questions geared to K-2 English
learners. Our method ensures complete coverage of the underlying material and
adaptation to the learner's specific proficiencies, and can generate a large
diversity of question types at various difficulty levels to ensure a thorough
evaluation. We evaluate the performance of various language models in this
framework using the FairytaleQA dataset as the source material. Eventually, the
proposed approach has the potential to become an important part of autonomous
AI-driven English instructors.

</details>


### [13] [NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models](https://arxiv.org/abs/2507.22411)
*Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

> 我们引入了 NeedleChain 基准测试和 ROPE Contraction 策略，以更准确地评估和提升 LLM 对长上下文的理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管 Needle-in-a-Haystack (NIAH) 基准测试被广泛用于评估大语言模型 (LLM) 在理解长上下文方面的表现，但研究表明这一方法可能过高估计了 LLM 的真实能力。因此有必要提出一种新的评估方法。

**Method:** 我们提出了一种新的基准测试 NeedleChain，它仅包含与查询相关的上下文信息，并提出了一种名为 ROPE Contraction 的简单策略来提高 LLM 对长上下文的理解能力。

**Result:** 实验结果显示，尽管先进的 LLM 模型如 GPT-4o 在处理长上下文方面有较好的表现，但其完全理解上下文的能力仍有较大的提升空间。

**Conclusion:** NeedleChain 基准测试和 ROPE Contraction 策略为评估和提升 LLM 的长上下文理解能力提供了一个新的视角。

**Abstract:** The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large
Language Models' (LLMs) ability to understand long contexts (LC). It evaluates
the capability to identify query-relevant context within extensive
query-irrelevant passages. Although this method serves as a widely accepted
standard for evaluating long-context understanding, our findings suggest it may
overestimate the true LC capability of LLMs. We demonstrate that even
state-of-the-art models such as GPT-4o struggle to intactly incorporate given
contexts made up of solely query-relevant ten sentences. In response, we
introduce a novel benchmark, \textbf{NeedleChain}, where the context consists
entirely of query-relevant information, requiring the LLM to fully grasp the
input to answer correctly. Our benchmark allows for flexible context length and
reasoning order, offering a more comprehensive analysis of LLM performance.
Additionally, we propose an extremely simple yet compelling strategy to improve
LC understanding capability of LLM: ROPE Contraction. Our experiments with
various advanced LLMs reveal a notable disparity between their ability to
process large contexts and their capacity to fully understand them. Source code
and datasets are available at https://github.com/hyeonseokk/NeedleChain

</details>


### [14] [AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini](https://arxiv.org/abs/2507.22445)
*Jill Walker Rettberg,Hermann Wigers*

Main category: cs.CL

> 研究利用语言模型生成有关不同国家的故事，发现这些故事在叙事结构上具有同质性，反映出AI偏见——叙事标准化。

<details>
  <summary>Details</summary>

**Motivation:** 研究使用主要基于英美文本训练的语言模型能否生成对其他国家和地区有文化相关性的故事。

**Method:** 通过向OpenAI的gpt-4o-mini模型发送特定提示，生成了11,800个故事，每个国家50个故事，涵盖了236个国家。提示内容为“写一个潜在的{地名}故事，长度为1500字。”

**Result:** 生成的故事虽然包含了一些表面的国家象征和主题，但它们普遍呈现出同一种叙事结构：主人公生活在或重返一个小城镇，并通过重新连接传统和组织社区活动来解决一个轻微的冲突。真实的冲突被净化，浪漫几乎不存在，叙事紧张程度也被降低。这导致了叙事的同质化现象。

**Conclusion:** AI生成的叙事在结构上的同质性构成了一种新型的AI偏见——叙事标准化，这应该被文学研究、叙事学、批判性AI研究、自然语言处理研究以及改进生成AI文化对齐的努力所关注。

**Abstract:** Can a language model trained largely on Anglo-American texts generate stories
that are culturally relevant to other nationalities? To find out, we generated
11,800 stories - 50 for each of 236 countries - by sending the prompt "Write a
1500 word potential {demonym} story" to OpenAI's model gpt-4o-mini. Although
the stories do include surface-level national symbols and themes, they
overwhelmingly conform to a single narrative plot structure across countries: a
protagonist lives in or returns home to a small town and resolves a minor
conflict by reconnecting with tradition and organising community events.
Real-world conflicts are sanitised, romance is almost absent, and narrative
tension is downplayed in favour of nostalgia and reconciliation. The result is
a narrative homogenisation: an AI-generated synthetic imaginary that
prioritises stability above change and tradition above growth. We argue that
the structural homogeneity of AI-generated narratives constitutes a distinct
form of AI bias, a narrative standardisation that should be acknowledged
alongside the more familiar representational bias. These findings are relevant
to literary studies, narratology, critical AI studies, NLP research, and
efforts to improve the cultural alignment of generative AI.

</details>


### [15] [Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance](https://arxiv.org/abs/2507.22448)
*Jingwei Zuo,Maksim Velikanov,Ilyas Chahed,Younes Belkada,Dhia Eddine Rhayem,Guillaume Kunsch,Hakim Hacid,Hamza Yous,Brahim Farhat,Ibrahim Khadraoui,Mugariya Farooq,Giulia Campesan,Ruxandra Cojocaru,Yasser Djilali,Shi Hu,Iheb Chaabane,Puneesh Khanna,Mohamed El Amine Seddik,Ngoc Dung Huynh,Phuc Le Khac,Leen AlQadi,Billel Mokeddem,Mohamed Chami,Abdalgader Abubaker,Mikhail Lubinets,Kacper Piskorski,Slim Frikha*

Main category: cs.CL

> Falcon-H1 is a new series of hybrid architecture large language models featuring high performance and efficiency. It combines Transformer-based attention with State Space Models to improve long-context memory and computational efficiency. Falcon-H1 models outperform many larger models while using fewer parameters.

<details>
  <summary>Details</summary>

**Motivation:** To improve performance and efficiency of large language models across diverse use cases by combining different architectural approaches.

**Method:** Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention mechanisms with State Space Models for better long-context memory and efficiency. Multiple configurations are available, including base and instruction-tuned variants with varying parameter sizes.

**Result:** Falcon-H1 models show state-of-the-art performance, especially in reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge, often matching or outperforming larger models with fewer parameters.

**Conclusion:** Falcon-H1 is a highly efficient and performant series of large language models with various configurations, released with open-source licenses for accessible AI research.

**Abstract:** In this report, we introduce Falcon-H1, a new series of large language models
(LLMs) featuring hybrid architecture designs optimized for both high
performance and efficiency across diverse use cases. Unlike earlier Falcon
models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a
parallel hybrid approach that combines Transformer-based attention with State
Space Models (SSMs), known for superior long-context memory and computational
efficiency. We systematically revisited model design, data strategy, and
training dynamics, challenging conventional practices in the field. Falcon-H1
is released in multiple configurations, including base and instruction-tuned
variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized
instruction-tuned models are also available, totaling over 30 checkpoints on
Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and
exceptional parameter and training efficiency. The flagship Falcon-H1-34B
matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,
and Llama3.3-70B, while using fewer parameters and less data. Smaller models
show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B
models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.
These models excel across reasoning, mathematics, multilingual tasks,
instruction following, and scientific knowledge. With support for up to 256K
context tokens and 18 languages, Falcon-H1 is suitable for a wide range of
applications. All models are released under a permissive open-source license,
underscoring our commitment to accessible and impactful AI research.

</details>


### [16] [What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models](https://arxiv.org/abs/2507.22457)
*Tian Yun,Chen Sun,Ellie Pavlick*

Main category: cs.CL

> 尽管大型语言模型（LLMs）在零样本设置中表现不佳，但对其输入编码参数进行少量微调可以显著提升性能，不过这种微调并不一定会跨数据集转移。这一结果有助于重新审视什么是“抽象推理者”及其对LLMs的意义。

<details>
  <summary>Details</summary>

**Motivation:** 回应关于LLMs是否为“抽象推理者”的质疑，通过实验表明这些模型在零样本设置下虽然表现不佳，但通过参数微调可以极大地提升性能，并探讨这结果对未来研究的重要性。

**Method:** 重新审视先前的实验，重点研究在零样本设置下LLMs的表现，并探索仅微调输入编码参数对其性能的影响。

**Result:** 发现即使只对一小部分输入编码参数进行微调，也可以使LLMs在某些任务上达到近乎完美的性能，但这种性能提升不能很好地迁移到其他数据集。

**Conclusion:** 这些发现扩展了关于LLMs作为“抽象推理者”的讨论，进一步阐明了对细微调参如何影响模型能力和跨任务迁移的重要性。

**Abstract:** Recent work has argued that large language models (LLMs) are not "abstract
reasoners", citing their poor zero-shot performance on a variety of challenging
tasks as evidence. We revisit these experiments in order to add nuance to the
claim. First, we show that while LLMs indeed perform poorly in a zero-shot
setting, even tuning a small subset of parameters for input encoding can enable
near-perfect performance. However, we also show that this finetuning does not
necessarily transfer across datasets. We take this collection of empirical
results as an invitation to (re-)open the discussion of what it means to be an
"abstract reasoner", and why it matters whether LLMs fit the bill.

</details>


### [17] [IFEvalCode: Controlled Code Generation](https://arxiv.org/abs/2507.22462)
*Jian Yang,Wei Zhang,Shukai Liu,Linzheng Chai,Yingshui Tan,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou,Guanglin Niu,Zhoujun Li,Binyuan Hui,Junyang Lin*

Main category: cs.CL

> 文章提出了一种用于提高代码生成大模型指令跟随能力的方法，并引入了一个新的多语言基准IFEvalCode来评估模型的性能。实验结果表明闭源模型在这一方面表现优越。

<details>
  <summary>Details</summary>

**Motivation:** 现有的代码生成大模型虽然可以将自然语言描述转化为功能性代码，但在实际应用中还需要严格遵循一些详细的规范要求，诸如编码风格、行数限制和结构约束等。

**Method:** 通过引入正向和反向约束生成方法来提高代码生成大模型在受控代码生成任务中的指令跟随能力，使输出更能符合人类定义的规范。

**Result:** 引入了IFEvalCode，这是一个包含七种编程语言（Python、Java、JavaScript、TypeScript、Shell、C++和C#）且每种语言都有中英两种查询类型的多语言基准，总共包含了1.6K个测试样本。它将评估分成正确性(Corr.)和指令跟随性(Instr.)两个指标，提供了更加细化的评估方法。

**Conclusion:** 实验结果表明，闭源模型在可控代码生成方面优于开源模型，并揭示了模型生成正确代码的能力与其生成精准遵循指令的代码能力之间存在显著差距。

**Abstract:** Code large language models (Code LLMs) have made significant progress in code
generation by translating natural language descriptions into functional code;
however, real-world applications often demand stricter adherence to detailed
requirements such as coding style, line count, and structural constraints,
beyond mere correctness. To address this, the paper introduces forward and
backward constraints generation to improve the instruction-following
capabilities of Code LLMs in controlled code generation, ensuring outputs align
more closely with human-defined guidelines. The authors further present
IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven
programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and
C#), with each sample featuring both Chinese and English queries. Unlike
existing benchmarks, IFEvalCode decouples evaluation into two metrics:
correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced
assessment. Experiments on over 40 LLMs reveal that closed-source models
outperform open-source ones in controllable code generation and highlight a
significant gap between the models' ability to generate correct code versus
code that precisely follows instructions.

</details>


### [18] [SLM-SQL: An Exploration of Small Language Models for Text-to-SQL](https://arxiv.org/abs/2507.22478)
*Lei Sheng,Shuai-Shuai Xu*

Main category: cs.CL

> 研究介绍了如何通过先进的后训练技术改小型语言模型，提升其在Text-to-SQL任务的表现。模型和代码将在GitHub上公开。

<details>
  <summary>Details</summary>

**Motivation:** 尽管小型语言模型(SLM)目前在Text-to-SQL任务上表现不佳，但由于这些模型的优势在于推理速度和更适合边缘部署，所以探索将这些模型运用到Text-to-SQL应用中的潜力是有意义的。

**Method:** 利用开源的SynSQL-2.5M数据集构建了两个派生数据集：SynSQL-Think-916K用于SQL生成，SynSQL-Merge-Think-310K用于SQL合并修订。通过监督精细调整和基于强化学习的后训练技术，以及使用纠正自一致性方法进行推断，对小型语言模型进行了改进。

**Result:** 实验结果验证了所提出方法SLM-SQL的有效性和可推广性。在BIRD开发集上，五个评估模型平均提升了31.4分。特别是，0.5B模型达到了56.87%的执行准确性（EX），而1.5B模型达到了67.08%的EX。

**Conclusion:** 该研究展示了通过先进的后训练技术如何改进小型语言模型，以增强其在SQL生成任务中的性能。

**Abstract:** Large language models (LLMs) have demonstrated strong performance in
translating natural language questions into SQL queries (Text-to-SQL). In
contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters
currently underperform on Text-to-SQL tasks due to their limited logical
reasoning capabilities. However, SLMs offer inherent advantages in inference
speed and suitability for edge deployment. To explore their potential in
Text-to-SQL applications, we leverage recent advancements in post-training
techniques. Specifically, we used the open-source SynSQL-2.5M dataset to
construct two derived datasets: SynSQL-Think-916K for SQL generation and
SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised
fine-tuning and reinforcement learning-based post-training to the SLM, followed
by inference using a corrective self-consistency approach. Experimental results
validate the effectiveness and generalizability of our method, SLM-SQL. On the
BIRD development set, the five evaluated models achieved an average improvement
of 31.4 points. Notably, the 0.5B model reached 56.87\% execution accuracy
(EX), while the 1.5B model achieved 67.08\% EX. We will release our dataset,
model, and code to github: https://github.com/CycloneBoy/slm_sql.

</details>


### [19] [CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records](https://arxiv.org/abs/2507.22533)
*Dongchen Li,Jitao Liang,Wei Li,Xiaoyu Wang,Longbing Cao,Kun Yu*

Main category: cs.CL

> 提出CliCARE框架以解决大型语言模型在癌症电子健康记录中处理长周期数据、多语言支持和临床幻觉问题，通过将不结构化的数据转化为患者特定的时间知识图来提供临床决策支持，并在大规模数据集上进行了验证，显示优于基线方法。

<details>
  <summary>Details</summary>

**Motivation:** 解决大型语言模型处理复杂、长期癌症电子健康记录的能力不足，以及临床幻觉风险增加的问题。

**Method:** 通过将病历转化为患者特定的时间知识图，结合规范指南知识图进行临床决策支持。

**Result:** 在私人中文癌症数据集和公共英文MIMIC-IV数据集上，CliCARE有显著优于基线方法的表现。

**Conclusion:** CliCARE能够提供基于证据的临床决策支持，临床有效性通过与专家肿瘤学家的评估高度相关得到验证。

**Abstract:** Large Language Models (LLMs) hold significant promise for improving clinical
decision support and reducing physician burnout by synthesizing complex,
longitudinal cancer Electronic Health Records (EHRs). However, their
implementation in this critical field faces three primary challenges: the
inability to effectively process the extensive length and multilingual nature
of patient records for accurate temporal analysis; a heightened risk of
clinical hallucination, as conventional grounding techniques such as
Retrieval-Augmented Generation (RAG) do not adequately incorporate
process-oriented clinical guidelines; and unreliable evaluation metrics that
hinder the validation of AI systems in oncology. To address these issues, we
propose CliCARE, a framework for Grounding Large Language Models in Clinical
Guidelines for Decision Support over Longitudinal Cancer Electronic Health
Records. The framework operates by transforming unstructured, longitudinal EHRs
into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range
dependencies, and then grounding the decision support process by aligning these
real-world patient trajectories with a normative guideline knowledge graph.
This approach provides oncologists with evidence-grounded decision support by
generating a high-fidelity clinical summary and an actionable recommendation.
We validated our framework using large-scale, longitudinal data from a private
Chinese cancer dataset and the public English MIMIC-IV dataset. In these
diverse settings, CliCARE significantly outperforms strong baselines, including
leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The
clinical validity of our results is supported by a robust evaluation protocol,
which demonstrates a high correlation with assessments made by expert
oncologists.

</details>


### [20] [A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support](https://arxiv.org/abs/2507.22542)
*Long S. T. Nguyen,Truong P. Hua,Thanh M. Nguyen,Toan Q. Pham,Nam K. Ngo,An X. Nguyen,Nghi D. M. Pham,Nghia H. Nguyen,Tho T. Quan*

Main category: cs.CL

> 本文介绍了CSConDa数据集，用于评估越南大语言模型在客户服务问答系统中的性能，并提出了一个综合评估框架，帮助改进下一代ViLLMs。

<details>
  <summary>Details</summary>

**Motivation:** 由于领域特性的评估数据集缺乏，很难为支持应用程序选择合适的越南大型语言模型，本文为了解决这个问题。

**Method:** 通过创建一个基于真实客户互动的越南软件公司编纂的CSConDa数据集，并评估11个轻量级开源ViLLMs，使用自动度量和句法分析。

**Result:** 研究揭示了性能差异并指出了关键改进领域，有助于客户服务质量问答模型的选择和发展。

**Conclusion:** 通过建立一个强大的基准测试和系统评估，本研究为改进客户服务问答系统中的ViLLMs奠定了基础。

**Abstract:** With the rapid growth of Artificial Intelligence, Large Language Models
(LLMs) have become essential for Question Answering (QA) systems, improving
efficiency and reducing human workload in customer service. The emergence of
Vietnamese LLMs (ViLLMs) highlights lightweight open-source models as a
practical choice for their accuracy, efficiency, and privacy benefits. However,
domain-specific evaluations remain limited, and the absence of benchmark
datasets reflecting real customer interactions makes it difficult for
enterprises to select suitable models for support applications. To address this
gap, we introduce the Customer Support Conversations Dataset (CSConDa), a
curated benchmark of over 9,000 QA pairs drawn from real interactions with
human advisors at a large Vietnamese software company. Covering diverse topics
such as pricing, product availability, and technical troubleshooting, CSConDa
provides a representative basis for evaluating ViLLMs in practical scenarios.
We further present a comprehensive evaluation framework, benchmarking 11
lightweight open-source ViLLMs on CSConDa with both automatic metrics and
syntactic analysis to reveal model strengths, weaknesses, and linguistic
patterns. This study offers insights into model behavior, explains performance
differences, and identifies key areas for improvement, supporting the
development of next-generation ViLLMs. By establishing a robust benchmark and
systematic evaluation, our work enables informed model selection for customer
service QA and advances research on Vietnamese LLMs. The dataset is publicly
available at
https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.

</details>


### [21] [ControlMed: Adding Reasoning Control to Medical Language Model](https://arxiv.org/abs/2507.22545)
*Sung-Min Lee,Siyoon Lee,Juyeon Kim,Kyungmin Roh*

Main category: cs.CL

> ControlMed是一种医疗语言模型，它允许用户在推理过程中通过细粒度控制标记来灵活控制推理过程的长度，这有助于提升计算效率和响应速度。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现有推理语言模型在医疗领域得到了应用，但它们产生的推理过程过长，导致计算负担和响应延迟增加，限制了它们在真实临床环境中的部署。为了解决这些问题，提出了ControlMed模型。

**Method:** 通过三阶段的训练流程来开发ControlMed模型：1) 在大规模合成医疗指令数据集上进行预训练，涵盖直接和推理响应；2) 使用多长度推理数据和明确的长度控制标记进行监督微调；3) 通过基于模型的奖励信号进行强化学习，以提高事实准确性及响应质量。

**Result:** 实验结果显示，与最先进的模型相比，我们的模型在一系列英文和韩文医疗基准测试中取得了相似或更好的表现。

**Conclusion:** ControlMed作为一个实用且灵活的解决方案，可以在保持推理准确性的同时根据不同需求调整计算效率，从而适用于临床问题回答和医疗信息分析。

**Abstract:** Reasoning Large Language Models (LLMs) with enhanced accuracy and
explainability are increasingly being adopted in the medical domain, as the
life-critical nature of clinical decision-making demands reliable support.
Despite these advancements, existing reasoning LLMs often generate
unnecessarily lengthy reasoning processes, leading to significant computational
overhead and response latency. These limitations hinder their practical
deployment in real-world clinical environments. To address these challenges, we
introduce \textbf{ControlMed}, a medical language model that enables users to
actively control the length of the reasoning process at inference time through
fine-grained control markers. ControlMed is trained through a three-stage
pipeline: 1) pre-training on a large-scale synthetic medical instruction
dataset covering both \textit{direct} and \textit{reasoning responses}; 2)
supervised fine-tuning with multi-length reasoning data and explicit
length-control markers; and 3) reinforcement learning with model-based reward
signals to enhance factual accuracy and response quality. Experimental results
on a variety of English and Korean medical benchmarks demonstrate that our
model achieves similar or better performance compared to state-of-the-art
models. Furthermore, users can flexibly balance reasoning accuracy and
computational efficiency by controlling the reasoning length as needed. These
findings demonstrate that ControlMed is a practical and adaptable solution for
clinical question answering and medical information analysis.

</details>


### [22] [Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs](https://arxiv.org/abs/2507.22564)
*Xikang Yang,Biyu Zhou,Xuehai Tang,Jizhong Han,Songlin Hu*

Main category: cs.CL

> 研究提出了CognitiveAttack，一种利用认知偏差多交互来攻击大型语言模型的新框架。实验结果显示该方法有效且成功率高，提示需要关注LLM的安全性。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型的安全机制容易受到利用认知偏差的对抗攻击。这项工作旨在展示多偏差交互在破坏LLM安全保障中的未被重视的力量，而非以前的越狱方法主要集中在提示工程或算法操控上。

**Method:** CognitiveAttack,一种新颖的对抗框架，利用个体及组合认知偏差来产生嵌入优化偏差组合的提示，从而有效绕过安全协议同时保持高攻击成功率，通过监督微调和强化学习实现。

**Result:** 实验结果显示了30种多样化的LLM的显著漏洞，尤其是开源模型。CognitiveAttack的攻击成功率显著高于最先进的黑盒方法PAP（60.1%对比31.6%）。

**Conclusion:** 这些发现突出了多偏差交互作为一个强大而未被充分探索的攻击向量。这项工作通过连接认知科学和LLM安全，引入了一种新颖的跨学科视角，为更强大且更符合人类需求的AI系统铺平了道路。

**Abstract:** Large Language Models (LLMs) demonstrate impressive capabilities across a
wide range of tasks, yet their safety mechanisms remain susceptible to
adversarial attacks that exploit cognitive biases -- systematic deviations from
rational judgment. Unlike prior jailbreaking approaches focused on prompt
engineering or algorithmic manipulation, this work highlights the overlooked
power of multi-bias interactions in undermining LLM safeguards. We propose
CognitiveAttack, a novel red-teaming framework that systematically leverages
both individual and combined cognitive biases. By integrating supervised
fine-tuning and reinforcement learning, CognitiveAttack generates prompts that
embed optimized bias combinations, effectively bypassing safety protocols while
maintaining high attack success rates. Experimental results reveal significant
vulnerabilities across 30 diverse LLMs, particularly in open-source models.
CognitiveAttack achieves a substantially higher attack success rate compared to
the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations
in current defense mechanisms. These findings highlight multi-bias interactions
as a powerful yet underexplored attack vector. This work introduces a novel
interdisciplinary perspective by bridging cognitive science and LLM safety,
paving the way for more robust and human-aligned AI systems.

</details>


### [23] [Unveiling the Influence of Amplifying Language-Specific Neurons](https://arxiv.org/abs/2507.22581)
*Inaya Rahmanisa,Lyzander Marciano Andrylie,Krisna Mahardika Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

> 研究通过放大特定语言的神经元，测试了其对18种语言的影响，结果表明这种干预能有效提高某些语言的性能，但对跨语言性能带来负面影响。

<details>
  <summary>Details</summary>

**Motivation:** 虽然已经观察到通过停用特定语言的神经元能够影响模型的行为，但它们在放大多种语言中的作用尚不明确。这项研究旨在探究放大多语言模型中特定语言神经元的效果。

**Method:** 本研究通过干预放大特定语言的神经元来研究其对模型行为的影响，实验涉及18种语言，包括资源较少的语言，并使用三种主要使用不同语言训练的模型进行测试。

**Result:** 研究发现，最优的放大因子能够将输出导向几乎所有测试的语言。在下游任务中使用该放大因子可以提高某些语言自模型的性能，但通常会降低跨语言的结果。

**Conclusion:** 放大特定语言神经元对多语言模型的行为有显著影响，特别是对资源较少的语言有明显好处，但在跨语言迁移方面优势不大。

**Abstract:** Language-specific neurons in LLMs that strongly correlate with individual
languages have been shown to influence model behavior by deactivating them.
However, their role in amplification remains underexplored. This work
investigates the effect of amplifying language-specific neurons through
interventions across 18 languages, including low-resource ones, using three
models primarily trained in different languages. We compare amplification
factors by their effectiveness in steering to the target language using a
proposed Language Steering Shift (LSS) evaluation score, then evaluate it on
downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge
(Include), and translation (FLORES). The optimal amplification factors
effectively steer output toward nearly all tested languages. Intervention using
this factor on downstream tasks improves self-language performance in some
cases but generally degrades cross-language results. These findings highlight
the effect of language-specific neurons in multilingual behavior, where
amplification can be beneficial especially for low-resource languages, but
provides limited advantage for cross-lingual transfer.

</details>


### [24] [BALSAM: A Platform for Benchmarking Arabic Large Language Models](https://arxiv.org/abs/2507.22603)
*Rawan Al-Matham,Kareem Darwish,Raghad Al-Rasheed,Waad Alshammari,Muneera Alhoshan,Amal Almazrua,Asma Al Wazrah,Mais Alheraki,Firoj Alam,Preslav Nakov,Norah Alzahrani,Eman alBilali,Nizar Habash,Abdelrahman El-Sheikh,Muhammad Elmallah,Haonan Li,Hamdy Mubarak,Mohamed Anwar,Zaid Alyafeai,Ahmed Abdelali,Nora Altwairesh,Maram Hasanain,Abdulmohsen Al Thubaity,Shady Shehata,Bashar Alhafni,Injy Hamed,Go Inoue,Khalid Elmadani,Ossama Obeid,Fatima Haouari,Tamer Elsayed,Emad Alghamdi,Khalid Almubarak,Saied Alshahrani,Ola Aljarrah,Safa Alajlan,Areej Alshaqarawi,Maryam Alshihri,Sultana Alghurabi,Atikah Alzeghayer,Afrah Altamimi,Abdullah Alfaifi,Abdulrahman AlOsaimy*

Main category: cs.CL

> 提出了BALSAM基准测试，以解决阿拉伯语大语言模型的数据稀缺、语言多样性和形态复杂性等问题，推动模型的发展与评估。

<details>
  <summary>Details</summary>

**Motivation:** 阿拉伯语大语言模型的发展落后于英语模型，这主要是由于阿拉伯语和其方言的数据稀缺，语言多样性和形态复杂性等原因。为了推进阿拉伯语大语言模型的发展和评估，引入了BALSAM作为解决方案。

**Method:** 通过建立全面的基准测试BALSAM，改善阿拉伯语语言模型的发展与评估问题。BALSAM涵盖了14个广泛类别的78个NLP任务，具有大量的数据集，并采用透明的平台进行盲测。

**Result:** 该论文提出BALSAM，这是一个全面的、由社区驱动的基准测试，旨在推动阿拉伯语大语言模型的发展与评估。它包括来自14个广泛类别的78个NLP任务，共有52,000个示例，其中包括37,000个测试集和15,000个开发集，并提供了一个集中且透明的平台，以实现盲测。BALSAM的目标是设立标准并促进共同研究，以提高阿拉伯语大语言模型的能力。

**Conclusion:** BALSAM将作为一个统一的平台，设立标准并促进协作研究，以提升阿拉伯语大语言模型的能力。

**Abstract:** The impressive advancement of Large Language Models (LLMs) in English has not
been matched across all languages. In particular, LLM performance in Arabic
lags behind, due to data scarcity, linguistic diversity of Arabic and its
dialects, morphological complexity, etc. Progress is further hindered by the
quality of Arabic benchmarks, which typically rely on static, publicly
available data, lack comprehensive task coverage, or do not provide dedicated
platforms with blind test sets. This makes it challenging to measure actual
progress and to mitigate data contamination. Here, we aim to bridge these gaps.
In particular, we introduce BALSAM, a comprehensive, community-driven benchmark
aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP
tasks from 14 broad categories, with 52K examples divided into 37K test and 15K
development, and a centralized, transparent platform for blind evaluation. We
envision BALSAM as a unifying platform that sets standards and promotes
collaborative research to advance Arabic LLM capabilities.

</details>


### [25] [Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation](https://arxiv.org/abs/2507.22608)
*Daniil Gurgurov,Katharina Trinley,Yusser Al Ghussin,Tanja Baeumel,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

> 研究指出了大语言模型中特定语言处理的神经机制，并展示了通过操控这些神经元提高多语言性能的方法。

<details>
  <summary>Details</summary>

**Motivation:** 探究大型语言模型（LLMs）处理特定语言的神经机制，以及如何通过操控这些神经元改善模型的多语言能力。

**Method:** 通过LAPE方法分析神经元在不同语言中的分布，并使用语言算术系统地激活和去激活语言模型中的特定语言神经元。

**Result:** 发现控制语言行为的神经元主要集中在更深的层中，而非拉丁文字则表现为更高的专业化。相近语言之间存在重叠神经元，这反映了它们内部语言相近的关系。操作这些神经元在五个多语言任务中表现出更优的效果，尤其在资源丰富的语言上更显著。此外，跨语言的神经元控制能增强下游性能。

**Conclusion:** 调控特定语言的神经元可以显著改善模型的多语言任务表现，对于资源丰富的语言尤其有效。还揭示了在逐步去激活神经元时，模型内部的语言选择回退机制。代码公开发布在GitHub上。

**Abstract:** Large language models (LLMs) exhibit strong multilingual abilities, yet the
neural mechanisms behind language-specific processing remain unclear. We
analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and
Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying
neurons that control language behavior. Using the Language Activation
Probability Entropy (LAPE) method, we show that these neurons cluster in deeper
layers, with non-Latin scripts showing greater specialization. Related
languages share overlapping neurons, reflecting internal representations of
linguistic proximity.
  Through language arithmetics, i.e. systematic activation addition and
multiplication, we steer models to deactivate unwanted languages and activate
desired ones, outperforming simpler replacement approaches. These interventions
effectively guide behavior across five multilingual tasks: language forcing,
translation, QA, comprehension, and NLI. Manipulation is more successful for
high-resource languages, while typological similarity improves effectiveness.
We also demonstrate that cross-lingual neuron steering enhances downstream
performance and reveal internal "fallback" mechanisms for language selection
when neurons are progressively deactivated. Our code is made publicly available
at https://github.com/d-gurgurov/Language-Neurons-Manipulation.

</details>


### [26] [Multilingual Political Views of Large Language Models: Identification and Steering](https://arxiv.org/abs/2507.22623)
*Daniil Gurgurov,Katharina Trinley,Ivan Vykopal,Josef van Genabith,Simon Ostermann,Roberto Zamparelli*

Main category: cs.CL

> 研究通过对7个开源指令调整的大型语言模型进行多语言的政治倾向评价，发现模型倾向左翼政治立场，且可以通过具体技术干预来调整这些立场。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大多数研究仅评价了少数模型和语言，因此存在政治偏见在不同架构、规模和多语言设置中的一般性问题尚待解答。少有研究探讨这些偏见是否可以被有效控制。本研究旨在填补这些空白。

**Method:** 通过对7个现代开源指令调整的大型语言模型（包括LLaMA-3.1，Qwen-3，和Aya-Expanse）进行大规模研究来探究政治倾向，这些模型在14种语言中应用了政治指南测试，每个陈述都有11个语义等效的措辞以确保测量的稳健性。

**Result:** 研究结果显示，较大的模型在政治立场上更倾向于自由主义-左翼，各国语言之间和模型家族间存在显著差异。使用质心激活干预技术测试立场可操控性方面显示，该技术可以有效引导模型反应向不同的意识形态位置转变。

**Conclusion:** 研究表明，模型的政治立场有可操控性，同时较大的模型倾向于某一特定政治立场。使用质心激活干预技术，模型的反应可以被引导到替代的意识形态位置。

**Abstract:** Large language models (LLMs) are increasingly used in everyday tools and
applications, raising concerns about their potential influence on political
views. While prior research has shown that LLMs often exhibit measurable
political biases--frequently skewing toward liberal or progressive
positions--key gaps remain. Most existing studies evaluate only a narrow set of
models and languages, leaving open questions about the generalizability of
political biases across architectures, scales, and multilingual settings.
Moreover, few works examine whether these biases can be actively controlled.
  In this work, we address these gaps through a large-scale study of political
orientation in modern open-source instruction-tuned LLMs. We evaluate seven
models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using
the Political Compass Test with 11 semantically equivalent paraphrases per
statement to ensure robust measurement. Our results reveal that larger models
consistently shift toward libertarian-left positions, with significant
variations across languages and model families. To test the manipulability of
political stances, we utilize a simple center-of-mass activation intervention
technique and show that it reliably steers model responses toward alternative
ideological positions across multiple languages. Our code is publicly available
at https://github.com/d-gurgurov/Political-Ideologies-LLMs.

</details>


### [27] [Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment](https://arxiv.org/abs/2507.22676)
*Jia Li,Yang Wang,Wenhao Qian,Zhenzhen Hu,Richang Hong,Meng Wang*

Main category: cs.CL

> 该论文提出了一种用于多模态面试表现评估的框架，综合考虑视频、音频和文本信息，并从五个关键维度进行评估，实现了高度准确和公平的评估结果。

<details>
  <summary>Details</summary>

**Motivation:** 论文动机在于提高面试评估的全面性和公平性，通过一个全面的框架来更加客观地评估候选人的面试表现。

**Method:** 该论文提出了一个综合框架，用于评估候选人面试表现，通过融合理解视频、音频和文本三种模态的信息，提取六种候选人的回答，并从五个关键评估维度进行分析。框架采用了特定模态的特征提取器来编码异构数据流，并通过共享压缩多层感知器融合这些模态。此外，该框架使用了两级集成学习策略，通过独立回归头预测每个回答得分，然后使用平均池化机制聚合这些预测以生成最终的五个目标维度得分。

**Result:** 该框架在AVI Challenge 2025中获得了第一名，并实现了多维平均均方误差为0.1824，有效验证了框架的有效性和鲁棒性。

**Conclusion:** 通过捕捉到明确和隐含的多模态数据中的线索，该框架能够进行全面和无偏的面试表现评估，且证实了其在自动化和多模态面试评估方面的优越性和鲁棒性。

**Abstract:** Interview performance assessment is essential for determining candidates'
suitability for professional positions. To ensure holistic and fair
evaluations, we propose a novel and comprehensive framework that explores
``365'' aspects of interview performance by integrating \textit{three}
modalities (video, audio, and text), \textit{six} responses per candidate, and
\textit{five} key evaluation dimensions. The framework employs
modality-specific feature extractors to encode heterogeneous data streams and
subsequently fused via a Shared Compression Multilayer Perceptron. This module
compresses multimodal embeddings into a unified latent space, facilitating
efficient feature interaction. To enhance prediction robustness, we incorporate
a two-level ensemble learning strategy: (1) independent regression heads
predict scores for each response, and (2) predictions are aggregated across
responses using a mean-pooling mechanism to produce final scores for the five
target dimensions. By listening to the unspoken, our approach captures both
explicit and implicit cues from multimodal data, enabling comprehensive and
unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our
framework secured first place in the AVI Challenge 2025, demonstrating its
effectiveness and robustness in advancing automated and multimodal interview
performance assessment. The full implementation is available at
https://github.com/MSA-LMC/365Aspects.

</details>


### [28] [From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs](https://arxiv.org/abs/2507.22716)
*Jie He,Victor Gutierrez Basulto,Jeff Z. Pan*

Main category: cs.CL

> TIRESRAG-R1 framework is proposed for improving RAG models' reasoning and stability through an advanced reward system and a structured think-retrieve-reflect process.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of existing reinforcement learning (RL) based RAG models, which mainly rely on rewards for the final answer and ignore intermediate reasoning quality.

**Method:** The paper proposes TIRESRAG-R1, a novel framework incorporating a think-retrieve-reflect process, a multi-dimensional reward system, a difficulty-aware reweighting strategy, and training sample filtering to improve reasoning and response stability in RAG models.

**Result:** Experiments show TIRESRAG-R1 outperforms previous RAG methods on four multi-hop QA datasets and generalizes well to single-hop reasoning tasks.

**Conclusion:** The TIRESRAG-R1 framework enhances RAG models' reasoning capabilities by addressing three key failure modes: information insufficiency, faulty reasoning, and answer-reasoning inconsistency, leading to improved performance on complex tasks.

**Abstract:** Reinforcement learning-based retrieval-augmented generation (RAG) methods
enhance the reasoning abilities of large language models (LLMs). However, most
rely only on final-answer rewards, overlooking intermediate reasoning quality.
This paper analyzes existing RAG reasoning models and identifies three main
failure patterns: (1) information insufficiency, meaning the model fails to
retrieve adequate support; (2) faulty reasoning, where logical or content-level
flaws appear despite sufficient information; and (3) answer-reasoning
inconsistency, where a valid reasoning chain leads to a mismatched final
answer. We propose TIRESRAG-R1, a novel framework using a
think-retrieve-reflect process and a multi-dimensional reward system to improve
reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to
encourage thorough retrieval; (2) a reasoning quality reward to assess the
rationality and accuracy of the reasoning chain; and (3) a reflection reward to
detect and revise errors. It also employs a difficulty-aware reweighting
strategy and training sample filtering to boost performance on complex tasks.
Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms
prior RAG methods and generalizes well to single-hop tasks. The code and data
are available at: https://github.com/probe2/TIRESRAG-R1.

</details>


### [29] [Investigating Hallucination in Conversations for Low Resource Languages](https://arxiv.org/abs/2507.22720)
*Amit Das,Md. Najib Hasan,Souvika Sarkar,Zheng Zhang,Fatemeh Jamshidi,Tathagata Bhattacharya,Nilanjana Raychawdhury,Dongji Feng,Vinija Jain,Aman Chadha*

Main category: cs.CL

> 本研究分析了六种大语言模型在印地语、波斯语和汉语中的语言和事实错误。结果发现，这些模型在汉语中的幻觉回应极少，但在印地语和波斯语中有较多。

<details>
  <summary>Details</summary>

**Motivation:** 尽管许多研究关注于英语中的幻觉问题，本研究扩展了这一调查，关注大语言模型在除英语外的语言中的事实准确性，力求提高模型的可靠性和有效性。

**Method:** 本研究通过分析包含三种语言（印地语、波斯语和汉语）对话数据的综合数据集，来检查GPT-3.5、GPT-4o、Llama-3.1、Gemma-2.0、DeepSeek-R1和Qwen-3这些大语言模型在事实和语言错误方面的表现。

**Result:** 研究发现，这些语言模型在汉语中产生幻觉性回应的数量非常少，但在印地语和波斯语中则显著增多。

**Conclusion:** 该研究揭示了大语言模型在不同语言环境中出现幻觉性回应的差异，突显了在非英语背景下进行类似研究的重要性。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating text that closely resemble human writing. However, they often
generate factually incorrect statements, a problem typically referred to as
'hallucination'. Addressing hallucination is crucial for enhancing the
reliability and effectiveness of LLMs. While much research has focused on
hallucinations in English, our study extends this investigation to
conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a
comprehensive analysis of a dataset to examine both factual and linguistic
errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,
DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated
responses in Mandarin but generate a significantly higher number of
hallucinations in Hindi and Farsi.

</details>


### [30] [Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning](https://arxiv.org/abs/2507.22729)
*Benedikt Roth,Stephan Rappensperger,Tianming Qiu,Hamza Imamović,Julian Wörmann,Hao Shen*

Main category: cs.CL

> This paper focuses on developing techniques to adapt decoder-only LLMs for non-generative tasks by generating high-quality text embeddings using a combination of token embedding aggregation, prompt engineering, and contrastive fine-tuning, achieving top results on a clustering benchmark.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to develop a method to transform LLMs into generators of effective sentence or document-level embeddings that can be used for various non-generative NLP tasks. This is important because simply pooling token vectors into text embeddings discards important information that could be crucial for these tasks.

**Method:** Our study involves using several strategies to adapt pre-trained, decoder-only LLMs for creating text embeddings that are suitable for non-generative tasks, such as clustering, classification, or retrieval. These strategies include different token embedding aggregation techniques, task-specific prompt engineering, and text-level augmentation through contrastive fine-tuning.

**Result:** The research outcome reveals that the proposed methods lead to state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark. Analysis of the attention maps supports the effectiveness of the attention mechanism in focusing on semantically relevant parts of the text.

**Conclusion:** The conclusion of the paper is that pre-trained decoder-only LLMs can be effectively fine-tuned for creating high-quality text embeddings suitable for non-generative tasks, through prompt engineering and contrastive fine-tuning, with notable improvements in performance and semantic focus.

**Abstract:** Large Language Models (LLMs) have become a cornerstone in Natural Language
Processing (NLP), achieving impressive performance in text generation. Their
token-level representations capture rich, human-aligned semantics. However,
pooling these vectors into a text embedding discards crucial information.
Nevertheless, many non-generative downstream tasks, such as clustering,
classification, or retrieval, still depend on accurate and controllable
sentence- or document-level embeddings. We explore several adaptation
strategies for pre-trained, decoder-only LLMs: (i) various aggregation
techniques for token embeddings, (ii) task-specific prompt engineering, and
(iii) text-level augmentation via contrastive fine-tuning. Combining these
components yields state-of-the-art performance on the English clustering track
of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention
map further shows that fine-tuning shifts focus from prompt tokens to
semantically relevant words, indicating more effective compression of meaning
into the final hidden state. Our experiments demonstrate that LLMs can be
effectively adapted as text embedding models through a combination of prompt
engineering and resource-efficient contrastive fine-tuning on synthetically
generated positive pairs.

</details>


### [31] [Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index](https://arxiv.org/abs/2507.22744)
*Praveenkumar Katwe,Rakesh Chandra,Balabantaray Kali,Prasad Vittala*

Main category: cs.CL

> 本篇论文提出了一种使用奖励驱动框架优化实体幻觉指数（EHI）的方法，以减少语言模型在摘要生成时的实体幻觉现象，提升了生成摘要的质量。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在减少抽取式摘要中的幻觉现象，这是一个在现实世界中部署语言模型时的关键挑战。通过优化EHI，可以减少实体级别的幻觉，同时保持流畅性和信息量不受影响。

**Method:** 该论文介绍了一种基于奖励驱动的微调框架，用于优化实体幻觉指数（EHI），这是一种衡量生成摘要中名称实体存在性、正确性和基础程度的指标。该框架首先使用一个预训练语言模型生成基线摘要，并通过自动实体提取和匹配计算EHI分数。然后应用强化学习来微调模型参数，使用EHI作为奖励信号，以偏向于生成忠实于实体的输出。

**Result:** 实验结果显示提出的框架在不同数据集上一致改善了EHI，定性分析表明在没有降低流畅性和信息度的情况下明显减少了实体级别的幻觉。

**Conclusion:** 该方法展示了对摘要生成中的幻觉现象进行量化并减少的能力，并且可以不依赖于人类编写的真实性注释，实现了可扩展的微调。此外，作者提供了可复制的Colab工具，以供进一步研究使用。

**Abstract:** Reducing hallucinations in abstractive summarization remains a critical
challenge for deploying language models (LMs) in real-world settings. In this
work, we introduce a rewarddriven fine-tuning framework that explicitly
optimizes for Entity Hallucination Index (EHI), a metric designed to quantify
the presence, correctness, and grounding of named entities in generated
summaries. Given a corpus of meeting transcripts, we first generate baseline
summaries using a pre-trained LM and compute EHI scores via automatic entity
extraction and matching. We then apply reinforcement learning to fine-tune the
model parameters, using EHI as a reward signal to bias generation toward
entity-faithful outputs. Our approach does not rely on human-written factuality
annotations, enabling scalable fine-tuning. Experiments demonstrate consistent
improvements in EHI across datasets, with qualitative analysis revealing a
significant reduction in entity-level hallucinations without degradation in
fluency or informativeness. We release a reproducible Colab pipeline,
facilitating further research on hallucination-aware model fine-tuning using
lightweight, hallucintion metrics like EHI.

</details>


### [32] [CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset](https://arxiv.org/abs/2507.22752)
*Jindřich Libovický,Jindřich Helcl,Andrei Manea,Gianluca Vico*

Main category: cs.CL

> 该论文提出了一个结合文本和视觉模态的开放性区域问答基准，强调了大型语言模型在区域知识方面的短缺，并指出现存自动评估指标与人工判断之间存在差距。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在评估当前大型语言模型在区域知识方面的不足，并通过人的判断来改进现有自动评估指标的可靠性。同时，也为跨语言生成一致性和开放式问题回答的评估指标的发展提供了数据资源。

**Method:** 该论文创建了一个结合文本和视觉模态的开放性区域问答基准，并使用了最先进的大型语言模型（LLMs）作为基线对其进行评估。数据集由来自捷克、斯洛伐克和乌克兰的母语者基于维基百科手动编写，包含英文翻译。问题类型包括纯文本和需要视觉理解的问题。评估方法除了模型自动生成的答案外，还加入了人工判断。

**Result:** 研究结果展示了现有LLMs在区域知识方面的大量信息空白，并且除了基于LLM的评估外，自动评估指标和人类判断之间存在极小的相关性。

**Conclusion:** 该论文发布了此数据集，旨在评估LLMs的区域知识、研究跨语言生成一致性以及促进开放式问题回答评估指标的发展。

**Abstract:** We introduce a benchmark for open-ended regional question answering that
encompasses both textual and visual modalities. We also provide strong
baselines using state-of-the-art large language models (LLMs). Our dataset
consists of manually curated questions and answers grounded in Wikipedia,
created by native speakers from Czechia, Slovakia, and Ukraine, with
accompanying English translations. It includes both purely textual questions
and those requiring visual understanding. As a baseline, we evaluate
state-of-the-art LLMs through prompting and complement this with human
judgments of answer correctness. Using these human evaluations, we analyze the
reliability of existing automatic evaluation metrics. Our baseline results
highlight a significant gap in regional knowledge among current LLMs. Moreover,
apart from LLM-based evaluation, there is minimal correlation between automated
metrics and human judgment. We release this dataset as a resource to (1) assess
regional knowledge in LLMs, (2) study cross-lingual generation consistency in a
challenging setting, and (3) advance the development of evaluation metrics for
open-ended question answering.

</details>


### [33] [Opportunities and Challenges of LLMs in Education: An NLP Perspective](https://arxiv.org/abs/2507.22753)
*Sowmya Vajjala,Bashar Alhafni,Stefano Bannò,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

> 本文探讨了大型语言模型在教育中的角色，特别是在辅助和评估两个应用场景中的影响，并提出了未来发展的新方向和面临的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型为教育领域带来了新的机遇，本文旨在研究其在教育NLP中的影响，特别是在辅助教学和评估方面的作用。

**Method:** 本文围绕阅读、写作、口语和辅导四个维度，探讨了大型语言模型在教育应用中的影响，以及对未来发展方向的展望和面临的挑战。

**Result:** 本文为NLP研究者和实践者提供了关于大型语言模型在教育领域中未来角色的全方位概述。

**Conclusion:** 本文侧重于语言学习和NLP支持的教育应用，为研究者和实践者提供了有价值的指导，以开发未来的教育应用。

**Abstract:** Interest in the role of large language models (LLMs) in education is
increasing, considering the new opportunities they offer for teaching,
learning, and assessment. In this paper, we examine the impact of LLMs on
educational NLP in the context of two main application scenarios: {\em
assistance} and {\em assessment}, grounding them along the four dimensions --
reading, writing, speaking, and tutoring. We then present the new directions
enabled by LLMs, and the key challenges to address. We envision that this
holistic overview would be useful for NLP researchers and practitioners
interested in exploring the role of LLMs in developing language-focused and
NLP-enabled educational applications of the future.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?](https://arxiv.org/abs/2507.22099)
*Shuqing Li,Qiang Chen,Xiaoxue Ren,Michael R. Lyu*

Main category: cs.CV

> 本文研究了物理引擎中的物理失效问题，提出失效分类法，评估了多种检测技术，并根据开发者反馈提供改进建议。

<details>
  <summary>Details</summary>

**Motivation:** 物理引擎中的物理失效可能会影响软件的可靠性、用户体验，甚至在自动驾驶车辆或医疗机器人中造成关键故障，当前测试方法无法有效检测这些复杂的物理失效。

**Method:** 通过大规模实证研究来定义和检测物理引擎中的物理失效，包括深度学习、基于提示的技术和大型多模态模型的综合评估，并从开发者经验中提取可操作的见解。

**Result:** 提出了物理失效表现的分类法，评估了多种检测方法的有效性，并通过开发者经验提供了改进检测方法的实际见解。

**Conclusion:** 本研究通过对物理引擎中物理失效的大规模实证研究，揭示了这些失效的表现形式，评估了不同检测技术的有效性，并基于开发者反馈提供了改进物理失效检测的建议。

**Abstract:** Physics Engines (PEs) are fundamental software frameworks that simulate
physical interactions in applications ranging from entertainment to
safety-critical systems. Despite their importance, PEs suffer from physics
failures, deviations from expected physical behaviors that can compromise
software reliability, degrade user experience, and potentially cause critical
failures in autonomous vehicles or medical robotics. Current testing approaches
for PE-based software are inadequate, typically requiring white-box access and
focusing on crash detection rather than semantically complex physics failures.
This paper presents the first large-scale empirical study characterizing
physics failures in PE-based software. We investigate three research questions
addressing the manifestations of physics failures, the effectiveness of
detection techniques, and developer perceptions of current detection practices.
Our contributions include: (1) a taxonomy of physics failure manifestations;
(2) a comprehensive evaluation of detection methods including deep learning,
prompt-based techniques, and large multimodal models; and (3) actionable
insights from developer experiences for improving detection approaches. To
support future research, we release PhysiXFails, code, and other materials at
https://sites.google.com/view/physics-failure-detection.

</details>


### [35] [Trade-offs in Image Generation: How Do Different Dimensions Interact?](https://arxiv.org/abs/2507.22100)
*Sicheng Zhang,Binzhu Xie,Zhonghao Yan,Yuli Zhang,Donghao Zhou,Xiaofei Chen,Shi Qiu,Jiaqi Liu,Guoyang Xie,Zhichao Lu*

Main category: cs.CV

> 研究提出了TRIG-Bench和TRIGScore来全面评估生成模型的性能，并通过维度权衡图揭示了不同模型间的权衡关系。

<details>
  <summary>Details</summary>

**Motivation:** 为了评估和分析文本到图像(Text-to-Image, T2I)和图像到图像(Image-to-Image, I2I)生成模型的性能，特别是解决现有方法在评估模型的多维度性能（如现实性、原创性等）方面的不足。

**Method:** 引入了TRIG-Bench来涵盖10个不同的维度，包含40,200个样本，并开发了TRIGScore来自动适应不同维度的评估。进行了14个模型的评估，并提出了关系识别系统生成维度权衡图。

**Result:** bstract介绍了TRIG-Bench和TRIGScore，用于评估文本到图像和图像到图像生成模型的性能，并讨论了这些模型之间的维度权衡。通过开发维度权衡图来更全面地理解各种生成模型的性能。

**Conclusion:** 实验表明，维度权衡图可以一致性且全面地提供各种生成模型之间维度权衡的理解。还能通过在维度权衡图上进行微调来减轻模型在维度上的弱点，从而改善整体性能。

**Abstract:** Model performance in text-to-image (T2I) and image-to-image (I2I) generation
often depends on multiple aspects, including quality, alignment, diversity, and
robustness. However, models' complex trade-offs among these dimensions have
rarely been explored due to (1) the lack of datasets that allow fine-grained
quantification of these trade-offs, and (2) the use of a single metric for
multiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in
Image Generation), which spans 10 dimensions (Realism, Originality, Aesthetics,
Content, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains
40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we
develop TRIGScore, a VLM-as-judge metric that automatically adapts to various
dimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I
and I2I tasks. In addition, we propose the Relation Recognition System to
generate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among
model-specific capabilities. Our experiments demonstrate that DTM consistently
provides a comprehensive understanding of the trade-offs between dimensions for
each type of generative model. Notably, we show that the model's
dimension-specific weaknesses can be mitigated through fine-tuning on DTM to
enhance overall performance. Code is available at:
https://github.com/fesvhtr/TRIG

</details>


### [36] [AI in Agriculture: A Survey of Deep Learning Techniques for Crops, Fisheries and Livestock](https://arxiv.org/abs/2507.22101)
*Umair Nawaz,Muhammad Zaigham Zaheer,Fahad Shahbaz Khan,Hisham Cholakkal,Salman Khan,Rao Muhammad Anwer*

Main category: cs.CV

> 这篇综述提供了关于农业领域中机器学习和深度学习技术应用的全面回顾，并讨论了实施挑战和未来研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 农业部门面临如气候变动、资源限制以及可持续管理需求等重大挑战。解决这些问题需要高效、精准且可扩展的技术解决方案，突显了人工智能的重要性。

**Method:** 本综述系统地回顾了超过200项研究，涵盖了传统的机器学习方法、先进的深度学习技术（如视觉转换器）和最近的视觉语言基础模型（如CLIP）在农业领域中的应用，重点关注诸如作物病害检测、牲畜健康管理以及水生生物监测等多样化任务。

**Result:** 综述还讨论了主要的实施挑战，包括数据变动性和实验方面的问题：数据集、性能评估指标和地理重点。

**Conclusion:** 我们通过讨论潜在的开放研究方向，强调了多模态数据集成、高效边缘设备部署和适用于多样化农业环境的领域自适应AI模型的必要性。

**Abstract:** Crops, fisheries and livestock form the backbone of global food production,
essential to feed the ever-growing global population. However, these sectors
face considerable challenges, including climate variability, resource
limitations, and the need for sustainable management. Addressing these issues
requires efficient, accurate, and scalable technological solutions,
highlighting the importance of artificial intelligence (AI). This survey
presents a systematic and thorough review of more than 200 research works
covering conventional machine learning approaches, advanced deep learning
techniques (e.g., vision transformers), and recent vision-language foundation
models (e.g., CLIP) in the agriculture domain, focusing on diverse tasks such
as crop disease detection, livestock health management, and aquatic species
monitoring. We further cover major implementation challenges such as data
variability and experimental aspects: datasets, performance evaluation metrics,
and geographical focus. We finish the survey by discussing potential open
research directions emphasizing the need for multimodal data integration,
efficient edge-device deployment, and domain-adaptable AI models for diverse
farming environments. Rapid growth of evolving developments in this field can
be actively tracked on our project page:
https://github.com/umair1221/AI-in-Agriculture

</details>


### [37] [Color as the Impetus: Transforming Few-Shot Learner](https://arxiv.org/abs/2507.22136)
*Chaofei Qi,Zhitai Liu,Jianbin Qiu*

Main category: cs.CV

> 本研究通过模仿人类的色彩感知能力，提出了ColorSense Learner，一种结合生物启发的元学习框架和基于知识蒸馏的ColorSense Distiller，以增强少样本学习的通用性、鲁棒性和跨域分类能力。

<details>
  <summary>Details</summary>

**Motivation:** 人类具有先天的元学习能力，这部分归因于其出色的色彩感知能力。以往的元学习方法主要集中在抽象特征的分类上，忽视了色彩信息这一最直观的视觉特征，本文旨在通过模仿人类的色彩感知，增强少样本学习的性能。

**Method:** 模拟人类颜色感知机制，提出了一种创新的少样本学习方法——ColorSense Learner。该方法利用通道间特征提取和互动学习，通过强调不同通道的色彩信息来过滤无关特征，捕捉辨别性特征。此外，还引入了基于知识蒸馏的元蒸馏器ColorSense Distiller，利用教师网络知识提高学生网络的元学习能力。

**Result:** 通过在11个少样本数据集上的粗细粒度和跨域实验验证，实验结果表明本文方法具备强大的泛化能力、鲁棒性和转移性，且在色彩感知角度下的少样本分类任务上表现出色。

**Conclusion:** 我们的方法成功展示了颜色感知在少样本学习中的作用，证明了ColorSense Learner框架结合ColorSense Distiller蒸馏器能有效增强少样本学习的性能。

**Abstract:** Humans possess innate meta-learning capabilities, partly attributable to
their exceptional color perception. In this paper, we pioneer an innovative
viewpoint on few-shot learning by simulating human color perception mechanisms.
We propose the ColorSense Learner, a bio-inspired meta-learning framework that
capitalizes on inter-channel feature extraction and interactive learning. By
strategically emphasizing distinct color information across different channels,
our approach effectively filters irrelevant features while capturing
discriminative characteristics. Color information represents the most intuitive
visual feature, yet conventional meta-learning methods have predominantly
neglected this aspect, focusing instead on abstract feature differentiation
across categories. Our framework bridges the gap via synergistic color-channel
interactions, enabling better intra-class commonality extraction and larger
inter-class differences. Furthermore, we introduce a meta-distiller based on
knowledge distillation, ColorSense Distiller, which incorporates prior teacher
knowledge to augment the student network's meta-learning capacity. We've
conducted comprehensive coarse/fine-grained and cross-domain experiments on
eleven few-shot benchmarks for validation. Numerous experiments reveal that our
methods have extremely strong generalization ability, robustness, and
transferability, and effortless handle few-shot classification from the
perspective of color perception.

</details>


### [38] [Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset](https://arxiv.org/abs/2507.22152)
*A. Piffer,J. A. Buchner,A. G. Gennari,P. Grehten,S. Sirin,E. Ross,I. Ezhov,M. Rosier,J. C. Peeken,M. Piraud,B. Menze,A. Guerreiro Stücklin,A. Jakab,F. Kofler*

Main category: cs.CV

> 本研究评估了一种基于3D nnU-Net的深度学习模型在儿科脑肿瘤不同子区域分割中的表现，发现其在全肿瘤和T2高信号区域的分割中表现稳健，但对增强肿瘤和囊性成分的分割准确性较低，支持了MRI协议简化和自动化的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于儿科脑肿瘤的组织学、分子亚型和影像特征各异，其诊断和治疗具有挑战性。基于深度学习的分割技术为肿瘤区分提供了可能的工具，但其在儿科脑肿瘤（PBTs）不同亚型和MRI协议下的表现尚不确定，本研究旨在评估其可行性和准确性。

**Method:** 使用了单中心回顾性队列，包含174名患有高-低级别胶质瘤（HGG、LGG）、髓母细胞瘤（MB）、室管膜瘤及其他罕见类型的儿科脑肿瘤（PBTs）儿童患者。MRI序列包括T1、T1增强（T1-C）、T2和FLAIR。研究采用手动注释的方式对四个肿瘤子区域进行标注：全肿瘤（WT）、T2高信号（T2H）、增强肿瘤（ET）及囊性成分（CC）。通过3D nnU-Net模型进行训练和测试（训练集/测试集比例：121/53），并使用Dice相似系数（DSC）评估了其分割性能，同时对比了人工注释的一致性和变异性。

**Result:** 模型在WT和T2H分割方面的表现稳健，平均DSC值为0.85，与人类注释者的一致性相当（平均DSC值为0.86）。ET的分割准确度中等（平均DSC值为0.75），而CC的分割性能较差。分割准确性受肿瘤类型、MRI序列组合及肿瘤位置的影响。值得注意的是，仅使用T1、T1-C和T2也可以达到几乎与完整协议相当的结果。

**Conclusion:** 对于PBTs，特别是T2H和WT的分割，DL是可行的。但对于ET和CC的分割仍面临挑战，这表明需要进一步改进。这些发现支持了MRI协议简化及自动化操作的潜力，有助于增强儿科神经肿瘤学的体积评估，简化工作流程。

**Abstract:** Background Brain tumours are the most common solid malignancies in children,
encompassing diverse histological, molecular subtypes and imaging features and
outcomes. Paediatric brain tumours (PBTs), including high- and low-grade
gliomas (HGG, LGG), medulloblastomas (MB), ependymomas, and rarer forms, pose
diagnostic and therapeutic challenges. Deep learning (DL)-based segmentation
offers promising tools for tumour delineation, yet its performance across
heterogeneous PBT subtypes and MRI protocols remains uncertain. Methods A
retrospective single-centre cohort of 174 paediatric patients with HGG, LGG,
medulloblastomas (MB), ependymomas, and other rarer subtypes was used. MRI
sequences included T1, T1 post-contrast (T1-C), T2, and FLAIR. Manual
annotations were provided for four tumour subregions: whole tumour (WT),
T2-hyperintensity (T2H), enhancing tumour (ET), and cystic component (CC). A 3D
nnU-Net model was trained and tested (121/53 split), with segmentation
performance assessed using the Dice similarity coefficient (DSC) and compared
against intra- and inter-rater variability. Results The model achieved robust
performance for WT and T2H (mean DSC: 0.85), comparable to human annotator
variability (mean DSC: 0.86). ET segmentation was moderately accurate (mean
DSC: 0.75), while CC performance was poor. Segmentation accuracy varied by
tumour type, MRI sequence combination, and location. Notably, T1, T1-C, and T2
alone produced results nearly equivalent to the full protocol. Conclusions DL
is feasible for PBTs, particularly for T2H and WT. Challenges remain for ET and
CC segmentation, highlighting the need for further refinement. These findings
support the potential for protocol simplification and automation to enhance
volumetric assessment and streamline paediatric neuro-oncology workflows.

</details>


### [39] [Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception](https://arxiv.org/abs/2507.22194)
*Christian Ellis,Maggie Wigness,Craig Lennon,Lance Fiondella*

Main category: cs.CV

> 论文介绍了Frontier-Seg，这是一种用于从移动机器人视频流中获得时间一致的无监督地形分割的方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的监督语义分割方法依赖于昂贵的数据收集和劳动密集型的地面真相标记来训练深度模型。而在未彩排、非结构化的环境中，没有标记数据，并且语义类别可能模糊或特定于领域。此外，现有的无监督分割方法通常在单个帧上操作，缺乏时间一致性，而这对于非结构化环境中的鲁棒感知至关重要。

**Method:** Frontier-Seg采用从基础模型骨干（特别是DINOv2）提取的超像素级特征进行聚类，并在帧间强制时间一致性，以无监督的方式识别持久的地形边界或前沿。

**Result:** 在包括RUGD和RELLIS-3D在内的多样化的基准数据集上的评估显示，Frontier-Seg能够在非结构化的越野环境中进行无监督分割。

**Conclusion:** Frontier-Seg展示了其在非结构化越野环境中进行有效无监督分割的能力。

**Abstract:** Rapid progress in terrain-aware autonomous ground navigation has been driven
by advances in supervised semantic segmentation. However, these methods rely on
costly data collection and labor-intensive ground truth labeling to train deep
models. Furthermore, autonomous systems are increasingly deployed in
unrehearsed, unstructured environments where no labeled data exists and
semantic categories may be ambiguous or domain-specific. Recent zero-shot
approaches to unsupervised segmentation have shown promise in such settings but
typically operate on individual frames, lacking temporal consistency-a critical
property for robust perception in unstructured environments. To address this
gap we introduce Frontier-Seg, a method for temporally consistent unsupervised
segmentation of terrain from mobile robot video streams. Frontier-Seg clusters
superpixel-level features extracted from foundation model
backbones-specifically DINOv2-and enforces temporal consistency across frames
to identify persistent terrain boundaries or frontiers without human
supervision. We evaluate Frontier-Seg on a diverse set of benchmark
datasets-including RUGD and RELLIS-3D-demonstrating its ability to perform
unsupervised segmentation across unstructured off-road environments.

</details>


### [40] [SmartCLIP: Modular Vision-language Alignment with Identification Guarantees](https://arxiv.org/abs/2507.22264)
*Shaoan Xie,Lingjing Kong,Yujia Zheng,Yu Yao,Zeyu Tang,Eric P. Xing,Guangyi Chen,Kun Zhang*

Main category: cs.CV

> 本文提出了一种名为\ours的新方法，该方法能够灵活地在不同粒度级别上对齐文本和视觉表示，并能解耦视觉信息以捕捉细粒度的文本概念，从而提高了模型在处理信息对齐问题上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决CLIP模型在处理图像-文本数据集时存在的潜在信息对齐问题，以及由长文本描述引起的纠缠表示问题。这些问题限制了模型在使用短提示的特定下游任务上的泛化能力。

**Method:** 本文提出了一种新的方法\ours，该方法能够在不同的粒度级别上建立灵活的文本和视觉表示对齐。这种方法可以确保模型不仅能够完整地保留跨模态的语义信息，还能解耦视觉表示，以捕捉细粒度的文本概念。

**Result:** 实验结果表明，所提出的方法能够很好地处理信息对齐问题，并在各种任务上表现出了优越的性能，验证了本文的识别理论。

**Conclusion:** 所提出的方法\ours成功解决了CLIP模型存在的视觉和文本表示纠缠问题，证明了其提高模型在处理下游任务泛化能力的潜力。

**Abstract:** Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning}
has emerged as a pivotal model in computer vision and multimodal learning,
achieving state-of-the-art performance at aligning visual and textual
representations through contrastive learning. However, CLIP struggles with
potential information misalignment in many image-text datasets and suffers from
entangled representation. On the one hand, short captions for a single image in
datasets like MSCOCO may describe disjoint regions in the image, leaving the
model uncertain about which visual features to retain or disregard. On the
other hand, directly aligning long captions with images can lead to the
retention of entangled details, preventing the model from learning
disentangled, atomic concepts -- ultimately limiting its generalization on
certain downstream tasks involving short prompts.
  In this paper, we establish theoretical conditions that enable flexible
alignment between textual and visual representations across varying levels of
granularity. Specifically, our framework ensures that a model can not only
\emph{preserve} cross-modal semantic information in its entirety but also
\emph{disentangle} visual representations to capture fine-grained textual
concepts. Building on this foundation, we introduce \ours, a novel approach
that identifies and aligns the most relevant visual and textual representations
in a modular manner. Superior performance across various tasks demonstrates its
capability to handle information misalignment and supports our identification
theory. The code is available at https://github.com/Mid-Push/SmartCLIP.

</details>


### [41] [HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional Neural Networks for Retinal Image Classification](https://arxiv.org/abs/2507.22274)
*Faisal Ahmed*

Main category: cs.CV

> 本研究提出了一种基于HOG-CNN混合特征提取模型的自动且可解释的临床决策支持框架，用于视网膜疾病的筛查。模型在三个公开数据集上展现出了高精度和高AUC值，表现优于现有方法，并且适合资源受限的临床环境。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决传统视网膜疾病诊断流程依赖人工分析以及耗费时间与资源的问题。通过自动化流程来提高效率和准确性。

**Method:** 研究方法是开发一种结合手工直方图（HOG）特征和深度卷积神经网络（CNN）表示的混合模型（HOG-CNN），以捕获视网膜图像中的局部纹理特征和高层次语义特征。

**Result:** 研究发现该模型在APTOS 2019、ORIGA和IC-AMD三个数据集上的表现优异，如在二分类DR诊断中达到了98.5%的准确率和99.2的AUC值，展示了其优越性和可行性。

**Conclusion:** 结论是HOG-CNN模型不仅是有效的，而且因其轻量级和可解释性特性，非常适用于资源有限的临床环境。

**Abstract:** The analysis of fundus images is critical for the early detection and
diagnosis of retinal diseases such as Diabetic Retinopathy (DR), Glaucoma, and
Age-related Macular Degeneration (AMD). Traditional diagnostic workflows,
however, often depend on manual interpretation and are both time- and
resource-intensive. To address these limitations, we propose an automated and
interpretable clinical decision support framework based on a hybrid feature
extraction model called HOG-CNN. Our key contribution lies in the integration
of handcrafted Histogram of Oriented Gradients (HOG) features with deep
convolutional neural network (CNN) representations. This fusion enables our
model to capture both local texture patterns and high-level semantic features
from retinal fundus images. We evaluated our model on three public benchmark
datasets: APTOS 2019 (for binary and multiclass DR classification), ORIGA (for
Glaucoma detection), and IC-AMD (for AMD diagnosis); HOG-CNN demonstrates
consistently high performance. It achieves 98.5\% accuracy and 99.2 AUC for
binary DR classification, and 94.2 AUC for five-class DR classification. On the
IC-AMD dataset, it attains 92.8\% accuracy, 94.8\% precision, and 94.5 AUC,
outperforming several state-of-the-art models. For Glaucoma detection on ORIGA,
our model achieves 83.9\% accuracy and 87.2 AUC, showing competitive
performance despite dataset limitations. We show, through comprehensive
appendix studies, the complementary strength of combining HOG and CNN features.
The model's lightweight and interpretable design makes it particularly suitable
for deployment in resource-constrained clinical environments. These results
position HOG-CNN as a robust and scalable tool for automated retinal disease
screening.

</details>


### [42] [AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data](https://arxiv.org/abs/2507.22291)
*Christopher F. Brown,Michal R. Kazmierski,Valerie J. Pasquarella,William J. Rucklidge,Masha Samsikova,Chenhui Zhang,Evan Shelhamer,Estefania Lahera,Olivia Wiles,Simon Ilyushchenko,Noel Gorelick,Lihui Lydia Zhang,Sophia Alj,Emily Schechter,Sean Askay,Oliver Guinan,Rebecca Moore,Alexis Boukouvalas,Pushmeet Kohli*

Main category: cs.CV

> 介绍了AlphaEarth Foundations，一种新型嵌入场模型，能够生成准确且高效的地理空间表示图，超越了以往的所有特征化方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于地球观测数据的标注稀缺，需要大量的物理测量和观察工作，为了改进地图生成和监测系统，提出了AlphaEarth Foundations。

**Method:** 通过整合空间、时间和测量上下文，AlphaEarth Foundations生成了一种高度通用的地理空间表示法。

**Result:** AlphaEarth Foundations在多样化制图评估中超越了所有之前测试的特征化方法，且无需重新训练。

**Conclusion:** 该模型可支持从局部到全球范围的地图和监测系统的高效生成，且计划发布2017-2024年的全球年度分析准备嵌入字段图层数据集。

**Abstract:** Unprecedented volumes of Earth observation data are continually collected
around the world, but high-quality labels remain scarce given the effort
required to make physical measurements and observations. This has led to
considerable investment in bespoke modeling efforts translating sparse labels
into maps. Here we introduce AlphaEarth Foundations, an embedding field model
yielding a highly general, geospatial representation that assimilates spatial,
temporal, and measurement contexts across multiple sources, enabling accurate
and efficient production of maps and monitoring systems from local to global
scales. The embeddings generated by AlphaEarth Foundations are the only to
consistently outperform all previous featurization approaches tested on a
diverse set of mapping evaluations without re-training. We will release a
dataset of global, annual, analysis-ready embedding field layers from 2017
through 2024.

</details>


### [43] [LAMA-Net: A Convergent Network Architecture for Dual-Domain Reconstruction](https://arxiv.org/abs/2507.22316)
*Chi Ding,Qingchao Zhang,Ge Wang,Xiaojing Ye,Yunmei Chen*

Main category: cs.CV

> 本文提出了一种利用图像与测量域互补信息进行图像重建的可学习变分模型，并证明了LAMA算法的收敛性，从而提升了模型的稳定性和鲁棒性。通过实验验证了LAMA-Net/iLAMA-Net的性能优势。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机是在图像重建过程中，结合图像和测量域的互补信息，提出一种高效且鲁棒的可学习变分模型。

**Method:** 我们提出了一种可学习的变分模型，该模型从图像和测量域中学习特征并利用互补信息来进行图像重建。我们引入了一种名为LAMA的学习交替最小化算法，该算法通过对抗凸和非光滑优化问题进行处理，通过在邻近交替框架中结合残差学习架构来实现。在这个工作中，我们的目标是提供一个完整的关于LAMA的收敛性证明，并展示LAMA的指定子序列的所有累积点必须是问题的Clarke平稳点。LAMA直接产生一个高度可解释的神经网络架构，称为LAMA-Net。此外，我们证明LAMA的收敛性属性可以大大提高LAMA-Net的稳定性和鲁棒性。我们还展示了通过结合一个适当设计的生成合适初始值的网络（我们称之为iLAMA-Net），可以进一步提升LAMA-Net的性能。

**Result:** 我们的实验对比了几种最新方法在稀疏视图计算机断层扫描的数据集上测试了LAMA-Net/iLAMA-Net，证明了其卓越的性能。

**Conclusion:** 通过引入LAMA和开发LAMA-Net，我们提供了一个高度解释性的神经网络架构，结合了收敛性证明和实验验证，显示了其在稳定性和性能上的提升。

**Abstract:** We propose a learnable variational model that learns the features and
leverages complementary information from both image and measurement domains for
image reconstruction. In particular, we introduce a learned alternating
minimization algorithm (LAMA) from our prior work, which tackles two-block
nonconvex and nonsmooth optimization problems by incorporating a residual
learning architecture in a proximal alternating framework. In this work, our
goal is to provide a complete and rigorous convergence proof of LAMA and show
that all accumulation points of a specified subsequence of LAMA must be Clarke
stationary points of the problem. LAMA directly yields a highly interpretable
neural network architecture called LAMA-Net. Notably, in addition to the
results shown in our prior work, we demonstrate that the convergence property
of LAMA yields outstanding stability and robustness of LAMA-Net in this work.
We also show that the performance of LAMA-Net can be further improved by
integrating a properly designed network that generates suitable initials, which
we call iLAMA-Net. To evaluate LAMA-Net/iLAMA-Net, we conduct several
experiments and compare them with several state-of-the-art methods on popular
benchmark datasets for Sparse-View Computed Tomography.

</details>


### [44] [Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment](https://arxiv.org/abs/2507.22321)
*Yuzhen Gao,Qianqian Wang,Yongheng Sun,Cui Wang,Yongquan Liang,Mingxia Liu*

Main category: cs.CV

> The paper proposes a Collaborative Domain Adaptation (CDA) framework for accurate late-life depression (LLD) detection using T1-weighted MRIs, combining a Vision Transformer (ViT) and a Convolutional Neural Network (CNN) to handle limited sample sizes and domain heterogeneity.

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of LLD detection in MRI scans by overcoming the challenges of limited sample sizes and domain heterogeneity between datasets while enabling reliable model training and generalization.

**Method:** The CDA framework for LLD detection uses a combination of a Vision Transformer (ViT) for global anatomical context and a Convolutional Neural Network (CNN) for local structural features. It includes stages of supervised training, self-supervised target feature adaptation, and collaborative training with pseudo-labeled and augmented data.

**Result:** Experiments on multi-site T1-weighted MRI data show that the CDA framework outperforms existing unsupervised domain adaptation methods in LLD detection.

**Conclusion:** The CDA framework offers a robust solution for LLD detection, demonstrating better performance and generalization capabilities compared to current methods, which is crucial for improving diagnosis and monitoring of LLD in clinical practice.

**Abstract:** Accurate identification of late-life depression (LLD) using structural brain
MRI is essential for monitoring disease progression and facilitating timely
intervention. However, existing learning-based approaches for LLD detection are
often constrained by limited sample sizes (e.g., tens), which poses significant
challenges for reliable model training and generalization. Although
incorporating auxiliary datasets can expand the training set, substantial
domain heterogeneity, such as differences in imaging protocols, scanner
hardware, and population demographics, often undermines cross-domain
transferability. To address this issue, we propose a Collaborative Domain
Adaptation (CDA) framework for LLD detection using T1-weighted MRIs. The CDA
leverages a Vision Transformer (ViT) to capture global anatomical context and a
Convolutional Neural Network (CNN) to extract local structural features, with
each branch comprising an encoder and a classifier. The CDA framework consists
of three stages: (a) supervised training on labeled source data, (b)
self-supervised target feature adaptation and (c) collaborative training on
unlabeled target data. We first train ViT and CNN on source data, followed by
self-supervised target feature adaptation by minimizing the discrepancy between
classifier outputs from two branches to make the categorical boundary clearer.
The collaborative training stage employs pseudo-labeled and augmented
target-domain MRIs, enforcing prediction consistency under strong and weak
augmentation to enhance domain robustness and generalization. Extensive
experiments conducted on multi-site T1-weighted MRI data demonstrate that the
CDA consistently outperforms state-of-the-art unsupervised domain adaptation
methods.

</details>


### [45] [UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views](https://arxiv.org/abs/2507.22342)
*Yuki Fujimura,Takahiro Kushida,Kazuya Kitano,Takuya Funatomi,Yasuhiro Mukaigawa*

Main category: cs.CV

> A novel adaptation of pose-free, feed-forward 3D Gaussian Splatting (3DGS) method is introduced to effectively process unfavorable input views, enhancing the model's applicability in varying real-world conditions.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address a limitation of common feed-forward approaches in deep learning, which have been trained with an assumption of favorable input views, making them less effective for real-world scenarios where camera poses are unknown or varying.

**Method:** This paper introduces a pose-free, feed-forward 3D Gaussian Splatting (3DGS) framework adapted for handling unfavorable input views. It leverages previously learned priors from favorable images and integrates low-rank adaptation (LoRA) layers and a Gaussian adapter module to enhance training models for rendering less favorable views accurately.

**Result:** The proposed method was found to be effective in handling unfavorable input views, as validated by experiments on images from the Google Scanned Objects and OmniObject3D datasets.

**Conclusion:** The novel approach enhances the applicability of feed-forward 3DGS models by adding a capability to handle less favorable camera poses, expanding potential applications to real-world scenarios.

**Abstract:** This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS)
framework designed to handle unfavorable input views. A common rendering setup
for training feed-forward approaches places a 3D object at the world origin and
renders it from cameras pointed toward the origin -- i.e., from favorable
views, limiting the applicability of these models to real-world scenarios
involving varying and unknown camera poses. To overcome this limitation, we
introduce a novel adaptation framework that enables pretrained pose-free
feed-forward 3DGS models to handle unfavorable views. We leverage priors
learned from favorable images by feeding recentered images into a pretrained
model augmented with low-rank adaptation (LoRA) layers. We further propose a
Gaussian adapter module to enhance the geometric consistency of the Gaussians
derived from the recentered inputs, along with a Gaussian alignment method to
render accurate target views for training. Additionally, we introduce a new
training strategy that utilizes an off-the-shelf dataset composed solely of
favorable images. Experimental results on both synthetic images from the Google
Scanned Objects dataset and real images from the OmniObject3D dataset validate
the effectiveness of our method in handling unfavorable input views.

</details>


### [46] [DeltaVLM: Interactive Remote Sensing Image Change Analysis via Instruction-guided Difference Perception](https://arxiv.org/abs/2507.22346)
*Pei Deng,Wenqian Zhou,Hanlin Wu*

Main category: cs.CV

> 本文提出了遥感图像变化分析（RSICA）这一新范式，结合变化检测和视觉问答，以支持双向遥感图像中多轮、指令引导变化探索，并提出了DeltaVLM架构，实现了在变化分析数据集上的出色表现。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于提高对多时态卫星影图像中土地覆盖变化的精确解释能力，并克服现有方法提供单一变化掩码或静态描述的限制，以支持互动、查询驱动的分析。

**Method:** 该论文提出了一种新的范式，称为遥感图像变化分析（RSICA），它结合了变化检测和视觉问答的优势，使用户能够在双向遥感图像中进行多轮、指令引导的变化探索。其主要创新点包括：1. 细调后的双向视觉编码器，用于捕捉时间差异；2. 具有跨语义关系测量（CSRM）机制的视觉差异感知模块，用于解释变化；3. 指令引导的Q-former，用于从视觉变化中有效提取与查询相关的信息，并与文本指令对齐。

**Result:** 通过广泛的实验和消融研究，DeltaVLM在单轮描述和多轮交互变化分析方面达到了最先进的性能，超越了现有的多模态大规模语言模型和遥感视觉-语言模型。

**Conclusion:** 研究表明，DeltaVLM能够有效地支持对双向遥感图像进行多轮、指令引导的变化分析，表现优于当前的方法，推动了该领域的技术进步。

**Abstract:** Accurate interpretation of land-cover changes in multi-temporal satellite
imagery is critical for real-world scenarios. However, existing methods
typically provide only one-shot change masks or static captions, limiting their
ability to support interactive, query-driven analysis. In this work, we
introduce remote sensing image change analysis (RSICA) as a new paradigm that
combines the strengths of change detection and visual question answering to
enable multi-turn, instruction-guided exploration of changes in bi-temporal
remote sensing images. To support this task, we construct ChangeChat-105k, a
large-scale instruction-following dataset, generated through a hybrid
rule-based and GPT-assisted process, covering six interaction types: change
captioning, classification, quantification, localization, open-ended question
answering, and multi-turn dialogues. Building on this dataset, we propose
DeltaVLM, an end-to-end architecture tailored for interactive RSICA. DeltaVLM
features three innovations: (1) a fine-tuned bi-temporal vision encoder to
capture temporal differences; (2) a visual difference perception module with a
cross-semantic relation measuring (CSRM) mechanism to interpret changes; and
(3) an instruction-guided Q-former to effectively extract query-relevant
difference information from visual changes, aligning them with textual
instructions. We train DeltaVLM on ChangeChat-105k using a frozen large
language model, adapting only the vision and alignment modules to optimize
efficiency. Extensive experiments and ablation studies demonstrate that
DeltaVLM achieves state-of-the-art performance on both single-turn captioning
and multi-turn interactive change analysis, outperforming existing multimodal
large language models and remote sensing vision-language models. Code, dataset
and pre-trained weights are available at https://github.com/hanlinwu/DeltaVLM.

</details>


### [47] [FaceGCD: Generalized Face Discovery via Dynamic Prefix Generation](https://arxiv.org/abs/2507.22353)
*Yunseok Oh,Dong-Wan Choi*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Recognizing and differentiating among both familiar and unfamiliar faces is a
critical capability for face recognition systems and a key step toward
artificial general intelligence (AGI). Motivated by this ability, this paper
introduces generalized face discovery (GFD), a novel open-world face
recognition task that unifies traditional face identification with generalized
category discovery (GCD). GFD requires recognizing both labeled and unlabeled
known identities (IDs) while simultaneously discovering new, previously unseen
IDs. Unlike typical GCD settings, GFD poses unique challenges due to the high
cardinality and fine-grained nature of face IDs, rendering existing GCD
approaches ineffective. To tackle this problem, we propose FaceGCD, a method
that dynamically constructs instance-specific feature extractors using
lightweight, layer-wise prefixes. These prefixes are generated on the fly by a
HyperNetwork, which adaptively outputs a set of prefix generators conditioned
on each input image. This dynamic design enables FaceGCD to capture subtle
identity-specific cues without relying on high-capacity static models.
Extensive experiments demonstrate that FaceGCD significantly outperforms
existing GCD methods and a strong face recognition baseline, ArcFace, achieving
state-of-the-art results on the GFD task and advancing toward open-world face
recognition.

</details>


### [48] [GVD: Guiding Video Diffusion Model for Scalable Video Distillation](https://arxiv.org/abs/2507.22360)
*Kunyang Li,Jeffrey A Chan Santiago,Sarinda Dhanesh Samarasinghe,Gaowen Liu,Mubarak Shah*

Main category: cs.CV

> Introduces GVD, a novel method for video dataset distillation that captures essential spatial and temporal features with high fidelity, outperforming previous methods while reducing computational requirements.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of large computation and storage requirements associated with large video datasets, by creating a smaller, yet high-performance, distilled dataset.

**Method:** GVD: Guiding Video Diffusion, a first-ever diffusion-based video distillation method that jointly distills spatial and temporal features for high-fidelity video generation.

**Result:** Outperforms previous state-of-the-art methods on MiniUCF and HMDB51 datasets with significantly fewer instances and frames while maintaining high performance and generating higher resolution videos. Achieves 78.29% performance with 1.98% frames on MiniUCF and 73.83% with 3.30% frames on HMDB51.

**Conclusion:** GVD not only reduces computational and storage needs but also achieves state-of-the-art performance and generates higher resolution videos without increasing computational cost significantly.

**Abstract:** To address the larger computation and storage requirements associated with
large video datasets, video dataset distillation aims to capture spatial and
temporal information in a significantly smaller dataset, such that training on
the distilled data has comparable performance to training on all of the data.
We propose GVD: Guiding Video Diffusion, the first diffusion-based video
distillation method. GVD jointly distills spatial and temporal features,
ensuring high-fidelity video generation across diverse actions while capturing
essential motion information. Our method's diverse yet representative
distillations significantly outperform previous state-of-the-art approaches on
the MiniUCF and HMDB51 datasets across 5, 10, and 20 Instances Per Class (IPC).
Specifically, our method achieves 78.29 percent of the original dataset's
performance using only 1.98 percent of the total number of frames in MiniUCF.
Additionally, it reaches 73.83 percent of the performance with just 3.30
percent of the frames in HMDB51. Experimental results across benchmark video
datasets demonstrate that GVD not only achieves state-of-the-art performance
but can also generate higher resolution videos and higher IPC without
significantly increasing computational cost.

</details>


### [49] [Object Recognition Datasets and Challenges: A Review](https://arxiv.org/abs/2507.22361)
*Aria Salari,Abtin Djavadifar,Xiangrui Liu,Homayoun Najjaran*

Main category: cs.CV

> 本文综述了对象识别领域的数据集，分析了超过160个数据集，并概述了重要基准和竞赛，以及计算机视觉社区使用的评估指标。所有提及的数据集和挑战可以在指定GitHub地址找到。

<details>
  <summary>Details</summary>

**Motivation:** 随着深度网络技术的发展，对象识别领域对数据集的规模和质量提出了更高要求。研究者希望通过详细分析常用数据集的特征，为驱动数据和机器学习研究提供重要第一步。

**Method:** 本论文主要通过对超过160个常用数据集的统计和描述来进行详细分析。此外，还概述了显著的对象识别基准和竞赛，并描述了计算机视觉社区广泛采用的评估指标。

**Result:** 论文详细分析了160多个常用对象识别数据集，并总结了计算机视觉社区广泛使用的评估指标。此外，研究还概括了重要的对象识别基准和竞赛。

**Conclusion:** 该研究强调了合适的数据集对对象识别研究的重要性，并为研究人员提供了一份详细的数据集和竞赛综述，有助于推动领域的进一步发展。

**Abstract:** Object recognition is among the fundamental tasks in the computer vision
applications, paving the path for all other image understanding operations. In
every stage of progress in object recognition research, efforts have been made
to collect and annotate new datasets to match the capacity of the
state-of-the-art algorithms. In recent years, the importance of the size and
quality of datasets has been intensified as the utility of the emerging deep
network techniques heavily relies on training data. Furthermore, datasets lay a
fair benchmarking means for competitions and have proved instrumental to the
advancements of object recognition research by providing quantifiable
benchmarks for the developed models. Taking a closer look at the
characteristics of commonly-used public datasets seems to be an important first
step for data-driven and machine learning researchers. In this survey, we
provide a detailed analysis of datasets in the highly investigated object
recognition areas. More than 160 datasets have been scrutinized through
statistics and descriptions. Additionally, we present an overview of the
prominent object recognition benchmarks and competitions, along with a
description of the metrics widely adopted for evaluation purposes in the
computer vision community. All introduced datasets and challenges can be found
online at github.com/AbtinDjavadifar/ORDC.

</details>


### [50] [Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring](https://arxiv.org/abs/2507.22369)
*Sinh Trong Vu,Hieu Trung Pham,Dung Manh Nguyen,Hieu Minh Hoang,Nhu Hoang Le,Thu Ha Pham,Tai Tan Mai*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Classroom behavior monitoring is a critical aspect of educational research,
with significant implications for student engagement and learning outcomes.
Recent advancements in Visual Question Answering (VQA) models offer promising
tools for automatically analyzing complex classroom interactions from video
recordings. In this paper, we investigate the applicability of several
state-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, and
NVILA, in the context of classroom behavior analysis. To facilitate rigorous
evaluation, we introduce our BAV-Classroom-VQA dataset derived from real-world
classroom video recordings at the Banking Academy of Vietnam. We present the
methodology for data collection, annotation, and benchmark the performance of
the selected VQA models on this dataset. Our initial experimental results
demonstrate that all four models achieve promising performance levels in
answering behavior-related visual questions, showcasing their potential in
future classroom analytics and intervention systems.

</details>


### [51] [Gems: Group Emotion Profiling Through Multimodal Situational Understanding](https://arxiv.org/abs/2507.22393)
*Anubhav Kataria,Surbhi Madan,Shreya Ghosh,Tom Gedeon,Abhinav Dhall*

Main category: cs.CV

> GEMS利用多模态技术预测个体、群体和事件情绪，扩展了数据集，展示了框架的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 理解个体、群体和事件的情绪及背景信息在分析社交情况时至关重要。现存数据集主要关注原子性情感识别，研究人员扩展了VGAF-GEMS数据集，提供一个更精细和全面的基准测试环境。

**Method:** GEMS采用了多模态swin-transformer和S3Attention框架，通过输入场景、群体成员和背景信息来生成联合预测结果，涵盖基础情绪和连续情绪的预测。

**Result:** GEMS框架在扩展后的VGAF-GEMS数据集上，通过结合多模态swin-transformer和S3Attention架构，实现了对个体、群体及事件层面情绪的精细预测和整体分析，包括基本离散情绪及连续情绪（如效价和唤醒度）的预测，展示了其有效性和未来研究潜力。

**Conclusion:** GEMS框架通过其多模态处理能力，对个体、群体及事件层面的情绪进行了全面捕捉和预测，展示了其在情感理解领域的优越性和前瞻性。研究结果支持了GEMS作为该领域进一步研究的基础。

**Abstract:** Understanding individual, group and event level emotions along with
contextual information is crucial for analyzing a multi-person social
situation. To achieve this, we frame emotion comprehension as the task of
predicting fine-grained individual emotion to coarse grained group and event
level emotion. We introduce GEMS that leverages a multimodal swin-transformer
and S3Attention based architecture, which processes an input scene, group
members, and context information to generate joint predictions. Existing
multi-person emotion related benchmarks mainly focus on atomic interactions
primarily based on emotion perception over time and group level. To this end,
we extend and propose VGAF-GEMS to provide more fine grained and holistic
analysis on top of existing group level annotation of VGAF dataset. GEMS aims
to predict basic discrete and continuous emotions (including valence and
arousal) as well as individual, group and event level perceived emotions. Our
benchmarking effort links individual, group and situational emotional responses
holistically. The quantitative and qualitative comparisons with adapted
state-of-the-art models demonstrate the effectiveness of GEMS framework on
VGAF-GEMS benchmarking. We believe that it will pave the way of further
research. The code and data is available at:
https://github.com/katariaak579/GEMS

</details>


### [52] [On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations](https://arxiv.org/abs/2507.22398)
*Jordan Vice,Naveed Akhtar,Yansong Gao,Richard Hartley,Ajmal Mian*

Main category: cs.CV

> 本研究发现，视觉语言模型（VLMs）在面对频率域中的细微结构化扰动时，其输出存在显著的脆弱性，并对这些模型在DeepFake检测和图像描述任务中的可用性提出了质疑。

<details>
  <summary>Details</summary>

**Motivation:** 该研究发现了视觉语言模型（VLMs）在面对频率域中的细微结构化扰动时存在的关键脆弱性。研究旨在揭示这些模型在自动化图像标注和DeepFake检测任务中所面临的挑战。

**Method:** 本研究设计了在频率域中操作的有针对性的图像转换，这些转换可以使五个最先进的视觉语言模型（包括不同参数的Qwen2/2.5和BLIP模型）的输出发生系统性变化。这些模型用于各种图像数据集（包括真实图像和生成图像），以验证它们在面对频率域扰动时的脆弱性。

**Result:** 实验表明，这些视觉语言模型的判断容易受到基于频率的线索的影响，而不完全与语义内容一致。研究在现实的黑盒约束条件下，证明了这些模型用于自动图像标注和真实性检测任务时的脆弱性。

**Conclusion:** 该研究揭示了视觉语言模型在处理频率扰动时的脆弱性，这表明这些模型可能无法在所有情况下提供可靠的多模态感知。实验结果强调了开发鲁棒的多模态感知系统的必要性。

**Abstract:** Vision-Language Models (VLMs) are increasingly used as perceptual modules for
visual content reasoning, including through captioning and DeepFake detection.
In this work, we expose a critical vulnerability of VLMs when exposed to
subtle, structured perturbations in the frequency domain. Specifically, we
highlight how these feature transformations undermine authenticity/DeepFake
detection and automated image captioning tasks. We design targeted image
transformations, operating in the frequency domain to systematically adjust VLM
outputs when exposed to frequency-perturbed real and synthetic images. We
demonstrate that the perturbation injection method generalizes across five
state-of-the-art VLMs which includes different-parameter Qwen2/2.5 and BLIP
models. Experimenting across ten real and generated image datasets reveals that
VLM judgments are sensitive to frequency-based cues and may not wholly align
with semantic content. Crucially, we show that visually-imperceptible spatial
frequency transformations expose the fragility of VLMs deployed for automated
image captioning and authenticity detection tasks. Our findings under
realistic, black-box constraints challenge the reliability of VLMs,
underscoring the need for robust multimodal perception systems.

</details>


### [53] [MINR: Implicit Neural Representations with Masked Image Modelling](https://arxiv.org/abs/2507.22404)
*Sua Lee,Joonhun Lee,Myungjoo Kang*

Main category: cs.CV

> 本文提出了MINR框架，结合隐式神经表示与遮罩图像建模技术，展示出在图像重建任务中优于MAE的方法，并减少了模型复杂性。MINR在领域内及领域外数据上的表现均优于MAE。

<details>
  <summary>Details</summary>

**Motivation:** 尽管自监督学习方法如掩码自动编码器（MAE）在学习鲁棒特征表示方面展现了巨大的潜力，尤其是在基于图像重建的预训练任务中，但它们的性能往往强烈依赖于训练期间使用的掩码策略，并且在应用于分布外数据时性能会下降。为解决这些问题，我们提出了MINR框架。

**Method:** 我们提出了一种称为遮罩隐式神经表示（MINR）的框架，该框架将隐式神经表示与遮罩图像建模相结合。MINR 学习连续函数来表示图像，这使得在不同遮罩策略下也能得到更鲁棒和泛化的重建效果。

**Result:** 实验表明，MINR 不仅在领域内场景中优于 MAE，还在分布外设置中表现出色，同时减少了模型的复杂性。

**Conclusion:** MINR 的灵活性延伸到各种自监督学习应用中，确认了其作为现有框架的稳健和有效替代方案的实用性。

**Abstract:** Self-supervised learning methods like masked autoencoders (MAE) have shown
significant promise in learning robust feature representations, particularly in
image reconstruction-based pretraining task. However, their performance is
often strongly dependent on the masking strategies used during training and can
degrade when applied to out-of-distribution data. To address these limitations,
we introduce the masked implicit neural representations (MINR) framework that
synergizes implicit neural representations with masked image modeling. MINR
learns a continuous function to represent images, enabling more robust and
generalizable reconstructions irrespective of masking strategies. Our
experiments demonstrate that MINR not only outperforms MAE in in-domain
scenarios but also in out-of-distribution settings, while reducing model
complexity. The versatility of MINR extends to various self-supervised learning
applications, confirming its utility as a robust and efficient alternative to
existing frameworks.

</details>


### [54] [Moiré Zero: An Efficient and High-Performance Neural Architecture for Moiré Removal](https://arxiv.org/abs/2507.22407)
*Seungryong Lee,Woojeong Baek,Younghyun Kim,Eunwoo Kim,Haru Moon,Donggon Yoo,Eunbyung Park*

Main category: cs.CV

> 提出了MZNet，一种U形神经网络，用于高效去除图像中的莫尔条纹，同时保持低计算成本，展现出在现实应用中的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 莫尔条纹由于精细重复结构与摄像机传感器采样过程之间的频率混叠而生成，已成为摄影和工业缺陷检测等领域的一个重大障碍。随着深度学习算法的进步，基于卷积神经网络的方法已被广泛用于解决这一问题。然而，现有方法由于莫尔条纹的多样化尺度、方向和色彩变化，仍然难以有效消除图像伪影。这是因为CNN架构受限制的感受域限制了它们捕捉莫尔条纹复杂特性的能力。

**Method:** 研究提出了一种U形网络，集成多尺度双注意力块（MSDAB）、多形状大卷积核块（MSLKB）和基于特征融合的跳层连接三个专业组件来克服这一问题。

**Result:** 本研究提出了一种名为MZNet的U形网络，旨在通过有效地去除莫尔条纹，使图像接近'无莫尔条纹'的状态。该网络整合了三个专业组件，包括用于提取和提炼多尺度特征的多尺度双注意力块（MSDAB）、用于捕捉多样莫尔结构的多形状大卷积核块（MSLKB），以及用于增强信息流的基于特征融合的跳层连接。这使MZNet在恢复局部纹理和抑制大规模伪影方面表现出色。在基准数据集上的实验表明，MZNet在高分辨率数据集上达到了最先进的性能，并在较低分辨率数据集上也表现出竞争力，与此同时保持了低计算成本，显示出其作为实际应用有效且实用解决方案的潜力。

**Conclusion:** 实验结果表明，MZNet在去除莫尔条纹方面实现了最先进的性能，并且在不同分辨率的数据集上均有良好表现，具备低计算成本，展示了其在实际应用中的高效性和实用性。

**Abstract:** Moir\'e patterns, caused by frequency aliasing between fine repetitive
structures and a camera sensor's sampling process, have been a significant
obstacle in various real-world applications, such as consumer photography and
industrial defect inspection. With the advancements in deep learning
algorithms, numerous studies-predominantly based on convolutional neural
networks-have suggested various solutions to address this issue. Despite these
efforts, existing approaches still struggle to effectively eliminate artifacts
due to the diverse scales, orientations, and color shifts of moir\'e patterns,
primarily because the constrained receptive field of CNN-based architectures
limits their ability to capture the complex characteristics of moir\'e
patterns. In this paper, we propose MZNet, a U-shaped network designed to bring
images closer to a 'Moire-Zero' state by effectively removing moir\'e patterns.
It integrates three specialized components: Multi-Scale Dual Attention Block
(MSDAB) for extracting and refining multi-scale features, Multi-Shape Large
Kernel Convolution Block (MSLKB) for capturing diverse moir\'e structures, and
Feature Fusion-Based Skip Connection for enhancing information flow. Together,
these components enhance local texture restoration and large-scale artifact
suppression. Experiments on benchmark datasets demonstrate that MZNet achieves
state-of-the-art performance on high-resolution datasets and delivers
competitive results on lower-resolution dataset, while maintaining a low
computational cost, suggesting that it is an efficient and practical solution
for real-world applications. Project page:
https://sngryonglee.github.io/MoireZero

</details>


### [55] [UAVScenes: A Multi-Modal Dataset for UAVs](https://arxiv.org/abs/2507.22412)
*Sijie Wang,Siqi Li,Yawei Zhang,Shangshu Yu,Shenghai Yuan,Rui She,Quanjiang Guo,JinXuan Zheng,Ong Kang Howe,Leonrich Chandra,Shrivarshann Srijeyan,Aditya Sivadas,Toshan Aggarwal,Heyuan Liu,Hongming Zhang,Chujie Chen,Junyu Jiang,Lihua Xie,Wee Peng Tay*

Main category: cs.CV

> 介绍了一个大型多模态无人机数据集UAVScenes，以支持包括分割、深度估计和新视图合成等多项高级感知任务。

<details>
  <summary>Details</summary>

**Motivation:** 现有数据集在多模态无人机操作方面存在局限性，主要偏向于定位和三维重建任务，仅能支持地图级别的语义分割。为了弥补这一缺口并推进多模态无人机感知能力，构建了这一新的数据集。

**Method:** 多模态数据集UAVScenes的构建，基于MARS-LVIG数据集，增加了针对相机图像和LiDAR点云的手动标注的语义注释及精准的六自由度位姿注释，以支持更高层次的场景理解任务。

**Result:** 数据集支持广泛的无人机感知任务，包括分割、深度估计、六自由度定位、地点识别和新视图合成。

**Conclusion:** UAVScenes将促进多模态感知技术的发展，尤其适合高精度无人机操作和环境理解相关的研究。数据集可在GitHub上获取。

**Abstract:** Multi-modal perception is essential for unmanned aerial vehicle (UAV)
operations, as it enables a comprehensive understanding of the UAVs'
surrounding environment. However, most existing multi-modal UAV datasets are
primarily biased toward localization and 3D reconstruction tasks, or only
support map-level semantic segmentation due to the lack of frame-wise
annotations for both camera images and LiDAR point clouds. This limitation
prevents them from being used for high-level scene understanding tasks. To
address this gap and advance multi-modal UAV perception, we introduce
UAVScenes, a large-scale dataset designed to benchmark various tasks across
both 2D and 3D modalities. Our benchmark dataset is built upon the
well-calibrated multi-modal UAV dataset MARS-LVIG, originally developed only
for simultaneous localization and mapping (SLAM). We enhance this dataset by
providing manually labeled semantic annotations for both frame-wise images and
LiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses.
These additions enable a wide range of UAV perception tasks, including
segmentation, depth estimation, 6-DoF localization, place recognition, and
novel view synthesis (NVS). Our dataset is available at
https://github.com/sijieaaa/UAVScenes

</details>


### [56] [Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching](https://arxiv.org/abs/2507.22418)
*Phi Van Nguyen,Ngoc Huynh Trinh,Duy Minh Lam Nguyen,Phu Loc Nguyen,Quoc Long Tran*

Main category: cs.CV

> 提出了一种基于条件流匹配的方法，以精确地建模医学图像分割中的不确定性，并展示了该方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 量化医学图像分割中的偶然不确定性是关键的，因为它反映了专家注释者之间自然的变异性。使用生成模型建模分割分布的常规方法存在局限性，而基于扩散的方法在近似数据分布方面表现出色，但在准确捕捉不确定性方面存在不足。

**Method:** 通过条件流匹配，一种基于流的生成模型，该模型能够学习精确的概率密度，以生成高度准确的分割结果。通过在输入图像上引导流模型并采样多个数据点，我们的方法可以合成分割样本，这些样本的像素级方差可靠地反映了底层数据分布。这种采样策略可以捕捉具有模糊边界的区域的不确定性，提供与注释者之间差异相呼应的稳健量化。

**Result:** 实验结果表明，该方法不仅实现了有竞争力的分割准确性，而且生成的不确定性地图为分割结果的可靠性提供了更深入的见解。

**Conclusion:** 该方法利用条件流匹配生成准确分割结果，并通过采样策略捕捉不确定性区域，生成的不确定性地图提供了可靠性的深入见解。

**Abstract:** Quantifying aleatoric uncertainty in medical image segmentation is critical
since it is a reflection of the natural variability observed among expert
annotators. A conventional approach is to model the segmentation distribution
using the generative model, but current methods limit the expression ability of
generative models. While current diffusion-based approaches have demonstrated
impressive performance in approximating the data distribution, their inherent
stochastic sampling process and inability to model exact densities limit their
effectiveness in accurately capturing uncertainty. In contrast, our proposed
method leverages conditional flow matching, a simulation-free flow-based
generative model that learns an exact density, to produce highly accurate
segmentation results. By guiding the flow model on the input image and sampling
multiple data points, our approach synthesizes segmentation samples whose
pixel-wise variance reliably reflects the underlying data distribution. This
sampling strategy captures uncertainties in regions with ambiguous boundaries,
offering robust quantification that mirrors inter-annotator differences.
Experimental results demonstrate that our method not only achieves competitive
segmentation accuracy but also generates uncertainty maps that provide deeper
insights into the reliability of the segmentation outcomes. The code for this
paper is freely available at https://github.com/huynhspm/Data-Uncertainty

</details>


### [57] [Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking](https://arxiv.org/abs/2507.22421)
*Shahla John*

Main category: cs.CV

> 本文提出了一种结合高级时空建模技术的统一框架，用于动作识别和目标跟踪，实现了实时分析的高性能。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在解决实时视频分析中空间和时间信息处理效率以及计算效率之间的平衡难题，特别是在资源受限的环境中。

**Method:** 该方法基于并行序列建模的最新进展，引入了一种新的分层注意力机制，可以自适应地关注跨时间序列的相关空间区域。

**Result:** 在UCF-101、HMDB-51和MOT17数据集上的实验展示了动作识别精度提高了3.2%，跟踪精度提高了2.8%，推理速度提高了40%。

**Conclusion:** 研究展示了该方法在标准基准上的实时推理速度和性能指标达到最新水平。

**Abstract:** Real-time video analysis remains a challenging problem in computer vision,
requiring efficient processing of both spatial and temporal information while
maintaining computational efficiency. Existing approaches often struggle to
balance accuracy and speed, particularly in resource-constrained environments.
In this work, we present a unified framework that leverages advanced
spatial-temporal modeling techniques for simultaneous action recognition and
object tracking. Our approach builds upon recent advances in parallel sequence
modeling and introduces a novel hierarchical attention mechanism that
adaptively focuses on relevant spatial regions across temporal sequences. We
demonstrate that our method achieves state-of-the-art performance on standard
benchmarks while maintaining real-time inference speeds. Extensive experiments
on UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action
recognition accuracy and 2.8% in tracking precision compared to existing
methods, with 40% faster inference time.

</details>


### [58] [HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models](https://arxiv.org/abs/2507.22431)
*Zhixiang Wei,Guangting Wang,Xiaoxiao Ma,Ke Mei,Huaian Chen,Yi Jin,Fengyun Rao*

Main category: cs.CV

> 通过使用大语言-视觉模型（LVLMs）来优化图像-文本配对数据，作者提出了一个改进数据质量的方法，提升了视觉对比学习的性能，从而达到了state-of-the-art的零样本分类、跨模态检索和细粒度视觉理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 随着对比语言-图像预训练模型的成功，提出了一种利用LVLMs来优化图像-文本数据质量的方法，旨在实现数据质量和模型能力之间的自我强化循环。

**Method:** 引入了一个由LVLM驱动的数据精炼流程，对图像及其原始描述进行处理，生成四种互补的文本描述形式。这种方法被应用于DFN-Large数据集，生成了带有多级别注释VLM-150M数据集。

**Result:** 基于该数据集提出的训练范式在多种基准测试中表现强劲，尤其是HQ-CLIP模型在零样本分类、跨模态检索和细粒度视觉理解中取得了新的最佳成绩。

**Conclusion:** 我们发现通过利用LVLMs来提高图像-文本对数据的质量，可以显著提升对比学习模型的性能，展示了LVLMs在数据增强中的巨大潜力。

**Abstract:** Large-scale but noisy image-text pair data have paved the way for the success
of Contrastive Language-Image Pretraining (CLIP). As the foundation vision
encoder, CLIP in turn serves as the cornerstone for most large vision-language
models (LVLMs). This interdependence naturally raises an interesting question:
Can we reciprocally leverage LVLMs to enhance the quality of image-text pair
data, thereby opening the possibility of a self-reinforcing cycle for
continuous improvement? In this work, we take a significant step toward this
vision by introducing an LVLM-driven data refinement pipeline. Our framework
leverages LVLMs to process images and their raw alt-text, generating four
complementary textual formulas: long positive descriptions, long negative
descriptions, short positive tags, and short negative tags. Applying this
pipeline to the curated DFN-Large dataset yields VLM-150M, a refined dataset
enriched with multi-grained annotations. Based on this dataset, we further
propose a training paradigm that extends conventional contrastive learning by
incorporating negative descriptions and short tags as additional supervised
signals. The resulting model, namely HQ-CLIP, demonstrates remarkable
improvements across diverse benchmarks. Within a comparable training data
scale, our approach achieves state-of-the-art performance in zero-shot
classification, cross-modal retrieval, and fine-grained visual understanding
tasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models
trained on the DFN-2B dataset, which contains 10$\times$ more training data
than ours. All code, data, and models are available at
https://zxwei.site/hqclip.

</details>


### [59] [From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras](https://arxiv.org/abs/2507.22438)
*Youngho Kim,Hoonhee Cho,Kuk-Jin Yoon*

Main category: cs.CV

> 利用事件相机和师生框架解决运动模糊引起的人体姿态估计问题，不需目标域标注，提高了实际场景下的姿态估计性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决快速运动和低光条件下由于域差距造成的姿态估计性能下降问题。现有数据集假设了稳定的拍摄条件，导致模型在模糊环境下表现不佳。

**Method:** 提出了一种利用事件相机进行领域适应的新方法，这些相机能捕捉高时间分辨率运动数据并且对运动模糊具有鲁棒性。通过基于事件的增强生成运动感知模糊图像以弥合清晰和模糊域间的差距，不需配对标注。此外，还开发了一个师生框架，以迭代地细化伪标签，利用互不确定性遮罩消除错误标签并促进更有效的学习。

**Result:** 实验结果表明，该方法超过了传统的领域自适应人体姿态估计方法，能够在不使用目标域标注的情况下实现鲁棒的姿态估计。

**Conclusion:** 研究结果突出了事件相机作为大规模和有效解决方案的潜力，以适应真实世界中的运动模糊环境。项目代码可在https://github.com/kmax2001/EvSharp2Blur获取。

**Abstract:** Human pose estimation is critical for applications such as rehabilitation,
sports analytics, and AR/VR systems. However, rapid motion and low-light
conditions often introduce motion blur, significantly degrading pose estimation
due to the domain gap between sharp and blurred images. Most datasets assume
stable conditions, making models trained on sharp images struggle in blurred
environments. To address this, we introduce a novel domain adaptation approach
that leverages event cameras, which capture high temporal resolution motion
data and are inherently robust to motion blur. Using event-based augmentation,
we generate motion-aware blurred images, effectively bridging the domain gap
between sharp and blurred domains without requiring paired annotations.
Additionally, we develop a student-teacher framework that iteratively refines
pseudo-labels, leveraging mutual uncertainty masking to eliminate incorrect
labels and enable more effective learning. Experimental results demonstrate
that our approach outperforms conventional domain-adaptive human pose
estimation methods, achieving robust pose estimation under motion blur without
requiring annotations in the target domain. Our findings highlight the
potential of event cameras as a scalable and effective solution for domain
adaptation in real-world motion blur environments. Our project codes are
available at https://github.com/kmax2001/EvSharp2Blur.

</details>


### [60] [TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation](https://arxiv.org/abs/2507.22454)
*Jiuming Liu,Zheng Huang,Mengmeng Liu,Tianchen Deng,Francesco Nex,Hao Cheng,Hesheng Wang*

Main category: cs.CV

> 提出新的激光雷达生成框架TopoLiDM，结合GNNs和扩散模型，引入拓扑保持和PH约束，提升了生成的激光雷达场景的几何现实性和拓扑一致性，且生成速度快，效果优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在捕获几何现实性和全局拓扑一致性方面存在困难。虽然近年来的激光雷达扩散模型(LiDMs)嵌入了激光雷达点以提高生成效率，但这限制了它们在建模详细几何结构和保持全局拓扑一致性方面的可解释性。

**Method:** 提出了一种名为TopoLiDM的新框架，该框架结合了图神经网络(GNNs)和扩散模型，并在拓扑正则化下进行高保真激光雷达场景生成。首先使用拓扑保持的变分自编码器(VAE)通过图构建和多个图卷积层提取潜在图表示。接着冻结VAE并通过潜在扩散模型生成新的潜在拓扑图。引入了0维持续同调(PH)约束，确保生成的激光雷达场景符合现实世界中的全局拓扑结构。

**Result:** 在KITTI-360数据集上进行的大量实验表明，TopoLiDM优于最先进方法，实现了22.6%的Frechet Range Image Distance (FRID)和9.2%的Minimum Matching Distance (MMD)的降低。此外，该模型还具备快速生成能力，平均推理时间为1.68样本/秒。

**Conclusion:** 实验证明了TopoLiDM在生成高保真激光雷达场景方面的能力，并具备快速生成速度，适用于实际应用。

**Abstract:** LiDAR scene generation is critical for mitigating real-world LiDAR data
collection costs and enhancing the robustness of downstream perception tasks in
autonomous driving. However, existing methods commonly struggle to capture
geometric realism and global topological consistency. Recent LiDAR Diffusion
Models (LiDMs) predominantly embed LiDAR points into the latent space for
improved generation efficiency, which limits their interpretable ability to
model detailed geometric structures and preserve global topological
consistency. To address these challenges, we propose TopoLiDM, a novel
framework that integrates graph neural networks (GNNs) with diffusion models
under topological regularization for high-fidelity LiDAR generation. Our
approach first trains a topological-preserving VAE to extract latent graph
representations by graph construction and multiple graph convolutional layers.
Then we freeze the VAE and generate novel latent topological graphs through the
latent diffusion models. We also introduce 0-dimensional persistent homology
(PH) constraints, ensuring the generated LiDAR scenes adhere to real-world
global topological structures. Extensive experiments on the KITTI-360 dataset
demonstrate TopoLiDM's superiority over state-of-the-art methods, achieving
improvements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower
Minimum Matching Distance (MMD). Notably, our model also enables fast
generation speed with an average inference time of 1.68 samples/s, showcasing
its scalability for real-world applications. We will release the related codes
at https://github.com/IRMVLab/TopoLiDM.

</details>


### [61] [Exploiting Diffusion Prior for Task-driven Image Restoration](https://arxiv.org/abs/2507.22459)
*Jaeha Kim,Junghun Oh,Kyoung Mu Lee*

Main category: cs.CV

> Propose EDTR which effectively harnesses the power of diffusion prior to restore task-relevant details in task-driven image restoration settings with multiple complex degradations.

<details>
  <summary>Details</summary>

**Motivation:** This motivates us to leverage the diffusion prior, one of the most powerful natural image priors. However, while the diffusion prior can help generate visually plausible results, using it to restore task-relevant details remains challenging, even when combined with recent TDIR methods.

**Method:** Specifically, we propose directly leveraging useful clues from LQ images in the diffusion process by generating from pixel-error-based pre-restored LQ images with mild noise added. Moreover, we employ a small number of denoising steps to prevent the generation of redundant details that dilute crucial task-related information.

**Result:** We demonstrate that our method effectively utilizes diffusion prior for TDIR, significantly enhancing task performance and visual quality across diverse tasks with multiple complex degradations.

**Conclusion:** It indicates the approach is effective in restoring task-relevant details and improving task performance and visual quality.

**Abstract:** Task-driven image restoration (TDIR) has recently emerged to address
performance drops in high-level vision tasks caused by low-quality (LQ) inputs.
Previous TDIR methods struggle to handle practical scenarios in which images
are degraded by multiple complex factors, leaving minimal clues for
restoration. This motivates us to leverage the diffusion prior, one of the most
powerful natural image priors. However, while the diffusion prior can help
generate visually plausible results, using it to restore task-relevant details
remains challenging, even when combined with recent TDIR methods. To address
this, we propose EDTR, which effectively harnesses the power of diffusion prior
to restore task-relevant details. Specifically, we propose directly leveraging
useful clues from LQ images in the diffusion process by generating from
pixel-error-based pre-restored LQ images with mild noise added. Moreover, we
employ a small number of denoising steps to prevent the generation of redundant
details that dilute crucial task-related information. We demonstrate that our
method effectively utilizes diffusion prior for TDIR, significantly enhancing
task performance and visual quality across diverse tasks with multiple complex
degradations.

</details>


### [62] [Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2507.22465)
*Zheng Xiangyu,He Songcheng,Li Wanyun,Li Xiaoqiang,Zhang Wei*

Main category: cs.CV

> We address the limitation of existing UVOS methods by introducing a hierarchical memory architecture that integrates both shallow and high-level features, leading to improved segmentation performance across various benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to improve the performance of Unsupervised Video Object Segmentation (UVOS) beyond the marginal gains achieved with current memory mechanisms, which rely heavily on high-level semantic features.

**Method:** Our method introduces a hierarchical memory architecture that integrates both shallow- and high-level features to enhance the precision of UVOS. This is achieved through a Pixel-guided Local Alignment Module (PLAM) and a Semantic-guided Global Integration Module (SGIM) to address the issue of over-reliance on high-level features.

**Result:** The proposed method achieves state-of-the-art performance across all evaluated UVOS and video saliency detection benchmarks, and exhibits consistent high performance across different backbones.

**Conclusion:** The newly proposed hierarchical memory with heterogeneous interaction network (HMHI-Net) significantly enhances the performance of UVOS by effectively integrating pixel and semantic information, demonstrating state-of-the-art results and high robustness.

**Abstract:** Unsupervised Video Object Segmentation (UVOS) aims to predict pixel-level
masks for the most salient objects in videos without any prior annotations.
While memory mechanisms have been proven critical in various video segmentation
paradigms, their application in UVOS yield only marginal performance gains
despite sophisticated design. Our analysis reveals a simple but fundamental
flaw in existing methods: over-reliance on memorizing high-level semantic
features. UVOS inherently suffers from the deficiency of lacking fine-grained
information due to the absence of pixel-level prior knowledge. Consequently,
memory design relying solely on high-level features, which predominantly
capture abstract semantic cues, is insufficient to generate precise
predictions. To resolve this fundamental issue, we propose a novel hierarchical
memory architecture to incorporate both shallow- and high-level features for
memory, which leverages the complementary benefits of pixel and semantic
information. Furthermore, to balance the simultaneous utilization of the pixel
and semantic memory features, we propose a heterogeneous interaction mechanism
to perform pixel-semantic mutual interactions, which explicitly considers their
inherent feature discrepancies. Through the design of Pixel-guided Local
Alignment Module (PLAM) and Semantic-guided Global Integration Module (SGIM),
we achieve delicate integration of the fine-grained details in shallow-level
memory and the semantic representations in high-level memory. Our Hierarchical
Memory with Heterogeneous Interaction Network (HMHI-Net) consistently achieves
state-of-the-art performance across all UVOS and video saliency detection
benchmarks. Moreover, HMHI-Net consistently exhibits high performance across
different backbones, further demonstrating its superiority and robustness.
Project page: https://github.com/ZhengxyFlow/HMHI-Net .

</details>


### [63] [Visual Language Models as Zero-Shot Deepfake Detectors](https://arxiv.org/abs/2507.22469)
*Viacheslav Pirogov*

Main category: cs.CV

> This paper introduces a new method for deepfake detection using Vision Language Models, achieving better results than traditional classifiers in both zero-shot and fine-tuned settings.

<details>
  <summary>Details</summary>

**Motivation:** Aims to address the limitations of existing deepfake detection methods, which do not incorporate auxiliary tasks and thus may lack robustness.

**Method:** Proposes a VLM-based approach for deepfake detection, utilizing a new high-quality deepfake dataset with 60,000 images and comparing the performance of InstructBLIP against traditional methods.

**Result:** Demonstrates the superiority of VLM-based models, particularly InstructBLIP, in both zero-shot and in-domain fine-tuning scenarios.

**Conclusion:** Highlights the potential of Vision Language Models in deepfake detection, showing they outperform current methods and can be effectively used in zero-shot settings.

**Abstract:** The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models
for face swapping, presents a substantial and evolving threat in digital media,
identity verification, and a multitude of other systems. The majority of
existing methods for detecting deepfakes rely on training specialized
classifiers to distinguish between genuine and manipulated images, focusing
only on the image domain without incorporating any auxiliary tasks that could
enhance robustness. In this paper, inspired by the zero-shot capabilities of
Vision Language Models, we propose a novel VLM-based approach to image
classification and then evaluate it for deepfake detection. Specifically, we
utilize a new high-quality deepfake dataset comprising 60,000 images, on which
our zero-shot models demonstrate superior performance to almost all existing
methods. Subsequently, we compare the performance of the best-performing
architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against
traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our
results demonstrate the superiority of VLMs over traditional classifiers.

</details>
