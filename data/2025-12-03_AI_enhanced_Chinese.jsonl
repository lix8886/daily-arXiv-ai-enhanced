{"id": "2512.02024", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.02024", "abs": "https://arxiv.org/abs/2512.02024", "authors": ["Yan Yang", "Mouxiao Bian", "Peiling Li", "Bingjian Wen", "Ruiyao Chen", "Kangkun Mao", "Xiaojun Ye", "Tianbin Li", "Pengcheng Chen", "Bing Han", "Jie Xu", "Kaifeng Qiu", "Junyan Wu"], "title": "Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.", "AI": {"tldr": "通过开发RxBench基准测试系统，针对处方审查进行了全面评估，揭示了前沿LLMs的能力和局限性，并促进了专项临床工具的可靠性和专业化。", "motivation": "研究的动机是系统且细致地评估大型语言模型（LLMs）在临床决策支持，特别是在处方审查中的应用能力。", "method": "开发了RxBench，一个包含1,150个单选题，230个多选题及879个简答题的全面基准测试，题目由经验丰富的临床药剂师审核。这些题目涵盖了常见的处方审查类别，并整合了14种常见的处方错误类型。", "result": "基准测试了18个最先进的LLMs，发现LLMs的性能在各任务中存在明显分层。其中，Gemini-2.5-pro-preview-05-06、Grok-4-0709和DeepSeek-R1-0528在准确性和鲁棒性方面表现最佳。与持证药剂师的比较结果显示，在某些任务中，领先的LLMs可以达到或超过人类的表现。通过针对中等层次模型进行针对性微调，研究也开发出一个可以与顶级通用型LLMs媲美的专项模型。", "conclusion": "RxBench为处方审查建立了一个标准化、基于错误类型的框架，不但揭示了前沿LLMs的能力和限制，还提供了一个构建更可靠和专业的临床工具的基础资源。"}}
{"id": "2512.02038", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.02038", "abs": "https://arxiv.org/abs/2512.02038", "authors": ["Zhengliang Shi", "Yiqun Chen", "Haitao Li", "Weiwei Sun", "Shiyu Ni", "Yougang Lyu", "Run-Ze Fan", "Bowen Jin", "Yixuan Weng", "Minjun Zhu", "Qiujie Xie", "Xinyu Guo", "Qu Yang", "Jiayi Wu", "Jujia Zhao", "Xiaqiang Tang", "Xinbei Ma", "Cunxiang Wang", "Jiaxin Mao", "Qingyao Ai", "Jen-Tse Huang", "Wenxuan Wang", "Yue Zhang", "Yiming Yang", "Zhaopeng Tu", "Zhaochun Ren"], "title": "Deep Research: A Systematic Survey", "comment": null, "summary": "Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "<tool_call>\n{{\"name\": \"Structure\", \"arguments\": {\"tldr\": \"本文综述了深度研究系统及其进展，提出了一个三层的路线图，定义了四个关键组件及其子类别，并概述了优化技术和评估标准，旨在指导未来发展方向。\", \"motivation\": \"大型语言模型虽然已经从文本生成器演变为强大的问题解决者，但其面临的关键思考、多源信息以及可验证输出等问题，促使研究者探索结合语言模型与外部工具的深度研究方法。\", \"method\": \"本文介绍了深度研究系统的四个关键组件：查询规划、信息获取、记忆管理和答案生成，并提到了优化技术包括提示调整、有监督微调和代理强化学习。\", \"result\": \"本文系统概述了深度研究系统，总结了深度研究关键挑战，并对今后的发展方向进行了探讨。\", \"conclusion\": \"随着深度研究领域的持续快速发展，本文致力于不断更新，反映该领域的最新进展。\"}}}\n<tool_call>", "conclusion": ""}}
{"id": "2512.02043", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02043", "abs": "https://arxiv.org/abs/2512.02043", "authors": ["Dina Sayed", "Heiko Schuldt"], "title": "Mirror, Mirror on the Wall -- Which is the Best Model of Them All?", "comment": null, "summary": "Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.", "AI": {"tldr": "This paper analyzes the process of selecting the most suitable Large Language Model (LLM) among state-of-the-art foundation models for specific use cases in fields such as medicine, proposing a Model Selection Methodology (MSM) as a guide for this process.", "motivation": "The rapid evolution and complexity of LLMs make it challenging to select the most appropriate model for particular domains and tasks. This paper aims to simplify this complexity through a systematic approach.", "method": "The authors analyze leaderboards and benchmarks to evaluate the performance of LLMs, with a focus on the medical domain as a case study. They also propose a Model Selection Methodology (MSM) aimed at guiding model selection.", "result": "The analysis reveals the evolution and current landscape of LLMs in the medical domain, highlighting the importance of both qualitative and quantitative evaluations in model selection.", "conclusion": "The conclusion underscores the necessity of a structured approach to LLM model selection, emphasizing the MSM as a tool for effectively prioritizing and selecting models that best fit specific use cases, particularly in the fast-paced and dynamic environment of LLM development."}}
{"id": "2512.02044", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02044", "abs": "https://arxiv.org/abs/2512.02044", "authors": ["Kecheng Chen", "Ziru Liu", "Xijia Tao", "Hui Liu", "Xinyu Fu", "Suiyun Zhang", "Dandan Tu", "Lingpeng Kong", "Rui Liu", "Haoliang Li"], "title": "Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models", "comment": null, "summary": "Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.", "AI": {"tldr": "一种用于提高扩散语言模型生成质量的新框架CCD，通过历史轨迹修正机制和自适应采样策略，改进了传统方法中的不足。", "motivation": "由于现有推理方法通常依赖于局部即时的度量标准，如置信度或熵，缺乏更可靠的视角，导致采样轨迹的不一致性和生成质量次优。", "method": "CCD方法包括历史轨迹修正机制，该机制利用历史上下文来提高序列连贯性，并通过条件互信息来度量一致性。此外，它还引入了自适应采样策略，根据一致性指标动态调整每一步的去掩码预算，从而提升生成质量和加速采样过程。", "result": "实验证明，该方法在Dream和LLaDA上同时提升了推理速度和模型性能，达到了最高3.48倍的加速和3.91%的性能提升。", "conclusion": "CCD通过引入历史轨迹修正和自适应采样策略改进了扩散语言模型的生成过程，提高了生成质量和加速了推理速度。"}}
{"id": "2512.02055", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02055", "abs": "https://arxiv.org/abs/2512.02055", "authors": ["Mirela G. Tulbure", "Julio Caineta", "Mark Broich", "Mollie D. Gaines", "Philippe Rufin", "Leon-Friedrich Thomas", "Hamed Alemohammad", "Jan Hemmerling", "Patrick Hostert"], "title": "Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale", "comment": null, "summary": "Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.\n  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.\n  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.", "AI": {"tldr": "Fine-tuning TerraMind GFM with FloodsNet dataset improves flood extent mapping accuracy and recall, but there are computational costs and limitations.", "motivation": "To understand the performance of Geospatial Foundation Models (GFMs) on diverse global flood events and enhance near-real-time flood mapping.", "method": "We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing Sentinel-1 and Sentinel-2 imagery for 85 flood events worldwide. Models were tested in four configurations and compared to a U-Net.", "result": "The base-unfrozen configuration of TerraMind provided the best balance of accuracy, precision, and recall at a lower computational cost. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall.", "conclusion": "Fine-tuning GFMs with multimodal data enhances flood mapping performance, demonstrating potential and limitations for climate adaptation and disaster resilience."}}
{"id": "2512.02056", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02056", "abs": "https://arxiv.org/abs/2512.02056", "authors": ["Eshed Gal", "Moshe Eliasof", "Javier Turek", "Uri Ascher", "Eran Treister", "Eldad Haber"], "title": "Reversing Large Language Models for Efficient Training and Fine-Tuning", "comment": null, "summary": "Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.", "AI": {"tldr": "研究提出了一种用于LLMs的内存高效、可逆架构，通过减少内存消耗提升了训练过程中的性能和吞吐量，同时提供一种将非可逆模型转变为可逆架构的方法。", "motivation": "鉴于LLMs训练成本高、耗时长，且通常在预训练权重基础上进行特定任务微调，研究旨在通过内存高效且可逆的架构来解决这些问题，以提升处理大规模批处理和改进吞吐量的能力。", "method": "本研究提出了基于对称和辛微分方程灵感的可逆架构，以提高大语言模型（LLMs）的内存效率。与标准架构需要存储所有中间激活不同，新方法通过时间可逆动力学在反向传播过程中再现隐藏状态，从而显著减少内存消耗。此外，通过微调，他们还提出了一种高效方法，能够将现有的非可逆LLMs转化为可逆架构。", "result": "研究结果表明，这些新型架构在多个数据集和基准测试上展示了相等或改进的性能，适用于多款LLMs，为减少从零开始训练及微调的内存和计算成本提供了可扩展和有效的方案。", "conclusion": "通过采用可逆机制，该研究为LLMs的内存和计算资源优化提供了一条有效途径，为构建更实用和高效的预训练模型铺平了道路。"}}
{"id": "2512.02152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02152", "abs": "https://arxiv.org/abs/2512.02152", "authors": ["Haojin Deng", "Yimin Yang"], "title": "Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework", "comment": "13 pages, 7 figures. Published in IEEE Transactions on Multimedia. Code available at: https://github.com/hdeng26/Contex", "summary": "Contrastive learning has gained popularity and pushes state-of-the-art performance across numerous large-scale benchmarks. In contrastive learning, the contrastive loss function plays a pivotal role in discerning similarities between samples through techniques such as rotation or cropping. However, this learning mechanism can also introduce information distortion from the augmented samples. This is because the trained model may develop a significant overreliance on information from samples with identical labels, while concurrently neglecting positive pairs that originate from the same initial image, especially in expansive datasets. This paper proposes a context-enriched contrastive loss function that concurrently improves learning effectiveness and addresses the information distortion by encompassing two convergence targets. The first component, which is notably sensitive to label contrast, differentiates between features of identical and distinct classes which boosts the contrastive training efficiency. Meanwhile, the second component draws closer the augmented samples from the same source image and distances all other samples. We evaluate the proposed approach on image classification tasks, which are among the most widely accepted 8 recognition large-scale benchmark datasets: CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA datasets. The experimental results demonstrate that the proposed method achieves improvements over 16 state-of-the-art contrastive learning methods in terms of both generalization performance and learning convergence speed. Interestingly, our technique stands out in addressing systematic distortion tasks. It demonstrates a 22.9% improvement compared to original contrastive loss functions in the downstream BiasedMNIST dataset, highlighting its promise for more efficient and equitable downstream training.", "AI": {"tldr": "本文提出了一个上下文丰富对比损失函数，提升了对比学习的效果并减少了信息失真。该方法解决了原有对比损失函数过度依赖相同标签样本的问题，并在多个大规模数据集上验证了其优越性，特别是在存在系统失真的任务中表现出色，相比原有对比损失函数，在BiasedMNIST数据集上有了22.9%的显著提升。", "motivation": "对比学习通过对比损失函数在众多大规模基准测试中取得了先进的性能，然而该机制同样可能引入来自增强样本的信息失真问题。", "method": "本文提出一种上下文丰富对比损失函数，分为两个收敛目标：一个是敏感于标签对比，提高相同和不同类特征区分能力；另一个拉近来自同一原始图片的增强样本距离，并推远其它样本。", "result": "在包括CIFAR、Caltech、ImageNet和CelebA在内的8个大型数据集上进行了图像分类任务实验，结果表明，本文提出的方法在16种最先进的对比学习方法中表现出更好的泛化性能和学习收敛速度。尤其是BiasedMNIST数据集上提高了22.9%。", "conclusion": "本文提出上下文丰富对比损失函数能有效提升学习效果和减少信息扭曲。该方法在改进对比学习以提升性能和泛化能力尤其是系统失真任务中的应用表现优异。"}}
{"id": "2512.02074", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.02074", "abs": "https://arxiv.org/abs/2512.02074", "authors": ["Zirui Lin", "Haris Gulzar", "Monnika Roslianna Busto", "Akiko Masaki", "Takeharu Eda", "Kazuhiro Nakadai"], "title": "Dialect Identification Using Resource-Efficient Fine-Tuning Approaches", "comment": "Published in APSIPA ASC 2025", "summary": "Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.", "AI": {"tldr": "研究将Memory-Efficient Fine-Tuning（MEFT）方法应用于通用预训练语音模型，以提高方言识别任务的效率，显著减少了内存使用和加速了训练速度。", "motivation": "方言识别（DI）是一项从语音信号中识别同一语言中的不同方言的任务。微调用于任务如DI的语音模型在计算成本和内存需求方面很昂贵。为了应对这些挑战，我们探索了Memory-Efficient Fine-Tuning（MEFT）方法。", "method": "我们探究了Memory-Efficient Fine-Tuning（MEFT）方法，并将其应用于通用预训练语音模型。我们基于不同的MEFT方法全面分析了GPU内存使用情况和微调速度。", "result": "我们将Whisper模型微调以识别来自KeSpeech数据集的六种汉语方言。这将GPU内存使用减少了多达73.25%，并加快了2.1倍的训练速度，同时保持了与普通微调和PEFT方法相当的准确性。", "conclusion": "通过应用Memory-Efficient Fine-Tuning（MEFT）方法，我们证明了它可以有效降低GPU内存占用和提升训练速度，同时保持了微调和PEFT方法的准确性。这种方法为方言识别等任务提供了更高效的选择。"}}
{"id": "2512.02161", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02161", "abs": "https://arxiv.org/abs/2512.02161", "authors": ["Kevin David Hayes", "Micah Goldblum", "Vikash Sehwag", "Gowthami Somepalli", "Ashwinee Panda", "Tom Goldstein"], "title": "FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges", "comment": "Accepted to NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.", "AI": {"tldr": "研究提出了一种联合评估T2I和VLM模型的方法，通过VLM来识别T2I生成图像的27种失败模式，并揭示了现有度量标准的不足，强调了更精确度量标准的重要性。", "motivation": "T2I模型在生成视觉上令人印象深刻的图像时，往往无法准确捕捉用户提示中的特定属性，如指定数量和颜色的对象，而现有视觉语言模型（VLM）的基准测试没有跟上VLM能够注释的场景复杂性。因此，需要一个层次化的评估框架来比较不同图像生成模型的提示遵循能力。", "method": "提出了一种结构化的方法来联合评估T2I模型和VLM模型，通过测试VLM模型是否能识别由具有挑战性提示条件下的T2I模型生成的27种特定的失败模式。", "result": "建立了一个数据集，包含了5种T2I模型（Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large）生成的提示和图像以及来自VLM模型（Molmo, InternVL3, Pixtral）的注释，由大语言模型（LLM，Llama3）生成，以测试VLM模型是否能正确识别生成图像中的失败模式。", "conclusion": "通过分析特定提示下的系统性错误，揭示了属性保真度和对象表示的系统性缺陷，提出现有度量标准不足以捕捉这些细微错误，强调了有针对性的基准测试的重要性，以推动生成模型的可靠性和可解释性发展。"}}
{"id": "2512.02141", "categories": ["cs.CL", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.02141", "abs": "https://arxiv.org/abs/2512.02141", "authors": ["Pritish N. Desai", "Tanay Kewalramani", "Srimanta Mandal"], "title": "Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation", "comment": null, "summary": "Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.", "AI": {"tldr": "通过减少训练数据集大小而不降低性能的策略，对BERT进行微调以识别仇恨言论，并通过TF-IDF选择最具信息量的样本，同时扩展分词器词汇来捕捉不断变化的仇恨言论术语，实现在计算效率上的改进。", "motivation": "社交媒体上的仇恨言论是一个持续且不断变化的挑战，经常出现新的俚语和隐藏的术语以规避检测系统。因此，需要一种数据高效的策略来处理这种问题。", "method": "使用TF-IDF的采样选择机制保留最具信息量的75%样本，同时扩展BERT分词器的词汇以捕捉特定领域的俚语和词汇变体。", "result": "在常用仇恨言论数据集上的实验表明，该方法能够达到和其他方法相当的表现，同时改进了计算效率。", "conclusion": "这种方法展示了它在可扩展和适应性的仇恨内容监控方面具有应用潜力。"}}
{"id": "2512.02162", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.02162", "abs": "https://arxiv.org/abs/2512.02162", "authors": ["Rahul Mehta"], "title": "Mapping of Lesion Images to Somatic Mutations", "comment": "https://dl.acm.org/doi/abs/10.1145/3340531.3414074#sec-terms", "summary": "Medical imaging is a critical initial tool used by clinicians to determine a patient's cancer diagnosis, allowing for faster intervention and more reliable patient prognosis. At subsequent stages of patient diagnosis, genetic information is extracted to help select specific patient treatment options. As the efficacy of cancer treatment often relies on early diagnosis and treatment, we build a deep latent variable model to determine patients' somatic mutation profiles based on their corresponding medical images. We first introduce a point cloud representation of lesions images to allow for invariance to the imaging modality. We then propose, LLOST, a model with dual variational autoencoders coupled together by a separate shared latent space that unifies features from the lesion point clouds and counts of distinct somatic mutations. Therefore our model consists of three latent space, each of which is learned with a conditional normalizing flow prior to account for the diverse distributions of each domain. We conduct qualitative and quantitative experiments on de-identified medical images from The Cancer Imaging Archive and the corresponding somatic mutations from the Pan Cancer dataset of The Cancer Genomic Archive. We show the model's predictive performance on the counts of specific mutations as well as it's ability to accurately predict the occurrence of mutations. In particular, shared patterns between the imaging and somatic mutation domain that reflect cancer type. We conclude with a remark on how to improve the model and possible future avenues of research to include other genetic domains.", "AI": {"tldr": "本研究开发了一种基于病变点云表示和变分自编码器的深度模型LLOST，不仅能预测体细胞突变的计数和发生情况，还能揭示医学图像与突变数据间的潜在模式，以期改善癌症诊疗。", "motivation": "鉴于早期癌症诊疗对于治疗效果的重要性，本文旨在建立一个深度潜在变量模型，依据医学图像来确定患者的体细胞突变谱型，从而改进癌症的早期诊断和治疗。", "method": "该研究构建了一个深度潜在变量模型，使用病变点云表示医学图像，并提出了LLOST模型，即通过两个变分自编码器共享一个潜在空间来统一病变点云和特定体细胞突变的计数。该模型包含三个潜在空间，并使用条件归一化流先验来学习每个领域不同的分布。", "result": "在The Cancer Imaging Archive中的去识别医学图像和The Cancer Genomic Archive的Pan Cancer数据集中的对应体细胞突变数据上的定性和定量实验表明，该模型能有效地预测特定突变的计数，并准确地预测突变的发生。特别地，该模型揭示了医学图像和体细胞突变领域之间共享的符合癌症类型的模式。", "conclusion": "该研究展示了通过结合医学图像信息和体细胞突变数据可以实现癌症类型的更准确预测。作者同时提出了改进模型和未来研究可能涉及的基因组其他领域的方向。"}}
{"id": "2512.02185", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02185", "abs": "https://arxiv.org/abs/2512.02185", "authors": ["Ziyan Wang", "Enmao Diao", "Qi Le", "Pu Wang", "Guanchu Wang", "Minwoo Lee", "Shu-ping Yeh", "Li Yang"], "title": "Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models", "comment": "7 pages, 3 figures", "summary": "Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.", "AI": {"tldr": "This paper introduces RESP, a pruning framework tailored for reasoning LLMs that maintains model performance even at high sparsity levels, addressing resource constraints without sacrificing accuracy.", "motivation": "The motivation for the study is to address the high computing and memory costs associated with deploying large RLMs and the adverse impacts of existing pruning techniques on these models.", "method": "RESP is a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration to maintain calibration fidelity as sparsity increases.", "result": "Experiments show that RESP significantly outperforms existing structured pruning methods, preserving near-dense accuracy with up to 20-30% sparsity and mitigating performance decline at higher sparsity levels. At 40% sparsity, it attains 81.3% accuracy on GSM8K and 59.6% on MathQA.", "conclusion": "The study concludes that RESP is a promising approach for reducing the computational resources required for deploying reasoning LLMs without compromising on their reasoning capabilities and accuracy."}}
{"id": "2512.02172", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02172", "abs": "https://arxiv.org/abs/2512.02172", "authors": ["Pranav Asthana", "Alex Hanson", "Allen Tu", "Tom Goldstein", "Matthias Zwicker", "Amitabh Varshney"], "title": "SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting", "comment": "Project Page: https://splatsure.github.io/", "summary": "3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.", "AI": {"tldr": "论文提出了一种新的方法——SplatSuRe，它能智能地在场景的适当部分应用超分辨率技术，从而生成比现有方法更为清晰和一致的3D图像，特别是在感兴趣的前景区域。", "motivation": "3D Gaussian Splatting 方式能够生成高质量的新视角合成，激发了生成高于训练期间可用分辨率的渲染的兴趣。独立地增强每个图像会导致多次视图的一致性问题，进而导致模糊的渲染。该研究旨在解决这个问题，通过引入SplatSuRe方法以解决多视图一致性问题。", "method": "SplatSuRe 方法选择性地在缺乏高频监督的欠采样区域应用超分辨率内容，以生成更清晰和一致的结果。该方法利用相机姿态与场景几何之间的关系来确定在哪里添加超分辨率内容。", "result": "在Tanks & Temples, Deep Blending 和 Mip-NeRF 360数据集上，SplatSuRe 方法在保真度和感知质量上都超过了基线方法，特别是对于细节丰富的前景区域效果显著。", "conclusion": "SplatSuRe提出了一种创新的超分辨率应用策略，能够提升3D场景渲染的质量，尤其是在前景区域提供更高的细节和更好的一致性。"}}
{"id": "2512.02195", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02195", "abs": "https://arxiv.org/abs/2512.02195", "authors": ["David Ph. Shakouri", "Crit Cremers", "Niels O. Schiller"], "title": "A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation", "comment": "23 pages, 7 figures, 11 tables. Related work: arXiv:2503.18702. This is the peer-reviewed publisher's version, downloadable from: https://www.clinjournal.org/clinj/article/view/193", "summary": "This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.", "AI": {"tldr": "MODOMA系统通过成人和儿童语言模型之间的互动进行语言习得实验，实验结果显示出成功习得离散的语法规则类别，验证了MODOMA方法的有效性。", "motivation": "研究目的是开发一种计算多代理实验环境，能够模拟能够进行无监督语言学习的过程。", "method": "MODOMA系统使用统计和基于规则的方法，使成人和儿童代理模型之间的交互来无监督地进行语言习得实验。", "result": "实验结果显示，基于成人代理生成的数据，儿童代理能够习得功能和内容类别，并且机器生成数据与人类数据呈现相似模式。", "conclusion": "MODOMA系统的实验结果证明了其对于模拟能够进行无监督语言习得的有效性。"}}
{"id": "2512.02188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02188", "abs": "https://arxiv.org/abs/2512.02188", "authors": ["Mansoor Ali", "Maksim Richards", "Gilberto Ochoa-Ruiz", "Sharib Ali"], "title": "RobustSurg: Tackling domain generalisation for out-of-distribution surgical scene segmentation", "comment": "Submitted to Medical Image Analysis", "summary": "While recent advances in deep learning for surgical scene segmentation have demonstrated promising results on single-centre and single-imaging modality data, these methods usually do not generalise to unseen distribution (i.e., from other centres) and unseen modalities. Current literature for tackling generalisation on out-of-distribution data and domain gaps due to modality changes has been widely researched but mostly for natural scene data. However, these methods cannot be directly applied to the surgical scenes due to limited visual cues and often extremely diverse scenarios compared to the natural scene data. Inspired by these works in natural scenes to push generalisability on OOD data, we hypothesise that exploiting the style and content information in the surgical scenes could minimise the appearances, making it less variable to sudden changes such as blood or imaging artefacts. This can be achieved by performing instance normalisation and feature covariance mapping techniques for robust and generalisable feature representations. Further, to eliminate the risk of removing salient feature representation associated with the objects of interest, we introduce a restitution module within the feature learning ResNet backbone that can enable the retention of useful task-relevant features. To tackle the lack of multiclass and multicentre data for surgical scene segmentation, we also provide a newly curated dataset that can be vital for addressing generalisability in this domain. Our proposed RobustSurg obtained nearly 23% improvement on the baseline DeepLabv3+ and from 10-32% improvement on the SOTA in terms of mean IoU score on an unseen centre HeiCholSeg dataset when trained on CholecSeg8K. Similarly, RobustSurg also obtained nearly 22% improvement over the baseline and nearly 11% improvement on a recent SOTA method for the target set of the EndoUDA polyp dataset.", "AI": {"tldr": "The paper introduces RobustSurg, a method that enhances surgical scene segmentation by improving generalisability through normalisation, feature mapping, and modular restitution. It demonstrates significant performance gains on unseen datasets across different centres and modalities, showcasing the ability to handle diverse surgical scenes effectively.", "motivation": "The motivation is to address the limitations of existing methods in handling generalisation issues and domain gaps in surgical scene segmentation caused by differences between centres and imaging modalities. Existing approaches for natural scenes are not suitable for surgical scenes due to their distinctive characteristics.", "method": "The method utilizes instance normalisation and feature covariance mapping techniques to robustify feature representations, and introduces a restitution module to retain task-relevant features, enhancing the model's ability to generalize across different surgical scenes and imaging modalities.", "result": "The proposed RobustSurg shows a notable performance improvement, achieving nearly 23% better results compared to the DeepLabv3+ baseline and up to 32% improvement over state-of-the-art methods on the HeiCholSeg dataset. For the EndoUDA polyp dataset, it saw an additional 11% improvement beyond the current top-performing method.", "conclusion": "The analysis supports the hypothesis that understanding and manipulating style and content information in surgical scenes via normalisation and restitution techniques enhances the model's robustness and generalisability to unseen distributions and imaging modalities."}}
{"id": "2512.02201", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02201", "abs": "https://arxiv.org/abs/2512.02201", "authors": ["Vukosi Marivatee", "Kayode Olaleye", "Sitwala Mundia", "Andinda Bakainga", "Unarine Netshifhefhe", "Mahmooda Milanzie", "Tsholofelo Hope Mogale", "Thapelo Sindane", "Zainab Abdulrasaq", "Kesego Mokgosi", "Chijioke Okorie", "Nia Zion Van Wyk", "Graham Morrissey", "Dale Dunbar", "Francois Smit", "Tsosheletso Chidi", "Rooweither Mabuya", "Andiswa Bukula", "Respect Mlambo", "Tebogo Macucwa", "Idris Abdulmumin", "and Seani Rananga"], "title": "Swivuriso: The South African Next Voices Multilingual Speech Dataset", "comment": "Work in Progress", "summary": "This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.", "AI": {"tldr": "本文介绍了Swivuriso，这是一个包含3000小时多语言语音数据集，旨在支持南非七种语言的自动语音识别（ASR）技术的发展与评估。", "motivation": "该数据集填补了现有ASR数据集在某些关键领域中的空白，并涵盖了农业、医疗和通用话题。", "method": "作者描述了数据集的设计原则、伦理考量以及数据收集程序。", "result": "展示了使用该数据集训练/微调ASR模型的基本结果，并与其他针对相关语言的ASR数据集进行了比较。", "conclusion": "通过对Swivuriso数据集的构建及其在ASR模型训练上的应用，研究者提供了重要的语料库支持，有助于提高多语言ASR技术的能力。"}}
{"id": "2512.02198", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02198", "abs": "https://arxiv.org/abs/2512.02198", "authors": ["Miguel L. Martins", "Miguel T. Coimbra", "Francesco Renna"], "title": "Multifractal Recalibration of Neural Networks for Medical Imaging Segmentation", "comment": "30 pages, 9 figures, journal paper", "summary": "Multifractal analysis has revealed regularities in many self-seeding phenomena, yet its use in modern deep learning remains limited. Existing end-to-end multifractal methods rely on heavy pooling or strong feature-space decimation, which constrain tasks such as semantic segmentation. Motivated by these limitations, we introduce two inductive priors: Monofractal and Multifractal Recalibration. These methods leverage relationships between the probability mass of the exponents and the multifractal spectrum to form statistical descriptions of encoder embeddings, implemented as channel-attention functions in convolutional networks.\n  Using a U-Net-based framework, we show that multifractal recalibration yields substantial gains over a baseline equipped with other channel-attention mechanisms that also use higher-order statistics. Given the proven ability of multifractal analysis to capture pathological regularities, we validate our approach on three public medical-imaging datasets: ISIC18 (dermoscopy), Kvasir-SEG (endoscopy), and BUSI (ultrasound).\n  Our empirical analysis also provides insights into the behavior of these attention layers. We find that excitation responses do not become increasingly specialized with encoder depth in U-Net architectures due to skip connections, and that their effectiveness may relate to global statistics of instance variability.", "AI": {"tldr": "The paper introduces two inductive priors, Monofractal and Multifractal Recalibration, using a U-Net-based framework, demonstrating significant improvements in medical image datasets compared to models with other channel-attention mechanisms.", "motivation": "To overcome the limitations of existing multifractal methods in deep learning, which are constrained by heavy pooling or strong feature-space decimation, particularly in tasks like semantic segmentation.", "method": "Structure", "result": "Multifractal recalibration yields substantial performance gains over a baseline model using other channel-attention mechanisms that also utilize higher-order statistics, as validated on three public medical-imaging datasets.", "conclusion": "The study provides valuable insights into the behavior of attention layers in U-Net architectures, specifically noting the impact of skip connections on excitation responses and the potential relation to global instance variability statistics."}}
{"id": "2512.02240", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02240", "abs": "https://arxiv.org/abs/2512.02240", "authors": ["Alexander Gurung", "Nikolay Malkin", "Mirella Lapata"], "title": "Lightweight Latent Reasoning for Narrative Tasks", "comment": null, "summary": "Large language models (LLMs) tackle complex tasks by generating long chains of thought or \"reasoning traces\" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.", "AI": {"tldr": "LiteReason is proposed for efficient latent reasoning in LLMs by introducing a lightweight module that can bypass certain reasoning steps, achieving comparable performance to more computationally expensive methods with significant reduction in reasoning length.", "motivation": "To reduce the computational cost associated with optimizing large language models for narrative tasks that require extensive reasoning.", "method": "LiteReason uses a Reasoning Projector module to generate continuous latent tokens that enable the model to bypass reasoning steps. The activation of this projector during reinforcement learning is handled by a policy model.", "result": "Experiments on plot hole detection and book chapter generation show improved performance over latent reasoning baselines and close results to non-latent RL training, with a reduction of 77-92% in the length of the reasoning required.", "conclusion": "LiteReason demonstrates an effective approach to balancing the performance and computational requirements for latent variable reasoning in language models, guiding the training to a more favorable part of the performance-computation tradeoff curve."}}
{"id": "2512.02224", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02224", "abs": "https://arxiv.org/abs/2512.02224", "authors": ["Chen Feng", "Tianhao Peng", "Fan Zhang", "David Bull"], "title": "Towards Unified Video Quality Assessment", "comment": "8 pages, 3 figures", "summary": "Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic\" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.", "AI": {"tldr": "Unified-VQA作为一个框架，使用MoE解决视频质量评估问题，通过多个感知专家和诊断多任务头生成全局质量评分和可解释的缺陷向量，在多种视频格式和失真类型中表现出优于众多基准方法的性能。", "motivation": "现有视频质量评估方法通常无法提供诊断性的反馈，且大部分是专门针对特定格式的指标。为了克服这些限制，提出了Unified-VQA框架，旨在提供一种通用的、可以应用于各种视频格式和失真类型的视频质量评估方法。", "method": "Unified-VQA采用诊断专家混合模型（MoE）框架，通过多个感知专家处理不同感知领域的问题，并使用排名启发的损失函数进行优化。此外，框架中加入诊断多任务头以生成全局质量评分和可解释的多维缺陷向量，使用弱监督学习策略进行优化。", "result": "使用固定模型参数，Unified-VQA在17个涵盖HD、UHD、HDR和HFR格式、多种流媒体缺陷的数据库中，对18种以上基准方法展现了一致且优越的性能。", "conclusion": "Unified-VQA在视频质量评估和诊断缺陷检测任务上展现了优于众多基准方法的表现，为实践性、行动性和可解释性的视频质量评估提供了重要步骤。"}}
{"id": "2512.02246", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02246", "abs": "https://arxiv.org/abs/2512.02246", "authors": ["Olivia Kim"], "title": "DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models", "comment": null, "summary": "Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.", "AI": {"tldr": "文章提出DETAIL框架，用于评估提示细节度对大型语言模型的影响，实验表明增加提示细节度可以提高模型的准确性，特别是小型模型和程序性任务。", "motivation": "研究动机在于探索提示细节度对大型语言模型推理性能的影响。目前已有的研究较少涉及这一点。", "method": "本研究提出了DETAIL框架，用于评估不同层级提示细节对大型语言模型性能的影响。通过使用GPT-4生成多层级提示，并利用困惑度量化提示的细节度，同时采用基于GPT的语义等价性来评估正确性。", "result": "实验结果显示，增加提示的细节度能够提高模型的准确性，特别是对于小型模型和程序性任务。", "conclusion": "结果强调了需要适应性提示策略的重要性，并且提供了支持进一步研究的工具和数据。"}}
{"id": "2512.02231", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02231", "abs": "https://arxiv.org/abs/2512.02231", "authors": ["Le Thien Phuc Nguyen", "Zhuoran Yu", "Samuel Low Yu Hang", "Subin An", "Jeongik Lee", "Yohan Ban", "SeungEun Chung", "Thanh-Huy Nguyen", "JuWan Maeng", "Soochahn Lee", "Yong Jae Lee"], "title": "See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models", "comment": "preprint", "summary": "Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.", "AI": {"tldr": "介绍AV-SpeakerBench基准测试，用于评估多模态大语言模型在真实视频中细粒度推理的能力，特别关注说话者的音频视觉理解。Gemini系列模型表现最佳。", "motivation": "现有的视频基准测试很少评估模型对人类言语的细粒度推理能力，而这一基准测试填补了这一空白，并有助于评估模型对视频中的说话者、所说内容及时机的精确理解和对齐。", "method": "分析模型在音频视觉理解中的表现，特别是针对人类言语的细粒度推理。基准测试AV-SpeakerBench包含3,212个选择题，旨在评估模型在真实视频中基于说话者的音频视觉推理能力。", "result": "综合评估显示，Gemini系列模型优于开源系统，其中Gemini 2.5 Pro表现最佳。Qwen3-Omni-30B接近Gemini 2.0 Flash，但在音频视觉融合方面不如Gemini 2.5 Pro强大。", "conclusion": "AV-SpeakerBench为未来多模态系统的细粒度音频视觉推理建立了严格的基础，有助于推动该领域的进一步研究和改进。"}}
{"id": "2512.02251", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.02251", "abs": "https://arxiv.org/abs/2512.02251", "authors": ["Liangji Kong", "Aditya Joshi", "Sarvnaz Karimi"], "title": "CAIRNS: Balancing Readability and Scientific Accuracy in Climate Adaptation Question Answering", "comment": "Short Paper; Under Review at The WebConf 2026 (single-blind submission)", "summary": "Climate adaptation strategies are proposed in response to climate change. They are practised in agriculture to sustain food production. These strategies can be found in unstructured data (for example, scientific literature from the Elsevier website) or structured (heterogeneous climate data via government APIs). We present Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS), a framework that enables experts -- farmer advisors -- to obtain credible preliminary answers from complex evidence sources from the web. It enhances readability and citation reliability through a structured ScholarGuide prompt and achieves robust evaluation via a consistency-weighted hybrid evaluator that leverages inter-model agreement with experts. Together, these components enable readable, verifiable, and domain-grounded question-answering without fine-tuning or reinforcement learning. Using a previously reported dataset of expert-curated question-answers, we show that CAIRNS outperforms the baselines on most of the metrics. Our thorough ablation study confirms the results on all metrics. To validate our LLM-based evaluation, we also report an analysis of correlations against human judgment.", "AI": {"tldr": "研究提出了一种框架CAIRNS，旨在使专家从复杂的数据源中获得可读性和引用可靠的初步答案，且在多个评价指标上优于基线模型。", "motivation": "旨在利用无结构数据（如Elsevier网站上的科学文献）或结构化数据（通过政府API获取的异构气候数据）中的适应性策略，以帮助维持农业领域的食品生产。", "method": "本研究提出了Climate Adaptation question-answering with Improved Readability and Noted Sources (CAIRNS)框架，该框架允许专家（如农民顾问）从复杂的网络证据来源中获取可信的初步答案。通过结构化的ScholarGuide提示增强了可读性和引用可靠性，并通过利用模型间的一致性与专家评价相结合的综合评估器实现了稳健的评估。", "result": "通过使用一个以前报告的专家策划的问题-答案的数据集，研究显示CAIRNS在大多数评价指标上都优于基线模型。详细的功能测试进一步确认了这些结果。", "conclusion": "CAIRNS框架能够提供可读、可验证且基于领域的问答，而无需微调或强化学习，并且其基于大型语言模型的评价通过与人类判断的相关性分析得到了验证。"}}
{"id": "2512.02258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02258", "abs": "https://arxiv.org/abs/2512.02258", "authors": ["Shuang Chen", "Tomas Krajnik", "Farshad Arvin", "Amir Atapour-Abarghouei"], "title": "Exploring the Potentials of Spiking Neural Networks for Image Deraining", "comment": "Accepted By AAAI2026", "summary": "Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.", "AI": {"tldr": "The paper proposes a novel visual LIF neuron and associated modules to enhance spiking neural networks' performance in image deraining, demonstrating superior results to current state-of-the-art methods with much lower energy consumption.", "motivation": "The study is motivated by the fact that Spiking Neural Networks (SNNs), which are biologically plausible and energy-efficient, have been underexplored in low-level vision tasks and often lack capabilities for spatial context understanding and hierarchical multi-scale representation.", "method": "The paper introduces the Visual LIF (VLIF) neuron to address the challenge of spatial contextual understanding in traditional spiking neurons and uses it in conjunction with the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation in the context of image deraining.", "result": "The proposed method significantly outperforms state-of-the-art SNN-based deraining approaches while using only 13% of their energy consumption on five benchmark deraining datasets.", "conclusion": "The findings support the potential for SNNs to be deployed in energy-efficient and high-performance low-level vision tasks by leveraging the novel VLIF neuron and associated modules."}}
{"id": "2512.02299", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02299", "abs": "https://arxiv.org/abs/2512.02299", "authors": ["Boya Zhang", "Alban Bornet", "Rui Yang", "Nan Liu", "Douglas Teodoro"], "title": "HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models", "comment": null, "summary": "How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.", "AI": {"tldr": "研究通过HealthContradict数据集评估语言模型在面对矛盾上下文时回答健康相关问题的能力，发现微调模型能有效利用正确信息并抵制误导信息。", "motivation": "研究动机在于了解语言模型如何利用上下文信息回答健康相关的问题，以及在其面对矛盾上下文时其回答是如何受到影响的。", "method": "该研究使用HealthContradict数据集，该数据集包含920个独特的实例，每个实例包括一个健康相关问题、一个由科学证据支持的事实答案，以及两个给出矛盾立场的文档。研究者评估了语言模型在给定正确、错误或矛盾的上下文时回答问题的能力。", "result": "实验表明，经过微调的生物医学语言模型不仅依赖于预训练中的参数知识，还在于其利用正确上下文同时抵御错误上下文的能力。", "conclusion": "该研究发现微调的生物医学语言模型在处理矛盾上下文时，不仅依靠预训练的知识，还能有效识别和利用正确的上下文信息，同时避免被错误信息误导。"}}
{"id": "2512.02268", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.02268", "abs": "https://arxiv.org/abs/2512.02268", "authors": ["Jeremy Andrew Irvin", "Jiaqi Han", "Zikui Wang", "Abdulaziz Alharbi", "Yufei Zhao", "Nomin-Erdene Bayarsaikhan", "Daniele Visioni", "Andrew Y. Ng", "Duncan Watson-Parris"], "title": "Spatiotemporal Pyramid Flow Matching for Climate Emulation", "comment": null, "summary": "Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .", "AI": {"tldr": "本文提出了用于气候模拟的时空金字塔流匹配方法SPF，通过分层次的空间时间建模，提供了更高效的气候预测。", "motivation": "鉴于之前使用的基于天气尺度自回归的气候生成方法耗时且不稳定，本文动机在于开发一种新的高效并能稳定工作的气候模拟方法。", "method": "本文提出了Spatiotemporal Pyramid Flows (SPF)，这是一种新的流匹配方法，能分层次地在空间和时间尺度上建模数据，以模拟气候。受级联视频模型的启发，SPF将生成轨迹划分为时空金字塔，逐步增加空间分辨率，减少计算量。", "result": "在ClimateBench上，SPF在年和月的时间尺度上优于强大的流匹配基线和预训练模型，并且在较粗的时间层次上更快。此外，SPF在不同的气候模型上具有良好的泛化性能。", "conclusion": "Spatiotemporal Pyramid Flows (SPF) 和 ClimateSuite 为跨时间尺度的准确、高效的概率气候模拟提供了基础，可以应用于许多未来的气候情景。"}}
{"id": "2512.02304", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02304", "abs": "https://arxiv.org/abs/2512.02304", "authors": ["Jack Lu", "Ryan Teehan", "Jinran Jin", "Mengye Ren"], "title": "When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers", "comment": null, "summary": "Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.", "AI": {"tldr": "研究了大型语言模型的自我验证与其他家族验证，发现跨家族验证效果更佳，后训练虽然减少了自我改进但在跨家族验证中更有利，数学和逻辑任务具有高度可验证性。", "motivation": "探讨了解决方案验证器在大型语言模型中的作用，尤其是在解决跨家族验证的效果以及后训练对验证效果的影响。", "method": "通过系统性研究37个模型，涵盖了多个家族、规模以及基础和后训练变体，评估了9个基准测试，包括逻辑推理、结构性谜题、符号计算、数学、常识、事实回忆以及领域知识。研究了自我验证与其他模型家族之间的验证方法。", "result": "研究发现跨家族验证尤其有效，后训练减少了自我改进却增强了跨家族改进能力，数学和逻辑任务具有最高的固有可验证性。", "conclusion": "此研究揭示了验证器在大型语言模型中的交互作用，特别是跨家族验证的效果，为未来研究提供了新视角和方向。"}}
{"id": "2512.02273", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02273", "abs": "https://arxiv.org/abs/2512.02273", "authors": ["Peng Kang", "Xijun Wang", "Yu Yuan"], "title": "Progressive Image Restoration via Text-Conditioned Video Generation", "comment": "First two authors contributed equally to this work. IEEE ICNC Accepted", "summary": "Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.", "AI": {"tldr": "将CogVideo模型重新用于视觉恢复任务，通过微调使其生成恢复轨迹而不是自然视频运动，展示了在超分辨率、去模糊和低光增强任务上的有效性，并且在真实场景中展示了零样本鲁棒性。", "motivation": "尽管文本到视频模型显示了强大的时间生成能力，但它们在图像恢复中的潜力尚未被充分研究。该研究旨在探索其在不同恢复任务中的有效性。", "method": "构建用于超分辨率、去模糊和低光增强的合成数据集，并通过两种提示策略对比，使用全统一文本提示和通过LLaVA多模态LLM生成并由ChatGPT调整的场景特定提示进行微调。", "result": "微调后的模型能够将时间进展与恢复质量关联起来，改善了如PSNR、SSIM和LPIPS等感知指标，展示了在恢复空间细节和光照一致性的同时维护时间一致性。", "conclusion": "模型不仅在合成数据集上表现良好，还在真实数据集ReLoBlur上展示了无需额外训练即可实现的零样本鲁棒性和时间恢复的可解释性。"}}
{"id": "2512.02363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02363", "abs": "https://arxiv.org/abs/2512.02363", "authors": ["Lei Fu", "Xiang Chen", "Kaige Gao Xinyue Huang", "Kejian Tong"], "title": "Memory-Augmented Knowledge Fusion with Safety-Aware Decoding for Domain-Adaptive Question Answering", "comment": null, "summary": "Domain-specific question answering (QA) systems for services face unique challenges in integrating heterogeneous knowledge sources while ensuring both accuracy and safety. Existing large language models often struggle with factual consistency and context alignment in sensitive domains such as healthcare policies and government welfare. In this work, we introduce Knowledge-Aware Reasoning and Memory-Augmented Adaptation (KARMA), a novel framework designed to enhance QA performance in care scenarios. KARMA incorporates a dual-encoder architecture to fuse structured and unstructured knowledge sources, a gated memory unit to dynamically regulate external knowledge integration, and a safety-aware controllable decoder that mitigates unsafe outputs using safety classification and guided generation techniques. Extensive experiments on a proprietary QA dataset demonstrate that KARMA outperforms strong baselines in both answer quality and safety. This study offers a comprehensive solution for building trustworthy and adaptive QA systems in service contexts.", "AI": {"tldr": "KARMA框架通过融合结构化和非结构化知识源、动态调节外部知识整合以及控制生成不安全输出，提高了特定服务场景下的问答系统性能和安全性。", "motivation": "由于现有大型语言模型在如医疗政策和政府福利等敏感领域中的事实一致性和语境对齐方面存在问题，本研究旨在解决这些挑战，以提高服务场景下的问答系统的准确性和安全性。", "method": "KARMA框架采用双编码器架构融合结构化和非结构化知识源，使用门控记忆单元动态调节外部知识整合，并通过安全分类和引导生成技术控制生成不安全输出。", "result": "在专有问答数据集上的广泛实验表明，KARMA在答案质量和安全性方面均优于强基准模型。", "conclusion": "本研究为在服务场景中构建可信和自适应的问答系统提供了全面的解决方案。"}}
{"id": "2512.02290", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02290", "abs": "https://arxiv.org/abs/2512.02290", "authors": ["Andre Juarez", "Luis Salsavilca", "Frida Coaquira", "Celso Gonzales"], "title": "Enhancing Cross Domain SAR Oil Spill Segmentation via Morphological Region Perturbation and Synthetic Label-to-SAR Generation", "comment": null, "summary": "Deep learning models for SAR oil spill segmentation often fail to generalize across regions due to differences in sea-state, backscatter statistics, and slick morphology, a limitation that is particularly severe along the Peruvian coast where labeled Sentinel-1 data remain scarce. To address this problem, we propose \\textbf{MORP--Synth}, a two-stage synthetic augmentation framework designed to improve transfer from Mediterranean to Peruvian conditions. Stage~A applies Morphological Region Perturbation, a curvature guided label space method that generates realistic geometric variations of oil and look-alike regions. Stage~B renders SAR-like textures from the edited masks using a conditional generative INADE model. We compile a Peruvian dataset of 2112 labeled 512$\\times$512 patches from 40 Sentinel-1 scenes (2014--2024), harmonized with the Mediterranean CleanSeaNet benchmark, and evaluate seven segmentation architectures. Models pretrained on Mediterranean data degrade from 67.8\\% to 51.8\\% mIoU on the Peruvian domain; MORP--Synth improves performance up to +6 mIoU and boosts minority-class IoU (+10.8 oil, +14.6 look-alike).", "AI": {"tldr": "MORP--Synth, a two-stage augmentation framework, improves deep learning models' performance on SAR oil spill segmentation from Mediterranean to Peruvian conditions.", "motivation": "To improve the generalization of deep learning models for SAR oil spill segmentation across different regions, especially from Mediterranean to Peruvian conditions, due to limited labeled data.", "method": "MORP--Synth is a two-stage synthetic augmentation framework. Stage A uses Morphological Region Perturbation for realistic geometric variations. Stage B uses a conditional generative INADE model to render SAR-like textures.", "result": "Models pretrained on Mediterranean data performed poorly on Peruvian data (67.8% to 51.8% mIoU). MORP--Synth improved the performance by up to +6 mIoU and increased minority-class IoU (+10.8 for oil, +14.6 for look-alike).", "conclusion": "The proposed MORP--Synth augmentation method successfully enhances the model's ability to generalize to the Peruvian region, demonstrating significant improvements in performance metrics."}}
{"id": "2512.02402", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.02402", "abs": "https://arxiv.org/abs/2512.02402", "authors": ["Yunchao Wang", "Guodao Sun", "Zihang Fu", "Zhehao Liu", "Kaixing Du", "Haidong Gao", "Ronghua Liang"], "title": "TaleFrame: An Interactive Story Generation System with Fine-Grained Control and Large Language Models", "comment": "11 pages", "summary": "With the advancement of natural language generation (NLG) technologies, creative story generation systems have gained increasing attention. However, current systems often fail to accurately translate user intent into satisfactory story outputs due to a lack of fine-grained control and unclear input specifications, limiting their applicability. To address this, we propose TaleFrame, a system that combines large language models (LLMs) with human-computer interaction (HCI) to generate stories through structured information, enabling precise control over the generation process. The innovation of TaleFrame lies in decomposing the story structure into four basic units: entities, events, relationships, and story outline. We leverage the Tinystories dataset, parsing and constructing a preference dataset consisting of 9,851 JSON-formatted entries, which is then used to fine-tune a local Llama model. By employing this JSON2Story approach, structured data is transformed into coherent stories. TaleFrame also offers an intuitive interface that supports users in creating and editing entities and events and generates stories through the structured framework. Users can control these units through simple interactions (e.g., drag-and-drop, attach, and connect), thus influencing the details and progression of the story. The generated stories can be evaluated across seven dimensions (e.g., creativity, structural integrity), with the system providing suggestions for refinement based on these evaluations. Users can iteratively adjust the story until a satisfactory result is achieved. Finally, we conduct quantitative evaluation and user studies that demonstrate the usefulness of TaleFrame. Dataset available at https://huggingface.co/datasets/guodaosun/tale-frame.", "AI": {"tldr": "TaleFrame combines LLMs with HCI to create a system for generating stories through a structured framework, allowing for precise control and evaluation of story outputs.", "motivation": "The motivation behind this paper is to improve the accuracy of translating user intent into satisfactory story outputs in creative story generation systems.", "method": "The method involves proposing a system named TaleFrame that uses structured information and HCI to enable precise control over the story generation process.", "result": "The results show that by using this structured approach, users can effectively control the details and progression of the story, with evaluations across seven dimensions and iterative refinement suggestions.", "conclusion": "The conclusion is that the structured framework and structured data to coherent story transformation (JSON2Story) enables a more precise and applicable user-controlled story generation system, supported by quantitative evaluation and user studies."}}
{"id": "2512.02339", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02339", "abs": "https://arxiv.org/abs/2512.02339", "authors": ["Chenshuang Zhang", "Kang Zhang", "Joon Son Chung", "In So Kweon", "Junmo Kim", "Chengzhi Mao"], "title": "Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision", "comment": "Accepted at NeurIPS 2025", "summary": "Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.", "AI": {"tldr": "通过利用预训练的视频扩散模型，研究开发了一种自我监督的跟踪器，它在区分视觉相似物体方面表现优于现有的自我监督方法。", "motivation": "现有的自我监督跟踪器在视觉线索模糊的情况下表现不佳，而本文希望探讨一种能在没有大量标签数据的情况下有效区分视觉相似物体的新方法。", "method": "研究发现预训练的视频扩散模型在其去噪过程中天然分离了早期阶段的运动表示，基于这一点，研究者设计了一种自我监督的跟踪器来利用这些扩散模型内在的运动表示能力。", "result": "新的自我监督跟踪器在标准基准上以及新的旨在测试视觉相似物体跟踪能力的测试中，对比现有自我监督方法，达到了6个百分点的性能提升。", "conclusion": "视觉化证据表明，这种方法基于扩散模型得到的运动表示能在跨越不同的视角变化和变形的情况下可靠地跟踪甚至完全相同的物体。"}}
{"id": "2512.02527", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02527", "abs": "https://arxiv.org/abs/2512.02527", "authors": ["Parth Pulkundwar", "Vivek Dhanawade", "Rohit Yadav", "Minal Sonkar", "Medha Asurlekar", "Sarita Rathod"], "title": "A Concise Review of Hallucinations in LLMs and their Mitigation", "comment": "7 pages", "summary": "Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.", "AI": {"tldr": "该论文概述了语言模型中幻觉问题的种类、起源及减少方法，提供了关于如何缓解幻觉问题的一站式资源。", "motivation": "理解当今出现的各种幻觉类型及其成因，找到减少幻觉的方法对自然语言处理领域至关重要，因此需要一个简洁明了的总结。", "method": "文章没有提供具体的方法部分，只是一份概述，旨在提供一个关于幻觉问题及可能缓解措施的整体视角。", "result": "没有给出具体的研究结果，而是提供了一个资源，帮助理解幻觉问题。", "conclusion": "通过提供一个明确的指南，该文档旨在帮助读者快速了解幻觉问题，并找到可能的减少幻觉的策略。"}}
{"id": "2512.02341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02341", "abs": "https://arxiv.org/abs/2512.02341", "authors": ["Fengyi Zhang", "Tianjun Zhang", "Kasra Khosoussi", "Zheng Zhang", "Zi Huang", "Yadan Luo"], "title": "TALO: Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction", "comment": null, "summary": "3D vision foundation models have shown strong generalization in reconstructing key 3D attributes from uncalibrated images through a single feed-forward pass. However, when deployed in online settings such as driving scenarios, predictions are made over temporal windows, making it non-trivial to maintain consistency across time. Recent strategies align consecutive predictions by solving global transformation, yet our analysis reveals their fundamental limitations in assumption validity, local alignment scope, and robustness under noisy geometry. In this work, we propose a higher-DOF and long-term alignment framework based on Thin Plate Spline, leveraging globally propagated control points to correct spatially varying inconsistencies. In addition, we adopt a point-agnostic submap registration design that is inherently robust to noisy geometry predictions. The proposed framework is fully plug-and-play, compatible with diverse 3D foundation models and camera configurations (e.g., monocular or surround-view). Extensive experiments demonstrate that our method consistently yields more coherent geometry and lower trajectory errors across multiple datasets, backbone models, and camera setups, highlighting its robustness and generality. Codes are publicly available at \\href{https://github.com/Xian-Bei/TALO}{https://github.com/Xian-Bei/TALO}.", "AI": {"tldr": "A new framework for aligning 3D predictions over time in online settings, offering better consistency and lower trajectory errors compared to current methods, robust to noisy geometry.", "motivation": "The motivation is to address the limitations of existing methods in maintaining consistency across temporal predictions in 3D scenes, which often fail under noisy conditions or due to assumptions that do not hold in practice. This work aims to increase robustness and alignment accuracy.", "method": "Our method is a higher-DOF and long-term alignment framework for 3D predictions in online settings, using Thin Plate Spline with globally propagated control points to align predictions over time. It also includes a point-agnostic submap registration to handle noisy geometry.", "result": "Experiments show the framework yields more coherent geometry and lower trajectory errors across multiple datasets, models, and camera setups, indicating robustness and generality.", "conclusion": "The proposed method is effective in improving consistency and robustness in 3D predictions in dynamic settings, being fully compatible with different 3D models and camera configurations."}}
{"id": "2512.02552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02552", "abs": "https://arxiv.org/abs/2512.02552", "authors": ["Francesco Paolo Savatteri", "Chahan Vidal-Gorène", "Florian Cafiero"], "title": "What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints", "comment": null, "summary": "We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.", "AI": {"tldr": "Evaluation of fake-news detection and virality prediction shows textual content is strong for the former, and virality prediction is complex and sensitive to data construction methods.", "motivation": "To evaluate practical tasks for online misinformation with rapid reaction needs, studying fake-news and virality in operational settings.", "method": "We compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features and sequence models for fake-news detection and virality prediction.", "result": "Textual content alone is strong for fake-news detection, while numeric-only pipelines are viable under compute constraints. Virality prediction is sensitive to label construction and non-linear structure is more informative than linear.", "conclusion": "The study provides insights into the effectiveness of different methodologies and the challenges in virality prediction, suggesting careful consideration in evaluation design and metric selection."}}
{"id": "2512.02344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02344", "abs": "https://arxiv.org/abs/2512.02344", "authors": ["Siyuan Sun", "Yongping Zhang", "Hongcheng Zeng", "Yamin Wang", "Wei Yang", "Wanting Yang", "Jie Chen"], "title": "A multi-weight self-matching visual explanation for cnns on sar images", "comment": null, "summary": "In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.", "AI": {"tldr": "A visual explanation method named MS-CAM is developed to improve CNN interpretability for synthetic aperture radar tasks, highlighting key features and increasing network transparency without compromising accuracy.", "motivation": "The complexity and lack of transparency of CNNs hinder their full application in SAR tasks where high reliability is necessary. This paper aims to improve the interpretability of CNNs to support their development and practical deployment in SAR.", "method": "The paper proposes a method called multi-weight self-matching class activation mapping (MS-CAM) to enhance the interpretability of CNNs in SAR tasks. MS-CAM combines the extracted feature maps and gradients from CNN with SAR images, applying both channel-wise and element-wise weights to visualize the decision basis.", "result": "Extensive experiments on a SAR target classification dataset show that MS-CAM enhances network interpretability by more accurately identifying the network's regions of interest and capturing detailed features in SAR images. Additionally, the effectiveness of MS-CAM for weakly-supervised object localization is demonstrated.", "conclusion": "The proposed MS-CAM method provides a way to increase the interpretability of CNNs when used in SAR applications. It highlights the network's key interest areas more accurately, thereby contributing to improved learning and understanding of feature representations in SAR images."}}
{"id": "2512.02555", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.02555", "abs": "https://arxiv.org/abs/2512.02555", "authors": ["Zheng Fang", "Donghao Xie", "Ming Pang", "Chunyuan Yuan", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo"], "title": "ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce", "comment": "Accepted by SIGIR 2025", "summary": "Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.", "AI": {"tldr": "提出的ADORE框架在克服数据稀缺性的同时改善推理能力，为工业应用中的资源高效、认知对齐的相关性建模建立了新的范式。", "motivation": "电子商务搜索中的相关性建模依然面临术语匹配方法和神经模型的语义差距以及领域特定难样例的稀缺性挑战。", "method": "ADORE框架结合了三个创新点：1) 基于规则的关联性判别模块，利用Chain-of-Thought LLM生成训练数据，并通过Kahneman-Tversky优化来与用户行为对齐；2) 具备错误类型意识的数据合成模块，可以自动生成对抗样本来增强鲁棒性；3) 基于关键属性的知识蒸馏模块，将领域特定属性层次结构注入可部署的学生模型中。方法克服了数据稀缺性，增强了推理能力。", "result": "大规模实验和线上A/B测试验证了ADORE框架的有效性。", "conclusion": "该框架为资源高效的认知对齐的相关性建模在工业应用中建立了新的范式。"}}
{"id": "2512.02351", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02351", "abs": "https://arxiv.org/abs/2512.02351", "authors": ["Shwai He", "Chaorui Deng", "Ang Li", "Shen Yan"], "title": "Understanding and Harnessing Sparsity in Unified Multimodal Models", "comment": "13 pages, 13 figures, 8 tables", "summary": "Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \\href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.", "AI": {"tldr": "本研究分析了统一多模态模型的组件，并提出了专家混合适应方法来提升生成模块的稀疏激活质量。", "motivation": "尽管统一多模态模型在理解与生成能力上取得了显著进展，但这种统一引入了推理效率不高的问题。本研究旨在系统了解这些低效率在不同组件中的表现形式。", "method": "本研究使用无训练修剪方法对统一多模态模型的各个组件进行了系统的分析，包括深度修剪和宽度缩减。此外，还提出了基于动态激活模式的专家混合适应方法，并将其应用于生成模块，以稀疏激活的方式恢复生成质量。", "result": "研究表明理解组件在理解和生成任务中具有高度的压缩性，这种压缩性在生成任务中尤为显著。与此相反，生成组件对压缩非常敏感，即使是适度的压缩比例也会导致性能急剧下降。", "conclusion": "通过使用稀疏激活方法，适应后的BAGEL模型在仅激活其大约一半的参数的情况下，实现了与全模型相当的性能。"}}
{"id": "2512.02556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02556", "abs": "https://arxiv.org/abs/2512.02556", "authors": ["DeepSeek-AI", "Aixin Liu", "Aoxue Mei", "Bangcai Lin", "Bing Xue", "Bingxuan Wang", "Bingzheng Xu", "Bochao Wu", "Bowei Zhang", "Chaofan Lin", "Chen Dong", "Chengda Lu", "Chenggang Zhao", "Chengqi Deng", "Chenhao Xu", "Chong Ruan", "Damai Dai", "Daya Guo", "Dejian Yang", "Deli Chen", "Erhang Li", "Fangqi Zhou", "Fangyun Lin", "Fucong Dai", "Guangbo Hao", "Guanting Chen", "Guowei Li", "H. Zhang", "Hanwei Xu", "Hao Li", "Haofen Liang", "Haoran Wei", "Haowei Zhang", "Haowen Luo", "Haozhe Ji", "Honghui Ding", "Hongxuan Tang", "Huanqi Cao", "Huazuo Gao", "Hui Qu", "Hui Zeng", "Jialiang Huang", "Jiashi Li", "Jiaxin Xu", "Jiewen Hu", "Jingchang Chen", "Jingting Xiang", "Jingyang Yuan", "Jingyuan Cheng", "Jinhua Zhu", "Jun Ran", "Junguang Jiang", "Junjie Qiu", "Junlong Li", "Junxiao Song", "Kai Dong", "Kaige Gao", "Kang Guan", "Kexin Huang", "Kexing Zhou", "Kezhao Huang", "Kuai Yu", "Lean Wang", "Lecong Zhang", "Lei Wang", "Liang Zhao", "Liangsheng Yin", "Lihua Guo", "Lingxiao Luo", "Linwang Ma", "Litong Wang", "Liyue Zhang", "M. S. Di", "M. Y Xu", "Mingchuan Zhang", "Minghua Zhang", "Minghui Tang", "Mingxu Zhou", "Panpan Huang", "Peixin Cong", "Peiyi Wang", "Qiancheng Wang", "Qihao Zhu", "Qingyang Li", "Qinyu Chen", "Qiushi Du", "Ruiling Xu", "Ruiqi Ge", "Ruisong Zhang", "Ruizhe Pan", "Runji Wang", "Runqiu Yin", "Runxin Xu", "Ruomeng Shen", "Ruoyu Zhang", "S. H. Liu", "Shanghao Lu", "Shangyan Zhou", "Shanhuang Chen", "Shaofei Cai", "Shaoyuan Chen", "Shengding Hu", "Shengyu Liu", "Shiqiang Hu", "Shirong Ma", "Shiyu Wang", "Shuiping Yu", "Shunfeng Zhou", "Shuting Pan", "Songyang Zhou", "Tao Ni", "Tao Yun", "Tian Pei", "Tian Ye", "Tianyuan Yue", "Wangding Zeng", "Wen Liu", "Wenfeng Liang", "Wenjie Pang", "Wenjing Luo", "Wenjun Gao", "Wentao Zhang", "Xi Gao", "Xiangwen Wang", "Xiao Bi", "Xiaodong Liu", "Xiaohan Wang", "Xiaokang Chen", "Xiaokang Zhang", "Xiaotao Nie", "Xin Cheng", "Xin Liu", "Xin Xie", "Xingchao Liu", "Xingkai Yu", "Xingyou Li", "Xinyu Yang", "Xinyuan Li", "Xu Chen", "Xuecheng Su", "Xuehai Pan", "Xuheng Lin", "Xuwei Fu", "Y. Q. Wang", "Yang Zhang", "Yanhong Xu", "Yanru Ma", "Yao Li", "Yao Li", "Yao Zhao", "Yaofeng Sun", "Yaohui Wang", "Yi Qian", "Yi Yu", "Yichao Zhang", "Yifan Ding", "Yifan Shi", "Yiliang Xiong", "Ying He", "Ying Zhou", "Yinmin Zhong", "Yishi Piao", "Yisong Wang", "Yixiao Chen", "Yixuan Tan", "Yixuan Wei", "Yiyang Ma", "Yiyuan Liu", "Yonglun Yang", "Yongqiang Guo", "Yongtong Wu", "Yu Wu", "Yuan Cheng", "Yuan Ou", "Yuanfan Xu", "Yuduan Wang", "Yue Gong", "Yuhan Wu", "Yuheng Zou", "Yukun Li", "Yunfan Xiong", "Yuxiang Luo", "Yuxiang You", "Yuxuan Liu", "Yuyang Zhou", "Z. F. Wu", "Z. Z. Ren", "Zehua Zhao", "Zehui Ren", "Zhangli Sha", "Zhe Fu", "Zhean Xu", "Zhenda Xie", "Zhengyan Zhang", "Zhewen Hao", "Zhibin Gou", "Zhicheng Ma", "Zhigang Yan", "Zhihong Shao", "Zhixian Huang", "Zhiyu Wu", "Zhuoshu Li", "Zhuping Zhang", "Zian Xu", "Zihao Wang", "Zihui Gu", "Zijia Zhu", "Zilin Li", "Zipeng Zhang", "Ziwei Xie", "Ziyi Gao", "Zizheng Pan", "Zongqing Yao", "Bei Feng", "Hui Li", "J. L. Cai", "Jiaqi Ni", "Lei Xu", "Meng Li", "Ning Tian", "R. J. Chen", "R. L. Jin", "S. S. Li", "Shuang Zhou", "Tianyu Sun", "X. Q. Li", "Xiangyue Jin", "Xiaojin Shen", "Xiaosha Chen", "Xinnan Song", "Xinyi Zhou", "Y. X. Zhu", "Yanping Huang", "Yaohui Li", "Yi Zheng", "Yuchen Zhu", "Yunxian Ma", "Zhen Huang", "Zhipeng Xu", "Zhongyu Zhang", "Dongjie Ji", "Jian Liang", "Jianzhong Guo", "Jin Chen", "Leyi Xia", "Miaojun Wang", "Mingming Li", "Peng Zhang", "Ruyi Chen", "Shangmian Sun", "Shaoqing Wu", "Shengfeng Ye", "T. Wang", "W. L. Xiao", "Wei An", "Xianzu Wang", "Xiaowen Sun", "Xiaoxiang Wang", "Ying Tang", "Yukun Zha", "Zekai Zhang", "Zhe Ju", "Zhen Zhang", "Zihua Qu"], "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "comment": null, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "AI": {"tldr": "DeepSeek-V3.2在保持高性能的同时提高了计算效率，并在一系列重要竞赛中展现了金牌水平的性能。", "motivation": "开发DeepSeek-V3.2旨在解决高性能模型的计算效率问题，同时提升推理和代理性能，尤其是在复杂的交互环境中。", "method": "引入了DeepSeek-V3.2模型，该模型在保持高性能的同时提高了计算效率。主要技术突破包括：(1)深度稀疏注意力机制(DSA)：减少了计算复杂性同时保持长上下文场景下的模型性能。(2)可扩展强化学习框架：通过实施强大的强化学习协议和扩展后训练计算，DeepSeek-V3.2的性能可以与GPT-5匹敌。(3)大规模代理任务合成管道：为了将推理整合进工具使用场景中，开发了一种系统性生成大量培训数据的新颖合成管道，有助于提高代理在复杂交互环境中的通用性和指令遵循可靠性。", "result": "高计算量变体DeepSeek-V3.2-Speciale超过了GPT-5，在2025国际数学奥林匹克(IMO)和国际信息学奥林匹克(IOI)中表现相当于Gemini-3.0-Pro，达到了金牌水平的性能。", "conclusion": "DeepSeek-V3.2及其高计算量变体展示了在计算效率和性能方面的显著提升，在复杂环境中表现出色。"}}
{"id": "2512.02359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.02359", "abs": "https://arxiv.org/abs/2512.02359", "authors": ["Bin Li", "Daijie Chen", "Qi Zhang"], "title": "WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting", "comment": "PRCV 2025", "summary": "Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
