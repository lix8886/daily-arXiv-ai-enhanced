<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration](https://arxiv.org/abs/2601.15296)
*Longxuan Wei,Yubo Zhang,Zijiao Zhang,Zhihu Wang,Shiwan Zhao,Tianyu Huang,Huiting Zhao,Chenfei Liu,Shenao Zhang,Junchi Yan*

Main category: cs.CL

> Entropy-Tree improves upon existing decoding strategies by using entropy as a guide for exploration, leading to better accuracy and reliability in reasoning tasks.

<details>
  <summary>Details</summary>

**Motivation:** To improve reasoning performance in large language models by avoiding blind or redundant exploration strategies.

**Method:** Entropy-Tree, a tree-based decoding method that uses entropy to guide branching decisions, expanding the tree only where the model shows uncertainty.

**Result:** Entropy-Tree outperforms Multi-chain in accuracy and calibration across multiple models and datasets and shows better AUROC in predictive entropy.

**Conclusion:** Entropy-Tree successfully integrates efficient structured exploration with reliable uncertainty estimation in a single procedure, leading to better reasoning performance.

**Abstract:** Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.

</details>


### [2] [AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports](https://arxiv.org/abs/2601.15297)
*Edward Ajayi*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.

</details>


### [3] [Embedding Retrofitting: Data Engineering for better RAG](https://arxiv.org/abs/2601.15298)
*Anantha Sharma*

Main category: cs.CL

> This paper explores the impact of data quality on word embedding retrofitting and proposes preprocessing methods to improve performance, emphasizing that preprocessing quality is more critical than the choice of retrofitting algorithm.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the issue of degraded data quality and its effects on the performance of word embedding retrofitting, especially in domain-specific retrieval tasks. It seeks to improve knowledge graph quality and, consequently, the effectiveness of retrofitting.

**Method:** The paper presents a data engineering framework that addresses issues of data quality in real-world corpora, specifically focusing on the impact of annotation artifacts like hashtags on the quality of knowledge graphs used in word embedding retrofitting.

**Result:** The analysis shows hashtag annotations lead to spurious edges in knowledge graphs, causing a decrease in retrofitting performance. Preprocessing techniques are found to significantly improve retrofitting performance with EWMA retrofitting achieving a 6.2% improvement.

**Conclusion:** The study concludes that preprocessing quality is the primary determinant of retrofitting success, as the impact of preprocessing on retrofitting performance exceeds the differences between retrofitting algorithms themselves.

**Abstract:** Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.
  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\%$ to $-5.2\%$, $p<0.05$). After preprocessing, \acrshort{ewma} retrofitting achieves $+6.2\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\%$ average). The gap between clean and noisy preprocessing (10\%+ swing) exceeds the gap between algorithms (3\%), establishing preprocessing quality as the primary determinant of retrofitting success.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

> 该论文探讨了在有限标注数据情况下改进排水系统中的涵洞和下水道管道自动缺陷分割的方法，提出了三种方案，分别是预处理策略、FORTRESS架构和少样本语义分割方法，有效提高了检测效果并减少了计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 涵洞和下水道是排水系统的重要组成部分，其失败会对公共安全和环境造成严重影响。由于数据收集和标注困难且耗时耗力，因此本论文致力于在数据稀缺的情况下优化缺陷分割技术。

**Method:** 论文提出了三种方法：1. 预处理策略如传统数据增强和动态标签注入。2. 提出FORTRESS架构，结合深度可分卷积、自适应KAN和多尺度注意力机制。3. 探索了少样本语义分割方法，使用双向原型网络和注意力机制。

**Result:** 通过测试表明，提出的预处理技术和FORTRESS架构能够有效改善Intersection over Union (IoU)和F1值，提升性能指标。同时，少样本学习达到了满意的评价指标结果。

**Conclusion:** 本论文的方法在有限标注数据条件下能够显著改善结构缺陷检测的分割性能，通过改进数据集或调整模型结构来应对数据稀缺问题。

**Abstract:** Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

</details>


### [5] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

> 本研究系统评估了最新的多模态大型语言模型（MLLMs）在异构人脸识别（HFR）中的性能，尤其是在不同传感模式下的表现，结果显示MLLMs在跨谱条件下存在显著性能差距，强调了其在人脸识别系统部署前需经过严格的生物识别评估的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 随着多模态大型语言模型在视觉语言任务中的出色表现，研究其在生物识别应用尤其是HFR中的潜力是本研究的动机。考虑到多传感模式下的复杂性，评估这些模型的实际能力显得尤为重要。

**Method:** 研究通过多个跨模态场景，如VIS-NIR、VIS-SWIR及VIS-THERMAL人脸识别，对多种开放源代码MLLMs进行基准测试，评估方法依据生物识别协议采用Acquire Rate、Equal Error Rate (EER)及True Accept Rate (TAR)等指标。

**Result:** 研究发现，即使经过近期的改进，MLLMs在HFR上的表现仍显著低于经典人脸识别系统，尤其是在具有挑战性的跨谱条件下。

**Conclusion:** 研究指出，当前MLLMs在HFR中的局限性，强调了在将其应用于人脸识别系统前，必须进行严格的生物识别评估以确保实际性能。

**Abstract:** Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [6] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

> 本文提出了CURE框架，可以无额外数据的情况下提高医学影像语言模型的定位准确性和报告质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态医学视觉语言模型在自动生成放射学报告时，存在视觉定位不准确和事实一致性问题。现有模型往往无法将文本表述与视觉证据对齐，产生不可靠或弱相关的预测结果。

**Method:** CURE采用了一种基于错误感知的课程学习框架，通过在短语定位、基于证据的报告生成和基于解剖学的报告生成任务上微调多模态指令模型来提高定位准确性和报告质量。该方法根据模型性能动态调整采样，重点处理较为困难的样本，以改善空间和文本上的对齐效果。

**Result:** CURE提升了定位精确度（IoU提高了0.37），报告质量（CXRFEScore提升了0.188），并且幻觉减少量达到了18.6%，是数据高效的方法，提升了定位准确性及报告可靠性。

**Conclusion:** CURE是提升多模态医学影像和文本结合报告生成质量的有效方法，它通过算法的优化和对样本的合理调整，实现了定位准确性和报告质量的提升。

**Abstract:** Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [7] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

> 本文提出了DuFal框架，通过High-Local Factorized Fourier Neural Operator和Cross-Attention Frequency Fusion模块，在稀疏锥束CT成像中有效恢复高频细节，比现有方法表现更好。

<details>
  <summary>Details</summary>

**Motivation:** 解决从有限的X射线投影中重建细粒度的解剖细节（对应于高频成分）这一问题，尤其是在稀疏锥束CT成像中，传统CNN方法难以恢复这些细小结构的问题。

**Method:** 介绍了一种名为DuFal的创新框架，该框架通过双路径架构将频域和空间域处理集成在一起。主要创新点是High-Local Factorized Fourier Neural Operator，它包含两个互补分支：一个用于捕捉全局频率模式的Global High-Frequency Enhanced Fourier Neural Operator和一个用于处理空间分割补丁并保持空间局部性的Local High-Frequency Enhanced Fourier Neural Operator。为了提高效率，设计了Spectral-Channel Factorization方案来减少参数数量，并设计了一个Cross-Attention Frequency Fusion模块来有效整合空间和频率特征。这些特征随后通过Feature Decoder解码产生投影表示，并通过Intensity Field Decoding pipeline重建最终的CT体积。

**Result:** 实验结果表明，在LUNA16和ToothFairy数据集上，特别是在极稀疏视角条件下，DuFal在保持高频解剖特征方面显著优于现有的最先进方法。

**Conclusion:** DuFal框架通过有效整合频域及空间域处理，提高了从有限角度X射线投影中重建高频解剖结构的能力。

**Abstract:** Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [8] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

> 本文提出了一种偏差引导的prompt学习框架，该框架结合了视觉-语言模型中的语义能力和基于偏差的评分机制，提高了基于少量正常样本的异常检测任务中的patch级别异常检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的FNSAD方法在辨别正常和异常提示方面表现较弱，并且缺乏针对patch级别的异常评分机制。

**Method:** Few-normal shot anomaly detection (FNSAD)通过结合视觉-语言模型中的语义能力和基于偏差的评分机制来改进异常检测。具体来说，方法使用可学习的上下文向量替换固定的prompt前缀，并为异常特定的后缀标记以实现类别感知的对齐。此外，通过Top-K多实例学习（MIL）中的偏差损失，采用Gaussian偏差模型来增强可分离性。

**Result:** 在MVTecAD和VISA基准测试中，该方法比PromptAD和其他基线表现更优，特别是在像素级检测性能上。消融研究进一步验证了可学习prompt，基于偏差的评分以及Top-K MIL策略的有效性。

**Conclusion:** 提出的方法提升了FNSAD任务中的区域定位和解释性。

**Abstract:** Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [9] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

> 本文提出了一种新的NeRF框架，用于从模糊的LDR图像和事件数据中恢复清晰的HDR 3D视图，解决了现有方法无法很好地处理极端光照条件下HDR和去模糊的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在处理模糊的LDR图像时，忽视了传感器物理特性与真实世界辐射之间的差异，导致HDR和去模糊效果不佳。因此，我们提出了一个新的框架，用于生成清晰的HDR视图。

**Method:** 我们提出了一种统一的基于传感器物理学的NeRF框架，用于从单一曝光的模糊LDR图像和相应的事件中进行清晰的HDR视图合成。使用NeRF直接表示3D场景的实际辐射，并将原始HDR场景光线模拟为物理世界中的传感器像素。引入了像素级RGB映射场，以对齐渲染像素值与输入图像的传感器记录的LDR像素值，同时设计了一种新的事件映射场，以弥合物理场景动态与实际事件传感器输出之间的差距。这两种映射场与NeRF网络一起被优化，利用事件中的空间和时间动态信息来增强清晰的HDR 3D表示学习。

**Result:** 实验结果表明，我们的方法在使用单曝光模糊LDR图像和事件数据时，能够实现最先进的去模糊HDR新型视图合成。

**Conclusion:** 该研究通过引入新的方法和优化策略，提高了从模糊LDR图像和事件数据中合成清晰HDR视图的效果，展现了其在极端光照条件下恢复高质量3D场景潜力。

**Abstract:** Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [10] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

> 通过在属性中立框架中用Vision Transformer替换U-Net，研究显示可以减少属性泄漏，同时保持胸部X光AI的诊断效果，为更公平的AI诊断提供了一条实际途径。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在评估使用基于Vision Transformer的编辑器能否在不影响诊断准确性的情况下减少性别和年龄相关的属性泄漏，从而解决胸部X光图像分类中的偏见问题。

**Method:** 研究采用数据高效的图像转换器小模型（DeiT-S）作为属性中立框架中的编辑器，代替原先的U-Net卷积编码器。该转换器模型在ChestX-ray14数据集上训练，并通过独立的人工智能模型和卷积神经网络对该模型编辑后的图像进行评估，检查性别识别率和疾病预测准确率。

**Result:** 实验结果显示，在中等编辑水平（alpha=0.5）下，基于Vision Transformer（ViT）的中立器减少了性别的识别率到约0.80，比原始框架的U-Net编码器减少了大约10个百分点。同时，它保持了疾病预测的准确性，主要接收者操作特性曲线下面积（ROC AUC）变化不超过5个百分点，并且最差的亚群AUC仍然接近0.70。

**Conclusion:** 研究证实Vision Transformer可以更有效地减少胸部X光图像分类中的性别和年龄相关的属性泄漏，且不影响诊断准确性，这为实现更加公平的AI诊断提供了一种有效的方法。

**Abstract:** Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

</details>
