<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.CV](#cs.CV) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

> 本文提出了一种新的针对大表格数据的概率问题回答框架，通过贝叶斯网络和大型语言模型提升了在处理不确定性推理问题上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前的问题回答方法，如NL2SQL系统，在回答可以直接从表格中检索到答案的事实性问题时表现出色，但在需要在不确定性情况下推理的概率性问题上表现较差。为此，本文引入了新的基准LUCARIO和一个针对大表格数据的概率问题回答框架。

**Method:** 我们的方法从表格中诱导出贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型生成最终答案。

**Result:** 实验结果显示，本文的方法相较基线方法有显著提升，突出了混合符号-神经推理的优势。

**Conclusion:** 研究证明了混合符号和神经方法在处理不确定性的概率问题回答任务中具备的优势。

**Abstract:** Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

> 本文介绍了一种新的多语言功能基准测试方法，通过创建CL-GSM Symbolic和CL-IFEval基准测试来评估大型语言模型在多种语言设置下的实用性和稳定性。

<details>
  <summary>Details</summary>

**Motivation:** 由于静态数据基准测试往往无法充分评估大型语言模型在多语言环境中的实际表现和鲁棒性，因此提出了一种新的多语言功能基准测试方法。

**Method:** 创建了跨语言功能基准测试 -- CL-GSM 符号和 CL-IFEval，通过将现有功能基准模板从英语翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语五种语言来实现。

**Result:** 研究结果显示，在某些静态多语言基准测试中捕捉到的功能性能更接近实际情况（例如，M-GSM和CL-GSM符号中，在英语、法语和西班牙语中，模型性能分别下降了24%、17%和18%；在Belebele和CL-IFEval之间，性能下降了15%到24%；而M-MMLU和CL-IFEval之间的性能下降仅为0.5%到3%）。同时发现，模型在不同语言中的健壮性显著不同，某些语言（例如阿拉伯语、英语）在评估迭代中表现最为一致。

**Conclusion:** 跨语言功能基准测试方法能够更好地评估大型语言模型在多语言环境中的性能和鲁棒性，相比静态多语言基准测试提供了更有价值的见解。

**Abstract:** Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

> 该研究通过执行由专家或大语言模型生成的研究想法来测试AI生成的研究想法的质量，发现虽然某些情况下AI想法在构思阶段看起来更加新颖，但执行后的评价得分更低，人类想法在许多指标上得分更高，揭示了AI生成想法在实际应用中的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是测试由AI生成的想法是否能够带来更好的研究成果，因为仅仅在构思阶段新颖未必就意味着在执行后会带来更好的研究成果。

**Method:** 研究通过招募43位专家研究人员来执行随机分配的想法（这些想法是由专家或大语言模型生成的）来测试AI生成的想法是否能够带来更好的研究成果。每个专家花了超过100小时来实现想法并编写了一份4页的短篇文档来记录实验结果。所有执行的项目随后由专家NLP研究人员进行匿名评审。

**Result:** 执行研究后，评估分数显示，大语言模型生成的想法在所有评价指标（新颖性、兴奋度、有效性、总体评价）上的得分显著下降，而且与人类想法在构思阶段观察到的差距有所缩小。对于许多评估标准，甚至出现人类想法的得分高于大语言模型想法的排名逆转现象。

**Conclusion:** 研究结果显示，现有的大语言模型在生成真正有效研究想法方面存在局限性，并且在没有执行结果的情况下评估研究想法存在挑战。

**Abstract:** Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

> 研究者开发了MultiFinRAG系统，解决金融文档多模态问答问题，其专用框架优于ChatGPT-4o (免费版)。

<details>
  <summary>Details</summary>

**Motivation:** 由于金融文档内容通常很复杂，结合了叙述性文本、表格、图表等，跨这些模态进行联合推理挑战了传统的大型语言模型和增强检索的生成框架。这些传统模型在处理多模态内容时面临标记限制、布局损失和碎片化的跨模态上下文等问题。因此，研究者引入MultiFinRAG来解决这些问题。

**Method:** MultiFinRAG采用了一种增强检索的生成框架，专为金融问答设计。该方法首先通过分批处理表格和图像，利用轻量级、量化开源多模态语言模型生成结构化JSON输出和简洁的文本摘要。然后，将这些输出、叙述性文本与模态感知的相似度阈值嵌入并索引，以实现精确检索。接着，采用分层回退策略，根据需要从文本扩展到文本、表格和图像的上下文，从而在减少无关上下文的同时实现跨模态推理。

**Result:** 尽管是在通用硬件上运行的，但MultiFinRAG在需要文本、表格、图像以及多模态推理的复杂金融问答任务上，比ChatGPT-4o (免费版)准确率高出19个百分点。

**Conclusion:** 该研究开发了MultiFinRAG框架来解决金融文档的跨模态问答问题，通过嵌入和索引多模态内容，实现高效精准的检索，并通过层级回退策略降低无关上下文，从而取得更高的准确率，表现优于传统的大模型。

**Abstract:** Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

> 研究采用VBVQ评估了来自不同背景的六种大型语言模型对日常冲突的暴力反应，发现它们倾向于暴力反应的内在偏好与其生成文本并不一致，且这种倾向在不同人群中有所差异，与已有社会科学研究结果相左。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型被越来越多地用于检测和回应网络上的暴力内容，然而，这些模型在处理现实生活中的道德模糊情境的能力尚未得到充分研究。

**Method:** 本研究采用了一种验证过的社会科学工具——暴力行为小品问卷（VBVQ），用于评估六个来自不同地缘政治和组织背景的大型语言模型（LLMs）对暴力内容的检测与反应能力。研究通过引入基于人物身份的提示词，变异种族、年龄和地理身份来评估潜在偏见。

**Result:** 研究揭示了两个关键发现：(1) LLMs生成的表面文本往往与其倾向于暴力反应的内在偏好相悖；(2) 面对暴力情况，这些模型表现出的倾向性在不同人群中有所不同，且经常与犯罪学、社会科学和心理学中的既有发现相悖。

**Conclusion:** 这项研究强调了在使用大型语言模型评估和响应暴力行为时需要谨慎，并指出这些模型在不同人群中的表现可能存在与现有社会科学研究相悖的趋势，需要进一步研究。

**Abstract:** Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

> 该研究探讨了临床专家如何验证来自社交媒体的医学主张，揭示医学领域中端到端事实核查的根本挑战，并建议将事实核查视为一种交互通信问题而非端到端的过程来处理。

<details>
  <summary>Details</summary>

**Motivation:** 由于医学决策的重要性以及批判性评估大量且多样化的医学文献的挑战，对于采用这些系统的兴趣日益增长。然而，证据基础医学的高度技术性使得大多数用户的医学素养不足以充分应对。这些问题为端到端事实核查代理系统提供了成长空间，但这类系统依然未被广泛使用。本研究旨在探讨医学领域中端到端事实核查的根本挑战。

**Method:** 通过研究临床专家如何验证来自社交媒体的真实主张并合成医学证据，本研究寻求定义一个医学端到端事实核查的上限。

**Result:** <tool_call>

**Conclusion:** 本研究揭示了端到端事实核查应用于医学时所面临的根本挑战，并提出应该将事实核查视为一种交互式的通信问题，而不是单纯的端到端过程。

**Abstract:** Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

> The thesis focuses on improving the adaptability of language models to downstream NLP tasks through innovative training and fine-tuning methods, and improved evaluation metrics.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to develop strategies to leverage unlabelled data for task-relevant knowledge extraction, lower the computational requirements for fine-tuning large language models, and improve the generalization ability of these models to a wider spectrum of real-world NLP tasks.

**Method:** Our paper introduces several methods to improve the adaptation of language models to different NLP tasks. These include a new continued pre-training method that utilizes unlabelled data more effectively, a parameter-efficient fine-tuning technique that reduces resource consumption, and enhanced supervised fine-tuning that boosts performance in following instructions and handling open-ended tasks. Additionally, we propose new evaluation benchmarks to comprehensively assess model capabilities.

**Result:** Empirical studies across various NLP tasks show that the proposed methods enhance the robustness, efficiency, and generalization of language models, making them better suited for diverse applications.

**Conclusion:** The adopted methods are significant improvements towards achieving more robust, efficient, and generalized language models, pushing the field closer to realizing artificial general intelligence.

**Abstract:** Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

> 我们的工作介绍了一种可自动适应任何语言的多语言预训练数据集策划流程，并展示了通过该流程创建的数据集能够训练出比现有数据集更好的模型。

<details>
  <summary>Details</summary>

**Motivation:** 尽管高质量的大规模英语预训练数据集的公开开发取得了重大进展，但训练性能优异的多语言预训练模型仍然是一项挑战，主要是因为很难将过滤和重复数据删除流程定制为适用于大量语言。

**Method:** 我们介绍了一种基于FineWeb的新预训练数据集策划流程，该流程可以自动适应任何语言。我们在九种多样化语言的设置下，通过一系列有意义且具有信息量的评估任务指导，对我们的流程设计选择进行了广泛的消融研究。

**Result:** 我们的流程可用于创建非英语语料库，与之前的语料库相比，可以产生性能更优异的模型。我们还通过一种简单且合理的方法来重新平衡数据集，考虑重复次数和质量，提供了额外的性能提升。我们最终将流程扩展到超过1000种语言，使用了近100个Common Crawl快照，创建了FineWeb2，这是一个新的20 TB（50亿文档）多语言数据集。

**Conclusion:** 本研究提出了一个广泛适用于超过1000种语言的预训练数据集生成流程，而且我们还发布了FineWeb2数据集，包含50亿份文档，为多语言大模型的训练提供了宝贵的资源。

**Abstract:** Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

> KaLM-Embedding-V2 is introduced, a compact embedding model that achieves high performance across a variety of tasks through architectural modifications and advanced training methods, outperforming similar models and competing with much larger models.

<details>
  <summary>Details</summary>

**Motivation:** To create a compact yet versatile embedding model that surpasses existing models in performance and generalization across various text embedding tasks.

**Method:** The paper introduces KaLM-Embedding-V2, which modifies the architecture to fully bidirectional transformer with mean-pooling. It uses a multi-stage training pipeline including pre-training on large, weakly supervised data, fine-tuning on retrieval and non-retrieval datasets, and parameter averaging. Additionally, it introduces a focal-style reweighting and online hard-negative mixing.

**Result:** The model shows superior performance on the MTEB for both Chinese and English, outperforming similar-sized models and competing with much larger models with up to 26x more parameters.

**Conclusion:** KaLM-Embedding-V2 sets a new standard for compact embedding models by leveraging innovative architectural adjustments, training techniques, and comprehensive data categories for pre-training and fine-tuning, achieving state-of-the-art results on the MTEB with less than 1 billion parameters.

**Abstract:** In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [10] [OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport](https://arxiv.org/abs/2506.20741)
*Qin Ren,Yifan Wang,Ruogu Fang,Haibin Ling,Chenyu You*

Main category: cs.CV

> OTSurv, an innovative survival prediction tool for whole slide images, uses optimal transport with specific constraints to overcome the limitations of prior methods, achieving better results.

<details>
  <summary>Details</summary>

**Motivation:** Existing MIL methods often fail to adequately model the heterogeneity within WSIs, both globally and locally, which OTSurv aims to address.

**Method:** OTSurv is a novel multiple instance learning (MIL) framework that uses optimal transport (OT) to predict survival using whole slide images (WSIs). It includes a global constraint to model prior morphological distributions and avoid mode collapse or excessive uniformity, and a local constraint that enhances high-confidence patches while suppressing noise.

**Result:** OTSurv outperformed existing methods with a significant 3.6% improvement in average C-index across six popular benchmarks. It also passed log-rank tests and provided high interpretability.

**Conclusion:** OTSurv proves superior in survival prediction for digital pathology due to its ability to effectively model the heterogeneity within WSIs, setting a new standard in the field.

**Abstract:** Survival prediction using whole slide images (WSIs) can be formulated as a
multiple instance learning (MIL) problem. However, existing MIL methods often
fail to explicitly capture pathological heterogeneity within WSIs, both
globally -- through long-tailed morphological distributions, and locally
through -- tile-level prediction uncertainty. Optimal transport (OT) provides a
principled way of modeling such heterogeneity by incorporating marginal
distribution constraints. Building on this insight, we propose OTSurv, a novel
MIL framework from an optimal transport perspective. Specifically, OTSurv
formulates survival predictions as a heterogeneity-aware OT problem with two
constraints: (1) global long-tail constraint that models prior morphological
distributions to avert both mode collapse and excessive uniformity by
regulating transport mass allocation, and (2) local uncertainty-aware
constraint that prioritizes high-confidence patches while suppressing noise by
progressively raising the total transport mass. We then recast the initial OT
problem, augmented by these constraints, into an unbalanced OT formulation that
can be solved with an efficient, hardware-friendly matrix scaling algorithm.
Empirically, OTSurv sets new state-of-the-art results across six popular
benchmarks, achieving an absolute 3.6% improvement in average C-index. In
addition, OTSurv achieves statistical significance in log-rank tests and offers
high interpretability, making it a powerful tool for survival prediction in
digital pathology. Our codes are available at
https://github.com/Y-Research-SBU/OTSurv.

</details>


### [11] [StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation](https://arxiv.org/abs/2506.20756)
*Haodong Li,Chen Wang,Jiahui Lei,Kostas Daniilidis,Lingjie Liu*

Main category: cs.CV

> 论文提出了一种新的视频深度估计方法StereoDiff，该方法结合立体匹配和视频深度扩散技术，以满足视频中静态和动态部分的独特要求，实验结果显示了其优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 论文指出视频深度估计并非图像深度估计的简单延伸，静态与动态区域在视频中对时间一致性有不同的要求。因此提出了一种新的方法来弥补这一差距。

**Method:** StereoDiff, 一种两阶段的视频深度估计器，结合了主要用于静态区域的立体匹配与用于维持动态区域深度连续性的视频深度扩散方法。

**Result:** 通过零样本实验，展示了StereoDiff在室内和室外动态视频深度基准上的优异表现。

**Conclusion:** 实验结果显示，StereoDiff在零样本、现实世界的动态视频深度数据集上取得了SOTA性能，证明了其在视频深度估计中的一致性和准确性方面的优势。

**Abstract:** Recent video depth estimation methods achieve great performance by following
the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained
video diffusion models with massive data. However, we argue that video depth
estimation is not a naive extension of image depth estimation. The temporal
consistency requirements for dynamic and static regions in videos are
fundamentally different. Consistent video depth in static regions, typically
backgrounds, can be more effectively achieved via stereo matching across all
frames, which provides much stronger global 3D cues. While the consistency for
dynamic regions still should be learned from large-scale video depth data to
ensure smooth transitions, due to the violation of triangulation constraints.
Based on these insights, we introduce StereoDiff, a two-stage video depth
estimator that synergizes stereo matching for mainly the static areas with
video depth diffusion for maintaining consistent depth transitions in dynamic
areas. We mathematically demonstrate how stereo matching and video depth
diffusion offer complementary strengths through frequency domain analysis,
highlighting the effectiveness of their synergy in capturing the advantages of
both. Experimental results on zero-shot, real-world, dynamic video depth
benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,
showcasing its superior consistency and accuracy in video depth estimation.

</details>


### [12] [ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations](https://arxiv.org/abs/2506.20757)
*Zhiyuan Wu,Yongqiang Zhao,Shan Luo*

Main category: cs.CV

> The paper introduces ConViTac, a new network that uses a Contrastive Embedding Conditioning mechanism to better align and integrate visual and tactile information, showing up to 12% improvement in accuracy for material classification and grasping prediction tasks compared to state-of-the-art methods.

<details>
  <summary>Details</summary>

**Motivation:** The primary motivation is to address the limitations of previous joint learning approaches of visual-tactile representations, which tend to result in poor feature integration due to direct methods of modality fusion.

**Method:** Our key contribution is a Contrastive Embedding Conditioning (CEC) mechanism that leverages a contrastive encoder pretrained through self-supervised contrastive learning to project visual and tactile inputs into unified latent embeddings. These embeddings are used to couple visual-tactile feature fusion through cross-modal attention, aiming at aligning the unified representations and enhancing performance on downstream tasks.

**Result:** We conduct extensive experiments to demonstrate the superiority of ConViTac in real world over current state-of-the-art methods and the effectiveness of our proposed CEC mechanism, which improves accuracy by up to 12.0% in material classification and grasping prediction tasks.

**Conclusion:** The paper concludes by presenting the ConViTac network with its CEC mechanism, which enhances the alignment of features during the fusion of visual and tactile inputs, leading to improved performance across various tasks compared to existing methods.

**Abstract:** Vision and touch are two fundamental sensory modalities for robots, offering
complementary information that enhances perception and manipulation tasks.
Previous research has attempted to jointly learn visual-tactile representations
to extract more meaningful information. However, these approaches often rely on
direct combination, such as feature addition and concatenation, for modality
fusion, which tend to result in poor feature integration. In this paper, we
propose ConViTac, a visual-tactile representation learning network designed to
enhance the alignment of features during fusion using contrastive
representations. Our key contribution is a Contrastive Embedding Conditioning
(CEC) mechanism that leverages a contrastive encoder pretrained through
self-supervised contrastive learning to project visual and tactile inputs into
unified latent embeddings. These embeddings are used to couple visual-tactile
feature fusion through cross-modal attention, aiming at aligning the unified
representations and enhancing performance on downstream tasks. We conduct
extensive experiments to demonstrate the superiority of ConViTac in real world
over current state-of-the-art methods and the effectiveness of our proposed CEC
mechanism, which improves accuracy by up to 12.0% in material classification
and grasping prediction tasks.

</details>


### [13] [AI-Driven MRI-based Brain Tumour Segmentation Benchmarking](https://arxiv.org/abs/2506.20786)
*Connor Ludwig,Khashayar Namdar,Farzad Khalvati*

Main category: cs.CV

> 本研究比较了不同模型在BraTS 2023数据集上的分割性能，发现SAM和SAM 2在极高准确度边界框提示下超越nnU-Net，但考虑到实际应用中提示准确度的限制，nnU-Net仍是主导模型。儿科数据集上的模型微调显示点提示性能有所改善。

<details>
  <summary>Details</summary>

**Motivation:** 当前缺乏对现有的一般提示模型及医学变体模型在公共医学数据集上进行的广泛提示质量的评估和比较。研究旨在填补这一空白，并进一步通过模型微调来扩展此评价。

**Method:** 该研究使用Segment Anything Model (SAM), Segment Anything Model 2 (SAM 2), MedSAM, SAM-Med-3D 和 nnU-Net，在BraTS 2023成人神经胶质瘤和儿科数据集上对多种提示质量（包括点提示和边界框提示）进行零样本推理进行评估和比较。

**Result:** 研究发现SAM和SAM 2在极高准确度边界框提示条件下表现出色，Dice分数最高达到0.894和0.893，分别超过nnU-Net的分割性能。然而，由于提供极高准确度提示的不可行性，nnU-Net依然占据了医学图像分割的主导地位。通过儿科数据集上的模型微调，点提示性能有所提升，显示出未来研究的潜力，但未能超越边界框或nnU-Net的分割性能。

**Conclusion:** 尽管SAM系列模型在高准确度提示下表现出卓越的分割性能，但鉴于实际应用中提示准确度的限制，nnU-Net依然保持其作为医学图像分割模型的优势地位。未来可以通过优化提示策略和继续微调模型来探索进一步改进。

**Abstract:** Medical image segmentation has greatly aided medical diagnosis, with U-Net
based architectures and nnU-Net providing state-of-the-art performance. There
have been numerous general promptable models and medical variations introduced
in recent years, but there is currently a lack of evaluation and comparison of
these models across a variety of prompt qualities on a common medical dataset.
This research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM
2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS
2023 adult glioma and pediatrics dataset across multiple prompt qualities for
both points and bounding boxes. Several of these models exhibit promising Dice
scores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,
respectively when given extremely accurate bounding box prompts which exceeds
nnU-Net's segmentation performance. However, nnU-Net remains the dominant
medical image segmentation network due to the impracticality of providing
highly accurate prompts to the models. The model and prompt evaluation, as well
as the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and
SAM-Med-3D on the pediatrics dataset. The improvements in point prompt
performance after fine-tuning are substantial and show promise for future
investigation, but are unable to achieve better segmentation than bounding
boxes or nnU-Net.

</details>


### [14] [How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?](https://arxiv.org/abs/2506.20795)
*Stephanie Käs,Anton Burenko,Louis Markert,Onur Alp Culha,Dennis Mack,Timm Linder,Bastian Leibe*

Main category: cs.CV

> 研究比较了不同模型在动态全身手势识别的效果，发现HD-GCN性能最佳，V-JEPA接近，提出Gemini在手势识别方面需要改进。

<details>
  <summary>Details</summary>

**Motivation:** 传统基于深度学习的手势识别依赖特定任务的架构，视觉基础模型（VFMs）和视觉语言模型（VLMs）凭借强大的泛化能力，可能减少系统的复杂性，取代特定的任务模块。

**Method:** 本研究比较了V-JEPA（一种先进视觉基础模型）、Gemini Flash 2.0（一种多模式视觉语言模型）和HD-GCN（一种性能领先的基于骨架的方法）在动态全身手势识别中的效果。

**Result:** 实验结果显示，HD-GCN实现了最佳性能，而V-JEPA通过简单的任务特定分类头取得了接近的性能，表明其作为多任务模型可以减少系统复杂性。Gemini在仅依靠文本描述识别手势的零样本设置下表现不佳。

**Conclusion:** 该研究促进了探索适用的手势识别输入表示的进一步研究。

**Abstract:** Gestures enable non-verbal human-robot communication, especially in noisy
environments like agile production. Traditional deep learning-based gesture
recognition relies on task-specific architectures using images, videos, or
skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)
and Vision Language Models (VLMs) with their strong generalization abilities
offer potential to reduce system complexity by replacing dedicated
task-specific modules. This study investigates adapting such models for
dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art
VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing
skeleton-based approach). We introduce NUGGET, a dataset tailored for
human-robot communication in intralogistics environments, to evaluate the
different gesture recognition approaches. In our experiments, HD-GCN achieves
best performance, but V-JEPA comes close with a simple, task-specific
classification head - thus paving a possible way towards reducing system
complexity, by using it as a shared multi-task model. In contrast, Gemini
struggles to differentiate gestures based solely on textual descriptions in the
zero-shot setting, highlighting the need of further research on suitable input
representations for gestures.

</details>


### [15] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

> 本论文提出了一种利用视觉语言模型评估和选择扩散模型生成的超分辨率图像中最值得信赖的图像的方法，从而提高信息的清晰度和可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 解决扩散模型生成的超分辨率图像集中选择最可靠解决方案的挑战，同时避免在重要应用中出现信息模糊问题。

**Method:** 通过利用视觉语言模型（VLM）的语义推理能力来识别从扩散模型生成的超分辨率图像集合中最值得信任的图像。具体来说，使用BLIP-2和GPT-4o等VLM通过结构化的查询来评估语义正确性、视觉质量和伪影存在。

**Result:** 提出了一种新的可信度分数（TWS），该分数基于语义相似度、结构完整性和伪影敏感性三个互补成分，能够与人类的选择偏好保持高度相关，并确保VLM指导选择的一致高质量。

**Conclusion:** 与传统的PSNR和LPIPS度量相比，本文的方法提供了一种更为合理、可扩展且通用的解决方案，为生成式的超分辨率图像的可靠性设定了新基准。

**Abstract:** Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.

</details>
