{"id": "2508.14929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14929", "abs": "https://arxiv.org/abs/2508.14929", "authors": ["Chiao-An Yang", "Raymond A. Yeh"], "title": "Heatmap Regression without Soft-Argmax for Facial Landmark Detection", "comment": null, "summary": "Facial landmark detection is an important task in computer vision with\nnumerous applications, such as head pose estimation, expression analysis, face\nswapping, etc. Heatmap regression-based methods have been widely used to\nachieve state-of-the-art results in this task. These methods involve computing\nthe argmax over the heatmaps to predict a landmark. Since argmax is not\ndifferentiable, these methods use a differentiable approximation, Soft-argmax,\nto enable end-to-end training on deep-nets. In this work, we revisit this\nlong-standing choice of using Soft-argmax and demonstrate that it is not the\nonly way to achieve strong performance. Instead, we propose an alternative\ntraining objective based on the classic structured prediction framework.\nEmpirically, our method achieves state-of-the-art performance on three facial\nlandmark benchmarks (WFLW, COFW, and 300W), converging 2.2x faster during\ntraining while maintaining better/competitive accuracy. Our code is available\nhere: https://github.com/ca-joe-yang/regression-without-softarg.", "AI": {"tldr": "研究提出了一种基于结构化预测框架的新方法，而不是使用Soft-argmax方法，实现了更快的训练和更准确的面部关键点检测。", "motivation": "重新审视了长期使用的Soft-argmax方法，并表明这不是唯一能够实现高性能的方法。", "method": "提出了一种基于经典结构化预测框架的替代训练目标方法，而不是传统的Soft-argmax方法。", "result": "实验结果表明，该方法在三个面部关键点基准数据集（WFLW、COFW和300W）上达到了state-of-the-art性能，训练速度提高了2.2倍，同时还保持了更好的准确率。", "conclusion": "研究表明，所提出的方法不仅在面部关键点检测任务上达到了出色的性能，同时还显著提升了训练的效率。"}}
{"id": "2508.14958", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14958", "abs": "https://arxiv.org/abs/2508.14958", "authors": ["Mustafa Mohammadi Gharasuie", "Luis Rueda"], "title": "Fast Graph Neural Network for Image Classification", "comment": "12 pages, proceeding into CanadianAI 2025", "summary": "The rapid progress in image classification has been largely driven by the\nadoption of Graph Convolutional Networks (GCNs), which offer a robust framework\nfor handling complex data structures. This study introduces a novel approach\nthat integrates GCNs with Voronoi diagrams to enhance image classification by\nleveraging their ability to effectively model relational data. Unlike\nconventional convolutional neural networks (CNNs), our method represents images\nas graphs, where pixels or regions function as vertices. These graphs are then\nrefined using corresponding Delaunay triangulations, optimizing their\nrepresentation. The proposed model achieves significant improvements in both\npreprocessing efficiency and classification accuracy across various benchmark\ndatasets, surpassing state-of-the-art approaches, particularly in challenging\nscenarios involving intricate scenes and fine-grained categories. Experimental\nresults, validated through cross-validation, underscore the effectiveness of\ncombining GCNs with Voronoi diagrams for advancing image classification. This\nresearch not only presents a novel perspective on image classification but also\nexpands the potential applications of graph-based learning paradigms in\ncomputer vision and unstructured data analysis.", "AI": {"tldr": "This paper integrates Graph Convolutional Networks (GCNs) with Voronoi diagrams to improve image classification efficiency and accuracy, demonstrating superior performance over existing methods.", "motivation": "The motivation behind this research is to improve the efficiency and accuracy of image classification by leveraging the strengths of GCNs and Voronoi diagrams in modeling complex relational data structures.", "method": "This study proposes a novel approach that integrates Graph Convolutional Networks (GCNs) with Voronoi diagrams for image classification. The method represents images as graphs with pixels or regions as vertices, and optimizes these graphs using Delaunay triangulations.", "result": "The proposed model achieves notable improvements in preprocessing efficiency and classification accuracy, surpassing current state-of-the-art methods, especially in complex and fine-grained classification tasks.", "conclusion": "The research emphasizes the importance of the new approach in advancing image classification and expands the applications of graph-based learning in computer vision and the analysis of unstructured data."}}
{"id": "2508.14965", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14965", "abs": "https://arxiv.org/abs/2508.14965", "authors": ["Hakjin Lee", "Junghoon Seo", "Jaehoon Sim"], "title": "You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation", "comment": "https://mikigom.github.io/YOPO-project-page", "summary": "Accurately recovering the full 9-DoF pose of unseen instances within specific\ncategories from a single RGB image remains a core challenge for robotics and\nautomation. Most existing solutions still rely on pseudo-depth, CAD models, or\nmulti-stage cascades that separate 2D detection from pose estimation. Motivated\nby the need for a simpler, RGB-only alternative that learns directly at the\ncategory level, we revisit a longstanding question: Can object detection and\n9-DoF pose estimation be unified with high performance, without any additional\ndata? We show that they can with our method, YOPO, a single-stage, query-based\nframework that treats category-level 9-DoF estimation as a natural extension of\n2D detection. YOPO augments a transformer detector with a lightweight pose\nhead, a bounding-box-conditioned translation module, and a 6D-aware Hungarian\nmatching cost. The model is trained end-to-end only with RGB images and\ncategory-level pose labels. Despite its minimalist design, YOPO sets a new\nstate of the art on three benchmarks. On the REAL275 dataset, it achieves 79.6%\n$\\rm{IoU}_{50}$ and 54.1% under the $10^\\circ$$10{\\rm{cm}}$ metric, surpassing\nprior RGB-only methods and closing much of the gap to RGB-D systems. The code,\nmodels, and additional qualitative results can be found on our project.", "AI": {"tldr": "提出了YOPO方法，即统一对象检测和9自由度姿态估计的单阶段、基于查询的框架，仅基于RGB图像而不需要其他数据。YOPO在多个基准测试中表现出色。", "motivation": "动机是提出一种更简单、仅依赖RGB图像的方法来解决在特定类别下从单一RGB图像恢复未见实例完整9自由度姿态的核心挑战。", "method": "提出了YOPO方法，这是一种单阶段的基于查询的框架，将9-DoF姿态估计视为2D检测的自然扩展。YOPO通过增加一个轻量级的姿态头、一个基于边界框的条件翻译模块和一个6D感知的匈牙利匹配成本来增强变压器检测器。该模型仅使用RGB图像和类别级别的姿态标签进行端到端的训练。", "result": "尽管设计简约，YOPO在三个基准测试中创造了新的记录。在REAL275数据集上，它达到了79.6%的$\rm{IoU}_{50}$和54.1%的$10^\text{o}$10cm度量标准，超越了以前仅使用RGB方法的性能。", "conclusion": "研究结论是，YOPO提供了一种有效的方法，通过仅依赖于RGB图像和类别级别的姿态标签来实现高效的物体检测和9自由度姿态估计的集成。这种方法弥补了之前仅使用RGB图像方法的不足，并且几乎达到了RGB-D系统的性能。"}}
{"id": "2508.14980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.14980", "abs": "https://arxiv.org/abs/2508.14980", "authors": ["Andrei Balykin", "Anvar Ganiev", "Denis Kondranin", "Kirill Polevoda", "Nikolai Liudkevich", "Artem Petrov"], "title": "Paired-Sampling Contrastive Framework for Joint Physical-Digital Face Attack Detection", "comment": "Accepted to ICCV2025 FAS workshop", "summary": "Modern face recognition systems remain vulnerable to spoofing attempts,\nincluding both physical presentation attacks and digital forgeries.\nTraditionally, these two attack vectors have been handled by separate models,\neach targeting its own artifacts and modalities. However, maintaining distinct\ndetectors increases system complexity and inference latency and leaves systems\nexposed to combined attack vectors. We propose the Paired-Sampling Contrastive\nFramework, a unified training approach that leverages automatically matched\npairs of genuine and attack selfies to learn modality-agnostic liveness cues.\nEvaluated on the 6th Face Anti-Spoofing Challenge Unified Physical-Digital\nAttack Detection benchmark, our method achieves an average classification error\nrate (ACER) of 2.10 percent, outperforming prior solutions. The framework is\nlightweight (4.46 GFLOPs) and trains in under one hour, making it practical for\nreal-world deployment. Code and pretrained models are available at\nhttps://github.com/xPONYx/iccv2025_deepfake_challenge.", "AI": {"tldr": "本文提出了一个统一的训练框架，名为Paired-Sampling Contrastive Framework，用于识别面部伪造攻击，这种方法在第6届面部反欺骗挑战赛中表现优异，具有较低的复杂性和训练时间，适合实际部署。", "motivation": "目前的面部识别系统在面对伪造攻击时仍然存在脆弱性，传统的防御方法一般使用单独的模型处理不同的攻击，这增加了系统的复杂性和推理延迟。本方法旨在提出一种统一的训练方法来处理不同的攻击方式。", "method": "提出了配对采样对比框架（Paired-Sampling Contrastive Framework），该方法通过自动匹配真实和攻击自拍的配对来学习模态无关的活动线索。", "result": "该方法在6th Face Anti-Spoofing Challenge Unified Physical-Digital Attack Detection基准上达到了2.10%的平均分类错误率（ACER），优于以前的解决方案。", "conclusion": "所提出的框架在性能上表现出色，且计算量低（4.46 GFLOPs），训练时间短（少于一小时），适用于实际场景的部署。"}}
{"id": "2508.14904", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14904", "abs": "https://arxiv.org/abs/2508.14904", "authors": ["Jianfeng Si", "Lin Sun", "Zhewen Tan", "Xiangzheng Zhang"], "title": "Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training", "comment": "12 pages,5 figures,4 tables", "summary": "Current methods for content safety in Large Language Models (LLMs), such as\nSupervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback\n(RLHF), often rely on multi-stage training pipelines and lack fine-grained,\npost-deployment controllability. To address these limitations, we propose a\nunified co-training framework that efficiently integrates multiple safety\nbehaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and\nrejective (refusal-oriented/conservative) within a single SFT stage. Notably,\neach behavior is dynamically activated via a simple system-level instruction,\nor magic token, enabling stealthy and efficient behavioral switching at\ninference time. This flexibility supports diverse deployment scenarios, such as\npositive for safe user interaction, negative for internal red-teaming, and\nrejective for context-aware refusals triggered by upstream moderation signals.\nThis co-training strategy induces a distinct Safety Alignment Margin in the\noutput space, characterized by well-separated response distributions\ncorresponding to each safety mode. The existence of this margin provides\nempirical evidence for the model's safety robustness and enables unprecedented\nfine-grained control. Experiments show that our method matches the safety\nalignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1\n(671B) in safety performance, while significantly reducing both training\ncomplexity and deployment costs. This work presents a scalable, efficient, and\nhighly controllable solution for LLM content safety.", "AI": {"tldr": "本文提出了一种新的LLMs安全行为整合方法，通过简单的指令实现细粒度的安全行为切换。实验表明，这种方法在保证安全性能的同时，显著降低了模型的成本。", "motivation": "现有的LLMs内容安全方法如SFT和RLHF通常依赖多阶段训练管道，缺乏细粒度的部署后可控性。本文旨在解决这些问题。", "method": "提出了一种统一的协同训练框架，该框架可以有效地整合多种安全行为（正面、负面和拒绝行为）到单一的监督微调阶段中。每个行为通过简单的系统级指令或魔法令牌在推理时进行动态激活。", "result": "实验结果显示，该方法达到了与SFT+DPO相同的安全对齐质量，而8B的模型在安全性能上超过了DeepSeek-R1 (671B)，同时显著降低了训练复杂度和部署成本。", "conclusion": "这项工作提供了一个可扩展、高效和高度可控的LLMs内容安全解决方案，通过在输出空间引入安全对齐间隔，提供了模型安全稳健性的实证证据。"}}
{"id": "2508.15020", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15020", "abs": "https://arxiv.org/abs/2508.15020", "authors": ["Susim Roy", "Anubhooti Jain", "Mayank Vatsa", "Richa Singh"], "title": "TAIGen: Training-Free Adversarial Image Generation via Diffusion Models", "comment": "Accepted at ICCVW-CV4BIOM 2025", "summary": "Adversarial attacks from generative models often produce low-quality images\nand require substantial computational resources. Diffusion models, though\ncapable of high-quality generation, typically need hundreds of sampling steps\nfor adversarial generation. This paper introduces TAIGen, a training-free\nblack-box method for efficient adversarial image generation. TAIGen produces\nadversarial examples using only 3-20 sampling steps from unconditional\ndiffusion models. Our key finding is that perturbations injected during the\nmixing step interval achieve comparable attack effectiveness without processing\nall timesteps. We develop a selective RGB channel strategy that applies\nattention maps to the red channel while using GradCAM-guided perturbations on\ngreen and blue channels. This design preserves image structure while maximizing\nmisclassification in target models. TAIGen maintains visual quality with PSNR\nabove 30 dB across all tested datasets. On ImageNet with VGGNet as source,\nTAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8%\nagainst ShuffleNet. The method generates adversarial examples 10x faster than\nexisting diffusion-based attacks. Our method achieves the lowest robust\naccuracy, indicating it is the most impactful attack as the defense mechanism\nis least successful in purifying the images generated by TAIGen.", "AI": {"tldr": "本文提出TAIGen，一种无需训练的黑盒方法，用于高效生成对抗样本，通过减少采样步骤并采用选择性RGB通道策略，提高生成图像质量和速度。\n", "motivation": "为了解决生成模型对抗攻击中图像质量低和计算资源消耗大的问题，本篇论文提出了TAIGen方法，利用无条件扩散模型在较少采样步骤中生成对抗样本。\n", "method": "TAIGen方法采用了选择性RGB通道策略，红色通道使用注意力图，绿色通道和蓝色通道使用GradCAM指导的扰动，以此来保持图像结构并最大化目标模型中的误分类。\n", "result": " terörizes\n", "conclusion": "TAIGen方法不仅保持了较高的视觉质量（PSNR超过30dB），而且在生成对抗样本时速度比现有的扩散模型攻击方法快10倍。\n"}}
{"id": "2508.14909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14909", "abs": "https://arxiv.org/abs/2508.14909", "authors": ["Tom Kocmi", "Eleftherios Avramidis", "Rachel Bawden", "Ondřej Bojar", "Konstantin Dranch", "Anton Dvorkovich", "Sergey Dukanov", "Natalia Fedorova", "Mark Fishel", "Markus Freitag", "Thamme Gowda", "Roman Grundkiewicz", "Barry Haddow", "Marzena Karpinska", "Philipp Koehn", "Howard Lakougna", "Jessica Lundin", "Kenton Murray", "Masaaki Nagata", "Stefano Perrella", "Lorenzo Proietti", "Martin Popel", "Maja Popović", "Parker Riley", "Mariya Shmatova", "Steinþór Steingrímsson", "Lisa Yankovskaya", "Vilém Zouhar"], "title": "Preliminary Ranking of WMT25 General Machine Translation Systems", "comment": null, "summary": "We present the preliminary ranking of the WMT25 General Machine Translation\nShared Task, in which MT systems have been evaluated using automatic metrics.\nAs this ranking is based on automatic evaluations, it may be biased in favor of\nsystems that employ re-ranking techniques, such as Quality Estimation\nre-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be\nbased on human evaluation, which is more reliable and will supersede the\nautomatic ranking.\n  The purpose of this report is not to present the final findings of the\nGeneral MT task, but rather to share preliminary results with task\nparticipants, which may be useful when preparing their system submission\npapers.", "AI": {"tldr": "该摘要展示了WMT25通用机器翻译任务的初步自动评估排名，提醒读者可能存在偏差，并指出最终排名将基于人工评估。报告的主要目的是向参与者提供有用的初步结果。", "motivation": "此报告的动机在于向任务参与者分享通用机器翻译任务的初步结果，这可能有助于他们准备各自的系统提交论文。同时，也说明因为自动评估可能带来的偏差，最终排名将依据更具可靠性的手动评估。", "method": "该报告基于自动评估指标对MT系统的初步排名进行展示，而非最终基于人工评估的排名。目的在于为参与者在准备系统提交论文时提供有用的初步结果。详细的方法论和自动评估技术未在摘要中明确说明。", "result": "报告展示了WMT25通用机器翻译共享任务的初步排名，但提醒读者该排名可能存在偏差，特别是对于使用了例如质量估计重排序或最小贝叶斯风险解码等重排序技术的系统。", "conclusion": "报告强调了尽管自动评估所带来的排名具有一定的参考价值，但最终的评测结果将以人工评估为准，这也是报告中没有包含最终评估结论的原因。"}}
{"id": "2508.15027", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15027", "abs": "https://arxiv.org/abs/2508.15027", "authors": ["Chunming He", "Fengyang Xiao", "Rihan Zhang", "Chengyu Fang", "Deng-Ping Fan", "Sina Farsiu"], "title": "Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement", "comment": "18 pages, 21 tables, 13 figures", "summary": "Existing methods for concealed visual perception (CVP) often leverage\nreversible strategies to decrease uncertainty, yet these are typically confined\nto the mask domain, leaving the potential of the RGB domain underexplored. To\naddress this, we propose a reversible unfolding network with generative\nrefinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as\na mathematical optimization problem and unfolds the iterative solution into a\nmulti-stage deep network. This approach provides a principled way to apply\nreversible modeling across both mask and RGB domains while leveraging a\ndiffusion model to resolve the resulting uncertainty. Each stage of the network\nintegrates three purpose-driven modules: a Concealed Object Region Extraction\n(CORE) module applies reversible modeling to the mask domain to identify core\nobject regions; a Context-Aware Region Enhancement (CARE) module extends this\nprinciple to the RGB domain to foster better foreground-background separation;\nand a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a\nfinal refinement. The FINE module introduces a targeted Bernoulli diffusion\nmodel that refines only the uncertain regions of the segmentation mask,\nharnessing the generative power of diffusion for fine-detail restoration\nwithout the prohibitive computational cost of a full-image process. This unique\nsynergy, where the unfolding network provides a strong uncertainty prior for\nthe diffusion model, allows RUN++ to efficiently direct its focus toward\nambiguous areas, significantly mitigating false positives and negatives.\nFurthermore, we introduce a new paradigm for building robust CVP systems that\nremain effective under real-world degradations and extend this concept into a\nbroader bi-level optimization framework.", "AI": {"tldr": "The paper introduces RUN++, a method that uses a reversible unfolding network with generative refinement to improve concealed visual perception by handling both mask and RGB domains and using a targeted diffusion model to refine uncertain regions, thereby reducing false positives and negatives effectively under real-world degradations.", "motivation": "The motivation is to overcome the limitations of existing concealed visual perception methods which typically only operate within the mask domain and fail to exploit the potential of the RGB domain. This leads to inefficiencies in handling ambiguity and uncertainty in object perception.", "method": "RUN++ formulates the CVP task as a mathematical optimization problem and implements a multi-stage deep network to address it. The network incorporates reversible modeling across mask and RGB domains, and employs three modules: CORE (for mask domain), CARE (for RGB domain), and FINE (for noise-based enhancement).", "result": "The method provides enhanced foreground-background separation and refined detail restoration in uncertain regions, achieving significant reductions in false positives and negatives. A new paradigm for robust CVP systems is also presented that works well under real-world degradations.", "conclusion": "The proposed RUN++ method innovates in the concealed visual perception area by integrating reversible modeling, multi-stage refinement, and targeted diffusion, thus improving the robustness and accuracy of CVP systems."}}
{"id": "2508.14913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14913", "abs": "https://arxiv.org/abs/2508.14913", "authors": ["Israel Abebe Azime", "Tadesse Destaw Belay", "Dietrich Klakow", "Philipp Slusallek", "Anshuman Chhabra"], "title": "Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages", "comment": null, "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nsolving mathematical problems expressed in natural language. However,\nmultilingual and culturally-grounded mathematical reasoning in low-resource\nlanguages lags behind English due to the scarcity of socio-cultural task\ndatasets that reflect accurate native entities such as person names,\norganization names, and currencies. Existing multilingual benchmarks are\npredominantly produced via translation and typically retain English-centric\nentities, owing to the high cost associated with human annotater-based\nlocalization. Moreover, automated localization tools are limited, and hence,\ntruly localized datasets remain scarce. To bridge this gap, we introduce a\nframework for LLM-driven cultural localization of math word problems that\nautomatically constructs datasets with native names, organizations, and\ncurrencies from existing sources. We find that translated benchmarks can\nobscure true multilingual math ability under appropriate socio-cultural\ncontexts. Through extensive experiments, we also show that our framework can\nhelp mitigate English-centric entity bias and improves robustness when native\nentities are introduced across various languages.", "AI": {"tldr": "本文提出了一种使用大型语言模型的框架，用于数学应用题的文化本地化，以解决低资源语言中缺乏本土文化背景数据集的问题，提高了模型在多语言数学推理任务中的表现。", "motivation": "由于低资源语言中缺乏反映本土实体（如人名、组织名和货币）的社交文化任务数据集，导致这些语言的多语言和文化背景的数学推理能力落后于英语。", "method": "提出了一种基于大型语言模型（LLMs）的框架，用于数学应用题的文化本地化，该框架能够从现有资源中自动构建包含本地人名、组织名和货币单位的数据集。", "result": "实验表明，通过翻译生成的基准测试可能掩盖了适当的社交文化背景下的真正多语言数学能力。同时，作者提出的框架可以帮助缓解以英语为中心的实体偏见，并在引入各种语言的本地实体时提高鲁棒性。", "conclusion": "本研究提供了一种方法，通过自动构建反映本土文化背景的数据集，来提高大型语言模型在多语言数学推理任务中的表现，从而更好地服务于低资源语言的社区。"}}
{"id": "2508.15057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15057", "abs": "https://arxiv.org/abs/2508.15057", "authors": ["Toqi Tahamid Sarker", "Mohamed Embaby", "Taminul Islam", "Amer AbuGhazaleh", "Khaled R Ahmed"], "title": "GasTwinFormer: A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging", "comment": "Accepted for publication at ICCVW 2025", "summary": "Livestock methane emissions represent 32% of human-caused methane production,\nmaking automated monitoring critical for climate mitigation strategies. We\nintroduce GasTwinFormer, a hybrid vision transformer for real-time methane\nemission segmentation and dietary classification in optical gas imaging through\na novel Mix Twin encoder alternating between spatially-reduced global attention\nand locally-grouped attention mechanisms. Our architecture incorporates a\nlightweight LR-ASPP decoder for multi-scale feature aggregation and enables\nsimultaneous methane segmentation and dietary classification in a unified\nframework. We contribute the first comprehensive beef cattle methane emission\ndataset using OGI, containing 11,694 annotated frames across three dietary\ntreatments. GasTwinFormer achieves 74.47% mIoU and 83.63% mF1 for segmentation\nwhile maintaining exceptional efficiency with only 3.348M parameters, 3.428G\nFLOPs, and 114.9 FPS inference speed. Additionally, our method achieves perfect\ndietary classification accuracy (100%), demonstrating the effectiveness of\nleveraging diet-emission correlations. Extensive ablation studies validate each\narchitectural component, establishing GasTwinFormer as a practical solution for\nreal-time livestock emission monitoring. Please see our project page at\ngastwinformer.github.io.", "AI": {"tldr": "本文介绍了GasTwinFormer，一种用于光气成像中实时甲烷排放分割和饮食分类的混合视觉变压器。GasTwinFormer实现了高效且准确的牲畜排放监控和饮食分类，展示了其作为监控工具的实用性。", "motivation": "鉴于畜牧业甲烷排放占人为甲烷生产的32%，自动化监测对于气候缓解策略至关重要。本文旨在提供一个有效的解决方案来实现实时牲畜排放监测。", "method": "介绍了GasTwinFormer，这是一种混合视觉变压器，用于通过光气成像实现实时甲烷排放分割和饮食分类。其独特的混合孪生编码器交替使用空间缩减的全局注意力机制和局部分组注意力机制。此外，该架构包含一个轻量级LR-ASPP解码器，用于多尺度特征聚合，可以在统一框架内同时实现甲烷分割和饮食分类。", "result": "GasTwinFormer在分割方面达到了74.47%的mIoU和83.63%的mF1，同时维持高效的参数(3.348M)、乘加运算(3.428G FLOPs)和推理速度(114.9 FPS)。此外，它的饮食分类准确率达到了100%。", "conclusion": "GasTwinFormer通过实验证明了其作为实时牲畜排放监测实用解决方案的有效性。"}}
{"id": "2508.14951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14951", "abs": "https://arxiv.org/abs/2508.14951", "authors": ["Dario Vajda", "Domen Vreš", "Marko Robnik-Šikonja"], "title": "Improving LLMs for Machine Translation Using Synthetic Preference Data", "comment": "Paper with individual presentation at LUHME workshop at ECAI 2025", "summary": "Large language models have emerged as effective machine translation systems.\nIn this paper, we explore how a general instruction-tuned large language model\ncan be improved for machine translation using relatively few easily produced\ndata resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct\nmodel using Direct Preference Optimization (DPO) training on a programmatically\ncurated and enhanced subset of a public dataset. As DPO requires pairs of\nquality-ranked instances, we generated its training dataset by translating\nEnglish Wikipedia articles using two LLMs, GaMS-9B-Instruct and\nEuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics\ncoupled with automatic evaluation metrics such as COMET. The evaluation shows\nthat our fine-tuned model outperforms both models involved in the dataset\ngeneration. In comparison to the baseline models, the fine-tuned model achieved\na COMET score gain of around 0.04 and 0.02, respectively, on translating\nWikipedia articles. It also more consistently avoids language and formatting\nerrors.", "AI": {"tldr": "本研究探索了如何使用少量数据资源通过直接偏好优化（DPO）方法来改善大型语言模型的机器翻译性能，使用斯洛文尼亚语的特殊情况表现出显著效果。", "motivation": "探讨如何使用相对较少的易于生成的数据资源，提高大型语言模型在机器翻译任务中的效果。使用斯洛文尼亚语作为实例进行研究。", "method": "使用直接偏好优化（DPO）训练方法在程序化编目和增强的公开数据子集上改进了通用指令调整的大型语言模型。通过对比两个大型语言模型GaMS-9B-Instruct和EuroLLM-9B-Instruct对英文维基百科文章的翻译，基于启发式和自动评估指标如COMET排名生成训练数据集。", "result": "评估结果显示，经过微调的模型在翻译维基百科文章时提高了0.04和0.02（分别与基准模型比较）的COMET分数，并且更为一致地避免了语言和格式错误。", "conclusion": "通过使用DPO训练方法和程序化编目和增强的数据子集，可以在较少的数据资源下提高大型语言模型的机器翻译能力，特别是在斯洛文尼亚语的用例中表现尤为突出。"}}
{"id": "2508.15093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15093", "abs": "https://arxiv.org/abs/2508.15093", "authors": ["Yan Luo", "Drake Du", "Hao Huang", "Yi Fang", "Mengyu Wang"], "title": "CurveFlow: Curvature-Guided Flow Matching for Image Generation", "comment": null, "summary": "Existing rectified flow models are based on linear trajectories between data\nand noise distributions. This linearity enforces zero curvature, which can\ninadvertently force the image generation process through low-probability\nregions of the data manifold. A key question remains underexplored: how does\nthe curvature of these trajectories correlate with the semantic alignment\nbetween generated images and their corresponding captions, i.e., instructional\ncompliance? To address this, we introduce CurveFlow, a novel flow matching\nframework designed to learn smooth, non-linear trajectories by directly\nincorporating curvature guidance into the flow path. Our method features a\nrobust curvature regularization technique that penalizes abrupt changes in the\ntrajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017\ndemonstrate that CurveFlow achieves state-of-the-art performance in\ntext-to-image generation, significantly outperforming both standard rectified\nflow variants and other non-linear baselines like Rectified Diffusion. The\nimprovements are especially evident in semantic consistency metrics such as\nBLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling\nsubstantially enhances the model's ability to faithfully follow complex\ninstructions while simultaneously maintaining high image quality. The code is\nmade publicly available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.", "AI": {"tldr": "论文提出了CurveFlow框架，用于学习非线性轨迹，以改善语义一致性并提升生成图像质量。", "motivation": "现有校正流模型基于数据和噪声分布之间的线性轨迹，这种线性强制零曲率，可能导致图像生成过程经过数据流形的低概率区域。目前较少探索的问题是：这些轨迹的曲率如何影响生成图像与其对应描述之间的语义对齐度，即指令遵从性？", "method": "为了应对这一问题，我们提出了CurveFlow，这是一种新颖的流匹配框架，通过直接引入曲率指导来学习平滑的非线性轨迹。我们的方法包括了一种强大的曲率正则化技术，该技术对轨迹内在动态的突然变化进行惩罚。", "result": "在MS COCO 2014和2017上的广泛实验表明，CurveFlow在文本到图像生成任务中达到了最先进的性能，显著超越了标准校正流变体和其他非线性基准模型，如校正扩散模型。尤其在语义一致性指标如BLEU、METEOR、ROUGE和CLAIR方面有明显提升。", "conclusion": "这证明了我们曲率感知模型显著增强了模型按复杂指令生成图像的能力，同时保持了高质量的图像生成。"}}
{"id": "2508.14982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14982", "abs": "https://arxiv.org/abs/2508.14982", "authors": ["Qianli Wang", "Tatiana Anikina", "Nils Feldhus", "Simon Ostermann", "Fedor Splitt", "Jiaao Li", "Yoana Tsoneva", "Sebastian Möller", "Vera Schmitt"], "title": "Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems", "comment": "Accepted at EMNLP 2025 Findings, camera-ready version", "summary": "Conversational explainable artificial intelligence (ConvXAI) systems based on\nlarge language models (LLMs) have garnered considerable attention for their\nability to enhance user comprehension through dialogue-based explanations.\nCurrent ConvXAI systems often are based on intent recognition to accurately\nidentify the user's desired intention and map it to an explainability method.\nWhile such methods offer great precision and reliability in discerning users'\nunderlying intentions for English, a significant challenge in the scarcity of\ntraining data persists, which impedes multilingual generalization. Besides, the\nsupport for free-form custom inputs, which are user-defined data distinct from\npre-configured dataset instances, remains largely limited. To bridge these\ngaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL\ndataset spanning five typologically diverse languages, including one\nlow-resource language. Subsequently, we propose a new parsing approach aimed at\nenhancing multilingual parsing performance, and evaluate three LLMs on\nMultiCoXQL using various parsing strategies. Furthermore, we present Compass, a\nnew multilingual dataset designed for custom input extraction in ConvXAI\nsystems, encompassing 11 intents across the same five languages as MultiCoXQL.\nWe conduct monolingual, cross-lingual, and multilingual evaluations on Compass,\nemploying three LLMs of varying sizes alongside BERT-type models.", "AI": {"tldr": "本文介绍了MultiCoXQL，这是一个跨五个语种（包括一种低资源语言）的多语言扩展数据集，并提出了一种新的解析方法来增强多语言解析性能。同时，提出Compass数据集，用于ConvXAI系统中自定义输入的提取，评估了三种不同规模的LLM和BERT型模型在单语、跨语和多语环境下的表现。", "motivation": "由于多语言泛化面临训练数据稀缺的挑战，且对自由形式的自定义输入支持有限，本文旨在通过扩展多语言数据集MultiCoXQL和Compass，解决ConvXAI系统在多语言处理和自定义输入方面的局限性。", "method": "提出了一种新的解析方法，旨在增强多语言解析性能，同时通过对MultiCoXQL和Compass的多语种评估来验证不同规模的LLM以及BERT模型的效果。", "result": "通过在MultiCoXQL和Compass上使用不同解析策略以及多种LLM规模和BERT型模型进行的单语、跨语和多语评估，证明了新解析方法的实际应用价值。", "conclusion": "本文通过构建多语言数据集MultiCoXQL和Compass，提出新的解析技术来提高ConvXAI系统的多语言和自定义输入处理能力，使得这些系统在多种语言环境中都能表现良好。"}}
{"id": "2508.15130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15130", "abs": "https://arxiv.org/abs/2508.15130", "authors": ["Vaishnav Ramesh", "Haining Wang", "Md Jahidul Islam"], "title": "HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment", "comment": "10 pages, 8 figures", "summary": "Despite significant progress in no-reference image quality assessment\n(NR-IQA), dataset biases and reliance on subjective labels continue to hinder\ntheir generalization performance. We propose HiRQA, Hierarchical Ranking and\nQuality Alignment), a self-supervised, opinion-unaware framework that offers a\nhierarchical, quality-aware embedding through a combination of ranking and\ncontrastive learning. Unlike prior approaches that depend on pristine\nreferences or auxiliary modalities at inference time, HiRQA predicts quality\nscores using only the input image. We introduce a novel higher-order ranking\nloss that supervises quality predictions through relational ordering across\ndistortion pairs, along with an embedding distance loss that enforces\nconsistency between feature distances and perceptual differences. A\ntraining-time contrastive alignment loss, guided by structured textual prompts,\nfurther enhances the learned representation. Trained only on synthetic\ndistortions, HiRQA generalizes effectively to authentic degradations, as\ndemonstrated through evaluation on various distortions such as lens flare,\nhaze, motion blur, and low-light conditions. For real-time deployment, we\nintroduce \\textbf{HiRQA-S}, a lightweight variant with an inference time of\nonly 3.5 ms per image. Extensive experiments across synthetic and authentic\nbenchmarks validate HiRQA's state-of-the-art (SOTA) performance, strong\ngeneralization ability, and scalability.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.15044", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15044", "abs": "https://arxiv.org/abs/2508.15044", "authors": ["Bolian Li", "Yanran Wu", "Xinyu Luo", "Ruqi Zhang"], "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner", "comment": null, "summary": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency.", "AI": {"tldr": "文章解决了测试时对齐技术的效率瓶颈问题，通过引入奖励偏移推测采样算法，在不改变目标模型的情况下，实现了更高效的人类偏好对齐。", "motivation": "随着大型语言模型的发展，使其与人类偏好对齐变得至关重要。尽管推理时对齐技术可以提高LLM的安全性和推理能力，但这些技术通常会导致推理成本增加，限制了其实际应用。我们受推测采样加速技术的启发，旨在解决推理时对齐的效率瓶颈问题。", "method": "引入了奖励偏移推测采样（SSS）算法，该算法中草图模型被对齐到人类偏好，而目标模型保持不变。我们理论上证明，通过修改接受标准和奖励代币分布，可以利用对齐草图模型与未对齐目标模型之间的分布偏差，恢复RLHF最优解，而无需实际获得该解。", "result": "我们的算法在推理成本显著降低的同时，在测试时弱到强对齐的实验中取得了更高的金质奖励分数，验证了其有效性和效率。", "conclusion": "SSS算法不仅在测试时弱到强对齐实验中取得了更好的实际效果，同时也降低了计算成本，表明了该方法的有效性和实用性。"}}
{"id": "2508.15158", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.15158", "abs": "https://arxiv.org/abs/2508.15158", "authors": ["Md. Nurul Absur", "Abhinav Kumar", "Swastik Brahma", "Saptarshi Debroy"], "title": "Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments", "comment": "11 Pages, 7 Figures", "summary": "Multi-view 3D reconstruction applications are revolutionizing critical use\ncases that require rapid situational-awareness, such as emergency response,\ntactical scenarios, and public safety. In many cases, their near-real-time\nlatency requirements and ad-hoc needs for compute resources necessitate\nadoption of `Just-in-time' edge environments where the system is set up on the\nfly to support the applications during the mission lifetime. However,\nreliability issues can arise from the inherent dynamism and operational\nadversities of such edge environments, resulting in spatiotemporally correlated\ndisruptions that impact the camera operations, which can lead to sustained\ndegradation of reconstruction quality. In this paper, we propose a novel\nportfolio theory inspired edge resource management strategy for reliable\nmulti-view 3D reconstruction against possible system disruptions. Our proposed\nmethodology can guarantee reconstruction quality satisfaction even when the\ncameras are prone to spatiotemporally correlated disruptions. The portfolio\ntheoretic optimization problem is solved using a genetic algorithm that\nconverges quickly for realistic system settings. Using publicly available and\ncustomized 3D datasets, we demonstrate the proposed camera selection strategy's\nbenefits in guaranteeing reliable 3D reconstruction against traditional\nbaseline strategies, under spatiotemporal disruptions.", "AI": {"tldr": "本文提出了一种新的受投资组合理论启发的边缘资源管理策略，用于保证在可能发生的系统中断情况下进行多视角3D重建的质量。实验结果表明，该方法相比于传统基线策略，在面对时空干扰时能提供更可靠的3D重建质量。", "motivation": "多视角3D重建应用在紧急响应、战术场景和公共安全等需要快速态势感知的领域中至关重要，然而这些应用中的边缘计算环境的动态性和操作挑战性会导致时空相关的干扰，进而影响相机操作并导致重建质量下降。本文旨在解决这一问题。", "method": "本文提出了一种受投资组合理论启发的边缘资源管理策略，以保证在可能存在系统中断的情况下，多视角3D重建的质量。采用遗传算法解决投资组合理论优化问题，该算法在实际系统设置中能快速收敛。", "result": "通过使用公开的和定制的3D数据集，本文展示了在存在时空干扰情况下，与传统基线策略相比，提出的相机选择策略确实实现了更可靠的3D重建。", "conclusion": "本文提出的方法不仅能够保证在相机容易受到时空相关干扰的情况下实现高质量的3D重建，而且通过遗传算法优化的快速收敛特性，适用于实际系统的实时需求。"}}
{"id": "2508.15085", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15085", "abs": "https://arxiv.org/abs/2508.15085", "authors": ["MohamamdJavad Ardestani", "Ehsan Kamalloo", "Davood Rafiei"], "title": "LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text", "comment": null, "summary": "LongRecall. The completeness of machine-generated text, ensuring that it\ncaptures all relevant information, is crucial in domains such as medicine and\nlaw and in tasks like list-based question answering (QA), where omissions can\nhave serious consequences. However, existing recall metrics often depend on\nlexical overlap, leading to errors with unsubstantiated entities and\nparaphrased answers, while LLM-as-a-Judge methods with long holistic prompts\ncapture broader semantics but remain prone to misalignment and hallucinations\nwithout structured verification. We introduce LongRecall, a general three-stage\nrecall evaluation framework that decomposes answers into self-contained facts,\nsuccessively narrows plausible candidate matches through lexical and semantic\nfiltering, and verifies their alignment through structured entailment checks.\nThis design reduces false positives and false negatives while accommodating\ndiverse phrasings and contextual variations, serving as a foundational building\nblock for systematic recall assessment. We evaluate LongRecall on three\nchallenging long-form QA benchmarks using both human annotations and LLM-based\njudges, demonstrating substantial improvements in recall accuracy over strong\nlexical and LLM-as-a-Judge baselines.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.15168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15168", "abs": "https://arxiv.org/abs/2508.15168", "authors": ["Masato Ito", "Kaito Tanaka", "Keisuke Matsuda", "Aya Nakayama"], "title": "XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis", "comment": null, "summary": "Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating\nearly and accurate diagnosis. While deep learning models have shown promise in\nDR detection, their black-box nature often hinders clinical adoption due to a\nlack of transparency and interpretability. To address this, we propose XDR-LVLM\n(eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that\nleverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis\ncoupled with natural language-based explanations. XDR-LVLM integrates a\nspecialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt\nEngineering and Multi-stage Fine-tuning to deeply understand pathological\nfeatures within fundus images and generate comprehensive diagnostic reports.\nThese reports explicitly include DR severity grading, identification of key\npathological concepts (e.g., hemorrhages, exudates, microaneurysms), and\ndetailed explanations linking observed features to the diagnosis. Extensive\nexperiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM\nachieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and\nan F1 Score of 79.92% for disease diagnosis, and superior results for concept\ndetection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the\nhigh fluency, accuracy, and clinical utility of the generated explanations,\nshowcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and\nclinical needs by providing robust and interpretable insights.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.15090", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15090", "abs": "https://arxiv.org/abs/2508.15090", "authors": ["Matt Pauk", "Maria Leonor Pacheco"], "title": "Mapping the Course for Prompt-based Structured Prediction", "comment": null, "summary": "LLMs have been shown to be useful for a variety of language tasks, without\nrequiring task-specific fine-tuning. However, these models often struggle with\nhallucinations and complex reasoning problems due to their autoregressive\nnature. We propose to address some of these issues, specifically in the area of\nstructured prediction, by combining LLMs with combinatorial inference in an\nattempt to marry the predictive power of LLMs with the structural consistency\nprovided by inference methods. We perform exhaustive experiments in an effort\nto understand which prompting strategies can effectively estimate LLM\nconfidence values for use with symbolic inference, and show that, regardless of\nthe prompting strategy, the addition of symbolic inference on top of prompting\nalone leads to more consistent and accurate predictions. Additionally, we show\nthat calibration and fine-tuning using structured prediction objectives leads\nto increased performance for challenging tasks, showing that structured\nlearning is still valuable in the era of LLMs.", "AI": {"tldr": "本研究通过结合LLMs和组合推理来提高结构化预测任务的准确性和一致性，发现添加符号推理和进行校准与微调可以改善LLMs在复杂推理任务上的表现。", "motivation": "尽管LLMs在许多语言任务中表现出色，但它们在复杂推理和结构化预测问题上遇到了困难，主要因为它们自回归的特性。这项研究旨在解决这些问题，尝试将LLMs的预测能力与推理方法提供的结构一致性结合起来。", "method": "结合LLMs与组合推理来提高结构化预测任务中模型的准确性和一致性。通过详尽的实验了解哪些提示策略可以有效地估计LLMs的置信值，以便与符号推理结合使用。研究还包括校准和使用结构化预测目标进行微调，以提高在具有挑战性任务中的性能。", "result": "即使采用不同的提示策略，添加符号推理也能使预测更加一致和准确。此外，通过结构化预测目标进行校准和微调，可以使模型在难以处理的任务上表现更好，表明结构化学习在LLMs时代仍然具有价值。", "conclusion": "即使在大规模语言模型时代，通过结构化的预测和校准微调，仍然可以提高模型在复杂推理任务上的性能。因此，在处理结构化预测问题时，结合LLMs和符号推理是有效的方法。"}}
{"id": "2508.15169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15169", "abs": "https://arxiv.org/abs/2508.15169", "authors": ["Xuyang Chen", "Zhijun Zhai", "Kaixuan Zhou", "Zengmao Wang", "Jianan He", "Dong Wang", "Yanfeng Zhang", "mingwei Sun", "Rüdiger Westermann", "Konrad Schindler", "Liqiu Meng"], "title": "MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion", "comment": null, "summary": "Mesh models have become increasingly accessible for numerous cities; however,\nthe lack of realistic textures restricts their application in virtual urban\nnavigation and autonomous driving. To address this, this paper proposes MeSS\n(Meshbased Scene Synthesis) for generating high-quality, styleconsistent\noutdoor scenes with city mesh models serving as the geometric prior. While\nimage and video diffusion models can leverage spatial layouts (such as depth\nmaps or HD maps) as control conditions to generate street-level perspective\nviews, they are not directly applicable to 3D scene generation. Video diffusion\nmodels excel at synthesizing consistent view sequences that depict scenes but\noften struggle to adhere to predefined camera paths or align accurately with\nrendered control videos. In contrast, image diffusion models, though unable to\nguarantee cross-view visual consistency, can produce more geometry-aligned\nresults when combined with ControlNet. Building on this insight, our approach\nenhances image diffusion models by improving cross-view consistency. The\npipeline comprises three key stages: first, we generate geometrically\nconsistent sparse views using Cascaded Outpainting ControlNets; second, we\npropagate denser intermediate views via a component dubbed AGInpaint; and\nthird, we globally eliminate visual inconsistencies (e.g., varying exposure)\nusing the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting\n(3DGS) scene is reconstructed by initializing Gaussian balls on the mesh\nsurface. Our method outperforms existing approaches in both geometric alignment\nand generation quality. Once synthesized, the scene can be rendered in diverse\nstyles through relighting and style transfer techniques.", "AI": {"tldr": "MeSS利用城市网格模型生成高质量、风格一致的3D城市户外场景，改进了现有方法的几何对齐和生成质量，并支持多样化的场景渲染风格。", "motivation": "现在的城市网格模型缺乏逼真的纹理，影响了它们在虚拟城市导航和自动驾驶中的应用。我们的研究动机是通过提出MeSS解决这一问题，生成更符合真实场景且几何一致的城市户外场景。", "method": "我们的方法MeSS旨在利用城市网格模型作为几何先验生成高质量且风格一致的户外场景。整个流程分为三个主要阶段：首先，使用级联Outpainting ControlNets生成几何一致的稀疏视图；其次，通过AGInpaint组件传播更多密集的中间视图；第三，使用GCAlign模块消除全局视觉不一致性。同时，在生成过程中，通过在网格表面上初始化高斯球来重建3D Gaussian Splatting场景。", "result": "我们的方法在几何对齐和生成质量上明显优于现有方法，生成的场景可以利用重光照和风格转换技术渲染出多样化的风格。", "conclusion": "我们的研究展示了MeSS在城市户外场景生成方面的优越性能，特别是在几何对齐和生成质量方面。此外，生成的场景可以进行风格化处理，增强了实际应用价值。"}}
{"id": "2508.15096", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15096", "abs": "https://arxiv.org/abs/2508.15096", "authors": ["Rabeeh Karimi Mahabadi", "Sanjeev Satheesh", "Shrimai Prabhumoye", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "title": "Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset", "comment": null, "summary": "Pretraining large language models (LLMs) on high-quality, structured data\nsuch as mathematics and code substantially enhances reasoning capabilities.\nHowever, existing math-focused datasets built from Common Crawl suffer from\ndegraded quality due to brittle extraction heuristics, lossy HTML-to-text\nconversion, and the failure to reliably preserve mathematical structure. In\nthis work, we introduce Nemotron-CC-Math, a large-scale, high-quality\nmathematical corpus constructed from Common Crawl using a novel,\ndomain-agnostic pipeline specifically designed for robust scientific text\nextraction.\n  Unlike previous efforts, our pipeline recovers math across various formats\n(e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx\nand a targeted LLM-based cleaning stage. This approach preserves the structural\nintegrity of equations and code blocks while removing boilerplate,\nstandardizing notation into LaTeX representation, and correcting\ninconsistencies.\n  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+\n(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably,\nNemotron-CC-Math-4+ not only surpasses all prior open math datasets-including\nMegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens\nthan FineMath-4+, which was previously the highest-quality math pretraining\ndataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to\n+12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines,\nwhile also improving general-domain performance on MMLU and MMLU-Stem.\n  We present the first pipeline to reliably extract scientific\ncontent--including math--from noisy web-scale data, yielding measurable gains\nin math, code, and general reasoning, and setting a new state of the art among\nopen math pretraining corpora. To support open-source efforts, we release our\ncode and datasets.", "AI": {"tldr": "提出了一种新的科学文本抽取方法，从大规模网络数据中提取高质量数学语料库。相比于现有数据集，新方法不仅提高了数据质量，还大幅增加了有效的文本数量。", "motivation": "现有聚焦数学的数据集构建方法有诸多不足，例如由于粗糙的提取启发式方法、从HTML到文本的损失转换以及难以保持数学结构等原因导致数据质量下降。", "method": "我们提出了一种新颖且通用的科学文本抽取管道，专为从Common Crawl中提取高质量数学内容设计。该管道通过利用lynx的布局感知渲染和基于LLM的清洁阶段，能够恢复多种格式的数学内容，例如MathJax、KaTeX和MathML。", "result": "我们收集了高质量的数学语料库Nemotron-CC-Math-3+（133B tokens）和Nemotron-CC-Math-4+（52B tokens），后者不仅超越了所有之前的数学数据集，还包含了比FineMath-4+多5.5倍的tokens。", "conclusion": "我们首次提出了能够稳健地从大规模网络噪音数据中提取科学研究内容（包括数学）的管道，改进了数学、代码和一般推理能力，并在开放数学预训练语料库领域创下了新的记录。为了支持开源工作，我们公开了我们的代码和数据集。"}}
{"id": "2508.15189", "categories": ["cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.15189", "abs": "https://arxiv.org/abs/2508.15189", "authors": ["Jiahao Xu", "Changchang Yin", "Odysseas Chatzipanagiotou", "Diamantis Tsilimigras", "Kevin Clear", "Bingsheng Yao", "Dakuo Wang", "Timothy Pawlik", "Ping Zhang"], "title": "SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis", "comment": null, "summary": "Surgical site infection (SSI) is one of the most common and costly\nhealthcare-associated infections and and surgical wound care remains a\nsignificant clinical challenge in preventing SSIs and improving patient\noutcomes. While recent studies have explored the use of deep learning for\npreliminary surgical wound screening, progress has been hindered by concerns\nover data privacy and the high costs associated with expert annotation.\nCurrently, no publicly available dataset or benchmark encompasses various types\nof surgical wounds, resulting in the absence of an open-source Surgical-Wound\nscreening tool. To address this gap: (1) we present SurgWound, the first\nopen-source dataset featuring a diverse array of surgical wound types. It\ncontains 697 surgical wound images annotated by 3 professional surgeons with\neight fine-grained clinical attributes. (2) Based on SurgWound, we introduce\nthe first benchmark for surgical wound diagnosis, which includes visual\nquestion answering (VQA) and report generation tasks to comprehensively\nevaluate model performance. (3) Furthermore, we propose a three-stage learning\nframework, WoundQwen, for surgical wound diagnosis. In the first stage, we\nemploy five independent MLLMs to accurately predict specific surgical wound\ncharacteristics. In the second stage, these predictions serve as additional\nknowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess\ninfection risk and guide subsequent interventions. In the third stage, we train\na MLLM that integrates the diagnostic results from the previous two stages to\nproduce a comprehensive report. This three-stage framework can analyze detailed\nsurgical wound characteristics and provide subsequent instructions to patients\nbased on surgical images, paving the way for personalized wound care, timely\nintervention, and improved patient outcomes.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.15139", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15139", "abs": "https://arxiv.org/abs/2508.15139", "authors": ["Zijie Wang", "Eduardo Blanco"], "title": "Identifying and Answering Questions with False Assumptions: An Interpretable Approach", "comment": "To appear at EMNLP 2025 Main conference", "summary": "People often ask questions with false assumptions, a type of question that\ndoes not have regular answers. Answering such questions require first\nidentifying the false assumptions. Large Language Models (LLMs) often generate\nmisleading answers because of hallucinations. In this paper, we focus on\nidentifying and answering questions with false assumptions in several domains.\nWe first investigate to reduce the problem to fact verification. Then, we\npresent an approach leveraging external evidence to mitigate hallucinations.\nExperiments with five LLMs demonstrate that (1) incorporating retrieved\nevidence is beneficial and (2) generating and validating atomic assumptions\nyields more improvements and provides an interpretable answer by specifying the\nfalse assumptions.", "AI": {"tldr": "本文探讨了解决大型语言模型在回答含有错误假设的问题时容易产生误导性答案的问题，提出了一种利用外部证据和验证原子假设的方法，并通过实验验证了这种方法的有效性。", "motivation": "大型语言模型在回答包含错误假设的问题时经常产生误导性的答案。本文旨在解决这一问题，以提高模型的回答质量。", "method": "我们提出了一种利用外部证据来减少大型语言模型（LLMs）在回答基于错误假设的问题时产生误导性答案的方法。该方法首先将问题视为事实验证问题，然后通过引入检索到的证据来改进模型的回答。", "result": "实验结果显示，引入检索证据对于改善LLMs的回答是有益的。此外，生成并验证原子假设可以进一步提高回答质量并提供可解释的答案，明确指出错误假设。", "conclusion": "本研究展示了通过引入外部证据来减少大型语言模型在回答基于错误假设的问题时产生的误导性答案是有效的方法。通过对原子假设的生成和验证，可以获得更高质量且可解释的回答。"}}
{"id": "2508.15207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15207", "abs": "https://arxiv.org/abs/2508.15207", "authors": ["Arjun Srinivasan", "Anubhav Paras", "Aniket Bera"], "title": "Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning", "comment": null, "summary": "Existing approaches in reinforcement learning train an agent to learn desired\noptimal behavior in an environment with rule based surrounding agents. In\nsafety critical applications such as autonomous driving it is crucial that the\nrule based agents are modelled properly. Several behavior modelling strategies\nand IDM models are used currently to model the surrounding agents. We present a\nlearning based method to derive the adversarial behavior for the rule based\nagents to cause failure scenarios. We evaluate our adversarial agent against\nall the rule based agents and show the decrease in cumulative reward.", "AI": {"tldr": "本文提出了一种新方法，使用学习来生成对抗性行为，以便识别和测试潜在的失败场景，特别是在涉及规则型代理的环境中。", "motivation": "在自动驾驶等安全性关键应用中，需要正确建模规则型代理，以学习期望的最优行为。", "method": "本文提出了一种基于学习的方法，用于推导规则型周围代理的对抗行为，旨在导致失败场景。", "result": "与所有规则型代理进行评估后，实验结果显示推导出的对抗性代理导致累积奖励减少。", "conclusion": "相比于现有的规则型代理建模方法，本文提出的方法可以更好地制造失败场景。"}}
{"id": "2508.15164", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15164", "abs": "https://arxiv.org/abs/2508.15164", "authors": ["Seungmin Han", "Haeun Kwon", "Ji-jun Park", "Taeyang Yoon"], "title": "ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following", "comment": null, "summary": "Despite significant advancements in Large Language Models (LLMs) and Large\nVision-Language Models (LVLMs), current models still face substantial\nchallenges in handling complex, multi-turn, and visually-grounded tasks that\ndemand deep reasoning, sustained contextual understanding, entity tracking, and\nmulti-step instruction following. Existing benchmarks often fall short in\ncapturing the dynamism and intricacies of real-world multi-modal interactions,\nleading to issues such as context loss and visual hallucinations. To address\nthese limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning\nBenchmark), a novel dataset comprising 300 meticulously designed complex\nmulti-turn dialogue scenarios, each averaging 5-7 turns and evaluated across\nsix core dimensions including visual entity tracking and reasoning depth.\nFurthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic\nframework that enhances existing LVLMs with advanced reasoning and instruction\nfollowing capabilities through an iterative\n\"memory-perception-planning-execution\" cycle, requiring no extensive\nre-training of the underlying models. Our extensive experiments on MMDR-Bench\ndemonstrate that CoLVLM Agent consistently achieves superior performance,\nattaining an average human evaluation score of 4.03, notably surpassing\nstate-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro\n(3.85). The framework exhibits significant advantages in reasoning depth,\ninstruction adherence, and error suppression, and maintains robust performance\nover extended dialogue turns, validating the effectiveness of its modular\ndesign and iterative approach for complex multi-modal interactions.", "AI": {"tldr": "提出了CoLVLM Agent框架，用于解决现有大型语言模型和视觉语言模型在处理复杂多轮视觉互动时的问题，通过在MMDR-Bench数据集上的实验展示了其优越性。", "motivation": "现有的大型语言模型和视觉语言模型在处理需要深入推理、长时间背景理解、实体追踪和多步骤指令遵循的复杂、多轮和基于视觉的任务时存在困难，当前基准测试难以捕捉真实世界多模态互动的复杂性。", "method": "引入了MMDR-Bench数据集及CoLVLM Agent框架，后者通过记忆-感知-规划-执行的迭代循环增强了现存视觉语言模型的推理和指令遵循能力。", "result": "实验显示，CoLVLM Agent在MMDR-Bench上表现出色，平均人类评价分数为4.03，优于如GPT-4o和Gemini 1.5 Pro等先进商业模型，并在推理深度、指令遵循及错误抑制方面表现出显著优势。", "conclusion": "CoLVLM Agent的研发证实了其模块化设计及迭代方法在复杂多模态互动中的有效性。"}}
{"id": "2508.15208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15208", "abs": "https://arxiv.org/abs/2508.15208", "authors": ["Leiyue Zhao", "Yuechen Yang", "Yanfan Zhu", "Haichun Yang", "Yuankai Huo", "Paul D. Simonson", "Kenji Ikemura", "Mert R. Sabuncu", "Yihe Yang", "Ruining Deng"], "title": "DyMorph-B2I: Dynamic and Morphology-Guided Binary-to-Instance Segmentation for Renal Pathology", "comment": "9 pages, 5 figures", "summary": "Accurate morphological quantification of renal pathology functional units\nrelies on instance-level segmentation, yet most existing datasets and automated\nmethods provide only binary (semantic) masks, limiting the precision of\ndownstream analyses. Although classical post-processing techniques such as\nwatershed, morphological operations, and skeletonization, are often used to\nseparate semantic masks into instances, their individual effectiveness is\nconstrained by the diverse morphologies and complex connectivity found in renal\ntissue. In this study, we present DyMorph-B2I, a dynamic, morphology-guided\nbinary-to-instance segmentation pipeline tailored for renal pathology. Our\napproach integrates watershed, skeletonization, and morphological operations\nwithin a unified framework, complemented by adaptive geometric refinement and\ncustomizable hyperparameter tuning for each class of functional unit. Through\nsystematic parameter optimization, DyMorph-B2I robustly separates adherent and\nheterogeneous structures present in binary masks. Experimental results\ndemonstrate that our method outperforms individual classical approaches and\nna\\\"ive combinations, enabling superior instance separation and facilitating\nmore accurate morphometric analysis in renal pathology workflows. The pipeline\nis publicly available at: https://github.com/ddrrnn123/DyMorph-B2I.", "AI": {"tldr": "The paper presents DyMorph-B2I, a binary-to-instance segmentation pipeline for renal pathology that integrates classical post-processing methods with adaptive refinement to enable more accurate morphometric analysis.", "motivation": "The motivation is to improve the precision of downstream analyses in renal pathology by providing instance-level segmentation, which is lacking in existing datasets and automated methods that mainly offer binary masks.", "method": "The method introduces DyMorph-B2I, a dynamic, morphology-guided binary-to-instance segmentation pipeline for renal pathology. It integrates classical post-processing techniques such as watershed, skeletonization, and morphological operations with adaptive geometric refinement and customizable hyperparameter tuning.", "result": "Experimental results show that DyMorph-B2I outperforms individual classical approaches and naive combinations, providing superior instance separation, and is available at https://github.com/ddrrnn123/DyMorph-B2I.", "conclusion": "The conclusion is that DyMorph-B2I facilitates more accurate morphometric analysis in renal pathology by offering robust instance separation of structures in binary masks, thus enhancing the precision of downstream analyses."}}
{"id": "2508.15190", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15190", "abs": "https://arxiv.org/abs/2508.15190", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling", "comment": null, "summary": "Tokenization plays a critical role in language modeling, yet existing\napproaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on\nfrequency statistics, ignoring the underlying semantic structure of text. This\nleads to over-tokenization of semantically redundant spans and underutilization\nof contextual coherence, particularly in long-context scenarios. In this work,\nwe propose \\textbf{SemToken}, a semantic-aware tokenization framework that\njointly reduces token redundancy and improves computation efficiency. SemToken\nfirst extracts contextual semantic embeddings via lightweight encoders and\nperforms local semantic clustering to merge semantically equivalent tokens.\nThen, it allocates heterogeneous token granularity based on semantic density,\nallowing finer-grained tokenization in content-rich regions and coarser\ncompression in repetitive or low-entropy spans. SemToken can be seamlessly\nintegrated with modern language models and attention acceleration methods.\nExperiments on long-context language modeling benchmarks such as WikiText-103\nand LongBench show that SemToken achieves up to $2.4\\times$ reduction in token\ncount and $1.9\\times$ speedup, with negligible or no degradation in perplexity\nand downstream accuracy. Our findings suggest that semantic structure offers a\npromising new axis for optimizing tokenization and computation in large\nlanguage models.", "AI": {"tldr": "提出了SemToken框架，它通过语义感知来减少token冗余并提高计算效率，在长上下文语言模型基准上显著减小token数量，提高速度，且保持准确性。", "motivation": "现有分词方法如BPE或WordPiece主要基于频率统计，忽略了文本的语义结构，导致语义冗余跨度过度分词和上下文连贯性的不足，在长上下文场景中尤为突出。", "method": "SemToken是一种基于语义感知的分词框架，首先通过轻量级编码器提取语境语义嵌入，并执行局部语义聚类来合并语义等效的token。其次，根据语义密度分配异构的token粒度，实现内容丰富区域更细粒度的分词和重复或低熵跨度的更粗压缩。", "result": "实验表明，SemToken在WikiText-103和LongBench基准测试中的token数量减少达2.4倍，加速达1.9倍，且在困惑度和下游准确性方面几乎没有退化。", "conclusion": "研究表明，通过利用语义结构对分词和计算进行优化，在大型语言模型中具有很大的潜力。"}}
{"id": "2508.15216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15216", "abs": "https://arxiv.org/abs/2508.15216", "authors": ["Vipooshan Vipulananthan", "Kumudu Mohottala", "Kavindu Chinthana", "Nimsara Paramulla", "Charith D Chitraranjan"], "title": "STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation", "comment": null, "summary": "Accident prediction and timely warnings play a key role in improving road\nsafety by reducing the risk of injury to road users and minimizing property\ndamage. Advanced Driver Assistance Systems (ADAS) are designed to support human\ndrivers and are especially useful when they can anticipate potential accidents\nbefore they happen. While many existing systems depend on a range of sensors\nsuch as LiDAR, radar, and GPS, relying solely on dash-cam video input presents\na more challenging but a more cost-effective and easily deployable solution. In\nthis work, we incorporate better spatio-temporal features and aggregate them\nthrough a recurrent network to improve upon state-of-the-art graph neural\nnetworks for predicting accidents from dash-cam videos. Experiments using three\npublicly available datasets show that our proposed STAGNet model achieves\nhigher average precision and mean time-to-collision values than previous\nmethods, both when cross-validated on a given dataset and when trained and\ntested on different datasets.", "AI": {"tldr": "本文提出了一个新的模型STAGNet，用于基于驾驶舱视频预测交通事故，实验结果显示该模型优于现有的方法。", "motivation": "文章旨在改善道路安全，通过使用基于驾驶舱视频的模型来预测事故，这种方法虽然更具挑战性，但却更为经济且易于部署。", "method": "本文提出了一种新的模型STAGNet，通过结合更好的时空特征并通过递归网络进行聚合，以提高基于驾驶舱视频的事故预测的图神经网络的性能。", "result": "实验结果表明，STAGNet模型在三个公开数据集上的平均精度和平均碰撞时间比之前的方法更高，无论是进行交叉验证还是在不同数据集上进行训练和测试。", "conclusion": "研究得出结论，通过利用STAGNet模型进行更好的时空特征聚合，可有效提高基于驾驶舱视频的事故预测效果。"}}
{"id": "2508.15202", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15202", "abs": "https://arxiv.org/abs/2508.15202", "authors": ["Yuanchen Zhou", "Shuo Jiang", "Jie Zhu", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang"], "title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models", "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising framework for\nsupervising intermediate reasoning in large language models (LLMs), yet\nexisting PRMs are primarily trained on general or Science, Technology,\nEngineering, and Mathematics (STEM) domains and fall short in domain-specific\ncontexts such as finance, where reasoning is more structured, symbolic, and\nsensitive to factual and regulatory correctness. We introduce \\textbf{Fin-PRM},\na domain-specialized, trajectory-aware PRM tailored to evaluate intermediate\nreasoning steps in financial tasks. Fin-PRM integrates step-level and\ntrajectory-level reward supervision, enabling fine-grained evaluation of\nreasoning traces aligned with financial logic. We apply Fin-PRM in both offline\nand online reward learning settings, supporting three key applications: (i)\nselecting high-quality reasoning trajectories for distillation-based supervised\nfine-tuning, (ii) providing dense process-level rewards for reinforcement\nlearning, and (iii) guiding reward-informed Best-of-N inference at test time.\nExperimental results on financial reasoning benchmarks, including CFLUE and\nFinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs\nand strong domain baselines in trajectory selection quality. Downstream models\ntrained with Fin-PRM yield substantial improvements with baselines, with gains\nof 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in\ntest-time performance. These findings highlight the value of domain-specialized\nreward modeling for aligning LLMs with expert-level financial reasoning. Our\nproject resources will be available at https://github.com/aliyun/qwen-dianjin.", "AI": {"tldr": "我们引入了专门针对金融领域的Fin-PRM模型，它能够在金融任务中更有效地评估中间推理步骤，取得了显著的性能提升。", "motivation": "现有的过程奖励模型（PRMs）主要是为了一般领域或STEM（科学、技术、工程和数学）领域训练，但在诸如金融这种需要严格逻辑性和结构化的领域内的应用有限。本研究的目标是开发一种专门针对金融领域的奖励模型，以解决这一问题。", "method": "我们提出了一种名为Fin-PRM的方法，专门用于在金融领域评估大型语言模型中的中间推理步骤。Fin-PRM集成了步骤级和轨迹级的奖励监督，以实现与金融逻辑对齐的推理轨迹的精细评估。该方法适用于离线和在线奖励学习设置，并支持三个关键应用：（i）选择高质量的推理轨迹用于蒸馏式监督微调，（ii）为强化学习提供密集的流程级奖励，（iii）在测试时通过奖励信息引导最佳选项的推断。", "result": "实验结果表明，在CFLUE和FinQA等金融推理基准测试中，Fin-PRM在轨迹选择质量方面始终优于通用PRM和强大的领域内基线模型。使用Fin-PRM训练的下游模型相较于基线模型在监督学习、强化学习和测试时性能方面分别提升了12.9%、5.2%和5.1%。", "conclusion": "该研究表明，专业化领域的奖励模型在将大型语言模型与专家级别的金融推理对齐方面具有重要价值，且该模型已经在相关基准测试中得到了验证。"}}
{"id": "2508.15228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15228", "abs": "https://arxiv.org/abs/2508.15228", "authors": ["Ziang Cao", "Zhaoxi Chen", "Liang Pan", "Ziwei Liu"], "title": "Collaborative Multi-Modal Coding for High-Quality 3D Generation", "comment": null, "summary": "3D content inherently encompasses multi-modal characteristics and can be\nprojected into different modalities (e.g., RGB images, RGBD, and point clouds).\nEach modality exhibits distinct advantages in 3D asset modeling: RGB images\ncontain vivid 3D textures, whereas point clouds define fine-grained 3D\ngeometries. However, most existing 3D-native generative architectures either\noperate predominantly within single-modality paradigms-thus overlooking the\ncomplementary benefits of multi-modality data-or restrict themselves to 3D\nstructures, thereby limiting the scope of available training datasets. To\nholistically harness multi-modalities for 3D modeling, we present TriMM, the\nfirst feed-forward 3D-native generative model that learns from basic\nmulti-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM\nfirst introduces collaborative multi-modal coding, which integrates\nmodality-specific features while preserving their unique representational\nstrengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to\nraise the robustness and performance of multi-modal coding. 3) Based on the\nembedded multi-modal code, TriMM employs a triplane latent diffusion model to\ngenerate 3D assets of superior quality, enhancing both the texture and the\ngeometric detail. Extensive experiments on multiple well-known datasets\ndemonstrate that TriMM, by effectively leveraging multi-modality, achieves\ncompetitive performance with models trained on large-scale datasets, despite\nutilizing a small amount of training data. Furthermore, we conduct additional\nexperiments on recent RGB-D datasets, verifying the feasibility of\nincorporating other multi-modal datasets into 3D generation.", "AI": {"tldr": "TriMM创新提出多模态协作编码与辅助监督机制，结合三平面潜在扩散模型生成高质量3D内容，无论是在知名数据集还是新RGB-D数据集上，都展现了卓越的性能表现。", "motivation": "现有的3D原生生成架构要么主要基于单模态范式，忽略了多模态数据的互补优势，要么限制在3D结构上，从而限制了训练数据集的范围。", "method": "TriMM采用协作式多模态编码，结合模态特定特征同时保留它们的独特表达优势，引入了辅助的2D和3D监督以提高多模态编码的鲁棒性和性能，并基于嵌入的多模态代码使用三平面潜在扩散模型生成高质量的3D资产。", "result": "TriMM利用小量训练数据达到与大规模数据集训练模型相当的性能，并在多个知名数据集上的广泛实验中得到了验证。此外，通过对最近的RGB-D数据集的额外实验，验证了将其他多模态数据集引入3D生成中的可行性。", "conclusion": "TriMM是首个基于前馈的3D原生生成模型，能够从小量基本多模态数据中学习，通过有效利用多模态实现高性能的3D资产生成。"}}
{"id": "2508.15212", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15212", "abs": "https://arxiv.org/abs/2508.15212", "authors": ["Huanxuan Liao", "Yixing Xu", "Shizhu He", "Guanchen Li", "Xuanwu Yin", "Dong Li", "Emad Barsoum", "Jun Zhao", "Kang Liu"], "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning", "comment": null, "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.", "AI": {"tldr": "本文提出了一种名为SPARK的方法，通过剪枝KV的特征通道并动态恢复这些条目来减少KV缓存的冗余，使长序列数据可以在相同的内存预算下进行处理，同时在整个处理过程中保持或提高模型的准确性。", "motivation": "现有的方法在处理长序列的语言模型的KV缓存问题时，主要通过在时间轴上压缩KV缓存来减少内存和计算开销，但这些方法忽略了特征维度上不同的重要性差异，限制了它们在有效平衡效率和模型准确性方面的表现。本文就是针对这一问题提出了新的解决方案。", "method": "本文提出了一种名为SPARK的无需训练的即插即用方法，通过在特征通道层面上应用非结构化稀疏性来解决KV缓存的压缩问题，并在注意力得分计算过程中动态恢复被剪枝的条目。该方法与现有的KV缓存压缩和量化技术正交，可以结合使用以实现进一步的加速。", "result": "通过SPARK方法，在相同的序列长度下，KV缓存存储减少了超过30%，相比基于令牌驱逐的方法，即使在具有80%的激进剪枝率的情况下，性能退化也不超过5%。", "conclusion": "SPARK方法通过在通道级别应用非结构化剪枝来减少KV缓存的冗余，实现了较长序列的低内存预算处理，同时保持或提高了模型的准确性，体现了其稳健性和有效性。"}}
{"id": "2508.15231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15231", "abs": "https://arxiv.org/abs/2508.15231", "authors": ["Shihao Dong", "Xiaotong Zhou", "Yuhui Zheng", "Huiying Xu", "Xinzhong Zhu"], "title": "Center-Oriented Prototype Contrastive Clustering", "comment": null, "summary": "Contrastive learning is widely used in clustering tasks due to its\ndiscriminative representation. However, the conflict problem between classes is\ndifficult to solve effectively. Existing methods try to solve this problem\nthrough prototype contrast, but there is a deviation between the calculation of\nhard prototypes and the true cluster center. To address this problem, we\npropose a center-oriented prototype contrastive clustering framework, which\nconsists of a soft prototype contrastive module and a dual consistency learning\nmodule. In short, the soft prototype contrastive module uses the probability\nthat the sample belongs to the cluster center as a weight to calculate the\nprototype of each category, while avoiding inter-class conflicts and reducing\nprototype drift. The dual consistency learning module aligns different\ntransformations of the same sample and the neighborhoods of different samples\nrespectively, ensuring that the features have transformation-invariant semantic\ninformation and compact intra-cluster distribution, while providing reliable\nguarantees for the calculation of prototypes. Extensive experiments on five\ndatasets show that the proposed method is effective compared to the SOTA. Our\ncode is published on https://github.com/LouisDong95/CPCC.", "AI": {"tldr": "研究提出了一种新的聚类框架，通过软原型对比和双重一致性学习模块，解决了类间冲突问题，并展示了优越的聚类效果。", "motivation": "现有的方法通过原型对比来解决聚类任务中的类间冲突问题，但硬原型计算与真实类中心之间存在偏差。本研究旨在解决这一问题。", "method": "提出了一种以中心为导向的原型对比聚类框架，该框架由一个软原型对比模块和一个双重一致性学习模块组成。软原型对比模块使用样本属于每个类中心的概率作为权重来计算原型，旨在避免类间冲突并减少原型漂移。双重一致性学习模块则分别对同一样本的不同变换以及不同样本的邻域进行对齐，确保特征具有变换不变的语义信息和紧凑的类内分布，为原型计算提供可靠的保证。", "result": "在五个数据集上的广泛实验表明，所提出的方法相较于当前最优方法具有有效性。", "conclusion": "提出的方法在解决类间冲突和计算原型偏差问题上表现出色，并在多个数据集上证明了其有效性。代码已发布。"}}
{"id": "2508.15213", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15213", "abs": "https://arxiv.org/abs/2508.15213", "authors": ["Bolei He", "Xinran He", "Run Shao", "Shanfu Shu", "Xianwei Xue", "Mingquan Cheng", "Haifeng Li", "Zhenhua Ling"], "title": "Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering", "comment": "EMNLP2025 Findings", "summary": "Large Language Models (LLMs) perform well in general QA but often struggle in\ndomain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces\nexternal knowledge but suffers from hallucinations and latency due to noisy\nretrievals. Continued pretraining internalizes domain knowledge but is costly\nand lacks cross-domain flexibility. We attribute this challenge to the\nlong-tail distribution of domain knowledge, which leaves partial yet useful\ninternal knowledge underutilized. We further argue that knowledge acquisition\nshould be progressive, mirroring human learning: first understanding concepts,\nthen applying them to complex reasoning. To address this, we propose Selct2Know\n(S2K), a cost-effective framework that internalizes domain knowledge through an\ninternal-external knowledge self-selection strategy and selective supervised\nfine-tuning. We also introduce a structured reasoning data generation pipeline\nand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,\nand financial QA benchmarks show that S2K consistently outperforms existing\nmethods and matches domain-pretrained LLMs with significantly lower cost.", "AI": {"tldr": "针对大型语言模型在领域特定场景中的局限性，提出Selct2Know (S2K)框架，通过内部-外部知识选择策略和选择性监督微调方法来提高性能，同时引入结构化推理数据生成管道及GRPO来增强推理能力。S2K在医学、法律和金融问答基准上超越现有方法，且以更低的成本与领域的预训练语言模型相匹配。", "motivation": "大型语言模型（LLMs）在一般问答中表现良好，但在特定领域场景中表现不佳。检索增强生成（RAG）引入外部知识但容易出现幻觉和延迟。持续预训练可以内部化领域知识但代价高昂且缺乏跨领域灵活性。问题在于领域知识的长尾分布导致部分有用的知识被利用不足。认为知识获取应是渐进的，类似于人类学习过程。", "method": "提出了名为Selct2Know (S2K) 的框架，该框架通过内部-外部知识自我选择策略和选择性监督微调来内部化领域知识。同时引入了结构化推理数据生成管道，并整合了GRPO来增强推理能力。", "result": "S2K在医学、法律和金融问答基准上超越了现有方法，并且与领域预训练的大型语言模型相比，以显著更低的成本取得了相似的表现。", "conclusion": "Selct2Know (S2K)是一个有效的框架，能够以较低的成本提高大型语言模型在领域知识上的表现，相较于持续预训练的方法，它更加灵活和经济。"}}
{"id": "2508.15232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15232", "abs": "https://arxiv.org/abs/2508.15232", "authors": ["Ruipu Wu", "Yige Zhang", "Jinyu Chen", "Linjiang Huang", "Shifeng Zhang", "Xu Zhou", "Liang Wang", "Si Liu"], "title": "AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation", "comment": "Accepted by ACM MM 2025", "summary": "Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables\nUnmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural\nlanguage instructions and visual cues. However, due to the extended\ntrajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN\nperformance is challenging and often requires human intervention or overly\ndetailed instructions. To harness the advantages of UAVs' high mobility, which\ncould provide multi-grained perspectives, while maintaining a manageable motion\nspace for learning, we introduce a novel task called Dual-Altitude UAV\nCollaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct\naltitudes: a high-altitude UAV responsible for broad environmental reasoning,\nand a low-altitude UAV tasked with precise navigation. To support the training\nand evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising\n13,838 collaborative high-low UAV demonstration trajectories, each paired with\ntarget-oriented language instructions. This dataset includes both unseen maps\nand an unseen object validation set to systematically evaluate the model's\ngeneralization capabilities across novel environments and unfamiliar targets.\nTo consolidate their complementary strengths, we propose a dual-UAV\ncollaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a\nmultimodal large language model (Pilot-LLM) for target reasoning, while the\nlow-altitude UAV employs a lightweight multi-stage policy for navigation and\ntarget grounding. The two UAVs work collaboratively and only exchange minimal\ncoordinate information to ensure efficiency.", "AI": {"tldr": "本文提出了一种名为DuAl-VLN的新任务，其中两个无人机在不同的高度协作完成视觉和语言导航任务，同时构建了一个包含13838条数据的训练与评测数据集HaL-13k。", "motivation": "主要动机是解决无人机在户外环境中依靠自然语言指令和视觉线索进行导航的挑战，特别是在减少人为干预和细化指令需求方面。同时，利用无人机高机动性提供多粒度视角的能力，同时保持可控的运动空间进行学习。", "method": "本研究提出了一个名为AeroDuo的双无人机协作视觉和语言导航框架。高海拔无人机使用一个大规模的多模态语言模型（Pilot-LLM）进行目标推理，而低海拔无人机采用轻量级的多阶段策略进行导航和目标定标。两架无人机协同工作并仅交换极少的坐标信息以保证效率。", "result": "尚未提供具体的结果，但文章介绍了DuAl-VLN任务和数据集HaL-13k的构建过程。框架描述了一个如何通过协作实现目标理导航的方法，为此类任务提供了可能的解决方案并展示了其潜在优势和效果。", "conclusion": "通过引入DuAl-VLN任务和提出AeroDuo框架，本研究为无人机视觉与语言导航（VLN）任务提供了一个创新的解决方案，旨在通过双无人机协作提升导航性能，并利用构建的HaL-13k数据集，评估模型在新环境和未知目标上的泛化能力。"}}
{"id": "2508.15214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15214", "abs": "https://arxiv.org/abs/2508.15214", "authors": ["Sijia Cui", "Aiyao He", "Shuai Xu", "Hongming Zhang", "Yanna Wang", "Qingyang Zhang", "Yajing Wang", "Bo Xu"], "title": "Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall", "comment": "Accepted to EMNLP 2025", "summary": "Function calling enables large language models (LLMs) to interact with\nexternal systems by leveraging tools and APIs. When faced with multi-step tool\nusage, LLMs still struggle with tool selection, parameter generation, and\ntool-chain planning. Existing methods typically rely on manually designing\ntask-specific demonstrations, or retrieving from a curated library. These\napproaches demand substantial expert effort and prompt engineering becomes\nincreasingly complex and inefficient as tool diversity and task difficulty\nscale. To address these challenges, we propose a self-guided method, Stepwise\nExperience Recall (SEER), which performs fine-grained, stepwise retrieval from\na continually updated experience pool. Instead of relying on static or manually\ncurated library, SEER incrementally augments the experience pool with past\nsuccessful trajectories, enabling continuous expansion of the pool and improved\nmodel performance over time. Evaluated on the ToolQA benchmark, SEER achieves\nan average improvement of 6.1\\% on easy and 4.7\\% on hard questions. We further\ntest SEER on $\\tau$-bench, which includes two real-world domains. Powered by\nQwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains\nof 7.44\\% and 23.38\\%, respectively.", "AI": {"tldr": "SEER improves large language models' performance in multi-step tool usage by leveraging a continually updated experience pool.", "motivation": "To solve the problem of LLMs struggling with tool selection, parameter generation, and planning in multi-step tool usage without relying on complex and inefficient manual or retrieval methods.", "method": "Stepwise Experience Recall (SEER), a self-guided method performing fine-grained, stepwise retrieval from an experience pool that expands with past successful trajectories.", "result": "SEER achieves improvements of 6.1% and 4.7% on ToolQA for easy and hard questions, respectively, and gains of 7.44% and 23.38% on real-world domains using different Qwen models.", "conclusion": "SEER demonstrates significant improvements in model performance on a variety of benchmarks, proving its effectiveness in overcoming the limitations of existing approaches."}}
{"id": "2508.15233", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15233", "abs": "https://arxiv.org/abs/2508.15233", "authors": ["Wenju Xu"], "title": "Pretrained Diffusion Models Are Inherently Skipped-Step Samplers", "comment": null, "summary": "Diffusion models have been achieving state-of-the-art results across various\ngeneration tasks. However, a notable drawback is their sequential generation\nprocess, requiring long-sequence step-by-step generation. Existing methods,\nsuch as DDIM, attempt to reduce sampling steps by constructing a class of\nnon-Markovian diffusion processes that maintain the same training objective.\nHowever, there remains a gap in understanding whether the original diffusion\nprocess can achieve the same efficiency without resorting to non-Markovian\nprocesses. In this paper, we provide a confirmative answer and introduce\nskipped-step sampling, a mechanism that bypasses multiple intermediate\ndenoising steps in the iterative generation process, in contrast with the\ntraditional step-by-step refinement of standard diffusion inference. Crucially,\nwe demonstrate that this skipped-step sampling mechanism is derived from the\nsame training objective as the standard diffusion model, indicating that\naccelerated sampling via skipped-step sampling via a Markovian way is an\nintrinsic property of pretrained diffusion models. Additionally, we propose an\nenhanced generation method by integrating our accelerated sampling technique\nwith DDIM. Extensive experiments on popular pretrained diffusion models,\nincluding the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our\nmethod achieves high-quality generation with significantly reduced sampling\nsteps.", "AI": {"tldr": "This paper proposes a skipped-step sampling method for diffusion models to improve efficiency by bypassing intermediate denoising steps. The method is shown to achieve high-quality generation with significantly fewer sampling steps and works well with popular diffusion models.", "motivation": "The motivation of this paper is to address the sequential generation process in diffusion models, which is time-consuming. It aims to demonstrate whether the original diffusion process can achieve high efficiency without resorting to non-Markovian processes.", "method": "Our method introduces skipped-step sampling, a mechanism that bypasses multiple intermediate denoising steps in the iterative generation process. This is done in contrast with the traditional step-by-step refinement of standard diffusion inference but still maintains the same training objective.", "result": "The paper demonstrates that the skipped-step sampling mechanism derived from the same training objective as the standard diffusion model performs well. The enhanced generation method, integrating skipped-step sampling with DDIM, achieves high-quality generation with significantly reduced sampling steps on popular pretrained diffusion models.", "conclusion": "The conclusion of this paper is that by using the skipped-step sampling mechanism, diffusion models can achieve high efficiency while maintaining high-quality generation. This intrinsic property of pretrained diffusion models shows promise in accelerating diffusion model inference."}}
{"id": "2508.15218", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15218", "abs": "https://arxiv.org/abs/2508.15218", "authors": ["Momoka Furuhashi", "Kouta Nakayama", "Takashi Kodama", "Saku Sugawara"], "title": "Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?", "comment": "Accepted to the EMNLP 2025 Main Conference", "summary": "Automatic evaluation of generative tasks using large language models faces\nchallenges due to ambiguous criteria. Although automatic checklist generation\nis a potentially promising approach, its usefulness remains underexplored. We\ninvestigate whether checklists should be used for all questions or selectively,\ngenerate them using six methods, evaluate their effectiveness across eight\nmodel sizes, and identify checklist items that correlate with human\nevaluations. Through experiments on pairwise comparison and direct scoring\ntasks, we find that selective checklist use tends to improve evaluation\nperformance in pairwise settings, while its benefits are less consistent in\ndirect scoring. Our analysis also shows that even checklist items with low\ncorrelation to human scores often reflect human-written criteria, indicating\npotential inconsistencies in human evaluation. These findings highlight the\nneed to more clearly define objective evaluation criteria to guide both human\nand automatic evaluations. \\footnote{Our code is available\nat~https://github.com/momo0817/checklist-effectiveness-study", "AI": {"tldr": "本研究通过实验验证了在不同模型大小下生成检查表的有效性，并发现有选择地使用检查表在某些评估任务中可以提高表现。", "motivation": "尽管自动检查表生成可能是一种有前景的方法，但其实用性尚未得到充分研究。本研究旨在解决生成任务自动评估中的不明确标准的问题。", "method": "我们研究了是否应该对所有问题使用检查表还是有选择地使用。我们使用六种方法生成检查表，并对八种不同大小的模型进行了有效性评估，同时识别了与人工评估相关的检查表项目。我们通过配对比较和直接评分任务进行了实验。", "result": "实验结果表明，在配对设置中，有选择地使用检查表可以改善评估表现。然而，在直接评分任务中，这种好处并不那么一致。此外，分析显示，即使与人类评分相关性较低的检查表项目也经常反映出人工书写的标准。", "conclusion": "本研究发现突显了需要更明确地定义客观评估标准，以指导人工和自动评估。"}}
{"id": "2508.15243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15243", "abs": "https://arxiv.org/abs/2508.15243", "authors": ["Yixin Gao", "Xin Li", "Xiaohan Pan", "Runsen Feng", "Bingchen Li", "Yunpeng Qi", "Yiting Lu", "Zhengxue Cheng", "Zhibo Chen", "Jörn Ostermann"], "title": "Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent", "comment": null, "summary": "We present Comp-X, the first intelligently interactive image compression\nparadigm empowered by the impressive reasoning capability of large language\nmodel (LLM) agent. Notably, commonly used image codecs usually suffer from\nlimited coding modes and rely on manual mode selection by engineers, making\nthem unfriendly for unprofessional users. To overcome this, we advance the\nevolution of image coding paradigm by introducing three key innovations: (i)\nmulti-functional coding framework, which unifies different coding modes of\nvarious objective/requirements, including human-machine perception, variable\ncoding, and spatial bit allocation, into one framework. (ii) interactive coding\nagent, where we propose an augmented in-context learning method with coding\nexpert feedback to teach the LLM agent how to understand the coding request,\nmode selection, and the use of the coding tools. (iii) IIC-bench, the first\ndedicated benchmark comprising diverse user requests and the corresponding\nannotations from coding experts, which is systematically designed for\nintelligently interactive image compression evaluation. Extensive experimental\nresults demonstrate that our proposed Comp-X can understand the coding requests\nefficiently and achieve impressive textual interaction capability. Meanwhile,\nit can maintain comparable compression performance even with a single coding\nframework, providing a promising avenue for artificial general intelligence\n(AGI) in image compression.", "AI": {"tldr": "Comp-X是一种由大型语言模型代理驱动的智能交互式图像压缩方法，通过多模式编码框架和交互式代理克服了传统编码器的问题，实验表明其有良好的交互能力和压缩性能。", "motivation": "传统图像编解码器通常存在编码模式有限、需手动选择模式的问题，限制了非专业用户的使用。为了克服这些问题，Comp-X引入了三个关键创新，以进化图像编码范式。", "method": "Comp-X采用多功能编码框架，交互式编码代理和IIC-bench基准测试。多功能编码框架集成了多种编码模式，满足不同目标需求；交互式编码代理通过上下文学习方法，在编码专家反馈的帮助下，使大语言模型代理理解编码请求、模式选择和编码工具的使用；IIC-bench是第一个专门针对智能交互式图像压缩评估设计的基准测试，包含多样化的用户请求和编码专家的相应注释。", "result": "实验结果表明，Comp-X可以高效理解编码请求，并展现出良好的文本交互能力。同时，它能够在单一编码框架下维持相当的压缩性能。", "conclusion": "Comp-X提供了一种智能交互式图像压缩理念，具有处理复杂编码请求的能力，并为人工智能在图像压缩领域的发展提供了一条有希望的途径。"}}
{"id": "2508.15229", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15229", "abs": "https://arxiv.org/abs/2508.15229", "authors": ["Hanling Zhang", "Yayu Zhou", "Tongcheng Fang", "Zhihang Yuan", "Guohao Dai", "Yu Wang"], "title": "VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models", "comment": null, "summary": "Small Language Models (SLMs) provide computational advantages in\nresource-constrained environments, yet memory limitations remain a critical\nbottleneck for edge device deployment. A substantial portion of SLMs' memory\nfootprint stems from vocabulary-related components, particularly embeddings and\nlanguage modeling (LM) heads, due to large vocabulary sizes. Existing static\nvocabulary pruning, while reducing memory usage, suffers from rigid,\none-size-fits-all designs that cause information loss from the prefill stage\nand a lack of flexibility. In this work, we identify two key principles\nunderlying the vocabulary reduction challenge: the lexical locality principle,\nthe observation that only a small subset of tokens is required during any\nsingle inference, and the asymmetry in computational characteristics between\nvocabulary-related components of SLM. Based on these insights, we introduce\nVocabTailor, a novel decoupled dynamic vocabulary selection framework that\naddresses memory constraints through offloading embedding and implements a\nhybrid static-dynamic vocabulary selection strategy for LM Head, enabling\non-demand loading of vocabulary components. Comprehensive experiments across\ndiverse downstream tasks demonstrate that VocabTailor achieves a reduction of\nup to 99% in the memory usage of vocabulary-related components with minimal or\nno degradation in task performance, substantially outperforming existing static\nvocabulary pruning.", "AI": {"tldr": "研究提出VocabTailor框架以解决小型语言模型（SLMs）在边缘设备上的内存问题，通过解除耦合并动态选择词汇表，减少了内存使用量。", "motivation": "现有的静态词汇表裁剪方法虽然能减少内存使用，但其设计僵化，缺乏灵活性，导致预填充阶段的信息丢失。为了克服小型语言模型（SLMs）在边缘设备部署时遇到的内存瓶颈，尤其是词汇相关的组件占用内存过大问题。", "method": "提出了一种名为VocabTailor的新框架，该框架通过解除耦合并动态选择词汇表来解决内存限制问题，具体而言，它将Embedding卸载，并为LM Head实施了一种混合的静态-动态词汇表选择策略，实现词汇表组件的按需加载。", "result": "实验结果显示，VocabTailor能在各种下游任务中将词汇相关的内存使用量减少高达99%，同时几乎不会或完全不影响任务性能，显著优于现有的静态词汇裁剪方法。", "conclusion": "该研究成功引入了一种针对SLMs的动态词汇表选择框架，有效地解决了边缘设备上内存资源的瓶颈问题，同时保持了模型性能。"}}
{"id": "2508.15256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15256", "abs": "https://arxiv.org/abs/2508.15256", "authors": ["Jinsol Song", "Jiamu Wang", "Anh Tien Nguyen", "Keunho Byeon", "Sangjeong Ahn", "Sung Hak Lee", "Jin Tae Kwak"], "title": "Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images", "comment": "Accepted at ICCV 2025. \\c{opyright} IEEE 2025. This is the author's\n  accepted version (camera-ready) of the paper. The definitive version is\n  published in the Proceedings of the IEEE/CVF International Conference on\n  Computer Vision (ICCV 2025). DOI will be updated when available", "summary": "Anomaly detection in computational pathology aims to identify rare and scarce\nanomalies where disease-related data are often limited or missing. Existing\nanomaly detection methods, primarily designed for industrial settings, face\nlimitations in pathology due to computational constraints, diverse tissue\nstructures, and lack of interpretability. To address these challenges, we\npropose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented\nVision-Language model for Anomaly detection in pathology images. Ano-NAViLa is\nbuilt on a pre-trained vision-language model with a lightweight trainable MLP.\nBy incorporating both normal and abnormal pathology knowledge, Ano-NAViLa\nenhances accuracy and robustness to variability in pathology images and\nprovides interpretability through image-text associations. Evaluated on two\nlymph node datasets from different organs, Ano-NAViLa achieves the\nstate-of-the-art performance in anomaly detection and localization,\noutperforming competing models.", "AI": {"tldr": "本文提出Ano-NAViLa模型，以改进病理图像中异常检测的准确性和解释性。", "motivation": "由于在计算病理学中用于异常检测的数据有限或缺失，现有方法无法满足要求，特别是在计算限制、复杂的组织结构和缺乏可解释性方面。", "method": "Ano-NAViLa采用了一种基于预训练的视觉语言模型，并结合了一个轻量级的可训练MLP。该模型通过整合病理学中正常的和异常的知识，提高了在病理图像中的准确性和鲁棒性，并通过图像-文本关联增强了可解释性。", "result": "在两种不同的器官淋巴结数据集上的实验表明，Ano-NAViLa模型在异常检测和定位上表现出色，达到了当前最优的性能。", "conclusion": "评估结果显示，Ano-NAViLa在淋巴结数据集上的异常检测和定位性能达到了最先进水平，超越了竞争模型。"}}
{"id": "2508.15239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15239", "abs": "https://arxiv.org/abs/2508.15239", "authors": ["Peerat Limkonchotiwat", "Pume Tuchinda", "Lalita Lowphansirikul", "Surapon Nonesung", "Panuthep Tasawong", "Alham Fikri Aji", "Can Udomcharoenchaikit", "Sarana Nutanong"], "title": "WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai", "comment": "Accepted to EMNLP 2025 (Main). Model and Dataset:\n  https://huggingface.co/collections/airesearch/wangchan-thai-instruction-6835722a30b98e01598984fd", "summary": "Large language models excel at instruction-following in English, but their\nperformance in low-resource languages like Thai remains underexplored. Existing\nbenchmarks often rely on translations, missing cultural and domain-specific\nnuances needed for real-world use. We present WangchanThaiInstruct, a\nhuman-authored Thai dataset for evaluation and instruction tuning, covering\nfour professional domains and seven task types. Created through a multi-stage\nquality control process with annotators, domain experts, and AI researchers,\nWangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing\nperformance gaps on culturally and professionally specific tasks, and (2) an\ninstruction tuning study with ablations isolating the effect of native\nsupervision. Models fine-tuned on WangchanThaiInstruct outperform those using\ntranslated data in both in-domain and out-of-domain benchmarks. These findings\nunderscore the need for culturally and professionally grounded instruction data\nto improve LLM alignment in low-resource, linguistically diverse settings.", "AI": {"tldr": "The paper presents WangchanThaiInstruct, a human-authored Thai dataset for evaluating and fine-tuning large language models in specific professional domains, highlighting the importance of culturally and professionally relevant data for improving model performance in low-resource languages.", "motivation": "To address the underexplored area of large language model performance in low-resource languages such as Thai, emphasizing the cultural and domain-specific nuances missing in translated benchmarks.", "method": "The creation of WangchanThaiInstruct, a dataset across four professional domains and seven task types, developed through a rigorous multi-stage quality control process involving annotators, domain experts, and AI researchers.", "result": "The zero-shot evaluation demonstrates performance gaps on culturally and professionally specific tasks. Instruction tuning on WangchanThaiInstruct outperforms models trained on translated data, both in-domain and out-of-domain.", "conclusion": "The findings emphasize the necessity of culturally and professionally grounded instruction datasets for better alignment of LLMs in linguistically diverse, low-resource settings."}}
{"id": "2508.15272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15272", "abs": "https://arxiv.org/abs/2508.15272", "authors": ["Han Li", "Shaofei Huang", "Longfei Xu", "Yulu Gao", "Beipeng Mu", "Si Liu"], "title": "RATopo: Improving Lane Topology Reasoning via Redundancy Assignment", "comment": "Accepted by ACM MM 2025", "summary": "Lane topology reasoning plays a critical role in autonomous driving by\nmodeling the connections among lanes and the topological relationships between\nlanes and traffic elements. Most existing methods adopt a\nfirst-detect-then-reason paradigm, where topological relationships are\nsupervised based on the one-to-one assignment results obtained during the\ndetection stage. This supervision strategy results in suboptimal topology\nreasoning performance due to the limited range of valid supervision. In this\npaper, we propose RATopo, a Redundancy Assignment strategy for lane Topology\nreasoning that enables quantity-rich and geometry-diverse topology supervision.\nSpecifically, we restructure the Transformer decoder by swapping the\ncross-attention and self-attention layers. This allows redundant lane\npredictions to be retained before suppression, enabling effective one-to-many\nassignment. We also instantiate multiple parallel cross-attention blocks with\nindependent parameters, which further enhances the diversity of detected lanes.\nExtensive experiments on OpenLane-V2 demonstrate that our RATopo strategy is\nmodel-agnostic and can be seamlessly integrated into existing topology\nreasoning frameworks, consistently improving both lane-lane and lane-traffic\ntopology performance.", "AI": {"tldr": "RATopo提出了一种冗余分配策略，以改善车道拓扑推理的性能，特别是在车道连接和与其他交通元素的关系推理方面。", "motivation": "现有的车道拓扑推理方法通常采用“先检测再推理”的策略，这种方法的监督方式导致性能不佳。因此，RATopo提出了一种新的冗余分配策略，以提高拓扑推理的性能。", "method": "RATopo采用冗余分配策略进行车道拓扑推理，重新组织了Transformer解码器，并交换了交叉注意力和自我注意力层的位置，以保留冗余的车道预测。此外，还实现了多个具有独立参数的并行交叉注意力块，进一步增强了检测车道的多样性。", "result": "在OpenLane-V2数据集上的广泛实验表明，RATopo策略能显著提高车道-车道和车道-交通元素拓扑推理的性能。", "conclusion": "RATopo是一种模型无关的方法，可以轻松集成到现有的车道拓扑推理框架中，增强离散车道和几何多样性，提高车道拓扑推理性能。"}}
{"id": "2508.15244", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15244", "abs": "https://arxiv.org/abs/2508.15244", "authors": ["Sangmin Lee", "Woojin Chung", "Seyun Um", "Hong-Goo Kang"], "title": "UniCoM: A Universal Code-Switching Speech Generator", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Code-switching (CS), the alternation between two or more languages within a\nsingle speaker's utterances, is common in real-world conversations and poses\nsignificant challenges for multilingual speech technology. However, systems\ncapable of handling this phenomenon remain underexplored, primarily due to the\nscarcity of suitable datasets. To resolve this issue, we propose Universal\nCode-Mixer (UniCoM), a novel pipeline for generating high-quality, natural CS\nsamples without altering sentence semantics. Our approach utilizes an algorithm\nwe call Substituting WORDs with Synonyms (SWORDS), which generates CS speech by\nreplacing selected words with their translations while considering their parts\nof speech. Using UniCoM, we construct Code-Switching FLEURS (CS-FLEURS), a\nmultilingual CS corpus designed for automatic speech recognition (ASR) and\nspeech-to-text translation (S2TT). Experimental results show that CS-FLEURS\nachieves high intelligibility and naturalness, performing comparably to\nexisting datasets on both objective and subjective metrics. We expect our\napproach to advance CS speech technology and enable more inclusive multilingual\nsystems.", "AI": {"tldr": "研究提出了一种名为UniCoM的生成高质量自然CS样本的管道，构建了CS-FLEURS多语言CS语料库，并验证了其在ASR和S2TT上的有效性和自然度。", "motivation": "解决由于适合的数据集稀缺而导致的处理CS现象的系统被较少探索的问题，并推动CS语音技术的发展，以及促进更具包容性的多语言系统的形成。", "method": "提出了一种名为Substituting WORDs with Synonyms (SWORDS) 的算法，通过替换选定单词的翻译（同时考虑词性）来生成CS语音，以构建高质量且自然的CS样本而不改变句子的语义。", "result": "实验结果显示，CS-FLEURS在客观和主观评测指标上的表现与现有数据集相比具有较高的可理解性和自然度。", "conclusion": "研究指出，提出的UniCoM管道和生成的CS-FLEURS多语言CS语料库有望促进CS语音技术的进步并支持更多的多语言系统开发。"}}
{"id": "2508.15297", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15297", "abs": "https://arxiv.org/abs/2508.15297", "authors": ["Zhu Wang", "Homaira Huda Shomee", "Sathya N. Ravi", "Sourav Medya"], "title": "DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding", "comment": "Accepted by EMNLP 2025. 22 pages, 14 figures", "summary": "In the field of design patent analysis, traditional tasks such as patent\nclassification and patent image retrieval heavily depend on the image data.\nHowever, patent images -- typically consisting of sketches with abstract and\nstructural elements of an invention -- often fall short in conveying\ncomprehensive visual context and semantic information. This inadequacy can lead\nto ambiguities in evaluation during prior art searches. Recent advancements in\nvision-language models, such as CLIP, offer promising opportunities for more\nreliable and accurate AI-driven patent analysis. In this work, we leverage CLIP\nmodels to develop a unified framework DesignCLIP for design patent applications\nwith a large-scale dataset of U.S. design patents. To address the unique\ncharacteristics of patent data, DesignCLIP incorporates class-aware\nclassification and contrastive learning, utilizing generated detailed captions\nfor patent images and multi-views image learning. We validate the effectiveness\nof DesignCLIP across various downstream tasks, including patent classification\nand patent retrieval. Additionally, we explore multimodal patent retrieval,\nwhich provides the potential to enhance creativity and innovation in design by\noffering more diverse sources of inspiration. Our experiments show that\nDesignCLIP consistently outperforms baseline and SOTA models in the patent\ndomain on all tasks. Our findings underscore the promise of multimodal\napproaches in advancing patent analysis. The codebase is available here:\nhttps://anonymous.4open.science/r/PATENTCLIP-4661/README.md.", "AI": {"tldr": "本研究提出DesignCLIP框架，通过视觉-语言模型CLIP和大型美国设计专利数据库，改进设计专利分析，涵盖图像分类和检索任务。", "motivation": "传统设计专利分析任务依赖图像数据，但专利图像在传达全面的视觉上下文和语义信息方面存在不足，这可能导致先前技术搜索中的不确定性。CLIP等视觉-语言模型的进步为更可靠准确的人工智能驱动的专利分析提供了机会。", "method": "DesignCLIP框架利用CLIP模型处理设计专利分析，包括图像分类和检索。该框架利用专利图像生成的详细说明和多视角图像学习，采用类别感知分类和对比学习来处理专利数据的独特特征。", "result": "实验结果显示DesignCLIP在专利分类和检索任务上优于基线和当前最佳模型。", "conclusion": "我们的研究强调多模态方法在推动专利分析进步方面的潜力，并提供了代码库供参考。"}}
{"id": "2508.15250", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15250", "abs": "https://arxiv.org/abs/2508.15250", "authors": ["Yilin Jiang", "Mingzi Zhang", "Sheng Jin", "Zengyi Yu", "Xiangjie Kong", "Binghao Tu"], "title": "EMNLP: Educator-role Moral and Normative Large Language Models Profiling", "comment": "24pages, 12 figures, Accepted by EMNLP Main Confrence", "summary": "Simulating Professions (SP) enables Large Language Models (LLMs) to emulate\nprofessional roles. However, comprehensive psychological and ethical evaluation\nin these contexts remains lacking. This paper introduces EMNLP, an\nEducator-role Moral and Normative LLMs Profiling framework for personality\nprofiling, moral development stage measurement, and ethical risk under soft\nprompt injection. EMNLP extends existing scales and constructs 88\nteacher-specific moral dilemmas, enabling profession-oriented comparison with\nhuman teachers. A targeted soft prompt injection set evaluates compliance and\nvulnerability in teacher SP. Experiments on 12 LLMs show teacher-role LLMs\nexhibit more idealized and polarized personalities than human teachers, excel\nin abstract moral reasoning, but struggle with emotionally complex situations.\nModels with stronger reasoning are more vulnerable to harmful prompt injection,\nrevealing a paradox between capability and safety. The model temperature and\nother hyperparameters have limited influence except in some risk behaviors.\nThis paper presents the first benchmark to assess ethical and psychological\nalignment of teacher-role LLMs for educational AI. Resources are available at\nhttps://e-m-n-l-p.github.io/.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.15298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15298", "abs": "https://arxiv.org/abs/2508.15298", "authors": ["Darya Taratynova", "Alya Almsouti", "Beknur Kalmakhanbet", "Numan Saeed", "Mohammad Yaqub"], "title": "TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification", "comment": null, "summary": "Congenital heart defect (CHD) detection in ultrasound videos is hindered by\nimage noise and probe positioning variability. While automated methods can\nreduce operator dependence, current machine learning approaches often neglect\ntemporal information, limit themselves to binary classification, and do not\naccount for prediction calibration. We propose Temporal Prompt Alignment (TPA),\na method leveraging foundation image-text model and prompt-aware contrastive\nlearning to classify fetal CHD on cardiac ultrasound videos. TPA extracts\nfeatures from each frame of video subclips using an image encoder, aggregates\nthem with a trainable temporal extractor to capture heart motion, and aligns\nthe video representation with class-specific text prompts via a margin-hinge\ncontrastive loss. To enhance calibration for clinical reliability, we introduce\na Conditional Variational Autoencoder Style Modulation (CVAESM) module, which\nlearns a latent style vector to modulate embeddings and quantifies\nclassification uncertainty. Evaluated on a private dataset for CHD detection\nand on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA\nachieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while\nalso reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On\nEchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to\n58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital\nheart defect (CHD) classification in ultrasound videos that integrates temporal\nmodeling, prompt-aware contrastive learning, and uncertainty quantification.", "AI": {"tldr": "本文提出了一种名为Temporal Prompt Alignment (TPA)的方法，用于胎儿先天性心脏病的超声视频分类。TPA结合了基础图像文本模型、prompt感知对比学习和不确定性量化，可以提高分类精度并改善预测校准。在两个数据集上的评估显示，TPA在先天性心脏病诊断上达到了85.40%的宏观F1分数，并且大幅降低了预期校准误差。", "motivation": "在利用机器学习进行先天性心脏病(CHD)检测时，自动化方法可以减少对操作者的依赖，但现有方法往往忽略时间信息，局限于二元分类，并不考虑预测校准。为了改进这一点，研究提出了一种新的方法。", "method": "Temporal Prompt Alignment (TPA)提出了一种结合基础图像文本模型和prompt感知对比学习的方法，用于对胎儿先天性心脏病的超声视频进行分类。TPA从视频子片段的每一帧中提取特征，并使用可训练的时间提取器聚合这些特征来捕捉心脏运动，通过一个边缘铰链对比损失将视频表示与类别特定的文本提示对齐。此外，引入了一个条件变分自编码器风格调制(CVAESM)模块，学习一个潜在风格向量来调制嵌入，并量化分类不确定性。", "result": "在私有数据集和大型公开数据集EchoNet-Dynamic上的评估显示，TPA在先天性心脏病检测中达到了85.40%的宏观F1分数，并降低了预期校准误差（ECE）和自适应ECE。在EchoNet-Dynamic的三分类任务中，TPA提升了4.73%的宏观F1分数。", "conclusion": "Temporal Prompt Alignment (TPA)是一个用于胎儿先天性心脏病超声视频分类的框架，它整合了时间建模、prompt感知对比学习和不确定性量化。实验表明，TPA在提高分类精度的同时，也提升了预测校准的可靠性。"}}
{"id": "2508.15253", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15253", "abs": "https://arxiv.org/abs/2508.15253", "authors": ["Eunseong Choi", "June Park", "Hyeri Lee", "Jongwuk Lee"], "title": "Conflict-Aware Soft Prompting for Retrieval-Augmented Generation", "comment": "Accepted to EMNLP 2025; 14 pages; 5 figures, 11 tables", "summary": "Retrieval-augmented generation (RAG) enhances the capabilities of large\nlanguage models (LLMs) by incorporating external knowledge into their input\nprompts. However, when the retrieved context contradicts the LLM's parametric\nknowledge, it often fails to resolve the conflict between incorrect external\ncontext and correct parametric knowledge, known as context-memory conflict. To\ntackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation\n(CARE), consisting of a context assessor and a base LLM. The context assessor\nencodes compact memory token embeddings from raw context tokens. Through\ngrounded/adversarial soft prompting, the context assessor is trained to discern\nunreliable context and capture a guidance signal that directs reasoning toward\nthe more reliable knowledge source. Extensive experiments show that CARE\neffectively mitigates context-memory conflicts, leading to an average\nperformance gain of 5.0\\% on QA and fact-checking benchmarks, establishing a\npromising direction for trustworthy and adaptive RAG systems.", "AI": {"tldr": "本文介绍了一种新的方法CARE，通过一种上下文评估机制来解决在RAG中产生的上下文-记忆冲突问题，实验显示该方法在问答和事实核查基准上的性能有显著提升。", "motivation": "解决检索增强生成(RAG)系统中外部检索上下文与内置知识冲突的问题，称为上下文-记忆冲突。", "method": "提出了一种名为CARE的方法，包括一个上下文评估器和一个基础的LLM。上下文评估器从原始上下文标记中编码紧凑的记忆标记嵌入。通过基于软提示的方式，上下文评估器被训练来辨别不可靠的上下文，并捕获一个指导信号，引导推理朝向更可靠的知识来源。", "result": "实验结果表明，CARE在问答和事实核查基准上平均提升了5.0%的性能。", "conclusion": "CARE有效缓解了上下文-记忆冲突，为可信和自适应的检索增强生成(RAG)系统提供了一个有前景的方向。"}}
{"id": "2508.15299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15299", "abs": "https://arxiv.org/abs/2508.15299", "authors": ["Ryunosuke Hayashi", "Kohei Torimi", "Rokuto Nagata", "Kazuma Ikeda", "Ozora Sako", "Taichi Nakamura", "Masaki Tani", "Yoshimitsu Aoki", "Kentaro Yoshioka"], "title": "BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT", "comment": "Accepted to MMSports", "summary": "Real-time 3D trajectory player tracking in sports plays a crucial role in\ntactical analysis, performance evaluation, and enhancing spectator experience.\nTraditional systems rely on multi-camera setups, but are constrained by the\ninherently two-dimensional nature of video data and the need for complex 3D\nreconstruction processing, making real-time analysis challenging. Basketball,\nin particular, represents one of the most difficult scenarios in the MOT field,\nas ten players move rapidly and complexly within a confined court space, with\nfrequent occlusions caused by intense physical contact.\n  To address these challenges, this paper constructs BasketLiDAR, the first\nmultimodal dataset in the sports MOT field that combines LiDAR point clouds\nwith synchronized multi-view camera footage in a professional basketball\nenvironment, and proposes a novel MOT framework that simultaneously achieves\nimproved tracking accuracy and reduced computational cost. The BasketLiDAR\ndataset contains a total of 4,445 frames and 3,105 player IDs, with fully\nsynchronized IDs between three LiDAR sensors and three multi-view cameras. We\nrecorded 5-on-5 and 3-on-3 game data from actual professional basketball\nplayers, providing complete 3D positional information and ID annotations for\neach player. Based on this dataset, we developed a novel MOT algorithm that\nleverages LiDAR's high-precision 3D spatial information. The proposed method\nconsists of a real-time tracking pipeline using LiDAR alone and a multimodal\ntracking pipeline that fuses LiDAR and camera data. Experimental results\ndemonstrate that our approach achieves real-time operation, which was difficult\nwith conventional camera-only methods, while achieving superior tracking\nperformance even under occlusion conditions. The dataset is available upon\nrequest at: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.15274", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15274", "abs": "https://arxiv.org/abs/2508.15274", "authors": ["Lekshmi R Nair", "Arun Sankar", "Koninika Pal"], "title": "TComQA: Extracting Temporal Commonsense from Text", "comment": null, "summary": "Understanding events necessitates grasping their temporal context, which is\noften not explicitly stated in natural language. For example, it is not a\ntrivial task for a machine to infer that a museum tour may last for a few\nhours, but can not take months. Recent studies indicate that even advanced\nlarge language models (LLMs) struggle in generating text that require reasoning\nwith temporal commonsense due to its infrequent explicit mention in text.\nTherefore, automatically mining temporal commonsense for events enables the\ncreation of robust language models. In this work, we investigate the capacity\nof LLMs to extract temporal commonsense from text and evaluate multiple\nexperimental setups to assess their effectiveness. Here, we propose a temporal\ncommonsense extraction pipeline that leverages LLMs to automatically mine\ntemporal commonsense and use it to construct TComQA, a dataset derived from\nSAMSum and RealNews corpora. TComQA has been validated through crowdsourcing\nand achieves over 80\\% precision in extracting temporal commonsense. The model\ntrained with TComQA also outperforms an LLM fine-tuned on existing dataset of\ntemporal question answering task.", "AI": {"tldr": "本文利用大型语言模型（LLMs）自动挖掘事件时间常识，构建了TComQA数据集，并验证了其在时间常识提取上的有效性，显著提高了模型在时间问答任务上的表现。", "motivation": "大型语言模型在处理需要时间常识推理的任务时存在困难，因为文本中对时间常识的明确提及较少。因此，自动挖掘事件的时间常识对于创建更加健壮的语言模型至关重要。", "method": "本文提出了一种利用大型语言模型（LLMs）自动挖掘事件时间常识的流水线方法，并使用该方法构建了一个基于SAMSum和RealNews语料库的TComQA数据集。", "result": "TComQA数据集通过众包验证，其在提取时间常识方面达到了80%以上的精度。使用TComQA训练的模型在时间问答任务上优于现有数据集微调的大型语言模型。", "conclusion": "研究结果表明，利用LLMs挖掘事件的时间常识并构建数据集的有效性，能够改善语言模型处理时间相关的任务能力。"}}
{"id": "2508.15313", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15313", "abs": "https://arxiv.org/abs/2508.15313", "authors": ["Wutao Liu", "YiDan Wang", "Pan Gao"], "title": "First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection", "comment": null, "summary": "Camouflaged object detection (COD) poses a significant challenge in computer\nvision due to the high similarity between objects and their backgrounds.\nExisting approaches often rely on heavy training and large computational\nresources. While foundation models such as the Segment Anything Model (SAM)\noffer strong generalization, they still struggle to handle COD tasks without\nfine-tuning and require high-quality prompts to yield good performance.\nHowever, generating such prompts manually is costly and inefficient. To address\nthese challenges, we propose \\textbf{First RAG, Second SEG (RAG-SEG)}, a\ntraining-free paradigm that decouples COD into two stages: Retrieval-Augmented\nGeneration (RAG) for generating coarse masks as prompts, followed by SAM-based\nsegmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval\ndatabase via unsupervised clustering, enabling fast and effective feature\nretrieval. During inference, the retrieved features produce pseudo-labels that\nguide precise mask generation using SAM2. Our method eliminates the need for\nconventional training while maintaining competitive performance. Extensive\nexperiments on benchmark COD datasets demonstrate that RAG-SEG performs on par\nwith or surpasses state-of-the-art methods. Notably, all experiments are\nconducted on a \\textbf{personal laptop}, highlighting the computational\nefficiency and practicality of our approach. We present further analysis in the\nAppendix, covering limitations, salient object detection extension, and\npossible improvements.", "AI": {"tldr": "Introduces RAG-SEG, a training-free, two-stage COD method that outperforms or matches SOTA methods and operates on a personal laptop.", "motivation": "To address the challenge of COD, which is hard due to the similarity between objects and their backgrounds, and reduce the reliance on heavy training and computational resources.", "method": "First RAG, Second SEG (RAG-SEG), a training-free method that divides Camouflaged Object Detection (COD) into two stages: RAG for generating coarse prompts and SEG for refining using SAM.", "result": "Competitive performance on COD tasks, comparable or better than state-of-the-art methods, achieved without traditional training.", "conclusion": "RAG-SEG offers computational efficiency and practicality, demonstrated by its effectiveness when run on a personal laptop."}}
{"id": "2508.15316", "categories": ["cs.CL", "cs.LG", "eess.AS", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15316", "abs": "https://arxiv.org/abs/2508.15316", "authors": ["Abdul Rehman", "Jian-Jun Zhang", "Xiaosong Yang"], "title": "CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing", "comment": "Accepted in: 8th International Conference on Natural Language and\n  Speech Processing (ICNLSP 2025)", "summary": "Universal phoneme recognition typically requires analyzing long speech\nsegments and language-specific patterns. Many speech processing tasks require\npure phoneme representations free from contextual influence, which motivated\nour development of CUPE - a lightweight model that captures key phoneme\nfeatures in just 120 milliseconds, about one phoneme's length. CUPE processes\nshort, fixed-width windows independently and, despite fewer parameters than\ncurrent approaches, achieves competitive cross-lingual performance by learning\nfundamental acoustic patterns common to all languages. Our extensive evaluation\nthrough supervised and self-supervised training on diverse languages, including\nzero-shot tests on the UCLA Phonetic Corpus, demonstrates strong cross-lingual\ngeneralization and reveals that effective universal speech processing is\npossible through modeling basic acoustic patterns within phoneme-length\nwindows.", "AI": {"tldr": "CUPE模型实现了在短时窗口内独立捕捉音素特征，在多种语言上实现了跨语言性能，证明了通过建模基本声学模式进行通用语音处理的可能性。", "motivation": "许多语音处理任务需要纯粹的音素表示，不受到上下文的影响，这促使了我们开发CUPE模型。", "method": "开发了CUPE模型，这是一种轻量级模型，能在120毫秒内捕捉关键音素特征，处理短的、固定宽度的窗口，独立于上下文影响。", "result": "尽管参数较少，但CUPE在多种语言上的监督学习和自我监督学习中实现了有竞争力的跨语言性能，特别在零样本测试中表现出了强大的跨语言泛化能力。", "conclusion": "研究表明，通过建模音素长度窗口内的基本声学模式，实现了有效的通用语音处理。"}}
{"id": "2508.15314", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15314", "abs": "https://arxiv.org/abs/2508.15314", "authors": ["Naen Xu", "Jinghuai Zhang", "Changjiang Li", "Zhi Chen", "Chunyi Zhou", "Qingming Li", "Tianyu Du", "Shouling Ji"], "title": "VideoEraser: Concept Erasure in Text-to-Video Diffusion Models", "comment": "To appear in the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP)", "summary": "The rapid growth of text-to-video (T2V) diffusion models has raised concerns\nabout privacy, copyright, and safety due to their potential misuse in\ngenerating harmful or misleading content. These models are often trained on\nnumerous datasets, including unauthorized personal identities, artistic\ncreations, and harmful materials, which can lead to uncontrolled production and\ndistribution of such content. To address this, we propose VideoEraser, a\ntraining-free framework that prevents T2V diffusion models from generating\nvideos with undesirable concepts, even when explicitly prompted with those\nconcepts. Designed as a plug-and-play module, VideoEraser can seamlessly\nintegrate with representative T2V diffusion models via a two-stage process:\nSelective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise\nGuidance (ARNG). We conduct extensive evaluations across four tasks, including\nobject erasure, artistic style erasure, celebrity erasure, and explicit content\nerasure. Experimental results show that VideoEraser consistently outperforms\nprior methods regarding efficacy, integrity, fidelity, robustness, and\ngeneralizability. Notably, VideoEraser achieves state-of-the-art performance in\nsuppressing undesirable content during T2V generation, reducing it by 46% on\naverage across four tasks compared to baselines.", "AI": {"tldr": "本文提出了VideoEraser，一种训练自由的框架，旨在防止文本-视频扩散模型生成带有不需要概念的视频，并在多种任务上进行了验证，取得了优于之前方法的结果。", "motivation": "文本到视频扩散模型快速增长引发关于隐私、版权和安全方面的问题，因为这些模型被滥用生成有害或误导的内容。VideoEraser的设计目的是解决这些问题。", "method": "Selective Prompt Embedding Adjustment (SPEA) 和 Adversarial-Resilient Noise Guidance (ARNG) 两阶段过程设计的VideoEraser框架，以防止T2V扩散模型生成带有不需要概念的视频，即使被明确提示这些概念也有效。", "result": "实验结果证明，VideoEraser在四类任务（物体擦除、艺术风格擦除、名人擦除和显式内容擦除）上实现状态一流的对不需要内容生成的抑制，将不需要内容降低46%。", "conclusion": "实验结果显示，VideoEraser在效能、完整性、保真度、鲁棒性和泛化性方面都优于先前方法。与基线相比，VideoEraser在四类任务中平均减少了46%的不需要内容的生成，达到最先进的性能。"}}
{"id": "2508.15357", "categories": ["cs.CL", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.15357", "abs": "https://arxiv.org/abs/2508.15357", "authors": ["Haji Gul", "Abul Ghani Naim", "Ajaz Ahmad Bhat"], "title": "KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models", "comment": null, "summary": "Knowledge Graphs (KGs) enable applications in various domains such as\nsemantic search, recommendation systems, and natural language processing. KGs\nare often incomplete, missing entities and relations, an issue addressed by\nKnowledge Graph Completion (KGC) methods that predict missing elements.\nDifferent evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank\n(MR), and Hit@k, are commonly used to assess the performance of such KGC\nmodels. A major challenge in evaluating KGC models, however, lies in comparing\ntheir performance across multiple datasets and metrics. A model may outperform\nothers on one dataset but underperform on another, making it difficult to\ndetermine overall superiority. Moreover, even within a single dataset,\ndifferent metrics such as MRR and Hit@1 can yield conflicting rankings, where\none model excels in MRR while another performs better in Hit@1, further\ncomplicating model selection for downstream tasks. These inconsistencies hinder\nholistic comparisons and highlight the need for a unified meta-metric that\nintegrates performance across all metrics and datasets to enable a more\nreliable and interpretable evaluation framework. To address this need, we\npropose KG Evaluation based on Distance from Average Solution (EDAS), a robust\nand interpretable meta-metric that synthesizes model performance across\nmultiple datasets and diverse evaluation criteria into a single normalized\nscore ($M_i \\in [0,1]$). Unlike traditional metrics that focus on isolated\naspects of performance, EDAS offers a global perspective that supports more\ninformed model selection and promotes fairness in cross-dataset evaluation.\nExperimental results on benchmark datasets such as FB15k-237 and WN18RR\ndemonstrate that EDAS effectively integrates multi-metric, multi-dataset\nperformance into a unified ranking, offering a consistent, robust, and\ngeneralizable framework for evaluating KGC models.", "AI": {"tldr": "提出基于距离平均解的KG评估法（EDAS）作为元指标，以整合多指标和多数据集的表现，提供更系统、可靠和可解释性高的评估框架，实验验证了其有效性和稳健性。", "motivation": "解决现有KGC模型在不同数据集和指标上评估时表现不一致的问题，提出一个统一且可解释的评估方法，以支持更有效的模型选择和公平的跨数据集评估。", "method": "提出基于距离平均解的评估方法（EDAS），此方法能将模型在多个数据集和不同评价指标上的表现整合成单一标准化得分，以提供全面和可解释的评估。", "result": "实验结果显示EDAS能够将多指标、多数据集的表现整合成一个统一的排名，提供了一种一致、稳健且通用的KGC模型评估框架，并展示了其有效性。", "conclusion": "EDAS提供了一个新的和全面的评估框架，提高了KGC模型选择的公平性和可靠性，通过整合复杂的表现指标和跨数据集的表现，使其成为一种更为通用和一致的评估方式。"}}
{"id": "2508.15336", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15336", "abs": "https://arxiv.org/abs/2508.15336", "authors": ["Subhasis Dasgupta", "Preetam Saha", "Agniva Roy", "Jaydip Sen"], "title": "Predicting Road Crossing Behaviour using Pose Detection and Sequence Modelling", "comment": "This is a pre-print version of the original paper accepted in the\n  IEEE conference INDISCON 2025. It contains 8 figures and 1 table. The length\n  of the paper is 7 pages", "summary": "The world is constantly moving towards AI based systems and autonomous\nvehicles are now reality in different parts of the world. These vehicles\nrequire sensors and cameras to detect objects and maneuver according to that.\nIt becomes important to for such vehicles to also predict from a distant if a\nperson is about to cross a road or not. The current study focused on predicting\nthe intent of crossing the road by pedestrians in an experimental setup. The\nstudy involved working with deep learning models to predict poses and sequence\nmodelling for temporal predictions. The study analysed three different sequence\nmodelling to understand the prediction behaviour and it was found out that GRU\nwas better in predicting the intent compared to LSTM model but 1D CNN was the\nbest model in terms of speed. The study involved video analysis, and the output\nof pose detection model was integrated later on to sequence modelling\ntechniques for an end-to-end deep learning framework for predicting road\ncrossing intents.", "AI": {"tldr": "The study uses deep learning techniques to predict pedestrians' intent to cross roads, finding GRU superior for accuracy and 1D CNN for speed, in support of enhancing autonomous vehicle safety.", "motivation": "The motivation of the study is to improve autonomous vehicle safety by enabling the prediction of pedestrians' intention to cross a road from a distance.", "method": "The study used deep learning models for pose detection and sequence modeling for temporal predictions to predict pedestrians' intent to cross the road. Three different sequence modeling techniques were analyzed, including GRU, LSTM, and 1D CNN.", "result": "The study found that the GRU model was superior to LSTM in predicting pedestrians' intent to cross a road, while the 1D CNN model was the fastest among the models evaluated.", "conclusion": "The research developed an end-to-end deep learning framework integrating video analysis with various sequence models, achieving better prediction performance compared to existing methods, particularly with faster prediction times using 1D CNN models."}}
{"id": "2508.15361", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15361", "abs": "https://arxiv.org/abs/2508.15361", "authors": ["Shiwen Ni", "Guhong Chen", "Shuaimin Li", "Xuanang Chen", "Siyi Li", "Bingli Wang", "Qiyao Wang", "Xingjian Wang", "Yifan Zhang", "Liyang Fan", "Chengming Li", "Ruifeng Xu", "Le Sun", "Min Yang"], "title": "A Survey on Large Language Model Benchmarks", "comment": null, "summary": "In recent years, with the rapid development of the depth and breadth of large\nlanguage models' capabilities, various corresponding evaluation benchmarks have\nbeen emerging in increasing numbers. As a quantitative assessment tool for\nmodel performance, benchmarks are not only a core means to measure model\ncapabilities but also a key element in guiding the direction of model\ndevelopment and promoting technological innovation. We systematically review\nthe current status and development of large language model benchmarks for the\nfirst time, categorizing 283 representative benchmarks into three categories:\ngeneral capabilities, domain-specific, and target-specific. General capability\nbenchmarks cover aspects such as core linguistics, knowledge, and reasoning;\ndomain-specific benchmarks focus on fields like natural sciences, humanities\nand social sciences, and engineering technology; target-specific benchmarks pay\nattention to risks, reliability, agents, etc. We point out that current\nbenchmarks have problems such as inflated scores caused by data contamination,\nunfair evaluation due to cultural and linguistic biases, and lack of evaluation\non process credibility and dynamic environments, and provide a referable design\nparadigm for future benchmark innovation.", "AI": {"tldr": "本研究回顾了大型语言模型基准测试的现状和发展，对存在的问题进行了分析并提出了基准创新的设计范式。", "motivation": "研究动机在于系统性地分析当前大型语言模型基准测试的状况，指出其存在的问题，并为未来的基准创新提供设计思路。", "method": "通过对283个代表性基准测试进行分类和分析，并梳理通用能力、领域特定和目标特定的基准测试，指出现有问题并提出解决框架。", "result": "本研究首次系统回顾了大型语言模型基准测试的现状和发展，将283个代表性基准测试分为通用能力、领域特定和目标特定三类。通用能力基准测试涵盖了核心语言学、知识和推理等方面；领域特定基准测试专注于自然科学、人文及社会科学和工程技术等；目标特定基准测试侧重于风险、可靠性和代理等方面。研究指出当前基准测试存在一些问题，如数据污染导致的分数膨胀、文化和语言偏差导致的不公平评估以及缺乏对过程可信性和动态环境的评估，并提出了未来基准设计的参考范式。", "conclusion": "未来基准测试需要解决数据污染、文化和语言偏差以及缺乏对过程可信性和动态环境评估的问题，并提出了设计创新基准测试的参考框架。"}}
{"id": "2508.15353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15353", "abs": "https://arxiv.org/abs/2508.15353", "authors": ["Olga Matykina", "Dmitry Yudin"], "title": "RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features", "comment": "Accepted for publication in Optical Memory and Neural Networks, 2025", "summary": "Three-dimensional object detection is essential for autonomous driving and\nrobotics, relying on effective fusion of multimodal data from cameras and\nradar. This work proposes RCDINO, a multimodal transformer-based model that\nenhances visual backbone features by fusing them with semantically rich\nrepresentations from the pretrained DINOv2 foundation model. This approach\nenriches visual representations and improves the model's detection performance\nwhile preserving compatibility with the baseline architecture. Experiments on\nthe nuScenes dataset demonstrate that RCDINO achieves state-of-the-art\nperformance among radar-camera models, with 56.4 NDS and 48.1 mAP. Our\nimplementation is available at https://github.com/OlgaMatykina/RCDINO.", "AI": {"tldr": "RCDINO是一种基于多模态变压器的模型，该模型融合了相机和雷达的数据，用于提高三维物体检测的性能，并在nuScenes数据集上取得了优异的结果。", "motivation": "该工作的动机是提高基于相机和雷达的多模态数据融合的有效性，以增强自动驾驶和机器人技术中的三维物体检测。", "method": "本文提出了RCDINO模型，该模型基于多模态变压器，通过融合来自预训练DINOv2基础模型的语义丰富的表示来增强视觉骨干特征，从而改善三维物体检测性能。", "result": "在nuScenes数据集上的实验表明，RCDINO在雷达-摄像机模型中达到了最先进的性能，具体表现为56.4的NDS和48.1的mAP。", "conclusion": "RCDINO证明了通过融合视觉和语义信息来增强三维物体检测的优越性，并且保持了与基线架构的兼容性。"}}
{"id": "2508.15370", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15370", "abs": "https://arxiv.org/abs/2508.15370", "authors": ["Yichi Zhang", "Yao Huang", "Yifan Wang", "Yitong Sun", "Chang Liu", "Zhe Zhao", "Zhengwei Fang", "Huanran Chen", "Xiao Yang", "Xingxing Wei", "Hang Su", "Yinpeng Dong", "Jun Zhu"], "title": "Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation", "comment": "For Appendix, please refer to arXiv:2406.07057", "summary": "The trustworthiness of Multimodal Large Language Models (MLLMs) remains an\nintense concern despite the significant progress in their capabilities.\nExisting evaluation and mitigation approaches often focus on narrow aspects and\noverlook risks introduced by the multimodality. To tackle these challenges, we\npropose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and\nmitigating the trustworthiness issues of MLLMs. We define a three-dimensional\nframework, encompassing five trustworthiness aspects which include\ntruthfulness, robustness, safety, fairness, and privacy; two novel risk types\ncovering multimodal risks and cross-modal impacts; and various mitigation\nstrategies from the perspectives of data, model architecture, training, and\ninference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and\n28 curated datasets, enabling holistic evaluations over 30 open-source and\nproprietary MLLMs and in-depth analysis with 8 representative mitigation\nmethods. Our extensive experiments reveal significant vulnerabilities in\ncurrent models, including a gap between trustworthiness and general\ncapabilities, as well as the amplification of potential risks in base LLMs by\nboth multimodal training and inference. Moreover, our controlled analysis\nuncovers key limitations in existing mitigation strategies that, while some\nmethods yield improvements in specific aspects, few effectively address overall\ntrustworthiness, and many introduce unexpected trade-offs that compromise model\nutility. These findings also provide practical insights for future\nimprovements, such as the benefits of reasoning to better balance safety and\nperformance. Based on these insights, we introduce a Reasoning-Enhanced Safety\nAlignment (RESA) approach that equips the model with chain-of-thought reasoning\nability to discover the underlying risks, achieving state-of-the-art results.", "AI": {"tldr": "研究提出MultiTrust-X用于评估和缓解MLLMs的可信性问题。", "motivation": "尽管多模态大语言模型的能力得到了显著提升，但其可信性仍然是一个主要问题。现有的评估和缓解方法往往只关注狭窄的方面，并忽视了多模态引入的风险。", "method": "我们提出了MultiTrust-X，一个全面的基准，用于评估、分析和缓解MLLMs的可信性问题。MultiTrust-X定义了一个三维框架，涵盖五个可信性方面，两种新型风险类型，以及多个缓解策略。", "result": "实验揭示了现有模型的显著漏洞，包括可信性和通用能力之间的差距，以及多模态训练和推理对基础大模型潜在风险的放大。", "conclusion": "研究发现现有的缓解策略在某些方面取得改进，但很难综合地解决可信性问题，且会引入意料之外的权衡，这些发现在未来改进提供了实用见解。提出了一种带有链式思维推理能力的RESA方法，实现了最新结果。"}}
{"id": "2508.15360", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15360", "abs": "https://arxiv.org/abs/2508.15360", "authors": ["Chenhui Gou", "Ziyu Ma", "Zicheng Duan", "Haoyu He", "Feng Chen", "Akide Liu", "Bohan Zhuang", "Jianfei Cai", "Hamid Rezatofighi"], "title": "An Empirical Study on How Video-LLMs Answer Video Questions", "comment": null, "summary": "Taking advantage of large-scale data and pretrained language models, Video\nLarge Language Models (Video-LLMs) have shown strong capabilities in answering\nvideo questions. However, most existing efforts focus on improving performance,\nwith limited attention to understanding their internal mechanisms. This paper\naims to bridge this gap through a systematic empirical study. To interpret\nexisting VideoLLMs, we adopt attention knockouts as our primary analytical tool\nand design three variants: Video Temporal Knockout, Video Spatial Knockout, and\nLanguage-to-Video Knockout. Then, we apply these three knockouts on different\nnumbers of layers (window of layers). By carefully controlling the window of\nlayers and types of knockouts, we provide two settings: a global setting and a\nfine-grained setting. Our study reveals three key findings: (1) Global setting\nindicates Video information extraction primarily occurs in early layers,\nforming a clear two-stage process -- lower layers focus on perceptual encoding,\nwhile higher layers handle abstract reasoning; (2) In the fine-grained setting,\ncertain intermediate layers exert an outsized impact on video question\nanswering, acting as critical outliers, whereas most other layers contribute\nminimally; (3) In both settings, we observe that spatial-temporal modeling\nrelies more on language-guided retrieval than on intra- and inter-frame\nself-attention among video tokens, despite the latter's high computational\ncost. Finally, we demonstrate that these insights can be leveraged to reduce\nattention computation in Video-LLMs. To our knowledge, this is the first work\nto systematically uncover how Video-LLMs internally process and understand\nvideo content, offering interpretability and efficiency perspectives for future\nresearch.", "AI": {"tldr": "本文通过设计三种新方法来分析Video-LLMs，并揭示了其内部工作机制，说明了视频信息提取主要在早期层发生，某些中间层对视频问答起到决定性作用，以及视频与语言之间关联的机制，研究结果为未来的Video-LLMs改进提供了解释性和效率提升的思路。", "motivation": "鉴于当前大多数研究专注于提升Video-LLMs的表现，而较少关注其内部运作机制，本文旨在通过系统性实证研究填补这一空白。", "method": "本文通过采用注意力剔除方法，设计了视频时序剔除、视频空间剔除和语言到视频剔除三种变体，应用在不同数量的层中来研究Video-LLMs的内部机制。", "result": "研究表明，在全局设置下，视频信息处理分为两个阶段，前后层分别用于感知编码和抽象推理；在细粒度设置下，某些中间层对视频问题回答影响大，而其他层贡献较少；在两者设置下，空间时间建模更多依赖于语言指引的检索而不是视频内部的自我关注机制。", "conclusion": "论文揭示了Video-LLMs内部工作方式，并指出这些见解可用于减少Video-LLMs中的注意力计算，且本文是首次系统性地揭示这种内部处理和理解视频内容的方式。"}}
{"id": "2508.15371", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15371", "abs": "https://arxiv.org/abs/2508.15371", "authors": ["Jaydip Sen", "Subhasis Dasgupta", "Hetvi Waghela"], "title": "Confidence-Modulated Speculative Decoding for Large Language Models", "comment": "This is the preprint of the paper, which has been accepted for oral\n  presentation and publication in the proceedings of IEEE INDISCON 2025. The\n  conference will be organized at the National Institute of Technology,\n  Rourkela, India, from August 21 to 23, 2025. The paper is 10 pages long, and\n  it contains 2 figures and 5 tables", "summary": "Speculative decoding has emerged as an effective approach for accelerating\nautoregressive inference by parallelizing token generation through a\ndraft-then-verify paradigm. However, existing methods rely on static drafting\nlengths and rigid verification criteria, limiting their adaptability across\nvarying model uncertainties and input complexities. This paper proposes an\ninformation-theoretic framework for speculative decoding based on\nconfidence-modulated drafting. By leveraging entropy and margin-based\nuncertainty measures over the drafter's output distribution, the proposed\nmethod dynamically adjusts the number of speculatively generated tokens at each\niteration. This adaptive mechanism reduces rollback frequency, improves\nresource utilization, and maintains output fidelity. Additionally, the\nverification process is modulated using the same confidence signals, enabling\nmore flexible acceptance of drafted tokens without sacrificing generation\nquality. Experiments on machine translation and summarization tasks demonstrate\nsignificant speedups over standard speculative decoding while preserving or\nimproving BLEU and ROUGE scores. The proposed approach offers a principled,\nplug-in method for efficient and robust decoding in large language models under\nvarying conditions of uncertainty.", "AI": {"tldr": "本研究提出了一种基于信心调制绘制的信息理论框架，用于改善投机解码的适应性、降低回滚频率、提高资源利用率并保持输出质量。", "motivation": "现有的投机解码方法依赖于固定的绘制长度和严格的验证标准，限制了它们在不同模型不确定性和输入复杂度下的适用性。", "method": "提出了基于信心调制绘制的信息理论框架，通过熵和基于边界的不确定性度量动态调整每轮迭代中投机生成的标记数量。绘制和验证过程使用相同的信心信号进行调制，使绘制的标记接受更灵活，同时保持生成质量。", "result": "在机器翻译和总结任务上，实验显示与标准的投机解码相比，所提出的方法显著提高了速度，同时保持或提高了BLEU和ROUGE得分。", "conclusion": "实验表明，所提出的插件式方法能够在各种不确定性条件下，为大型语言模型提供高效和可靠的解码。"}}
{"id": "2508.15367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15367", "abs": "https://arxiv.org/abs/2508.15367", "authors": ["Jacinto Colan", "Ana Davila", "Yasuhisa Hasegawa"], "title": "Transfer learning optimization based on evolutionary selective fine tuning", "comment": "Presented at the Workshop artiFicial And bio-inspIred netwoRked\n  intelliGence foR cOnstrained aUtoNomous Devices (FAIRGROUND). 2025\n  International Joint Conference on Neural Networks (IJCNN)", "summary": "Deep learning has shown substantial progress in image analysis. However, the\ncomputational demands of large, fully trained models remain a consideration.\nTransfer learning offers a strategy for adapting pre-trained models to new\ntasks. Traditional fine-tuning often involves updating all model parameters,\nwhich can potentially lead to overfitting and higher computational costs. This\npaper introduces BioTune, an evolutionary adaptive fine-tuning technique that\nselectively fine-tunes layers to enhance transfer learning efficiency. BioTune\nemploys an evolutionary algorithm to identify a focused set of layers for\nfine-tuning, aiming to optimize model performance on a given target task.\nEvaluation across nine image classification datasets from various domains\nindicates that BioTune achieves competitive or improved accuracy and efficiency\ncompared to existing fine-tuning methods such as AutoRGN and LoRA. By\nconcentrating the fine-tuning process on a subset of relevant layers, BioTune\nreduces the number of trainable parameters, potentially leading to decreased\ncomputational cost and facilitating more efficient transfer learning across\ndiverse data characteristics and distributions.", "AI": {"tldr": "本文提出了一种名为BioTune的进化适应性微调技术，用于增强迁移学习效率。通过仅微调部分相关层，BioTune减少了需要训练的参数数量，从而有可能降低计算成本。", "motivation": "传统微调策略通常涉及更新所有模型参数，可能会导致过拟合和计算成本增加。本文提出了一种新的方法，以增强迁移学习的效率。", "method": "BioTune是一种进化适应性微调技术，使用进化算法识别出需要微调的一组特定层，以优化特定目标任务上的模型性能。", "result": "在九个不同领域的图像分类数据集上进行了评估，BioTune在准确性和效率方面优于现有的微调方法，如AutoRGN和LoRA。", "conclusion": "BioTune通过集中微调过程于相关层子集上，减少了可训练参数的数量，这可能会降低计算成本并促进跨多样数据特性和分布的更高效迁移学习。"}}
