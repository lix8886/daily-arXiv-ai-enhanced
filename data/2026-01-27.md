<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Crystal-KV: Efficient KV Cache Management for Chain-of-Thought LLMs via Answer-First Principle](https://arxiv.org/abs/2601.16986)
*Zihan Wang,Cheng Tang,Lei Gong,Cheng Li,Chao Wang,teng wang,Wenqi Lou,Xuehai Zhou*

Main category: cs.CL

> 本文提出了一种专为CoT推理设计的高效KV缓存管理框架Crystal-KV，实现了高水平的KV缓存压缩，提高了系统吞吐量和响应速度，同时保持或改善了CoT推理的准确性。

<details>
  <summary>Details</summary>

**Motivation:** CoT推理在LLMs中提高了复杂任务的准确性，但长的推理序列导致了较大的内存开销。传统KV缓存压缩方法对CoT推理效果不佳，故提出Crystal-KV框架解决此问题。

**Method:** 通过引入Crystal-KV框架，提出了一种高效的KV缓存管理策略，解决了Chain-of-Thought（CoT）推理中的内存开销问题。通过区分对最终答案有贡献的CrystalKV和仅维持推理流但可能引入误导性背景的SlipKV，采用基于注意力的最少最近使用频率算法和自适应缓存预算分配算法，实现了高效的KV缓存压缩和更好的资源利用。

**Result:** 实验结果表明，Crystal-KV框架能够显著提高吞吐量和响应速度，同时维持甚至提升CoT推理的准确率。

**Conclusion:** Crystal-KV框架有效地改进了KV缓存管理，为CoT推理任务提供了更好的性能和资源利用效率，解决了因长推理序列带来的内存开销问题。

**Abstract:** Chain-of-Thought (CoT) reasoning in large language models (LLMs) significantly improves accuracy on complex tasks, yet incurs excessive memory overhead due to the long think-stage sequences stored in the Key-Value (KV) cache. Unlike traditional generation tasks where all tokens are uniformly important, CoT emphasizes the final answer, rendering conventional KV compression strategies ineffective. In this paper, we present Crystal-KV, an efficient KV cache management framework tailored for CoT reasoning. Our key insight is the answer-first principle. By mapping answer preferences into think-stage attention map, we distinguish between SlipKV, which mainly maintains the reasoning flow but may occasionally introduce misleading context, and CrystalKV, which truly contributes to the correctness of the final answer. Next, we propose an attention-based Least Recently Frequently Used algorithm. It precisely identifies when a SlipKV entry's utility expires and evicts it, retaining CrystalKV without disrupting reasoning flow. Finally, we introduce an adaptive cache budget allocation algorithm. Based on the dynamic proportion of CrystalKV, it estimates the importance of each layer/head and adjusts the KV cache budget during inference, amplifying critical components to improve budget utilization. Results show that Crystal-KV achieves state-of-the-art KV cache compression, significantly improves throughput, and enables faster response time, while maintaining, or even improving, answer accuracy for CoT reasoning.

</details>


### [2] [Evaluating Reward Model Generalization via Pairwise Maximum Discrepancy Competitions](https://arxiv.org/abs/2601.16987)
*Shunyang Luo,Peibei Cao,Zhihui Zhu,Kehua Feng,Zhihua Wang,Keyan Ding*

Main category: cs.CL

> 提出了一种新的动态框架 PMDC 来评估奖励模型的泛化能力，揭示了奖励模型在开放世界设置下的系统性泛化失败。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有的奖励模型评估基于静态的、预标注的偏好数据集，这些数据集提供了有限的涵盖范围，并且往往无法准确评估开放世界设置中的泛化能力。

**Method:** 提出了成对最大差异竞赛（PMDC），这是一种动态且注释高效的框架，用于评估奖励模型（RMs）在一大池未标记的开放域提示中的泛化能力。PMDC 积极选择成对提示-响应，以最大化两个奖励模型之间的分歧，从而产生一组高度争议的测试案例。

**Result:** PMDC 方法产生了一系列具有高度争议性的测试用例，这些用例由一个 oracle 裁决，并通过 Bradley-Terry 模型聚合，以生成 RMs 的全局排名和成对胜率图。

**Conclusion:** 通过使用 PMDC 重新评估了 10 个代表性的奖励模型，并观察到与传统基准测试相比，排名有大幅调整。定性分析进一步揭示了系统泛化失败的模式，为改进奖励模型提供了有价值的见解。

**Abstract:** Reward models (RMs) are central to aligning large language models, yet their practical effectiveness hinges on generalization to unseen prompts and shifting distributions. Most existing RM evaluations rely on static, pre-annotated preference datasets, which provide limited coverage and often fail to faithfully assess generalization in open-world settings. We introduce Pairwise Maximum Discrepancy Competition (PMDC), a dynamic and annotation-efficient framework for evaluating RM generalization using a large, unlabeled, open-domain prompt pool. PMDC actively selects prompt--response pairs that maximize disagreement between two RMs, yielding a compact set of highly contentious test cases. These cases are adjudicated by an oracle, and the resulting outcomes are aggregated via a Bradley--Terry model to produce a global ranking and pairwise win-rate landscape of RMs. We apply PMDC to re-evaluate 10 representative RMs and observe substantial rank reshuffling compared with conventional benchmarks. Qualitative analyses further uncover systematic generalization failures, providing valuable insights for improving reward modeling.

</details>


### [3] [Uncertainty Quantification for Named Entity Recognition via Full-Sequence and Subsequence Conformal Prediction](https://arxiv.org/abs/2601.16999)
*Matthew Singer,Srijan Sengupta,Karl Pazdernik*

Main category: cs.CL

> 本文提出了一种基于共形预测的NER模型不确定性估计框架，该方法提供有限样本覆盖保证，适用于多种NER模型和数据集，提高模型预测可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 当前NER模型通常仅输出单一预测标签序列而不提供不确定性度量，这可能导致下游应用出现级联错误。本文动机在于提高模型预测的可靠性，通过提供预测可靠性的形式化保障，类似经典统计学中的置信区间。

**Method:** 本文提出了一种基于序列标注的命名实体识别（NER）模型框架，该框架能够生成具有不确定性的预测集，这些预测集在用户指定的置信水平下保证包含正确的标注序列。方法基于共形预测，提供有限样本覆盖保证，且假设较少。通过设计有效的非一致性评分函数，框架支持无条件和类别条件下的校准预测集，并考虑了句子长度、语言、实体类型等异质性因素。

**Result:** 实验证明，本文提出的框架能够为NER模型提供不确定性估计，并保证预测集在指定置信水平下包含正确的标注。

**Conclusion:** 本文提出的框架证明了其广泛的适用性、有效性和准确性，适用于四种子类型的NER模型和三个基准数据集。

**Abstract:** Named Entity Recognition (NER) serves as a foundational component in many natural language processing (NLP) pipelines. However, current NER models typically output a single predicted label sequence without any accompanying measure of uncertainty, leaving downstream applications vulnerable to cascading errors. In this paper, we introduce a general framework for adapting sequence-labeling-based NER models to produce uncertainty-aware prediction sets. These prediction sets are collections of full-sentence labelings that are guaranteed to contain the correct labeling with a user-specified confidence level. This approach serves a role analogous to confidence intervals in classical statistics by providing formal guarantees about the reliability of model predictions. Our method builds on conformal prediction, which offers finite-sample coverage guarantees under minimal assumptions. We design efficient nonconformity scoring functions to construct efficient, well-calibrated prediction sets that support both unconditional and class-conditional coverage. This framework accounts for heterogeneity across sentence length, language, entity type, and number of entities within a sentence. Empirical experiments on four NER models across three benchmark datasets demonstrate the broad applicability, validity, and efficiency of the proposed methods.

</details>


### [4] [RAM-SD: Retrieval-Augmented Multi-agent framework for Sarcasm Detection](https://arxiv.org/abs/2601.17002)
*Ziyang Zhou,Ziqi Liu,Yan Wang,Yiming Lin,Yangbin Chen*

Main category: cs.CL

> RAM-SD是一种用于识别讽刺的多智能体框架，它将讽刺的识别任务分解为多个阶段，并针对不同输入进行优化推理。这种方法在多个基准上实现了迄今为止的最佳性能，同时提供了可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 讽刺识别具有挑战性，因为它依赖于细微的语境理解、世界知识及多维度语言线索。现有方法应用统一的推理策略来处理输入，难以应对讽刺识别多样化的分析需求。

**Method:** 介绍了一种名为RAM-SD的用于识别讽刺的检索增强多智能体框架。该框架包含四个阶段：上下文检索、元规划、特化智能体多视角分析、结果综合。每个阶段分别进行任务分类、合理计划选择、互补分析与最终判断和解释生成。

**Result:** 通过对四个标准基准进行测试，RAM-SD达到了77.74%的宏观F1分数，比强大的GPT-4o+CoC基线高出7.01个百分点。

**Conclusion:** RAM-SD不仅建立了新的性能标准，而且还提供了透明和可解释的推理轨迹，阐明了讽刺理解背后认知过程。

**Abstract:** Sarcasm detection remains a significant challenge due to its reliance on nuanced contextual understanding, world knowledge, and multi-faceted linguistic cues that vary substantially across different sarcastic expressions. Existing approaches, from fine-tuned transformers to large language models, apply a uniform reasoning strategy to all inputs, struggling to address the diverse analytical demands of sarcasm. These demands range from modeling contextual expectation violations to requiring external knowledge grounding or recognizing specific rhetorical patterns. To address this limitation, we introduce RAM-SD, a Retrieval-Augmented Multi-Agent framework for Sarcasm Detection. The framework operates through four stages: (1) contextual retrieval grounds the query in both sarcastic and non-sarcastic exemplars; (2) a meta-planner classifies the sarcasm type and selects an optimal reasoning plan from a predefined set; (3) an ensemble of specialized agents performs complementary, multi-view analysis; and (4) an integrator synthesizes these analyses into a final, interpretable judgment with a natural language explanation. Evaluated on four standard benchmarks, RAM-SD achieves a state-of-the-art Macro-F1 of 77.74%, outperforming the strong GPT-4o+CoC baseline by 7.01 points. Our framework not only sets a new performance benchmark but also provides transparent and interpretable reasoning traces, illuminating the cognitive processes behind sarcasm comprehension.

</details>


### [5] [From Emotion to Expression: Theoretical Foundations and Resources for Fear Speech](https://arxiv.org/abs/2601.17132)
*Vigneshwaran Shankaran,Gabriella Lapesa,Claudia Wagner*

Main category: cs.CL

> 本文通过跨学科视角研究恐惧言语，提出了一种新的分类法，以整合恐惧的不同维度，并为恐惧言语的研究提供理论和实践指导。

<details>
  <summary>Details</summary>

**Motivation:** 由于恐惧言语作为一种现象受到多个学科的贡献影响，使得其计算研究变得支离破碎且资源不足。本文旨在填补这一空白。

**Method:** 本文通过比较心理学、政治学、传播学以及语言学中的恐惧理论来进行跨学科分析，并回顾了现有的定义。接着，通过对相关研究领域的数据集进行调查，提出了一种恐惧言语研究的分类法，该分类法整合了恐惧的不同维度。

**Result:** 本文的工作提供了理论和实践上的指导，帮助创建数据集并推进恐惧言语的研究。

**Conclusion:** 通过对恐惧言语不同维度的整合，本文为未来研究提供了理论框架和分类体系，推动了恐惧言语研究的发展。

**Abstract:** Few forces rival fear in their ability to mobilize societies, distort communication, and reshape collective behavior. In computational linguistics, fear is primarily studied as an emotion, but not as a distinct form of speech. Fear speech content is widespread and growing, and often outperforms hate-speech content in reach and engagement because it appears "civiler" and evades moderation. Yet the computational study of fear speech remains fragmented and under-resourced. This can be understood by recognizing that fear speech is a phenomenon shaped by contributions from multiple disciplines. In this paper, we bridge cross-disciplinary perspectives by comparing theories of fear from Psychology, Political science, Communication science, and Linguistics. Building on this, we review existing definitions. We follow up with a survey of datasets from related research areas and propose a taxonomy that consolidates different dimensions of fear for studying fear speech. By reviewing current datasets and defining core concepts, our work offers both theoretical and practical guidance for creating datasets and advancing fear speech research.

</details>


### [6] [Dynamic Role Assignment for Multi-Agent Debate](https://arxiv.org/abs/2601.17152)
*Miao Zhang,Junsik Kim,Siyuan Xiang,Jian Gao,Cheng Cao*

Main category: cs.CL

> 提出了动态角色分配框架，通过元辩论在辩论之前选择合适的代理，提高了多代理系统在解决复杂问题时的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多代理大型语言模型（LLM）和视觉语言模型（VLM）辩论系统在复杂问题解决中使用了专门的角色，但没有利用模型的专长来决定哪个模型应该担任哪个角色。

**Method:** 我们提出了一种动态角色分配框架，该框架通过元辩论（Meta-Debate）选择合适的代理进行实际辩论。元辩论分为两个阶段：（1）提案，候选人提供适应角色的论点；（2）同行评审，依据数据和角色特定的标准对提案进行评分，从而选择每个位置的最佳代理。

**Result:** 实验结果表明，我们的方法在现有辩论系统上的表现优于均匀分配（将同一模型用于所有角色）最多74.8%，优于随机分配（不考虑模型适应性情况下分配角色）最多29.7%，具体取决于任务和特定的角色分配。

**Conclusion:** 本研究表明，从静态代理部署转向动态且具备能力意识的选择，为多代理系统设计创建了一种新的范式。

**Abstract:** Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.

</details>


### [7] [Interpretability of the Intent Detection Problem: A New Approach](https://arxiv.org/abs/2601.17156)
*Eduardo Sanchez-Karhunen,Jose F. Quesada-Moreno,Miguel A. Gutiérrez-Naranjo*

Main category: cs.CL

> 研究利用动力系统理论分析了RNN在平衡和不平衡数据集上的表现，揭示了隐藏状态空间几何结构受数据集属性影响的机制，提供了一种全新的神经网络计算解决方案的几何解释。

<details>
  <summary>Details</summary>

**Motivation:** 尽管深度学习技术在意图检测任务中占据主导地位，但其内部机制尚不完全理解。因此，研究试图解读RNN如何处理意图检测问题。

**Method:** 本研究采用动力系统理论来分析RNN架构如何解决意图检测任务，将句子解释为隐藏状态空间中的轨迹，并考察平衡SNIPS和不平衡ATIS数据集上的效果。

**Result:** 研究表明，网络在平衡数据集上学习到了理想解决方案，而在不平衡数据集上，低频意图的聚类质量下降。

**Conclusion:** 研究提出了一种解释现实世界性能差异的机理，揭示了RNN的动态是如何直接受到数据集特性的塑造。

**Abstract:** Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.

</details>


### [8] [Who Gets Which Message? Auditing Demographic Bias in LLM-Generated Targeted Text](https://arxiv.org/abs/2601.17172)
*Tunazzina Islam*

Main category: cs.CL

> 研究评估了三大语言模型在基于人口统计细分的定向信息任务中的表现，发现存在性别和年龄差异，并强调了需要开发具有偏见意识的生成管道和透明审计框架。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型有能力大规模生成个性化和有说服力的文本，这引发了一系列关于自动通信中的偏见和公平性问题。本文旨在提供一个系统性的分析，研究这些模型在面对基于人口统计学细分的定向信息任务时的行为。

**Method:** 本文介绍了针对三大主流大型语言模型GPT-4o、Llama-3.3和Mistral-Large 2.1的控制评估框架，该框架在两种生成设定下进行评估：独立生成和富含上下文的生成，以此来评估语言模型在面临基于人口统计细分的定向信息任务时的行为。评估维度包括词汇内容、语言风格及说服性框架。

**Result:** 研究发现，在气候沟通场景中，不同年龄和性别目标信息存在明显差异：针对男性和年轻群体的信息更强调主体性、创新和坚定，而针对女性和老年群体的信息则更侧重于关心、温暖和传统。富含主题和地域上下文的提示词会放大这些差异，针对年轻或男性受众的信息说服力得分显著较高。

**Conclusion:** 该研究展示了人口统计学刻板印象如何在LLM生成的定向沟通中出现并加剧，强调需要具有偏见意识的生成管道和透明的审计框架，以明确考虑社会敏感应用中的人口统计细分条件。

**Abstract:** Large language models (LLMs) are increasingly capable of generating personalized, persuasive text at scale, raising new questions about bias and fairness in automated communication. This paper presents the first systematic analysis of how LLMs behave when tasked with demographic-conditioned targeted messaging. We introduce a controlled evaluation framework using three leading models -- GPT-4o, Llama-3.3, and Mistral-Large 2.1 -- across two generation settings: Standalone Generation, which isolates intrinsic demographic effects, and Context-Rich Generation, which incorporates thematic and regional context to emulate realistic targeting. We evaluate generated messages along three dimensions: lexical content, language style, and persuasive framing. We instantiate this framework on climate communication and find consistent age- and gender-based asymmetries across models: male- and youth-targeted messages emphasize agency, innovation, and assertiveness, while female- and senior-targeted messages stress warmth, care, and tradition. Contextual prompts systematically amplify these disparities, with persuasion scores significantly higher for messages tailored to younger or male audiences. Our findings demonstrate how demographic stereotypes can surface and intensify in LLM-generated targeted communication, underscoring the need for bias-aware generation pipelines and transparent auditing frameworks that explicitly account for demographic conditioning in socially sensitive applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

> 研究通过系统的分析和评估方法，提出名为ImgCoder的框架，旨在提升基于程序的科学图像合成的结构精度，并证明合成科学图像在多模态推理任务中的实用性。

<details>
  <summary>Details</summary>

**Motivation:** 现有文本到图像（T2I）模型生成的图像在视觉上通常是可能的，但在科学上可能是错误的，导致视觉和逻辑之间存在持续的分歧，这限制了它们在多模态推理中的用途。因此，借助新型T2I模型的近期进展，我们的研究旨在改善科学图像合成。

**Method:** 我们研究了科学图像合成在生成范式、评估和下游应用方面的系统性问题。我们分析了基于像素的直接生成和基于程序的生成，并提出了ImgCoder框架，该框架遵循明确的“理解-规划-编码”工作流程以提高结构精度。此外，我们引入了SciGenBench评估基准，用于评估生成图像的信息效用和逻辑有效性。

**Result:** 我们的评估发现基于像素的模型存在系统性失败模式，并强调了表达能力和精确度之间的根本权衡。研究结果还表明，使用精细调优的多模态模型细粒度验证过的合成科学图像可以带来一致的推理增益。

**Conclusion:** 我们发现，基于像素的模型在科学图像合成方面存在一定失败模式，并揭示了表达能力和精确度之间存在的根本权衡。特别是，我们证明了使用精细调优的大型多模态模型（LMMs）在经过严格验证的合成科学图像上可以得到一致的推理增益，这为解锁大规模多模态推理能力奠定了基础。

**Abstract:** While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

</details>


### [10] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

> 本文提出了一种新的框架，结合了空间流形扩展和病变注入，通过利用隐式神经表示建模速度场来提高医学图像分割的数据效率和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 医学图像分割的性能越来越依赖于数据的有效利用，而不是仅仅依赖原始数据量。对复杂病理如脑膜瘤的准确分割要求模型充分利用有限高质量注释中的潜在信息。

**Method:** 本文提出了一个创新的双增强框架，该框架结合了空间流形扩展和语义对象注入。特别地，本文利用隐式神经表示（INR）来建模连续的速度场，并通过在集成变形场中进行线性混合来高效生成解剖上合理的变异。此外，本文引入了一个Sim2Real病变注入模块，通过将病变纹理移植到健康的解剖背景中，有效弥合了合成增强与现实病理之间的差距。

**Result:** 在混合数据集上的全面实验表明，该框架显著提高了状态-of-the-艺术模型（包括nnU-Net和U-Mamba）的数据效率和鲁棒性。

**Conclusion:** 该框架提出一种强大的策略，能够在有限的注释预算下实现高性能的医学图像分析。

**Abstract:** The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

</details>


### [11] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

> An automated method for analyzing RBC shapes and classifying them as normal or deformed using image analysis outperforms manual methods, offering a superior tool for diagnosing sickle cell anemia.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to develop an automated method for RBC deformation analysis that reduces the time consumed by manual inspection and minimizes observer error in the diagnosis of diseases like sickle cell anemia.

**Method:** The method uses peripheral blood smear image analysis with the Chan-Vese active contour model for segmentation. It classifies RBCs into normal or abnormal based on CSF and ESF. An elliptical adjustment is applied for occluded cells.

**Result:** The proposed method proved to be superior in the diagnosis of sickle cell anemia compared to other state-of-the-art methods, with an F-measure of 0.97 for normal cells and 0.95 for elongated ones.

**Conclusion:** The proposed automatic method for RBC enumeration and deformation analysis is sufficiently accurate for clinical diagnostic support, particularly for sickle cell anemia.

**Abstract:** Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

</details>


### [12] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

> 通过新基准测试揭示了各模型在基本视觉推理任务上的问题，为未来的跨模态对齐研究提供了框架。

<details>
  <summary>Details</summary>

**Motivation:** 虽然机器学习领域发展迅速，但视觉语言模型（VLMs）仍然无法理解和生成基本的视觉概念，如物体方向、数量或空间关系，这突显了基本视觉推理上的差距。

**Method:** 通过创建一个新的基准测试来探究多模态大语言模型（MLLMs）和图像生成模型（IGMs）在视觉推理方面的局限性，该基准测试用于系统地比较图像到文本和文本到图像任务中的故障模式，从而实现跨模态的视觉理解评价。

**Result:** 测试了11个MLLM和3个IGM在九类视觉推理任务中的表现，结果显示模型和模态之间共享部分故障模式，但某些故障具有特定性。

**Conclusion:** 这项工作为未来的跨模态对齐研究奠定了基础，提供了一个框架，来探究生成和解释故障是否源自共享的限制，从而指导未来对统一视觉语言模型的改进。

**Abstract:** We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

</details>


### [13] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

> 一篇关于C&D废弃物自动化分类的论文，展示了基于Xception特征与简单ML分类器的先进混合视觉管道系统。

<details>
  <summary>Details</summary>

**Motivation:** 建筑行业产生大量的废弃物，有效的分类和处理对于可持续的废弃物管理和资源回收至关重要。

**Method:** 该研究提出了一种基于视觉的混合管道，结合深度特征提取与传统的机器学习（ML）分类器，以实现建筑和拆除（C&D）废弃物的自动化分类。

**Result:** 实验结果表明，采用Xception特征与简单的分类器（如线性SVM、kNN和Bagged Trees）结合的混合管道达到了最先进的性能，最高准确率达到99.5%，超过了更复杂或端到端的深度学习方法。

**Conclusion:** 该方法在坚固性和现场部署识别方面提供了操作优势，并为未来与机器人和现场自动化系统的集成提供了途径。

**Abstract:** The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

</details>
