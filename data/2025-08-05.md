<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.CV](#cs.CV) [Total: 44]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches](https://arxiv.org/abs/2508.00864)
*Margarita Bugueño,Gerard de Melo*

Main category: cs.CL

> 本文提出了一种基于自注意力模型的句子依赖性学习方法来构建文档分类中的图结构。实验表明，该方法生成的图结构优于基于启发式的方法，提高了分类的准确性和稳定性，减少了领域依赖性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文档分类中的图表示依赖于启发式或领域特定规则，本文旨在消除对人工设计的依赖，提出一种数据驱动的图结构学习方法，以提高文档分类的效果和领域适应性。

**Method:** 本研究使用自注意力模型根据句子间的依赖关系来构建加权同质图，并采用统计过滤策略保留相关性高的句子，以提高图的质量和缩小图规模。

**Result:** 在三个文档分类数据集上的实验结果表明，自动学习的图结构在准确率和F1得分上显著优于启发式图结构，证明了统计过滤方法对分类稳定性的提升。

**Conclusion:** 研究结果表明，自动图生成技术相比于传统的启发式技术具有较高的潜力，为NLP中的广泛应用开辟了新的方向。

**Abstract:** In document classification, graph-based models effectively capture document
structure, overcoming sequence length limitations and enhancing contextual
understanding. However, most existing graph document representations rely on
heuristics, domain-specific rules, or expert knowledge. Unlike previous
approaches, we propose a method to learn data-driven graph structures,
eliminating the need for manual design and reducing domain dependence. Our
approach constructs homogeneous weighted graphs with sentences as nodes, while
edges are learned via a self-attention model that identifies dependencies
between sentence pairs. A statistical filtering strategy aims to retain only
strongly correlated sentences, improving graph quality while reducing the graph
size. Experiments on three document classification datasets demonstrate that
learned graphs consistently outperform heuristic-based graphs, achieving higher
accuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness
of the statistical filtering in improving classification robustness. These
results highlight the potential of automatic graph generation over traditional
heuristic approaches and open new directions for broader applications in NLP.

</details>


### [2] [FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts](https://arxiv.org/abs/2508.00889)
*Hagyeong Shin,Binoy Robin Dalal,Iwona Bialynicka-Birula,Navjot Matharu,Ryan Muir,Xingwei Yang,Samuel W. K. Wong*

Main category: cs.CL

> 研究提出了一种新的3D范式和FECT数据集，用于评估企业级客服对话转录中大语言模型生成解释性AI声明的事实性，支持业务决策的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 针对大语言模型在处理企业应用场景（如客服对话分析和总结）时，所产生的非事实性输出可能给业务决策带来的负面影响。

**Method:** 提出了3D（分解，解耦，分离）范式，作为人类注释指南和LLM评判者的提示，以语言学为基础的评估标准来 grounding 事实标签。还引入了一个新的基准数据集 FECT，用于评估解释性AI生成的客服对话转录事实声明。

**Result:** 研究发现采用3D范式可以使LLM评判者在评估客服对话转录解释性AI生成声明的事实性时达成一致。

**Conclusion:** 研究为自动评估分析客服对话的AI系统输出的事实性提供了新的方法。

**Abstract:** Large language models (LLMs) are known to hallucinate, producing natural
language outputs that are not grounded in the input, reference materials, or
real-world knowledge. In enterprise applications where AI features support
business decisions, such hallucinations can be particularly detrimental. LLMs
that analyze and summarize contact center conversations introduce a unique set
of challenges for factuality evaluation, because ground-truth labels often do
not exist for analytical interpretations about sentiments captured in the
conversation and root causes of the business problems. To remedy this, we first
introduce a \textbf{3D} -- \textbf{Decompose, Decouple, Detach} -- paradigm in
the human annotation guideline and the LLM-judges' prompt to ground the
factuality labels in linguistically-informed evaluation criteria. We then
introduce \textbf{FECT}, a novel benchmark dataset for \textbf{F}actuality
\textbf{E}valuation of Interpretive AI-Generated \textbf{C}laims in Contact
Center Conversation \textbf{T}ranscripts, labeled under our 3D paradigm.
Lastly, we report our findings from aligning LLM-judges on the 3D paradigm.
Overall, our findings contribute a new approach for automatically evaluating
the factuality of outputs generated by an AI system for analyzing contact
center conversations.

</details>


### [3] [XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML](https://arxiv.org/abs/2508.00924)
*Ernesto L. Estevanell-Valladares,Suilan Estevez-Velarde,Yoan Gutiérrez,Andrés Montoyo,Ruslan Mitkov*

Main category: cs.CL

> 介绍了XAutoLM，这是一个元学习增强的AutoML框架，可以重用过去的经历来优化语言模型的微调管线，极大地提高了效率。

<details>
  <summary>Details</summary>

**Motivation:** 虽然专家利用领域知识来导航模型选择、超参数优化和资源分配的决策，但是对于语言模型的微调来说，重复试验会产生巨大的计算开销和环境影响。现有的自动化框架没有同时解决整个模型选择和超参数优化任务。

**Method:** 从过去的成功和失败中学习，通过提取任务和系统级别的元特征来偏向有利的配置并远离代价高昂的死胡同。它在一个增强的AutoML框架中使用元学习来优化判别和生成语言模型的微调管线。

**Result:** 在四个文本分类和两个问答基准测试中，XAutoLM在六个任务中的五个任务上超过了零样本优化器的峰值F1，平均评估时间减少了多达4.5倍，错误比率减少了多达七倍，并且发现了超出零样本帕累托前沿50%以上的管线。

**Conclusion:** XAutoLM展示出了显著的性能提升，相较于简单的基于记忆的基线，存在负面影响，该研究进一步推进了资源高效的绿色AI微调，并开放了经验存储以加速NLP社区的发展。

**Abstract:** Experts in machine learning leverage domain knowledge to navigate decisions
in model selection, hyperparameter optimisation, and resource allocation. This
is particularly critical for fine-tuning language models (LMs), where repeated
trials incur substantial computational overhead and environmental impact.
However, no existing automated framework simultaneously tackles the entire
model selection and HPO task for resource-efficient LM fine-tuning. We
introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past
experiences to optimise discriminative and generative LM fine-tuning pipelines
efficiently. XAutoLM learns from stored successes and failures by extracting
task- and system-level meta-features to bias its sampling toward fruitful
configurations and away from costly dead ends. On four text classification and
two question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak
F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error
ratios by up to sevenfold, and uncovers up to 50% more pipelines above the
zero-shot Pareto front. In contrast, simpler memory-based baselines suffer
negative transfer. We release XAutoLM and our experience store to catalyse
resource-efficient, Green AI fine-tuning in the NLP community.

</details>


### [4] [MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.01005)
*Yiqun Chen,Erhan Zhang,Lingyong Yan,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Jiaxin Mao*

Main category: cs.CL

> 本文提出了一种基于多智能体编排的自适应Retrieval-Augmented Generation框架MAO-ARAG，改进了固定RAG流水线在复杂问答任务上的性能和成本平衡问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的RAG系统由于其固定的流水线结构无法在一个广泛的任务范围内同时达到性能和成本效率的优化，尤其是在面对复杂多样的真实查询时。为了应对这一挑战，提出了一种更加灵活和自适应的RAG框架。

**Method:** 本研究提出了一个自适应的Retrieval-Augmented Generation（RAG）框架叫做MAO-ARAG，它基于多智能体编排。该框架通过定义多个执行代理（如查询重构代理、文档选择代理和生成代理），并使用一个计划代理来智能选择和整合这些执行代理，形成针对每个查询的适当工作流。计划代理通过强化学习进行训练，目标是在保证答案质量的同时控制成本。

**Result:** 在多个问答数据集上的实验表明，MAO-ARAG能够在保证高质量答案的同时，维持成本和延迟在合理的范围内。

**Conclusion:** 研究证明了自适应调整工作流以应对不同查询的MAO-ARAG框架能够有效提高问答系统的答案质量，并保持在合理的成本和延迟范围内，是一个有前途的研究方向。

**Abstract:** In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has
become pivotal in enhancing response accuracy and reducing hallucination
issues. The architecture of RAG systems varies significantly, encompassing
single-round RAG, iterative RAG, and reasoning RAG, each tailored to address
different types of queries. Due to the varying complexity of real-world
queries, a fixed RAG pipeline often struggles to balance performance and cost
efficiency across different queries. To address this challenge, we propose an
adaptive RAG framework called MAO-ARAG, which leverages multi-agent
orchestration. Our adaptive RAG is conceived as a multi-turn framework.
Specifically, we define multiple executor agents, representing typical RAG
modules such as query reformulation agents, document selection agent, and
generation agents. A planner agent intelligently selects and integrates the
appropriate agents from these executors into a suitable workflow tailored for
each query, striving for high-quality answers while maintaining reasonable
costs. During each turn, the planner agent is trained using reinforcement
learning, guided by an outcome-based reward (F1 score) and a cost-based
penalty, continuously improving answer quality while keeping costs within a
reasonable range. Experiments conducted on multiple QA datasets demonstrate
that our approach, which dynamically plans workflows for each query, not only
achieves high answer quality but also maintains both cost and latency within
acceptable limits.The code of MAO-ARAG is on
https://github.com/chenyiqun/Agentic-RAG.

</details>


### [5] [UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu](https://arxiv.org/abs/2508.01006)
*Farah Adeeba,Brian Dillon,Hassan Sajjad,Rajesh Bhatt*

Main category: cs.CL

> 研究人员创建了一个乌尔都语基准数据集UrBLiMP，用于评估多语言大语言模型在乌尔都语语法知识方面的表现，测试结果显示当前的多语言大语言模型在捕捉低资源语言的细粒度语法知识方面既有潜力也有限制。

<details>
  <summary>Details</summary>

**Motivation:** 许多多语言大语言模型在处理高资源语言（如英语）时表现出色，但在低资源语言（如乌尔都语）的数据却相对较少，这促使研究人员创建了UrBLiMP来评估这些模型在乌尔都语语法知识方面的表现。

**Method:** 为了评估多语言大语言模型（LLMs）在乌尔都语的语法知识，研究人员创建了一个名为乌尔都语语言最小对基准（UrBLiMP）的数据集。该数据集包含5,696个最小对立对，这些对立对在语法可接受性方面稍有不同，从而评估这些模型在十个核心句法现象上的表现。

**Result:** 研究人员评估了20个多语言大语言模型在UrBLiMP上的表现，发现不同模型在接受性和使用性上的表现存在显著差异。例如，LLaMA-3-70B获得最高平均准确率（94.73%），但其表现与Gemma-3-27B-PT等其他顶尖模型相近。

**Conclusion:** 研究结果表明，尽管当前的多语言大语言模型在乌尔都语等低资源语言的细粒度句法知识方面表现出色，但仍然存在改进空间。这为未来的研究和模型优化提供了方向。

**Abstract:** Multilingual Large Language Models (LLMs) have shown remarkable performance
across various languages; however, they often include significantly less data
for low-resource languages such as Urdu compared to high-resource languages
like English. To assess the linguistic knowledge of LLMs in Urdu, we present
the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of
minimally different sentences that contrast in grammatical acceptability.
UrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,
carefully curated using the Urdu Treebank and diverse Urdu text corpora. A
human evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator
agreement, confirming the reliability of the dataset. We evaluate twenty
multilingual LLMs on UrBLiMP, revealing significant variation in performance
across linguistic phenomena. While LLaMA-3-70B achieves the highest average
accuracy (94.73%), its performance is statistically comparable to other top
models such as Gemma-3-27B-PT. These findings highlight both the potential and
the limitations of current multilingual LLMs in capturing fine-grained
syntactic knowledge in low-resource languages.

</details>


### [6] [Cross-Domain Web Information Extraction at Pinterest](https://arxiv.org/abs/2508.01096)
*Michael Farag,Patrick Halina,Andrey Zaytsev,Alekhya Munagala,Imtihan Ahmed,Junhao Wang*

Main category: cs.CL

> 本文描述了Pinterest的一种高效且具有成本效益的网页属性提取系统，该系统使用了一个结合文本、视觉和结构信息的紧凑网页表示方法，能够在大规模部署中实现比大型语言模型更好的性能。

<details>
  <summary>Details</summary>

**Motivation:** 互联网提供了大量非结构化信息的宝库，但将其转化为结构化格式是一个巨大的挑战。为此，Pinterest需要一种能够从电子商务网站中准确提取结构化产品数据的技术，以提升用户体验和改进内容分发。

**Method:** 本文介绍了一种新颖的网页表示方法，该方法融合了结构、视觉和文本三种模态，并且将每个可见的HTML节点及其文本、样式和布局信息进行了紧密的结合。这种方法改善了小模型的学习效果，通过简单的模型如eXtreme Gradient Boosting（XGBoost）可以实现比大型语言模型（LLMs）如Generative Pre-trained Transformer（GPT）更准确的属性抽取。

**Result:** 研究结果表明，该系统具备较高的可扩展性，能够每秒处理超过1,000个URL，而且相比最便宜的GPT替代品成本效益提高了1000倍。

**Conclusion:** 本文展示了如何以一种高效且经济的方式进行大规模的电子商务网站产品属性抽取，通过使用紧凑和优化的网页表示方法，简单模型就能够实现高准确率和高效率的属性提取。

**Abstract:** The internet offers a massive repository of unstructured information, but
it's a significant challenge to convert this into a structured format. At
Pinterest, the ability to accurately extract structured product data from
e-commerce websites is essential to enhance user experiences and improve
content distribution. In this paper, we present Pinterest's system for
attribute extraction, which achieves remarkable accuracy and scalability at a
manageable cost. Our approach leverages a novel webpage representation that
combines structural, visual, and text modalities into a compact form,
optimizing it for small model learning. This representation captures each
visible HTML node with its text, style and layout information. We show how this
allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract
attributes more accurately than much more complex Large Language Models (LLMs)
such as Generative Pre-trained Transformer (GPT). Our results demonstrate a
system that is highly scalable, processing over 1,000 URLs per second, while
being 1000 times more cost-effective than the cheapest GPT alternatives.

</details>


### [7] [Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates](https://arxiv.org/abs/2508.01159)
*Liam G. McCoy,Fateme Nateghi Haredasht,Kanav Chopra,David Wu,David JH Wu,Abass Conteh,Sarita Khemani,Saloni Kumar Maharaj,Vishnu Ravi,Arth Pahwa,Yingjie Weng,Leah Rosengaus,Lena Giang,Kelvin Zhenghao Li,Olivia Jee,Daniel Shirvani,Ethan Goh,Jonathan H. Chen*

Main category: cs.CL

> 研究评估了前沿大型语言模型生成结构化临床咨询模板的能力，并发现虽然模型能够生成全面的模板，但在长度控制和优先问题处理方面存在不足，特别是在以叙事为主导的领域。

<details>
  <summary>Details</summary>

**Motivation:** 评估大型语言模型生成结构化临床咨询模板的能力，以改善医生之间的结构化临床信息交流。

**Method:** 采用多智能体管道，结合提示优化、语义自动评分和优先级分析，评估大型语言模型生成结构化临床咨询模板的能力。

**Result:** 尽管像o3这样的模型在全面性上达到了92.2%，但它们生成的模板过长且未能正确地在长度限制内优先处理最重要的临床问题。性能在各专科领域有所差异，特别是在以叙事为主导的领域，如精神病学和疼痛医学。

**Conclusion:** 大型语言模型可以改善医生之间的结构化临床信息交流，同时也强调了需要更强大的评估方法来捕捉模型在时间限制内优先处理临床相关信息的能力。

**Abstract:** This study evaluates the capacity of large language models (LLMs) to generate
structured clinical consultation templates for electronic consultation. Using
145 expert-crafted templates developed and routinely used by Stanford's
eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,
Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to
produce clinically coherent, concise, and prioritized clinical question
schemas. Through a multi-agent pipeline combining prompt optimization, semantic
autograding, and prioritization analysis, we show that while models like o3
achieve high comprehensiveness (up to 92.2\%), they consistently generate
excessively long templates and fail to correctly prioritize the most clinically
important questions under length constraints. Performance varies across
specialties, with significant degradation in narrative-driven fields such as
psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance
structured clinical information exchange between physicians, while highlighting
the need for more robust evaluation methods that capture a model's ability to
prioritize clinically salient information within the time constraints of
real-world physician communication.

</details>


### [8] [CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages](https://arxiv.org/abs/2508.01161)
*Jiyu Chen,Necva Bölücü,Sarvnaz Karimi,Diego Mollá,Cécile L. Paris*

Main category: cs.CL

> 研究显示，针对不同语言的情感识别任务，对预训练的多语言大模型使用独立的语言层面的LoRA微调，是最有效的方法。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机在于应对多语言情感识别的挑战，这一挑战源于不同文化和语言中情感表达方式的差异。研究目的是实现一个能识别基于文本的基本情感状态及情感强度的情感识别器。

**Method:** 本研究通过针对不同语言的情感识别任务，采用了一系列模型适应策略，特别是对于预训练的多语言大模型使用了LoRA（低秩适应）方法，为每种语言独立进行微调，以提升模型在情感识别任务上的表现。

**Result:** 研究结果表明，为每种语言独立调整预训练多语言模型是情感识别任务中最有效的方法。

**Conclusion:** 结论是，独立为每种语言调整预训练的多语言大模型，使用LoRA设置进行微调，是当前处理多语言情感识别任务最有效的方法。

**Abstract:** Detecting emotions across different languages is challenging due to the
varied and culturally nuanced ways of emotional expressions. The
\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared
task was organised to investigate emotion recognition across different
languages. The goal of the task is to implement an emotion recogniser that can
identify the basic emotional states that general third-party observers would
attribute to an author based on their written text snippet, along with the
intensity of those emotions. We report our investigation of various
task-adaptation strategies for LLMs in emotion recognition. We show that the
most effective method for this task is to fine-tune a pre-trained multilingual
LLM with LoRA setting separately for each language.

</details>


### [9] [Adaptive Content Restriction for Large Language Models via Suffix Optimization](https://arxiv.org/abs/2508.01198)
*Yige Li,Peihai Jiang,Jun Sun,Peng Shu,Tianming Liu,Zhen Xiang*

Main category: cs.CL

> 文章提出了名为Adaptive Content Restriction (AdaCoRe)的新任务，以及一种名为Suffix Optimization (SOP)的方法，这种方法通过附加工优化后的简短后缀来限制语言模型生成受限内容。研究通过CoReBench基准测试验证了SOP的有效性，并展示了其在实际场景中的应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** 解决针对特定使用场景的大语言模型内容限制需求，而不过度依赖高效的计算、数据和存储资源。

**Method:** Suffix Optimization (SOP), 一种无需模型微调的轻量级策略，通过在任意提示后附加一个优化后的简短后缀来防止指定的大模型生成受限条款，同时保持输出质量。

**Result:** 提出的SOP方法在Content Restriction Benchmark (CoReBench)上的表现优于系统级基线方法，在多个大语言模型上平均限制率高出6%到17%不等。同时，证明了SOP在现实场景中具有实用性。

**Conclusion:** Suffix Optimization (SOP)作为一种轻量级策略，有效地满足了特定使用场景下对语言模型进行内容限制的需求，且不影响输出质量，对实际应用具有很高的实用价值。

**Abstract:** Large Language Models (LLMs) have demonstrated significant success across
diverse applications. However, enforcing content restrictions remains a
significant challenge due to their expansive output space. One aspect of
content restriction is preventing LLMs from generating harmful content via
model alignment approaches such as supervised fine-tuning (SFT). Yet, the need
for content restriction may vary significantly across user groups, change
rapidly over time, and not always align with general definitions of
harmfulness. Applying SFT to each of these specific use cases is impractical
due to the high computational, data, and storage demands. Motivated by this
need, we propose a new task called \textit{Adaptive Content Restriction}
(AdaCoRe), which focuses on lightweight strategies -- methods without model
fine-tuning -- to prevent deployed LLMs from generating restricted terms for
specific use cases. We propose the first method for AdaCoRe, named
\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to
any prompt to a) prevent a target LLM from generating a set of restricted
terms, while b) preserving the output quality. To evaluate AdaCoRe approaches,
including our SOP, we create a new \textit{Content Restriction Benchmark}
(CoReBench), which contains 400 prompts for 80 restricted terms across 8
carefully selected categories. We demonstrate the effectiveness of SOP on
CoReBench, which outperforms the system-level baselines such as system suffix
by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B,
Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also
demonstrate that SOP is effective on POE, an online platform hosting various
commercial LLMs, highlighting its practicality in real-world scenarios.

</details>


### [10] [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)
*Shengqi Zhu,Jeffrey M. Rzeszotarski,David Mimno*

Main category: cs.CL

> 文章探讨了分割聊天查询以分析用户行为的方法，并发现查询模式随时间变化，用户行为受模型能力影响。

<details>
  <summary>Details</summary>

**Motivation:** 聊天日志提供了关于LLM用户丰富信息的来源，但用户行为的模式经常被查询的可变性所掩盖。这项任务的目的是揭示聊天查询中的这些隐藏模式。

**Method:** 提出了一项新任务，即将聊天查询分割为请求内容、角色、查询特定上下文和附加表达。

**Result:** 发现，尽管聊天交互很常见，但在LLM查询中的请求制作与人类之间的交互显著不同。查询模式在早期以强调请求为主，随着时间的推移，个体用户探索不同的模式但倾向于随着经验的积累而收敛。模型能力会影响用户行为，尤其是在引入新模型时，这些影响在社区层面上是可追溯的。

**Conclusion:** 通过这项任务，引入了一个重要的时间序列分析用户表达的视角，发现查询模式随时间和用户经验变化，模型能力对用户行为有显著影响。

**Abstract:** Chat logs provide a rich source of information about LLM users, but patterns
of user behavior are often masked by the variability of queries. We present a
new task, segmenting chat queries into contents of requests, roles,
query-specific context, and additional expressions. We find that, despite the
familiarity of chat-based interaction, request-making in LLM queries remains
significantly different from comparable human-human interactions. With the data
resource, we introduce an important perspective of diachronic analyses with
user expressions. We find that query patterns vary between early ones
emphasizing requests, and individual users explore patterns but tend to
converge with experience. Finally, we show that model capabilities affect user
behavior, particularly with the introduction of new models, which are traceable
at the community level.

</details>


### [11] [WebDS: An End-to-End Benchmark for Web-based Data Science](https://arxiv.org/abs/2508.01222)
*Ethan Hsu,Hong Meng Yam,Ines Bouissou,Aaron Murali John,Raj Thota,Josh Koe,Vivek Sarath Putta,G K Dharesan,Alexander Spangher,Shikhar Murty,Tenghao Huang,Christopher D. Manning*

Main category: cs.CL

> 该论文提出了WebDS，这是一个针对网络数据科学基准的新框架，它由870个基于网络的数据科学任务组成，涵盖了29个不同的网站。这些任务要求代理执行复杂且多步骤的操作，并使用各种工具和异构数据格式，以此来评估现有的顶尖大型语言模型（LLM）的性能。结果显示，当前的LLM代理在完成这些任务方面存在显著性能差距。

<details>
  <summary>Details</summary>

**Motivation:** 作者指出，现有的网络基准通常侧重于简单的互动，如表单提交或电子商务交易，而传统的数据科学基准则集中在静态的、文本绑定的数据集上，忽略了端到端的工作流程。因此，提出了WebDS，以填补这些空白，提供一个更稳健且现实的测试环境，从而推动LLM在网络上进行数据科学的实际应用的发展。

**Method:** WebDS由870个基于网络的数据科学任务组成，这涉及从有结构的政府数据门户到无结构的新闻媒体等29个不同的网站。参与者需执行复杂、多步骤的操作，要求使用工具并处理异构数据格式。

**Result:** 对现有顶尖LLM代理的评估显示，它们在WebDS任务中的表现显著低于其他方面的基准。例如，打印交互平均成绩为15%，远低于在其他基准中的表现。这表明新的失败模式，如信息关联不足，重复行为和取捷径，是造成这一差距的主要原因。

**Conclusion:** 通过提供一个更复杂且更具代表性的测试环境，WebDS为LLM在数据科学领域的实际应用提供了重要的基础。该研究旨在推动大型语言模型克服新的挑战，从而提升其实用效能。

**Abstract:** A large portion of real-world data science tasks are complex and require
multi-hop web-based interactions: finding appropriate data available on the
internet, synthesizing real-time data of various modalities from different
locations, and producing summarized analyses. Existing web benchmarks often
focus on simplistic interactions, such as form submissions or e-commerce
transactions, and often do not require diverse tool-using capabilities required
for web based data science. Conversely, traditional data science benchmarks
typically concentrate on static, often textually bound datasets and do not
assess end-to-end workflows that encompass data acquisition, cleaning,
analysis, and insight generation. In response, we introduce WebDS, the first
end-to-end web-based data science benchmark. It comprises 870 web-based data
science tasks across 29 diverse websites from structured government data
portals to unstructured news media, challenging agents to perform complex,
multi-step operations requiring the use of tools and heterogeneous data formats
that better reflect the realities of modern data analytics. Evaluations of
current SOTA LLM agents indicate significant performance gaps in accomplishing
these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web
Voyager, successfully completes only 15% of tasks in WebDS, which our analysis
suggests is due to new failure modes like poor information grounding,
repetitive behavior and shortcut-taking that agents performing WebDS' tasks
display. By providing a more robust and realistic testing ground, WebDS sets
the stage for significant advances in the development of practically useful
LLM-based data science.

</details>


### [12] [WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework](https://arxiv.org/abs/2508.01245)
*Yue Chen,Minghua He,Fangkai Yang,Pu Zhao,Lu Wang,Yu Kang,Yifei Dong,Yuefeng Zhan,Hao Sun,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.CL

> 提出WarriorMath框架，通过缺陷感知的数据生成和渐进式训练提升LLMs解决数学问题的能力，实验结果在六个数学基准上平均提高了12.57%，达到新的SOTA。

<details>
  <summary>Details</summary>

**Motivation:** 现有的数据增强方法通过改写或难度提升来扩充数据集，但忽视了LLMs的特定失败模式。这导致生成的问题仍然是模型能够解决的，因此性能提升有限。

**Method:** 提出了一种称为WarriorMath的缺陷感知框架，该框架结合了目标数据合成和渐进式训练，以解决数学问题。在数据合成阶段，使用多个专家LLM协作生成、评估并完善问题。通过识别和迭代改进基础LLM无法解答的问题，产生高质量、缺陷感知的训练数据。在训练阶段，引入渐进式学习框架，通过针对模型弱点的逐步精细化训练，使用不断挑战的数据集进行迭代细化。

**Result:** 在六个数学基准测试中，WarriorMath框架相比强基线提高性能12.57%。

**Conclusion:** 实验结果证明了缺陷感知、多专家框架在提升LLMs解决数学问题能力方面的有效性。

**Abstract:** Large Language Models (LLMs) excel in solving mathematical problems, yet
their performance is often limited by the availability of high-quality, diverse
training data. Existing methods focus on augmenting datasets through rephrasing
or difficulty progression but overlook the specific failure modes of LLMs. This
results in synthetic questions that the model can already solve, providing
minimal performance gains. To address this, we propose WarriorMath, a
defect-aware framework for mathematical problem solving that integrates both
targeted data synthesis and progressive training. In the synthesis stage, we
employ multiple expert LLMs in a collaborative process to generate, critique,
and refine problems. Questions that base LLMs fail to solve are identified and
iteratively improved through expert-level feedback, producing high-quality,
defect-aware training data. In the training stage, we introduce a progressive
learning framework that iteratively fine-tunes the model using increasingly
challenging data tailored to its weaknesses. Experiments on six mathematical
benchmarks show that WarriorMath outperforms strong baselines by 12.57% on
average, setting a new state-of-the-art. Our results demonstrate the
effectiveness of a defect-aware, multi-expert framework for improving
mathematical ability.

</details>


### [13] [Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025](https://arxiv.org/abs/2508.01263)
*Long S. T. Nguyen,Khang H. N. Vo,Thu H. A. Nguyen,Tuan C. Bui,Duc Q. Nguyen,Thanh-Tung Tran,Anh D. Nguyen,Minh L. Nguyen,Fabien Baldacci,Thang H. Bui,Emanuel Di Nardo,Angelo Ciaramella,Son H. Le,Ihsan Ullah,Lorenzo Di Rocco,Tho T. Quan*

Main category: cs.CL

> 本文介绍了XAI挑战2025，一个促进教育领域中AI的透明度和解释性的黑客松比赛，并提供了对未来基于XAI的教育系统的见解。

<details>
  <summary>Details</summary>

**Motivation:** 比赛旨在通过促进透明度和解释性来解决教育领域中日益增长的AI需求。本文分析了比赛的动机、结构、数据集构建和评估协议，强调了将轻量级大规模语言模型和符号推理相结合以提高可解释性的重要性。

**Method:** 本文介绍了XAI挑战2025，这是一个由胡志明市理工大学和神经符号AI可信性和可靠性国际研讨会联合组织的黑客松比赛，作为2025年神经网络国际联合会议的一部分。比赛要求参赛者构建能够回答学生关于大学政策的问题，并生成清晰的基于逻辑的自然语言解释的问答系统。为了促进透明度和可信度，解决方案必须使用轻量级的大规模语言模型(LLMs)或混合的LLM-符号系统。

**Result:** 最终，比赛结果提供了对未来基于XAI的教育系统以及竞争性研究项目的有操作性的见解。

**Conclusion:** 通过构建能够提供清晰逻辑解释的问答系统，本文展示了在教育领域中，将轻量级大规模语言模型和符号推理相结合的重要价值。这些发现对未来基于XAI的教育系统的设计和实施具有行动导向的启示。

**Abstract:** The growing integration of Artificial Intelligence (AI) into education has
intensified the need for transparency and interpretability. While hackathons
have long served as agile environments for rapid AI prototyping, few have
directly addressed eXplainable AI (XAI) in real-world educational contexts.
This paper presents a comprehensive analysis of the XAI Challenge 2025, a
hackathon-style competition jointly organized by Ho Chi Minh City University of
Technology (HCMUT) and the International Workshop on Trustworthiness and
Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International
Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked
participants with building Question-Answering (QA) systems capable of answering
student queries about university policies while generating clear, logic-based
natural language explanations. To promote transparency and trustworthiness,
solutions were required to use lightweight Large Language Models (LLMs) or
hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed
via logic-based templates with Z3 validation and refined through expert student
review to ensure alignment with real-world academic scenarios. We describe the
challenge's motivation, structure, dataset construction, and evaluation
protocol. Situating the competition within the broader evolution of AI
hackathons, we argue that it represents a novel effort to bridge LLMs and
symbolic reasoning in service of explainability. Our findings offer actionable
insights for future XAI-centered educational systems and competitive research
initiatives.

</details>


### [14] [Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities](https://arxiv.org/abs/2508.01290)
*Zhichao Yan,Jiapu Wang,Jiaoyan Chen,Yanyan Wang,Hongye Tan,Jiye Liang,Xiaoli Li,Ru Li,Jeff Z. Pan*

Main category: cs.CL

> 研究展示了LLMs可以通过已嵌入的部分相关知识被激活，提出未见实体KGQA任务来应对现实挑战，并证明了该方法在实际应用中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 挑战在于如何有效地利用部分相关知识，特别是在知识库检索不完整的情况下。LLMs可以通过已嵌入的部分相关知识被激活，这一观点与传统观点相反。

**Method:** 通过从包含答案的路径中移除路径来构造不完全相关的知识，使用位于黄金推理路径中的三元组及其变体。此外，提出了一个新任务，未见实体KGQA，以模拟由于KG不完整导致实体链接失败的现实世界挑战。

**Result:** 实验表明，基于唤醒的方法在两个知识图谱问答数据集上优于依赖嵌入相似性的传统方法，后者倾向于返回噪声信息。

**Conclusion:** 基于唤醒的方法在实践中更为有效，尤其在应对知识库不完整导致的实体链接失败时。

**Abstract:** Retrieval-Augmented Generation (RAG) shows impressive performance by
supplementing and substituting parametric knowledge in Large Language Models
(LLMs). Retrieved knowledge can be divided into three types: explicit answer
evidence, implicit answer clue, and insufficient answer context which can be
further categorized into totally irrelevant and partially relevant information.
Effectively utilizing partially relevant knowledge remains a key challenge for
RAG systems, especially in incomplete knowledge base retrieval. Contrary to the
conventional view, we propose a new perspective: LLMs can be awakened via
partially relevant knowledge already embedded in LLMs. To comprehensively
investigate this phenomenon, the triplets located in the gold reasoning path
and their variants are used to construct partially relevant knowledge by
removing the path that contains the answer. We provide theoretical analysis of
the awakening effect in LLMs and support our hypothesis with experiments on two
Knowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we
present a new task, Unseen Entity KGQA, simulating real-world challenges where
entity linking fails due to KG incompleteness. Our awakening-based approach
demonstrates greater efficacy in practical applications, outperforms
traditional methods that rely on embedding-based similarity which are prone to
returning noisy information.

</details>


### [15] [KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference](https://arxiv.org/abs/2508.01302)
*Chenming Tang,Yutong Yang,Yunfang Wu*

Main category: cs.CL

> 本文提出了一种新的知识编辑对齐方法KEDAS，该方法在实验中表现出了比现有方法更优的编辑成功、局部性和可移植性等性能指标。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在提出更有效地修改大语言模型中过时知识的方法，同时保留其强大的能力，对比当前主要依赖参数级编辑或基于检索的方法，提出了一种新的方法。

**Method:** 本文提出了知识编辑对齐与多样增强和自适应推理(KEDAS)方法，以更好地实现大语言模型的知识编辑对齐。对齐阶段，模型通过低秩适应学会应用上下文编辑的知识。在编辑阶段，设计了一种多样编辑增强技术来提高编辑的召回率。之后，提出了一种自适应后对齐推理机制，其中运用过滤器的智能检索器进行动态推理路由选择。具体来说，不相关的查询直接通过原来的预对齐模型处理，而相关的查询及其相关的编辑则通过带有激活适配器的模型进行处理。

**Result:** 实验结果显示KEDAS在4个数据集上的36个案例中有35个取得了最高的总体分数，并且在编辑成功、局部性和可移植性方面的调和平均分数比最强的知识编辑对齐方法高约19.8。与参数级编辑和基于检索的基线相比，它有显著的性能优势。

**Conclusion:** KEDAS展示了其在知识编辑对齐方面的理想范式，通过分析计算成本和一般任务上的表现，进一步验证了其鲁棒性和效率。

**Abstract:** Knowledge editing aims to modify outdated knowledge in large language models
(LLMs) efficiently while retaining their powerful capabilities. Most existing
methods rely on either parameter-level editing or retrieval-based approaches.
In this work, we propose Knowledge Editing alignment with Diverse Augmentation
and Self-adaptive inference (KEDAS) to better align LLMs with knowledge
editing. In the alignment phase, LLMs learn to apply in-context edited
knowledge via low-rank adaptation. During editing, we design a diverse edit
augmentation technique to improve the recall of edits. After that, a
self-adaptive post-alignment inference mechanism is proposed, in which a
filter-based smart retriever is employed to perform a dynamic selection of
inference routing. Specifically, irrelevant queries will go through the
original pre-alignment model directly, while relevant ones, together with their
related edits, go through the model with aligned adapters activated. In
experiments, KEDAS secures the highest overall performance scores in 35 out of
36 cases across four datasets with three LLMs on three settings, surpassing its
strong knowledge editing alignment counterpart by about 19.8 harmonic mean
scores of edit success, locality and portability and outperforming both
parameter editing and retrieval-based baselines significantly. Analysis of
computational cost and performance on general tasks further validates the
robustness and efficiency of KEDAS, indicating that it presents an ideal
paradigm of knowledge editing alignment.

</details>


### [16] [D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation](https://arxiv.org/abs/2508.01309)
*Weibo Zhou,Lingbo Li,Shangsong Liang*

Main category: cs.CL

> D-SCoRE是一个无需训练、利用大型语言模型生成高质量、多样化QA数据集的流水线，它克服了现有QA生成方案的局限性，并在领域特定的微调中有很好的表现。

<details>
  <summary>Details</summary>

**Motivation:** 高成本和高质量QA数据集的稀缺限制了领域特定大型语言模型的有监督微调（SFT）。为了应对这一挑战，引入了D-SCoRE流水线，以生成高质量的QA数据集，从而解决数据限制问题。

**Method:** 介绍了D-SCoRE，一个无需训练的流水线，利用大型语言模型（LLM）和提示工程从任意文本源生成多样、高质量的问题-回答（QA）数据集。D-SCoRE 结合了文档中心处理、分段、CoT推理及结构化导出，生成针对性强的 QA-CoT 数据集，适用于领域特定的微调。通过语义角色转换、问题类型平衡和反事实材料等多维度控制机制，D-SCoRE 增强多样性和相关性，解决了现有QA生成的局限性。

**Result:** 与使用人类标注的QA数据集（如SQuAD、Covid-QA）进行微调的模型相比，在SQuADShifts和Covid-QA测试集上，使用D-SCoRE生成的QA数据集进行微调的模型在大多数领域上表现更好。D-SCoRE能够使用8B参数规模的LLM在消费者级硬件上以90秒为单位的时间生成每个100-200字文本的六个带有反事实材料的QA-CoT对。

**Conclusion:** D-SCoRE提供了一种简单且可扩展的方法，能够在不同领域中高效生成QA数据并对模型进行高质量的微调，不依赖于训练过程。

**Abstract:** The scarcity and high cost of high-quality question-answering (QA) datasets
hinder supervised fine-tuning (SFT) for domain-specific large language models
(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that
utilizes LLMs and prompt engineering to produce diverse, high-quality QA
datasets from arbitrary textual sources. D-SCoRE integrates
$\textbf{D}$ocument-centric processing, $\textbf{S}$egmentation, $\textbf{Co}$T
$\textbf{R}$easoning, and structured $\textbf{E}$xport to generate QA-COT
datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,
such as semantic role transformation, question type balancing, and
counterfactual materials, enhance diversity and relevance, overcoming
limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA
datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on
SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most
domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual
materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade
hardware. Its simplicity and scalability enable efficient QA generation and
high-performance fine-tuning across domains.

</details>


### [17] [LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points](https://arxiv.org/abs/2508.01317)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

> 研究提出了LinkSyn框架，生成了包含50B tokens的多样化多学科问答数据集LinkQA，实验表明这能提升语言模型在多个评测数据集上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型的发展受限于高质量和多样化的训练数据不足的问题，研究旨在提供一种灵活控制领域和难度分布的方法，同时平衡知识覆盖与流行度。

**Method:** LinkSyn通过构建知识点图并利用深度优先搜索技术来生成多样化的问题-答案数据，包含知识分布优化、扩散合成和难度增强三个组成部分。

**Result:** 研究通过引入LinkSyn框架，基于知识点图生成了大规模多样化的问题-答案数据集LinkQA，显著提升了大型语言模型在多个评测数据集上的表现，建立了新的SOTA结果。

**Conclusion:** 实验表明，使用LinkQA进行持续预训练能够显著提升Llama-3 8B在MMLU和CMMLU上的性能。

**Abstract:** The advancement of large language models (LLMs) struggles with the scarcity
of high-quality, diverse training data. To address this limitation, we propose
LinkSyn, a novel knowledge point (KP) graph-based synthesis framework that
enables flexible control over discipline and difficulty distributions while
balancing KP coverage and popularity. LinkSyn extracts KPs from
question-answering (QA) seed data and constructs a KP graph to synthesize
diverse QA data from multiple seeds strongly linked by KPs and sampled from
graph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution
value function to guide the adjustment of path sampling probability and balance
KP coverage and popularity during graph walks; (2) diffusion-based synthesis
via DeepSeek-R1 by leveraging multiple seeds with dense logical associations
along each path; and (3) high-difficulty QA enhancement within given
disciplines by flexible difficulty adjustments. By executing LinkSyn, we
synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.
Extensive experiments on Llama-3 8B demonstrate that continual pre-training
with LinkQA yields an average improvement of $\mathbf{11.51\%}$ on MMLU and
CMMLU, establishing new SOTA results. LinkQA consistently enhances performance
across model size and initial FLOPs scales.

</details>


### [18] [Large-Scale Diverse Synthesis for Mid-Training](https://arxiv.org/abs/2508.01326)
*Xuemiao Zhang,Chengying Tu,Can Ren,Rongxiang Weng,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.CL

> 论文通过设计一个新的数据合成框架来创建高质量且多样化的问答数据集BoostQA，显著提升了大型语言模型在多个领域的性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的语料库提供的信息有限，限制了高质量知识密集型训练数据的发展，影响大型语言模型的发展。为了克服跨领域环境中问答数据可扩展性和知识多样性的挑战，同时检测模型在STEM学科和高难度数据上的不足，我们设计了一个多元化流水线来合成新的数据集。

**Method:** 提出了一种新颖的多样化流水线来合成BoostQA，这是一种1000亿标记的大规模QA数据集。其合成框架通过从异构来源精选种子数据、利用DeepSeek-R1实施STEM专注于多级合成以增强数据多样性和高难度数据合成减弱难度退化、通过DeepSeek-V3完善答案来提高输出质量。

**Result:** 使用BoostQA在中期训练中优化领域特定知识的获取并提高数据质量。这种方法使Llama-3 8B模型在中期训练阶段使用40B标记数据集时，在MMLU和CMMLU上取得平均12.74%的性能提升，并在12个基准测试中展现出优越性能。此外，BoostQA表现出了强大的可扩展性，随着模型规模、数据量和初始FLOPs的增长，性能持续提升。

**Conclusion:** 提出的方法通过合成新的高质量多样化的QA数据集BoostQA，增强了语言模型在多领域和高难度任务上的性能，特别是在STEM学科和多个基准测试中的表现。

**Abstract:** The scarcity of high-quality, knowledge-intensive training data hinders the
development of large language models (LLMs), as traditional corpora provide
limited information. Previous studies have synthesized and integrated
corpora-dependent question-answering (QA) data to improve model performance but
face challenges in QA data scalability and knowledge diversity, particularly in
cross-domain contexts. Furthermore, leveraging our designed discipline and
difficulty annotation system, we probe model deficiencies in STEM disciplines
and high-difficulty data. To overcome these limitations, we propose a novel
diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA
dataset. Our synthesis framework: (1) curates seed data from heterogeneous
sources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade
synthesis to boost data diversity and high-difficulty synthesis to mitigate
difficulty degradation; (3) refines answers via DeepSeek-V3 to improve output
quality. We utilize BoostQA in mid-training, a mid-stage between pre-training
and post-training, to optimize domain-specific knowledge acquisition and
enhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token
dataset, to achieve an average improvement of $\mathbf{12.74\%}$ on MMLU and
CMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also
demonstrates robust scalability, with performance consistently improving as
model size, data volume, and initial FLOPs scale.

</details>


### [19] [MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis](https://arxiv.org/abs/2508.01370)
*Roman Koshkin,Pengyu Dai,Nozomi Fujikawa,Masahito Togami,Marco Visentini-Scarzanella*

Main category: cs.CL

> 一种利用LLMs的自动化框架，将端到端的业务分析和市场报告生成过程实现自动化。通过四个专门的代理合作，该框架能够完成报告的生成，并且通过自动化审查和迭代机制提升报告质量，实验显示其效果显著。

<details>
  <summary>Details</summary>

**Motivation:** 动机是为了实现高度自动化的市场报告生成过程，并能够以较低成本快速提供详尽的6页市场报告，整个过程只需7分钟，成本大约1美元。

**Method:** 我们提出了一种自主框架，该框架利用大型语言模型（LLMs）来实现从数据查询到市场报告生成的端到端业务分析的自动化。框架的核心是四个专门的代理：研究员（Researcher）、审核员（Reviewer）、作者（Writer）和检索员（Retriever），这些代理互相协作分析数据并生成报告。这些代理通过上下文学习，从亚马逊的真实专业顾问的演示材料中学习，以模仿专业的分析方法。框架执行的过程包括数据查询、数据分析、产生见解、创建可视化和编写市场报告。

**Result:** 实验结果表明，通过自动化审查周期和顾问的非结构化知识，报告的质量可以得到提升。我们的方法与人类专家的评估一致，同时实现了迭代改进机制，通过自动审查周期优化报告质量。

**Conclusion:** 本研究提出的框架是自动低成本生成市场见解的重要一步，展示了一种能够自动生成市场报告并不断提高质量的技术方法。

**Abstract:** We present an autonomous framework that leverages Large Language Models
(LLMs) to automate end-to-end business analysis and market report generation.
At its core, the system employs specialized agents - Researcher, Reviewer,
Writer, and Retriever - that collaborate to analyze data and produce
comprehensive reports. These agents learn from real professional consultants'
presentation materials at Amazon through in-context learning to replicate
professional analytical methodologies. The framework executes a multi-step
process: querying databases, analyzing data, generating insights, creating
visualizations, and composing market reports. We also introduce a novel
LLM-based evaluation system for assessing report quality, which shows alignment
with expert human evaluations. Building on these evaluations, we implement an
iterative improvement mechanism that optimizes report quality through automated
review cycles. Experimental results show that report quality can be improved by
both automated review cycles and consultants' unstructured knowledge. In
experimental validation, our framework generates detailed 6-page reports in 7
minutes at a cost of approximately \$1. Our work could be an important step to
automatically create affordable market insights.

</details>


### [20] [MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs](https://arxiv.org/abs/2508.01401)
*Ahmad Rezaie Mianroodi,Amirali Rezaie,Niko Grisel Todorov,Cyril Rakovski,Frank Rudzicz*

Main category: cs.CL

> MedSynth, a new dataset of synthetic medical dialogues and notes, is deemed as a valuable resource for training models in generating medical documentation from patient interactions, offering improvements and accessibility for enhancing medical documentation automation.

<details>
  <summary>Details</summary>

**Motivation:** This research aims to address the substantial burden on physicians associated with documenting clinical encounters, which is known to contribute to professional burnout, by providing a robust dataset for developing automation tools in medical documentation.

**Method:** We introduce MedSynth, a unique dataset of synthetic medical dialogues and corresponding notes, aimed at enhancing the performance of models in the Dial-2-Note and Note-2-Dial tasks. This dataset is informed by an analysis of disease distributions and includes more than 10,000 dialogue-note pairs covering over 2000 ICD-10 codes.

**Result:** The dataset demonstrates marked improvement in the performance of models for generating medical notes from dialogues, and dialogues from medical notes, providing a significant advancement in the field of medical documentation automation.

**Conclusion:** The MedSynth dataset not only fills the gap in accessible, privacy-compliant, and diverse training data but also advances the Dialogue-to-Note and Note-to-Dialogue tasks, supporting the development of efficient tools for medical documentation that could alleviate the documentation burden on healthcare professionals.

**Abstract:** Physicians spend significant time documenting clinical encounters, a burden
that contributes to professional burnout. To address this, robust automation
tools for medical documentation are crucial. We introduce MedSynth -- a novel
dataset of synthetic medical dialogues and notes designed to advance the
Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.
Informed by an extensive analysis of disease distributions, this dataset
includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We
demonstrate that our dataset markedly enhances the performance of models in
generating medical notes from dialogues, and dialogues from medical notes. The
dataset provides a valuable resource in a field where open-access,
privacy-compliant, and diverse training data are scarce. Code is available at
https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available
at https://huggingface.co/datasets/Ahmad0067/MedSynth.

</details>


### [21] [ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations](https://arxiv.org/abs/2508.01411)
*Rania Al-Sabbagh*

Main category: cs.CL

> ArzEn-MultiGenre是一个高质量的埃及阿拉伯语和英语平行对比数据集，对机器翻译模型的训练和研究有重要价值。

<details>
  <summary>Details</summary>

**Motivation:** 作者旨在创建一个用于机器翻译模型的新基准数据集，同时也可用于跨语言分析和其他领域研究。

**Method:** 描述了ArzEn-MultiGenre数据集的构成和创建过程，这是一个包含埃及阿拉伯语和英语平行对比的歌曲歌词、小说和电视字幕的数据集。

**Result:** 数据集包括25,557个段落配对，数据经过人工翻译和对齐，能用于多种研究场景及教育用途。

**Conclusion:** 该数据集有两个主要贡献：一是包含了现有平行数据集中未有的文本体裁；二是它是专业人员翻译和对齐的高质量数据集。

**Abstract:** ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,
novels, and TV show subtitles that are manually translated and aligned with
their English counterparts. The dataset contains 25,557 segment pairs that can
be used to benchmark new machine translation models, fine-tune large language
models in few-shot settings, and adapt commercial machine translation
applications such as Google Translate. Additionally, the dataset is a valuable
resource for research in various disciplines, including translation studies,
cross-linguistic analysis, and lexical semantics. The dataset can also serve
pedagogical purposes by training translation students and aid professional
translators as a translation memory. The contributions are twofold: first, the
dataset features textual genres not found in existing parallel Egyptian Arabic
and English datasets, and second, it is a gold-standard dataset that has been
translated and aligned by human experts.

</details>


### [22] [Discovering Bias Associations through Open-Ended LLM Generations](https://arxiv.org/abs/2508.01412)
*Jinhao Pan,Chahat Raj,Ziwei Zhu*

Main category: cs.CL

> 研究提出了一种名为BADF的框架，用于提取和分析大型语言模型中的身份概念关联，以更全面地了解开放生成中的偏见。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的评估方法依赖于预定义的身份概念关联，限制了它们检测新出现或未预期偏见形式的能力，因此需要一种新的方法来更全面地检测和分析语言模型中的偏见。

**Method:** 本研究提出了Bias Association Discovery Framework (BADF)，这是一种系统性方法，可以从大规模语言模型产生的开放式输出中提取出已知和未知的身份概念之间的关联。

**Result:** BADF方法在多种模型和实际场景下的实验中都显示出可以稳健地绘制和分析描述身份概念的各类特征，揭示了语言模型中的偏见。

**Conclusion:** BADF为识别和分析大规模语言模型中不同形式的偏见提供了一个可扩展的工具，有助于深入理解生成语言中的歧视问题。

**Abstract:** Social biases embedded in Large Language Models (LLMs) raise critical
concerns, resulting in representational harms -- unfair or distorted portrayals
of demographic groups -- that may be expressed in subtle ways through generated
language. Existing evaluation methods often depend on predefined
identity-concept associations, limiting their ability to surface new or
unexpected forms of bias. In this work, we present the Bias Association
Discovery Framework (BADF), a systematic approach for extracting both known and
previously unrecognized associations between demographic identities and
descriptive concepts from open-ended LLM outputs. Through comprehensive
experiments spanning multiple models and diverse real-world contexts, BADF
enables robust mapping and analysis of the varied concepts that characterize
demographic identities. Our findings advance the understanding of biases in
open-ended generation and provide a scalable tool for identifying and analyzing
bias associations in LLMs. Data, code, and results are available at
https://github.com/JP-25/Discover-Open-Ended-Generation

</details>


### [23] [From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs](https://arxiv.org/abs/2508.01424)
*Haonan Bian,Yutao Qi,Rui Yang,Yuanxi Che,Jiaqian Wang,Heming Xia,Ranran Zhen*

Main category: cs.CL

> 提出一种名为ORACLE的新框架，通过结合大型语言模型和知识图谱来克服复杂多跳问答中的推理挑战。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型虽然在问答任务上表现突出，但在需要非线性和结构化推理的复杂多跳问答任务上表现出局限性，较难捕捉实体间的深入概念关系。

**Method:** ORACLE框架结合了大语言模型的生成能力和知识图谱的结构优势，分为动态构建问题特定的知识本体、将这些本体转换为一阶逻辑推理链、以及系统性地分解原始查询为逻辑连贯的子问题三个阶段。

**Result:** 实验结果显示，该框架在多个标准的复杂多跳问答数据集上取得了与当前最先进模型相当的性能。

**Conclusion:** 详细分析表明，所提出的方法不仅提高了合理性和解释性，还展示了每个组件的有效性。

**Abstract:** Large Language Models (LLMs), despite their success in question answering,
exhibit limitations in complex multi-hop question answering (MQA) tasks that
necessitate non-linear, structured reasoning. This limitation stems from their
inability to adequately capture deep conceptual relationships between entities.
To overcome this challenge, we present **ORACLE** (**O**ntology-driven
**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a
training-free framework that combines LLMs' generative capabilities with the
structural benefits of knowledge graphs. Our approach operates through three
stages: (1) dynamic construction of question-specific knowledge ontologies
using LLMs, (2) transformation of these ontologies into First-Order Logic
reasoning chains, and (3) systematic decomposition of the original query into
logically coherent sub-questions. Experimental results on several standard MQA
benchmarks show that our framework achieves highly competitive performance,
rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses
further confirm the effectiveness of each component, while demonstrating that
our method generates more logical and interpretable reasoning chains than
existing approaches.

</details>


### [24] [Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data](https://arxiv.org/abs/2508.01450)
*Xinlin Zhuang,Feilong Tang,Haolin Yang,Ming Hu,Huifa Li,Haochen Xue,Yichen Li,Junjun He,Zongyuan Ge,Ying Qian,Imran Razzak*

Main category: cs.CL

> 文章提出一种新的数据选择策略DIQ，该方法结合样本难度和梯度影响来选择训练数据，以提高大型语言模型在医疗推理中的性能和效率。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法依赖未经筛选的数据集，数据中包含冗余和低质量样本，导致计算成本高且性能不佳。现有方法仅根据样本难度选择数据，忽视了每个样本对优化的贡献。

**Method:** 通过结合难度和梯度影响来进行数据选择，选择高难度与高梯度影响力样本，以平衡复杂的临床推理与参数的显著影响，该方法称为DIQ（难度-影响象限）。

**Result:** 使用DIQ精选的数据子集在临床推理方面更加符合专家实践，在病灶诊断、安全检查和证据引用方面表现良好。实验表明，仅用精选1%的数据微调即可达到全数据集性能，使用10%的数据则明显优于基线。

**Conclusion:** DIQ方法通过选择具有高难度和高梯度影响的样本，能够有效提高模型在医疗推理任务上的表现，并减少对大量数据的需求。

**Abstract:** Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language
Models (LLMs) to specialized domains such as medical reasoning. However,
existing SFT practices often rely on unfiltered datasets that contain redundant
and low-quality samples, leading to substantial computational costs and
suboptimal performance. Although existing methods attempt to alleviate this
problem by selecting data based on sample difficulty, defined by knowledge and
reasoning complexity, they overlook each sample's optimization utility
reflected in its gradient. Interestingly, we find that gradient-based influence
alone favors easy-to-optimize samples that cause large parameter shifts but
lack deep reasoning chains, while difficulty alone selects noisy or overly
complex cases that fail to guide stable optimization. Based on this
observation, we propose a data selection strategy, Difficulty-Influence
Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence
quadrant to balance complex clinical reasoning with substantial gradient
influence, enabling efficient medical reasoning with minimal fine-tuning data.
Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected
subsets demonstrate higher data quality and generate clinical reasoning that is
more aligned with expert practices in differential diagnosis, safety check, and
evidence citation, as DIQ emphasizes samples that foster expert-like reasoning
patterns. Extensive experiments on medical reasoning benchmarks demonstrate
that DIQ enables models fine-tuned on only 1% of selected data to match
full-dataset performance, while using 10% consistently outperforms the
baseline, highlighting the superiority of principled data selection over
brute-force scaling. The code and data are available at
https://github.com/mihara-bot/DIQ.

</details>


### [25] [TreeDiff: AST-Guided Code Generation with Diffusion LLMs](https://arxiv.org/abs/2508.01473)
*Yiming Zeng,Jinghan Cao,Zexin Li,Yiming Chen,Tao Ren,Dawei Xiang,Xidong Wu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

> 提出了一种新的语法感知的扩散框架来改善代码生成任务中的语法正确性和长期依赖关系的捕捉能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基于扩散的语言模型已经在可控性和双向序列生成方面打开了新的可能，然而将这些模型应用到像源代码这样有严格语法规则和层次组织结构的结构化领域中仍是一个重大挑战。

**Method:** 为了解决这一限制，我们提出了一个语法感知的扩散框架，该框架在去噪过程中整合了源自抽象语法树（ASTs）的结构先验。我们选择性地破坏来自AST子树的语法上有意义的代码片段，而不是随机掩码个别标记。

**Result:** 实验结果表明，语法感知的破坏显著改善了语法正确性、重构准确性和对未见过代码模式的泛化能力。这些发现强调了将结构信息纳入基于扩散的训练中的潜力，并表明语法引导下的去噪是促进基于扩散的语言模型在代码生成任务中发展的有前途的方向。

**Conclusion:** 这项研究展示了将结构化信息整合到基于扩散的训练中的重要性，特别是在代码生成任务中。其结论支持了语法引导去噪方法是未来发展的有效途径。

**Abstract:** Recent advances in diffusion-based language models have opened new
possibilities for controllable and bidirectional sequence generation. These
models provide an alternative to traditional autoregressive approaches by
framing text generation as an iterative denoising process. However, applying
diffusion models to structured domains such as source code remains a
significant challenge. Programming languages differ from natural language in
that they follow strict syntactic and semantic rules, with hierarchical
organization that must be preserved for correctness. Standard token-level
corruption techniques used during training often ignore this structure, which
may hinder the model's ability to learn meaningful representations of code. To
address this limitation, we propose a syntax-aware diffusion framework that
incorporates structural priors from Abstract Syntax Trees (ASTs) into the
denoising process. Instead of masking individual tokens at random, we
selectively corrupt syntactically meaningful code spans derived from AST
subtrees. This enables the model to reconstruct programs in a way that respects
grammatical boundaries and captures long-range dependencies. Experimental
results demonstrate that syntax-aware corruption significantly improves
syntactic correctness, reconstruction accuracy, and generalization to unseen
code patterns. These findings highlight the potential of incorporating
structural information into diffusion-based training and suggest that
syntax-guided denoising is a promising direction for advancing diffusion-based
language models in code generation tasks.

</details>


### [26] [Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach](https://arxiv.org/abs/2508.01480)
*Dimitra Panou,Alexandros C. Dimopoulos,Manolis Koubarakis,Martin Reczko*

Main category: cs.CL

> 研究使用多种开源大语言模型以投票和并集方式处理生物医学问答，并在BioASQ挑战中取得优异名次。

<details>
  <summary>Details</summary>

**Motivation:** 由于生物医学文献呈指数级增长，因此生物医学文本挖掘和问答系统变得至关重要且极具挑战性。研究旨在参与第13届BioASQ挑战中的任务，解决生物医学语义问答和开发主题的Synergy任务。

**Method:** 本研究部署了多种开源的大语言模型（LLMs）作为检索增强生成器来回答生物医学问题。采用了多数投票系统整合模型输出以确定Yes/No问题的答案，并采用答案的并集来解决列表和事实类型的问题。研究评估了13种最先进开源LLMs的各种模型组合，以生成特定类型问题的最佳LLM管道。

**Result:** 在2025年BioASQ挑战的四轮比赛中，系统取得了显著成果：在Synergy任务中，第2轮获得最佳答案第1名和精确答案第2名，以及第3轮和第4轮两次精确答案并列第1名。

**Conclusion:** 研究发现，特定问题类型下的某些LLM组合能持续产出更优结果，研究结论为优化生物医学问答提供了宝贵的见解。

**Abstract:** Biomedical text mining and question-answering are essential yet highly
demanding tasks, particularly in the face of the exponential growth of
biomedical literature. In this work, we present our participation in the 13th
edition of the BioASQ challenge, which involves biomedical semantic
question-answering for Task 13b and biomedical question-answering for
developing topics for the Synergy task. We deploy a selection of open-source
large language models (LLMs) as retrieval-augmented generators to answer
biomedical questions. Various models are used to process the questions. A
majority voting system combines their output to determine the final answer for
Yes/No questions, while for list and factoid type questions, the union of their
answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring
all possible model combinations to contribute to the final answer, resulting in
tailored LLM pipelines for each question type. Our findings provide valuable
insight into which combinations of LLMs consistently produce superior results
for specific question types. In the four rounds of the 2025 BioASQ challenge,
our system achieved notable results: in the Synergy task, we secured 1st place
for ideal answers and 2nd place for exact answers in round 2, as well as two
shared 1st places for exact answers in round 3 and 4.

</details>


### [27] [TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu](https://arxiv.org/abs/2508.01486)
*Vallabhaneni Raj Kumar,Ashwin S,Supriya Manna,Niladri Sett,Cheedella V S N M S Hema Harshitha,Kurakula Harshitha,Anand Kumar Sharma,Basina Deepakraj,Tanuj Sarkar,Bondada Navaneeth Krishna,Samanthapudi Shakeer*

Main category: cs.CL

> 本研究介绍了TeSent数据集，用于泰卢固语的情感分类，包括详细的情感分析和公平性评估，展示了使用理由训练模型的优点。

<details>
  <summary>Details</summary>

**Motivation:** 由于高质量注释资源的缺乏，泰卢固语在世界NLP和机器学习领域中代表性较低。本研究旨在为泰卢固语情绪分类问题提供一个全面的数据集，同时满足模型解释性和公平性的要求。

**Method:** 该研究构建了TeSent，这是一个全面的泰卢固语情感分类基准数据集。数据集涵盖了多个领域的泰卢固语文本，并提供了地面真实标签及其人类注释的理由。此外，还通过六个广泛使用的后验解释器对训练模型进行了可信性和忠实度评估，并创建了TeEEC，一个用于评估泰卢固语情感相关NLP任务公平性的语料库及公平性评估套件。模型通过有理据和无理据两种方式微调。

**Result:** 实验结果表明，通过使用理由训练模型，可以提高模型的准确性，减少模型的偏见，并使解释器的输出更好地与人类推理相一致。

**Conclusion:** 通过此工作，本研究为泰卢固语的NLP任务提供了一整套的情感分类和公平性评估工具，丰富了泰卢固语在NLP领域的数据资源，有助于推动该领域的发展。

**Abstract:** In the Indian subcontinent, Telugu, one of India's six classical languages,
is the most widely spoken Dravidian Language. Despite its 96 million speaker
base worldwide, Telugu remains underrepresented in the global NLP and Machine
Learning landscape, mainly due to lack of high-quality annotated resources.
This work introduces TeSent, a comprehensive benchmark dataset for sentiment
classification, a key text classification problem, in Telugu. TeSent not only
provides ground truth labels for the sentences, but also supplements with
provisions for evaluating explainability and fairness, two critical
requirements in modern-day machine learning tasks. We scraped Telugu texts
covering multiple domains from various social media platforms, news websites
and web-blogs to preprocess and generate 26,150 sentences, and developed a
custom-built annotation platform and a carefully crafted annotation protocol
for collecting the ground truth labels along with their human-annotated
rationales. We then fine-tuned several SOTA pre-trained models in two ways:
with rationales, and without rationales. Further, we provide a detailed
plausibility and faithfulness evaluation suite, which exploits the rationales,
for six widely used post-hoc explainers applied on the trained models. Lastly,
we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate
fairness of Telugu sentiment and emotion related NLP tasks, and provide a
fairness evaluation suite for the trained classifier models. Our experimental
results suggest that training with rationales may improve model accuracy,
reduce bias in models, and make the explainers' output more aligned to human
reasoning.

</details>


### [28] [The Homogenizing Effect of Large Language Models on Human Expression and Thought](https://arxiv.org/abs/2508.01491)
*Zhivar Sourati,Alireza S. Ziabari,Morteza Dehghani*

Main category: cs.CL

> 大规模语言模型可能会导致语言和推理标准化，从而损害认知多样性及其对应创造力和集体智能的价值。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于探讨大规模语言模型日益嵌入人们生活中所带来的潜在问题，即这些模型有可能使得语言和推理趋于标准化，从而削弱认知多样性及其在创造力和集体智能中的作用。

**Method:** 分析方法主要通过跨学科的综合研究，涵盖了语言学、认知科学和计算机科学等领域的证据，探讨了大规模语言模型（LLMs）如何反映和强化主导风格，同时边缘化了替代性声音和推理策略。

**Result:** 研究表明，大规模语言模型的设计和广泛应用通过模仿训练数据中的模式并在不同语境下放大趋同性来贡献于主导风格的强化和替代性声音的边缘化。

**Conclusion:** 如果不加以控制，这种同质化的趋势将有风险消解驱动集体智能和适应性的认知景观，从而对创造力和集体智能造成负面影响。

**Abstract:** Cognitive diversity, reflected in variations of language, perspective, and
reasoning, is essential to creativity and collective intelligence. This
diversity is rich and grounded in culture, history, and individual experience.
Yet as large language models (LLMs) become deeply embedded in people's lives,
they risk standardizing language and reasoning. This Review synthesizes
evidence across linguistics, cognitive, and computer science to show how LLMs
reflect and reinforce dominant styles while marginalizing alternative voices
and reasoning strategies. We examine how their design and widespread use
contribute to this effect by mirroring patterns in their training data and
amplifying convergence as all people increasingly rely on the same models
across contexts. Unchecked, this homogenization risks flattening the cognitive
landscapes that drive collective intelligence and adaptability.

</details>


### [29] [A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents](https://arxiv.org/abs/2508.01503)
*Clayton Cohn,Surya Rayala,Namrata Srivastava,Joyce Horn Fonteles,Shruti Jain,Xinying Luo,Divya Mereddy,Naveeduddin Mohammed,Gautam Biswas*

Main category: cs.CL

> 提出将证据中心设计和社会认知理论用于基于LLMs的STEM+C学习自适应支持系统，通过Inquizzitor展示，结果显示其提供了高质量评估和互动，获得了教师和学生的认可。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型（LLMs）在教育中的应用缺乏坚实的理论基础，而传统智能辅导系统则有此优势。为了弥补这一差距，该论文提出了一个结合证据中心设计和社会认知理论的框架，旨在用于基于LLMs的教师辅助系统。

**Method:** 结合证据中心设计和社会认知理论的框架，用于支持基于大语言模型（LLMs）的自适应学习支架，特别关注STEM+C领域。该框架通过Inquizzitor展示，这是一款基于LLMs的形成性评估代理，集成人-人工智能混合智能，并提供基于认知科学原则的反馈。

**Result:** 研究结果表明，Inquizzitor能提供高质量的评估和互动，其符合核心学习理论，为教师提供了有效的指导，学生也很认可。

**Conclusion:** 该研究强调了理论驱动的大语言模型（LLMs）在教育中的潜力，表明这些系统能够提供自适应和原则性的教学。

**Abstract:** Large language models (LLMs) present new opportunities for creating
pedagogical agents that engage in meaningful dialogue to support student
learning. However, the current use of LLM systems like ChatGPT in classrooms
often lacks the solid theoretical foundation found in earlier intelligent
tutoring systems. To bridge this gap, we propose a framework that combines
Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding
in LLM-based agents focused on STEM+C learning. We illustrate this framework
with Inquizzitor, an LLM-based formative assessment agent that integrates
human-AI hybrid intelligence and provides feedback grounded in cognitive
science principles. Our findings show that Inquizzitor delivers high-quality
assessment and interaction aligned with core learning theories, offering
teachers effective guidance that students value. This research underscores the
potential for theory-driven LLM integration in education, highlighting the
ability of these systems to provide adaptive and principled instruction.

</details>


### [30] [MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization](https://arxiv.org/abs/2508.01541)
*Sara Câmara,Eduardo Luz,Valéria Carvalho,Ivan Meneghini,Gladston Moreira*

Main category: cs.CL

> 本文介绍了MOPrompt，这是一个多目标进化优化框架，能够同时优化提示的准确性和上下文大小，实验结果表明MOPrompt在保持相同准确性的同时显著减少了token数量。

<details>
  <summary>Details</summary>

**Motivation:** 尽管手动设计提示通常复杂、非直观且耗时，但自动提示优化已成为一个研究领域。然而，在提示优化过程中存在一个显著的挑战，即处理任务性能（如准确性）和上下文大小之间的内在权衡。现有的自动化方法大多侧重于单一目标，通常是性能，而忽视了效率和效果之间的关键光谱。因此，为了弥补这一空白，进行了这项研究。

**Method:** 提出了一种新的多目标进化优化（EMO）框架MOPrompt，旨在同时优化提示的准确性和上下文大小（以token为单位）。

**Result:** 在葡萄牙语情感分析任务中用Gemma-2B和Sabiazinho-3作为评估模型进行的实验表明，MOPrompt显著优于基准框架。对于Sabiazinho模型，MOPrompt找到了一个提示，达到了与最佳基准解决方案相同的峰值准确率（0.97），但token长度减少了31%。

**Conclusion:** MOPrompt作为一项关键工具，对于在现实世界应用中部署大型语言模型（LLMs）至关重要，因为它提供了一系列有关上下文大小和性能之间权衡的解决方案。

**Abstract:** Prompt engineering is crucial for unlocking the potential of Large Language
Models (LLMs). Still, since manual prompt design is often complex,
non-intuitive, and time-consuming, automatic prompt optimization has emerged as
a research area. However, a significant challenge in prompt optimization is
managing the inherent trade-off between task performance, such as accuracy, and
context size. Most existing automated methods focus on a single objective,
typically performance, thereby failing to explore the critical spectrum of
efficiency and effectiveness. This paper introduces the MOPrompt, a novel
Multi-objective Evolutionary Optimization (EMO) framework designed to optimize
prompts for both accuracy and context size (measured in tokens) simultaneously.
Our framework maps the Pareto front of prompt solutions, presenting
practitioners with a set of trade-offs between context size and performance, a
crucial tool for deploying Large Language Models (LLMs) in real-world
applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,
using Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that
MOPrompt substantially outperforms the baseline framework. For the Sabiazinho
model, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)
as the best baseline solution, but with a 31% reduction in token length.

</details>


### [31] [Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models](https://arxiv.org/abs/2508.01554)
*Yujia Zheng,Tianhao Li,Haotian Huang,Tianyu Zeng,Jingyu Lu,Chuangxin Chu,Yuekai Huang,Ziyou Jiang,Qian Xiong,Yuyao Ge,Mingyang Li*

Main category: cs.CL

> PromptAnatomy dissects prompts to generate adversarial examples, achieving state-of-the-art attack success rates in evaluating the robustness of large language models.

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in considering the structural heterogeneity of prompts in adversarial attacks, ensuring linguistic plausibility, and mitigating distribution shifts.

**Method:** PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using the proposed method, ComPerturb.

**Result:** ComPerturb achieves state-of-the-art attack success rates, and the ablation studies show the benefits of prompt dissection and perplexity filtering.

**Conclusion:** The importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs is underscored.

**Abstract:** Prompt-based adversarial attacks have become an effective means to assess the
robustness of large language models (LLMs). However, existing approaches often
treat prompts as monolithic text, overlooking their structural
heterogeneity-different prompt components contribute unequally to adversarial
robustness. Prior works like PromptRobust assume prompts are value-neutral, but
our analysis reveals that complex, domain-specific prompts with rich structures
have components with differing vulnerabilities. To address this gap, we
introduce PromptAnatomy, an automated framework that dissects prompts into
functional components and generates diverse, interpretable adversarial examples
by selectively perturbing each component using our proposed method, ComPerturb.
To ensure linguistic plausibility and mitigate distribution shifts, we further
incorporate a perplexity (PPL)-based filtering mechanism. As a complementary
resource, we annotate four public instruction-tuning datasets using the
PromptAnatomy framework, verified through human review. Extensive experiments
across these datasets and five advanced LLMs demonstrate that ComPerturb
achieves state-of-the-art attack success rates. Ablation studies validate the
complementary benefits of prompt dissection and PPL filtering. Our results
underscore the importance of prompt structure awareness and controlled
perturbation for reliable adversarial robustness evaluation in LLMs. Code and
data are available at https://github.com/Yujiaaaaa/PACP.

</details>


### [32] [OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets](https://arxiv.org/abs/2508.01630)
*Maziyar Panahi*

Main category: cs.CL

> 研究团队提出了一个名为OpenMed NER的套件，此套件通过轻量级的领域适应性预训练结合低秩调整来提升生物医学命名实体识别模型的性能，模型在多个基准测试中展现了优秀的性能，同时训练效率高，计算成本低且环境友好。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在生物医学命名实体识别（NER）上已经取得了进步，但对于在跨多样性实体类型上获得最佳性能并保持计算效率仍然是一项挑战。作者试图通过降低模型参数更新的比例和使用轻量级领域适应性预训练来解决这一问题。

**Method:** 本文提出了OpenMed NER，这是一个开放源代码的套件，通过领域适应性预训练（DAPT）和参数高效低秩调整（LoRA）相结合的方法。来自多个公开可用的研究库和去标识化的临床记录，比如PubMed、arXiv 和 MIMIC-III的数据被用作训练语料库。模型先使用DeBERTa-v3、PubMedBERT和BioELECTRA作为基础模型进行轻量级的领域适应性预训练，然后在任务特定的领域进行细调。

**Result:** 作者在12个已建立的生物医学命名实体识别基准上评估了他们的模型，结果在其中10个基准上达到了新的最佳性能。对于基础的疾病和化学基准测试，例如BC5CDR-Disease，性能提高了2.70个百分点，而对于更专业的基因和临床细胞线数据集则提升了5.3和9.7个百分点。

**Conclusion:** 这项研究证明了通过策略性适应的开源模型可以超越封闭源代码的解决方案。模型不仅能高效训练而且在适应性、实用性和环境上都有优势，并且符合数据保护和AI法规要求。

**Abstract:** Named-entity recognition (NER) is fundamental to extracting structured
information from the >80% of healthcare data that resides in unstructured
clinical notes and biomedical literature. Despite recent advances with large
language models, achieving state-of-the-art performance across diverse entity
types while maintaining computational efficiency remains a significant
challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted
transformer models that combine lightweight domain-adaptive pre-training (DAPT)
with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs
cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,
publicly available research repositories and de-identified clinical notes
(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA
backbones. This is followed by task-specific fine-tuning with LoRA, which
updates less than 1.5% of model parameters. We evaluate our models on 12
established biomedical NER benchmarks spanning chemicals, diseases, genes, and
species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of
these 12 datasets, with substantial gains across diverse entity types. Our
models advance the state-of-the-art on foundational disease and chemical
benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger
improvements of over 5.3 and 9.7 percentage points on more specialized gene and
clinical cell line corpora. This work demonstrates that strategically adapted
open-source models can surpass closed-source solutions. This performance is
achieved with remarkable efficiency: training completes in under 12 hours on a
single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively
licensed, open-source checkpoints designed to help practitioners facilitate
compliance with emerging data protection and AI regulations, such as the EU AI
Act.

</details>


### [33] [Authorship Attribution in Multilingual Machine-Generated Texts](https://arxiv.org/abs/2508.01656)
*Lucio La Cava,Dominik Macko,Róbert Móro,Ivan Srba,Andrea Tagarelli*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** As Large Language Models (LLMs) have reached human-like fluency and
coherence, distinguishing machine-generated text (MGT) from human-written
content becomes increasingly difficult. While early efforts in MGT detection
have focused on binary classification, the growing landscape and diversity of
LLMs require a more fine-grained yet challenging authorship attribution (AA),
i.e., being able to identify the precise generator (LLM or human) behind a
text. However, AA remains nowadays confined to a monolingual setting, with
English being the most investigated one, overlooking the multilingual nature
and usage of modern LLMs. In this work, we introduce the problem of
Multilingual Authorship Attribution, which involves attributing texts to human
or multiple LLM generators across diverse languages. Focusing on 18 languages
-- covering multiple families and writing scripts -- and 8 generators (7 LLMs
and the human-authored class), we investigate the multilingual suitability of
monolingual AA methods, their cross-lingual transferability, and the impact of
generators on attribution performance. Our results reveal that while certain
monolingual AA methods can be adapted to multilingual settings, significant
limitations and challenges remain, particularly in transferring across diverse
language families, underscoring the complexity of multilingual AA and the need
for more robust approaches to better match real-world scenarios.

</details>


### [34] [CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions](https://arxiv.org/abs/2508.01674)
*Tae Soo Kim,Yoonjoo Lee,Yoonah Park,Jiho Kim,Young-Ho Kim,Juho Kim*

Main category: cs.CL

> CUPID数据集用于评估10个大语言模型在不同上下文中推断用户多轮反馈偏好的能力，结果显示它们的性能不佳，这强调了个性化交互中上下文相关性的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决现有大语言模型假设用户偏好是静态的，而不考虑动态变化的问题。为了实现更好的个性化服务，必须使模型能够推断并应用用户在不同上下文中的偏好。

**Method:** 通过引入CUPID数据集，评估大语言模型在多轮交互中推断用户偏好并生成满足这些偏好的响应的能力。CUPID包含756个人类策划的用户与基于LLM的聊天助手的交互历史，涵盖多个上下文。

**Result:** 评估了10个开放和专有的LLM，发现最先进的LLM在推断多轮交互中的用户偏好方面存在困难，精度低于50%，召回率低于65%。

**Conclusion:** 表明需要提高大语言模型的能力以实现更上下文化的个性化互动，并提议将CUPID作为推动改进的资源。

**Abstract:** Personalization of Large Language Models (LLMs) often assumes users hold
static preferences that reflect globally in all tasks. In reality, humans hold
dynamic preferences that change depending on the context. As users interact
with an LLM in various contexts, they naturally reveal their contextual
preferences, which a model must infer and apply in future contexts to ensure
alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated
interaction session histories between users and LLM-based chat assistants. In
each interaction session, the user provides a request in a specific context and
expresses their preference through multi-turn feedback. Given a new user
request and prior interaction sessions, our benchmark assesses whether LLMs can
infer the preference relevant to this request and generate a response that
satisfies this preference. With CUPID, we evaluated 10 open and proprietary
LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from
multi-turn interactions and fail to discern what previous context is relevant
to a new request -- under 50% precision and 65% recall. Our work highlights the
need to advance LLM capabilities for more contextually personalized
interactions and proposes CUPID as a resource to drive these improvements.

</details>


### [35] [The Bidirectional Process Reward Model](https://arxiv.org/abs/2508.01682)
*Lingyin Zhang,Jun Gao,Xiaoxue Ren,Ziqiang Cao*

Main category: cs.CL

> The paper introduces Bidirectional Process Reward Model (BiPRM) which combines L2R and R2L evaluation streams for improved global context usage and consistent earlier step verification. It leads to better performance in mathematical reasoning benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** Process Reward Models (PRMs) predominantly adopt a L2R evaluation paradigm, which limits their ability to leverage global context and verify the consistency of earlier steps based on later ones.

**Method:** We propose a novel bidirectional evaluation paradigm, named Bidirectional Process Reward Model (BiPRM), which incorporates a parallel right-to-left (R2L) evaluation stream alongside the conventional left-to-right (L2R) flow.

**Result:** The experiments were conducted on two mathematical reasoning benchmarks using samples generated by three different policy models. BiPRM outperformed unidirectional baselines, achieving up to a 31.9% improvement in stepwise reward evaluation.

**Conclusion:** The results demonstrate the effectiveness, robustness, and general applicability of BiPRM, paving the way for a promising new direction in process-based reward modeling.

**Abstract:** Process Reward Models (PRMs) have emerged as a promising approach to enhance
the reasoning quality of Large Language Models (LLMs) by assigning fine-grained
scores to intermediate reasoning steps within a solution trajectory. However,
existing PRMs predominantly adopt a unidirectional left-to-right (L2R)
evaluation paradigm, which limits their ability to leverage global context,
making it challenging to verify the consistency of earlier steps based on later
ones. In light of these challenges, we propose a novel bidirectional evaluation
paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly
incorporates a parallel right-to-left (R2L) evaluation stream alongside the
conventional L2R flow, enabling later reasoning steps to help assess earlier
ones in real time. Notably, the built-in R2L evaluation is implemented solely
through prompt modifications that reverse the original reasoning trajectory,
without any additional parameters or inference latency introduced. This ensures
BiPRM remains both efficient and broadly compatible with existing PRM studies.
We conduct extensive experiments on two mathematical reasoning benchmarks using
samples generated by three different policy models. Our method, BiPRM, is
evaluated across three backbones and three distinct PRM objectives. Across all
settings, BiPRM consistently outperforms unidirectional baselines, achieving up
to a 31.9% improvement in stepwise reward evaluation. Generally, our results
highlight BiPRM's effectiveness, robustness, and general applicability,
offering a promising new direction for process-based reward modeling.

</details>


### [36] [Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy](https://arxiv.org/abs/2508.01696)
*Yi Jiang,Sendong Zhao,Jianbo Li,Haochun Wang,Lizhe Zhang,Yan Liu,Bin Qin*

Main category: cs.CL

> A framework called Collaborative Chain-of-Agents is proposed for enhancing the integration of parametric and retrieved knowledge, showcasing superior performance in knowledge-intensive tasks.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing RAG methods in fully utilizing and integrating both internal and external knowledge.

**Method:** Collaborative Chain-of-Agents framework, including CoCoA-zero and CoCoA, to enhance synergy between parametric and retrieved knowledge.

**Result:** Experiments show that CoCoA-zero and CoCoA perform better on open-domain and multi-hop question answering tasks.

**Conclusion:** The proposed methods enhance the explicit integration of parametric and retrieved knowledge for more effective knowledge-intensive tasks.

**Abstract:** Retrieval-Augmented Generation (RAG) has emerged as a promising framework for
enhancing the capabilities of Large Language Models (LLMs), especially in
knowledge-intensive tasks. Despite its advantages, current RAG methods often
struggle to *fully exploit knowledge during generation*. In particular, the
synergy between the model's internal parametric knowledge and external
retrieved knowledge remains limited. Retrieved contents may sometimes mislead
generation, while certain generated content can guide the model toward more
accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a
framework designed to enhance explicitly synergy over both parametric and
retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent
RAG framework that first performs conditional knowledge induction and then
reasons answers. Building on this, we develop CoCoA, a long-chain training
strategy that synthesizes extended multi-agent reasoning trajectories from
CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability
to explicitly integrate and jointly leverage parametric and retrieved
knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior
performance on open-domain and multi-hop QA tasks.

</details>


### [37] [Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption](https://arxiv.org/abs/2508.01708)
*Berkay Köprü,Mehrzad Mashal,Yigit Gurses,Akos Kadar,Maximilian Schmitt,Ditty Mathew,Felix Burkhardt,Florian Eyben,Björn W. Schuller*

Main category: cs.CL

> This paper explores 'expression leakage' in LLMs—a new bias issue where models generate emotionally charged responses unrelated to their context. The study proposes an auto-evaluation pipeline to measure and manage this issue.

<details>
  <summary>Details</summary>

**Motivation:** Prior work on semantic leakage introduces bias in LLMs due to irrelevant semantic context. This paper introduces and analyzes a novel phenomenon called 'expression leakage,' where LLMs unintentionally generate sentimentally charged expressions unrelated to the input context.

**Method:** We collect a benchmark dataset and propose an automatic evaluation pipeline to analyze the expression leakage. This pipeline correlates well with human judgment and accelerates the benchmarking process by reducing the need for annotations.

**Result:** Experiments show that as the model scales in the parameter space, the expression leakage reduces within the same LLM family. Expression leakage mitigation is not achievable solely by prompting, and injecting negative sentiment in prompts increases the expression leakage rate.

**Conclusion:** The study concludes that expression leakage decreases as models scale within the same LLM family, and that addressing this leakage requires careful model building processes, rather than just prompting.

**Abstract:** Large language models (LLMs) have advanced natural language processing (NLP)
skills such as through next-token prediction and self-attention, but their
ability to integrate broad context also makes them prone to incorporating
irrelevant information. Prior work has focused on semantic leakage, bias
introduced by semantically irrelevant context. In this paper, we introduce
expression leakage, a novel phenomenon where LLMs systematically generate
sentimentally charged expressions that are semantically unrelated to the input
context. To analyse the expression leakage, we collect a benchmark dataset
along with a scheme to automatically generate a dataset from free-form text
from common-crawl. In addition, we propose an automatic evaluation pipeline
that correlates well with human judgment, which accelerates the benchmarking by
decoupling from the need of annotation for each analysed model. Our experiments
show that, as the model scales in the parameter space, the expression leakage
reduces within the same LLM family. On the other hand, we demonstrate that
expression leakage mitigation requires specific care during the model building
process, and cannot be mitigated by prompting. In addition, our experiments
indicate that, when negative sentiment is injected in the prompt, it disrupts
the generation process more than the positive sentiment, causing a higher
expression leakage rate.

</details>


### [38] [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710)
*Raviraj Joshi,Rakesh Paul,Kanishk Singla,Anusha Kamath,Michael Evans,Katherine Luna,Shaona Ghosh,Utkarsh Vaidya,Eileen Long,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

> 介绍了CultureGuard，一种新的解决方案，用于生成多语言文化的高质量安全数据集，并实现了多语言安全防护模型。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在代理应用中的使用日益增多，凸显了需要健壮的安全防护模型，尤其是在非英语语言中，由于收集文化对齐标注数据集的成本高而缺乏类似的进步。

**Method:** 介绍了一种四阶段合成数据生成与过滤管道：文化数据分隔、文化数据适应、机器翻译和质量过滤。这个管道使Nemotron-Content-Safety-Dataset-V2英文安全数据集可以转换和扩展到八种不同的语言。

**Result:** Nemotron-Content-Safety-Dataset-Multilingual-v1数据集包含了9种语言的386,661个样本，并使通过LoRA-based fine-tuning训练Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1成为可能。最终模型在多个多语言内容安全基准上达到了最先进的性能。

**Conclusion:** 这项工作代表了多语言LLMs安全防护能力进步的重要一步，推动了文化感知安全防护模型的发展。

**Abstract:** The increasing use of Large Language Models (LLMs) in agentic applications
highlights the need for robust safety guard models. While content safety in
English is well-studied, non-English languages lack similar advancements due to
the high cost of collecting culturally aligned labeled datasets. We present
CultureGuard, a novel solution for curating culturally aligned, high-quality
safety datasets across multiple languages. Our approach introduces a four-stage
synthetic data generation and filtering pipeline: cultural data segregation,
cultural data adaptation, machine translation, and quality filtering. This
pipeline enables the conversion and expansion of the
Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct
languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.
The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,
comprises 386,661 samples in 9 languages and facilitates the training of
Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.
The final model achieves state-of-the-art performance on several multilingual
content safety benchmarks. We also benchmark the latest open LLMs on
multilingual safety and observe that these LLMs are more prone to give unsafe
responses when prompted in non-English languages. This work represents a
significant step toward closing the safety gap in multilingual LLMs by enabling
the development of culturally aware safety guard models.

</details>


### [39] [Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction](https://arxiv.org/abs/2508.01739)
*Cheng Wang,ziru Liu,Pengcheng Tang,Mingyu Zhang,Quanyu Dai,Yue Zhu*

Main category: cs.CL

> 本文提出了一种新的对话数据生成框架IterChat，通过改进数据格式和利用GPT4预先定义偏好槽位，来生成高质量和多样化的对话数据集，从而提升了模型的性能和标注效率。

<details>
  <summary>Details</summary>

**Motivation:** 当前研究显示，使用大型语言模型（LLM）来微调任务特定偏好提取器在准确性方面取得了优异的结果，但由于难以获取高质量的多轮对话数据，这成为了主要挑战。本文旨在解决此问题。

**Method:** 我们提出了一种名为IterChat的新对话数据生成框架。首先，我们构建了一种新的数据格式，该格式将对话数据分类为属性历史偏好和单轮对话。这减少了标注错误的概率并提高了标注效率。然后，采用GPT4预定义目标偏好提取任务中的偏好槽位，并随机采样槽位及其对应的模式值来创建对话数据集。

**Result:** 实验结果表明，使用新对话格式进行微调或只有少量提示，相较于原始多轮对话，可以达到更好的性能。同时，新数据格式提高了标注员的效率，比原始多轮对话高出28.4%。

**Conclusion:** 研究证明，通过分解多轮偏好提取为迭代执行的单轮提取过程，可以更有效地生成高质量对话数据，提高标注效率并提升模型性能。

**Abstract:** Identifying user preferences in dialogue systems is a pivotal aspect of
providing satisfying services. Current research shows that using large language
models (LLMs) to fine-tune a task-specific preference extractor yields
excellent results in terms of accuracy and generalization. However, the primary
challenge stems from the inherent difficulty in obtaining high-quality labeled
multi-turn dialogue data. Accurately tracking user preference transitions
across turns not only demands intensive domain expertise and contextual
consistency maintenance for annotators (termed \textbf{``Annotating
Disaster''}) but also complicates model training due to error propagation in
sequential dependency learning. Inspired by the observation that multi-turn
preference extraction can be decomposed into iterative executions of one-turn
extraction processes. We propose a novel dialogue data generation framework
named \textbf{IterChat}. First, we construct a new data format that categorizes
the dialogue data into attributed historical preferences and one-turn
dialogues. This reduces the probability of annotation errors and improves
annotation efficiency. Then, to generate a high-quality and diverse dialogue
dataset, we adopt GPT4 to pre-define the preference slots in the target
preference extractor task and then randomly sample the subset of the slots and
their corresponding schema values to create the dialogue datasets. Experimental
results indicate that fine-tuning or only few-shot prompting with the new
dialogue format yields superior performance compared to the original multi-turn
dialogues. Additionally, the new data format improves annotator efficiency with
a win rate of 28.4\% higher than the original multi-turn dialogues.

</details>


### [40] [AI-Generated Text is Non-Stationary: Detection via Temporal Tomography](https://arxiv.org/abs/2508.01754)
*Alva West,Yixuan Weng,Minjun Zhu,Luodan Zhang,Zhen Lin,Guangsheng Bao,Yue Zhang*

Main category: cs.CL

> This paper introduces TDT, a signal processing-based detection method for AI-generated text, which preserves positional information and outperforms existing methods by a significant margin.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is the limitation of current AI-generated text detection methods which aggregate token-level measurements into scalar scores and discard positional information. This oversight causes existing detectors to fail against localized adversarial perturbations.

**Method:** Our method, Temporal Discrepancy Tomography (TDT), reformulates AI-generated text detection as a signal processing task, preserving positional information of statistical anomalies by treating token-level discrepancies as a time-series signal and applying Continuous Wavelet Transform to generate a two-dimensional time-scale representation.

**Result:** TDT achieves a 0.855 Area Under the Curve (AUROC), representing a 7.1% improvement over the best baseline on the RAID benchmark. Additionally, it shows a 14.1% AUROC improvement on HART Level 2 paraphrasing attacks.

**Conclusion:** This work establishes the non-stationarity of AI-generated text and demonstrates that preserving temporal dynamics is crucial for robust detection. Our method, TDT, not only enhances detection performance but also maintains practical efficiency.

**Abstract:** The field of AI-generated text detection has evolved from supervised
classification to zero-shot statistical analysis. However, current approaches
share a fundamental limitation: they aggregate token-level measurements into
scalar scores, discarding positional information about where anomalies occur.
Our empirical analysis reveals that AI-generated text exhibits significant
non-stationarity, statistical properties vary by 73.8\% more between text
segments compared to human writing. This discovery explains why existing
detectors fail against localized adversarial perturbations that exploit this
overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),
a novel detection paradigm that preserves positional information by
reformulating detection as a signal processing task. TDT treats token-level
discrepancies as a time-series signal and applies Continuous Wavelet Transform
to generate a two-dimensional time-scale representation, capturing both the
location and linguistic scale of statistical anomalies. On the RAID benchmark,
TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More
importantly, TDT demonstrates robust performance on adversarial tasks, with
14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its
sophisticated analysis, TDT maintains practical efficiency with only 13\%
computational overhead. Our work establishes non-stationarity as a fundamental
characteristic of AI-generated text and demonstrates that preserving temporal
dynamics is essential for robust detection.

</details>


### [41] [A comprehensive taxonomy of hallucinations in Large Language Models](https://arxiv.org/abs/2508.01781)
*Manuel Cossio*

Main category: cs.CL

> 报告对大型语言模型（LLMs）的幻觉现象进行了全面的定义和分类，分析了其成因，并提供了检测与缓解策略，强调了其不可避免性，和对持续人类监督的需求。

<details>
  <summary>Details</summary>

**Motivation:** 该报告旨在全面了解和系统化大型语言模型（LLMs）幻觉现象的复杂性，以期为未来的检测、缓解策略提供理论基础和实践指导。

**Method:** 该报告首先提出了大型语言模型（LLMs）幻觉现象的定义和理论框架，认为其在可计算的LLMs中是不可避免的。接着，报告详细探讨了幻觉现象的不同类型，包括内在的（与输入上下文矛盾）和外在的（与训练数据或现实不符），以及事实性（绝对正确）和忠实性（符合输入）。报告还具体介绍了幻觉的几种表现形式，如事实错误、上下文和逻辑不一致、时间紊乱、伦理违规等。然后，报告分析了导致幻觉现象的根本原因，将其归类为数据相关问题、模型相关因素和提示相关影响。此外，报告还探讨了影响幻觉感知的认知和人类因素，调查了评估基准和检测指标，并概述了架构和系统层面的缓解策略。最后，报告介绍了用于监测LLMs发布和性能的网络资源。

**Result:** 报告揭示了LLMs在可计算模型中的幻觉现象是不可避免的，并强调为了在关键应用中有责任感和可靠地部署，必须专注于健壮的检测、缓解以及持续的人类监督。

**Conclusion:** 报告指出，鉴于LLMs幻觉现象的理论不可避免性，未来的研究应专注于其检测、缓解以及持续的人类监督，以确保其在关键应用中的责任性和可靠性。

**Abstract:** Large language models (LLMs) have revolutionized natural language processing,
yet their propensity for hallucination, generating plausible but factually
incorrect or fabricated content, remains a critical challenge. This report
provides a comprehensive taxonomy of LLM hallucinations, beginning with a
formal definition and a theoretical framework that posits its inherent
inevitability in computable LLMs, irrespective of architecture or training. It
explores core distinctions, differentiating between intrinsic (contradicting
input context) and extrinsic (inconsistent with training data or reality), as
well as factuality (absolute correctness) and faithfulness (adherence to
input). The report then details specific manifestations, including factual
errors, contextual and logical inconsistencies, temporal disorientation,
ethical violations, and task-specific hallucinations across domains like code
generation and multimodal applications. It analyzes the underlying causes,
categorizing them into data-related issues, model-related factors, and
prompt-related influences. Furthermore, the report examines cognitive and human
factors influencing hallucination perception, surveys evaluation benchmarks and
metrics for detection, and outlines architectural and systemic mitigation
strategies. Finally, it introduces web-based resources for monitoring LLM
releases and performance. This report underscores the complex, multifaceted
nature of LLM hallucinations and emphasizes that, given their theoretical
inevitability, future efforts must focus on robust detection, mitigation, and
continuous human oversight for responsible and reliable deployment in critical
applications.

</details>


### [42] [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://arxiv.org/abs/2508.01812)
*Amir DN Cohen,Hilla Merhav,Yoav Goldberg,Reut Tsarfaty*

Main category: cs.CL

> 本文为解决希伯来语机器阅读理解的问题提供了新的数据集HeQ（希伯来语QA），并改进了评估指标，展示了形态丰富的语言对于自然语言理解的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 现有希伯来语自然语言处理的基准侧重于形态-句法任务，忽略了语言理解的语义维度。为了填补这一空白，本文致力于提供一个希伯来语机器阅读理解的数据集。

**Method:** 本文提出了一套新的指南、一个受控的众包协议及修改后的评估指标，这些旨在解决希伯来语这种形态丰富的语言在机器阅读理解中的问题。

**Result:** 实验显示，现有的评估指标如F1分数和精确匹配对于希伯来语（以及其他形态丰富的语言）来说并不合适，需要进行优化。同时，不同模型在形态句法任务上的表现与机器阅读理解的表现之间相关性较低。

**Conclusion:** HeQ的发展探索揭示了形态丰富的语言在自然语言理解中的某些挑战，促进了希伯来语及其他形态丰富语言NLU模型的进一步改进。

**Abstract:** Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly
on morpho-syntactic tasks, neglecting the semantic dimension of language
understanding. To bridge this gap, we set out to deliver a Hebrew Machine
Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive
Question Answering. The morphologically rich nature of Hebrew poses a challenge
to this endeavor: the indeterminacy and non-transparency of span boundaries in
morphologically complex forms lead to annotation inconsistencies,
disagreements, and flaws in standard evaluation metrics.
  To remedy this, we devise a novel set of guidelines, a controlled
crowdsourcing protocol, and revised evaluation metrics that are suitable for
the morphologically rich nature of the language. Our resulting benchmark, HeQ
(Hebrew QA), features 30,147 diverse question-answer pairs derived from both
Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation
reveals that standard evaluation metrics such as F1 scores and Exact Match (EM)
are not appropriate for Hebrew (and other MRLs), and we propose a relevant
enhancement.
  In addition, our experiments show low correlation between models' performance
on morpho-syntactic tasks and on MRC, which suggests that models designed for
the former might underperform on semantics-heavy tasks. The development and
exploration of HeQ illustrate some of the challenges MRLs pose in natural
language understanding (NLU), fostering progression towards more and better NLU
models for Hebrew and other MRLs.

</details>


### [43] [AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy](https://arxiv.org/abs/2508.01815)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Tan Chuan Fu,Yue Xiu,Dusit Niyato,Jonathan Z. Low,Eugene Ho Hong Zhuang,Daren Zong Loong Tan*

Main category: cs.CL

> 介绍了一种名为AgenticT$^2$S的模块化框架，通过专门的代理处理知识图谱问答（KGQA）中的不同子任务，证明该框架提高了执行准确性、F$_1$值，同时减少了提示长度。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文本到SPARQL方法依赖于大规模领域特定的微调或在单一图设置中运行，这限制了它们在低资源领域中的泛化能力和处理跨越多个图的查询的能力。这些挑战在如循环经济等领域尤为重要，这些领域中的信息分布在独立维护的知识图谱（KGs）中。

**Method:** AgenticT$^2$S，一个模块化的框架，将KGQA任务分解为多个子任务，由专门的代理负责检索、查询生成和验证。调度器使用弱到强的对齐策略将子目标分配给不同的图。双阶段验证器通过符号验证和反事实一致性检查来检测结构上无效和语义上未指定的查询。

**Result:** 在真实世界的循环经济KGs上的实验表明，与最佳基线相比，AgenticT$^2$S提高了17.3%的执行准确性和25.4%的三重水平F$_1$，同时还将平均提示长度减少了46.4%。

**Conclusion:** 这些结果展示了基于代理的模式感知推理对可扩展的KGQA的益处，并通过强大的跨图推理支持在可持续性领域的决策。

**Abstract:** Question answering over heterogeneous knowledge graphs (KGQA) involves
reasoning across diverse schemas, incomplete alignments, and distributed data
sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific
fine-tuning or operate within single-graph settings, limiting their
generalizability in low-resource domains and their ability to handle queries
spanning multiple graphs. These challenges are particularly relevant in domains
such as the circular economy, where information about classifications,
processes, and emissions is distributed across independently curated knowledge
graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes
KGQA into subtasks managed by specialized agents responsible for retrieval,
query generation, and verification. A scheduler assigns subgoals to different
graphs using weak-to-strong alignment strategies. A two-stage verifier detects
structurally invalid and semantically underspecified queries through symbolic
validation and counterfactual consistency checks. Experiments on real-world
circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy
by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing
the average prompt length by 46.4%. These results demonstrate the benefits of
agent-based schema-aware reasoning for scalable KGQA and support
decision-making in sustainability domains through robust cross-graph reasoning.

</details>


### [44] [MLP Memory: Language Modeling with Retriever-pretrained External Memory](https://arxiv.org/abs/2508.01832)
*Rubin Wei,Jiaqi Cao,Jiarui Wang,Jushi Kai,Qipeng Guo,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

> 研究提出了一个基于外部MLP记忆模块的架构，改善了LLM的性能，减少了生成文本中的幻觉问题，并提供了更快的推理速度。

<details>
  <summary>Details</summary>

**Motivation:** 现代纯解码器的LLM模型虽然在多个领域表现出色，但生成的文本中普遍存在幻觉问题，阻碍了其在知识密集型任务中的应用。本研究旨在解决这一问题。

**Method:** 本研究提出一种解耦记忆与LLM解码器的方法，使用一个预训练的、可微分的外部记忆模块。外部记忆是一个MLP，通过对整个预训练数据集模仿检索器的行为进行预训练。

**Result:** 新架构在WikiText-103和Web数据集上分别提高了17.5%和24.1%的性能，表现出更强的幂律扩展性，同时在三个幻觉基准和九个记忆密集型任务上表现出色。

**Conclusion:** 结果显示，带有外部MLP记忆模块的架构在多种基准任务上表现出色，速度上优于kNN-LM和纯解码器模型，且对推理能力有积极影响。

**Abstract:** While modern decoder-only LLMs achieve superior performance across various
domains, hallucinations have risen to be a common problem in their generated
text, hindering their application in knowledge-intensive tasks.
Retriever-augmented generation (RAG) offers a solution, but the non-parametric
nature of the retriever hinders its deep interaction with LLM. In this work, we
propose to decouple memorization from the LLM decoder using a pretrained,
differentiable external memory. The external memory is an MLP pretrained by
imitating the behavior of a retriever on the entire pretraining dataset. Our
resulting architecture, which comprises a transformer decoder and an external
MLP memory pretrained on language modeling and retriever imitation
respectively, demonstrates strong perplexity and performance on downstream
tasks. Experiments show our architecture exhibits steeper power-law scaling
with model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web
datasets compared to decoder-only models while benefiting from added training
without overfitting. We demonstrate superior performance on three hallucination
benchmarks and nine memory-intensive tasks. Additionally, our approach delivers
$80\times$ speedup over $k$NN-LM (500M tokens) and $1.3\times$ faster inference
than decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP
memory improves StrategyQA performance. We will open-source our code and models
in the future.

</details>


### [45] [Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents](https://arxiv.org/abs/2508.01858)
*Yuhan Guo,Cong Guo,Aiwen Sun,Hongliang He,Xinyu Yang,Yue Lu,Yingji Zhang,Xuntao Guo,Dong Zhang,Jianzhuang Liu,Jiang Duan,Yijia Xiao,Liangjian Wen,Hai-Ming Xu,Yong Dai*

Main category: cs.CL

> 本文提出了一个名为Web-CogKnowledge Framework的知识框架，强调了网络代理需要具备足够的知识以进行有效的认知推理，并设计了Web-CogDataset数据集和Web-CogReasoner代理模型，实现了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 为了使网络代理能更像人类一样感知并与数字环境互动，本文认为代理需要首先获取足够的知识内容，以进行有效的认知推理。

**Method:** 提出Web-CogKnowledge Framework以分类知识类型（实事、概念和程序），并通过Memorizing, Understanding和Exploring等过程来获取知识，并使用新型的基于知识的Chain-of-Thought（CoT）推理框架训练出了Web-CogReasoner代理模型。

**Result:** 实验显示该模型尤其在面对未知任务时，相较于现有模型表现出更为显著的性能优势。

**Conclusion:** 通过构建Web-CogBench评测套件，验证了Web-CogReasoner代理模型在知识领域和认知能力上的优越性。

**Abstract:** Multimodal large-scale models have significantly advanced the development of
web agents, enabling perception and interaction with digital environments akin
to human cognition. In this paper, we argue that web agents must first acquire
sufficient knowledge to effectively engage in cognitive reasoning. Therefore,
we decompose a web agent's capabilities into two essential stages: knowledge
content learning and cognitive processes. To formalize this, we propose
Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and
Procedural. In this framework, knowledge content learning corresponds to the
agent's processes of Memorizing and Understanding, which rely on the first two
knowledge types, representing the "what" of learning. Conversely, cognitive
processes correspond to Exploring, grounded in Procedural knowledge, defining
the "how" of reasoning and action. To facilitate knowledge acquisition, we
construct the Web-CogDataset, a structured resource curated from 14 real-world
websites, designed to systematically instill core knowledge necessary for web
agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon
which comprehension is built-as well as the basis for learning how to reason
and act. Building on this foundation, we operationalize these processes through
a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing
and training our proposed agent, the Web-CogReasoner. Extensive experimentation
reveals its significant superiority over existing models, especially in
generalizing to unseen tasks where structured knowledge is decisive. To enable
rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation
suite designed to assess and compare agent performance across the delineated
knowledge domains and cognitive capabilities. Our code and data is open sourced
at https://github.com/Gnonymous/Web-CogReasoner

</details>


### [46] [Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2508.01862)
*Yijun Feng*

Main category: cs.CL

> A novel approach called Counterfactual Probing is proposed to detect and mitigate hallucinations in Large Language Models by evaluating the model's sensitivity to counterfactual variations.

<details>
  <summary>Details</summary>

**Motivation:** To detect and mitigate the issue of hallucinations produced by Large Language Models, which are fluent but factually incorrect or unsupported.

**Method:** Counterfactual Probing, which involves generating counterfactual statements to test the robustness of model outputs.

**Result:** Counterfactual probing achieved superior detection performance compared to baseline methods and reduced hallucination scores by an average of 24.5% through adaptive mitigation strategies.

**Conclusion:** The proposed method effectively identifies hallucinations and can be integrated into existing LLM pipelines to enhance the factual accuracy of generated text without requiring model retraining.

**Abstract:** Large Language Models have demonstrated remarkable capabilities across
diverse tasks, yet they frequently generate hallucinations outputs that are
fluent but factually incorrect or unsupported. We propose Counterfactual
Probing, a novel approach for detecting and mitigating hallucinations in LLM
outputs. Our method dynamically generates counterfactual statements that appear
plausible but contain subtle factual errors, then evaluates the model's
sensitivity to these perturbations. We hypothesize that genuine knowledge
exhibits robustness to counterfactual variations, while hallucinated content
shows inconsistent confidence patterns when confronted with plausible
alternatives. Our comprehensive evaluation on TruthfulQA, factual statement
datasets, and curated hallucination examples demonstrates that counterfactual
probing achieves superior detection performance compared to baseline methods,
while our adaptive mitigation strategies reduce hallucination scores by an
average of 24.5%. The approach requires no model retraining and can be
integrated into existing LLM pipelines as a realtime verification mechanism.

</details>


### [47] [Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language](https://arxiv.org/abs/2508.01918)
*Jaskaranjeet Singh,Rakesh Thakur*

Main category: cs.CL

> 论文介绍了一个旁遮普语的大型语言模型PunGPT2及相关框架，旨在提高事实性和领域知识，同时引入了量子感知检索技术以增加上下文相关性，明显改善了低资源语言的生成性能。

<details>
  <summary>Details</summary>

**Motivation:** 目标是对抗低资源语言在自然语言处理领域长期被忽视的问题，提供一种提高此类语言中模型的实用性和性能的方案。

**Method:** 论文提出了PunGPT2，这是第一个完全开源的旁遮普语大型语言模型套件，训练过程中使用了35GB的跨领域语料库，并引入了Pun-RAG框架以提高事实性和领域记忆。此外，还开发了Pun-Instruct，以及创新性的Quantum-RAG系统，结合稀疏和稠密检索方法，并利用量子灵感语义匹配提高了上下文相关性。

**Result:** 研究中的模型在困惑度、事实性和流畅性方面明显优于多语言基线模型（mBERT, mT5, MuRIL）。

**Conclusion:** 该研究工作展示了一种可扩展、可重现的方法蓝图，以增强低资源语言的大型语言模型能力，并在低资源NLP中引入了量子感知检索技术。

**Abstract:** Despite the rapid advancement of large language models (LLMs), low-resource
languages remain largely excluded from the NLP landscape. We present PunGPT2,
the first fully open-source suite of Punjabi large language models, trained
from scratch on a 35GB domain-diverse corpus encompassing literature, religious
texts, news, and social discourse. Unlike prior multilingual approaches,
PunGPT2 captures rich syntactic and morphological features unique to Punjabi
through a tokenizer optimised with byte pair encoding and linguistically
aligned pretraining objectives. To improve factual grounding and domain recall,
we introduce Pun-RAG, a retrieval-augmented generation framework combining
PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We
further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant
using QLoRA, enabling robust zero-shot and instruction-following performance
with significantly reduced compute needs.
  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system
that fuses sparse (BM25) and dense methods with quantum-inspired semantic
matching. By encoding queries using amplitude-based embeddings and retrieving
via quantum kernel similarity, Quantum-RAG achieves improved contextual
relevance with minimal memory overhead marking the first practical integration
of quantum representations in low-resource language generation. Our models
significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in
perplexity, factuality, and fluency. This work provides a scalable,
reproducible blueprint for extending LLM capabilities to underrepresented
languages and pioneers quantum-aware retrieval in low-resource NLP

</details>


### [48] [Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback](https://arxiv.org/abs/2508.01930)
*Tom S. Juzek,Zina B. Ward*

Main category: cs.CL

> 研究揭示了人类反馈学习可能引起的LHF在大型语言模型中的词汇偏好问题，并强调了数据和目标透明度在对齐研究中的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型（LLMs）倾向于过度使用某些词汇，如“深入”和“复杂”，但其背后的动机尚不清楚。本研究的目的是阐明这种过度使用现象是由LHF引起的，并探讨其对远程支持的影响。

**Method:** 本研究采用Meta的Llama模型，通过一个简单的程序检测LLM的词汇偏好，这些偏好可能是由来自人类反馈的学习（LHF）引起的。进一步通过实验模拟LHF过程，验证参与者系统性偏爱包含某些特定词汇的文本变体。

**Result:** 通过实验模拟LHF过程，结果证实参与者倾向于偏好包含某些词汇的文本，进一步证明LHF与词汇过度使用之间的联系。

**Conclusion:** 研究结果表明，词汇过度使用现象可被视为一种偏离对齐，但也突显了不同群体，即LHF工人与LLM用户之间的词汇预期可能存在差异。

**Abstract:** Large Language Models (LLMs) are known to overuse certain terms like "delve"
and "intricate." The exact reasons for these lexical choices, however, have
been unclear. Using Meta's Llama model, this study investigates the
contribution of Learning from Human Feedback (LHF), under which we subsume
Reinforcement Learning from Human Feedback and Direct Preference Optimization.
We present a straightforward procedure for detecting the lexical preferences of
LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to
lexical overuse by experimentally emulating the LHF procedure and demonstrating
that participants systematically prefer text variants that include certain
words. This lexical overuse can be seen as a sort of misalignment, though our
study highlights the potential divergence between the lexical expectations of
different populations -- namely LHF workers versus LLM users. Our work
contributes to the growing body of research on explainable artificial
intelligence and emphasizes the importance of both data and procedural
transparency in alignment research.

</details>


### [49] [ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks](https://arxiv.org/abs/2508.01943)
*Philip Schroeder,Ondrej Biza,Thomas Weng,Hongyin Luo,James Glass*

Main category: cs.CL

> ROVER is a framework for recursive decomposition of long video sequences into shorter subtasks to improve reasoning capabilities of vision-language models in embodied scenarios, outperforming existing baselines.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enable vision-language models (VLMs) to better handle reasoning over extended sequences of camera frames from a video, which is essential for embodied settings where continuous visual input is required at each moment of a task attempt.

**Method:** The paper proposes ROVER (Reasoning Over VidEo Recursively), a framework that recursively decomposes long-horizon video trajectories into segments corresponding to shorter subtasks to enhance reasoning over temporally localized frame sequences without losing global context.

**Result:** ROVER successfully outperforms strong baselines across three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. It also mitigates hallucinations and reduces computational complexity.

**Conclusion:** By decomposing videos into segments corresponding to subtasks, ROVER facilitates more focused and accurate reasoning over temporally localized frame sequences, making it an effective solution for embodied settings that require long-horizon reasoning.

**Abstract:** Vision-language models (VLMs) have exhibited impressive capabilities across
diverse image understanding tasks, but still struggle in settings that require
reasoning over extended sequences of camera frames from a video. This limits
their utility in embodied settings, which require reasoning over long frame
sequences from a continuous stream of visual input at each moment of a task
attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo
Recursively), a framework that enables the model to recursively decompose
long-horizon video trajectories into segments corresponding to shorter subtasks
within the trajectory. In doing so, ROVER facilitates more focused and accurate
reasoning over temporally localized frame sequences without losing global
context. We evaluate ROVER, implemented using an in-context learning approach,
on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa
that consists of 543 videos showing both expert and perturbed non-expert
trajectories across 27 robotic manipulation tasks. ROVER outperforms strong
baselines across three video reasoning tasks: task progress estimation,
frame-level natural language reasoning, and video question answering. We
observe that, by reducing the number of frames the model reasons over at each
timestep, ROVER mitigates hallucinations, especially during unexpected or
non-optimal moments of a trajectory. In addition, by enabling the
implementation of a subtask-specific sliding context window, ROVER's time
complexity scales linearly with video length, an asymptotic improvement over
baselines. Demos, code, and data available at: https://rover-vlm.github.io

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [50] [Team PA-VCG's Solution for Competition on Understanding Chinese College Entrance Exam Papers in ICDAR'25](https://arxiv.org/abs/2508.00834)
*Wei Wu,Wenjie Wang,Yang Tan,Ying Liu,Liang Diao,Lin Huang,Kaihe Xu,Wenfeng Xie,Ziling Lin*

Main category: cs.CV

> Team PA-VGG通过高分辨率图像处理技术和特定领域后训练策略，获得了ICDAR'25竞赛的第一名，准确率达到89.6%。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是为了应对高考文件的独特挑战，包括密集文本提取和复杂布局，从而提高中国高考文件理解竞赛（ICDAR'25）的表现。

**Method:** 本研究采用了高分辨率图像处理技术，并使用多图像端到端输入策略来解决高考文件中的密集OCR提取和复杂文档布局问题。此外，还引入了特定领域的后训练策略以进一步提升性能。

**Result:** 实验结果表明，与其它方法相比，后训练策略表现出色，在ICDAR'25竞赛中以89.6%的准确率取得了第一名的成绩。

**Conclusion:** 结论是，结合高分辨率图像处理和多图像端到端输入策略，通过特定领域的后训练策略可以显著提升对密集文本和复杂布局文档的理解能力，从而获得ICDAR'25竞赛的第一名。

**Abstract:** This report presents Team PA-VGG's solution for the ICDAR'25 Competition on
Understanding Chinese College Entrance Exam Papers. In addition to leveraging
high-resolution image processing and a multi-image end-to-end input strategy to
address the challenges of dense OCR extraction and complex document layouts in
Gaokao papers, our approach introduces domain-specific post-training
strategies. Experimental results demonstrate that our post-training approach
achieves the most outstanding performance, securing first place with an
accuracy rate of 89.6%.

</details>


### [51] [Inclusive Review on Advances in Masked Human Face Recognition Technologies](https://arxiv.org/abs/2508.00841)
*Ali Haitham Abdul Amir,Zainab N. Nemer*

Main category: cs.CV

> 本文综述了口罩对面部识别技术带来的挑战及采用深度学习方法如CNN和Siamese网络克服这些挑战的技术进展。还探讨了光照变化、面部位置变化和口罩类型对识别准确性的影响。未来研究将关注提高性能和扩展应用领域。

<details>
  <summary>Details</summary>

**Motivation:** 旨在综述面部识别技术在口罩普及背景下遇到的新挑战及解决方案，特别是在深度学习技术，尤其是卷积神经网络（CNN）和孪生网络（Siamese网络）上的研究进展，解决面部部分遮挡带来的识别问题。

**Method:** 此论文主要通过综述深度学习技术特别是卷积神经网络（CNN）和孪生网络（Siamese网络）在遮挡面部识别中的应用来探讨缓解面部识别系统识别难度的方法。深网络设计、特征提取技术、评估标准和数据集用于该领域的研究也被讨论了。此外，还讨论了如何使用人工数据库进行数据增强和多媒体方法来提高系统的泛化能力。

**Result:** 论文综述了口罩种类对面部识别系统性能的影响、不同光线和面部位置对识别准确性的影响，并提供了使用深度学习技术（如CNN和Siamese网络）等先进方法提高遮挡面部识别技术的准确性的解决方案。

**Conclusion:** 未来的研究方向将集中在开发更高效的算法上，同时整合多媒体技术来提高识别系统的性能，并扩展其实际应用。

**Abstract:** Masked Face Recognition (MFR) is an increasingly important area in biometric
recognition technologies, especially with the widespread use of masks as a
result of the COVID-19 pandemic. This development has created new challenges
for facial recognition systems due to the partial concealment of basic facial
features. This paper aims to provide a comprehensive review of the latest
developments in the field, with a focus on deep learning techniques, especially
convolutional neural networks (CNNs) and twin networks (Siamese networks),
which have played a pivotal role in improving the accuracy of covering face
recognition. The paper discusses the most prominent challenges, which include
changes in lighting, different facial positions, partial concealment, and the
impact of mask types on the performance of systems. It also reviews advanced
technologies developed to overcome these challenges, including data enhancement
using artificial databases and multimedia methods to improve the ability of
systems to generalize. In addition, the paper highlights advance in deep
network design, feature extraction techniques, evaluation criteria, and data
sets used in this area. Moreover, it reviews the various applications of masked
face recognition in the fields of security and medicine, highlighting the
growing importance of these systems in light of recurrent health crises and
increasing security threats. Finally, the paper focuses on future research
trends such as developing more efficient algorithms and integrating multimedia
technologies to improve the performance of recognition systems in real-world
environments and expand their applications.

</details>


### [52] [HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models](https://arxiv.org/abs/2508.00892)
*Zhihao Zhu,Jiale Han,Yi Yang*

Main category: cs.CV

> HoneyImage通过在少量难以识别的样本中嵌入不易察觉但可验证的痕迹，实现了图像识别模型中数据集所有权的可靠验证，同时保持了数据集完整性，为数据安全共享提供了可能。

<details>
  <summary>Details</summary>

**Motivation:** 由于图像数据集通常含有敏感或专有内容，现有解决方案如后门水印和成员推断在验证有效性和保留数据完整性的权衡上存在问题，因此需要一种新的方法来验证是否未经授权使用了专有数据进行模型训练。

**Method:** HoneyImage是一种新型的数据集所有权验证方法，它通过选择性地修改少量难以识别的样本，嵌入不易察觉但可验证的痕迹，从而实现在保持数据集完整性的同时进行可靠的所有权验证。

**Result:** 广泛的实验结果表明，HoneyImage在四个基准数据集和多个模型架构上保持了强大的验证准确性，且对下游模型性能影响极小，保持了痕迹的难以察觉性。

**Conclusion:** HoneyImage为数据所有者提供了一种实用的机制来保护有价值图像数据集的所有权，促进安全共享，并释放数据驱动人工智能的全部变革潜力。

**Abstract:** Image-based AI models are increasingly deployed across a wide range of
domains, including healthcare, security, and consumer applications. However,
many image datasets carry sensitive or proprietary content, raising critical
concerns about unauthorized data usage. Data owners therefore need reliable
mechanisms to verify whether their proprietary data has been misused to train
third-party models. Existing solutions, such as backdoor watermarking and
membership inference, face inherent trade-offs between verification
effectiveness and preservation of data integrity. In this work, we propose
HoneyImage, a novel method for dataset ownership verification in image
recognition models. HoneyImage selectively modifies a small number of hard
samples to embed imperceptible yet verifiable traces, enabling reliable
ownership verification while maintaining dataset integrity. Extensive
experiments across four benchmark datasets and multiple model architectures
show that HoneyImage consistently achieves strong verification accuracy with
minimal impact on downstream performance while maintaining imperceptible. The
proposed HoneyImage method could provide data owners with a practical mechanism
to protect ownership over valuable image datasets, encouraging safe sharing and
unlocking the full transformative potential of data-driven AI.

</details>


### [53] [Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis](https://arxiv.org/abs/2508.00896)
*Hoang Hai Nam Nguyen,Minh Tien Tran,Hoheok Kim,Ho Won Lee*

Main category: cs.CV

> PF-DiffSeg是基于相分数控制的一阶段去噪扩散框架，用于生成微结构图像及其分割掩模，以此改善金属合金中的分割准确性，特别是在少数类中。

<details>
  <summary>Details</summary>

**Motivation:** 解决金属合金中由于缺乏人类标注的相掩模而导致的机器学习在金属显微组织分割中效果受限的问题，特别是在罕见或组成复杂的形态中。

**Method:** PF-DiffSeg，基于相分数控制的一阶段去噪扩散框架，该框架可以同时生成微结构图像及其对应的分割掩模。通过全局相分数向量进行条件设置，这些向量被增强以代表实际数据分布并强调少数类，从而生成组合有效且结构一致的微结构图像和掩模样本，以提高数据多样性和训练效率。

**Result:** 在MetalDAM基准测试中，特别是在少数类中，合成增强方法在分割准确性方面显著优于标准增强策略，并且还优于两阶段掩模引导扩散和生成对抗网络（GAN）基线，同时减少了推理时间。

**Conclusion:** PF-DiffSeg方法将生成和条件设置集成到一个统一的框架中，为金属组织应用数据增强提供了可扩展的解决方案。

**Abstract:** The effectiveness of machine learning in metallographic microstructure
segmentation is often constrained by the lack of human-annotated phase masks,
particularly for rare or compositionally complex morphologies within the metal
alloy. We introduce PF-DiffSeg, a phase-fraction controlled, one-stage
denoising diffusion framework that jointly synthesizes microstructure images
and their corresponding segmentation masks in a single generative trajectory to
further improve segmentation accuracy. By conditioning on global phase-fraction
vectors, augmented to represent real data distribution and emphasize minority
classes, our model generates compositionally valid and structurally coherent
microstructure image and mask samples that improve both data diversity and
training efficiency. Evaluated on the MetalDAM benchmark for additively
manufactured multiphase steel, our synthetic augmentation method yields notable
improvements in segmentation accuracy compared to standard augmentation
strategies especially in minority classes and further outperforms a two-stage
mask-guided diffusion and generative adversarial network (GAN) baselines, while
also reducing inference time compared to conventional approach. The method
integrates generation and conditioning into a unified framework, offering a
scalable solution for data augmentation in metallographic applications.

</details>


### [54] [Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models](https://arxiv.org/abs/2508.00898)
*Jose M. Sánchez Velázquez,Mingbo Cai,Andrew Coney,Álvaro J. García- Tejedor,Alberto Nogales*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** In recent years, advances in Artificial Intelligence have significantly
impacted computer science, particularly in the field of computer vision,
enabling solutions to complex problems such as video frame prediction. Video
frame prediction has critical applications in weather forecasting or autonomous
systems and can provide technical improvements, such as video compression and
streaming. Among Artificial Intelligence methods, Deep Learning has emerged as
highly effective for solving vision-related tasks, although current frame
prediction models still have room for enhancement. This paper evaluates several
hybrid deep learning approaches that combine the feature extraction
capabilities of autoencoders with temporal sequence modelling using Recurrent
Neural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related
architectures. The proposed solutions were rigorously evaluated on three
datasets that differ in terms of synthetic versus real-world scenarios and
grayscale versus color imagery. Results demonstrate that the approaches perform
well, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid
models utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale
videos with real data are the easiest to predict.

</details>


### [55] [TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras](https://arxiv.org/abs/2508.00913)
*Mohammad Mohammadi,Ziyi Wu,Igor Gilitschenski*

Main category: cs.CV

> TESPEC is an innovative self-supervised pre-training framework designed for event-based models that effectively captures long-term spatio-temporal information, outperforming existing methods in various perception tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind TESPEC is to address the limitations of existing self-supervised learning methods for event-based pre-training that do not sufficiently consider long-term temporal information. It seeks to enhance the performance of recurrent models on event-based perception tasks.

**Method:** TESPEC is a self-supervised pre-training framework specifically designed to capture spatio-temporal information from long event sequences, which is particularly beneficial for recurrent models. It uses a novel method to transform events into pseudo grayscale videos with high-level semantic information, aiming to maintain robustness against sensor noise and reduce motion blur.

**Result:** Experimental results show that the TESPEC framework achieves state-of-the-art performance in various downstream tasks such as object detection, semantic segmentation, and monocular depth estimation, highlighting the effectiveness of the proposed method in learning spatio-temporal information from long event sequences.

**Conclusion:** TESPEC introduces a valuable pre-training framework for event-based models that emphasizes the importance of capturing long-term temporal information. The framework demonstrates its capability to improve performance in downstream tasks and is a promising advancement in the field of event-based visual processing.

**Abstract:** Long-term temporal information is crucial for event-based perception tasks,
as raw events only encode pixel brightness changes. Recent works show that when
trained from scratch, recurrent models achieve better results than feedforward
models in these tasks. However, when leveraging self-supervised pre-trained
weights, feedforward models can outperform their recurrent counterparts.
Current self-supervised learning (SSL) methods for event-based pre-training
largely mimic RGB image-based approaches. They pre-train feedforward models on
raw events within a short time interval, ignoring the temporal information of
events. In this work, we introduce TESPEC, a self-supervised pre-training
framework tailored for learning spatio-temporal information. TESPEC is
well-suited for recurrent models, as it is the first framework to leverage long
event sequences during pre-training. TESPEC employs the masked image modeling
paradigm with a new reconstruction target. We design a novel method to
accumulate events into pseudo grayscale videos containing high-level semantic
information about the underlying scene, which is robust to sensor noise and
reduces motion blur. Reconstructing this target thus requires the model to
reason about long-term history of events. Extensive experiments demonstrate our
state-of-the-art results in downstream tasks, including object detection,
semantic segmentation, and monocular depth estimation. Project webpage:
https://mhdmohammadi.github.io/TESPEC_webpage.

</details>


### [56] [Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition](https://arxiv.org/abs/2508.00941)
*Hassan Ugail,Hamad Mansour Alawar,AbdulNasser Abbas Zehi,Ahmed Mohammad Alkendi,Ismail Lujain Jaleel*

Main category: cs.CV

> 本研究通过测试不同退化类别下的面部识别效果，展示了潜在扩散增强技术在法医人脸识别应用中的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 人脸识别系统在处理低质量的法医证据图像时，其性能严重下降。本文旨在评估潜在扩散增强对于提高法医相关退化情况下人脸识别性能的有效性。

**Method:** 使用含有3,000个个体的LFW数据集与24,000次识别尝试，本研究实现了结合Facezoom LoRA适应性的Flux.1 Kontext Dev流水线，测试了包括压缩伪影、模糊效应和噪声污染在内的七种退化类别。

**Result:** 提出的方法显著提高了识别准确性，从29.1%增加到84.5%，提高了55.4个百分点（95% CI: [54.1, 56.7]）。所有退化类型的统计分析都显示了显著的性能提升，效果大小超过了传统实践显著性的阈值。

**Conclusion:** 这些发现确立了复杂扩散增强技术在法医人脸识别应用中的潜力。

**Abstract:** Face recognition systems experience severe performance degradation when
processing low-quality forensic evidence imagery. This paper presents an
evaluation of latent diffusion-based enhancement for improving face recognition
under forensically relevant degradations. Using a dataset of 3,000 individuals
from LFW with 24,000 recognition attempts, we implement the Flux.1 Kontext Dev
pipeline with Facezoom LoRA adaptation to test against seven degradation
categories, including compression artefacts, blur effects, and noise
contamination. Our approach demonstrates substantial improvements, increasing
overall recognition accuracy from 29.1% to 84.5% (55.4 percentage point
improvement, 95% CI: [54.1, 56.7]). Statistical analysis reveals significant
performance gains across all degradation types, with effect sizes exceeding
conventional thresholds for practical significance. These findings establish
the potential of sophisticated diffusion based enhancement in forensic face
recognition applications.

</details>


### [57] [Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment](https://arxiv.org/abs/2508.00945)
*Yifan Wang,Hongfeng Ai,Quangao Liu,Maowei Jiang,Ruiyuan Kang,Ruiqi Li,Jiahua Dong,Mengting Xiao,Cheng Jiang,Chenzhong Li*

Main category: cs.CV

> 本文提出了一种新的框架CCRA，通过引入LPWCA以及PAI，解决视觉语言模型中跨模态嵌入学习中注意力不一致的问题，并在多个数据集上证明了这种方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决现有的视觉语言模型（VLMs）中跨模态嵌入学习中的注意力机制不协调问题，导致注意力分配不匹配，从而影响模型性能。

**Method:** Structure

**Result:** {
  "tldr": "本文提出了一种新的框架CCRA，通过引入LPWCA以及PAI，解决视觉语言模型中跨模态嵌入学习中注意力不一致的问题，并在多个数据集上证明了这种方法的有效性。",
  "motivation": "本文旨在解决现有的视觉语言模型（VLMs）中跨模态嵌入学习中的注意力机制不协调问题，导致注意力分配不匹配，从而影响模型性能。",
  "method": "提出了一种名为Consistent Cross-layer Regional Alignment (CCRA) 的框架，包含两个关键技术：Layer-Patch-wise Cross Attention (LPWCA)，用于捕捉细粒度的区域语义相关性；Progressive Attention Integration (PAI)，用于逐步协调不同层次和片段上的注意力机制。",
  "result": "在十个多样化的视觉-语言基准测试上，他们的增强模型LLaVA-v1.5-7B展示了最先进的性能，比所有基线方法都有所改进，同时仅仅增加了3.55M参数。",
  "conclusion": "该研究表明，通过CCRA增强的LLaVA-v1.5-7B模型不仅提高了性能，还在可解释性方面有所提升，表现为注意力分布更加聚焦和语义对齐。"]}

**Conclusion:** 该研究表明，通过CCRA增强的LLaVA-v1.5-7B模型不仅提高了性能，还在可解释性方面有所提升，表现为注意力分布更加聚焦和语义对齐。

**Abstract:** Vision Language Models (VLMs) face challenges in effectively coordinating
diverse attention mechanisms for cross-modal embedding learning, leading to
mismatched attention and suboptimal performance. We propose Consistent
Cross-layer Regional Alignment (CCRA), which introduces Layer-Patch-wise Cross
Attention (LPWCA) to capture fine-grained regional-semantic correlations by
jointly weighting patch and layer-wise embedding, and Progressive Attention
Integration (PAI) that systematically coordinates LPWCA, layer-wise, and
patch-wise attention mechanisms in sequence. This progressive design ensures
consistency from semantic to regional levels while preventing attention drift
and maximizing individual attention benefits. Experimental results on ten
diverse vision-language benchmarks demonstrate that our CCRA-enhanced
LLaVA-v1.5-7B model achieves state-of-the-art performance, outperforming all
baseline methods with only 3.55M additional parameters, while providing
enhanced interpretability through more regionally focused and semantically
aligned attention patterns.

</details>


### [58] [ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling](https://arxiv.org/abs/2508.00974)
*Daniel Andrés López,Vincent Weber,Severin Zentgraf,Barlo Hillen,Perikles Simon,Elmar Schömer*

Main category: cs.CV

> 本文通过改进自动标注技术，将红外热成像用于体育医学中暴露的腓肠肌区域热辐射评估。研究表明使用少量的手动标注数据进行微调可以改善深度学习模型在不同运动场景下的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在探索红外热成像技术在运动医学中的应用潜力，特别是在露出的腓肠肌区域的热辐射分析和评估。通过改进自动注释方法，提高这一技术在不同运动场景下的适用性和精确度。

**Method:** 本文提出了一种基于立体和多模态标记方法的迁移学习技术，旨在将先前在跑步机上运行的方法转移到骑行器上。这种方法结合了语义分割网络的自动标签训练和高精度的手动标注图像的微调。

**Result:** 研究结果显示，即便是使用少量的手动标注数据进行微调，也能显著提升深度神经网络的整体性能。

**Conclusion:** 自动标签与少量手动标注数据集的结合，可以加速深度神经网络在新应用场景中的适应能力，例如从跑步机场景转换到自行车场景。

**Abstract:** Infrared thermography is emerging as a powerful tool in sports medicine,
allowing assessment of thermal radiation during exercise and analysis of
anatomical regions of interest, such as the well-exposed calves. Building on
our previous advanced automatic annotation method, we aimed to transfer the
stereo- and multimodal-based labeling approach from treadmill running to
ergometer cycling. Therefore, the training of the semantic segmentation network
with automatic labels and fine-tuning on high-quality manually annotated images
has been examined and compared in different data set combinations. The results
indicate that fine-tuning with a small fraction of manual data is sufficient to
improve the overall performance of the deep neural network. Finally, combining
automatically generated labels with small manually annotated data sets
accelerates the adaptation of deep neural networks to new use cases, such as
the transition from treadmill to bicycle.

</details>


### [59] [ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation](https://arxiv.org/abs/2508.01008)
*Cihang Peng,Qiming Hou,Zhong Ren,Kun Zhou*

Main category: cs.CV

> 研究介绍了ROVI数据集和实验结果，展示了其在图像质量和分辨率方面超过了现有的检测数据集，并且包含的类别数量也要高出两个数量级。基于ROVI训练的文本到图像模型GLIGEN在实例定位准确性、提示保真度和审美质量方面显著超过了现有最先进的方法。

<details>
  <summary>Details</summary>

**Motivation:** 这项工作的动机在于为文本到图像的生成提供一个高质量的合成数据集，通过引入新的标注策略来改进当前的方法。

**Method:** 我们提出了ROVI，一个高质量的合成数据集用于实例化文本到图像的生成，通过标注精心挑选的100万张网络图片来创建。我们的关键创新是一种称为重新加标题的战略，重点在于预检测阶段。视觉语言模型（VLM）生成全面的视觉描述，然后由大语言模型（LLM）处理，提取开放词汇检测器（OVD）可以检测到的潜在类别的扁平列表。

**Result:** 实验结果表明，基于ROVI数据集训练的模型GLIGEN在实例接地准确度、提示保真度和审美质量方面显著优于当前最先进的模型。

**Conclusion:** 结论提到ROVI数据集很好地改善了文本到图像生成的质量，展示了该数据集在视觉生成任务上的潜力。相关数据和可重现流程可在指定的GitHub仓库中找到。

**Abstract:** We present ROVI, a high-quality synthetic dataset for instance-grounded
text-to-image generation, created by labeling 1M curated web images. Our key
innovation is a strategy called re-captioning, focusing on the pre-detection
stage, where a VLM (Vision-Language Model) generates comprehensive visual
descriptions that are then processed by an LLM (Large Language Model) to
extract a flat list of potential categories for OVDs (Open-Vocabulary
Detectors) to detect. This approach yields a global prompt inherently linked to
instance annotations while capturing secondary visual elements humans typically
overlook. Evaluations show that ROVI exceeds existing detection datasets in
image quality and resolution while containing two orders of magnitude more
categories with an open-vocabulary nature. For demonstrative purposes, a
text-to-image model GLIGEN trained on ROVI significantly outperforms
state-of-the-art alternatives in instance grounding accuracy, prompt fidelity,
and aesthetic quality. Our dataset and reproducible pipeline are available at
https://github.com/CihangPeng/ROVI.

</details>


### [60] [AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise](https://arxiv.org/abs/2508.01015)
*Byron Dowling,Jozef Probcin,Adam Czajka*

Main category: cs.CV

> AutoSIGHT系统利用眼动追踪数据来自动评估人类在执行视觉任务时的专业水平。实验表明，该系统在5秒的评估窗口下性能良好，随着评估窗口的增大，性能进一步提升。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨是否可以通过眼动追踪数据自动评估人类在解决视觉任务时的专业水平。

**Method:** 该研究提出了AutoSIGHT系统，一个基于眼动追踪数据自动评估人类视觉任务解决能力的系统。该系统使用来自执行视觉任务时的眼动追踪数据提取的特征集合对专家和非专家的表现进行分类。

**Result:** 实验结果显示，在使用5秒的评估窗口时，AutoSIGHT系统在科目分离的训练测试条件下，实现了平均ROC曲线下面积为0.751的性能。当评估窗口扩大到最多30秒时，AUROC值提高到0.8306，表明系统能有效利用更多信息，虽然决策略有延迟。

**Conclusion:** 这项研究开启了将人类和机器专业水平的自动权衡融入人类-人工智能配对设置的新研究领域，这种设置需要针对人类和人工智能参与者之间的非静态专业水平分布动态反应。

**Abstract:** Can we teach machines to assess the expertise of humans solving visual tasks
automatically based on eye tracking features? This paper proposes AutoSIGHT,
Automatic System for Immediate Grading of Human experTise, that classifies
expert and non-expert performers, and builds upon an ensemble of features
extracted from eye tracking data while the performers were solving a visual
task. Results on the task of iris Presentation Attack Detection (PAD) used for
this study show that with a small evaluation window of just 5 seconds,
AutoSIGHT achieves an average average Area Under the ROC curve performance of
0.751 in subject-disjoint train-test regime, indicating that such detection is
viable. Furthermore, when a larger evaluation window of up to 30 seconds is
available, the Area Under the ROC curve (AUROC) increases to 0.8306, indicating
the model is effectively leveraging more information at a cost of slightly
delayed decisions. This work opens new areas of research on how to incorporate
the automatic weighing of human and machine expertise into human-AI pairing
setups, which need to react dynamically to nonstationary expertise distribution
between the human and AI players (e.g. when the experts need to be replaced, or
the task at hand changes rapidly). Along with this paper, we offer the eye
tracking data used in this study collected from 6 experts and 53 non-experts
solving iris PAD visual task.

</details>


### [61] [3D Reconstruction via Incremental Structure From Motion](https://arxiv.org/abs/2508.01019)
*Muhammad Zeeshan,Umer Zaki,Syed Ahmed Pasha,Zaar Khizar*

Main category: cs.CV

> This paper presents an incremental SfM pipeline for improving 3D reconstruction robustness and flexibility, demonstrating its effectiveness with real data and statistical assessments.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the limitations of global SfM techniques, which are sensitive to noise and missing data. By presenting an incremental SfM method, the authors aim to provide a more robust and flexible solution for 3D reconstruction in practical applications.

**Method:** The paper focuses on a detailed implementation of the incremental Structure from Motion (SfM) pipeline, emphasizing the consistency of geometric estimation and the iterative refinement of the reconstruction process through bundle adjustment.

**Result:** The approach is demonstrated using a real dataset, and the quality of the reconstruction is assessed through reprojection error and camera trajectory coherence. The results indicate that incremental SfM is a reliable method for 3D reconstruction in sparse or partially overlapping datasets.

**Conclusion:** The conclusion is that incremental SfM provides a practical and reliable method for achieving 3D reconstructions in visually structured environments, particularly when dealing with sparse or partially overlapping datasets.

**Abstract:** Accurate 3D reconstruction from unstructured image collections is a key
requirement in applications such as robotics, mapping, and scene understanding.
While global Structure from Motion (SfM) techniques rely on full image
connectivity and can be sensitive to noise or missing data, incremental SfM
offers a more flexible alternative. By progressively incorporating new views
into the reconstruction, it enables the system to recover scene structure and
camera motion even in sparse or partially overlapping datasets. In this paper,
we present a detailed implementation of the incremental SfM pipeline, focusing
on the consistency of geometric estimation and the effect of iterative
refinement through bundle adjustment. We demonstrate the approach using a real
dataset and assess reconstruction quality through reprojection error and camera
trajectory coherence. The results support the practical utility of incremental
SfM as a reliable method for sparse 3D reconstruction in visually structured
environments.

</details>


### [62] [Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans](https://arxiv.org/abs/2508.01045)
*Theo Di Piazza,Carole Lazarus,Olivier Nempont,Loic Boussel*

Main category: cs.CV

> 本文提出了一种新的基于图的方法，用于提高3D CT扫描中的多标签异常分类性能。该方法通过轴向切片三元组节点在频域中进行卷积处理，从而表现出强大的跨数据集泛化能力和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 随着CT扫描检查数量的增加，需要自动方法，如器官分割、异常检测和报告生成来帮助放射科医生管理不断增加的工作量。多标签分类3D CT扫描仍然是一个关键且具有挑战性的任务，由于体积数据内复杂的空间关系和观察到的各种异常。

**Method:** 本研究提出了一种基于图的新方法，将CT扫描表示为结构化图，通过轴向切片三元组节点，并在频域中进行卷积处理，以提高多标签异常分类性能。

**Result:** 该方法表现出强大的跨数据集泛化能力和竞争力，同时对z轴平移具有鲁棒性。进行的消融研究评估了每个提出组件的贡献。

**Conclusion:** 提出的方法通过将CT扫描表示为结构化图，并在频域中对轴向切片三元组节点使用卷积，展示了比现有方法更强的多标签异常分类性能。该方法不仅跨数据集表现出色，还证明了其在z轴平移上的鲁棒性。

**Abstract:** With the increasing number of CT scan examinations, there is a need for
automated methods such as organ segmentation, anomaly detection and report
generation to assist radiologists in managing their increasing workload.
Multi-label classification of 3D CT scans remains a critical yet challenging
task due to the complex spatial relationships within volumetric data and the
variety of observed anomalies. Existing approaches based on 3D convolutional
networks have limited abilities to model long-range dependencies while Vision
Transformers suffer from high computational costs and often require extensive
pre-training on large-scale datasets from the same domain to achieve
competitive performance. In this work, we propose an alternative by introducing
a new graph-based approach that models CT scans as structured graphs,
leveraging axial slice triplets nodes processed through spectral domain
convolution to enhance multi-label anomaly classification performance. Our
method exhibits strong cross-dataset generalization, and competitive
performance while achieving robustness to z-axis translation. An ablation study
evaluates the contribution of each proposed component.

</details>


### [63] [Evading Data Provenance in Deep Neural Networks](https://arxiv.org/abs/2508.01074)
*Hongyu Zhu,Sichu Liang,Wenwen Wang,Zhuomeng Zhang,Fangqi Li,Shi-Lin Wang*

Main category: cs.CV

> 本文提出了一种统一的逃避数据集所有权验证(DOV)框架，通过任务相关的领域知识转移到代理模型上，以达到更好地平衡泛化能力和逃避效果，并在多样化的数据集上证明了该方法的有效性，揭示了当前DOV方法的关键漏洞，强调了长期发展的必要性。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的逃避DOV攻击过于简单，导致了安全性的错觉。本文旨在揭示DOV系统的潜在漏洞，并提出更为有效的逃避方法。

**Method:** 本文提出的方法包括使用教师模型从版权数据集学习，然后通过一个非分布（OOD）数据集将任务相关的领域知识转移到代理学生模型上，并提出有选择地转移任务相关知识以更好地平衡泛化能力和逃避效果。利用视觉语言模型和大型语言模型来挑选OOD数据集中的信息和可靠的子集作为最终的转移集。

**Result:** 实验结果表明，本文的方法在所有测试的DOV方法中同时消除了所有版权标识符，在泛化能力和逃避有效性方面显著优于九种最先进的逃避攻击方法，计算开销适度。

**Conclusion:** 本文揭示了现有数据集所有权验证方法的关键漏洞，并提出了一种有效地逃避这些验证系统的方法，强调了长期改进这些系统以提高其实际实用性的重要性。

**Abstract:** Modern over-parameterized deep models are highly data-dependent, with large
scale general-purpose and domain-specific datasets serving as the bedrock for
rapid advancements. However, many datasets are proprietary or contain sensitive
information, making unrestricted model training problematic. In the open world
where data thefts cannot be fully prevented, Dataset Ownership Verification
(DOV) has emerged as a promising method to protect copyright by detecting
unauthorized model training and tracing illicit activities. Due to its
diversity and superior stealth, evading DOV is considered extremely
challenging. However, this paper identifies that previous studies have relied
on oversimplistic evasion attacks for evaluation, leading to a false sense of
security. We introduce a unified evasion framework, in which a teacher model
first learns from the copyright dataset and then transfers task-relevant yet
identifier-independent domain knowledge to a surrogate student using an
out-of-distribution (OOD) dataset as the intermediary. Leveraging
Vision-Language Models and Large Language Models, we curate the most
informative and reliable subsets from the OOD gallery set as the final transfer
set, and propose selectively transferring task-oriented knowledge to achieve a
better trade-off between generalization and evasion effectiveness. Experiments
across diverse datasets covering eleven DOV methods demonstrate our approach
simultaneously eliminates all copyright identifiers and significantly
outperforms nine state-of-the-art evasion attacks in both generalization and
effectiveness, with moderate computational overhead. As a proof of concept, we
reveal key vulnerabilities in current DOV methods, highlighting the need for
long-term development to enhance practicality.

</details>


### [64] [DreamSat-2.0: Towards a General Single-View Asteroid 3D Reconstruction](https://arxiv.org/abs/2508.01079)
*Santiago Diaz,Xinghui Hu,Josiane Uwumukiza,Giovanni Lavezzi,Victor Rodriguez-Fernandez,Richard Linares*

Main category: cs.CV

> DreamSat-2.0 评估了3D 重构模型在航天器和小行星数据集上的性能，结果表明模型性能依赖于领域。Hunyuan-3D 在航天器图像质量和小行星几何重建方面均表现显著，超过了之前的工作。

<details>
  <summary>Details</summary>

**Motivation:** 为了提升小行星探索和自主航天器导航，引入了 DreamSat-2.0，其目的是评估3D重构模型在特定领域的性能。

**Method:** 通过使用自定义的航天器和小行星数据集，DreamSat-2.0 对三种最先进的 3D 重构模型（Hunyuan-3D, Trellis-3D, 和 Ouroboros-3D）进行了评估，利用了2D感知（图像质量）和3D几何（形状准确性）指标。

**Result:** 研究发现模型性能与领域相关。模型在更复杂的航天器上产生了高质量的图像，而在更简单的小行星形态上实现了更好的几何重建。Hunyuan-3D 在航天器上实现了最高的感知得分，并且在小行星上实现了最佳的几何精度。

**Conclusion:** 这项研究确立了新的基准，并表明特定领域的 3D 重构模型需要进行特定的优化。Hunyuan-3D 的表现标志着相对于先前工作的重要进展。

**Abstract:** To enhance asteroid exploration and autonomous spacecraft navigation, we
introduce DreamSat-2.0, a pipeline that benchmarks three state-of-the-art 3D
reconstruction models-Hunyuan-3D, Trellis-3D, and Ouroboros-3D-on custom
spacecraft and asteroid datasets. Our systematic analysis, using 2D perceptual
(image quality) and 3D geometric (shape accuracy) metrics, reveals that model
performance is domain-dependent. While models produce higher-quality images of
complex spacecraft, they achieve better geometric reconstructions for the
simpler forms of asteroids. New benchmarks are established, with Hunyuan-3D
achieving top perceptual scores on spacecraft but its best geometric accuracy
on asteroids, marking a significant advance over our prior work.

</details>


### [65] [COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition](https://arxiv.org/abs/2508.01087)
*Ryan Rabinowitz,Steve Cruz,Walter Scheirer,Terrance E. Boult*

Main category: cs.CV

> COSTARR方法通过利用训练中被忽略的衰减信息，显著提升开放集合识别（OSR）的能力，该方法结合了熟悉特征和Hadamard乘积特征，有效处理了新颖性检测问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有OSR方法主要依赖于熟悉性假设，但这种方法忽略了小权重衰减的特征，这些特征对于区分已知与未知类别具有潜在价值。

**Method:** 提出了一种新的衰减假说，并依据此假设开发了COSTARR方法，结合熟悉特征的需求和缺乏陌生特征的条件，同时引入了Hadamard乘积特征，以此来提升识别的准确性。

**Result:** 处理新颖性仍然是视觉识别系统中的关键挑战。现有的开放集合识别（OSR）方法依赖熟悉性假设，通过缺乏熟悉特征来检测新颖性。我们提出了一种新颖的衰减假说：在训练期间学习的小权重会衰减特征，并起到双重作用——区分已知类别同时丢弃有助于区分已知和未知类别的信息。为了利用这种被忽视的信息，我们提出了COSTARR，一种新颖的方法，它结合了对熟悉特征的需求和缺乏陌生特征的条件。我们对COSTARR得分提供了概率解释，将其与正确分类和属于已知类的似然性联系起来。为了确定预先和衰减后的特征对COSTARR性能的影响，我们进行了消融研究，表明确豫和未充分利用的Hadamard乘积特征对改善OSR都至关重要。此外，在使用ImageNet2012-1K作为已知数据以及使用NINCO、iNaturalist、OpenImage-O及其他数据集作为未知的情况下，我们对COSTARR进行了大规模设置测试，并通过多个现代预训练模型（ViTs，ConvNeXts，和ResNet）进行了评估。实验表明，COSTARR在各种架构中有效推广，并且通过整合先前被丢弃的衰减信息，显著超越了之前的最先进方法，从而提升了开放集识别的能力。

**Conclusion:** COSTARR方法能够有效处理新颖性识别问题，通过整合以往被忽略的衰减信息，它能够更好地区分已知类别和未知类别，这对于开放集识别能力的提升至关重要。

**Abstract:** Handling novelty remains a key challenge in visual recognition systems.
Existing open-set recognition (OSR) methods rely on the familiarity hypothesis,
detecting novelty by the absence of familiar features. We propose a novel
attenuation hypothesis: small weights learned during training attenuate
features and serve a dual role-differentiating known classes while discarding
information useful for distinguishing known from unknown classes. To leverage
this overlooked information, we present COSTARR, a novel approach that combines
both the requirement of familiar features and the lack of unfamiliar ones. We
provide a probabilistic interpretation of the COSTARR score, linking it to the
likelihood of correct classification and belonging in a known class. To
determine the individual contributions of the pre- and post-attenuated features
to COSTARR's performance, we conduct ablation studies that show both
pre-attenuated deep features and the underutilized post-attenuated Hadamard
product features are essential for improving OSR. Also, we evaluate COSTARR in
a large-scale setting using ImageNet2012-1K as known data and NINCO,
iNaturalist, OpenImage-O, and other datasets as unknowns, across multiple
modern pre-trained architectures (ViTs, ConvNeXts, and ResNet). The experiments
demonstrate that COSTARR generalizes effectively across various architectures
and significantly outperforms prior state-of-the-art methods by incorporating
previously discarded attenuation information, advancing open-set recognition
capabilities.

</details>


### [66] [AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions](https://arxiv.org/abs/2508.01095)
*Mikhail Bychkov,Matey Yordanov,Andrei Kuchma*

Main category: cs.CV

> AURA是一个创新的混合时空-色度框架，用于在工业烟雾排放的实时检测和分类中提供更高的准确性和减少误报。

<details>
  <summary>Details</summary>

**Motivation:** 现有监测系统往往无法区分烟雾类型，且难以应对环境变化。AURA框架旨在解决这些问题。

**Method:** AURA框架结合工业烟雾的动态运动模式和独特的颜色特征，旨在提高准确性并减少误报。

**Result:** 通过利用动态运动模式和颜色特征，AURA在工业烟雾检测和分类中表现出色，能提供更高的准确性。

**Conclusion:** AURA框架能够通过精确、自动化的工业排放监测，显著提升环境合规性、操作安全性及公众健康结果。

**Abstract:** This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework
designed for robust, real-time detection and classification of industrial smoke
emissions. The framework addresses critical limitations of current monitoring
systems, which often lack the specificity to distinguish smoke types and
struggle with environmental variability. AURA leverages both the dynamic
movement patterns and the distinct color characteristics of industrial smoke to
provide enhanced accuracy and reduced false positives. This framework aims to
significantly improve environmental compliance, operational safety, and public
health outcomes by enabling precise, automated monitoring of industrial
emissions.

</details>


### [67] [Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting](https://arxiv.org/abs/2508.01098)
*Yuekun Dai,Haitian Li,Shangchen Zhou,Chen Change Loy*

Main category: cs.CV

> 我们提出了Trans-Adapter，一种即插即用的适配器，它能使基于扩散的修复模型直接处理带有透明度的RGBA图像，并解决了透明度一致性的问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有图像修复方法无法有效处理RGBA图像的特点，尤其是在保持透明性一致性方面的难题。

**Method:** RGBA图像，由于额外的alpha通道，对于需要混合、遮罩或透明效果的应用非常重要，因此比标准RGB图像更为灵活。然而，现有的图像修复方法仅针对RGB图像设计。处理透明图像的传统方法通常涉及在RGBA图像下放置一个背景，然后采用两阶段过程：图像修复，接着是图像抠图。然而，这种管道难以保持编辑区域的透明度一致性，抠图可能会在透明边界处引入锯齿状边缘。为了解决这些问题，我们提出了Trans-Adapter，这是一个即插即用的适配器，使基于扩散的修复模型能够直接处理透明图像。Trans-Adapter还支持通过ControlNet实现可控编辑，并且可以无缝集成到各种社区模型中。为了评估我们的方法，我们引入了LayerBench，以及一种新的非参考alpha边缘质量评估指标，用于评估透明边缘质量。我们在LayerBench上进行了广泛的实验，以展示我们方法的有效性。

**Result:** 我们在LayerBench上进行了广泛的实验，展示通过引入新的非参考alpha边缘质量评估指标，Trans-Adapter能够有效改善透明边缘的质量。

**Conclusion:** Trans-Adapter为直接处理透明图像的修复任务提供了一种有效且灵活的解决方案，并且能够被无缝集成到各种现有的图像修复模型中。

**Abstract:** RGBA images, with the additional alpha channel, are crucial for any
application that needs blending, masking, or transparency effects, making them
more versatile than standard RGB images. Nevertheless, existing image
inpainting methods are designed exclusively for RGB images. Conventional
approaches to transparent image inpainting typically involve placing a
background underneath RGBA images and employing a two-stage process: image
inpainting followed by image matting. This pipeline, however, struggles to
preserve transparency consistency in edited regions, and matting can introduce
jagged edges along transparency boundaries. To address these challenges, we
propose Trans-Adapter, a plug-and-play adapter that enables diffusion-based
inpainting models to process transparent images directly. Trans-Adapter also
supports controllable editing via ControlNet and can be seamlessly integrated
into various community models. To evaluate our method, we introduce LayerBench,
along with a novel non-reference alpha edge quality evaluation metric for
assessing transparency edge quality. We conduct extensive experiments on
LayerBench to demonstrate the effectiveness of our approach.

</details>


### [68] [MASIV: Toward Material-Agnostic System Identification from Videos](https://arxiv.org/abs/2508.01112)
*Yizhou Zhao,Haoyu Chen,Chunjiang Liu,Zhenyang Li,Charles Herrmann,Junhwa Hur,Yinxiao Li,Ming-Hsuan Yang,Bhiksha Raj,Min Xu*

Main category: cs.CV

> MASIV, a vision-based framework, is introduced for material-agnostic system identification, overcoming the limitation of existing methods that depend on predefined material priors.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of current methods in handling unknown material properties, leading to unstable optimization and implausible behaviors.

**Method:** MASIV uses learnable neural constitutive models and dense geometric guidance to infer object dynamics without assuming a scene-specific material prior.

**Result:** Comprehensive experiments show that MASIV achieves state-of-the-art performance in geometric accuracy, rendering quality, and generalization ability.

**Conclusion:** MASIV provides a new direction for system identification by allowing the inference of dynamics without predefined material priors, enhancing performance in various metrics.

**Abstract:** System identification from videos aims to recover object geometry and
governing physical laws. Existing methods integrate differentiable rendering
with simulation but rely on predefined material priors, limiting their ability
to handle unknown ones. We introduce MASIV, the first vision-based framework
for material-agnostic system identification. Unlike existing approaches that
depend on hand-crafted constitutive laws, MASIV employs learnable neural
constitutive models, inferring object dynamics without assuming a
scene-specific material prior. However, the absence of full particle state
information imposes unique challenges, leading to unstable optimization and
physically implausible behaviors. To address this, we introduce dense geometric
guidance by reconstructing continuum particle trajectories, providing
temporally rich motion constraints beyond sparse visual cues. Comprehensive
experiments show that MASIV achieves state-of-the-art performance in geometric
accuracy, rendering quality, and generalization ability.

</details>


### [69] [The Promise of RL for Autoregressive Image Editing](https://arxiv.org/abs/2508.01119)
*Saba Ahmadi,Rabiul Awal,Ankur Sikarwar,Amirhossein Kazemnejad,Ge Ya Luo,Juan A. Rodriguez,Sai Rajeswar,Siva Reddy,Christopher Pal,Benno Krojer,Aishwarya Agrawal*

Main category: cs.CV

> 本文提出并研究了三种提升图像编辑任务性能的策略，发现结合强化学习和大模型验证的方法最为有效，并发布了相应的模型EARL。

<details>
  <summary>Details</summary>

**Motivation:** 旨在探索并优化自回归多模态模型在图像编辑任务中的性能，尤其是通过对比不同策略的效果，找出最有效的方案。

**Method:** 本文采用了一个自回归多模态模型来统一处理文本和视觉标记，研究了三种策略：监督微调（SFT）、强化学习（RL）和链式思考（CoT）推理，用于提升广泛的图像编辑任务的性能。

**Result:** 研究发现，结合大型多模态LLM验证器的RL策略最为有效。因此，作者发布了EARL，一种依靠RL的强大图像编辑模型，在广泛的编辑任务上表现优异，尽管其训练数据量相对较少。

**Conclusion:** 该研究推进了自回归多模态模型在图像编辑领域的前沿。

**Abstract:** We explore three strategies to enhance performance on a wide range of image
editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and
Chain-of-Thought (CoT) reasoning. In order to study all these components in one
consistent framework, we adopt an autoregressive multimodal model that
processes textual and visual tokens in a unified manner. We find RL combined
with a large multi-modal LLM verifier to be the most effective of these
strategies. As a result, we release EARL: Editing with Autoregression and RL, a
strong RL-based image editing model that performs competitively on a diverse
range of edits compared to strong baselines, despite using much less training
data. Thus, EARL pushes the frontier of autoregressive multimodal models on
image editing. We release our code, training data, and trained models at
https://github.com/mair-lab/EARL.

</details>


### [70] [UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation](https://arxiv.org/abs/2508.01126)
*Chaitanya Patel,Hiroki Nakamura,Yuta Kyuragi,Kazuki Kozuka,Juan Carlos Niebles,Ehsan Adeli*

Main category: cs.CV

> 本文提出了UniEgoMotion模型，可从第一人称视觉输入中进行运动重建、预测和生成，利用图像中的场景信息，不依赖于显式的3D场景；该模型在第一人称运动重建方面达到顶尖水平，并能从单一的第一人称图像生成运动，显著提升了第一人称应用的性能。

<details>
  <summary>Details</summary>

**Motivation:** 作者希望通过解决现有方法主要集中在第三人称运动合成的问题，特别是在第一人称视角设置下，如有限的视野、频繁的遮挡和动态相机等因素对场景感知的限制，来提高AR/VR体验、改善人机交互、提升辅助技术和支持自适应医疗服务。

**Method:** 本文提出了一种名为UniEgoMotion的统一条件运动扩散模型，该模型采用了一种适合第一人称视角设备的新头中心运动表示法，可以从第一人称视觉输入中进行运动重建、预测和生成。该方法利用第一人称图像进行场景感知的运动合成，不依赖于显式的3D场景。

**Result:** UniEgoMotion在第一人称运动重建方面达到了最先进的性能，并且首次从单个第一人称图像生成运动。

**Conclusion:** 通过广泛的评估，证明了该统一框架的有效性，为第一人称运动建模设立了新的基准，并为第一人称视角应用开辟了新的可能性。

**Abstract:** Egocentric human motion generation and forecasting with scene-context is
crucial for enhancing AR/VR experiences, improving human-robot interaction,
advancing assistive technologies, and enabling adaptive healthcare solutions by
accurately predicting and simulating movement from a first-person perspective.
However, existing methods primarily focus on third-person motion synthesis with
structured 3D scene contexts, limiting their effectiveness in real-world
egocentric settings where limited field of view, frequent occlusions, and
dynamic cameras hinder scene perception. To bridge this gap, we introduce
Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks
that utilize first-person images for scene-aware motion synthesis without
relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional
motion diffusion model with a novel head-centric motion representation tailored
for egocentric devices. UniEgoMotion's simple yet effective design supports
egocentric motion reconstruction, forecasting, and generation from first-person
visual inputs in a unified framework. Unlike previous works that overlook scene
semantics, our model effectively extracts image-based scene context to infer
plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a
large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth
3D motion annotations. UniEgoMotion achieves state-of-the-art performance in
egocentric motion reconstruction and is the first to generate motion from a
single egocentric image. Extensive evaluations demonstrate the effectiveness of
our unified framework, setting a new benchmark for egocentric motion modeling
and unlocking new possibilities for egocentric applications.

</details>


### [71] [Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach](https://arxiv.org/abs/2508.01137)
*Zeduo Zhang,Yalda Mohsenzadeh*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** To develop a domain-agnostic, semi-supervised anomaly detection framework
that integrates deep reinforcement learning (DRL) to address challenges such as
large-scale data, overfitting, and class imbalance, focusing on brain MRI
volumes. This retrospective study used publicly available brain MRI datasets
collected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and
578 T2-weighted MRI volumes (from healthy subjects) for training, while the
BraTS 2021 dataset provided 251 volumes for validation and 1000 for testing
(unhealthy subjects with Glioblastomas). Preprocessing included normalization,
skull-stripping, and co-registering to a uniform voxel size. Experiments were
conducted on both T1- and T2-weighted modalities. Additional experiments and
ablation analyses were also carried out on the industrial datasets. The
proposed method integrates DRL with feature representations to handle label
scarcity, large-scale data and overfitting. Statistical analysis was based on
several detection and segmentation metrics including AUROC and Dice score. The
proposed method achieved an AUROC of 88.7% (pixel-level) and 96.7%
(image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA)
methods. On industrial surface datasets, the model also showed competitive
performance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset,
indicating strong cross-domain generalization. Studies on anomaly sample size
showed a monotonic increase in AUROC as more anomalies were seen, without
evidence of overfitting or additional computational cost. The domain-agnostic
semi-supervised approach using DRL shows significant promise for MRI anomaly
detection, achieving strong performance on both medical and industrial
datasets. Its robustness, generalizability and efficiency highlight its
potential for real-world clinical applications.

</details>


### [72] [Dataset Condensation with Color Compensation](https://arxiv.org/abs/2508.01139)
*Huyu Wu,Duo Su,Junjie Hou,Guang Li*

Main category: cs.CV

> DC3是一种数据集浓缩框架，通过颜色补偿来优化图像颜色多样性，克服了现有方法的不足，在多个基准上展示了优越的表现和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 通过观察发现，在数据集浓缩过程中忽视了颜色作为信息载体和基本语义表示单元的双重角色，这影响了表现学习的效果。因此，作者提出提升浓缩图像的颜色丰富度对表示学习是有益的。

**Method:** DC3采用经过校准的选择策略，使用潜在扩散模型来增强图像的颜色多样性，而不是创造全新的图像。

**Result:** 实验结果表明，DC3在多个基准上超越了最先进的方法，FID结果显示，使用该高质数据集训练网络是可行的，不会出现模型崩溃或其他退化问题。

**Conclusion:** DC3是首个利用浓缩数据集微调预训练扩散模型的研究，证明了其在下游任务上的表现和高质数据集应用的可行性。

**Abstract:** Dataset condensation always faces a constitutive trade-off: balancing
performance and fidelity under extreme compression. Existing methods struggle
with two bottlenecks: image-level selection methods (Coreset Selection, Dataset
Quantization) suffer from inefficiency condensation, while pixel-level
optimization (Dataset Distillation) introduces semantic distortion due to
over-parameterization. With empirical observations, we find that a critical
problem in dataset condensation is the oversight of color's dual role as an
information carrier and a basic semantic representation unit. We argue that
improving the colorfulness of condensed images is beneficial for representation
learning. Motivated by this, we propose DC3: a Dataset Condensation framework
with Color Compensation. After a calibrated selection strategy, DC3 utilizes
the latent diffusion model to enhance the color diversity of an image rather
than creating a brand-new one. Extensive experiments demonstrate the superior
performance and generalization of DC3 that outperforms SOTA methods across
multiple benchmarks. To the best of our knowledge, besides focusing on
downstream tasks, DC3 is the first research to fine-tune pre-trained diffusion
models with condensed datasets. The FID results prove that training networks
with our high-quality datasets is feasible without model collapse or other
degradation issues. Code and generated data will be released soon.

</details>


### [73] [OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding](https://arxiv.org/abs/2508.01150)
*Dianyi Yang,Xihan Wang,Yu Gao,Shiyang Liu,Bohan Ren,Yufeng Yue,Yi Yang*

Main category: cs.CV

> OpenGS-Fusion是用于改进语义建模和提高对象级理解的新型开放词汇密集映射框架。

<details>
  <summary>Details</summary>

**Motivation:** 尽管3D场景理解在VR/AR和机器人应用方面取得了进展，但现有的方法受限于刚性的离线流程和无法对开放式的查询提供精确的3D对象级理解。

**Method:** OpenGS-Fusion采用了3D高斯表示与截断符号距离场结合的方式，实现语义特征的无损融合。此外，该方法还引入了一种名为MLLM辅助自适应阈值的多模态语言引导方法，以自适应调整相似度阈值，从而精细化3D对象的分割。

**Result:** 实验结果表明，OpenGS-Fusion在3D对象理解和场景重建质量方面优于现有方法，并在语言引导的场景交互中展示了其有效性。

**Conclusion:** 代码已开源，可供进一步研究和应用。该方法证明了在3D对象理解和场景重建上具有优越性，尤其是在语言引导的场景交互中展现了出色的性能。

**Abstract:** Recent advancements in 3D scene understanding have made significant strides
in enabling interaction with scenes using open-vocabulary queries, particularly
for VR/AR and robotic applications. Nevertheless, existing methods are hindered
by rigid offline pipelines and the inability to provide precise 3D object-level
understanding given open-ended queries. In this paper, we present
OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that
improves semantic modeling and refines object-level understanding.
OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed
Distance Field to facilitate lossless fusion of semantic features on-the-fly.
Furthermore, we introduce a novel multimodal language-guided approach named
MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D
objects by adaptively adjusting similarity thresholds, achieving an improvement
17\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments
demonstrate that our method outperforms existing methods in 3D object
understanding and scene reconstruction quality, as well as showcasing its
effectiveness in language-guided scene interaction. The code is available at
https://young-bit.github.io/opengs-fusion.github.io/ .

</details>


### [74] [Personalized Safety Alignment for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.01151)
*Yu Lei,Jinbin Bai,Qingyu Shi,Aosong Feng,Kaidong Yu*

Main category: cs.CV

> 提出个性化安全对齐（PSA）框架，用于控制文本到图像生成模型中的安全行为，使得生成的内容更好地匹配用户的个人安全偏好。结果表明PSA能够有效降低有害内容的生成，并提高了用户约束的满足程度。代码与数据已公开。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文本到图像生成模型的安全机制应用统一标准，忽略了由年龄、精神健康和个人信仰等因素形成的多样化的安全边界，无法满足个体用户的偏好。

**Method:** 提出Personalized Safety Alignment (PSA)框架，该框架在生成模型中集成了个性化的用户配置文件，通过交叉注意力机制调整模型的行为以匹配个人安全偏好，同时保持图像质量。引入了新的数据集Sage，用于捕捉用户特定的安全偏好。

**Result:** 实验结果表明，PSA在有害内容抑制方面优于现有方法，并更好地遵循用户设定的约束条件，实现了更高的胜率和通过率。

**Conclusion:** Personalized Safety Alignment (PSA)能够更有效地针对个人用户的安全偏好进行调整，提升生成内容的安全性与用户满意度。

**Abstract:** Text-to-image diffusion models have revolutionized visual content generation,
but current safety mechanisms apply uniform standards that often fail to
account for individual user preferences. These models overlook the diverse
safety boundaries shaped by factors like age, mental health, and personal
beliefs. To address this, we propose Personalized Safety Alignment (PSA), a
framework that allows user-specific control over safety behaviors in generative
models. PSA integrates personalized user profiles into the diffusion process,
adjusting the model's behavior to match individual safety preferences while
preserving image quality. We introduce a new dataset, Sage, which captures
user-specific safety preferences and incorporates these profiles through a
cross-attention mechanism. Experiments show that PSA outperforms existing
methods in harmful content suppression and aligns generated content better with
user constraints, achieving higher Win Rate and Pass Rate scores. Our code,
data, and models are publicly available at
https://torpedo2648.github.io/PSAlign/.

</details>


### [75] [LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation](https://arxiv.org/abs/2508.01152)
*Xinyu Yan,Meijun Sun,Ge-Peng Ji,Fahad Shahbaz Khan,Salman Khan,Deng-Ping Fan*

Main category: cs.CV

> 论文介绍了LawDIS框架，一种语言窗口驱动的可控二分图分割方法，利用语言和窗口控制策略生成和细化高质量对象遮罩，显著超越现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 为了开发一个高质量对象遮罩生成的可控二分图分割框架，需要解决现有方法在生成高精度和用户个性化应用方面的不足。

**Method:** LawDIS框架通过将DIS问题重新定义为基于潜在扩散模型的图像条件下的掩码生成任务，实现了用户控制的无缝集成。它具有宏模式和微模式两种控制模式。在宏模式下，通过语言控制的分割策略（LS）生成初始掩码，基于用户提供的语言提示。在微模式下，通过窗口控制的细化策略（WR）允许用户自定义区域内（即大小可调的窗口）进行灵活的细化调整。这些模式可以独立或联合操作，由模式切换器协调控制。

**Result:** 在DIS5K基准测试中，LawDIS框架在所有指标上显著超越了11种前沿方法。与第二好的模型MVANet相比，在DIS-TE数据集上，使用LS和WR策略的Fβω得分提升了4.6%，仅使用LS策略提升了3.6%。

**Conclusion:** LawDIS作为一个语言窗口驱动的可控二分图分割框架，实现了高精度个性化应用，其在DIS任务上的表现优于当前所有方法。

**Abstract:** We present LawDIS, a language-window-based controllable dichotomous image
segmentation (DIS) framework that produces high-quality object masks. Our
framework recasts DIS as an image-conditioned mask generation task within a
latent diffusion model, enabling seamless integration of user controls. LawDIS
is enhanced with macro-to-micro control modes. Specifically, in macro mode, we
introduce a language-controlled segmentation strategy (LS) to generate an
initial mask based on user-provided language prompts. In micro mode, a
window-controlled refinement strategy (WR) allows flexible refinement of
user-defined regions (i.e., size-adjustable windows) within the initial mask.
Coordinated by a mode switcher, these modes can operate independently or
jointly, making the framework well-suited for high-accuracy, personalised
applications. Extensive experiments on the DIS5K benchmark reveal that our
LawDIS significantly outperforms 11 cutting-edge methods across all metrics.
Notably, compared to the second-best model MVANet, we achieve $F_\beta^\omega$
gains of 4.6\% with both the LS and WR strategies and 3.6\% gains with only the
LS strategy on DIS-TE. Codes will be made available at
https://github.com/XinyuYanTJU/LawDIS.

</details>


### [76] [TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition](https://arxiv.org/abs/2508.01153)
*Xiahan Yang,Hui Zheng*

Main category: cs.CV

> 论文提出了一种新颖的训练范式 TEACH，旨在改善场景文本识别任务的准确性和鲁棒性，特别是在具有挑战性的情况下。通过模拟课程学习过程，TEACH 显著提升了多种基准测试中的模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 场景文本识别仍然是一项具有挑战性的任务，主要由于复杂的视觉外观和有限的语义先验知识。

**Method:** TEACH 是一种新的训练范式，它将真实文本注入模型作为辅助输入，并在训练过程中逐步减少其影响。通过将目标标签编码到嵌入空间中，并应用损失感知掩码，TEACH 模拟了一个课程学习过程，引导模型从基于标签的学习向完全基于视觉的识别转变。不同于基于语言模型的方法，TEACH 不需要外部预训练，并且在推理时没有额外开销。它是模型无关的，可以无缝集成到现有的编码器-解码器框架中。

**Result:** 大量实验表明，使用 TEACH 训练的模型在多个公开基准测试中实现了持续提高的准确性，尤其是在具有挑战性的情况下，这验证了 TEACH 的鲁棒性和通用适用性。

**Conclusion:** TEACH 方法展示了其在提高场景文本识别任务的模型鲁棒性和准确性方面的能力，尤其是在具有挑战的情况下，这使其成为一个非常有前景的训练范式。

**Abstract:** Scene Text Recognition (STR) remains a challenging task due to complex visual
appearances and limited semantic priors. We propose TEACH, a novel training
paradigm that injects ground-truth text into the model as auxiliary input and
progressively reduces its influence during training. By encoding target labels
into the embedding space and applying loss-aware masking, TEACH simulates a
curriculum learning process that guides the model from label-dependent learning
to fully visual recognition. Unlike language model-based approaches, TEACH
requires no external pretraining and introduces no inference overhead. It is
model-agnostic and can be seamlessly integrated into existing encoder-decoder
frameworks. Extensive experiments across multiple public benchmarks show that
models trained with TEACH achieve consistently improved accuracy, especially
under challenging conditions, validating its robustness and general
applicability.

</details>


### [77] [DELTAv2: Accelerating Dense 3D Tracking](https://arxiv.org/abs/2508.01170)
*Tuan Duc Ngo,Ashkan Mirzaei,Guocheng Qian,Hanwen Liang,Chuang Gan,Evangelos Kalogerakis,Peter Wonka,Chaoyang Wang*

Main category: cs.CV

> 我们提出了一种新的算法用于加速视频中的密集长期3D点追踪。该方法运用粗到细策略和优化相关特征计算成本来提升效率，在保持准确率的前提下比现有的方法快5-100倍。

<details>
  <summary>Details</summary>

**Motivation:** 通过分析现有的顶级追踪方法，我们发现了两个主要计算瓶颈，这限制了追踪效率。因此，我们的动机是改进现有方法，提高追踪速度而不牺牲准确性。

**Method:** 通过分析现有最先进的方法，我们确定了两个主要的计算瓶颈。首先，基于变压器的迭代追踪在处理大量轨迹时变得昂贵。为了解决这个问题，我们引入了一种粗到细的策略，从少量的点开始追踪，并逐步扩展跟踪轨迹的集合。新添加的轨迹使用可学习的插值模块初始化，该模块与跟踪网络一起端到端训练。其次，我们提出了一种优化方法，显著降低了相关特征计算的成本，这是先前方法中的另一个关键瓶颈。

**Result:** 这些改进使得我们的方法比现有方法快5-100倍，同时保持了最先进的跟踪准确性。

**Conclusion:** 我们的方法通过解决计算瓶颈实现了显著的速度提升，表明该方法在实际应用中非常有前景。

**Abstract:** We propose a novel algorithm for accelerating dense long-term 3D point
tracking in videos. Through analysis of existing state-of-the-art methods, we
identify two major computational bottlenecks. First, transformer-based
iterative tracking becomes expensive when handling a large number of
trajectories. To address this, we introduce a coarse-to-fine strategy that
begins tracking with a small subset of points and progressively expands the set
of tracked trajectories. The newly added trajectories are initialized using a
learnable interpolation module, which is trained end-to-end alongside the
tracking network. Second, we propose an optimization that significantly reduces
the cost of correlation feature computation, another key bottleneck in prior
methods. Together, these improvements lead to a 5-100x speedup over existing
approaches while maintaining state-of-the-art tracking accuracy.

</details>


### [78] [No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2508.01171)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

> 本文介绍了一种无需姿态监督的高效3D高斯散射图生成框架SPFSplat，支持在变化视角下进行高质量的新视图合成。

<details>
  <summary>Details</summary>

**Motivation:** 动机是为了实现一种无需姿态监督的训练范式和高效的单步前向设计，适用于实际应用，特别是在显著视角变化和图像重叠有限的情况下进行新视图合成。

**Method:** 提出了一种名为SPFSplat的高效框架，用于从稀疏的多视角图像生成3D高斯散射图，训练和推理过程中不需要真实姿态。该框架使用了一个共享特征提取骨干，可以同时预测3D高斯原始形状和相机姿态。为了增强几何约束，除了基于估计的新视角姿态的渲染损失外，还引入了投影损失。

**Result:** 实验表明，即使没有姿态监督，SPFSplat在新视图合成方面仍能达到最先进水平，尤其是在存在显著视角变化和图像重叠较少的场景下。在相对姿态估计上，SPFSplat超过了使用几何先验训练的其他方法。

**Conclusion:** 结论表明，SPFSplat即使在没有姿态监督的情况下，也能在新视图合成方面达到最先进的性能，并且在相对姿态估计上超越了训练中使用几何先验的近期方法。

**Abstract:** We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from
sparse multi-view images, requiring no ground-truth poses during training or
inference. It employs a shared feature extraction backbone, enabling
simultaneous prediction of 3D Gaussian primitives and camera poses in a
canonical space from unposed inputs within a single feed-forward step.
Alongside the rendering loss based on estimated novel-view poses, a
reprojection loss is integrated to enforce the learning of pixel-aligned
Gaussian primitives for enhanced geometric constraints. This pose-free training
paradigm and efficient one-step feed-forward design make SPFSplat well-suited
for practical applications. Remarkably, despite the absence of pose
supervision, SPFSplat achieves state-of-the-art performance in novel view
synthesis even under significant viewpoint changes and limited image overlap.
It also surpasses recent methods trained with geometry priors in relative pose
estimation. Code and trained models are available on our project page:
https://ranrhuang.github.io/spfsplat/.

</details>


### [79] [Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning](https://arxiv.org/abs/2508.01184)
*Xinhang Wan,Dongqiang Gou,Xinwang Liu,En Zhu,Xuming He*

Main category: cs.CV

> 本文提出了一种新型方法，通过跨模态3D表征和分阶段推理策略，解决了现有方法在3D对象操作学习中预测不一致和不完整的问题，提升了对象功能区域定位和分类的性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法在3D对象功能区域定位（3D功能性地物定位）和理解（功能分类）过程中的不一致性。这些问题主要由于缺乏对这两个任务之间相互依赖性的适当建模和它们的固定尺度操作导致。

**Method:** 提出一种新的方法，通过跨模态3D表征和融合多尺度几何特征传播，实现对完整功能区域的推理，并采用分两个阶段的预测机制，耦合定位和分类任务，以增强对功能的理解。

**Result:** 实验表明，该方法在功能区域定位和分类任务方面表现出更优的性能。

**Conclusion:** 新型方法通过改进现有方法的缺陷，如预测不一致和不完整，以及尺度适应性问题，提高了对象功能区域的定位与分类的准确性。

**Abstract:** A core problem of Embodied AI is to learn object manipulation from
observation, as humans do. To achieve this, it is important to localize 3D
object affordance areas through observation such as images (3D affordance
grounding) and understand their functionalities (affordance classification).
Previous attempts usually tackle these two tasks separately, leading to
inconsistent predictions due to lacking proper modeling of their dependency. In
addition, these methods typically only ground the incomplete affordance areas
depicted in images, failing to predict the full potential affordance areas, and
operate at a fixed scale, resulting in difficulty in coping with affordances
significantly varying in scale with respect to the whole object. To address
these issues, we propose a novel approach that learns an affordance-aware 3D
representation and employs a stage-wise inference strategy leveraging the
dependency between grounding and classification tasks. Specifically, we first
develop a cross-modal 3D representation through efficient fusion and
multi-scale geometric feature propagation, enabling inference of full potential
affordance areas at a suitable regional scale. Moreover, we adopt a simple
two-stage prediction mechanism, effectively coupling grounding and
classification for better affordance understanding. Experiments demonstrate the
effectiveness of our method, showing improved performance in both affordance
grounding and classification.

</details>


### [80] [A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding](https://arxiv.org/abs/2508.01197)
*Zhan Shi,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

> This paper presents GroundingOcc, a model for 3D occupancy grounding designed for autonomous driving applications, improving upon existing approaches that often fail to accurately capture fine-grained object details.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to improve the accuracy of object perception in autonomous driving scenarios. The authors aim to overcome the limitations of current visual grounding techniques that rely on bounding boxes and may not capture fine-grained details effectively, leading to inaccurate object representations.

**Method:** The paper proposes an end-to-end model named GroundingOcc for 3D occupancy grounding in challenging outdoor scenes, designed to address the shortcomings of traditional bounding box-based visual grounding tasks. The model integrates visual, textual, and point cloud features, using a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. It also incorporates a 2D grounding module and a depth estimation module to enhance geometric understanding.

**Result:** Experiments on the introduced 3D occupancy grounding benchmark validate the effectiveness of GroundingOcc. The model outperforms existing baselines, demonstrating its capability to accurately predict object locations and occupancy information.

**Conclusion:** The study concludes that GroundingOcc, by integrating visual, textual, and point cloud data with a specialized architecture for multi-modal learning, achieves superior performance in 3D occupancy grounding compared to existing methods, thus providing a more accurate means of object representation in autonomous driving scenarios.

**Abstract:** Visual grounding aims to identify objects or regions in a scene based on
natural language descriptions, essential for spatially aware perception in
autonomous driving. However, existing visual grounding tasks typically depend
on bounding boxes that often fail to capture fine-grained details. Not all
voxels within a bounding box are occupied, resulting in inaccurate object
representations. To address this, we introduce a benchmark for 3D occupancy
grounding in challenging outdoor scenes. Built on the nuScenes dataset, it
integrates natural language with voxel-level occupancy annotations, offering
more precise object perception compared to the traditional grounding task.
Moreover, we propose GroundingOcc, an end-to-end model designed for 3D
occupancy grounding through multi-modal learning. It combines visual, textual,
and point cloud features to predict object location and occupancy information
from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder
for feature extraction, an occupancy head for voxel-wise predictions, and a
grounding head to refine localization. Additionally, a 2D grounding module and
a depth estimation module enhance geometric understanding, thereby boosting
model performance. Extensive experiments on the benchmark demonstrate that our
method outperforms existing baselines on 3D occupancy grounding. The dataset is
available at https://github.com/RONINGOD/GroundingOcc.

</details>


### [81] [Deep Learning for Pavement Condition Evaluation Using Satellite Imagery](https://arxiv.org/abs/2508.01206)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Pan Lu,Jingran Sun*

Main category: cs.CV

> 研究利用深度学习模型分析卫星图像评估路面状况，收集超过3000张图像，准确率超过90%，为未来基础设施监测提供了快速经济的方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的手动或车辆自动调查方法评估基础设施状况耗时耗力。因此，该研究旨在探索利用卫星系统和图像处理算法改进基础设施监测和维护的方法，寻求更经济高效的方式。

**Method:** 该研究使用深度学习模型分析卫星图像以评估路面状况，利用TxDOT的PMIS数据库中的路面评估评级，收集了超过3000张路面区域的卫星图像。

**Result:** 研究表明，使用深度学习模型分析卫星图像评估路面状况的准确率超过90%，展示了利用技术改进监测基础设施状况的有效性。

**Conclusion:** 该研究为未来快速且经济高效地评估路面网络开辟了道路，证明了使用深度学习模型分析卫星图像的方法是可行的。

**Abstract:** Civil infrastructure systems covers large land areas and needs frequent
inspections to maintain their public service capabilities. The conventional
approaches of manual surveys or vehicle-based automated surveys to assess
infrastructure conditions are often labor-intensive and time-consuming. For
this reason, it is worthwhile to explore more cost-effective methods for
monitoring and maintaining these infrastructures. Fortunately, recent
advancements in satellite systems and image processing algorithms have opened
up new possibilities. Numerous satellite systems have been employed to monitor
infrastructure conditions and identify damages. Due to the improvement in
ground sample distance (GSD), the level of detail that can be captured has
significantly increased. Taking advantage of these technology advancement, this
research investigated to evaluate pavement conditions using deep learning
models for analyzing satellite images. We gathered over 3,000 satellite images
of pavement sections, together with pavement evaluation ratings from TxDOT's
PMIS database. The results of our study show an accuracy rate is exceeding 90%.
This research paves the way for a rapid and cost-effective approach to
evaluating the pavement network in the future.

</details>


### [82] [RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification](https://arxiv.org/abs/2508.01210)
*Tianze Wang,Zhang Zhang,Chao Yue,Nuoran Li,Chao Sun*

Main category: cs.CV

> 研究提出了一种新的方法RoadMamba，旨在通过结合局部和全局把握信息，改进基于视觉技术的道路表面分类，提高了道路表面识别的性能和精确度。

<details>
  <summary>Details</summary>

**Motivation:** 现有的Mamba架构在道路表面分类任务中，由于无法有效提取道路表面的局部纹理，导致无法达到最先进的性能。通过结合局部和全局信息，本研究旨在解决这个问题，提高道路表面条件的视觉识别精度。

**Method:** 本文提出了一种名为RoadMamba的方法，它有效地结合了局部和全局感知，用于道路表面分类任务。该方法利用了双重状态空间模型（DualSSM）有效地提取道路表面的全局语义和局部纹理，并通过双重注意力融合（DAF）进行了解码和特征融合。此外，还提出了双重辅助损失函数，显式约束双分支，防止网络只依赖于深层大感受野的全局语义信息，而忽视局部纹理。

**Result:** 新提出的方法RoadMamba达到了最先进的道路表面分类性能，在包含100万样本的大规模数据集上进行了验证。

**Conclusion:** 实验结果表明，RoadMamba在包含100万样本的大规模道路表面分类数据集上达到了最先进的性能，证明了其有效性和优势。

**Abstract:** Acquiring the road surface conditions in advance based on visual technologies
provides effective information for the planning and control system of
autonomous vehicles, thus improving the safety and driving comfort of the
vehicles. Recently, the Mamba architecture based on state-space models has
shown remarkable performance in visual processing tasks, benefiting from the
efficient global receptive field. However, existing Mamba architectures
struggle to achieve state-of-the-art visual road surface classification due to
their lack of effective extraction of the local texture of the road surface. In
this paper, we explore for the first time the potential of visual Mamba
architectures for road surface classification task and propose a method that
effectively combines local and global perception, called RoadMamba.
Specifically, we utilize the Dual State Space Model (DualSSM) to effectively
extract the global semantics and local texture of the road surface and decode
and fuse the dual features through the Dual Attention Fusion (DAF). In
addition, we propose a dual auxiliary loss to explicitly constrain dual
branches, preventing the network from relying only on global semantic
information from the deep large receptive field and ignoring the local texture.
The proposed RoadMamba achieves the state-of-the-art performance in experiments
on a large-scale road surface classification dataset containing 1 million
samples.

</details>


### [83] [StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling](https://arxiv.org/abs/2508.01215)
*Yuanlin Yang,Quanjian Song,Zhexian Gao,Ge Wang,Shanshan Li,Xiaoyan Zhang*

Main category: cs.CV

> StyDeco是一个无监督的风格迁移框架，它通过PGD和CSD策略克服了传统扩散模型在处理文本引导的风格迁移时存在的局限，提升了风格迁移任务中的语义结构保存和风格保真度。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于克服传统扩散模型在风格迁移时，将文本描述视为统一且不变的指导方式，这对语义结构和细粒度细节的保存造成了挑战。

**Method:** 文章提出了一种无监督的框架StyDeco，以解决文本引导的扩散模型在风格迁移任务上的局限性。该框架通过Prior-Guided Data Distillation (PGD)方法，利用冻结的生成模型自动合成伪配对数据。接着，引入了对比语义解耦（CSD）策略，采用特定领域的权重调整文本编码器，使源和目标表示在语义空间中形成不同的聚类。

**Result:** 在三个经典基准上的大量实验表明，该框架在风格保真度和结构保持方面优于多种现有方法，此外还支持去风格化过程，进一步证明了其灵活性和有效性。

**Conclusion:** 通过提出PGD和CSD策略，StyDeco框架在无监督情况下有效提升了风格迁移任务中的文本-视觉风格匹配精度和结构保持能力。其独特性还在于支持去风格化的过程，显示了方法的广泛适用性。

**Abstract:** Diffusion models have emerged as the dominant paradigm for style transfer,
but their text-driven mechanism is hindered by a core limitation: it treats
textual descriptions as uniform, monolithic guidance. This limitation overlooks
the semantic gap between the non-spatial nature of textual descriptions and the
spatially-aware attributes of visual style, often leading to the loss of
semantic structure and fine-grained details during stylization. In this paper,
we propose StyDeco, an unsupervised framework that resolves this limitation by
learning text representations specifically tailored for the style transfer
task. Our framework first employs Prior-Guided Data Distillation (PGD), a
strategy designed to distill stylistic knowledge without human supervision. It
leverages a powerful frozen generative model to automatically synthesize
pseudo-paired data. Subsequently, we introduce Contrastive Semantic Decoupling
(CSD), a task-specific objective that adapts a text encoder using
domain-specific weights. CSD performs a two-class clustering in the semantic
space, encouraging source and target representations to form distinct clusters.
Extensive experiments on three classic benchmarks demonstrate that our
framework outperforms several existing approaches in both stylistic fidelity
and structural preservation, highlighting its effectiveness in style transfer
with semantic preservation. In addition, our framework supports a unique
de-stylization process, further demonstrating its extensibility. Our code is
vailable at https://github.com/QuanjianSong/StyDeco.

</details>


### [84] [Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?](https://arxiv.org/abs/2508.01216)
*Bolei Chen,Shengsheng Yan,Yongzheng Cui,Jiaxu Kang,Ping Zhong,Jianxin Wang*

Main category: cs.CV

> 提出了一种利用无监督学习技术和分类器总结房间类型信息进行FLoc的方法，通过注入这些信息到FLoc算法，能够有效提升定位准确性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的FLoc方法要么依赖于匹配2D结构线索，要么基于3D几何约束的视觉预训练，忽略了视觉图像提供的丰富上下文信息。本文提出利用更广泛的视觉场景上下文来增强FLoc算法，利用场景布局先验知识消除定位的不确定性。

**Method:** 我们提出了一种无监督学习技术，并使用聚类约束在自行收集的未标记房间图像上预训练了一个房间分类器。这个分类器能够提取图像中隐藏的房间类型，并将其与其他房间类型区分开来。通过将分类器总结的场景上下文信息注入FLoc算法中，房间风格知识被有效地用于指导确定性的视觉FLoc。

**Result:** 我们在两个标准的视觉FLoc基准上开展了详尽的对比研究。实验表明，我们的方法不仅超越了最先进的方法，而且在鲁棒性和准确性方面也取得了显著提高。

**Conclusion:** 通过将视觉场景上下文的信息引入到FLoc算法中，我们成功地增强了定位的准确性和可靠性，超越了现有技术。

**Abstract:** Since a building's floorplan remains consistent over time and is inherently
robust to changes in visual appearance, visual Floorplan Localization (FLoc)
has received increasing attention from researchers. However, as a compact and
minimalist representation of the building's layout, floorplans contain many
repetitive structures (e.g., hallways and corners), thus easily result in
ambiguous localization. Existing methods either pin their hopes on matching 2D
structural cues in floorplans or rely on 3D geometry-constrained visual
pre-trainings, ignoring the richer contextual information provided by visual
images. In this paper, we suggest using broader visual scene context to empower
FLoc algorithms with scene layout priors to eliminate localization uncertainty.
In particular, we propose an unsupervised learning technique with clustering
constraints to pre-train a room discriminator on self-collected unlabeled room
images. Such a discriminator can empirically extract the hidden room type of
the observed image and distinguish it from other room types. By injecting the
scene context information summarized by the discriminator into an FLoc
algorithm, the room style knowledge is effectively exploited to guide definite
visual FLoc. We conducted sufficient comparative studies on two standard visual
Floc benchmarks. Our experiments show that our approach outperforms
state-of-the-art methods and achieves significant improvements in robustness
and accuracy.

</details>


### [85] [MoGaFace: Momentum-Guided and Texture-Aware Gaussian Avatars for Consistent Facial Geometry](https://arxiv.org/abs/2508.01218)
*Yujian Liu,Linlang Cao,Chuang Chen,Fanyu Geng,Dongxu Shen,Peng Cao,Shidang Xu,Xiaoli Liu*

Main category: cs.CV

> 本文介绍MoGaFace，一种新颖的3D头像重建框架，通过连续优化几何和纹理，在渲染过程中解决了现有方法中的不对齐问题，实现了高质量的头像重建尤其在复杂环境下。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决现有的3D头像重建方法中存在的网状结构和目标图像之间的不对齐问题，这会导致次优的渲染质量和细节损失。

**Method:** 本文提出了MoGaFace框架来解决现有的3D头像重建方法中的次优渲染质量和细节丢失问题。MoGaFace通过引入动量引导的一致几何模块和潜在纹理注意力机制在高斯渲染过程中连续优化面部几何和纹理属性。

**Result:** 实验结果表明，MoGaFace在头部头像重建中实现了高保真度，并显著提高了新颖视图合成的质量，即使在不准确的网格初始化和不受限制的真实世界环境中也是如此。

**Conclusion:** 通过引入动量引导的一致几何模块和潜在纹理注意力，MoGaFace框架在各种情况下均能提供高质量的3D头像重建成果，并改善了新颖视图的合成质量。

**Abstract:** Existing 3D head avatar reconstruction methods adopt a two-stage process,
relying on tracked FLAME meshes derived from facial landmarks, followed by
Gaussian-based rendering. However, misalignment between the estimated mesh and
target images often leads to suboptimal rendering quality and loss of fine
visual details. In this paper, we present MoGaFace, a novel 3D head avatar
modeling framework that continuously refines facial geometry and texture
attributes throughout the Gaussian rendering process. To address the
misalignment between estimated FLAME meshes and target images, we introduce the
Momentum-Guided Consistent Geometry module, which incorporates a
momentum-updated expression bank and an expression-aware correction mechanism
to ensure temporal and multi-view consistency. Additionally, we propose Latent
Texture Attention, which encodes compact multi-view features into head-aware
representations, enabling geometry-aware texture refinement via integration
into Gaussians. Extensive experiments show that MoGaFace achieves high-fidelity
head avatar reconstruction and significantly improves novel-view synthesis
quality, even under inaccurate mesh initialization and unconstrained real-world
settings.

</details>


### [86] [Eigen Neural Network: Unlocking Generalizable Vision with Eigenbasis](https://arxiv.org/abs/2508.01219)
*Anzhe Cheng,Chenzhong Yin,Mingxi Cheng,Shukai Duan,Shahin Nazarian,Paul Bogdan*

Main category: cs.CV

> The paper introduces the Eigen Neural Network (ENN), which reparameterizes weights in a learned orthonormal eigenbasis to improve feature clarity and learning dynamics. ENN outperforms state-of-the-art methods and its BP-free variant (ENN-$\ell$) achieves significant speedup and accuracy improvements.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of disordered weight structures in Deep Neural Networks (DNN) which harms feature clarity and degrades learning dynamics.

**Method:** Eigen Neural Network (ENN) reparameterizes each layer's weights in a layer-shared, learned orthonormal eigenbasis to enforce decorrelated, well-aligned weight dynamics.

**Result:** ENN consistently outperforms state-of-the-art methods on large-scale image classification benchmarks such as ImageNet and achieves superior performance in cross-modal image-text retrieval. ENN-$\ell$, a backpropagation-free (BP-free) variant of ENN, achieves over 2$\times$ training speedup and surpasses the accuracy of end-to-end backpropagation.

**Conclusion:** ENN presents a new architectural paradigm that remedies the representational deficiencies of gradient-based optimization, leading to enhanced performance and more efficient, parallelizable training.

**Abstract:** The remarkable success of Deep Neural Networks(DNN) is driven by
gradient-based optimization, yet this process is often undermined by its
tendency to produce disordered weight structures, which harms feature clarity
and degrades learning dynamics. To address this fundamental representational
flaw, we introduced the Eigen Neural Network (ENN), a novel architecture that
reparameterizes each layer's weights in a layer-shared, learned orthonormal
eigenbasis. This design enforces decorrelated, well-aligned weight dynamics
axiomatically, rather than through regularization, leading to more structured
and discriminative feature representations. When integrated with standard BP,
ENN consistently outperforms state-of-the-art methods on large-scale image
classification benchmarks, including ImageNet, and its superior representations
generalize to set a new benchmark in cross-modal image-text retrieval.
Furthermore, ENN's principled structure enables a highly efficient,
backpropagation-free(BP-free) local learning variant, ENN-$\ell$. This variant
not only resolves BP's procedural bottlenecks to achieve over 2$\times$
training speedup via parallelism, but also, remarkably, surpasses the accuracy
of end-to-end backpropagation. ENN thus presents a new architectural paradigm
that directly remedies the representational deficiencies of BP, leading to
enhanced performance and enabling a more efficient, parallelizable training
regime.

</details>


### [87] [ParaRevSNN: A Parallel Reversible Spiking Neural Network for Efficient Training and Inference](https://arxiv.org/abs/2508.01223)
*Changqing Xu,Guoqing Sun,Yi Liu,Xinfang Liao,Yintang Yang*

Main category: cs.CV

> ParaRevSNN通过解耦顺序依赖关系实现可逆脉冲神经网络的并行计算，提高了训练与推理速度，同时保持了内存优势，非常适合资源受限的场景。

<details>
  <summary>Details</summary>

**Motivation:** RevSNNs虽然可以通过在反向传播过程中重构前向激活来实现内存效率的提高，但受限于严格顺序计算导致的高延迟。因此，提出ParaRevSNN旨在解决这一问题，同时保持内存效率。

**Method:** ParaRevSNN提出了一种可并行的可逆脉冲神经网络架构，通过解耦可逆块之间的顺序依赖关系，同时保留可逆性，使得在训练和推理过程中可以实现跨块并行，从而提高了速度，同时保持了可逆性带来的内存节省优势。

**Result:** 在CIFAR10, CIFAR100, CIFAR10-DVS, 和DVS128 Gesture数据集上的实验显示，ParaRevSNN与标准RevSNN在精度上持平或更好，同时缩短了训练时间（最多35.2%）和推理时间（至多18.15%）。

**Conclusion:** ParaRevSNN不仅加速了训练和推理过程，而且保留了内存效率的特性，因此非常适合资源受限的部署场景。

**Abstract:** Reversible Spiking Neural Networks (RevSNNs) enable memory-efficient training
by reconstructing forward activations during backpropagation, but suffer from
high latency due to strictly sequential computation. To overcome this
limitation, we propose ParaRevSNN, a parallel reversible SNN architecture that
decouples sequential dependencies between reversible blocks while preserving
reversibility. This design enables inter-block parallelism, significantly
accelerating training and inference while retaining the memory-saving benefits
of reversibility. Experiments on CIFAR10, CIFAR100, CIFAR10-DVS, and DVS128
Gesture demonstrate that ParaRevSNN matches or exceeds the accuracy of standard
RevSNNs, while reducing training time by up to 35.2\% and inference time to
18.15\%, making it well-suited for deployment in resource-constrained
scenarios.

</details>


### [88] [Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models](https://arxiv.org/abs/2508.01225)
*Xinyu Chen,Haotian Zhai,Can Zhang,Xiupeng Shi,Ruirui Li*

Main category: cs.CV

> 研究提出多缓存增强原型测试时适应方法(MCP)，通过熵缓存、对齐缓存和负缓存提升原型质量，并开发MCP++框架，采用跨模态原型对齐和残差学习，实验表现出良好的泛化性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的缓存增强测试时适应方法依赖低熵标准来选择样本构建原型，但这种方法在分布变化下不可靠。本研究旨在通过更可靠的原型构建方法提高性能。

**Method:** 本研究提出了一种多缓存增强的原型测试时适应方法(MCP)，包括熵缓存、对齐缓存和负缓存三个部分，分别用于初始化原型表示、实现紧凑的类内分布和预测校准。进一步发展了MCP++框架，结合跨模态原型对齐和残差学习，引入原型残差微调。

**Result:** 在15个下游任务上的对比和消融实验表明，提出的方法和框架达到了最先进的泛化性能。

**Conclusion:** 本研究证明了缓存增强性能与类内紧凑性的正相关性，并且提出的MCP及MCP++框架显著提高了未知分布下的性能。

**Abstract:** In zero-shot setting, test-time adaptation adjusts pre-trained models using
unlabeled data from the test phase to enhance performance on unknown test
distributions. Existing cache-enhanced TTA methods rely on a low-entropy
criterion to select samples for prototype construction, assuming intra-class
compactness. However, low-entropy samples may be unreliable under distribution
shifts, and the resulting prototypes may not ensure compact intra-class
distributions. This study identifies a positive correlation between
cache-enhanced performance and intra-class compactness. Based on this
observation, we propose a Multi-Cache enhanced Prototype-based Test-Time
Adaptation (MCP) featuring three caches: an entropy cache for initializing
prototype representations with low-entropy samples, an align cache for
integrating visual and textual information to achieve compact intra-class
distributions, and a negative cache for prediction calibration using
high-entropy samples. We further developed MCP++, a framework incorporating
cross-modal prototype alignment and residual learning, introducing prototype
residual fine-tuning. Comparative and ablation experiments across 15 downstream
tasks demonstrate that the proposed method and framework achieve
state-of-the-art generalization performance.

</details>


### [89] [Enhancing Multi-view Open-set Learning via Ambiguity Uncertainty Calibration and View-wise Debiasing](https://arxiv.org/abs/2508.01227)
*Zihan Fang,Zhiyong Xu,Lan Du,Shide Du,Zhiling Cai,Shiping Wang*

Main category: cs.CV

> 本文提出了一种通过模糊不确定性校准和视角去偏差进行多视角开放集学习的新框架，在多个多视角基准测试中表现出色，提高了对未知类别的识别率。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多视角学习模型在开放集场景中表现不佳，这是由于它们隐含地假设了类别的完整性。此外，由训练过程中形成的视角-标签关联造成的静态视角偏差进一步降低了它们识别未知类别的能力。

**Method:** 我们提出了一个通过模糊不确定性校准和视角去偏差的多视角开放集学习框架。为了模拟模糊样本，我们设计了O-Mix，一种新的合成策略，用于生成具有校准开放集模糊不确定性的人工样本。这些样本进一步由辅助模糊感知网络处理，该网络捕捉非典型模式以改善开放集适应性。此外，我们还引入了一个基于HSIC的对比去偏差模块，该模块强制执行视角特定的模糊表示和视角一致表示之间的独立性，鼓励模型学习通用特征。

**Result:** 在不同的多视角基准测试上进行的广泛实验表明，该框架一致地增强了对未知类别的识别能力。

**Conclusion:** 广泛的实验结果表明，所提出的框架在增强对未知类别的识别能力的同时，保持了强大的闭集性能。

**Abstract:** Existing multi-view learning models struggle in open-set scenarios due to
their implicit assumption of class completeness. Moreover, static view-induced
biases, which arise from spurious view-label associations formed during
training, further degrade their ability to recognize unknown categories. In
this paper, we propose a multi-view open-set learning framework via ambiguity
uncertainty calibration and view-wise debiasing. To simulate ambiguous samples,
we design O-Mix, a novel synthesis strategy to generate virtual samples with
calibrated open-set ambiguity uncertainty. These samples are further processed
by an auxiliary ambiguity perception network that captures atypical patterns
for improved open-set adaptation. Furthermore, we incorporate an HSIC-based
contrastive debiasing module that enforces independence between view-specific
ambiguous and view-consistent representations, encouraging the model to learn
generalizable features. Extensive experiments on diverse multi-view benchmarks
demonstrate that the proposed framework consistently enhances unknown-class
recognition while preserving strong closed-set performance.

</details>


### [90] [Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models](https://arxiv.org/abs/2508.01236)
*Mingyu Fu,Wei Suo,Ji Ma,Lin Yuanbo Wu,Peng Wang,Yanning Zhang*

Main category: cs.CV

> 本论文提出了一种自适应内容补偿方法（ACCM），该方法可以通过一个轻量级的描述模型和一个选择器来减少视觉信息的损失，同时减少计算成本。实验表明，ACCM在使用较少的FLOPs的情况下显著优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型视觉语言模型（LVLMs）取得了巨大成功，但其高昂的计算成本严重限制了它们的广泛应用。现有的方法通过删除冗余的token，虽然在减少计算成本方面取得了一些进展，但由于视觉信息的丢失，这些方法在高剪枝率时性能显著下降。因此，我们提出了ACCM来解决这个问题。

**Method:** 我们的方法称为自适应内容补偿方法（ACCM），该方法通过图像描述有效地减轻了视觉信息丢失的问题。ACCM由两个关键部分组成：一个轻量级描述模型和一个选择器。首先，描述模型在用户指令的指导下生成与问题相关的描述，然后选择器进一步从多个候选中识别出上下文相关的描述。通过自监督学习，我们的模型可以在没有任何人工或自动标签的情况下被有效地学习。

**Result:** 我们在七个基准上进行了广泛的实验，结果表明ACCM的表现显著优于现有的方法，并且使用较少的FLOPs（例如，比SOTA高出20.6%，而FLOPs却少了6.5%）。

**Conclusion:** 实验表明，通过ACCM，我们成功地在减少计算成本的同时，保持了视觉信息的完整性，从而显著提升了性能。这种方法证明了在减少计算成本和保持性能之间找到一个好的平衡是可行的。

**Abstract:** Despite the great success of Large Vision Language Models (LVLMs), their high
computational cost severely limits their broad applications. The computational
cost of LVLMs mainly stems from the visual sequence of the input, which
consists of hundreds or even thousands of tokens. Although existing methods
have made progress by removing redundant tokens, they suffer from severe
performance degradation with high pruning rates due to the loss of visual
information. In this paper, we propose an Adaptive Content Compensation Method
(ACCM), which can effectively mitigate the visual information loss via an image
caption. Specifically, ACCM comprises two key components: a lightweight caption
model and a selector. Firstly the caption model generates question-related
descriptions under the guidance of the user instruction. Then the selector
further identifies a contextually appropriate caption from multiple candidates.
Leveraging self-supervised learning, our modules could be learned efficiently
without any human or automated labeling. We conduct extensive experiments
across seven benchmarks and the results show that ACCM significantly
outperforms existing methods with lower FLOPs (e.g., surpassing SOTA by 20.6%
with 6.5% fewer FLOPs).

</details>


### [91] [OCSplats: Observation Completeness Quantification and Label Noise Separation in 3DGS](https://arxiv.org/abs/2508.01239)
*Han Ling,Xian Xu,Yinghui Sun,Quansen Sun*

Main category: cs.CV

> A new framework, OCSplats, is introduced to enhance the performance of 3D Gaussian Splatting in noisy, real-world scenarios by improving label noise classification and reconstruction accuracy without needing parameter adjustments.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to tackle the issue of label noise in real-world 3DGS scenarios, which often leads to reconstruction errors. Existing methods either fail to separate noise effectively or require scene-specific fine-tuning.

**Method:** This paper introduces OCSplats, a novel framework that addresses label noise in 3D Gaussian Splatting (3DGS) reconstructions. It utilizes hybrid noise assessment and observation-based cognitive correction to improve the accuracy of noise classification in areas with cognitive differences.

**Result:** The experiments show that OCSplats achieves leading reconstruction performance and precise label noise classification across scenes of varying complexity without requiring any adjustments to parameters.

**Conclusion:** The paper concludes that OCSplats, through its innovative approach and label noise classification pipeline, can be widely applied in diverse scenarios without the necessity of scene-specific parameter tuning.

**Abstract:** 3D Gaussian Splatting (3DGS) has become one of the most promising 3D
reconstruction technologies. However, label noise in real-world scenarios-such
as moving objects, non-Lambertian surfaces, and shadows-often leads to
reconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods
either fail to separate noise effectively or require scene-specific fine-tuning
of hyperparameters, making them difficult to apply in practice. This paper
re-examines the problem of anti-noise reconstruction from the perspective of
epistemic uncertainty, proposing a novel framework, OCSplats. By combining key
technologies such as hybrid noise assessment and observation-based cognitive
correction, the accuracy of noise classification in areas with cognitive
differences has been significantly improved. Moreover, to address the issue of
varying noise proportions in different scenarios, we have designed a label
noise classification pipeline based on dynamic anchor points. This pipeline
enables OCSplats to be applied simultaneously to scenarios with vastly
different noise proportions without adjusting parameters. Extensive experiments
demonstrate that OCSplats always achieve leading reconstruction performance and
precise label noise classification in scenes of different complexity levels.

</details>


### [92] [NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2508.01248)
*Jiazhen Yan,Fan Wang,Weiwei Jiang,Ziqiang Li,Zhangjie Fu*

Main category: cs.CV

> The authors propose NS-Net, a detection framework that enhances the ability to distinguish AI-generated images by decoupling their semantic information, achieving a 7.4% improvement in detection accuracy over current methods.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitation of current detectors in accurately identifying AI-generated images, especially when the semantic content closely matches that of real images, by focusing on improving the differentiation ability through semantic and structural bias reduction.

**Method:** In this paper, the authors propose NS-Net, a detection framework which uses NULL-Space projection to separate semantic information from CLIP's visual features, followed by contrastive learning to distinguish between real and generated images. Additionally, they introduce a Patch Selection strategy to reduce the influence of global image structures in the semantic bias, while maintaining fine-grained artifacts.

**Result:** Experiments demonstrate NS-Net significantly outperforms existing methods on an open-world benchmark with a 7.4% increase in detection accuracy, showing strong generalization for both GAN- and diffusion-based image generation techniques.

**Conclusion:** NS-Net, through the integration of NULL-Space projection and contrastive learning, provides a more effective approach for detecting AI-generated images, leading to better performance and generalization over a wide range of generative models.

**Abstract:** The rapid progress of generative models, such as GANs and diffusion models,
has facilitated the creation of highly realistic images, raising growing
concerns over their misuse in security-sensitive domains. While existing
detectors perform well under known generative settings, they often fail to
generalize to unknown generative models, especially when semantic content
between real and fake images is closely aligned. In this paper, we revisit the
use of CLIP features for AI-generated image detection and uncover a critical
limitation: the high-level semantic information embedded in CLIP's visual
features hinders effective discrimination. To address this, we propose NS-Net,
a novel detection framework that leverages NULL-Space projection to decouple
semantic information from CLIP's visual features, followed by contrastive
learning to capture intrinsic distributional differences between real and
generated images. Furthermore, we design a Patch Selection strategy to preserve
fine-grained artifacts by mitigating semantic bias caused by global image
structures. Extensive experiments on an open-world benchmark comprising images
generated by 40 diverse generative models show that NS-Net outperforms existing
state-of-the-art methods, achieving a 7.4\% improvement in detection accuracy,
thereby demonstrating strong generalization across both GAN- and
diffusion-based image generation techniques.

</details>


### [93] [DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing](https://arxiv.org/abs/2508.01250)
*Xiaoqin Wang,Xianxu Hou,Meidan Ding,Junliang Chen,Kaijun Deng,Jinheng Xie,Linlin Shen*

Main category: cs.CV

> DisFaceRep, a novel framework for weakly supervised face parsing, separates co-occurring facial components using both explicit and implicit disentanglement mechanisms, exhibiting superior performance over existing weakly supervised methods.

<details>
  <summary>Details</summary>

**Motivation:** Current face parsing methods require dense pixel-level annotations, which are costly and labor-intensive to acquire. The paper aims to reduce this annotation burden by developing a technique that works with only weak supervision.

**Method:** The paper proposes DisFaceRep, a framework that uses both explicit and implicit mechanisms to separate co-occurring facial components in weak supervision. The explicit mechanism is a co-occurring component disentanglement strategy to mitigate dataset-level bias. The implicit mechanism is a text-guided component disentanglement loss, which guides component separation through language supervision.

**Result:** Experiments on CelebAMask-HQ, LaPa, and Helen datasets show that DisFaceRep effectively addresses the WSFP challenges, achieving better performance than existing weakly supervised semantic segmentation methods.

**Conclusion:** The proposed DisFaceRep framework demonstrates its capability in enhancing the accuracy of face parsing under weak supervision conditions, making it a promising approach for tasks involving large, unlabeled datasets.

**Abstract:** Face parsing aims to segment facial images into key components such as eyes,
lips, and eyebrows. While existing methods rely on dense pixel-level
annotations, such annotations are expensive and labor-intensive to obtain. To
reduce annotation cost, we introduce Weakly Supervised Face Parsing (WSFP), a
new task setting that performs dense facial component segmentation using only
weak supervision, such as image-level labels and natural language descriptions.
WSFP introduces unique challenges due to the high co-occurrence and visual
similarity of facial components, which lead to ambiguous activations and
degraded parsing performance. To address this, we propose DisFaceRep, a
representation disentanglement framework designed to separate co-occurring
facial components through both explicit and implicit mechanisms. Specifically,
we introduce a co-occurring component disentanglement strategy to explicitly
reduce dataset-level bias, and a text-guided component disentanglement loss to
guide component separation using language supervision implicitly. Extensive
experiments on CelebAMask-HQ, LaPa, and Helen demonstrate the difficulty of
WSFP and the effectiveness of DisFaceRep, which significantly outperforms
existing weakly supervised semantic segmentation methods. The code will be
released at
\href{https://github.com/CVI-SZU/DisFaceRep}{\textcolor{cyan}{https://github.com/CVI-SZU/DisFaceRep}}.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [94] [ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation](https://arxiv.org/abs/2508.01058)
*Sara Yavari,Rahul Nitin Pandya,Jacob Furst*

Main category: eess.IV

> 本文提出了一种无需使用真实标签的、基于概率模型的半监督方法，用于提高大脑肿瘤MRI图像的分割精度，特别是在BraTS 2021数据集上的效果比之前的方法有所提升。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是为了提高大脑肿瘤在MRI扫描中的准确分割，以支持临床诊断和治疗计划，同时降低对真实标签的需求。

**Method:** 该论文提出了一种两阶段的半监督框架，首先使用残差引导的去噪扩散概率模型（DDPM）执行跨模态合成，然后使用轻量级的U-Net进行肿瘤分割。

**Result:** 该方法在BraTS 2021数据集上实现了93.02%的dice分数和86.7%的IoU分数，优于之前的方法。

**Conclusion:** 研究结论表明，提出的方法在分割精度和多中心MRI数据集的可扩展性上均优于ReCoSeg方法。

**Abstract:** Accurate segmentation of brain tumors in MRI scans is critical for clinical
diagnosis and treatment planning. We propose a semi-supervised, two-stage
framework that extends the ReCoSeg approach to the larger and more
heterogeneous BraTS 2021 dataset, while eliminating the need for ground-truth
masks for the segmentation objective. In the first stage, a residual-guided
denoising diffusion probabilistic model (DDPM) performs cross-modal synthesis
by reconstructing the T1ce modality from FLAIR, T1, and T2 scans. The residual
maps, capturing differences between predicted and actual T1ce images, serve as
spatial priors to enhance downstream segmentation. In the second stage, a
lightweight U-Net takes as input the concatenation of residual maps, computed
as the difference between real T1ce and synthesized T1ce, with T1, T2, and
FLAIR modalities to improve whole tumor segmentation. To address the increased
scale and variability of BraTS 2021, we apply slice-level filtering to exclude
non-informative samples and optimize thresholding strategies to balance
precision and recall. Our method achieves a Dice score of $93.02\%$ and an IoU
of $86.7\%$ for whole tumor segmentation on the BraTS 2021 dataset,
outperforming the ReCoSeg baseline on BraTS 2020 (Dice: $91.7\%$, IoU:
$85.3\%$), and demonstrating improved accuracy and scalability for real-world,
multi-center MRI datasets.

</details>


### [95] [Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation](https://arxiv.org/abs/2508.01064)
*Fenghe Tang,Bingkun Nian,Jianrui Ding,Wenxin Ma,Quan Quan,Chengqi Dong,Jie Yang,Wei Liu,S. Kevin Zhou*

Main category: eess.IV

> 本文提出适用于移动设备的医学图像分割模型Mobile U-ViT，它在减少计算需求的同时，在八个公开数据集上达到了SOTA性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现有移动端模型在医学图像分析上的性能不佳和资源受限的问题，旨在开发出高效、通用且高性能的轻量化网络模型。

**Method:** 本文提出了一种名为Mobile U-shaped Vision Transformer (Mobile U-ViT) 的移动端模型，特别适用于医学图像分割任务。该方法包括使用ConvUtr进行层次化patch嵌入，引入Large-kernel Local-Global-Local (LGL) 块，浅层轻量级transformer瓶颈，以及带有下采样跳连的级联解码器。

**Result:** 尽管计算需求降低，该医学优化架构在八个公共2D和3D数据集上，涵盖多种成像方式，实现最佳性能，并包括零样本测试未见数据集。

**Conclusion:** Mobile U-ViT作为一种高效且强大的泛化解决方案，适用于移动设备上的医学图像分析。代码在https://github.com/FengheTan9/Mobile-U-ViT公开。

**Abstract:** In clinical practice, medical image analysis often requires efficient
execution on resource-constrained mobile devices. However, existing mobile
models-primarily optimized for natural images-tend to perform poorly on medical
tasks due to the significant information density gap between natural and
medical domains. Combining computational efficiency with medical
imaging-specific architectural advantages remains a challenge when developing
lightweight, universal, and high-performing networks. To address this, we
propose a mobile model called Mobile U-shaped Vision Transformer (Mobile U-ViT)
tailored for medical image segmentation. Specifically, we employ the newly
purposed ConvUtr as a hierarchical patch embedding, featuring a
parameter-efficient large-kernel CNN with inverted bottleneck fusion. This
design exhibits transformer-like representation learning capacity while being
lighter and faster. To enable efficient local-global information exchange, we
introduce a novel Large-kernel Local-Global-Local (LGL) block that effectively
balances the low information density and high-level semantic discrepancy of
medical images. Finally, we incorporate a shallow and lightweight transformer
bottleneck for long-range modeling and employ a cascaded decoder with
downsample skip connections for dense prediction. Despite its reduced
computational demands, our medical-optimized architecture achieves
state-of-the-art performance across eight public 2D and 3D datasets covering
diverse imaging modalities, including zero-shot testing on four unseen
datasets. These results establish it as an efficient yet powerful and
generalization solution for mobile medical image analysis. Code is available at
https://github.com/FengheTan9/Mobile-U-ViT.

</details>
