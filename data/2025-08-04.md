<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.CV](#cs.CV) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

> 本文评估前沿LLMs解决物理问题的能力，并引入一种新的物理问题评估基准PHYSICEVAL，其中包含19,609道来自各种教科书的问题及答案。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在评估前沿语言模型（LLMs）解决物理问题的能力，并探索如何通过代理框架和推理技术提升这些模型的表现。

**Method:** 本文采用多种推理时的技术和代理框架来提升前沿LLMs解决物理问题（包括数学和描述性问题）的能力。具体的改进措施包括通过其他较小的LLM代理对提出的解决方案进行累积的验证。

**Result:** 应用多代理框架后，在模型原本表现不佳的问题上有了显著改善。

**Conclusion:** 通过使用多代理框架和其他推理时间技术，模型在解决物理问题上的性能有了显著提高，尤其是原本处理不佳的问题类型。此外，公开了代码和数据，以便进一步的研究。

**Abstract:** The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [2] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
*Kelly Kendro,Jeffrey Maloney,Scott Jarvis*

Main category: cs.CL

> 研究发现语言生成模型（LLMs）生成的文本在词汇多样性上与人类书写存在显著差异，且较新的LLMs生成的文本比较旧的模型更不类似于人类的写作。

<details>
  <summary>Details</summary>

**Motivation:** 尽管这个问题收到了大量的实证关注，但LLMs所写出的文字是否真正如人类写出的一样，仍存在不确定性。此项研究旨在探讨这个问题。

**Method:** 本研究从词汇多样性视角探讨LLMs生成的文本是否真正与人类书写相似。具体来说，研究比较了四款ChatGPT模型（-3.5、-4、-o4迷你版和-4.5）生成的文本与240名英语母语者和非母语者在四个教育级别级别的文本，测量了每种文本的六个词汇多样性维度，包括量、丰度、多样性-重复、均匀性、差异和分散度。

**Result:** 研究结果显示，通过一元多变量方差分析（MANOVAs）、一元方差分析（ANOVAS）和支持向量机（SVMs），LLMs生成的文本与人类书写的文本在每个变量上均有显著差异，特别是在ChatGPT-o4迷你版和-4.5中差异最大。ChatGPT-4.5虽然产生的标记较少，但表现出更高的词汇多样性。人类作者的词汇多样性在不同子群体间没有显著差异。

**Conclusion:** 总体而言，结果表明LLMs生成的文本在词汇多样性方面并不像人类书写那样。事实上，更新的LLMs生成的文本比老模型更不具有人写文本的特点。我们讨论了这些结果对语言教学和相关应用的影响。

**Abstract:** The degree to which LLMs produce writing that is truly human-like remains
unclear despite the extensive empirical attention that this question has
received. The present study addresses this question from the perspective of
lexical diversity. Specifically, the study investigates patterns of lexical
diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,
and -4.5) in comparison with texts written by L1 and L2 English participants (n
= 240) across four education levels. Six dimensions of lexical diversity were
measured in each text: volume, abundance, variety-repetition, evenness,
disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and
Support Vector Machines revealed that the LLM-generated texts differed
significantly from human-written texts for each variable, with ChatGPT-o4 mini
and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated
higher levels of lexical diversity despite producing fewer tokens. The human
writers' lexical diversity did not differ across subgroups (i.e., education,
language status). Altogether, the results indicate that LLMs do not produce
human-like texts in relation to lexical diversity, and the newer LLMs produce
less human-like texts than older models. We discuss the implications of these
results for language pedagogy and related applications.

</details>


### [3] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

> 本文指出计算人文科学在知识论和解释清晰度方面需要更多的理论化，特别是强调了符号复杂性概念，指出将复杂数据看作简单数据是一种翻译错误，并提出了应对这些知识论问题的方法。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在为计算人文科学领域的知识论和解释的清晰度提供更多的理论化思考，以促进该领域的发展。作者认为计算人文学者在工作中需要像翻译工作者那样明确其过程的理论性，以确保内部一致性、避免微妙但重要的翻译错误以及提高解释透明度。

**Method:** 本文通过将计算人文科学中的建模工作比作一种翻译工作，来探讨计算人文科学中理论化的不足和由此产生的翻译错误。具体来说，作者提出了‘符号复杂性’的概念，即文本意义在不同解释视角下变化的程度，并指出在评价过程中将符号复杂性数据视为符号简单数据的做法是一种翻译错误。

**Result:** 本文指出了当前建模实践中忽略符号复杂性数据被视为符号简单数据的翻译错误，并认为这种做法是为了方便获取表面上的清晰性。

**Conclusion:** 最后，作者提出了一些关于如何在研究中更好地考虑这些知识论问题的建议。

**Abstract:** Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

</details>


### [4] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

> 本文介绍了FACTORY，一种通过人为验证扩大规模的提示集，旨在提高长篇事实评估的准确性。实验表明，FACTORY对于最新的语言模型来说是一个具有挑战性的基准。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基准测试往往缺乏人为验证，这可能导致质量问题。为了弥补这一不足，本文提出了一种新方法。

**Method:** 本文提出了一种大规模的人工验证提示集FACTORY，使用了模型循环方法并由人类精炼，包含了具有挑战性的提示，这些提示是事实性寻找的、可回答的和明确的。

**Result:** 通过使用FACTORY和现有数据集进行的人类评估表明，对于最先进的语言模型，大约40%的响应是在非事实的，而在其他数据集中只有10%。

**Conclusion:** 分析表明，FACTORY在之前的基准测试中具有优势，强调了其可靠性和模型在长尾事实推理中的必要性。

**Abstract:** Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [5] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
*Xiao Zhang,Johan bos*

Main category: cs.CL

> 研究发现，尽管效果较好，神经语义解析器在处理英语动词短语省略时表现不佳。

<details>
  <summary>Details</summary>

**Motivation:** 研究神经语义解析器在处理需大量语义信息复制语境下的表现，特别是英语动词短语省略现象。

**Method:** 通过构建包含120个省略实例及其完整语义表示的数据集，将其作为挑战数据集用于测试一系列神经语义解析器。

**Result:** 尽管这些解析器在标准测试集上表现非常好，但它们在包含省略的实例上失败了。

**Conclusion:** 研究表明，尽管神经语义解析器在多种语言现象上表现出良好的整体性能，但对于需要大量语义信息复制的强上下文敏感现象（例如英语动词短语省略），它们的表现不佳。

**Abstract:** Neural semantic parsers have shown good overall performance for a variety of
linguistic phenomena, reaching semantic matching scores of more than 90%. But
how do such parsers perform on strongly context-sensitive phenomena, where
large pieces of semantic information need to be duplicated to form a meaningful
semantic representation? A case in point is English verb phrase ellipsis, a
construct where entire verb phrases can be abbreviated by a single auxiliary
verb. Are the otherwise known as powerful semantic parsers able to deal with
ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with
their fully resolved meaning representation and used this as a challenge set
for a large battery of neural semantic parsers. Although these parsers
performed very well on the standard test set, they failed in the instances with
ellipsis. Data augmentation

</details>


### [6] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
*Alper Yaman,Jannik Schwab,Christof Nitsche,Abhirup Sinha,Marco Huber*

Main category: cs.CL

> 本文提供了一个不断更新的开源LLM比较列表，以帮助研究者和企业在选择合适的模型时，考虑许可证和硬件要求。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于过去两年中开源基础模型和微调模型的大量涌现，选择合适的LLM变得复杂。为了应对这一挑战，帮助研究者和企业在许可和硬件要求上更好地选择最优的LLM，提出了这篇论文。

**Method:** 通过构建一个比较列表来帮助研究人员和企业选择合适的语言模型，该列表涵盖了基础模型和领域特定模型，并重点关注了发布时间、许可、硬件要求等特征。

**Result:** 提供了一个发布在GitLab上，包含基础模型和领域特定模型比较的列表，该列表将定期更新。

**Conclusion:** 通过列出模型的关键特性和硬件需求，这项工作为研究人员和企业提供了选择和部署适合其需求的语言模型的清晰指南。

**Abstract:** Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

</details>


### [7] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

> 本文介绍了表格理解任务中的关键概念，并指出该领域存在的几个研究空白。

<details>
  <summary>Details</summary>

**Motivation:** 由于表格结构的多样性和复杂性，传统方法不足以应对这些挑战，因此本文提出了关键概念。

**Method:** 本文通过创建表格输入表示的分类法和介绍表格理解任务来解决这些挑战。

**Result:** 文章指出当前领域存在的几个关键差距，即任务集中在检索，对复杂表格处理困难，以及模型在不同表格表示和格式上泛化能力有限。

**Conclusion:** 研究结果表明，需要进一步的研究来解决表格理解中的挑战，尤其在复杂结构、大规模表格处理及模型泛化能力方面。

**Abstract:** Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [8] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
*Rana Aref Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

> 本文研究了离散小波变换（DWT）应用于词和句子嵌入的可行性，发现DWT能够高效压缩嵌入表示，同时保持或提升自然语言处理任务的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在展示DWT在分析嵌入表示和压缩表示以维持其整体质量方面的有效性。此外，评估DWT嵌入在语义相似性任务中的表现，以展现其在巩固嵌入向量中的重要语义信息方面的应用潜力。

**Method:** 本文采用了离散小波变换（DWT）应用于词和句子的嵌入表示，通过在不同分辨率级别分析嵌入表示，并在保持整体质量的同时进行压缩。

**Result:** 实验结果表明，DWT能够减少嵌入表示的维度50-93%，几乎不对语义相似性任务的表现产生变化，同时在大多数下游任务中达到更高的准确性。

**Conclusion:** 本文的发现为提升自然语言处理（NLP）应用中DWT的应用提供了可能性。

**Abstract:** Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

</details>


### [9] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

> 研究发现，2022年后，随着大型语言模型（如ChatGPT）的流行，人类在使用科学和技术播客时的词汇偏好开始与这些模型的输出相一致。

<details>
  <summary>Details</summary>

**Motivation:** 探讨语言生成中的词汇变化是否反映了人类语言系统的更广泛变化，特别是在大型语言模型（LLMs）的影响下。

**Method:** 构建了一个包含2210万个单词的数据集，这些单词来自未脚本化的科学和技术播客中的口语。分析了ChatGPT在2022年发布前后的词汇趋势，重点关注与大语言模型相关的词汇。

**Result:** 结果表明，在2022年后，与LLM相关的词汇使用显著增加，而基准同义词词汇没有显著的变化。

**Conclusion:** 这些发现表明，在短时间内有相当数量的词汇受到LLM的影响，可能是语言使用开始发生变化的迹象。这种变化可能是自然语言演化的结果，也可能是由于人工智能的使用导致的新变化。此外，这也可能反映了更广泛的采用模式，或训练中的上游错配最终导致了人类语言使用的变化。

**Abstract:** In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [10] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
*Peixian Li,Yu Tian,Ruiqi Tu,Chengkai Wu,Jingjing Ren,Jingsong Li*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

</details>


### [11] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

> 研究了剪枝、量化和丢弃标记等技术对大型语言模型在长上下文场景下的资源需求和性能的影响，并探讨了这些优化方法的组合和可扩展性问题。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在处理多样性自然语言任务时表现出色，但面临着资源需求高和上下文窗口有限的问题。目前有关这些技术在长上下文场景中的效果和整体系统评估分析不足。

**Method:** 分析不同的优化方法（如剪枝、量化和丢弃标记）对支持长上下文的两种大型语言模型架构的影响，并系统评估这些技术的组合效果。还研究了这些优化方法在拥有700亿参数的更大模型上的可扩展性。

**Result:** 研究表明，单纯依赖F1指标会隐藏问答任务中的精度-召回率权衡问题，并启示了一种将系统级分析与任务特定洞察相结合的方法。

**Conclusion:** 这项研究通过结合系统级别的性能剖析与特定任务的洞察力，帮助大型语言模型实务工作者和研究人员探索并平衡不同任务和硬件配置下的效率、准确性和可扩展性。同时，研究揭示了简单组合推理优化算法对更大模型可能带来负面影响，并指出这一点是与较小模型不同的。

**Abstract:** Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [12] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
*Kaiyan Zhao,Zhongtao Miao,Yoshimasa Tsuruoka*

Main category: cs.CL

> MCSEO方法通过结合使用对象检测和分割技术，改进了传统图像-标题对齐的不足，提升了多模态句子嵌入性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在解决图像-标题对中存在的噪声问题，如冗余或无关信息，以提高多模态句子嵌入的质量。

**Method:** MCSEO使用现有分割和对象检测模型提取精确的对象-短语对，利用对象-短语对应关系优化对比学习目标，从而增强了多模态句子嵌入。

**Result:** 实验结果显示，MCSEO在语义文本相似性任务上超越了强大的基线方法，证明了精确对象-短语对齐的重要性。

**Conclusion:** MCSEO通过引入细粒度的对象-短语对齐改善了多模态句子嵌入，展示了在多模态表示学习中精确对齐是关键因素。

**Abstract:** Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

</details>


### [13] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
*Keer Lu,Chong Chen,Bin Cui,Huang Leng,Wentao Zhang*

Main category: cs.CL

> 本文由于大语言模型(大语言模型)在一部分任务中的关键动态参与生成测试。输入：现有的任务特点参数要求参数。导向：AdaPlan及实验结果：PilotRL有效距离上GPT-4o-mini。

<details>
  <summary>Details</summary>

**Motivation:** 由于生成任务的大语言模型变概率无法较大级测试特点，在大语言模型输入不大级应用的任务时，竟然不能发现随后改进模型存在较大的变概率随后一定级测试特点(一步操作)。特点：本文的主要目，有三点：(1)提出一个第一级特定化值设置；(2)提出一个弹性的设计理念；(3)可注直通使用简单第一级存在模型

**Method:** Structure

**Result:** { "tldr": "\u7b80\u89e3\uff1a\u672c\u6587\u7531\u4e8e\u5927\u8bed\u6587\u6a21\u578b(Large\u5927\u8bed\u6587\u6a21\u578b)\u5728\u4e00\u90e8\u5206\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u52a8\u6001\u4e2d\u53c2\u4e0e\u751f\u6210\u6d4b\u8bd5\u3002\u8f93\u5165\uff1a\u5b58\u5728\u7684\u4efb\u52a1\u7279\u70b9\u53c2\u6570\u9700\u8981\u53c2\u6570\u3002\u5f00\u5c04\uff1aAdaPlan\u53ca\u5b9e\u9a8c\u7ed3\u679c\uff1aPilotRL\u6709\u6548\u8ddd\u79bb\u4e0aGPT-4o-mini\u3002", "motivation": "\u6027\u7279\uff1a\u7531\u4e8e\u751f\u6210\u4efb\u52a1\u7684\u5927\u8bed\u6587\u6a21\u578b\u53d8\u76df\u4e0d\u80fd\u8f83\u5927\u7ea7\u6d4b\u8bd5\u7279\u70b9\uff0c\u5728\u5927\u8bed\u6587\u6a21\u578b\u8f93\u5165\u4e0d\u5927\u7ea7\u5e94\u7528\u7684\u4efb\u52a1\u65f6\uff0c\u751a\u81f3\u4e0d\u80fd\u53d1\u73b0\u968f\u6765\u4fee\u6539\u6a21\u578b\u5b58\u5728\u7684\u8f83\u5927\u53d8\u76df\u4e0d\u80fd\u53d1\u73b0\u968f\u6765\u4e00\u5b9a\u7ea7\u6d4b\u8bd5\u7279\u70b9\uff08\u4e00\u6b65\u64cd\u4f5c\uff09\u3002\u7279\u70b9\uff1a\u672c\u6587\u76ee\u7684\u4e3b\u8981\u76ee\uff0c\u6709\u4e09\u70b9\uff1a(1)\u63a8\u51fa\u4e00\u4e2a\u7b2c\u4e00\u7ea7\u6a21\u7279\u5b9a\u5316\u503c\u8bbe\u7f6e\uff1b(2)\u63a8\u51fa\u4e00\u4e2a\u5f39\u548c\u7684\u8bbe\u8ba1\u601d\u60f3\uff1b(3)\u53ef\u6ce8\u76f4\u901a\u4f7f\u7528\u7b80\u5355\u7b2c\u4e00\u7ea7\u5b58\u5728\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u76f8\u5173\u4e0a\u5c04\u6a21\u578bAdaPlan\uff08\u4e25\u683c\u7684\u53d1\u73b0\uff09\uff0c\u7531\u6b64\u53d1\u73b0\u4e86\u4e00\u4e2a\u9ad8\u7ea7\u64cd\u4f5c\u7684\u4e0a\u5c04\u9879\u76ee\u5728\u4e00\u5b9a\u6a21\u578b\u5e94\u7528\u5e02\u573a\uff08\u7531\u5145\u58eb\u8868\u683c\uff09\uff0c\u5e76\u4e14\u5728ReAct\u9879\u6240\u6709\u5e02\u573a\u4e0b\u901a\u8fc7\u4ed8\u5165\u4e00\u4e2a\u65f6\u5c1a\u8868\u793a\u3002\u6b64\u65b9\u6cd5\u5728PilotRL\u9879\u800c\u88ab\u63d0\u51fa\uff0c\u5e0c\u814a\u5b9e\u9a8c\u6709\u6548\u6027\u8ddd\u79bb\u4e0a\u4e86\u6bcf\u4e2a\u6a21\u578b\uff0c\u4e0d\u8fc7\u5982\u540c\u6837\u5927\u7684\u6587\u6a21\u5b58\u5728\u590d\u6742\u72b6\u60c5\u800c\u5bfc\u81f4\u6bcf\u4e00\u6b65\u7684\u6700\u7ec8\u6a21\u5b58\u5b58\u5728\u60c5\u51b5\u4e0b\u901a\u8fc7\u5b9e\u9a8c\u5165\u53e3\u8868\u793a\u7684\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u65b9\u6cd5\uff1aPilotRL\u8868\u793a\u4e86\u9ad8\u7ea7\u52a8\u6001\u6d4b\u8bd5\uff1b\u4e00\u90e8\u5206\u8f83\u7f51\u7edc\u65b9\u6cd5\u6d4b\u8bd5\uff1b\u4e00\u5b9a\u6a21\u578b\u8868\u793a\u6027\u8ddd\u79bb\u5927\u7ea7\u6a21\u5173\uff1b\u9884\u6d4b\u5316\u5927\u7ea7\u6a21\u6d4b\u8bd5\u7cfb\u7edf\u7684\u5b9e\u9a8c\uff0c\u53e6\u5916\u8868\u793a\u4e86\u8868\u793a\u5e02\u573a\u4e0b\u7684\u5f02\u6d4b\u76ee\u7684\u5b9e\u9a8c\u7b49\u4e8b\u9879\u5b9e\u9a8c\u800c\u5728\u4e00\u90e8\u6a21\u578b\u8f6c\u58eb\u5185\u90e8\u6a21\u6d4b\u3002", "conclusion": "\u7ed3\u8bba\uff1a\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86PilotRL\u5728\u591a\u65b9\u9762\u4e0a\u4e0e\u5c06\u5f3a\u5927\u6d4b\u8bd5\u9879\u800c\u8d8a\u51fa\u73b0\u5f3a\u5927\u8f6c\u58eb\u5185\u7684\u6a21\u578b\u5e94\u7528\u5728\u5927\u8bed\u6587\u6a21\u578b\u4e0a\u6709\u6548\u7684\u6a21\u578b\u5e94\u7528\u529b\u3002\u53e6\u5916\u8bb2\u660e\u4e86\u4e00\u4e9b\u5bcc\u575a\u6d4b\u8bd5\u3001\u4e00\u90e8\u5206\u5bcc\u575a\u6d4b\u8bd5\u7b49\u8d8a\u51fa\u73b0\u7c7b\u578b\u5927\u7ea7\u6a21\u6d4b\u8bd5\u3002"}

**Conclusion:** 实验表明PilotRL在多方面上与将强大型测试项而越突出强势转轮内的模型应用在大语言模型上有有效的模型应用力。另外说明了一些广阔测试、一部分广阔测试等越出现类型大型测试

**Abstract:** Large Language Models (LLMs) have shown remarkable advancements in tackling
agent-oriented tasks. Despite their potential, existing work faces challenges
when deploying LLMs in agent-based environments. The widely adopted agent
paradigm ReAct centers on integrating single-step reasoning with immediate
action execution, which limits its effectiveness in complex tasks requiring
long-term strategic planning. Furthermore, the coordination between the planner
and executor during problem-solving is also a critical factor to consider in
agent design. Additionally, current approaches predominantly rely on supervised
fine-tuning, which often leads models to memorize established task completion
trajectories, thereby restricting their generalization ability when confronted
with novel problem contexts. To address these challenges, we introduce an
adaptive global plan-based agent paradigm AdaPlan, aiming to synergize
high-level explicit guidance with execution to support effective long-horizon
decision-making. Based on the proposed paradigm, we further put forward
PilotRL, a global planning-guided training framework for LLM agents driven by
progressive reinforcement learning. We first develop the model's ability to
follow explicit guidance from global plans when addressing agent tasks.
Subsequently, based on this foundation, we focus on optimizing the quality of
generated plans. Finally, we conduct joint optimization of the model's planning
and execution coordination. Experiments indicate that PilotRL could achieve
state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing
closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%
comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [14] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
*Alan Dao,Dinh Bach Vu,Alex Nguyen,Norapat Buppodom*

Main category: cs.CL

> 本文提出了一种新的范式，即视模型内部思考为动态任务向量机，通过这种机制，小模型能够实时构建和优化任务向量，从而提升性能。本研究训练的Lucy模型，在SimpleQA基准上的表现与许多大型模型相当。

<details>
  <summary>Details</summary>

**Motivation:** 小语言模型（SLMs）在知识密集型任务中因容量受限而表现不足。尽管测试时计算提供了性能增强的途径，但大部分方法都将推理视为一个固定或基于启发式的过程。该研究动机在于通过动态任务向量机制改进小语言模型的性能，使其能够与大型模型竞争。

**Method:** 本研究提出了一种新的范式，即视模型内部思考为动态任务向量机。这种方法不同于将括在<think>和</think>标签之间的内容视为单纯的思想痕迹，而是将其生成过程视为一种机制，通过这种机制模型能够实时构建和优化自己的任务向量。研究还提到开发了一种使用RLVR优化这种动态任务向量机的方法，以及成功训练了一个具有代理功能的网络搜索模型。

**Result:** 本研究开发了一种方法，并成功训练了一个名为Lucy的具有17亿参数的小语言模型，该模型结合了这种动态推理机制和MCP集成，在SimpleQA基准上实现了78.3%的准确率，与许多大型模型表现相当。

**Conclusion:** 研究表明，当小语言模型装备有结构化的、自构建任务推理时，它们能够与较大的模型相匹敌。这表明通过合理设计和优化内部推理过程，小模型也能表现出良好的性能。

**Abstract:** Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

</details>


### [15] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

> 论文提出了EdgeInfinite-Instruct，它通过分段监督微调和优化技术（如PTQ和固定形状计算图）解决了在边缘设备上部署长序列任务的效率和性能问题。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决现有的大型语言模型在资源受限的边缘设备上的部署难题，特别是长序列任务中的计算和内存消耗问题，以及现有解决方案在减少首词生成时间（TTFT）和保持性能方面的不足。

**Method:** 该论文提出了一种名为EdgeInfinite-Instruct的方法，它采用了分段监督微调(S-SFT)策略来解决长序列任务（如摘要和问答）的挑战，并通过细粒度的后训练量化(PTQ)和固定形状的计算图来优化在边缘NPU上的部署，以降低计算需求并保持准确率。

**Result:** 实验表明，该方法在长期上下文基准和实际移动任务上提高了特定领域的性能，同时在NPU加速的边缘设备上保持了高效。

**Conclusion:** EdgeInfinite-Instruct通过其创新策略和技术改进，在边缘设备上针对特定任务维持了性能和效率，克服了先前解决方案的局限性。

**Abstract:** Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [16] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

> 本文研究了示例无效的原因，提出了一种新的基于梯度流的示例选择方法GradS，并验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的研究主要假设了上下文学习（ICL）中示例的有效性，但是并非所有示例都是有效的。本文旨在探究示例无效的原因，并提出一种新的示例选择方法。

**Method:** 基于梯度流动和线性自我注意力模型进行分析，并提出了一种新的方法GradS，使用示例对于给定用户查询的梯度流的大小作为选取标准。

**Result:** 实验证实了有效性差异随着模型层数增加而放大，并验证了GradS在四种LLMs和五个主流数据集上的有效性。

**Conclusion:** 结果表明，提出的GradS相比于最强的基线方法平均提高了6.8%，这证明了该方法的有效性。

**Abstract:** Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [17] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
*Hengxing Cai,Jinhan Dong,Yijie Rao,Jingcheng Deng,Jingjun Tan,Qien Chen,Haidong Wang,Zhen Wang,Shiyu Huang,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

> 本文针对UAV VLN任务，提出了SA-GCS方法，以实现更有效的训练、加速收敛并提升模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机是解决现有基于强化学习的方法在训练数据使用效率低、收敛慢以及训练样本难度差异考虑不足等问题，从而提高UAV VLN任务的性能改善。

**Method:** 本文提出了一种名为语义感知高斯课程调度（Semantic-Aware Gaussian Curriculum Scheduling，SA-GCS）的新训练框架，该框架系统地将课程学习（CL）整合到强化学习（RL）中。SA-GCS使用语义感知难度估计器（SA-DE）量化训练样本的复杂性，并使用高斯课程调度器（GCS）动态调整采样分布，实现从简单任务到复杂任务的平稳过渡。

**Result:** 大规模实验表明，SA-GCS在CityNav基准上一致优于强有力的基线，实现了更快更稳定的收敛，且在不同规模的模型上具有良好的泛化能力。

**Conclusion:** 该方法能够显著提高训练效率和模型性能，展现了其鲁棒性和可扩展性。其实现代码已公开。

**Abstract:** Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable
agents to accurately localize targets and plan flight paths in complex
environments based on natural language instructions, with broad applications in
intelligent inspection, disaster rescue, and urban monitoring. Recent progress
in Vision-Language Models (VLMs) has provided strong semantic understanding for
this task, while reinforcement learning (RL) has emerged as a promising
post-training strategy to further improve generalization. However, existing RL
methods often suffer from inefficient use of training data, slow convergence,
and insufficient consideration of the difficulty variation among training
samples, which limits further performance improvement. To address these
challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling
(SA-GCS)}, a novel training framework that systematically integrates Curriculum
Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator
(SA-DE) to quantify the complexity of training samples and a Gaussian
Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution,
enabling a smooth progression from easy to challenging tasks. This design
significantly improves training efficiency, accelerates convergence, and
enhances overall model performance. Extensive experiments on the CityNav
benchmark demonstrate that SA-GCS consistently outperforms strong baselines
across all metrics, achieves faster and more stable convergence, and
generalizes well across models of different scales, highlighting its robustness
and scalability. The implementation of our approach is publicly available.

</details>


### [18] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
*Rana Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

> 研究提出并验证了一种基于小波变换和余弦变换的非参数模型，用于压缩NLP中的词和句子嵌入，实验证明该模型可以在减少嵌入维数的同时保持或提高任务效果。

<details>
  <summary>Details</summary>

**Motivation:** 由于小波变换在图像和信号处理方面取得了切实成果，研究旨在探索小波变换在自然语言处理中的应用潜力，特别是如何减少嵌入维数并保持信息。

**Method:** 该研究采用离散小波变换(DWT)应用于词和句子嵌入，同时结合离散余弦变换(DCT)，提出了一种无参数模型，用于压缩包含局部变化词汇特征的句子到一个固定大小的向量中，该模型可以有效地整合并减少词向量的维度，同时保留重要信息。

**Result:** 实验结果展示了该模型在下游任务上的有效性，且在某些任务中，其效果甚至优于原始嵌入。

**Conclusion:** 研究表明，通过结合DWT和DCT对词向量进行信息整合和维数压缩的方法是有效的，并且在一些NLP任务中可以取得与原始嵌入相当甚至更优的结果。

**Abstract:** Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

</details>


### [19] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

> 本文提出了Retrieval-augmented Graph Agentic Network (ReaGAN)，这是一个基于代理的框架，旨在解决图神经网络中节点信息不平衡和全局语义关系忽略的问题。通过节点级的自动决策和检索增强生成（RAG），ReaGAN在少样本情况下无需微调即可实现优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 标准的图神经网络在处理节点信息不平衡及忽略全局语义关系的问题上表现不佳，这限制了模型捕捉远程但相关数据的能力。为了改进这些问题，作者提出了一种基于代理的方法来增强节点的信息处理能力。

**Method:** 作者提出了ReaGAN架构，该架构通过赋予每个节点代理角色来实现自主决策和信息传播。每个节点能够根据其内部存储的信息决定其下一步行动，并辅以检索增强生成（RAG）技术以整合全局信息。

**Result:** {

**Conclusion:** 研究通过实验证明，即使使用冻结的预训练语言模型（LLM）而不进行微调，ReaGAN也能在少量示例的场景中取得良好效果，展示了代理规划和局部-全局检索在图学习中的应用潜力。

**Abstract:** Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [20] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
*Yuqi Tang,Kehua Feng,Yunfeng Wang,Zhiwen Chen,Chengfei Lv,Gang Yu,Qiang Zhang,Keyan Ding*

Main category: cs.CL

> The paper introduces an efficient multi-turn dialogue evaluator that consolidates multiple LLMs' preferences into a single model, offering cost-effective and reliable dialogue quality assessment.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the biases inherent in current evaluation methods by leveraging multiple LLMs' judgments efficiently, thus providing a more reliable and consistent evaluation at a lower computational cost.

**Method:** The paper proposes a method that aggregates the preference knowledge of multiple LLMs into a single model to evaluate dialogue quality, aiming to retain diverse feedback while reducing computational cost.

**Result:** Experiments on seven benchmarks showed that the proposed method outperformed existing baselines in both single rating and pairwise comparison dialogue evaluation, indicating its robustness and efficiency.

**Conclusion:** In conclusion, the proposed method enhances the evaluation of dialogue quality by efficiently aggregating the wisdom of multiple LLMs into a single model, leading to more reliable assessments with reduced computational overhead.

**Abstract:** Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

</details>


### [21] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
*Jeongwoo Kang,Markarit Vartampetian,Felix Herron,Yongxin Zhou,Diandra Fabre,Gabriela Gonzalez-Saez*

Main category: cs.CL

> The paper describes GETALP's participation in a shared task, using a combination of RAG and AMR to improve question-answering on meeting transcripts.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of this paper is to participate in Task B of the Third Run of the Automatic Minuting Shared Task at SIGDial 2025, which is question-answering based on meeting transcripts.

**Method:** Our method combines retrieval augmented generation (RAG) system with Abstract Meaning Representations (AMR) to propose three systems that aim to improve question-answering based on meeting transcripts.

**Result:** The results indicate that the incorporation of AMR leads to high-quality responses for about 35% of the questions and improves the ability to distinguish between different participants, particularly for 'who' type questions.

**Conclusion:** The conclusion is that combining RAG with AMR enhances the system's performance in answering questions from meeting transcripts, especially those involving participant distinction.

**Abstract:** This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

</details>


### [22] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
*Yixuan Tang,Jincheng Wang,Anthony K. H. Tung*

Main category: cs.CL

> 论文提出半真陈述检测任务，包含新的基准PolitiFact-Hidden和TRACER框架，用于识别基于省略的误导性陈述，提高了事实验证系统的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有事实验证系统通常假设一个声明的真实性完全依赖于其所述的事实内容。然而，很多现实世界中的声明是半真半假的，虽然事实正确但可能误导，因为忽略了关键背景。这些模型并不擅长处理疏忽关键背景的情况，所以需要开发新的方法。

**Method:** TRACER是一个模块化再评估框架，它结合证据对齐、推断隐含意图和估计隐藏内容的因果影响来检测基于省略的不真实信息。

**Result:** 论文引入了半真半假检测任务，并提出了一个包含15000个政治声明的新基准PolitiFact-Hidden。这些声明都标注了句子级别的证据对齐和推断声明意图。为应对这一挑战，论文提出了TRACER，一个模块化再评估框架，它通过证据对齐、推断隐含意图以及估计隐藏内容的因果影响来识别以省略为基础的错误信息。TRACER可以嵌入到现有的事实核查管道中，并且在多个强基线上一致提高了性能，特别是在半真分类F1分数上提高了多达16个点。

**Conclusion:** TRACER显著提升了半真半假声明检测的性能，表明为可信的事实验证建模遗漏内容的重要性。

**Abstract:** Fact verification systems typically assess whether a claim is supported by
retrieved evidence, assuming that truthfulness depends solely on what is
stated. However, many real-world claims are half-truths, factually correct yet
misleading due to the omission of critical context. Existing models struggle
with such cases, as they are not designed to reason about what is left unsaid.
We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a
new benchmark with 15k political claims annotated with sentence-level evidence
alignment and inferred claim intent. To address this challenge, we present
TRACER, a modular re-assessment framework that identifies omission-based
misinformation by aligning evidence, inferring implied intent, and estimating
the causal impact of hidden content. TRACER can be integrated into existing
fact-checking pipelines and consistently improves performance across multiple
strong baselines. Notably, it boosts Half-True classification F1 by up to 16
points, highlighting the importance of modeling omissions for trustworthy fact
verification.

</details>


### [23] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
*Jiaxin Deng,Qingcheng Zhu,Junbiao Pang,Linlin Yang,Zhongqian Fu,Baochang Zhang*

Main category: cs.CL

> This paper introduces Flat-LoRA and EFlat-LoRA methods to find flat minima for LoRA, improving generalization. EFlat-LoRA shows significant performance improvements over LoRA and full fine-tuning.

<details>
  <summary>Details</summary>

**Motivation:** The correlation between expressive ability and generalization ability of LoRA has not been fully explored. This work aims to find flat minima for LoRA to improve its generalization.

**Method:** Our method, Flat-LoRA, and its efficient version EFlat-LoRA, aim to seek flat minima for low-rank adaptation (LoRA). Theoretical demonstration and empirical verification are provided.

**Result:** EFlat-LoRA achieves comparable or better performance than LoRA and full fine-tuning, with improvements of 1.0% and 0.5% on the GLUE dataset and 1.5% and 1.0% on SQA and VizWiz datasets for vision-language models.

**Conclusion:** The method proposed, EFlat-LoRA, successfully finds flat minima and achieves better or comparable performance with LoRA and full fine-tuning while maintaining computational efficiency.

**Abstract:** Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [24] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
*Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow*

Main category: cs.CL

> 研究表明，说话者依据表情符号调整韵律，而听者能通过韵律变化识别表情符号的含义。表情符号在线上交流中的语用功能得到了验证。

<details>
  <summary>Details</summary>

**Motivation:** 研究表情符号如何影响语音中的韵律实现以及听者如何通过韵律线索恢复表情符号的意义，填补了相关研究的空白。

**Method:** 通过结构化但开放式生产和感知任务收集的实际人类语音数据，直接分析了表情符号与韵律之间的联系。

**Result:** 结果表明说话者依据表情符号调整他们的韵律，听者经常能仅凭韵律变化识别出预期的表情符号，表情符号之间的语义差异越大，其韵律表现越不同。

**Conclusion:** 表情符号作为韵律意图的有意义载体作用明显，为研究其在线交流中的沟通角色提供了新的见解。

**Abstract:** Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [25] [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/abs/2508.00053)
*Jie Zhu,Yiyang Su,Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

> 本文介绍了一种新的框架QME，它可以通过学习的方式改进全身生物识别的分数融合策略，解决了传统方法中的多种问题，实验结果表明使用QME能取得更好的效果。

<details>
  <summary>Details</summary>

**Motivation:** 传统的全身生物识别系统使用独立模型处理多种模态，通过分数融合达到最终结果，但这种方法忽略了各模态分数分布的变化，提高最终性能较为困难。

**Method:** 提出了一种名为QME（Quality-guided Mixture of Experts）的新框架，用于通过可学习的分数融合策略改进全身生物识别性能。该方法引入了一种新的伪质量损失来进行质量评估，并使用分数三元组损失来提高度量性能。

**Result:** 在多个全身生物识别数据集上的广泛实验表明，所提出的方法能够达到最先进的结果，优于基线方法。该方法有效解决了多模态和多模型中的模型对齐问题及数据质量变化问题。

**Conclusion:** 实验结果证明，所提出的方法有效解决了挑战，如相似度分数领域中的模型不对齐和数据质量变化问题。

**Abstract:** Whole-body biometric recognition is a challenging multimodal task that
integrates various biometric modalities, including face, gait, and body. This
integration is essential for overcoming the limitations of unimodal systems.
Traditionally, whole-body recognition involves deploying different models to
process multiple modalities, achieving the final outcome by score-fusion (e.g.,
weighted averaging of similarity matrices from each model). However, these
conventional methods may overlook the variations in score distributions of
individual modalities, making it challenging to improve final performance. In
this work, we present \textbf{Q}uality-guided \textbf{M}ixture of score-fusion
\textbf{E}xperts (QME), a novel framework designed for improving whole-body
biometric recognition performance through a learnable score-fusion strategy
using a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for
quality estimation with a modality-specific Quality Estimator (QE), and a score
triplet loss to improve the metric performance. Extensive experiments on
multiple whole-body biometric datasets demonstrate the effectiveness of our
proposed approach, achieving state-of-the-art results across various metrics
compared to baseline methods. Our method is effective for multimodal and
multi-model, addressing key challenges such as model misalignment in the
similarity score domain and variability in data quality.

</details>


### [26] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

> 研究评估了13个先进动作识别模型在不同数据集下的表现，发现这些模型在面新上下文时性能明显下降，并考察了三维物体运动数据集Syn-TA和两个自然视频数据集（Kinetics400-TA，Something-Something-v2-TA）在动作识别任务中的作用及模型在这些数据集上的表现，突出了空间线索和时间推理对模型性能的影响。

<details>
  <summary>Details</summary>

**Motivation:** 为了探究现有模型是否能在多样化和相似分布的情况下有效地转移高层次运动概念。

**Method:** 研究引入了一个动作转移性框架，包含三个数据集：Syn-TA（合成数据集，包含3D物体运动）、Kinetics400-TA和Something-Something-v2-TA（后者两个是从自然视频数据集修改而来）。

**Result:** 观察到模型在识别新型上下文中的高层次动作时性能显著下降。

**Conclusion:** 该研究确立了一个关键基准，用于评估动作识别中的运动转移性。

**Abstract:** Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [27] [The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking](https://arxiv.org/abs/2508.00088)
*Mateo de Mayo,Daniel Cremers,Taihú Pire*

Main category: cs.CV

> 本文提出了Monado SLAM数据集，该数据集包含多个虚拟现实头戴设备的真实序列，以解决现有VIO/SLAM系统在复杂场景中表现不佳的问题。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机在于现有的VIO和SLAM系统无法很好地处理头戴设备应用场景中的许多挑战性设置，例如高强度运动、动态遮挡、长时间跟踪会话、低纹理区域、不利的光照条件以及传感器饱和等情况。因此，作者创建了Monado SLAM数据集，以解决现有数据集未能充分涵盖这些问题的现状。

**Method:** 分析论文摘要的方法部分，本文没有明确说明具体的方法，而是介绍了数据集的创建目的和重要性。

**Result:** 本文主要成果是创建并发布了一个新的SLAM数据集，该数据集来自多个虚拟现实头戴设备的真实序列，并且以CC BY 4.0许可条款发布，以推动VIO/SLAM的研究和发展。

**Conclusion:** 作者通过创建Monado SLAM数据集，希望能够加大对VIO/SLAM系统在复杂头戴设备应用场景中挑战性问题的关注，并推动相关领域的研究和开发。

**Abstract:** Humanoid robots and mixed reality headsets benefit from the use of
head-mounted sensors for tracking. While advancements in visual-inertial
odometry (VIO) and simultaneous localization and mapping (SLAM) have produced
new and high-quality state-of-the-art tracking systems, we show that these are
still unable to gracefully handle many of the challenging settings presented in
the head-mounted use cases. Common scenarios like high-intensity motions,
dynamic occlusions, long tracking sessions, low-textured areas, adverse
lighting conditions, saturation of sensors, to name a few, continue to be
covered poorly by existing datasets in the literature. In this way, systems may
inadvertently overlook these essential real-world issues. To address this, we
present the Monado SLAM dataset, a set of real sequences taken from multiple
virtual reality headsets. We release the dataset under a permissive CC BY 4.0
license, to drive advancements in VIO/SLAM research and development.

</details>


### [28] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

> 本文提出了一种基于颜色图像的卷积神经网络模型，用于眼周区域的性别分类，实验结果表明该模型在CVBL和(Female and Male)数据集上分别达到了99%和96%的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 性别分类在安全、人机交互、监控和广告等领域具有重要意义，但化妆和伪装可能影响分类的准确性。研究集中于利用眼周区域的颜色图像进行性别分类，以改善这种准确性。

**Method:** 研究中提出了一种使用颜色眼周图像数据库评估性别分类效果的卷积神经网络(CNN)模型。

**Result:** 模型在CVBL数据集上达到了99%的准确率，且在(Female and Male)数据集上使用少量的可学习参数(7,235,089)达到了96%的准确率。

**Conclusion:** 通过广泛的评估指标验证，提出模型的性别分类效果优于其他先进的方法，为实际应用提供了潜在可能。

**Abstract:** Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [29] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

> 本文提出World Consistency Score (WCS)，一个新颖的视频评估指标，整合四个子组件以评估视频生成模型的连贯性，并通过实验验证其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决现有视频评估指标在仅关注视觉保真度或提示对齐方面的局限性，提出WCS来全面评估视频生成模型在一段时间内维持连贯“世界”的能力。

**Method:** 提出了World Consistency Score (WCS)，一个新颖的统一评估指标，用于评估生成视频的内在世界一致性。WCS整合了四个可解释的子组件——对象持久性、关系稳定性、因果符应性和闪烁处罚，每个子组件测量视频在时间和物理连贯性方面的不同方面。

**Result:** 通过使用VBench-2.0、EvalCrafter和LOVE等基准测试，以及敏感性分析，将WCS与现有指标（如FVD、CLIPScore、VBench、FVMD）进行比较和验证。

**Conclusion:** WCS为评价视频生成模型提供了全面且可解释的框架，对于提高生成视频的连贯性和可理解性具有重要意义。

**Abstract:** We introduce World Consistency Score (WCS), a novel unified evaluation metric
for generative video models that emphasizes internal world consistency of the
generated videos. WCS integrates four interpretable sub-components - object
permanence, relation stability, causal compliance, and flicker penalty - each
measuring a distinct aspect of temporal and physical coherence in a video.
These submetrics are combined via a learned weighted formula to produce a
single consistency score that aligns with human judgments. We detail the
motivation for WCS in the context of existing video evaluation metrics,
formalize each submetric and how it is computed with open-source tools
(trackers, action recognizers, CLIP embeddings, optical flow), and describe how
the weights of the WCS combination are trained using human preference data. We
also outline an experimental validation blueprint: using benchmarks like
VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human
evaluations, performing sensitivity analyses, and comparing WCS against
established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a
comprehensive and interpretable framework for evaluating video generation
models on their ability to maintain a coherent "world" over time, addressing
gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [30] [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/abs/2508.00152)
*Li Mi,Manon Bechaz,Zeming Chen,Antoine Bosselut,Devis Tuia*

Main category: cs.CV

> 本文提出GeoExplorer，通过好奇心驱动的探索策略增强了AGL的鲁棒性和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前的AGL方法依赖于基于距离的奖励来最小化相对距离，但是这种方法在遇到难以估计距离或未见过的目标和环境时，表现不佳，探索策略也不够可靠。

**Method:** GeoExplorer, 一个结合了基于好奇心的内在奖励机制的主动地理定位（AGL）智能体，通过有效建模环境来实现鲁棒、多样化且与上下文相关的探索，以此区别于基于距离的奖励。

**Result:** 在四个AGL基准测试中进行了广泛的实验，证明了GeoExplorer在多样化设置中的有效性和泛化能力，尤其是在定位不熟悉的目标和环境时。

**Conclusion:** GeoExplorer通过利用基于好奇心的内在奖励，提升了主动地理定位的鲁棒性和探索能力，使其在处理未知目标和环境时更为可靠。

**Abstract:** Active Geo-localization (AGL) is the task of localizing a goal, represented
in various modalities (e.g., aerial images, ground-level images, or text),
within a predefined search area. Current methods approach AGL as a
goal-reaching reinforcement learning (RL) problem with a distance-based reward.
They localize the goal by implicitly learning to minimize the relative distance
from it. However, when distance estimation becomes challenging or when
encountering unseen targets and environments, the agent exhibits reduced
robustness and generalization ability due to the less reliable exploration
strategy learned during training. In this paper, we propose GeoExplorer, an AGL
agent that incorporates curiosity-driven exploration through intrinsic rewards.
Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic,
enabling robust, diverse, and contextually relevant exploration based on
effective environment modeling. These capabilities have been proven through
extensive experiments across four AGL benchmarks, demonstrating the
effectiveness and generalization ability of GeoExplorer in diverse settings,
particularly in localizing unfamiliar targets and environments.

</details>


### [31] [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/abs/2508.00169)
*Bhavya Goyal,Felipe Gutierrez-Barragan,Wei Lin,Andreas Velten,Yin Li,Mohit Gupta*

Main category: cs.CV

> The paper introduces Probabilistic Point Clouds, a technique for enhancing 3D object detection in challenging scenarios by integrating uncertainty measurement into each point of the point cloud, demonstrating improved performance over conventional methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the key challenges faced by modern LiDARs in producing sparse or erroneous point clouds, especially in challenging real-world scenarios such as long-distance or low-albedo objects. These errors can significantly affect downstream perception models, leading to potential loss of accuracy.

**Method:** We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation where each point is augmented with a probability attribute that encapsulates the measurement uncertainty in the raw data. This representation is used in robust 3D object detection and can be implemented as computationally lightweight drop-in modules in 3D inference pipelines.

**Result:** The methods based on PPC outperform several baselines using LiDAR and even camera-LiDAR fusion models, demonstrating effectiveness in challenging indoor and outdoor scenarios.

**Conclusion:** The PPC-based approach is shown to improve the robustness and accuracy of 3D object detection in various challenging scenarios, underscoring the importance of integrating uncertainty estimation directly into the point cloud representation.

**Abstract:** LiDAR-based 3D sensors provide point clouds, a canonical 3D representation
used in various scene understanding tasks. Modern LiDARs face key challenges in
several real-world scenarios, such as long-distance or low-albedo objects,
producing sparse or erroneous point clouds. These errors, which are rooted in
the noisy raw LiDAR measurements, get propagated to downstream perception
models, resulting in potentially severe loss of accuracy. This is because
conventional 3D processing pipelines do not retain any uncertainty information
from the raw measurements when constructing point clouds.
  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation
where each point is augmented with a probability attribute that encapsulates
the measurement uncertainty (or confidence) in the raw data. We further
introduce inference approaches that leverage PPC for robust 3D object
detection; these methods are versatile and can be used as computationally
lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both
simulations and real captures, that PPC-based 3D inference methods outperform
several baselines using LiDAR as well as camera-LiDAR fusion models, across
challenging indoor and outdoor scenarios involving small, distant, and
low-albedo objects, as well as strong ambient light.
  Our project webpage is at https://bhavyagoyal.github.io/ppc .

</details>


### [32] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

> A study using Selective Modality Shifting shows Vision-Language Models tend to rely more on textual information than on visual cues in medical data, indicating the need for better integration of both modalities.

<details>
  <summary>Details</summary>

**Motivation:** To assess the reliance of Vision-Language Models on text versus image data in medical decision-making and to reveal modality-specific biases.

**Method:** Selective Modality Shifting (SMS), a perturbation-based approach to quantify a model's reliance on each modality in binary classification tasks by swapping images or text between samples with opposing labels.

**Result:** Reveals a marked dependency on text input over visual information despite the presence of complementary visual data, identified through model performance and calibration analysis in unperturbed and perturbed settings.

**Conclusion:** Highlights the need to genuinely integrate visual and textual cues in multimodal medical models to avoid relying on single-modality signals.

**Abstract:** Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [33] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

> 本文定义了一种称为结构化图谱系的概念，展示了如何利用它们执行在多尺度图和层级模型架构（称为“等级结构”）上的局部采样、搜索或优化算法，并展示了在深度神经网络和多网格数值方法中的应用。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在通过定义结构化的图谱系，探索一个适用于多项领域的数学模型的架构描述方法，尤其关注机器学习和计算科学中的应用。

**Method:** 本文提出了一种结构化的图谱系概念，这些图谱系按照层级增长，并具备多项特性，如图节点和边的数量随层级号成指数增长，以及定义了过程衍生的距离度量等。通过它们，作者导出了用于分级图的骨骼变体的标准代数图操作和类型构造器，并探讨了这些操作的代数和范畴理论性质。此外，还定义了空间效率高的单元操作，如加厚、升级等，适用于多尺度图谱系和搜索前沿的构建。

**Result:** 研究结果表明，通过图谱系的方法，可以构建空间效率高且具有相似但不完全相同的代数与范畴性质的操作符，并可以用这种方法定义“等级结构”，进而促进在多尺度图谱系框架下执行的局部算法的应用。

**Conclusion:** 本文提出的代数类型理论对于分级图和层次图谱系的构建具有广泛的意义，并且预期为定义复杂的模型架构和执行局部采样、搜索或优化算法提供了强力工具。

**Abstract:** Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [34] [Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition](https://arxiv.org/abs/2508.00205)
*Xiangyu Kong,Hengde Zhu,Haoqin Sun,Zhihao Guo,Jiayan Gu,Xinyi Ni,Wei Zhang,Shizhe Liu,Siyang Song*

Main category: cs.CV

> 本文提出了一种新的方法来模拟个体的内部认知过程，并使用二维图神经网络来提高自动真实个性识别的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有大多数自动真实个性识别解决方案作为外部观察者，基于目标个体的表达行为来推断个性印象，这往往与目标个体的真实个性存在显著偏差，并导致较差的识别性能。为了避免这种情况，作者受到真实个性与人类内部认知之间的关联的启发，提出了一个新的自动真实个性识别方法。

**Method:** 本文提出了一种新的自动真实个性识别方法，该方法通过模拟个人的内部认知过程来识别其个性，这个过程基于可获取的外部短音频-视频行为。这种方法通过个性化网络再现个体特异性面部反应，并将这些反应编码成一个二维节点和边特征矩阵的图，使用一种新型的二维图神经网络（2D-GNN）来从该图中推断真实个性特征。为了模拟个性相关的认知，设计了一个端到端的策略，联合训练认知模拟、二维图构建和个人识别模块。

**Result:** 未在摘要是提供具体结果，但是方法引入了一种创新的方式，通过模拟内部认知过程来提高个性化识别的准确性。这种方法强调了通过视频和音频数据模拟个性化内部认知的必要性，并利用2D-GNN从这种认知模型中识别个性特征的有效性。

**Conclusion:** 本文通过引入个性化内部认知的模拟过程，提出了一种基于短音频视频行为的新型自动真实个性识别方法。这种方法使用2D图神经网络从解锁的内部认识到个性特征，具有潜在的改进普通识别性能的可能性。

**Abstract:** Automatic real personality recognition (RPR) aims to evaluate human real
personality traits from their expressive behaviours. However, most existing
solutions generally act as external observers to infer observers' personality
impressions based on target individuals' expressive behaviours, which
significantly deviate from their real personalities and consistently lead to
inferior recognition performance. Inspired by the association between real
personality and human internal cognition underlying the generation of
expressive behaviours, we propose a novel RPR approach that efficiently
simulates personalised internal cognition from easy-accessible external short
audio-visual behaviours expressed by the target individual. The simulated
personalised cognition, represented as a set of network weights that enforce
the personalised network to reproduce the individual-specific facial reactions,
is further encoded as a novel graph containing two-dimensional node and edge
feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for
inferring real personality traits from it. To simulate real personality-related
cognition, an end-to-end strategy is designed to jointly train our cognition
simulation, 2D graph construction, and personality recognition modules.

</details>


### [35] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

> 本文提出了SAM-PTx，这是一种通过使用冻结的CLIP衍生文本嵌入来指导语义分割的参数高效方法，这使SAM能够更好地适应包含固定文本嵌入的输入，从而改进比纯粹基于空间提示的方法的分割性能。

<details>
  <summary>Details</summary>

**Motivation:** 段落任意模型（SAM）在基于提示的分割中表现出其强大的泛化性能，但相比于传统的基于点和框的空间提示，语义文本提示的潜力尚未得到充分研究。

**Method:** 本文提出了一种称为Parallel-Text的轻量级适配器设计，该设计将文本嵌入注入到SAM的图像编码器中，以便在保持大部分原始架构不变的情况下实现语义引导的分割。特别地，此适配器仅修改每个transformer块的MLP并行分支，保留了注意力路径用于空间推理。

**Result:** 实验验证了SAM-PTx在COD10K数据集以及COCO和ADE20K的低数据子集上的性能优于纯空间提示基线。

**Conclusion:** 研究表明，将语义条件整合到SAM的架构中是实现高效适应的实践且可扩展的方法，且计算复杂度极小。

**Abstract:** The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [36] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

> 研究显示，在少量样本图像分类任务中，利用目标对象在图像中的局部位置信息，可以显著提升分类性能。特别地，通过使用Segment Anything模型或无监督前景对象提取方法，可以实现大部分性能提升，而且只需要指出感兴趣对象的一个像素点即可。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决少量样本图像分类任务中由于图像歧义导致性能下降的问题，特别是当图像中包含多个对象或复杂背景的情况下。

**Method:** 通过引入目标对象在图像中的局部位置信息来改进图像分类性能。具体实现方法包括使用Segment Anything模型或无监督的前景对象提取技术。

**Result:** 在已建立的基准测试中，研究显示局部位置信息的引入显著提升了分类性能，而通过Segment Anything模型或无监督对象提取方法能够实现大部分提升效果，只需提供一个像素的信息。

**Conclusion:** 研究表明，在仅使用最少样本（一个实例/类别）的情况下，通过增加局部对象位置的信息可以极大地提高图像分类的准确性，而使用Segment Anything模型或无监督方法，甚至可以用最少的信息获取大幅度的性能改进。

**Abstract:** In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [37] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

> The paper introduces the MSF-UM model for depth map super-resolution, which uses an innovative multi-scale fusion structure guided by color images to achieve better results with fewer parameters.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the spatial resolution and high-frequency detail restoration in depth maps while addressing the limitations of traditional CNNs and transformers in modeling global context and computational efficiency for high-resolution depth maps.

**Method:** The paper proposes a multi-scale fusion U-shaped Mamba (MSF-UM) model for depth map super-resolution, integrating Mamba's efficient state-space modeling with a multi-scale U-shaped fusion structure guided by a color image.

**Result:** Experiments demonstrate that the MSF-UM model achieves better reconstruction accuracy with fewer model parameters compared to existing methods and shows excellent performance in large-scale depth map super-resolution tasks.

**Conclusion:** The conclusion is that the proposed MSF-UM model significantly enhances the accuracy and efficiency of depth map super-resolution, especially for high-resolution depth maps, and demonstrates strong generalization capabilities across various datasets.

**Abstract:** Depth map super-resolution technology aims to improve the spatial resolution
of low-resolution depth maps and effectively restore high-frequency detail
information. Traditional convolutional neural network has limitations in
dealing with long-range dependencies and are unable to fully model the global
contextual information in depth maps. Although transformer can model global
dependencies, its computational complexity and memory consumption are
quadratic, which significantly limits its ability to process high-resolution
depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba
(MSF-UM) model, a novel guided depth map super-resolution framework. The core
innovation of this model is to integrate Mamba's efficient state-space modeling
capabilities into a multi-scale U-shaped fusion structure guided by a color
image. The structure combining the residual dense channel attention block and
the Mamba state space module is designed, which combines the local feature
extraction capability of the convolutional layer with the modeling advantage of
the state space model for long-distance dependencies. At the same time, the
model adopts a multi-scale cross-modal fusion strategy to make full use of the
high-frequency texture information from the color image to guide the
super-resolution process of the depth map. Compared with existing mainstream
methods, the proposed MSF-UM significantly reduces the number of model
parameters while achieving better reconstruction accuracy. Extensive
experiments on multiple publicly available datasets validate the effectiveness
of the model, especially showing excellent generalization ability in the task
of large-scale depth map super-resolution.

</details>


### [38] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

> PointGauss框架利用点云指导高斯溅射表示中的实时多目标分割，解决了现有方法的初始化慢和多视角一致性差的问题，通过实验验证了其在多视角mIoU上的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在初始化和多视角一致性方面存在问题。因此，研究者提出了PointGauss框架，旨在实现高效的实时多目标分割，同时解决现有方法的局限性。

**Method:** 该框架主要包括两个关键创新点：(1)基于点云的高斯原语解码器，能够在一分钟内生成3D实例掩码；(2)加速2D掩码渲染系统，以确保多视角一致性。

**Result:** 实验结果显示，相较于先前的最佳方法，PointGauss在多视角mIoU上取得了1.89%到31.78%的性能提升，同时保持了极佳的计算效率。

**Conclusion:** 研究者还引入了DesktopObjects-360，这是一个用于辐射场中3D分割的新型综合数据集，旨在解决当前基准数据集中单一目标聚焦、不一致的3D评估、规模小和部分覆盖的问题。

**Abstract:** We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [39] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

> 研究提出了一种新的框架和方法，通过混合视觉投影器以基于具体指令上下文进行视觉到语言的翻译，改进了传统的持续学习方法，有效提升了模型对新视觉-语言任务的适应性。

<details>
  <summary>Details</summary>

**Motivation:** 先前的方法在通过调整视觉投影器来适应新任务时，可能会导致模型过分依赖视觉输入而忽视语言指令，特别是在具有重复类型的文本指令的学习任务中。为了克服这一问题并更充分地利用语言指令，本研究提出了新的框架和方法。

**Method:** 本研究提出了一种新的框架，通过将视觉信息的翻译基于语言模型的指令来进行。该框架引入了一种视觉投影器的混合方式，每个投影器作为根据给定指令上下文的专门视觉到语言翻译专家，以适应新的任务。此外，还提出了一种专家推荐策略，用于重用与之前学习的任务相似的专家，并采用专家剪枝方法来避免之前任务中累积激活的专家造成干扰。

**Result:** 在多样化的视觉语言任务中的广泛实验表明，该方法在生成遵循指令的响应方面优于现有的持续学习方法。

**Conclusion:** 通过使用专家混合和剪枝方法，该研究在连续学习中实现了一种更有效的方式，并提高了生成指令响应的性能。

**Abstract:** Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [40] [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265)
*Henghui Ding,Song Tang,Shuting He,Chang Liu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

> A comprehensive survey on multimodal referring segmentation detailing its background, datasets, unified architecture, and methods for images, videos, and 3D scenes, with focus on GREx for real-world complexity.

<details>
  <summary>Details</summary>

**Motivation:** The purpose is to summarize the advancements in multimodal referring segmentation due to improvements in convolutional neural networks, transformers, and large language models, which have enhanced multimodal perception capabilities.

**Method:** This paper provides a comprehensive survey of multimodal referring segmentation, focusing on a unified meta architecture for various visual scenes and Generalized Referring Expression (GREx) methods.

**Result:** The paper outlines the background, definitions, datasets, and key methods of multimodal referring segmentation, providing performance evaluations on standard benchmarks.

**Conclusion:** The survey highlights the importance of multimodal referring segmentation for practical applications, outlining the state-of-the-art methods and indicating the need for further research to address real-world complexity.

**Abstract:** Multimodal referring segmentation aims to segment target objects in visual
scenes, such as images, videos, and 3D scenes, based on referring expressions
in text or audio format. This task plays a crucial role in practical
applications requiring accurate object perception based on user instructions.
Over the past decade, it has gained significant attention in the multimodal
community, driven by advances in convolutional neural networks, transformers,
and large language models, all of which have substantially improved multimodal
perception capabilities. This paper provides a comprehensive survey of
multimodal referring segmentation. We begin by introducing this field's
background, including problem definitions and commonly used datasets. Next, we
summarize a unified meta architecture for referring segmentation and review
representative methods across three primary visual scenes, including images,
videos, and 3D scenes. We further discuss Generalized Referring Expression
(GREx) methods to address the challenges of real-world complexity, along with
related tasks and practical applications. Extensive performance comparisons on
standard benchmarks are also provided. We continually track related works at
https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

</details>


### [41] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

> 研究建立新基准评估语义对应关系在恶劣条件下的鲁棒性，发现所有现有方法表现不足，大规模视觉模型能提升鲁棒性，但微调会减弱，DINO与Stable Diffusion融合表现最佳，需针对任务加强鲁棒性设计。

<details>
  <summary>Details</summary>

**Motivation:** 语义对应在理想条件下取得了显著成果，但在具有挑战性的场景中的鲁棒性研究较少。为了弥补这一不足，本文提出一个在恶劣条件下评估语义对应关系的新基准。

**Method:** 本文构建了一个新的基准来评估在恶劣条件下的语义对应关系，该基准数据集包含了14种不同的具有挑战性的场景，例如几何失真、图像模糊、数字伪影和环境遮挡等。

**Result:** 经过广泛的评估，研究发现所有现有方法在恶劣条件下的表现都有所减弱，使用大规模视觉模型虽然有助于整体鲁棒性的提升，但在这些模型上的微调反而导致相对鲁棒性的减弱；DINO模型在相对鲁棒性上优于Stable Diffusion，而两者融合则在绝对鲁棒性上表现更佳。

**Conclusion:** 常规的数据增强策略对于提高语义对应的鲁棒性并无显著效果，提示需要进行任务特异性设计。这些结果在作者构建的数据集和真实世界基准中都是一致的。

**Abstract:** Semantic correspondence aims to identify semantically meaningful
relationships between different images and is a fundamental challenge in
computer vision. It forms the foundation for numerous tasks such as 3D
reconstruction, object tracking, and image editing. With the progress of
large-scale vision models, semantic correspondence has achieved remarkable
performance in controlled and high-quality conditions. However, the robustness
of semantic correspondence in challenging scenarios is much less investigated.
In this work, we establish a novel benchmark for evaluating semantic
correspondence in adverse conditions. The benchmark dataset comprises 14
distinct challenging scenarios that reflect commonly encountered imaging
issues, including geometric distortion, image blurring, digital artifacts, and
environmental occlusion. Through extensive evaluations, we provide several key
insights into the robustness of semantic correspondence approaches: (1) All
existing methods suffer from noticeable performance drops under adverse
conditions; (2) Using large-scale vision models can enhance overall robustness,
but fine-tuning on these models leads to a decline in relative robustness; (3)
The DINO model outperforms the Stable Diffusion in relative robustness, and
their fusion achieves better absolute robustness; Moreover, We evaluate common
robustness enhancement strategies for semantic correspondence and find that
general data augmentations are ineffective, highlighting the need for
task-specific designs. These results are consistent across both our dataset and
real-world benchmarks.

</details>


### [42] [Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning](https://arxiv.org/abs/2508.00287)
*Tran Viet Khoa,Do Hai Son,Mohammad Abu Alsheikh,Yibeltal F Alem,Dinh Thai Hoang*

Main category: cs.CV

> A novel framework combines SSA and LSTM for drowsiness detection, using federated learning and a customized tool for video processing, achieving an accuracy of 89.9%.

<details>
  <summary>Details</summary>

**Motivation:** The key motivation is to address the problem of accurately detecting driver drowsiness using decentralized and diverse facial data, which is critical for enhancing road safety.

**Method:** Our approach includes a novel Spatial Self-Attention (SSA) mechanism combined with a Long Short-Term Memory (LSTM) network for improved drowsiness detection. We also use a Gradient Similarity Comparison (GSC) for federated learning and a customized tool for video data processing.

**Result:** The detection accuracy achieved by our framework in the federated learning setting is 89.9%, outperforming existing methods under various deployment scenarios.

**Conclusion:** The proposed framework effectively handles real-world data variability, highlighting its potential for deployment in intelligent transportation systems to improve road safety through early and reliable drowsiness detection.

**Abstract:** Driver drowsiness is one of the main causes of road accidents and is
recognized as a leading contributor to traffic-related fatalities. However,
detecting drowsiness accurately remains a challenging task, especially in
real-world settings where facial data from different individuals is
decentralized and highly diverse. In this paper, we propose a novel framework
for drowsiness detection that is designed to work effectively with
heterogeneous and decentralized data. Our approach develops a new Spatial
Self-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)
network to better extract key facial features and improve detection
performance. To support federated learning, we employ a Gradient Similarity
Comparison (GSC) that selects the most relevant trained models from different
operators before aggregation. This improves the accuracy and robustness of the
global model while preserving user privacy. We also develop a customized tool
that automatically processes video data by extracting frames, detecting and
cropping faces, and applying data augmentation techniques such as rotation,
flipping, brightness adjustment, and zooming. Experimental results show that
our framework achieves a detection accuracy of 89.9% in the federated learning
settings, outperforming existing methods under various deployment scenarios.
The results demonstrate the effectiveness of our approach in handling
real-world data variability and highlight its potential for deployment in
intelligent transportation systems to enhance road safety through early and
reliable drowsiness detection.

</details>


### [43] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

> 提出了TITAN-Guide方法，解决了现有技术在文字到视频扩散模型应用场景下的内存和控制效果问题，优化了扩散潜变量的优化过程，提高了T2V模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 在条件扩散模型的发展中，仍然需要大量的监督微调来进行任务类别的控制。训练自由的引导框架要么内存需求巨大，要么由于粗略的估计导致控制效果不佳。这限制了对需要大量计算控制的扩散模型的应用，如文本到视频（T2V）扩散模型。为此我们提出了TITAN-Guide。

**Method:** 我们提出了一种名为TITAN-Guide的方法，该方法在推理过程中对齐指导，解决了内存空间问题，并且在引导过程中提供了更优的控制。特别地，我们开发了一种无需反向传播的高效方法来优化扩散潜变量。此外，我们研究了不同导向指令下的正向梯度下降在引导扩散任务中的应用。

**Result:** 实验表明，我们的方法能够在潜变量优化过程中有效地管理内存，而之前的方法在这方面有不足。我们提出的方法不仅降低了内存需求，而且在扩散指导基准上的T2V性能得到了显著提升。

**Conclusion:** 我们提出的方法有效地解决了现有技术在T2V模型应用中的内存问题和控制效果不佳的问题，其在扩散指导任务中的高效内存管理和优化效果显著优于现有方法。

**Abstract:** In the recent development of conditional diffusion models still require heavy
supervised fine-tuning for performing control on a category of tasks.
Training-free conditioning via guidance with off-the-shelf models is a
favorable alternative to avoid further fine-tuning on the base model. However,
the existing training-free guidance frameworks either have heavy memory
requirements or offer sub-optimal control due to rough estimation. These
shortcomings limit the applicability to control diffusion models that require
intense computation, such as Text-to-Video (T2V) diffusion models. In this
work, we propose Taming Inference Time Alignment for Guided Text-to-Video
Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues,
and provides more optimal control in the guidance process compared to the
counterparts. In particular, we develop an efficient method for optimizing
diffusion latents without backpropagation from a discriminative guiding model.
In particular, we study forward gradient descents for guided diffusion tasks
with various options on directional directives. In our experiments, we
demonstrate the effectiveness of our approach in efficiently managing memory
during latent optimization, while previous methods fall short. Our proposed
approach not only minimizes memory requirements but also significantly enhances
T2V performance across a range of diffusion guidance benchmarks. Code, models,
and demo are available at https://titanguide.github.io.

</details>


### [44] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

> AniMer+, an extended version of AniMer framework, uses a high-capacity, family-aware Vision Transformer (ViT) with a Mixture-of-Experts (MoE) design to reconstruct mammals and birds, introducing a diffusion-based pipeline for generating synthetic datasets, demonstrating superior performance on various benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** To develop a unified approach for accurately estimating the pose and shape of different species, addressing the limitations of previous methods in terms of network capacity and the lack of comprehensive multi-species datasets.

**Method:** AniMer+ uses a high-capacity, family-aware Vision Transformer (ViT) with a Mixture-of-Experts (MoE) design. Networks are partitioned into taxa-specific and taxa-shared components. A diffusion-based conditional image generation pipeline produces synthetic datasets for training, namely CtrlAni3D for quadrupeds and CtrlAVES3D for birds.

**Result:** The method demonstrates superior performance across a wide range of benchmarks, including the challenging out-of-domain Animal Kingdom dataset, indicating improved real-world application performance.

**Conclusion:** The novel architecture and synthetic datasets generated by AniMer+ significantly enhance performance in pose and shape estimation for mammals and birds, opening new avenues for biological research and spatial intelligence applications.

**Abstract:** In the era of foundation models, achieving a unified understanding of
different dynamic objects through a single network has the potential to empower
stronger spatial intelligence. Moreover, accurate estimation of animal pose and
shape across diverse species is essential for quantitative analysis in
biological research. However, this topic remains underexplored due to the
limited network capacity of previous methods and the scarcity of comprehensive
multi-species datasets. To address these limitations, we introduce AniMer+, an
extended version of our scalable AniMer framework. In this paper, we focus on a
unified approach for reconstructing mammals (mammalia) and birds (aves). A key
innovation of AniMer+ is its high-capacity, family-aware Vision Transformer
(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture
partitions network layers into taxa-specific components (for mammalia and aves)
and taxa-shared components, enabling efficient learning of both distinct and
common anatomical features within a single model. To overcome the critical
shortage of 3D training data, especially for birds, we introduce a
diffusion-based conditional image generation pipeline. This pipeline produces
two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for
birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for
birds, which is crucial for resolving single-view depth ambiguities. Trained on
an aggregated collection of 41.3k mammalian and 12.4k avian images (combining
real and synthetic data), our method demonstrates superior performance over
existing approaches across a wide range of benchmarks, including the
challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the
effectiveness of both our novel network architecture and the generated
synthetic datasets in enhancing real-world application performance.

</details>
