{"id": "2509.21357", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21357", "abs": "https://arxiv.org/abs/2509.21357", "authors": ["Wenkai Wang", "Vincent Lee", "Yizhen Zheng"], "title": "A Novel Differential Feature Learning for Effective Hallucination Detection and Classification", "comment": "10 pages, 7 figures, 13 tables", "summary": "Large language model hallucination represents a critical challenge where\noutputs deviate from factual accuracy due to distributional biases in training\ndata. While recent investigations establish that specific hidden layers exhibit\ndifferences between hallucinatory and factual content, the precise localization\nof hallucination signals within layers remains unclear, limiting the\ndevelopment of efficient detection methods. We propose a dual-model\narchitecture integrating a Projected Fusion (PF) block for adaptive inter-layer\nfeature weighting and a Differential Feature Learning (DFL) mechanism that\nidentifies discriminative features by computing differences between parallel\nencoders learning complementary representations from identical inputs. Through\nsystematic experiments across HaluEval's question answering, dialogue, and\nsummarization datasets, we demonstrate that hallucination signals concentrate\nin highly sparse feature subsets, achieving significant accuracy improvements\non question answering and dialogue tasks. Notably, our analysis reveals a\nhierarchical \"funnel pattern\" where shallow layers exhibit high feature\ndiversity while deep layers demonstrate concentrated usage, enabling detection\nperformance to be maintained with minimal degradation using only 1\\% of feature\ndimensions. These findings suggest that hallucination signals are more\nconcentrated than previously assumed, offering a pathway toward computationally\nefficient detection systems that could reduce inference costs while maintaining\naccuracy.", "AI": {"tldr": "文章提出了一种新架构，通过差异特征学习和特征的稀疏性来检测语言模型的幻觉，并取得了显著的结果，表明幻觉信号集中且易于检测。", "motivation": "大型语言模型幻觉表示了一个关键挑战，即输出由于训练数据中的分布偏差而偏离了事实准确性。尽管最近的研究表明，特定的隐藏层在幻觉和事实内容之间存在差异，但具体定位层内的幻觉信号尚不清楚，这限制了有效检测方法的发展。", "method": "我们提出了一种双模型架构，该架构结合了投影融合（PF）块进行自适应层间特征加权，以及差异特征学习（DFL）机制，通过计算平行编码器从相同输入中学习到的互补表示之间的差异来识别判别特征。", "result": "通过在HaluEval的问题回答、对话和总结数据集上的系统实验，我们证明了幻觉信号集中在高度稀疏的特征子集，显著提高了问题回答和对话任务的准确性。", "conclusion": "这些发现表明，幻觉信号比先前假定的更加集中，提供了向计算效率高的检测系统发展的途径，这可以降低推理成本同时保持准确性。"}}
{"id": "2509.21359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21359", "abs": "https://arxiv.org/abs/2509.21359", "authors": ["Jiale Deng", "Yanyan Shen", "Ziyuan Pei", "Youmin Chen", "Linpeng Huang"], "title": "Influence Guided Context Selection for Effective Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) addresses large language model (LLM)\nhallucinations by grounding responses in external knowledge, but its\neffectiveness is compromised by poor-quality retrieved contexts containing\nirrelevant or noisy information. While existing approaches attempt to improve\nperformance through context selection based on predefined context quality\nassessment metrics, they show limited gains over standard RAG. We attribute\nthis limitation to their failure in holistically utilizing available\ninformation (query, context list, and generator) for comprehensive quality\nassessment. Inspired by recent advances in data selection, we reconceptualize\ncontext quality assessment as an inference-time data valuation problem and\nintroduce the Contextual Influence Value (CI value). This novel metric\nquantifies context quality by measuring the performance degradation when\nremoving each context from the list, effectively integrating query-aware\nrelevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI\nvalue eliminates complex selection hyperparameter tuning by simply retaining\ncontexts with positive CI values. To address practical challenges of label\ndependency and computational overhead, we develop a parameterized surrogate\nmodel for CI value prediction during inference. The model employs a\nhierarchical architecture that captures both local query-context relevance and\nglobal inter-context interactions, trained through oracle CI value supervision\nand end-to-end generator feedback. Extensive experiments across 8 NLP tasks and\nmultiple LLMs demonstrate that our context selection method significantly\noutperforms state-of-the-art baselines, effectively filtering poor-quality\ncontexts while preserving critical information. Code is available at\nhttps://github.com/SJTU-DMTai/RAG-CSM.", "AI": {"tldr": "论文提出了一种新方法，通过引入上下文影响值（CI值）来改进检索增强生成(RAG)中上下文的选择，显著优于现有的基线方法，能够有效过滤掉质量差的上下文并保留关键信息。", "motivation": "现有的方法在通过基于预定的上下文质量评估指标选择上下文来提高性能方面的效果有限。我们将其归因于未能全面利用可用信息（查询、上下文列表和生成器）进行综合质量评估。", "method": "引入了上下文影响值（CI值）作为衡量上下文质量的新指标。CI值通过测量从上下文列表中移除每个上下文时的性能下降来量化其质量，从而整合了查询感知的相关性、列表感知的独特性和生成器感知的对齐。此外，CI值通过仅保留CI值为正的上下文，消除了复杂的筛选超参数调整。为了应对标签依赖和计算开销的实际挑战，开发了一个参数化的代理模型进行预测。该模型通过分层架构来捕捉局部的查询-上下文相关性以及全局的跨上下文交互，通过CI值的监督以及端到端生成器反馈进行训练。", "result": "实验结果展示了本方法如何在多个自然语言处理任务中以多种大型语言模型为基准的数据集上实现卓越的表现。", "conclusion": "实验表明，论文提出的方法在8个NLP任务和多个大型语言模型中显著优于现有基线方法，有效地过滤掉了质量差的上下文，同时保留了关键信息。"}}
{"id": "2509.21361", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.21361", "abs": "https://arxiv.org/abs/2509.21361", "authors": ["Norman Paulsen"], "title": "Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs", "comment": "20 pages, 4 charts", "summary": "Large language model (LLM) providers boast big numbers for maximum context\nwindow sizes. To test the real world use of context windows, we 1) define a\nconcept of maximum effective context window, 2) formulate a testing method of a\ncontext window's effectiveness over various sizes and problem types, and 3)\ncreate a standardized way to compare model efficacy for increasingly larger\ncontext window sizes to find the point of failure. We collected hundreds of\nthousands of data points across several models and found significant\ndifferences between reported Maximum Context Window (MCW) size and Maximum\nEffective Context Window (MECW) size. Our findings show that the MECW is, not\nonly, drastically different from the MCW but also shifts based on the problem\ntype. A few top of the line models in our test group failed with as little as\n100 tokens in context; most had severe degradation in accuracy by 1000 tokens\nin context. All models fell far short of their Maximum Context Window by as\nmuch as 99 percent. Our data reveals the Maximum Effective Context Window\nshifts based on the type of problem provided, offering clear and actionable\ninsights into how to improve model accuracy and decrease model hallucination\nrates.", "AI": {"tldr": "研究展示了报告的最大上下文窗口和实际有效上下文窗口之间存在巨大差异，并提出了一些改进模型性能和减少幻觉的见解。", "motivation": "测试上下文窗口在现实世界中的实际应用，特别是在大型语言模型(LLM)中的应用。", "method": "通过定义最大有效上下文窗口的概念，制定上下文窗口有效性测试方法，以及创建标准化的比较方法，来测试不同大小和问题类型的上下文窗口的效果，并发现在报告的最大上下文窗口（MCW）和最大有效上下文窗口（MECW）之间存在显著差异。", "result": "实验结果表明，一些顶级模型在测试组中，即使只有100个上下文标记也会出现严重问题；大多数模型在1000个上下文标记范围内，其准确性会显著下降。所有测试模型的MECW比其报告的最大上下文窗口小至99%。", "conclusion": "研究发现，不仅最大有效上下文窗口（MECW）与最大上下文窗口（MCW）存在巨大差异，并且MECW会根据问题类型变化。这项数据揭示了基于问题类型的最大有效上下文窗口转移现象，提供了提高模型准确性和降低模型幻觉率的明确且可操作的意见。"}}
{"id": "2509.21404", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21404", "abs": "https://arxiv.org/abs/2509.21404", "authors": ["Xiaotie Deng", "Hanyu Li"], "title": "How Large Language Models Need Symbolism", "comment": null, "summary": "We argue that AI's future requires more than scaling. To unlock genuine\ndiscovery, large language models need a compass: human-crafted symbols to guide\ntheir powerful but blind intuition.", "AI": {"tldr": "论文主张AI的发展不能仅依赖规模扩大，大型语言模型需借助人类设计的符号来引导创意发现。", "motivation": "强调AI未来需要超越单纯扩大规模，提出大型语言模型需要人类设计的符号来引导其强大的但盲目的直觉。", "method": "N/A", "result": "N/A", "conclusion": "N/A"}}
{"id": "2509.21351", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21351", "abs": "https://arxiv.org/abs/2509.21351", "authors": ["Valentin Samokhin", "Boris Shirokikh", "Mikhail Goncharov", "Dmitriy Umerenkov", "Maksim Bobrin", "Ivan Oseledets", "Dmitry Dylov", "Mikhail Belyaev"], "title": "Random Direct Preference Optimization for Radiography Report Generation", "comment": null, "summary": "Radiography Report Generation (RRG) has gained significant attention in\nmedical image analysis as a promising tool for alleviating the growing workload\nof radiologists. However, despite numerous advancements, existing methods have\nyet to achieve the quality required for deployment in real-world clinical\nsettings. Meanwhile, large Visual Language Models (VLMs) have demonstrated\nremarkable progress in the general domain by adopting training strategies\noriginally designed for Large Language Models (LLMs), such as alignment\ntechniques. In this paper, we introduce a model-agnostic framework to enhance\nRRG accuracy using Direct Preference Optimization (DPO). Our approach leverages\nrandom contrastive sampling to construct training pairs, eliminating the need\nfor reward models or human preference annotations. Experiments on supplementing\nthree state-of-the-art models with our Random DPO show that our method improves\nclinical performance metrics by up to 5%, without requiring any additional\ntraining data.", "AI": {"tldr": "我们提出了一种无模型框架，利用Direct Preference Optimization (DPO)提升放射学报告生成( RRG) 的质量，无需额外训练数据或人工偏好标注，改进了临床性能指标。", "motivation": "鉴于现有的放射学报告生成方法尚未达到在真实临床环境中部署所需的质量水平，我们提出了这种方法以改善这一状况。大型视觉语言模型（VLM）在采用大型语言模型（LLM）的训练策略方面取得了显著进展，我们的方法借鉴了这些进展。", "method": "我们的方法采用了一种无模型框架，利用Direct Preference Optimization (DPO)来提高放射学报告生成（RRG）的准确性。通过随机对比采样构建训练对，这种方法不需要奖励模型或人类偏好注释。", "result": "实验结果显示，在现有的三种先进模型中加入我们的Random DPO技术，临床性能指标提高了最多5%。", "conclusion": "通过我们的模型无关框架，可以有效地提高放射学报告生成的准确性，显著改善了性能而无需额外的数据或手工注释。"}}
{"id": "2509.21443", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21443", "abs": "https://arxiv.org/abs/2509.21443", "authors": ["Sualeha Farid", "Jayden Lin", "Zean Chen", "Shivani Kumar", "David Jurgens"], "title": "One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning", "comment": "22 pages, 11 figures, 6 tables", "summary": "Large Language Models (LLMs) are increasingly deployed in multilingual and\nmulticultural environments where moral reasoning is essential for generating\nethically appropriate responses. Yet, the dominant pretraining of LLMs on\nEnglish-language data raises critical concerns about their ability to\ngeneralize judgments across diverse linguistic and cultural contexts. In this\nwork, we systematically investigate how language mediates moral decision-making\nin LLMs. We translate two established moral reasoning benchmarks into five\nculturally and typologically diverse languages, enabling multilingual zero-shot\nevaluation. Our analysis reveals significant inconsistencies in LLMs' moral\njudgments across languages, often reflecting cultural misalignment. Through a\ncombination of carefully constructed research questions, we uncover the\nunderlying drivers of these disparities, ranging from disagreements to\nreasoning strategies employed by LLMs. Finally, through a case study, we link\nthe role of pretraining data in shaping an LLM's moral compass. Through this\nwork, we distill our insights into a structured typology of moral reasoning\nerrors that calls for more culturally-aware AI.", "AI": {"tldr": "研究探讨了语言如何影响大型语言模型的道德决策，揭示了预训练数据对LLMs道德判断的影响，提示需要开发更加文化适应性的AI。", "motivation": "大型语言模型（LLMs）越来越多地部署在多语言和多文化环境中，其中道德推理对于生成合乎伦理的响应至关重要。然而，LLMs的主导预训练主要基于英语数据，这一现状对它们在各种语言和文化背景下推广判断的能力提出了严峻挑战。", "method": "通过将两个已建立的道德推理基准翻译成五种文化上和类型上多样化的语言来进行多语言零样本评估，系统地研究了语言在LLMs道德决策中的中介作用。", "result": "分析显示了LLMs在不同语言中的道德判断存在显著不一致性，经常反映出文化上的不同。通过仔细构建的研究问题，揭示了这些差异的潜在驱动因素，包括LLMs使用的推理策略等。", "conclusion": "通过这项工作，将见解提炼为一种道德推理错误的结构化分类，呼吁更加注重文化的AI。"}}
{"id": "2509.21352", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21352", "abs": "https://arxiv.org/abs/2509.21352", "authors": ["William Saakyan", "Matthias Norden", "Lola Eversmann", "Simon Kirsch", "Muyu Lin", "Simon Guendelman", "Isabel Dziobek", "Hanna Drimalla"], "title": "Improving Autism Detection with Multimodal Behavioral Analysis", "comment": null, "summary": "Due to the complex and resource-intensive nature of diagnosing Autism\nSpectrum Condition (ASC), several computer-aided diagnostic support methods\nhave been proposed to detect autism by analyzing behavioral cues in patient\nvideo data. While these models show promising results on some datasets, they\nstruggle with poor gaze feature performance and lack of real-world\ngeneralizability. To tackle these challenges, we analyze a standardized video\ndataset comprising 168 participants with ASC (46% female) and 157 non-autistic\nparticipants (46% female), making it, to our knowledge, the largest and most\nbalanced dataset available. We conduct a multimodal analysis of facial\nexpressions, voice prosody, head motion, heart rate variability (HRV), and gaze\nbehavior. To address the limitations of prior gaze models, we introduce novel\nstatistical descriptors that quantify variability in eye gaze angles, improving\ngaze-based classification accuracy from 64% to 69% and aligning computational\nfindings with clinical research on gaze aversion in ASC. Using late fusion, we\nachieve a classification accuracy of 74%, demonstrating the effectiveness of\nintegrating behavioral markers across multiple modalities. Our findings\nhighlight the potential for scalable, video-based screening tools to support\nautism assessment.", "AI": {"tldr": "This study enhances ASC detection by developing improved statistical gaze descriptors and conducting a multimodal behavioral analysis on a large, balanced dataset, achieving a 74% classification accuracy via late fusion.", "motivation": "The motivation for this research stems from the difficulties in diagnosing ASC, particularly with the existing gaze feature performance and the need for models with better real-world generalizability. The researchers aimed to improve diagnostic support by enhancing the accuracy of gaze-based classifications and integrating multiple behavioral markers.", "method": "To address the limitations in prior gaze models when diagnosing ASC using computer-aided methods, the researchers developed novel statistical descriptors to quantify variability in eye gaze angles. They conducted a multimodal analysis of facial expressions, voice prosody, head motion, heart rate variability, and gaze behavior using a large, balanced dataset.", "result": "The research led to a significant increase in the gaze-based classification accuracy from 64% to 69%, while the multimodal late fusion analysis reached an overall classification accuracy of 74%. These improvements align with clinical findings on gaze aversion in ASC.", "conclusion": "The study emphasizes the potential for scalable, video-based screening tools to improve ASC assessment by offering more accurate and generalized diagnostic support through the integration of multimodal behavioral markers."}}
