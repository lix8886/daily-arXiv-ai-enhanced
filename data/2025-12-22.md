<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Women's Health Benchmark for Large Language Models](https://arxiv.org/abs/2512.17028)
*Victoria-Elisabeth Gruber,Razvan Marinescu,Diego Fajardo,Amin H. Nassar,Christopher Arkfeld,Alexandria Ludlow,Shama Patel,Mehrnoosh Samaei,Valerie Klug,Anna Huber,Marcel Gühner,Albert Botta i Orfila,Irene Lagoja,Kimya Tarr,Haleigh Larson,Mary Beth Howard*

Main category: cs.CL

> 论文介绍了Women's Health Benchmark (WHB)，一个评估大型语言模型在女性健康领域表现的基准，包含96个严格的模型样本，覆盖五个医学专科。评估结果显示当前模型在WHB上的失败率约为60%，显示了在女性健康领域提供可靠建议的能力仍有待提升。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型成为数以百万计人群获取健康信息的主要来源，但这些模型在女性健康领域的准确性却一直未得到充分审查，因此论文旨在通过引入WHB基准来评估这些模型的表现。

**Method:** 论文采用Women's Health Benchmark，该基准涵盖了五个医学专科、三种查询类型和八种错误类型来评估13个最先进的大型语言模型。

**Result:** 评估表明当前大型语言模型在WHB上的总体失败率为60%，不同专科和错误类型表现差异显著，尤其是在“延误紧迫性”这一方面普遍表现不佳。

**Conclusion:** 发现表明当前的AI聊天机器人尚未能够在女性健康领域提供可靠的建议。

**Abstract:** As large language models (LLMs) become primary sources of health information for millions, their accuracy in women's health remains critically unexamined. We introduce the Women's Health Benchmark (WHB), the first benchmark evaluating LLM performance specifically in women's health. Our benchmark comprises 96 rigorously validated model stumps covering five medical specialties (obstetrics and gynecology, emergency medicine, primary care, oncology, and neurology), three query types (patient query, clinician query, and evidence/policy query), and eight error types (dosage/medication errors, missing critical information, outdated guidelines/treatment recommendations, incorrect treatment advice, incorrect factual information, missing/incorrect differential diagnosis, missed urgency, and inappropriate recommendations). We evaluated 13 state-of-the-art LLMs and revealed alarming gaps: current models show approximately 60\% failure rates on the women's health benchmark, with performance varying dramatically across specialties and error types. Notably, models universally struggle with "missed urgency" indicators, while newer models like GPT-5 show significant improvements in avoiding inappropriate recommendations. Our findings underscore that AI chatbots are not yet fully able of providing reliable advice in women's health.

</details>


### [2] [Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL](https://arxiv.org/abs/2512.17053)
*Khushboo Thaker,Yony Bresler*

Main category: cs.CL

> 本文提出了一种知识蒸馏框架Struct-SQL，通过采用结构化的推理表示方法，提高了小模型在Text-to-SQL任务中的性能，减小了语法错误。

<details>
  <summary>Details</summary>

**Motivation:** 解决企业级部署准确的Text-to-SQL系统面临的成本、安全性和性能之间的三难困境。当前解决方案迫使企业在昂贵的专有大型语言模型和性能较差的小型语言模型之间进行选择。

**Method:** 提出了一种新颖的知识蒸馏框架（Struct-SQL），该框架使用查询执行计划作为形式化的蓝图，以获取结构化的推理表示，从而训练小型语言模型（SLM）来模拟强大的大型语言模型（LLM）的行为。

**Result:** 与基于非结构化的Chain-of-Thought蒸馏基线相比，结构化Chain-of-Thought蒸馏的小型语言模型在SQL生成任务上获得了8.1%的绝对提高，特别是在减少语法错误方面。

**Conclusion:** 研究表明，通过使用结构化逻辑模型进行推理，可以使得小模型在SQL生成任务中的可靠性大幅度提升。

**Abstract:** Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.

</details>


### [3] [XLM: A Python package for non-autoregressive language models](https://arxiv.org/abs/2512.17065)
*Dhruvesh Patel,Durga Prasad Maram,Sai Sreenivas Chintha,Benjamin Rozonoyer,Andrew McCallum*

Main category: cs.CL

> 本文介绍了一个名为XLM的Python包，旨在简化小型非自回归语言模型的实现过程，并提供一系列预训练模型供研究使用。

<details>
  <summary>Details</summary>

**Motivation:** 由于非自回归语言模型的实现大多为定制化，导致难以系统地比较不同方法，且每个模型都需要单独的数据整理、损失计算和预测逻辑，这不利于组件的复用。因此，研究动机是解决这些问题，促进非自回归语言模型的发展。

**Method:** 本研究提出了XLM Python包，旨在加速小型非自回归语言模型的实现，并通过伴随的xlm-models包提供一系列预训练模型，以供研究社区使用。

**Result:** 研究结果是开发了一个能够加快小型非自回归语言模型实现速度的工具包。

**Conclusion:** XLM工具包的开发证明了其可以有效提高小型非自回归语言模型的实现和研究效率，为研究者提供了一种新的工具选择，并旨在促进该领域的发展。

**Abstract:** In recent years, there has been a resurgence of interest in non-autoregressive text generation in the context of general language modeling. Unlike the well-established autoregressive language modeling paradigm, which has a plethora of standard training and inference libraries, implementations of non-autoregressive language modeling have largely been bespoke making it difficult to perform systematic comparisons of different methods. Moreover, each non-autoregressive language model typically requires it own data collation, loss, and prediction logic, making it challenging to reuse common components. In this work, we present the XLM python package, which is designed to make implementing small non-autoregressive language models faster with a secondary goal of providing a suite of small pre-trained models (through a companion xlm-models package) that can be used by the research community. The code is available at https://github.com/dhruvdcoder/xlm-core.

</details>


### [4] [Perturb Your Data: Paraphrase-Guided Training Data Watermarking](https://arxiv.org/abs/2512.17075)
*Pranav Shetty,Mirazul Haque,Petr Babkin,Zhiqiang Ma,Xiaomo Liu,Manuela Veloso*

Main category: cs.CL

> SPECTRA通过改写文本并选择评分相近的版本，实现对训练数据的可靠检测，甚至在占语料库极小比例的情况下也能检测到。相较于所有基线测试，SPECTRA实现了训练数据检测和未用于训练的数据检测之间九个数量级的p值差距。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型使用大量从互联网上抓取的文本进行训练，检测训练数据对于执行版权和数据许可规则至关重要。SPECTRA旨在通过水印方法让即使占训练语料库不到0.001%的训练数据也变得可检测到。

**Method:** SPECTRA采用大型语言模型（LLM）对文本进行改写，并通过一个单独的评分模型对每个改写版本的可能性进行评分。选择一个评分与原文相近的改写版本以避免引入分布偏移。检测嫌疑模型是否被水印数据训练过则是通过比较嫌疑模型的token概率与评分模型的一致性来进行的。

**Result:** SPECTRA在检测是否训练数据已用于训练方面，相对于所有测试的基线，实现了九个数量级的p值差距。

**Conclusion:** SPECTRA通过提供一种可扩展、发布前可部署的水印方式，即使在大规模LLM训练中，也使得数据所有者能够检测其数据是否被使用。

**Abstract:** Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.

</details>


### [5] [When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation](https://arxiv.org/abs/2512.17083)
*Michael H. Coen*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of prior work, evaluation practice in dialogue topic segmentation remains dominated by strict boundary matching and F1-based metrics, even as modern LLM-based conversational systems increasingly rely on segmentation to manage conversation history beyond the model's fixed context window, where unstructured context accumulation degrades efficiency and coherence.
  This paper introduces an evaluation objective for dialogue topic segmentation that treats boundary density and segment coherence as primary criteria, alongside window-tolerant F1 (W-F1). Through extensive cross-dataset empirical evaluation, we show that reported performance differences across dialogue segmentation benchmarks are driven not by model quality, but by annotation granularity mismatches and sparse boundary labels. This indicates that many reported improvements arise from evaluation artifacts rather than improved boundary detection.
  We evaluated multiple, structurally distinct dialogue segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Across these settings, we observe high segment coherence combined with extreme oversegmentation relative to sparse labels, producing misleadingly low exact-match F1 scores. We show that topic segmentation is best understood as selecting an appropriate granularity rather than predicting a single correct boundary set. We operationalize this view by explicitly separating boundary scoring from boundary selection.

</details>


### [6] [Data Augmentation Supporting a Conversational Agent Designed for Smoking Cessation Support Groups](https://arxiv.org/abs/2512.17092)
*Salar Hashemitaheri,Ian Harris*

Main category: cs.CL

> 通过合成和真实数据增强策略提升吸烟戒断在线支持群机器人对话效果。

<details>
  <summary>Details</summary>

**Motivation:** 提高低用户参与度和对自动对话代理的反应。

**Method:** 使用双级数据增强策略包括合成数据增强和真实数据增强，通过微调开源大型语言模型来识别低F1评分意图，并生成更多高质量的合成数据。

**Result:** 重新训练模型后，F1评分提升了32%。

**Conclusion:** 这种方法提供了一种可重复的框架来增强对话代理的性能。

**Abstract:** Online support groups for smoking cessation are economical and accessible, yet they often face challenges with low user engagement and stigma. The use of an automatic conversational agent would improve engagement by ensuring that all user comments receive a timely response.). We address the challenge of insufficient high-quality data by employing a two-level data augmentation strategy: synthetic data augmentation and real data augmentation. First, we fine-tuned an open source LLM to classify posts from our existing smoking cessation support groups and identify intents with low F1 (precision+recall) scores. Then, for these intents, we generate additional synthetic data using prompt engineering with the GPT model, with an average of 87\% of the generated synthetic posts deemed high quality by human annotators. Overall, the synthetic augmentation process resulted in 43\% of the original posts being selected for augmentation, followed by 140\% synthetic expansion of these posts. Additionally, we scraped more than 10,000 real posts from a related online support context, of which 73\% were validated as good quality by human annotators. Each synthetic or scraped post underwent rigorous validation involving human reviewers to ensure quality and relevance. The validated new data, combined with the original support group posts, formed an augmented dataset used to retrain the intent classifier. Performance evaluation of the retrained model demonstrated a 32\% improvement in F1, confirming the effectiveness of our data augmentation approach. Synthetic and real post augmentation led to similar performance improvements. This study provides a replicable framework for enhancing conversational agent performance in domains where data scarcity is a critical issue.

</details>


### [7] [Enhancing Long Document Long Form Summarisation with Self-Planning](https://arxiv.org/abs/2512.17179)
*Xiaotang Du,Rohit Saxena,Laura Perez-Beltrachini,Pasquale Minervini,Ivan Titov*

Main category: cs.CL

> 本文提出了一种基于突出引导生成的长上下文摘要新方法，强调了在长文档文档摘要中的优势，特别是在事实一致性、相关性和整体质量方面。

<details>
  <summary>Details</summary>

**Motivation:** 动机是解决现有方法在长上下文摘要中可追溯性和忠实性问题，通过引入句子级别的信息作为内容计划，提升生成摘要的质量。

**Method:** 方法是利用自我规划方法来识别关键内容，并基于规划生成摘要，试验了端到端和两阶段之间的方法，发现两阶段在信息密集的长文档中表现更优。

**Result:** 结果是该方法在长形式摘要数据集上的试验表明可以改善摘要的事实一致性，同时保持相关性和整体质量。在GovReport数据集上，最优方法ROUGE-L提升了4.1分，SummaC得分提升约为35%。

**Conclusion:** 结论是突出引导生成的方法在长文档摘要中表现更好，提高了事实一致性、相关性和整体质量，特别是在GovReport数据集上，取得了显著的性能提升。

**Abstract:** We introduce a novel approach for long context summarisation, highlight-guided generation, that leverages sentence-level information as a content plan to improve the traceability and faithfulness of generated summaries. Our framework applies self-planning methods to identify important content and then generates a summary conditioned on the plan. We explore both an end-to-end and two-stage variants of the approach, finding that the two-stage pipeline performs better on long and information-dense documents. Experiments on long-form summarisation datasets demonstrate that our method consistently improves factual consistency while preserving relevance and overall quality. On GovReport, our best approach has improved ROUGE-L by 4.1 points and achieves about 35% gains in SummaC scores. Qualitative analysis shows that highlight-guided summarisation helps preserve important details, leading to more accurate and insightful summaries across domains.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [V-Agent: An Interactive Video Search System Using Vision-Language Models](https://arxiv.org/abs/2512.16925)
*SunYoung Park,Jong-Hyeon Lee,Youngjune Kim,Daegyu Sung,Younghyun Yu,Young-rok Cha,Jeongho Ju*

Main category: cs.CV

> V-Agent, a new multi-agent platform for advanced video search and interactive user-system conversations, significantly enhances multimodal retrieval quality by integrating a fine-tuned vision-language model (VLM) and an efficient re-ranking module.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional text-based retrieval systems in multimodal environments, V-Agent is introduced to improve video search relevance and interactivity.

**Method:** The platform uses a VLM fine-tuned with a video preference dataset and an image-text retrieval model for multimodal representation. It also employs a collaborative structure of three agents for user intent processing and a re-ranking module to enhance retrieval quality.

**Result:** V-Agent showcases state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, underscoring its capability and potential for practical applications.

**Conclusion:** This research provides a robust framework for advanced multimodal video search and interaction by integrating improved multimodal representations and agent-based processing, demonstrating its practical utility and potential impact.

**Abstract:** We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.

</details>


### [9] [Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content](https://arxiv.org/abs/2512.16947)
*Reza Chandra,Adang Suhendra,Lintang Yuniar Banowosari,Prihandoko*

Main category: cs.CV

> 研究使用CNN和VGG-16模型来快速识别含有色情内容的网站。实验结果表明，在第五十轮迭代和0.001的学习率下，CNN模型在快速准确检测色情内容方面优于VGG-16模型，准确率达到了94.87%。

<details>
  <summary>Details</summary>

**Motivation:** 由于一些负面内容的网站即使被封锁，公众仍能通过VPN访问，因此研究旨在快速识别色情内容，开发出能够识别疑似含有色情图像内容网站的系统。

**Method:** 本研究采用了深度学习的方法，使用卷积神经网络（CNN）和视觉几何小组16（VGG-16）模型来识别和检测可能包含色情图像的网站。

**Result:** 实验结果表明，在第八次实验中，当轮数迭代为50且学习率为0.001时，CNN模型的测试结果最佳，准确率达0.9487或94.87%。

**Conclusion:** 研究结论指出，相较于VGG-16模型，CNN模型在快速且准确地检测色情内容方面更为有效。

**Abstract:** In 2020, a total of 59,741 websites were blocked by the Indonesian government due to containing negative content, including pornography, with 14,266 websites falling into this category. However, these blocked websites could still be accessed by the public using virtual private networks (VPNs). This prompted the research idea to quickly identify pornographic content. This study aims to develop a system capable of identifying websites suspected of containing pornographic image content, using a deep learning approach with convolutional neural network (CNN) and visual geometry group 16 (VGG-16) model. The two models were then explored comprehensively and holistically to determine which model was most effective in detecting pornographic content quickly. Based on the findings of the comparison between testing the CNN and VGG-16 models, research results showed that the best test results were obtained in the eighth experiment using the CNN model at an epoch value level of 50 and a learning rate of 0.001 of 0.9487 or 94.87%. This can be interpreted that the CNN model is more effective in detecting pornographic content quickly and accurately compared to using the VGG-16 model.

</details>


### [10] [AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals](https://arxiv.org/abs/2512.16948)
*Qi Xu,Shuai Gong,Xuming Ran,Haihua Luo,Yangfan Hu*

Main category: cs.CV

> 本文提出AVM，通过独立的子网络实现条件感知适应，保留冻结的Vision Transformer捕捉视觉特征，独立训练的路径负责不同刺激和个体的响应变化。实验结果表明AVM在预测准确性上优于最先进的模型，特别是在跨数据集适应情况下表现优越。

<details>
  <summary>Details</summary>

**Motivation:** 由于深度学习模型在模拟神经响应时往往无法明确区分稳定的视觉编码和条件特定的适应，这限制了它们在不同刺激和个体之间的泛化能力。

**Method:** 我们引入了自适应视觉模型（AVM），这是一个结构保持框架，通过独立的子网络实现条件感知适应，而不修改核心表示。AVM保留了一个冻结的Vision Transformer编码器来捕捉一致的视觉特征，同时独立训练的调制路径负责因刺激内容和受试者身份引起的神经响应变化。

**Result:** 在三个实验设置中评估了AVM，包括刺激级变化、跨受试者泛化和跨数据集适应，都涉及输入和个体的结构化变化。在两个大规模的小鼠V1数据集中，AVM在预测相关性方面比最先进的V1T模型高出约2%，特别是在跨数据集适应设置下，AVM解释方差(FEVE)提高了9.1%。

**Conclusion:** 结果表明，AVM为跨生物和实验条件的自适应神经建模提供了一个统一的框架，为在结构约束下提供了一个可扩展的解决方案，并为未来在神经科学和生物启发的AI系统中的皮层建模方法提供了启示。

**Abstract:** While deep learning models have shown strong performance in simulating neural responses, they often fail to clearly separate stable visual encoding from condition-specific adaptation, which limits their ability to generalize across stimuli and individuals. We introduce the Adaptive Visual Model (AVM), a structure-preserving framework that enables condition-aware adaptation through modular subnetworks, without modifying the core representation. AVM keeps a Vision Transformer-based encoder frozen to capture consistent visual features, while independently trained modulation paths account for neural response variations driven by stimulus content and subject identity. We evaluate AVM in three experimental settings, including stimulus-level variation, cross-subject generalization, and cross-dataset adaptation, all of which involve structured changes in inputs and individuals. Across two large-scale mouse V1 datasets, AVM outperforms the state-of-the-art V1T model by approximately 2% in predictive correlation, demonstrating robust generalization, interpretable condition-wise modulation, and high architectural efficiency. Specifically, AVM achieves a 9.1% improvement in explained variance (FEVE) under the cross-dataset adaptation setting. These results suggest that AVM provides a unified framework for adaptive neural modeling across biological and experimental conditions, offering a scalable solution under structural constraints. Its design may inform future approaches to cortical modeling in both neuroscience and biologically inspired AI systems.

</details>
