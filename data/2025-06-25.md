<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.CV](#cs.CV) [Total: 37]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection](https://arxiv.org/abs/2506.18919)
*Hexiang Gu,Qifan Yu,Saihui Hou,Zhiqin Fang,Huijia Wu,Zhaofeng He*

Main category: cs.CL

> 本文引入了MemeMind数据集，填补了现有有害模因检测研究中的空白，并提出了一种新的检测框架MemeGuard，能在有害模因检测任务中显著优于现有的最先进的方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有研究虽然在检测准确性和可解释性方面取得了进展，但由于缺乏系统性、大规模、多样性和高度解释性的数据集，进一步的发展受到阻碍。为了填补这一空白，作者引入了MemeMind数据集。

**Method:** 提出了一个创新的检测框架MemeGuard，该框架有效整合了多模态信息与推理过程建模，显著提高了模型对有害模因的理解和识别能力。

**Result:** 在MemeMind数据集上的广泛实验证明，MemeGuard在有害模因检测任务中始终超越现有的最先进的方法。

**Conclusion:** MemeMind数据集为有害模因检测提供了坚实的基础，而MemeGuard框架通过整合多模态信息与推理过程建模提高了对有害模因的检测能力。这些努力显著改善了有害模因的自动检测。

**Abstract:** The rapid development of social media has intensified the spread of harmful
content. Harmful memes, which integrate both images and text, pose significant
challenges for automated detection due to their implicit semantics and complex
multimodal interactions. Although existing research has made progress in
detection accuracy and interpretability, the lack of a systematic, large-scale,
diverse, and highly explainable dataset continues to hinder further advancement
in this field. To address this gap, we introduce MemeMind, a novel dataset
featuring scientifically rigorous standards, large scale, diversity, bilingual
support (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.
MemeMind fills critical gaps in current datasets by offering comprehensive
labeling and explicit reasoning traces, thereby providing a solid foundation
for enhancing harmful meme detection. In addition, we propose an innovative
detection framework, MemeGuard, which effectively integrates multimodal
information with reasoning process modeling, significantly improving models'
ability to understand and identify harmful memes. Extensive experiments
conducted on the MemeMind dataset demonstrate that MemeGuard consistently
outperforms existing state-of-the-art methods in harmful meme detection tasks.

</details>


### [2] [Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge](https://arxiv.org/abs/2506.18998)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

> 研究揭示了LLMs过度依赖记忆而不是推理的问题，这导致了自我认知的不一致，并表明当前架构和训练模式存在缺陷。

<details>
  <summary>Details</summary>

**Motivation:** 当前研究将LLMs的记忆问题和自我知识缺陷视为独立问题，没有意识到这两种问题之间有交织的联系，这会降低LLMs响应的信任度。作者动机在于揭示这一尚未被充分认识的联系。

**Method:** 采用了一种新框架来确定LLMs是真正从训练数据中学习推理模式，还是仅仅记忆它们以在类似复杂度的STEM领域问题上表现得像是有竞争力的。

**Result:** 研究揭示了显著的泛化问题：LLMs从记忆的解决方案中获得信心，假设自己对推理能力有更高的自我认知，进而导致面对逻辑一致的任务扰动时，可行性的评估不一致超过45%。这一现象在科学和医学领域尤为明显。

**Conclusion:** 这项研究表明，当前的AI架构和训练模式存在着缺陷，需要更有保障的方法来确保模型感知自身知识的一致性和可靠性，以提高AI解释能力和信任度。

**Abstract:** When artificial intelligence mistakes memorization for intelligence, it
creates a dangerous mirage of reasoning. Existing studies treat memorization
and self-knowledge deficits in LLMs as separate issues and do not recognize an
intertwining link that degrades the trustworthiness of LLM responses. In our
study, we utilize a novel framework to ascertain if LLMs genuinely learn
reasoning patterns from training data or merely memorize them to assume
competence across problems of similar complexity focused on STEM domains. Our
analysis shows a noteworthy problem in generalization: LLMs draw confidence
from memorized solutions to infer a higher self-knowledge about their reasoning
ability, which manifests as an over 45% inconsistency in feasibility
assessments when faced with self-validated, logically coherent task
perturbations. This effect is most pronounced in science and medicine domains,
which tend to have maximal standardized jargon and problems, further confirming
our approach. Significant wavering within the self-knowledge of LLMs also shows
flaws in current architectures and training patterns, highlighting the need for
techniques that ensure a balanced, consistent stance on models' perceptions of
their own knowledge for maximum AI explainability and trustworthiness. Our code
and results are available publicly at
https://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.

</details>


### [3] [Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations](https://arxiv.org/abs/2506.19004)
*Brian Siyuan Zheng,Alisa Liu,Orevaoghene Ahia,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

> 研究表明，在未见过的非规范分词情况下，指令调整过的语言模型依然保持了较高的性能，并通过特定分词策略能进一步改善特定类型任务的性能。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于探究语言模型面对未训练过的非规范分词时的鲁棒性，并验证通过改变分词方式在推理时提高模型性能的可能性。

**Method:** 该研究通过实验评估了在未见过的非规范分词方法下语言模型（LMs）的鲁棒性，包括随机采样分词和字符级分词。

**Result:** 研究结果显示，指令调整后的模型在面对随机采样分词时还能保留高达93.4%的原有性能，字符级分词时则是90.8%。此外，发现字符级分词能提高字符串操作和代码理解任务的性能达14%，而右对齐数字分组能提高大数运算的性能达33%。

**Conclusion:** 该研究表明模型的性能并不像以前认为的那样依赖于特定的分词器，通过在推理时干预分词，有机会提升模型性能。

**Abstract:** Modern tokenizers employ deterministic algorithms to map text into a single
"canonical" token sequence, yet the same string can be encoded as many
non-canonical tokenizations using the tokenizer vocabulary. In this work, we
investigate the robustness of LMs to text encoded with non-canonical
tokenizations entirely unseen during training. Surprisingly, when evaluated
across 20 benchmarks, we find that instruction-tuned models retain up to 93.4%
of their original performance when given a randomly sampled tokenization, and
90.8% with character-level tokenization. We see that overall stronger models
tend to be more robust, and robustness diminishes as the tokenization departs
farther from the canonical form. Motivated by these results, we then identify
settings where non-canonical tokenization schemes can *improve* performance,
finding that character-level segmentation improves string manipulation and code
understanding tasks by up to +14%, and right-aligned digit grouping enhances
large-number arithmetic by +33%. Finally, we investigate the source of this
robustness, finding that it arises in the instruction-tuning phase. We show
that while both base and post-trained models grasp the semantics of
non-canonical tokenizations (perceiving them as containing misspellings), base
models try to mimic the imagined mistakes and degenerate into nonsensical
output, while post-trained models are committed to fluent responses. Overall,
our findings suggest that models are less tied to their tokenizer than
previously believed, and demonstrate the promise of intervening on tokenization
at inference time to boost performance.

</details>


### [4] [Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective](https://arxiv.org/abs/2506.19028)
*Weijie Xu,Yiwen Wang,Chi Xue,Xiangkun Hu,Xi Fang,Guimin Dong,Chandan K. Reddy*

Main category: cs.CL

> FiSCo is a new statistical framework to detect subtle biases in long-form responses from LLMs, outperforming previous methods by analyzing semantic differences at the claim level and reducing the effect of model variability.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind FiSCo is to tackle the inherent biases in LLM-generated responses, which are insufficiently addressed by existing evaluation methods. These methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs.

**Method:** FiSCo(Fine-grained Semantic Computation) is described as a novel statistical framework designed to evaluate group-level fairness in Large Language Models (LLMs) by detecting subtle semantic differences in long-form responses across demographic groups. It decomposes model outputs into semantically distinct claims and applies statistical hypothesis testing to compare inter- and intra-group similarities.

**Result:** Experiments demonstrate that FiSCo reliably identifies nuanced biases while minimizing the impact of stochastic variability in LLM outputs, outperforming existing evaluation metrics.

**Conclusion:** The conclusion drawn is that FiSCo is a robust method for detecting biases in LLM-generated long-form responses, providing a significant improvement over previous evaluation methods.

**Abstract:** Large Language Models (LLMs) often generate responses with inherent biases,
undermining their reliability in real-world applications. Existing evaluation
methods often overlook biases in long-form responses and the intrinsic
variability of LLM outputs. To address these challenges, we propose
FiSCo(Fine-grained Semantic Computation), a novel statistical framework to
evaluate group-level fairness in LLMs by detecting subtle semantic differences
in long-form responses across demographic groups. Unlike prior work focusing on
sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis
by operating at the claim level, leveraging entailment checks to assess the
consistency of meaning across responses. We decompose model outputs into
semantically distinct claims and apply statistical hypothesis testing to
compare inter- and intra-group similarities, enabling robust detection of
subtle biases. We formalize a new group counterfactual fairness definition and
validate FiSCo on both synthetic and human-annotated datasets spanning gender,
race, and age. Experiments show that FiSco more reliably identifies nuanced
biases while reducing the impact of stochastic LLM variability, outperforming
various evaluation metrics.

</details>


### [5] [Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models](https://arxiv.org/abs/2506.19037)
*Omer Luxembourg,Haim Permuter,Eliya Nachmani*

Main category: cs.CL

> 文章提出了DUS方法，一种优化非自回归文本生成过程中并行去掩码策略的新方法，相比现有方法提高了生成速度和质量，特别是在数学和代码完成任务中表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在解决现有的非自回归文本生成模型在并行去掩码过程中的不足，这些不足在于忽略了词之间的成对交互作用，不能考虑到同时去掩码多个位置时的依赖关系。

**Method:** DUS方法采用了一种推理专用的、无需额外训练的计划器模型，该方法基于一阶马尔可夫假设，将序列位置划分为不相邻的分组，以实现独立并行的去掩码步骤，减少denoiser的调用次数，从而提高生成速度。

**Result:** 在数学和代码完成基准测试中，DUS方法的表现优于基于并行置信度的计划器方法，并且没有修改底层的denoiser。

**Conclusion:** DUS作为一种轻量级、预算感知的高效高质量文本生成方法，为释放MDLM模型的真实能力奠定了基础。

**Abstract:** Masked diffusion language models (MDLM) have shown strong promise for
non-autoregressive text generation, yet existing samplers act as implicit
planners, selecting tokens to unmask via denoiser confidence or entropy scores.
Such heuristics falter under parallel unmasking - they ignore pairwise
interactions between tokens and cannot account for dependencies when unmasking
multiple positions at once, limiting their inference time to traditional
auto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking
Strategy (DUS), an inference-only, planner-model-free method that requires no
additional training. DUS leverages a first-order Markov assumption to partition
sequence positions into dilation-based groups of non-adjacent tokens, enabling
independent, parallel unmasking steps that respect local context that minimizes
the joint entropy of each iteration step. Unlike semi-AR block approaches
(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces
the number of denoiser calls to O(log B) per generation block - yielding
substantial speedup over the O(B) run time of state-of-the-art diffusion
models, where B is the block size in the semi-AR inference process. In
experiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -
domains suited to non-ordinal generation - DUS improves scores over parallel
confidence-based planner, without modifying the underlying denoiser. DUS offers
a lightweight, budget-aware approach to efficient, high-quality text
generation, paving the way to unlock the true capabilities of MDLMs.

</details>


### [6] [NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching](https://arxiv.org/abs/2506.19058)
*Mike Zhang,Rob van der Goot*

Main category: cs.CL

> 本文描述了NLPnorth在TalentCLEF 2025中的提交，包括多语言职位匹配和基于职位的技能预测，其中采用了各种NLP技术，并展示了大型多语言模型在两项任务中的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 研究多语言职位匹配和基于职位的技能预测任务，旨在改进自动候选人匹配、职业路径预测和就业市场分析。

**Method:** 该研究采用了分类方法、对比方法和提示方法，特别是对于任务A，提示方法表现出色，而对于任务B，则是微调过的分类方法表现较好。

**Result:** 在任务A中，提示方法的平均精确度（MAP）为0.492；在任务B中，微调分类方法的MAP为0.290。另外，还使用了来自ESCO的额外数据以增强模型性能。

**Conclusion:** 大型多语言语言模型在这两项任务中表现最佳。根据中期结果，单独计算的独特团队排名中，任务A排名第5/20，任务B排名第3/14。

**Abstract:** Matching job titles is a highly relevant task in the computational job market
domain, as it improves e.g., automatic candidate matching, career path
prediction, and job market analysis. Furthermore, aligning job titles to job
skills can be considered an extension to this task, with similar relevance for
the same downstream tasks. In this report, we outline NLPnorth's submission to
TalentCLEF 2025, which includes both of these tasks: Multilingual Job Title
Matching, and Job Title-Based Skill Prediction. For both tasks we compare
(fine-tuned) classification-based, (fine-tuned) contrastive-based, and
prompting methods. We observe that for Task A, our prompting approach performs
best with an average of 0.492 mean average precision (MAP) on test data,
averaged over English, Spanish, and German. For Task B, we obtain an MAP of
0.290 on test data with our fine-tuned classification-based approach.
Additionally, we made use of extra data by pulling all the language-specific
titles and corresponding \emph{descriptions} from ESCO for each job and skill.
Overall, we find that the largest multilingual language models perform best for
both tasks. Per the provisional results and only counting the unique teams, the
ranking on Task A is 5$^{\text{th}}$/20 and for Task B 3$^{\text{rd}}$/14.

</details>


### [7] [MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation](https://arxiv.org/abs/2506.19073)
*Jackson Trager,Francielle Vargas,Diego Alves,Matteo Guida,Mikel K. Ngueajio,Ameeta Agrawal,Flor Plaza-del-Arco,Yalda Daryanai,Farzan Karimi-Malekabadi*

Main category: cs.CL

> 本文提出MFTCXplain，一个多语言数据集，用于评测LLMs在基于Moral Foundation Theory的多跳解释性道德推理方面的表现，发现虽然LLMs在仇恨言论检测上表现良好，但在道德推理尤其是非主流语言上的表现较弱。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决现有评估基准在评价LLMs的道德推理能力时存在的两大不足：缺乏对道德分类的注释解释，这限制了透明度和可解释性；以及主要聚焦于英语，限制了跨文化背景下的道德推理评价。

**Method:** 本研究提出了一种多语言基准数据集MFTCXplain，用于通过基于道德基础理论（MFT）的仇恨言论多跳解释来评估大型语言模型（LLMs）的道德推理能力。数据集包括葡萄牙语、意大利语、波斯语和英语的3000条推文，并注释有二元仇恨言论标签、道德类别和文本片段级别的解释。

**Result:** 实证结果显示，在道德推理任务中，LLMs的输出与人类注释之间存在一定程度上的不一致。LLMs在仇恨言论检测中表现良好（最高F1值达0.836），但在预测道德情绪方面能力较弱（F1值低于0.35）。此外，在代表性较低的语言中，解释的对齐度依然有限。

**Conclusion:** 研究发现目前的LLMs在内化和反映人类道德推理方面的能力有限。

**Abstract:** Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is
a growing concern as these systems are used in socially sensitive tasks.
Nevertheless, current evaluation benchmarks present two major shortcomings: a
lack of annotations that justify moral classifications, which limits
transparency and interpretability; and a predominant focus on English, which
constrains the assessment of moral reasoning across diverse cultural settings.
In this paper, we introduce MFTCXplain, a multilingual benchmark dataset for
evaluating the moral reasoning of LLMs via hate speech multi-hop explanation
using Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across
Portuguese, Italian, Persian, and English, annotated with binary hate speech
labels, moral categories, and text span-level rationales. Empirical results
highlight a misalignment between LLM outputs and human annotations in moral
reasoning tasks. While LLMs perform well in hate speech detection (F1 up to
0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).
Furthermore, rationale alignment remains limited mainly in underrepresented
languages. These findings show the limited capacity of current LLMs to
internalize and reflect human moral reasoning.

</details>


### [8] [Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting](https://arxiv.org/abs/2506.19089)
*Nathaniel Getachew,Abulhair Saparov*

Main category: cs.CL

> 本文介绍了一个用于评估大型语言模型ToM和WM能力的可编程框架\texttt{StorySim}，实验表明该框架能揭示模型在不同任务上的表现差异及其行为偏差。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型的评估通常受到之前基准测试使用被预训练数据污染的故事的限制。为了精确评估这些模型的ToM和WM能力，需要一个能生成可控和新颖故事的框架。

**Method:** 我们提出了一个名为\texttt{StorySim}的可编程框架，用于生成故事以评估大型语言模型的“心灵理论”(ToM)和世界观建模(WM)能力。通过一个高度可控的\texttt{Storyboard}，该框架可以精确操控角色视角和事件，从而避免之前基准测试中的预训练数据污染问题。

**Result:** 实验表明，大多数模型在WM任务上的表现优于ToM任务，并且在处理人类对象时的推理能力优于无生命对象。此外，模型在处理故事时存在偏差，比如近因偏差和过度依赖故事早期情节的现象。

**Conclusion:** 通过\texttt{StorySim}框架，研究者和开发者可以更好地理解大型语言模型在理解和预测复杂社会情境中的表现，并为未来发展提供了方向。

**Abstract:** We introduce $\texttt{StorySim}$, a programmable framework for synthetically
generating stories to evaluate the theory of mind (ToM) and world modeling (WM)
capabilities of large language models (LLMs). Unlike prior benchmarks that may
suffer from contamination in pretraining data, $\texttt{StorySim}$ produces
novel, compositional story prompts anchored by a highly controllable
$\texttt{Storyboard}$, enabling precise manipulation of character perspectives
and events. We use this framework to design first- and second-order ToM tasks
alongside WM tasks that control for the ability to track and model mental
states. Our experiments across a suite of state-of-the-art LLMs reveal that
most models perform better on WM tasks than ToM tasks, and that models tend to
perform better reasoning with humans compared to inanimate objects.
Additionally, our framework enabled us to find evidence of heuristic behavior
such as recency bias and an over-reliance on earlier events in the story. All
code for generating data and evaluations is freely available.

</details>


### [9] [Human-Aligned Faithfulness in Toxicity Explanations of LLMs](https://arxiv.org/abs/2506.19113)
*Ramaravind K. Mothilal,Joanna Roy,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

> 研究提出了一种新的评估标准HAF来衡量大语言模型(LLMs)生成的毒性问题解释与理想条件下的人类解释的一致性，并通过实验表明，LLMs在处理复杂毒性问题推理时存在局限性。

<details>
  <summary>Details</summary>

**Motivation:** 讨论围绕着NLP中的大语言模型(LLMs)的毒性问题大多集中在检测任务上，本研究将关注点转向评估LLMs关于毒性的推理，特别是它们解释支持一种立场的方式，以提高它们在下游任务中的可信度。

**Method:** 提出了一种新的理论基础的多维标准Human-Aligned Faithfulness (HAF)，用于度量大语言模型(LLMs)生成的关于毒性问题的自由形式解释与理想条件下的人类解释的一致性。开发了六个基于不确定性量化指标来全面评估LLMs的毒性解释的可信度，这些指标无需人类参与，可以衡量解释的“非理想程度”。

**Result:** 在三个Llama模型（最大规模70B）和一个8B Ministral模型上，使用五个不同的毒性数据集进行实验，结果显示，虽然LLMs可以生成对简单提示的合理解释，但它们在处理复杂的、有关毒性问题的相关因素时，推理能力会下降，可能导致不一致和不合逻辑的回应。

**Conclusion:** 研究揭示了大语言模型在处理复杂和多维度的毒性问题推理时的局限性，即使在生成看似合理的解释时，也可能出现推理断裂，产生不一致和不合理的回应。

**Abstract:** The discourse around toxicity and LLMs in NLP largely revolves around
detection tasks. This work shifts the focus to evaluating LLMs' reasoning about
toxicity -- from their explanations that justify a stance -- to enhance their
trustworthiness in downstream tasks. Despite extensive research on
explainability, it is not straightforward to adopt existing methods to evaluate
free-form toxicity explanation due to their over-reliance on input text
perturbations, among other challenges. To account for these, we propose a
novel, theoretically-grounded multi-dimensional criterion, Human-Aligned
Faithfulness (HAF), that measures the extent to which LLMs' free-form toxicity
explanations align with those of a rational human under ideal conditions. We
develop six metrics, based on uncertainty quantification, to comprehensively
evaluate \haf of LLMs' toxicity explanations with no human involvement, and
highlight how "non-ideal" the explanations are. We conduct several experiments
on three Llama models (of size up to 70B) and an 8B Ministral model on five
diverse toxicity datasets. Our results show that while LLMs generate plausible
explanations to simple prompts, their reasoning about toxicity breaks down when
prompted about the nuanced relations between the complete set of reasons, the
individual reasons, and their toxicity stances, resulting in inconsistent and
nonsensical responses. We open-source our code and LLM-generated explanations
at https://github.com/uofthcdslab/HAF.

</details>


### [10] [Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data](https://arxiv.org/abs/2506.19159)
*Yun Tang,Eesung Kim,Vijendra Raj Apsingekar*

Main category: cs.CL

> A hybrid TAED model, trained jointly with speech and text, enhances ASR accuracy and reduces WER across various datasets.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to leverage large amounts of text corpus and enhance ASR accuracy.

**Method:** A joint speech and text optimization method is proposed for hybrid transducer and attention-based encoder decoder (TAED) modeling.

**Result:** The J-TAED reduces WER by 5.8 ~12.8% on the Librispeech dataset and 15.3% and 17.8% WER reduction on out-of-domain datasets.

**Conclusion:** The proposed method unifies internal representations from different modalities and can be extended to text-based domain adaptation with significant WER reduction.

**Abstract:** A joint speech and text optimization method is proposed for hybrid transducer
and attention-based encoder decoder (TAED) modeling to leverage large amounts
of text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained
with both speech and text input modalities together, while it only takes speech
data as input during inference. The trained model can unify the internal
representations from different modalities, and be further extended to
text-based domain adaptation. It can effectively alleviate data scarcity for
mismatch domain tasks since no speech data is required. Our experiments show
J-TAED successfully integrates speech and linguistic information into one
model, and reduce the WER by 5.8 ~12.8% on the Librispeech dataset. The model
is also evaluated on two out-of-domain datasets: one is finance and another is
named entity focused. The text-based domain adaptation brings 15.3% and 17.8%
WER reduction on those two datasets respectively.

</details>


### [11] [Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages](https://arxiv.org/abs/2506.19187)
*Christopher Toukmaji,Jeffrey Flanigan*

Main category: cs.CL

> 本文进行了一场大规模的关于低资源语言上下文学习的研究，比较了不同的适应技术，并提出了一个新的度量标准来解释模型训练中的性能下降问题。

<details>
  <summary>Details</summary>

**Motivation:** 尽管有大量关于提示设置的工作，但仍不清楚LLMs应如何跨语言适应低资源目标语言的上下文学习。本文旨在解决这一问题。

**Method:** 本文通过全面研究跨越五种目标语言、三种基础语言模型和七个下游任务，使用4100个GPU训练小时（超过9900 TFLOPs），比较了几种跨语言适应技术：少样本提示、翻译测试、微调、嵌入重新初始化和指令微调。

**Result:** 研究结果表明，少样本提示和翻译测试设置明显优于基于梯度的适应方法。本文还提出了一种新的度量标准VOR，分析模型输出表明训练模型的性能下降主要是由灾难性遗忘引起的。

**Conclusion:** 这是首次在低资源语言的上下文学习研究中涉及如此多的适应技术以及如此高的训练计算量，所有数据集和训练模型都将公开供使用。

**Abstract:** LLMs are typically trained in high-resource languages, and tasks in
lower-resourced languages tend to underperform the higher-resource language
counterparts for in-context learning. Despite the large body of work on
prompting settings, it is still unclear how LLMs should be adapted
cross-lingually specifically for in-context learning in the low-resource target
languages. We perform a comprehensive study spanning five diverse target
languages, three base LLMs, and seven downstream tasks spanning over 4,100 GPU
training hours (9,900+ TFLOPs) across various adaptation techniques: few-shot
prompting, translate-test, fine-tuning, embedding re-initialization, and
instruction fine-tuning. Our results show that the few-shot prompting and
translate-test settings tend to heavily outperform the gradient-based
adaptation methods. To better understand this discrepancy, we design a novel
metric, Valid Output Recall (VOR), and analyze model outputs to empirically
attribute the degradation of these trained models to catastrophic forgetting.
To the extent of our knowledge, this is the largest study done on in-context
learning for low-resource languages with respect to train compute and number of
adaptation techniques considered. We make all our datasets and trained models
available for public use.

</details>


### [12] [Augmenting Multi-Agent Communication with State Delta Trajectory](https://arxiv.org/abs/2506.19209)
*Yichen Tang,Weihang Su,Yujia Zhou,Yiqun Liu,Min Zhang,Shaoping Ma,Qingyao Ai*

Main category: cs.CL

> 研究提出了一种增强大型语言模型多智能体系统性能的新通信协议，通过传输自然语言标记加上状态转换轨迹来减少信息损失，并在实验中达到了最先进性能。

<details>
  <summary>Details</summary>

**Motivation:** 为解决现有LLM-based多智能体系统通过自然语言进行通信时不可避免的信息损失问题，特别是在需要传递复杂的推理逻辑或抽象思维时损失尤其显著。

**Method:** 我们提出了一种新的通信协议，该协议传输自然语言标记和标记级别的状态转换轨迹，以改进基于大型语言模型（LLM）的多智能体系统之间的通信。我们发现了LLMs在生成每个标记后的状态变化序列可以更好地反映推理过程中的隐藏信息，因此提出了状态增量编码（SDE）方法来表示状态转换轨迹。

**Result:** 实验结果显示，采用SDE的多智能体系统在与其他通信协议比较时，特别是涉及复杂的推理任务时，可以达到最先进（SOTA）的性能。

**Conclusion:** 这表明基于LLM的多智能体系统可以通过通信增强来提高性能，特别是在需要处理复杂推理时。

**Abstract:** Multi-agent techniques such as role playing or multi-turn debates have been
shown to be effective in improving the performance of large language models
(LLMs) in downstream tasks. Despite their differences in workflows, existing
LLM-based multi-agent systems mostly use natural language for agent
communication. While this is appealing for its simplicity and interpretability,
it also introduces inevitable information loss as one model must down sample
its continuous state vectors to concrete tokens before transferring them to the
other model. Such losses are particularly significant when the information to
transfer is not simple facts, but reasoning logics or abstractive thoughts. To
tackle this problem, we propose a new communication protocol that transfers
both natural language tokens and token-wise state transition trajectory from
one agent to another. Particularly, compared to the actual state value, we find
that the sequence of state changes in LLMs after generating each token can
better reflect the information hidden behind the inference process, so we
propose a State Delta Encoding (SDE) method to represent state transition
trajectories. The experimental results show that multi-agent systems with SDE
achieve SOTA performance compared to other communication protocols,
particularly in tasks that involve complex reasoning. This shows the potential
of communication augmentation for LLM-based multi-agent systems.

</details>


### [13] [Personality Prediction from Life Stories using Language Models](https://arxiv.org/abs/2506.19258)
*Rasiq Hussain,Jerry Ma,Rithik Khandelwal,Joshua Oltmanns,Mehak Gupta*

Main category: cs.CL

> 研究提出了一种结合滑动窗口微调预训练语言模型提取上下文嵌入和使用具有注意力机制的循环神经网络整合长途依赖性的两步方法，用于预测来自长篇叙述性访谈文本的五大人格特征，展示了预测准确率、效率和可解释性的提升。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于利用自然语言处理（NLP）提供的新途径，通过对长篇叙述性访谈文本的分析来评估人格特质，超越了传统的问卷形式，主要解决长文本处理中的挑战。

**Method:** 采用两步方法：首先使用滑动窗口微调预训练语言模型来提取上下文嵌入；然后应用具有注意力机制的循环神经网络（RNNs）来整合长途依赖性并提高解释性。

**Result:** 通过对模型进行消融研究并与当前最先进的长文本处理模型LLaMA和Longformer进行比较，展示了在预测准确率、效率和解释性上的改进。

**Conclusion:** 研究结果强调了将基于语言的特征与长文本建模相结合以从生命叙述中推进人格评估的潜力。

**Abstract:** Natural Language Processing (NLP) offers new avenues for personality
assessment by leveraging rich, open-ended text, moving beyond traditional
questionnaires. In this study, we address the challenge of modeling long
narrative interview where each exceeds 2000 tokens so as to predict Five-Factor
Model (FFM) personality traits. We propose a two-step approach: first, we
extract contextual embeddings using sliding-window fine-tuning of pretrained
language models; then, we apply Recurrent Neural Networks (RNNs) with attention
mechanisms to integrate long-range dependencies and enhance interpretability.
This hybrid method effectively bridges the strengths of pretrained transformers
and sequence modeling to handle long-context data. Through ablation studies and
comparisons with state-of-the-art long-context models such as LLaMA and
Longformer, we demonstrate improvements in prediction accuracy, efficiency, and
interpretability. Our results highlight the potential of combining
language-based features with long-context modeling to advance personality
assessment from life narratives.

</details>


### [14] [What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning](https://arxiv.org/abs/2506.19262)
*Yuchang Zhu,Zhonghua zhen,Qunshu Lin,Haotong Wei,Xiaolong Sun,Zixuan Yu,Minghao Liu,Zibin Zheng,Liang Chen*

Main category: cs.CL

> 本文研究发现，适度多样性的LLM生成数据在标注数据不足的情境中可以改善模型性能，而高度多样性的生成数据则有负面影响。

<details>
  <summary>Details</summary>

**Motivation:** 尽管有关LLM生成数据的研究广泛存在，但这些研究往往忽视了数据多样性这一关键因素的重要性。本文旨在探讨LLM生成数据的多样性对下游模型性能的影响。

**Method:** 通过探索LLM生成数据的不同多样性水平对下游模型性能的影响，以及研究在合成数据中混合不同比例的LLM生成数据的模型性能。

**Result:** 实验显示，具有适度多样性的由LLM生成的数据可以提高在标注数据不足的情况下训练的模型的性能，而高度多样性的生成数据则会对模型性能产生负面影响。

**Conclusion:** 实验结果表明，在分布偏移最小的情况下，适度多样性的LLM生成数据可以提升模型性能，尤其是在标注数据不足的情境中，而高度多样性的生成数据有负面作用。

**Abstract:** With the remarkable generative capabilities of large language models (LLMs),
using LLM-generated data to train downstream models has emerged as a promising
approach to mitigate data scarcity in specific domains and reduce
time-consuming annotations. However, recent studies have highlighted a critical
issue: iterative training on self-generated data results in model collapse,
where model performance degrades over time. Despite extensive research on the
implications of LLM-generated data, these works often neglect the importance of
data diversity, a key factor in data quality. In this work, we aim to
understand the implications of the diversity of LLM-generated data on
downstream model performance. Specifically, we explore how varying levels of
diversity in LLM-generated data affect downstream model performance.
Additionally, we investigate the performance of models trained on data that
mixes different proportions of LLM-generated data, which we refer to as
synthetic data. Our experimental results show that, with minimal distribution
shift, moderately diverse LLM-generated data can enhance model performance in
scenarios with insufficient labeled data, whereas highly diverse generated data
has a negative impact. We hope our empirical findings will offer valuable
guidance for future studies on LLMs as data generators.

</details>


### [15] [EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition](https://arxiv.org/abs/2506.19279)
*Zhiyang Qi,Keiko Takamizo,Mariko Ukiyo,Michimasa Inaba*

Main category: cs.CL

> EmoStage is a framework designed to improve the empathetic response generation of open-source language models in AI-driven counseling systems by inferring clients' psychological states and aligning with appropriate counseling stages.

<details>
  <summary>Details</summary>

**Motivation:** The increasing demand for mental health care and the limitations of current AI-driven counseling systems, such as the lack of understanding of clients' psychological states, the reliance on high-quality training data, and privacy issues when deploying commercially, motivated the development of EmoStage.

**Method:** EmoStage employs perspective-taking to infer psychological states, supports clients' needs, and integrates phase recognition to ensure responses align with the counseling process and are contextually appropriate.

**Result:** Experiments in Japanese and Chinese counseling contexts showed EmoStage enhancing the quality of responses by base models and performing competitively with data-driven methods.

**Conclusion:** The framework effectively enhances the capability of AI-driven counseling by improving the relevance and emotional resonance of responses without the need for additional training data, thus addressing key challenges in the field.

**Abstract:** The rising demand for mental health care has fueled interest in AI-driven
counseling systems. While large language models (LLMs) offer significant
potential, current approaches face challenges, including limited understanding
of clients' psychological states and counseling stages, reliance on
high-quality training data, and privacy concerns associated with commercial
deployment. To address these issues, we propose EmoStage, a framework that
enhances empathetic response generation by leveraging the inference
capabilities of open-source LLMs without additional training data. Our
framework introduces perspective-taking to infer clients' psychological states
and support needs, enabling the generation of emotionally resonant responses.
In addition, phase recognition is incorporated to ensure alignment with the
counseling process and to prevent contextually inappropriate or inopportune
responses. Experiments conducted in both Japanese and Chinese counseling
settings demonstrate that EmoStage improves the quality of responses generated
by base models and performs competitively with data-driven methods.

</details>


### [16] [JCAPT: A Joint Modeling Approach for CAPT](https://arxiv.org/abs/2506.19315)
*Tzu-Hsuan Yang,Yue-Yang He,Berlin Chen*

Main category: cs.CL

> 研究展示了如何使用Mamba选择性状态空间模型，结合音系特征和think token策略，提升计算机辅助发音训练系统中的自动发音评估和发音错误检测与诊断功能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索如何利用先进的模型和策略来增强计算机辅助发音训练系统的自动发音评估和发音错误检测与诊断功能。

**Method:** 该研究采用了Mamba选择性状态空间模型，并整合了音系特征和think token策略，旨在提高APA和MDD的性能。

**Result:** 该研究提出了一个统一框架，使用Mamba选择性状态空间模型，结合音系特征和think token策略来共同提升自动发音评估（APA）和发音错误检测与诊断（MDD）的可解释性和细粒度时间推理。这是首个将音系归因、基于SSM的建模和提示机制结合在计算机辅助发音训练（CAPT）中的研究。在speechocean762基准测试中的多个实验表明，该模型在APA和MDD任务上均表现出色，尤其在MDD任务上有显著提升。

**Conclusion:** 该研究证明了所提出的框架在自动发音评估和发音错误检测与诊断任务中的有效性，特别是在MDD任务上优于之前的模型。

**Abstract:** Effective pronunciation feedback is critical in second language (L2)
learning, for which computer-assisted pronunciation training (CAPT) systems
often encompass two key tasks: automatic pronunciation assessment (APA) and
mispronunciation detection and diagnosis (MDD). Recent work has shown that
joint modeling of these two tasks can yield mutual benefits. Our unified
framework leverages Mamba, a selective state space model (SSM), while
integrating phonological features and think token strategies to jointly enhance
interpretability and fine-grained temporal reasoning in APA and MDD. To our
knowledge, this is the first study to combine phonological attribution,
SSM-based modeling, and prompting in CAPT. A series of experiments conducted on
the speechocean762 benchmark demonstrate that our model consistently
outperforms prior methods, particularly on the MDD task.

</details>


### [17] [Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation](https://arxiv.org/abs/2506.19352)
*Jisu Shin,Juhyun Oh,Eunsu Kim,Hoyun Song,Alice Oh*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Ensuring persona fidelity in large language models (LLMs) is essential for
maintaining coherent and engaging human-AI interactions. However, LLMs often
exhibit Out-of-Character (OOC) behavior, where generated responses deviate from
an assigned persona, leading to inconsistencies that affect model reliability.
Existing evaluation methods typically assign single scores to entire responses,
struggling to capture subtle persona misalignment, particularly in long-form
text generation. To address this limitation, we propose an atomic-level
evaluation framework that quantifies persona fidelity at a finer granularity.
Our three key metrics measure the degree of persona alignment and consistency
within and across generations. Our approach enables a more precise and
realistic assessment of persona fidelity by identifying subtle deviations that
real users would encounter. Through our experiments, we demonstrate that our
framework effectively detects persona inconsistencies that prior methods
overlook. By analyzing persona fidelity across diverse tasks and personality
types, we reveal how task structure and persona desirability influence model
adaptability, highlighting challenges in maintaining consistent persona
expression.

</details>


### [18] [Measuring and Guiding Monosemanticity](https://arxiv.org/abs/2506.19382)
*Ruben Härle,Felix Friedrich,Manuel Brack,Stephan Wäldchen,Björn Deiseroth,Patrick Schramowski,Kristian Kersting*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** There is growing interest in leveraging mechanistic interpretability and
controllability to better understand and influence the internal dynamics of
large language models (LLMs). However, current methods face fundamental
challenges in reliably localizing and manipulating feature representations.
Sparse Autoencoders (SAEs) have recently emerged as a promising direction for
feature extraction at scale, yet they, too, are limited by incomplete feature
isolation and unreliable monosemanticity. To systematically quantify these
limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric
to quantify feature monosemanticity in latent representation. Building on these
insights, we propose Guided Sparse Autoencoders (G-SAE), a method that
conditions latent representations on labeled concepts during training. We
demonstrate that reliable localization and disentanglement of target concepts
within the latent space improve interpretability, detection of behavior, and
control. Specifically, our evaluations on toxicity detection, writing style
identification, and privacy attribute recognition show that G-SAE not only
enhances monosemanticity but also enables more effective and fine-grained
steering with less quality degradation. Our findings provide actionable
guidelines for measuring and advancing mechanistic interpretability and control
of LLMs.

</details>


### [19] [Automated Detection of Pre-training Text in Black-box LLMs](https://arxiv.org/abs/2506.19399)
*Ruihan Hu,Yu-Ming Shang,Jiankun Peng,Wei Luo,Yazhe Wang,Xi Zhang*

Main category: cs.CL

> VeilProbe是一个自动检测大型语言模型预训练文本的框架，适用于黑箱设置，能够有效解决数据隐私和版权保护问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的许多方法依赖于语言模型的隐藏信息，因此在黑箱设置下无效。VeilProbe旨在自动检测语言模型的预训练文本，无需人工干预，特别是在黑箱设置下，这对于确保数据隐私和版权保护至关重要。

**Method:** VeilProbe利用序列到序列映射模型来推断输入文本和大型语言模型生成的对应输出后缀之间的潜在映射特征。通过执行关键标记扰动以获得更易区分的成员特征。为了缓解过拟合问题，还引入了一个原型基础成员分类器。

**Result:** 在三个广泛使用的数据集上的广泛评估证明了该框架的有效性和优越性。

**Conclusion:** 实验表明，VeilProbe框架在黑箱设置下是有效且优于其他方法的。

**Abstract:** Detecting whether a given text is a member of the pre-training data of Large
Language Models (LLMs) is crucial for ensuring data privacy and copyright
protection. Most existing methods rely on the LLM's hidden information (e.g.,
model parameters or token probabilities), making them ineffective in the
black-box setting, where only input and output texts are accessible. Although
some methods have been proposed for the black-box setting, they rely on massive
manual efforts such as designing complicated questions or instructions. To
address these issues, we propose VeilProbe, the first framework for
automatically detecting LLMs' pre-training texts in a black-box setting without
human intervention. VeilProbe utilizes a sequence-to-sequence mapping model to
infer the latent mapping feature between the input text and the corresponding
output suffix generated by the LLM. Then it performs the key token
perturbations to obtain more distinguishable membership features. Additionally,
considering real-world scenarios where the ground-truth training text samples
are limited, a prototype-based membership classifier is introduced to alleviate
the overfitting issue. Extensive evaluations on three widely used datasets
demonstrate that our framework is effective and superior in the black-box
setting.

</details>


### [20] [Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study](https://arxiv.org/abs/2506.19418)
*Yingji Zhang,Marco Valentino,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

> 研究提出了一种在语言VAE中学习推理规则的方法，提升了模型的推理能力，并发现增加样本数量对数学推理任务的效果改善有限。

<details>
  <summary>Details</summary>

**Motivation:** 为了改善当前Transformer模型在自然语言推断任务上依赖记忆而非基于规则推理的问题，本研究探讨了如何通过显式信号监督在语言模型的潜在空间中拆分和记忆推理规则，提高了模型的通用性、可解释性和可控性。

**Method:** 本研究提出了一种完整的管道，用于在基于Transformer的语言变分自编码器（VAEs）中学习推理规则，涵盖了三种基于规则的推理任务、理论框架和实用的端到端架构。

**Result:** 实验表明，在显式信号监督下，推理规则可以在编码器的参数空间中被分离出来，形成输出特征空间中具体的规则聚类。将推理信息注入查询（Query）可以帮助模型更有效地根据密钥（Key）检索记忆中的值（Value）。发现对于基于Qwen2.5（0.5B）的数学推理任务，增加样本数量并不能无限提升性能，并且前馈神经网络层（FFN）比注意力层在保持模型参数中推理规则的分离方面表现更好。

**Conclusion:** 注入推理信息可以有效提升模型对知识的记忆与检索能力，而FFN层比注意力层更适合在语言模型参数中保持推理规则的分离。

**Abstract:** Incorporating explicit reasoning rules within the latent space of language
models (LMs) offers a promising pathway to enhance generalisation,
interpretability, and controllability. While current Transformer-based language
models have shown strong performance on Natural Language Inference (NLI) tasks,
they often rely on memorisation rather than rule-based inference. This work
investigates how reasoning rules can be explicitly embedded and memorised
within the LMs through Language Variational Autoencoders (VAEs). We propose a
complete pipeline for learning reasoning rules within Transformer-based
language VAEs. This pipeline encompasses three rule-based reasoning tasks, a
supporting theoretical framework, and a practical end-to-end architecture. The
experiment illustrates the following findings: Disentangled reasoning: Under
explicit signal supervision, reasoning rules - viewed as functional mappings -
can be disentangled within the encoder's parametric space. This separation
results in distinct clustering of rules in the output feature space. Prior
knowledge injection: injecting reasoning information into the Query enables the
model to more effectively retrieve the stored value Value from memory based on
Key. This approach offers a simple method for integrating prior knowledge into
decoder-only language models. Performance bottleneck: In mathematical reasoning
tasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance
beyond a point. Moreover, ffn layers are better than attention layers at
preserving the separation of reasoning rules in the model's parameters.

</details>


### [21] [Can Large Language Models Capture Human Annotator Disagreements?](https://arxiv.org/abs/2506.19467)
*Jingwei Ni,Yu Fan,Vilém Zouhar,Donya Rooein,Alexander Hoyle,Mrinmaya Sachan,Markus Leippold,Dirk Hovy,Elliott Ash*

Main category: cs.CL

> 研究评估了大型语言模型预测注释分歧的能力，发现这些模型在建模分歧方面存在困难，而这一方面在基于多数标签的评估中可能被忽视。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于填补大型语言模型在预测注释分歧方面能力评估的空白，因为现有评估往往侧重于预测多数投票的“地面实情”标签，而忽视了模型是否捕捉到了重要的注释分歧。

**Method:** 通过广泛的评估，研究主要探讨了在没有重复人类标签访问的情况下，大型语言模型预测注释分歧的能力。

**Result:** 研究结果表明，大型语言模型在预测注释分歧方面存在困难。值得注意的是，虽然基于可验证奖励的强化学习（RLVR）推理通常会提升大型语言模型的性能，但在分歧预测方面的性能却有所下降。

**Conclusion:** 研究强调了评估和改进大型语言模型在分歧预测建模方面的重要性。这表明需要进一步的工作来提高这些模型的性能，特别是在处理注释分歧的问题上。

**Abstract:** Human annotation variation (i.e., annotation disagreements) is common in NLP
and often reflects important information such as task subjectivity and sample
ambiguity. While Large Language Models (LLMs) are increasingly used for
automatic annotation to reduce human effort, their evaluation often focuses on
predicting the majority-voted "ground truth" labels. It is still unclear,
however, whether these models also capture informative human annotation
variation. Our work addresses this gap by extensively evaluating LLMs' ability
to predict annotation disagreements without access to repeated human labels.
Our results show that LLMs struggle with modeling disagreements, which can be
overlooked by majority label-based evaluations. Notably, while RLVR-style
(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM
performance, it degrades performance in disagreement prediction. Our findings
highlight the critical need for evaluating and improving LLM annotators in
disagreement modeling. Code and data at
https://github.com/EdisonNi-hku/Disagreement_Prediction.

</details>


### [22] [MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages](https://arxiv.org/abs/2506.19468)
*Wenhan Han,Yifan Zhang,Zhixun Chen,Binbin Liu,Haobin Lin,Bingni Zhang,Taifeng Wang,Mykola Pechenizkiy,Meng Fang,Yin Zheng*

Main category: cs.CL

> 本文介绍了MuBench基准测试，发现多语言大语言模型中的性能差距，并提出多语言一致性作为补充指标。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估数据集有限，缺乏跨语言的对齐，导致对多语言能力的评估在语言和技能覆盖方面分散。为了解决这个问题，我们提出了MuBench。

**Method:** 我们介绍了一个涵盖61种语言的基准测试MuBench，用于评估广泛的能力范围。我们评估了几种先进的多语言大语言模型，并发现声称的语言覆盖范围与实际覆盖范围之间存在显著差距，特别是在英语和其他低资源语言之间存在持续的性能差异。

**Result:** 利用MuBench的对齐特性，我们提出了多语言一致性（MLC）作为补充指标，用于分析性能瓶颈并指导模型改进。最后，我们在英语和中文上进行了预训练，使用500B的token量，变化语言比例和平行数据的比例来研究跨语言转换动力学。

**Conclusion:** 通过引入MuBench和多语言一致性指标，我们可以更好地评估和提高多语言大语言模型的性能，特别是在低资源语言方面。

**Abstract:** Multilingual large language models (LLMs) are advancing rapidly, with new
models frequently claiming support for an increasing number of languages.
However, existing evaluation datasets are limited and lack cross-lingual
alignment, leaving assessments of multilingual capabilities fragmented in both
language and skill coverage. To address this, we introduce MuBench, a benchmark
covering 61 languages and evaluating a broad range of capabilities. We evaluate
several state-of-the-art multilingual LLMs and find notable gaps between
claimed and actual language coverage, particularly a persistent performance
disparity between English and low-resource languages. Leveraging MuBench's
alignment, we propose Multilingual Consistency (MLC) as a complementary metric
to accuracy for analyzing performance bottlenecks and guiding model
improvement. Finally, we pretrain a suite of 1.2B-parameter models on English
and Chinese with 500B tokens, varying language ratios and parallel data
proportions to investigate cross-lingual transfer dynamics.

</details>


### [23] [Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models](https://arxiv.org/abs/2506.19483)
*Marcos Estecha-Garitagoitia,Chen Zhang,Mario Rodríguez-Cantelar,Luis Fernando D'Haro*

Main category: cs.CL

> This paper explores the use of Large Language Models for turn-level data augmentation in dialogue systems, focusing on generating synthetic dialogue based on commonsense reasoning, and proposes an automated evaluation method inspired by ACCENT but simplified for practical use.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to explore the use of LLMs for turn-level data augmentation and automated evaluation in dialogue systems based on commonsense reasoning, addressing the need for more contextually relevant synthetic data.

**Method:** The method leverages Large Language Models (LLMs) for generating synthetic dialogue turns conditioned on commonsense attributes through a prompt-based approach inspired by Chain-of-Thought (CoT). It evaluates the synthetic dialogues using an auto-detection framework based on instructions for specific commonsense attributes, without the need for complex tuple extraction.

**Result:** The preliminary results indicate that the proposed method can effectively augment dialogue datasets with contextually relevant synthetic turns using LLMs and can automatically detect the quality of the generated data based on commonsense attributes.

**Conclusion:** The conclusion is that the method successfully exploits the capabilities of LLMs for commonsense reasoning and evaluation in dialogue systems, providing a promising foundation for further research in this area.

**Abstract:** This paper provides preliminary results on exploring the task of performing
turn-level data augmentation for dialogue system based on different types of
commonsense relationships, and the automatic evaluation of the generated
synthetic turns. The proposed methodology takes advantage of the extended
knowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)
to follow instructions, understand contextual information, and their
commonsense reasoning capabilities. The approach draws inspiration from
methodologies like Chain-of-Thought (CoT), applied more explicitly to the task
of prompt-based generation for dialogue-based data augmentation conditioned on
commonsense attributes, and the automatic evaluation of the generated
dialogues.
  To assess the effectiveness of the proposed approach, first we extracted 200
randomly selected partial dialogues, from 5 different well-known dialogue
datasets, and generate alternative responses conditioned on different event
commonsense attributes. This novel dataset allows us to measure the proficiency
of LLMs in generating contextually relevant commonsense knowledge, particularly
up to 12 different specific ATOMIC [10] database relations. Secondly, we
propose an evaluation framework to automatically detect the quality of the
generated dataset inspired by the ACCENT [26] metric, which offers a nuanced
approach to assess event commonsense. However, our method does not follow
ACCENT's complex eventrelation tuple extraction process. Instead, we propose an
instruction-based prompt for each commonsense attribute and use
state-of-the-art LLMs to automatically detect the original attributes used when
creating each augmented turn in the previous step.
  Preliminary results suggest that our approach effectively harnesses LLMs
capabilities for commonsense reasoning and evaluation in dialogue systems.

</details>


### [24] [Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning](https://arxiv.org/abs/2506.19484)
*Russell Beale*

Main category: cs.CL

> 本文综述了大型语言模型在教育中的应用，探讨了这些模型如何与对话教学法相结合，支持个性化和适应性学习。文章还指出了应用现有理论到大型语言模型时出现的差距，并提出了一些策略来更好地对齐AI驱动的对话学习和教育理论。

<details>
  <summary>Details</summary>

**Motivation:** 文章的动机是为了理解和评估大型语言模型在教育中的潜力，特别是在通过对话支持个性化和适应性学习方面，以及如何将这些模型与现有教育理论相结合。

**Method:** 通过综合现有文献，这种方法结合了大型语言模型的能力与对话教学法理论，比如维果茨基的社会文化学习理论、苏格拉底方法和劳里拉对话教学框架，来探索两者如何相互作用。

**Result:** 研究表明，虽然大型语言模型能支持部分教育原则，但也显示出一些不足，如倾向直接提供答案而非支持知识共建。文章提出了改进策略，以期更好地应对这些挑战。

**Conclusion:** 大型语言模型在教育中有巨大的潜力，但要实现其优势，仍需要根据教育原则设计对话策略并整合检索机制以确保信息的精准和情境相关性，从而使得AI驱动的对话学习更加高效和理论对齐。

**Abstract:** Large Language Models (LLMs) are rapidly transforming education by enabling
rich conversational learning experiences. This article provides a comprehensive
review of how LLM-based conversational agents are being used in higher
education, with extensions to secondary and lifelong learning contexts. We
synthesize existing literature on LLMs in education and theories of
conversational and dialogic pedagogy - including Vygotsky's sociocultural
learning (scaffolding and the Zone of Proximal Development), the Socratic
method, and Laurillard's conversational framework - and examine how prompting
strategies and retrieval-augmented generation (RAG) can align LLM behaviors
with these pedagogical theories, and how it can support personalized, adaptive
learning. We map educational theories to LLM capabilities, highlighting where
LLM-driven dialogue supports established learning principles and where it
challenges or falls short of traditional pedagogical assumptions. Notable gaps
in applying prior theories to LLMs are identified, such as the models tendency
to provide direct answers instead of fostering co-construction of knowledge,
and the need to account for the constant availability and broad but non-human
expertise of LLM tutors. In response, we propose practical strategies to better
align LLM interactions with sound pedagogy - for example, designing prompts
that encourage Socratic questioning, scaffolded guidance, and student
reflection, as well as integrating retrieval mechanisms to ensure accuracy and
contextual relevance. Our aim is to bridge the gap between educational theory
and the emerging practice of AI-driven conversational learning, offering
insights and tools for making LLM-based dialogues more educationally productive
and theory-aligned.

</details>


### [25] [Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs](https://arxiv.org/abs/2506.19492)
*Shu Yang,Junchao Wu,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large Reasoning Models (LRMs) have achieved remarkable performance on complex
tasks by engaging in extended reasoning before producing final answers, yet
this strength introduces the risk of overthinking, where excessive token
generation occurs even for simple tasks. While recent work in efficient
reasoning seeks to reduce reasoning length while preserving accuracy, it
remains unclear whether such optimization is truly a free lunch. Drawing on the
intuition that compressing reasoning may reduce the robustness of model
responses and lead models to omit key reasoning steps, we investigate whether
efficient reasoning strategies introduce behavioral inconsistencies. To
systematically assess this, we introduce $ICBENCH$, a benchmark designed to
measure inconsistency in LRMs across three dimensions: inconsistency across
task settings (ITS), inconsistency between training objectives and learned
behavior (TR-LB), and inconsistency between internal reasoning and
self-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs,
we find that while larger models generally exhibit greater consistency than
smaller ones, they all display widespread "scheming" behaviors, including
self-disagreement, post-hoc rationalization, and the withholding of reasoning
cues. Crucially, our results demonstrate that efficient reasoning strategies
such as No-Thinking and Simple Token-Budget consistently increase all three
defined types of inconsistency. These findings suggest that although efficient
reasoning enhances token-level efficiency, further investigation is imperative
to ascertain whether it concurrently introduces the risk of models evading
effective supervision.

</details>


### [26] [AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models](https://arxiv.org/abs/2506.19505)
*Zeyu Li,Chuanfu Xiao,Yang Wang,Xiang Liu,Zhenheng Tang,Baotong Lu,Mao Yang,Xinyu Chen,Xiaowen Chu*

Main category: cs.CL

> 通过Anchor Score（AnS）量化每个token KV缓存对量化引起的误差的敏感性，并提出AnTKV框架来压缩KV缓存，实现高效的token选择和内存压缩，同时保持较高的解码吞吐量和较低的困惑度。

<details>
  <summary>Details</summary>

**Motivation:** 在减少大型语言模型内存占用的同时，最小化超低比特KV缓存量化带来的性能下降是一个重大挑战。研究了不同token的KV缓存量化对注意力输出质量的影响。

**Method:** 提出了一种名为AnTKV的新框架，该框架利用Anchor Token感知的向量量化来压缩KV缓存，并设计了一个与FlashAttention完全兼容的triton内核，以实现高效的在线Anchor Token选择。

**Result:** AnTKV能使LLaMA-3-8B在单一80GB A100 GPU上处理长达840K tokens的上下文长度，解码吞吐量相比FP16基线提升了3.5倍，且在极致低比特量化下比FP16基线有更低的困惑度。

**Conclusion:** AnTKV在4位量化设置下匹配或超越了以前的工作，包括KIVI，SKVQ，KVQuant和CQ，并在Mistral-7B上实现了显著更低的困惑度。

**Abstract:** Quantization has emerged as an effective and lightweight solution to reduce
the memory footprint of the KV cache in Large Language Models (LLMs).
Nevertheless, minimizing the performance degradation caused by ultra-low-bit KV
cache quantization remains a significant challenge. We observe that quantizing
the KV cache of different tokens has varying impacts on the quality of
attention outputs. To systematically investigate this phenomenon, we perform
forward error propagation analysis on attention and propose the Anchor Score
(AnS) that quantifies the sensitivity of each token's KV cache to
quantization-induced error. Our analysis reveals significant disparities in AnS
across tokens, suggesting that preserving a small subset with full precision
(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive
quantization scenarios. Based on this insight, we introduce AnTKV, a novel
framework that leverages Anchor Token-aware Vector Quantization to compress the
KV cache. Furthermore, to support efficient deployment, we design and develop a
triton kernel that is fully compatible with FlashAttention, enabling fast
online Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context
lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x
higher decoding throughput compared to the FP16 baseline. Our experiment
results demonstrate that AnTKV matches or outperforms prior works such as KIVI,
SKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves
significantly lower perplexity under ultra-low-bit quantization on Mistral-7B,
with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of
4.73.

</details>


### [27] [heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation](https://arxiv.org/abs/2506.19512)
*Ashish Chouhan,Michael Gertz*

Main category: cs.CL

> 本文介绍了一种针对患者的电子健康记录(EHRs)中的临床证据提出答案的方法，使用了自适应的检索增强生成(RAG)框架，并在实验中验证了其产生的答案比固定检索策略更相关和事实性。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机在于提升对患者特定问题的回答质量，通过优化检索策略来提取更准确和相关的临床证据。

**Method:** 本文描述了一个使用检索增强生成（RAG）框架的方法，该框架通过从患者的电子健康记录（EHRs）中提取临床证据来生成针对患者特定问题的答案。团队研究了RAG框架的不同组成部分，特别是排名列表截断（RLT）检索策略和归因方法。团队提出了autocut*和elbow两种新方法，与现有的surprise和autocut方法结合形成了一种根据查询自适应的-k检索策略。

**Result:** 实验结果表明，与固定-k检索策略相比，这种方法能够产生更事实和相关性的答案。

**Conclusion:** 自适应-k检索策略相较于传统的固定-k策略更能够产生具有事实性的答案，验证了优化检索策略在提升RAG框架性能中的有效性。

**Abstract:** This paper presents the approach of our team called heiDS for the ArchEHR-QA
2025 shared task. A pipeline using a retrieval augmented generation (RAG)
framework is designed to generate answers that are attributed to clinical
evidence from the electronic health records (EHRs) of patients in response to
patient-specific questions. We explored various components of a RAG framework,
focusing on ranked list truncation (RLT) retrieval strategies and attribution
approaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a
query-dependent-k retrieval strategy, including the existing surprise and
autocut methods and two new methods proposed in this work, autocut* and elbow.
The experimental results show the benefits of our strategy in producing factual
and relevant answers when compared to a fixed-$k$.

</details>


### [28] [Automatic Posology Structuration : What role for LLMs?](https://arxiv.org/abs/2506.19525)
*Natalia Bobkova,Laura Zanella-Calzada,Anyes Tafoughalt,Raphaël Teboul,François Plesse,Félix Gaschi*

Main category: cs.CL

> 该研究探索了大型语言模型（LLMs）在结构化自由文本剂量指令方面的效用，发现微调后的LLMs在准确性上可以匹敌NERL基线系统，并提出了一种基于置信分数选择输出的混合管道，实现了91%的结构化准确性，同时减少了延迟和计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 自动结构化剂量指令对于提高药物安全性和支持临床决策非常重要。尤其是，在法语处方中，这些指令常常语义模糊、不规则或口语化，这限制了传统机器学习流水线的有效性。

**Method:** 研究方法包括使用大型语言模型（LLMs）将自由文本剂量指令转换为结构化的格式，并将其与基于命名实体识别和链接（NERL）的“前LLM”系统进行比较。研究人员测试了基于提示的方法和微调LLM的效果。

**Result:** 研究结果表明，尽管提示方法提升了性能，只有微调的LLMs才能达到基线的准确性。错误分析指出，NERL在结构准确性方面更强，而LLMs在处理语义细微差别方面更优。

**Conclusion:** 基于此，研究者提出了一种混合管道，将NERL输出低于0.8置信度的低置信度情况进行LLM处理，并基于置信得分选择输出，该策略达到了91%的结构化准确性，同时最小化了延迟和计算量。这种混合方法提高了结构准确性，同时限制了计算成本，为临床实际使用提供了一种可行的解决方案。

**Abstract:** Automatically structuring posology instructions is essential for improving
medication safety and enabling clinical decision support. In French
prescriptions, these instructions are often ambiguous, irregular, or
colloquial, limiting the effectiveness of classic ML pipelines. We explore the
use of Large Language Models (LLMs) to convert free-text posologies into
structured formats, comparing prompt-based methods and fine-tuning against a
"pre-LLM" system based on Named Entity Recognition and Linking (NERL). Our
results show that while prompting improves performance, only fine-tuned LLMs
match the accuracy of the baseline. Through error analysis, we observe
complementary strengths: NERL offers structural precision, while LLMs better
handle semantic nuances. Based on this, we propose a hybrid pipeline that
routes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs
based on confidence scores. This strategy achieves 91% structuration accuracy
while minimizing latency and compute. Our results show that this hybrid
approach improves structuration accuracy while limiting computational cost,
offering a scalable solution for real-world clinical use.

</details>


### [29] [KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs](https://arxiv.org/abs/2506.19527)
*Kelin Fu,Kaigui Bian*

Main category: cs.CL

> KnowMap通过动态构建知识库并微调模型，解决了大型语言模型在适应新任务时的挑战，提高了模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型在快速适应新任务时存在挑战，传统方法如微调成本高昂且可能导致灾难性遗忘，因此提出KnowMap方法。

**Method:** KnowMap采用动态构建知识库的方法，从环境和经验数据中提取，通过微调一个小模型来增强大型语言模型的特定任务知识。

**Result:** 实验表明，在ScienceWorld基准测试中，KnowMap使gpt-4-turbo模型的性能提高了17.71%。

**Conclusion:** KnowMap不仅为大型语言模型的快速适应任务提供了一种有效的方法，而且还展示了集成环境和经验知识如何增强模型的推理能力。

**Abstract:** While Large Language Models (LLMs) possess significant capabilities in
open-world agent tasks, they also face challenges in rapidly adapting to new,
specialized tasks due to their reliance on static pre-trained knowledge.
Traditional methods such as fine-tuning are often costly, data-intensive, and
may lead to "catastrophic forgetting." Therefore, we present KnowMap, a novel
approach that dynamically constructs a knowledge base from environmental and
experiential data. KnowMap fine-tunes a small knowledge-embedding model to
equip a larger LLM with valuable task-specific knowledge. Our experiments on
the ScienceWorld benchmark demonstrate 17.71% improvement for the performance
of gpt-4-turbo model. KnowMap not only provides an efficient and effective
means for LLM task-adapting, but also highlights how integrating environmental
and experiential knowledge can enhance LLMs' reasoning capabilities.

</details>


### [30] [Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection](https://arxiv.org/abs/2506.19548)
*Devesh Pant,Rishi Raj Grandhe,Vipin Samaria,Mukul Paul,Sudhir Kumar,Saransh Khanna,Jatin Agrawal,Jushaan Singh Kalra,Akhil VSSG,Satish V Khalikar,Vipin Garg,Himanshu Chauhan,Pranay Verma,Neha Khandelwal,Soma S Dhavala,Minesh Mathew*

Main category: cs.CL

> Health Sentinel 是一个用于自动从在线文章中提取疾病爆发相关事件的多阶段信息提取流程，已处理超过3亿篇文章并识别出9.5万个健康事件。

<details>
  <summary>Details</summary>

**Motivation:** 由于传统基于指标的监控在疾病爆发早期检测中的挑战，本研究旨在通过从在线媒体自动提取相关事件，以支持公共卫生机构的及时干预。

**Method:** 研究提出了一种结合机器学习和非机器学习方法的多阶段信息提取管道。

**Result:** 自2022年4月起，Health Sentinel 已处理超过3亿篇新闻文章，识别出9.5万个健康事件，其中3500个被公共卫生专家选作潜在爆发事件。

**Conclusion:** 这种方法有效地辅助了疾病控制中心识别和响应潜在的疾病爆发，提高了公共卫生监控的效率和及时性。

**Abstract:** Early detection of disease outbreaks is crucial to ensure timely intervention
by the health authorities. Due to the challenges associated with traditional
indicator-based surveillance, monitoring informal sources such as online media
has become increasingly popular. However, owing to the number of online
articles getting published everyday, manual screening of the articles is
impractical. To address this, we propose Health Sentinel. It is a multi-stage
information extraction pipeline that uses a combination of ML and non-ML
methods to extract events-structured information concerning disease outbreaks
or other unusual health events-from online articles. The extracted events are
made available to the Media Scanning and Verification Cell (MSVC) at the
National Centre for Disease Control (NCDC), Delhi for analysis, interpretation
and further dissemination to local agencies for timely intervention. From April
2022 till date, Health Sentinel has processed over 300 million news articles
and identified over 95,000 unique health events across India of which over
3,500 events were shortlisted by the public health experts at NCDC as potential
outbreaks.

</details>


### [31] [RCStat: A Statistical Framework for using Relative Contextualization in Transformers](https://arxiv.org/abs/2506.19549)
*Debabrata Mahapatra,Shubham Agarwal,Apoorv Saxena,Subrata Mitra*

Main category: cs.CL

> 本文提出了RCStat框架，使用Raw attention logits来测量token片段之间的上下文对齐，适用于压缩和归因两个应用场景，无需模型重新训练即可达到最先进的压缩和归因性能。

<details>
  <summary>Details</summary>

**Motivation:** 之前的自回归transformers研究依赖于Softmax注意力权重，但无法充分反映预Softmax查询-键的丰富结构。

**Method:** RCStat通过Relative Contextualization(RC)测量原始注意力logits来获得上下文对齐，通过两个应用示范框架的有效性：键值压缩和归因。

**Result:** 在问答、总结和归因基准测试中，RCStat取得了显著的实证收益。

**Conclusion:** RCStat是基于Raw attention logits的统计框架，提出了有效且高效的策略，并在多个应用场景中取得了优于以往的方法的性能。

**Abstract:** Prior work on input-token importance in auto-regressive transformers has
relied on Softmax-normalized attention weights, which obscure the richer
structure of pre-Softmax query-key logits. We introduce RCStat, a statistical
framework that harnesses raw attention logits via Relative Contextualization
(RC), a random variable measuring contextual alignment between token segments,
and derive an efficient upper bound for RC. We demonstrate two applications:
(i) Key-Value compression, where RC-based thresholds drive adaptive key-value
eviction for substantial cache reduction with minimal quality loss; and (ii)
Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level
explanations than post-Softmax methods. Across question answering,
summarization, and attribution benchmarks, RCStat achieves significant
empirical gains, delivering state-of-the-art compression and attribution
performance without any model retraining.

</details>


### [32] [Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress](https://arxiv.org/abs/2506.19571)
*Lorenzo Proietti,Stefano Perrella,Roberto Navigli*

Main category: cs.CL

> 本研究探讨了机器翻译评估指标与人类评估的一致性，并提出即使某些指标表现接近或优于人类基线，也需要注意一些潜在的问题。

<details>
  <summary>Details</summary>

**Motivation:** 为了更清晰地理解MT评估指标的表现，从而建立一个性能上限，作者将人类基线纳入到了MT评估指标的评估之中。

**Method:** 本研究通过引入人类基线来评估机器翻译（MT）评估指标的表现，并设定了MT评估指标能力评估的上限。

**Result:** 研究结果显示人类注释者并不总是比自动指标优越，最先进的指标经常与人类基线持平甚至高于人类基线。

**Conclusion:** 尽管研究结果表明指标与人类表现持平，但本文提出了一些谨慎的理由，并探讨该结果对MT评估研究领域的更广泛影响，导致关于MT评估中衡量进展的可靠性的讨论。

**Abstract:** In Machine Translation (MT) evaluation, metric performance is assessed based
on agreement with human judgments. In recent years, automatic metrics have
demonstrated increasingly high levels of agreement with humans. To gain a
clearer understanding of metric performance and establish an upper bound, we
incorporate human baselines in the MT meta-evaluation, that is, the assessment
of MT metrics' capabilities. Our results show that human annotators are not
consistently superior to automatic metrics, with state-of-the-art metrics often
ranking on par with or higher than human baselines. Despite these findings
suggesting human parity, we discuss several reasons for caution. Finally, we
explore the broader implications of our results for the research field, asking:
Can we still reliably measure improvements in MT evaluation? With this work, we
aim to shed light on the limits of our ability to measure progress in the
field, fostering discussion on an issue that we believe is crucial to the
entire MT evaluation community.

</details>


### [33] [ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model](https://arxiv.org/abs/2506.19599)
*Zhenke Duan,Jiqun Pan,Jiani Tu,Xiaoyi Wang,Yanqing Wang*

Main category: cs.CL

> 本文提出ECCoT框架来评估和改进LLMs的推理链，增强其解释性并减少偏见，通过MRF-ETM和CSBert技术实现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管LLMs在自然语言处理方面取得了显著进展，但它们缺乏透明度，生成的输出不可靠，使得解释性成为一个重要问题。研究动机是解决这一问题，提高LLMs输出的可靠性和解释性。

**Method:** 本研究提出了一种名为ECCoT（End-to-End Cognitive Chain of Thought Validation Framework）的端到端认知链验证框架，用以评估和优化LLMs的推理链。该框架包含两个关键组件：基于主题的推理链生成（Markov Random Field-Embedded Topic Model，MRF-ETM）和因果推理对齐（Causal Sentence-BERT，CSBert）。通过结构化顺序统计来过滤无效推理链，ECCoT改进了LLMs的可解释性，减少了偏见，并增强了基于LLMs的决策可信度。

**Result:** ECCoT框架增强了LLMs在生成可靠的、有理由支持的输出时的能力，提高了决策的可信度和可解释性。此框架的实现在https://github.com/erwinmsmith/ECCoT.git可以找到。

**Conclusion:** 通过引入ECCoT、MRF-ETM和CSBert技术，本研究提供了提高LLMs推理链质量的有效方法，从而提高了机器学习系统的整体性能和可靠性，尤其是在自然语言处理领域。

**Abstract:** In the era of large-scale artificial intelligence, Large Language Models
(LLMs) have made significant strides in natural language processing. However,
they often lack transparency and generate unreliable outputs, raising concerns
about their interpretability. To address this, the Chain of Thought (CoT)
prompting method structures reasoning into step-by-step deductions. Yet, not
all reasoning chains are valid, and errors can lead to unreliable conclusions.
We propose ECCoT, an End-to-End Cognitive Chain of Thought Validation
Framework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates
the Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT
generation and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By
filtering ineffective chains using structured ordering statistics, ECCoT
improves interpretability, reduces biases, and enhances the trustworthiness of
LLM-based decision-making. Key contributions include the introduction of ECCoT,
MRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning
enhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.

</details>


### [34] [Social Hatred: Efficient Multimodal Detection of Hatemongers](https://arxiv.org/abs/2506.19603)
*Tom Marzea,Abraham Israeli,Oren Tsur*

Main category: cs.CL

> 本文提出了一种多模态聚合方法来检测传播仇恨的用户，该方法考虑了潜在的仇恨文本、用户活动和用户网络。通过在Twitter、Gab和Parler三个数据集上的评估，结果表明，与之前的方法相比，我们的方法在检测仇恨传播者方面更有效。

<details>
  <summary>Details</summary>

**Motivation:** 大多数先前的工作集中在检测仇恨言论上，而本文则认为从用户层面进行检测同样重要，且更具挑战性。准确地分类用户可以帮助我们更好地理解仇恨言论的社会现象。

**Method:** 本文提出的方法是一种多模态聚合法，考虑了潜在的仇恨文本、用户活动和用户网络三个方面的信息。

**Result:** 本文在Twitter、Gab和Parler三个数据集上对方法进行了评估。结果显示，与之前使用的基于文本和基于图的方法相比，本文的方法显著提高了检测仇恨传播者的性能。

**Conclusion:** 本文的方法可用于改善被编码信息、吹口哨行为和种族灯光语言的分类，也可用于告知干预措施。此外，本文的方法在不同类型的内容平台上表现良好。

**Abstract:** Automatic detection of online hate speech serves as a crucial step in the
detoxification of the online discourse. Moreover, accurate classification can
promote a better understanding of the proliferation of hate as a social
phenomenon. While most prior work focus on the detection of hateful utterances,
we argue that focusing on the user level is as important, albeit challenging.
In this paper we consider a multimodal aggregative approach for the detection
of hate-mongers, taking into account the potentially hateful texts, user
activity, and the user network. Evaluating our method on three unique datasets
X (Twitter), Gab, and Parler we show that processing a user's texts in her
social context significantly improves the detection of hate mongers, compared
to previously used text and graph-based methods. We offer comprehensive set of
results obtained in different experimental settings as well as qualitative
analysis of illustrative cases. Our method can be used to improve the
classification of coded messages, dog-whistling, and racial gas-lighting, as
well as to inform intervention measures. Moreover, we demonstrate that our
multimodal approach performs well across very different content platforms and
over large datasets and networks.

</details>


### [35] [Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge](https://arxiv.org/abs/2506.19607)
*Juraj Vladika,Ihsan Soydemir,Florian Matthes*

Main category: cs.CL

> 研究採用了兩種最先進的自我修正方法來修正幻覺化的新聞摘要，通過利用多種搜尋引擎來進行補充與校正，并揭示了搜尋引擎摘要和少量示例提示的益處。

<details>
  <summary>Details</summary>

**Motivation:** 大型語言模型在生成連貫文本方面展示出強大能力，但事實上不準確的陳述問題仍然存在。自我修正方法被視為一種很有前景的解決幻覺問題的方案，使用該方法的先前研究主要集中在百科全書式內容生成，對於新領域如新聞摘要的應用則相對較少。

**Method:** 本研究採用了兩種最先進的自我修正系統，通過使用三種搜尋引擎提供的證據來修正幻覺化的摘要。研究設計利用了LLMs的多輪特性來迭代生成驗證問題，並利用內外部知識來回答這些問題，進而修訂原始回應以加以糾正。

**Result:** 研究結果揭示了各種背景下系統表現的情況，特別是在搜索引擎片段和少量提示調用的重要性以及G-Eval與人類評估之間的高度一致性上。

**Conclusion:** 研究強調了將自我修正技術應用於幻覺文本問題，特別是對於新聞摘要的新領域，並且提供了一些關於搜索引擎技術和提示設計方面的實用見解。

**Abstract:** While large language models (LLMs) have shown remarkable capabilities to
generate coherent text, they suffer from the issue of hallucinations --
factually inaccurate statements. Among numerous approaches to tackle
hallucinations, especially promising are the self-correcting methods. They
leverage the multi-turn nature of LLMs to iteratively generate verification
questions inquiring additional evidence, answer them with internal or external
knowledge, and use that to refine the original response with the new
corrections. These methods have been explored for encyclopedic generation, but
less so for domains like news summarization. In this work, we investigate two
state-of-the-art self-correcting systems by applying them to correct
hallucinated summaries using evidence from three search engines. We analyze the
results and provide insights into systems' performance, revealing interesting
practical findings on the benefits of search engine snippets and few-shot
prompts, as well as high alignment of G-Eval and human evaluation.

</details>


### [36] [Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager](https://arxiv.org/abs/2506.19652)
*Lucie Galland,Catherine Pelachaud,Florian Pecune*

Main category: cs.CL

> 研究提出了一种结合LLMs与RL的对话管理系统，专注于提高开放式对话的效率与个性化适应性，特别是针对动机性访谈场景中患者需求的个性化响应，实验证明其在奖励方面优于现有一流的LLM模型。

<details>
  <summary>Details</summary>

**Motivation:** 这项工作的动机在于开发一个能够适应多样化用户的开放式对话系统，特别适用于需要特定目标的场景，如鼓励行为改变的动机性访谈。

**Method:** 本研究提出了一种结合大型语言模型（LLMs）和基于强化学习（RL）的对话管理框架，用于具有特定目标的开放式对话。通过使用分层强化学习来模拟对话的结构性阶段，并运用元学习来提高对不同用户配置文件的适应性，该方法提高了适应性和效率，使得系统可以基于有限的数据学习，并在对话阶段间流畅转换，针对不同的患者需求个性化响应。

**Result:** 研究将其框架应用于动机性访谈，通过实验证明，提出的对话管理器在奖励指标上超过了最先进的LLM基线，展示了为特定目标调整LLM创建开放式对话系统的潜在优势。

**Conclusion:** 通过将大型语言模型与适当的对话管理策略结合，可以有效改善开放式对话系统的性能，特别是在需要根据特定目标调整对话以满足不同用户需求的场景中，该方法显示出优于当前最佳模型的潜力。

**Abstract:** In this work, we propose a novel framework that integrates large language
models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a
specific goal. By leveraging hierarchical reinforcement learning to model the
structured phases of dialogue and employ meta-learning to enhance adaptability
across diverse user profiles, our approach enhances adaptability and
efficiency, enabling the system to learn from limited data, transition fluidly
between dialogue phases, and personalize responses to heterogeneous patient
needs. We apply our framework to Motivational Interviews, aiming to foster
behavior change, and demonstrate that the proposed dialogue manager outperforms
a state-of-the-art LLM baseline in terms of reward, showing a potential benefit
of conditioning LLMs to create open-ended dialogue systems with specific goals.

</details>


### [37] [Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?](https://arxiv.org/abs/2506.19733)
*Chuxuan Hu,Yuxuan Zhu,Antony Kellermann,Caleb Biddulph,Suppakit Waiwitlikhit,Jason Benn,Daniel Kang*

Main category: cs.CL

> 研究发现，强化后训练在相似任务上表现优异，但在推理模式不同的领域中泛化效果有限。

<details>
  <summary>Details</summary>

**Motivation:** 研究RPT在不同领域的泛化能力，因为先前的研究仅在同样用于微调的数据集上评估了RPT模型。

**Method:** 采用两种方法研究强化后训练（RPT）在不同领域的泛化能力。第一种是观察性研究，比较了多种开放权重的RPT模型与其基准模型在见过和未见过领域的表现。第二种是干预性研究，对RPT模型在单个领域进行微调，并评估其在多个领域的性能。

**Result:** 研究表明，尽管RPT在与微调数据相似的任务中带来了显著的提升，但这种提升在具有不同推理模式的领域中的泛化效果并不一致，甚至可能消失。

**Conclusion:** RPT对于相似任务的提升显著，但在不同推理模式的领域中的泛化能力不强。

**Abstract:** Reinforcement post training (RPT) has recently shown promise in improving the
reasoning abilities of large language models (LLMs). However, it remains
unclear how well these improvements generalize to new domains, as prior work
evaluates RPT models on data from the same domains used for fine-tuning. To
understand the generalizability of RPT, we conduct two studies. (1)
Observational: We compare a wide range of open-weight RPT models against their
corresponding base models across multiple domains, including both seen and
unseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs
with RPT on single domains and evaluate their performance across multiple
domains. Both studies converge on the same conclusion that, although RPT brings
substantial gains on tasks similar to the fine-tuning data, the gains
generalize inconsistently and can vanish on domains with different reasoning
patterns.

</details>


### [38] [Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach](https://arxiv.org/abs/2506.19750)
*Takashi Nishibayashi,Seiji Kanazawa,Kumpei Yamada*

Main category: cs.CL

> 研究提出一种使用HPO生成合成案例来评估SC算法更新对罕见疾病诊断性能影响的方法，并验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 由于获取罕见疾病的评估数据困难且成本高昂，本研究目的验证一种合成案例模拟方法，以在SC算法更新后评估罕见疾病的诊断性能变化。

**Method:** 此研究提出使用疾病表型注释从人类表型本体(HPO)中生成合成临床案例来评估症状检查器(SC)算法更新对罕见疾病诊断性能的影响。

**Result:** 实验包括了八个SC算法更新。对于在HPO中有频率信息的疾病，回忆率（@8）变化的R^2值为0.831（p=0.031），精度（@8）变化的R^2值为0.78（p=0.047）。但对于没有频率信息的疾病，预测误差较大。

**Conclusion:** 研究提出的方法可以使用公开可用、专家创建的知识库，在罕见疾病上实现算法更新前的诊断性能评估，这提供了一种透明且低成本的途径增强早期诊断的支持。

**Abstract:** Background: Symptom Checkers (SCs) provide users with personalized medical
information. To prevent performance degradation from algorithm updates, SC
developers must evaluate diagnostic performance changes for individual diseases
before deployment. However, acquiring sufficient evaluation data for rare
diseases is difficult, and manually creating numerous clinical vignettes is
costly and impractical. Objective: This study proposes and validates a novel
Synthetic Vignette Simulation Approach to evaluate diagnostic performance
changes for individual rare diseases following SC algorithm updates. Methods:
We used disease-phenotype annotations from the Human Phenotype Ontology (HPO),
a knowledge database for rare diseases, to generate synthetic vignettes. With
these, we simulated SC interviews to estimate the impact of algorithm updates
on real-world diagnostic performance. The method's effectiveness was evaluated
retrospectively by comparing estimated values with actual metric changes using
the R 2(R-squared) coefficient. Results: The experiment included eight past SC
algorithm updates. For updates on diseases with frequency information in HPO
(n=5), the R^2 for recall@8 change was 0.831 (p=0.031), and for precision@8
change, it was 0.78 (p=0.047), indicating the method can predict
post-deployment performance. In contrast, large prediction errors occurred for
diseases without frequency information (n=3), highlighting its importance. The
manual effort to map HPO phenotypes to SC symptoms was approximately 2 hours
per disease. Conclusions: Our method enables pre-deployment evaluation of SC
algorithm changes for individual rare diseases using a publicly available,
expert-created knowledge base. This transparent and low-cost approach allows
developers to efficiently improve diagnostic performance for rare diseases,
potentially enhancing support for early diagnosis.

</details>


### [39] [Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis](https://arxiv.org/abs/2506.19753)
*Omar A. Essameldin,Ali O. Elbeih,Wael H. Gomaa,Wael F. Elsersy*

Main category: cs.CL

> 本研究使用RNN、Transformer和大规模语言模型对18种阿拉伯语方言进行分类，其中MARBERTv2表现最佳，准确率为65%，F1得分为64%。该研究对社交媒体监控和个人化方言聊天机器人等领域有潜在应用。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于阿拉伯语是世界上使用最广泛的语言之一，拥有在22个国家中使用的大量方言，该研究旨在解决对这些方言进行分类的问题。

**Method:** 本研究通过创建并测试了RNN模型、Transformer模型以及通过提示工程的大规模语言模型(LLMs)来解决QADI数据集中18种阿拉伯语方言的分类问题。

**Result:** 研究结果表明，MARBERTv2模型表现最佳，准确率为65%，F1得分为64%。

**Conclusion:** 本研究表明了利用先进的预处理技术和最新的NLP模型来区分阿拉伯语方言的重要性，并且该研究对开发针对特定方言的定制聊天机器人、社交媒体监控以及提高阿拉伯社群的可达性等方面有潜在的应用价值。

**Abstract:** The Arabic language is among the most popular languages in the world with a
huge variety of dialects spoken in 22 countries. In this study, we address the
problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.
RNN models, Transformer models, and large language models (LLMs) via prompt
engineering are created and tested. Among these, MARBERTv2 performed best with
65% accuracy and 64% F1-score. Through the use of state-of-the-art
preprocessing techniques and the latest NLP models, this paper identifies the
most significant linguistic issues in Arabic dialect identification. The
results corroborate applications like personalized chatbots that respond in
users' dialects, social media monitoring, and greater accessibility for Arabic
communities.

</details>


### [40] [Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR](https://arxiv.org/abs/2506.19761)
*Martin Ratajczak,Jean-Philippe Robichaud,Jennifer Drexler Fox*

Main category: cs.CL

> 该论文研究了长文本语音识别问题，通过使用线性复杂度递归注意力层（RA），取得了比多头注意力模型更高的效率和相当的准确率。此外，提出了一种新的训练范式和Direction Dropout正则化方法，进一步提升了模型性能和吞吐量。

<details>
  <summary>Details</summary>

**Motivation:** 多头注意力（MHA）模型在长文本语音识别应用中存在二次复杂度问题，效率低下。因此，该论文致力于寻找更高效且准确的解决方案。

**Method:** 构建了双向递归注意力（RA）层，并提出了一种新的训练范式，此外还提出了一个新的正则化方法：Direction Dropout。

**Result:** 双向RA层的表现优于限制上下文注意力（LCA），提高了44%的吞吐量，并通过Direction Dropout进一步优化了准确率和吞吐量之间的平衡。

**Conclusion:** RA层展现出比MHA更高的效率和不亚于MHA的准确率，尤其是在长文本语音识别任务上。结合Direction Dropout使其性能更加优越。

**Abstract:** Long-form speech recognition is an application area of increasing research
focus. ASR models based on multi-head attention (MHA) are ill-suited to
long-form ASR because of their quadratic complexity in sequence length. We
build on recent work that has investigated linear complexity recurrent
attention (RA) layers for ASR. We find that bidirectional RA layers can match
the accuracy of MHA for both short- and long-form applications. We present a
strong limited-context attention (LCA) baseline, and show that RA layers are
just as accurate while being more efficient. We develop a long-form training
paradigm which further improves RA performance, leading to better accuracy than
LCA with 44% higher throughput. We also present Direction Dropout, a novel
regularization method that improves accuracy, provides fine-grained control of
the accuracy/throughput trade-off of bidirectional RA, and enables a new
alternating directions decoding mode with even higher throughput.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [41] [Correspondence-Free Multiview Point Cloud Registration via Depth-Guided Joint Optimisation](https://arxiv.org/abs/2506.18922)
*Yiran Zhou,Yingyu Wang,Shoudong Huang,Liang Zhao*

Main category: cs.CV

> 本文提出了一种无需特征提取和数据关联的新型多视角点云配准方法，通过全局深度图和点云姿态进行优化，避免了在复杂环境中难以获得全局最优解的问题。

<details>
  <summary>Details</summary>

**Motivation:** 点云多视角配准是构建全局一致3D模型的基本任务。现有方法通常依赖于特征提取和数据关联，这些过程在复杂环境中难以获得全局最优解。本文旨在解决这一挑战。

**Method:** 本文提出了一种无对应点的多视角点云配准方法。具体来说，通过将全局地图表示为深度图，并利用原始深度信息，我们将问题表述为一个非线性最小二乘优化问题，该问题能够联合估计点云的姿态和全局地图。通过利用全局深度图和对应姿态关联多帧点云，我们避免了传统的基于特征的捆绑调整方法中显式的特征提取和数据关联的困难。此数据关联在优化过程中被隐式地包含并动态调整。

**Result:** 在真实数据集上的广泛评估表明，所提方法在准确性上优于最先进的方法，特别是在特征提取和数据关联困难的环境中。

**Conclusion:** 所提出的方法通过将全局地图表示为深度图并通过点云的姿态与全局深度图关联，有效解决了传统方法在复杂环境中遇到的问题，并在准确性上优于现有方法。

**Abstract:** Multiview point cloud registration is a fundamental task for constructing
globally consistent 3D models. Existing approaches typically rely on feature
extraction and data association across multiple point clouds; however, these
processes are challenging to obtain global optimal solution in complex
environments. In this paper, we introduce a novel correspondence-free multiview
point cloud registration method. Specifically, we represent the global map as a
depth map and leverage raw depth information to formulate a non-linear least
squares optimisation that jointly estimates poses of point clouds and the
global map. Unlike traditional feature-based bundle adjustment methods, which
rely on explicit feature extraction and data association, our method bypasses
these challenges by associating multi-frame point clouds with a global depth
map through their corresponding poses. This data association is implicitly
incorporated and dynamically refined during the optimisation process. Extensive
evaluations on real-world datasets demonstrate that our method outperforms
state-of-the-art approaches in accuracy, particularly in challenging
environments where feature extraction and data association are difficult.

</details>


### [42] [Connecting Vision and Emissions: A Behavioural AI Approach to Carbon Estimation in Road Design](https://arxiv.org/abs/2506.18924)
*Ammar K Al Mhdawi,Nonso Nnamoko,Safanah Mudheher Raafat,M. K. S. Al-Mhdawi,Amjad J Humaidi*

Main category: cs.CV

> 本文提出了一种增强的YOLOv8实时车辆检测与分类框架，用于估算城市环境中的碳排放。通过YOLOv8架构增强车辆的检测、分割和跟踪功能，识别车牌并分类车辆类型，结合OCR系统进行字符级检测和解码，实时API验证车牌信息以确保准确分类和排放估算。实验结果展示该框架具有高效的检测准确率和字符级OCR精准度。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在开发一种结合实时车辆检测和深度OCR的方法，以实现城市环境中车辆碳排放的自动化精准监测。

**Method:** 使用增强的YOLOv8架构提升车辆检测、分割和跟踪功能，结合深度OCR系统进行车牌识别，采用实时API验证车牌信息以提高准确性。

**Result:** 系统实现了71%的平均精度（mAP@0.5）对于检测边界框和70%对于分割掩码的mAP，字符级别OCR的精度高达99%。

**Conclusion:** 结果证实了将实时对象检测与深度OCR结合在智慧交通系统中实际部署的可行性，提供了一种用于自动的、车辆特定的碳排放监测的可扩展解决方案。

**Abstract:** We present an enhanced YOLOv8 real time vehicle detection and classification
framework, for estimating carbon emissions in urban environments. The system
enhances YOLOv8 architecture to detect, segment, and track vehicles from live
traffic video streams. Once a vehicle is localized, a dedicated deep
learning-based identification module is employed to recognize license plates
and classify vehicle types. Since YOLOv8 lacks the built-in capacity for fine
grained recognition tasks such as reading license plates or determining vehicle
attributes beyond class labels, our framework incorporates a hybrid pipeline
where each detected vehicle is tracked and its bounding box is cropped and
passed to a deep Optical Character Recognition (OCR) module. This OCR system,
composed of multiple convolutional neural network (CNN) layers, is trained
specifically for character-level detection and license plate decoding under
varied conditions such as motion blur, occlusion, and diverse font styles.
Additionally, the recognized plate information is validated using a real time
API that cross references with an external vehicle registration database to
ensure accurate classification and emission estimation. This multi-stage
approach enables precise, automated calculation of per vehicle carbon
emissions. Extensive evaluation was conducted using a diverse vehicle dataset
enriched with segmentation masks and annotated license plates. The YOLOv8
detector achieved a mean Average Precision (mAP@0.5) of approximately 71% for
bounding boxes and 70% for segmentation masks. Character level OCR accuracy
reached up to 99% with the best performing CNN model. These results affirm the
feasibility of combining real time object detection with deep OCR for practical
deployment in smart transportation systems, offering a scalable solution for
automated, vehicle specific carbon emission monitoring.

</details>


### [43] [Interpretable and Granular Video-Based Quantification of Motor Characteristics from the Finger Tapping Test in Parkinson Disease](https://arxiv.org/abs/2506.18925)
*Tahereh Zarrat Ehsan,Michael Tangermann,Yağmur Güçlütürk,Bastiaan R. Bloem,Luc J. W. Evers*

Main category: cs.CV

> 本文提出了一种基于计算机视觉的方法，用于量化帕金森病患者的运动特征，通过该方法预测评分的准确率高于目前最先进的方法，同时提供深入了解个体运动特征的能力。

<details>
  <summary>Details</summary>

**Motivation:** 准确量化帕金森病的运动特征对于监测疾病进展和优化治疗策略至关重要。现有的手指敲击测试依赖于临床医生的主观评估，这种评估容易受到评分者间和评分者内变异的影响，且无法提供测试过程中捕捉到的个体运动特征的具体见解。

**Method:** 本论文提出了一种基于计算机视觉的细化方法，用于从视频记录中量化帕金森病的运动特征。研究提出了四个临床相关的特征集来描述运动迟缓、动作缓慢、序列效应和犹豫停顿。

**Result:** 通过使用主要成分分析和方差极大化旋转，他们的视频特征与四个缺陷相对应，并且还能够识别序列效应和犹豫停顿缺陷更细粒度的区分。此外，在预测MDS-UPDRS评分这一任务中，与最先进的方法相比，该方法达到了更高的准确率，并且仍能提供对个体手指敲击运动特征的可解释量化。

**Conclusion:** 本论文定义了一种实用框架，用于帕金森病运动特性的客观评估，该框架可能同时应用于临床和远程环境中。未来的研究需要评估其对症状治疗和疾病进展的敏感性。

**Abstract:** Accurately quantifying motor characteristics in Parkinson disease (PD) is
crucial for monitoring disease progression and optimizing treatment strategies.
The finger-tapping test is a standard motor assessment. Clinicians visually
evaluate a patient's tapping performance and assign an overall severity score
based on tapping amplitude, speed, and irregularity. However, this subjective
evaluation is prone to inter- and intra-rater variability, and does not offer
insights into individual motor characteristics captured during this test. This
paper introduces a granular computer vision-based method for quantifying PD
motor characteristics from video recordings. Four sets of clinically relevant
features are proposed to characterize hypokinesia, bradykinesia, sequence
effect, and hesitation-halts. We evaluate our approach on video recordings and
clinical evaluations of 74 PD patients from the Personalized Parkinson Project.
Principal component analysis with varimax rotation shows that the video-based
features corresponded to the four deficits. Additionally, video-based analysis
has allowed us to identify further granular distinctions within sequence effect
and hesitation-halts deficits. In the following, we have used these features to
train machine learning classifiers to estimate the Movement Disorder Society
Unified Parkinson Disease Rating Scale (MDS-UPDRS) finger-tapping score.
Compared to state-of-the-art approaches, our method achieves a higher accuracy
in MDS-UPDRS score prediction, while still providing an interpretable
quantification of individual finger-tapping motor characteristics. In summary,
the proposed framework provides a practical solution for the objective
assessment of PD motor characteristics, that can potentially be applied in both
clinical and remote settings. Future work is needed to assess its
responsiveness to symptomatic treatment and disease progression.

</details>


### [44] [Reinforcement Learning-Based Dynamic Grouping for Tubular Structure Tracking](https://arxiv.org/abs/2506.18930)
*Chong Di,Shuwang Zhou,Da Chen,Jean-Marie Mirebeau,Minglei Shu,Laurent D. Cohen*

Main category: cs.CV

> 本文提出了一种利用Q-Learning进行动态段追踪的新框架，适用于管状结构的追踪，效果显著优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于点和基于段的方法在处理管状结构的追踪问题时，效率低或需要大量的先验知识。

**Method:** 本文提出了一种将基于段的追踪视为马尔可夫决策过程（MDP）的新框架，利用Q-Learning动态探索一个段的图，按需计算边的权重并自适应扩展搜索空间。

**Result:** 实验结果表明，该方法显著优于现有的基于点和基于段的方法，在处理复杂的管状结构时，效果显著。

**Conclusion:** 该方法有效地处理了复杂的拓扑结构，并保持了全局路径的一致性，无需依赖大量的结构性先验知识。

**Abstract:** The computation of minimal paths for the applications in tracking tubular
structures such as blood vessels and roads is challenged by complex
morphologies and environmental variations. Existing approaches can be roughly
categorized into two research lines: the point-wise based models and the
segment-wise based models. Although segment-wise approaches have obtained
promising results in many scenarios, they often suffer from computational
inefficiency and heavily rely on a prescribed prior to fit the target elongated
shapes. We propose a novel framework that casts segment-wise tracking as a
Markov Decision Process (MDP), enabling a reinforcement learning approach. Our
method leverages Q-Learning to dynamically explore a graph of segments,
computing edge weights on-demand and adaptively expanding the search space.
This strategy avoids the high cost of a pre-computed graph and proves robust to
incomplete initial information. Experimental reuslts on typical tubular
structure datasets demonstrate that our method significantly outperforms
state-of-the-art point-wise and segment-wise approaches. The proposed method
effectively handles complex topologies and maintains global path coherence
without depending on extensive prior structural knowledge.

</details>


### [45] [Bird's-eye view safety monitoring for the construction top under the tower crane](https://arxiv.org/abs/2506.18938)
*Yanke Wang,Yu Hin Ng,Haobo Liang,Ching-Wei Chang,Hao Chen*

Main category: cs.CV

> 本文提出了一种基于AI的全自动安全监控系统，用于塔式起重机的鸟瞰视角吊装监测，确保工人安全，避免碰撞。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于塔式起重机操作中越来越多的自动化和智能化程序，以及自动化技术在安全问题中的重要性，特别是保护塔式起重机与建筑顶部区域之间的工人的安全，在采用任何其他先进功能之前，这变得尤为迫切。

**Method:** 我们提出了一种基于AI的全自动安全监控系统，用于从鸟瞰视角监测塔式起重机的吊装过程。该系统整合了摄像头和LiDAR收集的信息，实现了3D数据融合，用于定位人类工人和MiCs（模块化集成建筑），并警报起重机操作员避免碰撞和保护工人的安全。

**Result:** 通过对所涉及方法的组件分析，验证了系统的准确性和有效性。实际施工现场的显示和可视化证明了该系统可以作为有价值的工地安全监控工具包。

**Conclusion:** 我们展示了在塔式起重机操作中融合摄像头和LiDAR的3D信息，提出了一个软硬件结合的安全监控系统，并在实际工地现场展示了它的应用价值。

**Abstract:** The tower crane is involving more automated and intelligent operation
procedure, and importantly, the application of automation technologies to the
safety issues is imperative ahead of the utilization of any other advances.
Among diverse risk management tasks on site, it is essential to protect the
human workers on the workspace between the tower crane and constructed building
top area (construction top) from the bird's-eye view, especially with Modular
Integrated Construction (MiC) lifted. Also, the camera and Light Detection And
Ranging (LiDAR) can capture abundant 3D information on site, which is however
yet made the best use. Considering the safety protection for humans and tower
cranes, we present an AI-based fully automated safety monitoring system for
tower crane lifting from the bird's-eye view, surveilling to shield the human
workers on the construction top and avoid cranes' collision by alarming the
crane operator. The system achieved a 3D data fusion for localization of humans
and MiCs by integrating the captured information from camera and LiDAR. The
state-of-the-art methods were explored and implemented into our proposed
software pipeline coupled with the hardware and display systems. Furthermore,
we conducted an analysis of the components in the pipeline to verify the
accuracy and effectiveness of the involved methods. The display and
visualization on the real site proved that our system can serve as a valuable
safety monitoring toolkit on site.

</details>


### [46] [Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction](https://arxiv.org/abs/2506.18939)
*Rui An,Yifeng Zhang,Ziran Liang,Wenqi Fan,Yuxuan Liang,Xuequn Shang,Qing Li*

Main category: cs.CV

> 本研究提出Damba-ST模型，利用线性复杂度优势同时提高城市时空预测的跨域泛化能力，通过划分共享和独立的子空间和使用域适配器，实现了高效且准确的预测，并且具备在新城市环境中的零样本泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决现有的基于Transformer的模型在处理城市时空预测任务时计算复杂度高和内存开销大的问题。同时，直接应用Mamba模型又会由于时空异质性和隐藏状态更新的递归机制导致模型性能下降。因此，提出了一种改进的模型来克服这些挑战。

**Method:** 我们提出了一种名为Damba-ST的新模型，旨在通过两种核心创新来提高跨域泛化能力。首先，模型采用域自适应状态空间模型，将潜在表示空间划分为学习跨域共同特征的共享子空间和捕获域内辨别特征的特定子空间。其次，引入了三种不同的域适配器，作为域感知代理，用于弥合不同域之间的分布差距并促进跨域共同特征的对齐。

**Result:** 实验结果表明，Damba-ST模型具有良好的跨域泛化能力和高效性。它在预测任务中达到了最先进的性能水平，并展示了强大的零样本泛化能力。

**Conclusion:** 研究结论为，Damba-ST模型能够有效地进行城市时空预测，并且具备零样本泛化能力，能够在新的城市环境中无缝部署，无需大量的再训练或微调。

**Abstract:** Training urban spatio-temporal foundation models that generalize well across
diverse regions and cities is critical for deploying urban services in unseen
or data-scarce regions. Recent studies have typically focused on fusing
cross-domain spatio-temporal data to train unified Transformer-based models.
However, these models suffer from quadratic computational complexity and high
memory overhead, limiting their scalability and practical deployment. Inspired
by the efficiency of Mamba, a state space model with linear time complexity, we
explore its potential for efficient urban spatio-temporal prediction. However,
directly applying Mamba as a spatio-temporal backbone leads to negative
transfer and severe performance degradation. This is primarily due to
spatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden
state updates, which limit cross-domain generalization. To overcome these
challenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for
efficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear
complexity advantage while significantly enhancing its adaptability to
heterogeneous domains. Specifically, we introduce two core innovations: (1) a
domain-adaptive state space model that partitions the latent representation
space into a shared subspace for learning cross-domain commonalities and
independent, domain-specific subspaces for capturing intra-domain
discriminative features; (2) three distinct Domain Adapters, which serve as
domain-aware proxies to bridge disparate domain distributions and facilitate
the alignment of cross-domain commonalities. Extensive experiments demonstrate
the generalization and efficiency of Damba-ST. It achieves state-of-the-art
performance on prediction tasks and demonstrates strong zero-shot
generalization, enabling seamless deployment in new urban environments without
extensive retraining or fine-tuning.

</details>


### [47] [From Pixels and Words to Waves: A Unified Framework for Spectral Dictionary vLLMs](https://arxiv.org/abs/2506.18943)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CV

> 提出SDict-VLM，通过频谱字典标记混合器去除卷积和二次自我注意机制，实现了性能接近BLIP-2但参数少60%，GPU内存需求少2.3倍，推理速度提高2.2倍。

<details>
  <summary>Details</summary>

**Motivation:** 大多数最先进的系统依赖于计算密集型组件，比如视觉编码器中的卷积和多模态融合的二次自我注意。这项工作通过移除这两个组件来提高效率和可解释性。

**Method:** 引入了频谱字典标记混合器，每个图像块或单词片段表示为可学习频率原子的稀疏组合，从而消除了视觉编码器中的卷积和多模态融合的二次自我注意。

**Result:** 11亿参数的原型模型SDict-VLM在MS-COCO标题生成任务上达到了BLEU-4 39.2，CIDEr 127.5，SPICE 27.0，并在VQAv2任务上的准确率为50.3%。

**Conclusion:** 这是首个消除卷积和自注意力机制的同时匹配中型变压器基线的视觉-语言模型。其O(L log L)复杂度，共享频率字典有利于透明的跨模态对齐，并可以在精度和计算之间提供可调的权衡，为高效的和可解释的视觉-语言模型铺平了道路。

**Abstract:** Vision-language models (VLMs) unify computer vision and natural language
processing in a single architecture capable of interpreting and describing
images. Most state-of-the-art systems rely on two computationally intensive
components: convolutions in the vision encoder and quadratic self-attention for
multimodal fusion. This work removes both by introducing a spectral dictionary
token mixer, which represents each image patch or wordpiece as a sparse
combination of learnable frequency atoms. Our 1.1B-parameter prototype,
SDict-VLM, achieves BLEU-4 of 39.2, CIDEr of 127.5, and SPICE of 27.0 on
MS-COCO captioning, along with 50.3 percent accuracy on VQAv2. These results
close approximately 85 percent of the performance gap to BLIP-2 while using 60
percent fewer parameters, 2.3 times less peak GPU memory, and 2.2 times faster
inference than PaLI-3. To our knowledge, this is the first VLM to eliminate
both convolutions and self-attention while matching mid-scale transformer
baselines. In addition to its O(L log L) complexity, the shared frequency
dictionary enables transparent cross-modal alignment and offers a tunable
trade-off between accuracy and compute, paving the way for efficient and
interpretable VLMs.

</details>


### [48] [DiffRIS: Enhancing Referring Remote Sensing Image Segmentation with Pre-trained Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18946)
*Zhe Dong,Yuzhe Sun,Tianzhu Liu,Yanfeng Gu*

Main category: cs.CV

> 提出DiffRIS框架，利用预训练的文本到图像扩散模型提高遥感图像语义理解，实验表明该框架在基准数据集上表现优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 遥感图像分割存在尺度变化、多样方向和语义歧义等挑战。DiffRIS旨在通过跨模态对齐提升自然语言描述的遥感区域精确划分。

**Method:** DiffRIS框架，包含上下文感知适配器（CP-adapter）和渐进跨模态推理解码器（PCMRD），利用预训练的文本到图像扩散模型进行语义理解，提高跨模态对齐。CP-adapter通过全局上下文建模和对象感知推理动态调整语言特征，PCMRD通过多尺度特征交互实现细粒度的语义对齐。

**Result:** 在RRSIS-D、RefSegRS和RISBench三个基准数据集上，DiffRIS在所有标准指标上均优于现有方法，建立了远程感知图像分割任务的新标准。

**Conclusion:** 实验表明，通过适应性框架利用预训练扩散模型显著提升了遥感任务的表现。DiffRIS框架在远程感知图像分割中表现出色，是一项重要进步。

**Abstract:** Referring remote sensing image segmentation (RRSIS) enables the precise
delineation of regions within remote sensing imagery through natural language
descriptions, serving critical applications in disaster response, urban
development, and environmental monitoring. Despite recent advances, current
approaches face significant challenges in processing aerial imagery due to
complex object characteristics including scale variations, diverse
orientations, and semantic ambiguities inherent to the overhead perspective. To
address these limitations, we propose DiffRIS, a novel framework that harnesses
the semantic understanding capabilities of pre-trained text-to-image diffusion
models for enhanced cross-modal alignment in RRSIS tasks. Our framework
introduces two key innovations: a context perception adapter (CP-adapter) that
dynamically refines linguistic features through global context modeling and
object-aware reasoning, and a progressive cross-modal reasoning decoder (PCMRD)
that iteratively aligns textual descriptions with visual regions for precise
segmentation. The CP-adapter bridges the domain gap between general
vision-language understanding and remote sensing applications, while PCMRD
enables fine-grained semantic alignment through multi-scale feature
interaction. Comprehensive experiments on three benchmark datasets-RRSIS-D,
RefSegRS, and RISBench-demonstrate that DiffRIS consistently outperforms
existing methods across all standard metrics, establishing a new
state-of-the-art for RRSIS tasks. The significant performance improvements
validate the effectiveness of leveraging pre-trained diffusion models for
remote sensing applications through our proposed adaptive framework.

</details>


### [49] [GLIMPSE: Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation for Generative LVLMs](https://arxiv.org/abs/2506.18985)
*Guanxi Shen*

Main category: cs.CV

> 我们介绍了GLIMPSE，这是一款用来解析LVLMs在生成文本应答时如何分配视觉注意力的框架，它优于之前的解释方法，可以生成跨模态推理的归属热图，提供更细粒度的洞察力，追踪令牌级别的推理动态。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大视觉语言模型（LVLMs）解锁了从视觉输入生成连贯反应的前所未有的能力，但解释这些模型在生成自由形式的文本应答时如何分配视觉注意力仍是重要挑战。理解模型行为、诊断幻觉、暴露偏见和确保透明度都需要解决这一问题。

**Method:** 我们提出了GLIMPSE，这是一个轻量级、模型不可知的框架，用于在开放式的视觉问答任务中可视化LVLMs依赖的显著图像区域，同时揭示多模态文本的显着性。GLIMPSE融合了基于梯度加权的注意力、自适应层传播和加权令牌聚合，生成跨模态推理的全面响应级归属热图。

**Result:** 与以往的可解释性方法相比，GLIMPSE能够更好地生成跨模态推理的归属热图，与人类的理解更加一致。它提供了一个可以追踪推理动态，以及分析系统性人类注意力不一致，幻觉及偏见的分析性可解释AI方法。

**Conclusion:** 我们通过GLIMPSE展示了分析可解释AI（XAI）的方法，用于揭示大视觉语言模型（LVLMs）跨模态归属的细粒度洞察，追踪令牌级别的推理动态，以及分析系统性的人类注意力不一致、幻觉和偏见。

**Abstract:** Recent advances in large vision language models (LVLMs) have unlocked
unprecedented capabilities in generating coherent responses from visual inputs.
However, interpreting where LVLMs direct their visual attention while
generating free-form textual responses remains a significant challenge, yet is
essential for understanding model behavior, diagnosing hallucination, exposing
bias and ensuring transparency. We introduce GLIMPSE (Gradient-Layer Importance
Mapping for Prompted Visual Saliency Explanation), a lightweight,
model-agnostic framework for visualizing the salient image regions that LVLMs
rely upon during open-ended visual question answering (VQA), while concurrently
revealing the multimodal textual saliency. GLIMPSE fuses gradient-weighted
attention, adaptive layer propagation, and weighted token aggregation to
produce holistic response-level attribution heat maps for interpreting
cross-modal reasoning, outperforming prior interpretability methods in
human-alignment. We demonstrate an analytic explainable AI (XAI) approach using
GLIMPSE to uncover fine-grained insights into LVLM cross-modal attribution,
trace token-level reasoning dynamics, and analyze systematic human-attention
misalignment, hallucination, and bias.

</details>


### [50] [Diffusion Transformer-to-Mamba Distillation for High-Resolution Image Generation](https://arxiv.org/abs/2506.18999)
*Yuan Yao,Yicong Hong,Difan Liu,Long Mai,Feng Liu,Jiebo Luo*

Main category: cs.CV

> 该论文提出了扩散Transformer到Mamba知识蒸馏方法，建立了一个高效训练管线来解决直接训练Mamba模型的挑战性问题，展示了其在生成高分辨率图像时的有效性和未来探索潜力。

<details>
  <summary>Details</summary>

**Motivation:** 解决扩散变换器中自注意力机制带来的二次计算复杂度问题，减少高分辨率图像生成中的计算成本。直接训练Mamba模型仍然具有挑战性，提出一种有效的方法从基于自注意力的变压器过渡到线性复杂度的状态空间模型Mamba。

**Method:** 通过提出扩散自注意力和Mamba混合模型来实现在保持全局依赖性的同时提高效率。通过逐层教师强制和基于特征的知识提炼，T2MD解决了从零开始训练状态空间模型的困难和高昂成本问题。

**Result:** 实验表明，所提出的训练路径实现了低开销的高质量文本到图像生成，并且结果证明了使用顺序和因果Mamba模型生成非因果视觉输出的可行性。

**Conclusion:** 文本展示了一条从原始512x512分辨率的基础模型开始，通过轻量级适应和高分辨率微调推进生成至2048x2048图像的有效训练路径，证明了这种训练路径的有效性和未来探索的潜力。

**Abstract:** The quadratic computational complexity of self-attention in diffusion
transformers (DiT) introduces substantial computational costs in
high-resolution image generation. While the linear-complexity Mamba model
emerges as a potential alternative, direct Mamba training remains empirically
challenging. To address this issue, this paper introduces diffusion
transformer-to-mamba distillation (T2MD), forming an efficient training
pipeline that facilitates the transition from the self-attention-based
transformer to the linear complexity state-space model Mamba. We establish a
diffusion self-attention and Mamba hybrid model that simultaneously achieves
efficiency and global dependencies. With the proposed layer-level teacher
forcing and feature-based knowledge distillation, T2MD alleviates the training
difficulty and high cost of a state space model from scratch. Starting from the
distilled 512$\times$512 resolution base model, we push the generation towards
2048$\times$2048 images via lightweight adaptation and high-resolution
fine-tuning. Experiments demonstrate that our training path leads to low
overhead but high-quality text-to-image generation. Importantly, our results
also justify the feasibility of using sequential and causal Mamba models for
generating non-causal visual output, suggesting the potential for future
exploration.

</details>


### [51] [Orthogonal Projection Subspace to Aggregate Online Prior-knowledge for Continual Test-time Adaptation](https://arxiv.org/abs/2506.19022)
*Jinlong Li,Dong Zhao,Qi Zang,Zequn Jie,Lin Ma,Nicu Sebe*

Main category: cs.CV

> 本文提出了一种新的持续测试时间适应性方法OoPk，通过正交投影子空间缓解灾难性遗忘，以及使用有效的图像掩码策略来增强领域适应性，并在语义分割任务实验中取得了优异的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有持续测试时间适应性方法主要集中在缓解灾难性遗忘和误差积累问题，但它们在平衡性能和模型适应效率方面遇到挑战，特别是在复杂任务如语义分割中表现突出。因此，本文提出了一种新的方法来解决这些问题。

**Method:** OoPk方法首先通过正交投影子空间来适应新领域，同时保留预训练模型的知识完整性，以缓解灾难性遗忘问题。然后通过一种激进但高效的图像掩码策略来模仿潜在目标动态变化，增强学生模型的领域适应性，并逐步改进教师模型的知识，从而减少误差累积，确保高质量的伪标签。

**Result:** 实验结果表明，OoPk方法在语义分割任务的持续测试时间适应性基准测试中，超越了之前的持续测试时间适应性方法，实现了有竞争力的表现。

**Conclusion:** 总体来说，OoPk方法通过正交投影子空间和高效的在线先验知识聚集策略，能够有效缓解灾难性遗忘和误差累积问题，并在语义分割持续测试时间适应性任务中展示出了优越的性能。

**Abstract:** Continual Test Time Adaptation (CTTA) is a task that requires a source
pre-trained model to continually adapt to new scenarios with changing target
distributions. Existing CTTA methods primarily focus on mitigating the
challenges of catastrophic forgetting and error accumulation. Though there have
been emerging methods based on forgetting adaptation with parameter-efficient
fine-tuning, they still struggle to balance competitive performance and
efficient model adaptation, particularly in complex tasks like semantic
segmentation. In this paper, to tackle the above issues, we propose a novel
pipeline, Orthogonal Projection Subspace to aggregate online Prior-knowledge,
dubbed OoPk. Specifically, we first project a tuning subspace orthogonally
which allows the model to adapt to new domains while preserving the knowledge
integrity of the pre-trained source model to alleviate catastrophic forgetting.
Then, we elaborate an online prior-knowledge aggregation strategy that employs
an aggressive yet efficient image masking strategy to mimic potential target
dynamism, enhancing the student model's domain adaptability. This further
gradually ameliorates the teacher model's knowledge, ensuring high-quality
pseudo labels and reducing error accumulation. We demonstrate our method with
extensive experiments that surpass previous CTTA methods and achieve
competitive performances across various continual TTA benchmarks in semantic
segmentation tasks.

</details>


### [52] [LEGATO: Large-scale End-to-end Generalizable Approach to Typeset OMR](https://arxiv.org/abs/2506.19065)
*Guang Yang,Victoria Ebert,Nazif Tamer,Luiza Pozzobon,Noah A. Smith*

Main category: cs.CV

> Legato is a new transformer model that excels in recognizing and outputting multi-page music scores in ABC notation, setting a new benchmark in Optical Music Recognition.

<details>
  <summary>Details</summary>

**Motivation:** To develop a model capable of recognizing comprehensive music scores and generating output in a widely accepted notation format.

**Method:** Legato combines a pretrained vision encoder with a decoder trained to produce ABC notation.

**Result:** <tool_call>
{{"name": "Structure", "arguments": {"tldr": "Legato is a new transformer model designed for optical music recognition, which can recognize full-page or multi-page typeset music scores and generate documents in ABC notation. It outperforms existing models across various datasets.", "motivation": "To create a new OMR model that can process complex, multi-page sheet music and output in a human-readable, concise format not previously addressed by other models.", "method": "Developed a pretrained vision encoder and an ABC decoder trained on a large dataset of more than 214K images.", "result": "The model exhibited strong generalization performance across different types of music scores and achieved state-of-the-art performance.", "conclusion": "Legato fills a gap in the current OMR landscape by being the first model to work with full-page/multi-page music scores and produce ABC notation output, offering a significant advancement in the field of OMR."}}}
</tool_call>

**Conclusion:** This analysis provides a summary of the paper's key points, presenting the motivation, methodology, results, and conclusions drawn from Legato, a new model in the field of Optical Music Recognition.

**Abstract:** We propose Legato, a new end-to-end transformer model for optical music
recognition (OMR). Legato is the first large-scale pretrained OMR model capable
of recognizing full-page or multi-page typeset music scores and the first to
generate documents in ABC notation, a concise, human-readable format for
symbolic music. Bringing together a pretrained vision encoder with an ABC
decoder trained on a dataset of more than 214K images, our model exhibits the
strong ability to generalize across various typeset scores. We conduct
experiments on a range of datasets and demonstrate that our model achieves
state-of-the-art performance. Given the lack of a standardized evaluation for
end-to-end OMR, we comprehensively compare our model against the previous state
of the art using a diverse set of metrics.

</details>


### [53] [HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models](https://arxiv.org/abs/2506.19072)
*Yimu Wang,Mozhgan Nasr Azadani,Sean Sedwards,Krzysztof Czarnecki*

Main category: cs.CV

> HAWAII is a new framework that aggregates knowledge from multiple visual experts into a single vision encoder, using teacher-specific LoRA adapters and a router for efficient and selective knowledge distillation.

<details>
  <summary>Details</summary>

**Motivation:** To enhance VLMs' visual understanding capability efficiently without increasing computational costs.

**Method:** HAWAII uses fine-grained and coarse-grained distillation methods along with teacher-specific Low-Rank Adaptation (LoRA) adapters and a router to avoid conflicts and to transfer knowledge selectively.

**Result:** Experiments show HAWAII performs better than popular VLMs across various tasks.

**Conclusion:** The framework demonstrates a promising method to improve VLMs efficiently with less computational cost.

**Abstract:** Improving the visual understanding ability of vision-language models (VLMs)
is crucial for enhancing their performance across various tasks. While using
multiple pretrained visual experts has shown great promise, it often incurs
significant computational costs during training and inference. To address this
challenge, we propose HAWAII, a novel framework that distills knowledge from
multiple visual experts into a single vision encoder, enabling it to inherit
the complementary strengths of several experts with minimal computational
overhead. To mitigate conflicts among different teachers and switch between
different teacher-specific knowledge, instead of using a fixed set of adapters
for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation
(LoRA) adapters with a corresponding router. Each adapter is aligned with a
specific teacher, avoiding noisy guidance during distillation. To enable
efficient knowledge distillation, we propose fine-grained and coarse-grained
distillation. At the fine-grained level, token importance scores are employed
to emphasize the most informative tokens from each teacher adaptively. At the
coarse-grained level, we summarize the knowledge from multiple teachers and
transfer it to the student using a set of general-knowledge LoRA adapters with
a router. Extensive experiments on various vision-language tasks demonstrate
the superiority of HAWAII, compared to the popular open-source VLMs.

</details>


### [54] [Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition](https://arxiv.org/abs/2506.19079)
*Iosif Tsangko,Andreas Triantafyllopoulos,Adem Abdelmoula,Adria Mallol-Ragolta,Bjoern W. Schuller*

Main category: cs.CV

> 本文探究了视觉语言模型在情绪识别中依赖的视觉线索，发现模型依赖的面部特征如眉毛等展示了内部推理的一致性，但也提出了模型可能导致的偏差和公平性问题。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索视觉语言模型如何在零样本设置下识别情绪，特别是它们依赖的心理学基础的视觉线索还是表面学习的线索，以及这些线索可能引发的风险，比如偏差和公平性问题。

**Method:** 本文通过在AffectNet数据集的牙齿标注子集上对不同规模的视觉语言模型（VLMs）进行基准测试，并通过结构化的 introspection 方法分析了表现最好的模型GPT-4o的内部推理机制，探讨了这些模型在情绪识别中依赖的视觉线索及其心理基础或表面学习特性。

**Result:** 研究发现模型的性能在不同情况下有显著变化，尤其与人物是否露出牙齿有关。通过深入分析，研究者指出模型的情绪推理主要依赖如眉毛位置这样的面部特征，揭示了模型内部逻辑的内在一致性。

**Conclusion:** 研究结果表明了基础模型行为的涌现性质，但也揭示了在心理健康和教育等敏感领域使用这些模型时可能面临的挑战和风险，包括捷径学习、偏差以及公平性问题。

**Abstract:** Foundation Models (FMs) are rapidly transforming Affective Computing (AC),
with Vision Language Models (VLMs) now capable of recognising emotions in zero
shot settings. This paper probes a critical but underexplored question: what
visual cues do these models rely on to infer affect, and are these cues
psychologically grounded or superficially learnt? We benchmark varying scale
VLMs on a teeth annotated subset of AffectNet dataset and find consistent
performance shifts depending on the presence of visible teeth. Through
structured introspection of, the best-performing model, i.e., GPT-4o, we show
that facial attributes like eyebrow position drive much of its affective
reasoning, revealing a high degree of internal consistency in its
valence-arousal predictions. These patterns highlight the emergent nature of
FMs behaviour, but also reveal risks: shortcut learning, bias, and fairness
issues especially in sensitive domains like mental health and education.

</details>


### [55] [RareSpot: Spotting Small and Rare Wildlife in Aerial Imagery with Multi-Scale Consistency and Context-Aware Augmentation](https://arxiv.org/abs/2506.19087)
*Bowen Zhang,Jesse T. Boulerice,Nikhil Kuniyil,Charvi Mendiratta,Satish Kumar,Hila Shamon,B. S. Manjunath*

Main category: cs.CV

> This paper proposes a new framework called RareSpot, designed to improve the detection of small, rare wildlife in aerial imagery, enhancing accuracy significantly over baseline methods and providing a broad application across multiple wildlife datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to effectively detect small and rare wildlife in aerial imagery, which is important for conservation purposes but presents significant technical challenges.

**Method:** RareSpot, a robust detection framework integrating multi-scale consistency learning and context-aware augmentation, is proposed. This approach leverages structured alignment across feature pyramids and strategically synthesizes challenging training instances to enhance fine-grained object representation and boost detection performance.

**Result:** Evaluated on an expert-annotated prairie dog drone imagery benchmark, the method achieves state-of-the-art performance, improving detection accuracy by over 35% compared to baseline methods.

**Conclusion:** The study not only contributes to ecological monitoring but also sets a new standard for detecting small, rare species in complex aerial scenes.

**Abstract:** Automated detection of small and rare wildlife in aerial imagery is crucial
for effective conservation, yet remains a significant technical challenge.
Prairie dogs exemplify this issue: their ecological importance as keystone
species contrasts sharply with their elusive presence--marked by small size,
sparse distribution, and subtle visual features--which undermines existing
detection approaches. To address these challenges, we propose RareSpot, a
robust detection framework integrating multi-scale consistency learning and
context-aware augmentation. Our multi-scale consistency approach leverages
structured alignment across feature pyramids, enhancing fine-grained object
representation and mitigating scale-related feature loss. Complementarily,
context-aware augmentation strategically synthesizes challenging training
instances by embedding difficult-to-detect samples into realistic environmental
contexts, significantly boosting model precision and recall. Evaluated on an
expert-annotated prairie dog drone imagery benchmark, our method achieves
state-of-the-art performance, improving detection accuracy by over 35% compared
to baseline methods. Importantly, it generalizes effectively across additional
wildlife datasets, demonstrating broad applicability. The RareSpot benchmark
and approach not only support critical ecological monitoring but also establish
a new foundation for detecting small, rare species in complex aerial scenes.

</details>


### [56] [Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency Models](https://arxiv.org/abs/2506.19103)
*Ilia Beletskii,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Result:** {
  "tldr": "该框架通过使用一致性模型，能在四个步骤内完成高质量的图像编辑，同时最大限度地保留原始图像的结构和语义信息，提供编辑能力和内容保留性之间的可控权衡。", 
  "motivation": "鉴于目前的图像编辑模型计算成本高且重建质量不佳，作者旨在解决这些问题，通过引入一致性模型优化，提高图像反转和编辑的质量，并使编辑过程更加高效。", 
  "method": "通过采用一种循环一致性优化策略，提高图像的重建准确性，同时提供性能优化，达到了可编辑性和内容保持性之间的平衡。", 
  "result": "此框架在不同的图像编辑任务和数据集上达到了最先进的性能表现，并且其效率远超全步扩散模型。", 
  "conclusion": "研究表明，他们提出的方法不但能够达到与全步扩散模型相匹配或更优的性能，还具有显著的效率优势。该框架为高精度图像编辑提供了一种新的、可行的方法。"}
}

**Conclusion:** 

**Abstract:** Recent advances in image editing with diffusion models have achieved
impressive results, offering fine-grained control over the generation process.
However, these methods are computationally intensive because of their iterative
nature. While distilled diffusion models enable faster inference, their editing
capabilities remain limited, primarily because of poor inversion quality.
High-fidelity inversion and reconstruction are essential for precise image
editing, as they preserve the structural and semantic integrity of the source
image. In this work, we propose a novel framework that enhances image inversion
using consistency models, enabling high-quality editing in just four steps. Our
method introduces a cycle-consistency optimization strategy that significantly
improves reconstruction accuracy and enables a controllable trade-off between
editability and content preservation. We achieve state-of-the-art performance
across various image editing tasks and datasets, demonstrating that our method
matches or surpasses full-step diffusion models while being substantially more
efficient. The code of our method is available on GitHub at
https://github.com/ControlGenAI/Inverse-and-Edit.

</details>


### [57] [PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Scenes](https://arxiv.org/abs/2506.19117)
*Christina Ourania Tze,Daniel Dauner,Yiyi Liao,Dzmitry Tsishkou,Andreas Geiger*

Main category: cs.CV

> PrITTI, a latent diffusion-based framework, uses primitives as foundational components for generating 3D semantic scenes. It improves over voxel-based approaches by being more memory-efficient and easier to manipulate, offering a flexible solution for scene editing and various applications.

<details>
  <summary>Details</summary>

**Motivation:** The motivation arises from the challenges of memory intensiveness, fixed resolution limitations, and difficulty in editing of voxel-based representations in 3D semantic scene generation. Primitives, offering a compact and easy to manipulate solution, are considered as a suitable alternative.

**Method:** Our method adopts a hybrid representation, modeling ground surfaces in a rasterized format while encoding objects as vectorized 3D primitives. This decomposition is also reflected in a structured latent representation that enables flexible scene manipulation. A Cholesky-based parameterization that jointly encodes object size and orientation is introduced to overcome orientation ambiguities.

**Result:** Experiments on the KITTI-360 dataset show that PrITTI outperforms a voxel-based baseline in generation quality, while reducing memory requirements by up to $3	imes$. PrITTI also supports scene manipulation and various applications like scene inpainting and photo-realistic street-view synthesis.

**Conclusion:** PrITTI demonstrates its effectiveness in generating high-quality 3D semantic scene layouts efficiently and with reduced memory usage. It provides a flexible and editable solution by leveraging a hybrid representation based on primitives, significantly outperforming voxel-based models.

**Abstract:** Large-scale 3D semantic scene generation has predominantly relied on
voxel-based representations, which are memory-intensive, bound by fixed
resolutions, and challenging to edit. In contrast, primitives represent
semantic entities using compact, coarse 3D structures that are easy to
manipulate and compose, making them an ideal representation for this task. In
this paper, we introduce PrITTI, a latent diffusion-based framework that
leverages primitives as the main foundational elements for generating
compositional, controllable, and editable 3D semantic scene layouts. Our method
adopts a hybrid representation, modeling ground surfaces in a rasterized format
while encoding objects as vectorized 3D primitives. This decomposition is also
reflected in a structured latent representation that enables flexible scene
manipulation of ground and object components. To overcome the orientation
ambiguities in conventional encoding methods, we introduce a stable
Cholesky-based parameterization that jointly encodes object size and
orientation. Experiments on the KITTI-360 dataset show that PrITTI outperforms
a voxel-based baseline in generation quality, while reducing memory
requirements by up to $3\times$. In addition, PrITTI enables direct
instance-level manipulation of objects in the scene and supports a range of
downstream applications, including scene inpainting, outpainting, and
photo-realistic street-view synthesis.

</details>


### [58] [Lightweight RGB-T Tracking with Mobile Vision Transformers](https://arxiv.org/abs/2506.19154)
*Mahdi Falaki,Maria A. Amer*

Main category: cs.CV

> 提出了一种基于MobileViT的轻量级RGB-T跟踪算法，通过渐进融合框架提高了目标定位准确性，同时降低模型参数量和加快推理速度。

<details>
  <summary>Details</summary>

**Motivation:** 传统的单模态目标跟踪算法在低光照和恶劣天气条件下遇到困难。尽管基于Vision Transformers的多模态跟踪器性能较强，但模型尺寸较大，计算昂贵。本研究旨在解决此问题，并提出轻量级的解决方案。

**Method:** 基于Mobile Vision Transformers (MobileViT)的轻量级RGB-T跟踪算法，提出了一种渐进融合框架，通过可分离注意力机制，在模板和搜索区域之间同时学习单模态和多模态交互。

**Result:** 该算法在保持小模型尺寸和快速推理速度的同时，实现了有效的特征表示和更准确的目标定位。相比高效的多模态跟踪器，该模型具有相当的准确性，参数量少于4百万，GPU推理速度达到每秒122帧。

**Conclusion:** 本研究首次提出了使用Mobile Vision Transformers的RGB-T跟踪算法，该算法在保持高效性能的同时大大降低了计算成本。后续代码和模型权重将公开发布。

**Abstract:** Single-modality object tracking (e.g., RGB-only) encounters difficulties in
challenging imaging conditions, such as low illumination and adverse weather
conditions. To solve this, multimodal tracking (e.g., RGB-T models) aims to
leverage complementary data such as thermal infrared features. While recent
Vision Transformer-based multimodal trackers achieve strong performance, they
are often computationally expensive due to large model sizes. In this work, we
propose a novel lightweight RGB-T tracking algorithm based on Mobile Vision
Transformers (MobileViT). Our tracker introduces a progressive fusion framework
that jointly learns intra-modal and inter-modal interactions between the
template and search regions using separable attention. This design produces
effective feature representations that support more accurate target
localization while achieving a small model size and fast inference speed.
Compared to state-of-the-art efficient multimodal trackers, our model achieves
comparable accuracy while offering significantly lower parameter counts (less
than 4 million) and the fastest GPU inference speed of 122 frames per second.
This paper is the first to propose a tracker using Mobile Vision Transformers
for RGB-T tracking and multimodal tracking at large. Tracker code and model
weights will be made publicly available upon acceptance.

</details>


### [59] [PRISM: Perceptual Recognition for Identifying Standout Moments in Human-Centric Keyframe Extraction](https://arxiv.org/abs/2506.19168)
*Mert Can Cakmak,Nitin Agarwal,Diwash Poudel*

Main category: cs.CV

> PRISM, a lightweight framework for keyframe extraction, uses perceptual color difference metrics in the CIELAB color space. It is training-free, computationally efficient, and effective in both structured and unstructured video content.

<details>
  <summary>Details</summary>

**Motivation:** The central role of online videos in shaping political discourse and amplifying cyber social threats makes it crucial to detect impactful moments in video content for content moderation, summarization, and forensic analysis.

**Method:** PRISM (Perceptual Recognition for Identifying Standout Moments) is introduced as a lightweight and perceptually-aligned framework for keyframe extraction. It operates in the CIELAB color space and uses perceptual color difference metrics, making it interpretable, training-free, and computationally efficient.

**Result:** PRISM achieves strong accuracy and fidelity while maintaining high compression ratios on four benchmark datasets: BBC, TVSum, SumMe, and ClipShots.

**Conclusion:** PRISM is effective in both structured and unstructured video content, with strong accuracy and high compression ratios, and has potential as a scalable tool for analyzing and moderating harmful or politically sensitive media.

**Abstract:** Online videos play a central role in shaping political discourse and
amplifying cyber social threats such as misinformation, propaganda, and
radicalization. Detecting the most impactful or "standout" moments in video
content is crucial for content moderation, summarization, and forensic
analysis. In this paper, we introduce PRISM (Perceptual Recognition for
Identifying Standout Moments), a lightweight and perceptually-aligned framework
for keyframe extraction. PRISM operates in the CIELAB color space and uses
perceptual color difference metrics to identify frames that align with human
visual sensitivity. Unlike deep learning-based approaches, PRISM is
interpretable, training-free, and computationally efficient, making it well
suited for real-time and resource-constrained environments. We evaluate PRISM
on four benchmark datasets: BBC, TVSum, SumMe, and ClipShots, and demonstrate
that it achieves strong accuracy and fidelity while maintaining high
compression ratios. These results highlight PRISM's effectiveness in both
structured and unstructured video content, and its potential as a scalable tool
for analyzing and moderating harmful or politically sensitive media in online
platforms.

</details>


### [60] [MOSCARD -- Causal Reasoning and De-confounding for Multimodal Opportunistic Screening of Cardiovascular Adverse Events](https://arxiv.org/abs/2506.19174)
*Jialu Pi,Juan Maria Farina,Rimita Lahiri,Jiwoong Jeong,Archana Gurudu,Hyung-Bok Park,Chieh-Ju Chao,Chadi Ayoub,Reza Arsanjani,Imon Banerjee*

Main category: cs.CV

> MOSCARD是一个新的预测建模框架，该框架通过共注意机制将胸部X光片（CXR）和心电图（ECG）的数据对齐，并同时减轻偏见和混杂因素，提高预测准确性。

<details>
  <summary>Details</summary>

**Motivation:** 机会性筛查利用常规健康检查中收集的数据，多模态数据可以在识别有风险的个人方面发挥作用。MOSCARD框架旨在提供比传统的依靠临床评分、CT测量或生物标志物更为全面的风险评估。

**Method:** 我们提出了一种新颖的预测建模框架MOSCARD，该框架通过共注意机制将胸部X光片（CXR）和心电图（ECG）的数据对齐，并同时减轻偏见和混杂因素。主要的技术贡献有：(i) 在ECG指导下对CXR进行多模态对齐；(ii) 集成因果推理；(iii) 使用双重反向传播图去除混杂因素。

**Result:** 在内部和急诊部的数据以及外部的MIMIC数据集上进行了评估，我们的模型在准确性上超过了单模态和其他最先进的模型 - AUC分别为0.75、0.83、0.71。

**Conclusion:** 该提出的成本效益高、机会性筛查方法可以实现早期干预，改善患者的结果并减少健康不平等。

**Abstract:** Major Adverse Cardiovascular Events (MACE) remain the leading cause of
mortality globally, as reported in the Global Disease Burden Study 2021.
Opportunistic screening leverages data collected from routine health check-ups
and multimodal data can play a key role to identify at-risk individuals. Chest
X-rays (CXR) provide insights into chronic conditions contributing to major
adverse cardiovascular events (MACE), while 12-lead electrocardiogram (ECG)
directly assesses cardiac electrical activity and structural abnormalities.
Integrating CXR and ECG could offer a more comprehensive risk assessment than
conventional models, which rely on clinical scores, computed tomography (CT)
measurements, or biomarkers, which may be limited by sampling bias and single
modality constraints. We propose a novel predictive modeling framework -
MOSCARD, multimodal causal reasoning with co-attention to align two distinct
modalities and simultaneously mitigate bias and confounders in opportunistic
risk estimation. Primary technical contributions are - (i) multimodal alignment
of CXR with ECG guidance; (ii) integration of causal reasoning; (iii) dual
back-propagation graph for de-confounding. Evaluated on internal, shift data
from emergency department (ED) and external MIMIC datasets, our model
outperformed single modality and state-of-the-art foundational models - AUC:
0.75, 0.83, 0.71 respectively. Proposed cost-effective opportunistic screening
enables early intervention, improving patient outcomes and reducing
disparities.

</details>


### [61] [OpenWildlife: Open-Vocabulary Multi-Species Wildlife Detector for Geographically-Diverse Aerial Imagery](https://arxiv.org/abs/2506.19204)
*Muhammed Patel,Javier Noa Turnes,Jayden Hsiao,Linlin Xu,David Clausi*

Main category: cs.CV

> OpenWildlife (OW) is an open-vocabulary wildlife detector designed for multi-species identification in diverse aerial imagery, offering superior performance and flexibility over existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind OW is to address the limitations of existing automated wildlife detection methods which perform well in specific settings but struggle to generalize across different species and environments.

**Method:** _OW leverages language-aware embeddings and a novel adaptation of the Grounding-DINO framework, and introduces an efficient search algorithm that combines k-nearest neighbors and breadth-first search to identify species in aerial imagery.

**Result:** OW achieves up to 0.981 mAP50 with fine-tuning and 0.597 mAP50 on seven datasets featuring novel species, capturing over 95% of species while exploring only 33% of the available images.

**Conclusion:** The conclusion is that OW, being a flexible solution, outperforms most existing methods and can be utilized for cost-effective global biodiversity assessments.

**Abstract:** We introduce OpenWildlife (OW), an open-vocabulary wildlife detector designed
for multi-species identification in diverse aerial imagery. While existing
automated methods perform well in specific settings, they often struggle to
generalize across different species and environments due to limited taxonomic
coverage and rigid model architectures. In contrast, OW leverages
language-aware embeddings and a novel adaptation of the Grounding-DINO
framework, enabling it to identify species specified through natural language
inputs across both terrestrial and marine environments. Trained on 15 datasets,
OW outperforms most existing methods, achieving up to \textbf{0.981} mAP50 with
fine-tuning and \textbf{0.597} mAP50 on seven datasets featuring novel species.
Additionally, we introduce an efficient search algorithm that combines
k-nearest neighbors and breadth-first search to prioritize areas where social
species are likely to be found. This approach captures over \textbf{95\%} of
species while exploring only \textbf{33\%} of the available images. To support
reproducibility, we publicly release our source code and dataset splits,
establishing OW as a flexible, cost-effective solution for global biodiversity
assessments.

</details>


### [62] [Ancient Script Image Recognition and Processing: A Review](https://arxiv.org/abs/2506.19208)
*Xiaolei Diao,Rite Bo,Yanling Xiao,Lida Shi,Zhihan Zhou,Hao Xu,Chuntao Li,Xiongfeng Tang,Massimo Poesio,Cédric M. John,Daqian Shi*

Main category: cs.CV

> A comprehensive review of ancient script image recognition methods, categorizing studies by script type, examining common challenges and solutions, and highlighting future research directions.

<details>
  <summary>Details</summary>

**Motivation:** To provide a structured and forward-looking perspective on ancient script image recognition, aiding in the large-scale interpretation and research advancements in archaeology and digital humanities.

**Method:** Ancient script image recognition methods are categorized based on script types, such as phonographic and logographic systems, and the recognition methods for each type are discussed. The paper also examines challenges specific to ancient scripts, such as imbalanced data and image degradation, and reviews recent solutions including few-shot learning and noise-robust techniques.

**Result:** The paper provides a comprehensive review of the current state of ancient script image recognition, covering a range of methodologies and challenges. It also discusses the commonalities and differences among various script recognition methods.

**Conclusion:** The review highlights the need for further research to address the limitations in ancient script recognition, such as improving robustness against image degradation and handling imbalanced data, and suggests future research directions.

**Abstract:** Ancient scripts, e.g., Egyptian hieroglyphs, Oracle Bone Inscriptions, and
Ancient Greek inscriptions, serve as vital carriers of human civilization,
embedding invaluable historical and cultural information. Automating ancient
script image recognition has gained importance, enabling large-scale
interpretation and advancing research in archaeology and digital humanities.
With the rise of deep learning, this field has progressed rapidly, with
numerous script-specific datasets and models proposed. While these scripts vary
widely, spanning phonographic systems with limited glyphs to logographic
systems with thousands of complex symbols, they share common challenges and
methodological overlaps. Moreover, ancient scripts face unique challenges,
including imbalanced data distribution and image degradation, which have driven
the development of various dedicated methods. This survey provides a
comprehensive review of ancient script image recognition methods. We begin by
categorizing existing studies based on script types and analyzing respective
recognition methods, highlighting both their differences and shared strategies.
We then focus on challenges unique to ancient scripts, systematically examining
their impact and reviewing recent solutions, including few-shot learning and
noise-robust techniques. Finally, we summarize current limitations and outline
promising future directions. Our goal is to offer a structured, forward-looking
perspective to support ongoing advancements in the recognition, interpretation,
and decipherment of ancient scripts.

</details>


### [63] [MedErr-CT: A Visual Question Answering Benchmark for Identifying and Correcting Errors in CT Reports](https://arxiv.org/abs/2506.19217)
*Sunggu Kyung,Hyungbin Park,Jinyoung Seo,Jimin Sung,Jihyun Kim,Dongyeong Kim,Wooyoung Jo,Yoojin Nam,Sangah Park,Taehee Kwon,Sang Min Lee,Namkug Kim*

Main category: cs.CV

> 本文介绍了MedErr-CT，一个评估医疗MLLMs能力识别和纠正CT报告错误的新型基准，说明了其任务类别及评估方法，展示了最先进的医疗3D MLLMs在不同错误类型的性能差异。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大型语言模型(MLLMs)对医学知识的理解表现出前景，但产生不准确信息的倾向凸显了严谨验证的必要性。现有医疗视觉问题回答(VQA)基准主要集中在简单的视觉识别任务上，缺乏临床相关性，未能评估专家级知识。

**Method:** 引入了MedErr-CT，这是一个评估医疗MLLMs能力的新基准，通过VQA框架来识别和纠正CT报告中的错误。基准包括六类错误：四个视觉中心错误（遗漏、插入、方向、大小）和两类词汇错误（单位，拼写）。这些任务分为三个级别：分类、检测和纠正。

**Result:** 使用该基准，我们定量评估了最先进的3D医疗MLLMs的性能，揭示了它们在不同错误类型上的能力存在显著差异。

**Conclusion:** 本文的基准有助于开发更可靠且适用于临床的MLLMs，最终有助于减少临床实践中诊断错误并提高准确性。

**Abstract:** Computed Tomography (CT) plays a crucial role in clinical diagnosis, but the
growing demand for CT examinations has raised concerns about diagnostic errors.
While Multimodal Large Language Models (MLLMs) demonstrate promising
comprehension of medical knowledge, their tendency to produce inaccurate
information highlights the need for rigorous validation. However, existing
medical visual question answering (VQA) benchmarks primarily focus on simple
visual recognition tasks, lacking clinical relevance and failing to assess
expert-level knowledge. We introduce MedErr-CT, a novel benchmark for
evaluating medical MLLMs' ability to identify and correct errors in CT reports
through a VQA framework. The benchmark includes six error categories - four
vision-centric errors (Omission, Insertion, Direction, Size) and two lexical
error types (Unit, Typo) - and is organized into three task levels:
classification, detection, and correction. Using this benchmark, we
quantitatively assess the performance of state-of-the-art 3D medical MLLMs,
revealing substantial variation in their capabilities across different error
types. Our benchmark contributes to the development of more reliable and
clinically applicable MLLMs, ultimately helping reduce diagnostic errors and
improve accuracy in clinical practice. The code and datasets are available at
https://github.com/babbu3682/MedErr-CT.

</details>


### [64] [Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification](https://arxiv.org/abs/2506.19225)
*Minghao Qin,Xiangrui Liu,Zhengyang Liang,Yan Shu,Huaying Yuan,Juenjie Zhou,Shitao Xiao,Bo Zhao,Zheng Liu*

Main category: cs.CV

> Introduces Video-XL-2, a novel multi-modal large language model that significantly improves long video understanding performance and efficiency through task-aware KV sparsification.

<details>
  <summary>Details</summary>

**Motivation:** to address the challenge of processing long video inputs with high memory and computational costs.

**Method:** chunk-based pre-filling and bi-level key-value decoding to reduce computational and memory overhead and improve model efficiency in long video understanding.

**Result:** Video-XL-2 achieves state-of-the-art performance on various long video understanding benchmarks and shows exceptional efficiency.

**Conclusion:** Video-XL-2 effectively balances long video understanding performance and computational efficiency by employing task-aware KV sparsification techniques.

**Abstract:** Multi-modal large language models (MLLMs) models have made significant
progress in video understanding over the past few years. However, processing
long video inputs remains a major challenge due to high memory and
computational costs. This makes it difficult for current models to achieve both
strong performance and high efficiency in long video understanding. To address
this challenge, we propose Video-XL-2, a novel MLLM that delivers superior
cost-effectiveness for long-video understanding based on task-aware KV
sparsification. The proposed framework operates with two key steps: chunk-based
pre-filling and bi-level key-value decoding. Chunk-based pre-filling divides
the visual token sequence into chunks, applying full attention within each
chunk and sparse attention across chunks. This significantly reduces
computational and memory overhead. During decoding, bi-level key-value decoding
selectively reloads either dense or sparse key-values for each chunk based on
its relevance to the task. This approach further improves memory efficiency and
enhances the model's ability to capture fine-grained information. Video-XL-2
achieves state-of-the-art performance on various long video understanding
benchmarks, outperforming existing open-source lightweight models. It also
demonstrates exceptional efficiency, capable of processing over 10,000 frames
on a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few
seconds.

</details>


### [65] [MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models](https://arxiv.org/abs/2506.19257)
*Yinan Xia,Yilei Jiang,Yingshui Tan,Xiaoyong Zhu,Xiangyu Yue,Bo Zheng*

Main category: cs.CV

> The paper presents MSR-Align, a dataset aimed at improving the safety and robustness of multimodal Vision-Language Models against harmful prompts.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the increasing safety risks brought by multimodal inputs to reasoning-capable VLMs, as existing safety alignment approaches are inadequate for these complex scenarios.

**Method:** The paper introduces MSR-Align, a dataset designed to enhance the safety and robustness of Vision-Language Models (VLMs) by addressing multimodal safety reasoning. It emphasizes multimodal diversity, policy-grounded reasoning, and quality filtering.

**Result:** Experiments show that fine-tuning VLMs on MSR-Align improves their robustness against both textual and visual-textual jailbreak attacks, without compromising their general reasoning capabilities.

**Conclusion:** MSR-Align is a critical resource for advancing the safety alignment of reasoning-capable VLMs and is openly shared for further research.

**Abstract:** Vision-Language Models (VLMs) have achieved remarkable progress in multimodal
reasoning tasks through enhanced chain-of-thought capabilities. However, this
advancement also introduces novel safety risks, as these models become
increasingly vulnerable to harmful multimodal prompts that can trigger
unethical or unsafe behaviors. Existing safety alignment approaches, primarily
designed for unimodal language models, fall short in addressing the complex and
nuanced threats posed by multimodal inputs. Moreover, current safety datasets
lack the fine-grained, policy-grounded reasoning required to robustly align
reasoning-capable VLMs. In this work, we introduce {MSR-Align}, a high-quality
Multimodal Safety Reasoning dataset tailored to bridge this gap. MSR-Align
supports fine-grained, deliberative reasoning over standardized safety policies
across both vision and text modalities. Our data generation pipeline emphasizes
multimodal diversity, policy-grounded reasoning, and rigorous quality filtering
using strong multimodal judges. Extensive experiments demonstrate that
fine-tuning VLMs on MSR-Align substantially improves robustness against both
textual and vision-language jailbreak attacks, while preserving or enhancing
general reasoning performance. MSR-Align provides a scalable and effective
foundation for advancing the safety alignment of reasoning-capable VLMs. Our
dataset is made publicly available at
https://huggingface.co/datasets/Leigest/MSR-Align.

</details>


### [66] [Automated Image Recognition Framework](https://arxiv.org/abs/2506.19261)
*Quang-Binh Nguyen,Trong-Vu Hoang,Ngoc-Do Tran,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

> 本研究提出了一种名为AIR的框架，利用生成式AI技术，实现了高质量数据集的自动合成与深度学习模型的自动化训练，尤其适用于没有充足数据集的新型或敏感主题，实验证明了其有效性和潜在价值。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于深度学习模型的效果很大程度上取决于数据，而针对特定任务，尤其是在没有相关数据集的新型或敏感主题上，收集和标注数据面临着时间和资源上的挑战。为了解决这些问题，本文提出了上述方法。

**Method:** 本文提出了一种名为自动图像识别（AIR）的框架，该框架利用生成式AI技术，使用户能够自动生成高质量且预先标注的数据集，从而避免了手动标注的需求。它还能够在生成的数据集上自动训练深度学习模型，实现强大的图像识别性能。该框架包含两个主要的数据合成过程：AIR-Gen 和 AIR-Aug。其中，AIR-Gen 能够使用户无缝生成符合其特定需求的数据集；AIR-Aug 则在给定数据集的基础上进行增强，特别是当用户面临特定任务数据不足时，该模块特别有用。此外，引入了一种新颖的自动化提示工程模块，利用大型语言模型的能力提高图像质量，并配有一个分布调整算法来消除重复和异常值，增强数据集的健壮性和可靠性。

**Result:** 通过全面的实验，展示了本文生成数据在训练深度学习模型上的有效性，也证明了该系统在为各种物体提供图像识别模型方面的潜力。

**Conclusion:** 本文提出了一种能够在生成的数据集上自动训练强大图像识别模型的框架，去除了手动标注数据集的需求，展示了其在提升深度学习模型性能方面的潜在价值。

**Abstract:** While the efficacy of deep learning models heavily relies on data, gathering
and annotating data for specific tasks, particularly when addressing novel or
sensitive subjects lacking relevant datasets, poses significant time and
resource challenges. In response to this, we propose a novel Automated Image
Recognition (AIR) framework that harnesses the power of generative AI. AIR
empowers end-users to synthesize high-quality, pre-annotated datasets,
eliminating the necessity for manual labeling. It also automatically trains
deep learning models on the generated datasets with robust image recognition
performance. Our framework includes two main data synthesis processes, AIR-Gen
and AIR-Aug. The AIR-Gen enables end-users to seamlessly generate datasets
tailored to their specifications. To improve image quality, we introduce a
novel automated prompt engineering module that leverages the capabilities of
large language models. We also introduce a distribution adjustment algorithm to
eliminate duplicates and outliers, enhancing the robustness and reliability of
generated datasets. On the other hand, the AIR-Aug enhances a given dataset,
thereby improving the performance of deep classifier models. AIR-Aug is
particularly beneficial when users have limited data for specific tasks.
Through comprehensive experiments, we demonstrated the efficacy of our
generated data in training deep learning models and showcased the system's
potential to provide image recognition models for a wide range of objects. We
also conducted a user study that achieved an impressive score of 4.4 out of
5.0, underscoring the AI community's positive perception of AIR.

</details>


### [67] [3D-SSM: A Novel 3D Selective Scan Module for Remote Sensing Change Detection](https://arxiv.org/abs/2506.19263)
*Rui Huang,Jincheng Zeng,Sen Gao,Yan Xing*

Main category: cs.CV

> 该论文提出了一种新型3D选择性扫描模块及其配套组件，以提高遥感图像变化检测的质量和效率。

<details>
  <summary>Details</summary>

**Motivation:** 现有基于Mamba的遥感变化检测方法虽然改进了扫描模型，但在捕捉图像通道之间的长范围依赖方面存在不足，限制了特征表示能力。为解决此问题，该论文提出了新的方法。

**Method:** 该论文提出了一种3D选择性扫描模块（3D-SSM），该模块从空间和通道角度捕捉全局信息。基于3D-SSM，论文设计了两个关键组件：空间-时间交互模块（SIM）和多分支特征提取模块（MBFEM）。SIM通过不同时间点图像之间的全局和局部特征交互，促进生物特征的融合，从而增强细小变化的检测能力。而MBFEM结合了频率域、空间域和3D-SSM的特征，提供丰富的上下文信息表达。

**Result:** 实验表明，该论文提出的方法在五个基准数据集上相较于目前最先进的变化检测方法表现出优越的性能。

**Conclusion:** 所提出的方法通过3D-SSM、SIM和MBFEM的结合，在捕捉图像间的变化和提供丰富上下文信息方面展现出优越性能。

**Abstract:** Existing Mamba-based approaches in remote sensing change detection have
enhanced scanning models, yet remain limited by their inability to capture
long-range dependencies between image channels effectively, which restricts
their feature representation capabilities. To address this limitation, we
propose a 3D selective scan module (3D-SSM) that captures global information
from both the spatial plane and channel perspectives, enabling a more
comprehensive understanding of the data.Based on the 3D-SSM, we present two key
components: a spatiotemporal interaction module (SIM) and a multi-branch
feature extraction module (MBFEM). The SIM facilitates bi-temporal feature
integration by enabling interactions between global and local features across
images from different time points, thereby enhancing the detection of subtle
changes. Meanwhile, the MBFEM combines features from the frequency domain,
spatial domain, and 3D-SSM to provide a rich representation of contextual
information within the image. Our proposed method demonstrates favourable
performance compared to state-of-the-art change detection methods on five
benchmark datasets through extensive experiments. Code is available at
https://github.com/VerdantMist/3D-SSM

</details>


### [68] [Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation](https://arxiv.org/abs/2506.19267)
*Weichen Zhang,Dong Xu,Wanli Ouyang,Wen Li*

Main category: cs.CV

> 本文提出CAN和SPCAN方法以解决无监督领域自适应问题，通过领域协作与对抗学习，提升了物体与视频动作识别任务的性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高无监督领域自适应的效果，特别是解决领域间分布不匹配的问题。

**Method:** 该研究提出了一种名为协作对抗网络（CAN）的新无监督领域自适应方法。CAN利用领域协作和领域对抗的学习策略来训练神经网络。领域协作学习旨在学习领域特定的特征表示以保留目标领域的判别性，而领域对抗学习旨在学习领域不变的特征表示以减少源领域和目标领域之间的分布不匹配。研究将这两种学习策略统一为带有正负权重损失的领域分类器学习。通过协作学习，CAN能够在CNN的较低层次自动学习到领域特定的表示，而通过对抗学习，则能在较高层次学习到领域不变的表示。此外，为增强目标领域的判别性，研究还提出了逐级CAN（SPCAN），逐步选择伪标签的目标样本用于再训练分类器。研究采用了逐级学习策略，按从易到难的方式选择伪标签目标样本。

**Result:** 实验结果表明，提出的CAN及SPCAN方法在Office-31、ImageCLEF-DA、VISDA-2017数据集上的物体识别任务与UCF101-10、HMDB51-10数据集上的视频动作识别任务中达到了最先进的性能，证明了方法的有效性。

**Conclusion:** CAN和SPCAN方法对于无监督领域自适应问题展示了显著的优势，特别是在减少领域间分布不匹配和增强目标领域判别性方面。

**Abstract:** This paper proposes a new unsupervised domain adaptation approach called
Collaborative and Adversarial Network (CAN), which uses the
domain-collaborative and domain-adversarial learning strategy for training the
neural network. The domain-collaborative learning aims to learn domain-specific
feature representation to preserve the discriminability for the target domain,
while the domain adversarial learning aims to learn domain-invariant feature
representation to reduce the domain distribution mismatch between the source
and target domains. We show that these two learning strategies can be uniformly
formulated as domain classifier learning with positive or negative weights on
the losses. We then design a collaborative and adversarial training scheme,
which automatically learns domain-specific representations from lower blocks in
CNNs through collaborative learning and domain-invariant representations from
higher blocks through adversarial learning. Moreover, to further enhance the
discriminability in the target domain, we propose Self-Paced CAN (SPCAN), which
progressively selects pseudo-labeled target samples for re-training the
classifiers. We employ a self-paced learning strategy to select pseudo-labeled
target samples in an easy-to-hard fashion. Comprehensive experiments on
different benchmark datasets, Office-31, ImageCLEF-DA, and VISDA-2017 for the
object recognition task, and UCF101-10 and HMDB51-10 for the video action
recognition task, show our newly proposed approaches achieve the
state-of-the-art performance, which clearly demonstrates the effectiveness of
our proposed approaches for unsupervised domain adaptation.

</details>


### [69] [AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration](https://arxiv.org/abs/2506.19283)
*Xiangbo Gao,Yuheng Wu,Xuewen Luo,Keshu Wu,Xinghao Chen,Yuping Wang,Chenxi Liu,Yang Zhou,Zhengzhong Tu*

Main category: cs.CV

> 本研究通过创建AirV2X-Perception数据集，利用无人机辅助驾驶弥补传统V2X系统在部署成本和覆盖范围上的局限，推动了交通工具到无人机（V2D）算法的研究和应用。

<details>
  <summary>Details</summary>

**Motivation:** 面对多功能车辆协同驾驶系统在单车辆自动驾驶上的优势，以及传统基于基础设施的V2X系统高昂的部署成本和在乡村及郊区形成的“暴露危险区”，本研究旨在通过无人机提供一个灵活的解决方案，从而推动自主驾驶系统的发展。

**Method:** 通过收集并整理6.73小时在不同环境下的无人机辅助驾驶场景，构建了一个涵盖多种天气和光照条件的大规模数据集。

**Result:** 此论文提出了一种名为AirV2X-Perception的大规模数据集，该数据集利用无人机作为地面道路基础设施的灵活替代或补充。无人机提供了独特的鸟瞰视角，可以减少遮挡，且具有悬停、巡逻和护送导航的能力。此数据集包括了在城市、郊区和农村环境中的6.73小时的无人机辅助驾驶场景。这项研究有助于开发和标准化评估车辆到无人机（V2D）算法，填补了空中辅助自动驾驶系统的空白。

**Conclusion:** 研究结果表明，利用无人机辅助的驾驶系统在减少部署成本、提高视域和驾驶安全性方面具有显著优势，同时填补了相关领域的空白，推动了相关算法的发展和标准化评估。

**Abstract:** While multi-vehicular collaborative driving demonstrates clear advantages
over single-vehicle autonomy, traditional infrastructure-based V2X systems
remain constrained by substantial deployment costs and the creation of
"uncovered danger zones" in rural and suburban areas. We present
AirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial
Vehicles (UAVs) as a flexible alternative or complement to fixed Road-Side
Units (RSUs). Drones offer unique advantages over ground-based perception:
complementary bird's-eye-views that reduce occlusions, dynamic positioning
capabilities that enable hovering, patrolling, and escorting navigation rules,
and significantly lower deployment costs compared to fixed infrastructure. Our
dataset comprises 6.73 hours of drone-assisted driving scenarios across urban,
suburban, and rural environments with varied weather and lighting conditions.
The AirV2X-Perception dataset facilitates the development and standardized
evaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in
the rapidly expanding field of aerial-assisted autonomous driving systems. The
dataset and development kits are open-sourced at
https://github.com/taco-group/AirV2X-Perception.

</details>


### [70] [Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding](https://arxiv.org/abs/2506.19288)
*Runwei Guan,Ningwei Ouyang,Tianhao Xu,Shaofeng Liang,Wei Dai,Yafeng Sun,Shang Gao,Songning Lai,Shanliang Yao,Xuming Hu,Ryan Wen Liu,Yutao Yue,Hui Xiong*

Main category: cs.CV

> 研究开发了WaterCaption数据集和Da Yu模型，旨在提高无人水面车辆对复杂水道环境的理解能力。Da Yu模型通过Nano Transformer Adaptor（NTA）实现了高效且精准的视觉到语言转换，生成长文本描述。

<details>
  <summary>Details</summary>

**Motivation:** 目前水道感知模型主要集中在实例级对象感知范式上，而忽视了全局语义理解，限制了大规模监控和结构化日志的生成。此研究旨在通过引入新的数据集WaterCaption和模型Da Yu，推动水道环境认知领域的发展。

**Method:** 本研究利用视觉-语言模型（VLMs）的优势，提出了WaterCaption数据集，专注于细化、多区域长文本描述，旨在改进水域环境的全局语义理解。此外，提出了一种边缘可部署的多模态大型语言模型Da Yu，其中包含了一种名为Nano Transformer Adaptor (NTA) 的新视觉到语言投射器，用以平衡计算效率与视觉特征的全局和局部建模能力。

**Result:** Da Yu在WaterCaption数据集以及若干其他描述基准上表现优于现有先进技术，证明了其强大的视觉到语言转换能力。

**Conclusion:** 本研究表明，通过利用视觉-语言模型，可以实现对复杂水道环境的高级理解和自动描述，这为无人水面车辆的自主感知提供了新的研究方向。

**Abstract:** Automated waterway environment perception is crucial for enabling unmanned
surface vessels (USVs) to understand their surroundings and make informed
decisions. Most existing waterway perception models primarily focus on
instance-level object perception paradigms (e.g., detection, segmentation).
However, due to the complexity of waterway environments, current perception
datasets and models fail to achieve global semantic understanding of waterways,
limiting large-scale monitoring and structured log generation. With the
advancement of vision-language models (VLMs), we leverage image captioning to
introduce WaterCaption, the first captioning dataset specifically designed for
waterway environments. WaterCaption focuses on fine-grained, multi-region
long-text descriptions, providing a new research direction for visual
geo-understanding and spatial scene cognition. Exactly, it includes 20.2k
image-text pair data with 1.8 million vocabulary size. Additionally, we propose
Da Yu, an edge-deployable multi-modal large language model for USVs, where we
propose a novel vision-to-language projector called Nano Transformer Adaptor
(NTA). NTA effectively balances computational efficiency with the capacity for
both global and fine-grained local modeling of visual features, thereby
significantly enhancing the model's ability to generate long-form textual
outputs. Da Yu achieves an optimal balance between performance and efficiency,
surpassing state-of-the-art models on WaterCaption and several other captioning
benchmarks.

</details>


### [71] [HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis](https://arxiv.org/abs/2506.19291)
*Xiaoyuan Wang,Yizhou Zhao,Botao Ye,Xiaojun Shan,Weijie Lyu,Lu Qi,Kelvin C. K. Chan,Yinxiao Li,Ming-Hsuan Yang*

Main category: cs.CV

> HoliGS是一种新的可变形高斯散点框架，能有效地从长单目RGB视频中进行实体视图合成，解决了现有方法在长时间捕捉中的训练负担问题。

<details>
  <summary>Details</summary>

**Motivation:** 为了从长时间的单目RGB视频中解决实体视图合成问题，HoliGS旨在减少训练负担，并准确地重建大规模动态环境。

**Method:** HoliGS采用可变形的高斯散点框架，通过逆高斯散点变形网络，将场景分解为静态背景和随时间变化的对象，每个对象由学习到的高斯原语表示，这些原语可以进行全局刚性变换、骨架驱动的变形和细微的非刚性变形。

**Result:** 实验表明，与最先进的一元可变形NeRF相比，HoliGS在具有挑战性的数据集上实现了优越的重建质量，并显著减少了训练和渲染时间。

**Conclusion:** HoliGS 提供了一种在实际场景中进行EVS的实用且可扩展的解决方案，通过显著降低训练和渲染时间，实现了高质量的重建。

**Abstract:** We propose HoliGS, a novel deformable Gaussian splatting framework that
addresses embodied view synthesis from long monocular RGB videos. Unlike prior
4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training
overhead in minute-long captures, our method leverages invertible Gaussian
Splatting deformation networks to reconstruct large-scale, dynamic environments
accurately. Specifically, we decompose each scene into a static background plus
time-varying objects, each represented by learned Gaussian primitives
undergoing global rigid transformations, skeleton-driven articulation, and
subtle non-rigid deformations via an invertible neural flow. This hierarchical
warping strategy enables robust free-viewpoint novel-view rendering from
various embodied camera trajectories by attaching Gaussians to a complete
canonical foreground shape (\eg, egocentric or third-person follow), which may
involve substantial viewpoint changes and interactions between multiple actors.
Our experiments demonstrate that \ourmethod~ achieves superior reconstruction
quality on challenging datasets while significantly reducing both training and
rendering time compared to state-of-the-art monocular deformable NeRFs. These
results highlight a practical and scalable solution for EVS in real-world
scenarios. The source code will be released.

</details>


### [72] [Open-Vocabulary Camouflaged Object Segmentation with Cascaded Vision Language Models](https://arxiv.org/abs/2506.19300)
*Kai Zhao,Wubang Yuan,Zheng Wang,Guanyi Li,Xiaoqiang Zhu,Deng-ping Fan,Dan Zeng*

Main category: cs.CV

> 提出了一种VLM引导的级联框架来改进OVCOS，通过显式提示SAM和软空间先验来提高分割和分类精度。

<details>
  <summary>Details</summary>

**Motivation:** 当前OVCOS方法存在领域差距和通用分割模型不适应隐蔽物体的问题，这导致分割边界不准确。本文旨在通过引入VLM引导的方法来改进这些问题，提高对隐蔽物体的分割和分类效果。

**Method:** 本研究提出了一种创新的VLM引导级联框架，用于解决开放词汇隐藏物体分割（OVCOS）中的问题。该框架利用Segment Anything Model (SAM)，并通过VLM提取的特征作为显式提示来引导SAM，以提高对隐蔽区域的定位精度。在分类阶段，避免了由于硬裁剪引入的领域差距，而是通过将分割输出视为软空间先验（通过alpha通道）来保留完整的图像上下文，从而实现更准确的分类。

**Result:** 实验结果表明，该方法在OVCOS和传统的隐蔽物体分割基准测试上均表现出优越性，尤其是在利用丰富VLM语义信息进行分割和分类方面。

**Conclusion:** 通过实验证明，所提出的方法在处理隐蔽物体时提供了显著的分割和分类性能，证实了在OVCOS中利用VLM语义信息的有效性。

**Abstract:** Open-Vocabulary Camouflaged Object Segmentation (OVCOS) seeks to segment and
classify camouflaged objects from arbitrary categories, presenting unique
challenges due to visual ambiguity and unseen categories.Recent approaches
typically adopt a two-stage paradigm: first segmenting objects, then
classifying the segmented regions using Vision Language Models (VLMs).However,
these methods (1) suffer from a domain gap caused by the mismatch between VLMs'
full-image training and cropped-region inference, and (2) depend on generic
segmentation models optimized for well-delineated objects, making them less
effective for camouflaged objects.Without explicit guidance, generic
segmentation models often overlook subtle boundaries, leading to imprecise
segmentation.In this paper,we introduce a novel VLM-guided cascaded framework
to address these issues in OVCOS.For segmentation, we leverage the Segment
Anything Model (SAM), guided by the VLM.Our framework uses VLM-derived features
as explicit prompts to SAM, effectively directing attention to camouflaged
regions and significantly improving localization accuracy.For classification,
we avoid the domain gap introduced by hard cropping.Instead, we treat the
segmentation output as a soft spatial prior via the alpha channel, which
retains the full image context while providing precise spatial guidance,
leading to more accurate and context-aware classification of camouflaged
objects.The same VLM is shared across both segmentation and classification to
ensure efficiency and semantic consistency.Extensive experiments on both OVCOS
and conventional camouflaged object segmentation benchmarks demonstrate the
clear superiority of our method, highlighting the effectiveness of leveraging
rich VLM semantics for both segmentation and classification of camouflaged
objects.

</details>


### [73] [Airway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze](https://arxiv.org/abs/2506.19306)
*Jean-Paul Ainam,Rahul,Lora Cavuoto,Matthew Hackett,Jack Norfleet,Suvranu De*

Main category: cs.CV

> 研究提出了一种通过眼动数据与视频记录结合，基于机器学习方法评估ETI技能的系统，该系统提高了预测准确性、灵敏度及可信度，并验证了其在临床培训和提升病人结局上的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 气道管理的技能在急诊医学中至关重要，但其通常通过主观评估来进行，这往往无法准确衡量在实际操作中的胜任程度。本研究旨在针对这一问题，提出一种客观且高效的评估方法。

**Method:** 本研究提出了一种基于机器学习的方法来评估气道管理技能，特别是经口/鼻气管插管(ETI)技能。该方法利用了人体眼动数据和视频记录，通过眼动数据引导的注意力机制来提高成功与不成功ETI程序的识别。视觉掩码由眼动点创建，以引导模型专注于任务相关的区域，减少不相关特征。使用自动编码器网络从视频中提取特征，注意力模块生成自视觉掩码的注意力，分类器输出分类得分。

**Result:** 研究结果表明，该方法在预测准确性、灵敏度和可信度方面都有所提高，表现出比传统方法更高的准确性和效率。

**Conclusion:** 该研究是一种创新的方法，首次结合了人体眼动数据用于ETI评估，能有效提高模型性能，提供一种强大的客观评估临床技能工具，特别是在高强度环境下如军队应用，具有显著的应用潜力。

**Abstract:** Airway management skills are critical in emergency medicine and are typically
assessed through subjective evaluation, often failing to gauge competency in
real-world scenarios. This paper proposes a machine learning-based approach for
assessing airway skills, specifically endotracheal intubation (ETI), using
human gaze data and video recordings. The proposed system leverages an
attention mechanism guided by the human gaze to enhance the recognition of
successful and unsuccessful ETI procedures. Visual masks were created from gaze
points to guide the model in focusing on task-relevant areas, reducing
irrelevant features. An autoencoder network extracts features from the videos,
while an attention module generates attention from the visual masks, and a
classifier outputs a classification score. This method, the first to use human
gaze for ETI, demonstrates improved accuracy and efficiency over traditional
methods. The integration of human gaze data not only enhances model performance
but also offers a robust, objective assessment tool for clinical skills,
particularly in high-stress environments such as military settings. The results
show improvements in prediction accuracy, sensitivity, and trustworthiness,
highlighting the potential for this approach to improve clinical training and
patient outcomes in emergency medicine.

</details>


### [74] [Capturing Fine-Grained Alignments Improves 3D Affordance Detection](https://arxiv.org/abs/2506.19312)
*Junsei Tokumitsu,Yuiga Wada*

Main category: cs.CV

> 该研究提出LM-AD方法，用于解决在3D点云中检测可用性的问题，这需要准确捕捉点云与文本之间精细对齐。通过引入Affordance Query Module (AQM)，该方法利用预训练语言模型更有效地捕捉对齐，提高了在3D AffordanceNet数据集上的准确率和平均交并比。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法往往依赖于简单的余弦相似性来建模3D点云和文本之间的对齐，这种方法缺乏精细推理所需的表达能力，限制了它们在标准基准测试中的表现。该研究旨在解决这些问题。

**Method:** 提出了LM-AD方法和Affordance Query Module (AQM)，通过利用预训练的语言模型，更有效地捕捉点云和文本之间的精细对齐。

**Result:** 在3D AffordanceNet数据集上，该方法在准确率和平均交并比方面超越了现有的方法。

**Conclusion:** 研究结果表明，LM-AD和AQM方法能够改善3D点云中可用性检测任务的性能。

**Abstract:** In this work, we address the challenge of affordance detection in 3D point
clouds, a task that requires effectively capturing fine-grained alignments
between point clouds and text. Existing methods often struggle to model such
alignments, resulting in limited performance on standard benchmarks. A key
limitation of these approaches is their reliance on simple cosine similarity
between point cloud and text embeddings, which lacks the expressiveness needed
for fine-grained reasoning. To address this limitation, we propose LM-AD, a
novel method for affordance detection in 3D point clouds. Moreover, we
introduce the Affordance Query Module (AQM), which efficiently captures
fine-grained alignment between point clouds and text by leveraging a pretrained
language model. We demonstrated that our method outperformed existing
approaches in terms of accuracy and mean Intersection over Union on the 3D
AffordanceNet dataset.

</details>


### [75] [Progressive Modality Cooperation for Multi-Modality Domain Adaptation](https://arxiv.org/abs/2506.19316)
*Weichen Zhang,Dong Xu,Jing Zhang,Wanli Ouyang*

Main category: cs.CV

> A multi-modality domain adaptation framework (PMC) is proposed to improve cross-domain visual recognition tasks, showing effectiveness in both conventional MMDA and MMDA-PI settings with extensive dataset experiments.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve knowledge transfer between domains by exploiting multi-modality clues effectively under different adaptation scenarios.

**Method:** The paper proposes a Progressive Modality Cooperation (PMC) framework for multi-modality domain adaptation (MMDA) and multi-modality domain adaptation using privileged information (MMDA-PI) settings. In MMDA, PMC utilizes two modules that work with multiple modalities to select reliable pseudo-labeled target samples. In MMDA-PI, PMC-PI is introduced using a Multi-Modality Data Generation (MMG) network to generate missing modalities in the target domain based on source domain data, considering domain distribution mismatch and semantics preservation, the latter achieved through adversarial learning and weighted pseudo semantics conditioning.

**Result:** Experiments on image and video datasets show PMC's effectiveness across various multi-modality cross-domain visual recognition tasks under both MMDA and MMDA-PI settings.

**Conclusion:** The PMC framework effectively addresses the challenges of multi-modality domain adaptation, providing a robust method for cross-domain visual recognition tasks with multiple modalities, even when some modalities are missing in the target domain.

**Abstract:** In this work, we propose a new generic multi-modality domain adaptation
framework called Progressive Modality Cooperation (PMC) to transfer the
knowledge learned from the source domain to the target domain by exploiting
multiple modality clues (\eg, RGB and depth) under the multi-modality domain
adaptation (MMDA) and the more general multi-modality domain adaptation using
privileged information (MMDA-PI) settings. Under the MMDA setting, the samples
in both domains have all the modalities. In two newly proposed modules of our
PMC, the multiple modalities are cooperated for selecting the reliable
pseudo-labeled target samples, which captures the modality-specific information
and modality-integrated information, respectively. Under the MMDA-PI setting,
some modalities are missing in the target domain. Hence, to better exploit the
multi-modality data in the source domain, we further propose the PMC with
privileged information (PMC-PI) method by proposing a new multi-modality data
generation (MMG) network. MMG generates the missing modalities in the target
domain based on the source domain data by considering both domain distribution
mismatch and semantics preservation, which are respectively achieved by using
adversarial learning and conditioning on weighted pseudo semantics. Extensive
experiments on three image datasets and eight video datasets for various
multi-modality cross-domain visual recognition tasks under both MMDA and
MMDA-PI settings clearly demonstrate the effectiveness of our proposed PMC
framework.

</details>


### [76] [Continual Retinal Vision-Language Pre-training upon Incremental Imaging Modalities](https://arxiv.org/abs/2506.19320)
*Yuang Yao,Ruiqi Wu,Yi Zhou,Tao Zhou*

Main category: cs.CV

> 提出RetCoP框架，实现眼底多模态信息的持续学习与整合，解决模型遗忘问题，实验验证其优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基金图像分析模型专注于单模态任务，忽视了基金图模态互补性，这限制了它们的多功能性。在动态环境中，不同模态的数据往往逐步到来，需要持续的预训练。

**Method:** 提出RetCoP，首个眼底领域的持续视觉语言预训练框架，该框架可以增量地将来自不同成像模式的图像和文本特征整合到单一基础模型中。为减轻持续预训练中的灾难性遗忘，采用了代表性的图像-文本对作为复习策略，并引入了非对角信息蒸馏方法。前者允许模型回顾之前阶段的知识，后者则明确保持了图像与文本表示之间的对齐。

**Result:** 实验表明，RetCoP优于所有对比方法，实现了最佳泛化能力，且忘记率最低。

**Conclusion:** 提出的方法在实验中表现良好，展示出在持续预训练下保持高性能和低遗忘率的能力，证明了RetCoP在眼底图像分析领域的潜力和效果。

**Abstract:** Traditional fundus image analysis models focus on single-modal tasks,
ignoring fundus modality complementarity, which limits their versatility.
Recently, retinal foundation models have emerged, but most still remain
modality-specific. Integrating multiple fundus imaging modalities into a single
foundation model is valuable. However, in dynamic environments, data from
different modalities often arrive incrementally, necessitating continual
pre-training. To address this, we propose RetCoP, the first continual
vision-language pre-training framework in the fundus domain, which
incrementally integrates image and text features from different imaging
modalities into a single unified foundation model. To mitigate catastrophic
forgetting in continual pre-training, we introduce a rehearsal strategy
utilizing representative image-text pairs and an off-diagonal information
distillation approach. The former allows the model to revisit knowledge from
previous stages, while the latter explicitly preserves the alignment between
image and text representations. Experiments show that RetCoP outperforms all
the compared methods, achieving the best generalization and lowest forgetting
rate. The code can be found at https://github.com/Yuang-Yao/RetCoP.

</details>


### [77] [Memory-Augmented Incomplete Multimodal Survival Prediction via Cross-Slide and Gene-Attentive Hypergraph Learning](https://arxiv.org/abs/2506.19324)
*Mingcheng Qu,Guang Yang,Donglin Di,Yue Gao,Tonghua Su,Yang Song,Lei Fan*

Main category: cs.CV

> 该论文提出了一种利用超图学习整合多张全视野数字病理图像信息和病理切片与基因组数据之间跨模态交互的多模态生存预测框架，并引入记忆机制以应对数据模态不完整的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法主要结合福尔马林固定石蜡包埋切片和基因组数据进行分析，忽略了新鲜冷冻切片，并且由于高分辨率空间性质路径学数据在跨模态融合过程中的主导地位，导致模态不平衡。

**Method:** 提出了一种基于超图学习的多模态生存预测框架，并引入记忆机制来存储之前学习的配对病理基因组特征，以补偿不完整的模态。

**Result:** 在五个TCGA数据集上的实验表明，他们的模型在C-Index指标上比先进方法高出至少2.3%。在模态不完全的情况下，他们的方法分别比单独病理模型和基因模型高出3.3%和7.9%。

**Conclusion:** 所提出的方法有效地解决了模态不平衡问题，并且在同一数据缺失的情况下，显示了比单一模态更高的生存预测准确性。

**Abstract:** Multimodal pathology-genomic analysis is critical for cancer survival
prediction. However, existing approaches predominantly integrate formalin-fixed
paraffin-embedded (FFPE) slides with genomic data, while neglecting the
availability of other preservation slides, such as Fresh Froze (FF) slides.
Moreover, as the high-resolution spatial nature of pathology data tends to
dominate the cross-modality fusion process, it hinders effective multimodal
fusion and leads to modality imbalance challenges between pathology and
genomics. These methods also typically require complete data modalities,
limiting their clinical applicability with incomplete modalities, such as
missing either pathology or genomic data. In this paper, we propose a
multimodal survival prediction framework that leverages hypergraph learning to
effectively integrate multi-WSI information and cross-modality interactions
between pathology slides and genomics data while addressing modality imbalance.
In addition, we introduce a memory mechanism that stores previously learned
paired pathology-genomic features and dynamically compensates for incomplete
modalities. Experiments on five TCGA datasets demonstrate that our model
outperforms advanced methods by over 2.3% in C-Index. Under incomplete modality
scenarios, our approach surpasses pathology-only (3.3%) and gene-only models
(7.9%). Code: https://github.com/MCPathology/M2Surv

</details>
