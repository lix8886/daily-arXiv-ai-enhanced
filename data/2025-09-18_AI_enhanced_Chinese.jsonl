{"id": "2509.13480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13480", "abs": "https://arxiv.org/abs/2509.13480", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs", "comment": "Accepted at CLiC-it 2025", "summary": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation.", "AI": {"tldr": "本研究通过比较和微调大型语言模型，实现了意大利语性别中立重写任务的显著提升。", "motivation": "性别中立重写的目标是在不改变含义的情况下消除文本中的不必要的性别规格，但这一任务在有语法性别的语言（如意大利语）中尤为具有挑战性。本研究旨在评估现有语言模型在此任务中的表现。", "method": "本研究首次系统评估了最先进的大型语言模型（LLMs）在意大利语性别中立重写（GNR）任务中的表现，提出了一个二维框架来测量中立性和语义保真度。研究人员比较了多个LLMs的少样本提示结果，对选定模型进行了微调，并应用了针对性的清理以增强任务相关性。", "result": "研究表明，开源权重的LLMs在意大利语性别中立重写上优于唯一现有的专门模型。微调模型在任务中的表现也能匹敌甚至超越最大的LLMs，而且所需模型规模显著减小。", "conclusion": "研究人员讨论了在优化训练数据以保证中立性和意义保留之间的权衡，强调了在保持意思的同时改进性别中立性的重要性。"}}
{"id": "2509.13539", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13539", "abs": "https://arxiv.org/abs/2509.13539", "authors": ["Alisa Kanganis", "Katherine A. Keith"], "title": "Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning", "comment": null, "summary": "The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets\nmonetary policy, affecting the borrowing and spending decisions of millions of\npeople. In this work, we release Op-Fed, a dataset of 1044 human-annotated\nsentences and their contexts from FOMC transcripts. We faced two major\ntechnical challenges in dataset creation: imbalanced classes -- we estimate\nfewer than 8% of sentences express a non-neutral stance towards monetary policy\n-- and inter-sentence dependence -- 65% of instances require context beyond the\nsentence-level. To address these challenges, we developed a five-stage\nhierarchical schema to isolate aspects of opinion, monetary policy, and stance\ntowards monetary policy as well as the level of context needed. Second, we\nselected instances to annotate using active learning, roughly doubling the\nnumber of positive instances across all schema aspects. Using Op-Fed, we found\na top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion\nclassification but only 0.61 zero-shot accuracy classifying stance towards\nmonetary policy -- below our human baseline of 0.89. We expect Op-Fed to be\nuseful for future model training, confidence calibration, and as a seed dataset\nfor future annotation efforts.", "AI": {"tldr": "本研究发布了一套针对FOMC会议记录的标注数据集Op-Fed，解决了类别不平衡和句子间依赖问题，分析显示对于货币政策态度的机器分类表现仍低于人工。", "motivation": "本研究的动机在于解决FOMC会议记录中关于货币政策表达的不平衡分类问题以及句子间依赖问题，创建一个对其意见和态度进行标注的数据集。", "method": "本研究开发了一个五阶段的分层架构来分离意见、货币政策及对货币政策的态度等不同方面，并解决跨句子依赖的问题。同时采用了主动学习方法来选择需要标注的实例，将所有架构方面中的积极实例数量大约翻了一番。", "result": "使用Op-Fed数据集，顶级封闭权重大语言模型在意见分类中实现了0.80的零样本准确率，但在对货币政策态度的分类中仅实现了0.61的零样本准确率，低于人工设定的0.89基准。", "conclusion": "Op-Fed数据集预示着对未来模型训练、信心校准以及作为未来标注努力种子数据集的潜在用途。"}}
{"id": "2509.13569", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13569", "abs": "https://arxiv.org/abs/2509.13569", "authors": ["John Mendonça", "Lining Zhang", "Rahul Mallidi", "Alon Lavie", "Isabel Trancoso", "Luis Fernando D'Haro", "João Sedoc"], "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12", "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12", "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks.", "AI": {"tldr": "DSTC12 Track 1评估了对话系统的多维度评分和安全检测，表明在文化感知安全性方面存在改进空间。", "motivation": "随着大型语言模型的发展，对话系统评估的需求变得更加迫切，但全面评估仍具挑战性。传统的评估方法往往不够，对安全性的考量也常受限或带有文化偏见。", "method": "此论文通过DSTC12 Track 1的任务，评估了对话系统的多维性和在不同语言和文化背景下的安全性。任务分为两部分：多维度自动评估指标和多语言及跨文化安全检测。其中，Llama-3-8B 和 Llama-Guard-3-1B模型被用作基准模型。", "result": "在任务1中，Llama-3-8B模型取得了最高的平均Spearman's相关系数0.1681，显示出了改进的空间。任务2中，虽然参赛队伍在多语言安全子集上显著优于基准模型Llama-Guard-3-1B（最佳ROC-AUC为0.9648），但在文化子集上，基准模型的表现更好（ROC-AUC为0.5126），突显了文化感知安全性的需求。", "conclusion": "该论文评估了对话系统在多维度评价和跨语言、跨文化安全检测中的表现，指出在文化感知安全性方面的需求迫切。"}}
{"id": "2509.13624", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13624", "abs": "https://arxiv.org/abs/2509.13624", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation.", "AI": {"tldr": "本文提出了一种分析框架，以解决语言模型在未经训练的任务上的应用问题，发现影响交叉任务转移学习效果的因素不仅仅是表层数据集相似度，还包括隐藏的统计因素。", "motivation": "受实际需求的驱动，我们需要依赖使用具有不同特征的数据集进行转移学习，并预期出现分布外请求。", "method": "我们提出了一个分析框架，通过建立转移学习矩阵和降维来解析这些跨任务交互。", "result": "研究发现，性能的提升常常无法用浅层次的数据集相似度或源数据质量来解释。相反，源数据集的隐藏统计因素，如类别分布和生成长度倾向，以及特定的语义特征，实际上影响更大。", "conclusion": "这项工作为LLM的适应提供了深入的见解，有助于实现更加可预测和有效的LLM适应。"}}
{"id": "2509.13338", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "68T07, 68T09"], "pdf": "https://arxiv.org/pdf/2509.13338", "abs": "https://arxiv.org/abs/2509.13338", "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Kasra Ranjbarigderi", "Fang Chen", "Amir H. Gandomi"], "title": "Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks", "comment": "15 pages, 4 figures, 3 tables", "summary": "This work proposes an evidence-retrieval mechanism for uncertainty-aware\ndecision-making that replaces a single global cutoff with an\nevidence-conditioned, instance-adaptive criterion. For each test instance,\nproximal exemplars are retrieved in an embedding space; their predictive\ndistributions are fused via Dempster-Shafer theory. The resulting fused belief\nacts as a per-instance thresholding mechanism. Because the supporting evidences\nare explicit, decisions are transparent and auditable. Experiments on\nCIFAR-10/100 with BiT and ViT backbones show higher or comparable\nuncertainty-aware performance with materially fewer confidently incorrect\noutcomes and a sustainable review load compared with applying threshold on\nprediction entropy. Notably, only a few evidences are sufficient to realize\nthese gains; increasing the evidence set yields only modest changes. These\nresults indicate that evidence-conditioned tagging provides a more reliable and\ninterpretable alternative to fixed prediction entropy thresholds for\noperational uncertainty-aware decision-making.", "AI": {"tldr": "This study introduces a new evidence-retrieval method for uncertainty-aware decision-making that uses proximal exemplars' fused belief as a per-instance threshold, leading to higher performance and more transparent decisions compared to fixed prediction entropy thresholds.", "motivation": "To replace a single global cutoff with an evidence-conditioned, instance-adaptive criterion for uncertainty-aware decision-making, aiming for transparent decisions.", "method": "This paper proposes an evidence-retrieval mechanism that uses proximal exemplars retrieved in an embedding space and their predictive distributions fused via Dempster-Shafer theory to act as a per-instance thresholding mechanism, ensuring transparent and auditable decisions.", "result": "Experiments on CIFAR-10/100 with BiT and ViT backbones demonstrate higher or comparable uncertainty-aware performance with fewer confidently incorrect outcomes and a sustainable review load compared with using a prediction entropy threshold.", "conclusion": "Evidence-conditioned tagging offers a more reliable and interpretable alternative to fixed prediction entropy thresholds."}}
{"id": "2509.13664", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13664", "abs": "https://arxiv.org/abs/2509.13664", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior.", "AI": {"tldr": "研究展示了大型语言模型中的问题模糊性是以线性编码在内部表示中并可在神经元级别上检测和控制。", "motivation": "大型语言模型（LLMs）在面对模糊性问题时通常给出自信的答复，而不是寻求澄清。本文目的展示LLMs中的问题模糊性如何在线性编码在内部表示中，如何在神经元级别上进行检测和控制。", "method": "通过对LLMs的内部表示进行分析，识别出少数神经元（最少一个）编码问题的模糊信息。这些被称为模糊编码神经元（AENs）。通过探针训练，检测模糊性并在不同数据集上泛化，性能超过提示和表示基础线。层次分析表明，AENs出现在浅层，表明早期编码模糊信号。通过操作AENs，还可以控制LLMs的行为，从直接回答到避免回答。", "result": "探针训练的AENs在模糊性检测任务中表现强劲，泛化性能优于基线方法。层级分析显示AENs出现在浅层。通过操作AENs，可以控制从直接回答到避免回答的行为。这些发现表明了LLMs形成了关于问题模糊性的紧凑内部表示。", "conclusion": "本研究揭示了大型语言模型形成了关于问题模糊性的紧凑内部表示，这种表示可以使得行为具有透明性和可控性。"}}
{"id": "2509.13353", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13353", "abs": "https://arxiv.org/abs/2509.13353", "authors": ["Muhammad Adnan Shahzad"], "title": "Hybrid Quantum-Classical Model for Image Classification", "comment": null, "summary": "This study presents a systematic comparison between hybrid quantum-classical\nneural networks and purely classical models across three benchmark datasets\n(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and\nrobustness. The hybrid models integrate parameterized quantum circuits with\nclassical deep learning architectures, while the classical counterparts use\nconventional convolutional neural networks (CNNs). Experiments were conducted\nover 50 training epochs for each dataset, with evaluations on validation\naccuracy, test accuracy, training time, computational resource usage, and\nadversarial robustness (tested with $\\epsilon=0.1$ perturbations).Key findings\ndemonstrate that hybrid models consistently outperform classical models in\nfinal accuracy, achieving {99.38\\% (MNIST), 41.69\\% (CIFAR100), and 74.05\\%\n(STL10) validation accuracy, compared to classical benchmarks of 98.21\\%,\n32.25\\%, and 63.76\\%, respectively. Notably, the hybrid advantage scales with\ndataset complexity, showing the most significant gains on CIFAR100 (+9.44\\%)\nand STL10 (+10.29\\%). Hybrid models also train 5--12$\\times$ faster (e.g.,\n21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\% fewer parameters} while\nmaintaining superior generalization to unseen test data.Adversarial robustness\ntests reveal that hybrid models are significantly more resilient on simpler\ndatasets (e.g., 45.27\\% robust accuracy on MNIST vs. 10.80\\% for classical) but\nshow comparable fragility on complex datasets like CIFAR100 ($\\sim$1\\%\nrobustness for both). Resource efficiency analyses indicate that hybrid models\nconsume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization\n(9.5\\% vs. 23.2\\% on average).These results suggest that hybrid\nquantum-classical architectures offer compelling advantages in accuracy,\ntraining efficiency, and parameter scalability, particularly for complex vision\ntasks.", "AI": {"tldr": "研究比较了混合量子经典模型与纯经典模型在三个数据集上的表现。结果显示，混合模型无论是在最终准确率、训练速度、参数量上都优于经典模型，特别在复杂任务上更显优势。", "motivation": "研究的动机是对混合量子经典神经网络与经典模型在性能、效率和鲁棒性方面的系统比较，特别是在处理复杂视觉任务时的潜在优势。", "method": "该研究比较了混合量子经典神经网络与纯经典模型在三个基准数据集（MNIST、CIFAR100和STL10）上的表现。混合模型结合了参数化量子电路和传统深度学习架构，经典模型则使用传统的卷积神经网络（CNN）。", "result": "实验结果显示，混合模型在最终准确率上始终优于经典模型，特别是对于复杂的数据集。混合模型的训练速度快5至12倍，使用的参数更少，同时保持了对未见测试数据更好的泛化能力。", "conclusion": "研究得出结论，混合量子经典架构在准确性、训练效率和参数量扩展性方面有着显著的优势，特别是在处理复杂视觉任务上。"}}
{"id": "2509.13672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13672", "abs": "https://arxiv.org/abs/2509.13672", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.", "AI": {"tldr": "本研究提出了CL$^2$GEC，第一个针对多学科中文语法学错误纠正的持续学习基准。结果显示，基于正则化的持续学习方法比基于回放或简单的顺序方法更能有效地防止遗忘。", "motivation": "现有中文语法错误纠正研究缺乏一个多学科的学术写作风格基准，忽略了持续学习作为解决领域特定语言变异问题的潜力。研究旨在填补这一空白。", "method": "介绍CL$^2$GEC，一个用于多学科中文语文学术写作的语法错误纠正的持续学习基准。基准包含了10000个涵盖10个学科的人工标注句子，每个学科有独特的语言风格和错误模式。", "result": "实验结果显示，基于正则化的持续学习方法比基于回放的或简单的顺序方法更能有效抵抗遗忘。", "conclusion": "本研究的基准为未来在跨多个学术领域的自适应语法错误纠正研究提供了一个严格的基础。"}}
{"id": "2509.13361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13361", "abs": "https://arxiv.org/abs/2509.13361", "authors": ["Tong Yulin", "Liang Xuechen"], "title": "Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention", "comment": null, "summary": "Expressway traffic congestion severely reduces travel efficiency and hinders\nregional connectivity. Existing \"detection-prediction\" systems have critical\nflaws: low vehicle perception accuracy under occlusion and loss of\nlong-sequence dependencies in congestion forecasting. This study proposes an\nintegrated technical framework to resolve these issues.For traffic flow\nperception, two baseline algorithms were optimized. Traditional YOLOv11 was\nupgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort\nwas improved by fusing Mahalanobis (motion) and cosine (appearance) distances.\nExperiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\\%\nmAP (6.5 percentage points higher than baseline) with 5.3\\% occlusion miss\nrate. DeepSort reached 93.8\\% MOTA (11.3 percentage points higher than SORT)\nwith only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km\nhigh-density scenarios), speed and density showed a strong negative correlation\n(r=-0.97), conforming to traffic flow theory. For congestion warning, a\nGRU-Attention model was built to capture congestion precursors. Trained 300\nepochs with flow, density, and speed, it achieved 99.7\\% test accuracy (7-9\npercentage points higher than traditional GRU). In 10-minute advance warnings\nfor 30-minute congestion, time error was $\\leq$ 1 minute. Validation with an\nindependent video showed 95\\% warning accuracy, over 90\\% spatial overlap of\ncongestion points, and stable performance in high-flow ($>$5 vehicles/second)\nscenarios.This framework provides quantitative support for expressway\ncongestion control, with promising intelligent transportation applications.", "AI": {"tldr": "The paper proposes and validates an advanced traffic management system for expressways, integrating enhanced YOLOv11-DIoU, improved DeepSort, and a GRU-Attention model for improved congestion forecast and vehicle tracking, significantly reducing errors and improving overall efficiency compared to traditional methods.", "motivation": "The primary motivation behind this study is to address the limitations of existing traffic flow management systems in expressways, such as poor detection accuracy under occlusion and loss of long-term dependencies in congestion prediction, which are critical factors in maintaining efficient regional connectivity and traffic flow.", "method": "The study proposes an integrated framework that optimizes traffic flow perception and congestion forecasting. It upgrades YOLOv11 to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss for better vehicle perception accuracy, especially under occlusion. Also, the DeepSort algorithm is enhanced by incorporating Mahalanobis and cosine distances for more reliable object tracking. For congestion prediction, a GRU-Attention model is developed to enhance the understanding of congestion patterns.", "result": "The experiments revealed that the YOLOv11-DIoU model achieved a Mean Average Precision (mAP) of 95.7%, a 6.5 percentage point increase from the base YOLOv11, with a 5.3% decrease in occlusion miss rates. The modified DeepSort method demonstrated a Multi-Object Tracking Accuracy (MOTA) of 93.8%, surpassing the baseline by 11.3 percentage points, with minimal identity switching errors. The GRU-Attention model showed a 99.7% test accuracy in predicting congestion, praising its stability even in high flow scenarios, and achieved precise 10-minute advance warnings for 30-minute congestion events, with a time error of no more than 1 minute.", "conclusion": "The integrated framework successfully improves expressway traffic congestion management by enhancing vehicle detection accuracy and traffic prediction reliability, actionable for intelligent transportation systems."}}
{"id": "2509.13677", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13677", "abs": "https://arxiv.org/abs/2509.13677", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.", "AI": {"tldr": "A scalable framework named AgentCTG is introduced to improve complex control over text generation through multi-agent simulation and an auto-prompt module, showing state-of-the-art results on public datasets and in the Character-Driven Rewriting task.", "motivation": "The motivation stems from the challenges in achieving conditional control over text generation, especially in real-world and online applications where scalability, domain knowledge, and cost-effectiveness are crucial.", "method": "The paper proposes a framework named AgentCTG that simulates multi-agent workflows to improve precise control over text generation. It also introduces an auto-prompt module to enhance generation effectiveness.", "result": "AgentCTG achieves state-of-the-art results on multiple public datasets and demonstrates effectiveness in practical applications such as the Character-Driven Rewriting task.", "conclusion": "The proposed AgentCTG framework not only achieves high performance in public datasets but also significantly enhances the experience in online applications by improving content delivery and personalization."}}
{"id": "2509.13366", "categories": ["cs.CV", "68U99", "J.2"], "pdf": "https://arxiv.org/pdf/2509.13366", "abs": "https://arxiv.org/abs/2509.13366", "authors": ["Tony Rohe", "Martin Margreiter", "Markus Moertl"], "title": "Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks", "comment": "10 pages, 5 figures", "summary": "This research is part of a study of a real-time, cloud-based on-street\nparking service using crowd-sourced in-vehicle fleet data. The service provides\nreal-time information about available parking spots by classifying\ncrowd-sourced detections observed via ultrasonic sensors. The goal of this\nresearch is to optimize the current parking service quality by analyzing the\nautomation of the existing test process for ground truth tests. Therefore,\nmethods from the field of machine learning, especially image pattern\nrecognition, are applied to enrich the database and substitute human\nengineering work in major areas of the analysis process. After an introduction\ninto the related areas of machine learning, this paper explains the methods and\nimplementations made to achieve a high level of automation, applying\nconvolutional neural networks. Finally, predefined metrics present the\nperformance level achieved, showing a time reduction of human resources up to\n99.58 %. The overall improvements are discussed, summarized, and followed by an\noutlook for future development and potential application of the analysis\nautomation tool.", "AI": {"tldr": "该研究利用机器学习技术，特别是卷积神经网络，自动测试并优化实时云基路侧停车服务的地面实况，大幅减少了人工时间。", "motivation": "优化现有的实时云基路侧停车服务，通过自动化分析过程中的人工工程工作，来提高服务质量。", "method": "使用卷积神经网络(CNN)和其他机器学习方法，特别是图像模式识别技术，来优化现有的基于人群来源的泊车服务测试过程自动化。", "result": "实现了高达99.58%的人力资源时间减少，提升了自动化水平。", "conclusion": "讨论了整体改进，并对分析自动化工具的未来发展和潜在应用进行了展望。"}}
{"id": "2509.13683", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13683", "abs": "https://arxiv.org/abs/2509.13683", "authors": ["Suyuchen Wang", "Jinlin Wang", "Xinyu Wang", "Shiqi Li", "Xiangru Tang", "Sirui Hong", "Xiao-Wen Chang", "Chenglin Wu", "Bang Liu"], "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning", "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks.", "AI": {"tldr": "论文提出CARE框架，通过模型自身的检索能力增强推理精度和答案生成性能，实验显示显著优于现有方法。", "motivation": "大型语言模型（LLMs）在处理上下文保真度时经常遇到问题，导致在回答问题时产生不一致的答案。当前的方法要么依赖于昂贵的监督微调来生成证据，要么训练模型执行网络搜索但不一定改善给定上下文的利用。", "method": "本文提出了CARE框架，这是一种新颖的本机检索增强推理框架，通过模型自身的检索能力来明确整合上下文证据。这种方法需要少量的标记证据数据，并通过策略性地检索推理链中的上下文令牌来显著提高检索精度和答案生成性能。", "result": "在多个现实世界和反事实问答基准上的广泛实验表明，此方法显著优于监督微调、传统的检索增强生成方法和外部检索解决方案。", "conclusion": "这项工作代表着在使大型语言模型更准确、更可靠、更高效地执行知识密集型任务方面的一项根本性进展。"}}
{"id": "2509.13375", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13375", "abs": "https://arxiv.org/abs/2509.13375", "authors": ["Yuxiao Lee", "Xiaofeng Cao", "Wei Ye", "Jiangchao Yao", "Jingkuan Song", "Heng Tao Shen"], "title": "An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity", "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot out-of-distribution (OOD) detection capabilities, vital for reliable\nAI systems. Despite this promising capability, a comprehensive understanding of\n(1) why they work so effectively, (2) what advantages do they have over\nsingle-modal methods, and (3) how is their behavioral robustness -- remains\nnotably incomplete within the research community. This paper presents a\nsystematic empirical analysis of VLM-based OOD detection using in-distribution\n(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and\nformalize key operational properties within the VLM embedding space that\nfacilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the\nsuperiority of these models over established single-modal approaches,\nattributing this distinct advantage to the VLM's capacity to leverage rich\nsemantic novelty. (3) Sensitivity: We uncovers a significant and previously\nunder-explored asymmetry in their robustness profile: while exhibiting\nresilience to common image noise, these VLM-based methods are highly sensitive\nto prompt phrasing. Our findings contribute a more structured understanding of\nthe strengths and critical vulnerabilities inherent in VLM-based OOD detection,\noffering crucial, empirically-grounded guidance for developing more robust and\nreliable future designs.", "AI": {"tldr": "本文通过系统性地分析基于VLM的OOD检测，量化了其在OOD检测上的优势，也揭示了其对提示措辞的高度敏感性，为设计更可靠的AI系统提供了重要指导。", "motivation": "尽管VLM，如CLIP，展示了显著的零样本OOD检测能力，但对其为什么如此有效、与单模态方法相比有何优势以及其行为鲁棒性等方面的全面理解仍不完整。本文旨在通过系统性的实证研究填补这些空白，从而为未来更强大、更可靠的AI系统设计提供实证基础的指导。", "method": "本文通过使用在分布（ID）和出分布（OOD）提示，对基于视觉语言模型（VLM）的OOD检测进行了系统性的实证分析。首先，系统性地描述并形式化了VLM嵌入空间中促进零样本OOD检测的关键操作属性。接着，通过实证量化这些模型相对于已建立的单模态方法的优势，归因于VLM能够利用丰富的语义新颖性。最后，揭示了其鲁棒性配置文件中的一个重要且之前研究不足的不对称性：虽然对常见的图像噪声表现出韧性，但这些基于VLM的方法对提示措辞高度敏感。", "result": "研究表明，基于VLM的OOD检测展现出了零样本能力，但其鲁棒性存在不对称性，对提示措辞高度敏感。", "conclusion": "研究结果为基于VLM的OOD检测内在的优势和关键脆弱性提供了更结构化的理解，同时也为设计未来更强大、更可靠的VLM系统提供了重要的实证依据。"}}
{"id": "2509.13695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13695", "abs": "https://arxiv.org/abs/2509.13695", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?", "comment": "To appear in Proceedings of the 16th International Conference on\n  Computational Semantics (IWCS 2025)", "summary": "Large Language Models (LLMs) perform remarkably well in Natural Language\nInference (NLI). However, NLI involving numerical and logical expressions\nremains challenging. Comparatives are a key linguistic phenomenon related to\nsuch inference, but the robustness of LLMs in handling them, especially in\nlanguages that are not dominant in the models' training data, such as Japanese,\nhas not been sufficiently explored. To address this gap, we construct a\nJapanese NLI dataset that focuses on comparatives and evaluate various LLMs in\nzero-shot and few-shot settings. Our results show that the performance of the\nmodels is sensitive to the prompt formats in the zero-shot setting and\ninfluenced by the gold labels in the few-shot examples. The LLMs also struggle\nto handle linguistic phenomena unique to Japanese. Furthermore, we observe that\nprompts containing logical semantic representations help the models predict the\ncorrect labels for inference problems that they struggle to solve even with\nfew-shot examples.", "AI": {"tldr": "本文针对大规模语言模型在日语比较级自然语言推理任务中的不足进行了研究，构建了特定的数据集，并展示了模型在不同设置下的表现和潜在的改进步骤。", "motivation": "大规模语言模型在自然语言推理任务中表现优秀，但在涉及数值和逻辑表达的推理方面仍然具有挑战性。特别是对于数据集不足的语言，如日语，模型在处理比较级时的鲁棒性研究尚不充分。", "method": "我们构建了一个专门针对日语中比较级的自然语言推理数据集，并在零样本和少样本设置下评估了各种大规模语言模型的表现。", "result": "实验结果表明，在零样本设置中，模型的表现受提示格式的影响显著；而在少样本设置中，则受黄金标签的影响。此外，这些模型在处理日语特有的语言现象时表现出困难。值得注意的是，包含逻辑语义表示的提示有助于模型正确解答它们在少样本情况下都难以解决的推理问题。", "conclusion": "研究揭示了大规模语言模型在处理日语中的比较级推理任务时的局限性，并指出了通过改进提示格式，可以改善模型的表现。这为未来研究提供了新的方向。"}}
{"id": "2509.13385", "categories": ["cs.CV", "cs.DM", "cs.LG", "51K05 (primary) 57-08, 53Z50, 55U10 (secondary)", "G.2.2"], "pdf": "https://arxiv.org/pdf/2509.13385", "abs": "https://arxiv.org/abs/2509.13385", "authors": ["Charlotte Beylier", "Parvaneh Joharinad", "Jürgen Jost", "Nahid Torbati"], "title": "Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension", "comment": "31 pages, 14 figures", "summary": "Utilizing recently developed abstract notions of sectional curvature, we\nintroduce a method for constructing a curvature-based geometric profile of\ndiscrete metric spaces. The curvature concept that we use here captures the\nmetric relations between triples of points and other points. More\nsignificantly, based on this curvature profile, we introduce a quantitative\nmeasure to evaluate the effectiveness of data representations, such as those\nproduced by dimensionality reduction techniques. Furthermore, Our experiments\ndemonstrate that this curvature-based analysis can be employed to estimate the\nintrinsic dimensionality of datasets. We use this to explore the large-scale\ngeometry of empirical networks and to evaluate the effectiveness of\ndimensionality reduction techniques.", "AI": {"tldr": "本文提出了一种基于截面曲率的概念来评估数据表示有效性的方法。", "motivation": "提出一种新的基于曲率的度量来评估数据表示的有效性，尤其是那些由降维技术产生的表示。", "method": "利用截面曲率的最新抽象概念，提出了一种基于曲率的离散度量空间几何轮廓构建方法。", "result": "实验表明，基于曲率的分析可用于估计数据集的内在维度，并用于探索经验网络的大规模几何结构。", "conclusion": "该研究展示了基于曲率的分析方法在评估降维技术有效性方面的潜力。"}}
{"id": "2509.13696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13696", "abs": "https://arxiv.org/abs/2509.13696", "authors": ["Iyadh Ben Cheikh Larbi", "Ajay Madhavan Ravichandran", "Aljoscha Burchardt", "Roland Roller"], "title": "Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes", "comment": "Presented and published at BioCreative IX", "summary": "Large language models (LLMs) excel at text generation, but their ability to\nhandle clinical classification tasks involving structured data, such as time\nseries, remains underexplored. In this work, we adapt instruction-tuned LLMs\nusing DSPy-based prompt optimization to process clinical notes and structured\nEHR inputs jointly. Our results show that this approach achieves performance on\npar with specialized multimodal systems while requiring less complexity and\noffering greater adaptability across tasks.", "AI": {"tldr": "研究使用基于DSPy的prompt优化技术改进了大型语言模型处理临床和结构化数据的能力，取得了与专业多模态系统相当的结果，且更为简单和灵活。", "motivation": "尽管大型语言模型在文本生成方面表现出色，但它们处理包括时间序列在内的结构化数据的临床分类任务的能力尚未得到充分研究。", "method": "研究者通过使用基于DSPy的prompt优化技术，将指令调整后的大型语言模型(LLMs)应用于处理临床笔记和结构化电子健康记录(EHR)输入的联合分析。", "result": "研究结果显示，这种方法在临床任务上的性能可与专业化的多模态系统相媲美。", "conclusion": "同时，这种方法相较于专业的多模态系统更为简单，且在不同任务之间的适应性更强。"}}
{"id": "2509.13388", "categories": ["cs.CV", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.13388", "abs": "https://arxiv.org/abs/2509.13388", "authors": ["Yadvendra Gurjar", "Ruoni Wan", "Ehsan Farahbakhsh", "Rohitash Chandra"], "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji", "comment": null, "summary": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.", "AI": {"tldr": "研究通过机器学习和遥感技术，对比分析了2013年至2024年斐济纳迪地区的土地使用和土地覆盖变化，目标是为该地区的土地覆盖/土地使用建模和变化检测提供技术支持。", "motivation": "斐济作为一个发展中国家，正面临城市化加速，本研究旨在为土地覆盖/土地使用建模和变化检测提供技术支持。", "method": "此研究使用了Landsat-8卫星图像，并借助Google Earth Engine及无监督机器学习中的k-均值聚类方法生成土地覆盖地图。同时使用卷积神经网络对选定区域的土地覆盖类型进行分类。", "result": "通过使用机器学习和遥感技术，研究呈现了2013年至2024年间斐济纳迪地区土地使用和土地覆盖变化的对比。", "conclusion": "该研究提供了一种可视化变化检测的方法，用于监测地图上的城市区域变化。"}}
{"id": "2509.13702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13702", "abs": "https://arxiv.org/abs/2509.13702", "authors": ["Xiao Zheng"], "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models", "comment": null, "summary": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality.", "AI": {"tldr": "Introduces DSCC-HS, a framework that uses proxy models to enhance the factual consistency of LLMs during text generation, achieving state-of-the-art results.", "motivation": "The motivation is to address the issue of hallucinations in Large Language Models (LLMs), which currently limits their reliable deployment by making them more factual and consistent.", "method": "Dynamic Self-reinforcing Calibration for Hallucination Suppression (DSCC-HS) uses compact proxy models for real-time adjustment to encode factual information and avoid hallucinations during the generation of text.", "result": "Experiments showed that DSCC-HS achieved the best performance on factual consistency and scores in benchmarks like TruthfulQA and BioGEN.", "conclusion": "DSCC-HS is an efficient method for improving the factual reliability of LLMs during the autoregressive decoding process, making it a promising solution for addressing LLM hallucinations."}}
{"id": "2509.13396", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13396", "abs": "https://arxiv.org/abs/2509.13396", "authors": ["Xinan Wang", "Di Shi", "Fengyu Wang"], "title": "Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence", "comment": "12 page Journal paper, accepted by IEEE Open Access Journal of Power\n  and Energy", "summary": "This paper presents a novel three-stage framework for real-time foreign\nobject intrusion (FOI) detection and tracking in power transmission systems.\nThe framework integrates: (1) a YOLOv7 segmentation model for fast and robust\nobject localization, (2) a ConvNeXt-based feature extractor trained with\ntriplet loss to generate discriminative embeddings, and (3) a feature-assisted\nIoU tracker that ensures resilient multi-object tracking under occlusion and\nmotion. To enable scalable field deployment, the pipeline is optimized for\ndeployment on low-cost edge hardware using mixed-precision inference. The\nsystem supports incremental updates by adding embeddings from previously unseen\nobjects into a reference database without requiring model retraining. Extensive\nexperiments on real-world surveillance and drone video datasets demonstrate the\nframework's high accuracy and robustness across diverse FOI scenarios. In\naddition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's\npracticality and scalability for real-world edge applications.", "AI": {"tldr": "本文提出了一种新型三阶段框架，用于电力传输系统中实时的外来物体入侵检测和跟踪，通过优化边缘计算硬件实现了高效的实地部署。", "motivation": "该研究的动机在于解决电力传输系统中实时FOI检测和跟踪的问题，以提高系统的安全性，并确保在遮挡和移动等复杂情况下的鲁棒性。", "method": "该论文提出了一种用于电力传输系统中实时外来物体入侵(FOI)检测和跟踪的新型三阶段框架。框架整合了：(1) 使用YOLOv7分割模型进行快速和鲁棒的目标定位；(2) 使用三元组损失训练的ConvNeXt特征提取器，生成判别性嵌入；(3) 配备特征辅助IoU跟踪器，可以确保遮挡和运动下的鲁棒多目标跟踪。为了实现可扩展的实地部署，该流水线被优化为在低成本边缘硬件上使用混合精度推理。", "result": "大量的实验表明，该框架在真实世界的监控视频和无人机视频数据集上具有高精度和对各种外来物体入侵场景的鲁棒性。此外，硬件基准测试使用了NVIDIA Jetson设备，验证了框架对于实际边缘应用的实用性和可扩展性。", "conclusion": "该框架通过YOLOv7分割模型、ConvNeXt特征提取器和特征辅助IoU跟踪器的有效结合，展示了在电力传输系统中外来物体入侵检测和跟踪中的高精度和鲁棒性，并且对于低功耗边缘设备的部署也实用且可扩展。"}}
{"id": "2509.13706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13706", "abs": "https://arxiv.org/abs/2509.13706", "authors": ["Peter Beidler", "Mark Nguyen", "Kevin Lybarger", "Ola Holmberg", "Eric Ford", "John Kang"], "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models", "comment": null, "summary": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset.", "AI": {"tldr": "研究开发了一种跨机构自然语言处理（NLP）模型，用于检测放射肿瘤学中心的事故报告，其检测高严重程度报告的能力与人类相似。", "motivation": "事故报告是医疗安全和质量改进的重要工具，但手动审查耗时且需要专业知识。本文介绍一种用于放射肿瘤学中检测高严重程度事故报告的自然语言处理（NLP）筛选工具。", "method": "使用了两个文本数据集来训练和评估自然语言处理（NLP）模型：一个是来自作者所在机构的7,094份报告，另一个是来自IAEA SAFRON的571份报告，所有报告都由临床内容专家标注了严重程度得分。训练了两种类型的模型：基线支持向量机（SVM）和预先训练过的大型语言模型BlueBERT，后者基于PubMed摘要和住院患者数据进行预训练。", "result": "基线SVM模型和BlueBERT模型在作者机构测试数据上的分类性能分别为AUROC 0.82和0.81。而在没有跨机构迁移学习的情况下，这两种模型在SF测试数据上的表现较差，分别为AUROC 0.42和0.56。经过跨机构迁移学习的BlueBERT_TRANSF模型在SF测试集上的性能提升到AUROC 0.78，与人类评审员的表现相当（AUROC 0.81）。", "conclusion": "本研究成功开发了跨机构的自然语言处理模型来评估辐射肿瘤学中心的事故报告文本，并且这些模型在编辑过的数据集上能够像人类一样准确地检测出高严重程度的报告。"}}
{"id": "2509.13399", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13399", "abs": "https://arxiv.org/abs/2509.13399", "authors": ["Tianyu Chen", "Yasi Zhang", "Zhi Zhang", "Peiyu Yu", "Shu Wang", "Zhendong Wang", "Kevin Lin", "Xiaofei Wang", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Jianwen Xie", "Oscar Leong", "Lijuan Wang", "Ying Nian Wu", "Mingyuan Zhou"], "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing", "comment": "Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan\n  Wang, Ying Nian Wu, and Mingyuan Zhou advised equally", "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.", "AI": {"tldr": "该论文提出了EdiVal-Agent框架及EdiVal-Bench基准测试，用于评估多轮指令驱动的图像编辑模型的性能，通过结合零样本视觉语言模型、开放词表对象检测器和其他工具进行评估。", "motivation": "旨在解决当前基于指令的图像编辑领域的可靠性和可解释性评估难题。现有方法要么依赖成对的参考图像，这导致覆盖范围有限且继承自先前生成模型的偏差，要么仅依赖零样本视觉语言模型（VLMs），这些模型对于指令从属、内容一致性和视觉质量的评估往往不够精确。", "method": "介绍了一种名为EdiVal-Agent的自动化、可扩展、细粒度的多轮指令图像编辑评估框架，该框架支持一系列专家工具。首先，将图像分解为语义上有意义的对象，然后合成多样化的上下文感知编辑指令。在评估过程中，结合零样本视觉语言模型（VLMs）和开放词表对象检测器来评估指令从属，使用语义级特征提取器来评估内容一致性，并利用人类偏好模型来判断视觉质量。此外，该管道的模块化设计允许未来工具的无缝集成，提高评估准确性。", "result": "构建了EdiVal-Bench基准测试，涵盖9种指令类型和11种最先进的编辑模型，包括自回归（AR）模型、流匹配模型和扩散模型。展示了EdiVal-Agent框架可用于识别现有模型的失败模式，从而指导新模型的发展。", "conclusion": "结合VLMs和对象检测器在指令从属评估中与人类判断的协议比单独使用VLMs和CLIP度量标准更强，而且EdiVal-Agent能够识别现有失败模式，为进一步开发下一代编辑模型提供指导。"}}
{"id": "2509.13723", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13723", "abs": "https://arxiv.org/abs/2509.13723", "authors": ["Yaxin Gao", "Yao Lu", "Zongfei Zhang", "Jiaqi Nie", "Shanqing Yu", "Qi Xuan"], "title": "DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage processing (NLP) tasks. To achieve more accurate output, the prompts\nused to drive LLMs have become increasingly longer, which incurs higher\ncomputational costs. To address this prompt inflation problem, prompt\ncompression has been proposed. However, most existing methods require training\na small auxiliary model for compression, incurring a significant amount of\nadditional computation. To avoid this, we propose a two-stage, training-free\napproach, called Dual-Stage Progressive Compression (DSPC). In the\ncoarse-grained stage, semantic-related sentence filtering removes sentences\nwith low semantic value based on TF-IDF. In the fine-grained stage, token\nimportance is assessed using attention contribution, cross-model loss\ndifference, and positional importance, enabling the pruning of low-utility\ntokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct\nand GPT-3.5-Turbo under a constrained token budget and observe consistent\nimprovements. For instance, in the FewShot task of the Longbench dataset, DSPC\nachieves a performance of 49.17 by using only 3x fewer tokens, outperforming\nthe best state-of-the-art baseline LongLLMLingua by 7.76.", "AI": {"tldr": "The paper introduces DSPC, a two-stage training-free method for prompt compression in large language models, which improves performance while reducing the number of tokens, outperforming existing methods in benchmark tests.", "motivation": "The motivation behind this work is to address the prompt inflation problem in large language models, where increasingly longer prompts are needed to achieve more accurate output, incurring higher computational costs. Existing prompt compression methods often require training small auxiliary models, which in turn incur additional computation.", "method": "The paper proposes Dual-Stage Progressive Compression (DSPC), a two-stage, training-free method for prompt compression in large language models. In the first stage, semantic-related sentences are filtered based on TF-IDF to remove low semantic value sentences. In the second stage, tokens are pruned based on their importance assessed by attention contribution, cross-model loss difference, and positional importance.", "result": "The DSPC method was validated on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo under a constrained token budget. In the FewShot task of the Longbench dataset, DSPC achieved a performance of 49.17 using 3x fewer tokens, outperforming the best state-of-the-art baseline LongLLMLingua by 7.76.", "conclusion": "The study concludes that the proposed DSPC method efficiently compresses prompts in large language models, significantly reducing computational costs without sacrificing performance. The approach outperforms state-of-the-art baselines under a constrained token budget."}}
{"id": "2509.13414", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13414", "abs": "https://arxiv.org/abs/2509.13414", "authors": ["Nikhil Keetha", "Norman Müller", "Johannes Schönberger", "Lorenzo Porzi", "Yuchen Zhang", "Tobias Fischer", "Arno Knapitsch", "Duncan Zauss", "Ethan Weber", "Nelson Antunes", "Jonathon Luiten", "Manuel Lopez-Antequera", "Samuel Rota Bulò", "Christian Richardt", "Deva Ramanan", "Sebastian Scherer", "Peter Kontschieder"], "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "comment": "Project Page: https://map-anything.github.io/", "summary": "We introduce MapAnything, a unified transformer-based feed-forward model that\ningests one or more images along with optional geometric inputs such as camera\nintrinsics, poses, depth, or partial reconstructions, and then directly\nregresses the metric 3D scene geometry and cameras. MapAnything leverages a\nfactored representation of multi-view scene geometry, i.e., a collection of\ndepth maps, local ray maps, camera poses, and a metric scale factor that\neffectively upgrades local reconstructions into a globally consistent metric\nframe. Standardizing the supervision and training across diverse datasets,\nalong with flexible input augmentation, enables MapAnything to address a broad\nrange of 3D vision tasks in a single feed-forward pass, including uncalibrated\nstructure-from-motion, calibrated multi-view stereo, monocular depth\nestimation, camera localization, depth completion, and more. We provide\nextensive experimental analyses and model ablations demonstrating that\nMapAnything outperforms or matches specialist feed-forward models while\noffering more efficient joint training behavior, thus paving the way toward a\nuniversal 3D reconstruction backbone.", "AI": {"tldr": "MapAnything是一种统一模型，能够处理多个3D视觉任务并提供高效的联合训练，旨在成为3D场景重建的基础模型。", "motivation": "通过标准化跨不同数据集的监督和训练，以及灵活的输入增强，MapAnything旨在解决3D重建中的多个任务，且在单次前馈传递中就可完成。这样做可以通向一个通用的3D重建基础模型。", "method": "我们介绍了一种称为MapAnything的统一变压器基础前馈模型，该模型可以处理一个或多个图像以及可选的几何输入，如相机内在参数、姿态、深度或部分重建，并直接回归出3D场景的度量几何和相机参数。MapAnything利用多视角场景几何的分解表示，即一系列的深度图、局部光线图、相机姿态和一个度量尺度因子，有效地将局部重建升级为全局一致的度量框架。", "result": "实验分析和模型消融研究证明，MapAnything在广泛的3D视觉任务上，包括未校正的从运动中获取结构、校正的多视角立体视觉、单目深度估计、相机定位、深度填充等，都能超越或匹配到专业的前馈模型，并提供更高效的联合训练行为。", "conclusion": "MapAnything展示了作为一个通用3D重建基础模型的潜力，能够高效地解决多种不同的3D视觉任务。"}}
{"id": "2509.13734", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13734", "abs": "https://arxiv.org/abs/2509.13734", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Implementing a Logical Inference System for Japanese Comparatives", "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)", "summary": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs.", "AI": {"tldr": "本研究针对日语中的比较推理问题，提出了一种基于组合语义的逻辑推理系统ccg-jcomp，并通过实验证明其有效性。", "motivation": "尽管一些方法依赖于大型语言模型(LLMs)，但这些方法难以直接应用于日语中的比较表达，因为日语和英语之间存在形态和语义上的差异。为了解决这一问题，本研究提出了ccg-jcomp系统。", "method": "提出了一种基于组合语义的日语比较推理系统ccg-jcomp，用以处理日语中的比较表达，并在日语NLI数据集上进行了评估。", "result": "通过与现有LLMs的准确性进行对比，证明了ccg-jcomp系统的有效性。", "conclusion": "ccg-jcomp系统能够有效地处理日语中的比较表达，并且在准确性上超过了现有的一些LLMs。"}}
{"id": "2509.13474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13474", "abs": "https://arxiv.org/abs/2509.13474", "authors": ["Yujia Lin", "Nicholas Evans"], "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization", "comment": null, "summary": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods.", "AI": {"tldr": "本文提出了一个结合RGB图像高级语义信息用于LiDAR地图中的鲁棒定位的方法SCM-PR，该方法在多个数据集上显示出了优于其他跨模态定位方法的性能。", "motivation": "保证没有GPS能力的环境中机器人的准确定位是一个挑战。现有的基于RGB的方法对光照、天气和其他季节性变化都很敏感。现有的跨模态定位方法也存在复杂场景、细粒度或高分辨率匹配和视角变化的困难。", "method": "Semantic-Enhanced Cross-Modal Place Recognition (SCM-PR)框架，它结合了RGB图像的高级语义信息，用于在LiDAR地图中实现鲁棒定位。该方法包括：使用VMamba骨干网络提取RGB图像特征；使用语义感知特征融合(SAFF)模块同时利用位置描述符和分割掩码；结合语义和几何信息的LiDAR描述符；以及NetVLAD中的跨模态语义注意力机制，提高匹配效果。", "result": "在KITTI和KITTI-360数据集上的实验表明，SCM-PR方法在与其他跨模态位置识别方法的对比中达到了最先进的性能。", "conclusion": "通过结合语义信息，提出的方法增强了跨模态位置识别的能力，并实现了在多种环境条件下的更精确定位。"}}
{"id": "2509.13775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13775", "abs": "https://arxiv.org/abs/2509.13775", "authors": ["Vani Kanjirangat", "Ljiljana Dolamic", "Fabio Rinaldi"], "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications", "comment": "4 main pages, 4 additional, 5 figures", "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.", "AI": {"tldr": "本研究表明在阿拉伯语方言识别任务中，软提示策略和LoRA重参数化方法可以有效提升性能，其中基于LoRA的微调模型实现最佳结果。", "motivation": "研究的动机在于优化资源利用效率，探究可用于阿拉伯语方言识别的高效数据使用和参数使用方法。希望通过使用较少数据和较少参数的方法改善方言识别的性能。", "method": "本文研究了不同的数据高效和参数高效方法在阿拉伯语方言识别（ADI）中的应用。具体而言，我们探讨了各种软提示策略，包括前缀调优、提示调优、P调优及其新版P调优V2，以及LoRA重参数化。在数据高效策略方面，我们分析了零样本和少样本推理下的硬提示来评估LLM（大型语言模型）的方言识别能力。对于参数高效的PEFT方法，我们在多个主要数据集上使用了专用的阿拉伯文编码模型进行了实验。我们还分析了开源的仅有解码器模型、一个通用的多语言模型（Phi-3.5）和一个专门的阿拉伯文模型（SILMA）的n样本推理表现。", "result": "我们发现，尽管LLMs在几种不同的条件下评估表现都不佳，尤其在零样本和少样本设定中难以区分方言变化，但经过软提示的编码器变体表现得较好，而基于LoRA的微调模型表现最佳，甚至超过了全量微调的效果。", "conclusion": "研究结论是软提示方法和LoRA重参数化能够为阿拉伯语方言识别提供高效且性能优越的解决方案，尤其是在资源有限的情况下。"}}
{"id": "2509.13482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13482", "abs": "https://arxiv.org/abs/2509.13482", "authors": ["Hao Xu", "Xiaolin Wu", "Xi Zhang"], "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization", "comment": "Code available at https://github.com/hxu160/SALVQ", "summary": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its\nphotorealistic rendering quality and real-time performance, but it generates\nmassive amounts of data. Hence compressing 3DGS data is necessary for the cost\neffectiveness of 3DGS models. Recently, several anchor-based neural compression\nmethods have been proposed, achieving good 3DGS compression performance.\nHowever, they all rely on uniform scalar quantization (USQ) due to its\nsimplicity. A tantalizing question is whether more sophisticated quantizers can\nimprove the current 3DGS compression methods with very little extra overhead\nand minimal change to the system. The answer is yes by replacing USQ with\nlattice vector quantization (LVQ). To better capture scene-specific\ncharacteristics, we optimize the lattice basis for each scene, improving LVQ's\nadaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a\nbalance between the R-D efficiency of vector quantization and the low\ncomplexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS\ncompression architectures, enhancing their R-D performance with minimal\nmodifications and computational overhead. Moreover, by scaling the lattice\nbasis vectors, SALVQ can dynamically adjust lattice density, enabling a single\nmodel to accommodate multiple bit rate targets. This flexibility eliminates the\nneed to train separate models for different compression levels, significantly\nreducing training time and memory consumption.", "AI": {"tldr": "The paper proposes Scene-Adaptive Lattice Vector Quantization (SALVQ) to improve the compression performance and efficiency of 3D Gaussian Splatting (3DGS) models without substantially increasing computational costs or complexity.", "motivation": "To enhance the compression efficiency of 3DGS models while keeping computational costs low by replacing the commonly used uniform scalar quantization (USQ) with a more adaptable quantization method.", "method": "The authors introduce Scene-Adaptive Lattice Vector Quantization (SALVQ), which optimizes lattice basis for each specific scene and can dynamically adjust lattice density to accommodate different bit rate targets.", "result": "SALVQ significantly improves R-D performance over USQ with minimal computational overhead, and it can be integrated into existing 3DGS compression models. Additional benefits include reduced training times and memory consumption.", "conclusion": "SALVQ represents an effective upgrade to current 3DGS compression methods by offering better adaptability and efficiency without compromising the simplicity and cost-effectiveness of the system."}}
{"id": "2509.13790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13790", "abs": "https://arxiv.org/abs/2509.13790", "authors": ["Yangning Li", "Tingwei Lu", "Yinghui Li", "Yankai Chen", "Wei-Chieh Huang", "Wenhao Jiang", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu"], "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "comment": "EMNLP 2025 Findings", "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.", "AI": {"tldr": "提出了一种称为CAMPUS的框架，通过动态选择子课程、适应能力的课程计划调整以及基于多种难度的调度方法，以解决课程调整中的僵化问题，实验表明其在大规模语言模型的指令微调上有更优表现。", "motivation": "当前的课程调整方法由于依赖于静态的难度指标，导致模型在训练过程中无法适应其不断发展的能力，从而产生固定的、可能次优的学习轨迹。", "method": "动态子课程选择，适应能力的课程计划调整以及基于多种难度的调度方法组成了CAMPUS框架。", "result": "实验结果证明，CAMPUS在有效的指令微调上比其他最先进的基线模型表现出更优的性能。", "conclusion": "CAMPUS在解决课程调整僵化问题和提高大规模语言模型的指令微调性能方面表现出色。"}}
{"id": "2509.13484", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13484", "abs": "https://arxiv.org/abs/2509.13484", "authors": ["Liu Liu", "Alexandra Kudaeva", "Marco Cipriano", "Fatimeh Al Ghannam", "Freya Tan", "Gerard de Melo", "Andres Sevtsuk"], "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes", "comment": "13 pages, 4 figures, under review at AAAI 2026", "summary": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.", "AI": {"tldr": "该论文针对公共空间中的群体级社会互动检测问题，提出了一种新的MINGLE模型，该模型使用了现成的人体检测、基于VLM的推理和轻量级的空间聚合算法，并创建了一个注释全面的新数据集以支持该任务并鼓励未来研究。", "motivation": "理解公共空间中的群体级社会互动对于城市规划和设计具有活力和包容性的环境至关重要。从图像中检测这类互动涵盖对关系、接近度和共运动等微妙视觉线索的解读，这些信号在语义上比传统的对象检测更为复杂。", "method": "我们提出了MINGLE（建模人际群体级互动），这是一种分三个阶段的模块化流水线。包括：（1）使用现成的人体检测和深度估计方法；（2）使用基于视觉语言模型（VLM）的推理来分类人际关系；（3）使用轻量级的空间聚合算法来定位社会互动群体。", "result": "该研究通过引入MINGLE模型和新的数据集，展示了在检测公共空间中社会互动群体方面的潜在应用价值。", "conclusion": "我们引入了一个新的数据集，并提出了一种新的方法来检测公共空间中的社会互动群体，这对于城市规划和设计更符合社会活力的环境具有重要意义。"}}
{"id": "2509.13803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13803", "abs": "https://arxiv.org/abs/2509.13803", "authors": ["Laura García-Sardiña", "Hermenegildo Fabregat", "Daniel Deniz", "Rabih Zbib"], "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages", "comment": null, "summary": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias.", "AI": {"tldr": "本文探讨了语法性别在工作头衔中的明确指派如何影响自动工作排名系统的结果，并提供了一种使用RBO来评估偏见的方法。", "motivation": "这项工作的目的是研究工作头衔中明确的语法性别分配如何影响自动工作排名系统的结果。", "method": "我们提出使用控制性别变量的评估指标（如RBO）来评价职业头衔排名系统的性别偏见。我们生成了四个具有语法性别语言的职业匹配任务测试集，这些职业以阳性或阴性形式出现，并且按性别和匹配相关性进行了标注。", "result": "通过使用新的测试集和提出的评估方法，研究人员对几个开箱即用的多语言模型进行了性别偏见评估，结果表明所有模型都表现出不同程度的性别偏见。", "conclusion": "研究表明，被评估的所有多语言模型都存在不同程度的性别偏见，这为未来的改进提供了基准。"}}
{"id": "2509.13496", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13496", "abs": "https://arxiv.org/abs/2509.13496", "authors": ["Rajatsubhra Chakraborty", "Xujun Che", "Depeng Xu", "Cori Faklaris", "Xi Niu", "Shuhan Yuan"], "title": "BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation", "comment": null, "summary": "Bias discovery is critical for black-box generative models, especiall\ntext-to-image (TTI) models. Existing works predominantly focus on output-level\ndemographic distributions, which do not necessarily guarantee concept\nrepresentations to be disentangled post-mitigation. We propose BiasMap, a\nmodel-agnostic framework for uncovering latent concept-level representational\nbiases in stable diffusion models. BiasMap leverages cross-attention\nattribution maps to reveal structural entanglements between demographics (e.g.,\ngender, race) and semantics (e.g., professions), going deeper into\nrepresentational bias during the image generation. Using attribution maps of\nthese concepts, we quantify the spatial demographics-semantics concept\nentanglement via Intersection over Union (IoU), offering a lens into bias that\nremains hidden in existing fairness discovery approaches. In addition, we\nfurther utilize BiasMap for bias mitigation through energy-guided diffusion\nsampling that directly modifies latent noise space and minimizes the expected\nSoftIoU during the denoising process. Our findings show that existing fairness\ninterventions may reduce the output distributional gap but often fail to\ndisentangle concept-level coupling, whereas our mitigation method can mitigate\nconcept entanglement in image generation while complementing distributional\nbias mitigation.", "AI": {"tldr": "本文提出 BiasMap，一个用于揭示稳定扩散模型中潜在概念层面表示偏差并进行偏差缓解的框架。", "motivation": "当前的研究主要集中在输出级别的统计分布上，这种方法无法保证概念表示在偏差消除后能很好地分离。因此，本文提出 BiasMap，以更深入地揭示生成模型中的表示偏差。", "method": "BiasMap 是一个用于揭示稳定扩散模型中潜在概念层面表示偏差的模型无关框架。它利用跨注意力归因图来揭示人口统计学（如性别、种族）和语义（如职业）之间的结构纠缠。通过这些概念的归因映射，BiasMap 量化解的统计学-语义概念纠缠，从而提供一个深入理解隐藏在现有公平性发现方法中的偏差的视角。此外，BiasMap 还进一步利用能量引导扩散采样直接修改噪声空间以实现偏差缓解。", "result": "研究发现，现有的公平性干预措施可能缩小了输出分布差距，但通常未能在概念层面实现解耦，而本文的缓解方法可以在图像生成过程中同时减轻概念级纠缠并补充分布偏差缓解。", "conclusion": "BiasMap 通过能量引导扩散采样直接修改噪声空间，以最小化预期软交并比 (SoftIoU)，从而在图像生成时实现概念级纠缠的减轻，而无需依赖输出级别的统计分布。"}}
{"id": "2509.13813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13813", "abs": "https://arxiv.org/abs/2509.13813", "authors": ["Edward Phillips", "Sean Wu", "Soheila Molaei", "Danielle Belgrave", "Anshul Thakur", "David Clifton"], "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "comment": null, "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy.", "AI": {"tldr": "研究者提出了一种新的几何框架，通过原型分析的方法，可以同时提供全局和局部的不确定性估计，有效改善了大语言模型的幻觉现象。", "motivation": "现有的黑盒方法无法同时提供全局和局部的不确定性估计，研究者希望通过引入几何框架来解决这个问题，从而改善大语言模型的幻觉现象。", "method": "提出了基于响应批次的原型分析几何框架。全局层面，提出几何体积来度量由响应嵌入导出的原型的凸包体积；局部层面，通过几何怀疑度对响应进行可靠性排序，通过优选响应选择减少幻觉。", "result": "", "conclusion": "实验表明该框架在短问答数据集上表现至少与现有方法持平甚至更好，在医学数据集上则有更佳表现。此外，通过理论证明了凸包体积与熵之间的联系。"}}
{"id": "2509.13504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13504", "abs": "https://arxiv.org/abs/2509.13504", "authors": ["Uriel Garcilazo-Cruz", "Joseph O. Okeme", "Rodrigo A. Vargas--Hernández"], "title": "LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming", "comment": "8 pages, 10 figures, SM, 5 pages, 4 figures", "summary": "The lack of flexible annotation tools has hindered the deployment of AI\nmodels in some scientific areas. Most existing image annotation software\nrequires users to upload a precollected dataset, which limits support for\non-demand pipelines and introduces unnecessary steps to acquire images. This\nconstraint is particularly problematic in laboratory environments, where\nreal-time data acquisition from instruments such as microscopes is increasingly\ncommon. In this work, we introduce \\texttt{LivePixel}, a Python-based graphical\nuser interface that integrates with imaging systems, such as webcams,\nmicroscopes, and others, to enable real-time image annotation. LivePyxel is\ndesigned to be easy to use through a simple interface that allows users to\nprecisely delimit areas for annotation using tools commonly found in commercial\ngraphics editing software. Of particular interest is the availability of\nB\\'ezier splines and binary masks, and the software's capacity to work with\nnon-destructive layers that enable high-performance editing. LivePyxel also\nintegrates a wide compatibility across video devices, and it's optimized for\nobject detection operations via the use of OpenCV in combination with\nhigh-performance libraries designed to handle matrix and linear algebra\noperations via Numpy effectively. LivePyxel facilitates seamless data\ncollection and labeling, accelerating the development of AI models in\nexperimental workflows. LivePyxel freely available at\nhttps://github.com/UGarCil/LivePyxel", "AI": {"tldr": "Introduces LivePixel, a tool for real-time image annotation from imaging systems, designed to overcome challenges in pre-existing tools by providing a user-friendly interface and integrating with video devices effectively.", "motivation": "To address the limitations of pre-existing image annotation tools in supporting laboratory environments, where real-time image acquisition is increasingly important.", "method": "Introduces LivePixel, a Python-based graphical user interface that allows real-time image annotation directly from imaging systems. It supports a simple interface designed for precise annotation and uses Bezier splines, binary masks, and non-destructive layers. Integrates well with video devices and uses OpenCV and Numpy for high-performance operations in object detection.", "result": "LivePixel enables real-time data collection and labeling with a user-friendly interface, effectively supporting on-demand pipelines for AI model development in experimental settings.", "conclusion": "The tool facilitates AI model development in scientific research by overcoming the limitations of traditional annotation tools with real-time capabilities and a simple, effective interface."}}
{"id": "2509.13814", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13814", "abs": "https://arxiv.org/abs/2509.13814", "authors": ["Kartik Shinde", "Laurent Besacier", "Ondrej Bojar", "Thibaut Thonet", "Tirthankar Ghosal"], "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge", "comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/", "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks.", "AI": {"tldr": "This paper describes AutoMin 2025, a shared task for automatic meeting summarization and QA, with baseline evaluations on minuting and QA tasks due to low team participation.", "motivation": "The motivation is to advance research in automatic meeting summarization and introduce a new QA task to explore cross-lingual understanding and question answering based on meeting transcripts.", "method": "This paper introduces the AutoMin shared task for automatic meeting summarization and a new QA task based on meeting minutes. It involves two languages and two domains for the minuting task, and two settings for the QA task.", "result": "Due to limited participation, baseline systems were included for evaluation of the latest large language models on the minuting and QA tasks.", "conclusion": "AutoMin 2025 provided a setup to assess the capabilities of large language models in meeting summarization and cross-lingual question answering, despite lower-than-expected participation by research teams."}}
{"id": "2509.13506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13506", "abs": "https://arxiv.org/abs/2509.13506", "authors": ["Xingzi Xu", "Qi Li", "Shuwen Qiu", "Julien Han", "Karim Bouyarmane"], "title": "DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform", "comment": "Published in 2025 CVPR Workshop", "summary": "Diffusion models enable high-quality virtual try-on (VTO) with their\nestablished image synthesis abilities. Despite the extensive end-to-end\ntraining of large pre-trained models involved in current VTO methods,\nreal-world applications often prioritize limited training and inference,\nserving, and deployment budgets for VTO. To solve this obstacle, we apply\nDoob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained\nunconditional models for downstream image-conditioned VTO abilities. DEFT\nfreezes the pre-trained model's parameters and trains a small h-transform\nnetwork to learn a conditional h-transform. The h-transform network allows\ntraining only 1.42 percent of the frozen parameters, compared to a baseline of\n5.52 percent in traditional parameter-efficient fine-tuning (PEFT).\n  To further improve DEFT's performance and decrease existing models' inference\ntime, we additionally propose an adaptive consistency loss. Consistency\ntraining distills slow but high-performing diffusion models into a fast one\nwhile retaining performance by enforcing consistencies along the inference\npath. Inspired by constrained optimization, instead of distillation, we combine\nthe consistency loss and the denoising score matching loss in a data-adaptive\nmanner for fine-tuning existing VTO models at a low cost. Empirical results\nshow the proposed DEFT-VTON method achieves state-of-the-art performance on VTO\ntasks, with as few as 15 denoising steps, while maintaining competitive\nresults.", "AI": {"tldr": "The paper proposes DEFT-VTON, a method that fine-tunes large pre-trained models for virtual try-on (VTO) with minimal computational overhead, achieving state-of-the-art results.", "motivation": "To enable efficient and high-quality VTO with limited computational resources, the authors address the challenge of adapting large models for real-world applications.", "method": "DEFT-VTON uses Doob's h-transform to fine-tune large pre-trained non-conditional models into conditional ones for VTO, combined with an adaptive consistency loss to improve performance further.", "result": "The method achieves state-of-the-art VTO performance with fewer denoising steps, showcasing its efficiency and effectiveness.", "conclusion": "DEFT-VTON provides an efficient fine-tuning approach for VTO tasks, making it feasible to deploy high-quality models with limited resources."}}
{"id": "2509.13835", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13835", "abs": "https://arxiv.org/abs/2509.13835", "authors": ["Minh Duc Bui", "Carolin Holtermann", "Valentin Hofmann", "Anne Lauscher", "Katharina von der Wense"], "title": "Large Language Models Discriminate Against Speakers of German Dialects", "comment": "Accepted to EMNLP 2025 Main", "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.", "AI": {"tldr": "研究发现了大型语言模型对德国方言使用者存在显著的方言命名和方言使用偏见，这体现在负面形容词关联方面。这种偏见也展现在模型的决策过程中。同时，当明确标出语言人口统计信息时，会比隐含的语言使用提示更能加剧偏见。", "motivation": "本研究旨在探讨个体在使用方言时所面临的负面社会刻板印象是否会在大型语言模型中反映出来。考虑到方言对于文化的巨大意义，以及方言使用者所面临的社会压力，研究其在现代技术中的表现尤为重要。", "method": "本研究基于社会语言学关于方言感知的文献，分析了与方言使用者相关的一些共同特质。根据这些特质，研究者设计了两个任务来评估大型语言模型（LLMs）的方言命名偏见和方言使用偏见：关联任务和决策任务。为了评估模型的方言使用偏见，研究者构建了一个新的评估语料库，该语料库包括七种地区性德语方言（例如，阿勒曼尼语和巴伐利亚语）的句子及其标准德语对应的句子进行配对。", "result": "研究结果表明：1) 在关联任务中，所有评估的大型语言模型（LLMs）都显示出显著的方言命名和方言使用偏见；2) 所有模型在决策过程中也会再现这些方言命名和方言使用偏见；3) 相反于以前的研究显示，在明确地标出语言人口统计信息时会比隐含的语言使用提示加剧偏见。", "conclusion": "研究表明，大型语言模型中存在针对德国方言使用者的偏见，这不仅体现在负面形容词关联上，也影响了模型决策过程中。此外，明确标出语言人口统计信息的方式会比隐含提示的语言使用加剧偏见，这对于我们理解现代技术中的语言公平性提出了挑战。"}}
{"id": "2509.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13507", "abs": "https://arxiv.org/abs/2509.13507", "authors": ["Artem Savkin", "Thomas Lapotre", "Kevin Strauss", "Uzair Akbar", "Federico Tombari"], "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving", "comment": null, "summary": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation.", "AI": {"tldr": "本文提出了一种利用数据增强技术改进自动驾驶中行人识别的方法，并提出了一种新的生成网络架构用于改善光照条件下的数据增强效果。", "motivation": "在自动驾驶领域，合成数据对于覆盖特定的交通场景至关重要，这些场景自动驾驶车辆必须能处理。然而，这些数据通常会在合成域和真实域之间引入领域差异。", "method": "通过使用数据增强技术生成包含虚拟行人（VRUs）的自定义交通场景，以提高行人识别能力。具体而言，提出了市容分割数据集Cityscapes的增强流程，并提出了一种新的生成网络架构，用于对抗性学习数据集的光照条件，以提高增强的真实感。", "result": "评估了这种方法在语义分割和实例分割任务中的表现。", "conclusion": "提出的方法旨在通过数据增强和对抗性学习改进光照条件，以弥合合成域与真实域之间的差距，并提高行人识别的性能。"}}
{"id": "2509.13869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13869", "abs": "https://arxiv.org/abs/2509.13869", "authors": ["Yang Liu", "Chenhui Chu"], "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs", "comment": "38 pages, 31 figures", "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.", "AI": {"tldr": "通过分析12个大型语言模型在不同偏见场景下的表现，该研究发现LLMs对人类价值观的对齐率与模型尺寸无明确关系，且表现出对特定类型场景的偏好。此外，研究也证明了进行过迁移学习的小模型在解释社会偏见的人类价值观时产生的解释更易于理解，但模型一致性较低。", "motivation": "尽管之前的研究所展示了LLMs与人类价值观的不对齐，但不清楚这种不对齐是否会在不同类型的偏见场景中有所不同，如负面与非负面问题。因此，这项研究旨在通过具体的实验分析来填补这一空白。", "method": "通过详细分析来自四个模型家族的12个大型语言模型(LLMs)和四个数据集，本文研究了LLMs在不同类型偏见场景下与人类价值观的对齐情况。", "result": "研究结果显示，参数规模较大的LLMs并不一定具有较低的不对齐率和攻击成功率。LLMs在处理具体类型的场景时表现出一定的偏好，来自同一模型家族的LLMs更趋于具有较高的判断一致性。不过，LLMs理解社会偏见的人类价值观(HVSB)的能力在模型间没有显著差异，且模型倾向于喜欢自己生成的解释说明。", "conclusion": "总的来说，该研究强调了即使是规模较大的LLMs也可能不能完全解决问题；对模型在偏见场景中的对齐性、解释偏好以及小模型通过迁移学习能力来解释HVSB的能力进行了深入探讨。"}}
{"id": "2509.13508", "categories": ["cs.CV", "I.4.3; I.4.6"], "pdf": "https://arxiv.org/pdf/2509.13508", "abs": "https://arxiv.org/abs/2509.13508", "authors": ["Maksim Penkin", "Andrey Krylov"], "title": "FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation", "comment": "9 pages, 5 figures, submitted to the Fortieth AAAI Conference on\n  Artificial Intelligence (AAAI-26)", "summary": "Medical image enhancement and segmentation are critical yet challenging tasks\nin modern clinical practice, constrained by artifacts and complex anatomical\nvariations. Traditional deep learning approaches often rely on complex\narchitectures with limited interpretability. While Kolmogorov-Arnold networks\noffer interpretable solutions, their reliance on flattened feature\nrepresentations fundamentally disrupts the intrinsic spatial structure of\nimaging data. To address this issue we propose a Functional Kolmogorov-Arnold\nNetwork (FunKAN) -- a novel interpretable neural framework, designed\nspecifically for image processing, that formally generalizes the\nKolmogorov-Arnold representation theorem onto functional spaces and learns\ninner functions using Fourier decomposition over the basis Hermite functions.\nWe explore FunKAN on several medical image processing tasks, including Gibbs\nringing suppression in magnetic resonance images, benchmarking on IXI dataset.\nWe also propose U-FunKAN as state-of-the-art binary medical segmentation model\nwith benchmarks on three medical datasets: BUSI (ultrasound images), GlaS\n(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting\nbreast cancer, glands and polyps, respectively. Experiments on those diverse\ndatasets demonstrate that our approach outperforms other KAN-based backbones in\nboth medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work\nbridges the gap between theoretical function approximation and medical image\nanalysis, offering a robust, interpretable solution for clinical applications.", "AI": {"tldr": "本文介绍了用于医学图像处理的新框架FunKAN和U-FunKAN，它们在多个医学图像任务中展示了优越性能，同时提供理论与医学图像分析之间的桥梁。", "motivation": "为了应对传统深度学习方法复杂且不具解释性，以及Kolmogorov-Arnold网络依赖于破坏图像空间结构的特征扁平化表示的问题。", "method": "提出了一种新的可解释神经框架——功能Kolmogorov-Arnold网络(FunKAN)，它在功能空间上形式化地推广了Kolmogorov-Arnold表示定理并使用傅里叶分解通过Hermite函数基础学习内部函数。", "result": "在医学图像增强(MRI的Gibbs环抑制)和分割(乳腺癌、腺体和息肉的检测)任务上，使用IXI、BUSI、GlaS和CVC-ClinicDB数据集证明了相比其他KAN基础框架的优越性能。", "conclusion": "FunKAN和U-FunKAN为医学图像处理提供了健壮且可解释的解决方案，适用于临床应用。"}}
{"id": "2509.13879", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13879", "abs": "https://arxiv.org/abs/2509.13879", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking", "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025", "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER.", "AI": {"tldr": "研究提出了CER框架，结合科学证据检索和大型语言模型，有效应对医疗领域虚假信息的问题。", "motivation": "鉴于虚假医疗信息对公共卫生和民众对医疗系统的信任构成威胁，现有的机器学习和自然语言处理技术虽然在自动事实核查方面有所进步，但验证生物医学声明仍然面临着复杂术语、领域专业知识需求及科学证据基础的挑战。", "method": "我们引入了CER（结合证据和推理）框架，该框架将科学证据检索、大型语言模型推理以及监督真伪预测相结合。通过将大型语言模型的文本生成能力与高质量的生物医学科学证据检索技术相结合，CER有效地避免了幻觉风险，确保生成的输出基于可验证证据来源。", "result": "在专家标注的数据集（HealthFC、BioASQ-7b、SciFact）上的评估显示，CER达到了最先进的性能，并表现出良好的跨数据集泛化能力。", "conclusion": "该研究为解决医疗领域的虚假信息问题提供了一种新方法，通过结合科学证据检索和大型语言模型，取得了显著的效果。"}}
{"id": "2509.13515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13515", "abs": "https://arxiv.org/abs/2509.13515", "authors": ["Jiangbei Yue", "Shuonan Yang", "Tailin Chen", "Jianbo Jiao", "Zeyu Fu"], "title": "Multimodal Hate Detection Using Dual-Stream Graph Neural Networks", "comment": null, "summary": "Hateful videos present serious risks to online safety and real-world\nwell-being, necessitating effective detection methods. Although multimodal\nclassification approaches integrating information from several modalities\noutperform unimodal ones, they typically neglect that even minimal hateful\ncontent defines a video's category. Specifically, they generally treat all\ncontent uniformly, instead of emphasizing the hateful components. Additionally,\nexisting multimodal methods cannot systematically capture structured\ninformation in videos, limiting the effectiveness of multimodal fusion. To\naddress these limitations, we propose a novel multimodal dual-stream graph\nneural network model. It constructs an instance graph by separating the given\nvideo into several instances to extract instance-level features. Then, a\ncomplementary weight graph assigns importance weights to these features,\nhighlighting hateful instances. Importance weights and instance features are\ncombined to generate video labels. Our model employs a graph-based framework to\nsystematically model structured relationships within and across modalities.\nExtensive experiments on public datasets show that our model is\nstate-of-the-art in hateful video classification and has strong explainability.\nCode is available:\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.", "AI": {"tldr": "本文提出了一种新的多模态双流图神经网络模型，用于改进当前多模态方法在识别视频中轻微的憎恨内容上的不足，该模型系统地建模了模态内的结构化关系，实验显示在憎恨视频分类任务上优于其他方法。", "motivation": "虽然多模态分类方法比单一模态方法更有效，但它们通常忽略了即便是极少量的令人憎恨的内容就可以定义整个视频的类别。现有的多模态方法无法系统地捕捉视频中的结构化信息，这限制了多模态融合的效果。为了克服这些局限性，我们提出了这个模型。", "method": "我们提出了一种新颖的多模态双流图神经网络模型。该模型通过将给定视频分割成多个实例来构建实例图，从而提取实例级别的特征。接着，一个互补权重图给这些特征分配重要性权重，突出令人憎恨的实例。通过结合重要性权重和实例特征来生成视频标签。我们的模型采用基于图的框架，系统地建模模态内的结构化关系。", "result": "在公共数据集上的广泛实验表明，我们的模型在憎恨视频分类方面是最先进的，并且具有很强的可解释性。", "conclusion": "本文提出的多模态双流图神经网络模型，通过突出憎恨内容并系统地捕捉视频中的结构化信息，实现了憎恨视频的高效分类，实验结果证明了模型的有效性，并且开源了代码供进一步研究。"}}
{"id": "2509.13888", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13888", "abs": "https://arxiv.org/abs/2509.13888", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification", "comment": null, "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER", "AI": {"tldr": "CER框架集成科学证据检索、推理和真伪预测，用于解决生物医学事实核查的挑战，展现了顶尖性能。", "motivation": "旨在解决因复杂术语、领域专业知识需求和科学研究证据的重要性而带来的生物医学声明验证的独特挑战。", "method": "介绍了一种名为CER（结合证据和推理）的新框架，用于生物医学事实核查。该框架集成了科学证据检索、大型语言模型推理以及监督真伪预测。", "result": "在专家标注的数据集（HealthFC, BioASQ-7b, SciFact）上的评估展示了该框架达到的顶尖性能和有前景的跨数据集泛化能力。", "conclusion": "通过结合大型语言模型的文本生成能力与先进的高质量生物医学科学证据检索技术，CER有效降低了幻觉风险，确保生成的内容基于可靠证据。"}}
{"id": "2509.13525", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13525", "abs": "https://arxiv.org/abs/2509.13525", "authors": ["Romain Hardy", "Tyler Berzin", "Pranav Rajpurkar"], "title": "ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors", "comment": "12 pages, 8 figures", "summary": "Three-dimensional (3D) scene understanding in colonoscopy presents\nsignificant challenges that necessitate automated methods for accurate depth\nestimation. However, existing depth estimation models for endoscopy struggle\nwith temporal consistency across video sequences, limiting their applicability\nfor 3D reconstruction. We present ColonCrafter, a diffusion-based depth\nestimation model that generates temporally consistent depth maps from monocular\ncolonoscopy videos. Our approach learns robust geometric priors from synthetic\ncolonoscopy sequences to generate temporally consistent depth maps. We also\nintroduce a style transfer technique that preserves geometric structure while\nadapting real clinical videos to match our synthetic training domain.\nColonCrafter achieves state-of-the-art zero-shot performance on the C3VD\ndataset, outperforming both general-purpose and endoscopy-specific approaches.\nAlthough full trajectory 3D reconstruction remains a challenge, we demonstrate\nclinically relevant applications of ColonCrafter, including 3D point cloud\ngeneration and surface coverage assessment.", "AI": {"tldr": "研究提出了一种名为ColonCrafter的深度估计模型，采用扩散方法生成时间一致的深度图，从单目结肠镜视频中提取3D信息，并在临床相关应用中展示了优越性能。", "motivation": "三维结肠镜场景理解需要精确的深度估计自动化方法，但现有的深度估计模型在视频序列之间的时间一致性方面存在问题，限制了它们在3D重建中的应用。", "method": "提出了一种基于扩散模型的深度估计方法ColonCrafter，可以从单目结肠镜视频中生成具有时间一致性的深度图。该方法通过学习从合成结肠镜序列中的鲁棒几何先验来生成时间一致的深度图，并引入一种风格转换技术，保留几何结构的同时将真实临床视频适应到合成训练域中。", "result": "在C3VD数据集上实现了零样本的最佳性能，超过了通用方法和特定于内窥镜的方法。尽管完全轨迹的3D重建仍是一个挑战，但展示了ColonCrafter的临床相关应用，包括3D点云生成和表面覆盖评估。", "conclusion": "尽管完全轨迹的3D重建仍是一个挑战，但我们展示了ColonCrafter的临床相关应用，包括3D点云生成和表面覆盖评估。"}}
{"id": "2509.13905", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13905", "abs": "https://arxiv.org/abs/2509.13905", "authors": ["Domenico Meconi", "Simone Stirpe", "Federico Martelli", "Leonardo Lavalle", "Roberto Navigli"], "title": "Do Large Language Models Understand Word Senses?", "comment": "20 pages, to be published in EMNLP2025", "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.", "AI": {"tldr": "The study evaluates and finds that LLMs can effectively perform word sense disambiguation and generate accurate explanations, showcasing robust understanding of word senses.", "motivation": "To address the underexplored area of how well LLMs truly understand word senses, the research seeks to examine their WSD capabilities and their performance in generating definitions, explanations, and examples.", "method": "The paper evaluates the Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs, comparing their performance with state-of-the-art systems. It also assesses LLMs' ability to understand word senses in three generative settings: definition generation, free-form explanation, and example generation.", "result": "LLMs like GPT-4 and DeepSeek-V3 match the performance of specialized WSD systems and show greater robustness. In generative tasks, LLMs can accurately explain word meanings in context, with the highest accuracy observed in free-form explanations.", "conclusion": "The research concludes that LLMs can effectively understand word senses, competing with specialized WSD systems and excelling in the generative ability of providing free-form explanations."}}
{"id": "2509.13536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13536", "abs": "https://arxiv.org/abs/2509.13536", "authors": ["Yinlong Bai", "Hongxin Zhang", "Sheng Zhong", "Junkai Niu", "Hai Li", "Yijia He", "Yi Zhou"], "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM", "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant\nimpact on rendering and reconstruction techniques. Current research\npredominantly focuses on improving rendering performance and reconstruction\nquality using high-performance desktop GPUs, largely overlooking applications\nfor embedded platforms like micro air vehicles (MAVs). These devices, with\ntheir limited computational resources and memory, often face a trade-off\nbetween system performance and reconstruction quality. In this paper, we\nimprove existing methods in terms of GPU memory usage while enhancing rendering\nquality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we\npropose merging them in voxel space based on geometric similarity. This reduces\nGPU memory usage without impacting system runtime performance. Furthermore,\nrendering quality is improved by initializing 3D Gaussian primitives via\nPatch-Grid (PG) point sampling, enabling more accurate modeling of the entire\nscene. Quantitative and qualitative evaluations on publicly available datasets\ndemonstrate the effectiveness of our improvements.", "AI": {"tldr": "The paper proposes an improved 3D Gaussian Splatting technique that enhances rendering quality and reduces GPU memory usage without sacrificing performance, specifically for embedded platforms like MAVs.", "motivation": "The motivation behind this research is to address the limitations of existing 3D Gaussian Splatting techniques when applied to resource-constrained embedded platforms, such as micro air vehicles (MAVs).", "method": "The method merges redundant 3D Gaussian primitives based on their geometric similarity in voxel space to reduce GPU memory usage, and introduces Patch-Grid point sampling to initialize 3D Gaussian primitives for better rendering quality.", "result": "Quantitative and qualitative evaluations on public datasets show that the proposed method successfully reduces GPU memory usage while improving the quality of reconstruction and rendering.", "conclusion": "The conclusion is that the improved 3D Gaussian Splatting technique effectively addresses the challenges faced by embedded platforms like MAVs, balancing between memory usage, performance, and reconstruction quality."}}
{"id": "2509.13930", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13930", "abs": "https://arxiv.org/abs/2509.13930", "authors": ["Dayeon Ki", "Marine Carpuat", "Paul McNamee", "Daniel Khashabi", "Eugene Yang", "Dawn Lawrie", "Kevin Duh"], "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "comment": "33 pages, 20 figures", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior.", "AI": {"tldr": "研究发现，mRAG系统在处理英语查询时倾向于引用英语文献，此现象在资源较少的语言和文档位于中间上下文时更显著。", "motivation": "研究mRAG系统中不同文档语言混合对生成和引用的意外影响。", "method": "通过引入控制方法论，使用模型内部测量语言偏好，保持文档相关性等因素恒定。", "result": "该研究探讨了多语言检索增强生成系统（mRAG）在处理知识密集型查询时，不同文档语言的混合是否会对生成和引用产生意料之外的影响。研究通过控制模型内部来测量语言偏好，结果发现当查询语言为英语时，模型倾向于引用英语源，并且这种偏向在资源较少的语言和位于中间上下文的文档中更为明显。模型有时会因为语言偏好而牺牲文档的相关性，表明引文选择并非完全由信息量决定。该研究揭示了语言模型如何利用多语言上下文和影响引文行为。", "conclusion": "mRAG系统在多语言检索生成过程中，存在由于语言偏好导致的不均匀引文问题，提示需要更多的语言表示公平性研究。"}}
{"id": "2509.13577", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13577", "abs": "https://arxiv.org/abs/2509.13577", "authors": ["Tongfei Guo", "Lili Su"], "title": "Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles", "comment": "8 pages, 7 figures", "summary": "Trajectory prediction is central to the safe and seamless operation of\nautonomous vehicles (AVs). In deployment, however, prediction models inevitably\nface distribution shifts between training data and real-world conditions, where\nrare or underrepresented traffic scenarios induce out-of-distribution (OOD)\ncases. While most prior OOD detection research in AVs has concentrated on\ncomputer vision tasks such as object detection and segmentation,\ntrajectory-level OOD detection remains largely underexplored. A recent study\nformulated this problem as a quickest change detection (QCD) task, providing\nformal guarantees on the trade-off between detection delay and false alarms\n[1]. Building on this foundation, we propose a new framework that introduces\nadaptive mechanisms to achieve robust detection in complex driving\nenvironments. Empirical analysis across multiple real-world datasets reveals\nthat prediction errors -- even on in-distribution samples -- exhibit\nmode-dependent distributions that evolve over time with dataset-specific\ndynamics. By explicitly modeling these error modes, our method achieves\nsubstantial improvements in both detection delay and false alarm rates.\nComprehensive experiments on established trajectory prediction benchmarks show\nthat our framework significantly outperforms prior UQ- and vision-based OOD\napproaches in both accuracy and computational efficiency, offering a practical\npath toward reliable, driving-aware autonomy.", "AI": {"tldr": "研究提出了一种新的框架，用于改进自动驾驶车辆轨迹预测中的异常检测性能，通过自适应机制和显式建模误差模式，该方法在检测延迟和误报率上表现出色。", "motivation": "在自动驾驶汽车的实际部署中，预测模型面临训练数据和实际驾驶条件之间的分布差异。大多数关于异常检测的研究集中在计算机视觉任务上，而轨迹级别的异常检测研究较少。", "method": "采用了一种新的框架，该框架通过引入自适应机制来实现复杂驾驶环境下的鲁棒检测。该方法显式地建模了随时间演变的误差模式，这些模式因数据集而异。", "result": "实验证明，相比之前的不确定性量化和基于视觉的异常检测方法，该框架在精度和计算效率上都有显著提升。", "conclusion": "该研究提出了一种新的异常检测方法，通过显式建模误差模式，大幅减少了检测延迟，并降低了误报率，为实现可靠的驾驶感知自主性提供了实践路径。"}}
{"id": "2509.13980", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13980", "abs": "https://arxiv.org/abs/2509.13980", "authors": ["Sami Ul Haq", "Chinonso Cynthia Osuji", "Sheila Castilho", "Brian Davis"], "title": "Long-context Reference-based MT Quality Estimation", "comment": null, "summary": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments.", "AI": {"tldr": "作者提出了一种基于COMET框架的方法，使用长上下文信息来改善翻译质量评估模型，使其更好地预测段落级别的错误。该方法通过整合多个数据集并引入长上下文数据提升了与人类判断的相关性。", "motivation": "为了参加第十届机器翻译会议（WMT25）自动化翻译质量评估的共享任务，作者提出了他们的提交内容。", "method": "我们的系统基于COMET框架，使用增强的长上下文数据来训练预测段落级别错误跨度注释（ESA）分数。为了构建长上下文训练数据，我们将领域内的、人工注释的句子进行连接，并计算它们分数的加权平均。我们将多个人工判断数据集（MQM、SQM和DA）通过标准化其尺度整合起来，并训练多语言回归模型来从源文本、假设翻译和参考翻译中预测质量分数。", "result": "实验结果表明，与仅在短片段上训练的模型相比，引入长上下文信息可以提高与人类判断的相关性。", "conclusion": "结论未在摘要中明确提及，但可以推断出使用长上下文信息对于提高自动化翻译质量评估的有效性是有益的。"}}
{"id": "2509.13586", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM", "I.2; I.4; I.7; H.3"], "pdf": "https://arxiv.org/pdf/2509.13586", "abs": "https://arxiv.org/abs/2509.13586", "authors": ["Nathalie Neptune", "Josiane Mothe"], "title": "Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection", "comment": null, "summary": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.13990", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13990", "abs": "https://arxiv.org/abs/2509.13990", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.", "AI": {"tldr": "Slim-SC 是一种逐步剪枝策略，通过链之间的思想相似性识别并移除冗余链，从而减少SC方法中的计算冗余，实现在不降低或提高准确性的前提下，推理延迟和KVC使用量分别减少45%和26%。", "motivation": "研究旨在理论和实证上分析SC方法的低效率，并提出了Slim-SC作为减少计算成本同时保持或提高其性能的有效策略。", "method": "分析SC方法中的低效问题，并提出Slim-SC，利用链之间的相似性来进行逐步剪枝，减少冗余链。", "result": "在三个STEM推理数据集和两种新的LLM架构上的实验表明，与SC相比，Slim-SC可减少45%的推理延迟和26%的KVC使用。", "conclusion": "Slim-SC作为SC方法的一种简化且高效的测试时间缩放（TTS）替代方案，显示出减少计算成本的同时保持或提高模型准确性。"}}
{"id": "2509.13590", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13590", "abs": "https://arxiv.org/abs/2509.13590", "authors": ["Samer Al-Hamadani"], "title": "Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation", "comment": "32 pages, 14 figures, 6 tables", "summary": "The rapid advancement of artificial intelligence (AI) in healthcare imaging\nhas revolutionized diagnostic medicine and clinical decision-making processes.\nThis work presents an intelligent multimodal framework for medical image\nanalysis that leverages Vision-Language Models (VLMs) in healthcare\ndiagnostics. The framework integrates Google Gemini 2.5 Flash for automated\ntumor detection and clinical report generation across multiple imaging\nmodalities including CT, MRI, X-ray, and Ultrasound. The system combines visual\nfeature extraction with natural language processing to enable contextual image\ninterpretation, incorporating coordinate verification mechanisms and\nprobabilistic Gaussian modeling for anomaly distribution. Multi-layered\nvisualization techniques generate detailed medical illustrations, overlay\ncomparisons, and statistical representations to enhance clinical confidence,\nwith location measurement achieving 80 pixels average deviation. Result\nprocessing utilizes precise prompt engineering and textual analysis to extract\nstructured clinical information while maintaining interpretability.\nExperimental evaluations demonstrated high performance in anomaly detection\nacross multiple modalities. The system features a user-friendly Gradio\ninterface for clinical workflow integration and demonstrates zero-shot learning\ncapabilities to reduce dependence on large datasets. This framework represents\na significant advancement in automated diagnostic support and radiological\nworkflow efficiency, though clinical validation and multi-center evaluation are\nnecessary prior to widespread adoption.", "AI": {"tldr": "本文介绍了一个基于Vision-Language Models的智能多模态医学影像分析框架，通过视觉和语言模型结合，实现了自动肿瘤检测和临床报告生成，并进行了实验验证，展示了高级检测性能。", "motivation": "随着人工智能在医疗影像中的快速发展，它正在改变诊断医学和临床决策过程。本文的主要动机是开发一个能提高自动检测支持和放射学工作流程效率的框架。", "method": "该框架结合了视觉特征提取与自然语言处理，利用Vision-Language Models (VLMs) 提取多重医学影像模态中的信息，包括CT、MRI、X光和超声波影像，通过Google Gemini 2.5 Flash实现了自动肿瘤检测和临床报告生成，且加入了坐标验证机制和概率高斯模型来分析异常分布。", "result": "实验评估显示，该系统在多重影像模态中具有高异常检测性能。系统采用多层可视化技术生成详细的医学插图、叠加对比和统计表示，以提高临床信心，平均位置测量偏差为80像素。", "conclusion": "虽然该框架代表了自动化诊疗支持和放射工作流程效率的重要进展，但其临床应用需要进一步临床验证和多中心评估。"}}
{"id": "2509.14004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14004", "abs": "https://arxiv.org/abs/2509.14004", "authors": ["Minjia Mao", "Bowen Yin", "Yu Zhu", "Xiao Fang"], "title": "Early Stopping Chain-of-thoughts in Large Language Models", "comment": null, "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.", "AI": {"tldr": "本文提出了ES-CoT方法，通过检测答案的收敛性来提前终止大语言模型的长思维链生成过程，从而减少推理成本，同时保持较高的推理准确性。", "motivation": "长思维链虽然可以增强大语言模型解决复杂问题的能力，但同时也会导致较高的推理成本。为了在减少计算成本的同时保持推理的准确性，本文提出了ES-CoT方法。", "method": "在推理过程中，该方法通过检测答案的收敛性来缩短思维链（CoT）生成的过程，并提前终止推理以减少计算成本。具体而言，在每一步推理结束时，会提示大语言模型输出当前的最终答案，并将这些答案称为步骤答案。通过跟踪连续相同步骤答案的长度来衡量答案的收敛性，当连续相同答案的长度突然增加并超过预设阈值时，终止生成。", "result": "实验结果表明，ES-CoT可以在保持与标准CoT相当的准确性的前提下，将推理过程中的token数量平均减少约41%。此外，该方法与自一致性提示技术兼容，并在不同的超参数选择下都能保持稳定效果。", "conclusion": "ES-CoT是一种有效且实用的方法，可以在减少推理成本的同时保持大语言模型的推理准确性，是一种提高推理效率的有效手段。"}}
{"id": "2509.13605", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13605", "abs": "https://arxiv.org/abs/2509.13605", "authors": ["Ruochen Hou", "Gabriel I. Fernandez", "Alex Xu", "Dennis W. Hong"], "title": "A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms", "comment": null, "summary": "In previous work, we introduced a 2D localization algorithm called CLAP,\nClustering to Localize Across $n$ Possibilities, which was used during our\nchampionship win in RoboCup 2024, an international autonomous humanoid soccer\ncompetition. CLAP is particularly recognized for its robustness against\noutliers, where clustering is employed to suppress noise and mitigate against\nerroneous feature matches. This clustering-based strategy provides an\nalternative to traditional outlier rejection schemes such as RANSAC, in which\ncandidates are validated by reprojection error across all data points. In this\npaper, CLAP is extended to a more general framework beyond 2D localization,\nspecifically to 3D localization and image stitching. We also show how CLAP,\nRANSAC, and Hough transforms are related. The generalization of CLAP is widely\napplicable to many different fields and can be a useful tool to deal with noise\nand uncertainty.", "AI": {"tldr": "本文将2D定位算法CLAP扩展到了3D定位和图像拼接，强调了CLAP处理噪声和不确定性的能力，并指出它在多个领域中的广泛适用性。", "motivation": "CLAP算法在RoboCup 2024中表现出色，尤其是在处理离群点方面，作者希望能够将该算法的适用性扩展到更广泛的领域。", "method": "本文将CLAP算法从2D定位扩展到更通用的框架，包括3D定位和图像拼接，并探讨了CLAP、RANSAC和霍夫变换之间的关系。", "result": "CLAP的扩展版本被证明在处理噪声和不确定性方面非常有效，适用于多个不同领域。", "conclusion": "CLAP的扩展性为处理噪声和不确定性提供了广泛的工具，这表明了其在多个应用领域的潜力。"}}
{"id": "2509.14008", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14008", "abs": "https://arxiv.org/abs/2509.14008", "authors": ["Hasan Abed Al Kader Hammoud", "Mohammad Zbeeb", "Bernard Ghanem"], "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale", "comment": "Technical Report", "summary": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.", "AI": {"tldr": "文章介绍了Hala系列模型，通过先进技术和高效的数据处理方式，创建了在阿拉伯语指令遵循任务方面表现出色的模型。", "motivation": "开发Hala是为了创建针对阿拉伯语应用场景的高性能模型，解决现有模型在阿拉伯语处理中不足的问题。", "method": "研究主要采用压缩技术来提高计算效率，同时保持质量不变，使用翻译和调整的流水线为轻量级语言模型提供高质量的双语监督数据，从而生成针对阿拉伯语的高效指令模型。", "result": "{\\textquotedblleft}Hala{\\textquotedblright}是专为阿拉伯语设计的指令和翻译模型系列，通过使用压缩的强大AR$\\leftrightarrow$EN教师来创建高质量的双语监督，随后对轻量级语言模型进行微调，并将高质量的英语指令集翻译成阿拉伯语，生成针对指令遵循的百万级语料库。Hala模型在350M、700M、1.2B和9B参数下进行训练。在针对阿拉伯语的基准测试中，Hala在{\\textquotedblleft}纳米{\\textquotedblright}($\\leq$2B)和{\\textquotedblleft}小{\\textquotedblright}(7-9B)类别中取得了最先进的结果，优于基础模型。", "conclusion": "研究提供了专用于阿拉伯语自然语言处理(NLP)的研究模型、数据、评估方法和配方，以促进该领域的研究。"}}
{"id": "2509.13629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13629", "abs": "https://arxiv.org/abs/2509.13629", "authors": ["Yue He", "Min Liu", "Qinghao Liu", "Jiazheng Wang", "Yaonan Wang", "Hang Zhang", "Xiang Chen"], "title": "SAMIR, an efficient registration framework via robust feature learning from SAM", "comment": null, "summary": "Image registration is a fundamental task in medical image analysis.\nDeformations are often closely related to the morphological characteristics of\ntissues, making accurate feature extraction crucial. Recent weakly supervised\nmethods improve registration by incorporating anatomical priors such as\nsegmentation masks or landmarks, either as inputs or in the loss function.\nHowever, such weak labels are often not readily available, limiting their\npractical use. Motivated by the strong representation learning ability of\nvisual foundation models, this paper introduces SAMIR, an efficient medical\nimage registration framework that utilizes the Segment Anything Model (SAM) to\nenhance feature extraction. SAM is pretrained on large-scale natural image\ndatasets and can learn robust, general-purpose visual representations. Rather\nthan using raw input images, we design a task-specific adaptation pipeline\nusing SAM's image encoder to extract structure-aware feature embeddings,\nenabling more accurate modeling of anatomical consistency and deformation\npatterns. We further design a lightweight 3D head to refine features within the\nembedding space, adapting to local deformations in medical images.\nAdditionally, we introduce a Hierarchical Feature Consistency Loss to guide\ncoarse-to-fine feature matching and improve anatomical alignment. Extensive\nexperiments demonstrate that SAMIR significantly outperforms state-of-the-art\nmethods on benchmark datasets for both intra-subject cardiac image registration\nand inter-subject abdomen CT image registration, achieving performance\nimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code\nwill be publicly available on GitHub following the acceptance of this paper.", "AI": {"tldr": "本文提出了SAMIR，一个用于医学图像配准的高效框架，它利用预训练的Segment Anything Model（SAM）来增强特征提取，改善了解剖对齐并显著优于现有的方法。", "motivation": "传统的弱监督方法需要依赖于分割掩码或地标等弱标签作为输入或在损失函数中的应用，这些弱标签往往难以获得，限制了它们的实际应用。基于视觉基础模型强大的表示学习能力，本文提出了SAMIR框架，以解决这一问题。", "method": "引入SAMIR，这是一个高效的医学图像配准框架，利用Segment Anything Model（SAM）来增强特征提取。SAM在大规模自然图像数据集上预训练，可以学习健壯且通用的视觉表征。本文设计了一个任务特定的适应管道，使用SAM的图像编码器来提取结构感知特征嵌入，使解剖一致性和变形模式的建模更为准确。此外，还设计了一种轻量级的3D头在嵌入空间内细化特征，以适应医学图像中的局部变形，并引入了分层特征一致性损失来指导粗到细的特征匹配，以改善解剖对齐。", "result": "实验表明，SAMIR在心脏图像和腹部CT图像的配准上显著优于现有方法，分别在ACDC数据集上提高了2.68%，在腹部数据集上提高了6.44%。", "conclusion": "SAMIR通过利用SAM进行特征提取并设计适应管道和损失函数，在医学图像配准任务中表现出色，优于现有方法，并且该代码将在GitHub上公开。"}}
{"id": "2509.14023", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14023", "abs": "https://arxiv.org/abs/2509.14023", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks.", "AI": {"tldr": "本研究通过对比音频和文本两种方式评估10个机器翻译系统的质量，发现基于音频的评估结果与文本评估基本一致但在某些情况下能揭示出不同系统之间的显著差异，提议未来将基于语音的评估纳入机器翻译评价体系中。", "motivation": "现有机器翻译质量评估主要基于文本，而实际应用中往往涉及语音输出。为了探索更自然的评估方式，作者提出了基于音频的评估方法。", "method": "使用10个机器翻译系统，通过Amazon Mechanical Turk收集群众的音频和文本评分，并进行统计显著性测试及自我复制实验以测试音频评估方法的可靠性和一致性。", "result": "基于音频的群众评估结果大多数情况下与基于文本的评估一致，但在某些情况下能够显示出系统之间的显著差异；这可能是因为音频提供了更加自然和丰富的模态。", "conclusion": "提出应该在未来机器翻译质量评估框架中加入基于语音的评估方法。"}}
{"id": "2509.13631", "categories": ["cs.CV", "cs.DC", "14J60", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13631", "abs": "https://arxiv.org/abs/2509.13631", "authors": ["Yuvraj Dutta", "Aaditya Sikder", "Basabdatta Palit"], "title": "Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery", "comment": "6 pages, 7 figures, accepted at IEEE INDISCON 2025", "summary": "Accurate identification of deforestation from satellite images is essential\nin order to understand the geographical situation of an area. This paper\nintroduces a new distributed approach to identify as well as locate\ndeforestation across different clients using Federated Learning (FL). Federated\nLearning enables distributed network clients to collaboratively train a model\nwhile maintaining data privacy and security of the active users. In our\nframework, a client corresponds to an edge satellite center responsible for\nlocal data processing. Moreover, FL provides an advantage over centralized\ntraining method which requires combining data, thereby compromising with data\nsecurity of the clients. Our framework leverages the FLOWER framework with RAY\nframework to execute the distributed learning workload. Furthermore, efficient\nclient spawning is ensured by RAY as it can select definite amount of users to\ncreate an emulation environment. Our FL framework uses YOLOS-small (a Vision\nTransformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN\nwith a MobileNetV3 backbone models trained and tested on publicly available\ndatasets. Our approach provides us a different view for image\nsegmentation-based tasks on satellite imagery.", "AI": {"tldr": "本文提出了一种基于联邦学习的分布式系统，允许用户在不共享数据的情况下共同训练森林砍伐识别模型，使用了多个现有的深度学习模型。", "motivation": "准确识别森林砍伐对于理解地区地理状况至关重要。联邦学习使分布式网络客户端能够在维护数据安全和隐私的同时共同训练模型，从而提供比集中式训练方法更有优势的数据安全策略。", "method": "本论文提出了一种基于联邦学习的分布式方法来识别和定位不同客户端的卫星图像中的森林砍伐。方法利用FLOWER框架和RAY框架进行联邦学习的分布式工作负载执行，实现了高效的客户端生成。所采用的模型包括YOLOS-small、具有ResNet50骨干网的Faster R-CNN以及具有MobileNetV3骨干网的Faster R-CNN，这些模型是在公开数据集上进行训练和测试的。", "result": "未详细列出具体结果，但表明框架允许有效的图像分割任务在卫星图像上的应用，证明了所提出方法的潜力。", "conclusion": "该论文提供了一种在卫星图像上基于图像分割任务的新的视角。通过联邦学习，能够在保持数据安全与隐私的同时，实现大规模的卫星图像森林砍伐识别与定位任务。"}}
{"id": "2509.14031", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14031", "abs": "https://arxiv.org/abs/2509.14031", "authors": ["Paweł Mąka", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models", "comment": "EMNLP 2025 main conference", "summary": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively.", "AI": {"tldr": "研究发现训练数据中缺乏上下文丰富示例是难以利用上下文的主要瓶颈，并提出两种有效策略来改善这一点。", "motivation": "鉴于标准训练数据中上下文丰富示例的稀缺被认为是模型难以有效利用上下文的主要原因，本研究旨在验证这一假设并提出改进策略。", "method": "通过构造包含不同比例上下文相关示例的训练数据集，在单语言和多语言环境中系统验证缺乏上下文丰富示例对模型性能的影响。并提出了两种训练策略来提高可用数据的利用。", "result": "研究结果显示，在单语言和多语言设置下，提出的数据利用策略分别带来了6和8个百分点的准确率改善。同时确认了训练数据上下文稀疏性对模型表现的关键限制作用。发现上下文现象的改进并不互相推广，且跨语言转移效应没有显著提升。", "conclusion": "验证了训练数据稀缺性与模型性能之间的强关联性，并发现对一种特定上下文现象所做的改进不会推广到其他现象。虽然观察到了跨语言的知识迁移，但同一家族语言间提升并不显著。实验中提出的数据利用策略带来了显著的准确性改善。"}}
{"id": "2509.13652", "categories": ["cs.CV", "I.4.8; I.4.5"], "pdf": "https://arxiv.org/pdf/2509.13652", "abs": "https://arxiv.org/abs/2509.13652", "authors": ["Yumin Li", "Dylan Campbell"], "title": "Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction", "comment": "12 pages, 4 figures, accepted by AJCAI 2025", "summary": "Estimating metric relative camera pose from a pair of images is of great\nimportance for 3D reconstruction and localisation. However, conventional\ntwo-view pose estimation methods are not metric, with camera translation known\nonly up to a scale, and struggle with wide baselines and textureless or\nreflective surfaces. This paper introduces GARPS, a training-free framework\nthat casts this problem as the direct alignment of two independently\nreconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and\na Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model\n(GMM) for each image. It then refines an initial pose from a feed-forward\ntwo-view pose estimator by optimising a differentiable GMM alignment objective.\nThis objective jointly considers geometric structure, view-independent colour,\nanisotropic covariance, and semantic feature consistency, and is robust to\nocclusions and texture-poor regions without requiring explicit 2D\ncorrespondences. Extensive experiments on the Real\\-Estate10K dataset\ndemonstrate that GARPS outperforms both classical and state-of-the-art\nlearning-based methods, including MASt3R. These results highlight the potential\nof bridging single-view perception with multi-view geometry to achieve robust\nand metric relative pose estimation.", "AI": {"tldr": "GARPS is a novel, training-free framework for metric relative camera pose estimation that integrates single-view perception with multi-view geometry, outperforming existing methods on the Real-Estate10K dataset.", "motivation": "The motivation is to address the limitations of conventional two-view pose estimation methods, such as lack of metric translation information and challenges with wide baselines, textureless, or reflective surfaces. The aim is to improve the robustness and accuracy of metric relative camera pose estimation.", "method": "The paper introduces GARPS, a training-free framework that casts two-view pose estimation as the direct alignment of two independently reconstructed 3D scenes. GARPS uses a metric monocular depth estimator and a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model (GMM) for each image. An initial pose from a feed-forward two-view pose estimator is refined by optimizing a differentiable GMM alignment objective that considers geometric structure, view-independent color, anisotropic covariance, and semantic feature consistency.", "result": "Extensive experiments on the Real-Estate10K dataset demonstrated that GARPS outperformed both classical and state-of-the-art learning-based methods, including MASt3R.", "conclusion": "The conclusion is that GARPS, by leveraging single-view perception with multi-view geometry, can achieve robust and metric relative pose estimation, highlighting the potential of bridging these two domains."}}
{"id": "2509.14034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14034", "abs": "https://arxiv.org/abs/2509.14034", "authors": ["Zijie Lin", "Bryan Hooi"], "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression", "comment": "EMNLP'25 Findings", "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.", "AI": {"tldr": "提出在多智能体辩论系统（MAD）中引入自信度表达的方法，并开发了一个名为ConfMAD的框架，展示了该方法的有效性及对未来MAD系统设计的洞察。", "motivation": "多智能体辩论（MAD）系统尽管利用多个大语言模型（LLMs）模拟人类辩论，以提高任务性能，但各LLMs在某些任务上可能具有更高的知识或推理能力却不能有效传达这一优势。缺乏自信度表达导致LLMs可能会顽固地坚持错误的观点或过早地达成次优结论，从而降低辩论效率和系统的整体性能。", "method": "提出在多智能体辩论系统（MAD）中引入自信度表达的方法，以使大语言模型（LLMs）能够在辩论过程中明确地传达其自信度水平。为此，开发了一个名为ConfMAD的MAD框架，该框架在整个辩论过程中整合了自信度表达。", "result": "实验结果展示了该方法的有效性，并进一步分析了自信度如何影响辩论的动力学，提供了设计自信度感知的MAD系统的见解。", "conclusion": "通过引入自信度表达，可以改善大语言模型（LLMs）在多智能体辩论系统（MAD）中的表现，提高任务执行的效率和准确性。"}}
{"id": "2509.13662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13662", "abs": "https://arxiv.org/abs/2509.13662", "authors": ["Yulan Guo", "Longguang Wang", "Wendong Mao", "Xiaoyu Dong", "Yingqian Wang", "Li Liu", "Wei An"], "title": "Deep Lookup Network", "comment": null, "summary": "Convolutional neural networks are constructed with massive operations with\ndifferent types and are highly computationally intensive. Among these\noperations, multiplication operation is higher in computational complexity and\nusually requires {more} energy consumption with longer inference time than\nother operations, which hinders the deployment of convolutional neural networks\non mobile devices. In many resource-limited edge devices, complicated\noperations can be calculated via lookup tables to reduce computational cost.\nMotivated by this, in this paper, we introduce a generic and efficient lookup\noperation which can be used as a basic operation for the construction of neural\nnetworks. Instead of calculating the multiplication of weights and activation\nvalues, simple yet efficient lookup operations are adopted to compute their\nresponses. To enable end-to-end optimization of the lookup operation, we\nconstruct the lookup tables in a differentiable manner and propose several\ntraining strategies to promote their convergence. By replacing computationally\nexpensive multiplication operations with our lookup operations, we develop\nlookup networks for the image classification, image super-resolution, and point\ncloud classification tasks. It is demonstrated that our lookup networks can\nbenefit from the lookup operations to achieve higher efficiency in terms of\nenergy consumption and inference speed while maintaining competitive\nperformance to vanilla convolutional networks. Extensive experiments show that\nour lookup networks produce state-of-the-art performance on different tasks\n(both classification and regression tasks) and different data types (both\nimages and point clouds).", "AI": {"tldr": "本文提出了一种基于查找操作的卷积神经网络优化方法，显著提升了能效和推理性能。", "motivation": "由于卷积神经网络中的乘法操作在计算复杂度、能耗和推理时间方面较高，禁止其在移动设备上的部署。因此本文受查找表方法启发，提出了一种通用高效的查找操作。", "method": "代替传统的乘法操作，本文提出了用于神经网络构建的高效查找表操作。通过采用简洁但有效的查找操作来执行权重和激活值的计算，减少了计算成本。同时，为了实现查找表操作的端到端优化，本文采用微分方式构建查找表，并提出多种训练策略以促进收敛。", "result": "实验显示，查找网络在多种任务（分类和回归）和数据类型（图像和点云）上达到了最先进的性能。", "conclusion": "通过将计算昂贵的乘法操作替换为查找操作，构建了具有更高能效和推理速度的查找网络，同时保持与传统卷积网络相当的性能。实验验证展示了本文工作的优越性能，适用于不同的任务和数据类型。"}}
{"id": "2509.14036", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14036", "abs": "https://arxiv.org/abs/2509.14036", "authors": ["Zekang Liu", "Wei Feng", "Fanhua Shang", "Lianyu Hu", "Jichao Feng", "Liqing Gao"], "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation", "comment": null, "summary": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality.", "AI": {"tldr": "本文提出了基于提问的手语翻译(QB-SLT)的概念及相应的跨模态自我监督学习方法SSL-SSAW，利用对话和提问来提高翻译质量，并在新构建的数据集上展示了SOTA的性能。", "motivation": "本文旨在探索对话如何有效集成到手语翻译中，通过对话为翻译提供重要的上下文线索。相比手势注释（手语转录），对话在交流中自然出现，更易于标注，因此引入了基于提问的手语翻译(QB-SLT)。", "method": "本文提出了一种名为SSL-SSAW（自监督学习与Sigmoid自注意力加权）的跨模态融合方法，以解决多模态特征对齐的挑战。该方法通过对比学习对多模态特征进行对齐，并引入了自适应特征提取模块SSAW（Sigmoid自注意力加权）。此外，利用自监督学习充分利用问题文本，增强表达力和翻译能力。", "result": "在新构建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上评估显示，SSL-SSAW方法取得了领先的性能，轻松可得的问题辅助可以实现或甚至超越手势辅助的性能。", "conclusion": "结果证明，对话的加入极大地提高了翻译质量，基于提问的手语翻译(QB-SLT)方法有潜力改善聋哑人与听者之间的沟通。"}}
{"id": "2509.13676", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13676", "abs": "https://arxiv.org/abs/2509.13676", "authors": ["Xiaobo Yang", "Xiaojin Gong"], "title": "Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation", "comment": null, "summary": "Recently, Referring Image Segmentation (RIS) frameworks that pair the\nMultimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)\nhave achieved impressive results. However, adapting MLLM to segmentation is\ncomputationally intensive, primarily due to visual token redundancy. We observe\nthat traditional patch-wise visual projectors struggle to strike a balance\nbetween reducing the number of visual tokens and preserving semantic clarity,\noften retaining overly long token sequences to avoid performance drops.\nInspired by text tokenizers, we propose a novel semantic visual projector that\nleverages semantic superpixels generated by SAM to identify \"visual words\" in\nan image. By compressing and projecting semantic superpixels as visual tokens,\nour approach adaptively shortens the token sequence according to scene\ncomplexity while minimizing semantic loss in compression. To mitigate loss of\ninformation, we propose a semantic superpixel positional embedding to\nstrengthen MLLM's awareness of superpixel geometry and position, alongside a\nsemantic superpixel aggregator to preserve both fine-grained details inside\nsuperpixels and global context outside. Experiments show that our method cuts\nvisual tokens by 93% without compromising performance, notably speeding up MLLM\ntraining and inference, and outperforming existing compressive visual\nprojectors on RIS.", "AI": {"tldr": "A novel technique for Referring Image Segmentation using a semantic projector that compresses visual tokens while integrating positional embeddings to enhance model understanding of the image's geometric and positional features, improving performance and efficiency.", "motivation": "The motivation is to overcome the computational burden imposed by the visual token redundancy when combining Multimodal Large Language Models (MLLM) with the Segment Anything Model (SAM) for Referring Image Segmentation (RIS).", "method": "Our method proposes a semantic visual projector that uses semantic superpixels from SAM to compress visual tokens, reducing their number adaptively according to scene complexity. Additionally, a semantic superpixel positional embedding and aggregator are introduced to maintain the semantic information and positional awareness.", "result": "The proposed method reduces the number of visual tokens by 93%, without a performance drop, thereby accelerating MLLM training and inference.", "conclusion": "This approach improvement in accelerating MLLM performance in RIS without compromising the quality of the segmentation."}}
{"id": "2509.14128", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14128", "abs": "https://arxiv.org/abs/2509.14128", "authors": ["Monica Sekoyan", "Nithin Rao Koluguri", "Nune Tadevosyan", "Piotr Zelasko", "Travis Bartley", "Nick Karpov", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST", "comment": "Mini Version of it Submitted to ICASSP 2026", "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.", "AI": {"tldr": "Canary-1B-v2是快速、健壮的多语言ASR和AST模型，表现出色且速度快于其他大型模型，支持25种语言。论文还发布了参数更少的Parakeet-TDT-0.6B-v3模型。", "motivation": "论文旨在开发一个更加快速和健壮的多语言模型，用于自动语音识别和语音转文本翻译，以应对现有模型在速度和泛化性能上的不足。", "method": "该论文介绍了Canary-1B-v2模型，一个快速、健壮的多语言自动语音识别和语音转文本翻译模型。该模型基于FastConformer编码器和Transformer解码器构建，支持25种主要为欧洲的语言。模型使用1.7M小时的数据进行训练，包括Granary和NeMo ASR Set 3.0数据集，以及加入非语音音频来减少ASR/AST的幻听问题。该模型采用了两阶段的预训练和微调过程，并进行了nGPT编码器的实验。", "result": "实验结果显示，nGPT在大规模数据上表现良好，而FineConformer在微调后表现出色。Canary-1B-v2模型在英语ASR任务上超过了Whisper-large-v3，速度提高了10倍，在多语言ASR和AST任务上与其他大的模型如Seamless-M4T-v2-large和基于LLM的系统表现相当。", "conclusion": "Canary-1B-v2模型在自动语音识别和语音转文本翻译方面取得了很好的效果，并且相较于其他大型模型在性能和速度上都有优势。该研究还发布了Parakeet-TDT-0.6B-v3模型，该模型在同等任务下只需要600M参数。"}}
{"id": "2509.13681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13681", "abs": "https://arxiv.org/abs/2509.13681", "authors": ["Hang Li", "Dianmo Sheng", "Qiankun Dong", "Zichun Wang", "Zhiwei Xu", "Tao Li"], "title": "FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras", "comment": "8 pages, 4 figures", "summary": "As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)\nsegmentation has recently achieved remarkable progress with pinhole cameras.\nHowever, it is non-trivial to extend the existing methods to fisheye cameras\nwith severe geometric distortion, ambiguous multi-view correspondences and\nunstable temporal dynamics, all of which significantly degrade BEV performance.\nTo address these challenges, we propose FishBEV, a novel BEV segmentation\nframework specifically tailored for fisheye cameras. This framework introduces\nthree complementary innovations, including a Distortion-Resilient Multi-scale\nExtraction (DRME) backbone that learns robust features under distortion while\npreserving scale consistency, an Uncertainty-aware Spatial Cross-Attention\n(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view\nalignment, a Distance-aware Temporal Self-Attention (D-TSA) module that\nadaptively balances near field details and far field context to ensure temporal\ncoherence. Extensive experiments on the Synwoodscapes dataset demonstrate that\nFishBEV consistently outperforms SOTA baselines, regarding the performance\nevaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.", "AI": {"tldr": "为解决广角相机在BEV分割应用中的挑战，我们提出了一种专为广角相机设计的新型BEV分割框架FishBEV，它引入了三个创新点，并在Synwoodscapes数据集上验证了其有效性。", "motivation": "现有的BEV分割方法难以应用于广角相机，因为后者会带来严重的几何失真，模糊的多视图对应关系和不稳定的动态时间变化，这些都会显著降低BEV性能。", "method": "FishBEV框架引入了三个互补的创新点，即一种在变形下学习鲁棒特征同时保持尺度一致的抗变形多尺度提取（DRME）主干，一种利用不确定性估计进行可靠跨视图对齐的不确定性空间交叉注意（U-SCA）机制，以及一种自适应平衡近距离细节和远距离上下文以确保时间一致性距离时间自注意（D-TSA）模块。", "result": "FishBEV在Synwoodscapes数据集上进行的大量实验证明，它始终优于SOTA基线，在评估关于围绕视图鱼眼BEV分割任务的性能时。", "conclusion": "FishBEV在Synwoodscapes数据集上的大量实验证明，它在处理围绕视图鱼眼BEV分割任务时，始终优于最新的基线方法。"}}
{"id": "2509.14161", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14161", "abs": "https://arxiv.org/abs/2509.14161", "authors": ["Brian Yan", "Injy Hamed", "Shuichiro Shimizu", "Vasista Lodagala", "William Chen", "Olga Iakovenko", "Bashar Talafha", "Amir Hussein", "Alexander Polok", "Kalvin Chang", "Dominik Klement", "Sara Althubaiti", "Puyuan Peng", "Matthew Wiesner", "Thamar Solorio", "Ahmed Ali", "Sanjeev Khudanpur", "Shinji Watanabe", "Chih-Chen Chen", "Zhen Wu", "Karim Benharrak", "Anuj Diwan", "Samuele Cornell", "Eunjung Yeo", "Kwanghee Choi", "Carlos Carvalho", "Karen Rosero"], "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset", "comment": null, "summary": "We present CS-FLEURS, a new dataset for developing and evaluating\ncode-switched speech recognition and translation systems beyond high-resourced\nlanguages. CS-FLEURS consists of 4 test sets which cover in total 113 unique\ncode-switched language pairs across 52 languages: 1) a 14 X-English language\npair set with real voices reading synthetically generated code-switched\nsentences, 2) a 16 X-English language pair set with generative text-to-speech\n3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the\ngenerative text-to-speech, and 4) a 45 X-English lower-resourced language pair\ntest set with concatenative text-to-speech. Besides the four test sets,\nCS-FLEURS also provides a training set with 128 hours of generative\ntext-to-speech data across 16 X-English language pairs. Our hope is that\nCS-FLEURS helps to broaden the scope of future code-switched speech research.\nDataset link: https://huggingface.co/datasets/byan/cs-fleurs.", "AI": {"tldr": "CS-FLEURS是一个为混合语言语音识别和翻译系统开发与评估提供支持的新数据集，包含多种测试集和一个大型训练集。", "motivation": "动机是拓宽未来混合语言语音研究的范围，提供一个可以评估和开发混合语言语音识别和翻译系统的数据集。", "method": "提出的CS-FLEURS是一个新数据集，用于开发和评估超出了高资源语言的混合语言语音识别和翻译系统。CS-FLEURS包括4种测试集，总共涵盖113个独特的混合语言对，涉及52种语言，并提供了一个训练集，包含128小时的16种X-英语语言对的生成语音合成数据。", "result": "提供了4种不同的测试集和一个包含128小时的16种X-英语语言对的生成语音合成数据的训练集。", "conclusion": "CS-FLEURS数据集的发表有助于拓宽混合语言语音研究的范围，为未来的研究提供了宝贵的数据资源。"}}
{"id": "2509.13687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13687", "abs": "https://arxiv.org/abs/2509.13687", "authors": ["Kaniz Fatema", "Emad A. Mohammed", "Sukhjit Singh Sehra"], "title": "Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification", "comment": null, "summary": "Effective and interpretable classification of medical images is a challenge\nin computer-aided diagnosis, especially in resource-limited clinical settings.\nThis study introduces spline-based Kolmogorov-Arnold Networks (KANs) for\naccurate medical image classification with limited, diverse datasets. The\nmodels include SBTAYLOR-KAN, integrating B-splines with Taylor series;\nSBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,\nembedding B-splines in Morlet wavelet transforms. These approaches leverage\nspline-based function approximation to capture both local and global\nnonlinearities. The models were evaluated on brain MRI, chest X-rays,\ntuberculosis X-rays, and skin lesion images without preprocessing,\ndemonstrating the ability to learn directly from raw data. Extensive\nexperiments, including cross-dataset validation and data reduction analysis,\nshowed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%\naccuracy, with a balanced F1-score, maintaining over 86% accuracy using only\n30% of the training data across three datasets. Despite class imbalance in the\nskin cancer dataset, experiments on both imbalanced and balanced versions\nshowed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.\nUnlike traditional CNNs, which require millions of parameters (e.g., ResNet50\nwith 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872\ntrainable parameters, making it more suitable for constrained medical\nenvironments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used\nfor interpretability, highlighting relevant regions in medical images. This\nframework provides a lightweight, interpretable, and generalizable solution for\nmedical image classification, addressing the challenges of limited datasets and\ndata-scarce scenarios in clinical AI applications.", "AI": {"tldr": "研究引入了基于样条函数的KANs模型用于医学影像分类，提高了在数据资源有限情况下的分类精度和可解释性。", "motivation": "医学影像的有效且可解释的分类是一个挑战，特别是在资源有限的临床环境中。这项研究旨在解决资源有限环境下医学影像分类的难题，同时在保持高精度的同时提供算法的可解释性。", "method": "本研究引入了基于样条的Kolmogorov-Arnold网络（KANs）用于在有限和多样化的数据集上进行准确的医学影像分类。这些模型包括整合了B样条和泰勒级数的SBTAYLOR-KAN，结合了B样条和径向基函数的SBRBF-KAN，以及嵌入了Morlet小波变换的B样条的SBWAVELET-KAN。这些方法利用了基于样条的函数近似性能捕捉局部和全局非线性。", "result": "这些模型在脑MRI、胸部X光、肺结核X光和皮肤病变图像上均进行了评估，未进行预处理，证明能够直接从原始数据中学习。广泛测试表明，SBTAYLOR-KAN在使用仅30%的训练数据时仍能保持超过86%的分类精度，同时可以解释影像中的关键区域。", "conclusion": "该框架为医学影像分类提供了轻量级、可解释且通用的学习方法，适用于医疗环境中的数据稀缺场景。相较于需要数百万参数的传统卷积网络模型，SBTAYLOR-KAN仅需2,872个可训练参数即能获得类似性能，更适合资源受限的医疗场景。"}}
{"id": "2509.14171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14171", "abs": "https://arxiv.org/abs/2509.14171", "authors": ["Yifan Liu", "Wenkuan Zhao", "Shanshan Zhong", "Jinghui Qin", "Mingfu Liang", "Zhongzhan Huang", "Wushao Wen"], "title": "AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity", "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have garnered\nsignificant attention, offering a promising pathway toward artificial general\nintelligence (AGI). Among the essential capabilities required for AGI,\ncreativity has emerged as a critical trait for MLLMs, with association serving\nas its foundation. Association reflects a model' s ability to think creatively,\nmaking it vital to evaluate and understand. While several frameworks have been\nproposed to assess associative ability, they often overlook the inherent\nambiguity in association tasks, which arises from the divergent nature of\nassociations and undermines the reliability of evaluations. To address this\nissue, we decompose ambiguity into two types-internal ambiguity and external\nambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative\nability while circumventing the ambiguity through a hybrid computational\nmethod. We then conduct extensive experiments on MLLMs, revealing a strong\npositive correlation between cognition and association. Additionally, we\nobserve that the presence of ambiguity in the evaluation process causes MLLMs'\nbehavior to become more random-like. Finally, we validate the effectiveness of\nour method in ensuring more accurate and reliable evaluations. See Project Page\nfor the data and codes.", "AI": {"tldr": "本文分析了评估多模态大型语言模型(MLLMs)创造力时面对的模糊性问题，提出了AssoCiAm基准测试方法，并通过实验验证了其有效性。", "motivation": "MLLMs是迈向通用人工智能（AGI）的潜在路径，而创造力被视作AGI的重要特质。现有的评估方法忽略了关联任务中的内在模糊性，这影响了评估的可靠性。本文旨在通过解决这一问题来改进评估方法。", "method": "本文提出了一种新的评估多模态大型语言模型（MLLMs）创造力的方法，将模糊性分解为内部模糊性和外部模糊性两种类型，并引入了AssoCiAm基准测试方法以绕过模糊性问题，进行更准确的评估。实验通过混合计算方法在MLLMs上进行。", "result": "实验揭示了认知与联想能力之间的强正相关性。同时观察到评估过程中存在的模糊性会导致MLLMs的行为更具有随机性。", "conclusion": "本文提出的方法确保了评估的准确性和可靠性，通过AssoCiAm基准测试验证了这一结论。"}}
{"id": "2509.13711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13711", "abs": "https://arxiv.org/abs/2509.13711", "authors": ["Qiuyu Tang", "Joshua Krinsky", "Aparna Bharati"], "title": "StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models", "comment": null, "summary": "The rapid advancement of generative models, particularly diffusion-based\napproaches, has inadvertently facilitated their potential for misuse. Such\nmodels enable malicious exploiters to replicate artistic styles that capture an\nartist's creative labor, personal vision, and years of dedication in an\ninexpensive manner. This has led to a rise in the need and exploration of\nmethods for protecting artworks against style mimicry. Although generic\ndiffusion models can easily mimic an artistic style, finetuning amplifies this\ncapability, enabling the model to internalize and reproduce the style with\nhigher fidelity and control. We hypothesize that certain cross-attention layers\nexhibit heightened sensitivity to artistic styles. Sensitivity is measured\nthrough activation strengths of attention layers in response to style and\ncontent representations, and assessing their correlations with features\nextracted from external models. Based on our findings, we introduce an\nefficient and lightweight protection strategy, StyleProtect, that achieves\neffective style defense against fine-tuned diffusion models by updating only\nselected cross-attention layers. Our experiments utilize a carefully curated\nartwork dataset based on WikiArt, comprising representative works from 30\nartists known for their distinctive and influential styles and cartoon\nanimations from the Anita dataset. The proposed method demonstrates promising\nperformance in safeguarding unique styles of artworks and anime from malicious\ndiffusion customization, while maintaining competitive imperceptibility.", "AI": {"tldr": "本文提出了一种名为StyleProtect的防护策略，旨在防御针对艺术作品和动漫作品的风格模仿攻击。该策略通过优化注意力层来实现对独特艺术风格的有效保护，同时保持较低的可感知度。实验在WikiArt和Anita数据集上展示了良好的性能。", "motivation": "随着生成模型，尤其是扩散模型的发展，艺术作品的风格被低成本复制，构成了对艺术家原创性的威胁。因此，研究保护艺术作品免受风格模仿的方法变得尤为重要。", "method": "Structure", "result": "{\"tldr\": \"本文提出了一种名为StyleProtect的防护策略，旨在防御针对艺术作品和动漫作品的风格模仿攻击。该策略通过优化注意力层来实现对独特艺术风格的有效保护，同时保持较低的可感知度。实验在WikiArt和Anita数据集上展示了良好的性能。\", \"motivation\": \"随着生成模型，尤其是扩散模型的发展，艺术作品的风格被低成本复制，构成了对艺术家原创性的威胁。因此，研究保护艺术作品免受风格模仿的方法变得尤为重要。\", \"method\": \"本文通过对注意力层敏感性的分析，提出StyleProtect策略，该策略通过选择性更新一些交叉注意力层以达到保护艺术作品独特风格的目的。\", \"result\": \"实验结果表明，与传统的防护方法相比，StyleProtect在保护艺术作品独特风格的同时，还具有较低的可感知度，这就使其能够在实际中得到应用。\", \"conclusion\": \"研究结果证明了StyleProtect的有效性，该策略可以有效地保护艺术和动漫作品的独特风格，对抗其被扩散模型细粒度调整的风险。\"}", "conclusion": "研究结果证明了StyleProtect的有效性，该策略可以有效地保护艺术和动漫作品的独特风格，对抗其被扩散模型细粒度调整的风险。"}}
{"id": "2509.14180", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2509.14180", "abs": "https://arxiv.org/abs/2509.14180", "authors": ["Akhil Theerthala"], "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs", "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available", "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.", "AI": {"tldr": "研究提出了一种新颖的框架，通过精心的数据整理和行为金融学整合，在成本降低80%的情况下，使Qwen-3-8B模型在事实准确性、流畅性和个性化度量方面达到了与更大模型相当的性能表现。", "motivation": "本研究旨在解决当前个人理财任务中代理管道成本高昂且效益低的问题。此类管道的成本高昂，实际收益不及预期的25%。", "method": "本研究提出了一种新颖且可重现的框架，该框架结合了相关金融背景与行为金融学研究，以构建端到端金融顾问的监督数据。", "result": "通过使用该框架和对Qwen-3-8B模型进行精细训练，在事实准确度、流畅性和个性化等指标上，8亿参数的模型在成本大幅降低的情况下与更大型基线模型（14-32亿参数）表现相当。", "conclusion": "本研究证明，通过整合行为金融学和适当的数据架构，能够显著提高成本效益和个人化理财建议的质量。"}}
{"id": "2509.13713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13713", "abs": "https://arxiv.org/abs/2509.13713", "authors": ["Tae-Wook Um", "Ki-Hyeon Kim", "Hyun-Duck Choi", "Hyo-Sung Ahn"], "title": "UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry", "comment": null, "summary": "Monocular depth estimation has been increasingly adopted in robotics and\nautonomous driving for its ability to infer scene geometry from a single\ncamera. In self-supervised monocular depth estimation frameworks, the network\njointly generates and exploits depth and pose estimates during training,\nthereby eliminating the need for depth labels. However, these methods remain\nchallenged by uncertainty in the input data, such as low-texture or dynamic\nregions, which can cause reduced depth accuracy. To address this, we introduce\nUM-Depth, a framework that combines motion- and uncertainty-aware refinement to\nenhance depth accuracy at dynamic object boundaries and in textureless regions.\nSpecifically, we develop a teacherstudent training strategy that embeds\nuncertainty estimation into both the training pipeline and network\narchitecture, thereby strengthening supervision where photometric signals are\nweak. Unlike prior motion-aware approaches that incur inference-time overhead\nand rely on additional labels or auxiliary networks for real-time generation,\nour method uses optical flow exclusively within the teacher network during\ntraining, which eliminating extra labeling demands and any runtime cost.\nExtensive experiments on the KITTI and Cityscapes datasets demonstrate the\neffectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves\nstate-of-the-art results in both self-supervised depth and pose estimation on\nthe KITTI datasets.", "AI": {"tldr": "UM-Depth是一个旨在提高单目深度估计在动态物体边界和无纹理区域精度的框架，通过引入运动感知和不确定性感知的优化策略，实现了自监督深度和姿态估计的最优结果，且无需额外标签或辅助网络。", "motivation": "由于自监督单目深度估计方法在处理无纹理或动态区域时精度较低的问题，UM-Depth的动机是通过引入运动感知和不确定性感知的优化来改进这些区域的深度估计精度。", "method": "UM-Depth采用了一种基于教师-学生训练策略的方法，该策略将不确定性估计嵌入到训练流程和网络架构中，以应对弱光度信号条件下的监督不足。特别地，它在训练过程中仅使用光流信息，消除了对额外标签或辅助网络的需求，并降低了运行时的成本。", "result": "实验表明，UM-Depth在动态物体边界和无纹理区域的深度估计方面取得了更好的结果，特别是在KITTI和Cityscapes数据集上的表现优于以往的方法。", "conclusion": "UM-Depth在KITTI数据集上实现了自监督深度和姿态估计的最先进结果，且通过优化策略在训练过程中大大降低了光度信号弱区域的深度误差。"}}
{"id": "2509.14197", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14197", "abs": "https://arxiv.org/abs/2509.14197", "authors": ["Vahid Ghafouri", "Robert McNeil", "Teodor Yankov", "Madeleine Sumption", "Luc Rocher", "Scott A. Hale", "Adam Mahdi"], "title": "Framing Migration: A Computational Analysis of UK Parliamentary Discourse", "comment": null, "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts.", "AI": {"tldr": "该研究利用LLMs分析了超过75年时间跨度的英国议会关于移民的讨论，并将其与美国国会的讨论进行了比较；发现与美国相比，英国各政党对于移民的态度相对一致，但存在明显的短暂意识形态差异；在叙述框架上，英国的讨论转向了安全化议题，而长期的社会融合议题则有所减少。", "motivation": "为了理解时间跨度超过75年的英国议会辩论中的移民相关话语，并将其与美国国会讨论进行比较，旨在探索这些话语随时间的变化及其背后的叙述框架的变化。", "method": "采用大型语言模型(LLMs)对关于移民的陈述进行标注，识别对移民的态度，并随着时间推移和政党差异追踪这些态度的变化。在英国的数据分析中还使用了半自动框架提取细微的叙述框架以捕捉移民话语中的细微差别。", "result": "研究发现，虽然美国的移民讨论越来越极化，但在英国议会的态度在各党派间相对一致，但工党和保守党之间存在持续的观点差异，这种差异在2025年达到最负面。此外，英国议会讨论中的叙述框架发生了从长期的社会融合框架向安全化的叙述框架如边境控制和非法移民的转变，同时，移民相关的国家法律讨论逐渐被国际法和人权的讨论所取代。", "conclusion": "整体而言，研究展示了如何利用LLMs来支持在政治和历史背景下进行可扩展的、细致的移民话语分析。这不仅可以揭示话语随时间的变化规律，还可以识别不同叙述框架在国家和国际法律中的作用。"}}
{"id": "2509.13722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13722", "abs": "https://arxiv.org/abs/2509.13722", "authors": ["Dingwei Zhang", "Dong Zhang", "Jinhui Tang"], "title": "Mitigating Query Selection Bias in Referring Video Object Segmentation", "comment": null, "summary": "Recently, query-based methods have achieved remarkable performance in\nReferring Video Object Segmentation (RVOS) by using textual static object\nqueries to drive cross-modal alignment. However, these static queries are\neasily misled by distractors with similar appearance or motion, resulting in\n\\emph{query selection bias}. To address this issue, we propose Triple Query\nFormer (TQF), which factorizes the referring query into three specialized\ncomponents: an appearance query for static attributes, an intra-frame\ninteraction query for spatial relations, and an inter-frame motion query for\ntemporal association. Instead of relying solely on textual embeddings, our\nqueries are dynamically constructed by integrating both linguistic cues and\nvisual guidance. Furthermore, we introduce two motion-aware aggregation modules\nthat enhance object token representations: Intra-frame Interaction Aggregation\nincorporates position-aware interactions among objects within a single frame,\nwhile Inter-frame Motion Aggregation leverages trajectory-guided alignment\nacross frames to ensure temporal coherence. Extensive experiments on multiple\nRVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our\nstructured query design and motion-aware aggregation modules.", "AI": {"tldr": "本文提出了一种名为Triple Query Former的新方法，改进了现有RVOS任务中的查询方式以提高准确性。", "motivation": "现有的RVOS方法依赖于静态对象查询，容易被外观或运动上相似的干扰项误导，因此提出了一种新的查询方式来解决这个问题。", "method": "本文提出了Triple Query Former (TQF)方法，将参照查询分解为三个专门的组成部分：外观查询、帧内交互查询和帧间运动查询，以用于处理视频对象分割问题中的查询选择偏差。", "result": "实验结果表明，TQF及其结构化查询设计和运动感知聚合模块在多个RVOS基准测试中表现优异。", "conclusion": "研究表明，通过将查询分解成专门的组成部分以及引入运动感知聚合模块，可以提高RVOS任务的表现。"}}
{"id": "2509.14233", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14233", "abs": "https://arxiv.org/abs/2509.14233", "authors": ["Alejandro Hernández-Cano", "Alexander Hägele", "Allen Hao Huang", "Angelika Romanou", "Antoni-Joan Solergibert", "Barna Pasztor", "Bettina Messmer", "Dhia Garbaya", "Eduard Frank Ďurech", "Ido Hakimi", "Juan García Giraldo", "Mete Ismayilzada", "Negar Foroutan", "Skander Moalla", "Tiancheng Chen", "Vinko Sabolčec", "Yixuan Xu", "Michael Aerni", "Badr AlKhamissi", "Ines Altemir Marinas", "Mohammad Hossein Amani", "Matin Ansaripour", "Ilia Badanin", "Harold Benoit", "Emanuela Boros", "Nicholas Browning", "Fabian Bösch", "Maximilian Böther", "Niklas Canova", "Camille Challier", "Clement Charmillot", "Jonathan Coles", "Jan Deriu", "Arnout Devos", "Lukas Drescher", "Daniil Dzenhaliou", "Maud Ehrmann", "Dongyang Fan", "Simin Fan", "Silin Gao", "Miguel Gila", "María Grandury", "Diba Hashemi", "Alexander Hoyle", "Jiaming Jiang", "Mark Klein", "Andrei Kucharavy", "Anastasiia Kucherenko", "Frederike Lübeck", "Roman Machacek", "Theofilos Manitaras", "Andreas Marfurt", "Kyle Matoba", "Simon Matrenok", "Henrique Mendoncça", "Fawzi Roberto Mohamed", "Syrielle Montariol", "Luca Mouchel", "Sven Najem-Meyer", "Jingwei Ni", "Gennaro Oliva", "Matteo Pagliardini", "Elia Palme", "Andrei Panferov", "Léo Paoletti", "Marco Passerini", "Ivan Pavlov", "Auguste Poiroux", "Kaustubh Ponkshe", "Nathan Ranchin", "Javi Rando", "Mathieu Sauser", "Jakhongir Saydaliev", "Muhammad Ali Sayfiddinov", "Marian Schneider", "Stefano Schuppli", "Marco Scialanga", "Andrei Semenov", "Kumar Shridhar", "Raghav Singhal", "Anna Sotnikova", "Alexander Sternfeld", "Ayush Kumar Tarun", "Paul Teiletche", "Jannis Vamvas", "Xiaozhe Yao", "Hao Zhao Alexander Ilic", "Ana Klimovic", "Andreas Krause", "Caglar Gulcehre", "David Rosenthal", "Elliott Ash", "Florian Tramèr", "Joost VandeVondele", "Livio Veraldi", "Martin Rajman", "Thomas Schulthess", "Torsten Hoefler", "Antoine Bosselut", "Martin Jaggi", "Imanol Schlag"], "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments", "comment": null, "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.", "AI": {"tldr": "Apertus是一个完全开放的大型语言模型套件，解决了数据合规和多语言表示问题。它通过维护可重现的数据管道、尊重内容所有者权利、采用Goldfish目标防止记忆来增强数据合规性。同时，它扩大了多语言覆盖范围，训练了来自超过1800种语言的15T令牌的数据。", "motivation": "解决当前开放模型生态系统中存在数据合规性和多语言表示的两大系统性问题。", "method": "Apertus 通过使用可重现的数据管道，并过滤掉未经授权、有毒或包含个人身份信息的内容来预训练模型。此外，它采用Goldfish目标来防止直接背诵训练数据，同时保持下游任务的性能。Apertus还在超过1800种语言的15T令牌上进行了训练，其中约40%的数据是非英语内容。", "result": "以8B和70B的规模发布，Apertus在全开放模型的多语言基准测试中几乎达到了最先进的结果，与开放权重的对应模型相比肩甚至是超越。", "conclusion": "Apertus不仅提供模型权重，还开放了开发周期中的所有科学机制，包括数据准备脚本、检查点、评估套件和训练代码，这使得透明的审计和扩展成为可能。"}}
{"id": "2509.13747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13747", "abs": "https://arxiv.org/abs/2509.13747", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-Jiang Liu", "Lingfeng Yang", "Zhenhua Feng", "Wankou Yang", "Jingdong Wang"], "title": "Improving Generalized Visual Grounding with Instance-aware Joint Learning", "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) in September 2025", "summary": "Generalized visual grounding tasks, including Generalized Referring\nExpression Comprehension (GREC) and Segmentation (GRES), extend the classical\nvisual grounding paradigm by accommodating multi-target and non-target\nscenarios. Specifically, GREC focuses on accurately identifying all referential\nobjects at the coarse bounding box level, while GRES aims for achieve\nfine-grained pixel-level perception. However, existing approaches typically\ntreat these tasks independently, overlooking the benefits of jointly training\nGREC and GRES to ensure consistent multi-granularity predictions and streamline\nthe overall process. Moreover, current methods often treat GRES as a semantic\nsegmentation task, neglecting the crucial role of instance-aware capabilities\nand the necessity of ensuring consistent predictions between instance-level\nboxes and masks. To address these limitations, we propose InstanceVG, a\nmulti-task generalized visual grounding framework equipped with instance-aware\ncapabilities, which leverages instance queries to unify the joint and\nconsistency predictions of instance-level boxes and masks. To the best of our\nknowledge, InstanceVG is the first framework to simultaneously tackle both GREC\nand GRES while incorporating instance-aware capabilities into generalized\nvisual grounding. To instantiate the framework, we assign each instance query a\nprior reference point, which also serves as an additional basis for target\nmatching. This design facilitates consistent predictions of points, boxes, and\nmasks for the same instance. Extensive experiments obtained on ten datasets\nacross four tasks demonstrate that InstanceVG achieves state-of-the-art\nperformance, significantly surpassing the existing methods in various\nevaluation metrics. The code and model will be publicly available at\nhttps://github.com/Dmmm1997/InstanceVG.", "AI": {"tldr": "提出了InstanceVG框架，同时处理广义指代表达理解和分割任务，并引入实例感知能力，显著超越现有方法。", "motivation": "现有的方法通常独立处理广义指代表达理解和分割任务，忽视了联合训练的好处。此外，现有方法往往将分割视为语义分割任务，忽视了实例感知的重要性。", "method": "提出了一个名为InstanceVG的多任务广义视觉接地框架，该框架配备了实例感知能力，利用实例查询统一实例级别边界框和掩码的联合与一致性预测。", "result": "在四个任务的十个数据集上进行了广泛实验，InstanceVG实现了最先进的性能，在各种评估指标上显著超越现有方法。", "conclusion": "InstanceVG是第一个同时处理广义指代表达理解和分割任务的框架，引入了实例感知能力，展现了优越的性能。"}}
{"id": "2509.13754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13754", "abs": "https://arxiv.org/abs/2509.13754", "authors": ["Hao Yin", "Xin Man", "Feiyu Chen", "Jie Shao", "Heng Tao Shen"], "title": "Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval", "comment": null, "summary": "Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that\naims to retrieve the most relevant person images based on a given text query.\nThe key challenge in TIPR lies in achieving effective alignment between textual\nand visual modalities within a common latent space. To address this challenge,\nprior approaches incorporate attention mechanisms for implicit cross-modal\nlocal alignment. However, they lack the ability to verify whether all local\nfeatures are correctly aligned. Moreover, existing methods primarily focus on\nhard negative samples during model updates, with the goal of refining\ndistinctions between positive and negative pairs, often neglecting incorrectly\nmatched positive pairs. To alleviate these issues, we propose FMFA, a\ncross-modal Full-Mode Fine-grained Alignment framework, which enhances global\nmatching through explicit fine-grained alignment and existing implicit\nrelational reasoning -- hence the term ``full-mode\" -- without requiring\nadditional supervision. Specifically, we design an Adaptive Similarity\nDistribution Matching (A-SDM) module to rectify unmatched positive sample\npairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint\nembedding space, thereby achieving more precise global alignment. Additionally,\nwe introduce an Explicit Fine-grained Alignment (EFA) module, which makes up\nfor the lack of verification capability of implicit relational reasoning. EFA\nstrengthens explicit cross-modal fine-grained interactions by sparsifying the\nsimilarity matrix and employs a hard coding method for local alignment. Our\nproposed method is evaluated on three public datasets, achieving\nstate-of-the-art performance among all global matching methods. Our code is\navailable at https://github.com/yinhao1102/FMFA.", "AI": {"tldr": "本文提出了FMFA模型，通过显式和隐式的方式进行细粒度对齐，解决了先前模型在文本到图像的Person Retrieval任务中对局部特征对齐验证不足和忽视不正确的正样本对的问题，取得了良好的结果。", "motivation": "先前的方法虽然引入了注意力机制来实现跨模态的局部对齐，但缺乏验证所有局部特性是否正确对齐的能力，并且忽视了不正确的正样本对，这促使了FMFA的提出。", "method": "FMFA 是一种跨模态全模式细粒度对齐框架，通过显式的细粒度对齐和现有的隐式关系推理来增强全局匹配，并提出了适应性相似度分布匹配模块（A-SDM）来纠正不匹配的正样本对，并引入显式细粒度对齐模块（EFA）来增强显式的跨模态细粒度交互。", "result": "该方法在三个公共数据集上进行了评估，并在全球匹配方法中取得了最先进的性能。", "conclusion": "实验表明，FMFA通过显式和隐式的细粒度对齐方法，提升了文本到图像的跨模态匹配任务的性能，达到了最先进的水平。"}}
{"id": "2509.13836", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13836", "abs": "https://arxiv.org/abs/2509.13836", "authors": ["Weihang Wang", "Xinhao Li", "Ziyue Wang", "Yan Pang", "Jielei Zhang", "Peiyi Li", "Qiang Zhang", "Longwen Gao"], "title": "Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models", "comment": "Accepted by EMNLP2025 Finding", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly\nimpedes their real-world applicability. As the primary component for accurately\ninterpreting visual information, the choice of visual encoder is pivotal. We\nhypothesize that the diverse training paradigms employed by different visual\nencoders instill them with distinct inductive biases, which leads to their\ndiverse hallucination performances. Existing benchmarks typically focus on\ncoarse-grained hallucination detection and fail to capture the diverse\nhallucinations elaborated in our hypothesis. To systematically analyze these\neffects, we introduce VHBench-10, a comprehensive benchmark with approximately\n10,000 samples for evaluating LVLMs across ten fine-grained hallucination\ncategories. Our evaluations confirm encoders exhibit unique hallucination\ncharacteristics. Building on these insights and the suboptimality of simple\nfeature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.\nIt employs global visual features to generate routing signals, dynamically\naggregating visual features from multiple specialized experts. Comprehensive\nexperiments confirm the effectiveness of VisionWeaver in significantly reducing\nhallucinations and improving overall model performance.", "AI": {"tldr": "This paper analyzes object hallucination in LVLMs and proposes VisionWeaver, a method that significantly reduces hallucinations.", "motivation": "To systematically analyze the diverse hallucinations caused by different visual encoders and to address the issue of object hallucination, which impedes the real-world applicability of Large Vision-Language Models (LVLMs).", "method": "We introduce VHBench-10, a comprehensive benchmark for evaluating LVLMs across ten fine-grained hallucination categories, and propose VisionWeaver, a novel Context-Aware Routing Network that dynamically aggregates visual features from multiple specialized experts using global visual features to generate routing signals.", "result": "Evaluations confirm the unique hallucination characteristics of different encoders, and VisionWeaver significantly reduces hallucinations and improves overall model performance.", "conclusion": "VisionWeaver effectively reduces hallucinations and improves model performance, suggesting a promising direction for future research on reducing hallucinations in LVLMs."}}
{"id": "2509.13756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13756", "abs": "https://arxiv.org/abs/2509.13756", "authors": ["Yuqi Yang", "Dongliang Chang", "Yuanchen Fang", "Yi-Zhe SonG", "Zhanyu Ma", "Jun Guo"], "title": "Controllable-Continuous Color Editing in Diffusion Model via Color Mapping", "comment": null, "summary": "In recent years, text-driven image editing has made significant progress.\nHowever, due to the inherent ambiguity and discreteness of natural language,\ncolor editing still faces challenges such as insufficient precision and\ndifficulty in achieving continuous control. Although linearly interpolating the\nembedding vectors of different textual descriptions can guide the model to\ngenerate a sequence of images with varying colors, this approach lacks precise\ncontrol over the range of color changes in the output images. Moreover, the\nrelationship between the interpolation coefficient and the resulting image\ncolor is unknown and uncontrollable. To address these issues, we introduce a\ncolor mapping module that explicitly models the correspondence between the text\nembedding space and image RGB values. This module predicts the corresponding\nembedding vector based on a given RGB value, enabling precise color control of\nthe generated images while maintaining semantic consistency. Users can specify\na target RGB range to generate images with continuous color variations within\nthe desired range, thereby achieving finer-grained, continuous, and\ncontrollable color editing. Experimental results demonstrate that our method\nperforms well in terms of color continuity and controllability.", "AI": {"tldr": "本文解决文本驱动图像编辑过程中的颜色控制问题，提出了颜色映射模块，实现了更细粒度、连续和可控制的颜色编辑。", "motivation": "尽管近年来基于文本驱动的图像编辑取得了显著的进步，但由于自然语言的内在模糊性和离散性，颜色编辑仍然面临精度不足和难以实现连续控制的挑战。", "method": "本论文引入了一种颜色映射模块，该模块可以明确建立文本嵌入空间与图像RGB值之间的对应关系。通过预测给定RGB值对应的嵌入向量，该模块可以实现对生成图像颜色的精确控制，并保持语义一致性。", "result": "实验结果显示提出的方法能在颜色连续性和可控制性方面表现出色。", "conclusion": "实验结果表明，该方法在颜色连续性和可控性方面表现良好。"}}
