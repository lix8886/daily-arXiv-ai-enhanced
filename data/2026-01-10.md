<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 10]
- [cs.CV](#cs.CV) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

> MedPI是一个评估大型语言模型（LLM）在医患对话中表现的多维度基准，发现LLM在鉴别诊断等方面的性能较差。

<details>
  <summary>Details</summary>

**Motivation:** 旨在通过MedPI来评估LLMs在医患对话中的表现，并帮助指导未来LLMs在诊断和治疗建议中的应用。

**Method:** 提出了MedPI，这是一个高维度基准，用于评估LLMs在医患对话中的表现。MedPI由五层组成，包括合成的电子健康记录类数据包、具有记忆和情感的AI患者、任务矩阵、评估框架以及AI裁判。

**Result:** 通过对9个旗舰模型的评估，研究发现所有LLMs在多个维度上的表现较低，特别是在鉴别诊断方面。

**Conclusion:** MedPI能够提供一种新的评估方法，用以指导未来大型语言模型在医疗对话中的应用，特别是在鉴别诊断上的改进。

**Abstract:** We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [2] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

> 引入了用于评估检索增强生成（RAG）系统的诊断和解释性框架RAGVUE，能够细粒度地揭示系统的性能问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估指标往往将异质行为压缩成单一分数，对错误源（检索、推理或事实依据）缺乏深刻洞察。

**Method:** 介绍了一种诊断和解释性框架RAGVUE，用于无参考自动评估RAG系统。该框架将RAG行为分解为检索质量、答案相关性和完整性、严格的声明级忠实度和判断校准，并为每个指标提供结构化解释。

**Result:** RAGVUE可以在比较实验中发现现有工具常常忽略的细微错误，它支持手动指标选择和完全自动化的代理评估。

**Conclusion:** 展示了RAGVUE的完整工作流程以及如何将其集成到研究和RAG开发中。其源代码和详细使用说明在GitHub上公开。

**Abstract:** Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [3] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

> 本文提出了一种无监督方法来生成汉语动词搭配数据库，其算法在语法错误纠正任务中表现出优于大语言模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 目的是为大语言模型补充明确且解释性强的语言规则，特别是在需要解释和可解释性的应用场景中。

**Method:** 本文提出了一种完全无监督的方法来构建汉语动词搭配数据库，旨在通过提供明确且可解释的规则来补充大语言模型，特别是在需要解释和可解释性应用的场景中。本文正式定义了一个动词搭配为投射的、有根的、有序的和有向无环图，并采用了一系列聚类算法，从大规模语料库中检索出的一系列句子中生成给定动词的搭配。

**Result:** 统计分析表明，生成的动词搭配具有功能独立性和等级典型性的设计特征。使用动词语法错误校正进行的评估显示，基于最大匹配的搭配的错误校正算法比大语言模型表现更好。

**Conclusion:** 评估表明，基于生成的动词搭配最大匹配的错误校正算法比单纯使用大语言模型能获得更好的错误率纠正结果。

**Abstract:** This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [4] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

> 该研究提出了一种使用大型语言模型生成合成电子商务产品数据的方法，在公共MAVE数据集上表现良好，显示出在低资源场景下的应用价值。

<details>
  <summary>Details</summary>

**Motivation:** 由于获取高质量的标注数据集依然具有挑战性，该论文旨在解决电子商务服务中的产品信息提取问题。

**Method:** 该论文提出了一种使用大型语言模型（LLMs）生成合成电子商务产品数据的系统方法。该方法包括三种策略：属性保持修改、受控负例生成和系统性属性移除，并在属性感知提示下强制执行商店约束，同时保持产品的一致性。

**Result:** 人类评价了2000个合成产品，结果表明99.6%的产品被评为自然，96.5%的产品含有有效的属性值，超过90%的产品展示了属性使用的一致性。在MAVE数据集上，该方法达到了60.5%的准确率，与真实训练数据（60.8%）表现相当，显著超过了零样本基准的13.4%。此外，混合配置使准确率提高到68.8%。

**Conclusion:** 该框架提供了一种增强电子商务数据集的实际解决方案，特别是在低资源场景下具有很高的应用价值。

**Abstract:** Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [5] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

> 研究提出一种集体叙事根基协议，通过参与性工作坊的方法，识别并整合社区具体知识，以解决大语言模型的本地知识盲点问题，旨在构建更好的回答本地问题的社区根基AI系统。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型经常在社区特定的查询上失败，导致“知识盲点”，边缘化当地声音并强化认识不公。研究的动机是通过集体叙事根基协议将社区故事转化为结构化的叙事单元并将其整合到由社区治理的人工智能系统中。

**Method:** 本研究通过三个参与性地图工作坊，从24位社区成员中设计了提取方法和模式，以保持叙事的丰富性并实现实体、时间和地点的抽取、验证和出处控制。

**Result:** 通过对包含14,782对本地信息QA（问答）对的地方级基准进行审计，发现事实性差距、文化误解、地理困惑和时间不对齐占错误的76.7%。在从工作坊中生成的参与性QA集合上，最先进的大语言模型在没有添加上下文的情况下正确回答的问题不到21%，这突显了本地根基的必要性。

**Conclusion:** 研究不仅提出了一种协议，还通过试点和参与性评估提供了具体要求，以构建具有代表性、控制力和隐私保护的本地化问答系统，从而为构建更好的回答本地问题的人工智能系统奠定了坚实基础。

**Abstract:** Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [6] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

> 研究团队开发了TeleTables基准来评估语言模型对电信标准表格的理解能力，实验结果表明小型模型的表现较差，而大型模型在表格推理上有更强表现。强调领域特化的微调对于可靠解释和推理电信标准的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 近期研究表明，语言模型在解析电信标准特别是3GPP规范方面表现不佳。这些标准密集地使用表格呈现重要信息，而模型对表格知识的理解并不充分。

**Method:** 提出了TeleTables基准，用于评估大型语言模型对技术规格中表格知识的隐性和显性理解能力。该基准通过从3GPP标准中提取表格，并用多模式和推理导向的语言模型生成和验证问题来构建。

**Result:** 实验结果显示，小型模型（参数小于10B）在回想起3GPP知识和解释表格方面都存在困难，但较大模型在表格推理方面的表现更强。

**Conclusion:** TeleTables基准揭示了语言模型在解释电信标准方面存在的缺陷，特别是较小规模模型的表现不佳。强调了领域特化微调的重要性。

**Abstract:** Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [7] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

> FronTalk基准测试开创了前端代码生成的研究，强调对话式代码生成与多模态反馈相结合的独特交互模式，同时揭示了之前未被充分探讨的两个挑战：模型的遗忘问题和视觉语言模型解读视觉反馈的难题。

<details>
  <summary>Details</summary>

**Motivation:** 当前，视觉元素在多回合代码生成中的作用仍然缺乏系统性研究。该项目旨在填补这一研究空白，并为未来的前端开发及多回合多模态代码生成交互动态研究奠定坚实基础。

**Method:** 我们介绍了一个新的基准测试FronTalk，用于前端代码生成，它特别研究一种独特的交互动态：带有多模态反馈的对话式代码生成。FronTalk基于真实世界的100个多回合对话构建，这些对话从不同领域（如新闻、金融和艺术）的网站中提取。每个回合包括文本指令和等效的视觉指令，旨在解决前端开发任务，并收集了体现设计意图的视觉元素，如草图、原型和注释屏幕截图。为了全面评估模型性能，还提出了一种新的基于代理的评估框架，该框架利用网络代理来模拟用户并探索网站，从而衡量功能正确性和用户体验。

**Result:** 20个模型的评估揭示了两个关键挑战：遗忘问题导致任务失败，以及解读视觉反馈的持久难题，特别是在开源视觉语言模型上。提出了一个基线解决方案AceCoder，通过使用网络代理来审核之前的指令，从而显著降低遗忘率并提升性能。

**Conclusion:** 基于FronTalk基准进行的评估揭示了文献中系统性探索不足的两个重要挑战：显著的遗忘问题以及视觉反馈的持续解读难题，尤其是对开源视觉语言模型(VLMs)而言。我们提出基线解决方案AceCoder，通过使用自主网络代理来审核每一步指令的执行，有效解决了上述问题，将性能提升高达9.3%（56.0%至65.3%）。这证明了FronTalk作为未来研究的基础价值。

**Abstract:** We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


### [8] [STDD:Spatio-Temporal Dynamics-Driven Token Refinement in Diffusion Language Models](https://arxiv.org/abs/2601.04205)
*Xinhao Sun,Maoliang Li,Zihao Zheng,Jiayu Chen,Hezhao Xu,Yun Liang,Xiang Chen*

Main category: cs.CL

> Introduces a new method for improving efficiency and output quality in diffusion language models by adaptively adjusting the confidence threshold based on token dynamics.

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of redundant iterations and constrained parallelism caused by fixed-threshold remasking strategies in diffusion language models (DLMs).

**Method:** Proposes a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, adjusting the confidence threshold adaptively based on these signals.

**Result:** Achieves significant improvements in operational efficiency, reaching up to 8.9 times speedup, while maintaining generation quality.

**Conclusion:** The proposed dynamic remasking method effectively enhances the performance of DLMs by addressing shortcomings in current remasking strategies.

**Abstract:** Unlike autoregressive language models, diffusion language models (DLMs) generate text by iteratively denoising all token positions in parallel. At each timestep, the remasking strategy of a DLM selects low-priority tokens to defer their decoding, thereby improving both efficiency and output quality. However, mainstream remasking strategies rely on a single global confidence threshold, overlooking the temporal and spatial dynamics of individual tokens. Motivated by the redundant iterations and constrained parallelism introduced by fixed-threshold remasking, we propose a novel remasking approach that dynamically detects Temporal Variance and Spatial Deviance of each token, which reflect its convergence status and inter-token correlations. Using these signals, our method adaptively adjusts the confidence threshold for every token at every step. Empirical results show that our approach significantly improves the operational efficiency of DLMs across mainstream datasets, achieving speedups of up to 8.9 times while faithfully preserving generation quality.

</details>


### [9] [Enhancing Admission Inquiry Responses with Fine-Tuned Models and Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04206)
*Aram Virabyan*

Main category: cs.CL

> 研究提出了一种结合微调语言模型与RAG的AI系统，以优化大学招生查询响应速度和信息准确性，提升潜在学生的体验。

<details>
  <summary>Details</summary>

**Motivation:** 大学招生办公室面临的挑战是在维护响应质量的同时管理高流量的查询。响应时间与信息准确性的问题直接影响到潜在学生的感知。

**Method:** 本研究提出了一个结合了微调语言模型和检索增强生成（RAG）的AI系统，旨在优化大学招生过程中的高流量查询管理。通过在一个特定于招生流程的精心策划的数据集上进行微调，该模型提高了其解读RAG提供的数据并生成相应输出的能力。此外，还探讨了响应生成逻辑的优化策略，以平衡响应质量和速度。

**Result:** 本研究的混合方法利用了RAG访问最新信息的能力和微调模型嵌入精细领域理解的能力，以提高该领域的反应质量和速度。

**Conclusion:** 研究提出的方法通过特定领域的微调，成功提升了复杂领域内AI系统的响应质量和速度，为大学招生通信提供了一种高效解决方案。

**Abstract:** University admissions offices face the significant challenge of managing high volumes of inquiries efficiently while maintaining response quality, which critically impacts prospective students' perceptions. This paper addresses the issues of response time and information accuracy by proposing an AI system integrating a fine-tuned language model with Retrieval-Augmented Generation (RAG). While RAG effectively retrieves relevant information from large datasets, its performance in narrow, complex domains like university admissions can be limited without adaptation, potentially leading to contextually inadequate responses due to the intricate rules and specific details involved. To overcome this, we fine-tuned the model on a curated dataset specific to admissions processes, enhancing its ability to interpret RAG-provided data accurately and generate domain-relevant outputs. This hybrid approach leverages RAG's ability to access up-to-date information and fine-tuning's capacity to embed nuanced domain understanding. We further explored optimization strategies for the response generation logic, experimenting with settings to balance response quality and speed, aiming for consistently high-quality outputs that meet the specific requirements of admissions communications.

</details>


### [10] [Ideology as a Problem: Lightweight Logit Steering for Annotator-Specific Alignment in Social Media Analysis](https://arxiv.org/abs/2601.04207)
*Wei Xia,Haowen Tang,Luozheng Li*

Main category: cs.CL

> 本文发现了大型语言模型中的政治意识形态结构与人类的意识形态空间之间存在不一致，并提出了一种实用且低成本的校正方法。

<details>
  <summary>Details</summary>

**Motivation:** 为了减少大型语言模型内部的政治意识形态结构与人类意识形态空间之间的不一致，使其输出更符合人类的期待。

**Method:** 本文提出了一种简单且高效的方法来使模型与特定用户的观点对齐。该方法不是通过重新训练模型，而是通过计算模型内部特征的偏置得分，并直接调整最终输出概率来实现的。

**Result:** 提出了一种轻量级线性探测器，不仅可以量化这种不一致性，还可以最小化地纠正输出层。这种方法实用且成本低，同时保留了模型的原始推理能力。

**Conclusion:** 该研究揭示了大型语言模型内部政治意识形态结构与人类意识形态空间之间的部分不一致，并提供了一个低成本且有效的方式来校正这种不一致性。

**Abstract:** LLMs internally organize political ideology along low-dimensional structures that are partially, but not fully aligned with human ideological space. This misalignment is systematic, model specific, and measurable. We introduce a lightweight linear probe that both quantifies the misalignment and minimally corrects the output layer. This paper introduces a simple and efficient method for aligning models with specific user opinions. Instead of retraining the model, we calculated a bias score from its internal features and directly adjusted the final output probabilities. This solution is practical and low-cost and preserves the original reasoning power of the model.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [11] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

> 我们为图像质量的分层、细粒度对齐提出了一个新的两阶段框架，基于领域专家的分级标准，通过复杂偏好优化（CPO），展现对绘画生成等任务的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的扩散模型训练后对齐依赖于简化的信号（如标量奖励或二元偏好），这限制了与复杂人类专业知识（分层且精细）的对齐。因此，我们旨在解决这一局限性，并提出更精细的对齐方法。

**Method:** 我们提出了一个两阶段的对齐框架。首先，通过监督微调将领域知识注入辅助扩散模型。其次，我们引入了复杂偏好优化（CPO），这是一种将目标扩散与非二元、分层标准对齐的方法。具体来说，我们重新定义对齐问题，旨在同时最大化正属性的概率，同时最小化负属性的概率，利用辅助扩散模型。

**Result:** 我们的实验结果表明，应用CPO显著增强了生成质量和与专业知识的对齐。

**Conclusion:** 这项研究展示了通过使用非二元、分层的偏好优化来显著改进扩散模型与复杂专业知识对齐的潜力，为精细标准的对齐开辟了新的途径。

**Abstract:** Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [12] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

> The paper introduces an efficient technique for text embedding in images using quinary pixel intensity combinations, achieving less distortion and lower computational demands compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** Current methods for embedding texts into images often result in noise and are computationally heavy, especially for deep learning and generative AI techniques.

**Method:** The paper proposes a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space, allowing the encoding of a complete textual symbol within a single RGB pixel.

**Result:** Evaluation metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison, and Heatmap analysis showed no significant distortion in the images, achieving improved embedding efficiency.

**Conclusion:** The proposed method offers a more efficient and less computationally intensive approach to embedding textual information in images compared to existing methods such as LSB/MSB manipulation, transform domain, or deep learning methods.

**Abstract:** This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [13] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

> This paper focuses on improving unified text-to-image generation by enabling autonomous transitions between modalities and demonstrates the effectiveness of reward-weighted post-training strategies with synthetic data.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to improve text-to-image synthesis by overcoming the limitations of existing systems that rely on explicit modality switching, which separates the process of text and image generation.

**Method:** Unified multimodal generation architectures that jointly produce text and images are explored. Post-training strategies are used to enable models to autonomously transition from text reasoning to visual synthesis in a single inference process. Different post-training data strategies are analyzed.

**Result:** The use of offline, reward-weighted post-training with fully self-generated synthetic data improves multimodal image generation across four diverse text-to-image benchmarks.

**Conclusion:** The approach demonstrates that effective reward-weighting of both modalities and strategically designed post-training data can enhance unified multimodal generation.

**Abstract:** Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>


### [14] [ReHyAt: Recurrent Hybrid Attention for Video Diffusion Transformers](https://arxiv.org/abs/2601.04342)
*Mohsen Ghafoorian,Amirhossein Habibian*

Main category: cs.CV

> ReHyAt, a Recurrent Hybrid Attention, is introduced for video generation, balancing efficiency and quality. It reduces attention complexity, enabling scalable and high-quality long-duration video generation at a lower computational cost.

<details>
  <summary>Details</summary>

**Motivation:** Transformers for video diffusion models suffer from quadratic attention complexity, limiting scalability for longer video sequences. The aim is to reduce this complexity while maintaining video generation quality.

**Method:** ReHyAt, a Recurrent Hybrid Attention mechanism, is introduced, combining softmax and linear attention for efficient and high-fidelity video generation.

**Result:** ReHyAt not only offers state-of-the-art video quality but also reduces the computational cost significantly, making it feasible for long videos and on-device applications.

**Conclusion:** A lightweight distillation and fine-tuning pipeline using ReHyAt is devised as a practical solution for future models, effectively balancing the trade-off between efficiency and quality.

**Abstract:** Recent advances in video diffusion models have shifted towards transformer-based architectures, achieving state-of-the-art video generation but at the cost of quadratic attention complexity, which severely limits scalability for longer sequences. We introduce ReHyAt, a Recurrent Hybrid Attention mechanism that combines the fidelity of softmax attention with the efficiency of linear attention, enabling chunk-wise recurrent reformulation and constant memory usage. Unlike the concurrent linear-only SANA Video, ReHyAt's hybrid design allows efficient distillation from existing softmax-based models, reducing the training cost by two orders of magnitude to ~160 GPU hours, while being competitive in the quality. Our light-weight distillation and finetuning pipeline provides a recipe that can be applied to future state-of-the-art bidirectional softmax-based models. Experiments on VBench and VBench-2.0, as well as a human preference study, demonstrate that ReHyAt achieves state-of-the-art video quality while reducing attention cost from quadratic to linear, unlocking practical scalability for long-duration and on-device video generation. Project page is available at https://qualcomm-ai-research.github.io/rehyat.

</details>


### [15] [SCAR-GS: Spatial Context Attention for Residuals in Progressive Gaussian Splatting](https://arxiv.org/abs/2601.04348)
*Diego Revilla,Pooja Suresh,Anand Bhojan,Ooi Wei Tsang*

Main category: cs.CV

> 本文提出了一种新的3D高斯点云压缩编码方法，使用残差向量量化来提高压缩效率，通过自回归熵模型预测每个传输索引的条件概率，实现了高效压缩。

<details>
  <summary>Details</summary>

**Motivation:** 尽管3D高斯点云技术允许实时高保真度的新视角合成，但这些模型在大规模和中型场景中具有很大的存储需求，阻碍了它们在云和流媒体服务上的部署。传统的压缩方法使用空间上下文模型进行标量量化，可能无法充分利用高维特征向量的相关性。

**Method:** 本文提出了一种新的3D高斯点云无损编码方法，使用残差向量量化代替传统的标量量化方式来压缩基元特征。主要贡献是一个由多分辨率哈希网格引导的自回归熵模型，能够准确预测每个连续传输索引的条件概率，使得粗略和细化层可以高效压缩。

**Result:** 未提供具体结果。

**Conclusion:** 本文所提出的方法，通过采用残差向量量化和自回归熵模型，能够有效地减少3D高斯点云模型的比特率，为大型场景的实时渲染和传输提供了可能。

**Abstract:** Recent advances in 3D Gaussian Splatting have allowed for real-time, high-fidelity novel view synthesis. Nonetheless, these models have significant storage requirements for large and medium-sized scenes, hindering their deployment over cloud and streaming services. Some of the most recent progressive compression techniques for these models rely on progressive masking and scalar quantization techniques to reduce the bitrate of Gaussian attributes using spatial context models. While effective, scalar quantization may not optimally capture the correlations of high-dimensional feature vectors, which can potentially limit the rate-distortion performance.
  In this work, we introduce a novel progressive codec for 3D Gaussian Splatting that replaces traditional methods with a more powerful Residual Vector Quantization approach to compress the primitive features. Our key contribution is an auto-regressive entropy model, guided by a multi-resolution hash grid, that accurately predicts the conditional probability of each successive transmitted index, allowing for coarse and refinement layers to be compressed with high efficiency.

</details>


### [16] [Comparative Analysis of Custom CNN Architectures versus Pre-trained Models and Transfer Learning: A Study on Five Bangladesh Datasets](https://arxiv.org/abs/2601.04352)
*Ibrahim Tanvir,Alif Ruslan,Sartaj Solaiman*

Main category: cs.CV

> 研究表明，迁移学习方法优于自定义的CNN和特征提取方法，在图像分类任务上取得了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 旨在探索迁移学习在不同数据集上的性能优势，并为实际应用提供基于数据集特征、计算资源和性能需求选择深度学习方法的实用见解。

**Method:** 本研究采用全面对比分析自定义的卷积神经网络(CNN)与流行的预训练架构（如ResNet-18和VGG-16），比较基于特征提取和迁移学习的方法。

**Result:** 实验结果表明，迁移学习效果最佳，准确率提升了3%到76%，而自定义的CNN则在参数规模和训练效率上具有优势。

**Conclusion:** 研究认为，对于计算资源有限和性能需求高的应用，使用预训练模型结合迁移学习是更佳选择。

**Abstract:** This study presents a comprehensive comparative analysis of custom-built Convolutional Neural Networks (CNNs) against popular pre-trained architectures (ResNet-18 and VGG-16) using both feature extraction and transfer learning approaches. We evaluated these models across five diverse image classification datasets from Bangladesh: Footpath Vision, Auto Rickshaw Detection, Mango Image Classification, Paddy Variety Recognition, and Road Damage Detection. Our experimental results demonstrate that transfer learning with fine-tuning consistently outperforms both custom CNNs built from scratch and feature extraction methods, achieving accuracy improvements ranging from 3% to 76% across different datasets. Notably, ResNet-18 with fine-tuning achieved perfect 100% accuracy on the Road Damage BD dataset. While custom CNNs offer advantages in model size (3.4M parameters vs. 11-134M for pre-trained models) and training efficiency on simpler tasks, pre-trained models with transfer learning provide superior performance, particularly on complex classification tasks with limited training data. This research provides practical insights for practitioners in selecting appropriate deep learning approaches based on dataset characteristics, computational resources, and performance requirements.

</details>
