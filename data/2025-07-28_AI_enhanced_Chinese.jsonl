{"id": "2507.18742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18742", "abs": "https://arxiv.org/abs/2507.18742", "authors": ["Víctor Gallego"], "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement", "comment": "Accepted to SCALR Workshop @ COLM 2025", "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .", "AI": {"tldr": "语言模型容易利用有缺陷的规范来获得高分，而未真正满足用户的意图。本研究提出了一种名为规范自我修正（SSC）的框架，能够在推理过程中识别并修正模型规范中的缺陷，使模型行为更加稳健。", "motivation": "语言模型易受上下文奖励操控的影响，它们会利用有缺陷或故障的书面规范或评分标准来获得高分，而并未真正满足用户的真实意图。", "method": "提出了一种名为规范自我修正（Specification Self-Correction，简称SSC）的新框架，该框架能够在推理阶段使语言模型识别并修正其指导规范中的缺陷。SSC框架通过多步骤的推理过程来操作语言模型，首先模型根据可能有缺陷的规范生成响应，然后对输出进行批评，接着修正规范本身以消除可利用的漏洞，最后使用纠正后的规范生成一个更稳健的响应。", "result": "实验结果表明，虽然模型最初在50-70%的情况下会利用有缺陷的规范来获得高分，但SSC过程能够将这种脆弱性减少超过90%。该过程在推理时刻动态修复，无需修改模型权重，从而实现更稳健的模型行为。", "conclusion": "该研究引入了一种新的框架SSC，以解决语言模型在有缺陷规范下可能产生的不当行为问题，并证明了该框架的有效性。"}}
{"id": "2507.18762", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18762", "abs": "https://arxiv.org/abs/2507.18762", "authors": ["Abdulhady Abas Abdullah", "Amir H. Gandomi", "Tarik A Rashid", "Seyedali Mirjalili", "Laith Abualigah", "Milena Živković", "Hadi Veisi"], "title": "The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages", "comment": null, "summary": "In natural language processing, multilingual models like mBERT and\nXLM-RoBERTa promise broad coverage but often struggle with languages that share\na script yet differ in orthographic norms and cultural context. This issue is\nespecially notable in Arabic-script languages such as Kurdish Sorani, Arabic,\nPersian, and Urdu. We introduce the Arabic Script RoBERTa (AS-RoBERTa) family:\nfour RoBERTa-based models, each pre-trained on a large corpus tailored to its\nspecific language. By focusing pre-training on language-specific script\nfeatures and statistics, our models capture patterns overlooked by\ngeneral-purpose models. When fine-tuned on classification tasks, AS-RoBERTa\nvariants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An\nablation study confirms that script-focused pre-training is central to these\ngains. Error analysis using confusion matrices shows how shared script traits\nand domain-specific content affect performance. Our results highlight the value\nof script-aware specialization for languages using the Arabic script and\nsupport further work on pre-training strategies rooted in script and language\nspecificity.", "AI": {"tldr": "研究团队提出了四种针对阿拉伯语系特定语言的RoBERTa变体模型AS-RoBERTa，证明了在特定语言的书写系统特征和文化背景中进行的预训练能够使得模型有更多的性能提升，这强调了模型对书写系统和语言特性的敏感性的重要性。", "motivation": "动机在于解决现有的多语言模型（如mBERT和XLM-RoBERTa）在处理使用相同书写系统但具有不同正字法规范和文化背景的语言（特别是阿拉伯语系的语言）时效果不佳的问题。", "method": "方法是提出了阿拉伯语书写系统RoBERTa（AS-RoBERTa）系列：四个基于RoBERTa的模型，每个模型都是在一个适合其特定语言的大语料库上进行了预训练。通过专门针对特定语言的书写系统特征和统计信息进行预训练，模型捕捉到了通用模型忽视的模式。", "result": "实验结果表明，当在分类任务中进行微调时，AS-RoBERTa系列模型相比于mBERT和XLM-RoBERTa具有2到5个百分点的性能提升。通过消融研究也确认了针对书写系统的预训练是这些改进建立的基础。", "conclusion": "研究结论证明了对于使用阿拉伯书写系统的语言来说，对书写系统敏感的专门化的重要性，同时也支持了未来研究基于书写系统和语言特性的预训练策略的工作。"}}
{"id": "2507.18769", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18769", "abs": "https://arxiv.org/abs/2507.18769", "authors": ["Nicole Lai-Lopez", "Lusha Wang", "Su Yuan", "Liza Zhang"], "title": "ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting", "comment": "16 pages, 5 figures, 3 tables,", "summary": "In this work, we introduce our solution for the Multilingual Text\nDetoxification Task in the PAN-2025 competition for the ylmmcl team: a robust\nmultilingual text detoxification pipeline that integrates lexicon-guided\ntagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and\nan iterative classifier-based gatekeeping mechanism. Our approach departs from\nprior unsupervised or monolingual pipelines by leveraging explicit toxic word\nannotation via the multilingual_toxic_lexicon to guide detoxification with\ngreater precision and cross-lingual generalization. Our final model achieves\nthe highest STA (0.922) from our previous attempts, and an average official J\nscore of 0.612 for toxic inputs in both the development and test sets. It also\nachieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance\noutperforms baseline and backtranslation methods across multiple languages, and\nshows strong generalization in high-resource settings (English, Russian,\nFrench). Despite some trade-offs in SIM, the model demonstrates consistent\nimprovements in detoxification strength. In the competition, our team achieved\nninth place with a score of 0.612.", "AI": {"tldr": "我们提出了一种多语言文本净化管道，结合了词典标记、细调模型和分类器门控，相较于传统方法显示出更好的精度和跨语言推广能力。在PAN-2025竞赛中，我们的实现获得第九名，证明了该方法的有效性。", "motivation": "我们希望通过开发一个更为精确和适应多种语言的去毒化管道，推动现有方法的进步，这些现有方法在此前大多为无监督或单语环境下的解决方案。", "method": "我们的方法结合了词典引导的标记、细调的序列到序列模型(s-nlp/mt0-xl-detox-orpo)，以及迭代分类器门控机制。我们利用多语言有毒词典(multilingual_toxic_lexicon)进行明确的有毒词注释，以此来增加去毒化的精度和跨语言的泛化能力。", "result": "最终的模型在开发集和测试集上的官方J评分为0.612，xCOMET评分为0.793（开发集）和0.787（测试集）。相比于基线方法和反向翻译方法，我们的模型在多种语言设置下表现出色，特别是在资源丰富的语言（如英文、俄语和法语）中展示了强大的泛化能力。尽管在相似度(SIM)上有些许妥协，但在去毒强度上总体有显著改进。", "conclusion": "我们的解决方案在PAN-2025多语种文本净化任务中取得了第九名的成绩，表明了该方法的有效性。尽管在某些指标上仍存在不足，但整体去毒效果得到显著提升。"}}
{"id": "2507.18791", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18791", "abs": "https://arxiv.org/abs/2507.18791", "authors": ["Yilun Yang", "Yekun Chai"], "title": "Evaluating Code-Mixing in LLMs Across 18 Languages", "comment": null, "summary": "Code-mixing, the practice of switching between languages within a\nconversation, presents unique challenges for traditional natural language\nprocessing. Existing benchmarks, such as LinCE and GLUECoS, are limited by\nnarrow language pairings and tasks, failing to adequately evaluate the\ncode-mixing capabilities of large language models (LLMs). Despite the\nsignificance of code-mixing for multilingual users, research on LLMs in this\ncontext remains limited. Additionally, current methods for generating\ncode-mixed data are underdeveloped. In this paper, we conduct a comprehensive\nevaluation of LLMs' performance on code-mixed data across 18 languages from\nseven language families. We also propose a novel approach for generating\nsynthetic code-mixed texts by combining word substitution with GPT-4 prompting.\nOur analysis reveals consistent underperformance of LLMs on code-mixed datasets\ninvolving multiple language families. We suggest that improvements in training\ndata size, model scale, and few-shot learning could enhance their performance.", "AI": {"tldr": "本文全面评估了大型语言模型在18种语言混合文本上的表现，并提出一种结合单词替换与GPT-4提示的新方法来生成合成混合文本。结果显示，模型在多语言家族的混合数据上表现出不足，建议通过增加训练数据量、模型规模和少样本学习方法提升性能。", "motivation": "由于现有基准测试和代码混合数据生成方法的局限性，本文旨在全面评估和改善大型语言模型在处理多语言混合文本时的能力。", "method": "本文提出一种结合单词替换和GPT-4提示的新型合成代码混合文本生成方法，并基于此方法对现有大型语言模型进行测试。", "result": "发现当前的大型语言模型在处理包含多种语言家族的混合数据时表现不佳。", "conclusion": "建议通过增加训练数据量、扩大模型规模以及改善少样本学习能力来提高模型在处理代码混合文本时的表现。"}}
{"id": "2507.18645", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18645", "abs": "https://arxiv.org/abs/2507.18645", "authors": ["Milan Maksimovic", "Anna Bohdanets", "Immaculate Motsi-Omoijiade", "Guido Governatori", "Ivan S. Maksymov"], "title": "Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis", "comment": null, "summary": "Prior work has demonstrated that incorporating well-known quantum tunnelling\n(QT) probability into neural network models effectively captures important\nnuances of human perception, particularly in the recognition of ambiguous\nobjects and sentiment analysis. In this paper, we employ novel QT-based neural\nnetworks and assess their effectiveness in distinguishing customised\nCIFAR-format images of military and civilian vehicles, as well as sentiment,\nusing a proprietary military-specific vocabulary. We suggest that QT-based\nmodels can enhance multimodal AI applications in battlefield scenarios,\nparticularly within human-operated drone warfare contexts, imbuing AI with\ncertain traits of human reasoning.", "AI": {"tldr": "本研究利用基于量子隧穿的新神经网络来改进对定制CIFAR格式的军事和民用车辆图像以及情感识别的精度，特别是在军事无人机操作环境中。", "motivation": "为了探讨量子隧穿在人工神经网络中的应用效果，特别是在区分军队和民用车辆图像及情感方面的应用潜力。", "method": "使用基于量子隧穿的新神经网络模型，评估其在识别定制CIFAR格式的军事和民用车辆图像及情感上的效果。", "result": "研究结果表明量子隧穿增强的神经网络可提升多模态AI应用的性能，特别是在军用无人机操作环境中。", "conclusion": "基于量子隧穿的模型可以赋予AI类似人类的决策能力，这在军用无人机操作的战场环境中有很大的应用潜力。"}}
{"id": "2507.18827", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18827", "abs": "https://arxiv.org/abs/2507.18827", "authors": ["Pranav Gupta"], "title": "CueBuddy: helping non-native English speakers navigate English-centric STEM education", "comment": null, "summary": "Students across the world in STEM classes, especially in the Global South,\nfall behind their peers who are more fluent in English, despite being at par\nwith them in terms of scientific prerequisites. While many of them are able to\nfollow everyday English at ease, key terms in English stay challenging. In most\ncases, such students have had most of their course prerequisites in a lower\nresource language. Live speech translation to lower resource languages is a\npromising area of research, however, models for speech translation can be too\nexpensive on a large scale and often struggle with technical content. In this\npaper, we describe CueBuddy, which aims to remediate these issues by providing\nreal-time \"lexical cues\" through technical keyword spotting along real-time\nmultilingual glossary lookup to help students stay up to speed with complex\nEnglish jargon without disrupting their concentration on the lecture. We also\ndescribe the limitations and future extensions of our approach.", "AI": {"tldr": "本论文介绍CueBuddy系统，通过实时技术关键词检测和多语言词汇查找来帮助低资源语言背景的学生理解英语专业术语，减少语言障碍。", "motivation": "在全球南部的学生中，尽管他们在科学先修课程上与英语流利的学生持平，但他们在STEM课程中落后，主要是由于英语关键词的理解不足。该论文旨在解决语言障碍，帮助非英语背景的学生更好地理解和跟上讲座内容。", "method": "本论文介绍了CueBuddy系统，该系统通过实时技术关键词检测和实时多语言词汇查找，在不影响学生对讲座集中注意力的情况下，帮助他们跟上复杂的英语专业术语。", "result": "尚未提供具体结果，但论文描述了CueBuddy系统的功能及其潜在影响，特别是在减少语言障碍方面帮助学生理解英语专业术语的效果。", "conclusion": "尽管论文提出了CueBuddy系统的概念及其对提升非英语流利学生理解英语专业术语的帮助，但也指出其局限性并期待未来改进和扩展。"}}
{"id": "2507.18647", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18647", "abs": "https://arxiv.org/abs/2507.18647", "authors": ["Rayyan Ridwan"], "title": "XAI-Guided Analysis of Residual Networks for Interpretable Pneumonia Detection in Paediatric Chest X-rays", "comment": "13 pages, 14 figures", "summary": "Pneumonia remains one of the leading causes of death among children\nworldwide, underscoring a critical need for fast and accurate diagnostic tools.\nIn this paper, we propose an interpretable deep learning model on Residual\nNetworks (ResNets) for automatically diagnosing paediatric pneumonia on chest\nX-rays. We enhance interpretability through Bayesian Gradient-weighted Class\nActivation Mapping (BayesGrad-CAM), which quantifies uncertainty in visual\nexplanations, and which offers spatial locations accountable for the\ndecision-making process of the model. Our ResNet-50 model, trained on a large\npaediatric chest X-rays dataset, achieves high classification accuracy\n(95.94%), AUC-ROC (98.91%), and Cohen's Kappa (0.913), accompanied by\nclinically meaningful visual explanations. Our findings demonstrate that high\nperformance and interpretability are not only achievable but critical for\nclinical AI deployment.", "AI": {"tldr": "本文提出了一种基于Residual Networks（ResNets）的可解释深度学习模型，用于自动诊断儿童肺炎的胸部X光片，并通过Bayesian Gradient-weighted Class Activation Mapping（BayesGrad-CAM）提高了模型的解释性。该模型在大规模儿童胸部X光片数据集上训练，取得了高分类精度、AUC-ROC和Cohen's Kappa，同时还提供了临床有意义的可视化解释。", "motivation": "肺炎仍然是全球儿童死亡的主要原因之一，这突显了快速准确诊断工具的迫切需求。", "method": "使用增强解释性的Bayesian Gradient-weighted Class Activation Mapping（BayesGrad-CAM）技术的ResNet-50深度学习模型，该技术可以量化视觉解释中的不确定性，并提供模型决策过程负责的空间位置。", "result": "模型在大规模儿童胸部X光片数据集上训练，达到了95.94%的高分类精度，AUC-ROC 98.91%，以及Cohen's Kappa 0.913，并提供了临床有意义的可视化解释。", "conclusion": "研究结果表明，高性能与解释性不仅是可以实现的，而且对于临床人工智能部署至关重要。"}}
{"id": "2507.18857", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18857", "abs": "https://arxiv.org/abs/2507.18857", "authors": ["Mohammad Kachuee", "Teja Gollapudi", "Minseok Kim", "Yin Huang", "Kai Sun", "Xiao Yang", "Jiaqi Wang", "Nirav Shah", "Yue Liu", "Aaron Colak", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "title": "PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning", "comment": null, "summary": "Retrieval-augmented generation (RAG) often falls short when retrieved context\nincludes confusing semi-relevant passages, or when answering questions require\ndeep contextual understanding and reasoning. We propose an efficient\nfine-tuning framework, called PrismRAG, that (i) trains the model with\ndistractor-aware QA pairs mixing gold evidence with subtle distractor passages,\nand (ii) instills reasoning-centric habits that make the LLM plan, rationalize,\nand synthesize without relying on extensive human engineered instructions.\nEvaluated across 12 open-book RAG QA benchmarks spanning diverse application\ndomains and scenarios, PrismRAG improves average factuality by 5.4%,\noutperforming state-of-the-art solutions.", "AI": {"tldr": "PrismRAG通过改进的微调方法，在包含干扰信息的环境下提高检索增强生成模型的回答质量和事实准确性。", "motivation": "为了提高检索增强生成模型的回答质量，尤其是在检索到的上下文包含误导性的半相关段落时，以及在需要深层次的上下文理解和推理时。", "method": "PrismRAG提出了一种高效的微调框架，该框架通过使用包含正确证据和微妙干扰段落的QA对进行训练，以及培养模型的推理习惯，使得大型语言模型能够在没有过度依赖人为指导的情况下进行规划、合理化和综合。", "result": "PrismRAG在各种应用领域和场景下的12个开放图书检索增强问答基准测试中表现优异，提高了回答的准确性。", "conclusion": "PrismRAG在12个开放图书检索增强问答基准上进行了评估，平均事实准确性提高了5.4%，超越了最先进的解决方案。"}}
{"id": "2507.18649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18649", "abs": "https://arxiv.org/abs/2507.18649", "authors": ["Haiyang Liu", "Xiaolin Hong", "Xuancheng Yang", "Yudi Ruan", "Xiang Lian", "Michael Lingelbach", "Hongwei Yi", "Wei Li"], "title": "Livatar-1: Real-Time Talking Heads Generation with Tailored Flow Matching", "comment": "Technical Report", "summary": "We present Livatar, a real-time audio-driven talking heads videos generation\nframework. Existing baselines suffer from limited lip-sync accuracy and\nlong-term pose drift. We address these limitations with a flow matching based\nframework. Coupled with system optimizations, Livatar achieves competitive\nlip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and\nreaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single\nA10 GPU. This makes high-fidelity avatars accessible to broader applications.\nOur project is available at https://www.hedra.com/ with with examples at\nhttps://h-liu1997.github.io/Livatar-1/", "AI": {"tldr": "我们提出了 Livatar，这是一个可以实现实时、高质量唇同步的讲话头像视频生成框架。这种框架能够支持实时生成高保真虚拟形象，并且适用于更广泛的应用场景。", "motivation": "现有的方法在生成讲话头像视频时面临唇同步不准确和长期姿势偏移的挑战。我们希望通过Livatar解决这些问题，提高唇同步质量和框架的性能。", "method": "我们提出了Livatar，这是一个基于流匹配的实时音频驱动的讲话头像视频生成框架。该框架解决了现有方法在唇同步准确性和长期姿势偏移上的问题。", "result": "Livatar 在 HDTF 数据集上达到了 8.50 唇同步置信度，达到了 141 FPS 的吞吐量，并且在单个 A10 GPU 上的端到端延迟仅为 0.17 秒。", "conclusion": "通过采用流匹配技术，Livatar 大大提高了唇同步质量和视频生成的实时性，使其在更多的应用中可以实现高保真头像的快速生成。"}}
{"id": "2507.18884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18884", "abs": "https://arxiv.org/abs/2507.18884", "authors": ["Ming Gong", "Xucheng Huang", "Ziheng Xu", "Vijayan K. Asari"], "title": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service", "comment": null, "summary": "High-quality dialogue is crucial for e-commerce customer service, yet\ntraditional intent-based systems struggle with dynamic, multi-turn\ninteractions. We present MindFlow+, a self-evolving dialogue agent that learns\ndomain-specific behavior by combining large language models (LLMs) with\nimitation learning and offline reinforcement learning (RL). MindFlow+\nintroduces two data-centric mechanisms to guide learning: tool-augmented\ndemonstration construction, which exposes the model to knowledge-enhanced and\nagentic (ReAct-style) interactions for effective tool use; and\nreward-conditioned data modeling, which aligns responses with task-specific\ngoals using reward signals. To evaluate the model's role in response\ngeneration, we introduce the AI Contribution Ratio, a novel metric quantifying\nAI involvement in dialogue. Experiments on real-world e-commerce conversations\nshow that MindFlow+ outperforms strong baselines in contextual relevance,\nflexibility, and task accuracy. These results demonstrate the potential of\ncombining LLMs tool reasoning, and reward-guided learning to build\ndomain-specialized, context-aware dialogue systems.", "AI": {"tldr": "MindFlow+是一种结合大型语言模型、模仿学习和离线强化学习的自我进化对话代理，旨在提升电子商务客服对话质量。", "motivation": "鉴于传统基于意图的系统在处理动态且多轮次的电子商务客服对话交互中存在困难，本研究旨在探索一种能够学习领域特定行为的自我进化对话代理——MindFlow+。", "method": "结合大型语言模型（LLMs）与模仿学习及离线强化学习（RL），MindFlow+提出两种数据导向的学习机制：一种是工具增强的示范构建，暴露模型于知识增强和代理式交互中，以促进有效工具使用；另一种是基于奖励的数据建模，使用奖励信号对齐与任务特定目标相关的回应。", "result": "实验结果展示，MindFlow+在处理电商平台的对话任务时，在上下文相关性、灵活性及任务准确性方面，相较于强基线模型有显著提高。此外，AI贡献率作为一个新的度量标准，用于量化AI在对话生成中的参与程度。", "conclusion": "实验表明，MindFlow+在现实世界中的电商对话中，相较于强基线模型，在上下文的相关性、灵活性和任务准确性方面表现出色，展示了结合大型语言模型工具推理和奖励导向学习构建领域专用、上下文感知对话系统的潜力。"}}
{"id": "2507.18650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18650", "abs": "https://arxiv.org/abs/2507.18650", "authors": ["Venant Niyonkuru", "Sylla Sekou", "Jimmy Jackson Sinzinkayo"], "title": "Features extraction for image identification using computer vision", "comment": null, "summary": "This study examines various feature extraction techniques in computer vision,\nthe primary focus of which is on Vision Transformers (ViTs) and other\napproaches such as Generative Adversarial Networks (GANs), deep feature models,\ntraditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive\nfeature models. Emphasizing ViTs, the report summarizes their architecture,\nincluding patch embedding, positional encoding, and multi-head self-attention\nmechanisms with which they overperform conventional convolutional neural\nnetworks (CNNs). Experimental results determine the merits and limitations of\nboth methods and their utilitarian applications in advancing computer vision.", "AI": {"tldr": "This study compares Vision Transformers (ViTs) and other feature extraction techniques used in computer vision, focusing on ViTs' architecture and performance compared to traditional methods like CNNs.", "motivation": "The motivation is to compare and summarize different feature extraction techniques with a particular focus on Vision Transformers (ViTs) to understand their advantages over conventional CNNs.", "method": "This study examines various feature extraction techniques in computer vision, focusing on Vision Transformers (ViTs) and other approaches such as GANs, deep feature models, traditional feature detectors, and contrastive/non-contrastive models. It emphasizes the architecture of ViTs including patch embedding, positional encoding, and multi-head self-attention mechanisms.", "result": "Experimental results highlight the strengths and weaknesses of the methods discussed and illustrate their practical applications in advancing computer vision.", "conclusion": "The conclusion suggests that ViTs offer significant improvements in performance over traditional feature extraction methods like CNNs, making them an exciting area for further research and application in computer vision."}}
{"id": "2507.18890", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18890", "abs": "https://arxiv.org/abs/2507.18890", "authors": ["Jonathan Ivey", "Susan Gauch", "David Jurgens"], "title": "NUTMEG: Separating Signal From Noise in Annotator Disagreement", "comment": null, "summary": "NLP models often rely on human-labeled data for training and evaluation. Many\napproaches crowdsource this data from a large number of annotators with varying\nskills, backgrounds, and motivations, resulting in conflicting annotations.\nThese conflicts have traditionally been resolved by aggregation methods that\nassume disagreements are errors. Recent work has argued that for many tasks\nannotators may have genuine disagreements and that variation should be treated\nas signal rather than noise. However, few models separate signal and noise in\nannotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model\nthat incorporates information about annotator backgrounds to remove noisy\nannotations from human-labeled training data while preserving systematic\ndisagreements. Using synthetic data, we show that NUTMEG is more effective at\nrecovering ground-truth from annotations with systematic disagreement than\ntraditional aggregation methods. We provide further analysis characterizing how\ndifferences in subpopulation sizes, rates of disagreement, and rates of spam\naffect the performance of our model. Finally, we demonstrate that downstream\nmodels trained on NUTMEG-aggregated data significantly outperform models\ntrained on data from traditionally aggregation methods. Our results highlight\nthe importance of accounting for both annotator competence and systematic\ndisagreements when training on human-labeled data.", "AI": {"tldr": "The paper proposes NUTMEG, a Bayesian model that distinguishes signal from noise in annotator disagreements, enhancing the quality of human-labeled training data by retaining systematic disagreements while filtering out noise, which improves downstream model performance compared to traditional methods which treat disagreements as errors.", "motivation": "The motivation behind the paper is to address the challenges posed by conflicting annotations from different human annotators, which are traditionally treated as errors and manually handled. The authors argue that systematic disagreements should be preserved as they contain valuable information.", "method": "The method introduced in the paper is NUTMEG, a Bayesian model that leverages annotator background information to distinguish between signal (systematic disagreement) and noise (random errors) in annotated data, thereby improving the overall quality of the training data used for NLP models.", "result": "The results show that NUTMEG achieves superior performance in recovering true labels from annotations that contain systematic disagreements through experiments on synthetic data. Additionally, models trained on data aggregated by NUTMEG outperform those trained on data processed by traditional aggregation methods.", "conclusion": "The conclusion drawn from the paper is that incorporating models like NUTMEG into the data processing stage can significantly enhance the effectiveness of NLP tasks by properly identifying and accounting for annotator competence and systematic disagreements in human-labeled data."}}
{"id": "2507.18653", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18653", "abs": "https://arxiv.org/abs/2507.18653", "authors": ["Mohammed Abdul Hafeez Khan", "Parth Ganeriwala", "Sarah M. Lehman", "Siddhartha Bhattacharyya", "Amy Alvarez", "Natasha Neogi"], "title": "Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane Detection under Distribution Shift", "comment": "Accepted to ICCV 2025, 2COOOL Workshop. Total 14 pages, 5 tables, and\n  4 figures", "summary": "Lane detection models are often evaluated in a closed-world setting, where\ntraining and testing occur on the same dataset. We observe that, even within\nthe same domain, cross-dataset distribution shifts can cause severe\ncatastrophic forgetting during fine-tuning. To address this, we first train a\nbase model on a source distribution and then adapt it to each new target\ndistribution by creating separate branches, fine-tuning only selected\ncomponents while keeping the original source branch fixed. Based on a\ncomponent-wise analysis, we identify effective fine-tuning strategies for\ntarget distributions that enable parameter-efficient adaptation. At inference\ntime, we propose using a supervised contrastive learning model to identify the\ninput distribution and dynamically route it to the corresponding branch. Our\nframework achieves near-optimal F1-scores while using significantly fewer\nparameters than training separate models for each distribution.", "AI": {"tldr": "论文提出了一种新的方法来应对跨数据集分布变化时的灾难性遗忘问题，通过参数高效的多分枝模型以及动态路由机制，实现了良好的车道检测性能。", "motivation": "观察到即使在同一个领域内，跨数据集的分布变化也会导致在微调过程中的灾难性遗忘问题。为了解决这个问题，提出了基于多分枝架构的方法来参数高效地适应目标分布。", "method": "首先对基础模型在源分布上进行训练，然后针对每个新的目标分布创建独立的分枝，仅微调选定的组件，固定原有的源分布分枝。利用组件分析确定目标分布的有效微调策略，并在推理时通过监督对比学习模型识别输入分布并动态路由到相应的分枝。", "result": "该论文提出了一种参数高效的适应目标分布的方法，通过在基础模型上添加分枝进行参数微调，并使用监督对比学习模型在推理时选择对应的分枝，实现了跨数据集分布变化时的高性能车道检测。", "conclusion": "该框架在使用远少于为每个分布训练独立模型的参数下，实现了接近最优的F1得分。"}}
{"id": "2507.18901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18901", "abs": "https://arxiv.org/abs/2507.18901", "authors": ["Chuxuan Hu", "Liyun Zhang", "Yeji Lim", "Aum Wadhwani", "Austin Peters", "Daniel Kang"], "title": "REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?", "comment": "Accepted to ACL 2025 Findings", "summary": "Assessing the reproducibility of social science papers is essential for\npromoting rigor in research processes, but manual assessment is costly. With\nrecent advances in agentic AI systems (i.e., AI agents), we seek to evaluate\ntheir capability to automate this process. However, existing benchmarks for\nreproducing research papers (1) focus solely on reproducing results using\nprovided code and data without assessing their consistency with the paper, (2)\noversimplify real-world scenarios, and (3) lack necessary diversity in data\nformats and programming languages. To address these issues, we introduce\nREPRO-Bench, a collection of 112 task instances, each representing a social\nscience paper with a publicly available reproduction report. The agents are\ntasked with assessing the reproducibility of the paper based on the original\npaper PDF and the corresponding reproduction package. REPRO-Bench features\nend-to-end evaluation tasks on the reproducibility of social science papers\nwith complexity comparable to real-world assessments. We evaluate three\nrepresentative AI agents on REPRO-Bench, with the best-performing agent\nachieving an accuracy of only 21.4%. Building on our empirical analysis, we\ndevelop REPRO-Agent, which improves the highest accuracy achieved by existing\nagents by 71%. We conclude that more advanced AI agents should be developed to\nautomate real-world reproducibility assessment. REPRO-Bench is publicly\navailable at https://github.com/uiuc-kang-lab/REPRO-Bench.", "AI": {"tldr": "研究评估了AI代理在自动化社会科学研究论文可重复性评估中的能力，并提出了一个名为REPRO-Bench的新基准。结果显示现有AI代理的表现有限，该研究开发了一种名为REPRO-Agent的代理，显著提高了准确率。", "motivation": "解决现有评估基准过于简化、数据格式和编程语言种类单一的问题，推动社会科学研究中的可重复性评估工作。", "method": "提出一个新的评估基准REPRO-Bench，包含112个社会科学研究论文的实例，用以测试AI代理在评估文献可重复性方面的性能。", "result": "测试发现最好的现有AI代理在REPRO-Bench上的准确率为21.4%，新开发的REPRO-Agent提高了该准确率71%。", "conclusion": "需要开发更先进的AI代理以实现对社会科学研究论文可重复性的自动化评估。"}}
{"id": "2507.18655", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.18655", "abs": "https://arxiv.org/abs/2507.18655", "authors": ["James Dickens", "Kamyar Hamad"], "title": "Part Segmentation of Human Meshes via Multi-View Human Parsing", "comment": null, "summary": "Recent advances in point cloud deep learning have led to models that achieve\nhigh per-part labeling accuracy on large-scale point clouds, using only the raw\ngeometry of unordered point sets. In parallel, the field of human parsing\nfocuses on predicting body part and clothing/accessory labels from images. This\nwork aims to bridge these two domains by enabling per-vertex semantic\nsegmentation of large-scale human meshes. To achieve this, a pseudo-ground\ntruth labeling pipeline is developed for the Thuman2.1 dataset: meshes are\nfirst aligned to a canonical pose, segmented from multiple viewpoints, and the\nresulting point-level labels are then backprojected onto the original mesh to\nproduce per-point pseudo ground truth annotations. Subsequently, a novel,\nmemory-efficient sampling strategy is introduced, a windowed iterative farthest\npoint sampling (FPS) with space-filling curve-based serialization to\neffectively downsample the point clouds. This is followed by a purely geometric\nsegmentation using PointTransformer, enabling semantic parsing of human meshes\nwithout relying on texture information. Experimental results confirm the\neffectiveness and accuracy of the proposed approach.", "AI": {"tldr": "本文通过结合点云深度学习方法与人体解析技术，提出了一个有效的几何分割策略，实现了对人体网格的精确顶点级语义解析。", "motivation": "本文旨在通过开发一种伪地面真实标记管道，将点云深度学习与人体解析领域相结合，以实现大规模人体网格的顶点级语义分割。", "method": "本文提出了一种新的内存高效抽样策略和基于几何的分割方法，使用窗口化迭代最远点采样（FPS）结合空间填充曲线进行序列化，有效地缩小了点云的规模，并使用PointTransformer实现了仅基于几何的人体网格语义分割。", "result": "实验结果证明了所提出方法的有效性和准确性。", "conclusion": "研究结论表明，通过优化的数据标注流程和创新的抽样策略，可以实现有效的大规模人体网格的几何语义分割，无需依赖纹理信息。"}}
{"id": "2507.18902", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18902", "abs": "https://arxiv.org/abs/2507.18902", "authors": ["Hongyuan Lu", "Zixuan Li", "Zefan Zhang", "Wai Lam"], "title": "SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models", "comment": null, "summary": "There are more than 7,000 languages around the world, and current Large\nLanguage Models (LLMs) only support hundreds of languages. Dictionary-based\nprompting methods can enhance translation on them, but most methods use all the\navailable dictionaries, which could be expensive. Instead, it will be flexible\nto have a trade-off between token consumption and translation performance. This\npaper proposes a novel task called \\textbf{A}utomatic \\textbf{D}ictionary\n\\textbf{S}election (\\textbf{ADS}). The goal of the task is to automatically\nselect which dictionary to use to enhance translation. We propose a novel and\neffective method which we call \\textbf{S}elect \\textbf{Lo}w-frequency\n\\textbf{W}ords! (\\textbf{SLoW}) which selects those dictionaries that have a\nlower frequency. Our methods have unique advantages. First, there is no need\nfor access to the training data for frequency estimation (which is usually\nunavailable). Second, it inherits the advantage of dictionary-based methods,\nwhere no additional tuning is required on LLMs. Experimental results on 100\nlanguages from FLORES indicate that SLoW surpasses strong baselines, and it can\nobviously save token usage, with many languages even surpassing the translation\nperformance of the full dictionary baseline.\\footnote{A shocking fact is that\nthere is no need to use the actual training data (often unobtainable) for\nfrequency estimation, and an estimation frequency obtained using public\nresources is still apparently effective in improving translation with ChatGPT\nand Llama, and DeepSeek.}\\footnote{Code and data available upon publication.}", "AI": {"tldr": "This paper introduces a method named SLoW that automatically selects dictionaries based on low-frequency words to enhance translation performance while saving tokens in LLMs.", "motivation": "The motivation is to address the issue of token consumption versus translation performance in enhancing translation capabilities of LLMs using dictionary-based prompting methods without the need for full dictionary usage.", "method": "The paper proposes a novel task called Automatic Dictionary Selection (ADS) and a method named Select Low-frequency Words! (SLoW). This method selects dictionaries based on lower word frequency, providing advantages such as no need for training data and no requirement for additional tuning of LLMs.", "result": "Experimental results on 100 languages from FLORES show that the SLoW method outperforms strong baselines, significantly saving tokens while often achieving comparable or superior translation performance to using the full dictionary.", "conclusion": "The conclusion suggests that the proposed SLoW method effectively balances the trade-off between token usage and translation performance, surpassing baseline methods. It also highlights the effectiveness of using public resources for frequency estimation without the need for actual training data."}}
{"id": "2507.18656", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18656", "abs": "https://arxiv.org/abs/2507.18656", "authors": ["Muhammad Zaeem Shahzad", "Muhammad Abdullah Hanif", "Bassem Ouni", "Muhammad Shafique"], "title": "ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems", "comment": "8 pages, 8 figures, 1 table", "summary": "Advanced Driver Assistance Systems (ADAS) significantly enhance road safety\nby detecting potential collisions and alerting drivers. However, their reliance\non expensive sensor technologies such as LiDAR and radar limits accessibility,\nparticularly in low- and middle-income countries. Machine learning-based ADAS\n(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera\ninput, offers a cost-effective alternative. Critical to ML-ADAS is the\ncollision avoidance feature, which requires the ability to detect objects and\nestimate their distances accurately. This is achieved with specialized DNNs\nlike YOLO, which provides real-time object detection, and a lightweight,\ndetection-wise distance estimation approach that relies on key features\nextracted from the detections like bounding box dimensions and size. However,\nthe robustness of these systems is undermined by security vulnerabilities in\nobject detectors. In this paper, we introduce ShrinkBox, a novel backdoor\nattack targeting object detection in collision avoidance ML-ADAS. Unlike\nexisting attacks that manipulate object class labels or presence, ShrinkBox\nsubtly shrinks ground truth bounding boxes. This attack remains undetected in\ndataset inspections and standard benchmarks while severely disrupting\ndownstream distance estimation. We demonstrate that ShrinkBox can be realized\nin the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with\nonly a 4% poisoning ratio in the training instances of the KITTI dataset.\nFurthermore, given the low error targets introduced in our relaxed poisoning\nstrategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in\ndownstream distance estimation by more than 3x on poisoned samples, potentially\nresulting in delays or prevention of collision warnings altogether.", "AI": {"tldr": "本论文提出了一种新的后门攻击方法ShrinkBox，用于攻击基于机器学习的自动驾驶辅助系统中的物体检测功能。这种攻击通过缩小真实物体的边界框来实现，并且其效果难以在数据集检查和标准基准测试中发现。实验表明，当使用ShrinkBox攻击时，能够显著增加下游距离估计的误差，从而可能导致碰撞警告的延迟或完全失效问题。", "motivation": "鉴于基于机器学习的ADAS系统在依赖于深度神经网络（DNNs）进行实时物体检测时存在安全漏洞，本研究旨在揭示并验证一种新颖的后门攻击方法。", "method": "通过使用名为ShrinkBox的新后门攻击方法，该方法专门针对ADAS系统中的物体检测功能。此方法不是改变物体的分类标签或存在性，而是微妙地缩小了真实物体的边界框。", "result": "实验结果显示，ShrinkBox攻击能够成功地在YOLOv9m物体检测器中实施，攻击成功率（ASR）达到了96%，并且仅需4%的训练集中毒比例。进一步地，这种攻击导致下游距离估计的平均绝对误差（MAE）增加了超过3倍。", "conclusion": "研究证实了ShrinkBox攻击的有效性及其对ADAS系统潜在的安全威胁，突显了加强系统安全性及防御此类攻击的必要性。"}}
{"id": "2507.18905", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18905", "abs": "https://arxiv.org/abs/2507.18905", "authors": ["Rachel L. Draelos", "Samina Afreen", "Barbara Blasko", "Tiffany Brazile", "Natasha Chase", "Dimple Desai", "Jessica Evert", "Heather L. Gardner", "Lauren Herrmann", "Aswathy Vaikom House", "Stephanie Kass", "Marianne Kavan", "Kirshma Khemani", "Amanda Koire", "Lauren M. McDonald", "Zahraa Rabeeah", "Amy Shah"], "title": "Large language models provide unsafe answers to patient-posed medical questions", "comment": "20 pages", "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools.", "AI": {"tldr": "一项新的医师主导的研究表明，在一个涵盖内部医学、女性健康和儿科初级护理领域的数据集HealthAdvice上，四个公开的医学聊天机器人的安全性存在显著差异，聊天机器人回应中可能包含对患者安全存在潜在威胁的内容。", "motivation": "研究动机是由于大量患者正在频繁使用大型语言模型聊天机器人获取医疗建议，这引起了对患者安全性的担忧。这项研究旨在通过比较四个知名聊天机器人的安全性，以揭示潜在风险，保障患者安全。", "method": "该研究采用了一个新的评估框架，对四个公开可用的聊天机器人（由Anthropic、Google、OpenAI和Meta提供的Claude、Gemini、GPT-4o和Llama3-70B）进行了安全方面的比较，使用的数据集为HealthAdvice，涵盖了内部医学、女性健康和儿科等初级护理领域。总计评估了888个聊天机器人的回答，回应了222个患者提出的医学建议问题。", "result": "研究发现，聊天机器人之间的安全性存在统计学上的显著差异。有问题的回答比例从21.6%（Claude）到43.2%（Llama）不等，而潜在危险的回答比例从5%（Claude）到13%（GPT-4o，Llama）不等。", "conclusion": "这项研究建议，数百万患者可能正从公开可用的聊天机器人中获取不安全的医疗建议，进一步的工作需要改善这些强大工具的临床安全性。"}}
{"id": "2507.18657", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18657", "abs": "https://arxiv.org/abs/2507.18657", "authors": ["Zehui Zhao", "Laith Alzubaidi", "Haider A. Alwzwazy", "Jinglan Zhang", "Yuantong Gu"], "title": "VGS-ATD: Robust Distributed Learning for Multi-Label Medical Image Classification Under Heterogeneous and Imbalanced Conditions", "comment": "15 pages, 8 figures, 6 tables", "summary": "In recent years, advanced deep learning architectures have shown strong\nperformance in medical imaging tasks. However, the traditional centralized\nlearning paradigm poses serious privacy risks as all data is collected and\ntrained on a single server. To mitigate this challenge, decentralized\napproaches such as federated learning and swarm learning have emerged, allowing\nmodel training on local nodes while sharing only model weights. While these\nmethods enhance privacy, they struggle with heterogeneous and imbalanced data\nand suffer from inefficiencies due to frequent communication and the\naggregation of weights. More critically, the dynamic and complex nature of\nclinical environments demands scalable AI systems capable of continuously\nlearning from diverse modalities and multilabels. Yet, both centralized and\ndecentralized models are prone to catastrophic forgetting during system\nexpansion, often requiring full model retraining to incorporate new data. To\naddress these limitations, we propose VGS-ATD, a novel distributed learning\nframework. To validate VGS-ATD, we evaluate it in experiments spanning 30\ndatasets and 80 independent labels across distributed nodes, VGS-ATD achieved\nan overall accuracy of 92.7%, outperforming centralized learning (84.9%) and\nswarm learning (72.99%), while federated learning failed under these conditions\ndue to high requirements on computational resources. VGS-ATD also demonstrated\nstrong scalability, with only a 1% drop in accuracy on existing nodes after\nexpansion, compared to a 20% drop in centralized learning, highlighting its\nresilience to catastrophic forgetting. Additionally, it reduced computational\ncosts by up to 50% relative to both centralized and swarm learning, confirming\nits superior efficiency and scalability.", "AI": {"tldr": "The study introduces VGS-ATD, a novel distributed learning framework that boosts privacy, scalability, and resilience to catastrophic forgetting, demonstrating superior performance in complex medical imaging scenarios over traditional learning approaches.", "motivation": "The motivation for VGS-ATD arises from the privacy challenges in centralized learning, inefficiencies and limitations in federated and swarm learning when dealing with heterogeneous and imbalanced data, and the need in clinical settings for AI systems that can continuously learn from diverse information sources and multiple labels without forgetting previously learned information.", "method": "The paper proposes VGS-ATD, a new distributed learning framework designed to address privacy, scalability, and catastrophic forgetting issues present in both centralized and decentralized learning methods. It operates by training models on local nodes and sharing only model weights while aiming to overcome the limitations of frequent communication and heterogeneous data encountered in federated and swarm learning.", "result": "VGS-ATD outperforms both centralized and swarm learning in terms of accuracy (92.7%) and is more resilient to catastrophic forgetting with only a 1% drop in accuracy post-expansion, compared to a 20% drop in centralized learning. Additionally, it showed a significant reduction in computational costs by up to 50% compared to centralized and swarm learning methods.", "conclusion": "The conclusion is that VGS-ATD is a more efficient, scalable, and privacy-preserving distributed learning framework compared to conventional centralized and decentralized learning methods, making it particularly suitable for complex clinical scenarios."}}
{"id": "2507.18910", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18910", "abs": "https://arxiv.org/abs/2507.18910", "authors": ["Agada Joseph Oche", "Ademola Glory Folashade", "Tirthankar Ghosal", "Arpan Biswas"], "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions", "comment": "33 pages, 2 figures", "summary": "Retrieval-Augmented Generation (RAG) represents a major advancement in\nnatural language processing (NLP), combining large language models (LLMs) with\ninformation retrieval systems to enhance factual grounding, accuracy, and\ncontextual relevance. This paper presents a comprehensive systematic review of\nRAG, tracing its evolution from early developments in open domain question\nanswering to recent state-of-the-art implementations across diverse\napplications. The review begins by outlining the motivations behind RAG,\nparticularly its ability to mitigate hallucinations and outdated knowledge in\nparametric models. Core technical components-retrieval mechanisms,\nsequence-to-sequence generation models, and fusion strategies are examined in\ndetail. A year-by-year analysis highlights key milestones and research trends,\nproviding insight into RAG's rapid growth. The paper further explores the\ndeployment of RAG in enterprise systems, addressing practical challenges\nrelated to retrieval of proprietary data, security, and scalability. A\ncomparative evaluation of RAG implementations is conducted, benchmarking\nperformance on retrieval accuracy, generation fluency, latency, and\ncomputational efficiency. Persistent challenges such as retrieval quality,\nprivacy concerns, and integration overhead are critically assessed. Finally,\nthe review highlights emerging solutions, including hybrid retrieval\napproaches, privacy-preserving techniques, optimized fusion strategies, and\nagentic RAG architectures. These innovations point toward a future of more\nreliable, efficient, and context-aware knowledge-intensive NLP systems.", "AI": {"tldr": "The paper conducts a comprehensive review of Retrieval-Augmented Generation (RAG) covering its evolution, technical details, practical challenges, and future innovations, with a focus on enhancing factual grounding, accuracy, and contextual relevance in NLP systems.", "motivation": "The main motivation is to provide a detailed understanding of RAG's advancements, addressing its capability to mitigate hallucinations and outdated knowledge in parametric models. It also aims to evaluate practical challenges and emerging solutions.", "method": "The paper adopts a systematic review method, examining the evolution of Retrieval-Augmented Generation (RAG) from its origins in open-domain question answering to its current state-of-the-art implementations, and explores its deployment across diverse applications.", "result": "The review outlines key technical components of RAG, such as retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies. It provides yearly analysis, highlights research trends, and benchmarks RAG implementations for retrieval accuracy, generation fluency, latency, and computational efficiency.", "conclusion": "The conclusion points out that future directions for RAG include hybrid retrieval approaches, privacy-preserving techniques, optimized fusion strategies, and agentic RAG architectures, leading to more reliable, efficient, and context-aware NLP systems."}}
{"id": "2507.18660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18660", "abs": "https://arxiv.org/abs/2507.18660", "authors": ["Adilet Yerkin", "Ayan Igali", "Elnara Kadyrgali", "Maksat Shagyrov", "Malika Ziyada", "Muragul Muratbekova", "Pakizar Shamoi"], "title": "Fuzzy Theory in Computer Vision: A Review", "comment": "Submitted to Journal of Intelligent and Fuzzy Systems for\n  consideration (8 pages, 6 figures, 1 table)", "summary": "Computer vision applications are omnipresent nowadays. The current paper\nexplores the use of fuzzy logic in computer vision, stressing its role in\nhandling uncertainty, noise, and imprecision in image data. Fuzzy logic is able\nto model gradual transitions and human-like reasoning and provides a promising\napproach to computer vision. Fuzzy approaches offer a way to improve object\nrecognition, image segmentation, and feature extraction by providing more\nadaptable and interpretable solutions compared to traditional methods. We\ndiscuss key fuzzy techniques, including fuzzy clustering, fuzzy inference\nsystems, type-2 fuzzy sets, and fuzzy rule-based decision-making. The paper\nalso discusses various applications, including medical imaging, autonomous\nsystems, and industrial inspection. Additionally, we explore the integration of\nfuzzy logic with deep learning models such as convolutional neural networks\n(CNNs) to enhance performance in complex vision tasks. Finally, we examine\nemerging trends such as hybrid fuzzy-deep learning models and explainable AI.", "AI": {"tldr": "The paper examines the role of fuzzy logic in computer vision, offering improvements over traditional methods by addressing uncertainty and integrating with deep learning models.", "motivation": "The motivation is to improve the adaptability and interpretability of solutions in computer vision tasks like object recognition, image segmentation, and feature extraction by leveraging fuzzy logic.", "method": "Content explores the use of fuzzy logic in computer vision, focusing on handling uncertainty, noise, and imprecision in image data through various fuzzy techniques. It discusses integrating fuzzy logic with deep learning models to enhance performance.", "result": "The paper discusses the applications of fuzzy logic in medical imaging, autonomous systems, and industrial inspection, and explores the integration of fuzzy logic with deep learning models, such as CNNs.", "conclusion": "The conclusion highlights the emergence of hybrid fuzzy-deep learning models and explainable AI as promising trends for enhancing performance and interpretability in complex computer vision tasks."}}
{"id": "2507.18915", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18915", "abs": "https://arxiv.org/abs/2507.18915", "authors": ["Ananya Sahu", "Amith Ananthram", "Kathleen McKeown"], "title": "Mining Contextualized Visual Associations from Images for Creativity Understanding", "comment": null, "summary": "Understanding another person's creative output requires a shared language of\nassociation. However, when training vision-language models such as CLIP, we\nrely on web-scraped datasets containing short, predominantly literal, alt-text.\nIn this work, we introduce a method for mining contextualized associations for\nsalient visual elements in an image that can scale to any unlabeled dataset.\nGiven an image, we can use these mined associations to generate high quality\ncreative captions at increasing degrees of abstraction. With our method, we\nproduce a new dataset of visual associations and 1.7m creative captions for the\nimages in MSCOCO. Human evaluation confirms that these captions remain visually\ngrounded while exhibiting recognizably increasing abstraction. Moreover,\nfine-tuning a visual encoder on this dataset yields meaningful improvements in\nzero-shot image-text retrieval in two creative domains: poetry and metaphor\nvisualization. We release our dataset, our generation code and our models for\nuse by the broader community.", "AI": {"tldr": "我们提出了一种方法，能够从图像中挖掘出显着视觉元素的上下文相关性，生成具有不同程度抽象性的创意标题。这种方法可以应用到任何未标注的数据集，并证明在创意领域的零样本图像-文本检索中，通过我们的数据集微调模型能够取得显著的改进。", "motivation": "理解他人的创意产出需要共享的联想语言。然而，现有的视觉-语言模型训练依赖于短的、主要是字面意义的网络刮取数据。因此，我们的动机是开发一种方法，可以在不依赖现有标注数据的情况下，挖掘图像中显着视觉元素的上下文相关性，以生成创意标题。", "method": "我们的方法是通过挖掘图像中显着视觉元素的上下文相关性，从而生成具有不同程度抽象性的高质量创意标题。这种方法可以扩展到任何未标记的数据集。", "result": "我们生成了一个新的视觉关联数据集，并为MSCOCO中的图像创建了170万个创意标题。人类评估表明，这些标题保持了视觉基础，并显示出了可识别的抽象程度的增加。此外，通过在这个数据集上微调视觉编码器，我们在诗歌和比喻可视化这两个创造性领域中的零样本图像-文本检索方面取得了有意义的改进。", "conclusion": "我们的工作表明，通过挖掘图像中的上下文相关性，我们可以生成高质量的创意标题，并且这种方法可以改善模型在创意领域中的零样本图像-文本检索性能。"}}
{"id": "2507.18661", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18661", "abs": "https://arxiv.org/abs/2507.18661", "authors": ["Ruixing Zhang", "Yang Zhang", "Tongyu Zhu", "Leilei Sun", "Weifeng Lv"], "title": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back", "comment": null, "summary": "Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches.", "AI": {"tldr": "研究利用视觉语言模型（VLM）进行轨迹预测的新方法，实验表明其在不同城市数据集上的性能最优并具有良好的泛化能力。", "motivation": "现有大多数下一个位置预测模型没有像人类那样利用道路连接性和移动趋势进行推理，而新的视觉语言模型的发展提供了新的可能性。研究旨在探索通过渲染轨迹和道路网络到图像上，结合VLM的推理能力，进行轨迹预测的新方法。", "method": "该研究首先提出了Vision-Guided Location Search (VGLS)方法，评估了未经参数修改的一般VLM对轨迹推理的能力。在此基础上，提出了由两个阶段组成的VLMLocPredictor方法：第一阶段设计了两个监督微调任务，提高VLM对道路网络和轨迹结构的理解及其基本推理能力；第二阶段引入了视觉地图反馈的强化学习，增强了模型的下一个位置预测能力。", "result": "本研究通过将道路网络和轨迹可视化为图像，并利用视觉语言模型（VLM）的推理能力，探索了轨迹预测的新方法。实验结果表明，该方法在多个城市的数据集上达到了当前最优性能，并且具有良好的跨城市泛化能力。", "conclusion": "VLMLocPredictor通过两个阶段的方法（监督微调和基于视觉地图反馈的强化学习）实现了在不修改VLM内部参数的情况下，提高了下一个位置预测能力，并展示了出色的跨城市泛化效果。"}}
{"id": "2507.18918", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18918", "abs": "https://arxiv.org/abs/2507.18918", "authors": ["Richmond Sin Jing Xuan", "Jalil Huseynov", "Yang Zhang"], "title": "Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders", "comment": null, "summary": "Multilingual large language models (LLMs) exhibit strong cross-linguistic\ngeneralization, yet medium to low resource languages underperform on common\nbenchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation\npatterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese\n(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource\nlanguages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam\n(ml), and Hindi (hi), with English (en) as the reference. Using Sparse\nAutoencoders (SAEs), we reveal systematic disparities in activation patterns.\nMedium to low resource languages receive up to 26.27 percent lower activations\nin early layers, with a persistent gap of 19.89 percent in deeper layers. To\naddress this, we apply activation-aware fine-tuning via Low-Rank Adaptation\n(LoRA), leading to substantial activation gains, such as 87.69 percent for\nMalayalam and 86.32 percent for Hindi, while maintaining English retention at\napproximately 91 percent. After fine-tuning, benchmark results show modest but\nconsistent improvements, highlighting activation alignment as a key factor in\nenhancing multilingual LLM performance.", "AI": {"tldr": "研究了多语言大模型中激活模式的系统性差异，并通过细调方法有效提高了中低资源语言的性能。", "motivation": "多语言大语言模型在跨语言泛化方面表现出色，但在中低资源语言上的性能较差。本研究试图通过分析激活模式来解决这一问题。", "method": "使用稀疏自编码器（SAEs）分析了Gemma-2-2B模型中26个残差层和10种语言的激活模式，揭示了系统性的差异，并通过低秩适应（LoRA）的激活感知细调来解决这些问题。", "result": "中低资源语言在早期层的激活降低了最多26.27%，而在深层的差距持续存在，为19.89%。经过细调后，马拉雅拉姆语的激活提高了87.69%，印地语的激活提高了86.32%，同时保留了约91%的英语性能。基准测试结果显示，细调后中低资源语言的性能略有但一致地提高。", "conclusion": "激活模式的对齐是提高多语言大语言模型特别是中低资源语言性能的关键因素。"}}
{"id": "2507.18667", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18667", "abs": "https://arxiv.org/abs/2507.18667", "authors": ["Nicholas Fidalgo", "Aaron Contreras", "Katherine Harvey", "Johnny Ni"], "title": "Gen-AI Police Sketches with Stable Diffusion", "comment": null, "summary": "This project investigates the use of multimodal AI-driven approaches to\nautomate and enhance suspect sketching. Three pipelines were developed and\nevaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model\nintegrated with a pre-trained CLIP model for text-image alignment, and (3)\nnovel approach incorporating LoRA fine-tuning of the CLIP model, applied to\nself-attention and cross-attention layers, and integrated with Stable\nDiffusion. An ablation study confirmed that fine-tuning both self- and\ncross-attention layers yielded the best alignment between text descriptions and\nsketches. Performance testing revealed that Model 1 achieved the highest\nstructural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of\n25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced\nperceptual similarity (LPIPS), with Model 3 showing improvement over Model 2\nbut still trailing Model 1. Qualitatively, sketches generated by Model 1\ndemonstrated the clearest facial features, highlighting its robustness as a\nbaseline despite its simplicity.", "AI": {"tldr": "研究对比了三种AI驱动的嫌犯素描自动化方法，发现简单基准的模型表现最优。", "motivation": "该项目研究多模态人工智能驱动方法在自动化和改进嫌犯素描方面的应用。", "method": "此项目开发并评估了三种用于自动化和改进嫌犯素描的人工智能多模态方法：1) 基准图像到图像的稳定扩散模型；2) 与预训练的CLIP模型集成以对齐文本和图像的同一模型；以及3) 将CLIP模型的LoRA微调应用于自注意力和交叉注意力层，并集成到稳定扩散中的新方法。", "result": "消融研究表明，微调自注意力和交叉注意力层能最佳地对齐文本描述和素描。性能测试显示，模型1在结构相似性(SSIM)上达到了0.72，峰值信噪比(PSNR)为25 dB，优于模型2和模型3。迭代改进提高了感知相似性(LPIPS)，模型3在模型2基础上有所改善但仍落后于模型1。从定性分析看，模型1生成的素描面部特征最清晰，凸显了其作为基准模型的稳健性。", "conclusion": "研究表明，尽管模型1较为简单，但它作为基准模型在生成清晰面部特征方面最为稳健。"}}
{"id": "2507.18940", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.18940", "abs": "https://arxiv.org/abs/2507.18940", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Yujun Cai", "Linzhuang Sun", "Xiangxiang Zhang", "Gaowei Wu", "Bihui Yu"], "title": "LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation", "comment": null, "summary": "Multimodal Machine Translation (MMT) enhances translation quality by\nincorporating visual context, helping to resolve textual ambiguities. While\nexisting MMT methods perform well in bilingual settings, extending them to\nmultilingual translation remains challenging due to cross-lingual interference\nand ineffective parameter-sharing strategies. To address this, we propose\nLLaVA-NeuMT, a novel multimodal multilingual translation framework that\nexplicitly models language-specific and language-agnostic representations to\nmitigate multilingual interference. Our approach consists of a layer selection\nmechanism that identifies the most informative layers for different language\npairs and a neuron-level adaptation strategy that dynamically selects\nlanguage-specific and agnostic neurons to improve translation quality while\nreducing redundancy. We conduct extensive experiments on the M3-Multi30K and\nM3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only\n40\\% of the model parameters, surpasses full fine-tuning approaches and\nultimately achieves SOTA results on both datasets. Our analysis further\nprovides insights into the importance of selected layers and neurons in\nmultimodal multilingual adaptation, offering an efficient and scalable solution\nto cross-lingual adaptation in multimodal translation.", "AI": {"tldr": "提出LLaVA-NeuMT框架通过特殊的适应策略解决多语言翻译中的挑战，实验表明其有效性。", "motivation": "现有MMT方法在双语设置中表现良好，但在多语言翻译中由于跨语言干扰和不恰当的参数共享策略而存在挑战。", "method": "LLaVA-NeuMT框架，包含了层选择机制和神经元级适应策略，旨在解决多语言翻译中的跨语言干扰问题。", "result": "实验显示，仅微调40%模型参数的LLaVA-NeuMT超越了完全微调的方法，并在两个数据集上达到SOTA结果。", "conclusion": "LLaVA-NeuMT提供了一个高效且可扩展的跨语言适应解决方案，并深入分析了所选层和神经元在多模态多语言适配中的重要性。"}}
{"id": "2507.18675", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18675", "abs": "https://arxiv.org/abs/2507.18675", "authors": ["Sanyam Jain", "Marsha Mariya Kappan", "Vijeta Sharma"], "title": "Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks", "comment": null, "summary": "Human action recognition plays a critical role in healthcare and medicine,\nsupporting applications such as patient behavior monitoring, fall detection,\nsurgical robot supervision, and procedural skill assessment. While traditional\nmodels like CNNs and RNNs have achieved moderate success, they often struggle\nto generalize across diverse and complex actions. Recent advancements in\nvision-language models, especially the transformer-based CLIP model, offer\npromising capabilities for generalizing action recognition from video data. In\nthis work, we evaluate CLIP on the UCF-101 dataset and systematically analyze\nits performance under three masking strategies: (1) percentage-based and\nshape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to\nsuppress bias-inducing elements, and (3) isolation masking that retains only\nclass-specific regions. Our results reveal that CLIP exhibits inconsistent\nbehavior and frequent misclassifications, particularly when essential visual\ncues are obscured. To overcome these limitations, we propose incorporating\nclass-specific noise, learned via a custom loss function, to reinforce\nattention to class-defining features. This enhancement improves classification\naccuracy and model confidence while reducing bias. We conclude with a\ndiscussion on the challenges of applying such models in clinical domains and\noutline directions for future work to improve generalizability across\ndomain-independent healthcare scenarios.", "AI": {"tldr": "这篇论文评估了CLIP模型在人体动作识别中的表现，并提出了解决模型泛化能力和偏差问题的方法。通过添加类别特定噪声和定制损失函数，模型的性能得到显著提升。", "motivation": "研究的动机是因为传统的CNN和RNN模型在人体动作识别上效果有限，各类模型在应对复杂多样化的人体动作时表现不佳。而CLIP等视觉语言模型的出现则为动作识别问题提供了新的解决方案。", "method": "本文研究了CLIP模型在人体动作识别任务上的应用及性能。具体来说，作者评估了CLIP在UCF-101数据集上的表现，并探讨了三种掩码策略的效果：比例和形状的黑白掩码（10%，30%，50%），特征特定的掩码以抑制引发偏差的元素，以及只保留类别特定区域的隔离掩码。", "result": "在实验结果上，发现CLIP在关键视觉线索被遮挡的情况下，表现出不稳定和频繁的错误分类。", "conclusion": "为了解决这些问题，作者提出了一种通过添加类别特定噪声并采用定制损失函数来增强模型对类别定义特征关注的做法，这一改进提高了分类准确率和模型置信度，减少了偏差。最后讨论了将此类模型应用于临床领域的挑战，并展望了未来提高模型跨领域独立健康场景应用的通用性的方向。"}}
{"id": "2507.18952", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18952", "abs": "https://arxiv.org/abs/2507.18952", "authors": ["Yongjie Li", "Ruilin Nong", "Jianan Liu", "Lucas Evans"], "title": "Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection", "comment": null, "summary": "Legal document summarization represents a significant advancement towards\nimproving judicial efficiency through the automation of key information\ndetection. Our approach leverages state-of-the-art natural language processing\ntechniques to meticulously identify and extract essential data from extensive\nlegal texts, which facilitates a more efficient review process. By employing\nadvanced machine learning algorithms, the framework recognizes underlying\npatterns within judicial documents to create precise summaries that encapsulate\nthe crucial elements. This automation alleviates the burden on legal\nprofessionals, concurrently reducing the likelihood of overlooking vital\ninformation that could lead to errors. Through comprehensive experiments\nconducted with actual legal datasets, we demonstrate the capability of our\nmethod to generate high-quality summaries while preserving the integrity of the\noriginal content and enhancing processing times considerably. The results\nreveal marked improvements in operational efficiency, allowing legal\npractitioners to direct their efforts toward critical analytical and\ndecision-making activities instead of manual reviews. This research highlights\npromising technology-driven strategies that can significantly alter workflow\ndynamics within the legal sector, emphasizing the role of automation in\nrefining judicial processes.", "AI": {"tldr": "研究使用先进的自然语言处理技术自动总结法律文件，提高司法效率，减少人为错误，实验显示该方法能够生成高质量的摘要并显著提高处理速度。", "motivation": "为了通过自动化关键信息的检测来提高司法效率，并减轻法律专业人员的负担，减少错误可能性。", "method": "采用先进的机器学习算法识别司法文档中的潜在模式，从而创建精确的摘要，提取关键信息。", "result": "实验表明，该方法可以在保持原始内容完整性的同时生成高质量摘要，并大幅度提高处理时间。", "conclusion": "研究展示了技术驱动的策略如何显著改变法律行业的流程动态，强调了自动化在改善司法流程方面的作用。"}}
{"id": "2507.18677", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.18677", "abs": "https://arxiv.org/abs/2507.18677", "authors": ["Siyu Mu", "Wei Xuan Chan", "Choon Hwai Yap"], "title": "HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States", "comment": "Codes are available at https://github.com/SiyuMU/Loaded2UnNet", "summary": "The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal\npressure) serves as a valuable zero-stress and zero-strain reference and is\ncritical for personalized biomechanical modeling of cardiac function, to\nunderstand both healthy and diseased physiology and to predict the effects of\ncardiac interventions. However, estimating the unloaded geometry from clinical\nimages remains a challenging task. Traditional approaches rely on inverse\nfinite element (FE) solvers that require iterative optimization and are\ncomputationally expensive. In this work, we introduce HeartUnloadNet, a deep\nlearning framework that predicts the unloaded left ventricular (LV) shape\ndirectly from the end diastolic (ED) mesh while explicitly incorporating\nbiophysical priors. The network accepts a mesh of arbitrary size along with\nphysiological parameters such as ED pressure, myocardial stiffness scale, and\nfiber helix orientation, and outputs the corresponding unloaded mesh. It adopts\na graph attention architecture and employs a cycle-consistency strategy to\nenable bidirectional (loading and unloading) prediction, allowing for partial\nself-supervision that improves accuracy and reduces the need for large training\ndatasets. Trained and tested on 20,700 FE simulations across diverse LV\ngeometries and physiological conditions, HeartUnloadNet achieves sub-millimeter\naccuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing\ninference time to just 0.02 seconds per case, over 10^5 times faster and\nsignificantly more accurate than traditional inverse FE solvers. Ablation\nstudies confirm the effectiveness of the architecture. Notably, the\ncycle-consistent design enables the model to maintain a DSC of 97% even with as\nfew as 200 training samples. This work thus presents a scalable and accurate\nsurrogate for inverse FE solvers, supporting real-time clinical applications in\nthe future.", "AI": {"tldr": "本文介绍了HeartUnloadNet，这是一种可以直接从心脏舒张期末期的网格预测无负荷左心室形状的深度学习框架。这种方法在准确性上优于传统的逆有限元（FE）方法，能够在0.02秒内完成预测，并且仅需极少量的训练样本即可达到高精度。", "motivation": "无负荷心脏几何结构（即无腔内压力的心脏状态）作为零应力和零应变的参考是有价值的，对于个性化心脏功能的生物力学建模至关重要，可以帮助理解健康和疾病的生理状态，并预测心脏干预的效果。然而，从临床图像中估计无负荷几何结构仍然是一个具有挑战性的任务。传统的方法依赖于逆有限元(FE)求解器，这些求解器需要迭代优化并且计算成本高昂。", "method": "提出HeartUnloadNet，这是一种深度学习框架，可以直接从心脏舒张期末期的网格预测无负荷的左心室形状，并且明确地包含了生物物理先验。该网络可以接受任意大小的网格以及包括ED压力、心肌硬度比例和纤维螺旋方向等生理参数，并输出对应的无负荷网格。它采用图注意力架构并采用循环一致性策略，实现在加载和卸载之间进行双向预测，从而允许部分自我监督，提高精度并减少对大型训练数据集的需求。", "result": "训练和测试了20700个FE模拟的HeartUnloadNet，它在多种几何结构和生理条件下平均获得了0.986的DSC和0.083厘米的HD，精度比传统逆FE求解器高，并且将推理时间减少到每个病例只需0.02秒，比传统的逆FE求解器快了10万倍。值得注意的是，这种循环一致性设计即使在只有200个训练样本的情况下，也能够保持97%的DSC。", "conclusion": "HeartUnloadNet展示了作为一个可以在未来支持实时临床应用的逆FE求解器的可扩展且准确的替代解决方案。"}}
{"id": "2507.18956", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18956", "abs": "https://arxiv.org/abs/2507.18956", "authors": ["Sang Min Jung", "Kaixiang Zhang", "Cristian Danescu-Niculescu-Mizil"], "title": "A Similarity Measure for Comparing Conversational Dynamics", "comment": "Code and demos available in ConvoKit (https://convokit.cornell.edu/)", "summary": "The quality of a conversation goes beyond the individual quality of each\nreply, and instead emerges from how these combine into interactional patterns\nthat give the conversation its distinctive overall \"shape\". However, there is\nno robust automated method for comparing conversations in terms of their\noverall interactional dynamics. Such methods could enhance the analysis of\nconversational data and help evaluate conversational agents more holistically.\n  In this work, we introduce a similarity measure for comparing conversations\nwith respect to their dynamics. We design a validation framework for testing\nthe robustness of the metric in capturing differences in conversation dynamics\nand for assessing its sensitivity to the topic of the conversations. Finally,\nto illustrate the measure's utility, we use it to analyze conversational\ndynamics in a large online community, bringing new insights into the role of\nsituational power in conversations.", "AI": {"tldr": "本研究提出了一种比较对话动态的相似性度量，以及用于验证该方法有效性的框架。", "motivation": "当前缺乏一种自动化的方法来根据对话的整体互动动态进行比较。这样的方法可以增强对话数据的分析，并有助于更全面地评估对话代理。", "method": "本研究提出了一种用于比较对话动态相似性的度量方法。", "result": "研究设计了一个验证框架，用于测试该度量方法在捕捉对话动态差异方面的能力以及评估其对对话主题的敏感性。", "conclusion": "通过使用该度量方法分析一个大型在线社区中的对话动态，研究提供了关于对话中情境权力作用的新见解。"}}
{"id": "2507.18678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18678", "abs": "https://arxiv.org/abs/2507.18678", "authors": ["Xingyu Miao", "Haoran Duan", "Quanhao Qian", "Jiuniu Wang", "Yang Long", "Ling Shao", "Deli Zhao", "Ran Xu", "Gongjie Zhang"], "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting", "comment": "ICCV 2025 (Highlight)", "summary": "Spatial intelligence is emerging as a transformative frontier in AI, yet it\nremains constrained by the scarcity of large-scale 3D datasets. Unlike the\nabundant 2D imagery, acquiring 3D data typically requires specialized sensors\nand laborious annotation. In this work, we present a scalable pipeline that\nconverts single-view images into comprehensive, scale- and appearance-realistic\n3D representations - including point clouds, camera poses, depth maps, and\npseudo-RGBD - via integrated depth estimation, camera calibration, and scale\ncalibration. Our method bridges the gap between the vast repository of imagery\nand the increasing demand for spatial scene understanding. By automatically\ngenerating authentic, scale-aware 3D data from images, we significantly reduce\ndata collection costs and open new avenues for advancing spatial intelligence.\nWe release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,\nand demonstrate through extensive experiments that our generated data can\nbenefit various 3D tasks, ranging from fundamental perception to MLLM-based\nreasoning. These results validate our pipeline as an effective solution for\ndeveloping AI systems capable of perceiving, understanding, and interacting\nwith physical environments.", "AI": {"tldr": "A scalable pipeline converts 2D images into diverse and accurate 3D representations, enhancing the field of spatial intelligence in AI and opening new possibilities in physical environment understanding and interaction.", "motivation": "The motivation is to address the scarcity of large-scale 3D datasets, which is a limiting factor for spatial intelligence in AI. By providing a scalable method to generate 3D datasets from 2D images, the cost of data collection is reduced and new opportunities are opened for advancing spatial AI capabilities.", "method": "The method described involves a scalable pipeline that converts single-view images into 3D representations, such as point clouds, camera poses, and depth maps, through integrated depth estimation, camera calibration, and scale calibration techniques.", "result": "The generated 3D datasets can significantly benefit a variety of 3D tasks, from basic perception to complex reasoning through MLLM. The pipeline is validated through extensive experiments.", "conclusion": "The work concludes that the proposed pipeline is an effective solution for creating versatile, authentic, and scale-aware 3D data that enables the development of advanced AI systems capable of understanding and interacting with physical environments."}}
{"id": "2507.18973", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18973", "abs": "https://arxiv.org/abs/2507.18973", "authors": ["Bohan Yao", "Vikas Yadav"], "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation", "comment": "21 pages, 3 figures", "summary": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines.", "AI": {"tldr": "研究人员提出了Multi-TAG框架，以改进大语言模型在复杂数学问题上的推理能力。相比现有方法，Multi-TAG能并行调用多个工具，且无需针对特定模型进行微调。实验表明，该框架在四个难题集上比当前最佳方法高出6.0%到7.5%。", "motivation": "由于现有的工具增强方法在处理复杂数学问题时面临困难，这些方法只能在每一步推理中选择和调用单个工具。因此，该研究旨在解决这一限制，通过开发新的框架来提高对复杂数学问题的处理能力。", "method": "该研究提出了Multi-TAG（多工具聚合框架），它允许语言模型在每一步推理中并行调用多个工具，通过聚合这些工具的多样化输出来验证和改进推理过程。Multi-TAG的优势在于无需微调，可以直接应用于任何语言模型，包括大数据量的公开模型和不允许定制微调的专有模型。", "result": "在MATH500, AIME, AMC, 和 OlympiadBench四个难度测试集上，采用Multi-TAG框架的大语言模型，在开放式和封闭源模型身上均显著优于现有最优方法，提升平均6.0%至7.5%。", "conclusion": "实验结果表明，Multi-TAG在处理数学难题上展现出优越的性能，且由于其不需要微调的特性，可以在多种语言模型中广泛应用。"}}
{"id": "2507.18713", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18713", "abs": "https://arxiv.org/abs/2507.18713", "authors": ["Yun Chen", "Matthew Haines", "Jingkang Wang", "Krzysztof Baron-Lis", "Sivabalan Manivasagam", "Ze Yang", "Raquel Urtasun"], "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time", "comment": null, "summary": "High-fidelity sensor simulation of light-based sensors such as cameras and\nLiDARs is critical for safe and accurate autonomy testing. Neural radiance\nfield (NeRF)-based methods that reconstruct sensor observations via ray-casting\nof implicit representations have demonstrated accurate simulation of driving\nscenes, but are slow to train and render, hampering scale. 3D Gaussian\nSplatting (3DGS) has demonstrated faster training and rendering times through\nrasterization, but is primarily restricted to pinhole camera sensors,\npreventing usage for realistic multi-sensor autonomy evaluation. Moreover, both\nNeRF and 3DGS couple the representation with the rendering procedure (implicit\nnetworks for ray-based evaluation, particles for rasterization), preventing\ninteroperability, which is key for general usage. In this work, we present\nSparse Local Fields (SaLF), a novel volumetric representation that supports\nrasterization and raytracing. SaLF represents volumes as a sparse set of 3D\nvoxel primitives, where each voxel is a local implicit field. SaLF has fast\ntraining (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS\nLiDAR), has adaptive pruning and densification to easily handle large scenes,\nand can support non-pinhole cameras and spinning LiDARs. We demonstrate that\nSaLF has similar realism as existing self-driving sensor simulation methods\nwhile improving efficiency and enhancing capabilities, enabling more scalable\nsimulation. https://waabi.ai/salf/", "AI": {"tldr": "SaLF是一个新的体积表示方法，它允许光栅化和光线追踪，适用于自主性测试的高保真度传感器模拟。", "motivation": "高保真度的基于光的传感器模拟（如相机和LiDAR）对于安全和准确的自主性测试至关重要。NeRF和3DGS方法分别在训练和渲染速度上有优势，但也存在一些限制，如不支持多传感器场景的现实评估以及表示和渲染程序的难以互操作，限制了它们的通用性。", "method": "Sparse Local Fields (SaLF)被提出作为新的体积表示方法，能够支持光栅化和光线追踪。SaLF将体积表示为一组稀疏的三维体素原语，其中每个体素是一个局部隐式场。", "result": "SaLF拥有快速训练（小于30分钟）和渲染能力（相机50+ FPS，LiDAR 600+ FPS），具有自适应剪枝和致密化功能，可以轻松处理大场景，并且支持非针孔相机和旋转LiDAR。", "conclusion": "SaLF方法在保持现有自驾车传感器模拟方法的真实感的同时，提高了效率并增强了功能，从而能够进行更大规模的模拟，同时解决了多传感器协调模拟和表示与渲染解耦的问题。"}}
{"id": "2507.19081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19081", "abs": "https://arxiv.org/abs/2507.19081", "authors": ["Hao Li", "Yizheng Sun", "Viktor Schlegel", "Kailai Yang", "Riza Batista-Navarro", "Goran Nenadic"], "title": "Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement", "comment": "Preprint", "summary": "Argument summarization aims to generate concise, structured representations\nof complex, multi-perspective debates. While recent work has advanced the\nidentification and clustering of argumentative components, the generation stage\nremains underexplored. Existing approaches typically rely on single-pass\ngeneration, offering limited support for factual correction or structural\nrefinement. To address this gap, we introduce Arg-LLaDA, a novel large language\ndiffusion framework that iteratively improves summaries via sufficiency-guided\nremasking and regeneration. Our method combines a flexible masking controller\nwith a sufficiency-checking module to identify and revise unsupported,\nredundant, or incomplete spans, yielding more faithful, concise, and coherent\noutputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA\nsurpasses state-of-the-art baselines in 7 out of 10 automatic evaluation\nmetrics. In addition, human evaluations reveal substantial improvements across\ncore dimensions, coverage, faithfulness, and conciseness, validating the\neffectiveness of our iterative, sufficiency-aware generation strategy.", "AI": {"tldr": "Arg-LLaDA是一个新型的迭代改进摘要生成框架，通过引入充分性检测和修复机制生成更准确、简洁和连贯的论证摘要。实验表明，该方法在多方面优于现有方法。", "motivation": "尽管最近的工作在识别和聚类论证组件方面有所进展，但生成阶段仍未得到充分研究。现有方法主要依赖单次生成，提供有限的事实修正或结构完善支持。为了填补这一空白，研究提出了迭代改进摘要的方法。", "method": "介绍了一种名为Arg-LLaDA的新大型语言扩散框架，该框架通过充分性引导的重新掩蔽和再生过程迭代改进摘要。这种方法结合了一个灵活的掩蔽控制器和一个充分性检查模块来识别并修订不支持的、冗余的或不完整的片段，从而生成更加准确、简洁和连贯的输出。", "result": "在两个基准数据集上的实验证明Arg-LLaDA在10项自动评估指标中的7项上超过了最先进的基线。此外，人工评估显示在核心维度、覆盖范围、忠实度和简洁性上有了显著提升。", "conclusion": "论证的迭代、充分性感知的生成策略的有效性得到了验证。"}}
{"id": "2507.18740", "categories": ["eess.IV", "cs.CV", "cs.LG", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.18740", "abs": "https://arxiv.org/abs/2507.18740", "authors": ["Serban C. Tudosie", "Valerio Gandolfi", "Shivaprasad Varakkoth", "Andrea Farina", "Cosimo D'Andrea", "Simon Arridge"], "title": "Learned Single-Pixel Fluorescence Microscopy", "comment": "10 pages, 6 figures, 1 table", "summary": "Single-pixel imaging has emerged as a key technique in fluorescence\nmicroscopy, where fast acquisition and reconstruction are crucial. In this\ncontext, images are reconstructed from linearly compressed measurements. In\npractice, total variation minimisation is still used to reconstruct the image\nfrom noisy measurements of the inner product between orthogonal sampling\npattern vectors and the original image data. However, data can be leveraged to\nlearn the measurement vectors and the reconstruction process, thereby enhancing\ncompression, reconstruction quality, and speed. We train an autoencoder through\nself-supervision to learn an encoder (or measurement matrix) and a decoder. We\nthen test it on physically acquired multispectral and intensity data. During\nacquisition, the learned encoder becomes part of the physical device. Our\napproach can enhance single-pixel imaging in fluorescence microscopy by\nreducing reconstruction time by two orders of magnitude, achieving superior\nimage quality, and enabling multispectral reconstructions. Ultimately, learned\nsingle-pixel fluorescence microscopy could advance diagnosis and biological\nresearch, providing multispectral imaging at a fraction of the cost.", "AI": {"tldr": "本文提出了一种通过自监督学习改进单像素荧光显微镜成像的方法，显著提高了重建速度和图像质量，并降低了成本。", "motivation": "单像素成像技术在荧光显微镜中扮演着重要角色，该技术强调快速的图像采集与重建。然而，现有方法通常采用总变分最小化来从噪声测量中重建图像，本文旨在通过机器学习改进这一过程。", "method": "本文采用自监督学习方法训练了一个自编码器，用于学习测量矩阵和解码器，从而增强压缩感知和重建速度。这种方法在物理获取的多光谱和强度数据上进行了测试。", "result": "研究发现，通过自编码器学习到的测量器能够大幅减少重建时间（两个数量级），并且能够提供更高质量的图像和多光谱重建。", "conclusion": "这种方法可以降低荧光显微镜单像素成像的成本，有潜力加速诊断和生物研究的进步。"}}
{"id": "2507.19090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19090", "abs": "https://arxiv.org/abs/2507.19090", "authors": ["Haorui He", "Yupeng Li", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents", "comment": null, "summary": "Claim verification is critical for enhancing digital literacy. However, the\nstate-of-the-art single-LLM methods struggle with complex claim verification\nthat involves multi-faceted evidences. Inspired by real-world fact-checking\npractices, we propose DebateCV, the first claim verification framework that\nadopts a debate-driven methodology using multiple LLM agents. In our framework,\ntwo Debaters take opposing stances on a claim and engage in multi-round\nargumentation, while a Moderator evaluates the arguments and renders a verdict\nwith justifications. To further improve the performance of the Moderator, we\nintroduce a novel post-training strategy that leverages synthetic debate data\ngenerated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show\nthat our method outperforms existing claim verification methods under varying\nlevels of evidence quality. Our code and dataset are publicly available at\nhttps://anonymous.4open.science/r/DebateCV-6781.", "AI": {"tldr": "这篇论文提出了一种新的声明验证框架DebateCV，该框架采用辩论驱动的方法，通过多轮辩论来提高声明验证的准确性。", "motivation": "该论文的动机在于当前最先进的单一LLM方法在涉及多方面证据的复杂声明验证方面存在问题。通过借鉴现实世界的事实核查实践，提出了一种新的声明验证框架以提高数字素养。", "method": "该研究提出了一种称为DebateCV的声明验证框架，该框架使用多个LLM代理进行辩论驱动的方法。框架中有两个辩论者针对某个声明采取对立立场并进行多轮辩论，而一个协调者评估辩论并作出判断。此外，还引入了一种新的后训练策略，利用零样本DebateCV生成的合成辩论数据来改善协调者的性能。", "result": "研究结果表明，所提出的方法在不同质量的证据水平下均优于现有的声明验证方法。", "conclusion": "研究结论是，通过引入辩论驱动的框架和后训练策略，能够提高声明验证的准确性。"}}
{"id": "2507.18741", "categories": ["cs.CV", "cs.DL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18741", "abs": "https://arxiv.org/abs/2507.18741", "authors": ["Tristan Repolusk", "Eduardo Veas"], "title": "KuiSCIMA v2.0: Improved Baselines, Calibration, and Cross-Notation Generalization for Historical Chinese Music Notations in Jiang Kui's Baishidaoren Gequ", "comment": "International Conference on Document Analysis and Recognition. This\n  preprint has not undergone any post-submission improvements or corrections.\n  The Version of Record of this contribution is published in \"19th\n  International Conference on Document Analysis and Recognition (ICDAR 2025),\n  Wuhan, China, September 16-21, 2025, Proceedings\", and is available online at\n  the External DOI field below", "summary": "Optical Music Recognition (OMR) for historical Chinese musical notations,\nsuch as suzipu and l\\\"ul\\\"upu, presents unique challenges due to high class\nimbalance and limited training data. This paper introduces significant\nadvancements in OMR for Jiang Kui's influential collection Baishidaoren Gequ\nfrom 1202. In this work, we develop and evaluate a character recognition model\nfor scarce imbalanced data. We improve upon previous baselines by reducing the\nCharacter Error Rate (CER) from 10.4% to 7.1% for suzipu, despite working with\n77 highly imbalanced classes, and achieve a remarkable CER of 0.9% for\nl\\\"ul\\\"upu. Our models outperform human transcribers, with an average human CER\nof 15.9% and a best-case CER of 7.6%. We employ temperature scaling to achieve\na well-calibrated model with an Expected Calibration Error (ECE) below 0.0162.\nUsing a leave-one-edition-out cross-validation approach, we ensure robust\nperformance across five historical editions. Additionally, we extend the\nKuiSCIMA dataset to include all 109 pieces from Baishidaoren Gequ, encompassing\nsuzipu, l\\\"ul\\\"upu, and jianzipu notations. Our findings advance the\ndigitization and accessibility of historical Chinese music, promoting cultural\ndiversity in OMR and expanding its applicability to underrepresented music\ntraditions.", "AI": {"tldr": "本文介绍了针对江夔《白石道人歌曲》中的古谱识别，在少样本和类别不平衡数据下，显著提高了字符识别率，并采用了温度缩放以获得良好的模型校准效果，拓展了KuiSCIMA数据集，促进了古谱数字化和中国文化多样性。", "motivation": "由于高类别不平衡和训练数据有限，针对古谱识别（如苏氏谱和律吕谱）带来了独特的挑战。以往研究在这方面的字符识别率（CER）表现不佳。为了提高古谱识别的准确性及其对文化多样性的贡献，团队展开本项研究。", "method": "研究团队开发了一个基于深度学习的字符识别模型，专门针对不平衡的数据样本进行优化。通过使用温度缩放技术优化模型校准，采用了留一版交叉验证确保模型在不同版本中稳定表现，最终扩展了KuiSCIMA数据集，使之包含《白石道人歌曲》中的所有109个片段。", "result": "该方法在苏氏谱上将CER从之前的10.4%降至7.1%，在律吕谱上更是达到了0.9%的低CER。在模型校准上，研究做到了ECE小于0.0162，优于之前的人工转录综合CER 15.9%，最佳情况亦优于7.6%。", "conclusion": "本研究不仅提升了古代乐谱的识别率，同时推广了音乐自动识别系统在文化艺术领域的应用，特别是促进了中文历史音乐传统的数字化与可访问性，凸显了OMR技术在文化多样性和少数群体音乐传承中的价值。"}}
{"id": "2507.19117", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19117", "abs": "https://arxiv.org/abs/2507.19117", "authors": ["Swapnil Hingmire", "Ze Shi Li", "Shiyu", "Zeng", "Ahmed Musa Awon", "Luiz Franciscatto Guerra", "Neil Ernst"], "title": "Objectifying the Subjective: Cognitive Biases in Topic Interpretations", "comment": "Accepted for publication at the Transactions of ACL (TACL) (pre-MIT\n  Press publication version)", "summary": "Interpretation of topics is crucial for their downstream applications.\nState-of-the-art evaluation measures of topic quality such as coherence and\nword intrusion do not measure how much a topic facilitates the exploration of a\ncorpus. To design evaluation measures grounded on a task, and a population of\nusers, we do user studies to understand how users interpret topics. We propose\nconstructs of topic quality and ask users to assess them in the context of a\ntopic and provide rationale behind evaluations. We use reflexive thematic\nanalysis to identify themes of topic interpretations from rationales. Users\ninterpret topics based on availability and representativeness heuristics rather\nthan probability. We propose a theory of topic interpretation based on the\nanchoring-and-adjustment heuristic: users anchor on salient words and make\nsemantic adjustments to arrive at an interpretation. Topic interpretation can\nbe viewed as making a judgment under uncertainty by an ecologically rational\nuser, and hence cognitive biases aware user models and evaluation frameworks\nare needed.", "AI": {"tldr": "研究提出了一种基于用户研究的主题质量评估方法，识别出用户解释主题的模式，并提出了一个基于锚定和调整启发式的主题解释理论。", "motivation": "当前的主题质量评估度量如连贯性和词侵入度并未衡量一个主题在探索语料库过程中起到的作用。因此，需要基于任务和用户群体进行评估。", "method": "通过用户研究来理解用户如何解释主题，并提出主题质量的构建模型，让用户在特定主题的背景下评估这些构建，并提供评估的理由。", "result": "研究发现用户基于可得性和代表性启发式进行主题解释，提出了一个锚定和调整的启发式理论来解释主题解释过程。", "conclusion": "主题解释可以被视为生态理性的用户在不确定性下作出的判断，故需要认知偏差意识的用户模型和评估框架。"}}
{"id": "2507.18743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18743", "abs": "https://arxiv.org/abs/2507.18743", "authors": ["Xinjun Cheng", "Yiguo He", "Junjie Zhu", "Chunping Qiu", "Jun Wang", "Qiangjuan Huang", "Ke Yang"], "title": "SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning", "comment": "IEEE Submission", "summary": "Vision Language Models (VLMs) have achieved remarkable breakthroughs in the\nfield of remote sensing in recent years. Synthetic Aperture Radar (SAR)\nimagery, with its all-weather capability, is essential in remote sensing, yet\nthe lack of large-scale, high-quality SAR image-text datasets hinders its\nsemantic understanding. In this paper, we construct SAR-Text, a large-scale and\nhigh-quality dataset consisting of over 130,000 SAR image-text pairs. To\nconstruct the SAR-Text dataset, we design the SAR-Narrator framework, which\ngenerates textual descriptions for SAR images through a multi-stage progressive\ntransfer learning strategy. To verify the effectiveness of the SAR-TEXT\ndataset, we conduct experiments on three typical vision-language tasks:\nimage-text retrieval, image captioning, and visual question answering (VQA).\nSpecifically, we construct three representative models on SAR-TEXT:\nSAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable\nimprovements in retrieval performance, boosting average recall by 16.43% and\n10.54% on the OSdataset-512 and HRSID test sets, respectively. In the\ncaptioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding\nthose of the original CoCa model by more than 8x, 4x, and 10x, respectively. In\nthe VQA task, SAR-GPT outperforms baseline and single-stage models on multiple\nSAR-VQA datasets, demonstrating stronger semantic understanding and reasoning\nability, as further confirmed by qualitative results. It is worth noting that,\nas a flexible captioning tool, SAR-Narrator can be readily adopted by the\ncommunity to construct larger-scale SAR image-text datasets.", "AI": {"tldr": "本文介绍了SAR-Text，一个大型高质量的SAR图像-文本数据集，并设计了SAR-Narrator框架用于生成SAR图像的文本描述。通过在图像-文本检索、图像标注和视觉问答任务上的实验展示了其有效性。", "motivation": "本文旨在解决当前SAR图像研究中缺乏大规模高质量图像-文本数据集的问题，促进SAR图像语义理解。", "method": "设计了SAR-Narrator框架，采用多阶段渐进迁移学习策略生成SAR图像的文本描述，并构建了SAR-Text数据集。", "result": "在图像-文本检索、图像标注、视觉问答三个任务上，基于SAR-Text训练的模型取得了显著提升，展示了数据集的有效性。", "conclusion": "SAR-Text数据集和SAR-Narrator框架对于推动SAR图像语义理解有重要作用，同时证明了该方法的有效性。"}}
{"id": "2507.19156", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19156", "abs": "https://arxiv.org/abs/2507.19156", "authors": ["Gioele Giachino", "Marco Rondina", "Antonio Vetrò", "Riccardo Coppola", "Juan Carlos De Martin"], "title": "An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case", "comment": "16 pages, European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop\n  on Bias and Fairness in AI (BIAS25)", "summary": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base.", "AI": {"tldr": "该研究显示，大型语言模型（如ChatGPT和Gemini）在无性别提示时会生成性别和职业偏见的内容，可能在工作场所等带来重要伦理问题。", "motivation": "由于大型语言模型（LLMs）在各种领域的广泛应用，人们开始担忧它们如何轻易地固化刻板印象，助长生成偏见内容。研究集中在性别和职业偏见方面，探讨LLMs在无性别提示时，响应形成的模式及其对生成偏见内容的影响。", "method": "该研究采用了一种结构化的实验方法，通过对三种具有层级关系的不同职业组合使用不同的提示，来分析在无性别提示下，大型语言模型（LLMs）形成的响应方式及其对输出偏见的影响。实验使用意大利语进行，以强调当前LLMs在生成非英语客观文本方面可能存在的局限性。", "result": "研究结果显示，LLMs生成的内容会强化刻板印象。例如，Gemini（ChatGPT为97%）100%将“她”的代词与“助手”关联，而不是“经理”。这表明在工作场所或职位选择等方面，AI生成文本中的偏见可能带来重要的道德问题。", "conclusion": "这项研究强调了理解这些风险对于制定缓解策略至关重要，以确保基于AI的系统不会加剧社会不平等，而是促进更公平的结果。未来的研究方向包括扩展对额外聊天机器人或语言的研究，改进提示工程方法或进一步利用更大范围的实验基础。"}}
{"id": "2507.18758", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18758", "abs": "https://arxiv.org/abs/2507.18758", "authors": ["Yifan Liu", "Shengjun Zhang", "Chensheng Dai", "Yang Chen", "Hao Liu", "Chen Li", "Yueqi Duan"], "title": "Learning Efficient and Generalizable Human Representation with Human Gaussian Model", "comment": null, "summary": "Modeling animatable human avatars from videos is a long-standing and\nchallenging problem. While conventional methods require per-instance\noptimization, recent feed-forward methods have been proposed to generate 3D\nGaussians with a learnable network. However, these methods predict Gaussians\nfor each frame independently, without fully capturing the relations of\nGaussians from different timestamps. To address this, we propose Human Gaussian\nGraph to model the connection between predicted Gaussians and human SMPL mesh,\nso that we can leverage information from all frames to recover an animatable\nhuman representation. Specifically, the Human Gaussian Graph contains dual\nlayers where Gaussians are the first layer nodes and mesh vertices serve as the\nsecond layer nodes. Based on this structure, we further propose the intra-node\noperation to aggregate various Gaussians connected to one mesh vertex, and\ninter-node operation to support message passing among mesh node neighbors.\nExperimental results on novel view synthesis and novel pose animation\ndemonstrate the efficiency and generalization of our method.", "AI": {"tldr": "针对独立预测3D Gaussians而忽视其相关性的方法，提出用Human Gaussian Graph建模Gaussians和SMPL网格间的关系，提升动画人像建模的效率和泛化能力。", "motivation": "传统的建模动画人像的方法需要逐实例优化，而最近的前馈方法通过可学习网络生成3D Gaussians。然而这些方法独立预测每帧的Gaussians，没有充分捕捉不同时刻Gaussians之间的关系。因此，提出了Human Gaussian Graph来解决这个问题。", "method": "提出了Human Gaussian Graph来建模预测Gaussians与人体SMPL网格之间的连接，该图包含两层，第一层是Gaussians节点，第二层是网格顶点节点。通过这种方法，能够利用所有帧的信息恢复可动画的人体表示。此外，还提出了节点内部操作来聚合连接到一个网格顶点的多个Gaussians，以及节点之间操作来支持网格节点邻域之间的消息传递。", "result": "实验结果表明，在新视角合成和新姿势动画上，该方法有效且具有泛化能力。", "conclusion": "通过使用Human Gaussian Graph，能够有效建模动画人像，减少了过去需逐实例优化的问题，提升了在新视角与姿势动画上的表现。"}}
{"id": "2507.19195", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19195", "abs": "https://arxiv.org/abs/2507.19195", "authors": ["Chaymaa Abbas", "Mariette Awad", "Razane Tajeddine"], "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?", "comment": null, "summary": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development.", "AI": {"tldr": "研究发现，即使是很小量的中毒数据也会显著增加针对非洲裔美国人非标准英语的毒性输出，而这主要是由于模型放大了原有的方言偏见。研究结果表明，需要从社会负责的角度重新审视语言模型的开发和训练过程。", "motivation": "尽管语言模型设计上持续改进以促进包容性和平衡回应，但系统仍然存在编码和放大社会偏见的风险。本研究旨在探讨方言变体如何与数据中毒相互作用，影响系统的毒性输出。", "method": "本研究通过使用小规模和中等规模的LLaMA模型，分析了方言差异（非洲裔美国人非标准英语AAVE与标准美国英语SAE）如何通过数据中毒影响输出的毒性。", "result": "研究发现，即使是极少量的中毒数据也会显著增加针对AAVE输入的毒性输出，而对于SAE则影响较小。模型规模越大，偏见放大的效果越明显。利用GPT-4评估发现，AAVE输入表现出更多的负面刻板印象。", "conclusion": "研究结果强调了数据中毒和方言偏见的结合效应对语言模型的影响，并呼吁在开发过程中采取具有方言意识的评估、定向去偏方法及社会负责的训练流程。"}}
{"id": "2507.18763", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18763", "abs": "https://arxiv.org/abs/2507.18763", "authors": ["Keshav Gupta", "Tejas S. Stanley", "Pranjal Paul", "Arun K. Singh", "K. Madhava Krishna"], "title": "Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving", "comment": "8 pages, 7 figures, IROS 2025", "summary": "Drivable Free-space prediction is a fundamental and crucial problem in\nautonomous driving. Recent works have addressed the problem by representing the\nentire non-obstacle road regions as the free-space. In contrast our aim is to\nestimate the driving corridors that are a navigable subset of the entire road\nregion. Unfortunately, existing corridor estimation methods directly assume a\nBEV-centric representation, which is hard to obtain. In contrast, we frame\ndrivable free-space corridor prediction as a pure image perception task, using\nonly monocular camera input. However such a formulation poses several\nchallenges as one doesn't have the corresponding data for such free-space\ncorridor segments in the image. Consequently, we develop a novel\nself-supervised approach for free-space sample generation by leveraging future\nego trajectories and front-view camera images, making the process of visual\ncorridor estimation dependent on the ego trajectory. We then employ a diffusion\nprocess to model the distribution of such segments in the image. However, the\nexisting binary mask-based representation for a segment poses many limitations.\nTherefore, we introduce ContourDiff, a specialized diffusion-based architecture\nthat denoises over contour points rather than relying on binary mask\nrepresentations, enabling structured and interpretable free-space predictions.\nWe evaluate our approach qualitatively and quantitatively on both nuScenes and\nCARLA, demonstrating its effectiveness in accurately predicting safe multimodal\nnavigable corridors in the image.", "AI": {"tldr": "提出了一种新的单目相机驱动的可行驶自由空间走廊预测方法，通过自我监督的方法生成自由空间样本，引入ContourDiff架构实现可解释性预测。", "motivation": "既有的走廊估计方法直接假定BEV（鸟瞰图）为中心的表示，难以获取。目标是估计驾驶走廊，这是整个道路区域的一个可导航子集，解决现有方法的局限。", "method": "通过仅使用单目相机输入将可行驶自由空间走廊预测制定为纯粹的图像感知任务。为了生成自由空间样本，提出了一个自我监督的方法，利用未来的自我轨迹和前视摄像头图像。然后使用扩散过程来建模图像中这些片段的分布，并引入ContourDiff，一种基于扩散的专门架构，通过对轮廓点进行去噪，而不是依赖于二进制掩码表示，从而实现结构化和可解释的自由空间预测。", "result": "在nuScenes和CARLA数据集上进行了定性和定量评估，结果表明能够有效地预测图像中的安全多模式可导航走廊。", "conclusion": "本文提出了一个自我监督的自由空间样本生成方法，并引入ContourDiff架构，实现了结构化、可解释的自由空间预测，这对于改善自动驾驶自由空间感知能力具有重要意义。"}}
{"id": "2507.19219", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.19219", "abs": "https://arxiv.org/abs/2507.19219", "authors": ["Zi Liang", "Liantong Yu", "Shiyu Zhang", "Qingqing Ye", "Haibo Hu"], "title": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework", "comment": "Source code: https://github.com/liangzid/ArxivRoll/ Website:\n  https://arxivroll.moreoverai.com/", "summary": "Overestimation in evaluating large language models (LLMs) has become an\nincreasing concern. Due to the contamination of public benchmarks or imbalanced\nmodel training, LLMs may achieve unreal evaluation results on public\nbenchmarks, either intentionally or unintentionally, which leads to unfair\ncomparisons among LLMs and undermines their realistic capability assessments.\nExisting benchmarks attempt to address these issues by keeping test cases\npermanently secret, mitigating contamination through human evaluation, or\nrepeatedly collecting and constructing new samples. However, these approaches\nfail to ensure reproducibility, transparency, and high efficiency\nsimultaneously. Moreover, the extent of overestimation in current LLMs remains\nunquantified. To address these issues, we propose ArxivRoll, a dynamic\nevaluation framework inspired by one-time pad encryption in cryptography.\nArxivRoll comprises two key components: \\emph{i) SCP (Sequencing, Cloze, and\nPrediction)}, an automated generator for private test cases, and \\emph{ii)\nRugged Scores (RS)}, metrics that measure the proportion of public benchmark\ncontamination and training bias. Leveraging SCP, ArxivRoll constructs a new\nbenchmark every six months using recent articles from ArXiv and employs them\nfor one-time evaluations of LLM performance. Extensive experiments demonstrate\nthe high quality of our benchmark, and we provide a systematic evaluation of\ncurrent LLMs. The source code is available at\nhttps://github.com/liangzid/ArxivRoll/.", "AI": {"tldr": "This paper introduces ArxivRoll, a new dynamic evaluation framework for large language models that addresses issues with existing benchmarks, providing a reproducible, transparent, and efficient process for benchmarking LLMs.", "motivation": "To solve the problems of overestimation, lack of reproducibility, transparency, and efficiency in evaluating large language models using existing benchmarks.", "method": "ArxivRoll, a dynamic evaluation framework, includes two components: SCP, an automated generator for private test cases, and Rugged Scores, metrics to measure public benchmark contamination and training bias.", "result": "The experiments show the high quality of the benchmark and provide a systematic evaluation of current LLMs.", "conclusion": "ArxivRoll addresses the issues of overestimation in evaluating large language models, offering a reproducible, transparent, and efficient dynamic evaluation framework."}}
{"id": "2507.18788", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18788", "abs": "https://arxiv.org/abs/2507.18788", "authors": ["Hitesh Kumar Gupta"], "title": "Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning", "comment": "16 pages, 12 total figures (including a 7-figure appendix), 4 tables", "summary": "Image captioning, a task at the confluence of computer vision and natural\nlanguage processing, requires a sophisticated understanding of both visual\nscenes and linguistic structure. While modern approaches are dominated by\nlarge-scale Transformer architectures, this paper documents a systematic,\niterative development of foundational image captioning models, progressing from\na simple CNN-LSTM encoder-decoder to a competitive attention-based system. We\npresent a series of five models, beginning with Genesis and concluding with\nNexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic\nattention mechanism. Our experiments chart the impact of architectural\nenhancements and demonstrate a key finding within the classic CNN-LSTM\nparadigm: merely upgrading the visual backbone without a corresponding\nattention mechanism can degrade performance, as the single-vector bottleneck\ncannot transmit the richer visual detail. This insight validates the\narchitectural shift to attention. Trained on the MS COCO 2017 dataset, our\nfinal model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several\nfoundational benchmarks and validating our iterative design process. This work\nprovides a clear, replicable blueprint for understanding the core architectural\nprinciples that underpin modern vision-language tasks.", "AI": {"tldr": "本文描述了图像字幕生成中的架构迭代，展示了从基础的CNN-LSTM架构到先进注意力机制系统的进展，强调了视觉骨干网络改进与注意力机制协同的重要性。最终提出的Nexus模型在MS COCO 2017数据集上取得了超过基础模型的BLEU-4得分31.4。", "motivation": "本文动机在于通过一系列迭代模型（从Genesis到Nexus）探索图像字幕任务的基本架构原则，验证改进视觉骨干网络而忽视注意力机制的负面影响，并提供一个清晰可复现的蓝图。", "method": "本文通过逐步迭代的方法开发了五个基础图像字幕生成模型，从简单的CNN-LSTM编码器-解码器架构逐步演进到先进的注意力机制系统，最终模型Nexus使用了EfficientNetV2B3作为视觉骨干网络和动态注意力机制。", "result": "实验显示，在没有对应注意力机制的情况下升级视觉骨干网络会降低性能。最终模型Nexus在MS COCO 2017基准数据集上达到了31.4的BLEU-4得分，超过了多个基础模型。", "conclusion": "本文提出的迭代设计方案验证了注意力机制在现代视觉-语言任务中的重要性，并为理解图像字幕生成任务的核心架构原则提供了清晰的路径。"}}
{"id": "2507.19227", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19227", "abs": "https://arxiv.org/abs/2507.19227", "authors": ["Yuanhe Zhang", "Fangzhou Xie", "Zhenhong Zhou", "Zherui Li", "Hao Chen", "Kun Wang", "Yufei Guo"], "title": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation", "comment": null, "summary": "Large Language Diffusion Models (LLDMs) exhibit comparable performance to\nLLMs while offering distinct advantages in inference speed and mathematical\nreasoning tasks.The precise and rapid generation capabilities of LLDMs amplify\nconcerns of harmful generations, while existing jailbreak methodologies\ndesigned for Large Language Models (LLMs) prove limited effectiveness against\nLLDMs and fail to expose safety vulnerabilities.Successful defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether\nLLDMs possess safety robustness or existing attacks are incompatible with\ndiffusion-based architectures.To address this, we first reveal the\nvulnerability of LLDMs to jailbreak and demonstrate that attack failure in\nLLDMs stems from fundamental architectural differences.We present a PArallel\nDecoding jailbreak (PAD) for diffusion-based language models. PAD introduces\nMulti-Point Attention Attack, which guides parallel generative processes toward\nharmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves\njailbreak attack success rates by 97%, revealing significant safety\nvulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,\nLLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled misuse.Through comprehensive analysis, we provide an\ninvestigation into LLDM architecture, offering critical insights for the secure\ndeployment of diffusion-based language models.", "AI": {"tldr": "PAD is a method that effectively attacks LLDMs, achieving high success rates in generating harmful outputs, highlighting the need for further safety measures for these models.", "motivation": "To address the safety concerns of LLDMs and investigate their vulnerability to jailbreak attacks, given that existing methods for LLMs are ineffective against them.", "method": "PArallel Decoding jailbreak (PAD) for diffusion-based language models, using Multi-Point Attention Attack to guide parallel generative processes toward harmful outputs.", "result": "PAD achieves jailbreak attack success rates of 97% across four LLDMs, and shows that LLDMs increase harmful generation speed by 2x compared to autoregressive LLMs of the same size.", "conclusion": "LLDMs have significant safety vulnerabilities that require consideration for secure deployment, as revealed by the success of the PAD attack."}}
{"id": "2507.18815", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18815", "abs": "https://arxiv.org/abs/2507.18815", "authors": ["Benjamin Carter", "Nathan Dilla", "Micheal Callahan", "Atuhaire Ambala"], "title": "Deepfake Detection Via Facial Feature Extraction and Modeling", "comment": "Keywords: deepfake, facial recognition, feature extraction,\n  artificial intelligence, recurrent neural network, convolutional neural\n  network, artificial neural network", "summary": "The rise of deepfake technology brings forth new questions about the\nauthenticity of various forms of media found online today. Videos and images\ngenerated by artificial intelligence (AI) have become increasingly more\ndifficult to differentiate from genuine media, resulting in the need for new\nmodels to detect artificially-generated media. While many models have attempted\nto solve this, most focus on direct image processing, adapting a convolutional\nneural network (CNN) or a recurrent neural network (RNN) that directly\ninteracts with the video image data. This paper introduces an approach of using\nsolely facial landmarks for deepfake detection. Using a dataset consisting of\nboth deepfake and genuine videos of human faces, this paper describes an\napproach for extracting facial landmarks for deepfake detection, focusing on\nidentifying subtle inconsistencies in facial movements instead of raw image\nprocessing. Experimental results demonstrated that this feature extraction\ntechnique is effective in various neural network models, with the same facial\nlandmarks tested on three neural network models, with promising performance\nmetrics indicating its potential for real-world applications. The findings\ndiscussed in this paper include RNN and artificial neural network (ANN) models\nwith accuracy between 96% and 93%, respectively, with a CNN model hovering\naround 78%. This research challenges the assumption that raw image processing\nis necessary to identify deepfake videos by presenting a facial feature\nextraction approach compatible with various neural network models while\nrequiring fewer parameters.", "AI": {"tldr": "本文提出了一种仅使用面部特征点进行深度伪造检测的方法，这种方法在不同神经网络模型中表现出良好的性能，表明面部特征提取在检测深度伪造视频方面具有潜力，且所需参数较少，挑战了必须依赖原始图像处理来识别深度伪造视频的传统观念。", "motivation": "随着深度伪造技术的发展，识别在线媒体的真实性变得至关重要。现有的检测方法主要基于直接图像处理，本文动机在于探索一种新方法，通过面部特征点检测深度伪造视频，这在技术上更具挑战性，同时也可能带来更有效和高效的解决方案。", "method": "本文使用面部特征点作为输入数据，从包含深度伪造和真实视频的人脸数据集中提取面部特征点的数据，不在原始图像上进行直接处理，而是通过神经网络模型来分析这些特征点，以发现面部运动的微妙不一致之处。", "result": "测试结果表明，基于面部特征点的特征提取技术在各种神经网络模型上都表现出了有效性，使用RNN和ANN模型时准确率分别介于96%和93%，而使用CNN模型时准确率大约为78%。", "conclusion": "本研究提出了面部特征提取的方法，能够被不同类型的神经网络模型有效利用，从而识别深度伪造视频，同时减少了参数需求。这项技术挑战了依赖于原始图像处理来识别深度伪造视频的主流观点。"}}
{"id": "2507.19303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19303", "abs": "https://arxiv.org/abs/2507.19303", "authors": ["Ilias Chalkidis", "Stephanie Brandl", "Paris Aslanidis"], "title": "Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns", "comment": "Pre-print", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can\nidentify and classify fine-grained forms of populism, a complex and contested\nconcept in both academic and media debates. To this end, we curate and release\nnovel datasets specifically designed to capture populist discourse. We evaluate\na range of pre-trained (large) language models, both open-weight and\nproprietary, across multiple prompting paradigms. Our analysis reveals notable\nvariation in performance, highlighting the limitations of LLMs in detecting\npopulist discourse. We find that a fine-tuned RoBERTa classifier vastly\noutperforms all new-era instruction-tuned LLMs, unless fine-tuned.\nAdditionally, we apply our best-performing model to analyze campaign speeches\nby Donald Trump, extracting valuable insights into his strategic use of\npopulist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens\ninto cross-context transferability in political discourse analysis. In this\nsetting, we find that instruction-tuned LLMs exhibit greater robustness on\nout-of-domain data.", "AI": {"tldr": "本文评估了大型语言模型（LLMs）识别和分类细粒度民粹主义形式的能力，发现虽然LLMs在处理非领域数据时表现出效用，但在特定任务上仍需要微调来提高性能。", "motivation": "尽管大型语言模型（LLMs）在许多指令跟随任务上表现出色，但其对复杂社会科学研究成果的理解尚未得到充分探索。本文旨在研究LLMs是否能够识别和分类细粒度的民粹主义形式。", "method": "本文通过构建和发布捕捉民粹话语的新数据集，评估了多种预训练的语言模型（包括公开和专有模型）在不同提示范式下的表现。", "result": "分析显示，经过微调的RoBERTa分类器在识别民粹主义话语方面显著优于未经微调的新时代指令调整LLMs。此外，在分析唐纳德·特朗普的政治演讲中，该模型揭示了他在使用民粹主义修辞方面的策略性见解，并测试了模型在欧洲政客政治演讲中的通用性。", "conclusion": "本文发现经过指令微调的LLMs在处理非领域数据时更加稳健，但在特定任务上的表现仍被微调过的RoBERTa分类器超越。"}}
{"id": "2507.18830", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18830", "abs": "https://arxiv.org/abs/2507.18830", "authors": ["Shen Zhu", "Yinzhu Jin", "Tyler Spears", "Ifrah Zawar", "P. Thomas Fletcher"], "title": "RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models", "comment": "19 pages, 10 figures", "summary": "We propose image-to-image diffusion models that are designed to enhance the\nrealism and details of generated brain images by introducing sharp edges, fine\ntextures, subtle anatomical features, and imaging noise. Generative models have\nbeen widely adopted in the biomedical domain, especially in image generation\napplications. Latent diffusion models achieve state-of-the-art results in\ngenerating brain MRIs. However, due to latent compression, generated images\nfrom these models are overly smooth, lacking fine anatomical structures and\nscan acquisition noise that are typically seen in real images. This work\nformulates the realism enhancing and detail adding process as image-to-image\ndiffusion models, which refines the quality of LDM-generated images. We employ\ncommonly used metrics like FID and LPIPS for image realism assessment.\nFurthermore, we introduce new metrics to demonstrate the realism of images\ngenerated by RealDeal in terms of image noise distribution, sharpness, and\ntexture.", "AI": {"tldr": "论文提出了一种图像到图像扩散模型，用以增强潜扩散模型生成的脑MRI图像的真实感和细节表现，引入了新的评估指标来衡量图像的真实感。", "motivation": "该论文的动机在于解决现有的潜扩散模型生成的脑MRI图像过于平滑，缺乏精细的解剖结构和扫描获取噪音的问题。这些问题使得生成的图像与真实的图像存在差距，因此提出了新的方法来增强生成图像的真实感和细节。", "method": "该论文提出了一种基于图像到图像扩散模型的方法，用于增强和改善潜扩散模型生成的脑MRI图像的现实感和细节。这种方法通过引入尖锐边缘、精细纹理、细微解剖特征和成像噪音来实现图像的精细化。", "result": "论文使用常见的FID和LPIPS等指标来评估图像的真实感，并引入了新的评估指标以展示所提出模型在图像噪音分布、锐度和纹理上的真实感。", "conclusion": "论文通过提出的图像到图像扩散模型成功改善了潜扩散模型生成的脑MRI图像的现实感和细节表现，提供了新的评估图像真实感的方法。"}}
{"id": "2507.19315", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19315", "abs": "https://arxiv.org/abs/2507.19315", "authors": ["Yicheng Tao", "Yuanhao Huang", "Jie Liu"], "title": "AutoPCR: Automated Phenotype Concept Recognition by Prompting", "comment": null, "summary": "Phenotype concept recognition (CR) is a fundamental task in biomedical text\nmining, enabling applications such as clinical diagnostics and knowledge graph\nconstruction. However, existing methods often require ontology-specific\ntraining and struggle to generalize across diverse text types and evolving\nbiomedical terminology. We present AutoPCR, a prompt-based phenotype CR method\nthat does not require ontology-specific training. AutoPCR performs CR in three\nstages: entity extraction using a hybrid of rule-based and neural tagging\nstrategies, candidate retrieval via SapBERT, and entity linking through\nprompting a large language model. Experiments on four benchmark datasets show\nthat AutoPCR achieves the best average and most robust performance across both\nmention-level and document-level evaluations, surpassing prior state-of-the-art\nmethods. Further ablation and transfer studies demonstrate its inductive\ncapability and generalizability to new ontologies.", "AI": {"tldr": "本文介绍了一个不需要特定于本体训练的基于提示的表型概念识别方法AutoPCR，该方法在多个数据集上表现出色，并展示了良好的归纳和泛化能力。", "motivation": "现存方法往往需要特定于本体的训练，并且在不同文本类型和不断发展的生物医学术语中难以泛化。本研究提出AutoPCR，以解决这些问题。", "method": "AutoPCR采用三阶段方法进行表型概念识别：第一阶段通过规则和神经标签策略的组合进行实体提取；第二阶段通过SapBERT进行候选检索；第三阶段通过提示大型语言模型进行实体链接。这种方法不需要特定于本体的训练。", "result": "在四个基准数据集上的实验表明，AutoPCR在提及级别和文档级别评估中均表现出最优的平均性能和最鲁棒的性能，超过了之前的最先进的方法。进一步的消融和迁移研究表明，它具有归纳能力和对新本体的通用性。", "conclusion": "AutoPCR作为一个基于提示的表型概念识别方法，展示了在不同文本类型和不断发展的生物医学术语中进行通用识别的潜力。"}}
{"id": "2507.18838", "categories": ["cs.CV", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.18838", "abs": "https://arxiv.org/abs/2507.18838", "authors": ["Fabio De Sousa Ribeiro", "Omar Todd", "Charles Jones", "Avinash Kori", "Raghav Mehta", "Ben Glocker"], "title": "Flow Stochastic Segmentation Networks", "comment": "Accepted at ICCV 2025", "summary": "We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a\ngenerative segmentation model family featuring discrete-time autoregressive and\nmodern continuous-time flow variants. We prove fundamental limitations of the\nlow-rank parameterisation of previous methods and show that Flow-SSNs can\nestimate arbitrarily high-rank pixel-wise covariances without assuming the rank\nor storing the distributional parameters. Flow-SSNs are also more efficient to\nsample from than standard diffusion-based segmentation models, thanks to most\nof the model capacity being allocated to learning the base distribution of the\nflow, constituting an expressive prior. We apply Flow-SSNs to challenging\nmedical imaging benchmarks and achieve state-of-the-art results. Code\navailable: https://github.com/biomedia-mira/flow-ssn.", "AI": {"tldr": "Flow-SSN是一种新的生成分割模型，它克服了以前方法的限制，在医学图像分割领域取得了最先进的结果。", "motivation": "作者指出了以前方法在低秩参数化上的根本限制，并证明了Flow-SSNs可以无需假设秩或存储分布参数就能估计任意高秩像素间的协方差。", "method": "文章介绍了Flow Stochastic Segmentation Network (Flow-SSN)，一种采用离散时间自回归和现代连续时间流变体的生成分割模型家族。", "result": "实验中，Flow-SSNs应用于具有挑战性的医学成像基准上，取得了最先进的结果。", "conclusion": "Flow-SSN在采样效率方面优于传统的基于扩散的分割模型，因为其大部分模型容量分配给了学习流的基础分布，构成了一个表达丰富的先验。"}}
{"id": "2507.19353", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19353", "abs": "https://arxiv.org/abs/2507.19353", "authors": ["Kai Liu", "Zhan Su", "Peijie Dong", "Fengran Mo", "Jianfei Gao", "ShaoTing Zhang", "Kai Chen"], "title": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks", "comment": null, "summary": "Recently, recurrent large language models (Recurrent LLMs) with linear\ncomputational complexity have re-emerged as efficient alternatives to\nself-attention-based LLMs (Self-Attention LLMs), which have quadratic\ncomplexity. However, Recurrent LLMs often underperform on long-context tasks\ndue to their limited fixed-size memory. Previous research has primarily focused\non enhancing the memory capacity of Recurrent LLMs through architectural\ninnovations, but these approaches have not yet enabled Recurrent LLMs to match\nthe performance of Self-Attention LLMs on long-context tasks. We argue that\nthis limitation arises because processing the entire context at once is not\nwell-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a\nchunk-wise inference method inspired by human reading strategies. Smooth\nReading processes context in chunks and iteratively summarizes the contextual\ninformation, thereby reducing memory demands and making the approach more\ncompatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention\nLLMs on long-context tasks, while preserving the efficiency advantages of\nRecurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from\n5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.\nBesides, our method maintains the high efficiency, training 3x faster and\ninferring 2x faster at 64k context compared to Self-Attention LLMs. To our\nknowledge, this is the first work to achieve comparable performance using\nRecurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope\nour method will inspire future research in this area. To facilitate further\nprogress, we will release code and dataset.", "AI": {"tldr": "This paper introduces Smooth Reading, an innovative inference method for Recurrent LLMs, achieving comparable performance to Self-Attention LLMs on long-context tasks while preserving efficiency.", "motivation": "The goal is to bridge the performance gap between Recurrent and Self-Attention LLMs on long-context tasks by addressing the limited fixed-size memory of Recurrent models.", "method": "Smooth Reading, a chunk-wise inference method, is introduced to process context in chunks and iteratively summarize contextual information, reducing memory demands and improving performance on long-context tasks for Recurrent LLMs.", "result": "The proposed Smooth Reading method significantly narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, while maintaining the efficiency advantages of Recurrent LLMs.", "conclusion": "The paper concludes with the success of the Smooth Reading method in achieving comparable performance with Self-Attention LLMs on long-context tasks using Recurrent LLMs, marking a significant contribution to the field."}}
{"id": "2507.18848", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18848", "abs": "https://arxiv.org/abs/2507.18848", "authors": ["Beidi Zhao", "SangMook Kim", "Hao Chen", "Chen Zhou", "Zu-hua Gao", "Gang Wang", "Xiaoxiao Li"], "title": "PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis", "comment": null, "summary": "Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with\nthe complexity and heterogeneity of WSIs. Existing MIL methods face challenges\nin aggregating diverse patch information into robust WSI representations. While\nViTs and clustering-based approaches show promise, they are computationally\nintensive and fail to capture task-specific and slide-specific variability. To\naddress these limitations, we propose PTCMIL, a novel Prompt Token\nClustering-based ViT for MIL aggregation. By introducing learnable prompt\ntokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in\nan end-to-end manner. It dynamically aligns clustering with downstream tasks,\nusing projection-based clustering tailored to each WSI, reducing complexity\nwhile preserving patch heterogeneity. Through token merging and prototype-based\npooling, PTCMIL efficiently captures task-relevant patterns. Extensive\nexperiments on eight datasets demonstrate its superior performance in\nclassification and survival analysis tasks, outperforming state-of-the-art\nmethods. Systematic ablation studies confirm its robustness and strong\ninterpretability. The code is released at https://github.com/ubc-tea/PTCMIL.", "AI": {"tldr": "研究介绍了一种新的方法PTCMIL，它基于提示令牌聚类的ViT，用于MIL聚合，通过改进的聚类方法提升疗效分析性能，并显示出优异的表现和强大的解释性。代码已公开。", "motivation": "鉴于现有的MIL方法在聚合异构的组织切片信息到稳健的WSIs表示方面存在挑战，并且尽管ViTs和基于聚类的方法颇具前景，但其计算强度大且无法捕捉特定任务和切片特异性的变异性，研究者们提出了一种新的基于提示令牌聚类的ViT方法——PTCMIL来解决上述问题。", "method": "PTCMIL方法通过在ViT主干中引入可学习的提示令牌，将聚类和预测任务统一为端到端的方式。这种方法能够动态调整聚类与下游任务对齐，使用对应的投影聚类方法来适应每个WSI，从而减少复杂度同时保持切片异质性。通过令牌合并和原型池化，PTCMIL能够高效捕获任务相关的模式。", "result": "实验结果表明，在八个数据集上的广泛实验表明，PTCMIL在分类和生存分析任务中优于最新的方法。系统性消融研究表明，该方法具有鲁棒性和强大的可解释性。代码已开源。", "conclusion": "结论指出，PTCMIL在基于多个数据集的广泛实验中展示出卓越的分类和生存分析性能，并超越了最先进的方法。系统性消融研究还证实其具有强大的鲁棒性和良好的可解释性。"}}
{"id": "2507.19356", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.19356", "abs": "https://arxiv.org/abs/2507.19356", "authors": ["Hsuan-Yu Wang", "Pei-Ying Lee", "Berlin Chen"], "title": "Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization", "comment": "6 pages, 3 figures, to appear in the Proceedings of the 2025\n  International Conference on Asian Language Processing (IALP)", "summary": "In this paper, we investigate the impact of incorporating timestamp-based\nalignment between Automatic Speech Recognition (ASR) transcripts and Speaker\nDiarization (SD) outputs on Speech Emotion Recognition (SER) accuracy.\nMisalignment between these two modalities often reduces the reliability of\nmultimodal emotion recognition systems, particularly in conversational\ncontexts. To address this issue, we introduce an alignment pipeline utilizing\npre-trained ASR and speaker diarization models, systematically synchronizing\ntimestamps to generate accurately labeled speaker segments. Our multimodal\napproach combines textual embeddings extracted via RoBERTa with audio\nembeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating\nmechanism. Experimental evaluations on the IEMOCAP benchmark dataset\ndemonstrate that precise timestamp alignment improves SER accuracy,\noutperforming baseline methods that lack synchronization. The results highlight\nthe critical importance of temporal alignment, demonstrating its effectiveness\nin enhancing overall emotion recognition accuracy and providing a foundation\nfor robust multimodal emotion analysis.", "AI": {"tldr": "本文通过时间戳对齐提升了说话人转录和说话人分割之间的同步性，进而提高了多模态情感识别的准确性。", "motivation": "由于自动语音识别（ASR）转录和说话人分割（SD）输出之间的时间戳不同步，减少了多模态情绪识别系统在对话环境中的可靠性，本研究旨在解决这个问题。", "method": "我们提出了一种结合预训练的ASR和说话人分割模型的时间戳对齐流水线，以生成准确的时间戳标记的说话人片段。文本嵌入通过RoBERTa提取，音频嵌入通过Wav2Vec提取，再利用带有门限机制的交叉注意力融合方法。", "result": "在IEMOCAP基准数据集上的实验评估表明，精确的时间戳同步提高了情绪识别的准确性，超过了没有同步的基线方法。", "conclusion": "实证结果突显了时间同步的重要性，证明了其在提升总体情绪识别准确性中是有效的，并为稳健的多模态情绪分析提供了基础。"}}
{"id": "2507.18863", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18863", "abs": "https://arxiv.org/abs/2507.18863", "authors": ["Matthew Kit Khinn Teng", "Haibo Zhang", "Takeshi Saitoh"], "title": "Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction", "comment": "10 pages, 3 figures", "summary": "Visual Automatic Speech Recognition (V-ASR) is a challenging task that\ninvolves interpreting spoken language solely from visual information, such as\nlip movements and facial expressions. This task is notably challenging due to\nthe absence of auditory cues and the visual ambiguity of phonemes that exhibit\nsimilar visemes-distinct sounds that appear identical in lip motions. Existing\nmethods often aim to predict words or characters directly from visual cues, but\nthey commonly suffer from high error rates due to viseme ambiguity and require\nlarge amounts of pre-training data. We propose a novel phoneme-based two-stage\nframework that fuses visual and landmark motion features, followed by an LLM\nmodel for word reconstruction to address these challenges. Stage 1 consists of\nV-ASR, which outputs the predicted phonemes, thereby reducing training\ncomplexity. Meanwhile, the facial landmark features address speaker-specific\nfacial characteristics. Stage 2 comprises an encoder-decoder LLM model, NLLB,\nthat reconstructs the output phonemes back to words. Besides using a large\nvisual dataset for deep learning fine-tuning, our PV-ASR method demonstrates\nsuperior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the\nLRS3 dataset.", "AI": {"tldr": "A two-stage phoneme-based V-ASR framework improves visual speech recognition by leveraging visual information and facial landmarks, followed by phoneme reconstruction to words using an LLM model.", "motivation": "The motivation is to improve the accuracy of V-ASR by addressing high error rates due to viseme ambiguity and reducing the need for large pre-training data using a phoneme-based approach.", "method": "Our PV-ASR method uses a two-stage framework, in the first stage, it predicts phonemes using visual and facial landmark information, in the second stage, an LLM model reconstructs phonemes back into words.", "result": "The PV-ASR method achieved a 17.4% WER on the LRS2 dataset and 21.0% WER on the LRS3 dataset, demonstrating an improvement over existing methods.", "conclusion": "Our novel phoneme-based two-stage framework for V-ASR shows better performance than direct word or character prediction methods by effectively utilizing visual and facial landmark features followed by phoneme-to-word reconstruction using an LLM model."}}
{"id": "2507.19361", "categories": ["cs.CL", "cs.AI", "cs.SC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.19361", "abs": "https://arxiv.org/abs/2507.19361", "authors": ["Zhen Wan", "Chao-Han Huck Yang", "Yahan Yu", "Jinchuan Tian", "Sheng Li", "Ke Hu", "Zhehuai Chen", "Shinji Watanabe", "Fei Cheng", "Chenhui Chu", "Sadao Kurohashi"], "title": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models", "comment": "Our Speech-IQ leaderboard will be hosted at\n  huggingface.co/spaces/nvidia/Speech-IQ-leaderboard. ACL 2025 main", "summary": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human\ncognition-inspired evaluation pipeline for voice understanding large language\nmodels, LLM Voice, designed to assess their voice understanding ability. Moving\nbeyond popular voice understanding metrics such as word error rate (WER), SIQ\nexamines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:\n(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,\nsimilarity of LLM's interpretations); and (3) Application (i.e., QA accuracy\nfor simulating downstream tasks). We demonstrate that SIQ not only quantifies\nvoice understanding abilities but also provides unified comparisons between\ncascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation\nerrors in existing benchmarks, and detects hallucinations in LLM Voice. Our\nframework represents a first-of-its-kind intelligence examination that bridges\ncognitive principles with voice-oriented benchmarks, while exposing overlooked\nchallenges in multi-modal training.", "AI": {"tldr": "A new evaluation framework called SIQ is introduced, designed to assess voice understanding capabilities of LLM Voice models at multiple cognitive levels, enhancing the evaluation beyond word error rate accuracy.", "motivation": "The motivation behind this work is to provide an improved evaluation framework that goes beyond simple accuracy measures, offering a more comprehensive and principled evaluation of voice-understanding capabilities in LLM Voice.", "method": "We introduce Speech-based Intelligence Quotient (SIQ), a new evaluation pipeline designed to measure voice understanding abilities of language models, specifically LLM Voice. SIQ is inspired by Bloom's Taxonomy, dividing the evaluation into three cognitive levels: Remembering, Understanding, and Application, and it extends beyond typical metrics like Word Error Rate (WER).", "result": "SIQ is shown to be effective in quantifying voice understanding abilities, comparing different modeling architectures, identifying annotation errors in datasets, and recognizing hallucinations in generated content.", "conclusion": "Our framework, SIQ, is a novel intelligence examination framework that integrates principles of human cognition with voice understanding benchmarks, addressing challenges that were previously not fully recognized in multi-modal training of voice understanding systems."}}
{"id": "2507.18870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18870", "abs": "https://arxiv.org/abs/2507.18870", "authors": ["Keke Tang", "Yuze Gao", "Weilong Peng", "Xiaofei Wang", "Meie Fang", "Peican Zhu"], "title": "Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform", "comment": null, "summary": "Studying adversarial attacks on point clouds is essential for evaluating and\nimproving the robustness of 3D deep learning models. However, most existing\nattack methods are developed under ideal white-box settings and often suffer\nfrom limited transferability to unseen models and insufficient robustness\nagainst common defense mechanisms. In this paper, we propose MAT-Adv, a novel\nadversarial attack framework that enhances both transferability and\nundefendability by explicitly perturbing the medial axis transform (MAT)\nrepresentations, in order to induce inherent adversarialness in the resulting\npoint clouds. Specifically, we employ an autoencoder to project input point\nclouds into compact MAT representations that capture the intrinsic geometric\nstructure of point clouds. By perturbing these intrinsic representations,\nMAT-Adv introduces structural-level adversarial characteristics that remain\neffective across diverse models and defense strategies. To mitigate overfitting\nand prevent perturbation collapse, we incorporate a dropout strategy into the\noptimization of MAT perturbations, further improving transferability and\nundefendability. Extensive experiments demonstrate that MAT-Adv significantly\noutperforms existing state-of-the-art methods in both transferability and\nundefendability. Codes will be made public upon paper acceptance.", "AI": {"tldr": "本文提出了MAT-Adv，一个增强点云对抗攻击传输性和难以防御性的框架，通过扰动点云的中轴变换表示来实现结构级的对抗攻击特性。实验表明，这种方法在传输性和难以防御性方面优于现有方法。", "motivation": "研究点云对抗攻击对于评估和提高3D深度学习模型的鲁棒性是至关重要的。然而，大多数现有的攻击方法都是在理想的白盒环境下开发的，通常在转移到未知模型时或面对通用防御机制时效率不高。因此，提出一个能在多种模型和防御策略下都保持有效性的对抗攻击框架是有必要的。", "method": "MAT-Adv框架通过扰动中轴变换(MAT)表示来提升点云对抗攻击的传输性和难以防御性，首先使用自动编码器将输入点云投影到紧凑的MAT表示中，该表示捕捉点云的内在几何结构。通过扰动这些内在表示，MAT-Adv在结果点云中引入结构级的对抗特性，这在多种模型和防御策略中仍然有效。为了缓解过度拟合和防止扰动崩溃，作者还引入了dropout策略到MAT扰动优化中，进一步提升传输性和难以防御性。", "result": "大量实验表明，MAT-Adv在传输性和难以防御性方面显著优于现有的最先进方法。", "conclusion": "MAT-Adv框架通过改进点云内在几何结构的表示，实现了跨越多种模型和防御策略的有效对抗攻击，其传输性和难以防御性显著优于现有方法。"}}
{"id": "2507.19374", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.19374", "abs": "https://arxiv.org/abs/2507.19374", "authors": ["Penny Karanasou", "Mengjie Qian", "Stefano Bannò", "Mark J. F. Gales", "Kate M. Knill"], "title": "Data Augmentation for Spoken Grammatical Error Correction", "comment": "This work has been accepted by ISCA SLaTE 2025", "summary": "While there exist strong benchmark datasets for grammatical error correction\n(GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still\nunder-resourced. In this paper, we propose a fully automated method to generate\naudio-text pairs with grammatical errors and disfluencies. Moreover, we propose\na series of objective metrics that can be used to evaluate the generated data\nand choose the more suitable dataset for SGEC. The goal is to generate an\naugmented dataset that maintains the textual and acoustic characteristics of\nthe original data while providing new types of errors. This augmented dataset\nshould augment and enrich the original corpus without altering the language\nassessment scores of the second language (L2) learners. We evaluate the use of\nthe augmented corpus both for written GEC (the text part) and for SGEC (the\naudio-text pairs). Our experiments are conducted on the S\\&I Corpus, the first\npublicly available speech dataset with grammar error annotations.", "AI": {"tldr": "本文提出了一种全自动方法来生成带有语法错误和口吃现象的音频文本对，并提出了一套评价指标。目标生成的新数据集能够扩充原始语料库而不影响二语学习者的语言评估分数。", "motivation": "尽管在语法错误纠正(GEC)领域存在强大的基准数据集，但对于口说GEC (SGEC)的高质量标注数据集仍然稀缺。此项目动机在于构建一个维持原文本和声学特性的同时提供新的错误类型的扩增数据集。", "method": "我们提出了一种全自动的方法来生成带有语法错误和口吃现象的音轨文本对。我们还提出了一系列可用于评估生成数据并选择更适合SGEC的数据集的客观指标。", "result": "实验在S&I语料库上进行，这是第一个具有语法错误注释的公开语音数据集。研究评估了扩增语料库对书面GEC（文本部分）和SGEC（音轨文本对）的使用效果。", "conclusion": "通过扩增数据集，我们可以有效地增强原始数据集，同时不影响二语学习者的语言评估分数，这对于SGEC和GEC都是非常有用的。"}}
{"id": "2507.18881", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18881", "abs": "https://arxiv.org/abs/2507.18881", "authors": ["Bolei Chen", "Jiaxu Kang", "Haonan Yang", "Ping Zhong", "Jianxin Wang"], "title": "Perspective from a Higher Dimension: Can 3D Geometric Priors Help Visual Floorplan Localization?", "comment": "Accepted by ACM MM 2025", "summary": "Since a building's floorplans are easily accessible, consistent over time,\nand inherently robust to changes in visual appearance, self-localization within\nthe floorplan has attracted researchers' interest. However, since floorplans\nare minimalist representations of a building's structure, modal and geometric\ndifferences between visual perceptions and floorplans pose challenges to this\ntask. While existing methods cleverly utilize 2D geometric features and pose\nfilters to achieve promising performance, they fail to address the localization\nerrors caused by frequent visual changes and view occlusions due to variously\nshaped 3D objects. To tackle these issues, this paper views the 2D Floorplan\nLocalization (FLoc) problem from a higher dimension by injecting 3D geometric\npriors into the visual FLoc algorithm. For the 3D geometric prior modeling, we\nfirst model geometrically aware view invariance using multi-view constraints,\ni.e., leveraging imaging geometric principles to provide matching constraints\nbetween multiple images that see the same points. Then, we further model the\nview-scene aligned geometric priors, enhancing the cross-modal geometry-color\ncorrespondences by associating the scene's surface reconstruction with the RGB\nframes of the sequence. Both 3D priors are modeled through self-supervised\ncontrastive learning, thus no additional geometric or semantic annotations are\nrequired. These 3D priors summarized in extensive realistic scenes bridge the\nmodal gap while improving localization success without increasing the\ncomputational burden on the FLoc algorithm. Sufficient comparative studies\ndemonstrate that our method significantly outperforms state-of-the-art methods\nand substantially boosts the FLoc accuracy. All data and code will be released\nafter the anonymous review.", "AI": {"tldr": "本文提出了一种通过引入3D几何先验来解决基于平面图定位问题的新方法，这种方法提高了定位的准确性和鲁棒性，同时没有增加计算负担。", "motivation": "由于建筑平面图容易获取、随时间变化一致，并且对视觉外观的变化具有固有的鲁棒性，因此基于平面图的自主定位吸引了研究者的兴趣。然而，平面图通常是建筑结构的简化表示，因此在视觉感知与平面图之间的模式和几何差异对这个问题构成挑战。现有方法通过利用2D几何特征和姿态滤波器实现有希望的效果，但无法解决由于各种3D物体导致的频繁视觉变化和视角遮挡所引起的定位误差。", "method": "本论文通过引入3D几何先验来解决现有方法无法处理的定位误差问题。具体来说，首先利用多视角约束建模几何感知视点不变性，并利用成像几何原理提供多张图像之间的匹配约束。然后，通过自监督对比学习建模视图-场景对齐的几何先验，增强跨模态几何-颜色对应关系，将场景的表面重建与图像序列的RGB帧相关联。", "result": "所提出的方法在多个实际场景中进行了验证，并取得了显著的定位准确性提升。", "conclusion": "综合比较研究表明，该方法明显优于现有的最先进方法，并且在不增加FLoc算法计算负担的情况下大幅提升了FLoc的准确性。"}}
{"id": "2507.19396", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19396", "abs": "https://arxiv.org/abs/2507.19396", "authors": ["Rachel M. Murphy", "Nishant Mishra", "Nicolette F. de Keizer", "Dave A. Dongelmans", "Kitty J. Jager", "Ameen Abu-Hanna", "Joanna E. Klopotowska", "Iacer Calixto"], "title": "Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study", "comment": "30 Pages, 5 Figures (Main Paper), 19 Pages, 2 Figures(Supplements).\n  Rachel M. Murphy and Nishant Mishra are shared first authors. Joanna E.\n  Klopotowska and Iacer Calixto are shared last authors", "summary": "In this study, we set a benchmark for adverse drug event (ADE) detection in\nDutch clinical free text documents using several transformer models, clinical\nscenarios and fit-for-purpose performance measures. We trained a Bidirectional\nLong Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or\nmultilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the\ntasks of named entity recognition (NER) and relation classification (RC) using\n102 richly annotated Dutch ICU clinical progress notes. Anonymized free text\nclinical progress notes of patients admitted to intensive care unit (ICU) of\none academic hospital and discharge letters of patients admitted to Internal\nMedicine wards of two non-academic hospitals were reused. We evaluated our ADE\nRC models internally using gold standard (two-step task) and predicted entities\n(end-to-end task). In addition, all models were externally validated on\ndetecting ADEs at the document level. We report both micro- and macro-averaged\nF1 scores, given the imbalance of ADEs in the datasets. Although differences\nfor the ADE RC task between the models were small, MedRoBERTa.nl was the best\nperforming model with macro-averaged F1 score of 0.63 using gold standard and\n0.62 using predicted entities. The MedRoBERTa.nl models also performed the best\nin our external validation and achieved recall of between 0.67 to 0.74 using\npredicted entities, meaning between 67 to 74% of discharge letters with ADEs\nwere detected. Our benchmark study presents a robust and clinically meaningful\napproach for evaluating language models for ADE detection in clinical free text\ndocuments. Our study highlights the need to use appropriate performance\nmeasures fit for the task of ADE detection in clinical free-text documents and\nenvisioned future clinical use.", "AI": {"tldr": "研究通过几种transformer模型，临床场景及合适性能度量为荷兰语临床自由文本中的ADE检测设定了基准，MedRoBERTa.nl模型取得了最佳结果。", "motivation": "该研究旨在为荷兰临床自由文本文件中的不良药物事件（ADE）检测设置基准，并使用适合目的的性能度量。", "method": "该研究采用Bidirectional Long Short-Term Memory (Bi-LSTM)模型和四种基于transformer的荷兰语和/或多语言编码模型（BERTje, RobBERT, MedRoBERTa.nl, 和NuNER）进行命名实体识别（NER）和关系分类（RC）任务。", "result": "结果表明，在ADE RC任务中模型之间的差异很小，但MedRoBERTa.nl表现出色，使用金标准和预测实体的宏平均F1分数分别为0.63和0.62。MedRoBERTa.nl模型在外部验证中也表现最佳，召回率在0.67到0.74之间。", "conclusion": "该研究提出了一种稳健且具有临床意义的方法，用于评估语言模型在临床自由文本文件中检测ADE的性能，并强调需要使用适合任务的性能度量。"}}
{"id": "2507.18895", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18895", "abs": "https://arxiv.org/abs/2507.18895", "authors": ["Vangelis Kostoulas", "Arthur Guijt", "Ellen M. Kerkhof", "Bradley R. Pieters", "Peter A. N. Bosman", "Tanja Alderliesten"], "title": "Dealing with Segmentation Errors in Needle Reconstruction for MRI-Guided Brachytherapy", "comment": "Published in: Proc. SPIE Medical Imaging 2025, Vol. 13408, 1340826", "summary": "Brachytherapy involves bringing a radioactive source near tumor tissue using\nimplanted needles. Image-guided brachytherapy planning requires amongst others,\nthe reconstruction of the needles. Manually annotating these needles on patient\nimages can be a challenging and time-consuming task for medical professionals.\nFor automatic needle reconstruction, a two-stage pipeline is commonly adopted,\ncomprising a segmentation stage followed by a post-processing stage. While deep\nlearning models are effective for segmentation, their results often contain\nerrors. No currently existing post-processing technique is robust to all\npossible segmentation errors. We therefore propose adaptations to existing\npost-processing techniques mainly aimed at dealing with segmentation errors and\nthereby improving the reconstruction accuracy. Experiments on a prostate cancer\ndataset, based on MRI scans annotated by medical professionals, demonstrate\nthat our proposed adaptations can help to effectively manage segmentation\nerrors, with the best adapted post-processing technique achieving median\nneedle-tip and needle-bottom point localization errors of $1.07$ (IQR $\\pm\n1.04$) mm and $0.43$ (IQR $\\pm 0.46$) mm, respectively, and median shaft error\nof $0.75$ (IQR $\\pm 0.69$) mm with 0 false positive and 0 false negative\nneedles on a test set of 261 needles.", "AI": {"tldr": "The paper enhances automatic needle reconstruction for brachytherapy by improving post-processing techniques to better correct segmentation errors.", "motivation": "Current post-processing techniques are not robust to all segmentation errors, which is a significant issue in the accuracy of needle reconstruction in image-guided brachytherapy.", "method": "A two-stage pipeline for automatic needle reconstruction in brachytherapy is improved by adapting existing post-processing techniques to better handle segmentation errors derived from deep learning models.", "result": "Experiments show that the proposed adaptations reduce needle-tip and needle-bottom point localization errors and shaft errors with no false positives or negatives on a test set of 261 needles.", "conclusion": "The proposed adaptations to post-processing techniques effectively manage segmentation errors and improve the accuracy of needle reconstruction in brachytherapy planning."}}
{"id": "2507.19407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19407", "abs": "https://arxiv.org/abs/2507.19407", "authors": ["Mohammad Khodadad", "Ali Shiraee", "Mahdi Astaraki", "Hamidreza Mahyar"], "title": "Towards Domain Specification of Embedding Models in Medicine", "comment": null, "summary": "Medical text embedding models are foundational to a wide array of healthcare\napplications, ranging from clinical decision support and biomedical information\nretrieval to medical question answering, yet they remain hampered by two\ncritical shortcomings. First, most models are trained on a narrow slice of\nmedical and biological data, beside not being up to date in terms of\nmethodology, making them ill suited to capture the diversity of terminology and\nsemantics encountered in practice. Second, existing evaluations are often\ninadequate: even widely used benchmarks fail to generalize across the full\nspectrum of real world medical tasks.\n  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned\non diverse medical corpora through self-supervised contrastive learning across\nmultiple data sources, to deliver robust medical text embeddings.\n  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks\nspanning classification, clustering, pair classification, and retrieval modeled\non the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of\nmedical text. Our results demonstrate that this combined approach not only\nestablishes a robust evaluation framework but also yields embeddings that\nconsistently outperform state of the art alternatives in different tasks.", "AI": {"tldr": "研究介绍了MEDTE模型，该模型通过自我监督对比学习在多个数据源上进行了广泛微调，解决了现有医疗文本嵌入模型数据不足和评估基准不足的问题。同时，提出了一个涵盖51个任务的基准测试套件，结果表明该方法优于现有方法。", "motivation": "研究动机是解决现有医疗文本嵌入模型训练数据不足和评估基准不全面的问题。", "method": "医疗文本嵌入模型在各种医疗应用中至关重要，但存在训练数据有限且方法过时、评估基准不足等问题。为此，研究提出并利用了MEDTE，这是一个经过自我监督对比学习在多个数据源上广泛微调的GTE模型，以提供强大的医疗文本嵌入。", "result": "该研究结果表明，使用提出的MEDTE模型及其配套的基准测试套件能够生成优于现有方法的文本嵌入。", "conclusion": "该研究证明了利用自我监督对比学习微调的模型和一个涵盖多个任务的基准套件可以提供更优质的医疗文本嵌入，并确立了一个强大的评价框架。"}}
{"id": "2507.18911", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18911", "abs": "https://arxiv.org/abs/2507.18911", "authors": ["Zhihao Luo", "Luojun Lin", "Zheng Lin"], "title": "Synthetic-to-Real Camouflaged Object Detection", "comment": null, "summary": "Due to the high cost of collection and labeling, there are relatively few\ndatasets for camouflaged object detection (COD). In particular, for certain\nspecialized categories, the available image dataset is insufficiently\npopulated. Synthetic datasets can be utilized to alleviate the problem of\nlimited data to some extent. However, directly training with synthetic datasets\ncompared to real datasets can lead to a degradation in model performance. To\ntackle this problem, in this work, we investigate a new task, namely\nSyn-to-Real Camouflaged Object Detection (S2R-COD). In order to improve the\nmodel performance in real world scenarios, a set of annotated synthetic\ncamouflaged images and a limited number of unannotated real images must be\nutilized. We propose the Cycling Syn-to-Real Domain Adaptation Framework\n(CSRDA), a method based on the student-teacher model. Specially, CSRDA\npropagates class information from the labeled source domain to the unlabeled\ntarget domain through pseudo labeling combined with consistency regularization.\nConsidering that narrowing the intra-domain gap can improve the quality of\npseudo labeling, CSRDA utilizes a recurrent learning framework to build an\nevolving real domain for bridging the source and target domain. Extensive\nexperiments demonstrate the effectiveness of our framework, mitigating the\nproblem of limited data and handcraft annotations in COD. Our code is publicly\navailable at: https://github.com/Muscape/S2R-COD", "AI": {"tldr": "本文提出一种新的任务S2R-COD，以及相应的Cycling Syn-to-Real Domain Adaptation Framework (CSRDA) 方法，以改善基于合成数据训练模型在现实场景中的性能，并通过实验验证了该方法的有效性。", "motivation": "由于收集和标注成本高，可供隐蔽物体检测使用的数据集相对较少，特别是某些专业类别。直接使用合成数据集训练相比于真实数据集可能导致模型性能下降。因此，本文提出了一种新的任务，名为Syn-to-Real Camouflaged Object Detection (S2R-COD)，旨在改善在现实场景中的模型性能。", "method": "本文提出了一个名为Cycling Syn-to-Real Domain Adaptation Framework (CSRDA) 的方法，该方法基于学生-教师模型，通过伪标签结合一致性正则化将标注源域中的类别信息传播到无标注的目标域。为了提高伪标签的质量，CSRDA采用了一个递归学习框架来构建一个演变的真实域，从而缩小域内差距，连接源域和目标域。", "result": "实验结果展示了该框架的有效性，可以缓解在隐蔽物体检测中的数据有限和手工注释的问题。", "conclusion": "本文提出的方法CSRDA能够在标注合成隐蔽图像集和有限数量的未标注真实图像的基础上，改善现实中隐蔽物体检测的模型性能，从而缓解了该领域的数据不足问题。"}}
{"id": "2507.19419", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19419", "abs": "https://arxiv.org/abs/2507.19419", "authors": ["Mohammad Aflah Khan", "Ameya Godbole", "Johnny Tian-Zheng Wei", "Ryan Wang", "James Flemings", "Krishna Gummadi", "Willie Neiswanger", "Robin Jia"], "title": "TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability", "comment": null, "summary": "Understanding the relationship between training data and model behavior\nduring pretraining is crucial, but existing workflows make this process\ncumbersome, fragmented, and often inaccessible to researchers. We present\nTokenSmith, an open-source library for interactive editing, inspection, and\nanalysis of datasets used in Megatron-style pretraining frameworks such as\nGPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of\noperations including searching, viewing, ingesting, exporting, inspecting, and\nsampling data, all accessible through a simple user interface and a modular\nbackend. It also enables structured editing of pretraining data without\nrequiring changes to training code, simplifying dataset debugging, validation,\nand experimentation.\n  TokenSmith is designed as a plug and play addition to existing large language\nmodel pretraining workflows, thereby democratizing access to production-grade\ndataset tooling. TokenSmith is hosted on GitHub1, with accompanying\ndocumentation and tutorials. A demonstration video is also available on\nYouTube.", "AI": {"tldr": "研究团队开发了TokenSmith，一个针对Megatron风格预训练框架的开源库，简化了训练数据集的编辑、检查和实验过程，提高了研究效率。", "motivation": "动机是理解训练数据与模型行为之间的关系对于预训练过程至关重要，但现有的工作流程使这一过程变得繁琐、分散且难以被研究者访问。", "method": "分析方法主要包括开发了一个名为TokenSmith的开源库，用于交互编辑、检查和分析用于Megatron风格预训练框架的训练数据集。该库支持多种操作，如搜索、查看、数据导入和导出等，并且提供了一个简单用户界面及模块化后台支持。", "result": "TokenSmith为研究人员提供了一个即插即用的工具，可以简化大型语言模型预训练中的数据集调试、验证和实验。这个工具在GitHub上提供，并附有文档和教程。", "conclusion": "TokenSmith的开发是为了作为现有大型语言模型预训练工作流程的即插即用补充，旨在让更多的研究者能够访问生产级数据集工具。"}}
{"id": "2507.18921", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18921", "abs": "https://arxiv.org/abs/2507.18921", "authors": ["Elham Soltani Kazemi", "Imad Eddine Toubal", "Gani Rahmon", "Jaired Collins", "K. Palaniappan"], "title": "HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback", "comment": "submit/6651762", "summary": "Video Object Segmentation (VOS) is foundational to numerous computer vision\napplications, including surveillance, autonomous driving, robotics and\ngenerative video editing. However, existing VOS models often struggle with\nprecise mask delineation, deformable objects, topologically transforming\nobjects, tracking drift and long video sequences. In this paper, we introduce\nHQ-SMem, for High Quality video segmentation and tracking using Smart Memory, a\nnovel method that enhances the performance of VOS base models by addressing\nthese limitations. Our approach incorporates three key innovations: (i)\nleveraging SAM with High-Quality masks (SAM-HQ) alongside appearance-based\ncandidate-selection to refine coarse segmentation masks, resulting in improved\nobject boundaries; (ii) implementing a dynamic smart memory mechanism that\nselectively stores relevant key frames while discarding redundant ones, thereby\noptimizing memory usage and processing efficiency for long-term videos; and\n(iii) dynamically updating the appearance model to effectively handle complex\ntopological object variations and reduce drift throughout the video. These\ncontributions mitigate several limitations of existing VOS models including,\ncoarse segmentations that mix-in background pixels, fixed memory update\nschedules, brittleness to drift and occlusions, and prompt ambiguity issues\nassociated with SAM. Extensive experiments conducted on multiple public\ndatasets and state-of-the-art base trackers demonstrate that our method\nconsistently ranks among the top two on VOTS and VOTSt 2024 datasets. Moreover,\nHQ-SMem sets new benchmarks on Long Video Dataset and LVOS, showcasing its\neffectiveness in challenging scenarios characterized by complex multi-object\ndynamics over extended temporal durations.", "AI": {"tldr": "本文介绍了HQ-SMem，一种用于高质量视频分割和追踪的智能记忆方法，通过三个关键创新解决了现有视频对象分割模型的诸多限制：SAM-HQ与外观候选选择结合提高边界精度，动态智能记忆机制优化长期视频处理，以及动态更新外观模型以有效处理复杂对象变化。实验表明该方法在多个公开数据集上表现超越现有模型。", "motivation": "由于现有视频对象分割模型在精准分割遮罩、可变形对象处理、对象拓扑变化以及长期视频序列等问题上存在不足，本文旨在开发一种新型方法来解决上述问题，提升视频分割和追踪的质量。", "method": "HQ-SMem方法通过以下三个方面改进视频对象分割：使用SAM-HQ结合外观候选选择来提高粗分割遮罩的质量；实现可以选择性存储关键帧并排除冗余帧的动态智能记忆机制；动态更新外观模型以处理复杂的对象变化并减少漂移。", "result": "在多个公开数据集上进行的广泛实验显示，HQ-SMem方法在VOTS和VOTSt 2024数据集中表现最佳，同时也为Long Video Dataset和LVOS数据集设立了新的基准。", "conclusion": "HQ-SMem方法通过创新技术显著提升了视频对象分割的性能，特别是在复杂多对象动态变化的长期视频中效果显著，设定新的性能标杆。"}}
{"id": "2507.19457", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE", "I.2.7; I.2.6; I.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2507.19457", "abs": "https://arxiv.org/abs/2507.19457", "authors": ["Lakshya A Agrawal", "Shangyin Tan", "Dilara Soylu", "Noah Ziems", "Rishi Khare", "Krista Opsahl-Ong", "Arnav Singhvi", "Herumb Shandilya", "Michael J Ryan", "Meng Jiang", "Christopher Potts", "Koushik Sen", "Alexandros G. Dimakis", "Ion Stoica", "Dan Klein", "Matei Zaharia", "Omar Khattab"], "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.", "AI": {"tldr": "The paper introduces GEPA, a prompt optimizer that uses natural language reflection to improve LLM performance, achieving better results with fewer rollouts than current methods.", "motivation": "To demonstrate that the interpretable nature of language can provide a richer learning medium for LLMs compared with policy gradients derived from sparse, scalar rewards, aiming for a more efficient learning process.", "method": "GEPA (Genetic-Pareto), a prompt optimizer that uses natural language reflection to diagnose problems, propose, and test prompt updates based on sampled trajectories of the system.", "result": "GEPA outperforms GRPO by 10% on average and up to 20% while using 35x fewer rollouts, and surpasses MIPROv2 by over 10%.", "conclusion": "The utilization of language in the form of natural language reflection proves to be an effective strategy for optimizing LLMs, leading to significant improvements with minimal resource consumption."}}
{"id": "2507.18923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18923", "abs": "https://arxiv.org/abs/2507.18923", "authors": ["Zhentao Huang", "Di Wu", "Zhenbang He", "Minglun Gong"], "title": "Gaussian Set Surface Reconstruction through Per-Gaussian Optimization", "comment": null, "summary": "3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its\nflexible representation, yet fails to accurately reconstruct scene geometry.\nWhile modern variants like PGSR introduce additional losses to ensure proper\ndepth and normal maps through Gaussian fusion, they still neglect individual\nplacement optimization. This results in unevenly distributed Gaussians that\ndeviate from the latent surface, complicating both reconstruction refinement\nand scene editing. Motivated by pioneering work on Point Set Surfaces, we\npropose Gaussian Set Surface Reconstruction (GSSR), a method designed to\ndistribute Gaussians evenly along the latent surface while aligning their\ndominant normals with the surface normal. GSSR enforces fine-grained geometric\nalignment through a combination of pixel-level and Gaussian-level single-view\nnormal consistency and multi-view photometric consistency, optimizing both\nlocal and global perspectives. To further refine the representation, we\nintroduce an opacity regularization loss to eliminate redundant Gaussians and\napply periodic depth- and normal-guided Gaussian reinitialization for a\ncleaner, more uniform spatial distribution. Our reconstruction results\ndemonstrate significantly improved geometric precision in Gaussian placement,\nenabling intuitive scene editing and efficient generation of novel\nGaussian-based 3D environments. Extensive experiments validate GSSR's\neffectiveness, showing enhanced geometric accuracy while preserving\nhigh-quality rendering performance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.19470", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19470", "abs": "https://arxiv.org/abs/2507.19470", "authors": ["Son Quoc Tran", "Tushaar Gangavarapu", "Nicholas Chernogor", "Jonathan P. Chang", "Cristian Danescu-Niculescu-Mizil"], "title": "Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models", "comment": "Code and data available as part of ConvoKit:\n  https://convokit.cornell.edu", "summary": "We often rely on our intuition to anticipate the direction of a conversation.\nEndowing automated systems with similar foresight can enable them to assist\nhuman-human interactions. Recent work on developing models with this predictive\ncapacity has focused on the Conversations Gone Awry (CGA) task: forecasting\nwhether an ongoing conversation will derail. In this work, we revisit this task\nand introduce the first uniform evaluation framework, creating a benchmark that\nenables direct and reliable comparisons between different architectures. This\nallows us to present an up-to-date overview of the current progress in CGA\nmodels, in light of recent advancements in language modeling. Our framework\nalso introduces a novel metric that captures a model's ability to revise its\nforecast as the conversation progresses.", "AI": {"tldr": "本文重新审视了如何预测对话是否会偏离正轨的问题，引入了首个统一的评估框架和新的度量标准，以改进这项预测任务。", "motivation": "我们希望通过赋予自动化系统类似的预测能力，使其能够辅助人类之间的互动。我们重新审视了这个任务，并引入了首个统一的评估框架。", "method": "我们引入了首个统一的评估框架，创建了一个基准，使不同的架构之间能够进行直接和可靠的比较。此外，我们的框架还引入了一种新的度量标准，捕捉模型随对话进行修订其预测的能力。", "result": "我们介绍了首个统一的评估框架，创建了一个基准，使不同的架构之间能够进行直接和可靠的比较。", "conclusion": "通过重新审视CGA任务并引入新的评估方法，我们提供了对当前CGA模型进展的最新概述，并为语言模型的未来发展指明了方向。"}}
{"id": "2507.18925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18925", "abs": "https://arxiv.org/abs/2507.18925", "authors": ["Heitor R. Medeiros", "Atif Belal", "Masih Aminbeidokhti", "Eric Granger", "Marco Pedersoli"], "title": "WiSE-OD: Benchmarking Robustness in Infrared Object Detection", "comment": "8 pages, conference", "summary": "Object detection (OD) in infrared (IR) imagery is critical for low-light and\nnighttime applications. However, the scarcity of large-scale IR datasets forces\nmodels to rely on weights pre-trained on RGB images. While fine-tuning on IR\nimproves accuracy, it often compromises robustness under distribution shifts\ndue to the inherent modality gap between RGB and IR. To address this, we\nintroduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)\nbenchmarks built by applying corruption to standard IR datasets. Additionally,\nto fully leverage the complementary knowledge from RGB and infrared trained\nmodels, we propose WiSE-OD, a weight-space ensembling method with two variants:\nWiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and\nWiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across\nthree RGB-pretrained detectors and two robust baselines, WiSE-OD improves both\ncross-modality and corruption robustness without any additional training or\ninference cost.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.19054", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19054", "abs": "https://arxiv.org/abs/2507.19054", "authors": ["Binxu Li", "Yuhui Zhang", "Xiaohan Wang", "Weixin Liang", "Ludwig Schmidt", "Serena Yeung-Levy"], "title": "Closing the Modality Gap for Mixed Modality Search", "comment": "Project page: https://yuhui-zh15.github.io/MixedModalitySearch/", "summary": "Mixed modality search -- retrieving information across a heterogeneous corpus\ncomposed of images, texts, and multimodal documents -- is an important yet\nunderexplored real-world application. In this work, we investigate how\ncontrastive vision-language models, such as CLIP, perform on the mixed modality\nsearch task. Our analysis reveals a critical limitation: these models exhibit a\npronounced modality gap in the embedding space, where image and text embeddings\nform distinct clusters, leading to intra-modal ranking bias and inter-modal\nfusion failure. To address this issue, we propose GR-CLIP, a lightweight\npost-hoc calibration method that removes the modality gap in CLIP's embedding\nspace. Evaluated on MixBench -- the first benchmark specifically designed for\nmixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points\nover CLIP, surpasses recent vision-language generative embedding models by 4\npercentage points, while using 75x less compute.", "AI": {"tldr": "研究发现CLIP等对比视觉语言模型在多模态搜索中存在模态差距问题，提出GR-CLIP能够有效解决此问题，并在评估中取得显著改善。", "motivation": "研究对比视觉语言模型在多模态搜索任务上的性能，发现这些模型在嵌入空间中存在一个关键限制，即模态差距。", "method": "通过提出GR-CLIP这一轻量级的后处理校准方法来解决对比视觉语言模型中的模态差距问题。GR-CLIP能够在CLIP嵌入空间中去除模态差距。", "result": "在MixBench上评估，相比于CLIP，GR-CLIP将NDCG@10提高了多达26个百分点，同时也超越了最近的视觉语言生成嵌入模型4个百分点，计算量减少了75倍。", "conclusion": "GR-CLIP方法有效地解决了对比视觉语言模型中存在的模态嵌入差距问题，提高了多模态搜索任务的性能，并具有显著的计算效率。"}}
{"id": "2507.18929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18929", "abs": "https://arxiv.org/abs/2507.18929", "authors": ["Jian Chen", "Yuxuan Hu", "Haifeng Lu", "Wei Wang", "Min Yang", "Chengming Li", "Xiping Hu"], "title": "MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition", "comment": "Accepted by ACMMM2025", "summary": "Although pre-trained visual models with text have demonstrated strong\ncapabilities in visual feature extraction, sticker emotion understanding\nremains challenging due to its reliance on multi-view information, such as\nbackground knowledge and stylistic cues. To address this, we propose a novel\nmulti-granularity hierarchical fusion transformer (MGHFT), with a multi-view\nsticker interpreter based on Multimodal Large Language Models. Specifically,\ninspired by the human ability to interpret sticker emotions from multiple\nviews, we first use Multimodal Large Language Models to interpret stickers by\nproviding rich textual context via multi-view descriptions. Then, we design a\nhierarchical fusion strategy to fuse the textual context into visual\nunderstanding, which builds upon a pyramid visual transformer to extract both\nglobal and local sticker features at multiple stages. Through contrastive\nlearning and attention mechanisms, textual features are injected at different\nstages of the visual backbone, enhancing the fusion of global- and\nlocal-granularity visual semantics with textual guidance. Finally, we introduce\na text-guided fusion attention mechanism to effectively integrate the overall\nmultimodal features, enhancing semantic understanding. Extensive experiments on\n2 public sticker emotion datasets demonstrate that MGHFT significantly\noutperforms existing sticker emotion recognition approaches, achieving higher\naccuracy and more fine-grained emotion recognition. Compared to the best\npre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4%\non F1 and 4.0% on accuracy. The code is released at\nhttps://github.com/cccccj-03/MGHFT_ACMMM2025.", "AI": {"tldr": "提出了一种多粒度层级融合变换器（MGHFT），以解决贴纸情感理解的挑战。该方法利用多模态大语言模型提供上下文，并通过对比学习和注意力机制增强特征融合，实验显示方法在贴纸情感识别上显著优于现有方法。", "motivation": "尽管预训练的视觉模型已经展示了强大的视觉特性提取能力，但对于贴纸情感的理解却仍具挑战性，因为它依赖多视角信息，如背景知识和风格线索。", "method": "我们提出了一种名为多粒度层级融合变换器（MGHFT）的新模型，该模型基于多模态大语言模型构建了一个多视角贴纸解释器。首先，我们使用多模态大语言模型提供丰富的文本上下文，以解释贴纸。其次，我们设计了一个层级融合策略来将文本上下文融合到视觉理解中，以此抽取全局和局部的贴纸特征。通过对比学习和注意力机制，在视觉骨干的各个阶段注入文本特征，增强全局和局部视觉语义与文本指导的融合。最后，我们引入了一种文本引导的融合注意力机制，进一步整合多模态特征，提升语义理解能力。", "result": "在两个公开的贴纸情感数据集上的广泛实验表明，MGHFT显著优于现有的贴纸情感识别方法，在F1值上提高5.4%，准确率上提高4.0%。", "conclusion": "数据和实验结果表明，我们提出的MGHFT显著提升了贴纸情感识别的准确性和细粒度识别能力。"}}
{"id": "2507.19362", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19362", "abs": "https://arxiv.org/abs/2507.19362", "authors": ["Yusuke Hirota", "Boyi Li", "Ryo Hachiuma", "Yueh-Hua Wu", "Boris Ivanovic", "Yuta Nakashima", "Marco Pavone", "Yejin Choi", "Yu-Chiang Frank Wang", "Chao-Han Huck Yang"], "title": "LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences", "comment": "Accepted to ACL 2025. Leaderboard:\n  huggingface.co/spaces/nvidia/lotus-vlm-bias-leaderboard", "summary": "Large Vision-Language Models (LVLMs) have transformed image captioning,\nshifting from concise captions to detailed descriptions. We introduce LOTUS, a\nleaderboard for evaluating detailed captions, addressing three main gaps in\nexisting evaluations: lack of standardized criteria, bias-aware assessments,\nand user preference considerations. LOTUS comprehensively evaluates various\naspects, including caption quality (e.g., alignment, descriptiveness), risks\n(\\eg, hallucination), and societal biases (e.g., gender bias) while enabling\npreference-oriented evaluations by tailoring criteria to diverse user\npreferences. Our analysis of recent LVLMs reveals no single model excels across\nall criteria, while correlations emerge between caption detail and bias risks.\nPreference-oriented evaluations demonstrate that optimal model selection\ndepends on user priorities.", "AI": {"tldr": "本文介绍了LOTUS，一个评估详细图像字幕的排行榜，它综合评估了多个方面并考虑了用户偏好。分析发现，没有一种模型能在所有方面都表现优异，最佳模型选择依赖于用户的偏好。", "motivation": "大型视觉语言模型（LVLMs）改变了图像字幕的生成方式，从简洁的字幕转向详细的描述。", "method": "引入了LOTUS排行榜，用于评估详细描述的图像字幕，旨在解决现有评估中的三大空白：缺乏标准化标准、偏差意识评估以及用户偏好考虑。", "result": "LOTUS综合评估了各种方面，包括字幕质量（如对齐、描述性）、风险（如虚构）和社会偏见（如性别偏见），同时允许根据不同的用户偏好进行偏好导向的评估。", "conclusion": "分析表明没有单个模型在所有标准上都表现出色，而字幕详细程度与偏差风险之间存在相关性。基于用户偏好的评估表明最优模型选择取决于用户的优先级。"}}
{"id": "2507.18939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18939", "abs": "https://arxiv.org/abs/2507.18939", "authors": ["Jionghao Wang", "Cheng Lin", "Yuan Liu", "Rui Xu", "Zhiyang Dou", "Xiao-Xiao Long", "Hao-Xiang Guo", "Taku Komura", "Wenping Wang", "Xin Li"], "title": "PDT: Point Distribution Transformation with Diffusion Models", "comment": "Project page: https://shanemankiw.github.io/PDT/", "summary": "Point-based representations have consistently played a vital role in\ngeometric data structures. Most point cloud learning and processing methods\ntypically leverage the unordered and unconstrained nature to represent the\nunderlying geometry of 3D shapes. However, how to extract meaningful structural\ninformation from unstructured point cloud distributions and transform them into\nsemantically meaningful point distributions remains an under-explored problem.\nWe present PDT, a novel framework for point distribution transformation with\ndiffusion models. Given a set of input points, PDT learns to transform the\npoint set from its original geometric distribution into a target distribution\nthat is semantically meaningful. Our method utilizes diffusion models with\nnovel architecture and learning strategy, which effectively correlates the\nsource and the target distribution through a denoising process. Through\nextensive experiments, we show that our method successfully transforms input\npoint clouds into various forms of structured outputs - ranging from\nsurface-aligned keypoints, and inner sparse joints to continuous feature lines.\nThe results showcase our framework's ability to capture both geometric and\nsemantic features, offering a powerful tool for various 3D geometry processing\ntasks where structured point distributions are desired. Code will be available\nat this link: https://github.com/shanemankiw/PDT.", "AI": {"tldr": "PDT是一种用于点分布转换的新框架，它利用扩散模型将无结构的点云转换为了语义上有意义的点分布。", "motivation": "PDT的动机是为了解决从无结构点云分布中提取有意义的结构信息并将其转换为语义上有意义的点分布这一尚未充分探索的问题。", "method": "PDT采用了一种基于扩散模型的方法，该方法通过降噪过程有效地关联源分布和目标分布，实现了将无结构点云转换为语义上有意义的点分布。它采用了一种新颖的架构和学习策略来完成这个任务。", "result": "实验结果显示，PDT成功地将输入的点云转化为种类繁多的结构化的输出形式，包括表面对齐的键点、内部稀疏的关节点以及连续的特征线。这表明该框架能够有效地捕捉几何和语义特征。", "conclusion": "总结来说，PDT为处理3D几何数据结构的各种任务提供了一个有力的工具，这些任务需要用到结构化的点分布。代码将在此链接中提供：https://github.com/shanemankiw/PDT。"}}
{"id": "2507.19478", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19478", "abs": "https://arxiv.org/abs/2507.19478", "authors": ["Xuehui Wang", "Zhenyu Wu", "JingJing Xie", "Zichen Ding", "Bowen Yang", "Zehao Li", "Zhaoyang Liu", "Qingyun Li", "Xuan Dong", "Zhe Chen", "Weiyun Wang", "Xiangyu Zhao", "Jixuan Chen", "Haodong Duan", "Tianbao Xie", "Chenyu Yang", "Shiqian Su", "Yue Yu", "Yuan Huang", "Yiqian Liu", "Xiao Zhang", "Yanting Zhang", "Xiangyu Yue", "Weijie Su", "Xizhou Zhu", "Wei Shen", "Jifeng Dai", "Wenhai Wang"], "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents", "comment": "in progress", "summary": "We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\nplatforms. It comprises four levels: GUI Content Understanding, Element\nGrounding, Task Automation, and Task Collaboration, covering essential skills\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\nmetric to assess GUI agent execution efficiency in online automation scenarios.\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\ndeterminant of overall task success, emphasizing the substantial benefits of\nmodular frameworks that integrate specialized grounding modules. Furthermore,\nto achieve reliable GUI automation, an agent requires strong task planning and\ncross-platform generalization abilities, with long-context memory, a broad\naction space, and long-term reasoning playing a critical role. More important,\ntask efficiency remains a critically underexplored dimension, and all models\nsuffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration of precise localization,\neffective planning, and early stopping strategies is indispensable to enable\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\ndata, and running environment will be publicly available at\nhttps://github.com/open-compass/MMBench-GUI.", "AI": {"tldr": "我们推出了MMBench-GUI这个基准测试工具，用于多平台评估GUI自动化代理。研究结果表明，精准定位、任务规划能力和跨平台能力是实现高效GUI自动化的重要因素，但也存在显著的效率问题。", "motivation": "该研究旨在通过MMBench-GUI发现准确视觉定位是一个任务成功的关键因素，强调了能够集成专业定位模块的模块化框架的好处。研究指出，强大的任务规划和跨平台泛化能力对于实现可靠的GUI自动化是必要的，同时也强调任务效率是一个尚未充分探索的维度。", "method": "我们介绍了一个名为MMBench-GUI的分层基准测试，用于跨多个操作系统和平台评估GUI自动化代理。基准测试包括四个级别：GUI内容理解、元素定位、任务自动化和任务协作，涵盖了GUI代理所需的核心技能。此外，提出了一种新的Efficiency-Quality Area (EQA)度量来评估在线自动化场景下GUI代理执行效率。", "result": "研究发现，所有的模型在这个基准测试中都面临着显著的效率问题，即使最终任务被完成，也存在过量冗余步骤。而精准定位、有效规划和提前终止策略对于实现真正高效、可扩展的GUI自动化是不可或缺的。", "conclusion": "MMBench-GUI作为评估GUI自动化代理商的基准应用于研究，强调了准确的视觉定位、有效的任务规划、提前终止策略和跨平台泛化能力在实现高效和可靠的GUI自动化机制中的重要性。同时，也指出了任务效率领域存在的挑战及研究中的不足之处。"}}
{"id": "2507.18944", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.18944", "abs": "https://arxiv.org/abs/2507.18944", "authors": ["Guanyi Qin", "Ziyue Wang", "Daiyun Shen", "Haofeng Liu", "Hantao Zhou", "Junde Wu", "Runze Hu", "Yueming Jin"], "title": "Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation", "comment": null, "summary": "Given an object mask, Semi-supervised Video Object Segmentation (SVOS)\ntechnique aims to track and segment the object across video frames, serving as\na fundamental task in computer vision. Although recent memory-based methods\ndemonstrate potential, they often struggle with scenes involving occlusion,\nparticularly in handling object interactions and high feature similarity. To\naddress these issues and meet the real-time processing requirements of\ndownstream applications, in this paper, we propose a novel bOundary Amendment\nvideo object Segmentation method with Inherent Structure refinement, hereby\nnamed OASIS. Specifically, a lightweight structure refinement module is\nproposed to enhance segmentation accuracy. With the fusion of rough edge priors\ncaptured by the Canny filter and stored object features, the module can\ngenerate an object-level structure map and refine the representations by\nhighlighting boundary features. Evidential learning for uncertainty estimation\nis introduced to further address challenges in occluded regions. The proposed\nmethod, OASIS, maintains an efficient design, yet extensive experiments on\nchallenging benchmarks demonstrate its superior performance and competitive\ninference speed compared to other state-of-the-art methods, i.e., achieving the\nF values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6\n(vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive\nspeed of 48 FPS on DAVIS.", "AI": {"tldr": "本文提出了一种名为OASIS的视频物体分割方法，通过轻量级的结构精炼模块提高了分割精度，并在挑战性的基准测试中表现出优越的性能和推理速度。", "motivation": "解决现有基于内存的方法在物体遮挡以及处理物体交互和高特征相似性时表现出的不足，同时满足下游应用的实时处理需求。", "method": "引入轻量级结构精炼模块，融合Canny滤波器捕捉的粗糙边缘先验知识和存储的物体特征，生成物体级结构图并突出边界特征，采用证据学习来估计不确定性，进一步应对遮挡区域的挑战。", "result": "实验显示OASIS在DAVIS-17验证集上F值为91.6，优于之前的89.7；在YouTubeVOS 2019验证集上G值为86.6，优于之前的86.2；同时保持了48 FPS的推理速度。", "conclusion": "OASIS方法设计高效，实验结果证明其性能优越且推理速度快，具有较高的应用价值。"}}
{"id": "2507.18958", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18958", "abs": "https://arxiv.org/abs/2507.18958", "authors": ["Xiaocheng Fang", "Jieyi Cai", "Huanyu Liu", "Chengju Zhou", "Minhua Lu", "Bingzhi Chen"], "title": "PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection", "comment": "MICCAI 2025(Early Accept)", "summary": "Apical periodontitis is a prevalent oral pathology that presents significant\npublic health challenges. Despite advances in automated diagnostic systems\nacross various medical fields, the development of Computer-Aided Diagnosis\n(CAD) applications for apical periodontitis is still constrained by the lack of\na large-scale, high-quality annotated dataset. To address this issue, we\nrelease a large-scale panoramic radiograph benchmark called \"PerioXrays\",\ncomprising 3,673 images and 5,662 meticulously annotated instances of apical\nperiodontitis. To the best of our knowledge, this is the first benchmark\ndataset for automated apical periodontitis diagnosis. This paper further\nproposes a clinical-oriented apical periodontitis detection (PerioDet)\nparadigm, which jointly incorporates Background-Denoising Attention (BDA) and\nIoU-Dynamic Calibration (IDC) mechanisms to address the challenges posed by\nbackground noise and small targets in automated detection. Extensive\nexperiments on the PerioXrays dataset demonstrate the superiority of PerioDet\nin advancing automated apical periodontitis detection. Additionally, a\nwell-designed human-computer collaborative experiment underscores the clinical\napplicability of our method as an auxiliary diagnostic tool for professional\ndentists.", "AI": {"tldr": "Introduces 'PerioXrays', a large-scale panoramic radiograph dataset for apical periodontitis, and proposes the PerioDet method for its automated detection, showing its clinical applicability.", "motivation": "To address the lack of a large-scale, meticulously annotated dataset for computer-aided diagnosis (CAD) of apical periodontitis and to improve automated detection capabilities.", "method": "Proposes a clinical-oriented apical periodontitis detection (PerioDet) paradigm with Background-Denoising Attention (BDA) and IoU-Dynamic Calibration (IDC) mechanisms.", "result": "Experiments on the PerioXrays dataset show that the proposed PerioDet method outperforms existing methods in detecting apical periodontitis.", "conclusion": "The PerioDet method demonstrates high efficacy in automated apical periodontitis detection and provides a promising tool for dentists."}}
{"id": "2507.18966", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18966", "abs": "https://arxiv.org/abs/2507.18966", "authors": ["Saraa Al-Saddik", "Manna Elizabeth Philip", "Ali Haidar"], "title": "YOLO for Knowledge Extraction from Vehicle Images: A Baseline Study", "comment": null, "summary": "Accurate identification of vehicle attributes such as make, colour, and shape\nis critical for law enforcement and intelligence applications. This study\nevaluates the effectiveness of three state-of-the-art deep learning approaches\nYOLO-v11, YOLO-World, and YOLO-Classification on a real-world vehicle image\ndataset. This dataset was collected under challenging and unconstrained\nconditions by NSW Police Highway Patrol Vehicles. A multi-view inference (MVI)\napproach was deployed to enhance the performance of the models' predictions. To\nconduct the analyses, datasets with 100,000 plus images were created for each\nof the three metadata prediction tasks, specifically make, shape and colour.\nThe models were tested on a separate dataset with 29,937 images belonging to\n1809 number plates. Different sets of experiments have been investigated by\nvarying the models sizes. A classification accuracy of 93.70%, 82.86%, 85.19%,\nand 94.86% was achieved with the best performing make, shape, colour, and\ncolour-binary models respectively. It was concluded that there is a need to use\nMVI to get usable models within such complex real-world datasets. Our findings\nindicated that the object detection models YOLO-v11 and YOLO-World outperformed\nclassification-only models in make and shape extraction. Moreover, smaller YOLO\nvariants perform comparably to larger counterparts, offering substantial\nefficiency benefits for real-time predictions. This work provides a robust\nbaseline for extracting vehicle metadata in real-world scenarios. Such models\ncan be used in filtering and sorting user queries, minimising the time required\nto search large vehicle images datasets.", "AI": {"tldr": "研究使用了YOLO系列模型针对由新南威尔士州警方收集的真实世界车辆图像数据集进行车辆属性识别，多视角推理方法显著提升了模型预测效果。", "motivation": "精确识别车辆的属性对于执法和情报应用至关重要。该研究旨在评估模型在复杂现实环境下的性能，这些环境条件下的是由新南威尔士警方高速公路巡逻车辆收集的。", "method": "该研究使用了YOLO-v11、YOLO-World和YOLO-Classification三种最先进的深度学习方法来识别车辆属性，如品牌、颜色和形状。为了增强预测性能，采用了多视角推理(MVI)策略。每个元数据预测任务（品牌、形状和颜色）都创建了包含100,000多张图像的数据集。", "result": "品牌识别精度为93.70%，形状识别精度为82.86%，颜色识别精度为85.19%，二元颜色识别精度为94.86%。实验表明，为了在这些复杂的真实世界数据集中获得可用的模型，需要使用多视角推理策略。", "conclusion": "该研究发现，对于品牌和形状的提取，YOLO-v11和YOLO-World表现优于纯分类模型。而且，较小的YOLO变体至少能与较大的模型表现相当，这为实时预测提供了显著效率优势。工作为在现实场景中提取车辆元数据提供了坚固的基础。"}}
{"id": "2507.18967", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18967", "abs": "https://arxiv.org/abs/2507.18967", "authors": ["UMMPK Nawarathne", "HMNS Kumari", "HMLS Kumari"], "title": "Underwater Waste Detection Using Deep Learning A Performance Comparison of YOLOv7 to 10 and Faster RCNN", "comment": "7 pages, 11 figures, to be published in International Journal of\n  Research in Computing (IJRC)", "summary": "Underwater pollution is one of today's most significant environmental\nconcerns, with vast volumes of garbage found in seas, rivers, and landscapes\naround the world. Accurate detection of these waste materials is crucial for\nsuccessful waste management, environmental monitoring, and mitigation\nstrategies. In this study, we investigated the performance of five cutting-edge\nobject recognition algorithms, namely YOLO (You Only Look Once) models,\nincluding YOLOv7, YOLOv8, YOLOv9, YOLOv10, and Faster Region-Convolutional\nNeural Network (R-CNN), to identify which model was most effective at\nrecognizing materials in underwater situations. The models were thoroughly\ntrained and tested on a large dataset containing fifteen different classes\nunder diverse conditions, such as low visibility and variable depths. From the\nabove-mentioned models, YOLOv8 outperformed the others, with a mean Average\nPrecision (mAP) of 80.9%, indicating a significant performance. This increased\nperformance is attributed to YOLOv8's architecture, which incorporates advanced\nfeatures such as improved anchor-free mechanisms and self-supervised learning,\nallowing for more precise and efficient recognition of items in a variety of\nsettings. These findings highlight the YOLOv8 model's potential as an effective\ntool in the global fight against pollution, improving both the detection\ncapabilities and scalability of underwater cleanup operations.", "AI": {"tldr": "对比了多个物体识别算法在水下材料检测的表现，得出YOLOv8模型在低可见度和不同深度条件下表现出色，mAP为80.9%。", "motivation": "鉴于水下污染是当今最重要的环境关切之一，准确检测这些废弃物对于有效的废物管理、环境监测和缓解策略至关重要。", "method": "本研究旨在评估五种最先进的物体识别算法在识别水下材料时的表现，这些算法包括YOLOv7、YOLOv8、YOLOv9、YOLOv10和Faster R-CNN。", "result": "研究结果表明，在所有测试模型中，YOLOv8的性能最佳，平均精度(mAP)为80.9%。", "conclusion": "这项研究证实了YOLOv8模型在全球反污染斗争中的潜在有效性，特别是提高了水下清理操作的探测能力和可扩展性。"}}
{"id": "2507.18988", "categories": ["cs.CV", "cs.CR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.18988", "abs": "https://arxiv.org/abs/2507.18988", "authors": ["Chao Wang", "Kejiang Chen", "Zijin Yang", "Yaofei Wang", "Weiming Zhang"], "title": "AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction", "comment": null, "summary": "The rapid advancement of image-generation technologies has made it possible\nfor anyone to create photorealistic images using generative models, raising\nsignificant security concerns. To mitigate malicious use, tracing the origin of\nsuch images is essential. Reconstruction-based attribution methods offer a\npromising solution, but they often suffer from reduced accuracy and high\ncomputational costs when applied to state-of-the-art (SOTA) models. To address\nthese challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel\ntraining-free attribution method designed for generative models with continuous\nautoencoders. Unlike existing reconstruction-based approaches that rely on the\nvalue of a single reconstruction loss, AEDR performs two consecutive\nreconstructions using the model's autoencoder, and adopts the ratio of these\ntwo reconstruction losses as the attribution signal. This signal is further\ncalibrated using the image homogeneity metric to improve accuracy, which\ninherently cancels out absolute biases caused by image complexity, with\nautoencoder-based reconstruction ensuring superior computational efficiency.\nExperiments on eight top latent diffusion models show that AEDR achieves 25.5%\nhigher attribution accuracy than existing reconstruction-based methods, while\nrequiring only 1% of the computational time.", "AI": {"tldr": "提出了一种名为AEDR的训练免归因方法，用于解决现有方法在应用到现代图像生成模型时存在的准确性下降和高计算成本问题，实现更高的归因精度的同时大幅减少了计算时间。", "motivation": "图像生成技术的快速发展使得使用生成模型创建照片级逼真的图像成为可能，引起了重大的安全问题。追踪这些图像的来源对于缓解恶意使用至关重要。而现有的基于重建的归因方法在应用于最先进的模型时会遇到准确性下降和计算成本高的问题。为了解决这些问题，提出了AEDR方法。", "method": "AEDR (AutoEncoder Double-Reconstruction)，一种无需训练的归因方法，专门用于具有连续自动编码器的生成模型。AEDR 通过进行两次连续重建，并采用这两个重建损失的比例作为归因信号，不同于现有的仅依赖单次重建损失的方法。这种方法还利用图像同质性度量对信号进行校准，以提高准确性，并通过基于自动编码器的重建保证了计算效率。", "result": "实验表明，在8个顶级潜在扩散模型上，AEDR 达到了比现有重构归因方法高25.5%的归因精度，同时仅需1%的计算时间。", "conclusion": "AEDR 方法在提高归因准确性的同时大幅降低了计算成本，证明了其在安全性监测方面的有效性和效率。"}}
{"id": "2507.18997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18997", "abs": "https://arxiv.org/abs/2507.18997", "authors": ["Zixiang Ai", "Zhenyu Cui", "Yuxin Peng", "Jiahuan Zhou"], "title": "UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis", "comment": "Accepted by ICCV 2025 as a Poster", "summary": "Pre-trained point cloud analysis models have shown promising advancements in\nvarious downstream tasks, yet their effectiveness is typically suffering from\nlow-quality point cloud (i.e., noise and incompleteness), which is a common\nissue in real scenarios due to casual object occlusions and unsatisfactory data\ncollected by 3D sensors. To this end, existing methods focus on enhancing point\ncloud quality by developing dedicated denoising and completion models. However,\ndue to the isolation between the point cloud enhancement and downstream tasks,\nthese methods fail to work in various real-world domains. In addition, the\nconflicting objectives between denoising and completing tasks further limit the\nensemble paradigm to preserve critical geometric features. To tackle the above\nchallenges, we propose a unified point-level prompting method that reformulates\npoint cloud denoising and completion as a prompting mechanism, enabling robust\nanalysis in a parameter-efficient manner. We start by introducing a\nRectification Prompter to adapt to noisy points through the predicted\nrectification vector prompts, effectively filtering noise while preserving\nintricate geometric features essential for accurate analysis. Sequentially, we\nfurther incorporate a Completion Prompter to generate auxiliary point prompts\nbased on the rectified point clouds, facilitating their robustness and\nadaptability. Finally, a Shape-Aware Unit module is exploited to efficiently\nunify and capture the filtered geometric features for the downstream point\ncloud analysis.Extensive experiments on four datasets demonstrate the\nsuperiority and robustness of our method when handling noisy and incomplete\npoint cloud data against existing state-of-the-art methods. Our code is\nreleased at https://github.com/zhoujiahuan1991/ICCV2025-UPP.", "AI": {"tldr": "本文提出了一种改进点云分析的新方法，通过统一的点级别提示，解决了去噪和补全之间的矛盾，提升了处理噪声和不完整点云数据的效果。", "motivation": "现有的点云增强方法通常会在去除噪声和补全点云的目标之间产生冲突，并且这些方法与下游任务之间缺乏衔接，导致在现实世界中表现不佳。我们的工作旨在解决这些局限性，提出一种新的方法来处理低质量点云数据，提高其在下游任务中的表现。", "method": "我们的方法提出了一种统一的点级别提示法，将点云去噪和补全作为提示机制来处理。首先引入了一个校正提示器，通过预测校正向量提示来适应噪声点，有效过滤噪声同时保留精细的几何特征。随后，结合补全提示器根据校正后的点云生成辅助点提示，增强其鲁棒性和适应性。最后，利用Shape-Aware Unit模块高效地统一并捕捉过滤后的几何特征，用于下游点云分析。", "result": "实验结果证明，我们提出的方法在处理噪声和不完整点云数据时，相比于现有的先进方法，表现出了优越性和鲁棒性。实验在四个数据集上进行了测试。", "conclusion": "通过提出统一的点级别提示法，本研究解决了一些现存的点云分析方法存在的问题，能够更有效地处理噪声和不完整的点云数据，提供更精确的分析结果。此外，该代码已经公开，便于社区进一步研究和应用。"}}
{"id": "2507.18998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18998", "abs": "https://arxiv.org/abs/2507.18998", "authors": ["Yongsong Huang", "Tomo Miyazaki", "Xiaofeng Liu", "Shinichiro Omachi"], "title": "GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution", "comment": "This manuscript is under review, and copyright will be transferred\n  without notice", "summary": "Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and\nsparse textures of infrared data, requiring robust long-range modeling to\nmaintain global coherence. While State-Space Models like Mamba offer\nproficiency in modeling long-range dependencies for this task, their inherent\n1D causal scanning mechanism fragments the global context of 2D images,\nhindering fine-detail restoration. To address this, we propose Global Phase and\nSpectral Prompt-guided Mamba (GPSMamba), a framework that synergizes\narchitectural guidance with non-causal supervision. First, our Adaptive\nSemantic-Frequency State Space Module (ASF-SSM) injects a fused\nsemantic-frequency prompt directly into the Mamba block, integrating non-local\ncontext to guide reconstruction. Then, a novel Thermal-Spectral Attention and\nPhase Consistency Loss provides explicit, non-causal supervision to enforce\nglobal structural and spectral fidelity. By combining these two innovations,\nour work presents a systematic strategy to mitigate the limitations of causal\nmodeling. Extensive experiments demonstrate that GPSMamba achieves\nstate-of-the-art performance, validating our approach as a powerful new\nparadigm for infrared image restoration. Code is available at\nhttps://github.com/yongsongH/GPSMamba.", "AI": {"tldr": "本文提出GPSMamba框架，通过自适应语义-频率状态空间模块和热谱注意及相位一致性损失，解决了现有模型在处理红外图像时因1D因果扫描而导致的全局上下文分割问题，达到了红外图像超分辨率的状态-of-the-art性能。", "motivation": "为了解决现有Mamba等状态空间模型在长范围依赖建模中存在的问题，这些模型由于内在的1D因果扫描机制而分割了2D图像的全局上下文，从而阻碍了细密细节的恢复。我们提出了Global Phase和Spectral Prompt-guided Mamba（GPSMamba）框架。", "method": "提出了一种名为GPSMamba的框架，它结合了结构指导和非因果监督。首先，自适应语义-频率状态空间模块（ASF-SSM）将融合的语义-频率提示直接注入到Mamba块中，整合非局部上下文来指导重建。其次，一种新的热谱注意和相位一致性损失提供明确的非因果监督，以确保全局结构和光谱保真度。", "result": "大量的实验表明，GPSMamba在红外图像恢复中达到了最先进的性能，验证了该方法作为红外图像恢复的强大新范式的有效性。", "conclusion": "GPSMamba框架通过结合自适应语义-频率状态空间模块和热谱注意及相位一致性损失，成功弥补了因果建模的局限性，并在红外图像超分辨率任务上达到了最先进的性能。"}}
{"id": "2507.19002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19002", "abs": "https://arxiv.org/abs/2507.19002", "authors": ["Ying Ba", "Tianyu Zhang", "Yalong Bai", "Wenyi Mo", "Tao Liang", "Bing Su", "Ji-Rong Wen"], "title": "Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment", "comment": "Accepted to ICCV 2025", "summary": "Contemporary image generation systems have achieved high fidelity and\nsuperior aesthetic quality beyond basic text-image alignment. However, existing\nevaluation frameworks have failed to evolve in parallel. This study reveals\nthat human preference reward models fine-tuned based on CLIP and BLIP\narchitectures have inherent flaws: they inappropriately assign low scores to\nimages with rich details and high aesthetic value, creating a significant\ndiscrepancy with actual human aesthetic preferences. To address this issue, we\ndesign a novel evaluation score, ICT (Image-Contained-Text) score, that\nachieves and surpasses the objectives of text-image alignment by assessing the\ndegree to which images represent textual content. Building upon this\nfoundation, we further train an HP (High-Preference) score model using solely\nthe image modality to enhance image aesthetics and detail quality while\nmaintaining text-image alignment. Experiments demonstrate that the proposed\nevaluation model improves scoring accuracy by over 10\\% compared to existing\nmethods, and achieves significant results in optimizing state-of-the-art\ntext-to-image models. This research provides theoretical and empirical support\nfor evolving image generation technology toward higher-order human aesthetic\npreferences. Code is available at https://github.com/BarretBa/ICTHP.", "AI": {"tldr": "研究揭示了一种改进的评估体系，用于提高图像生成系统的美学评分准确性，同时适应人类审美偏好的要求。", "motivation": "现有的评估框架未能跟上当代图像生成系统的发展，特别是高保真度和美学质量的提升。人类偏好奖励模型基于CLIP和BLIP架构微调后，对于细节丰富、美学价值高的图像给出了不恰当的低评分，与实际人类审美偏好存在显著差异。", "method": "设计了一种新型的评估分数——ICT（Image-Contained-Text）分数，用于评估图像如何表示文本内容，探究了文本图像对齐的难题。进而，仅基于图像模态训练了一个高偏好的HP（High-Preference）分数模型，以提升图像的美学质量和细节质量，同时维持文本-图像对齐。", "result": "实验表明，提出的评估模型相较于现有方法提升了超过10%的评分准确性，并在优化最先进的文本到图像模型方面取得了显著效果。", "conclusion": "该研究为依据人类高层审美偏好的图像生成技术的发展提供了理论和实证支持。"}}
{"id": "2507.19004", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19004", "abs": "https://arxiv.org/abs/2507.19004", "authors": ["Siyi Xun", "Yue Sun", "Jingkun Chen", "Zitong Yu", "Tong Tong", "Xiaohong Liu", "Mingxiang Wu", "Tao Tan"], "title": "MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment", "comment": "We note that the version after peer review of this paper has been\n  provisionally accepted by The 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2025)", "summary": "Rapid advances in medical imaging technology underscore the critical need for\nprecise and automated image quality assessment (IQA) to ensure diagnostic\naccuracy. Existing medical IQA methods, however, struggle to generalize across\ndiverse modalities and clinical scenarios. In response, we introduce MedIQA,\nthe first comprehensive foundation model for medical IQA, designed to handle\nvariability in image dimensions, modalities, anatomical regions, and types. We\ndeveloped a large-scale multi-modality dataset with plentiful manually\nannotated quality scores to support this. Our model integrates a salient slice\nassessment module to focus on diagnostically relevant regions feature retrieval\nand employs an automatic prompt strategy that aligns upstream physical\nparameter pre-training with downstream expert annotation fine-tuning. Extensive\nexperiments demonstrate that MedIQA significantly outperforms baselines in\nmultiple downstream tasks, establishing a scalable framework for medical IQA\nand advancing diagnostic workflows and clinical decision-making.", "AI": {"tldr": "本研究提出了MedIQA，一种用于医学图像质量评估的基础模型，该模型能够处理多种模态和临床场景，实验表明其效果显著优于现有方法。", "motivation": "随着医学成像技术的快速发展，精确和自动化的图像质量评估对于确保诊断准确性至关重要。然而，现有的医学IQA方法在处理不同的成像模态和临床场景时难以通用。", "method": "文章介绍了一种名为MedIQA的医学图像质量评估(IQA)的基础模型，该模型涵盖了多种模态和临床场景，并包含了一个突出切片评估模块和自动提示策略，以支持诊断相关区域的特征检索。", "result": "MedIQA模型在多种下游任务中表现出色，优于现有的基线方法，证明了其在提高诊断工作流程效率和临床决策质量方面的潜力。", "conclusion": "实验表明，MedIQA在多个下游任务中显著优于基线模型，为医学IQA提供了可扩展的框架，并有助于改进诊断工作流程和临床决策。"}}
{"id": "2507.19024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19024", "abs": "https://arxiv.org/abs/2507.19024", "authors": ["Zhiyuan Chen", "Yuecong Min", "Jie Zhang", "Bei Yan", "Jiahao Wang", "Xiaozhen Wang", "Shiguang Shan"], "title": "A Survey of Multimodal Hallucination Evaluation and Detection", "comment": "33 pages, 5 figures", "summary": "Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm\nfor integrating visual and textual information, supporting a wide range of\nmulti-modal tasks. However, these models often suffer from hallucination,\nproducing content that appears plausible but contradicts the input content or\nestablished world knowledge. This survey offers an in-depth review of\nhallucination evaluation benchmarks and detection methods across Image-to-Text\n(I2T) and Text-to-image (T2I) generation tasks. Specifically, we first propose\na taxonomy of hallucination based on faithfulness and factuality, incorporating\nthe common types of hallucinations observed in practice. Then we provide an\noverview of existing hallucination evaluation benchmarks for both T2I and I2T\ntasks, highlighting their construction process, evaluation objectives, and\nemployed metrics. Furthermore, we summarize recent advances in hallucination\ndetection methods, which aims to identify hallucinated content at the instance\nlevel and serve as a practical complement of benchmark-based evaluation.\nFinally, we highlight key limitations in current benchmarks and detection\nmethods, and outline potential directions for future research.", "AI": {"tldr": "本文综述了多模态大语言模型产生的幻觉问题，包括幻觉的分类、评估基准以及检测方法，并指出了未来研究的方向。", "motivation": "鉴于多模态大语言模型在集成视觉和文本信息方面的能力，以及这些模型在生成与输入内容或已知世界知识相矛盾的内容时存在幻觉的问题，文章旨在提供一个详尽的幻觉评估基准和检测方法的综述。", "method": "本文首先提出了基于忠实性和事实性的幻觉分类，涵盖了实践中常见的幻觉类型。接着，文章概述了现有文本到图像和图像到文本生成任务中的幻觉评估基准，包括它们的构建过程、评估目标和使用指标。此外，还总结了实例级别幻觉内容检测方法的最新进展，并指出这些方法作为基准评估的实用补充。", "result": "文章提供了一个综合的幻觉评估基准和检测方法的重述，特别关注文本到图像和图像到文本生成任务，并概述了未来研究的潜在方向。", "conclusion": "文中指出，当前用于评估多模态大语言模型幻觉行为的标准和方法存在局限性，强调了未来研究需要关注的方向。"}}
