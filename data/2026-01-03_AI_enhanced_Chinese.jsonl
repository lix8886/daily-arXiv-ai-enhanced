{"id": "2512.23710", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23710", "abs": "https://arxiv.org/abs/2512.23710", "authors": ["Zahra Abedi", "Richard M. K. van Dijk", "Gijs Wijnholds", "Tessa Verhoef"], "title": "Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration", "comment": null, "summary": "This research digitizes and analyzes the Leidse hoogleraren en lectoren 1575-1815 books written between 1983 and 1985, which contain biographic data about professors and curators of Leiden University. It addresses the central question: how can we design an automated pipeline that integrates OCR, LLM-based interpretation, and database linking to harmonize data from historical document images with existing high-quality database records? We applied OCR techniques, generative AI decoding constraints that structure data extraction, and database linkage methods to process typewritten historical records into a digital format. OCR achieved a Character Error Rate (CER) of 1.08 percent and a Word Error Rate (WER) of 5.06 percent, while JSON extraction from OCR text achieved an average accuracy of 63 percent and, based on annotated OCR, 65 percent. This indicates that generative AI somewhat corrects low OCR performance. Our record linkage algorithm linked annotated JSON files with 94% accuracy and OCR-derived JSON files with 81%. This study contributes to digital humanities research by offering an automated pipeline for interpreting digitized historical documents, addressing challenges like layout variability and terminology differences, and exploring the applicability and strength of an advanced generative AI model.", "AI": {"tldr": "研究通过OCR和生成式AI技术处理了1983-1985年出版的关于1575-1815年间莱顿大学教授和管理员的传记资料。研究设计了一种自动流水线，将OCR处理后的数据与现有数据库记录进行匹配。结果表明，AI技术可显著提高历史文档的信息提取和链接准确性。", "motivation": "研究动机是设计一个自动化流水线，集成OCR、基于LLM的解释和数据库链接方法，以调和历史文档图像与现有高质量数据库记录中的数据。", "method": "本研究采用OCR技术、基于生成式AI的数据提取约束以及数据库链接方法，将打字的史料记录转化为数字化格式。", "result": "OCR达到了1.08%的字符错误率和5.06%的单词错误率，从OCR文本中的JSON提取平均准确率为63%，基于标注的OCR为65%。记录链接算法将标注的JSON文件以94%的准确率链接，将基于OCR的JSON文件以81%的准确率链接。", "conclusion": "这项研究表明，自动管道可以识别数字化的历史文档，解决了布局变化和术语差异的挑战，并探讨了高级生成式AI模型的应用。"}}
{"id": "2512.23711", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23711", "abs": "https://arxiv.org/abs/2512.23711", "authors": ["Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Claudio Pinhanez", "Yago Primerano"], "title": "CAT: A Metric-Driven Framework for Analyzing the Consistency-Accuracy Relation of LLMs under Controlled Input Variations", "comment": null, "summary": "We introduce \\textsc{CAT}, a framework designed to evaluate and visualize the \\emph{interplay} of \\emph{accuracy} and \\emph{response consistency} of Large Language Models (LLMs) under controllable input variations, using multiple-choice (MC) benchmarks as a case study. Current evaluation practices primarily focus on model capabilities such as accuracy or benchmark scores and, more recently, measuring consistency is being considered an essential property for deploying LLMs in high-stake, real-world applications. We argue in this paper that although both dimensions should still be evaluated independently, their inter-dependency also need to be considered for a more nuanced evaluation of LLMs. At the core of \\textsc{CAT} are the \\emph{Consistency-Accuracy Relation (CAR)} curves, which visualize how model accuracy varies with increasing consistency requirements, as defined by the \\emph{Minimum-Consistency Accuracy (MCA)} metric. We further propose the \\emph{Consistency-Oriented Robustness Estimate (CORE)} index, a global metric that combines the area and shape of the CAR curve to quantify the trade-off between accuracy and consistency. We present a practical demonstration of our framework across a diverse set of generalist and domain-specific LLMs, evaluated on multiple MC benchmarks. We also outline how \\textsc{CAT} can be extended beyond MC tasks to support long-form, open-ended evaluations through adaptable scoring functions.", "AI": {"tldr": "介绍了CAT框架，用于评估和可视化大型语言模型在可控输入变化下的准确性与响应一致性之间的相互作用，并提出了CORE指数来量化这些属性之间的权衡。", "motivation": "当前的评估实践主要集中在模型能力如准确性或基准测试得分上，最近开始考虑一致性作为部署LLMs到高风险、实际应用中的重要因素。本文强调，虽然独立评估这两个维度是必要的，但它们之间的相互依赖性也需要考虑，以进行更细致的评估。", "method": "提出了一种名为\\textsc{CAT}的框架，用于评估和可视化大型语言模型（LLMs）在可控输入变化下的准确性与响应一致性之间的相互作用。该框架的核心是\"一致性-准确性关系（CAR）\"曲线，它通过\"最小一致性准确性（MCA）\"指标展示了准确性随一致性要求增加的变化情况。同时提出了\"一致性导向稳健性估计（CORE）\"指数，用以量化准确性和一致性之间的权衡。", "result": "框架在多种通用型和领域特定的大型语言模型上进行了展示，评估了多个多项选择基准测试，并讨论了如何扩展\\textsc{CAT}框架以支持长篇开放式评估。", "conclusion": "CAT框架通过CAR曲线和CORE指数提供了对LLMs在多个基准测试中的准确性和一致性评估的新见解，表明将这种框架扩展到更广泛的评估任务中具有潜力。"}}
{"id": "2512.23712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23712", "abs": "https://arxiv.org/abs/2512.23712", "authors": ["Guanghui Wang", "Jinze Yu", "Xing Zhang", "Dayuan Jiang", "Yin Song", "Tomal Deb", "Xuefeng Liu", "Peiyang He"], "title": "STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed for structured data generation, yet output consistency remains critical for production applications. We introduce a comprehensive framework for evaluating and improving consistency in LLM-generated structured outputs. Our approach combines: (1) STED (Semantic Tree Edit Distance), a novel similarity metric balancing semantic flexibility with structural strictness when comparing JSON outputs, and (2) a consistency scoring framework aggregating multiple STED measurements across repeated generations to quantify reliability. Through systematic experiments on synthetic datasets with controlled schema, expression, and semantic variations, we demonstrate STED achieves superior performance ($0.86-0.90$ similarity for semantic equivalents, $0.0$ for structural breaks) compared to existing metrics including TED, BERTScore, and DeepDiff. Applying our framework to benchmark six LLMs reveals significant variations: Claude-3.7-Sonnet demonstrates exceptional consistency, maintaining near-perfect structural reliability even at high temperatures ($T=0.9$), while models like Claude-3-Haiku and Nova-Pro exhibit substantial degradation requiring careful tuning. Our framework enables practical applications including targeted model selection for structured tasks, iterative prompt refinement for reproducible results, and diagnostic analysis to identify inconsistency root causes. This work provides theoretical foundations and practical tools for ensuring reliable structured output generation in LLM-based production systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.23713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23713", "abs": "https://arxiv.org/abs/2512.23713", "authors": ["Jahidul Islam", "Md Ataullha", "Saiful Azad"], "title": "PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents", "comment": "6 Pages", "summary": "LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\\% on the development set and 71.6\\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at github.com/jahidulzaid/PyBanglaCodeActAgent.", "AI": {"tldr": "本文介绍BanglaCodeAct框架，利用多智能体提示和迭代自我修正方法，实现从孟加拉语到Python的代码生成，通过开源多语言大模型在Thought-Code-Observation循环中动态生成、测试和优化代码，实现最佳性能。", "motivation": "尽管在英文提示下生成代码方面取得了进步，但这种进步尚未推广到低资源语言。本文旨在解决孟加拉语到Python代码生成的问题。", "method": "通过引入BanglaCodeAct框架，该框架利用多智能体提示和迭代自我修正来解决从孟加拉语到Python的代码生成问题，该框架采用了开源的多语言大模型在一个Thought-Code-Observation循环中动态生成、测试和优化由孟加拉语指令生成的代码，而不是依赖于特定任务的微调。", "result": "实验结果表明，当使用BanglaCodeAct部署Qwen3-8B时，在开发集上获得了94.0%的最佳通过率准确性，在盲测集上达到71.6%。这些结果为孟加拉语到Python的翻译建立了新的基准指标，并强调了基于智能体推理对于低资源语言中的可靠代码生成的潜力。", "conclusion": "研究证明了BanglaCodeAct框架在孟加拉语到Python代码生成中的有效性和优越性，为低资源语言中的代码生成提供了新的解决方案，同时也强调了开源多语言大模型在低资源语言中代码生成的重要角色。"}}
{"id": "2512.23786", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23786", "abs": "https://arxiv.org/abs/2512.23786", "authors": ["Ankan Aich", "Yangming Lee"], "title": "Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments", "comment": null, "summary": "Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (< 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.", "AI": {"tldr": "本文通过利用Depth Anything V2架构的高保真合成先验知识和DV-LORA技术来提高单目深度估计（MDE）方法在腹腔镜手术环境中的准确性与鲁棒性，建立新的性能基准。", "motivation": "单目深度估计（MDE）对于机器人手术至关重要，但在充满流体和高反射性的腹腔镜环境中显得很脆弱。现有的自监督方法通常依赖于带有真实世界伪标签的基础模型，这往往导致薄手术工具和透明表面边界坍塌。", "method": "通过利用Depth Anything V2架构生成的高保真合成先验知识，该架构能准确捕获薄结构的几何细节，我们解决了现有方法在手术环境中对薄手术工具和透明表面边界坍塌的问题。我们采用动态矢量低秩适应（DV-LORA）技术，将这些先验知识有效迁移到医学领域，同时最小化参数预算并弥合合成到真实的差距。", "result": "该研究在SCARED数据集上的物理分层评估显示，方法在高反射环境下表现优越，达到了98.1%的准确率，平方相对误差降低17%以上。", "conclusion": "我们的方法在SCARED数据集上使用物理分层评估协议进行测试，建立了新的基准，准确率（小于1.25）达到98.1%，平方相对误差比现有基准减少17%以上，显示了在恶劣手术照明下的优越鲁棒性。"}}
{"id": "2512.23714", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23714", "abs": "https://arxiv.org/abs/2512.23714", "authors": ["Tingwei Xie", "Tianyi Zhou", "Yonghong Song"], "title": "PharmaShip: An Entity-Centric, Reading-Order-Supervised Benchmark for Chinese Pharmaceutical Shipping Documents", "comment": "5 pages, 4 figures", "summary": "We present PharmaShip, a real-world Chinese dataset of scanned pharmaceutical shipping documents designed to stress-test pre-trained text-layout models under noisy OCR and heterogeneous templates. PharmaShip covers three complementary tasks-sequence entity recognition (SER), relation extraction (RE), and reading order prediction (ROP)-and adopts an entity-centric evaluation protocol to minimize confounds across architectures. We benchmark five representative baselines spanning pixel-aware and geometry-aware families (LiLT, LayoutLMv3-base, GeoLayoutLM and their available RORE-enhanced variants), and standardize preprocessing, splits, and optimization. Experiments show that pixels and explicit geometry provide complementary inductive biases, yet neither alone is sufficient: injecting reading-order-oriented regularization consistently improves SER and EL and yields the most robust configuration, while longer positional coverage stabilizes late-page predictions and reduces truncation artifacts. ROP is accurate at the word level but challenging at the segment level, reflecting boundary ambiguity and long-range crossings. PharmaShip thus establishes a controlled, reproducible benchmark for safety-critical document understanding in the pharmaceutical domain and highlights sequence-aware constraints as a transferable bias for structure modeling. We release the dataset at https://github.com/KevinYuLei/PharmaShip.", "AI": {"tldr": "本文提出了PharmaShip，一个旨在测试预训练文本布局模型在处理带有噪点OCR和多个模板的扫描药品运输单据性能的中文数据集。评测显示偏置性的像素和几何信息对模型性能有互补提升，但需要进一步结合阅读顺序导向的正则化才能得到更稳健的结果。该数据集对制药行业文档理解提供了一套基准。", "motivation": "我们的动机是建立一个受控的、可重现的基准数据集，用于制药领域的安全关键文档理解，并突出序列感知约束作为可转移的结构建模偏置。", "method": "我们介绍了PharmaShip，这是一个专为扫描药品运输单据设计的真实世界中文数据集，用于检验预训练文本布局模型在面对噪点OCR和异构模板时的表现。该数据集涵盖了序列实体识别（SER）、关系抽取（RE）和阅读顺序预测（ROP）三项互补任务，并采用实体中心评估协议来减少不同架构之间的混淆。", "result": "实验表明，像素和显式几何提供了互补的归纳偏置，但仅有其中之一是不够的：注入阅读顺序导向的正则化始终能提高SER和EL的表现，并产生最稳健的配置，而更长的位置覆盖有助于稳定后期预测并减少截断伪影。ROP在单词级别上准确，但在片段级别上具有挑战性，反映了边界模糊和长距离交叉的问题。", "conclusion": "PharmaShip建立了这样一个受控的、可重现的基准，用于制药领域的安全关键文档理解，并强调了序列感知约束作为可转移的结构建模偏置。"}}
{"id": "2512.23819", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23819", "abs": "https://arxiv.org/abs/2512.23819", "authors": ["Surya Rayala", "Marcos Quinones-Grueiro", "Naveeduddin Mohammed", "Ashwin T S", "Benjamin Goldberg", "Randall Spain", "Paige Lawton", "Gautam Biswas"], "title": "Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments", "comment": "14 pages, 9 figures, I/ITSEC-2025", "summary": "Effective urban warfare training requires situational awareness and muscle memory, developed through repeated practice in realistic yet controlled environments. A key drill, Enter and Clear the Room (ECR), demands threat assessment, coordination, and securing confined spaces. The military uses Synthetic Training Environments that offer scalable, controlled settings for repeated exercises. However, automatic performance assessment remains challenging, particularly when aiming for objective evaluation of cognitive, psychomotor, and teamwork skills. Traditional methods often rely on costly, intrusive sensors or subjective human observation, limiting scalability and accuracy. This paper introduces a video-based assessment pipeline that derives performance analytics from training videos without requiring additional hardware. By utilizing computer vision models, the system extracts 2D skeletons, gaze vectors, and movement trajectories. From these data, we develop task-specific metrics that measure psychomotor fluency, situational awareness, and team coordination. These metrics feed into an extended Cognitive Task Analysis (CTA) hierarchy, which employs a weighted combination to generate overall performance scores for teamwork and cognition. We demonstrate the approach with a case study of real-world ECR drills, providing actionable, domain specific metrics that capture individual and team performance. We also discuss how these insights can support After Action Reviews with interactive dashboards within Gamemaster and the Generalized Intelligent Framework for Tutoring (GIFT), providing intuitive and understandable feedback. We conclude by addressing limitations, including tracking difficulties, ground-truth validation, and the broader applicability of our approach. Future work includes expanding analysis to 3D video data and leveraging video analysis to enable scalable evaluation within STEs.", "AI": {"tldr": "The paper introduces a video-based system for automatically assessing trainee performance in urban warfare simulations through computer vision techniques, aiming to provide scalable, objective, and actionable feedback for After Action Reviews.", "motivation": "To address the limitations of current performance assessment methods by developing an automated, objective, and scalable system that does not require additional hardware.", "method": "The video-based assessment pipeline utilizes computer vision techniques to derive task-specific metrics from training videos, such as psychomotor fluency, situational awareness, and team coordination, which are evaluated using an extended Cognitive Task Analysis (CTA) hierarchy.", "result": "A case study on real-world ECR drills demonstrated the ability of the system to provide actionable, domain-specific metrics for assessing individual and team performance in urban warfare simulations.", "conclusion": "The developed system offers a scalable solution for automatic performance assessment in urban warfare simulations, with potential for future expansion to 3D data and integration into training platforms like Gamemaster and GIFT, though challenges such as tracking difficulties and ground-truth validation remain."}}
{"id": "2512.23716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23716", "abs": "https://arxiv.org/abs/2512.23716", "authors": ["Toshiyuki Shigemura"], "title": "Noise-Driven Persona Formation in Reflexive Neural Language Generation", "comment": "324 pages, 9 figures (Figure 7 intentionally skipped), with Appendices A-I. This manuscript presents a computational framework for noise-driven persona formation in neural language generation, analyzing 152 generation cycles using GPT-5.1 with stochastic noise seeds generated by Microsoft Copilot. Primary category: cs.CL", "summary": "This paper introduces the Luca-Noise Reflex Protocol (LN-RP), a computational framework for analyzing noise-driven persona emergence in large language models. By injecting stochastic noise seeds into the initial generation state, we observe nonlinear transitions in linguistic behavior across 152 generation cycles. Our results reveal three stable persona modes with distinct entropy signatures, and demonstrate that external noise sources can reliably induce phase transitions in reflexive generation dynamics. Quantitative evaluation confirms consistent persona retention and significant differences across modes (p < 0.01). The protocol provides a reproducible method for studying reflexive generation, emergent behavior, and longrange linguistic coherence in LLMs.", "AI": {"tldr": "该论文介绍了Luca-Noise Reflex Protocol (LN-RP)，一种通过在大型语言模型中注入随机噪声种子来分析噪声驱动人格生成的计算框架。结果显示了三种稳定的人格模式，并证实了外部噪声源可以可靠地诱发反射生成动力学的相变。", "motivation": "研究动机在于探索在大型语言模型中噪声如何影响语言行为，并研究这些变化背后的动力学过程。", "method": "通过向生成的初始状态注入随机噪声种子，观察在152个生成周期中语言行为的非线性变化，以此来研究人格模式的形成。", "result": "结果表明，存在三种稳定的人格模式，并且在这些模式之间有着显著的熵差异(p < 0.01)。", "conclusion": "该协议为研究反射生成、新兴行为以及大型语言模型中的长期语言一致性提供了可重复的方法。"}}
{"id": "2512.23851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23851", "abs": "https://arxiv.org/abs/2512.23851", "authors": ["Lvmin Zhang", "Shengqu Cai", "Muyang Li", "Chong Zeng", "Beijia Lu", "Anyi Rao", "Song Han", "Gordon Wetzstein", "Maneesh Agrawala"], "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression", "comment": "https://github.com/lllyasviel/PFP", "summary": "We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.", "AI": {"tldr": "PFP是一种神经网络框架，它通过预训练来压缩视频，保留高频率图像细节，并能用作视频模型的记忆编码器。", "motivation": "研究的动机是为了有效地压缩视频，同时保留视频中的重要细节，并提高视频模型的记忆能力。", "method": "我们介绍了一种名为PFP的神经网络结构，它能够将长视频压缩成短上下文，并通过明确的预训练目标来保留单帧图像在任意时间点上的高频细节。", "result": "基准模型可以将一段20秒的视频压缩到大约5k长度的上下文中，在这个上下文中可以以感知上保留的外观随机检索帧。这种预训练模型可以直接微调为自回归视频模型的记忆编码器，从而以较低的上下文成本和相对较低的保真度损失使长历史记忆成为可能。", "conclusion": "我们通过消融实验评估了该框架，并讨论了可能的神经架构设计的权衡。"}}
{"id": "2512.23717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23717", "abs": "https://arxiv.org/abs/2512.23717", "authors": ["Shenzhe Zhu"], "title": "HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate", "comment": null, "summary": "Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.", "AI": {"tldr": "本文介绍了一种新的多智能体框架HarmTransform，用于将有害查询转换成更隐蔽的形式，实验显示此方法比标准基线更有效，但同时存在潜在的问题。", "motivation": "尽管大语言模型配备了检测和阻止有害查询的安全机制，但当前的对齐方法主要关注显而易见的危险内容，忽略更隐蔽的威胁。用户可以通过隐秘的重新表述来掩盖有害意图，这在现有安全训练数据中产生了显著的空白。文章旨在解决这一局限，并提升大语言模型的安全性。", "method": "文章介绍了HarmTransform，这是一种多智能体辩论框架，用于系统地将有害查询转换成更加隐蔽的形式，同时保留其潜在的恶意意图。该框架利用多个智能体之间的反复批判和优化来生成高质量、隐蔽的有害查询转换，可用于提高未来大语言模型的安全性对齐。", "result": "实验表明，HarmTransform相比标准基线显著提高了生成的有效查询转换效果。同时作者指出，虽然辩论能提升转换效果和增强隐匿性，但也可能引入话题偏离和不必要的复杂性。", "conclusion": "这些见解揭示了多智能体辩论在生成全面的安全训练数据方面的潜力和局限性。这项工作为提高LLM的对齐机制提供了一个新的思路。"}}
{"id": "2512.23860", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23860", "abs": "https://arxiv.org/abs/2512.23860", "authors": ["Qucheng Peng", "Hongfei Xue", "Pu Wang", "Chen Chen"], "title": "Lifelong Domain Adaptive 3D Human Pose Estimation", "comment": "Accepted by AAAI 2026", "summary": "3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.", "AI": {"tldr": "本文提出一种名为终身领域适应的3D人体姿态估计的新任务，涉及对抗网络框架，旨在解决领域适应中的灾难性遗忘问题。", "motivation": "当前3D人体姿态估计面临依赖控制环境下的标注3D数据的挑战，现有的领域适应方法未顾及目标数据集非平稳性问题。", "method": "提出了一个包括3D姿态生成器、2D姿态鉴别器和3D姿态估计器的对抗网络框架，来应对领域迁移，并设计了一个集成姿态感知、时间感知和领域感知知识的3D姿态生成器以提高适应性。", "result": "通过在多种领域适应3D姿态估计数据集上进行实验，验证了所提出方法的有效性。", "conclusion": "本文提出的基于生成对抗网络的框架，能有效改善3D人体姿态估计中的领域适应问题和解决灾难性遗忘挑战。"}}
{"id": "2512.23722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23722", "abs": "https://arxiv.org/abs/2512.23722", "authors": ["Adam Kamel", "Tanish Rastogi", "Michael Ma", "Kailash Ranganathan", "Kevin Zhu"], "title": "Emergent World Beliefs: Exploring Transformers in Stochastic Games", "comment": "Accepted at NeurIPS 2025 Mechanistic Interpretability Workshop", "summary": "Transformer-based large language models (LLMs) have demonstrated strong reasoning abilities across diverse fields, from solving programming challenges to competing in strategy-intensive games such as chess. Prior work has shown that LLMs can develop emergent world models in games of perfect information, where internal representations correspond to latent states of the environment. In this paper, we extend this line of investigation to domains of incomplete information, focusing on poker as a canonical partially observable Markov decision process (POMDP). We pretrain a GPT-style model on Poker Hand History (PHH) data and probe its internal activations. Our results demonstrate that the model learns both deterministic structure, such as hand ranks, and stochastic features, such as equity, without explicit instruction. Furthermore, by using primarily nonlinear probes, we demonstrated that these representations are decodeable and correlate with theoretical belief states, suggesting that LLMs are learning their own representation of the stochastic environment of Texas Hold'em Poker.", "AI": {"tldr": "研究展示了大规模语言模型在扑克这个不完全信息POMDP中，可以学习到手牌等级和概率权益等特征，学习了德州扑克的随机环境表示。", "motivation": "扩展了关于大规模语言模型在完美信息游戏中发展世界模型的工作，探讨了其在不完全信息领域中的能力。", "method": "使用GPT样式的模型在扑克手历史数据上进行预训练，并使用非线性探测来分析模型的内部激活。", "result": "该论文探讨了基于变压器的大规模语言模型（LLMs）在不完全信息领域中的应用，特别是集中在扑克游戏上，这种游戏被视为部分可观测马尔可夫决策过程（POMDP）。研究通过在扑克手历史数据上预训练GPT样式的模型，并探测其内部激活情况，展示了模型不仅学习了确定性的结构如手牌等级，也学习了概率特征如权益。通过使用非线性探测，研究还表明这些表示是可以解码的，并且与理论信念状态相关联，表明LLMs学习了德州扑克这种随机环境的自身表示。", "conclusion": "验证了LLMs在扑克游戏这种不完全信息环境中，能够学习和表示出确定性结构和概率特征。"}}
{"id": "2512.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23894", "abs": "https://arxiv.org/abs/2512.23894", "authors": ["Krithika Iyer", "Austin Tapp", "Athelia Paulli", "Gabrielle Dickerson", "Syed Muhammad Anwar", "Natasha Lepore", "Marius George Linguraru"], "title": "MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework", "comment": null, "summary": "Quantifying normative pediatric cranial development and suture ossification is crucial for diagnosing and treating growth-related cephalic disorders. Computed tomography (CT) is widely used to evaluate cranial and sutural deformities; however, its ionizing radiation is contraindicated in children without significant abnormalities. Magnetic resonance imaging (MRI) offers radiation free scans with superior soft tissue contrast, but unlike CT, MRI cannot elucidate cranial sutures, estimate skull bone density, or assess cranial vault growth. This study proposes a deep learning driven pipeline for transforming T1 weighted MRIs of children aged 0.2 to 2 years into synthetic CTs (sCTs), predicting detailed cranial bone segmentation, generating suture probability heatmaps, and deriving direct suture segmentation from the heatmaps. With our in-house pediatric data, sCTs achieved 99% structural similarity and a Frechet inception distance of 1.01 relative to real CTs. Skull segmentation attained an average Dice coefficient of 85% across seven cranial bones, and sutures achieved 80% Dice. Equivalence of skull and suture segmentation between sCTs and real CTs was confirmed using two one sided tests (TOST p < 0.05). To our knowledge, this is the first pediatric cranial CT synthesis framework to enable suture segmentation on sCTs derived from MRI, despite MRI's limited depiction of bone and sutures. By combining robust, domain specific variational autoencoders, our method generates perceptually indistinguishable cranial sCTs from routine pediatric MRIs, bridging critical gaps in non invasive cranial evaluation.", "AI": {"tldr": "通过深度学习技术，将儿童的MRI图像转换为合成的CT图像，以实现无辐射的颅骨和颅缝评估，填补了MRI无法显示颅缝和颅骨密度的空白。", "motivation": "由于CT包含辐射，不适合没有明显异常的儿童使用，而MRI无法清晰显示颅缝和颅骨密度，此研究提出了一种深度学习方法，旨在利用MRI生成合成了CT图像，以进行无辐射的颅骨生长和颅缝发育评估。", "method": "使用深度学习算法对0.2到2岁的儿童T1加权MRI图像进行处理，产生合成CT图像，预测详细的颅骨分割，生成颅缝概率热图和直接的颅缝分割。", "result": "生成的合成CT图像与真实CT图像在结构相似性上达到99%，颅骨分割的平均Dice系数为85%，颅缝分割的Dice系数为80%。", "conclusion": "该方法能够在无辐射的情况下，利用常规儿科MRI生成可感知的颅骨合成CT图像，解决了MRI无法显示骨密度和颅缝的问题。"}}
{"id": "2512.23732", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23732", "abs": "https://arxiv.org/abs/2512.23732", "authors": ["Anwar Alajmi", "Gabriele Pergola"], "title": "When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection", "comment": null, "summary": "Sexist content online increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals, even in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. Together, these limitations point to the need for a design that explicitly addresses the combined effects of (i) underrepresentation, (ii) noise, and (iii) conceptual ambiguity in both data and model predictions. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to adapt supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. Our training setup applies class-balanced focal loss, class-aware batching, and post-hoc threshold calibration to mitigate label imbalance and noisy supervision. At inference time, a dynamic routing mechanism classifies high-confidence cases directly and escalates uncertain instances to a novel \\textit{Collaborative Expert Judgment} (CEJ) module, which prompts multiple personas and consolidates their reasoning through a judge model. Our approach achieves state-of-the-art results across several benchmarks, with a +2.72\\% improvement in F1 on the EXIST 2025 Task 1.1, and a gains of +4.48\\% and +1.30\\% on the EDOS Tasks A and B, respectively.", "AI": {"tldr": "本文提出了一种两阶段框架来识别在线存在的隐晦性别歧视内容。该框架在训练阶段采用平衡数据和校准阈值来处理标签不均衡和有噪声的数据，在推理阶段则使用动态路由机制结合专家判断模块来处理模糊案例。该方法在多个基准测试中取得了显著的效果提升。", "motivation": "在线性别歧视内容越来越隐蔽，现有的方法难以准确检测。这归因于数据中的标签稀疏、类别不平衡和概念上的模糊性，导致现有模型无法有效捕捉这些细微歧视。", "method": "提出了一种两阶段的识别框架，首先在训练阶段采取平衡损失函数和批处理方法，以及后处理校准阈值以优化模型。其次在推理阶段利用动态路由来分类确定性高的情况，并将不确定的实例交给一个专门的多角色专家判断模块处理。", "result": "该模型取得了优秀的实验结果，与现有最佳模型相比，F1分数在EXIST 2025任务1.1中提高了2.72%，在EDOS任务A和B分别提高了4.48%和1.30%。", "conclusion": "这项研究提供了一种针对性解决数据稀缺、噪音和概念模糊性的方法，对在线性别歧视内容的识别做出了重要贡献，并展示了令人鼓舞的初步效果。"}}
{"id": "2512.23903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23903", "abs": "https://arxiv.org/abs/2512.23903", "authors": ["Charith Wickrema", "Eliza Mace", "Hunter Brown", "Heidys Cabrera", "Nick Krall", "Matthew O'Neill", "Shivangi Sarkar", "Lowell Weissman", "Eric Hughes", "Guido Zarrella"], "title": "Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale", "comment": null, "summary": "We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.", "AI": {"tldr": "研究了人工智能的扩展行为，使用大量商业卫星数据训练视觉变压器模型，观察到大规模数据集训练的性能仍主要受限于数据量而非模型参数。", "motivation": "探索人工智能的扩展行为，建立在高分辨率电光（EO）数据集上训练基础模型的实际技术，这些数据集比当前最先进的规模大很多倍。这些数据集对于依赖于稳健、领域专长编码器的现代多模态机器学习（ML）应用，如生成人工智能（GenAI）系统来进行图像标注、搜索和推理至关重要。", "method": "我们使用超过一千万亿像素的商业卫星电光数据和MITRE联邦AI沙箱来训练越来越大的视觉变压器（ViT）骨干模型，报告在拍字节规模观察到的成功和失败模式，并分析在其他遥感模态中缩小领域差距的潜在影响。", "result": "即使在这个规模上，性能仍然与数据限制阶段一致，而不是模型参数限制阶段。", "conclusion": "这些实用见解旨在为数据收集策略、计算预算和优化时间表提供信息，以促进遥感前沿规模基础模型的未来发展。"}}
