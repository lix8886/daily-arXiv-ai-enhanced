<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 17]
- [cs.CV](#cs.CV) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational NLP Resources at University Scale](https://arxiv.org/abs/2512.05179)
*Aurélie Montfrond*

Main category: cs.CL

> 研究开发了一个为爱尔兰利默里克大学电子与计算机工程系学生提供课程信息的聊天机器人，通过微调BERT模型来提升教育领域问答系统的性能。

<details>
  <summary>Details</summary>

**Motivation:** 此前的研究主要集中在开发面向通用对话的聊天机器人，缺乏针对特定领域知识推理的模型微调研究。本研究旨在填补将基础模型应用于教育领域的空白。

**Method:** 使用BERT模型和PyTorch进行微调，用于解答大学电子与计算机工程系课程相关的问题。

**Result:** 实验结果表明，即便是适度的模型微调也能够显著提升假设构建和知识提取的性能。

**Conclusion:** 通过微调BERT以适应学术领域的问答对，展示了开发大学课程材料专用基础模型的可行性，对构建自主教育知识系统具有潜在价值。

**Abstract:** Prior work on scientific question answering has largely emphasized chatbot-style systems, with limited exploration of fine-tuning foundation models for domain-specific reasoning. In this study, we developed a chatbot for the University of Limerick's Department of Electronic and Computer Engineering to provide course information to students. A custom dataset of 1,203 question-answer pairs in SQuAD format was constructed using the university book of modules, supplemented with manually and synthetically generated entries. We fine-tuned BERT (Devlin et al., 2019) using PyTorch and evaluated performance with Exact Match and F1 scores. Results show that even modest fine-tuning improves hypothesis framing and knowledge extraction, demonstrating the feasibility of adapting foundation models to educational domains. While domain-specific BERT variants such as BioBERT and SciBERT exist for biomedical and scientific literature, no foundation model has yet been tailored to university course materials. Our work addresses this gap by showing that fine-tuning BERT with academic QA pairs yields effective results, highlighting the potential to scale towards the first domain-specific QA model for universities and enabling autonomous educational knowledge systems.

</details>


### [2] [Unveiling Affective Polarization Trends in Parliamentary Proceedings](https://arxiv.org/abs/2512.05231)
*Gili Goldin,Ella Rabinovich,Shuly Wintner*

Main category: cs.CL

> The study introduced a new method to quantify affective polarization through emotional discourse analysis in the Israeli parliament, showing an increasing trend over time between government and opposition members.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to detect and operationalize the concept of affective polarization from a new angle that focuses on emotional responses instead of ideological differences, given the observed increase in polarized discourse worldwide.

**Method:** The paper proposes a novel method for quantifying polarization based on the emotional style of the discourse, using measures of Valence, Arousal and Dominance.

**Result:** The findings indicate that the emotional styles of government and opposition members differ, and the level of affective polarization, as detected by this method, shows a significant increase over time.

**Conclusion:** The conclusion is that the proposed method, by focusing on the emotional style of discourse, can effectively measure affective polarization in political settings, such as the Israeli parliament, revealing its temporal dynamics.

**Abstract:** Recent years have seen an increase in polarized discourse worldwide, on various platforms. We propose a novel method for quantifying polarization, based on the emotional style of the discourse rather than on differences in ideological stands. Using measures of Valence, Arousal and Dominance, we detect signals of emotional discourse and use them to operationalize the concept of affective polarization. Applying this method to a recently released corpus of proceedings of the Knesset, the Israeli parliament (in Hebrew), we find that the emotional style of members of government differs from that of opposition members; and that the level of affective polarization, as reflected by this style, is significantly increasing with time.

</details>


### [3] [Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting](https://arxiv.org/abs/2512.05243)
*P. D. Edgar,Alia Hall*

Main category: cs.CL

> 本研究提出诗歌提示模式作为创意文本提示的一种形式，可以成为提示工程中的有用工具。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索大型语言模型在创意写作方面的潜力，特别是通过诗歌提示模式评估和测试这些模型描述和评价创造力作品的能力。

**Method:** 使用诗歌提示模式来评估和测试三个模型对知名诗人作品的描述和评价，并考察模型对不同受众的适应或改写创意作品的意愿。

**Result:** 研究展示了诗歌提示模式在评估大型语言模型创作倾向和偏见上的潜力。

**Conclusion:** 诗歌提示是一种有潜力的提示工程方法，可以进一步研究其在扩展语言模型的创意边界方面的应用。

**Abstract:** Prompt engineering has emerged as a useful way studying the algorithmic tendencies and biases of large language models. Meanwhile creatives and academics have leveraged LLMs to develop creative works and explore the boundaries of their writing capabilities through text generation and code. This study suggests that creative text prompting, specifically Poetry Prompt Patterns, may be a useful addition to the toolbox of the prompt engineer, and outlines the process by which this approach may be taken. Then, the paper uses poetic prompts to assess descriptions and evaluations of three models of a renowned poet and test the consequences of the willingness of models to adapt or rewrite original creative works for presumed audiences.

</details>


### [4] [Enhancing Clinical Note Generation with ICD-10, Clinical Ontology Knowledge Graphs, and Chain-of-Thought Prompting Using GPT-4](https://arxiv.org/abs/2512.05256)
*Ivan Makohon,Mohamad Najafi,Jian Wu,Mathias Brochhausen,Yaohang Li*

Main category: cs.CL

> 本文提出了一种使用Chain-of-Thought (CoT) 提示工程技术和语义搜索结合的知识图谱来增强大型语言模型，生成更高质量的临床记录的方法。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于美国由于2009年HITECH法案和2016年21世纪治愈法案的政策环境，电子健康记录（EHR）数据量在过去十年出现激增。然而，医生手工编写临床记录消耗了大量的时间，因此，本研究旨在通过语言模型优化这一过程。

**Method:** 本研究采用Chain-of-Thought (CoT) 提示工程技术，结合语义搜索结果和从临床本体构建的知识图谱，以国际疾病分类（ICD）代码和基本患者信息为输入，生成临床记录。

**Result:** 在六个从CodiEsp测试数据集选择的临床案例上，本研究的方法相比标准一次式提示生成的临床记录表现更优。

**Conclusion:** 使用Chain-of-Thought (CoT) 提示工程技术和语义搜索结果结合知识图谱的策略能够提升大型语言模型生成的临床记录质量。

**Abstract:** In the past decade a surge in the amount of electronic health record (EHR) data in the United States, attributed to a favorable policy environment created by the Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 and the 21st Century Cures Act of 2016. Clinical notes for patients' assessments, diagnoses, and treatments are captured in these EHRs in free-form text by physicians, who spend a considerable amount of time entering and editing them. Manually writing clinical notes takes a considerable amount of a doctor's valuable time, increasing the patient's waiting time and possibly delaying diagnoses. Large language models (LLMs) possess the ability to generate news articles that closely resemble human-written ones. We investigate the usage of Chain-of-Thought (CoT) prompt engineering to improve the LLM's response in clinical note generation. In our prompts, we use as input International Classification of Diseases (ICD) codes and basic patient information. We investigate a strategy that combines the traditional CoT with semantic search results to improve the quality of generated clinical notes. Additionally, we infuse a knowledge graph (KG) built from clinical ontology to further enrich the domain-specific knowledge of generated clinical notes. We test our prompting technique on six clinical cases from the CodiEsp test dataset using GPT-4 and our results show that it outperformed the clinical notes generated by standard one-shot prompts.

</details>


### [5] [To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples](https://arxiv.org/abs/2512.05318)
*Vignesh Kothapalli,Ata Fatahibaarzi,Hamed Firooz,Maziar Sanjabi*

Main category: cs.CL

> 研究提出了一种名为CoT-Recipe的技术来调整元训练序列中CoT示例和非CoT示例的混合比例，以提高大型语言模型在新型任务上的推理准确性。

<details>
  <summary>Details</summary>

**Motivation:** 链式思考（CoT）提示结合少量的场景内学习（ICL）解锁了大型语言模型（LLMs）的重要推理能力。然而，当预训练知识不足时，包含CoT示例的ICL在处理新型任务时效果不佳。

**Method:** 研究采用CoT-ICL Lab框架在一个受控环境中研究了这种问题，并提出在元训练中学习新型抽象推理任务的技术。为了减轻由于元训练中大量包含CoT示例而对性能造成的影响，提出了一种名为CoT-Recipe的方法，该方法使用正式的方式来调节元训练序列中CoT示例和非CoT示例的比例。

**Result:** 研究表明，通过CoT-Recipe的精细调节，即使在没有场景内CoT示例的情况下，也能将变换器在新型任务上的准确性提高高达300%。将其应用于具有符号推理任务的预训练大型语言模型（Qwen2.5系列）时，准确性提高高达130%。

**Conclusion:** 实验结果表明，CoT-Recipe方法能够有效调节元训练序列中CoT示例和非CoT示例的比例，显著提升大型语言模型在处理新型任务时的性能，尤其是提高其推理能力和准确性。

**Abstract:** Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.

</details>


### [6] [LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning](https://arxiv.org/abs/2512.05325)
*Ömer Faruk Akgül,Yusuf Hakan Kalaycı,Rajgopal Kannan,Willie Neiswanger,Viktor Prasanna*

Main category: cs.CL

> LYNX presents an innovative approach to reduce the inference-time compute usage of large reasoning models by making them stop their reasoning process early when enough information is gathered for correct answers.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the issue of overthinking in large reasoning models, where they continue to reason unnecessarily, wasting compute resources and potentially lowering accuracy.

**Method:** LYNX, an online early-exit mechanism, is introduced. It transforms the model's internal hidden-state into decisions to stop reasoning early. By attaching exit decisions to naturally occurring reasoning cues during generation, it trains a lightweight probe on hidden states using forced exits as supervision and wraps the scores in split conformal prediction to control premature exits.

**Result:** The method is trained and calibrated once on a generic mathematical corpus and is reused across benchmarks, temperatures, and non-mathematical tasks. It reduces the number of tokens needed by 40--65% without hurting accuracy on GSM8K, and up to 70% fewer tokens on CommonsenseQA while slightly improving accuracy.

**Conclusion:** LYNX provides strong accuracy-efficiency tradeoffs, outperforming or matching state-of-the-art early-exit methods. It is fully online, does not require proxy models, and offers explicit, tunable confidence guarantees.

**Abstract:** Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often "overthink": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., "hmm", "wait") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.

</details>


### [7] [Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats](https://arxiv.org/abs/2512.05331)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee,Mostafa Musharrat,Sai Vishnu Vamsi*

Main category: cs.CL

> 本文针对低质量自动生成的"粉滑肉"新闻文章进行研究，发现这些文章通过大型语言模型的修改可以显著影响现有检测系统的性能。为此，我们开发了一个新的学习框架，有效提升了对大型语言模型生成内容的检测。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决日益增长的低质量自动生成新闻对人们获取可靠信息的威胁，特别是这些内容通过大型语言模型进行修改时，常规的检测方法将面临重大挑战。

**Method:** 本文提出了一种新的学习框架，用于检测受到大型语言模型影响的粉滑肉新闻。该框架设计用于抵抗这些模型的对抗攻击，并能适应自动化粉滑肉新闻的变化。

**Result:** 研究表明，现有检测系统在面对大型语言模型生成的内容时，其性能最多可降低40%的F1分数，而采用本文提出的新框架后，检测性能提高了27%。

**Conclusion:** 研究证明了大型语言模型对自动检测系统的严重威胁，并提出了有效的对抗性学习框架来抵抗这种威胁，从而保护新闻的真实性和可靠性。

**Abstract:** The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.

</details>


### [8] [Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change](https://arxiv.org/abs/2512.05364)
*Ananth Hariharan,David Mortensen*

Main category: cs.CL

> 研究通过弱监督混合方法分析梵语音位演化，揭示其形态复杂性未减少，反而重新分配至其他领域，并建立了信心加权的系统以增强计算古典学的可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究挑战了语言变化等于简化的朴素假设，通过对2000多年梵语进行定量分析，以此证明混合神经符号方法如何为形态丰富、资源较少语言的演化提供新的见解。

**Method:** 此研究采用混合神经符号方法，通过弱监督技术解决了数据稀缺的问题。研究使用100多种高精度的正则表达式模式生成伪标签，用于微调多语言BERT模型。然后通过一种新的信心加权聚合方式，融合了符号和神经网络的输出，以建立一个既可扩展又可解释的系统。

**Result:** 该系统应用于147万个字的历时语料库中，实现了52.4%的特征检测率。结果表明，梵语的整体形态复杂性并未减少，而是动态重新分配：早期动词特征表现出周期性下降的趋势，而复杂性则转移到了其他领域，表现为词缀的显著扩展和新哲学术语的出现。

**Conclusion:** 系统的不确定性估计经过良好校准，其信心度与准确度高度相关，低整体校准误差，提高了发现结果的可靠性。这为计算古典学提供了重要的参考依据。

**Abstract:** This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.

</details>


### [9] [Mitigating Self-Preference by Authorship Obfuscation](https://arxiv.org/abs/2512.05379)
*Taslim Mahbub,Shi Feng*

Main category: cs.CL

> 本文研究了减少语言模型裁判自我偏好的策略，通过扰动混淆作者身份以降低自我偏好，但发现完全消除这一难题仍具挑战性。

<details>
  <summary>Details</summary>

**Motivation:** 减轻语言模型裁判在评估中的自我偏好难题，提高评估的公正性。自我偏好难以消除，因为前沿的语言模型能够识别出自己的输出。

**Method:** 我们提出了一种通过在成对比较的评估候选人上应用黑盒扰动来减少语言模型裁判自我识别的方法，以混淆作者身份并降低自我偏好。

**Result:** 发现简单的同义词替换等扰动可以可预测地降低自我偏好，但在尝试更全面地中和评估候选人之间的风格差异时，自我偏好会恢复。

**Conclusion:** 研究发现自我识别和自我偏好可以在多个语义层面上发生，尽管初步结果显示有希望，但完全缓解这一偏见仍具有挑战性。

**Abstract:** Language models (LMs) judges are widely used to evaluate the quality of LM outputs. Despite many advantages, LM judges display concerning biases that can impair their integrity in evaluations. One such bias is self-preference: LM judges preferring their own answers over those produced by other LMs or humans. The bias is hard to eliminate as frontier LM judges can distinguish their own outputs from those of others, even when the evaluation candidates are not labeled with their sources. In this paper, we investigate strategies to mitigate self-preference by reducing the LM judges' ability to recognize their own outputs. We apply black-box perturbations to evaluation candidates in pairwise comparison to obfuscate the authorship and reduce self-recognition. We find that perturbations as simple as synonym replacement for a few words predictably reduce self-preference. However, we also uncover fundamental challenges to eliminating the bias: when we extrapolate our perturbations to a more complete neutralization of stylistic differences between the evaluation candidates, self-preference recovers. Our findings suggest that self-recognition and self-preference can happen on many semantic levels, and complete mitigation remains challenging despite promising initial results.

</details>


### [10] [Learning from Self Critique and Refinement for Faithful LLM Summarization](https://arxiv.org/abs/2512.05387)
*Ting-Yao Hu,Hema Swetha Koppula,Hadi Pouransari,Cem Koc,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

> 本文提出了一种自我监督框架SCRPO，用于通过偏好学习改善LLM的忠实总结能力，实验表明该方法在忠实度指标上优于现有方法，同时保持或提高其他质量指标。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现有方法在减少LLMs的幻觉问题时存在的额外测试时计算成本高或依赖强大教师模型的问题，提出了一种更为有效且实用的方法。

**Method:** 提出了一种自我监督训练框架Self Critique and Refinement-based Preference Optimization (SCRPO)，该框架首先通过利用LLM自身的批评和改进能力构建偏好数据集，然后应用偏好学习来提升LLM以实现忠实的总结。

**Result:** 实验结果表明该方法在三个总结基准测试(XSUM, CNNDM和SAMSum)中，在忠实度指标上优于最先进的自我监督学习方法，同时在保持或提高其他衡量摘要整体质量的指标上也表现出色。

**Conclusion:** SCRPO方法不仅提升了效率，而且生成了更加忠实的摘要，展示了其在忠实摘要生成方面的优越性。

**Abstract:** Large Language Models (LLMs) often suffer from hallucinations: output content that is not grounded in the input context, when performing long-form text generation tasks such as summarization. Prior works have shown that hallucinations can be reduced by iteratively critiquing and refining previously generated outputs using either the same model or a more powerful teacher model as the critique. However, these approaches either require additional test-time compute or assume access to more powerful teacher models, making them costly and less practical. In this work, we propose Self Critique and Refinement-based Preference Optimization (SCRPO), which is a self-supervised training framework that first constructs a preference dataset by leveraging the LLM's own critique and refinement capabilities, and then applies preference learning to improve the same LLM for faithful summarization. Experiments on three summarization benchmarks (XSUM CNNDM and SAMSum), demonstrate that our approach outperforms state-of-the-art self-supervised learning methods in terms of faithfulness metrics while either maintaining or improving other metrics that measure the overall quality of the summary. Moreover, compared to test-time refinement, our approach not only improves efficiency but also results in more faithful summaries.

</details>


### [11] [SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs](https://arxiv.org/abs/2512.05409)
*Ruixuan Huang,Hao Zeng,Hantao Huang,Jinyuan Shi,Minghui Yu,Ian En-Hsu Yen,Shuai Wang*

Main category: cs.CL

> 本文提出了一种新的数据格式SQ-format，解决了低比特量化和稀疏化技术难以平衡准确性和效率的问题，特别适合激活状态不等的场景，并展示了SQ格式在PTQ中的优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的低比特量化和稀疏化技术难以平衡准确性和效率，因为硬件支持有限。例如，W4A8只能达到与W8A8相同的峰值TOPS，而GPU支持的稀疏数据格式（2:4半结构化稀疏）由于准确性损失很少被采用。为解决这一问题，在此文中提出了一种新的解决方案。

**Method:** 提出稀疏量化格式（SQ格式），利用稀疏矩阵可以在高精度加速，同时低精度矩阵乘法也能加速的特性，以实现性能和吞吐量之间的帕累托改进。

**Result:** 通过提出稀疏量化格式（SQ格式），实现了量化和稀疏化之间的统一数据格式，适用于新硬件和现有GPU，提高了性能和吞吐量之间的帕累托改进，特别适合处理具有离群值不等状态的激活，并展示了SQ格式的最先进的PTQ性能。

**Conclusion:** 成功展示了SQ格式的PTQ性能，并提出了支持SQ格式所需的硬件，此外，还为下一代AI加速器的设计探索提供了洞察。

**Abstract:** Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.

</details>


### [12] [LMSpell: Neural Spell Checking for Low-Resource Languages](https://arxiv.org/abs/2512.05414)
*Akesh Gunathilakea,Nadil Karunarathnea,Tharusha Bandaranayakea,Nisansa de Silvaa,Surangika Ranathunga*

Main category: cs.CL

> 本研究通过实证分析发现，在大规模的微调数据集中，大型语言模型（LLMs）对于拼写纠正任务，尤其是在低资源语言中，表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决低资源语言中的拼写纠正问题，并探索不同类型的PLM在拼写纠正任务中的效果。

**Method:** 该研究是关于预训练语言模型（PLM）在拼写纠正中的有效性的首个实证研究，特别是针对低资源语言（LRLs）。研究中比较了不同类型的语言模型（LLMs、编码器模型和编解码器模型）的性能。

**Result:** 研究表明，当微调数据集较大时，大型语言模型（LLMs）比编码器模型和编解码器模型表现更好，这一结论在PLM未预训练的语言中同样成立。

**Conclusion:** 提出了一个名为LMSpell的拼写纠正工具包，包括一个评估功能来补偿LLMs的幻觉问题，并通过斯里兰卡语的一个案例研究展示了LLMs在低资源语言拼写纠正中的潜力。

**Abstract:** Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.

</details>


### [13] [ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering](https://arxiv.org/abs/2512.05430)
*Daeyong Kwon,SeungHeon Doh,Juhan Nam*

Main category: cs.CL

> 研究者提出了一种在音乐问答任务中提升事实准确性与上下文推理能力的方法，通过构建MusWikiDB数据库和ArtistMus测试基准，利用RAG（检索增强生成）来增强音乐领域的事实检索和问题回答。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在开放式问题回答方面取得了进展，但它们在音乐相关推理方面的有效性仍然有限，这是因为预训练数据中的音乐知识相对稀少。而音乐信息检索和计算音乐学虽然探索了结构化和多模态理解，但很少有资源支持基于艺术家元数据或历史背景的事实性和上下文音乐问题回答。

**Method:** 我们引入了MusWikiDB，这是来自144K个音乐相关维基百科页面的320万段文章的向量数据库，以及ArtistMus，这是一个针对500位不同艺术家的1000个问题的基准，包含了诸如流派、出道年份和主题等元数据。这些资源能够系统地评估MQA中检索增强生成（RAG）的效果。

**Result:** 实验显示，RAG显著提高了事实准确性；开源模型最多提高了56.8个百分点（例如，Qwen3 8B从35.0提高到了91.8），达到了专有模型的性能。RAG式的微调进一步提升了事实检索和上下文推理，提高了领域内和领域外的基准测试结果。MusWikiDB的准确性也比通用维基百科语料库高出约6个百分点，检索速度快了40%。

**Conclusion:** 我们发布了MusWikiDB和ArtistMus，以推进音乐信息检索和领域特定问题回答的研究，为音乐等文化丰富领域的检索增强推理提供了基础。

**Abstract:** Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.

</details>


### [14] [Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment](https://arxiv.org/abs/2512.05464)
*Panatchakorn Anantaprayoon,Nataliia Babina,Jad Tarifi,Nima Asgharbeygi*

Main category: cs.CL

> 本文提出了一个更全面的AI对齐目标——集体能动性（CA）和一个可自我改进的对齐方法——动态对齐。这种方法在保持泛用NLP能力的同时，能够成功实现模型的对齐。

<details>
  <summary>Details</summary>

**Motivation:** 当前的AI系统通过偏好数据或预定义原则进行对齐，但这种方法在迈向通用人工智能和超人工智能时可能变得不足。此外，基于人类反馈的对齐方法在资源需求和可扩展性方面存在困难。本研究旨在解决这些问题，提出更全面和更具可扩展性的对齐方法。

**Method:** 本研究提出了一种更全面的对齐目标——集体能动性（CA），以及一种可扩展的自我升级对齐方法——动态对齐。动态对齐框架包括两部分：1. 使用大型语言模型生成自动训练数据集；2. 自我奖励机制，策略模型评估其输出候选并为其分配奖励，以进行基于目标奖励的策略优化（GRPO）学习。

**Result:** 实验结果证明，该方法能够使模型对准集体能动性目标，同时保持泛用的自然语言处理能力。

**Conclusion:** 集体能动性和动态对齐是一个有潜力的方向，为未来的AI提供了一个可能的对齐方法，这有助于解决迈向超人工智能时对齐方面的挑战。

**Abstract:** Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.

</details>


### [15] [SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures](https://arxiv.org/abs/2512.05501)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

> 研究人员开发了一项针对东南亚地区的第一个安全基准测试SEA-SafeguardBench，发现当前的大型语言模型和防护措施在东南亚的文化和危害场景中表现不佳。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评价方法多以英语为主，忽视了语言和文化的多样性。东南亚语言在此方面的研究较少，但该地区有独特的安全性关注点。因此，研究旨在通过当地语言和规范反映本地的危害场景。

**Method:** 该研究介绍了SEA-SafeguardBench，这是一个面向东南亚地区的首个由人类验证的安全基准测试。该基准测试涵盖八种语言，包括21,640个样本，并分为三个子集：通用、真实世界和内容生成。

**Result:** 实验结果表明，即使是最先进的LLM和防护措施也难以应对东南亚地区特有的文化和危害场景，并且在与英文文本比较时表现不佳。

**Conclusion:** SEA-SafeguardBench凸显了跨文化内容检测和防护的挑战，指出当前的LLM和安全措施在东南亚地区的实际应用场景中存在不足，强调了更加多样化和本地化评估的重要性。

**Abstract:** Safeguard models help large language models (LLMs) detect and block harmful content, but most evaluations remain English-centric and overlook linguistic and cultural diversity. Existing multilingual safety benchmarks often rely on machine-translated English data, which fails to capture nuances in low-resource languages. Southeast Asian (SEA) languages are underrepresented despite the region's linguistic diversity and unique safety concerns, from culturally sensitive political speech to region-specific misinformation. Addressing these gaps requires benchmarks that are natively authored to reflect local norms and harm scenarios. We introduce SEA-SafeguardBench, the first human-verified safety benchmark for SEA, covering eight languages, 21,640 samples, across three subsets: general, in-the-wild, and content generation. The experimental results from our benchmark demonstrate that even state-of-the-art LLMs and guardrails are challenged by SEA cultural and harm scenarios and underperform when compared to English texts.

</details>


### [16] [Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy Evaluation of LLM-Based and Supervised Approaches](https://arxiv.org/abs/2512.05537)
*Namu Park,Farzad Ahmed,Zhaoyi Sun,Kevin Lybarger,Ethan Breinhorst,Julie Hu,Ozlem Uzuner,Martin Gunn,Meliha Yetisgen*

Main category: cs.CL

> The study compared LLMs and supervised models for lesion-level detection of incidentalomas in radiology reports, showing that enhanced LLMs significantly outperform baseline models.

<details>
  <summary>Details</summary>

**Motivation:** The goal was to evaluate large language models (LLMs) against supervised baselines for detecting incidentalomas at the fine-grained, lesion-level. This addresses the limitations of existing document-level classification systems.

**Method:** We used a dataset of 400 annotated radiology reports with 1,623 verified lesion findings to compare three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) and four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). A novel inference strategy using lesion-tagged inputs and anatomy-aware prompting was introduced to enhance model reasoning. Performance was evaluated with class-specific F1-scores.

**Result:** The anatomy-informed GPT-OSS-20b model achieved the highest performance, with an incidentaloma-positive macro-F1 score of 0.79, surpassing all supervised baselines (maximum macro-F1: 0.70) and approaching the inter-annotator agreement of 0.76. Explicit anatomical grounding significantly improved performance across GPT-based models, with an ensemble of the top systems further boosting the macro-F1 to 0.90.

**Conclusion:** Generative LLMs, when enhanced with structured lesion tagging and anatomical context, are superior to traditional supervised models and perform comparably to human experts. This approach promises a reliable and interpretable method for automated incidental finding surveillance in radiology.

**Abstract:** Objective: To evaluate large language models (LLMs) against supervised baselines for fine-grained, lesion-level detection of incidentalomas requiring follow-up, addressing the limitations of current document-level classification systems.
  Methods: We utilized a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings. We compared three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) against four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). We introduced a novel inference strategy using lesion-tagged inputs and anatomy-aware prompting to ground model reasoning. Performance was evaluated using class-specific F1-scores.
  Results: The anatomy-informed GPT-OSS-20b model achieved the highest performance, yielding an incidentaloma-positive macro-F1 of 0.79. This surpassed all supervised baselines (maximum macro-F1: 0.70) and closely matched the inter-annotator agreement of 0.76. Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05), while a majority-vote ensemble of the top systems further improved the macro-F1 to 0.90. Error analysis revealed that anatomy-aware LLMs demonstrated superior contextual reasoning in distinguishing actionable findings from benign lesions.
  Conclusion: Generative LLMs, when enhanced with structured lesion tagging and anatomical context, significantly outperform traditional supervised encoders and achieve performance comparable to human experts. This approach offers a reliable, interpretable pathway for automated incidental finding surveillance in radiology workflows.

</details>


### [17] [Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems](https://arxiv.org/abs/2512.05580)
*Aurprita Mahmood,Sabrin alam,Neloy kumer Sagor,Md. Abdul Hadi,Md. Sehab Al Islam,Minhajul Islam*

Main category: cs.CL

> 该研究通过实验对比了标准提示、链式思维（CoT）和树状思维（ToT）在解决孟加拉语数学问题上的表现，结果表明ToT方法可以显著提高大型模型的准确性，特别是在处理低资源语言时更为有效。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究的动机是解决数学问题在自然语言处理中的挑战，特别是需要多步逻辑推理的问题，并且希望克服CoT提示方法中线性结构带来的错误传播问题。

**Method:** 研究者在SOMADHAN数据集上通过树状思维（ToT）方法对100个代表性问题进行系统研究，评估了包括GPT-OSS和不同版本的LLaMA在内的多个大型语言模型的表现。

**Result:** 研究表明，CoT方法可以将标准提示的准确率从78%提高到83%，而ToT方法则进一步提高性能到最高88%（使用GPT-OSS-120B模型）。

**Conclusion:** 研究结论展示，ToT作为一种结构化的推理方法，特别适用于解决大型和中型规模模型处理的数学问题，它能够为多语言NLP提供更可靠和一致的解决方案。

**Abstract:** Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [18] [AREA3D: Active Reconstruction Agent with Unified Feed-Forward 3D Perception and Vision-Language Guidance](https://arxiv.org/abs/2512.05131)
*Tianling Xu,Shengzhe Gan,Leslie Gu,Yuelei Li,Fangneng Zhan,Hanspeter Pfister*

Main category: cs.CV

> The paper introduces AREA3D, a novel framework for active 3D reconstruction that leverages feed-forward models and vision-language guidance to improve reconstruction quality and efficiency, especially in sparse-view settings.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing active reconstruction methods that rely on hand-crafted geometric heuristics and often result in redundant observations without significantly improving the quality of the reconstruction.

**Method:** Active 3D reconstruction method proposed is called AREA3D, which leverages feed-forward 3D reconstruction models and vision-language guidance to decouple view-uncertainty modeling from the underlying reconstructor, enabling efficient uncertainty estimation and eliminating the need for expensive online optimization.

**Result:** Experiments show that AREA3D achieves state-of-the-art reconstruction accuracy, especially in scenarios with sparse views.

**Conclusion:** The proposed AREA3D framework for active 3D reconstruction achieves superior geometry reconstruction results and demonstrates the potential for informed decision-making in viewpoint selection beyond traditional geometric considerations.

**Abstract:** Active 3D reconstruction enables an agent to autonomously select viewpoints to efficiently obtain accurate and complete scene geometry, rather than passively reconstructing scenes from pre-collected images. However, existing active reconstruction methods often rely on hand-crafted geometric heuristics, which can lead to redundant observations without substantially improving reconstruction quality. To address this limitation, we propose AREA3D, an active reconstruction agent that leverages feed-forward 3D reconstruction models and vision-language guidance. Our framework decouples view-uncertainty modeling from the underlying feed-forward reconstructor, enabling precise uncertainty estimation without expensive online optimization. In addition, an integrated vision-language model provides high-level semantic guidance, encouraging informative and diverse viewpoints beyond purely geometric cues. Extensive experiments on both scene-level and object-level benchmarks demonstrate that AREA3D achieves state-of-the-art reconstruction accuracy, particularly in the sparse-view regime. Code will be made available at: https://github.com/TianlingXu/AREA3D .

</details>


### [19] [Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training](https://arxiv.org/abs/2512.05132)
*Wenshuo Wang,Fan Zhang*

Main category: cs.CV

> The paper introduces Frequency Representation Learning to solve the Scale Anchoring problem in zero-shot super-resolution forecasting, achieving better results with moderate computational cost.

<details>
  <summary>Details</summary>

**Motivation:** To improve multi-resolution generalization and accuracy in zero-shot super-resolution spatiotemporal forecasting tasks, by addressing the fundamental issue of Scale Anchoring where errors are incorrectly interpreted as successful generalization due to the limitations of training data.

**Method:** Frequency Representation Learning is proposed to address the problem of Scale Anchoring in zero-shot super-resolution spatiotemporal forecasting. Frequency representations aligned with resolution and spectral consistency training are used.

**Result:** FRL-enhanced variants show more stable frequency response in high-frequency bands on grids with higher Nyquist frequencies, leading to decreased errors with higher resolution and outperformance of baselines with modest computational overhead.

**Conclusion:** The proposed Frequency Representation Learning effectively reduces computational overhead while improving the performance of zero-shot super-resolution spatiotemporal forecasting by tackling the Scale Anchoring issue.

**Abstract:** Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider maintaining similar error across different resolutions as indicative of successful multi-resolution generalization. However, deep learning models serving as alternatives to numerical solvers should reduce error as resolution increases. The fundamental limitation is, the upper bound of physical law frequencies that low-resolution data can represent is constrained by its Nyquist frequency, making it difficult for models to process signals containing unseen frequency components during high-resolution inference. This results in errors being anchored at low resolution, incorrectly interpreted as successful generalization. We define this fundamental phenomenon as a new problem distinct from existing issues: Scale Anchoring. Therefore, we propose architecture-agnostic Frequency Representation Learning. It alleviates Scale Anchoring through resolution-aligned frequency representations and spectral consistency training: on grids with higher Nyquist frequencies, the frequency response in high-frequency bands of FRL-enhanced variants is more stable. This allows errors to decrease with resolution and significantly outperform baselines within our task and resolution range, while incurring only modest computational overhead.

</details>


### [20] [InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models](https://arxiv.org/abs/2512.05134)
*Zihao Wu*

Main category: cs.CV

> 研究者提出InvarDiff方法，通过利用确定性采样中的相对时间不变性，在扩散模型中减少冗余计算，显著提升推理速度而不影响生成结果的保真度。

<details>
  <summary>Details</summary>

**Motivation:** 扩散模型虽然能生成高质量的合成数据，但由于迭代采样过程导致运行速度较慢。研究者希望通过减少冗余计算来加速这一过程。

**Method:** InvarDiff方法通过计算每个时间步、每层和每个模块的二进制缓存计划矩阵来加速扩散模型的推理过程，该矩阵确定哪些模块在哪个步骤被重用而非重新计算。同时使用再采样修正来避免连续缓存时的漂移问题。

**Result:** 实验表明，InvarDiff在DiT和FLUX模型上实现了2-3倍的推理加速，对标准质量度量几乎没有影响，在视觉质量上也没有观察到退化。

**Conclusion:** InvarDiff作为无需训练的加速方法，通过计算并利用缓存策略，可以显著提升扩散模型的推理速度，同时保持生成结果的高质量。

**Abstract:** Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.

</details>


### [21] [Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes](https://arxiv.org/abs/2512.05136)
*Yujie Xiao,Gongzhen Tang,Deyun Zhang,Jun Li,Guangkun Nie,Haoyu Wang,Shun Huang,Tong Liu,Qinghao Zhao,Kangyin Chen,Shenda Hong*

Main category: cs.CV

> 本研究表明，一种新型的AI-ECG模型可以在无需CCTA的情况下准确预测冠状动脉的狭窄程度，为CAD的快速筛查提供了一种潜在的非侵入性方法。其性能在内部和外部验证中显示出稳定性和可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 冠状动脉疾病（CAD）仍然是全球主要的健康负担。准确识别罪犯血管和评估狭窄严重程度对于指导个性化治疗至关重要。尽管冠状动脉CT血管造影（CCTA）是CAD诊断的第一线非侵入性方法，但其对高端设备的依赖、辐射暴露和患者合作的严格要求限制了其在大范围的使用。随着人工智能（AI）的进步和心电图（ECG）的广泛可用性，AI-ECG为CAD筛查提供了一种有吸引力的替代方案。

**Method:** 开发了一种可解释的人工智能-心电图（AI-ECG）模型来预测冠状动脉CT血管造影（CCTA）中四条主要冠状动脉（右冠状动脉RCA、左主冠状动脉LM、左前降支LAD和左回旋支LCX）的严重或完全阻塞。

**Result:** 在内部验证集中，模型对于RCA、LM、LAD和LCX的AUC（曲线下面积）分别为0.794、0.818、0.744和0.755；在外部验证集中，AUC分别达到了0.749、0.971、0.667和0.727。表现力在规范的ECG子集下保持稳定，表明该模型具有超出明显ECG异常的强大稳定性。按人口统计学和采集时间分割的亚组分析进一步证实了模型的稳定性。基于血管特异性发病率阈值的风险分层在校准曲线和累积事件曲线上显示出一致性区分。

**Conclusion:** AI-ECG模型在预测冠状动脉狭窄的严重性上表现出良好的效果，该模型在不同验证集中的性能保持稳定，为CAD的非侵入性筛查提供了有力工具。通过解释性分析揭示了高危与低危人群的波形差异，突出了对模型决策作出贡献的关键电生理区域，并对冠状动脉狭窄的ECG关联提供了新的见解。

**Abstract:** Coronary artery disease (CAD) remains a major global health burden. Accurate identification of the culprit vessel and assessment of stenosis severity are essential for guiding individualized therapy. Although coronary CT angiography (CCTA) is the first-line non-invasive modality for CAD diagnosis, its dependence on high-end equipment, radiation exposure, and strict patient cooperation limits large-scale use. With advances in artificial intelligence (AI) and the widespread availability of electrocardiography (ECG), AI-ECG offers a promising alternative for CAD screening. In this study, we developed an interpretable AI-ECG model to predict severe or complete stenosis of the four major coronary arteries on CCTA. On the internal validation set, the model's AUCs for the right coronary artery (RCA), left main coronary artery (LM), left anterior descending artery (LAD), and left circumflex artery (LCX) were 0.794, 0.818, 0.744, and 0.755, respectively; on the external validation set, the AUCs reached 0.749, 0.971, 0.667, and 0.727, respectively. Performance remained stable in a clinically normal-ECG subset, indicating robustness beyond overt ECG abnormalities. Subgroup analyses across demographic and acquisition-time strata further confirmed model stability. Risk stratification based on vessel-specific incidence thresholds showed consistent separation on calibration and cumulative event curves. Interpretability analyses revealed distinct waveform differences between high- and low-risk groups, highlighting key electrophysiological regions contributing to model decisions and offering new insights into the ECG correlates of coronary stenosis.

</details>


### [22] [ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images](https://arxiv.org/abs/2512.05137)
*Yunfei Zhang,Yizhuo He,Yuanxun Shao,Zhengtao Yao,Haoyan Xu,Junhao Dong,Zhen Yao,Zhikang Dong*

Main category: cs.CV

> 提出ChromouVQA基准测试以改进VLMs在色度伪装图像上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型在面对目标嵌入杂乱背景时仍面临挑战，特别是需要进行图形与背景的分割时。

**Method:** 引入了基于Ishihara样式的色觉伪装图像的大型多任务基准ChromouVQA。扩展了经典的点板填充几何，并变化了色度分离、密度、大小、遮挡和旋转，记录了完整的元数据以保证可重复性。

**Result:** 人类和VLMs在评估中显示出显著差距，尤其是在微妙的色度对比或破坏性几何填充下。提出了一种模型无关的对比配方，改善了整体形状的恢复。

**Conclusion:** ChromouVQA为再现性评估和扩展提供了一个紧凑且受控的基准。代码和数据集在https://github.com/Chromou-VQA-Benchmark/Chromou-VQA可用。

**Abstract:** Vision-Language Models (VLMs) have advanced multimodal understanding, yet still struggle when targets are embedded in cluttered backgrounds requiring figure-ground segregation. To address this, we introduce ChromouVQA, a large-scale, multi-task benchmark based on Ishihara-style chromatic camouflaged images. We extend classic dot plates with multiple fill geometries and vary chromatic separation, density, size, occlusion, and rotation, recording full metadata for reproducibility. The benchmark covers nine vision-question-answering tasks, including recognition, counting, comparison, and spatial reasoning. Evaluations of humans and VLMs reveal large gaps, especially under subtle chromatic contrast or disruptive geometric fills. We also propose a model-agnostic contrastive recipe aligning silhouettes with their camouflaged renderings, improving recovery of global shapes. ChromouVQA provides a compact, controlled benchmark for reproducible evaluation and extension. Code and dataset are available at https://github.com/Chromou-VQA-Benchmark/Chromou-VQA.

</details>


### [23] [Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models](https://arxiv.org/abs/2512.05139)
*Yang Xiang,Jingwen Zhong,Yige Yan,Petros Koutrakis,Eric Garshick,Meredith Franklin*

Main category: cs.CV

> 本文介绍了一个通过迁移学习和生成模型，从低分辨率输入重建高分辨率卫星图像的技术，这一技术在实验中展示了优越的性能和稳定性，适用于长时间序列的降尺度处理，对提高环境监测能力有重要意义。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机在于，通过利用迁移学习生成模型，解决从低分辨率环境数据中重建高分辨率图像的难题，以提高环境暴露评估和长期环境监测的能力。

**Method:** 本文提出了一种迁移学习生成性降尺度框架，用于从低分辨率数据重建高分辨率卫星影像。该方法结合了轻量级U-Net迁移编码器和基于扩散的生成模型。U-Net首先在长时间序列的低分辨率数据上预训练，以学习时空表示，然后将其编码器冻结并转移到更大的降尺度模型中作为物理意义明确的潜在特征。

**Result:** 实验结果表明，该模型在不同季节区域的分割下具有优异的表现（R2 = 0.65到0.94），优于基准模型，包括确定性U-Net、变分自编码器和之前的迁移学习基线。

**Conclusion:** 研究结果表明，迁移增强扩散模型为长时间序列低分辨率图像的降尺度提供了一个稳健且符合物理特性的解决方案，特别适用于训练期有限的情况。

**Abstract:** We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.

</details>


### [24] [FlowEO: Generative Unsupervised Domain Adaptation for Earth Observation](https://arxiv.org/abs/2512.05140)
*Georges Le Bellier,Nicolas Audebert*

Main category: cs.CV

> 本文提出了FlowEO框架，用于解决地球观测图像中的领域适应问题，通过流匹配技术实现了在分类和语义分割任务上的改进。

<details>
  <summary>Details</summary>

**Motivation:** 由于环境遥感数据的异质性，预训练模型的泛化能力受到领域分布偏移的严重限制。因此，无监督领域适应对于真实世界的应用变得至关重要。

**Method:** 我们引入了FlowEO框架，利用生成模型进行图像空间的无监督领域适应。通过流匹配学习一个语义保持的映射，将源领域和目标领域的图像分布连接起来。

**Result:** 实验结果表明，FlowEO在领域适应方面超越了现有的图像翻译方法，并且在感知图像质量方面达到了同样或更高的水平。

**Conclusion:** 这项研究强调了基于流匹配的无监督领域适应在遥感领域的潜力。

**Abstract:** The increasing availability of Earth observation data offers unprecedented opportunities for large-scale environmental monitoring and analysis. However, these datasets are inherently heterogeneous, stemming from diverse sensors, geographical regions, acquisition times, and atmospheric conditions. Distribution shifts between training and deployment domains severely limit the generalization of pretrained remote sensing models, making unsupervised domain adaptation (UDA) crucial for real-world applications. We introduce FlowEO, a novel framework that leverages generative models for image-space UDA in Earth observation. We leverage flow matching to learn a semantically preserving mapping that transports from the source to the target image distribution. This allows us to tackle challenging domain adaptation configurations for classification and semantic segmentation of Earth observation images. We conduct extensive experiments across four datasets covering adaptation scenarios such as SAR to optical translation and temporal and semantic shifts caused by natural disasters. Experimental results demonstrate that FlowEO outperforms existing image translation approaches for domain adaptation while achieving on-par or better perceptual image quality, highlighting the potential of flow-matching-based UDA for remote sensing.

</details>


### [25] [Self-Improving VLM Judges Without Human Annotations](https://arxiv.org/abs/2512.05145)
*Inna Wanyin Lin,Yushi Hu,Shuyue Stella Li,Scott Geng,Pang Wei Koh,Luke Zettlemoyer,Tim Althoff,Marjan Ghazvininejad*

Main category: cs.CV

> A self-training framework for a Vision-Language Model (VLM) judge without human annotations improves overall accuracy and performance across multiple dimensions compared to larger models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the drawbacks of relying on large-scale human preference annotations for training VLM judges, which can be costly and quickly become outdated as models rapidly advance.

**Method:** The method involves a self-training framework for a Vision-Language Model (VLM) judge without human preference annotations. It includes three stages: generating diverse multimodal instruction-response pairs, creating reasoning traces and judgments, and training based on the correct judge answers and reasoning traces.

**Result:** The resulting judge model shows improvements in overall accuracy on VL-RewardBench, especially in general, hallucination, and reasoning dimensions, and often outperforms larger models.

**Conclusion:** The study suggests a potential for a future self-judge that can evolve along with the rapidly improving VLM capabilities without the need for human annotations.

**Abstract:** Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.

</details>


### [26] [TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows](https://arxiv.org/abs/2512.05150)
*Zhenglin Cheng,Peng Sun,Jianguo Li,Tao Lin*

Main category: cs.CV

> 提出TwinFlow框架，用于训练一步生成模型，避免预训练教师模型和对抗网络，提高效率并减少计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 现有的少步方法存在明确的局限性，如渐进蒸馏和一致性蒸馏等方法需要迭代蒸馏程序或在非常少的步骤下表现明显退化。将对抗训练集成到蒸馏中，虽然能增强性能但会引入训练不稳定性和较高的GPU显存开销。

**Method:** TwinFlow是一种简单而有效的训练一步生成模型的框架，它不需要固定的预训练教师模型，并在训练过程中避免使用标准的对抗网络，这使其适合构建大规模高效的模型。

**Result:** 在文本到图像任务中，该方法在1-NFE时达到了0.83的GenEval分数，超过了强大的基线方法如SANA-Sprint和RCGM。此外，通过在Qwen-Image-20B上进行全参数训练，TwinFlow能被转换成一个高效的少步生成器，在GenEval和DPG-Bench基准上，仅使用1-NFE即与原有100-NFE模型的性能相当，计算成本降低100倍。

**Conclusion:** TwinFlow框架证明了其在生成模型中的高效性和可扩展性，能够在显著降低计算成本的同时保持生成质量。

**Abstract:** Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\times$ with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.

</details>


### [27] [EFDiT: Efficient Fine-grained Image Generation Using Diffusion Transformer Models](https://arxiv.org/abs/2512.05152)
*Kun Wang,Donglin Di,Tonghua Su,Lei Fan*

Main category: cs.CV

> The paper presents a new method, including a tiered embedder and super-resolution technique, to improve the semantic incorporation and detail level of fine-grained images generated by diffusion models.

<details>
  <summary>Details</summary>

**Motivation:** The aim is to address the limitations of existing class-conditional generation methods based on diffusion models, including issues of semantic entanglement and insufficient detail in fine-grained images.

**Method:** This paper proposes a tiered embedder for fine-grained image generation, combining semantic information from both super and child classes within a diffusion model. Additionally, it introduces a super-resolution mechanism to improve the details of fine-grained images during the perceptual information generation stage, and an efficient ProAttention mechanism to be implemented in the diffusion model.

**Result:** The proposed method outperforms state-of-the-art fine-tuning methods on public benchmarks with extensive experimental evaluations.

**Conclusion:** The tiered embedder along with the super-resolution and ProAttention mechanisms significantly enhances the performance of fine-grained image generation using diffusion models.

**Abstract:** Diffusion models are highly regarded for their controllability and the diversity of images they generate. However, class-conditional generation methods based on diffusion models often focus on more common categories. In large-scale fine-grained image generation, issues of semantic information entanglement and insufficient detail in the generated images still persist. This paper attempts to introduce a concept of a tiered embedder in fine-grained image generation, which integrates semantic information from both super and child classes, allowing the diffusion model to better incorporate semantic information and address the issue of semantic entanglement. To address the issue of insufficient detail in fine-grained images, we introduce the concept of super-resolution during the perceptual information generation stage, enhancing the detailed features of fine-grained images through enhancement and degradation models. Furthermore, we propose an efficient ProAttention mechanism that can be effectively implemented in the diffusion model. We evaluate our method through extensive experiments on public benchmarks, demonstrating that our approach outperforms other state-of-the-art fine-tuning methods in terms of performance.

</details>


### [28] [Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning](https://arxiv.org/abs/2512.05172)
*Wentao Wang,Chunyang Liu,Kehua Sheng,Bo Zhang,Yan Wang*

Main category: cs.CV

> 论文介绍了一种新的视觉语言模型框架Semore，通过同时提取语义和运动表征以改进视觉强化学习任务，实验表明该方法高效且适应性强。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在解决基于大型语言模型（LLM）的强化学习方法仅关注控制策略指导且面临骨干网络表征能力有限的挑战。通过引入Semore框架，作者尝试提高这些方法的有效性。

**Method:** 该论文提出了一种名为增强语义运动表征（Semore）的新框架，用于视觉强化学习。该框架通过双路径骨干网络同时从RGB流中提取语义和运动表征。该方法利用具有常识知识的视觉语言模型（VLM）从观察中检索关键信息，并使用预训练的clip实现文本-图像对齐，将真实表征嵌入骨干网络中。为了高效融合语义和运动表征以做出决策，该方法采用了单独监督的方法，能够同时引导语义和运动的提取，同时允许它们自发地进行交互。

**Result:** 实验结果表明，在特征层面受到VLM指导的情况下，该方法相较于现有先进方法展示了高效且适应性强的特点。

**Conclusion:** 实验证明了基于VLM指导的Semore方法在视觉强化学习任务中的有效性和适应性，表明它是一个有前景的研究方向。

**Abstract:** The growing exploration of Large Language Models (LLM) and Vision-Language Models (VLM) has opened avenues for enhancing the effectiveness of reinforcement learning (RL). However, existing LLM-based RL methods often focus on the guidance of control policy and encounter the challenge of limited representations of the backbone networks. To tackle this problem, we introduce Enhanced Semantic Motion Representations (Semore), a new VLM-based framework for visual RL, which can simultaneously extract semantic and motion representations through a dual-path backbone from the RGB flows. Semore utilizes VLM with common-sense knowledge to retrieve key information from observations, while using the pre-trained clip to achieve the text-image alignment, thereby embedding the ground-truth representations into the backbone. To efficiently fuse semantic and motion representations for decision-making, our method adopts a separately supervised approach to simultaneously guide the extraction of semantics and motion, while allowing them to interact spontaneously. Extensive experiments demonstrate that, under the guidance of VLM at the feature level, our method exhibits efficient and adaptive ability compared to state-of-art methods. All codes are released.

</details>


### [29] [Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models](https://arxiv.org/abs/2512.05198)
*Rowan Bradbury,Dazhi Zhong*

Main category: cs.CV

> 通过像素等效潜在合成PELC解决传统的基于变分自编码器（VAE）的图像修复产生的伪影和颜色变化问题，并通过引入DecFormer实现了精确的掩模控制和高保真的图像修复，显著提高了图像修复的质量。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于变分自编码器（VAE）的隐空间插值在图像修复中会产生边界处的大面积伪影和全局颜色变化的问题，我们希望通过PELC原则解决这些问题。

**Method:** 我们提出了一个关键原则，即像素等效潜在合成（PELC），并引入了DecFormer，一个具有770万参数的变压器，用于预测每通道混合权重和一个离流形残差修正，以实现与掩模一致的潜在融合。DecFormer通过训练使得融合后的解码匹配像素空间中的alpha合成，且与现有的扩散模型兼容，无需调整基础模型，仅增加参数量的0.07%和计算量的3.5%。

**Result:** DecFormer在FLUX.1家族中恢复了全局颜色一致性，支持软掩码，实现了尖锐的边界和高质量的掩盖，在边缘附近的错误度量减少了高达53%。

**Conclusion:** 这项研究证明了PELC原则在实现像素等效潜在编辑以及解决现有VAE插值方法带来的颜色偏移和伪影问题上的有效性，且DecFormer在网络中实现的开销极小。这不仅提升了图像修复质量，还提出了一个通用的像素等效潜在编辑解决方案。

**Abstract:** Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask. We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC). An equivalent latent compositor should be the same as compositing in pixel space. This principle enables full-resolution mask control and true soft-edge alpha compositing, even though VAEs compress images 8x spatially. Modern VAEs capture global context beyond patch-aligned local structure, so linear latent blending cannot be pixel-equivalent: it produces large artifacts at mask seams and global degradation and color shifts. We introduce DecFormer, a 7.7M-parameter transformer that predicts per-channel blend weights and an off-manifold residual correction to realize mask-consistent latent fusion. DecFormer is trained so that decoding after fusion matches pixel-space alpha compositing, is plug-compatible with existing diffusion pipelines, requires no backbone finetuning and adds only 0.07% of FLUX.1-Dev's parameters and 3.5% FLOP overhead. On the FLUX.1 family, DecFormer restores global color consistency, soft-mask support, sharp boundaries, and high-fidelity masking, reducing error metrics around edges by up to 53% over standard mask interpolation. Used as an inpainting prior, a lightweight LoRA on FLUX.1-Dev with DecFormer achieves fidelity comparable to FLUX.1-Fill, a fully finetuned inpainting model. While we focus on inpainting, PELC is a general recipe for pixel-equivalent latent editing, as we demonstrate on a complex color-correction task.

</details>


### [30] [DEAR: Dataset for Evaluating the Aesthetics of RenderingDEAR: Dataset for Evaluating the Aesthetics of Rendering](https://arxiv.org/abs/2512.05209)
*Vsevolod Plohotnuk,Artyom Panshin,Nikola Banić,Simone Bianco,Michael Freeman,Egor Ershov*

Main category: cs.CV

> 本文提出了一个新的数据集DEAR，专注于图像渲染美学的质量评估。通过大规模众包收集了超过一万多个样本，为开发专注于美学评价而非仅限于技术质量的图像模型提供了数据支持。

<details>
  <summary>Details</summary>

**Motivation:** 传统的图像质量评估主要集中于量化噪声、模糊或压缩伪影等技术退化，但针对图像渲染美学评价的数据集相对不足。DEAR数据集的引入旨在填补这一空白，特别是在基于主观人类审美喜好的图像质量评估领域。

**Method:** 本文介绍了一个新的基准数据集DEAR，该数据集旨在模拟人类对图像渲染风格的美学判断。DEAR基于MIT-Adobe FiveK数据集，并通过大规模众包收集了每对图像的25位独立人类评估者的偏好分数。这些注释捕捉了细微且上下文相关的美学偏好，支持开发和评估超越传统基于失真的图像质量评估(IQA)的新任务：图像渲染美学评估(EAR)。

**Result:** 描述了数据收集流程，分析了人类投票模式，并概述了多种应用场景，包括风格偏好预测、美学基准测试和个人化美学建模。通过这一数据集，作者为研究更深层次的美学影响和个性化偏好提供了基础。

**Conclusion:** DEAR数据集是第一个系统地解决图像美学评估问题的数据集，该评估基于人类的主观偏好，对于开发和评估新颖的美学评价模型具有重要意义。

**Abstract:** Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).

</details>


### [31] [IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction](https://arxiv.org/abs/2512.05240)
*Dmitrii Torbunov,Onur Okuducu,Yi Huang,Odera Dim,Rebecca Coles,Yonggang Cui,Yihui Ren*

Main category: cs.CV

> The paper presents a novel hybrid capture approach for reducing power consumption in video monitoring, which reconstructs standard RGB video from sparse keyframes and event streams. Two methods, based on autoregressive and diffusion models, are explored, with the diffusion model offering superior visual quality.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to reduce the power consumption in continuous video monitoring systems while maintaining the quality of video output for downstream applications by combining the benefits of RGB keyframes and event camera data.

**Method:** Our experiments investigate two architectural strategies to address the IE2Video task: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation.

**Result:** Experiments demonstrate that the diffusion-based approach outperforms the autoregressive baseline in terms of perceptual quality (0.283 vs 0.422 LPIPS) and shows robust cross-dataset generalization.

**Conclusion:** The proposed IE2Video task and diffusion-based method successfully reconstruct high-quality RGB video from sparse RGB keyframes and continuous event streams, promising reduced capture power consumption for video monitoring applications with maintained standard video output quality.

**Abstract:** Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.

</details>


### [32] [Age-Inclusive 3D Human Mesh Recovery for Action-Preserving Data Anonymization](https://arxiv.org/abs/2512.05259)
*Georgios Chatzichristodoulou,Niki Efthymiou,Panagiotis Filntisis,Georgios Pavlakos,Petros Maragos*

Main category: cs.CV

> AionHMR is a framework for accurate 3D shape and pose estimation across all ages, from infants to adults, using an optimization-based method and a specialized transformer model.

<details>
  <summary>Details</summary>

**Motivation:** The existing 3D shape and pose estimation methods generally fail to generalize to children and infants, and this paper aims to address this limitation.

**Method:** The approach involves the extension of a top-performing model with the SMPL-A body model and the development of a specialized transformer-based deep learning model.

**Result:** Extensive experiments show that the methods significantly improve shape and pose estimation for children and infants without affecting the accuracy on adults.

**Conclusion:** The paper presents a novel framework for inclusive 3D human modeling that bridges the domain gap and establishes a privacy-preserving approach for modeling human subjects across all ages.

**Abstract:** While three-dimensional (3D) shape and pose estimation is a highly researched area that has yielded significant advances, the resulting methods, despite performing well for the adult population, generally fail to generalize effectively to children and infants. This paper addresses this challenge by introducing AionHMR, a comprehensive framework designed to bridge this domain gap. We propose an optimization-based method that extends a top-performing model by incorporating the SMPL-A body model, enabling the concurrent and accurate modeling of adults, children, and infants. Leveraging this approach, we generated pseudo-ground-truth annotations for publicly available child and infant image databases. Using these new training data, we then developed and trained a specialized transformer-based deep learning model capable of real-time 3D age-inclusive human reconstruction. Extensive experiments demonstrate that our methods significantly improve shape and pose estimation for children and infants without compromising accuracy on adults. Importantly, our reconstructed meshes serve as privacy-preserving substitutes for raw images, retaining essential action, pose, and geometry information while enabling anonymized datasets release. As a demonstration, we introduce the 3D-BabyRobot dataset, a collection of action-preserving 3D reconstructions of children interacting with robots. This work bridges a crucial domain gap and establishes a foundation for inclusive, privacy-aware, and age-diverse 3D human modeling.

</details>


### [33] [CARD: Correlation Aware Restoration with Diffusion](https://arxiv.org/abs/2512.05268)
*Niki Nezakati,Arnab Ghosh,Amit Roy-Chowdhury,Vishwanath Saragadam*

Main category: cs.CV

> 本文提出了CARD方法，一种处理相关噪声的去噪扩散模型扩展，通过白化噪声并调整扩散恢复步骤来有效处理现实世界中的空间相关噪声。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有去噪扩散模型假设噪声为独立同分布的高斯噪声，而现实世界的传感器通常因为读取机制而产生空间相关的噪声，这限制了这些模型的实际效果。因此，提出了CARD方法。

**Method:** CARD方法是对DDRM模型的无训练扩展，它首先对带有噪声的观测进行白化处理，将噪声转换为独立同分布的形式，然后用噪声白化的更新步替代扩散恢复步骤，从而能够处理相关噪声。

**Result:** 实验结果显示，CARD方法在标准基准和合成相关噪声数据集，特别是新提出的CIN-D（真实滚动快门传感器噪声数据集）上，针对去噪、去模糊和超分辨率任务，均超越了现有方法。

**Conclusion:** CARD方法通过处理相关高斯噪声，改进了去噪扩散模型的实际性能，并通过新数据集CIN-D加强了相关噪声条件下图像恢复方法的评价。

**Abstract:** Denoising diffusion models have achieved state-of-the-art performance in image restoration by modeling the process as sequential denoising steps. However, most approaches assume independent and identically distributed (i.i.d.) Gaussian noise, while real-world sensors often exhibit spatially correlated noise due to readout mechanisms, limiting their practical effectiveness. We introduce Correlation Aware Restoration with Diffusion (CARD), a training-free extension of DDRM that explicitly handles correlated Gaussian noise. CARD first whitens the noisy observation, which converts the noise into an i.i.d. form. Then, the diffusion restoration steps are replaced with noise-whitened updates, which inherits DDRM's closed-form sampling efficiency while now being able to handle correlated noise. To emphasize the importance of addressing correlated noise, we contribute CIN-D, a novel correlated noise dataset captured across diverse illumination conditions to evaluate restoration methods on real rolling-shutter sensor noise. This dataset fills a critical gap in the literature for experimental evaluation with real-world correlated noise. Experiments on standard benchmarks with synthetic correlated noise and on CIN-D demonstrate that CARD consistently outperforms existing methods across denoising, deblurring, and super-resolution tasks.

</details>


### [34] [Inferring Compositional 4D Scenes without Ever Seeing One](https://arxiv.org/abs/2512.05272)
*Ahmet Berke Gokmen,Ajad Chhatkuli,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

> COM4D方法可以从2D视频中重建多对象互动的4D场景，不需要依赖额外的4D组合训练数据，实现了4D和3D对象的结构和时空配置的预测。

<details>
  <summary>Details</summary>

**Motivation:** 由于真实世界中的场景通常由多个静态和动态对象组成，虽然捕捉它们的4D结构及其空间时间配置非常有趣，但这也非常困难。因此，现有研究往往一次只专注于一个目标，依靠特定类的参数化形状模型来处理动态对象。这会导致场景配置上的不一致，并且仅限于所建模的目标类别。为了克服这些问题，提出了COM4D方法。

**Method:** COM4D (Compositional 4D) 方法，能够从静态多对象或动态单对象的监督中，一致且联合地预测4D/3D对象的结构和时间空间配置。通过仔细设计2D视频输入下的空间和时间注意力的训练来实现。将学习分为对象组合和视频中单个对象的动态学习，完全避免依赖4D组合训练数据。在推理阶段，提出的注意力混合机制独立学习的注意力，无需任何4D组合示例。通过交替的空间和时间推理，COM4D直接从单眼视频中重建多对象互动的完整和持续的4D场景。

**Result:** COM4D提供了现有分离问题中的4D物体和组合3D重建的最先进结果，尽管它是纯粹数据驱动的。

**Conclusion:** COM4D方法能够重建出多对象互动的4D场景，且不依赖额外的4D组合训练数据，展示了其在复杂场景建模中的潜力和有效性。

**Abstract:** Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.

</details>


### [35] [From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model](https://arxiv.org/abs/2512.05277)
*Kevin Cannons,Saeed Ranjbar Alvar,Mohammad Asiful Hossain,Ahmad Rezaei,Mohsen Gholami,Alireza Heidarikhazaei,Zhou Weimin,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

> 该研究提出了TAD基准，专注于自动驾驶中时间理解的挑战，并提出了两种无训练增强方法，显著提高了VLMs在TAD上的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的Vision-Language Models在自动驾驶的时空理解方面存在不足，之前的工作专注于其他类型的视频内容。为解决自动驾驶特有的挑战，研究者提出了TAD基准。

**Method:** TAD基准包括近6000个问答对，涵盖了7种人类设计的任务。此外，还评估了9种公开源代码和私有源代码的通用模型以及自动驾驶领域的顶级模型。

**Result:** 现有顶级模型在TAD上的准确性较低，主要由于对细微动作理解不足。研究提出的方法通过结合思维链和自我中心的时间认知图显著提升了模型性能。

**Conclusion:** 该工作通过引入TAD基准，评估多种顶级模型，并提出有效的增强方法，促进了自动驾驶中时序理解的未来研究。

**Abstract:** Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \href{https://github.com/vbdi/tad_bench}{Github}, respectively.

</details>


### [36] [SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling](https://arxiv.org/abs/2512.05343)
*Elisabetta Fedele,Francis Engelmann,Ian Huang,Or Litany,Marc Pollefeys,Leonidas Guibas*

Main category: cs.CV

> 提出SpaceControl，一种无训练的3D生成空间控制方法，它可以接受多种几何输入，与预训练生成模型无缝集成，控制几何保真度和逼真度的平衡，展示了优于对比方法的几何忠实度和高视觉质量，提供了交互式用户界面以实现纹理3D资产的在线编辑。

<details>
  <summary>Details</summary>

**Motivation:** 尽管3D资产生成方法已经取得了显著进展，但提供对物体几何形状的直观和精确控制仍然是一个关键挑战。目前大多数方法依赖于文本或图像提示，但这些提示在几何具体性方面往往不足：语言很容易产生歧义，而图像则难以编辑。

**Method:** 介绍了一种称为SpaceControl的无训练测试时方法，该方法可以在3D生成中进行显式的空间控制。它接受从粗略的基元到详细的网格等多种几何输入，并能无缝地与现代的预训练生成模型集成，无需额外的训练。其中一个可控参数可以让用户在几何保真度和输出逼真度之间进行权衡。

**Result:** 广泛的定量评估和用户研究表明，SpaceControl在几何忠实度方面优于基于训练和基于优化的基线方法，同时保持了高质量的视觉输出。

**Conclusion:** 该研究提出了一种新的方法，可以有效地解决3D生成中几何控制的问题，并提供了一个交互式用户界面，以便在线编辑超四维图形并直接转换为纹理3D资产，从而在创意工作流程中实现实用部署。

**Abstract:** Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/

</details>


### [37] [SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training](https://arxiv.org/abs/2512.05354)
*Yang Zheng,Hao Tan,Kai Zhang,Peng Wang,Leonidas Guibas,Gordon Wetzstein,Wang Yifan*

Main category: cs.CV

> 新论文介绍了一种新颖的状态感知前馈模型，允许从用户提供的2D视图中连续编辑3D高斯资产，实现了互动速度下的局部和全局编辑能力，克服了现有技术的限制。

<details>
  <summary>Details</summary>

**Motivation:** 根据文中所述，3D高斯点阵化技术已经革新了3D资产的写实创建，但目前尚缺少一种适合互动修改和编辑的方法。当前基于扩散或优化的方法由于速度慢、破坏原始资产的特性或缺乏精细控制精度而不适用。

**Method:** 我\\ourmethod,是一个状态感知的前馈模型，它从用户提供的2D视图直接预测3D高斯资产属性的更新，实现了3D高斯资产的连续编辑。这一方法利用了测试时训练来创建状态感知的迭代工作流程。

**Result:** 方法允许多种任务的执行，包括高保真细节精修、局部绘制和一致的全局重新着色，所有这些都能在互动速度下完成。

**Conclusion:** 正文介绍的方法为流畅、直观的3D内容创作开辟了新的路径。

**Abstract:** The rise of 3D Gaussian Splatting has revolutionized photorealistic 3D asset creation, yet a critical gap remains for their interactive refinement and editing. Existing approaches based on diffusion or optimization are ill-suited for this task, as they are often prohibitively slow, destructive to the original asset's identity, or lack the precision for fine-grained control. To address this, we introduce \ourmethod, a state-aware feedforward model that enables continuous editing of 3D Gaussian assets from user-provided 2D view(s). Our method directly predicts updates to the attributes of a compact, feature-rich Gaussian representation and leverages Test-Time Training to create a state-aware, iterative workflow. The versatility of our approach allows a single architecture to perform diverse tasks, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring, all at interactive speeds, paving the way for fluid and intuitive 3D content authoring.

</details>
