{"id": "2506.14900", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14900", "abs": "https://arxiv.org/abs/2506.14900", "authors": ["Imane Guellil", "Salomé Andres", "Atul Anand", "Bruce Guthrie", "Huayu Zhang", "Abul Hasan", "Honghan Wu", "Beatrice Alex"], "title": "Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings", "comment": "Accepted and will be published at ACL2025 (main conference)", "summary": "In this work, we present a manually annotated corpus for Adverse Event (AE)\nextraction from discharge summaries of elderly patients, a population often\nunderrepresented in clinical NLP resources. The dataset includes 14 clinically\nsignificant AEs-such as falls, delirium, and intracranial haemorrhage, along\nwith contextual attributes like negation, diagnosis type, and in-hospital\noccurrence. Uniquely, the annotation schema supports both discontinuous and\noverlapping entities, addressing challenges rarely tackled in prior work. We\nevaluate multiple models using FlairNLP across three annotation granularities:\nfine-grained, coarse-grained, and coarse-grained with negation. While\ntransformer-based models (e.g., BERT-cased) achieve strong performance on\ndocument-level coarse-grained extraction (F1 = 0.943), performance drops\nnotably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly\nfor rare events and complex attributes. These results demonstrate that despite\nhigh-level scores, significant challenges remain in detecting underrepresented\nAEs and capturing nuanced clinical language. Developed within a Trusted\nResearch Environment (TRE), the dataset is available upon request via DataLoch\nand serves as a robust benchmark for evaluating AE extraction methods and\nsupporting future cross-dataset generalisation.", "AI": {"tldr": "研究创建了适用于老年人不良事件（AE）提取的手动标注数据集，并使用多个模型进行了评估。研究结果表明，尽管模型在粗粒度提取上表现良好，但在细粒度实体级别上的表现还有待提高。", "motivation": "本研究的动机是开发一个专门针对老年患者在出院总结中的不良事件进行标注的数据集，因为老年人群体在临床自然语言处理资源中经常被忽视。", "method": "本论文介绍了为老年人出院总结中不良事件（AE）提取手动标注的数据集。该数据集包含14种临床意义上的不良事件，并带有上下文属性如否定、诊断类型和住院期间发生情况。注释模式独特地支持不连续和重叠实体，解决了现有研究中很少触及的挑战。", "result": "研究使用FlairNLP对多个模型进行评估，发现在三个注释颗粒度级别上的表现有所差异。特别提及变压器模型（如BERT-cased）在粗粒度提取任务上表现出色，但在细粒度和复杂属性上的精细实体级别任务表现较差。", "conclusion": "研究结果显示，尽管基于文档级别的粗粒度提取任务上取得了F1分数为0.943的良好表现，但在细粒度实体级别任务上的表现则显著下降，提示现有模型在检测未充分表示的不良事件和捕捉细微的临床语言方面还存在挑战。"}}
{"id": "2506.14901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14901", "abs": "https://arxiv.org/abs/2506.14901", "authors": ["Marija Šakota", "Robert West"], "title": "Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction", "comment": null, "summary": "Many recent approaches to structured NLP tasks use an autoregressive language\nmodel $M$ to map unstructured input text $x$ to output text $y$ representing\nstructured objects (such as tuples, lists, trees, code, etc.), where the\ndesired output structure is enforced via constrained decoding. During training,\nthese approaches do not require the model to be aware of the constraints, which\nare merely implicit in the training outputs $y$. This is advantageous as it\nallows for dynamic constraints without requiring retraining, but can lead to\nlow-quality output during constrained decoding at test time. We overcome this\nproblem with Boosted Constrained Decoding (BoostCD), which combines constrained\nand unconstrained decoding in two phases: Phase 1 decodes from the base model\n$M$ twice, in constrained and unconstrained mode, obtaining two weak\npredictions. In phase 2, a learned autoregressive boosted model combines the\ntwo weak predictions into one final prediction. The mistakes made by the base\nmodel with vs. without constraints tend to be complementary, which the boosted\nmodel learns to exploit for improved performance. We demonstrate the power of\nBoostCD by applying it to closed information extraction. Our model, BoostIE,\noutperforms prior approaches both in and out of distribution, addressing\nseveral common errors identified in those approaches.", "AI": {"tldr": "BoostCD combines constrained and unconstrained decoding to improve the performance of structured NLP tasks like closed information extraction without needing retraining for new constraints.", "motivation": "To address the issue of low-quality outputs during constrained decoding and to improve performance in structured NLP tasks such as closed information extraction without the need for retraining due to dynamic constraints.", "method": "Boosted Constrained Decoding (BoostCD) is used to combine constrained and unconstrained decoding in two phases to improve the output quality for structured NLP tasks. Phase 1 obtains two weak predictions. Phase 2 uses a learned boosted model to combine these predictions.", "result": "The model, BoostIE, outperforms previous approaches both in and out-of-distribution for closed information extraction, addressing common errors found in those approaches.", "conclusion": "BoostCD effectively enhances the output quality and addresses common errors found in previous methods by leveraging the complementary weaknesses in constrained and unconstrained model predictions."}}
{"id": "2506.14912", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14912", "abs": "https://arxiv.org/abs/2506.14912", "authors": ["Dyah Adila", "Shuai Zhang", "Boran Han", "Bonan Min", "Yuyang Wang"], "title": "CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision", "comment": null, "summary": "The integration of contextual information has significantly enhanced the\nperformance of large language models (LLMs) on knowledge-intensive tasks.\nHowever, existing methods often overlook a critical challenge: the credibility\nof context documents can vary widely, potentially leading to the propagation of\nunreliable information. In this paper, we introduce CrEst, a novel weakly\nsupervised framework for assessing the credibility of context documents during\nLLM inference--without requiring manual annotations. Our approach is grounded\nin the insight that credible documents tend to exhibit higher semantic\ncoherence with other credible documents, enabling automated credibility\nestimation through inter-document agreement. To incorporate credibility into\nLLM inference, we propose two integration strategies: a black-box approach for\nmodels without access to internal weights or activations, and a white-box\nmethod that directly modifies attention mechanisms. Extensive experiments\nacross three model architectures and five datasets demonstrate that CrEst\nconsistently outperforms strong baselines, achieving up to a 26.86% improvement\nin accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst\nmaintains robust performance even under high-noise conditions.", "AI": {"tldr": "我们提出了CrEst框架，用于评估大型语言模型（LLM）推理过程中上下文文件的可信度，通过文档间的语义一致性进行自动化评估，并提出了两种集成策略，明显优于基线模型。", "motivation": "现有的方法往往忽视了一个关键挑战：上下文文件的可信度差异可能很大，这可能导致不可靠信息的传播。我们的研究旨在提出一种解决方案，以解决这个问题。", "method": "我们提出了CrEst，这是一种新颖的弱监督框架，用于在大型语言模型（LLM）推理过程中评估上下文文档的可信度，且不需要手动标注。该方法基于这样的见解，即可信的文件通常与其他可信文件在语义上高度一致，从而可以通过文档间的一致性来自动评估可信度。为了将可信度纳入LLM的推理过程中，我们提出了两种集成策略：一种是黑盒方法，适用于无法访问内部权重或激活值的模型；另一种是白盒方法，直接修改注意力机制。", "result": "实验显示，CrEst在三种模型架构和五个数据集上均优于强基线模型，准确率最高提升了26.86%，F1分数提高了3.49%。进一步的分析表明，即使在高噪声条件下，CrEst也能保持稳健的性能。", "conclusion": "CrEst框架证明了在大型语言模型的推理过程中评估和利用上下文文档的可信度是可行的，并且能显著提高模型在知识密集型任务上的表现。"}}
{"id": "2506.14927", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14927", "abs": "https://arxiv.org/abs/2506.14927", "authors": ["Joseph J. Peper", "Wenzhao Qiu", "Ali Payani", "Lu Wang"], "title": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance", "comment": "ACL 2025 Findings", "summary": "Natural language processing evaluation has made significant progress, largely\ndriven by the proliferation of powerful large language mod-els (LLMs). New\nevaluation benchmarks are of increasing priority as the reasoning capabilities\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\nreasoning is an area of extreme relevance given LLM capabilities in handling\nlonger-context inputs, few benchmarks exist to rigorously examine model\nbehavior in this setting. Moreover, the multi-document setting is historically\nchallenging for benchmark creation due to the expensive cost of annotating long\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\non the task of multi-document reasoning. Notably, MDBench is created through a\nnovel synthetic generation process, allowing us to controllably and efficiently\ngenerate challenging document sets and the corresponding question-answer (QA)\nexamples. Our novel technique operates on condensed structured seed knowledge,\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\nchallenges. We then convert this structured knowledge into a natural text\nsurface form, generating a document set and corresponding QA example. We\nanalyze the behavior of popular LLMs and prompting techniques, finding that\nMDBENCH poses significant challenges for all methods, even with relatively\nshort document sets. We also see our knowledge-guided generation technique (1)\nallows us to readily perform targeted analysis of MD-specific reasoning\ncapabilities and (2) can be adapted quickly to account for new challenges and\nfuture modeling improvements.", "AI": {"tldr": "MDBench, a novel synthetic dataset for evaluating LLMs in multi-document reasoning, is introduced, showing significant challenges for all methods and the effectiveness of a knowledge-guided generation technique.", "motivation": "The expansion of LLMs' reasoning capabilities has made new evaluation benchmarks increasingly important, especially for multi-document reasoning, which is both highly relevant and historically challenging to benchmark due to the high cost of annotating long inputs.", "method": "MDBench, a new dataset for evaluating LLMs on multi-document reasoning, is created through a novel synthetic generation process, using condensed structured seed knowledge modified through LLM-assisted edits and then converted into natural text.", "result": "The analysis shows that MDBENCH poses significant challenges for all LLM methods, even for relatively short document sets, and the knowledge-guided generation technique allows for targeted analysis of MD-specific reasoning capabilities and quick adaption to new challenges.", "conclusion": "MDBench addresses the need for rigorous evaluation of LLMs in multi-document reasoning tasks and demonstrates the potential of synthetic generation methods for creating complex and purposeful benchmark datasets."}}
{"id": "2506.14791", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14791", "abs": "https://arxiv.org/abs/2506.14791", "authors": ["Jingxuan Zhou", "Yuehao Wu", "Yibo Zhang", "Yeyubei Zhang", "Yunchong Liu", "Bolin Huang", "Chunhong Yuan"], "title": "SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection", "comment": "5 pages, 3 figures", "summary": "Aiming at the problem of difficulty in accurately identifying graphical\nimplicit correlations in multimodal irony detection tasks, this paper proposes\na Semantic Irony Recognition Network (SemIRNet). The model contains three main\ninnovations: (1) The ConceptNet knowledge base is introduced for the first time\nto acquire conceptual knowledge, which enhances the model's common-sense\nreasoning ability; (2) Two cross-modal semantic similarity detection modules at\nthe word level and sample level are designed to model graphic-textual\ncorrelations at different granularities; and (3) A contrastive learning loss\nfunction is introduced to optimize the spatial distribution of the sample\nfeatures, which improves the separability of positive and negative samples.\nExperiments on a publicly available multimodal irony detection benchmark\ndataset show that the accuracy and F1 value of this model are improved by 1.64%\nand 2.88% to 88.87% and 86.33%, respectively, compared with the existing\noptimal methods. Further ablation experiments verify the important role of\nknowledge fusion and semantic similarity detection in improving the model\nperformance.", "AI": {"tldr": "This paper proposes the Semantic Irony Recognition Network (SemIRNet) to improve the accuracy in multimodal irony detection by integrating ConceptNet knowledge and cross-modal semantic similarity detection, enhancing the performance through a contrastive learning loss function.", "motivation": "To address the challenge of accurately identifying graphical implicit correlations in multimodal irony detection tasks, a novel network SemIRNet is proposed.", "method": "The paper introduces a Semantic Irony Recognition Network (SemIRNet) with three key innovations: (1) Utilization of the ConceptNet knowledge base to boost the common-sense reasoning capability of the model; (2) Implementation of cross-modal semantic similarity detection modules at the word and sample levels for better understanding of graphic-textual correlations; and (3) Integration of a contrastive learning loss function to enhance the separability of positive and negative samples.", "result": "Experiments on a publicly available multimodal irony detection dataset demonstrate an improvement of 1.64% and 2.88% in accuracy and F1 value to 88.87% and 86.33%, respectively, over existing optimal methods. Ablation studies confirm the significance of knowledge fusion and semantic similarity detection in performance enhancement.", "conclusion": "The Semantic Irony Recognition Network (SemIRNet) proves to be effective in identifying graphical implicit correlations in multimodal irony detection tasks, showcasing improvements in accuracy and F1 score, with ablation experiments validating the benefits of knowledge fusion and semantic similarity detection."}}
{"id": "2506.14949", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.14949", "abs": "https://arxiv.org/abs/2506.14949", "authors": ["Shadman Sakib", "Oishy Fatema Akhand", "Ajwad Abrar"], "title": "From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?", "comment": "Accepted in 1st IEEE QPAIN 2025", "summary": "While Machine Learning (ML) and Deep Learning (DL) models have been widely\nused for diabetes prediction, the use of Large Language Models (LLMs) for\nstructured numerical data is still not well explored. In this study, we test\nthe effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and\nthree-shot prompting methods. We conduct an empirical analysis using the Pima\nIndian Diabetes Database (PIDD). We evaluate six LLMs, including four\nopen-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We\nalso test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we\ncompare their performance with three traditional machine learning models:\nRandom Forest, Logistic Regression, and Support Vector Machine (SVM). We use\naccuracy, precision, recall, and F1-score as evaluation metrics. Our results\nshow that proprietary LLMs perform better than open-source ones, with GPT-4o\nand Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,\nGemma-2-27B also outperforms the traditional ML models in terms of F1-score.\nHowever, there are still issues such as performance variation across prompting\nstrategies and the need for domain-specific fine-tuning. This study shows that\nLLMs can be useful for medical prediction tasks and encourages future work on\nprompt engineering and hybrid approaches to improve healthcare predictions.", "AI": {"tldr": "本研究通过使用多种提示方法对六个语言模型进行糖尿病预测测试，相较于传统机器学习模型，专有和大型语言模型表现更佳，尤其在少量样本情况下。", "motivation": "尽管机器学习和深度学习模型在糖尿病预测中被广泛应用，但大型语言模型（LLMs）在结构化数值数据上的应用尚未被深入探索。因此，本研究旨在通过实证分析探索LLMs在糖尿病预测任务中的有效性。", "method": "本研究测试了大型语言模型（LLMs）在使用零样本、单样本和三样本提示方法预测糖尿病方面的有效性。研究使用了Pima印度糖尿病数据库（PIDD），并对六个LLMs模型进行了评估，包括四个开源模型：Gemma-2-27B，Mistral-7B，Llama-3.1-8B，Llama-3.2-2B，以及两个专有模型：GPT-4o和Gemini Flash 2.0。同时，将其与三种传统机器学习模型（随机森林，逻辑回归和支持向量机(SVM)）的表现进行了比较。实验结果用准确性，精确度，召回率和F1分数作为评估指标。", "result": "专有LLMs模型的表现优于开源模型，GPT-4o和Gemma-2-27B在少量样本情况下达到了最高的准确性。值得注意的是，Gemma-2-27B的F1分数也超过了传统的ML模型。不过，研究还指出存在提示策略间性能波动，以及需要领域特定微调的问题。", "conclusion": "该研究表明，LLMs可以在医疗预测任务中发挥作用，并鼓励对未来工作在提示工程和混合方法上的研究以改善医疗预测。"}}
{"id": "2506.14805", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.14805", "abs": "https://arxiv.org/abs/2506.14805", "authors": ["Yang Yao", "Lingyu Li", "Jiaxin Song", "Chiyu Chen", "Zhenqi He", "Yixu Wang", "Xin Wang", "Tianle Gu", "Jie Li", "Yan Teng", "Yingchun Wang"], "title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?", "comment": null, "summary": "As Multimodal Large Language Models (MLLMs) continue to evolve, their\ncognitive and reasoning capabilities have seen remarkable progress. However,\nchallenges in visual fine-grained perception and commonsense causal inference\npersist. This paper introduces Argus Inspection, a multimodal benchmark with\ntwo levels of difficulty, emphasizing detailed visual recognition while\nincorporating real-world commonsense understanding to evaluate causal reasoning\nabilities. Expanding on it, we present the Eye of Panoptes framework, which\nintegrates a binary parametric Sigmoid metric with an indicator function,\nenabling a more holistic evaluation of MLLMs' responses in opinion-based\nreasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the\nhighest performance in visual fine-grained reasoning reaches only 0.46,\nhighlighting considerable potential for enhancement. Our research offers\nvaluable perspectives for the continued refinement of MLLMs.", "AI": {"tldr": "论文介绍了Argus Inspection基准测试和Eye of Panoptes框架，以评估多模态大语言模型在视觉细粒度感知和常识因果推理方面的能力，发现这些模型的性能有限，指出未来研究的方向和潜力。", "motivation": "多模态大语言模型（MLLMs）在认知和推理能力上取得了显著进步，但仍面临视觉细粒度感知和常识因果推理的挑战。", "method": "提出Argus Inspection多模态基准测试，分为两个难度级别，强调详细视觉识别，同时纳入现实世界的常识理解来评估因果推理能力。并介绍了Panoptes之眼框架，该框架整合了一个二元参数逻辑斯蒂度量和一个指示函数，以更全面地评估MLLMs在基于观点的推理任务中的表现。", "result": "", "conclusion": "实验结果显示，26个主流MLLMs中，视觉细粒度推理的最佳性能仅为0.46，表明有很大的改进空间。研究提供了对MLLMs进一步精进的宝贵见解。"}}
{"id": "2506.15001", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15001", "abs": "https://arxiv.org/abs/2506.15001", "authors": ["Ignacio Sastre", "Aiala Rosá"], "title": "Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings", "comment": "This paper will be presented at The First Workshop on Large Language\n  Model Memorization (L2M2) at ACL 2025", "summary": "In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.", "AI": {"tldr": "研究发现可以生成一种特殊的嵌入，能让大语言模型精确重建原始文本，而无需修改模型权重，这一现象在不同语言、序列和模型尺寸上进行了验证。", "motivation": "观察到可以生成可逆的句子嵌入，使得大语言模型（LLM）在不改变模型权重的情况下精确重建原始文本的现象，以此揭示LLM的能力并探索其潜在应用。", "method": "通过引入特殊的记忆令牌，该令牌的嵌入是在固定序列上训练优化得到的。当模型被提示这个嵌入时，可以准确地重建固定序列。", "result": "实验表明，对于英语和西班牙语数据集、最多约240个令牌的序列以及从1亿到80亿参数的模型规模范围内，Llama 3.1 8B能够成功重建所有测试的序列。", "conclusion": "这一发现揭示了LLM的有趣能力，并暗示了在基于记忆的检索、压缩和受控文本生成方面潜在的应用。"}}
{"id": "2506.14816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14816", "abs": "https://arxiv.org/abs/2506.14816", "authors": ["Alavikunhu Panthakkan", "Zubair Medammal", "S M Anzar", "Fatma Taher", "Hussain Al-Ahmad"], "title": "A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease Detection", "comment": null, "summary": "Falconry, a revered tradition involving the training and hunting with\nfalcons, requires meticulous health surveillance to ensure the health and\nsafety of these prized birds, particularly in hunting scenarios. This paper\npresents an innovative method employing a hybrid of ConvNeXt and EfficientNet\nAI models for the classification of falcon diseases. The study focuses on\naccurately identifying three conditions: Normal, Liver Disease and\n'Aspergillosis'. A substantial dataset was utilized for training and validating\nthe model, with an emphasis on key performance metrics such as accuracy,\nprecision, recall, and F1-score. Extensive testing and analysis have shown that\nour concatenated AI model outperforms traditional diagnostic methods and\nindividual model architectures. The successful implementation of this hybrid AI\nmodel marks a significant step forward in precise falcon disease detection and\npaves the way for future developments in AI-powered avian healthcare solutions.", "AI": {"tldr": "本文提出了一种结合ConvNeXt和EfficientNet的AI模型，用于猎鹰疾病的精准分类与诊断，展示了该模型在疾病检测效能上的优越性。", "motivation": "在猎鹰训练和狩猎中，对猎鹰进行严格的健康管理是必要的。本文旨在提出一种AI模型来提升猎鹰疾病的诊断精度，保障猎鹰健康。", "method": "研究采用了ConvNeXt和EfficientNet两种AI模型的结合，形成一个混合AI模型，用于猎鹰疾病的分类。", "result": "本次研究提出了一种结合ConvNeXt和EfficientNet的AI模型，用于准确分类猎鹰的疾病状况，包括正常状态、肝病和曲霉菌病。通过大规模数据集的训练和验证，研究展示了该模型在准确率、精确度、召回率和F1分数上的出色表现，优于传统诊断方法及单一模型结构。这代表了在猎鹰疾病检测中实现精准医疗的重要进展，并为未来基于AI的鸟类健康解决方案铺平了道路。", "conclusion": "这项研究通过混合AI模型大幅提升了猎鹰疾病的诊断精度，为猎鹰及其他鸟类的健康监测提供了有效的技术支持，对于未来进一步发展AI在动物健康领域的应用具有重要意义。"}}
{"id": "2506.15030", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15030", "abs": "https://arxiv.org/abs/2506.15030", "authors": ["Drew Walker", "Swati Rajwal", "Sudeshna Das", "Snigdha Peddireddy", "Abeed Sarker"], "title": "Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods", "comment": "22 pages, 2 figures, 5 tables", "summary": "Social isolation and loneliness, which have been increasing in recent years\nstrongly contribute toward suicide rates. Although social isolation and\nloneliness are not currently recorded within the US National Violent Death\nReporting System's (NVDRS) structured variables, natural language processing\n(NLP) techniques can be used to identify these constructs in law enforcement\nand coroner medical examiner narratives. Using topic modeling to generate\nlexicon development and supervised learning classifiers, we developed\nhigh-quality classifiers (average F1: .86, accuracy: .82). Evaluating over\n300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic\nsocial isolation. Decedents had higher odds of chronic social isolation\nclassification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =\n3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).\nWe found significant predictors for other social isolation topics of recent or\nimpending divorce, child custody loss, eviction or recent move, and break-up.\nOur methods can improve surveillance and prevention of social isolation and\nloneliness in the United States.", "AI": {"tldr": "Identified social isolation and loneliness in suicide cases using NLP techniques, with significant predictors including gender, sexual orientation, and marital status, enhancing surveillance and prevention efforts.", "motivation": "To address the increasing issue of social isolation and loneliness, and their strong contribution to suicide rates, despite their lack of recording in the US NVDRS structured variables.", "method": "Using topic modeling for lexicon development and supervised learning classifiers to identify social isolation and loneliness in law enforcement and coroner medical examiner narratives within the NVDRS.", "result": "Developed high-quality classifiers with an average F1 score of .86 and accuracy of .82. Identified 1,198 cases mentioning chronic social isolation, finding higher odds of classification among men, gay individuals, and those who were divorced.", "conclusion": "The developed methods can potentially improve the surveillance and prevention efforts against social isolation and loneliness in the United States."}}
{"id": "2506.14823", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14823", "abs": "https://arxiv.org/abs/2506.14823", "authors": ["Harsha Koduri"], "title": "ViLLa: A Neuro-Symbolic approach for Animal Monitoring", "comment": null, "summary": "Monitoring animal populations in natural environments requires systems that\ncan interpret both visual data and human language queries. This work introduces\nViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for\ninterpretable animal monitoring. ViLLa integrates three core components: a\nvisual detection module for identifying animals and their spatial locations in\nimages, a language parser for understanding natural language queries, and a\nsymbolic reasoning layer that applies logic-based inference to answer those\nqueries. Given an image and a question such as \"How many dogs are in the\nscene?\" or \"Where is the buffalo?\", the system grounds visual detections into\nsymbolic facts and uses predefined rules to compute accurate answers related to\ncount, presence, and location. Unlike end-to-end black-box models, ViLLa\nseparates perception, understanding, and reasoning, offering modularity and\ntransparency. The system was evaluated on a range of animal imagery tasks and\ndemonstrates the ability to bridge visual content with structured,\nhuman-interpretable queries.", "AI": {"tldr": "ViLLa是一个用于动物监测的神经符号框架，能够理解和回答有关动物数量、存在以及位置的自然语言查询。", "motivation": "动物种群监测需要系统能够解读视觉数据和人类语言查询，ViLLa旨在提供一个模块化且透明的解决方案，以解决现有黑盒模型的不可解释性问题。", "method": "本论文提出了ViLLa，一个基于视觉、语言和逻辑的神经符号框架，用于可解释的动物监测。它整合了视觉检测模块、语言解析器和符号推理层三个核心组件，以理解和回答关于动物的自然语言查询。", "result": "该系统在一系列动物图像任务中进行了评估，展示了将视觉内容与结构化的、人类可解读的查询相连接的能力。", "conclusion": "研究表明ViLLa能够有效地处理视觉内容的理解和自然语言查询的回应，为动物监测提供了一种模块化和透明的方法。"}}
{"id": "2506.15068", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15068", "abs": "https://arxiv.org/abs/2506.15068", "authors": ["Zongxia Li", "Yapei Chang", "Yuhang Zhou", "Xiyang Wu", "Zichao Liang", "Yoo Yeon Sung", "Jordan Lee Boyd-Graber"], "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation", "comment": null, "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.", "AI": {"tldr": "引入PrefBERT评估开放性长文本生成，优于传统指标，并在多个评估标准中验证了其可靠性和与人类偏好的一致性。", "motivation": "现有的评估方法经常忽略连贯性、风格或相关性等关键要素，或受到预训练数据的偏见影响，这使得开放性长文本评估成为一个未充分探索的问题。", "method": "我们提出PrefBERT，一种用于评估开放性长文本生成的评分模型，它通过在GRPO中使用不同的奖励来区分好与差的输出，从而指导训练。", "result": "通过包括语言模型作为裁判、人工评分和定性分析在内的全面评估，我们证明PrefBERT在多句子和段落长度的响应训练中保持了可靠性，并且与GRPO所需的可验证奖励有很好的一致性。人工评估确认使用PrefBERT作为奖励信号来训练策略模型生成的响应更符合人类偏好。", "conclusion": "PrefBERT在评估开放性长文本生成方面提供了比传统指标ROUGE-L和BERTScore更好的语义反馈支持，其代码在https://github.com/zli12321/long_form_rl可获得。"}}
{"id": "2506.14825", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14825", "abs": "https://arxiv.org/abs/2506.14825", "authors": ["Ke Song", "Yunhe Wu", "Chunchit Siu", "Huiyuan Xiong"], "title": "GraphGSOcc: Semantic and Geometric Graph Transformer for 3D Gaussian Splating-based Occupancy Prediction", "comment": null, "summary": "Addressing the task of 3D semantic occupancy prediction for autonomous\ndriving, we tackle two key issues in existing 3D Gaussian Splating (3DGS)\nmethods: (1) unified feature aggregation neglecting semantic correlations among\nsimilar categories and across regions, and (2) boundary ambiguities caused by\nthe lack of geometric constraints in MLP iterative optimization. We propose the\nGraphGSOcc model, a novel framework that combines semantic and geometric graph\nTransformer for 3D Gaussian Splating-based Occupancy Prediction. We propose the\nDual Gaussians Graph Attenntion, which dynamically constructs dual graph\nstructures: a geometric graph adaptively calculating KNN search radii based on\nGaussian poses, enabling large-scale Gaussians to aggregate features from\nbroader neighborhoods while compact Gaussians focus on local geometric\nconsistency; a semantic graph retaining top-M highly correlated nodes via\ncosine similarity to explicitly encode semantic relationships within and across\ninstances. Coupled with the Multi-scale Graph Attention framework, fine-grained\nattention at lower layers optimizes boundary details, while coarse-grained\nattention at higher layers models object-level topology. Experiments on the\nSurroundOcc dataset achieve an mIoU of 24.10%, reducing GPU memory to 6.1 GB,\ndemonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to\nGaussianWorld", "AI": {"tldr": "研究开发了GraphGSOcc模型解决3D高斯渲染的占用预测问题，提升了性能和降低了内存使用。", "motivation": "为了解决现有的3D高斯渲染方法中的两个关键问题：统一特征聚集不考虑语义相关性，以及由于缺乏几何约束导致边界模糊，影响了自动驾驶中的3D语义占用预测。", "method": "我们提出了GraphGSOcc模型，这是一种结合语义和几何图Transformer的3D高斯渲染占用预测新框架。通过Dual Gaussians Graph Attention，动态构建了两种图结构：一种几何图表基于高斯姿态自适应地计算k近邻搜索半径，实现了大尺度的高斯特征聚集；另一种语义图表通过余弦相似性保留最相关的节点，以编码实例内的语义关系。结合多尺度图注意力框架，低层进行细粒度注意力优化边界细节，高层进行粗粒度注意力建模对象拓扑。", "result": "在SurroundOcc数据集的实验实现了24.10%的mIoU，减少了GPU内存至6.1GB，相比GaussianWorld提升了1.97%的mIoU和13.7%的内存使用。", "conclusion": "GraphGSOcc模型通过结合几何和语义图变换器，有效提升了3D高斯渲染的占用预测准确性和效率，减少了内存使用，展示了在自动驾驶应用中的潜力。"}}
{"id": "2506.15076", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15076", "abs": "https://arxiv.org/abs/2506.15076", "authors": ["Ruihan Wu", "Konstantin Garov", "Kamalika Chaudhuri"], "title": "Learning-Time Encoding Shapes Unlearning in LLMs", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.", "AI": {"tldr": "研究发现，利用改写的短语描述提升删除效果并且从一段文本中单独删除信息具有挑战性，这暗示了在实现可靠的后置删除中，学习时的知识编码方式发挥着关键作用。", "motivation": "随着大规模语言模型在现实世界中的应用越来越广泛，删除特定知识片段的能力变得非常重要，其原因涵盖隐私规定到纠正过时或有害内容等。", "method": "该研究通过实证调查学习时知识编码的选择是如何影响事实性知识的“删除”效果的。", "result": "该研究的摘要主要探讨了在大规模语言模型中删除特定知识片段的“删除”能力的重要性。研究通过实验揭示了使用同义改写短语描述可以提升“删除”效果，以及从一段文本中删除单一知识点具有挑战性。这表明，知识编码的学习方式在实现可靠的后置删除中可能发挥关键作用。", "conclusion": "学习时的知识编码方式可能在实现可靠的后置删除中扮演关键角色。"}}
{"id": "2506.14827", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14827", "abs": "https://arxiv.org/abs/2506.14827", "authors": ["Yifeng Gao", "Yifan Ding", "Hongyu Su", "Juncheng Li", "Yunhan Zhao", "Lin Luo", "Zixing Chen", "Li Wang", "Xin Wang", "Yixu Wang", "Xingjun Ma", "Yu-Gang Jiang"], "title": "DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning", "comment": null, "summary": "As AI-generated video becomes increasingly pervasive across media platforms,\nthe ability to reliably distinguish synthetic content from authentic footage\nhas become both urgent and essential. Existing approaches have primarily\ntreated this challenge as a binary classification task, offering limited\ninsight into where or why a model identifies a video as AI-generated. However,\nthe core challenge extends beyond simply detecting subtle artifacts; it\nrequires providing fine-grained, persuasive evidence that can convince auditors\nand end-users alike. To address this critical gap, we introduce DAVID-X, the\nfirst dataset to pair AI-generated videos with detailed defect-level,\ntemporal-spatial annotations and written rationales. Leveraging these rich\nannotations, we present DAVID-XR1, a video-language model designed to deliver\nan interpretable chain of visual reasoning-including defect categorization,\ntemporal-spatial localization, and natural language explanations. This approach\nfundamentally transforms AI-generated video detection from an opaque black-box\ndecision into a transparent and verifiable diagnostic process. We demonstrate\nthat a general-purpose backbone, fine-tuned on our compact dataset and enhanced\nwith chain-of-thought distillation, achieves strong generalization across a\nvariety of generators and generation modes. Our results highlight the promise\nof explainable detection methods for trustworthy identification of AI-generated\nvideo content.", "AI": {"tldr": "本论文介绍了DAVID-X数据集和DAVID-XR1模型，用于增强对AI生成视频的可解释性检测方法，使其成为一个透明且可验证的诊断过程。", "motivation": "现有方法主要将识别AI生成视频视为一个二元分类任务，这限制了识别过程中的信息深度。本文旨在提供更加精细和具有说服力的证据，使审核者和终端用户都能信服。", "method": "提出了DAVID-XR1，这是一个视频-语言模型，旨在提供一个可解释的视觉推理链，包括缺陷分类、时空定位和自然语言解释。该方法利用了DAVID-X数据集，此数据集首次为AI生成的视频提供详细的缺陷级、时空注释和书面理由。", "result": "研究表明，通过泛化到我们的紧凑型数据集并增强链式思维蒸馏，一个通用的主干网络能够很好地泛化到多种生成器和生成模式。", "conclusion": "研究结果强调了可信识别AI生成视频内容的可解释性检测方法的潜力。"}}
{"id": "2506.15081", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15081", "abs": "https://arxiv.org/abs/2506.15081", "authors": ["Yaxin Fan", "Peifeng Li", "Qiaoming Zhu"], "title": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification", "comment": "Accepted by ACL2025(main conference)", "summary": "Dialogue discourse parsing aims to identify and analyze discourse relations\nbetween the utterances within dialogues. However, linguistic features in\ndialogues, such as omission and idiom, frequently introduce ambiguities that\nobscure the intended discourse relations, posing significant challenges for\nparsers. To address this issue, we propose a Discourse-aware Clarification\nModule (DCM) to enhance the performance of the dialogue discourse parser. DCM\nemploys two distinct reasoning processes: clarification type reasoning and\ndiscourse goal reasoning. The former analyzes linguistic features, while the\nlatter distinguishes the intended relation from the ambiguous one. Furthermore,\nwe introduce Contribution-aware Preference Optimization (CPO) to mitigate the\nrisk of erroneous clarifications, thereby reducing cascading errors. CPO\nenables the parser to assess the contributions of the clarifications from DCM\nand provide feedback to optimize the DCM, enhancing its adaptability and\nalignment with the parser's requirements. Extensive experiments on the STAC and\nMolweni datasets demonstrate that our approach effectively resolves ambiguities\nand significantly outperforms the state-of-the-art (SOTA) baselines.", "AI": {"tldr": "本文提出了一种话语感知澄清模块 (DCM) 以提高对话话语解析器的性能。通过澄清类型推理和话语目标推理两种不同的推理过程来处理对话中的歧义。同时引入贡献感知偏好优化 (CPO) 来降低错误澄清的风险。实验结果显示，该方法显著优于现有最先进的模型。", "motivation": "对话话语解析中，口语语言特征如省略和成语等会造成歧义，干扰话语关系的识别，增加了句子解析的难度。此研究旨在解决这些问题。", "method": "提出了一个包含澄清类型推理和话语目标推理的DCM来解决话语歧义问题，并引入了CPO来降低错误澄清的风险，优化DCM。", "result": "实验结果表明，该方法在STAC和Molweni数据集上能有效解决歧义问题，并在性能上超越当前最先进的模型。", "conclusion": "研究表明口头话语模块（DCM）和贡献感知偏好优化（CPO）能够增强解析器对对话中歧义处理的能力，总体性能优于现有方法。"}}
{"id": "2506.14831", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.14831", "abs": "https://arxiv.org/abs/2506.14831", "authors": ["Céline Finet", "Stephane Da Silva Martins", "Jean-Bernard Hayet", "Ioannis Karamouzas", "Javad Amirian", "Sylvie Le Hégarat-Mascle", "Julien Pettré", "Emanuel Aldea"], "title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review", "comment": "30 pages", "summary": "With the emergence of powerful data-driven methods in human trajectory\nprediction (HTP), gaining a finer understanding of multi-agent interactions\nlies within hand's reach, with important implications in areas such as\nautonomous navigation and crowd modeling. This survey reviews some of the most\nrecent advancements in deep learning-based multi-agent trajectory prediction,\nfocusing on studies published between 2020 and 2024. We categorize the existing\nmethods based on their architectural design, their input representations, and\ntheir overall prediction strategies, placing a particular emphasis on models\nevaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges\nand future research directions in the field of multi-agent HTP.", "AI": {"tldr": "这篇论文综述了2020年至2024年之间发布的深度学习在多智能体轨迹预测上的最新进展，重点分析了这些模型的架构设计、输入表示及预测策略，并集中在ETH/UCY基准上的模型评估，还指出了该领域的关键挑战和未来研究方向。", "motivation": "动机在于通过综述近年来深度学习在多智能体轨迹预测方面的进展，以深化对多智能体交互的理解并推动自动驾驶导航和人群建模等领域的技术进步。", "method": "论文通过分类现有的方法，基于它们的架构设计、输入表示和整体预测策略进行综述，特别是关注在ETH/UCY基准上的实验评估。", "result": "结果部分重点讨论了分类的模型和策略，以及它们在多智能体轨迹预测领域中的应用。", "conclusion": "结论指出，尽管在多智能体轨迹预测方面有显著进展，但仍面临诸多挑战，提出了未来的研究方向。"}}
{"id": "2506.15118", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15118", "abs": "https://arxiv.org/abs/2506.15118", "authors": ["Junke Wang", "Hongshun Ling", "Li Zhang", "Longqian Zhang", "Fang Wang", "Yuan Gao", "Zhi Li"], "title": "CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records", "comment": "20 pages,5 figures", "summary": "Electronic Health Records (EHR)-based disease prediction models have\ndemonstrated significant clinical value in promoting precision medicine and\nenabling early intervention. However, existing large language models face two\nmajor challenges: insufficient representation of medical knowledge and low\nefficiency in clinical deployment. To address these challenges, this study\nproposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which\nachieves efficient and accurate disease risk prediction through knowledge\ndistillation techniques. Specifically, the large language model Qwen2.5-7B is\nfirst fine-tuned on medical knowledge-enhanced data to serve as the teacher\nmodel.It then generates interpretable soft labels through a multi-granularity\nattention distillation mechanism. Finally, the distilled knowledge is\ntransferred to a lightweight BERT student model. Experimental results show that\non the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline\nmodel:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and\na 22.2 times inference speedup is achieved. This innovative solution not only\ngreatly improves resource utilization efficiency but also significantly\nenhances the accuracy and timeliness of diagnosis, providing a practical\ntechnical approach for resource optimization in clinical settings. The code and\ndata for this research are available athttps://github.com/209506702/CKD_EHR.", "AI": {"tldr": "CKD-EHR框架采用知识蒸馏技术，提升基于电子健康记录的疾病预测模型的效率和准确性，显著优于基线模型。", "motivation": "现有的大型语言模型在医学知识表示和临床应用效率方面面临挑战，这促使研究者提出一个更高效和准确的病患风险预测框架。", "method": "通过知识蒸馏技术，首先在增强医学知识的数据上对大型语言模型Qwen2.5-7B进行微调，作为教师模型。然后，通过多粒度注意力蒸馏机制生成可解释的软标签。最后，将知识转移给轻量级的BERT学生模型。", "result": "实验结果显示，CKD-EHR框架在MIMIC-III数据集上的诊断准确率提高了9%，F1值提高了27%，推理速度提升了22.2倍。", "conclusion": "该方法不仅提高了资源利用率，还显著提高了诊断的准确性和及时性，为临床资源优化提供了实际的技术方案。"}}
{"id": "2506.14832", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14832", "abs": "https://arxiv.org/abs/2506.14832", "authors": ["Jun Yin", "Jing Zhong", "Pengyu Zeng", "Peilin Li", "Zixuan Dai", "Miao Zhang", "Shuai Lu"], "title": "ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes", "comment": "22 pages, 8 figures", "summary": "In contemporary architectural design, the growing complexity and diversity of\ndesign demands have made generative plugin tools essential for quickly\nproducing initial concepts and exploring novel 3D forms. However, objectively\nanalyzing the differences between human-designed and machine-generated 3D forms\nremains a challenge, limiting our understanding of their respective strengths\nand hindering the advancement of generative tools.\n  To address this, we built ArchForms-4000, a dataset containing 2,000\narchitect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet,\na 3D convolutional neural network tailored for classifying and analyzing\narchitectural forms, incorporating a saliency module to highlight key spatial\nfeatures aligned with architectural reasoning; And conducted comparative\nexperiments showing our model outperforms human experts in distinguishing form\norigins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall.\n  This study not only highlights the distinctive advantages of human-designed\nforms in spatial organization, proportional harmony, and detail refinement but\nalso provides valuable insights for enhancing generative design tools in the\nfuture.", "AI": {"tldr": "该研究提出了一种新的模型ArchShapeNet，用于分类和分析建筑3D形式，并展示了它在区分人类设计和机器生成形式方面的优越性能。", "motivation": "在当代建筑设计中，设计需求的复杂性和多样性日益增长，使得生成式插件工具对于快速产生初步概念和探索新型3D形式变得至关重要。然而，客观分析人类设计和机器生成的3D形式之间的差异仍然是一个挑战，这限制了我们对各自优势的理解，并阻碍了生成式工具的发展。", "method": "构建了ArchForms-4000数据集，包含2000个人类设计的和2000个Evomass生成的3D形式；提出了ArchShapeNet，一种专门用于分类和分析建筑形状的3D卷积神经网络，包含了一个能够突出关键空间特征的显着性模块，这些特征与建筑设计逻辑相匹配；进行了对比实验，结果显示模型在区分形式来源方面优于人类专家，准确率为94.29%，精确率为96.2%，召回率为98.51%。", "result": "研究表明，模型在区分形状来源方面优于人类专家，达到94.29%的准确率，96.2%的精确率和98.51%的召回率。", "conclusion": "该研究不仅显现了人类设计形式在空间组织、比例和谐及细节优化中的独特优势，也为未来增强生成设计工具提供了宝贵见解。"}}
{"id": "2506.15131", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15131", "abs": "https://arxiv.org/abs/2506.15131", "authors": ["Jing Yang Lee", "Kong-Aik Lee", "Woon-Seng Gan"], "title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs", "comment": null, "summary": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.", "AI": {"tldr": "本文通过分解开放域对话生成为多响应生成(MRG)和基于偏好的选择(PS)两个步骤，采用了新的策略和评估指标来改进对话模型的能力，尤其是提高了小规模语言模型的表现。", "motivation": "尽管之前的研究表明，建模开放域对话的多对一（o2m）属性可以提高响应的多样性，但大多数现代基于大型语言模型的对话代理并未明确这样做。因此，本文旨在通过分解OD生成为两个关键任务来提升响应的多样性和质量。", "method": "本文提出了一种两阶段框架来优化开放域对话系统的多样性响应生成。首先是多响应生成（MRG），生成给定对话上下文的n个语义和词汇多样的高质量响应；然后再通过基于偏好的选择（PS）选择一个单一响应。", "result": "通过应用所提的方法到小规模语言模型上，其开放域对话生成能够显著提升响应多样性，同时保持上下文连贯性，并将响应质量提升90%。", "conclusion": "实验证明，此两阶段框架应用于较小的大型语言模型可以显著提高开放域对话生成的响应多样性，同时保持上下文连贯性，并将响应质量提高约90%，使得小模型的表现接近于大模型。"}}
{"id": "2506.14833", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14833", "abs": "https://arxiv.org/abs/2506.14833", "authors": ["Poojashree Chandrashekar Pankaj M Sajjanar"], "title": "Real-Time, Low-Latency Surveillance Using Entropy-Based Adaptive Buffering and MobileNetV2 on Edge Devices", "comment": "& pages", "summary": "This paper describes a high-performance, low-latency video surveillance\nsystem designed for resource-constrained environments. We have proposed a\nformal entropy-based adaptive frame buffering algorithm and integrated that\nwith MobileNetV2 to achieve high throughput with low latency. The system is\ncapable of processing live streams of video with sub-50ms end-to-end inference\nlatency on resource-constrained devices (embedding platforms) such as Raspberry\nPi, Amazon, and NVIDIA Jetson Nano. Our method maintains over 92% detection\naccuracy on standard datasets focused on video surveillance and exhibits\nrobustness to varying lighting, backgrounds, and speeds. A number of\ncomparative and ablation experiments validate the effectiveness of our design.\nFinally, our architecture is scalable, inexpensive, and compliant with stricter\ndata privacy regulations than common surveillance systems, so that the system\ncould coexist in a smart city or embedded security architecture.", "AI": {"tldr": "The paper introduces a high-performance, low-latency surveillance system adapted for resource-limited devices with high accuracy in varying conditions, offering scalability, cost-effectiveness, and better privacy.", "motivation": "The aim is to develop a scalable, inexpensive, and privacy-compliant video surveillance system suitable for smart cities or embedded security architecture, designed for use on limited resource devices.", "method": "This paper proposes a formal entropy-based adaptive frame buffering algorithm integrated with MobileNetV2 for high-performance and low-latency video surveillance.", "result": "The system maintains over 92% detection accuracy on standard datasets and demonstrates robustness with varying conditions. It operates with sub-50ms latency on resource-constrained devices.", "conclusion": "The proposed system offers high throughput, low latency, and robust surveillance capabilities on resource-limited devices, making it suitable for smart city and embedded security scenarios."}}
{"id": "2506.15138", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15138", "abs": "https://arxiv.org/abs/2506.15138", "authors": ["Gyeongje Cho", "Yeonkyoun So", "Chanwoo Park", "Sangmin Lee", "Sungmok Jung", "Jaejin Lee"], "title": "Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models", "comment": null, "summary": "This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce\ntoken fertility without compromising model performance. Our approach uses a\nrule-based pre-tokenization method that aligns with the linguistic structure of\nthe Korean language. We also create a seed vocabulary containing tokens that\nresemble linguistic units and employ a branching entropy-based selection\nalgorithm. These techniques increase the average token length, thus lowering\nfertility while preserving linguistic information. Experimental results\nindicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces\nthe number of tokens by 10%, improving the inference speed by 10%) compared to\nBPE without compromising performance across various downstream tasks. These\nfindings demonstrate that our linguistically informed approach is effective and\npractical for designing efficient tokenizers for language models.", "AI": {"tldr": "论文介绍了Thunder-Tok，一种新的韩语Tokenizer，通过增加平均Token长度来减少多产性，实验表明它可以提高10%的推理速度同时保持性能。", "motivation": "当前分词器在某些语言上的多产性高，论文旨在为韩语开发一种既能减少多产性又能保持模型性能的Tokenizer。", "method": "采用基于规则的预分词方法，构建种子词汇表，并使用二叉熵选择算法优化Token，以增加平均Token长度，减少多产性。", "result": "该论文引入了Thunder-Tok，这是一种新的韩语分词器，旨在减少token的多产性而不影响模型性能。该研究采用了一种与韩语语言结构相匹配的基于规则的预分词方法，并创建了一个包含类似语言单元token的种子词汇表，同时运用二叉熵选择算法。这些技术提高了平均token长度，从而减少了多产性，同时保留了语言信息。实验结果显示，与BPE相比，Thunder-Tok将多产性降低了大约10%，即减少了10%的token数量，提高了10%的推理速度，同时在各种下游任务中不牺牲性能。这些发现表明，我们以语言为指导的方法对于设计高效的分词器是有效的且可行的。", "conclusion": "该研究证明了一种基于语言学的Tokenizer设计方法能够有效减少多产性，提高推理速度，同时保持语言模型的性能。Thunder-Tok在不影响任务性能的前提下，改善了韩语模型的效率。"}}
{"id": "2506.14835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14835", "abs": "https://arxiv.org/abs/2506.14835", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Duc Dung Nguyen"], "title": "MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation", "comment": null, "summary": "Precisely localizing 3D objects from a single image constitutes a central\nchallenge in monocular 3D detection. While DETR-like architectures offer a\npowerful paradigm, their direct application in this domain encounters inherent\nlimitations, preventing optimal performance. Our work addresses these\nchallenges by introducing MonoVQD, a novel framework designed to fundamentally\nadvance DETR-based monocular 3D detection. We propose three main contributions.\nFirst, we propose the Mask Separated Self-Attention mechanism that enables the\nintegration of the denoising process into a DETR architecture. This improves\nthe stability of Hungarian matching to achieve a consistent optimization\nobjective. Second, we present the Variational Query Denoising technique to\naddress the gradient vanishing problem of conventional denoising methods, which\nseverely restricts the efficiency of the denoising process. This explicitly\nintroduces stochastic properties to mitigate this fundamental limitation and\nunlock substantial performance gains. Finally, we introduce a sophisticated\nself-distillation strategy, leveraging insights from later decoder layers to\nsynergistically improve query quality in earlier layers, thereby amplifying the\niterative refinement process. Rigorous experimentation demonstrates that\nMonoVQD achieves superior performance on the challenging KITTI monocular\nbenchmark. Highlighting its broad applicability, MonoVQD's core components\nseamlessly integrate into other architectures, delivering significant\nperformance gains even in multi-view 3D detection scenarios on the nuScenes\ndataset and underscoring its robust generalization capabilities.", "AI": {"tldr": "论文介绍了 MonoVQD，一种用于改进 DETR 基于的单目 3D 检测框架，引入了新的自注意力机制和去噪技术，并提出了一种高级的自我蒸馏策略，从而在多个数据集上提升了 3D 检测性能。", "motivation": "由于 DETR 类似架构直接应用于单目 3D 检测时遇到固有限制，无法达到最优性能，因此提出了 MonoVQD 以克服这些挑战。", "method": "Mask Separated Self-Attention 机制和 Variational Query Denoising 技术被引入来改进 DETR 框架，以增强其在单目 3D 检测中的性能。此外，还开发了一种高级自我蒸馏策略来进一步优化查询质量。", "result": "实验表明，MonoVQD 在具有挑战性的 KITTI 单目基准测试中达到了优越的性能。同时，MonoVQD 的核心组件也可以无缝整合到其他架构中，甚至在 nuScenes 数据集中的多视角 3D 检测场景中也展现出了显著的性能提升。", "conclusion": "MonoVQD 的提出不仅提升了单目 3D 检测的效果，而且展示了其广泛的适用性和良好的泛化能力。"}}
{"id": "2506.15156", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15156", "abs": "https://arxiv.org/abs/2506.15156", "authors": ["Muhammad Cendekia Airlangga", "Hilal AlQuabeh", "Munachiso S Nwadike", "Kentaro Inui"], "title": "Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View", "comment": null, "summary": "We study memory in state-space language models using primacy and recency\neffects as behavioral tools to uncover how information is retained and\nforgotten over time. Applying structured recall tasks to the Mamba\narchitecture, we observe a consistent U-shaped accuracy profile, indicating\nstrong performance at the beginning and end of input sequences. We identify\nthree mechanisms that give rise to this pattern. First, long-term memory is\nsupported by a sparse subset of channels within the model's selective state\nspace block, which persistently encode early input tokens and are causally\nlinked to primacy effects. Second, short-term memory is governed by\ndelta-modulated recurrence: recent inputs receive more weight due to\nexponential decay, but this recency advantage collapses when distractor items\nare introduced, revealing a clear limit to memory depth. Third, we find that\nmemory allocation is dynamically modulated by semantic regularity: repeated\nrelations in the input sequence shift the delta gating behavior, increasing the\ntendency to forget intermediate items. We validate these findings via targeted\nablations and input perturbations on two large-scale Mamba-based language\nmodels: one with 1.4B and another with 7B parameters.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.14837", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14837", "abs": "https://arxiv.org/abs/2506.14837", "authors": ["Chengzhi Xu", "Yuyang Wang", "Lai Wei", "Lichao Sun", "Weiran Huang"], "title": "Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction", "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.", "AI": {"tldr": "本文提出了ChartIR方法，该方法通过区分视觉理解和代码翻译两个子任务，并引入结构化指令，有效提升了多模态大语言模型在图表生成任务上的表现，实验结果显示其在对比方法中表现出色。", "motivation": "尽管多模态大语言模型在各种视觉任务中表现出色，但在图表生成任务上的性能仍然不尽人意。这促使研究人员提出ChartIR方法，以改进图表理解和代码翻译的精确性。", "method": "ChartIR方法通过对结构化指令的设计，实现了图表生成任务的逐步优化。该方法首先将视觉理解和代码翻译两个任务区分开，并设计了描述和差异两种结构化指令来完成视觉理解阶段。描述指令用于捕捉参考图表的视觉元素，差异指令用于描述参考图表与生成图表之间的差异。通过这种方式，能够有效地将视觉特征转化为语言表示，进而促进后續的代码翻译流程。其次，该方法将整体的图表生成管道分解成初始代码生成和迭代优化两个阶段，以逐步提升最终输出的质量。", "result": "实验结果表明，在开源模型Qwen2-VL和闭源模型GPT-4o上，相比于其他方法，ChartIR方法均展现出了更优的性能。", "conclusion": "ChartIR方法通过结构化指令实现了对图表生成任务的优化，证明了其在提升视觉理解和代码翻译任务中的有效性。"}}
{"id": "2506.15208", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15208", "abs": "https://arxiv.org/abs/2506.15208", "authors": ["Andrea Cadeddu", "Alessandro Chessa", "Vincenzo De Leo", "Gianni Fenu", "Enrico Motta", "Francesco Osborne", "Diego Reforgiato Recupero", "Angelo Salatino", "Luca Secchi"], "title": "A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals", "comment": "Submitted to IEEE Access", "summary": "In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).", "AI": {"tldr": "\\u8be5\\u7a7a\\u95f4\\u7528\\u4e8e\\u5206\\u6790\\u6bcd\\u7248\\u672c\\u548c\\u5f00\\u653e\\u6765\\u6e90\\u7684\\u5927\\u8fbe\\u4e4b\\u591a\\u8bed\\u6587\\u6a21\\u578b\\u5bf9\\u53ef\\u6301\\u7eed\\u53d1\\u5c55\\u7684\\u76ee\\u6807\\u8fdb\\u884c\\u6587\\u672c\\u5206\\u7c7b\\u7684\\u529b\\u529b\\uff0c\\u5e76\\u8d5b\\u52a8\\u4e86\\u4e00\\u5b9e\\u767b\\u8bbf\\u5b9e\\u9a8c\\u548c\\u5c0f\\u989f\\u767b\\u8bbf\\u5b9e\\u9a8c\\u7b49\\u65b9\\u6cd5\\u7684\\u6548\\u679c\\u3002\\u7ed3\\u679c\\u8868\\u793a\\uff0c\\u9009\\u62e9\\u4e00\\u5b9a\\u7684\\u63d0\\u9192\\u5de5\\u7a0b\\uff0c\\u5c0f\\u6a21\\u578b\\u53ef\\u4ee5\\u53d6\\u4ee3\\u5c0f\\u5927\\u7684\\u6a21\\u578b\\u3002", "motivation": "\\u5bf9\\u5360\\u7528\\u4e00\\u5b9a\\u7684\\u63d0\\u9192\\u5de5\\u7a0b\\uff0c\\u5c0f\\u6a21\\u578b\\u5728\\u5bf9\\u5e94\\u8d5b\\u52a8\\u4e00\\u5b9e\\u9a8c\\u548c\\u5c0f\\u989f\\u767b\\u8bbf\\u5b9e\\u9a8c\\u7b49\\u65b9\\u6cd5\\u4e4b\\u540e\\uff0c\\u53ef\\u4ee5\\u5f62\\u6210\\u4e00\\u79cd\\u7b49\\u540c\\u5c0f\\u5927\\u6a21\\u578b\\u7684\\u6548\\u679c\\u3002", "method": "Structure", "result": "{\"tldr\": \"\\u8be5\\u7a7a\\u95f4\\u7528\\u4e8e\\u5206\\u6790\\u6bcd\\u7248\\u672c\\u548c\\u5f00\\u653e\\u6765\\u6e90\\u7684\\u5927\\u8fbe\\u4e4b\\u591a\\u8bed\\u6587\\u6a21\\u578b\\u5bf9\\u53ef\\u6301\\u7eed\\u53d1\\u5c55\\u7684\\u76f8\\u5e94\\u76ee\\u6807\\u8fdb\\u884c\\u6587\\u672c\\u5206\\u7c7b\\u7684\\u529b\\u529b\\uff0c\\u5e76\\u8d5b\\u52a8\\u4e86\\u4e00\\u5148\\u767b\\u8bbf\\u5b9e\\u9a8c\\u548c\\u5c0f\\u9891\\u767b\\u8bbf\\u5b9e\\u9a8c\\u7b49\\u65b9\\u6cd5\\u7684\\u6548\\u679c\\u3002\\u7ed3\\u679c\\u8868\\u793a\\uff0c\\u9009\\u62e9\\u4e00\\u5b9a\\u7684\\u63d0\\u9192\\u5de5\\u7a0b\\uff0c\\u5c0f\\u6a21\\u578b\\u53ef\\u4ee5\\u53d6\\u4ee3\\u5c0f\\u5927\\u7684\\u6a21\\u578b\\u3002\", \"motivation\": \"\\u5bf9\\u5360\\u7528\\u4e00\\u5b9a\\u7684\\u63d0\\u9192\\u5de5\\u7a0b\\uff0c\\u5c0f\\u6a21\\u578b\\u5728\\u5bf9\\u5e94\\u8d5b\\u52a8\\u4e00\\u5b9e\\u9a8c\\u548c\\u5c0f\\u9891\\u767b\\u8bbf\\u5b9e\\u9a8c\\u7b49\\u65b9\\u6cd5\\u4e4b\\u540e\\uff0c\\u53ef\\u4ee5\\u5f62\\u6210\\u4e00\\u79cd\\u7b49\\u540c\\u5c0f\\u5927\\u6a21\\u578b\\u7684\\u6548\\u679c\\u3002\", \"method\": \"\\u7a0b\\u5e8f\\u4f7f\\u7528\\u4e86\\u51e0\\u4e2a\\u8f93\\u5165\\u5c40\\u90e8\\u5f00\\u5c40\\u4e8e\\u5927\\u8fbe\\u4e4b\\u591a\\u8bed\\u6587\\u6a21\\u578b\\u5728\\u6587\\u672c\\u5206\\u7c7b\\u8d5b\\u52a8\\u4e00\\u5b9e\\u9a8c\\u548c\\u5c0f\\u989f\\u5b9e\\u9a8c\\u4e2d\\u7684\\u6548\\u679c\\uff0c\\u5e76\\u8fdb\\u884c\\u4e86\\u8c03\\u7528\\u548c\\u6bd5\\u987a\\u5b9a\\u5750\\u4e4b\\u540e\\u7684\\u7ec4\\u5408\\u64cd\\u4f5c\\u3002\", \"result\": \"\\u5c0f\\u6a21\\u578b\\u5728\\u7ec4\\u5408\\u64cd\\u4f5c\\u4e4b\\u540e\\u53ef\\u5f62\\u6210\\u4e00\\u79cd\\u7b49\\u540c\\u5927\\u6a21\\u578b\\u7684\\u6548\\u679c\\u3002\", \"conclusion\": \"\\u5c0f\\u6a21\\u578b\\u901a\\u8fc7\\u4e00\\u5b9e\\u9a8c\\u548c\\u5c0f\\u989f\\u9a8c\\u8bc1\\u53ef\\u4ee5\\u5f62\\u6210\\u8d8a\\u52a8\\u8d5b\\u52a8\\u6240\\u83b7\\u5f97\\u6a21\\u578b\\u7684\\u7ed3\\u679c\\uff0c\\u6b63\\u786e\\u89e3\\u9898\\u80fd\\u529b\\u76f8\\u540c\\u3002\"}", "conclusion": "\\u5c0f\\u6a21\\u578b\\u901a\\u8fc7\\u4e00\\u5b9e\\u9a8c\\u548c\\u5c0f\\u989f\\u9a8c\\u8bc1\\u53ef\\u4ee5\\u5f62\\u6210\\u8d8a\\u52a8\\u8d5b\\u52a8\\u6240\\u83b7\\u5f97\\u6a21\\u578b\\u7684\\u7ed3\\u679c\\uff0c\\u6b63\\u786e\\u89e3\\u9898\\u80fd\\u529b\\u76f8\\u540c\\u3002"}}
{"id": "2506.14842", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14842", "abs": "https://arxiv.org/abs/2506.14842", "authors": ["Lukas Schiesser", "Cornelius Wolff", "Sophie Haas", "Simon Pukrop"], "title": "PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers", "comment": "15 pages, 10 figures", "summary": "Building image classification models remains cumbersome in data-scarce\ndomains, where collecting large labeled datasets is impractical. In-context\nlearning (ICL) has emerged as a promising paradigm for few-shot image\nclassification (FSIC), enabling models to generalize across domains without\ngradient-based adaptation. However, prior work has largely overlooked a\ncritical component of ICL-based FSIC pipelines: the role of image embeddings.\nIn this work, we present PictSure, an ICL framework that places the embedding\nmodel -- its architecture, pretraining, and training dynamics -- at the center\nof analysis. We systematically examine the effects of different visual encoder\ntypes, pretraining objectives, and fine-tuning strategies on downstream FSIC\nperformance. Our experiments show that the training success and the\nout-of-domain performance are highly dependent on how the embedding models are\npretrained. Consequently, PictSure manages to outperform existing ICL-based\nFSIC models on out-of-domain benchmarks that differ significantly from the\ntraining distribution, while maintaining comparable results on in-domain tasks.\nCode can be found at https://github.com/PictSure/pictsure-library.", "AI": {"tldr": "PictSure框架优化嵌入模型的预训练和训练方式，显着提升了少量样本图像分类在跨域任务中的表现，同时在域内任务中表现良好。", "motivation": "本文的研究动机在于解决数据稀缺领域构建图像分类模型时的挑战，并通过在上下文学习（ICL）框架下优化图像嵌入模型的性能来提升少量样本图像分类（FSIC）的能力。", "method": "本文通过PictSure框架系统性地分析了不同视觉编码器类型、预训练目标和微调策略对下游少量样本图像分类任务性能的影响。PictSure框架将嵌入模型及其架构、预训练和训练动态作为中心分析对象。", "result": "实验结果显示，训练成功和跨域性能的提升高度依赖于嵌入模型的预训练方式。PictSure可以超越现有的ICL基础FSIC模型在显著不同于训练分布的跨域基准测试上的表现，同时在域内任务上保持可比的结果。", "conclusion": "研究表明，正确的预训练策略能够大大提升嵌入模型在少量样本（few-shot）分类中的表现。PictSure通过优化这些策略，证明了其在跨域图像分类任务上的优越性能。"}}
{"id": "2506.15211", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15211", "abs": "https://arxiv.org/abs/2506.15211", "authors": ["Feng He", "Zijun Chen", "Xinnian Liang", "Tingting Ma", "Yunqi Qiu", "Shuangzhi Wu", "Junchi Yan"], "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs", "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.", "AI": {"tldr": "本文假设大型推理模型的跨领域泛化能力源自于跨任务共享的抽象推理原型。基于此假设，我们提出了一个增强推理能力的框架——ProtoReasoning，该框架通过转换问题为原型表示、利用验证系统以及在原型空间内合成问题，实现了显著的性能提升，并验证了我们的假设。", "motivation": "大型推理模型在跨领域泛化方面的机制理解尚不充分。我们假设跨领域的泛化是由于在不同领域的推理任务中存在共有的抽象推理原型，这些原型抽象出了问题的本质，揭示了看似不同的任务所具有的共通的推理结构。", "method": "我们提出了一个名为ProtoReasoning的框架，该框架通过利用可扩展且可验证的原型表示（通过Prolog进行逻辑推理，通过PDDL进行规划）来增强大模型的推理能力。ProtoReasoning包括：(1)一个自动构建原型的流水线，可以将问题转换为对应的原型表示；(2)一个综合验证系统，通过Prolog和PDDL解释器提供可靠的反馈；(3)在原型空间内合成问题的可扩展性，并确保正确性。", "result": "实验证明，ProtoReasoning在逻辑推理（Enigmata-Eval）上比基线模型提高了4.7%，在规划任务上提高了6.3%，在综合推理（MMLU）上提高了4.0%，在数学（AIME24）上提高了1.0%。显著的是，我们的消融研究表明，在原型空间中学习的模型在面对结构相似的问题时，展示了比仅在自然语言表示上训练的模型更好的泛化能力。", "conclusion": "实验结果验证了我们的假设，即推理原型是大型语言模型中可泛化推理的基础。"}}
{"id": "2506.14846", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.14846", "abs": "https://arxiv.org/abs/2506.14846", "authors": ["Shreyas Rajeev", "B Sathish Babu"], "title": "Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach", "comment": null, "summary": "Kernel size selection in Convolutional Neural Networks (CNNs) is a critical\nbut often overlooked design decision that affects receptive field, feature\nextraction, computational cost, and model accuracy. This paper proposes the\nBest Kernel Size Estimation Function (BKSEF), a mathematically grounded and\nempirically validated framework for optimal, layer-wise kernel size\ndetermination. BKSEF balances information gain, computational efficiency, and\naccuracy improvements by integrating principles from information theory, signal\nprocessing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100,\nImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided\narchitectures achieve up to 3.1 percent accuracy improvement and 42.8 percent\nreduction in FLOPs compared to traditional models using uniform 3x3 kernels.\nTwo real-world case studies further validate the approach: one for medical\nimage classification in a cloud-based setup, and another for traffic sign\nrecognition on edge devices. The former achieved enhanced interpretability and\naccuracy, while the latter reduced latency and model size significantly, with\nminimal accuracy trade-off. These results show that kernel size can be an\nactive, optimizable parameter rather than a fixed heuristic. BKSEF provides\npractical heuristics and theoretical support for researchers and developers\nseeking efficient and application-aware CNN designs. It is suitable for\nintegration into neural architecture search pipelines and real-time systems,\noffering a new perspective on CNN optimization.", "AI": {"tldr": "本文提出了BKSEF，一个数学上和经验上被验证的框架，用于确定CNN中层间的最优核尺寸，实验结果表明相比传统的使用统一3x3核的模型，BKSEF指导的架构在准确性和FLOPs减少方面都有所提高。", "motivation": "卷积神经网络（CNN）中的核尺寸选择是一个关键但常被忽视的设计决策，它影响感受野、特征提取、计算成本和模型准确性。", "method": "提出了最佳核尺寸估算函数（BKSEF），通过结合信息论、信号处理和学习理论的原则，实现层间最优核尺寸的确定，平衡信息增益、计算效率和准确性提高。", "result": "在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上的实验表明，BKSEF指导的架构相比传统模型实现了最高3.1%的准确度提升和42.8%的FLOPs减少。而且，两个实际案例研究--一个用于基于云的医学图像分类，另一个用于边缘设备上的交通标志识别--进一步验证了这种方法的有效性。", "conclusion": "BKSEF提供了一种在CNN优化方面的全新视角，适用于集成到神经架构搜索管道和实时系统中。"}}
{"id": "2506.15215", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15215", "abs": "https://arxiv.org/abs/2506.15215", "authors": ["Yongqi Fan", "Yating Wang", "Guandong Wang", "Jie Zhai", "Jingping Liu", "Qi Ye", "Tong Ruan"], "title": "MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs", "comment": null, "summary": "Open-ended question answering (QA) is a key task for evaluating the\ncapabilities of large language models (LLMs). Compared to closed-ended QA, it\ndemands longer answer statements, more nuanced reasoning processes, and diverse\nexpressions, making refined and interpretable automatic evaluation both crucial\nand challenging. Traditional metrics like ROUGE and BERTScore struggle to\ncapture semantic similarities due to different patterns between model responses\nand reference answers. Current LLM-based evaluation approaches, such as\npairwise or listwise comparisons of candidate answers, lack intuitive\ninterpretability. While pointwise scoring of each response provides some\ndescriptions, it fails to adapt across different question contents. Most\nnotably, existing methods overlook the distinction between factoid and\nnon-factoid questions. To address these challenges, we propose\n\\textbf{MinosEval}, a novel evaluation method that first distinguishes\nopen-ended questions and then ranks candidate answers using different\nevaluation strategies. For factoid questions, it applies an adaptive key-point\nscoring strategy, while for non-factoid questions, it uses an instance-aware\nlistwise ranking strategy. Experiments on multiple open-ended QA datasets,\nincluding self-built ones with more candidate responses to complement community\nresources, show that MinosEval better aligns with human annotations and offers\nmore interpretable results.", "AI": {"tldr": "论文提出MinosEval方法，针对开放式问题的回答评估，根据不同类型问题采用不同评估策略，实验结果表明其评估效果较好且结果可解释。", "motivation": "开放式问题回答评估在传统的评估方法（如ROUGE和BERTScore）中面临困难，这些方法难以捕捉语义相似度且不具备直观的可解释性。同时，现有评估方法未区分事实型和非事实型问题，这影响了评估的效果和合理性。因此，需要一种能够有效区分和评估开放式问题的回答的新方法。", "method": "文章提出了MinosEval方法，首先通过问题类型区分模块来区分开放性问题（包括事实型和非事实型），然后根据问题类型选择合适的评分策略：事实型问题采用关键词评分策略，非事实型问题采用实例感知的列表排名策略。", "result": "该论文提出一种新颖的开放式问题回答评估方法MinosEval，该方法首先区分事实型和非事实型问题，然后采用不同的评估策略对候选答案进行排序。对于事实型问题，采用自适应关键点评分策略；对于非事实型问题，采用实例感知的列表排名策略。实验表明，MinosEval评估结果更符合人类标注，并且具有更好的可解释性。", "conclusion": "实验结果表明，MinosEval在多种开放式问题回答数据集上，尤其是自建的数据集上，能更好地与人类标注保持一致，并提供更具有可解释性的评估结果。"}}
{"id": "2506.14854", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14854", "abs": "https://arxiv.org/abs/2506.14854", "authors": ["Varun Mannam", "Zhenyu Shi"], "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis", "comment": "Submitting to ICCV 2025 workshop:\n  https://retailvisionworkshop.github.io/", "summary": "Accurate video annotation plays a vital role in modern retail applications,\nincluding customer behavior analysis, product interaction detection, and\nin-store activity recognition. However, conventional annotation methods heavily\nrely on time-consuming manual labeling by human annotators, introducing\nnon-robust frame selection and increasing operational costs. To address these\nchallenges in the retail domain, we propose a deep learning-based approach that\nautomates key-frame identification in retail videos and provides automatic\nannotations of products and customers. Our method leverages deep neural\nnetworks to learn discriminative features by embedding video frames and\nincorporating object detection-based techniques tailored for retail\nenvironments. Experimental results showcase the superiority of our approach\nover traditional methods, achieving accuracy comparable to human annotator\nlabeling while enhancing the overall efficiency of retail video annotation.\nRemarkably, our approach leads to an average of 2 times cost savings in video\nannotation. By allowing human annotators to verify/adjust less than 5% of\ndetected frames in the video dataset, while automating the annotation process\nfor the remaining frames without reducing annotation quality, retailers can\nsignificantly reduce operational costs. The automation of key-frame detection\nenables substantial time and effort savings in retail video labeling tasks,\nproving highly valuable for diverse retail applications such as shopper journey\nanalysis, product interaction detection, and in-store security monitoring.", "AI": {"tldr": "简而言之：本文提出一种基于深度学习的算法来自动识别零售视频中的关键帧并进行标注，这比传统的人工方法更加高效同时节省成本。", "motivation": "动机：传统视频注释方法依赖人工标注，导致成本高、效率低和标注不准确。本文旨在通过自动化视频关键帧识别和自动注释减少运营成本并提高效率。", "method": "方法：本文利用深度神经网络从视频帧中学习判别特征，并采用专为零售环境设计的对象检测技术，实现零售视频中关键帧的自动识别与注释。", "result": "内容概述：本文提出了一种基于深度学习的自动关键帧识别方法，用于零售视频中的产品和客户自动注释，这种方法解决了传统方法中人工标注耗时、成本高的问题。实验表明，该方法的准确率可与人工标注相媲美，并且能够大幅提高零售视频标注的工作效率，平均节省两倍的标注成本。通过仅让人工标注员审核/调整不到5%的检测到的帧，该方法实现了高效且高质量的自动注释。这项技术对多种零售应用具有重大价值，如顾客行为分析、产品互动检测和店内安全监控。", "conclusion": "结论：提出的自动化方法能够显著提高零售视频标注的效率，并确保与人工标注相同的高质量。通过自动化的关键帧检测，显著提升了时间和人力成本效益，为多种零售应用提供了高价值的解决方案。"}}
{"id": "2506.15239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15239", "abs": "https://arxiv.org/abs/2506.15239", "authors": ["Jaione Bengoetxea", "Itziar Gonzalez-Dios", "Rodrigo Agerri"], "title": "Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants", "comment": null, "summary": "In this paper, we evaluate the capacity of current language technologies to\nunderstand Basque and Spanish language varieties. We use Natural Language\nInference (NLI) as a pivot task and introduce a novel, manually-curated\nparallel dataset in Basque and Spanish, along with their respective variants.\nOur empirical analysis of crosslingual and in-context learning experiments\nusing encoder-only and decoder-based Large Language Models (LLMs) shows a\nperformance drop when handling linguistic variation, especially in Basque.\nError analysis suggests that this decline is not due to lexical overlap, but\nrather to the linguistic variation itself. Further ablation experiments\nindicate that encoder-only models particularly struggle with Western Basque,\nwhich aligns with linguistic theory that identifies peripheral dialects (e.g.,\nWestern) as more distant from the standard. All data and code are publicly\navailable.", "AI": {"tldr": "本文评估了当前语言技术处理巴斯克语和西班牙语变体的能力，并发现这些技术在处理某些语言变体时性能下降，尤其是巴斯克语方言。", "motivation": "本文旨在评估当今语言技术理解巴斯克语和西班牙语变体的能力。", "method": "本文采用自然语言推理（NLI）作为关键任务，并引入了一个新颖的手动整理的平行数据集，包含巴斯克语和西班牙语及其各自变体。", "result": "实证分析显示，当处理语言变体，尤其是在巴斯克语时，跨语言和上下文学习实验中的大型语言模型（LLMs）的性能下降。错误分析表明这种下降并非由于词汇重叠问题，而是由于语言变体问题本身。进一步的消融实验表明，基于编码器的模型在处理西部巴斯克语变体时尤其困难，并且这与认为边缘方言（如西部方言）离标准语言更远的语言理论相吻合。", "conclusion": "所有的数据和代码都是公开可用的，这有助于进一步的研究和验证本文的结果。"}}
{"id": "2506.14856", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14856", "abs": "https://arxiv.org/abs/2506.14856", "authors": ["Zhengquan Zhang", "Feng Xu", "Mengmi Zhang"], "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction", "comment": "9 pages, 3 figures in the main text. Under review for NeurIPS 2025", "summary": "Some perspectives naturally provide more information than others. How can an\nAI system determine which viewpoint offers the most valuable insight for\naccurate and efficient 3D object reconstruction? Active view selection (AVS)\nfor 3D reconstruction remains a fundamental challenge in computer vision. The\naim is to identify the minimal set of views that yields the most accurate 3D\nreconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian\nSplatting, from a current observation and computing uncertainty for each\ncandidate viewpoint, we introduce a novel AVS approach guided by neural\nuncertainty maps predicted by a lightweight feedforward deep neural network,\nnamed UPNet. UPNet takes a single input image of a 3D object and outputs a\npredicted uncertainty map, representing uncertainty values across all possible\ncandidate viewpoints. By leveraging heuristics derived from observing many\nnatural objects and their associated uncertainty patterns, we train UPNet to\nlearn a direct mapping from viewpoint appearance to uncertainty in the\nunderlying volumetric representations. Next, our approach aggregates all\npreviously predicted neural uncertainty maps to suppress redundant candidate\nviewpoints and effectively select the most informative one. Using these\nselected viewpoints, we train 3D neural rendering models and evaluate the\nquality of novel view synthesis against other competitive AVS methods.\nRemarkably, despite using half of the viewpoints than the upper bound, our\nmethod achieves comparable reconstruction accuracy. In addition, it\nsignificantly reduces computational overhead during AVS, achieving up to a 400\ntimes speedup along with over 50\\% reductions in CPU, RAM, and GPU usage\ncompared to baseline methods. Notably, our approach generalizes effectively to\nAVS tasks involving novel object categories, without requiring any additional\ntraining.", "AI": {"tldr": "本文提出一种基于UPNet预测的神经不确定性地图引导的新AVS方法，实现了减少计算开销和保持高质量重建，同时具有良好的新对象类别的概括性。", "motivation": "主动视点选择是对3D重建的基本挑战。作者尝试通过提出一个新的方法来确定构建精准且高效的3D物体重建，最少视点组合提供了最大的信息。这一方法改变了现有的神经网络，从当前观察学出辐射场并计算每个候选视点的不确定性。", "method": "本文提出了一种新的主动视点选择（AVS）方法，该方法由UPNet预测的神经不确定地图引导。UPNet是一个轻量级的前馈深度神经网络，它接收3D物体的单个输入图像，并输出表示所有可能候选视点的不确定值的预测不确定地图。通过利用来自观察许多自然物体及其相关不确定性模式的启发式方法，训练UPNet学习直接将视点外观映射到底层体积表示的不确定性。然后，该方法聚合所有已预测的神经不确定地图，压缩冗余的候选视点，有效地选择最具信息量的视点。", "result": "相较于基线方法，使用新方法最多可以降低计算负担400倍，并且可以减少50%的CPU、RAM和GPU的使用。此方法在使用仅为上界一半的视点情况下仍能达到可比较的重建准确性。", "conclusion": "研究表明，由于使用UPNet预测视点不确定性，该方法能够减少计算负担，并有效选择最具信息量的视点，提高了3D重建效率和质量。而无需额外训练，该方法具有良好的通用性。"}}
{"id": "2506.15241", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15241", "abs": "https://arxiv.org/abs/2506.15241", "authors": ["Yang Fan", "Zhang Qi", "Xing Wenqian", "Liu Chang", "Liu Liu"], "title": "Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs", "comment": null, "summary": "This article addresses domain knowledge gaps in general large language models\nfor historical text analysis in the context of computational humanities and\nAIGC technology. We propose the Graph RAG framework, combining chain-of-thought\nprompting, self-instruction generation, and process supervision to create a The\nFirst Four Histories character relationship dataset with minimal manual\nannotation. This dataset supports automated historical knowledge extraction,\nreducing labor costs. In the graph-augmented generation phase, we introduce a\ncollaborative mechanism between knowledge graphs and retrieval-augmented\ngeneration, improving the alignment of general models with historical\nknowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,\nwith Simplified Chinese input and chain-of-thought prompting, achieves optimal\nperformance in relation extraction (F1 = 0.68). The DeepSeek model integrated\nwith GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation\nextraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),\neffectively alleviating hallucinations phenomenon, and improving\ninterpretability. This framework offers a low-resource solution for classical\ntext knowledge extraction, advancing historical knowledge services and\nhumanities research.", "AI": {"tldr": "通过Graph RAG框架，本文提出了一种低资源的解决方案，用于历史文本分析，特别是人物关系的数据集和知识抽取，显著改善了模型性能，减少了幻觉现象，提高了可解释性。", "motivation": "本文旨在填补一般大型语言模型在历史文本分析方面的领域知识空白，特别是在计算人文和AIGC技术的背景下。", "method": "本文提出了Graph RAG框架，结合了链式思维提示、自指令生成和过程监督，用于创建《史记》中的人物关系数据集，极大地减少了手动标注的工作量。", "result": "实验表明，特定领域的模型Xunzi-Qwen1.5-14B在关系抽取上取得了最佳性能（F1=0.68）。而集成了GraphRAG的DeepSeek模型在开放领域的C-CLUE关系抽取数据集上，F1值提升了11%（0.08-0.19），超过了Xunzi-Qwen1.5-14B的F1值0.12。", "conclusion": "本框架提供了一种低资源的解决方案，可应用于古典文本知识抽取，从而推动历史知识服务和人文研究的发展。"}}
{"id": "2506.14903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14903", "abs": "https://arxiv.org/abs/2506.14903", "authors": ["Renjith Prasad", "Abhilekh Borah", "Hasnat Md Abdullah", "Chathurangi Shyalika", "Gurpreet Singh", "Ritvik Garimella", "Rajarshi Roy", "Harshul Surana", "Nasrin Imanpour", "Suranjana Trivedy", "Amit Sheth", "Amitava Das"], "title": "DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization", "comment": "59 pages, 10 figures", "summary": "Alignment is crucial for text-to-image (T2I) models to ensure that generated\nimages faithfully capture user intent while maintaining safety and fairness.\nDirect Preference Optimization (DPO), prominent in large language models\n(LLMs), is extending its influence to T2I systems. This paper introduces\nDPO-Kernels for T2I models, a novel extension enhancing alignment across three\ndimensions: (i) Hybrid Loss, integrating embedding-based objectives with\ntraditional probability-based loss for improved optimization; (ii) Kernelized\nRepresentations, employing Radial Basis Function (RBF), Polynomial, and Wavelet\nkernels for richer feature transformations and better separation between safe\nand unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's\ndefault Kullback-Leibler (KL) regularizer by incorporating Wasserstein and\nR'enyi divergences for enhanced stability and robustness. We introduce\nDETONATE, the first large-scale benchmark of its kind, comprising approximately\n100K curated image pairs categorized as chosen and rejected. DETONATE\nencapsulates three axes of social bias and discrimination: Race, Gender, and\nDisability. Prompts are sourced from hate speech datasets, with images\ngenerated by leading T2I models including Stable Diffusion 3.5 Large, Stable\nDiffusion XL, and Midjourney. Additionally, we propose the Alignment Quality\nIndex (AQI), a novel geometric measure quantifying latent-space separability of\nsafe/unsafe image activations, revealing hidden vulnerabilities. Empirically,\nwe demonstrate that DPO-Kernels maintain strong generalization bounds via\nHeavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are\npublicly released.", "AI": {"tldr": "DPO-Kernels introduced for T2I models to enhance alignment with improved optimization techniques, evaluated on a large-scale benchmark DETONATE.", "motivation": "To improve alignment in text-to-image models, ensuring safety and fairness, by extending the principles of Direct Preference Optimization (DPO) used in large language models.", "method": "DPO-Kernels, a novel extension to enhance alignment in T2I models, introduces three dimensions: Hybrid Loss, Kernelized Representations, and Divergence Selection.", "result": "The performance of DPO-Kernels is evaluated on DETONATE, a large-scale benchmark, and it maintains strong generalization bounds via Heavy-Tailed Self-Regularization (HT-SR).", "conclusion": "DPO-Kernels effectively improve alignment in T2I models, addressing safety and fairness through the introduction of new optimization techniques and a novel benchmark."}}
{"id": "2506.15246", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15246", "abs": "https://arxiv.org/abs/2506.15246", "authors": ["Juli Bakagianni", "John Pavlopoulos", "Aristidis Likas"], "title": "TopClustRAG at SIGIR 2025 LiveRAG Challenge", "comment": null, "summary": "We present TopClustRAG, a retrieval-augmented generation (RAG) system\ndeveloped for the LiveRAG Challenge, which evaluates end-to-end question\nanswering over large-scale web corpora. Our system employs a hybrid retrieval\nstrategy combining sparse and dense indices, followed by K-Means clustering to\ngroup semantically similar passages. Representative passages from each cluster\nare used to construct cluster-specific prompts for a large language model\n(LLM), generating intermediate answers that are filtered, reranked, and finally\nsynthesized into a single, comprehensive response. This multi-stage pipeline\nenhances answer diversity, relevance, and faithfulness to retrieved evidence.\nEvaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in\nfaithfulness and 7th in correctness on the official leaderboard, demonstrating\nthe effectiveness of clustering-based context filtering and prompt aggregation\nin large-scale RAG systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.14907", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.14907", "abs": "https://arxiv.org/abs/2506.14907", "authors": ["Yizhen Zhang", "Yang Ding", "Shuoshuo Zhang", "Xinchen Zhang", "Haoling Li", "Zhong-zhi Li", "Peijie Wang", "Jie Wu", "Lei Ji", "Yelong Shen", "Yujiu Yang", "Yeyun Gong"], "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning", "comment": null, "summary": "Inspired by the impressive reasoning capabilities demonstrated by\nreinforcement learning approaches like DeepSeek-R1, recent emerging research\nhas begun exploring the use of reinforcement learning (RL) to enhance\nvision-language models (VLMs) for multimodal reasoning tasks. However, most\nexisting multimodal reinforcement learning approaches remain limited to spatial\nreasoning within single-image contexts, yet still struggle to generalize to\nmore complex and real-world scenarios involving multi-image positional\nreasoning, where understanding the relationships across images is crucial. To\naddress this challenge, we propose a general reinforcement learning approach\nPeRL tailored for interleaved multimodal tasks, and a multi-stage strategy\ndesigned to enhance the exploration-exploitation trade-off, thereby improving\nlearning efficiency and task performance. Specifically, we introduce\npermutation of image sequences to simulate varied positional relationships to\nexplore more spatial and positional diversity. Furthermore, we design a rollout\nfiltering mechanism for resampling to focus on trajectories that contribute\nmost to learning optimal behaviors to exploit learned policies effectively. We\nevaluate our model on 5 widely-used multi-image benchmarks and 3 single-image\nbenchmarks. Our experiments confirm that PeRL trained model consistently\nsurpasses R1-related and interleaved VLM baselines by a large margin, achieving\nstate-of-the-art performance on multi-image benchmarks, while preserving\ncomparable performance on single-image tasks.", "AI": {"tldr": "为了应对现有的多模态强化学习方法难以推广到多图像位置推理问题，作者提出了新方法PeRL，展示其在多图像基准测试上达到最先进的性能。", "motivation": "现有的多模态强化学习方法仍然局限于单图像情境中的空间推理，难以推广到涉及多图像位置推理的更复杂、更真实的场景。这些场景需要理解图像之间的关系。为了应对这一挑战，提出了PeRL方法。", "method": "文中提出了一种针对交错多模态任务的通用强化学习方法PeRL，以及多阶段策略来优化探索与利用之间的权衡，提高学习效率和任务性能。具体来说，引入了图像序列的排列来模拟不同的位置关系，探索更多的空间和位置多样性，并设计了一种回放缓冲过滤机制来重新采样，专注于有助于学习最优行为的轨迹。", "result": "在五个广泛使用的多图像基准测试和三个单图像基准测试上对模型进行了评估。实验结果证实，经过PeRL训练的模型在多图像基准测试上超过了R1相关和其他交错VLM基线，实现了最先进的性能，同时在单图像任务上保持了相当的性能。", "conclusion": "PeRL方法成功地提升了在涉及多图像位置推理任务上的性能，不仅在多图像基准上表现优异，也在单图像任务中保持了竞争力。"}}
{"id": "2506.15266", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15266", "abs": "https://arxiv.org/abs/2506.15266", "authors": ["Sungen Hahm", "Heejin Kim", "Gyuseong Lee", "Hyunji Park", "Jaejin Lee"], "title": "Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments", "comment": null, "summary": "To ensure a balance between open access to justice and personal data\nprotection, the South Korean judiciary mandates the de-identification of court\njudgments before they can be publicly disclosed. However, the current\nde-identification process is inadequate for handling court judgments at scale\nwhile adhering to strict legal requirements. Additionally, the legal\ndefinitions and categorizations of personal identifiers are vague and not\nwell-suited for technical solutions. To tackle these challenges, we propose a\nde-identification framework called Thunder-DeID, which aligns with relevant\nlaws and practices. Specifically, we (i) construct and release the first Korean\nlegal dataset containing annotated judgments along with corresponding lists of\nentity mentions, (ii) introduce a systematic categorization of Personally\nIdentifiable Information (PII), and (iii) develop an end-to-end deep neural\nnetwork (DNN)-based de-identification pipeline. Our experimental results\ndemonstrate that our model achieves state-of-the-art performance in the\nde-identification of court judgments.", "AI": {"tldr": "文章提出了一种新的去标识化框架Thunder-DeID，设计了特定的韩语法律数据集并引入了一套系统化的个人身份信息分类方法，并采用基于深度神经网络的去标识化流程，实验数据证明，该模型达到了最先进的性能。", "motivation": "为了在开放司法访问和个人数据保护之间取得平衡，韩国司法体系要求在公共公开之前必须对法院判决进行去标识化，但目前的去标识化过程无法在遵守严格法律要求的同时处理大量法院判决，并且现有关于个人标识符的法律定义和技术解决方案之间的契合度不高。", "method": "开发了一个名为Thunder-DeID的去标识化框架，该框架包含三大组成部分：1) 构建并发布了首个包含注释判决和对应的实体提及列表的韩语法律数据集；2) 引入了一种系统性的个人身份信息类别划分；3) 开发了一个端到端的基于深度神经网络的去标识化流程。", "result": "实验结果表明，该模型在法院判决的去标识化任务中达到了最先进的性能。", "conclusion": "Thunder-DeID框架的成功实施，说明了通过创造性的技术和法律定义改进现有去标识化方法的可能性及其高效性。"}}
{"id": "2506.14919", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.14919", "abs": "https://arxiv.org/abs/2506.14919", "authors": ["Xinkai Zhao", "Yuta Tokuoka", "Junichiro Iwasawa", "Keita Oda"], "title": "Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models", "comment": null, "summary": "The increasing use of diffusion models for image generation, especially in\nsensitive areas like medical imaging, has raised significant privacy concerns.\nMembership Inference Attack (MIA) has emerged as a potential approach to\ndetermine if a specific image was used to train a diffusion model, thus\nquantifying privacy risks. Existing MIA methods often rely on diffusion\nreconstruction errors, where member images are expected to have lower\nreconstruction errors than non-member images. However, applying these methods\ndirectly to medical images faces challenges. Reconstruction error is influenced\nby inherent image difficulty, and diffusion models struggle with high-frequency\ndetail reconstruction. To address these issues, we propose a\nFrequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical\nimage diffusion models. By focusing on reconstruction errors within a specific\nmid-frequency range and excluding both high-frequency (difficult to\nreconstruct) and low-frequency (less informative) regions, our\nfrequency-selective approach mitigates the confounding factor of inherent image\ndifficulty. Specifically, we analyze the reverse diffusion process, obtain the\nmid-frequency reconstruction error, and compute the structural similarity index\nscore between the reconstructed and original images. Membership is determined\nby comparing this score to a threshold. Experiments on several medical image\ndatasets demonstrate that our FCRE method outperforms existing MIA methods.", "AI": {"tldr": "论文提出了一种新的会员推断攻击（MIA）方法，即频率校准重建误差（FCRE），专门用于医学图像扩散模型，该方法通过专注于中频重建误差，提高了隐私风险的量化能力。实验表明该方法优于现有的MIA方法。", "motivation": "随着扩散模型在图像生成，尤其是在医学成像等敏感领域的应用增加，隐私问题变得越来越显著。会员推断攻击（MIA）作为一种方法，被用来确定特定的图像是否被用来训练扩散模型，从而量化隐私风险。然而，在将这些方法应用于医学图像时遇到了挑战。现有方法通常依赖于扩散重建误差，其中成员图像的重建误差预期低于非成员图像。然而，重建误差受图像本身难度的影响，而且扩散模型在高频率细节重建方面存在困难。为此，作者提出了针对医学图像扩散模型的频率校准重建误差（FCRE）方法来解决这些问题。", "method": "作者提出了一种针对医学图像扩散模型的频率校准重建误差（FCRE）方法，通过专注于特定中频段的重建误差，排除高频率（难以重建）和低频率（不具信息量）的区域，来减轻图像内在难度影响。具体地说，他们分析了逆扩散过程，获得了中频重建误差，并计算了重建图像和原始图像之间的结构相似性指数得分。", "result": "实验结果表明，作者提出的FCRE方法在几个医学图像数据集上优于现有的MIA方法，其效果在相同设定下得以验证。", "conclusion": "作者通过引入频率校准重建误差（FCRE）方法，解决了现有MIA方法在医学图像扩散模型中应用时遇到的问题，证明了该方法在提升隐私风险量化能力方面具有优势。"}}
{"id": "2506.15301", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15301", "abs": "https://arxiv.org/abs/2506.15301", "authors": ["Shrestha Ghosh", "Moritz Schneider", "Carina Reinicke", "Carsten Eickhoff"], "title": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment", "comment": null, "summary": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.", "AI": {"tldr": "本文是关于使用LLM（大型语言模型）在临床试验患者匹配中的应用的第一项综述性研究。", "motivation": "虽然大型语言模型在通用领域表现出色，但在临床试验这样的关键领域其应用仍有限。该领域的自然语言处理任务可以从LLM的知识汇总和推理能力中受益。", "method": "分析最近的论文摘要，提取其主要信息。", "result": "本论文是第一个全面分析文献中临床试验患者匹配任务并考量新兴LLM方法的研究。它检查了现有基准、方法和评估框架，指出了采用LLM技术面临的挑战。", "conclusion": "本综述为未来在临床试验患者匹配任务中采用LLM技术开辟了新方向，并指出了该领域的一些挑战。"}}
{"id": "2506.14934", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.14934", "abs": "https://arxiv.org/abs/2506.14934", "authors": ["Md Abrar Jahin", "Shahriar Soudeep", "Arian Rahman Aditta", "M. F. Mridha", "Nafiz Fahad", "Md. Jakir Hossen"], "title": "Vision Transformers for End-to-End Quark-Gluon Jet Classification from Calorimeter Images", "comment": "Accepted in Third International Workshop on Generalizing from Limited\n  Resources in the Open World Workshop at International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025", "summary": "Distinguishing between quark- and gluon-initiated jets is a critical and\nchallenging task in high-energy physics, pivotal for improving new physics\nsearches and precision measurements at the Large Hadron Collider. While deep\nlearning, particularly Convolutional Neural Networks (CNNs), has advanced jet\ntagging using image-based representations, the potential of Vision Transformer\n(ViT) architectures, renowned for modeling global contextual information,\nremains largely underexplored for direct calorimeter image analysis, especially\nunder realistic detector and pileup conditions. This paper presents a\nsystematic evaluation of ViTs and ViT-CNN hybrid models for quark-gluon jet\nclassification using simulated 2012 CMS Open Data. We construct multi-channel\njet-view images from detector-level energy deposits (ECAL, HCAL) and\nreconstructed tracks, enabling an end-to-end learning approach. Our\ncomprehensive benchmarking demonstrates that ViT-based models, notably\nViT+MaxViT and ViT+ConvNeXt hybrids, consistently outperform established CNN\nbaselines in F1-score, ROC-AUC, and accuracy, highlighting the advantage of\ncapturing long-range spatial correlations within jet substructure. This work\nestablishes the first systematic framework and robust performance baselines for\napplying ViT architectures to calorimeter image-based jet classification using\npublic collider data, alongside a structured dataset suitable for further deep\nlearning research in this domain.", "AI": {"tldr": "该论文评估了Vision Transformer (ViT)及与其结合的CNN混合模型对喷注分类的性能，实验证明ViT模型由于能够更好地捕获喷注子结构的长距离空间关联性，其优于现有的CNN基线模型。", "motivation": "区别夸克和胶子引发的喷注对于大型强子对撞机中的新物理学搜索和精确测量来说至关重要但充满挑战。虽然深度学习，特别是使用基于图像表示方式的卷积神经网络，已经推动了喷注标记的进步，但ViT架构在直接分析计数器图像下特别是在模拟的检测器和堆积条件下的潜力尚未得到充分探索。", "method": "该论文通过使用Vision Transformer (ViT)及其与Convolutional Neural Networks (CNN)的混合模型，对夸克-胶子喷注分类进行了系统评估。采用模拟的2012年CMS开放数据集，构建了来自探测器级别能量沉积（电磁量能器、强子量能器）和重建轨迹的多通道喷注视图图像，实现了端到端学习方法。", "result": "论文表明基于ViT的模型，特别是ViT+MaxViT和ViT+ConvNeXt混合模型，在F1得分、ROC-AUC和准确性方面始终优于现有的CNN基线模型。这表明ViT模型在捕捉喷注子结构中的长距离空间相关性方面具有显著优势。", "conclusion": "该工作为使用公共碰撞数据应用ViT架构对计数器图像基于的喷注分类建立了第一个系统框架，并建立了稳健的性能基准。同时也构建了适合该领域进一步深度学习研究的结构化数据集。"}}
{"id": "2506.15304", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15304", "abs": "https://arxiv.org/abs/2506.15304", "authors": ["Negar Foroutan", "Jakhongir Saydaliev", "Ye Eun Kim", "Antoine Bosselut"], "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language Identification", "comment": "Submitted to EMNLP", "summary": "Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.", "AI": {"tldr": "本文提出了一个监督对比学习方法来解决低资源语言在语言识别任务上的性能问题，特别是在外域数据上的表现。", "motivation": "尽管许多研究集中在收集多样的训练数据来改进语言识别模型的表现，低资源语言的表现仍然不理想，通常局限于单一领域数据。本文旨在解决这类语言的类别不平衡和偏置问题。", "method": "我们提出了一种新的监督对比学习（SCL）方法，以学习低资源语言的领域不变表示。", "result": "通过广泛的分析，我们展示了我们的方法提高了低资源语言在外域数据上的语言识别性能，提升了3.2%。", "conclusion": "我们的方法有效地提升了低资源语言在语言识别模型中的表现，特别是在外域数据上的性能有所提升。"}}
{"id": "2506.14980", "categories": ["cs.CV", "cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2506.14980", "abs": "https://arxiv.org/abs/2506.14980", "authors": ["Ziteng Li", "Malte Kuhlmann", "Ilana Nisky", "Nicolás Navarro-Guerrero"], "title": "Advances in Compliance Detection: Novel Models Using Vision-Based Tactile Sensors", "comment": "Accepted in the IEEE International Conference on Development and\n  Learning (ICDL). The paper contains 8 pages and 7 figures", "summary": "Compliance is a critical parameter for describing objects in engineering,\nagriculture, and biomedical applications. Traditional compliance detection\nmethods are limited by their lack of portability and scalability, rely on\nspecialized, often expensive equipment, and are unsuitable for robotic\napplications. Moreover, existing neural network-based approaches using\nvision-based tactile sensors still suffer from insufficient prediction\naccuracy. In this paper, we propose two models based on Long-term Recurrent\nConvolutional Networks (LRCNs) and Transformer architectures that leverage RGB\ntactile images and other information captured by the vision-based sensor\nGelSight to predict compliance metrics accurately. We validate the performance\nof these models using multiple metrics and demonstrate their effectiveness in\naccurately estimating compliance. The proposed models exhibit significant\nperformance improvement over the baseline. Additionally, we investigated the\ncorrelation between sensor compliance and object compliance estimation, which\nrevealed that objects that are harder than the sensor are more challenging to\nestimate.", "AI": {"tldr": "本研究提出两种基于长短期递归卷积网络（LRCNs）和Transformer架构的模型，结合RGB触觉图像和其他GelSight传感器信息来准确预测合规性指标，模型在多个评估指标上表现优于基线模型，特别是对于比传感器更硬的对象的预测更具挑战性。", "motivation": "传统的合规性检测方法存在便携性和可扩展性问题，且依赖昂贵的专用设备；现有基于视觉的触觉传感器的神经网络方法预测精度不足。", "method": "研究设计了两种模型，分别基于LRCNs和Transformer架构，利用RGB触觉图像及其他GelSight传感器捕获的信息进行合规性指标预测。", "result": "实验结果表明，这两个模型在多个评估指标上的表现优于基线模型。", "conclusion": "提出的模型对于预测合规性指标表现出优越的性能，特别是在探索传感器与对象合规性之间的相关性时发现，对于比传感器硬度大的对象，其合规性更难以预测。"}}
{"id": "2506.15339", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15339", "abs": "https://arxiv.org/abs/2506.15339", "authors": ["Camila Zurdo Tagliabue", "Heloisa Oss Boll", "Aykut Erdem", "Erkut Erdem", "Iacer Calixto"], "title": "DeVisE: Behavioral Testing of Medical Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.", "AI": {"tldr": "研究通过DeVisE框架评估五种LLMs在医疗决策中的表现，并展示了零样本模型与微调模型的不同特点。", "motivation": "大型语言模型在临床决策支持中的应用日益增加，但目前的评估方法往往无法区分真正意义上的医疗推理与表面模式。", "method": "本研究引入了DeVisE（人口统计和生命体征评估）行为测试框架，用于检测细微的临床理解。数据集由MIMIC-IV的ICU出院总结构成，包括现实世界数据和基于模板的合成数据，后者通过单一变量反事实场景专注于人口统计（年龄、性别、种族）和生命体征属性。研究评估了五种LLMs，包括通用型和医疗微调型模型，在零样本和微调环境下进行测试。评估模型行为通过输入层面的敏感性（反事实如何改变笔记出现概率），以及下游推理（它们如何影响预测住院时长）进行。", "result": "研究结果显示，零样本模型在反事实推理模式上表现得更为连贯，而微调模型则更稳定但对临床有意义的变化响应较小。值得注意的是，人口统计因素细微但一致地影响模型输出，强调了公平评估的重要性。", "conclusion": "这项工作强调了行为测试在揭示临床LLMs推理策略和设计更安全、透明的医疗AI系统方面的实用性。"}}
{"id": "2506.15010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15010", "abs": "https://arxiv.org/abs/2506.15010", "authors": ["Yijun Lin", "Yao-Yi Chiang"], "title": "Hyper-Local Deformable Transformers for Text Spotting on Historical Maps", "comment": "Published in KDD2024", "summary": "Text on historical maps contains valuable information providing georeferenced\nhistorical, political, and cultural contexts. However, text extraction from\nhistorical maps is challenging due to the lack of (1) effective methods and (2)\ntraining data. Previous approaches use ad-hoc steps tailored to only specific\nmap styles. Recent machine learning-based text spotters (e.g., for scene\nimages) have the potential to solve these challenges because of their\nflexibility in supporting various types of text instances. However, these\nmethods remain challenges in extracting precise image features for predicting\nevery sub-component (boundary points and characters) in a text instance. This\nis critical because map text can be lengthy and highly rotated with complex\nbackgrounds, posing difficulties in detecting relevant image features from a\nrough text region. This paper proposes PALETTE, an end-to-end text spotter for\nscanned historical maps of a wide variety. PALETTE introduces a novel\nhyper-local sampling module to explicitly learn localized image features around\nthe target boundary points and characters of a text instance for detection and\nrecognition. PALETTE also enables hyper-local positional embeddings to learn\nspatial interactions between boundary points and characters within and across\ntext instances. In addition, this paper presents a novel approach to\nautomatically generate synthetic map images, SynthMap+, for training text\nspotters for historical maps. The experiment shows that PALETTE with SynthMap+\noutperforms SOTA text spotters on two new benchmark datasets of historical\nmaps, particularly for long and angled text. We have deployed PALETTE with\nSynthMap+ to process over 60,000 maps in the David Rumsey Historical Map\ncollection and generated over 100 million text labels to support map searching.\nThe project is released at\nhttps://github.com/kartta-foundation/mapkurator-palette-doc.", "AI": {"tldr": "PALETTE is developed to enhance text spotting in historical maps, addressing the issues of varied map styles and insufficient training data, and demonstrates superior performance on historical map datasets.", "motivation": "To address the challenges of text extraction from historical maps, including the lack of effective methods and training data, while improving precision in detecting text in complex, varied map styles.", "method": "PALETTE, a novel text spotting model that introduces a hyper-local sampling module for learning localized image features and positional embeddings for spatial interactions.", "result": "PALETTE with SynthMap+ outperforms state-of-the-art text spotters on new historical map benchmark datasets and processes a large number of maps.", "conclusion": "PALETTE effectively spots text in historical maps with high precision, especially for long and angled text, facilitating the processing of extensive map collections and enabling more effective map searching."}}
{"id": "2506.15355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15355", "abs": "https://arxiv.org/abs/2506.15355", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Sriparna Saha"], "title": "SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture", "comment": "ACL 2025 Findings", "summary": "Language Models (LMs) are indispensable tools shaping modern workflows, but\ntheir global effectiveness depends on understanding local socio-cultural\ncontexts. To address this, we introduce SANSKRITI, a benchmark designed to\nevaluate language models' comprehension of India's rich cultural diversity.\nComprising 21,853 meticulously curated question-answer pairs spanning 28 states\nand 8 union territories, SANSKRITI is the largest dataset for testing Indian\ncultural knowledge. It covers sixteen key attributes of Indian culture: rituals\nand ceremonies, history, tourism, cuisine, dance and music, costume, language,\nart, festivals, religion, medicine, transport, sports, nightlife, and\npersonalities, providing a comprehensive representation of India's cultural\ntapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic\nLanguage Models (ILMs), and Small Language Models (SLMs), revealing significant\ndisparities in their ability to handle culturally nuanced queries, with many\nmodels struggling in region-specific contexts. By offering an extensive,\nculturally rich, and diverse dataset, SANSKRITI sets a new standard for\nassessing and improving the cultural understanding of LMs.", "AI": {"tldr": "论文介绍了一个名为SANSKRITI的基准测试，用于评估语言模型对印度丰富文化多样性理解的能力。该基准包含21,853个精心策划的问题和答案对，涵盖印度28个州和8个中央直辖地区的文化知识。SANSKRITI旨在解决语言模型在处理文化细微差别方面的不足，并为评估和改进语言模型的文化理解力设定了新标准。", "motivation": "动机在于解决语言模型对不同地区文化背景理解不足的问题，尤其是在处理文化细微差别的能力方面。印度文化非常丰富且多样化，通过制定这个基准测试，可以更好地测试和改进语言模型对这种复杂性的理解和应用能力。", "method": "该研究通过设计SANSKRITI基准测试来评估语言模型，特别是大型语言模型、印地语语言模型和小型语言模型对印度文化多样性的理解情况。SANSKRITI包括16个关键的文化属性，涵盖了印度文化和生活方式的不同方面。", "result": "研究表明，不同规模的语言模型在处理文化细腻的查询上存在显著差距，尤其在处理地区特定情境下表现较差，这表明当前的语言模型在文化理解方面还有很大的提升空间。", "conclusion": "SANSKRITI提供了一个大规模、文化丰富的数据集，用于评估语言模型对印度文化理解的能力。此基准测试将助力于进一步提升语言模型在处理文化细微差别上的性能。"}}
{"id": "2506.15033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15033", "abs": "https://arxiv.org/abs/2506.15033", "authors": ["Gary Song Yan", "Yusen Zhang", "Jinyu Zhao", "Hao Zhang", "Zhangping Yang", "Guanye Xiong", "Yanfei Liu", "Tao Zhang", "Yujie He", "Siyuan Tian", "Yao Gou", "Min Li"], "title": "Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?", "comment": null, "summary": "In this pioneering study, we introduce StyleWallfacer, a groundbreaking\nunified training and inference framework, which not only addresses various\nissues encountered in the style transfer process of traditional methods but\nalso unifies the framework for different tasks. This framework is designed to\nrevolutionize the field by enabling artist level style transfer and text driven\nstylization. First, we propose a semantic-based style injection method that\nuses BLIP to generate text descriptions strictly aligned with the semantics of\nthe style image in CLIP space. By leveraging a large language model to remove\nstyle-related descriptions from these descriptions, we create a semantic gap.\nThis gap is then used to fine-tune the model, enabling efficient and drift-free\ninjection of style knowledge. Second, we propose a data augmentation strategy\nbased on human feedback, incorporating high-quality samples generated early in\nthe fine-tuning process into the training set to facilitate progressive\nlearning and significantly reduce its overfitting. Finally, we design a\ntraining-free triple diffusion process using the fine-tuned model, which\nmanipulates the features of self-attention layers in a manner similar to the\ncross-attention mechanism. Specifically, in the generation process, the key and\nvalue of the content-related process are replaced with those of the\nstyle-related process to inject style while maintaining text control over the\nmodel. We also introduce query preservation to mitigate disruptions to the\noriginal content. Under such a design, we have achieved high-quality\nimage-driven style transfer and text-driven stylization, delivering\nartist-level style transfer results while preserving the original image\ncontent. Moreover, we achieve image color editing during the style transfer\nprocess for the first time.", "AI": {"tldr": "研究介绍了一个创新的统一风格转移框架StyleWallfacer，利用多项策略实现了高质量的风格转移，保持原图内容，并首次实现了在风格转移过程中的颜色编辑。", "motivation": "为了克服传统风格转移方法中存在的种种问题，并统一不同任务的框架，该研究引入了一个革命性的统一训练和推理框架。旨在实现艺术家级别的风格转移和基于文本的样式化。", "method": "论文中介绍了StyleWallfacer框架，该框架包含三个主要部分：基于语义的风格注入方法、基于人类反馈的数据增强策略、以及无训练三扩散过程。首先，提出了一种基于语义的风格注入方法，通过利用BLIP为其样式图像生成语义描述，并使用大语言模型移除与风格相关的描述从而创建语义差距，以此细化模型。其次，数据增强策略基于人类反馈，将早期训练样本纳入训练集以促进渐进学习。最后，设计了一个无训练的三扩散过程，使用细调后的模型对自注意力层进行特征变换，以实现风格注入同时保持文本控制。此外，还引入查询保持以减少对原始内容的干扰。", "result": "该框架在高质量的图像驱动风格转移和文本驱动风格化中取得了艺术家级别的效果，同时保持了原图内容，实现了在风格迁移过程中的图像色彩编辑。", "conclusion": "StyleWallfacer框架通过一系列创新方法，如基于语义的风格注入、数据增强策略和无训练的三扩散过程，成功实现了高质量的风格转移，并保持了原图内容的完整性，同时还实现了在风格转移过程中的颜色编辑，实现了一次技术上的突破。"}}
{"id": "2506.15372", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15372", "abs": "https://arxiv.org/abs/2506.15372", "authors": ["Raghvendra Kumar", "S. A. Mohammed Salman", "Aryan Sahu", "Tridib Nandi", "Pragathi Y. P.", "Sriparna Saha", "Jose G. Moreno"], "title": "COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation", "comment": "ACL 2025 MAINs", "summary": "Despite progress in comment-aware multimodal and multilingual summarization\nfor English and Chinese, research in Indian languages remains limited. This\nstudy addresses this gap by introducing COSMMIC, a pioneering comment-sensitive\nmultimodal, multilingual dataset featuring nine major Indian languages. COSMMIC\ncomprises 4,959 article-image pairs and 24,484 reader comments, with\nground-truth summaries available in all included languages. Our approach\nenhances summaries by integrating reader insights and feedback. We explore\nsummarization and headline generation across four configurations: (1) using\narticle text alone, (2) incorporating user comments, (3) utilizing images, and\n(4) combining text, comments, and images. To assess the dataset's\neffectiveness, we employ state-of-the-art language models such as LLama3 and\nGPT-4. We conduct a comprehensive study to evaluate different component\ncombinations, including identifying supportive comments, filtering out noise\nusing a dedicated comment classifier using IndicBERT, and extracting valuable\ninsights from images with a multilingual CLIP-based classifier. This helps\ndetermine the most effective configurations for natural language generation\n(NLG) tasks. Unlike many existing datasets that are either text-only or lack\nuser comments in multimodal settings, COSMMIC uniquely integrates text, images,\nand user feedback. This holistic approach bridges gaps in Indian language\nresources, advancing NLP research and fostering inclusivity.", "AI": {"tldr": "本研究通过介绍COSMMIC数据集解决了印度语言在评论意识下的多媒体和多语言摘要生成研究的空白，此数据集包含文章文本、用户评论和图片信息，并进行了基于LLama3和GPT-4等语言模型的评估以找到最有效的配置。", "motivation": "该研究的动机在于填补印度语言在评论相关的多媒体和多语言摘要生成研究方面的空白。现有数据集大多专注于英文和中文，而在处理其他语言，特别是印度各类语言的方法和发展还相当有限。COSMMIC旨在通过构建一个具有代表性的数据集来弥补这一空白，促进NLP领域的研究，并增进其包容性。", "method": "本文介绍并使用了COSMMIC数据集，这是一个专为九种主要印度语言设计的、带有评论的多语言多媒体数据集。数据集包含了文章-图片对以及读者评论。研究中探索了四种不同的摘要生成配置：仅使用文章文本，整合用户评论，利用图片信息以及结合文本、评论和图片的综合方法。为了评估数据集的有效性，研究采用了最先进的语言模型如LLama3和GPT-4。同时，研究中还使用了IndicBERT进行评论分类以筛选出有价值的评论，并利用多语言CLIP分类器从图片中提取有意义的信息。", "result": "通过结合文章文本、用户评论和图片信息，本文的方法为印度语言的NLP任务提供了新视角，并在不同模型和配置下进行了评估，证明了该方法的可行性和高效性。", "conclusion": "COSMMIC数据集填补了印度语言资源在NLP研究中的空白，并提供了处理用户评论和多媒体数据的新方式。其综合策略促进了NLP领域的发展，并且对于多语言、多媒体的自然语言生成任务具有重要贡献。"}}
{"id": "2506.15078", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.15078", "abs": "https://arxiv.org/abs/2506.15078", "authors": ["Xianghong Fang", "Litao Guo", "Hengchao Chen", "Yuxuan Zhang", "XiaofanXia", "Dingjie Song", "Yexin Liu", "Hao Wang", "Harry Yang", "Yuan Yuan", "Qiang Sun"], "title": "Enhancing Vector Quantization with Distributional Matching: A Theoretical and Empirical Study", "comment": null, "summary": "The success of autoregressive models largely depends on the effectiveness of\nvector quantization, a technique that discretizes continuous features by\nmapping them to the nearest code vectors within a learnable codebook. Two\ncritical issues in existing vector quantization methods are training\ninstability and codebook collapse. Training instability arises from the\ngradient discrepancy introduced by the straight-through estimator, especially\nin the presence of significant quantization errors, while codebook collapse\noccurs when only a small subset of code vectors are utilized during training. A\ncloser examination of these issues reveals that they are primarily driven by a\nmismatch between the distributions of the features and code vectors, leading to\nunrepresentative code vectors and significant data information loss during\ncompression. To address this, we employ the Wasserstein distance to align these\ntwo distributions, achieving near 100\\% codebook utilization and significantly\nreducing the quantization error. Both empirical and theoretical analyses\nvalidate the effectiveness of the proposed approach.", "AI": {"tldr": "本文提出了一种新的向量化方法，采用Wasserstein距离来解决训练不稳定性和码本崩溃问题，提高了码本利用率，减小了量化误差。", "motivation": "本文的动机在于解决现有向量化技术中的两个关键问题，即训练不稳定性和码本崩溃，这些问题主要由特征分布与码本向量分布之间的不匹配引起。", "method": "本文提出使用Wasserstein距离来对齐特征分布和代码向量分布，以解决训练不稳定性和码本崩溃问题，这有助于实现接近100%的码本利用率和显著降低量化误差。", "result": "通过对提出的方案进行实证和理论分析，结果显示该方法有效解决了训练不稳定性和码本崩溃的问题。", "conclusion": "通过引入Wasserstein距离，论文成功改善了向量化过程中的数据信息保留，验证了该方法的有效性。"}}
{"id": "2506.15415", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.15415", "abs": "https://arxiv.org/abs/2506.15415", "authors": ["Stanley Ngugi"], "title": "Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning", "comment": "11 pages, 3 figures, 2 tables. Research on parameter-efficient\n  fine-tuning (PEFT) for low-resource languages (Swahili). Investigates\n  cross-lingual lexical alignment in Lugha-Llama using LoRA and contrastive\n  learning", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir performance in low-resource languages (LRLs), such as Swahili, often lags\ndue to data scarcity and underrepresentation in pre-training. A key challenge\nis achieving robust cross-lingual lexical alignment, crucial for tasks like\ntranslation and cross-lingual information retrieval. This paper introduces\nTargeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.\nWe first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits\nstrong, near-perfect lexical alignment for Swahili-English word pairs in its\nearly internal layers (specifically Layer 2, with ~0.99998 average cosine\nsimilarity based on a pilot study), a capability not fully reflected in its\nfinal output representations (baseline ~0.32 similarity on our evaluation set).\nTLI leverages this insight by using Low-Rank Adaptation (LoRA) and a\ncontrastive learning objective to fine-tune the model, specifically targeting\nembeddings from this empirically identified optimal early layer. Our\nexperiments show that TLI significantly improves the output-level lexical\nalignment for 623 trained Swahili-English word pairs, increasing average cosine\nsimilarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More\nimportantly, these improvements generalize remarkably well to 63 unseen control\nword pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17\nx 10^-27). These findings suggest TLI enhances the model's ability to preserve\nand propagate its inherent early-layer cross-lingual knowledge, offering a\nparameter-efficient and effective strategy for improving lexical alignment in\nLRL-focused LLMs.", "AI": {"tldr": "本研究针对资源不足语言在大语言模型中词汇对齐不佳的问题，提出了一种名为Targeted Lexical Injection (TLI) 的新方法，实验表明该方法显著提升了Swahili-English词汇对的对齐效果，而且对于未见过的单词对也有很好的泛化效果。", "motivation": "大语言模型在资源不足的语言上的表现较差，主要是由于这些语言的数据稀缺和预训练中的代表性不足所导致。研究旨在通过改进词汇对齐来提升这些模型在诸如翻译及跨语言信息检索等任务上的性能。", "method": "本研究提出了一种名为Targeted Lexical Injection (TLI) 的新方法，利用Low-Rank Adaptation (LoRA) 和对比学习目标，专注于Lugha-Llama-8B-wura模型的第2层词汇对齐优化，该层在Swahili-English词汇对的对齐上表现出色。", "result": "实验表明，通过TLI方法，623组训练过的Swahili-English词汇对的相似度从0.3211提升到了0.4113，增加了28.08%。同时，对于63个未见过的单词对也表现出良好的泛化能力，相似度从0.3143提升到了0.4033，增幅为28.32%。", "conclusion": "TLI方法能够有效提升目标语言对的词汇对齐性能，尤其体现在从模型内部早期层向最终输出表示的传播方面。这一方法为资源不足语言的模型优化提供了一种参数有效和高效的方法。"}}
{"id": "2506.15153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15153", "abs": "https://arxiv.org/abs/2506.15153", "authors": ["Yufei Liu", "Haoke Xiao", "Jiaxing Chai", "Yongcun Zhang", "Rong Wang", "Zijie Meng", "Zhiming Luo"], "title": "SynPo: Boosting Training-Free Few-Shot Medical Segmentation via High-Quality Negative Prompts", "comment": null, "summary": "The advent of Large Vision Models (LVMs) offers new opportunities for\nfew-shot medical image segmentation. However, existing training-free methods\nbased on LVMs fail to effectively utilize negative prompts, leading to poor\nperformance on low-contrast medical images. To address this issue, we propose\nSynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core\ninsight: improving the quality of negative prompts. To select point prompts in\na more reliable confidence map, we design a novel Confidence Map Synergy Module\nby combining the strengths of DINOv2 and SAM. Based on the confidence map, we\nselect the top-k pixels as the positive points set and choose the negative\npoints set using a Gaussian distribution, followed by independent K-means\nclustering for both sets. Then, these selected points are leveraged as\nhigh-quality prompts for SAM to get the segmentation results. Extensive\nexperiments demonstrate that SynPo achieves performance comparable to\nstate-of-the-art training-based few-shot methods.", "AI": {"tldr": "针对现有基于LVM的零训练方法在低对比度医学图像上表现不佳的问题，提出了一种改进负提示质量的方法SynPo，在零训练下实现了与有训练的少样本方法相当的性能。", "motivation": "现有的基于大视觉模型（LVMs）的零训练方法未能有效利用负提示，导致在低对比度的医学图像上表现不佳。因此，提出了一种改进负提示质量的新方法。", "method": "通过结合DINOv2和SAM的优势设计了一个新的置信图协同模块，来更可靠地选择点提示。基于置信图，选择前k个像素作为正点集，并使用高斯分布选择负点集，然后分别对两个集合进行独立的K-means聚类。最终将这些选定的点作为高质量提示用于SAM进行分割。", "result": "大量的实验表明，SynPo的性能与最先进的训练基础上的少样本方法相当。", "conclusion": "该研究提出的方法SynPo在零训练环境下通过提高负提示的质量，显著增强了对低对比度医疗图像的分割效果，达到了与训练方法相近的性能水平。"}}
{"id": "2506.15425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15425", "abs": "https://arxiv.org/abs/2506.15425", "authors": ["Xingjian Tao", "Yiwei Wang", "Yujun Cai", "Zhicheng Yang", "Jing Tang"], "title": "Understanding GUI Agent Localization Biases through Logit Sharpness", "comment": null, "summary": "Multimodal large language models (MLLMs) have enabled GUI agents to interact\nwith operating systems by grounding language into spatial actions. Despite\ntheir promising performance, these models frequently exhibit\nhallucinations-systematic localization errors that compromise reliability. We\npropose a fine-grained evaluation framework that categorizes model predictions\ninto four distinct types, revealing nuanced failure modes beyond traditional\naccuracy metrics. To better quantify model uncertainty, we introduce the Peak\nSharpness Score (PSS), a metric that evaluates the alignment between semantic\ncontinuity and logits distribution in coordinate prediction. Building on this\ninsight, we further propose Context-Aware Cropping, a training-free technique\nthat improves model performance by adaptively refining input context. Extensive\nexperiments demonstrate that our framework and methods provide actionable\ninsights and enhance the interpretability and robustness of GUI agent behavior.", "AI": {"tldr": "文章提出了一种细粒度评估框架和Context-Aware Cropping技术，用于提升多模态大语言模型在GUI交互中的性能和可靠性。", "motivation": "针对多模态大语言模型在GUI操作中常出现的系统性定位错误问题，此研究旨在提供更精细的评估方法和提高模型可靠性的策略。", "method": "我们提出了一种细粒度的评估框架，将模型预测分类为四种不同的类型，并引入Peak Sharpness Score（PSS）来更好地量化模型的不确定性。此外，我们提出了一种无需训练的技术——Context-Aware Cropping，通过自适应地调整输入上下文来提高模型性能。", "result": "实验广泛展示了我们的框架和方法提供了可操作的见解，增强了GUI代理行为的解释性和鲁棒性。", "conclusion": "我们的工作不仅揭示了多模态语言模型在GUI操作中的失败机制，还提供了一种提高其可靠性的有效途径。"}}
{"id": "2506.15160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15160", "abs": "https://arxiv.org/abs/2506.15160", "authors": ["Jiaqi Shi", "Jin Xiao", "Xiaoguang Hu", "Boyang Song", "Hao Jiang", "Tianyou Chen", "Baochang Zhang"], "title": "Enhancing point cloud analysis via neighbor aggregation correction based on cross-stage structure correlation", "comment": "17 papes, 7 figures", "summary": "Point cloud analysis is the cornerstone of many downstream tasks, among which\naggregating local structures is the basis for understanding point cloud data.\nWhile numerous works aggregate neighbor using three-dimensional relative\ncoordinates, there are irrelevant point interference and feature hierarchy gap\nproblems due to the limitation of local coordinates. Although some works\naddress this limitation by refining spatial description though explicit\nmodeling of cross-stage structure, these enhancement methods based on direct\ngeometric structure encoding have problems of high computational overhead and\nnoise sensitivity. To overcome these problems, we propose the Point\nDistribution Set Abstraction module (PDSA) that utilizes the correlation in the\nhigh-dimensional space to correct the feature distribution during aggregation,\nwhich improves the computational efficiency and robustness. PDSA distinguishes\nthe point correlation based on a lightweight cross-stage structural descriptor,\nand enhances structural homogeneity by reducing the variance of the neighbor\nfeature matrix and increasing classes separability though long-distance\nmodeling. Additionally, we introducing a key point mechanism to optimize the\ncomputational overhead. The experimental result on semantic segmentation and\nclassification tasks based on different baselines verify the generalization of\nthe method we proposed, and achieve significant performance improvement with\nless parameter cost. The corresponding ablation and visualization results\ndemonstrate the effectiveness and rationality of our method. The code and\ntraining weight is available at: https://github.com/AGENT9717/PointDistribution", "AI": {"tldr": "提出了一种用于点云数据处理的新模块 PDSA，它通过高维空间中的相关性修正来进行特征聚合，显著提高了计算效率和鲁棒性，并在不同任务上展示出了优良的表现。", "motivation": "现有的基于三维相对坐标聚合同邻点的方法存在无关点干扰和特征层次差距问题。虽然有些工作通过建模跨阶段结构来改进空间描述，但这些基于直接几何结构编码的方法存在高计算开销和噪声敏感性的问题。", "method": "提出了一种名为点分布集抽象模块（PDSA）的新方法，该方法利用高维空间中的相关性来修正聚合过程中的特征分布，从而提高计算效率和鲁棒性。PDSA 通过基于轻量级的跨阶段结构描述符，增强点关联，并通过消除特征矩阵的方差和增强类别可分性来改进结构同质性。此外，还引入了关键词机制来优化计算开销。", "result": "在基于不同基线的语义分割和分类任务中，实验结果验证了所提出方法的泛化能力，并且在参数消耗较少的情况下性能显著提高。", "conclusion": "PDSA 的有效性和合理性通过消融实验和可视化结果得到了验证。"}}
{"id": "2506.15451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15451", "abs": "https://arxiv.org/abs/2506.15451", "authors": ["Zhouhong Gu", "Xiaoxuan Zhu", "Yin Cai", "Hao Shen", "Xingzhou Chen", "Qingyi Wang", "Jialin Li", "Xiaoran Shi", "Haoran Guo", "Wenxuan Huang", "Hongwei Feng", "Yanghua Xiao", "Zheyu Ye", "Yao Hu", "Shaosheng Cao"], "title": "AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need", "comment": null, "summary": "Large language model based multi-agent systems have demonstrated significant\npotential in social simulation and complex task resolution domains. However,\ncurrent frameworks face critical challenges in system architecture design,\ncross-domain generalizability, and performance guarantees, particularly as task\ncomplexity and number of agents increases. We introduces AgentGroupChat-V2, a\nnovel framework addressing these challenges through three core innovations: (1)\na divide-and-conquer fully parallel architecture that decomposes user queries\ninto hierarchical task forest structures enabling dependency management and\ndistributed concurrent processing. (2) an adaptive collaboration engine that\ndynamically selects heterogeneous LLM combinations and interaction modes based\non task characteristics. (3) agent organization optimization strategies\ncombining divide-and-conquer approaches for efficient problem decomposition.\nExtensive experiments demonstrate AgentGroupChat-V2's superior performance\nacross diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best\nbaseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME\n(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance\nadvantages become increasingly pronounced with higher task difficulty,\nparticularly on Level 5 MATH problems where improvements exceed 11 percentage\npoints compared to state-of-the-art baselines. These results confirm that\nAgentGroupChat-V2 provides a comprehensive solution for building efficient,\ngeneral-purpose LLM multi-agent systems with significant advantages in complex\nreasoning scenarios. Code is available at\nhttps://github.com/MikeGu721/AgentGroupChat-V2.", "AI": {"tldr": "AgentGroupChat-V2是一个克服现有框架在系统架构设计、跨域通用性和性能保证方面挑战的新型框架，特别适用于任务复杂度和代理数增加的场景。它通过三种核心创新机制表现出了优异的性能：全并行架构、自适应协作引擎、以及高效的代理组织优化策略。", "motivation": "为了克服基于大型语言模型的多代理系统在系统架构设计、跨域通用性和性能保证方面的挑战，尤其是在任务复杂性和代理数量增加时出现的问题，引入了AgentGroupChat-V2框架。", "method": "AgentGroupChat-V2通过三种核心设计实现此目的：(1)全并行架构，用于分解用户查询并管理依赖关系；(2)自适应协作引擎，根据任务特性动态选择不同类型的大型语言模型组合及其互动方式；(3)代理组织优化策略，以提高问题分解效率。", "result": "实验表明，AgentGroupChat-V2在不同领域表现出优异性能：在GSM8K上准确率达到91.50%（超过最佳基线5.6个百分点），在AIME比赛中准确率达到30.4%，在HumanEval上通过率达到79.20%。随着任务难度增加，其性能优势更为明显，特别是在Level 5 MATH问题上，其改进超过了11个百分点。", "conclusion": "这些结果证实了AgentGroupChat-V2在构建高效、通用的多代理系统上提供了全面解决方案，特别是在复杂推理场景下有显著优势。代码可以在此项目中获取：https://github.com/MikeGu721/AgentGroupChat-V2。"}}
{"id": "2506.15166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15166", "abs": "https://arxiv.org/abs/2506.15166", "authors": ["Abdur Rahman", "Keerthiveena Balraj", "Manojkumar Ramteke", "Anurag Singh Rathore"], "title": "Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography", "comment": "Version of record published in Discover Applied Sciences (Springer\n  Nature). The definitive article is available at\n  https://doi.org/10.1007/s42452-025-07055-5", "summary": "Recent advancements in diffusion probabilistic models (DPMs) have\nrevolutionized image processing, demonstrating significant potential in medical\napplications. Accurate segmentation of the left ventricle (LV) in\nechocardiograms is crucial for diagnostic procedures and necessary treatments.\nHowever, ultrasound images are notoriously noisy with low contrast and\nambiguous LV boundaries, thereby complicating the segmentation process. To\naddress these challenges, this paper introduces Echo-DND, a novel dual-noise\ndiffusion model specifically designed for this task. Echo-DND leverages a\nunique combination of Gaussian and Bernoulli noises. It also incorporates a\nmulti-scale fusion conditioning module to improve segmentation precision.\nFurthermore, it utilizes spatial coherence calibration to maintain spatial\nintegrity in segmentation masks. The model's performance was rigorously\nvalidated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations\ndemonstrate that the proposed framework outperforms existing SOTA models. It\nachieves high Dice scores of 0.962 and 0.939 on these datasets, respectively.\nThe proposed Echo-DND model establishes a new standard in echocardiogram\nsegmentation, and its architecture holds promise for broader applicability in\nother medical imaging tasks, potentially improving diagnostic accuracy across\nvarious medical domains. Project page: https://abdur75648.github.io/Echo-DND", "AI": {"tldr": "Echo-DND在提高超声图像中左心室分割精度方面取得了显著进步，其在两个基准数据集上的实验结果优于现有最先进的方法。", "motivation": "超声图像由于其噪声大、对比度低以及左心室边界模糊，给准确分割带来了极大的挑战。准确的左心室分割对诊断过程和必要的治疗至关重要。", "method": "Echo-DND采用了一种结合高斯噪声和伯努利噪声的独特方法，同时引入了多尺度融合条件模块和空间一致性校准机制，以提高对左心室的分割精度并保持空间完整性。", "result": "该论文提出了一种新颖的双噪声扩散模型Echo-DND，用于解决超声图像中左心室（LV）边界模糊和噪声大的问题。该模型结合了高斯噪声和伯努利噪声，并引入了多尺度融合条件模块和空间一致性校准机制，用以提高分割精度和保持空间完整性。实验结果表明，该方法在CAMUS和EchoNet-Dynamic数据集上表现优于现有SOTA模型，Dice系数分别为0.962和0.939，显示了其强大的分割性能。Echo-DND模型不仅在超声图像分割中建立了新标准，而且在其他医学影像任务中也具有广泛的适用前景。", "conclusion": "Echo-DND模型建立了超声心动图分割的新标准，并且其架构有潜力在更广泛的医学图像任务中应用，从而可能提高各类医学诊断的准确性。"}}
{"id": "2506.15455", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15455", "abs": "https://arxiv.org/abs/2506.15455", "authors": ["Xinnuo Xu", "Rachel Lawrence", "Kshitij Dubey", "Atharva Pandey", "Risa Ueno", "Fabian Falck", "Aditya V. Nori", "Rahul Sharma", "Amit Sharma", "Javier Gonzalez"], "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation", "comment": "ICML 2025", "summary": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.", "AI": {"tldr": "RE-IMAGINE框架用于评估大语言模型的真实推理能力，通过生成不可单纯依靠记忆解决的问题来测试其性能。", "motivation": "旨在区分大语言模型的高基准成绩是源于真正的推理能力还是训练集的统计记忆。", "method": "Structure", "result": "{\n  \"tldr\": \"RE-IMAGINE框架用于评估大语言模型的真实推理能力，通过生成不可单纯依靠记忆解决的问题来测试其性能。\",\n  \"motivation\": \"旨在区分大语言模型的高基准成绩是源于真正的推理能力还是训练集的统计记忆。\",\n  \"method\": \"基于因果关系的三个层次设计了RE-IMAGINE框架，并通过一个自动化管道产生不同层次的问题变化。\",\n  \"result\": \"在四个广泛使用的基准测试上评估了多个家族的大型语言模型，并观察到在问题变化时性能降低。\",\n  \"conclusion\": \"该框架揭示了大型语言模型过去成绩中的记忆依赖，并为不同推理层次的模型能力研究提供了方向。 \"\n}", "conclusion": "该框架揭示了大型语言模型过去成绩中的记忆依赖，并为不同推理层次的模型能力研究提供了方向。"}}
{"id": "2506.15180", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15180", "abs": "https://arxiv.org/abs/2506.15180", "authors": ["Ziling Huang", "Yidan Zhang", "Shin'ichi Satoh"], "title": "ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections", "comment": null, "summary": "Large-scale visual search engines are expected to solve a dual problem at\nonce: (i) locate every image that truly contains the object described by a\nsentence and (ii) identify the object's bounding box or exact pixels within\neach hit. Existing techniques address only one side of this challenge. Visual\ngrounding yields tight boxes and masks but rests on the unrealistic assumption\nthat the object is present in every test image, producing a flood of false\nalarms when applied to web-scale collections. Text-to-image retrieval excels at\nsifting through massive databases to rank relevant images, yet it stops at\nwhole-image matches and offers no fine-grained localization. We introduce\nReferring Search and Discovery (ReSeDis), the first task that unifies\ncorpus-level retrieval with pixel-level grounding. Given a free-form\ndescription, a ReSeDis model must decide whether the queried object appears in\neach image and, if so, where it is, returning bounding boxes or segmentation\nmasks. To enable rigorous study, we curate a benchmark in which every\ndescription maps uniquely to object instances scattered across a large, diverse\ncorpus, eliminating unintended matches. We further design a task-specific\nmetric that jointly scores retrieval recall and localization precision.\nFinally, we provide a straightforward zero-shot baseline using a frozen\nvision-language model, revealing significant headroom for future study. ReSeDis\noffers a realistic, end-to-end testbed for building the next generation of\nrobust and scalable multimodal search systems.", "AI": {"tldr": " YYS直指同时解决大规模视觉搜索引擎中的两个问题：找到完全包含查询对象的图片并找出对象的具体位置。YS提出Referring Search and Discovery (ReSeDis)任务，结合了语料级别检索与像素级别定位。研究构建了一个新的基准数据集，为该任务设计了一个特定指标，并提出了一种简单直接的零样本基线模型。YS表明这一任务为构建下一代鲁棒且可扩展的多模态搜索系统提供了现实的测试平台。", "motivation": " YYS为了克服现有技术只解决上述问题之一的局限，提出了结合语料级别检索与像素级别定位的任务ReSeDis。", "method": "욧지와 동일", "result": " YYS提出了ReSeDis任务，开发了一个基准数据集，设计了一种特定任务的指标，并展示了零样本基线模型的性能。", "conclusion": " YYS表明ReSeDis为构建鲁棒且可扩展的多模态搜索系统提供了现实的测试平台。"}}
{"id": "2506.15480", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.15480", "abs": "https://arxiv.org/abs/2506.15480", "authors": ["Hyunji Lee", "Seunghyun Yoon", "Yunjae Won", "Hanseok Oh", "Geewook Kim", "Trung Bui", "Franck Dernoncourt", "Elias Stengel-Eskin", "Mohit Bansal", "Minjoon Seo"], "title": "Context-Informed Grounding Supervision", "comment": null, "summary": "Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.", "AI": {"tldr": "本文提出了一种CINGS训练方式，用以提高大型语言模型在文本和视觉语言任务中的内容一致性，而不影响其泛化性能。", "motivation": "研究动机源于现有方法在推理过程中仅仅添加上下文并不能确保生成的响应是有根据的。希望通过CINGS方法加强模型输出与提供的外部上下文之间的一致性。", "method": "提出了一种称为上下文引导监督(CINGS)的方法，在模型训练时将相关上下文添加到响应的前面，但在计算损失时仅计算响应标记，而忽略上下文。", "result": "研究表明，经过CINGS训练的模型在文本和视觉语言两个领域都表现出了强烈的内容一致性。在文本领域，CINGS在11个寻求信息的数据集上超过了其他训练方法。在视觉语言领域，CINGS减少了四个基准的幻觉，同时保持了生成响应中的事实一致性。", "conclusion": "研究表明，在不影响一般性能的前提下，CINGS可以在文本和视觉语言任务中提供更强的反应定性和更少的幻觉现象，提升了模型对提供的外部内容的依赖和一致性。同时，CINGS导致了模型在先前知识和行为方面的变化。"}}
{"id": "2506.15200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.15200", "abs": "https://arxiv.org/abs/2506.15200", "authors": ["Alessio Negrini", "Simon Reiß"], "title": "Conquering the Retina: Bringing Visual in-Context Learning to OCT", "comment": null, "summary": "Recent advancements in medical image analysis have led to the development of\nhighly specialized models tailored to specific clinical tasks. These models\nhave demonstrated exceptional performance and remain a crucial research\ndirection. Yet, their applicability is limited to predefined tasks, requiring\nexpertise and extensive resources for development and adaptation. In contrast,\ngeneralist models offer a different form of utility: allowing medical\npractitioners to define tasks on the fly without the need for task-specific\nmodel development. In this work, we explore how to train generalist models for\nthe domain of retinal optical coherence tomography using visual in-context\nlearning (VICL), i.e., training models to generalize across tasks based on a\nfew examples provided at inference time. To facilitate rigorous assessment, we\npropose a broad evaluation protocol tailored to VICL in OCT. We extensively\nevaluate a state-of-the-art medical VICL approach on multiple retinal OCT\ndatasets, establishing a first baseline to highlight the potential and current\nlimitations of in-context learning for OCT. To foster further research and\npractical adoption, we openly release our code.", "AI": {"tldr": "本文探讨了使用视觉上下文学习（VICL）训练通用模型以实现视网膜OCT图像分析的方法，并提出了评估该方法的一个全面协议，建立了第一个基线来展示上下文学习在OCT中的潜力与局限性。", "motivation": "鉴于专门模型在医学图像分析中的局限性，本文提出了一种通过训练通用模型来实现视网膜光学相干断层扫描任务的方法，以满足临床需求。", "method": "Structure", "result": "{\"tldr\": \"本文探讨了使用视觉上下文学习（VICL）训练通用模型以实现视网膜OCT图像分析的方法，并提出了评估该方法的一个全面协议，建立了第一个基线来展示上下文学习在OCT中的潜力与局限性。\", \"motivation\": \"鉴于专门模型在医学图像分析中的局限性，本文提出了一种通过训练通用模型来实现视网膜光学相干断层扫描任务的方法，以满足临床需求。\", \"method\": \"该研究使用视觉上下文学习（VICL）技术训练模型以实现跨任务的泛化能力，即基于推理时提供的少量示例进行任务定义。\", \"result\": \"研究在多个视网膜OCT数据集上广泛评估了当前先进的医疗上文学习方法，建立了第一个基线，展示了其在OCT中的潜力与当前局限性。\", \"conclusion\": \"研究公开发布了代码，旨在促进更深入的研究和实际应用，而评估协议和基线数据将为未来的改进提供重要参考。\"}", "conclusion": "研究公开发布了代码，旨在促进更深入的研究和实际应用，而评估协议和基线数据将为未来的改进提供重要参考。"}}
