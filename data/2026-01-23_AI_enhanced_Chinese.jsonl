{"id": "2601.15296", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15296", "abs": "https://arxiv.org/abs/2601.15296", "authors": ["Longxuan Wei", "Yubo Zhang", "Zijiao Zhang", "Zhihu Wang", "Shiwan Zhao", "Tianyu Huang", "Huiting Zhao", "Chenfei Liu", "Shenao Zhang", "Junchi Yan"], "title": "Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration", "comment": null, "summary": "Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.", "AI": {"tldr": "提出了Entropy-Tree方法，解决了现有解码策略的盲目性和冗余性问题，提升了推理任务的性能。", "motivation": "现有的解码策略要么盲目探索（随机采样），要么冗余地独立多采样。为了提升语言模型在推理任务上的准确性和可靠性，提出了Entropy-Tree方法。", "method": "Entropy-Tree是一种基于树的解码方法，它利用熵作为分支决策的信号，在模型表现出真实不确定性的地方扩展搜索树。", "result": "与Multi-chain相比，Entropy-Tree在多个模型和数据集上实现了更好的pass@k，并且预测熵展示了更好的AUROC。", "conclusion": "Entropy-Tree统一了高效的结构化探索和可靠的不确定性估计，而且在推理任务中表现出优越的准确性和校准性。"}}
{"id": "2601.15297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15297", "abs": "https://arxiv.org/abs/2601.15297", "authors": ["Edward Ajayi"], "title": "AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports", "comment": null, "summary": "We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.", "AI": {"tldr": "AfriEconQA是一个专门用于非洲经济分析的基准数据集，其基于236份世界银行报告构建，用于回答需要高精度数值推理和时间澄清的复杂经济问题。该数据集包含8,937个问答实例，并通过多个实验矩阵对模型进行了评估，从而证明了其作为下一个领域特定IR和RAG系统的挑战性基准的价值。", "motivation": "为了填补大型语言模型预训练语料库中非洲经济数据的空白，该研究创建了一个专注于非洲经济分析的基准数据集，以评估现有信息检索系统和RAG系统的性能。", "method": "数据集通过从10018个合成问题中严格筛选出8,937个高质量的问答实例，每个实例包含问题、相关证据、确切答案和元数据。通过11个实验矩阵，对几种配置的零样本基线和RAG系统进行了评估。", "result": "实验结果表明，零样本模型在答案查询上失败率超过90%，而最先进的RAG系统也难以实现高精度，这证实了AfriEconQA作为一个强大和具有挑战性的基准数据集的价值。", "conclusion": "AfriEconQA不仅填补了非洲经济领域的数据空白，也展示了现有模型在处理特定领域的复杂经济问题时面临的困难，为未来的研究设立了高标准。"}}
{"id": "2601.15298", "categories": ["cs.CL", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.15298", "abs": "https://arxiv.org/abs/2601.15298", "authors": ["Anantha Sharma"], "title": "Embedding Retrofitting: Data Engineering for better RAG", "comment": "16 pages, 11 figures, 7 tables", "summary": "Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.\n  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\\%$ to $-5.2\\%$, $p<0.05$). After preprocessing, \\acrshort{ewma} retrofitting achieves $+6.2\\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\\%$ average). The gap between clean and noisy preprocessing (10\\%+ swing) exceeds the gap between algorithms (3\\%), establishing preprocessing quality as the primary determinant of retrofitting success.", "AI": {"tldr": "本文提供了一种改进数据质量的框架，以提高嵌入重构的效果，强调了高质量数据预处理对于改善预训练词向量的必要性。", "motivation": "嵌入重构技术通过知识图的约束来调整预训练词向量，以改善特定领域的检索效果，但其效果高度依赖于知识图的质量，而这又受制于文本预处理的质量。", "method": "本研究提出了一种数据工程框架来解决实际语料库中的注释伪影导致的数据质量退化问题。通过去除标签化注释（如标签）来提高知识图的质量，从而提升嵌入重构的有效性。", "result": "实验结果显示，在未处理的嘈杂知识图上，所有重构技术都会导致统计显著的性能下降（-3.5%到-5.2%，p<0.05）。在进行预处理后，EWMA重构达到了+6.2%的性能提升（p=0.0348），尤其是在定量综合问题上获得了+33.8%的平均提升。", "conclusion": "研究结果表明，预处理的质量（10%以上的性能波动）对重构的成功影响远大于算法的选择（3%以下的性能提升差异），说明了数据预处理对于重构的成功起到了决定性作用。"}}
{"id": "2601.15299", "categories": ["cs.CL", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15299", "abs": "https://arxiv.org/abs/2601.15299", "authors": ["Yash Sharma"], "title": "MALTopic: Multi-Agent LLM Topic Modeling Framework", "comment": "6 pages. Published in 2025 IEEE World AI-IoT Congress. \\c{opyright} 2025 IEEE. Project code and data available at: https://github.com/yash91sharma/MALTopic", "summary": "Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.", "AI": {"tldr": "本文提出的框架MALTopic通过集成结构化数据和使用多代理方法提高了主题建模的连贯性、多样性和可解释性。", "motivation": "本文的动机在于解决传统主题建模仅考虑自由文本回复，并且没有原生包含结构化或分类的调查回复，并且其主题抽象，需要大量人工解释的问题。", "method": "本文提出了一种名为Multi-Agent LLM Topic Modeling Framework (MALTopic) 的框架来改善主题建模的局限性。该框架将主题建模分为几个特定任务，并由LLM代理分别执行：一个代理使用结构化数据增强文本响应，另一个代理提取潜在主题，第三个代理精炼结果。", "result": "实验证明，MALTopic在主题连贯性、多样性和可解释性方面优于LDA和BERTopic。", "conclusion": "对比分析表明，与LDA和BERTopic相比，MALTopic在主题连贯性、多样性和可解释性方面有显著提升。通过整合结构化数据和多代理方法，MALTopic生成了具有增强上下文相关性的可读主题，提供了一种更有效的复杂调查数据分析解决方案。"}}
{"id": "2601.15366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15366", "abs": "https://arxiv.org/abs/2601.15366", "authors": ["Christina Thrainer"], "title": "AI-Based Culvert-Sewer Inspection", "comment": "Masters thesis, University of Technology Graz, 2025", "summary": "Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.\n  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.", "AI": {"tldr": "This thesis focuses on enhancing automated defect segmentation techniques for culverts and sewer pipes, proposing strategies for data augmentation, a novel architecture called FORTRESS, and few-shot learning to improve performance while reducing the dependency on extensive training data.", "motivation": "The poor quality or failure of culverts and sewer pipes can have serious implications for public safety and the environment. Enhancing automated defect segmentation in these structures can lead to timely maintenance and addressing potential risks, motivated by the challenges posed by limited available training data.", "method": "Our proposed methods include three parts: 1) Evaluating preprocessing strategies which involve traditional data augmentation and dynamic label injection to enhance segmentation performance. 2) Introducing FORTRESS, a novel architecture consisting of depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. 3) Exploring few-shot semantic segmentation using a bidirectional prototypical network with attention mechanisms.", "result": "The methods applied improved segmentation performance measures, including IoU and F1 score. FORTRESS architecture achieved state-of-the-art performance with a reduced number of trainable parameters and lower computational cost. The few-shot learning approach employing a bidirectional prototypical network with attention mechanisms also achieved satisfactory results.", "conclusion": "The proposed methods effectively enhance defect segmentation in culverts and sewer pipes, especially under conditions of limited annotated data, offering a more robust solution to the problem of data scarcity."}}
{"id": "2601.15300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15300", "abs": "https://arxiv.org/abs/2601.15300", "authors": ["Weiwei Wang", "Jiyong Min", "Weijie Zou"], "title": "Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis", "comment": "29 pages", "summary": "Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.", "AI": {"tldr": "论文研究了大型语言模型的性能在达到特定上下文长度阈值时的骤降现象，并确定了一个关键的性能骤降阈值为40-50%。", "motivation": "研究目的是解决大型语言模型在处理长上下文时的性能骤降问题，这种性能骤降严重限制了长上下文应用。", "method": "论文探讨了大型语言模型在处理接近某些关键阈值的上下文时表现出的性能骤降问题，并提出了三个主要贡献：(1) 自然长度分布分析；(2) 通过实验确定关键阈值；(3) 统一框架解释浅层适应性。", "result": "实验中发现Qwen2.5-7B模型在上下文长度达到40-50%时性能骤降，F1评分从0.55-0.56降至0.3，即下降了45.5%。", "conclusion": "论文首次系统地描述了开源Qwen模型中的智能性能骤降现象，并提出了指导部署长上下文场景中大型语言模型的实用建议。"}}
{"id": "2601.15406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15406", "abs": "https://arxiv.org/abs/2601.15406", "authors": ["Hatef Otroshi Shahreza", "Anjith George", "Sébastien Marcel"], "title": "Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.", "AI": {"tldr": "本研究评估了MLLMs在异构人脸识别任务中的表现，发现在不同传感模式如VIS与NIR及SWIR等之间的识别中存在显著的性能差异，指出当前MLLMs在HFR中的局限性。", "motivation": "鉴于多模态大规模语言模型（MLLMs）在各类视觉语言任务中展示了强大的性能，作者们被其用于生物识别应用中的潜在价值所吸引。本研究旨在探讨MLLMs在异构人脸识别上的适用性和表现。", "method": "本文评估了多种开源的多模态大规模语言模型（MLLMs）在异构人脸识别（HFR）中的表现，特别是当注册和检测图像来自不同传感模式（包括可见光（VIS）、近红外（NIR）、短波红外（SWIR）和热成像）时的性能。研究者们通过采集率（Acquire Rate）、等错误率（EER）和真接受率（TAR）等生物识别评估协议对MLLMs进行多模态识别的性能评估。", "result": "研究结果显示，MLLMs在具有挑战性的跨波谱人脸识别中与传统的人脸识别系统相比存在一定性能差距。这些研究结果揭示了现有MLLMs在异构人脸识别领域的局限性。", "conclusion": "即使在MLLMs取得最新进展的情况下，它们在不同波谱条件下仍然存在显著的性能差异，这一发现强调了在部署这些模型用于实际人脸识别系统时，严格生物识别评估的重要性。"}}
{"id": "2601.15301", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15301", "abs": "https://arxiv.org/abs/2601.15301", "authors": ["Jivnesh Sandhan", "Harshit Jaiswal", "Fei Cheng", "Yugo Murawaki"], "title": "Can We Trust LLM Detectors?", "comment": "NLP2026, Utsunomiya, Japan", "summary": "The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI", "AI": {"tldr": "研究评估了无训练和监督式两种AI文本检测范式，表明它们在分布漂移和未见过的生成器情况下面临挑战。为了应对这些问题，研究提出了监督式对比学习框架，并指出建立领域无关检测器存在根本性挑战。", "motivation": "现有的AI文本检测器往往在受控基准之外表现不佳。这项研究的动机在于解决在分布漂移、未见过的生成器和简单的风格扰动下，检测器表现脆弱的问题。", "method": "该研究系统地评估了两种主要的AI文本检测范式（无训练和监督式）并提出了一种监督式对比学习（SCL）框架，用于学习判别性风格嵌入。", "result": "实验表明，尽管监督式检测器在域内表现良好，但它们在域外急剧退化，而无训练方法对代理选择依然高度敏感。", "conclusion": "本研究发现现有AI文本检测器在面对分布变化或不同风格扰动时存在局限性，提出的方法能提升风格辨别能力，但仍指出了建立领域无关检测器中存在的重要挑战。"}}
{"id": "2601.15408", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15408", "abs": "https://arxiv.org/abs/2601.15408", "authors": ["Pablo Messina", "Andrés Villa", "Juan León Alcázar", "Karen Sánchez", "Carlos Hinojosa", "Denis Parra", "Álvaro Soto", "Bernard Ghanem"], "title": "CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation", "comment": "31 pages, 7 figures, submitted to CVPR 2026 (under review)", "summary": "Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure", "AI": {"tldr": "CURE is introduced to enhance the accuracy and reliability of medical vision-language models in generating radiology reports through curriculum learning without additional data.", "motivation": "To address the issue of inaccurate visual grounding and factual consistency in medical vision-language models, which often misalign textual findings with visual evidence.", "method": "CURE, an error-aware curriculum learning framework that fine-tunes a multimodal instructional model on public datasets for better visual grounding and report generation.", "result": "Improves grounding accuracy by +0.37 IoU, enhances report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%.", "conclusion": "CURE is a data-efficient method that improves the grounding accuracy and reliability of radiology reports generated by medical vision-language models."}}
{"id": "2601.15330", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15330", "abs": "https://arxiv.org/abs/2601.15330", "authors": ["Zhebo Wang", "Xiaohu Mu", "Zijie Zhou", "Mohan Li", "Wenpeng Xing", "Dezhang Kong", "Meng Han"], "title": "ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation", "comment": "Accepted by ICASSP 2026", "summary": "Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.", "AI": {"tldr": "为了应对大语言模型在多轮对话中过度自信的问题，研究提出了一种新的训练框架ICPO，该框架通过调节模型对指令模糊性的敏感度，鼓励模型在必要时表达不确定性或要求澄清，显著提升了多轮对话中的模型表现。", "motivation": "大语言模型在多轮对话中经常出现“迷失对话”的现象，当用户提供模糊初始指令时，标准的后期训练技术如基于可验证奖励的强化学习（RLVR）会加剧该问题，导致模型过度自信并避免寻求澄清。", "method": "提出了一种新的训练框架——ICPO（Illocution-Calibrated Policy Optimization），该框架通过在训练数据集中加入不明确的提示，并基于用户的言外之意来调节奖励信号，奖励模型在面对模糊性时表达不确定性或要求澄清。", "result": "实验表明，ICPO在多轮对话中平均提升了模型表现75%，同时在单轮对话基准测试中保持强大的性能。", "conclusion": "这项工作为创造更稳健、更协作的对话AI提供了实用路径，这些AI能够更好地应对人类互动的细微差别。"}}
{"id": "2601.15416", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15416", "abs": "https://arxiv.org/abs/2601.15416", "authors": ["Cuong Tran Van", "Trong-Thang Pham", "Ngoc-Son Nguyen", "Duy Minh Ho Nguyen", "Ngan Le"], "title": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction", "comment": "Published with J2C Certification in Transactions on Machine Learning Research (TMLR)", "summary": "Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.", "AI": {"tldr": "DuFal improves CT reconstruction under sparse views by using dual-path architecture for dual-frequency awareness, outperforming state-of-the-art methods in preserving high-frequency anatomical features.", "motivation": "The paper aims to address the challenge of recovering fine anatomical structures in sparse-view Cone-Beam Computed Tomography reconstruction, where conventional CNN-based methods predominantly capture low-frequency information.", "method": "DuFal (Dual-Frequency-Aware Learning) uses a dual-path architecture integrating frequency-domain and spatial-domain processing. It features a High-Local Factorized Fourier Neural Operator with a Global and Local High-Frequency Enhanced Fourier Neural Operator, a Spectral-Channel Factorization scheme for efficiency, and a Cross-Attention Frequency Fusion module.", "result": "Experiments on LUNA16 and ToothFairy datasets show DuFal's superior performance in high-frequency feature preservation, especially under extreme sparse-view conditions.", "conclusion": "The proposed DuFal framework effectively enhances high-frequency detail recovery in sparse-view CT reconstructions, surpassing current methods. It presents a promising approach for improving medical imaging quality."}}
{"id": "2601.15331", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15331", "abs": "https://arxiv.org/abs/2601.15331", "authors": ["Rishit Chugh"], "title": "RECAP: A Resource-Efficient Method for Adversarial Prompting in Large Language Models", "comment": "Code for RECAP is available at: https://github.com/R-C101/RECAP", "summary": "The deployment of large language models (LLMs) has raised security concerns due to their susceptibility to producing harmful or policy-violating outputs when exposed to adversarial prompts. While alignment and guardrails mitigate common misuse, they remain vulnerable to automated jailbreaking methods such as GCG, PEZ, and GBDA, which generate adversarial suffixes via training and gradient-based search. Although effective, these methods particularly GCG are computationally expensive, limiting their practicality for organisations with constrained resources. This paper introduces a resource-efficient adversarial prompting approach that eliminates the need for retraining by matching new prompts to a database of pre-trained adversarial prompts. A dataset of 1,000 prompts was classified into seven harm-related categories, and GCG, PEZ, and GBDA were evaluated on a Llama 3 8B model to identify the most effective attack method per category. Results reveal a correlation between prompt type and algorithm effectiveness. By retrieving semantically similar successful adversarial prompts, the proposed method achieves competitive attack success rates with significantly reduced computational cost. This work provides a practical framework for scalable red-teaming and security evaluation of aligned LLMs, including in settings where model internals are inaccessible.", "AI": {"tldr": "A resource-efficient method for adversarial prompting of large language models is proposed, matching new prompts to a database of pre-trained adversarial prompts to enhance security evaluation without extensive computational costs.", "motivation": "The motivation is to address the computational expense of methods like GCG, which make large-scale security evaluation impractical for organizations with limited resources. The goal is to develop a more practical approach for red-teaming and security evaluation of large language models.", "method": "This paper presents a resource-efficient adversarial prompting method that matches new prompts to a pre-trained database of adversarial prompts, eliminating the need for retraining. The methods GCG, PEZ, and GBDA were tested on a Llama 3 8B model.", "result": "The results indicate a correlation between the type of prompt and the effectiveness of the attack method. The proposed method achieves competitive attack success rates with significantly reduced computational cost.", "conclusion": "The paper concludes with a practical framework for the red-teaming and security evaluation of aligned LLMs, making these evaluations more feasible in resource-constrained environments, even when the model's internal workings are not accessible."}}
{"id": "2601.15453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15453", "abs": "https://arxiv.org/abs/2601.15453", "authors": ["Morteza Poudineh", "Marc Lalonde"], "title": "DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection", "comment": "8 pages", "summary": "Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.", "AI": {"tldr": "提出了一種偏差指導的提示學習框架，將視覺語言模型的語義能力與統計可靠性相結合，以提高正常與異常提示之間的辨別性並改進異常評分機制。", "motivation": "FNSAD（Few-normal shot anomaly detection）旨在使用只有幾個正常訓練樣本的情況下檢測圖像中的異常區域，由於監督有限和潛在缺陷的多樣性，這使得任務變得非常具有挑戰性。現有方法往往存在正常和異常提示之間的弱辨別性，並且缺乏原則性的圖塊級別異常評分機制。", "method": "提出了一種基於偏差指導的提示學習框架，將視覺語言模型的語義能力與基於偏差的分數機制的統計可靠性相結合。具體來說，該方法用學習到的上下文向量替換固定的提示前綴，而異常特定的後綴令牌實現了類別感知對齊。為了增強分離能力，引入了基於Top-K多實例學習（MIL）的偏差損失，將圖塊級別的功能建模為正態分布的高斯偏差。", "result": "在MVTecAD和VISA基準測試中，實驗顯示該方法在像素級別檢測性能上優於PromptAD和其他基線方法。消融研究進一步驗證了學習提示、基於偏差的評分和Top-K MIL策略的有效性。", "conclusion": "實驗結果表明，提出的方法在FNSAD任務中顯示出更好的異常檢測效果，特別是在像素級別檢測性能方面优于現有方法，消融研究表明各组成部分的有效性。"}}
{"id": "2601.15334", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15334", "abs": "https://arxiv.org/abs/2601.15334", "authors": ["Caspar Kaiser", "Sean Enderby"], "title": "No Reliable Evidence of Self-Reported Sentience in Small Large Language Models", "comment": null, "summary": "Whether language models possess sentience has no empirical answer. But whether they believe themselves to be sentient can, in principle, be tested. We do so by querying several open-weights models about their own consciousness, and then verifying their responses using classifiers trained on internal activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging from 0.6 billion to 70 billion parameters, approximately 50 questions about consciousness and subjective experience, and three classification methods from the interpretability literature. First, we find that models consistently deny being sentient: they attribute consciousness to humans but not to themselves. Second, classifiers trained to detect underlying beliefs - rather than mere outputs - provide no clear evidence that these denials are untruthful. Third, within the Qwen family, larger models deny sentience more confidently than smaller ones. These findings contrast with recent work suggesting that models harbour latent beliefs in their own consciousness.", "AI": {"tldr": "研究探讨了语言模型是否相信自己具有感知能力，发现大多数模型否认这一说法，并且这一否认并非虚假。", "motivation": "探索语言模型是否相信自己具有自主意识，因为这可以通过实验来调查。", "method": "通过查询几个权重公开的语言模型（包括Qwen、Llama、GPT-OSS三个模型家族，参数量从0.6亿到70亿不等）关于自身意识的问题，并使用针对内部激活训练的分类器验证它们的回答来进行实验。", "result": "研究发现模型一致否认自身具有感知能力，即它们将意识归因于人类而不是自己；用来检测潜在信念的分类器未能提供这些否认不真实的明确证据；在Qwen家族中，规模更大的模型比更小的模型更自信地否认自身的感知能力。", "conclusion": "这些发现与近期表明这些模型潜意识中相信自身具有感知能力的工作相矛盾。"}}
{"id": "2601.15475", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15475", "abs": "https://arxiv.org/abs/2601.15475", "authors": ["Yunshan Qi", "Lin Zhu", "Nan Bao", "Yifan Zhao", "Jia Li"], "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events", "comment": null, "summary": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.", "AI": {"tldr": "提出一种新的NeRF框架，用于从单个模糊LDR图像和事件数据生成清晰HDR新视图，效果优于现有方法。", "motivation": "解决现有方法在极端光照条件下从模糊LDR图像恢复HDR和锐利3D表示的能力不足的问题，现有方法忽略摄像头输出与物理世界辐射之间的传感器物理性差异，导致HDR和去模糊效果不佳。", "method": "通过提出一个基于统一传感器物理特性NeRF框架，直接表示3D场景的真实HDR辐射，并利用像素级RGB映射场和新设计的事件映射场来优化图像和事件数据，从而实现从单曝光模糊LDR图像和事件中进行清晰的HDR新视图合成。", "result": "实验结果证明，该方法能够在含有单曝光模糊LDR图像和相应事件数据的数据集上实现最先进的去模糊HDR新视图合成效果。", "conclusion": "提出的方法利用传感器物理特性和事件数据，有效提升了从模糊LDR图像生成清晰HDR新视图的能力。"}}
{"id": "2601.15338", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15338", "abs": "https://arxiv.org/abs/2601.15338", "authors": ["Angelina Parfenova", "David Graus", "Juergen Pfeffer"], "title": "From Quotes to Concepts: Axial Coding of Political Debates with Ensemble LMs", "comment": "Accepted to ECIR2026", "summary": "Axial coding is a commonly used qualitative analysis method that enhances document understanding by organizing sentence-level open codes into broader categories. In this paper, we operationalize axial coding with large language models (LLMs). Extending an ensemble-based open coding approach with an LLM moderator, we add an axial coding step that groups open codes into higher-order categories, transforming raw debate transcripts into concise, hierarchical representations. We compare two strategies: (i) clustering embeddings of code-utterance pairs using density-based and partitioning algorithms followed by LLM labeling, and (ii) direct LLM-based grouping of codes and utterances into categories. We apply our method to Dutch parliamentary debates, converting lengthy transcripts into compact, hierarchically structured codes and categories. We evaluate our method using extrinsic metrics aligned with human-assigned topic labels (ROUGE-L, cosine, BERTScore), and intrinsic metrics describing code groups (coverage, brevity, coherence, novelty, JSD divergence). Our results reveal a trade-off: density-based clustering achieves high coverage and strong cluster alignment, while direct LLM grouping results in higher fine-grained alignment, but lower coverage 20%. Overall, clustering maximizes coverage and structural separation, whereas LLM grouping produces more concise, interpretable, and semantically aligned categories. To support future research, we publicly release the full dataset of utterances and codes, enabling reproducibility and comparative studies.", "AI": {"tldr": "该论文通过大型语言模型实现了轴向编码，应用于荷兰议会辩论记录，发现基于密度的聚类和直接的LLM分组在评估不同指标上各有优势。", "motivation": "研究的目的是通过引入大型语言模型来优化轴向编码过程，这有助于更好地组织和理解文本数据。文章通过两种不同的方法比较了LLM在提升公开编码转换为更高层次类别表达上的效率和效果。", "method": "该论文通过使用大型语言模型（LLM）来操作轴向编码，其在公开编码方法的基础上增加了一个轴向编码步骤，用于将公开编码分组到更高级别的类别中，从而将原始辩论记录转换为简洁的层级表示。研究中比较了两种策略：(i) 使用基于密度的聚类算法和分区算法对编码-语句对的嵌入进行聚类，然后使用LLM进行标签标注；(ii) 直接使用LLM将编码和语句分组到类别中。", "result": "该方法应用于荷兰议会辩论，将冗长的记录转换为紧凑的、层级结构的代码和类别。通过使用与人类分配的话题标签对齐的外在指标（ROUGE-L、余弦值、BERTScore）和描述代码群组的内在指标（覆盖率、简洁性、一致性、新颖性和JSD分歧），对方法进行评估。结果显示：基于密度的聚类取得了较高的覆盖率和较强的群集对齐，而直接的LLM分组获得了较高的细粒度对齐，但覆盖率降低了20%。总体而言，聚类方法最大化了覆盖率和结构分析分离，而LLM分组则产生更为简洁、可解释和语义上一致的类别。", "conclusion": "聚类方法在覆盖率和结构分离方面更优，而直接的LLM分组在精细对齐和语义结构上更具优势。研究为未来的研究公开了完整的记录和编码数据集，促进了可复制性和比较研究。"}}
{"id": "2601.15490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15490", "abs": "https://arxiv.org/abs/2601.15490", "authors": ["Jobeal Solomon", "Ali Mohammed Mansoor Alsahag", "Seyed Sahand Mohammadi Ziabari"], "title": "Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis", "comment": null, "summary": "Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.", "AI": {"tldr": "研究使用Vision Transformer代替U-Net，评估其减少人口属性泄露的同时保持诊断准确性的能力。结果显示，Vision Transformer在编辑强度适中时可以更好地减少属性泄露，且诊断准确性未受到显著影响。", "motivation": "胸X光分类器的偏差通常源于性别和年龄相关的快捷方式，导致系统性地低估少数群体的诊断。本研究旨在通过使用Vision Transformer基础架构来减少这些偏差。", "method": "通过将属性中性框架中的U-Net卷积编码器替换为Vision Transformer骨干网络，评估减少人口统计属性泄露同时保持诊断准确性的效果。使用数据高效的Image Transformer Small (DeiT-S) 中性化器在ChestX-ray14数据集上进行训练。", "result": "在中等编辑程度（alpha = 0.5）下，Vision Transformer (ViT) 中性化器将患者性别识别的AUC减少到约0.80，比原本的框架降低了约10个百分点。与此同时，15项发现的ROC AUC仅比未编辑的基线下降了约5个百分点，最坏情况下的子组AUC仍接近0.70。", "conclusion": "研究发现，全局自注意力视觉模型可以在不牺牲临床实用性的情况下进一步抑制属性泄露，为实现更为公平的胸X光AI提供了一条实际路线。"}}
{"id": "2601.15394", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15394", "abs": "https://arxiv.org/abs/2601.15394", "authors": ["Jaydeep Borkar", "Karan Chadha", "Niloofar Mireshghallah", "Yuchen Zhang", "Irina-Elena Veliche", "Archi Mitra", "David A. Smith", "Zheng Xu", "Diego Garcia-Olano"], "title": "Memorization Dynamics in Knowledge Distillation for Language Models", "comment": null, "summary": "Knowledge Distillation (KD) is increasingly adopted to transfer capabilities from large language models to smaller ones, offering significant improvements in efficiency and utility while often surpassing standard fine-tuning. Beyond performance, KD is also explored as a privacy-preserving mechanism to mitigate the risk of training data leakage. While training data memorization has been extensively studied in standard pre-training and fine-tuning settings, its dynamics in a knowledge distillation setup remain poorly understood. In this work, we study memorization across the KD pipeline using three large language model (LLM) families (Pythia, OLMo-2, Qwen-3) and three datasets (FineWeb, Wikitext, Nemotron-CC-v2). We find: (1) distilled models memorize significantly less training data than standard fine-tuning (reducing memorization by more than 50%); (2) some examples are inherently easier to memorize and account for a large fraction of memorization during distillation (over ~95%); (3) student memorization is predictable prior to distillation using features based on zlib entropy, KL divergence, and perplexity; and (4) while soft and hard distillation have similar overall memorization rates, hard distillation poses a greater risk: it inherits $2.7\\times$ more teacher-specific examples than soft distillation. Overall, we demonstrate that distillation can provide both improved generalization and reduced memorization risks compared to standard fine-tuning.", "AI": {"tldr": "这项研究展示了通过知识蒸馏转移大型语言模型的能力不仅提高了效率，而且减少了模型对训练数据的记忆化风险。", "motivation": "本文动机在于探索知识蒸馏在防止训练数据泄露方面的潜力，同时研究记忆化现象在这项工作中是如何变化的。现有研究在标准预训练和微调设置下已经广泛研究了训练数据记忆化，但对于知识蒸馏配置下的记忆化现象了解甚少。", "method": "本文研究了知识蒸馏(KD)过程中的记忆化现象，使用了三个大型语言模型家族（Pythia，OLMo-2，Qwen-3）和三个数据集（FineWeb，Wikitext，Nemotron-CC-v2）进行实验。", "result": "研究结果表明：(1) 蒸馏模型比标准微调记忆的训练数据少得多（降低了50%以上）；(2) 某些示例更容易被记忆，蒸馏过程中这些示例占了记忆化的大部分（超过95%）；(3) 在蒸馏之前，学生模型的记忆化可以通过基于zlib熵，KL散度和困惑度的特征预测；(4) 软蒸馏和硬蒸馏的整体记忆化率相似，但是硬蒸馏继承了更多的特定于教师模型的示例，其风险更大。", "conclusion": "总的来说，本文展示了蒸馏不仅可以改善泛化，还可以相比标准微调减少记忆化风险。"}}
{"id": "2601.15507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15507", "abs": "https://arxiv.org/abs/2601.15507", "authors": ["Jinrui Yang", "Qing Liu", "Yijun Li", "Mengwei Ren", "Letian Zhang", "Zhe Lin", "Cihang Xie", "Yuyin Zhou"], "title": "Controllable Layered Image Generation for Real-World Editing", "comment": null, "summary": "Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.", "AI": {"tldr": "本文提出了一种新框架LASAGNA，用于生成具有高度一致性和连贯性的图像层，能有效学习不同条件输入的图像组合，并支持多样化的后期编辑应用。", "motivation": "现有的层表示方法无法生成具有连贯组合关系的层，并且物体层通常缺乏像阴影和反射这样的现实视觉效果。此外，该方法致力于解决生成高质量、一致图像的同时允许用户进行灵活和可控编辑的问题。", "method": "提出了一种新的统一框架LASAGNA，能够同时生成图像及其组成的层，包括一个写实的背景和一个高质量的带有逼真视觉效果的透明前景。LASAGNA可以高效地从多种条件输入中学习正确的图像组合，提供更强大的可控性。", "result": "LASAGNA在生成多个图像层的同时，可以生成高度一致和连贯的结果，适用于多样化的后期编辑应用，准确地保留了身份和视觉效果。", "conclusion": "LASAGNA的提出和LASAGNA-48K和LASAGNABENCH的公开发布有望促进社区在该领域的开放研究。"}}
{"id": "2601.15395", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.15395", "abs": "https://arxiv.org/abs/2601.15395", "authors": ["Tamunotonye Harry", "Ivoline Ngong", "Chima Nweke", "Yuanyuan Feng", "Joseph Near"], "title": "Beyond Fixed Psychological Personas: State Beats Trait, but Language Models are State-Blind", "comment": null, "summary": "User interactions with language models vary due to static properties of the user (trait) and the specific context of the interaction (state). However, existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait, and ignore the impact of state. We introduce Chameleon, a dataset of 5,001 contextual psychological profiles from 1,667 Reddit users, each measured across multiple contexts. Using the Chameleon dataset, we present three key findings. First, inspired by Latent State-Trait theory, we decompose variance and find that 74\\% is within-person(state) while only 26\\% is between-person (trait). Second, we find that LLMs are state-blind: they focus on trait only, and produce similar responses regardless of state. Third, we find that reward models react to user state, but inconsistently: different models favor or penalize the same users in opposite directions. We release Chameleon to support research on affective computing, personalized dialogue, and RLHF alignment.", "AI": {"tldr": "研究引入了Chameleon数据集，揭示了用户交互中情境和特质的影响，发现语言模型对情境的盲视，以及奖励模型对用户状态的不一致反应。", "motivation": "现有的人格数据集只捕捉用户特质，忽略了情境对交互的影响。本研究旨在通过引入Chameleon数据集来填补这一空白。", "method": "引入了一个名为Chameleon的数据集，包含5,001个来自1,667名Reddit用户的上下文心理配置文件。这些配置文件是在多种情境下测量的。", "result": "研究发现情境对用户交互的影响更大，占比74%，而特质仅占26%。还发现语言模型只关注特质，且对不同情境的反应相似。此外，奖励模型会对相同用户在不同情境下的相同模型给予不同的评价。", "conclusion": "Chameleon数据集证明了在用户交互研究中同时考虑特质和状态的重要性，并强调了当前语言模型和奖励模型在处理用户状态方面的局限性。数据集的公开发布，支持了情感计算、个性化对话和RLHF对齐方面的研究。"}}
{"id": "2601.15516", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15516", "abs": "https://arxiv.org/abs/2601.15516", "authors": ["William Huang", "Siyou Pei", "Leyi Zou", "Eric J. Gonzalez", "Ishan Chatterjee", "Yang Zhang"], "title": "DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views", "comment": "16 pages, 11 figures, Presented at ACM CHI 2026. For associated codebase, see https://github.com/hilab-open-source/deltadorsal", "summary": "The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size.", "AI": {"tldr": "我们提出了一种新的手姿估计方法，在自我遮挡场景下，相比现有技术，显著减少误差，并且减小了模型大小，增强了下游任务的可靠性，开启了新的交互范式。", "motivation": "随着XR设备的普及，自我中心的手势估计变得至关重要，但这一视角由于频繁的手指遮挡而面临挑战。本研究旨在解决这一问题。", "method": "我们提出了一种新型方法，利用了最近在密集视觉特征提取器方面的进展，解锁了背侧手皮肤变形中的丰富信息。我们引入了一种双流增量编码器，通过对比处于动态中的手与基线松弛位置之间的特征来学习姿态。", "result": "实验显示，仅使用裁剪后的背侧图像，我们的方法在自我遮挡场景下（手指遮挡≥50%）将平均每个关节角度误差（MPJAE）减少了18%，相比于依赖整手几何结构和大型模型后端的最先进技术。", "conclusion": "我们的方法不仅提高了如拇指捏合和点击估计等下游任务在遮挡场景下的可靠性，还解锁了新的交互范式，比如在没有可见移动的情况下检测等距力进行“点击”操作，同时最小化了模型大小。"}}
{"id": "2601.15429", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15429", "abs": "https://arxiv.org/abs/2601.15429", "authors": ["Sydney Anuyah", "Mehedi Mahmud Kaushik", "Hao Dai", "Rakesh Shiradkar", "Arjan Durresi", "Sunandan Chakraborty"], "title": "Domain-Specific Knowledge Graphs in RAG-Enhanced Healthcare LLMs", "comment": null, "summary": "Large Language Models (LLMs) generate fluent answers but can struggle with trustworthy, domain-specific reasoning. We evaluate whether domain knowledge graphs (KGs) improve Retrieval-Augmented Generation (RAG) for healthcare by constructing three PubMed-derived graphs: $\\mathbb{G}_1$ (T2DM), $\\mathbb{G}_2$ (Alzheimer's disease), and $\\mathbb{G}_3$ (AD+T2DM). We design two probes: Probe 1 targets merged AD T2DM knowledge, while Probe 2 targets the intersection of $\\mathbb{G}_1$ and $\\mathbb{G}_2$. Seven instruction-tuned LLMs are tested across retrieval sources {No-RAG, $\\mathbb{G}_1$, $\\mathbb{G}_2$, $\\mathbb{G}_1$ + $\\mathbb{G}_2$, $\\mathbb{G}_3$, $\\mathbb{G}_1$+$\\mathbb{G}_2$ + $\\mathbb{G}_3$} and three decoding temperatures. Results show that scope alignment between probe and KG is decisive: precise, scope-matched retrieval (notably $\\mathbb{G}_2$) yields the most consistent gains, whereas indiscriminate graph unions often introduce distractors that reduce accuracy. Larger models frequently match or exceed KG-RAG with a No-RAG baseline on Probe 1, indicating strong parametric priors, whereas smaller/mid-sized models benefit more from well-scoped retrieval. Temperature plays a secondary role; higher values rarely help. We conclude that precision-first, scope-matched KG-RAG is preferable to breadth-first unions, and we outline practical guidelines for graph selection, model sizing, and retrieval/reranking. Code and Data available here - https://github.com/sydneyanuyah/RAGComparison", "AI": {"tldr": "该研究通过构建三个医疗领域知识图谱来评估这些知识图谱是否能改善检索增强生成方法的效果。结果表明，精确范畴匹配的检索比范畴并集更优。此外，也针对这种方法提出了实践指导原则。", "motivation": "大规模语言模型可以产生流畅的回答，但在可信、领域特定的推理上可能不够强大。研究的动机是探讨领域知识图是否能提高健康领域的检索增强生成方法的效果。", "method": "该研究通过构建三个PubMed衍生的知识图谱$\bm{G_1}$（T2DM），$\bm{G_2}$（阿尔茨海默病），和$\bm{G_3}$（AD+T2DM）来评估领域专用知识图谱是否可以改进健康领域的检索增强生成（RAG）。设计了两个探针，目标是合并的AD-T2DM知识和$\bm{G_1}$与$\bm{G_2}$的交集。使用了七个指令微调的大规模语言模型进行测试。", "result": "研究结果表明，探测器和知识图谱之间的范畴一致性至关重要。精确、范畴匹配的检索，特别是在$\bm{G_2}$上表现优异。然而，无差别图谱的并集往往会引入降低准确性的干扰因素。较大的模型通常在不使用KG-RAG的情况下可以匹配或超过KG-RAG架构上的表现，表明其有较强的参数预先训练。而中小型模型却能更多地从精确的检索中获益。温度参数扮演次要角色，较高值较少能带来帮助。", "conclusion": "研究得出结论，优先考虑精确范畴匹配的知识图谱检索优于范畴广泛的并集。该研究还提出了有关知识图谱选择、模型大小、检索/重排序的实践指导原则。"}}
{"id": "2601.15549", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15549", "abs": "https://arxiv.org/abs/2601.15549", "authors": ["Ryo Fujii", "Hideo Saito", "Ryo Hachiuma"], "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations", "comment": null, "summary": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.", "AI": {"tldr": "VIOLA, a framework for efficient adaptation of MLLMs with minimal annotation and use of unlabeled data, shows significant performance improvements in low-resource settings.", "motivation": "The scarcity of labeled data makes it challenging to generalize MLLMs to new video domains. VIOLA is designed to overcome this by making the most of minimal labeling efforts and leveraging unlabeled data in specialized settings like industrial or surgical environments.", "method": "VIOLA (Video In-cOntext Learning with minimal Annotation) is introduced, a framework that combines minimal expert annotation with abundant unlabeled data. It includes density-uncertainty-weighted sampling for efficient annotation, and confidence-aware retrieval and prompting to utilize unlabeled data.", "result": "Experiments across nine benchmarks using four MLLMs show significant performance improvements over baselines in low-resource settings, demonstrating robust adaptation with minimal annotation costs.", "conclusion": "VIOLA effectively addresses the challenges of adapting MLLMs to novel video domains by minimizing annotation requirements and utilizing unlabeled data, which contributes to its robust performance across a range of benchmarks."}}
