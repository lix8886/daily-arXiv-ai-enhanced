{"id": "2509.09699", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09699", "abs": "https://arxiv.org/abs/2509.09699", "authors": ["Mingyang Li", "Viktor Schlegel", "Tingting Mu", "Warren Del-Pinto", "Goran Nenadic"], "title": "Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs", "comment": null, "summary": "Mapping clinical documents to standardised clinical vocabularies is an\nimportant task, as it provides structured data for information retrieval and\nanalysis, which is essential to clinical research, hospital administration and\nimproving patient care. However, manual coding is both difficult and\ntime-consuming, making it impractical at scale. Automated coding can\npotentially alleviate this burden, improving the availability and accuracy of\nstructured clinical data. The task is difficult to automate, as it requires\nmapping to high-dimensional and long-tailed target spaces, such as the\nInternational Classification of Diseases (ICD). While external knowledge\nsources have been readily utilised to enhance output code representation, the\nuse of external resources for representing the input documents has been\nunderexplored. In this work, we compute a structured representation of the\ninput documents, making use of document-level knowledge graphs (KGs) that\nprovide a comprehensive structured view of a patient's condition. The resulting\nknowledge graph efficiently represents the patient-centred input documents with\n23\\% of the original text while retaining 90\\% of the information. We assess\nthe effectiveness of this graph for automated ICD-9 coding by integrating it\ninto the state-of-the-art ICD coding architecture PLM-ICD. Our experiments\nyield improved Macro-F1 scores by up to 3.20\\% on popular benchmarks, while\nimproving training efficiency. We attribute this improvement to different types\nof entities and relationships in the KG, and demonstrate the improved\nexplainability potential of the approach over the text-only baseline.", "AI": {"tldr": "研究采用知识图谱集成到临床文档中，实现了23%文本信息的高效表示，同时提高了自动化ICD-9编码的效率和精确度以及模型的解释能力，Macro-F1最高提升3.20%。", "motivation": "临床文档映射到标准化临床词汇表是一项重要任务，因为它为信息检索和分析提供了结构化数据，这对临床研究、医院管理和改善患者护理至关重要。然而，手动编码既困难又耗时，这使得在更大规模上操作不切实际。", "method": "本研究通过计算输入文档的结构化表示，使用了文档级别的知识图谱（KG），以提供患者病情的全面结构化视图。该知识图谱能够以原文本23%的长度有效表示以患者为中心的输入文件，同时保留了90%的信息。", "result": "将知识图谱整合到最新的ICD编码架构PLM-ICD中进行自动化ICD-9编码时，实验结果显示在流行基准上的Macro-F1得分最高提高了3.20%，同时提高了训练效率。", "conclusion": "本研究展示了使用知识图谱作为输入表示对于改善自动化临床编码的效果，提高了模型的性能和训练效率，并增强了模型的可解释性。"}}
{"id": "2509.09700", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09700", "abs": "https://arxiv.org/abs/2509.09700", "authors": ["Malavika Suresh", "Rahaf Aljundi", "Ikechukwu Nkisi-Orji", "Nirmalie Wiratunga"], "title": "Cross-Layer Attention Probing for Fine-Grained Hallucination Detection", "comment": "To be published at the TRUST-AI workshop, ECAI 2025", "summary": "With the large-scale adoption of Large Language Models (LLMs) in various\napplications, there is a growing reliability concern due to their tendency to\ngenerate inaccurate text, i.e. hallucinations. In this work, we propose\nCross-Layer Attention Probing (CLAP), a novel activation probing technique for\nhallucination detection, which processes the LLM activations across the entire\nresidual stream as a joint sequence. Our empirical evaluations using five LLMs\nand three tasks show that CLAP improves hallucination detection compared to\nbaselines on both greedy decoded responses as well as responses sampled at\nhigher temperatures, thus enabling fine-grained detection, i.e. the ability to\ndisambiguate hallucinations and non-hallucinations among different sampled\nresponses to a given prompt. This allows us to propose a detect-then-mitigate\nstrategy using CLAP to reduce hallucinations and improve LLM reliability\ncompared to direct mitigation approaches. Finally, we show that CLAP maintains\nhigh reliability even when applied out-of-distribution.", "AI": {"tldr": "The paper introduces CLAP, an effective technique for hallucination detection in LLMs, leading to better reliability.", "motivation": "To address the reliability concerns of LLMs due to their tendency to generate inaccurate text.", "method": "Cross-Layer Attention Probing (CLAP), a novel activation probing technique for hallucination detection in LLMs.", "result": "CLAP improves hallucination detection compared to baselines and allows for a detect-then-mitigate strategy to reduce hallucinations.", "conclusion": "CLAP not only enhances the detection of hallucinations but also ensures high reliability even when applied out-of-distribution."}}
{"id": "2509.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09701", "abs": "https://arxiv.org/abs/2509.09701", "authors": ["JungHo Jung", "Junhyun Lee"], "title": "Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task", "comment": null, "summary": "End-to-end speech-to-text translation typically suffers from the scarcity of\npaired speech-text data. One way to overcome this shortcoming is to utilize the\nbitext data from the Machine Translation (MT) task and perform Multi-Task\nLearning (MTL). In this paper, we formulate MTL from a regularization\nperspective and explore how sequences can be regularized within and across\nmodalities. By thoroughly investigating the effect of consistency\nregularization (different modality) and R-drop (same modality), we show how\nthey respectively contribute to the total regularization. We also demonstrate\nthat the coefficient of MT loss serves as another source of regularization in\nthe MTL setting. With these three sources of regularization, we introduce the\noptimal regularization contour in the high-dimensional space, called the\nregularization horizon. Experiments show that tuning the hyperparameters within\nthe regularization horizon achieves near state-of-the-art performance on the\nMuST-C dataset.", "AI": {"tldr": "本文通过正则化方法在MTL中研究语音到文本翻译，提出正则化视界策略并在实验中显示出良好的性能。", "motivation": "端到端的语音到文本翻译通常因配对的语音-文本数据稀缺而受限。一种克服这一缺点的方法是利用来自机器翻译（MT）任务的双语数据并执行多任务学习（MTL）。", "method": "本文从正则化的角度制定了多任务学习（MTL），并探索了如何在不同模态内和跨模态内在序列中应用正则化。通过彻底研究一致性正则化（不同模态）和R-drop（相同模态）的影响，展示了它们对总正则化的贡献。还证明了在MTL设置中，机器翻译（MT）损失的系数是另一种正则化来源。通过这三个正则化的来源，引入了高维空间中的最优正则化轮廓——称为正则化视界。", "result": "实验表明，在正则化视界内调整超参数可以在MuST-C数据集上实现接近最先进性能的结果。", "conclusion": "研究表明，通过调节超参数以位于正则化视界内，可以实现接近最先进性能的结果。"}}
{"id": "2509.09702", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09702", "abs": "https://arxiv.org/abs/2509.09702", "authors": ["Ninad Bhat", "Kieran Browne", "Pip Bingemann"], "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM models", "comment": "30 Pages, 14 figures", "summary": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows.", "AI": {"tldr": "研究提出针对LLMs在营销创意评估的Creativity Benchmark框架，结果显示没有模型能显著超越其他模型，建议专家人类评估和多样化的评估方法对于此类任务是必要的。", "motivation": "目的是评估大规模语言模型在营销创意上的效能，并通过人类专家评估来验证自动评估之间的相关性以及对品牌限定任务的有效性。", "method": "提出Creativity Benchmark评估框架，以评估大规模语言模型（LLMs）在营销创意方面的能力。该框架覆盖100个品牌（12个类别）和三种提示类型（洞察、创意、大胆创意）。通过Bradley-Terry模型分析了678位实践创意人员的人类对对比较偏好（共11,012个匿名比较）。", "result": "发现性能紧密聚集，没有模型能在品牌或提示类型上占据优势：最高与最低模型的差距是$\\Delta\\theta \\approx 0.45$，意味着一对一胜利概率为$0.61$；最高评价模型仅能在约$61\textbackslash\textpercent$的情况下胜过最低评价的模型。此外，通过余弦距离分析模型多样性，捕捉模型内部和之间的变化，并对提示重新构建的敏感性进行分析。比较三个以LLM为评委的设置与人类排名揭示了较弱且不一致的相关性以及评分人特定的偏见，凸显出自动化评委无法替代人类评估。传统创造力测试部分地适用于品牌约束的任务。", "conclusion": "总体而言，研究结果表明需要专家人类评估和多样化工作流程。"}}
{"id": "2509.09720", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09720", "abs": "https://arxiv.org/abs/2509.09720", "authors": ["Akansel Cosgun", "Lachlan Chumbley", "Benjamin J. Meyer"], "title": "Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision", "comment": null, "summary": "This paper introduces the Australian Supermarket Object Set (ASOS), a\ncomprehensive dataset comprising 50 readily available supermarket items with\nhigh-quality 3D textured meshes designed for benchmarking in robotics and\ncomputer vision applications. Unlike existing datasets that rely on synthetic\nmodels or specialized objects with limited accessibility, ASOS provides a\ncost-effective collection of common household items that can be sourced from a\nmajor Australian supermarket chain. The dataset spans 10 distinct categories\nwith diverse shapes, sizes, and weights. 3D meshes are acquired by a\nstructure-from-motion techniques with high-resolution imaging to generate\nwatertight meshes. The dataset's emphasis on accessibility and real-world\napplicability makes it valuable for benchmarking object detection, pose\nestimation, and robotics applications.", "AI": {"tldr": "Introduces ASOS, a comprehensive and accessible 3D dataset of common supermarket items for benchmarking robotics and computer vision tasks, acquired via structure-from-motion techniques. It comprises 50 items from 10 categories and focuses on real-world applicability.", "motivation": "To provide a real-world and accessible dataset for benchmarking purposes, addressing the limitations of existing datasets that are either synthetically created or consist of specialized objects.", "method": "The dataset, ASOS, was created using structure-from-motion techniques to generate high-quality 3D textured meshes of 50 common supermarket items, sourced from a major Australian supermarket chain.", "result": "A comprehensive 3D textured mesh dataset consisting of 50 supermarket items from 10 categories, which are diverse in shape, size, and weight, and are designed for benchmarking in robotics and computer vision applications.", "conclusion": "The ASOS dataset offers a valuable resource for researchers and developers focused on object detection, pose estimation, and robotics, by providing a cost-effective and accessible set of real-world objects with high-quality 3D representations."}}
{"id": "2509.09703", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09703", "abs": "https://arxiv.org/abs/2509.09703", "authors": ["Zhenhua Xu", "Xixiang Zhao", "Xubin Yue", "Shengwei Tian", "Changting Lin", "Meng Han"], "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor", "comment": "Accepted by EMNLP2025 MainConference", "summary": "The widespread deployment of large language models (LLMs) has intensified\nconcerns around intellectual property (IP) protection, as model theft and\nunauthorized redistribution become increasingly feasible. To address this,\nmodel fingerprinting aims to embed verifiable ownership traces into LLMs.\nHowever, existing methods face inherent trade-offs between stealthness,\nrobustness, and generalizability, being either detectable via distributional\nshifts, vulnerable to adversarial modifications, or easily invalidated once the\nfingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven\nfingerprinting framework that encodes contextual correlations across multiple\ndialogue turns, such as counterfactual, rather than relying on token-level or\nsingle-turn triggers. CTCC enables fingerprint verification under black-box\naccess while mitigating false positives and fingerprint leakage, supporting\ncontinuous construction under a shared semantic rule even if partial triggers\nare exposed. Extensive experiments across multiple LLM architectures\ndemonstrate that CTCC consistently achieves stronger stealth and robustness\nthan prior work. Our findings position CTCC as a reliable and practical\nsolution for ownership verification in real-world LLM deployment scenarios. Our\ncode and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.", "AI": {"tldr": "CTCC is a novel rule-driven framework for model fingerprinting of large language models, offering improved stealth and robustness over existing methods.", "motivation": "The motivation is to address the problems of existing model fingerprinting methods which are either detectable, vulnerable to adversarial modifications, or invalidated once revealed.", "method": "The paper introduces CTCC, a rule-driven fingerprinting framework that encodes contextual correlations across multiple dialogue turns to embed verifiable ownership traces into large language models.", "result": "The experiments show that CTCC achieves stronger stealth and robustness compared to previous methods across multiple LLM architectures.", "conclusion": "CTCC is positioned as a reliable and practical solution for ownership verification in real-world deployment scenarios of LLMs, considering its strength in stealth and robustness."}}
{"id": "2509.09721", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09721", "abs": "https://arxiv.org/abs/2509.09721", "authors": ["Jiayi Miao", "Dingxin Lu", "Zhuqi Wang"], "title": "A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval", "comment": null, "summary": "After natural disasters, accurate evaluations of damage to housing are\nimportant for insurance claims response and planning of resources. In this\nwork, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)\nframework. On top of classical RAG architecture, we further the framework to\ndevise a two-branch multimodal encoder structure that the image branch employs\na visual encoder composed of ResNet and Transformer to extract the\ncharacteristic of building damage after disaster, and the text branch harnesses\na BERT retriever for the text vectorization of posts as well as insurance\npolicies and for the construction of a retrievable restoration index. To impose\ncross-modal semantic alignment, the model integrates a cross-modal interaction\nmodule to bridge the semantic representation between image and text via\nmulti-head attention. Meanwhile, in the generation module, the introduced modal\nattention gating mechanism dynamically controls the role of visual evidence and\ntext prior information during generation. The entire framework takes end-to-end\ntraining, and combines the comparison loss, the retrieval loss and the\ngeneration loss to form multi-task optimization objectives, and achieves image\nunderstanding and policy matching in collaborative learning. The results\ndemonstrate superior performance in retrieval accuracy and classification index\non damage severity, where the Top-1 retrieval accuracy has been improved by\n9.6%.", "AI": {"tldr": "本论文提出了一种多模态检索增强生成框架，能够提升受灾建筑的损坏评估精度，它结合了图像和文本信息，提高了信息检索和分类的准确性。", "motivation": "在自然灾害发生后，对房屋损坏的准确评估对于保险理赔响应和资源规划来说至关重要。本研究为了改进损害评估的准确性和效率，提出了新的方法和技术。", "method": "本论文提出了一种新型的多模态检索增强生成框架（MM-RAG）。该框架在传统的RAG架构之上，设计了两分支的多模态编码结构，其中图像分支使用ResNet和Transformer组成的视觉编码器来提取受灾后建筑损坏的特征，文本分支则使用BERT检索器来将帖子及保险政策转换为文本向量，并构建一个可检索的恢复索引。为了实现跨模态语义对齐，模型整合了一个跨模态交互模块，通过多头注意力机制连接图像和文本的语义表示。在生成模块中，引入的模态注意力门控机制可以动态控制生成过程中视觉证据和文本先验信息的作用。整个框架采用端到端训练，结合对比损失、检索损失与生成损失形成多任务优化目标，实现了图像理解和政策匹配的协同学习。", "result": "实验结果表明，该框架在检索准确性和损坏严重程度分类指数上表现出色，Top-1检索准确率提升了9.6%。", "conclusion": "本论文中的方法在基于图像和文本数据的灾害损害评估上取得了很好的效果，显示出多模态融合方法在灾害评估中的潜力和优越性。"}}
{"id": "2509.09704", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.09704", "abs": "https://arxiv.org/abs/2509.09704", "authors": ["Ali Mazyaki", "Mohammad Naghizadeh", "Samaneh Ranjkhah Zonouzaghi", "Hossein Setareh"], "title": "Temporal Preferences in Language Models for Long-Horizon Assistance", "comment": null, "summary": "We study whether language models (LMs) exhibit future- versus\npresent-oriented preferences in intertemporal choice and whether those\npreferences can be systematically manipulated. Using adapted human experimental\nprotocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them\nagainst a sample of human decision makers. We introduce an operational metric,\nthe Manipulability of Time Orientation (MTO), defined as the change in an LM's\nrevealed time preference between future- and present-oriented prompts. In our\ntests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)\nchoose later options under future-oriented prompts but only partially\npersonalize decisions across identities or geographies. Moreover, models that\ncorrectly reason about time orientation internalize a future orientation for\nthemselves as AI decision makers. We discuss design implications for AI\nassistants that should align with heterogeneous, long-horizon goals and outline\na research agenda on personalized contextual calibration and socially aware\ndeployment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09722", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09722", "abs": "https://arxiv.org/abs/2509.09722", "authors": ["Taylor Archibald", "Tony Martinez"], "title": "Improving MLLM Historical Record Extraction with Test-Time Image", "comment": null, "summary": "We present a novel ensemble framework that stabilizes LLM based text\nextraction from noisy historical documents. We transcribe multiple augmented\nvariants of each image with Gemini 2.0 Flash and fuse these outputs with a\ncustom Needleman Wunsch style aligner that yields both a consensus\ntranscription and a confidence score. We present a new dataset of 622\nPennsylvania death records, and demonstrate our method improves transcription\naccuracy by 4 percentage points relative to a single shot baseline. We find\nthat padding and blurring are the most useful for improving accuracy, while\ngrid warp perturbations are best for separating high and low confidence cases.\nThe approach is simple, scalable, and immediately deployable to other document\ncollections and transcription models.", "AI": {"tldr": "A novel ensemble framework stabilizes text extraction from noisy historical documents using multiple transcriptions and a custom aligner, improving accuracy by 4 percentage points.", "motivation": "To stabilize text extraction from noisy historical documents using a novel ensemble framework.", "method": "We transcribe multiple augmented variants of each image using Gemini 2.0 Flash and fuse these outputs with a custom Needleman Wunsch style aligner to yield a consensus transcription and a confidence score.", "result": "The method improves transcription accuracy by 4 percentage points relative to a single shot baseline.", "conclusion": "The approach is simple, scalable, and can be deployed to other document collections and transcription models."}}
{"id": "2509.09705", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09705", "abs": "https://arxiv.org/abs/2509.09705", "authors": ["Claudio Pinhanez", "Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Yago Primerano"], "title": "The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks", "comment": null, "summary": "This work explores the consistency of small LLMs (2B-8B parameters) in\nanswering multiple times the same question. We present a study on known,\nopen-source LLMs responding to 10 repetitions of questions from the\nmultiple-choice benchmarks MMLU-Redux and MedQA, considering different\ninference temperatures, small vs. medium models (50B-80B), finetuned vs. base\nmodels, and other parameters. We also look into the effects of requiring\nmulti-trial answer consistency on accuracy and the trade-offs involved in\ndeciding which model best provides both of them. To support those studies, we\npropose some new analytical and graphical tools. Results show that the number\nof questions which can be answered consistently vary considerably among models\nbut are typically in the 50%-80% range for small models at low inference\ntemperatures. Also, accuracy among consistent answers seems to reasonably\ncorrelate with overall accuracy. Results for medium-sized models seem to\nindicate much higher levels of answer consistency.", "AI": {"tldr": "本研究通过分析小规模和中型LLMs在多次回答相同问题时的表现，以确定模型的准确性和一致性之间的关系。研究表明，模型在低推理温度下的一致性表现较好，而且一致性越高，准确性相对也会越高。", "motivation": "本项工作的动机在于提供一种方法来评估小规模和中型LLMs在重复回答时的表现，特别是在准确性和一致性方面，以及探讨在这种情况下最优模型的选择。", "method": "本研究通过分析小规模LLMs（2B-8B参数）在多次回答同一问题时的一致性来探讨其表现。研究对象包括知名开源LLMs，它们需重复回答来自MMLU-Redux和MedQA多选题基准的10次提问。研究考虑了不同的推理温度（低温和中温）、小模型（50B-80B）与微调模型与基础模型的对比以及其他参数的影响。", "result": "结果表明，能够一致回答的问题数量在不同模型之间差异显著，但对于低温推理的小模型而言，这一比例通常在50%到80%之间。对于中型模型，测试结果显示其答案一致性水平更高。此外，一致性答案的准确性似乎与总体准确性有合理的关系。", "conclusion": "研究提出了一些新的分析和图形工具来支持这些研究。得出的结论是，对于小规模和中型LLMs，其答案的一致性与准确性之间存在一定的相关性，这为选择最佳模型提供了依据。"}}
{"id": "2509.09730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09730", "abs": "https://arxiv.org/abs/2509.09730", "authors": ["Kaikai Zhao", "Zhaoxiang Liu", "Peng Wang", "Xin Wang", "Zhicheng Ma", "Yajun Xu", "Wenjing Zhang", "Yibing Nan", "Kai Wang", "Shiguo Lian"], "title": "MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance", "comment": "accepted by Image and Vision Computing", "summary": "General-domain large multimodal models (LMMs) have achieved significant\nadvances in various image-text tasks. However, their performance in the\nIntelligent Traffic Surveillance (ITS) domain remains limited due to the\nabsence of dedicated multimodal datasets. To address this gap, we introduce\nMITS (Multimodal Intelligent Traffic Surveillance), the first large-scale\nmultimodal benchmark dataset specifically designed for ITS. MITS includes\n170,400 independently collected real-world ITS images sourced from traffic\nsurveillance cameras, annotated with eight main categories and 24 subcategories\nof ITS-specific objects and events under diverse environmental conditions.\nAdditionally, through a systematic data generation pipeline, we generate\nhigh-quality image captions and 5 million instruction-following visual\nquestion-answer pairs, addressing five critical ITS tasks: object and event\nrecognition, object counting, object localization, background analysis, and\nevent reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream\nLMMs on this dataset, enabling the development of ITS-specific applications.\nExperimental results show that MITS significantly improves LMM performance in\nITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905\n(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to\n0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the\ndataset, code, and models as open-source, providing high-value resources to\nadvance both ITS and LMM research.", "AI": {"tldr": "本研究发布了一个为智能交通监控设计的大规模多模态数据集MITS，此数据集有助于改进现有模型在ITS任务中的表现。", "motivation": "为了应对现有的多模态模型在智能交通监控(ITS)领域的性能限制，提出了一种专门的数据集解决方案。", "method": "引入了MITS（多模态智能交通监控），这是首个专为ITS领域设计的大规模多模态基准数据集。MITS包含了170,400张从交通监控摄像头独立收集的真实ITS图像，并根据不同的环境条件，标注了8个主要类别的24个ITS特定对象和事件子类别。此外，通过一个系统化数据生成流程，生成了高质量的图像说明以及5百万个遵循指令的视觉问答对，专门针对五项关键的ITS任务：对象和事件识别、对象计数、对象定位、背景分析和事件推理。", "result": "实验结果显示MITS大大提高了各种主流LMM模型在ITS任务上的表现。例如，LLaVA-1.5的表现从0.494提升到0.905（+83.2%），LLaVA-1.6从0.678提升到0.921（+35.8%），Qwen2-VL从0.584提升到0.926（+58.6%），以及Qwen2.5-VL从0.732提升到0.930（+27.0%）。", "conclusion": "开放资源的发布为推动ITS和LMM研究提供了极具价值的工具。"}}
{"id": "2509.09708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09708", "abs": "https://arxiv.org/abs/2509.09708", "authors": ["Nirmalendu Prakash", "Yeo Wei Jie", "Amir Abdullah", "Ranjan Satapathy", "Erik Cambria", "Roy Ka Wei Lee"], "title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal", "comment": null, "summary": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned\nlarge language models (LLMs), yet the internal causes of this behaviour remain\npoorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT\nand LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on\nresidual-stream activations. Given a harmful prompt, we search the SAE latent\nspace for feature sets whose ablation flips the model from refusal to\ncompliance, demonstrating causal influence and creating a jailbreak. Our search\nproceeds in three stages: (1) Refusal Direction: find a refusal-mediating\ndirection and collect SAE features near that direction; (2) Greedy Filtering:\nprune to a minimal set; and (3) Interaction Discovery: fit a factorization\nmachine (FM) that captures nonlinear interactions among the remaining active\nfeatures and the minimal set. This pipeline yields a broad set of\njailbreak-critical features, offering insight into the mechanistic basis of\nrefusal. Moreover, we find evidence of redundant features that remain dormant\nunless earlier features are suppressed. Our findings highlight the potential\nfor fine-grained auditing and targeted intervention in safety behaviours by\nmanipulating the interpretable latent space.", "AI": {"tldr": "研究了两个公开的指令调优模型，并通过稀疏自动编码器和特征选择技术揭示了模型拒绝有害提示行为的内在机制。结果表明存在冗余特征，这些特征在早期特征被抑制时才会活跃，指出可以通过操作可解释的潜层空间来进行细粒度审计和针对性干涉安全行为。", "motivation": "理解指令调优大型语言模型（LLMs）在面对有害提示时的拒绝行为的内在原因。", "method": "使用稀疏自动编码器（SAEs）对残差流激活进行训练，以研究两个公开的指令调优模型（Gemma-2-2B-IT 和 LLaMA-3.1-8B-IT）在面对有害提示时拒绝行为的内部原因。具体步骤包括：（1）寻找一个介导拒绝的方向并收集附近的 SAE 特征；（2）通过贪婪筛选来简化特征集；（3）使用因子分解机（FM）拟合剩余活跃特征和最小特征集之间的非线性交互。", "result": "发现了能将模型从拒绝转变为顺从特征集，揭示了拒绝行为的因果影响，并且证明存在冗余特征，这些特征在早期特征受到抑制时才会活跃。", "conclusion": "通过对可解释潜层空间的操作，可以实现对安全行为的细粒度审计和技术干预，为模型的安全性改进提供了新的方向。"}}
{"id": "2509.09732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09732", "abs": "https://arxiv.org/abs/2509.09732", "authors": ["Sary Elmansoury", "Islam Mesabah", "Gerrit Großmann", "Peter Neigel", "Raj Bhalwankar", "Daniel Kondermann", "Sebastian J. Vollmer"], "title": "Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs", "comment": null, "summary": "Vision language models (VLMs) excel at zero-shot visual classification, but\ntheir performance on fine-grained tasks and large hierarchical label spaces is\nunderstudied. This paper investigates whether structured, tree-based reasoning\ncan enhance VLM performance. We introduce a framework that decomposes\nclassification into interpretable decisions using decision trees and evaluates\nit on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the\nmodel achieves 98.2% accuracy in understanding the tree knowledge, tree-based\nreasoning consistently underperforms standard zero-shot prompting. We also\nexplore enhancing the tree prompts with LLM-generated classes and image\ndescriptions to improve alignment. The added description enhances the\nperformance of the tree-based and zero-shot methods. Our findings highlight\nlimitations of structured reasoning in visual classification and offer insights\nfor designing more interpretable VLM systems.", "AI": {"tldr": "尽管有结构化推理框架提升了模型的可解释性，但并未显著改进其在细粒度和粗粒度任务中的分类表现。", "motivation": "研究动机在于调查有结构的、基于树的推理能否提高视觉语言模型在细粒度任务和大规模层级标签空间中的表现。", "method": "该研究采用框架将分类任务分解为使用决策树的可解释决策，并在细粒度（GTSRB）和粗粒度（CIFAR-10）数据集上进行评估。", "result": "尽管该模型在理解树知识上的准确率达到了98.2%，但其基于树的推理表现始终不如标准的零样本提示。同时发现，增加LLM生成的类别和图像描述能够提升基于树的和零样本方法的表现。", "conclusion": "研究结果揭示了结构化推理在视觉分类中的局限性，并为设计更具可解释性的视觉语言模型提供了见解。"}}
{"id": "2509.09709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09709", "abs": "https://arxiv.org/abs/2509.09709", "authors": ["Jing Ren", "Weiqi Wang"], "title": "Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement", "comment": null, "summary": "Large language models (LLMs) like ChatGPT are increasingly used in academic\nwriting, yet issues such as incorrect or fabricated references raise ethical\nconcerns. Moreover, current content quality evaluations often rely on\nsubjective human judgment, which is labor-intensive and lacks objectivity,\npotentially compromising the consistency and reliability. In this study, to\nprovide a quantitative evaluation and enhance research proposal writing\ncapabilities of LLMs, we propose two key evaluation metrics--content quality\nand reference validity--and an iterative prompting method based on the scores\nderived from these two metrics. Our extensive experiments show that the\nproposed metrics provide an objective, quantitative framework for assessing\nChatGPT's writing performance. Additionally, iterative prompting significantly\nenhances content quality while reducing reference inaccuracies and\nfabrications, addressing critical ethical challenges in academic contexts.", "AI": {"tldr": "本文提出了一种定量评估大型语言模型写作质量的方法，包括内容质量和引用有效性两个方面，基于这些指标，迭代提示方法提升了写作质量并减少了伦理问题。", "motivation": "解决大型语言模型在学术写作中由于无效或捏造引用而产生的伦理问题，以及目前内容质量评估依赖于耗时且缺乏客观性的人类判断的问题。", "method": "提出两个关键评估指标——内容质量和引用有效性，并基于这两个指标的得分提出了一种迭代提示方法。", "result": "实验结果显示，提出的指标为评估ChatGPT的写作性能提供了客观、定量的框架，并且迭代提示显著提高了内容质量，减少了引用不准确和捏造的问题。", "conclusion": "该方法能够有效解决学术环境中使用LLMs时的重要伦理挑战。"}}
{"id": "2509.09737", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09737", "abs": "https://arxiv.org/abs/2509.09737", "authors": ["Klemen Kotar", "Wanhee Lee", "Rahul Venkatesh", "Honglin Chen", "Daniel Bear", "Jared Watrous", "Simon Kim", "Khai Loong Aw", "Lilian Naing Chen", "Stefan Stojanov", "Kevin Feigelis", "Imran Thobani", "Alex Durango", "Khaled Jedoui", "Atlas Kazemian", "Dan Yamins"], "title": "World Modeling with Probabilistic Structure Integration", "comment": null, "summary": "We present Probabilistic Structure Integration (PSI), a system for learning\nrichly controllable and flexibly promptable world models from data. PSI\nconsists of a three-step cycle. The first step, Probabilistic prediction,\ninvolves building a probabilistic graphical model Psi of the data, in the form\nof a random-access autoregressive sequence model. Psi supports a complete set\nof learned conditional distributions describing the dependence of any variables\nin the data on any other set of variables. In step 2, Structure extraction, we\nshow how to extract underlying low-dimensional properties in the data,\ncorresponding to a diverse set of meaningful \"intermediate structures\", in a\nzero-shot fashion via causal inference on Psi. Step 3, Integration, completes\nthe cycle by converting these structures into new token types that are then\ncontinually mixed back into the training diet as conditioning signals and\nprediction targets. Each such cycle augments the capabilities of Psi, both\nallowing it to model the underlying data better, and creating new control\nhandles -- akin to an LLM-like universal prompting language. We train an\ninstance of Psi on 1.4 trillion tokens of internet video data; we use it to\nperform a variety of useful video prediction and understanding inferences; we\nextract state-of-the-art optical flow, self-supervised depth and object\nsegmentation; and we use these structures to support a full cycle of predictive\nimprovements.", "AI": {"tldr": "本文介绍了Psi系统，能够通过独特的三步循环从大规模视频数据中学习世界模型，并在视频预测和理解方面取得了显著成效。", "motivation": "开发Psi系统，旨在从数据中学习丰富可控制和灵活提示的世界模型。", "method": "Psi系统由三步循环构成：概率预测、结构提取与整合。结构提取通过因果推断从Psi中零样本提取低维特性，转化为新的token类型，每次循环都增强Psi的能力，提升其对数据的建模能力和创建新的控制手段。", "result": "使用Psi对1.4万亿个视频数据标记进行训练，能够执行多种有用的视频预测和理解推理，提取出最先进的光流、自我监督深度和物体分割，并用这些结构支持完整的预测改进周期。", "conclusion": "Psi系统通过其独特的方法在视频理解和预测方面展现了卓越的能力，为建立丰富可控制的世界模型开辟了一条新途径。"}}
{"id": "2509.09710", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09710", "abs": "https://arxiv.org/abs/2509.09710", "authors": ["Sepehr Golrokh Amin", "Devin Rhoads", "Fatemeh Fakhrmoosavi", "Nicholas E. Lownes", "John N. Ivan"], "title": "Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data", "comment": null, "summary": "This study introduces a Large Language Model (LLM) scheme for generating\nindividual travel diaries in agent-based transportation models. While\ntraditional approaches rely on large quantities of proprietary household travel\nsurveys, the method presented in this study generates personas stochastically\nfrom open-source American Community Survey (ACS) and Smart Location Database\n(SLD) data, then synthesizes diaries through direct prompting. This study\nfeatures a novel one-to-cohort realism score: a composite of four metrics (Trip\nCount Score, Interval Score, Purpose Score, and Mode Score) validated against\nthe Connecticut Statewide Transportation Study (CSTS) diaries, matched across\ndemographic variables. The validation utilizes Jensen-Shannon Divergence to\nmeasure distributional similarities between generated and real diaries. When\ncompared to diaries generated with classical methods (Negative Binomial for\ntrip generation; Multinomial Logit for mode/purpose) calibrated on the\nvalidation set, LLM-generated diaries achieve comparable overall realism (LLM\nmean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and\ndemonstrates greater consistency (narrower realism score distribution), while\nclassical models lead in numerical estimates of trip count and activity\nduration. Aggregate validation confirms the LLM's statistical\nrepresentativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot\nviability and establishing a quantifiable metric of diary realism for future\nsynthetic diary evaluation systems.", "AI": {"tldr": "本文介绍了一种用于生成智能交通模型中个人旅行日记的大语言模型（LLM）方案。该方法使用开源数据创建随机角色，并通过直接提示生成日记。该研究提出了一种新颖的群体现实主义评分，用于验证生成的日记与实际日记之间的相似性。结果显示，LLM生成的日记在总体现实性和统计代表性方面与传统方法相当甚至更好。", "motivation": "传统的旅行日记生成方法依赖于大量的专有家庭旅行调查数据，而本文的方法依赖于开源数据进行随机角色生成，并使用直接提示技术合成日记，以提高数据的开放性和可用性。", "method": "研究开发了一种新颖的群体现实主义评分，这款评分由四个指标组成（旅行次数评分、时间间隔评分、目的评分和模式评分），用于验证生成的日记和实际日记之间的分布相似性。", "result": "验证结果显示，LLM生成的日记在确定旅行目的以及一致性上有更好的表现，虽然传统的负二项式和多项式逻辑模型在旅行次数和活动时长的数值估计上略胜一筹，但LLM在整体现实性和统计代表性方面还是能够与传统方法媲美。", "conclusion": "LLM生成的旅行日记在统计代表性和总体现实性方面具有较强的零样本执行力，进而证明其对于未来合成日记评估系统的可行性。"}}
{"id": "2509.09742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09742", "abs": "https://arxiv.org/abs/2509.09742", "authors": ["Md Fazle Rasul", "Alanood Alqobaisi", "Bruhadeshwar Bezawada", "Indrakshi Ray"], "title": "Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning", "comment": null, "summary": "Federated learning (FL) allows multiple entities to train a shared model\ncollaboratively. Its core, privacy-preserving principle is that participants\nonly exchange model updates, such as gradients, and never their raw, sensitive\ndata. This approach is fundamental for applications in domains where privacy\nand confidentiality are important. However, the security of this very mechanism\nis threatened by gradient inversion attacks, which can reverse-engineer private\ntraining data directly from the shared gradients, defeating the purpose of FL.\nWhile the impact of these attacks is known for image, text, and tabular data,\ntheir effect on video data remains an unexamined area of research. This paper\npresents the first analysis of video data leakage in FL using gradient\ninversion attacks. We evaluate two common video classification approaches: one\nemploying pre-trained feature extractors and another that processes raw video\nframes with simple transformations. Our initial results indicate that the use\nof feature extractors offers greater resilience against gradient inversion\nattacks. We also demonstrate that image super-resolution techniques can enhance\nthe frames extracted through gradient inversion attacks, enabling attackers to\nreconstruct higher-quality videos. Our experiments validate this across\nscenarios where the attacker has access to zero, one, or more reference frames\nfrom the target environment. We find that although feature extractors make\nattacks more challenging, leakage is still possible if the classifier lacks\nsufficient complexity. We, therefore, conclude that video data leakage in FL is\na viable threat, and the conditions under which it occurs warrant further\ninvestigation.", "AI": {"tldr": "本篇论文分析了在联邦学习中视频数据泄漏的情况，特别是通过梯度逆向攻击技术。结果表明，与处理原始视频帧的方法相比，使用预训练特征提取器的方法对梯度逆向攻击更具抵抗力。然而，即使使用特征提取器，若分类器复杂度不足，泄露仍然是可能的。", "motivation": "视频数据在FL中的梯度逆向攻击效果尚未被广泛研究。此研究旨在填补这一空白，探索视频数据在FL中的隐私保护情况。", "method": "研究评估了两种常见的视频分类方法：一种使用预训练特征提取器，另一种处理经过简单变换的原始视频帧。通过这些方法来研究视频数据在梯度逆向攻击中的泄漏情况。", "result": "初步结果表明，使用预训练特征提取器的方法对梯度逆向攻击更具抵抗力。此外，还展示了如何使用图像超分辨率技术来增强通过梯度逆向攻击获取的帧，从而允许攻击者恢复出更高清晰度的视频。", "conclusion": "论文得出结论，FL中的视频数据泄漏是一个现实的威胁，且其发生的条件需要进一步的研究才能更全面地理解。"}}
{"id": "2509.09711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09711", "abs": "https://arxiv.org/abs/2509.09711", "authors": ["Aya E. Fouda", "Abdelrahamn A. Hassan", "Radwa J. Hanafy", "Mohammed E. Fouda"], "title": "Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry", "comment": null, "summary": "Large language models (LLMs) hold great promise in enhancing psychiatric\npractice, from improving diagnostic accuracy to streamlining clinical\ndocumentation and therapeutic support. However, existing evaluation resources\nheavily rely on small clinical interview corpora, social media posts, or\nsynthetic dialogues, which limits their clinical validity and fails to capture\nthe full complexity of psychiatric reasoning. In this work, we introduce\nPsychiatryBench, a rigorously curated benchmark grounded exclusively in\nauthoritative, expert-validated psychiatric textbooks and casebooks.\nPsychiatryBench comprises eleven distinct question-answering tasks ranging from\ndiagnostic reasoning and treatment planning to longitudinal follow-up,\nmanagement planning, clinical approach, sequential case analysis, and\nmultiple-choice/extended matching formats totaling over 5,300 expert-annotated\nitems. We evaluate a diverse set of frontier LLMs (including Google Gemini,\nDeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models\n(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an\n\"LLM-as-judge\" similarity scoring framework. Our results reveal substantial\ngaps in clinical consistency and safety, particularly in multi-turn follow-up\nand management tasks, underscoring the need for specialized model tuning and\nmore robust evaluation paradigms. PsychiatryBench offers a modular, extensible\nplatform for benchmarking and improving LLM performance in high-stakes mental\nhealth applications.", "AI": {"tldr": "本研究推出PsychiatryBench，以规范大型语言模型在精神健康领域的应用，发现现有模型在临床一致性与安全性上存在显著不足。", "motivation": "大型语言模型（LLMs）在提升精神健康实践中有巨大潜力，但现有的评估资源存在局限，依赖小样本临床访谈、社交媒体帖子或合成对话，这限制了其临床有效性。通过PsychiatryBench，研究旨在提供一个更有效的评估方法。", "method": "本研究引入了PsychiatryBench，这是一个基于权威和专家验证的教科书及案例集构建的严格筛选基准。该基准包含十一项不同的问答任务，涵盖了从诊断推理和治疗规划到纵向随访、管理规划、临床方式、序列案例分析以及多种选择/扩展匹配格式，总共超过5,300个专家标注的项目。", "result": "研究结果表明，在多回合随访和管理任务上，模型在临床一致性与安全性上存在显著差距，这反映了当前模型需要进行专门调优并存在更具挑战性的评估范式需求。", "conclusion": "PsychiatryBench提供了一种模块化、可扩展的模型测评与提升平台，特别是在高风险的心理健康应用领域。"}}
{"id": "2509.09750", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09750", "abs": "https://arxiv.org/abs/2509.09750", "authors": ["Hossein Yazdanjouei", "Arash Mansouri", "Mohammad Shokouhifar"], "title": "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images", "comment": null, "summary": "This study proposes a semi-supervised co-training framework for object\ndetection in densely packed retail environments, where limited labeled data and\ncomplex conditions pose major challenges. The framework combines Faster R-CNN\n(utilizing a ResNet backbone) for precise localization with YOLO (employing a\nDarknet backbone) for global context, enabling mutual pseudo-label exchange\nthat improves accuracy in scenes with occlusion and overlapping objects. To\nstrengthen classification, it employs an ensemble of XGBoost, Random Forest,\nand SVM, utilizing diverse feature representations for higher robustness.\nHyperparameters are optimized using a metaheuristic-driven algorithm, enhancing\nprecision and efficiency across models. By minimizing reliance on manual\nlabeling, the approach reduces annotation costs and adapts effectively to\nfrequent product and layout changes common in retail. Experiments on the\nSKU-110k dataset demonstrate strong performance, highlighting the scalability\nand practicality of the proposed framework for real-world retail applications\nsuch as automated inventory tracking, product monitoring, and checkout systems.", "AI": {"tldr": "本文提出了一种半监督协同训练框架以解决密集零售环境下的物体检测问题，通过结合Faster R-CNN和YOLO，同时使用多种分类算法优化了物体检测的准确性。", "motivation": "该研究旨在解决零售环境中物体检测的高难度问题，这些问题包括标记数据有限、环境复杂以及频繁的商品和布局变化。为了提高检测准确性同时降低成本，提出了本方法。", "method": "本文提出了一种半监督协同训练框架，用于解决密集零售环境中物体检测问题，这种方法结合了Faster R-CNN（使用ResNet作为骨干网络）进行精确定位和YOLO（使用Darknet作为骨干网络）进行全局环境感知，通过相互交换伪标签提高了遮挡和重叠物体场景下的准确性。此外，为了增强分类性能，利用了XGBoost、随机森林和SVM的集成方法，采用了多种特征表示方式提高鲁棒性。", "result": "实验结果显示，所提出的框架能有效改善遮挡和重叠物体检测问题的准确性，减少了手动标注的成本，并适应了零售环境中频繁的商品和布局变动。", "conclusion": "实验结果展示了所提出的框架在SKU-110k数据集上的出色性能，证明了其在实际零售应用（如自动化库存追踪、产品监控和结算系统）中的扩展性和实用性。"}}
{"id": "2509.09712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09712", "abs": "https://arxiv.org/abs/2509.09712", "authors": ["Talha Tahir"], "title": "The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization", "comment": null, "summary": "Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral\ntherapy with emerging evidence of efficacy in several psychiatric conditions.\nThis study investigates the impact of post-training methodology and explicit\nreasoning on the ability of a small open-weight large language model (LLM) to\ndeliver ACT. Using 50 sets of synthetic ACT transcripts generated by\nMistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,\nsupervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each\nwith and without an explicit chain-of-thought (COT) reasoning step. Performance\nwas evaluated by comparing these four post-trained variants against the base\nInstruct model. These models were benchmarked in simulated therapy sessions,\nwith performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)\nand the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned\non human evaluations. Our findings demonstrate that the ORPO-trained models\nsignificantly outperformed both their SFT and Instruct counterparts on ACT\nfidelity ($\\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\\chi^2(5) =\n140.37, p < .001$). The effect of COT was conditional as it provided a\nsignificant benefit to SFT models, improving ACT-FM scores by an average of\n2.68 points ($p < .001$), while offering no discernible advantage to the\nsuperior ORPO or instruct-tuned variants. We posit that the superiority of ORPO\nstems from its ability to learn the therapeutic `process' over imitating\n`content,' a key aspect of ACT, while COT acts as a necessary scaffold for\nmodels trained only via imitation. This study establishes that\npreference-aligned policy optimization can effectively instill ACT competencies\nin small LLMs, and that the utility of explicit reasoning is highly dependent\non the underlying training paradigm.", "AI": {"tldr": "本研究发现，相较于监督微调（SFT），通过赔率比策略优化（ORPO）训练的小型语言模型在ACT忠实体现出色，并在治疗共情方面优于仅采用SFT或基础模型。链式思维（COT）仅对SFT有提升作用。", "motivation": "研究旨在探讨训练后的方法和技术对小型开放权重大语言模型（LLM）提供ACT疗法能力的影响。", "method": "本研究通过两种不同的方法对Llama-3.2-3b-Instruct模型进行了训练，分别是监督微调（SFT）和赔率比策略优化（ORPO），每种方法分别加入或不加入显式的链式思维（COT）步骤。使用由Mistral-Large生成的50套合成ACT对话记录来进行训练。", "result": "研究发现，经过ORPO训练的模型在ACT保真度和治疗共情方面显著优于SFT和基础Instruct模型。加入COT步骤仅对SFT模型有显著的积极作用，提高了ACT-FM分数，而对ORPO和基础Instruct模型没有明显好处。", "conclusion": "研究表明，偏好对齐的策略优化可以有效地在小型LLM中嵌入ACT能力，并且显式推理的实用价值高度依赖于基础训练范式。"}}
{"id": "2509.09785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09785", "abs": "https://arxiv.org/abs/2509.09785", "authors": ["Moslem Yazdanpanah", "Ali Bahri", "Mehrdad Noori", "Sahar Dastani", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging", "comment": null, "summary": "Test-time adaptation (TTA) is crucial for mitigating performance degradation\ncaused by distribution shifts in 3D point cloud classification. In this work,\nwe introduce Token Purging (PG), a novel backpropagation-free approach that\nremoves tokens highly affected by domain shifts before they reach attention\nlayers. Unlike existing TTA methods, PG operates at the token level, ensuring\nrobust adaptation without iterative updates. We propose two variants: PG-SP,\nwhich leverages source statistics, and PG-SF, a fully source-free version\nrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,\nShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of\n+10.3\\% higher accuracy than state-of-the-art backpropagation-free methods,\nwhile PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is\n12.4 times faster and 5.5 times more memory efficient than our baseline, making\nit suitable for real-world deployment. Code is available at\n\\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}", "AI": {"tldr": "Token Purging (PG) is a novel backpropagation-free test-time adaptation approach for 3D point cloud classification, offering higher accuracy and efficiency over state-of-the-art methods.", "motivation": "The motivation is to address the issue of performance degradation due to distribution shifts in 3D point cloud classification and to introduce a robust adaptation mechanism without iterative updates.", "method": "Token Purging (PG) is introduced as a backpropagation-free approach for test-time adaptation (TTA) in 3D point cloud classification. It removes domain-shift-affected tokens before the attention layers. Two variants are proposed: PG-SP using source statistics and PG-SF, a fully source-free method using CLS-token-driven adaptation.", "result": "Evaluations on ModelNet40-C, ShapeNet-C, and ScanObjectNN-C show that PG-SP provides an average of +10.3% higher accuracy than state-of-the-art backpropagation-free methods, while PG-SF sets new benchmarks for source-free adaptation.", "conclusion": "The conclusion is that Token Purging (PG) is effective as a test-time adaptation technique, showing significant improvements in accuracy and being more efficient in terms of speed and memory usage compared to the baseline."}}
{"id": "2509.09713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09713", "abs": "https://arxiv.org/abs/2509.09713", "authors": ["Duolin Sun", "Dan Yang", "Yue Shen", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Lianzhen Zhong", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering", "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering\nsystems and dialogue generation tasks by integrating information retrieval (IR)\ntechnologies with large language models (LLMs). This strategy, which retrieves\ninformation from external knowledge bases to bolster the response capabilities\nof generative models, has achieved certain successes. However, current RAG\nmethods still face numerous challenges when dealing with multi-hop queries. For\ninstance, some approaches overly rely on iterative retrieval, wasting too many\nretrieval steps on compound queries. Additionally, using the original complex\nquery for retrieval may fail to capture content relevant to specific\nsub-queries, resulting in noisy retrieved content. If the noise is not managed,\nit can lead to the problem of noise accumulation. To address these issues, we\nintroduce HANRAG, a novel heuristic-based framework designed to efficiently\ntackle problems of varying complexity. Driven by a powerful revelator, HANRAG\nroutes queries, decomposes them into sub-queries, and filters noise from\nretrieved documents. This enhances the system's adaptability and noise\nresistance, making it highly capable of handling diverse queries. We compare\nthe proposed framework against other leading industry methods across various\nbenchmarks. The results demonstrate that our framework obtains superior\nperformance in both single-hop and multi-hop question-answering tasks.", "AI": {"tldr": "HANRAG框架通过改进查询处理过程，提高了应对单跳和多跳问题的性能。", "motivation": "该研究旨在解决现有RAG方法在处理多跳查询时遇到的问题，如过多依赖迭代检索和使用原始复杂查询检索等问题，旨在提高处理多样化查询的能力。", "method": "HANRAG采用了一种基于启发式的方法，通过一个强大的指示器将查询进行路由、分解成子查询，并从检索到的文档中过滤噪音，从而提高系统的适应性和噪声抵抗力。", "result": "实验结果显示，HANRAG框架在单跳和多跳问题的回答任务中都表现出优越的性能。", "conclusion": "HANRAG作为一种新颖的启发式框架，提升了系统对于不同复杂度查询问题的处理能力，并且在多种基准测试中优于其他工业方法。"}}
{"id": "2509.09792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09792", "abs": "https://arxiv.org/abs/2509.09792", "authors": ["Zimin Xia", "Chenghao Xu", "Alexandre Alahi"], "title": "Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors", "comment": null, "summary": "We propose an accurate and highly interpretable fine-grained cross-view\nlocalization method that estimates the 3 Degrees of Freedom pose of a\nground-level image by matching its local features with a reference aerial\nimage. Previous methods typically transform the ground image into a bird's-eye\nview (BEV) representation and then align it with the aerial image for\nlocalization. However, this transformation often leads to information loss due\nto perspective distortion or compression of height information, thereby\ndegrading alignment quality with the aerial view. In contrast, our method\ndirectly establishes correspondences between ground and aerial images and lifts\nonly the matched keypoints to BEV space using monocular depth prior. Notably,\nmodern depth predictors can provide reliable metric depth when the test samples\nare similar to the training data. When the depth distribution differs, they\nstill produce consistent relative depth, i.e., depth accurate up to an unknown\nscale. Our method supports both metric and relative depth. It employs a\nscale-aware Procrustes alignment to estimate the camera pose from the\ncorrespondences and optionally recover the scale when using relative depth.\nExperimental results demonstrate that, with only weak supervision on camera\npose, our method learns accurate local feature correspondences and achieves\nsuperior localization performance under challenging conditions, such as\ncross-area generalization and unknown orientation. Moreover, our method is\ncompatible with various relative depth models without requiring per-model\nfinetuning. This flexibility, combined with strong localization performance,\nmakes it well-suited for real-world deployment.", "AI": {"tldr": "A new approach for accurate and interpretable fine-grained cross-view localization between ground-level and aerial images, overcoming limitations of previous methods through direct correspondence matching and scale-aware alignment.", "motivation": "To improve the alignment quality with the aerial view by avoiding the information loss due to perspective distortion or compression of height information that occurs in the previous alignment methods.", "method": "Our method involves direct correspondence matching between ground and aerial images, lifting matched keypoints to BEV space, and using scale-aware Procrustes alignment for estimating the camera pose, which supports both metric and relative depth.", "result": "The method learns accurate local feature correspondences and achieves superior localization performance, especially under challenging conditions, such as cross-area generalization and unknown orientation.", "conclusion": "The proposed method displays strong localization performance and is flexible as it is compatible with various relative depth models without per-model finetuning, making it suitable for real-world deployment."}}
{"id": "2509.09714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09714", "abs": "https://arxiv.org/abs/2509.09714", "authors": ["Serge Lionel Nikiema", "Albérick Euraste Djire", "Abdoul Aziz Bonkoungou", "Micheline Bénédicte Moumoula", "Jordan Samhi", "Abdoul Kader Kabore", "Jacques Klein", "Tegawendé F. Bissyande"], "title": "How Small Transformation Expose the Weakness of Semantic Similarity Measures", "comment": null, "summary": "This research examines how well different methods measure semantic\nsimilarity, which is important for various software engineering applications\nsuch as code search, API recommendations, automated code reviews, and\nrefactoring tools. While large language models are increasingly used for these\nsimilarity assessments, questions remain about whether they truly understand\nsemantic relationships or merely recognize surface patterns.\n  The study tested 18 different similarity measurement approaches, including\nword-based methods, embedding techniques, LLM-based systems, and\nstructure-aware algorithms. The researchers created a systematic testing\nframework that applies controlled changes to text and code to evaluate how well\neach method handles different types of semantic relationships.\n  The results revealed significant issues with commonly used metrics. Some\nembedding-based methods incorrectly identified semantic opposites as similar up\nto 99.9 percent of the time, while certain transformer-based approaches\noccasionally rated opposite meanings as more similar than synonymous ones. The\nstudy found that embedding methods' poor performance often stemmed from how\nthey calculate distances; switching from Euclidean distance to cosine\nsimilarity improved results by 24 to 66 percent. LLM-based approaches performed\nbetter at distinguishing semantic differences, producing low similarity scores\n(0.00 to 0.29) for genuinely different meanings, compared to embedding methods\nthat incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.", "AI": {"tldr": "研究评估了18种不同的语义相似度测量方法，包括基于词的方法、嵌入技术、基于LLM的系统和结构感知算法。结果显示，一些嵌入方法在识别语义相反的概念时存在严重问题，而基于LLM的方法表现更好。", "motivation": "研究旨在探讨不同方法测量语义相似度的能力，特别是在软件工程应用中的表现，如代码搜索、API推荐、自动化代码审查和重构工具。", "method": "Structure", "result": "一些基于嵌入的方法错误地将语义相反的概念识别为相似，而基于LLM的方法在区分语义差异方面表现更好。同时，嵌入方法的性能可以通过将距离计算从欧几里得距离改为余弦相似度来提高。", "conclusion": "基于嵌入的方法在识别语义相似度时表现不佳，而基于LLM的方法则显示出更高的准确性和区分度。"}}
{"id": "2509.09808", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09808", "abs": "https://arxiv.org/abs/2509.09808", "authors": ["Judith Massmann", "Alexander Lichtenstein", "Francisco M. López"], "title": "Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test", "comment": "Accepted at IEEE ICDL 2025. 6 pages, 7 figures, 2 tables", "summary": "Numerous visual impairments can be detected in red-eye reflex images from\nyoung children. The so-called Bruckner test is traditionally performed by\nophthalmologists in clinical settings. Thanks to the recent technological\nadvances in smartphones and artificial intelligence, it is now possible to\nrecreate the Bruckner test using a mobile device. In this paper, we present a\nfirst study conducted during the development of KidsVisionCheck, a free\napplication that can perform vision screening with a mobile device using\nred-eye reflex images. The underlying model relies on deep neural networks\ntrained on children's pupil images collected and labeled by an ophthalmologist.\nWith an accuracy of 90% on unseen test data, our model provides highly reliable\nperformance without the necessity of specialist equipment. Furthermore, we can\nidentify the optimal conditions for data collection, which can in turn be used\nto provide immediate feedback to the users. In summary, this work marks a first\nstep toward accessible pediatric vision screenings and early intervention for\nvision abnormalities worldwide.", "AI": {"tldr": "本文介绍了一项研究，开发了名为KidsVisionCheck的应用，利用智能手机和深度学习技术进行儿童视力筛查，准确率达到90%，并在数据收集条件上提供了反馈。", "motivation": "随着智能手机和人工智能技术的发展，希望开发一种应用，使用移动设备通过红眼反射图像进行视觉筛查，从而普及儿科视力筛查。", "method": "本研究采用深度神经网络模型，该模型基于由眼科医生收集并标注的儿童瞳孔图像进行训练。", "result": "模型在未见过的测试数据上达到了90%的准确率，且无需专用设备。", "conclusion": "这项工作标志着向全球可及的儿科视力筛查和早期干预视力异常迈进了一步。"}}
{"id": "2509.09715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09715", "abs": "https://arxiv.org/abs/2509.09715", "authors": ["Naveen Lamba", "Sanju Tiwari", "Manas Gaur"], "title": "Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA", "comment": null, "summary": "Hallucination in Large Language Models (LLMs) is a well studied problem.\nHowever, the properties that make LLM intrinsically vulnerable to\nhallucinations have not been identified and studied. This research identifies\nand characterizes the key properties, allowing us to pinpoint vulnerabilities\nwithin the model's internal mechanisms. To solidify on these properties, we\nutilized two established datasets, HaluEval and TruthfulQA and convert their\nexisting format of question answering into various other formats to narrow down\nthese properties as the reason for the hallucinations. Our findings reveal that\nhallucination percentages across symbolic properties are notably high for\nGemma-2-2B, averaging 79.0% across tasks and datasets. With increased model\nscale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,\nreflecting a 15 percentage point reduction overall. Although the hallucination\nrate decreases as the model size increases, a substantial amount of\nhallucination caused by symbolic properties still persists. This is especially\nevident for modifiers (ranging from 84.76% to 94.98%) and named entities\n(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.\nThese findings indicate that symbolic elements continue to confuse the models,\npointing to a fundamental weakness in how these LLMs process such\ninputs--regardless of their scale.", "AI": {"tldr": "研究识别了使大语言模型（LLMs）固有易受幻觉影响的关键特性，并通过转换现有问答数据集格式证实了这些特性，发现即使随着模型规模增大，由符号性元素导致的幻觉现象依然显著存在。", "motivation": "LLM内在的幻觉问题已被广泛研究，但其内在脆弱性的具体特性尚未明确。本研究旨在识别并描述这些特性，从而找出模型内部机制中的弱点。", "method": "使用了HaluEval和TruthfulQA两个已建立的数据集，将现有问答格式转换为其他格式，以便缩小问题特性。", "result": "发现对于Gemma-2-2B，其幻觉率平均为79.0%，随着模型规模的增加到Gemma-2-9B幻觉率降低到73.6%，Gemma-2-27B时幻觉率降为63.9%。但是，象征性元素依然导致显著幻觉。", "conclusion": "研究发现随着模型规模的增加，尽管幻觉率有所下降，由象征性元素（如修饰语和专有名词）引起的显著幻觉仍然存在，这表明LLM处理这类输入有基础性缺陷。"}}
{"id": "2509.09828", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09828", "abs": "https://arxiv.org/abs/2509.09828", "authors": ["Tim Broedermannn", "Christos Sakaridis", "Luigi Piccinelli", "Wim Abbeloos", "Luc Van Gool"], "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception", "comment": "Code and models will be available at\n  https://github.com/timbroed/DGFusion", "summary": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion", "AI": {"tldr": "DGFusion利用深度信息进行多模态融合，改善了自动驾驶环境感知能力，尤其在复杂的条件下表现优秀。", "motivation": "在挑战性条件下，传统传感器融合方法表现不佳。DGFusion提出通过利用深度信息调整传感器数据的融合策略，以提升感知性能。", "method": "structure", "result": "<tool_call>\n{{\"name\": \"Structure\", \"arguments\": {\"tldr\": \"提出了一种基于深度引导的多模态融合方法DGFusion，通过利用LiDAR测量和学习深度信息，提高了在复杂环境下的语义感知能力。实验表明，该方法在MUSES和DELIVER数据集上实现了最先进的全景和语义分割性能。\", \"motivation\": \"现有的传感器融合方法在空间上均匀处理传感器数据，遇到具有挑战性条件时表现不佳。为了解决这个问题，提出了DGFusion方法，它利用深度信息动态调整传感器融合策略。\", \"method\": \"DGFusion方法将多模态分割作为多任务问题处理，使用LiDAR测量作为输入和学习深度的地面事实。网络中的辅助深度头帮助学习深度感知特征，并将其编码为空间变化的本地深度令牌，以调节交叉模态融合。\", \"result\": \"DGFusion在具有挑战性的MUSES和DELIVER数据集上实现了最先进的全景和语义分割性能。\", \"conclusion\": \"总之，DGFusion方法在有效地结合来自具有互补优势的多个传感器的数据方面取得了成功，尤其是在复杂环境下。该方法有助于提高自动驾驶车辆的语义感知性能。\"}}}\n</tool_call>", "conclusion": "DGFusion展示了其在复杂环境下的优势，巩固了其在语义感知技术中的地位。"}}
{"id": "2509.09723", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME", "I.2.6; J.4; I.5.1; H.3.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2509.09723", "abs": "https://arxiv.org/abs/2509.09723", "authors": ["Kai R. Larsen", "Sen Yan", "Roland Müller", "Lan Sang", "Mikko Rönkkö", "Ravi Starzl", "Donald Edmondson"], "title": "ALIGNS: Unlocking nomological networks in psychological measurement through a large language model", "comment": null, "summary": "Psychological measurement is critical to many disciplines. Despite advances\nin measurement, building nomological networks, theoretical maps of how concepts\nand measures relate to establish validity, remains a challenge 70 years after\nCronbach and Meehl proposed them as fundamental to validation. This limitation\nhas practical consequences: clinical trials may fail to detect treatment\neffects, and public policy may target the wrong outcomes. We introduce Analysis\nof Latent Indicators to Generate Nomological Structures (ALIGNS), a large\nlanguage model-based system trained with validated questionnaire measures.\nALIGNS provides three comprehensive nomological networks containing over\n550,000 indicators across psychology, medicine, social policy, and other\nfields. This represents the first application of large language models to solve\na foundational problem in measurement validation. We report classification\naccuracy tests used to develop the model, as well as three evaluations. In the\nfirst evaluation, the widely used NIH PROMIS anxiety and depression instruments\nare shown to converge into a single dimension of emotional distress. The second\nevaluation examines child temperament measures and identifies four potential\ndimensions not captured by current frameworks, and questions one existing\ndimension. The third evaluation, an applicability check, engages expert\npsychometricians who assess the system's importance, accessibility, and\nsuitability. ALIGNS is freely available at nomologicalnetwork.org,\ncomplementing traditional validation methods with large-scale nomological\nanalysis.", "AI": {"tldr": "ALIGNS, a large language model, is introduced to help build nomological networks, leading to more insightful classification and validation of questionnaire measures across various fields.", "motivation": "To address the challenges in building nomological networks for validated measurements, which is critical for fields like clinical trials and public policy, leading to more effective outcomes.", "method": "Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures.", "result": "ALIGNS created three comprehensive nomological networks with over 550,000 indicators across various fields. Three evaluations were conducted demonstrating its capacity to provide new insights and serve as an important complement to traditional validation methods.", "conclusion": "ALIGNS represents an innovative approach by using large language models to build nomological networks, which helps overcome the limitations of traditional validation methods by offering large-scale analysis."}}
{"id": "2509.09841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09841", "abs": "https://arxiv.org/abs/2509.09841", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework", "comment": null, "summary": "Rosacea, which is a chronic inflammatory skin condition that manifests with\nfacial redness, papules, and visible blood vessels, often requirs precise and\nearly detection for significantly improving treatment effectiveness. This paper\npresents new patch-based automatic rosacea detection strategies using the\nResNet-18 deep learning framework. The contributions of the proposed strategies\ncome from the following aspects. First, various image pateches are extracted\nfrom the facial images of people in different sizes, shapes, and locations.\nSecond, a number of investigation studies are carried out to evaluate how the\nlocalized visual information influences the deep learing model performance.\nThird, thorough experiments are implemented to reveal that several patch-based\nautomatic rosacea detection strategies achieve competitive or superior accuracy\nand sensitivity than the full-image based methods. And finally, the proposed\npatch-based strategies, which use only localized patches, inherently preserve\npatient privacy by excluding any identifiable facial features from the data.\nThe experimental results indicate that the proposed patch-based strategies\nguide the deep learning model to focus on clinically relevant regions, enhance\nrobustness and interpretability, and protect patient privacy. As a result, the\nproposed strategies offer practical insights for improving automated\ndermatological diagnostics.", "AI": {"tldr": "The paper introduces patch-based strategies using ResNet-18 for detecting rosacea, showing improved accuracy and privacy protection compared to full-image methods.", "motivation": "The motivation behind this research is to improve the precision and early detection of rosacea, which in turn increases the effectiveness of its treatment by focusing on clinically relevant areas while also preserving patient privacy.", "method": "This paper proposes a patch-based automatic rosacea detection method using the ResNet-18 deep learning framework. It extracts various facial image patches of different sizes, shapes, and locations for analysis.", "result": "The patch-based strategies were found to have competitive or superior accuracy and sensitivity compared to full-image based methods, enhancing robustness and interpretability.", "conclusion": "The proposed strategies are beneficial for improving automated dermatological diagnostics as they focus on relevant areas for detection, enhance robustness, and protect patient privacy."}}
{"id": "2509.09724", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T09"], "pdf": "https://arxiv.org/pdf/2509.09724", "abs": "https://arxiv.org/abs/2509.09724", "authors": ["Wonyoung Kim", "Sujeong Seo", "Juhyun Lee"], "title": "DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model", "comment": "5 figures", "summary": "Technology opportunities are critical information that serve as a foundation\nfor advancements in technology, industry, and innovation. This paper proposes a\nframework based on the temporal relationships between technologies to identify\nemerging technology opportunities. The proposed framework begins by extracting\ntext from a patent dataset, followed by mapping text-based topics to discover\ninter-technology relationships. Technology opportunities are then identified by\ntracking changes in these topics over time. To enhance efficiency, the\nframework leverages a large language model to extract topics and employs a\nprompt for a chat-based language model to support the discovery of technology\nopportunities. The framework was evaluated using an artificial intelligence\npatent dataset provided by the United States Patent and Trademark Office. The\nexperimental results suggest that artificial intelligence technology is\nevolving into forms that facilitate everyday accessibility. This approach\ndemonstrates the potential of the proposed framework to identify future\ntechnology opportunities.", "AI": {"tldr": "本文提出了一种基于专利数据的技术机会识别框架，通过追踪技术主题的变化来发现新技术机会，实验证明了该框架的有效性。", "motivation": "技术机会是推动技术、行业和创新发展的关键信息。本文提出的框架旨在通过分析专利数据中的技术时间关系来识别新兴技术机会。", "method": "本文提出了一种基于技术之间时间关系的框架来识别新兴技术机会。该框架首先从专利数据集中提取文本，然后通过映射文本主题来发现技术之间的关系。通过追踪这些主题随时间的变化来识别技术机会。为了提高效率，该框架利用大型语言模型提取主题，并使用对话式语言模型的提示来支持技术机会的发现。", "result": "该框架通过使用美国专利商标局提供的AI专利数据集进行了验证，实验结果表明人工智能技术正在朝向便于日常使用的方向发展。", "conclusion": "本文研究表明，所提出的框架具有识别未来技术机会的潜力。"}}
{"id": "2509.09844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09844", "abs": "https://arxiv.org/abs/2509.09844", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection", "comment": null, "summary": "Rosacea is a common but underdiagnosed inflammatory skin condition that\nprimarily affects the central face and presents with subtle redness, pustules,\nand visible blood vessels. Automated detection remains challenging due to the\ndiffuse nature of symptoms, the scarcity of labeled datasets, and privacy\nconcerns associated with using identifiable facial images. A novel\nprivacy-preserving automated rosacea detection method inspired by clinical\npriors and trained entirely on synthetic data is presented in this paper.\nSpecifically, the proposed method, which leverages the observation that rosacea\nmanifests predominantly through central facial erythema, first constructs a\nfixed redness-informed mask by selecting regions with consistently high red\nchannel intensity across facial images. The mask thus is able to focus on\ndiagnostically relevant areas such as the cheeks, nose, and forehead and\nexclude identity-revealing features. Second, the ResNet-18 deep learning\nmethod, which is trained on the masked synthetic images, achieves superior\nperformance over the full-face baselines with notable gains in terms of\naccuracy, recall and F1 score when evaluated using the real-world test data.\nThe experimental results demonstrate that the synthetic data and clinical\npriors can jointly enable accurate and ethical dermatological AI systems,\nespecially for privacy sensitive applications in telemedicine and large-scale\nscreening.", "AI": {"tldr": "The paper presents a novel privacy-preserving automated rosacea detection method using deep learning, which achieves superior performance by leveraging redness-informed mask and synthetic data.", "motivation": "To address the challenges of automated rosacea detection, including the diffuse nature of symptoms, scarcity of labeled datasets, and privacy concerns.", "method": "The method involves constructing a redness-informed mask to focus on diagnostically relevant areas and excluding identity-revealing features, followed by training a ResNet-18 model on synthetic masked facial images.", "result": "The model exhibits notable improvements in accuracy, recall, and F1 score over full-face baseline models when tested on real-world data.", "conclusion": "The results suggest that using synthetic data and clinical priors can lead to effective and ethical AI systems for diagnosing rosacea, with particular relevance to privacy-sensitive applications such as telemedicine."}}
{"id": "2509.09725", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09725", "abs": "https://arxiv.org/abs/2509.09725", "authors": ["Chunyu Li", "Xindi Zheng", "Siqi Liu"], "title": "BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025", "comment": null, "summary": "Entity linking (EL) for biomedical text is typically benchmarked on\nEnglish-only corpora with flat mentions, leaving the more realistic scenario of\nnested and multilingual mentions largely unexplored. We present our system for\nthe BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task\n(English & Russian), closing this gap with a lightweight pipeline that keeps\nthe original EL model intact and modifies only three task-aligned components:\nTwo-stage retrieval-ranking. We leverage the same base encoder model in both\nstages: the retrieval stage uses the original pre-trained model, while the\nranking stage applies domain-specific fine-tuning. Boundary cues. In the\nranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing\nthe encoder with an explicit, language-agnostic span before robustness to\noverlap and nesting. Dataset augmentation. We also automatically expand the\nranking training corpus with three complementary data sources, enhancing\ncoverage without extra manual annotation. On the BioNNE 2025 leaderboard, our\ntwo stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual\ntrack, demonstrating the effectiveness and competitiveness of these minimal yet\nprincipled modifications. Code are publicly available at\nhttps://github.com/Kaggle-Competitions-Code/BioNNE-L.", "AI": {"tldr": "本文针对生物医学文本中的多语言和嵌套实体链接问题，提出了一种通过少量调整实现显著改善的方法，该方法在比赛中表现良好。", "motivation": "现有的生物医学实体链接研究主要针对英文并且只处理扁平提及，忽略了嵌套和多语言提及的复杂情况。本文旨在通过最小化修改解决这一问题。", "method": "本研究采用了一种轻量级流水线，并修改其中三个组件：两阶段检索-排序、边界提示和数据集增强，来改进现有的EL模型。", "result": "本文研究了生物医学文本中的实体链接（EL），特别是针对英文和俄文的嵌套及多语言提及情况。研究采用了一种轻量级的流水线，通过只修改三个任务相关组件（两阶段检索-排序、边界提示、数据集增强）来改进了现有的EL模型。这些改进包括使用相同的预训练模型进行检索阶段，进行领域特定微调进行排序阶段，以及通过三种互补的数据来源自动扩展排序训练语料库。基于这些改进，该研究提出的系统（BIBERT-Pipe）在BioNNE 2025多语言赛道的排行榜中位列第三，代码在GitHub上公开。", "conclusion": "通过上述改进方法，本文提出的系统在BioNNE 2025多语言赛道中表现出了有效性与竞争力。代码已经公开，供后续研究使用。"}}
{"id": "2509.09849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09849", "abs": "https://arxiv.org/abs/2509.09849", "authors": ["Chengyu Yang", "Chengjun Liu"], "title": "Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking", "comment": null, "summary": "To rigorously assess the effectiveness and necessity of individual components\nwithin the recently proposed ULW framework for laparoscopic image desmoking,\nthis paper presents a comprehensive ablation study. The ULW approach combines a\nU-Net based backbone with a compound loss function that comprises mean squared\nerror (MSE), structural similarity index (SSIM) loss, and perceptual loss. The\nframework also incorporates a differentiable, learnable Wiener filter module.\nIn this study, each component is systematically ablated to evaluate its\nspecific contribution to the overall performance of the whole framework. The\nanalysis includes: (1) removal of the learnable Wiener filter, (2) selective\nuse of individual loss terms from the composite loss function. All variants are\nbenchmarked on a publicly available paired laparoscopic images dataset using\nquantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative\nvisual comparisons.", "AI": {"tldr": "本文通过详尽的消融研究来评估ULW框架中各个组件的有效性和必要性，该框架用于腹腔镜图像去烟雾处理。", "motivation": "动机在于分析和确认ULW框架中的各个组件对整体性能的具体贡献，特别是当这些组件被单独移除或更改时。", "method": "采用U-Net作为主干网络和包含MSE、SSIM和感知损失的复合损失函数，框架中还包括一个可学习的维纳滤波器模块。研究中逐一移除这些组件，以评估其贡献。", "result": "通过使用SSIM、PSNR、MSE和CIEDE-2000等定量指标以及定性的视觉比较，所有变体在公共腹腔镜图像数据集上进行基准测试。", "conclusion": "这个消融研究旨在提供对ULW框架组件作用的深入理解，并可能指导未来框架的改进。"}}
{"id": "2509.09726", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09726", "abs": "https://arxiv.org/abs/2509.09726", "authors": ["Seiji Hattori", "Takuya Matsuzaki", "Makoto Fujiwara"], "title": "Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure", "comment": "Submitted to INLG 2025 (accepted)", "summary": "This paper proposes a natural language translation method for\nmachine-verifiable formal proofs that leverages the informalization\n(verbalization of formal language proof steps) and summarization capabilities\nof LLMs. For evaluation, it was applied to formal proof data created in\naccordance with natural language proofs taken from an undergraduate-level\ntextbook, and the quality of the generated natural language proofs was analyzed\nin comparison with the original natural language proofs. Furthermore, we will\ndemonstrate that this method can output highly readable and accurate natural\nlanguage proofs by applying it to existing formal proof library of the Lean\nproof assistant.", "AI": {"tldr": "A method using large language models for translating formal proofs into natural language is proposed and evaluated, showing outputs that are highly readable and accurate.", "motivation": "The motivation is to make formal proofs more accessible and understandable by translating them into natural language proofs, improving readability and accessibility for non-expert users.", "method": "The paper employs large language models (LLMs) to informally translate machine-verifiable formal proofs into natural language through informalization and summarization capabilities.", "result": "The proposed method generates natural language proofs that are highly readable and accurate, as demonstrated by its application to formal proof data from a textbook and an existing Lean proof assistant library.", "conclusion": "The study concludes that leveraging LLMs to translate formal proofs into natural language can significantly enhance the readability and accuracy of such proofs, making them more accessible to a broader audience."}}
{"id": "2509.09859", "categories": ["cs.CV", "cs.LG", "68W99"], "pdf": "https://arxiv.org/pdf/2509.09859", "abs": "https://arxiv.org/abs/2509.09859", "authors": ["Razvan Stefanescu", "Ethan Oh", "Ruben Vazquez", "Chris Mesterharm", "Constantin Serban", "Ritu Chadha"], "title": "WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector", "comment": "11 pages, 11 figures", "summary": "We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and\nacoustic signals for robust real-life UAV object detection. Our approach fuses\nvisual and acoustic features in a unified object detector model relying on the\nDeformable DETR and Wav2Vec2 architectures, achieving strong performance under\nchallenging environmental conditions. Our work leverage the existing\nDrone-vs-Bird dataset and the newly generated ARDrone dataset containing more\nthan 7,500 synchronized images and audio segments. We show how the acoustic\ninformation is used to improve the performance of the Deformable DETR object\ndetector on the real ARDrone dataset. We developed, trained and tested four\ndifferent fusion configurations based on a gated mechanism, linear layer, MLP\nand cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi\nresolution feature mappings of the Deformable DETR and enhance the object\ndetection performance over all drones dimensions. The best performer is the\ngated fusion approach, which improves the mAP of the Deformable DETR object\ndetector on our in-distribution and out-of-distribution ARDrone datasets by\n11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.\nThe mAP scores for medium and large drones are also enhanced, with overall\ngains across all drone sizes ranging from 3.27% to 5.84%.", "AI": {"tldr": "这篇论文介绍了一种使用可见光RGB和声学信号来提高无人机检测精度的多模态方法。通过将声学信息与Deformable DETR架构结合，提升了无人机在实际条件下的检测性能。", "motivation": "为了提高无人机在复杂环境下的检测性能，我们结合了视觉和声学两种模态，利用现有的Drone-vs-Bird数据集和新生成的ARDrone数据集（超过7,500个同步图像和音频片段）进行研究。", "method": "我们提出了一种多模态的WAVE-DETR无人机检测器，结合可见光RGB和声学信号，在具有挑战性的环境下实现鲁棒的UAV目标检测。我们的方法在依赖于Deformable DETR和Wav2Vec2架构的统一目标检测模型中融合了视觉和声学特征。", "result": "通过四种不同的融合配置，基于门控机制、线性层、多层感知机（MLP）和交叉注意力机制，我们的方法成功地将Wav2Vec2声学嵌入与Deformable DETR的多分辨率特征映射融合，显著提高了目标检测性能。其中，门控融合方法在ARDrone数据集（无论是分布内还是分布外）上，将Deformable DETR目标检测器的小型无人机mAP提高了11.1%到15.3%。中型和大型无人机的mAP也得到了增强，总体而言，所有尺寸无人机的mAP增益范围为3.27%到5.84%。", "conclusion": "研究结果表明，通过融合视觉和声学信息，并采用特定的融合策略，可以显著提升基于Deformable DETR的无人机检测器性能。门控融合机制表现尤为突出，为无人机检测的准确性和鲁棒性提供了有效提升。"}}
{"id": "2509.09727", "categories": ["cs.CL", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.09727", "abs": "https://arxiv.org/abs/2509.09727", "authors": ["Andy Zhu", "Yingjun Du"], "title": "A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs", "comment": "8 pages, 6 figures, Underreview", "summary": "Question answering (QA) plays a central role in financial education, yet\nexisting large language model (LLM) approaches often fail to capture the\nnuanced and specialized reasoning required for financial problem-solving. The\nfinancial domain demands multistep quantitative reasoning, familiarity with\ndomain-specific terminology, and comprehension of real-world scenarios. We\npresent a multi-agent framework that leverages role-based prompting to enhance\nperformance on domain-specific QA. Our framework comprises a Base Generator, an\nEvidence Retriever, and an Expert Reviewer agent that work in a single-pass\niteration to produce a refined answer. We evaluated our framework on a set of\n3,532 expert-designed finance education questions from Study.com, an online\nlearning platform. We leverage retrieval-augmented generation (RAG) for\ncontextual evidence from 6 finance textbooks and prompting strategies for a\ndomain-expert reviewer. Our experiments indicate that critique-based refinement\nimproves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,\nwith the highest performance from Gemini-2.0-Flash. Furthermore, our method\nenables GPT-4o-mini to achieve performance comparable to the finance-tuned\nFinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to\nenhancing financial QA and offer insights for further research in multi-agent\nfinancial LLM systems.", "AI": {"tldr": "针对金融领域问答，本文提出了一种新的多智能体框架，通过基于角色的提示和检索增强生成技术，提升了金融问答系统的准确性和成本效益。", "motivation": "在金融教育领域，现有的大型语言模型在处理财务问题解决所需的复杂且专业化的推理时常常不足，该领域要求多步定量推理、熟悉领域特定术语以及现实场景的理解。因此，有必要提出一个更适合金融教育问答的方法。", "method": "我们提出了一种基于多智能体框架的方法来增强特定领域的问答能力。该框架包括一个基础生成器(Base Generator)、一个证据检索器(Evidence Retriever)和一个专家评审员(Expert Reviewer)，这些智能体通过单次迭代工作来生成一个优化的答案。这里利用了基于角色的提示(role-based prompting)策略，以及检索增强生成(RAG)来提供来自6本金融教材的上下文证据。", "result": "在Study.com提供的3,532个专家设计的金融教育问题的数据集上进行的实验表明，基于批评的优化(refinement)将答案的准确性提高了6.6-8.3%，相较于零样本链式思考基线（zero-shot Chain-of-Thought baselines）。特别是，Gemini-2.0-Flash表现最佳。此外，我们的方法使GPT-4o-mini能达到与经过金融微调的FinGPT-mt_Llama3-8B_LoRA相当的性能。", "conclusion": "该研究表明，我们的多智能体框架可以以成本效益的方式优化金融问答，并为未来的研究提供有关多智能体金融LLM系统的见解。"}}
{"id": "2509.09869", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09869", "abs": "https://arxiv.org/abs/2509.09869", "authors": ["Yihao Liu", "Junyu Chen", "Lianrui Zuo", "Shuwen Wei", "Brian D. Boyd", "Carmen Andreescu", "Olusola Ajilore", "Warren D. Taylor", "Aaron Carass", "Bennett A. Landman"], "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration", "comment": null, "summary": "Objective: Deep learning-based deformable image registration has achieved\nstrong accuracy, but remains sensitive to variations in input image\ncharacteristics such as artifacts, field-of-view mismatch, or modality\ndifference. We aim to develop a general training paradigm that improves the\nrobustness and generalizability of registration networks. Methods: We introduce\nsurrogate supervision, which decouples the input domain from the supervision\ndomain by applying estimated spatial transformations to surrogate images. This\nallows training on heterogeneous inputs while ensuring supervision is computed\nin domains where similarity is well defined. We evaluate the framework through\nthree representative applications: artifact-robust brain MR registration,\nmask-agnostic lung CT registration, and multi-modal MR registration. Results:\nAcross tasks, surrogate supervision demonstrated strong resilience to input\nvariations including inhomogeneity field, inconsistent field-of-view, and\nmodality differences, while maintaining high performance on well-curated data.\nConclusions: Surrogate supervision provides a principled framework for training\nrobust and generalizable deep learning-based registration models without\nincreasing complexity. Significance: Surrogate supervision offers a practical\npathway to more robust and generalizable medical image registration, enabling\nbroader applicability in diverse biomedical imaging scenarios.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09728", "categories": ["cs.CL", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09728", "abs": "https://arxiv.org/abs/2509.09728", "authors": ["Elena Rohde", "Jonas Klingwort", "Christian Borgs"], "title": "A meta-analysis on the performance of machine-learning based language models for sentiment analysis", "comment": null, "summary": "This paper presents a meta-analysis evaluating ML performance in sentiment\nanalysis for Twitter data. The study aims to estimate the average performance,\nassess heterogeneity between and within studies, and analyze how study\ncharacteristics influence model performance. Using PRISMA guidelines, we\nsearched academic databases and selected 195 trials from 20 studies with 12\nstudy features. Overall accuracy, the most reported performance metric, was\nanalyzed using double arcsine transformation and a three-level random effects\nmodel. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,\n0.84]. This paper provides two key insights: 1) Overall accuracy is widely used\nbut often misleading due to its sensitivity to class imbalance and the number\nof sentiment classes, highlighting the need for normalization. 2) Standardized\nreporting of model performance, including reporting confusion matrices for\nindependent test sets, is essential for reliable comparisons of ML classifiers\nacross studies, which seems far from common practice.", "AI": {"tldr": "该论文通过对Twitter数据中的情感分析进行了元分析来评估机器学习性能。研究估计了平均性能，评估了研究间和研究内的异质性，分析了研究特征如何影响模型性能。", "motivation": "研究的动机是估计机器学习在Twitter情感分析中的平均性能，评估研究间的差异，并分析哪些研究特征对模型性能有影响。", "method": "Structure", "result": "{\n  \"tldr\": \"该论文通过对Twitter数据中的情感分析进行了元分析来评估机器学习性能。研究估计了平均性能，评估了研究间和研究内的异质性，分析了研究特征如何影响模型性能。\", \n  \"motivation\": \"研究的动机是估计机器学习在Twitter情感分析中的平均性能，评估研究间的差异，并分析哪些研究特征对模型性能有影响。\", \n  \"method\": \"使用PRISMA指南，搜索了学术数据库并从20项研究中选取了195项试验，这些研究包含12个研究特性。整体准确率是最常用的性能度量，使用双反正弦变换和三层次随机效应模型进行分析。\", \n  \"result\": \"最终，AIC优化模型的平均整体准确率为0.80 [0.76, 0.84]。论文提出了两个关键点：1)整体准确率常被误解因为它对类别不均衡和情感分类数量敏感，强调需要进行规范化；2)需要规范报告模型性能，包括报告独立测试集的混淆矩阵，这对于跨研究的ML分类器比较至关重要。\", \n  \"conclusion\": \"该研究强调了规范化报告的重要性以及整体准确率指标的局限性，指出为了进行可靠的研究间模型比较，必须采用标准的性能报告方法。\"}\n", "conclusion": "该研究强调了规范化报告的重要性以及整体准确率指标的局限性，指出为了进行可靠的研究间模型比较，必须采用标准的性能报告方法。"}}
{"id": "2509.09911", "categories": ["cs.CV", "cs.AI", "68T07 (Primary)"], "pdf": "https://arxiv.org/pdf/2509.09911", "abs": "https://arxiv.org/abs/2509.09911", "authors": ["Barkin Buyukcakir", "Jannick De Tobel", "Patrick Thevissen", "Dirk Vandermeulen", "Peter Claes"], "title": "An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars", "comment": "21 pages, 11 figures, Scientific Reports", "summary": "The practical adoption of deep learning in high-stakes forensic applications,\nsuch as dental age estimation, is often limited by the 'black box' nature of\nthe models. This study introduces a framework designed to enhance both\nperformance and transparency in this context. We use a notable performance\ndisparity in the automated staging of mandibular second (tooth 37) and third\n(tooth 38) molars as a case study. The proposed framework, which combines a\nconvolutional autoencoder (AE) with a Vision Transformer (ViT), improves\nclassification accuracy for both teeth over a baseline ViT, increasing from\n0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond\nimproving performance, the framework provides multi-faceted diagnostic\ninsights. Analysis of the AE's latent space metrics and image reconstructions\nindicates that the remaining performance gap is data-centric, suggesting high\nintra-class morphological variability in the tooth 38 dataset is a primary\nlimiting factor. This work highlights the insufficiency of relying on a single\nmode of interpretability, such as attention maps, which can appear anatomically\nplausible yet fail to identify underlying data issues. By offering a\nmethodology that both enhances accuracy and provides evidence for why a model\nmay be uncertain, this framework serves as a more robust tool to support expert\ndecision-making in forensic age estimation.", "AI": {"tldr": "The study proposes a framework combining a convolutional autoencoder with a Vision Transformer, improving dental age estimation accuracy and providing multi-faceted diagnostic insights, showing high intra-class morphological variability in tooth 38 is a limiting factor.", "motivation": "The motivation is to overcome the 'black box' nature of deep learning models and enhance both performance and transparency in high-stakes applications such as dental age estimation.", "method": "The framework uses a combination of a convolutional autoencoder (AE) and a Vision Transformer (ViT) to improve classification accuracy and provide diagnostic insights.", "result": "The proposed framework improves classification accuracy for molars 37 and 38, and provides insights indicating that the remaining performance gap is due to data-centric issues.", "conclusion": "The study concludes that relying on single modes of interpretability alone is insufficient and that the framework, which enhances model accuracy and provides clear reasons for its uncertainties, can better support expert decision-making in forensic age estimation."}}
{"id": "2509.09729", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09729", "abs": "https://arxiv.org/abs/2509.09729", "authors": ["Gerard Sant", "Zifan Jiang", "Carlos Escolano", "Amit Moryossef", "Mathias Müller", "Rico Sennrich", "Sarah Ebling"], "title": "MultimodalHugs: Enabling Sign Language Processing in Hugging Face", "comment": null, "summary": "In recent years, sign language processing (SLP) has gained importance in the\ngeneral field of Natural Language Processing. However, compared to research on\nspoken languages, SLP research is hindered by complex ad-hoc code,\ninadvertently leading to low reproducibility and unfair comparisons. Existing\ntools that are built for fast and reproducible experimentation, such as Hugging\nFace, are not flexible enough to seamlessly integrate sign language\nexperiments. This view is confirmed by a survey we conducted among SLP\nresearchers.\n  To address these challenges, we introduce MultimodalHugs, a framework built\non top of Hugging Face that enables more diverse data modalities and tasks,\nwhile inheriting the well-known advantages of the Hugging Face ecosystem. Even\nthough sign languages are our primary focus, MultimodalHugs adds a layer of\nabstraction that makes it more widely applicable to other use cases that do not\nfit one of the standard templates of Hugging Face. We provide quantitative\nexperiments to illustrate how MultimodalHugs can accommodate diverse modalities\nsuch as pose estimation data for sign languages, or pixel data for text\ncharacters.", "AI": {"tldr": "手语处理研究面临复杂定制代码的挑战，MultimodalHugs框架提供了解决方案，提高了实验的灵活性和复现性。", "motivation": "现有的工具，如Hugging Face，对于手语处理实验不够灵活，导致手语处理研究在复现性和公平性上存在问题。此外，这项观点从对手语处理研究人员的调查中得到了证实。", "method": "介绍了一个名为MultimodalHugs的新框架，该框架基于Hugging Face构建，可以支持更多样化的数据模态和任务，同时继承了Hugging Face的优点。", "result": "通过定量实验展示了MultimodalHugs如何适应多样化的模态，如用于手语的姿势估计数据或用于文本字符的像素数据。", "conclusion": "MultimodalHugs作为一个新框架，不仅专注于手语处理，其抽象层使其在其他不符合Hugging Face标准模板的情况下也能广泛适用。"}}
{"id": "2509.09935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09935", "abs": "https://arxiv.org/abs/2509.09935", "authors": ["Chirayu Agrawal", "Snehasis Mukherjee"], "title": "SCoDA: Self-supervised Continual Domain Adaptation", "comment": "Submitted to ICVGIP 2025", "summary": "Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a\nmodel to a target domain without access to the data of the source domain.\nPrevailing methods typically start with a source model pre-trained with full\nsupervision and distill the knowledge by aligning instance-level features.\nHowever, these approaches, relying on cosine similarity over L2-normalized\nfeature vectors, inadvertently discard crucial geometric information about the\nlatent manifold of the source model. We introduce Self-supervised Continual\nDomain Adaptation (SCoDA) to address these limitations. We make two key\ndepartures from standard practice: first, we avoid the reliance on supervised\npre-training by initializing the proposed framework with a teacher model\npre-trained entirely via self-supervision (SSL). Second, we adapt the principle\nof geometric manifold alignment to the SFDA setting. The student is trained\nwith a composite objective combining instance-level feature matching with a\nSpace Similarity Loss. To combat catastrophic forgetting, the teacher's\nparameters are updated via an Exponential Moving Average (EMA) of the student's\nparameters. Extensive experiments on benchmark datasets demonstrate that SCoDA\nsignificantly outperforms state-of-the-art SFDA methods.", "AI": {"tldr": "SCoDA, a new method for Source-Free Domain Adaptation, improves over existing methods by using self-supervised learning and incorporating geometric information about the latent manifold without source domain data.", "motivation": "To address the limitations of standard SFDA methods which discard geometric information and require source domain data by introducing a novel approach using self-supervision and geometric manifold alignment.", "method": "Self-supervised Continual Domain Adaptation (SCoDA) introduces two key approaches: initializing the framework with a self-supervised pre-trained teacher model, and using a composite objective that combines instance-level feature matching with Space Similarity Loss, while updating the teacher's parameters via an Exponential Moving Average (EMA) of the student's parameters.", "result": "The proposed SCoDA method, tested extensively on benchmark datasets, achieves better performance than state-of-the-art SFDA methods.", "conclusion": "Experiments show that SCoDA provides significant improvement over state-of-the-art SFDA methods by introducing a composite objective that considers both instance-level feature matching and Space Similarity Loss."}}
{"id": "2509.09731", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09731", "abs": "https://arxiv.org/abs/2509.09731", "authors": ["Haiyang Yu", "Yuchuan Wu", "Fan Shi", "Lei Liao", "Jinghui Lu", "Xiaodong Ge", "Han Wang", "Minghan Zhuo", "Xuecheng Wu", "Xiang Fei", "Hao Feng", "Guozhi Tang", "An-Lan Wang", "Hanshen Zhu", "Yangfan He", "Quanhuan Liang", "Liyuan Meng", "Chao Feng", "Can Huang", "Jingqun Tang", "Bin Li"], "title": "Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning", "comment": null, "summary": "Chinese ancient documents, invaluable carriers of millennia of Chinese\nhistory and culture, hold rich knowledge across diverse fields but face\nchallenges in digitization and understanding, i.e., traditional methods only\nscan images, while current Vision-Language Models (VLMs) struggle with their\nvisual and linguistic complexity. Existing document benchmarks focus on English\nprinted texts or simplified Chinese, leaving a gap for evaluating VLMs on\nancient Chinese documents. To address this, we present AncientDoc, the first\nbenchmark for Chinese ancient documents, designed to assess VLMs from OCR to\nknowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular\ntranslation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and\ncovers 14 document types, over 100 books, and about 3,000 pages. Based on\nAncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by\na human-aligned large language model for scoring.", "AI": {"tldr": "介绍AncientDoc，首个用于评估视觉语言模型在处理中文古籍OCR及知识推理方面性能的基准测试。", "motivation": "传统方法仅限于图像扫描，现有的视觉语言模型难以处理中文古籍的视觉和语言复杂性。现有文档基准主要侧重于英文印刷文本或简体中文，缺乏针对中文古籍评估视觉语言模型的基准。", "method": "建立AncientDoc基准测试以评估视觉语言模型在中文古籍上的表现，涵盖五项任务：页面级OCR、白话翻译、基于推理的QA、基于知识的QA和语言变体QA。", "result": "AncientDoc基准测试包含了14种文档类型，超过100本书籍，大约3000页，用于评估主流视觉语言模型的表现。", "conclusion": "通过AncientDoc基准测试，使用多种度量标准评估主流视觉语言模型，并辅以与人类评分一致的大规模语言模型进行评分。"}}
{"id": "2509.09943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09943", "abs": "https://arxiv.org/abs/2509.09943", "authors": ["Zhu Chen", "Mert Edgü", "Er Jin", "Johannes Stegmaier"], "title": "Segment Anything for Cell Tracking", "comment": null, "summary": "Tracking cells and detecting mitotic events in time-lapse microscopy image\nsequences is a crucial task in biomedical research. However, it remains highly\nchallenging due to dividing objects, low signal-tonoise ratios, indistinct\nboundaries, dense clusters, and the visually similar appearance of individual\ncells. Existing deep learning-based methods rely on manually labeled datasets\nfor training, which is both costly and time-consuming. Moreover, their\ngeneralizability to unseen datasets remains limited due to the vast diversity\nof microscopy data. To overcome these limitations, we propose a zero-shot cell\ntracking framework by integrating Segment Anything 2 (SAM2), a large foundation\nmodel designed for general image and video segmentation, into the tracking\npipeline. As a fully-unsupervised approach, our method does not depend on or\ninherit biases from any specific training dataset, allowing it to generalize\nacross diverse microscopy datasets without finetuning. Our approach achieves\ncompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos\nwhile eliminating the need for dataset-specific adaptation.", "AI": {"tldr": "提出了一种无监督的零样本细胞追踪方法，通过集成SAM2，实现了在多种显微镜数据集中的有效追踪，同时不需要手动标记的数据集和微调。", "motivation": "由于细胞分割、低信噪比、边界不清晰、密集簇和细胞间视觉上相似的外观等挑战，追踪细胞和检测显微镜图像序列中的分裂事件仍然是一个困难的任务。本研究旨在克服现有的深度学习方法依赖于手动标记数据集训练以及在未见过的数据集中泛化能力有限的问题。", "method": "通过将设计用于通用图像和视频分割的大型基础模型Segment Anything 2 (SAM2)集成到跟踪流程中，提出了一种零样本细胞追踪框架。这种方法是完全无监督的，不依赖于特定的训练数据集，从而能够在多种显微镜数据集之间进行泛化，而无需微调。", "result": "该方法在2D和大规模3D时差显微视频中达到了具有竞争力的准确性，同时消除了对数据集特定适应的需要。", "conclusion": "该零样本细胞追踪方法克服了现有依赖于手动标记数据集训练的方法的成本高、耗时长和泛化能力有限的问题，实现了更高效的细胞追踪和分裂事件检测。"}}
{"id": "2509.09734", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09734", "abs": "https://arxiv.org/abs/2509.09734", "authors": ["Zikang Guo", "Benfeng Xu", "Chiwei Zhu", "Wentao Hong", "Xiaorui Wang", "Zhendong Mao"], "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools", "comment": null, "summary": "The Model Context Protocol (MCP) is rapidly emerging as a pivotal open\nstandard, designed to enhance agent-tool integration and interoperability, and\nis positioned to unlock a new era of powerful, interconnected, and genuinely\nutilitarian agentic AI. However, despite MCP's growing adoption, existing\nbenchmarks often fail to capture real-world agent performance within this new\nparadigm, leading to a distorted perception of their true operational value and\nan inability to reliably differentiate proficiencies. To bridge this critical\nevaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark\nspecifically engineered to rigorously assess language agent capabilities in\nMCP-mediated tool interactions. Core contributions of MCP-AgentBench include:\nthe establishment of a robust MCP testbed comprising 33 operational servers\nwith 188 distinct tools; the development of a benchmark featuring 600\nsystematically designed queries distributed across 6 distinct categories of\nvarying interaction complexity; and the introduction of MCP-Eval, a novel\noutcome-oriented evaluation methodology prioritizing real-world task success.\nThrough extensive empirical evaluation of leading language agents, we provide\nfoundational insights. MCP-AgentBench aims to equip the research community with\na standardized and reliable framework to build, validate, and advance agents\ncapable of fully leveraging MCP's transformative benefits, thereby accelerating\nprogress toward truly capable and interoperable AI systems.", "AI": {"tldr": "提出MCP-AgentBench，以填补对基于MCP工作的代理进行准确评估的空白，改进现有工具评估标准。", "motivation": "现有的基准测试无法捕捉到MCP环境下代理的真实性能，导致对其操作价值的误解，无法可靠地区分其能力。因此，需要一个专门的基准来填补这一评估空白。", "method": "通过建立MCP-AgentBench基准测试来评估语言代理在MCP中介工具交互中的能力，该基准包括33个操作服务器和188个不同工具的测试环境，以及包含600个系统设计查询的基准测试。", "result": "引入了MCP-Eval，这是一种专注于现实任务成功的新评估方法，并通过广泛的实证评估向领先的语言代理提供了初步见解。", "conclusion": "MCP-AgentBench旨在为研究社区提供标准化和可靠的框架，以建设和验证能够充分利用MCP变革性优势的代理，加速真正有能力且高度互操作的AI系统的进展。"}}
{"id": "2509.09946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09946", "abs": "https://arxiv.org/abs/2509.09946", "authors": ["Vu-Minh Le", "Thao-Anh Tran", "Duc Huy Do", "Xuan Canh Do", "Huong Ninh", "Hai Tran"], "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation", "comment": "Accepted at ICCVW 2025", "summary": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard.", "AI": {"tldr": "本文提出了一种扩展在线2D多相机跟踪系统到3D空间的方法，该方法利用深度信息和点云空间，同时引入一种增强的数据关联机制，最终在比赛数据集中获得第三名。", "motivation": "目标是在不需要替换所有2D跟踪组件的情况下，将现有的多目标多相机跟踪系统扩展到3D空间。", "method": "提出了一种方法，通过利用深度信息将目标重建到点云空间中，并通过聚类和偏航角度调整来恢复3D边界框，以将任何在线2D多相机跟踪系统扩展到3D空间。同时还引入了一种增强的在线数据关联机制，利用目标的局部ID一致性来跨帧分配全局ID。", "result": "该框架在2025 AI City Challenge的3D MTMC数据集上进行了评估，取得了排行榜第三名的成绩。", "conclusion": "为了增强自动大规模监控能力，该方法提供了一种不需要根本性的更改便可扩展现有系统到3D空间的方法，并展示了其在实际数据集上的有效性。"}}
{"id": "2509.09735", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09735", "abs": "https://arxiv.org/abs/2509.09735", "authors": ["Willem Huijzer", "Jieying Chen"], "title": "Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation", "comment": "7 pages", "summary": "The rapid integration of Large Language Models (LLMs) into various domains\nraises concerns about societal inequalities and information bias. This study\nexamines biases in LLMs related to background, gender, and age, with a focus on\ntheir impact on decision-making and summarization tasks. Additionally, the\nresearch examines the cross-lingual propagation of these biases and evaluates\nthe effectiveness of prompt-instructed mitigation strategies. Using an adapted\nversion of the dataset by Tamkin et al. (2023) translated into Dutch, we\ncreated 151,200 unique prompts for the decision task and 176,400 for the\nsummarisation task. Various demographic variables, instructions, salience\nlevels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed\nthat both models were significantly biased during decision-making, favouring\nfemale gender, younger ages, and certain backgrounds such as the\nAfrican-American background. In contrast, the summarisation task showed minimal\nevidence of bias, though significant age-related differences emerged for\nGPT-3.5 in English. Cross-lingual analysis showed that bias patterns were\nbroadly similar between English and Dutch, though notable differences were\nobserved across specific demographic categories. The newly proposed mitigation\ninstructions, while unable to eliminate biases completely, demonstrated\npotential in reducing them. The most effective instruction achieved a 27\\% mean\nreduction in the gap between the most and least favorable demographics.\nNotably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts\nin English, indicating the specific potential for prompt-based mitigation\nwithin newer models. This research underscores the importance of cautious\nadoption of LLMs and context-specific bias testing, highlighting the need for\ncontinued development of effective mitigation strategies to ensure responsible\ndeployment of AI.", "AI": {"tldr": "本研究揭示了在GPT-3.5和GPT-4o中性别和年龄的偏见，并测试了跨语言偏见传播和缓解策略，强调了在采用LLMs时需要进行谨慎考虑并制定缓解偏见的策略。", "motivation": "研究旨在了解语言模型在背景、性别和年龄方面的偏见，及其对决策和总结任务的影响，并评估跨语言偏见传播和提示策略缓解偏见的有效性。", "method": "本研究通过使用Tamkin等人的数据集的荷兰语翻译版本，生成了151,200个关于决策任务的唯一提示和176,400个关于摘要生成任务的唯一提示，并针对GPT-3.5和GPT-4o进行了测试，以研究语言模型中的偏见问题。", "result": "研究发现，两个模型在决策任务中有显著偏见，倾向于女性、年轻年龄以及如非洲裔美国人背景。然而，在摘要生成任务方面，除了年龄相关的差异外，基本没有偏见。通过跨语言分析，偏见模式在英语和荷兰语之间相似，但在特定的人口统计类别之间有所不同。", "conclusion": "研究强调了在不同背景下对LLMs采用谨慎态度，并进行特定偏见测试的重要性。同时表明了新模型中提示缓解的潜在有效性，需要继续开发有效的缓解策略，确保AI的负责任部署。"}}
{"id": "2509.09958", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09958", "abs": "https://arxiv.org/abs/2509.09958", "authors": ["Jeffrey Liu", "Rongbin Hu"], "title": "Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification", "comment": null, "summary": "Referring Expression Comprehension (REC) is usually addressed with\ntask-trained grounding models. We show that a zero-shot workflow, without any\nREC-specific training, can achieve competitive or superior performance. Our\napproach reformulates REC as box-wise visual-language verification: given\nproposals from a COCO-clean generic detector (YOLO-World), a general-purpose\nVLM independently answers True/False queries for each region. This simple\nprocedure reduces cross-box interference, supports abstention and multiple\nmatches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our\nmethod not only surpasses a zero-shot GroundingDINO baseline but also exceeds\nreported results for GroundingDINO trained on REC and GroundingDINO+CRG.\nControlled studies with identical proposals confirm that verification\nsignificantly outperforms selection-based prompting, and results hold with open\nVLMs. Overall, we show that workflow design, rather than task-specific\npretraining, drives strong zero-shot REC performance.", "AI": {"tldr": "本文提出了一种零样本工作流程，用于指代表达式理解任务，该方法将此任务重新定义为基于框的视觉语言验证，无需专门的微调，并在多个数据集上取得了优异的性能。", "motivation": "我们展示了不进行任何任务特异性训练的零样本工作流程可以达到具有竞争力甚至更优的性能。与传统的任务训练分词模型相对，我们试图探索一种无需专门训练的解决方案。", "method": "我们的方法将指代表达式理解（REC）重新定义为基于框的视觉语言验证：给定来自通用检测器（YOLO-World）的提议，一个通用的VLM独立回答每个区域的True/False查询。这种方法减少了框间干扰，支持放弃和多个匹配，并且不需要微调。", "result": "在RefCOCO、RefCOCO+和RefCOCOg数据集上，我们的方法不仅超过了零样本基准GroundingDINO的表现，而且还超过了那些针对REC任务进行了训练的GroundingDINO和GroundingDINO + CRG的方法。", "conclusion": "总体而言，我们展示了工作流程设计而不是任务特定的预训练是驱动强大的零样本指代表达式理解性能的关键。"}}
{"id": "2509.09801", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50, 68T05", "I.2.7; I.2.6; C.4"], "pdf": "https://arxiv.org/pdf/2509.09801", "abs": "https://arxiv.org/abs/2509.09801", "authors": ["Brennen Hill"], "title": "HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning", "comment": null, "summary": "The adaptation of large language models (LLMs) to specialized reasoning tasks\nis fundamentally constrained by computational resources. Parameter-Efficient\nFine-Tuning (PEFT) methods have emerged as a powerful solution, yet the\nlandscape of these techniques is diverse, with distinct methods operating in\neither the model's weight space or its representation space. This paper\ninvestigates the hypothesis that a synergistic combination of these paradigms\ncan unlock superior performance and efficiency. We introduce HEFT (Hierarchical\nEfficient Fine-Tuning), a novel hierarchical adaptation strategy that composes\ntwo distinct PEFT methods in a coarse-to-fine manner: first, a broad,\nfoundational adaptation in the weight space using Low-Rank Adaptation (LoRA),\nfollowed by a precise, surgical refinement of internal activations using\nRepresentation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a\nLlama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential\nreasoning. Our results reveal a profound synergistic effect. A model fine-tuned\nfor only three epochs with our HEFT strategy achieves an accuracy of 85.17\\%,\nexceeding the performance of models trained for 20 epochs with either LoRA-only\n(85.05\\%) or ReFT-only (83.36\\%) methodologies. This work demonstrates that the\nthoughtful composition of PEFT methods is a potent algorithmic innovation,\noffering a more efficient and effective path toward advancing the reasoning\ncapabilities of language models. By achieving superior results with a fraction\nof the computational budget, our findings present a principled approach to\novercoming the obstacles inherent in adapting large-scale models for complex\ncognitive tasks.", "AI": {"tldr": "本文提出了HEFT方法，通过组合权重空间和表示空间的微调技术，实现了在计算资源有限的情况下提升语言模型推理任务性能的目标。", "motivation": "这篇文章探讨了假设，即这些范式的协同组合可以解锁更优的性能和效率。这一研究的动机在于找到一种更有效率和效果更好的方法来提升大型语言模型在特定推理任务上的能力。", "method": "HEFT (Hierarchical Efficient Fine-Tuning) 被提出，这是一种分层适配策略，将两种不同的参数高效微调方法以粗细结合的方式组合在一起：首先，在权重空间中利用低秩适配（LoRA）进行广泛的、基础性的适应，然后，利用表示微调（ReFT）对内部激活进行精确的、外科手术式的细化。", "result": "在对Llama-2-7B模型进行BoolQ基准测试时，该框架在仅经过三个纪元的微调后便达到了85.17%的准确性，超过了用LoRA-only或ReFT-only方法训练20个纪元的模型。", "conclusion": "本研究展示了一种通过HEFT框架对PEFT方法的精心组合来推进语言模型推理能力的有效算法创新，证明了在相对较少的计算资源下达到更优结果的可能性。"}}
{"id": "2509.09961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09961", "abs": "https://arxiv.org/abs/2509.09961", "authors": ["Tianqi Wei", "Xin Yu", "Zhi Chen", "Scott Chapman", "Zi Huang"], "title": "Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation", "comment": null, "summary": "Accurate segmentation of foliar diseases and insect damage in wheat is\ncrucial for effective crop management and disease control. However, the insect\ndamage typically occupies only a tiny fraction of annotated pixels. This\nextreme pixel-level imbalance poses a significant challenge to the segmentation\nperformance, which can result in overfitting to common classes and insufficient\nlearning of rare classes, thereby impairing overall performance. In this paper,\nwe propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to\naddress the pixel imbalance problem. Specifically, we extract rare\ninsect-damage patches from annotated training images and apply random geometric\ntransformations to simulate variations. The transformed patches are then pasted\nin appropriate regions while avoiding overlaps with lesions or existing damaged\nregions. In addition, we apply a random projection filter to the pasted\nregions, refining local features and ensuring a natural blend with the new\nbackground. Experiments show that our method substantially improves\nsegmentation performance on the insect damage class, while maintaining or even\nslightly enhancing accuracy on other categories. Our results highlight the\neffectiveness of targeted augmentation in mitigating extreme pixel imbalance,\noffering a straightforward yet effective solution for agricultural segmentation\nproblems.", "AI": {"tldr": "本文提出了一种名为随机投影复制粘贴（RPCP）的数据增强技术，以解决小麦叶片病害和虫害分割中由于像素不平衡导致的性能问题。实验表明，该方法显著提高了虫害分割的性能，同时保持或稍微提高了其他类别的准确性。", "motivation": "作者提出了RPCP技术，旨在解决小麦叶片图像中虫害区域像素极为稀少导致的分割性能不佳的问题。", "method": "RPCP技术通过从标注的训练图像中提取稀有虫害图像补丁，应用随机几何变换来模拟变化，并将其粘贴到适当区域以避免与现有的病斑或损坏区域重叠。此外还应用随机投影滤波器以使局部特征更加自然地与新背景融合。", "result": "<tool_call>\ndata: {", "conclusion": "实验结果表明，该方法在提高虫害分割性能的同时，维持或略提高了其他类别的准确性，证明了该方法在缓解极端像素不平衡中的有效性。"}}
{"id": "2509.09804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09804", "abs": "https://arxiv.org/abs/2509.09804", "authors": ["Helen de Andrade Abreu", "Tiago Timponi Torrent", "Ely Edison da Silva Matos"], "title": "Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization", "comment": "Paper submitted to Language Sciences Journal", "summary": "This paper proposes a framework for modeling multimodal conversational turn\norganization via the proposition of correlations between language and\ninteractive gestures, based on analysis as to how pragmatic frames are\nconceptualized and evoked by communicators. As a means to provide evidence for\nthe analysis, we developed an annotation methodology to enrich a multimodal\ndataset (annotated for semantic frames) with pragmatic frames modeling\nconversational turn organization. Although conversational turn organization has\nbeen studied by researchers from diverse fields, the specific strategies,\nespecially gestures used by communicators, had not yet been encoded in a\ndataset that can be used for machine learning. To fill this gap, we enriched\nthe Frame2 dataset with annotations of gestures used for turn organization. The\nFrame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo\nMundo annotated for semantic frames evoked in both video and text. This dataset\nallowed us to closely observe how communicators use interactive gestures\noutside a laboratory, in settings, to our knowledge, not previously recorded in\nrelated literature. Our results have confirmed that communicators involved in\nface-to-face conversation make use of gestures as a tool for passing, taking\nand keeping conversational turns, and also revealed variations of some gestures\nthat had not been documented before. We propose that the use of these gestures\narises from the conceptualization of pragmatic frames, involving mental spaces,\nblending and conceptual metaphors. In addition, our data demonstrate that the\nannotation of pragmatic frames contributes to a deeper understanding of human\ncognition and language.", "AI": {"tldr": "提出了一种通过语言和互动手势之间的关联来建模多模态对话轮次组织的框架，并展示了如何利用语用框架的注释增强现有的多模态数据集，以加深对人类认知和语言的理解。", "motivation": "论文旨在填补一个空白，即针对对话轮次组织中参与者使用的具体策略，特别是手势，开发用于机器学习的编码数据集。", "method": "开发了一种注释方法论，以在现有的多模态数据集Frame2上增加语用层面的注释，使数据集不仅包括口语和视频中的语义框架，还包括了用于组织对话轮次的手势的注释。", "result": "该论文提出了一种通过分析语言与互动手势之间的关联来建模多模态对话轮次组织的框架。论文开发了一种注释方法以丰富现有的多模态数据集，涵盖了对话轮次组织的语用框架模型。通过注释巴西电视剧《Pedro Pelo Mundo》的片段，论文展示了参与者如何在面对面交谈中利用手势进行对话轮次的传递、获取和维持。研究结果表明，语用框架的概念化，包括心理空间、概念合成和概念隐喻，影响了这些手势的使用，并有助于更深入地理解人类认知和语言。", "conclusion": "研究结果证实了参与者在面对面交谈中如何使用手势作为一种传递、获取和维持对话轮次的工具，并揭示了一些之前未被记录的手势变体。进一步的数据表明，语用框架的注释有助于揭示人类认知和语言的更深层面。"}}
{"id": "2509.09962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09962", "abs": "https://arxiv.org/abs/2509.09962", "authors": ["Anne Marthe Sophie Ngo Bibinbe", "Chiron Bang", "Patrick Gagnon", "Jamie Ahloy-Dallaire", "Eric R. Paquet"], "title": "An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock", "comment": "13 pages, 7 figures, 1 table, accepted at CVPR animal workshop 2024,\n  submitted to IJCV", "summary": "The need for long-term multi-object tracking (MOT) is growing due to the\ndemand for analyzing individual behaviors in videos that span several minutes.\nUnfortunately, due to identity switches between objects, the tracking\nperformance of existing MOT approaches decreases over time, making them\ndifficult to apply for long-term tracking. However, in many real-world\napplications, such as in the livestock sector, it is possible to obtain\nsporadic identifications for some of the animals from sources like feeders. To\naddress the challenges of long-term MOT, we propose a new framework that\ncombines both uncertain identities and tracking using a Hidden Markov Model\n(HMM) formulation. In addition to providing real-world identities to animals,\nour HMM framework improves the F1 score of ByteTrack, a leading MOT approach\neven with re-identification, on a 10 minute pig tracking dataset with 21\nidentifications at the pen's feeding station. We also show that our approach is\nrobust to the uncertainty of identifications, with performance increasing as\nidentities are provided more frequently. The improved performance of our HMM\nframework was also validated on the MOT17 and MOT20 benchmark datasets using\nboth ByteTrack and FairMOT. The code for this new HMM framework and the new\n10-minute pig tracking video dataset are available at:\nhttps://github.com/ngobibibnbe/uncertain-identity-aware-tracking", "AI": {"tldr": "研究提出了一种结合HMM的新型框架，用于长期多目标跟踪任务，这在畜牧业中有很大应用前景，并提高了现有模型的性能。", "motivation": "由于身份切换，现有的多对象跟踪（MOT）方法表现不佳，特别是在长时间跟踪中。本研究旨在解决此种长期跟踪的挑战。", "method": "本研究提出了一种新的框架，结合了不确定的身份和跟踪，采用了隐马尔可夫模型（HMM）的制定方式。", "result": "该HMM框架提高了ByteTrack的F1分数，这在10分钟的猪跟踪数据集上的实验中得到了验证。此外，在MOT17和MOT20基准数据集上的测试进一步证实了改进的效果。", "conclusion": "研究表明，所提出的HMM框架在长时多目标跟踪任务中表现出色，尤其是当有更频繁的身份确认时，性能更好。"}}
{"id": "2509.09852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09852", "abs": "https://arxiv.org/abs/2509.09852", "authors": ["Chuyuan Li", "Austin Xu", "Shafiq Joty", "Giuseppe Carenini"], "title": "Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization", "comment": null, "summary": "A key challenge in Multi-Document Summarization (MDS) is effectively\nintegrating information from multiple sources while maintaining coherence and\ntopical relevance. While Large Language Models have shown impressive results in\nsingle-document summarization, their performance on MDS still leaves room for\nimprovement. In this paper, we propose a topic-guided reinforcement learning\napproach to improve content selection in MDS. We first show that explicitly\nprompting models with topic labels enhances the informativeness of the\ngenerated summaries. Building on this insight, we propose a novel topic reward\nwithin the Group Relative Policy Optimization (GRPO) framework to measure topic\nalignment between the generated summary and source documents. Experimental\nresults on the Multi-News and Multi-XScience datasets demonstrate that our\nmethod consistently outperforms strong baselines, highlighting the\neffectiveness of leveraging topical cues in MDS.", "AI": {"tldr": "本文提出了一个基于主题引导的强化学习方法，通过Group Relative Policy Optimization (GRPO)框架中的主题奖励提高多文档摘要生成的性能，并通过实验证明了该方法的有效性。", "motivation": "多文档摘要的关键挑战是有效地整合多个来源的信息，同时保持连贯性和主题相关性。尽管大型语言模型在单文档摘要生成中表现出色，但在多文档摘要生成中的表现仍有提升空间。本文旨在通过引入主题引导技术提高该领域的性能。", "method": "提出了一种基于主题引导的强化学习方法来改善多文档摘要中的内容选择。该方法首先展示通过显式地使用主题标签提示模型，可以提高生成摘要的信息量。然后在Group Relative Policy Optimization (GRPO)框架中提出了一个新颖的主题奖励机制，用于衡量生成摘要与源文档之间的主题一致性。", "result": "在Multi-News和Multi-XScience数据集上的实验结果展示了该方法的表现优于强大的基线模型，强调了在多文档摘要中利用主题线索的有效性。", "conclusion": "基于主题引导的强化学习方法通过增强内容选择能力，能更有效地生成多文档摘要。"}}
{"id": "2509.09971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09971", "abs": "https://arxiv.org/abs/2509.09971", "authors": ["Aupendu Kar", "Vishnu Raj", "Guan-Ming Su"], "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey", "comment": null, "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.", "AI": {"tldr": "This survey explores the advancements in fusing event-stream captures with traditional frame-based captures for enhancing image/video restoration and 3D reconstruction using deep learning techniques, and discusses the benefits under challenging conditions along with a list of open datasets for reproducible research.", "motivation": "The motivation is to highlight the synergistic benefits of combining event camera data with traditional image and video data, focusing on improving the quality and performance of visual tasks through deep learning methods.", "method": "The method involves a systematic review of the major contributions in deep learning for enhancing image/video qualities and 3D reconstruction tasks, categorizing the techniques into temporal and spatial enhancements.", "result": "The result is an in-depth understanding of recent improvements in image/video enhancement and 3D reconstruction tasks enabled by the fusion of event-stream data and frame-based data, providing a clear pathway for leveraging event cameras in computer vision tasks.", "conclusion": "The conclusion is that the combination of event-driven sensor data with traditional video/image data through deep learning holds significant potential for advancing visual media restoration and enhancement, and encourages further research in this area."}}
{"id": "2509.09871", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.09871", "abs": "https://arxiv.org/abs/2509.09871", "authors": ["Bastián González-Bustamante", "Nando Verelst", "Carla Cisternas"], "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case", "comment": "Working paper: 18 pages, 4 tables, 2 figures", "summary": "Large Language Models (LLMs) offer promising avenues for methodological and\napplied innovations in survey research by using synthetic respondents to\nemulate human answers and behaviour, potentially mitigating measurement and\nrepresentation errors. However, the extent to which LLMs recover aggregate item\ndistributions remains uncertain and downstream applications risk reproducing\nsocial stereotypes and biases inherited from training data. We evaluate the\nreliability of LLM-generated synthetic survey responses against ground-truth\nhuman responses from a Chilean public opinion probabilistic survey.\nSpecifically, we benchmark 128 prompt-model-question triplets, generating\n189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,\nprecision, recall, and F1-score) in a meta-analysis across 128\nquestion-subsample pairs to test for biases along key sociodemographic\ndimensions. The evaluation spans OpenAI's GPT family and o-series reasoning\nmodels, as well as Llama and Qwen checkpoints. Three results stand out. First,\nsynthetic responses achieve excellent performance on trust items (F1-score and\naccuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform\ncomparably on this task. Third, synthetic-human alignment is highest among\nrespondents aged 45-59. Overall, LLM-based synthetic samples approximate\nresponses from a probabilistic sample, though with substantial item-level\nheterogeneity. Capturing the full nuance of public opinion remains challenging\nand requires careful calibration and additional distributional tests to ensure\nalgorithmic fidelity and reduce errors.", "AI": {"tldr": "研究评估了多种大语言模型生成合成调查响应的可靠性，并发现尽管存在高异质性，这些模型能够可靠地模拟特定领域的公共意见。", "motivation": "探究大语言模型在模拟人类答案和行为方面，为调查研究开辟了新的途径，但也存在再现训练数据中社会刻板印象和偏见的风险。本研究旨在评估这些模型在调查响应生成上的可靠性及偏差。", "method": "通过使用来自智利公共意见概率抽样调查的真实人类响应数据，评估了LLM生成的合成调查响应的可靠性。具体来说，他们对128个提示-模型-问题三元组进行了基准测试，生成了189,696个合成配置文件，并对128个问题子样本对进行了元分析，以测试沿关键社会经济维度的偏差。评测涵盖了OpenAI的GPT系列和o系列推理模型，以及Llama和Qwen检查点。", "result": "研究表明，合成响应在信任项上表现出色（F1度量和准确度 > 0.90）。GPT-4o, GPT-4o-mini, 和 Llama 4 Maverick在此任务上的表现相近。45-59岁的受访者中，合成与人类响应的吻合度最高。总体而言，基于大语言模型的合成样本可以近似概率样本的响应，但也存在较大的项目层面异质性。", "conclusion": "尽管研究结果表明大语言模型生成的合成调查响应可以近似实际的人类响应，但要捕捉公共意见的全部细微差别仍极具挑战性，需要谨慎调校和额外的分布测试以确保算法准确性和减少错误。"}}
{"id": "2509.09977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09977", "abs": "https://arxiv.org/abs/2509.09977", "authors": ["Siying Liu", "Zikai Wang", "Hanle Zheng", "Yifan Hu", "Xilin Wang", "Qingkai Yang", "Jibin Wu", "Hao Guo", "Lei Deng"], "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking", "comment": "15 pages, 8 figures", "summary": "RGB-Event tracking has become a promising trend in visual object tracking to\nleverage the complementary strengths of both RGB images and dynamic spike\nevents for improved performance. However, existing artificial neural networks\n(ANNs) struggle to fully exploit the sparse and asynchronous nature of event\nstreams. Recent efforts toward hybrid architectures combining ANNs and spiking\nneural networks (SNNs) have emerged as a promising solution in RGB-Event\nperception, yet effectively fusing features across heterogeneous paradigms\nremains a challenge. In this work, we propose ISTASTrack, the first\ntransformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped\nwith \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model\nemploys a vision transformer to extract spatial context from RGB inputs and a\nspiking transformer to capture spatio-temporal dynamics from event streams. To\nbridge the modality and paradigm gap between ANN and SNN features, we\nsystematically design a model-based ISTA adapter for bidirectional feature\ninteraction between the two branches, derived from sparse representation theory\nby unfolding the iterative shrinkage thresholding algorithm. Additionally, we\nincorporate a temporal downsampling attention module within the adapter to\nalign multi-step SNN features with single-step ANN features in the latent\nspace, improving temporal fusion. Experimental results on RGB-Event tracking\nbenchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that\nISTASTrack achieves state-of-the-art performance while maintaining high energy\nefficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN\ndesigns for robust visual tracking. The code is publicly available at\nhttps://github.com/lsying009/ISTASTrack.git.", "AI": {"tldr": "提出了ISTASTrack，一种结合人工神经网络和尖峰神经网络的跟踪器，设计了ISTA适配器，以提高RGB-事件跟踪的性能和能效。", "motivation": "已有的人工神经网络难以充分利用事件流的稀疏性和非同步性，近期一些基于人工神经网络(ANN)和尖峰神经网络(SNN)的混合架构的研究虽有进展，但如何有效地融合异构范式的特征依旧是挑战。", "method": "提出了一种称为ISTASTrack的跟踪器，这是一种基于Transformer的ANN-SNN混合跟踪器，配备了ISTA适配器，用于RGB-事件跟踪。该模型的两个分支分别使用视觉Transformer从RGB输入中提取空间上下文，以及使用尖峰Transformer从事件流中捕获时空动态。为了弥合ANN和SNN特征之间的模态和范式差距，系统地设计了一种基于模型的ISTA适配器，用于两个分支之间的双向特征交互，适配器内部还集成了时间下采样注意力模块，改善了时间融合。", "result": "实验证明ISTASTrack在像FE240hz，VisEvent，COESOT 和FELT这样的RGB-事件跟踪基准数据集上，达到了最先进的性能，并且保持了高能效，突显了ANN-SNN混合设计的有效性和实用性。", "conclusion": "提出的ISTASTrack跟踪器通过结合Transformer和ISTA适配器，成功实现了RGB-事件跟踪中的模态和范式桥梁，证明了ANN-SNN混合架构在稳健视觉跟踪中的有效性和实用性。"}}
{"id": "2509.09969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09969", "abs": "https://arxiv.org/abs/2509.09969", "authors": ["Zhitian Hou", "Zihan Ye", "Nanli Zeng", "Tianyong Hao", "Kun Zeng"], "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey", "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI.", "AI": {"tldr": "本文综述了16种法律LLM系列和47个基于LLM的法律任务框架，提供了15个基准和29个数据集来评估法律能力，旨在促进未来的研究。资源可从GitHub获取。", "motivation": "为了促进法律领域中基于大型语言模型的研究和应用，本文对16种法律LLM系列和47个基于LLM的法律任务框架进行了全面回顾，并整理了15个基准和29个数据集来评估不同的法律能力。", "method": "本文通过回顾分析16种法律大型语言模型系列和47个基于LLM的法律任务框架，以及收集15个基准和29个数据集来评估不同的法律能力。", "result": "本文为法律领域中的大型语言模型(LLM)方法提供了系统的介绍，并分析了面临的挑战，讨论了未来的研究方向。", "conclusion": "本文为法律领域中的大型语言模型(LLM)方法提供了系统的介绍，旨在鼓励未来的研究，并为初学者提供帮助。资源可在https://github.com/ZhitianHou/LLMs4LegalAI获得。"}}
{"id": "2509.09988", "categories": ["cs.CV", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2509.09988", "abs": "https://arxiv.org/abs/2509.09988", "authors": ["Yusuke Takagi", "Shunya Nagashima", "Komei Sugiura"], "title": "FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction", "comment": "Accepted for presentation at ICONIP2025", "summary": "Accurate and reliable solar flare predictions are essential to mitigate\npotential impacts on critical infrastructure. However, the current performance\nof solar flare forecasting is insufficient. In this study, we address the task\nof predicting the class of the largest solar flare expected to occur within the\nnext 72 hours. Existing methods often fail to adequately address the severe\nclass imbalance across flare classes. To address this issue, we propose a solar\nflare prediction model based on multiple deep state space models. In addition,\nwe introduce the frequency & local-boundary-aware reliability loss (FLARE loss)\nto improve predictive performance and reliability under class imbalance.\nExperiments were conducted on a multi-wavelength solar image dataset covering a\nfull 11-year solar activity cycle. As a result, our method outperformed\nbaseline approaches in terms of both the Gandin-Murphy-Gerrity score and the\ntrue skill statistic, which are standard metrics in terms of the performance\nand reliability.", "AI": {"tldr": "The study introduces an advanced solar flare prediction model using deep state space models and the FLARE loss, which shows significant improvement over baseline methods in predicting solar flare classes under conditions of class imbalance.", "motivation": "The motivation is to improve the prediction accuracy and reliability of solar flare forecasting, especially in the context of severe class imbalance across flare classes, which is critical for protecting infrastructure from potential impacts caused by solar flares.", "method": "The paper proposes a solar flare prediction model based on multiple deep state space models along with an introduced method called the frequency & local-boundary-aware reliability loss (FLARE loss) to improve performance under conditions of class imbalance.", "result": "Experiments conducted on a multi-wavelength solar image dataset showed that the new method outperformed baseline approaches in terms of the Gandin-Murphy-Gerrity score and the true skill statistic.", "conclusion": "The proposed model demonstrates better performance and reliability in predicting the class of the largest solar flare expected within the next 72 hours, particularly in addressing the challenge of class imbalance across flare classes."}}
{"id": "2509.09990", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09990", "abs": "https://arxiv.org/abs/2509.09990", "authors": ["Guixian Xu", "Zeli Su", "Ziyin Zhang", "Jianing Liu", "XU Han", "Ting Zhang", "Yushuang Dong"], "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China", "comment": null, "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional\nMongolian, face significant challenges due to their unique writing systems,\nwhich differ from international standards. This discrepancy has led to a severe\nlack of relevant corpora, particularly for supervised tasks like headline\ngeneration. To address this gap, we introduce a novel dataset, Chinese Minority\nHeadline Generation (CMHG), which includes 100,000 entries for Tibetan, and\n50,000 entries each for Uyghur and Mongolian, specifically curated for headline\ngeneration tasks. Additionally, we propose a high-quality test set annotated by\nnative speakers, designed to serve as a benchmark for future research in this\ndomain. We hope this dataset will become a valuable resource for advancing\nheadline generation in Chinese minority languages and contribute to the\ndevelopment of related benchmarks.", "AI": {"tldr": "本文构建了CMHG数据集，目的在于填补少数民族语言特别是藏语、维吾尔语及蒙古语新闻标题生成任务语料库的匮乏，并提供一个高质量测试集作为未来研究的基准。", "motivation": "由于中国少数民族语言（如藏语、维吾尔语和传统蒙古语）书写系统独特，与国际标准不同，导致缺乏相关语料库，特此解决了这一问题。", "method": "本文提出了一个名为中国少数民族新闻标题生成（CMHG）的新数据集，该数据集包含10万条藏文和各5万条维吾尔文及蒙文字条目，专为新闻标题生成任务设计。同时，还提供了一个由本土人士标注的高质量测试集，作为未来研究的基准。", "result": "建立了包含藏语、维吾尔语及蒙古语的新闻标题生成数据集，并创建了有价值的测试基准。", "conclusion": "该数据集有望成为少数民族语言新闻标题生成研究的重要资源，并推动相关基准的发展。"}}
{"id": "2509.10005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10005", "abs": "https://arxiv.org/abs/2509.10005", "authors": ["Xiaodong Guo", "Tong Liu", "Yike Li", "Zi'ang Lin", "Zhihong Deng"], "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion", "comment": null, "summary": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.", "AI": {"tldr": "本文提出了TUNI方法，通过一种新的RGB-T编码器和局部模块实现优越的RGB-T分割效果，不仅在性能上优于现有模型，还实现了更高的实时效率。", "motivation": "动机在于改善现有RGB-T模型存在的问题，包括受限的热特征提取，次优的跨模态特征融合效果以及冗余编码器导致的实时效率不佳。", "method": "本文提出了一种名为TUNI的方法，使用RGB-T编码器，该编码器由多个堆叠的块组成，能够在多个模态进行特征提取和跨模态特征融合。并且引入了RGB-T局部模块，通过自适应余弦相似性来增强跨模态局部特征融合能力。编码器通过大规模预训练增强其融合特征的能力，同时通过精简热侧分支来获得更紧凑的架构。", "result": "实验结果显示，TUNI在FMB，PST900和CART数据集上取得了与现有最先进的模型相竞争的表现，且参数更少，计算成本更低。同时，在Jetson Orin NX设备上实现了27 FPS的推理速度，展示了其实际部署中的实时能力。", "conclusion": "本文提出的TUNI方法在优化RGB-T分割的效果和实时性方面表现出色，并验证了其在资源受限设备上的应用潜力。"}}
{"id": "2509.10004", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10004", "abs": "https://arxiv.org/abs/2509.10004", "authors": ["Ponhvoan Srey", "Xiaobao Wu", "Anh Tuan Luu"], "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes", "comment": "To appear in EMNLP 2025", "summary": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection.", "AI": {"tldr": "IRIS提出了一种无监督的幻觉检测框架，通过利用LLMs的内部表示和响应不确定性作为软伪标签，提高检测的准确性和泛化能力。实验表明该方法优于现有方法，且计算成本低，适合实时应用。", "motivation": "IRIS旨在解决现有的无监督幻觉内容检测方法依赖于与事实准确性无关的代理信号的问题，这些信号导致检测偏向于表面或非真相相关方面，从而限制了在不同数据集和场景下的通用性。通过利用与事实准确性相关的内部表示，IRIS能够克服这些限制。", "method": "IRIS方法利用大型语言模型（LLMs）内部与事实准确性相关的表示来检测幻觉内容。该方法首先通过提示LLM仔细验证给定陈述的真实性，然后获取其经过上下文化处理的嵌入作为训练的有信息量的特征。同时，每个响应的不确定性被视为真实性的软伪标签。", "result": "实验结果表明，IRIS在现有的无监督方法中表现出色，保持了高准确性和鲁棒性。该方法不仅不需要人工标注且计算成本低，即使在少量数据的情况下也能实现良好性能，非常适合实时检测应用。", "conclusion": "IRIS提出了无监督的现象检测框架，证明通过利用内部表示可以提高幻觉内容检测的准确性和鲁棒性。"}}
{"id": "2509.10006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10006", "abs": "https://arxiv.org/abs/2509.10006", "authors": ["Masaki Akiba", "Shumpei Takezaki", "Daichi Haraguchi", "Seiichi Uchida"], "title": "Few-Part-Shot Font Generation", "comment": "ICDAR 2025 Workshop on Machine Learning", "summary": "This paper proposes a novel model of few-part-shot font generation, which\ndesigns an entire font based on a set of partial design elements, i.e., partial\nshapes. Unlike conventional few-shot font generation, which requires entire\ncharacter shapes for a couple of character classes, our approach only needs\npartial shapes as input. The proposed model not only improves the efficiency of\nfont creation but also provides insights into how partial design details\ninfluence the entire structure of the individual characters.", "AI": {"tldr": "提出了一种基于部分设计元素生成完整字体的新型少部分样本字体生成模型，提高了字体创建效率并提供了关于部分设计细节如何影响整体字符结构的见解。", "motivation": "解决传统少样本字体生成需要完整字符形状的问题，提高字体设计的灵活性和效率。", "method": "基于部分设计元素设计整个字体，而不是传统的需要完整字符形状的方法。", "result": "实现了更高效的字体生成方法，并提供了部分设计细节影响整体字符结构的见解。", "conclusion": "提出了一个有效的模型来根据部分字体元素设计整个字体，相比于传统方法更为灵活高效。"}}
{"id": "2509.10010", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.10010", "abs": "https://arxiv.org/abs/2509.10010", "authors": ["Adnan Ahmad", "Philine Kowol", "Stefan Hillmann", "Sebastian Möller"], "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs", "comment": null, "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots.", "AI": {"tldr": "本篇论文分析了使用开源大型语言模型（LLM）进行多标签意图分类的效果，研究了三个热门的开源预训练LLM在MultiWOZ 2.1数据集上的性能差异，并比较了指令驱动微调和监督学习方法。结果显示，在少样本设定下，Mistral-7B-v0.1 在14个意图类别中的11个类别上表现最佳，但是基于BERT的监督分类器表现优于所有少样本生成式LLM。", "motivation": "研究动机在于探索基于开源且可在普通硬件上运行的大型语言模型进行多意图对话分析的可行性，特别是在对话系统领域，以便为任务导向型聊天机器人的自然语言理解提供改进方案。", "method": "研究中使用了MultiWOZ 2.1数据集，对LLama2-7B-hf、Mistral-7B-v0.1 和 Yi-6B这三种模型进行了少样本设置下的分类任务，同时将这些模型的性能与基于BertForSequenceClassification的小型变压器模型的监督学习方法进行了对比。", "result": "评估的指标包括精确度、召回率以及微平均、宏平均和加权F1分数，结果显示Mistral-7B-v0.1模型在11个意图类别上的加权平均F-Score为0.50，这是在少样本设定下最佳的表现。", "conclusion": "研究证明了开源LLM在处理复杂多意图对话中的检测能力，即使相比先进的监督分类器如基于BERT的模型，仍有改进的空间。这为部署在资源受限环境中的开源LLM提供了支持。"}}
{"id": "2509.10021", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.10021", "abs": "https://arxiv.org/abs/2509.10021", "authors": ["Jonas Kühne", "Christian Vogt", "Michele Magno", "Luca Benini"], "title": "Efficient and Accurate Downfacing Visual Inertial Odometry", "comment": "This article has been accepted for publication in the IEEE Internet\n  of Things Journal (IoT-J)", "summary": "Visual Inertial Odometry (VIO) is a widely used computer vision method that\ndetermines an agent's movement through a camera and an IMU sensor. This paper\npresents an efficient and accurate VIO pipeline optimized for applications on\nmicro- and nano-UAVs. The proposed design incorporates state-of-the-art feature\ndetection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and\nquantized for emerging RISC-V-based ultra-low-power parallel systems on chips\n(SoCs). Furthermore, by employing a rigid body motion model, the pipeline\nreduces estimation errors and achieves improved accuracy in planar motion\nscenarios. The pipeline's suitability for real-time VIO is assessed on an\nultra-low-power SoC in terms of compute requirements and tracking accuracy\nafter quantization. The pipeline, including the three feature tracking methods,\nwas implemented on the SoC for real-world validation. This design bridges the\ngap between high-accuracy VIO pipelines that are traditionally run on\ncomputationally powerful systems and lightweight implementations suitable for\nmicrocontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates\nan average reduction in RMSE of up to a factor of 3.65x over the baseline\npipeline when using the ORB feature tracker. The analysis of the computational\ncomplexity of the feature trackers further shows that PX4FLOW achieves on-par\ntracking accuracy with ORB at a lower runtime for movement speeds below 24\npixels/frame.", "AI": {"tldr": "本文提出了一种适用于微型和纳米级无人机的高效且精确的视觉惯性里程计(VIO)流水线，基于RISC-V超低功耗并行SoC。通过刚体运动模型减少估计误差，并针对不同特征追踪方法进行了优化和实施。结果表明，优化后的流水线在GAP9超低功耗SoC上使用ORB特征追踪时，RMS误差平均降低至基线流水线的3.65倍。同时，PX4FLOW在低于24像素/帧的速度下与ORB的追踪精度相当，但运行时间更短。", "motivation": "本文旨在开发适用于微型和纳米级无人机的VIO流水线，解决传统高精度VIO通常运行在计算能力强大的系统上的问题，为微控制器提供轻量级的VIO实现。", "method": "文中提出了几种用于VIO流水线的特征检测和跟踪方法的最新状态（SuperPoint, PX4FLOW, ORB），所有这些方法均已经过优化和量化以适配新兴的RISC-V超低功耗SoC，并通过刚体运动模型减少运动估计误差。", "result": "实验评估显示，所提出的VIO流水线在低功耗SoC中具有良好的计算需求和追踪精度表现。通过量化实施，与用ORB特征追踪的基线流水线相比，该流水线在GAP9超低功耗SoC上RMS误差降低了3.65倍。PX4FLOW还可以在较低速度下的优秀追踪性能下保持实时运行。", "conclusion": "该研究开发了一种实验性VIO管道，适用于微小型无人机应用领域，在保持高准确性的同时具备较低能量需求，克服了高精度VIO通常依赖于强大的计算系统的问题，并通过量化和实施实现实时跟踪需求。"}}
{"id": "2509.10035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10035", "abs": "https://arxiv.org/abs/2509.10035", "authors": ["Laurin Plank", "Armin Zlomuzica"], "title": "Linguistic trajectories of bipolar disorder on social media", "comment": "Pre-print", "summary": "Language provides valuable markers of affective disorders such as bipolar\ndisorder (BD), yet clinical assessments remain limited in scale. In response,\nanalyses of social media (SM) language have gained prominence due to their high\ntemporal resolution and longitudinal scope. Here, we introduce a method to\ndetermine the timing of users' diagnoses and apply it to study language\ntrajectories from 3 years before to 21 years after BD diagnosis - contrasted\nwith uses reporting unipolar depression (UD) and non-affected users (HC). We\nshow that BD diagnosis is accompanied by pervasive linguistic alterations\nreflecting mood disturbance, psychiatric comorbidity, substance abuse,\nhospitalization, medical comorbidities, unusual thought content, and\ndisorganized thought. We further observe recurring mood-related language\nchanges across two decades after the diagnosis, with a pronounced 12-month\nperiodicity suggestive of seasonal mood episodes. Finally, trend-level evidence\nsuggests an increased periodicity in users estimated to be female. In sum, our\nfindings provide evidence for language alterations in the acute and chronic\nphase of BD. This validates and extends recent efforts leveraging SM for\nscalable monitoring of mental health.", "AI": {"tldr": "研究通过分析社交媒体上的语言变化来监测双相情感障碍（BD）患者的情绪变化，并发现诊断前后语言上存在显著的情绪困扰、精神共病等特征，且这些变化具有一定的季节性周期性。", "motivation": "临床评估的局限性促使研究人员转向分析社交媒体语言，利用其高时间分辨率和纵向视角，以更好地理解及监测双相情感障碍。", "method": "提出了一种确定用户诊断时间的方法，并将其应用于BD确诊前后共24年内的语言轨迹研究。", "result": "确诊双相情感障碍的行为伴随着情绪困扰、精神共病等一系列语言特征变化，并在确诊后二十年内观察到了每12个月一次的情绪语言变化周期。", "conclusion": "研究证实社交媒体语言分析在监测双相情感障碍急性阶段和慢性阶段语言变化的重要性，为大规模监测精神健康提供了新的视角。"}}
{"id": "2509.10024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10024", "abs": "https://arxiv.org/abs/2509.10024", "authors": ["Danling Cao"], "title": "Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images", "comment": "This work was completed during the author's MPhil studies at the\n  University of Manchester", "summary": "Recovering 3D face models from 2D in-the-wild images has gained considerable\nattention in the computer vision community due to its wide range of potential\napplications. However, the lack of ground-truth labeled datasets and the\ncomplexity of real-world environments remain significant challenges. In this\nchapter, we propose a convolutional neural network-based approach, the\nHierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face\nmodels from single in-the-wild images. Our model predicts detailed facial\ngeometry, texture, pose, and illumination parameters from a single image.\nSpecifically, we employ a pre-trained hierarchical backbone network and\nintroduce multi-level attention mechanisms at different stages of 2D face image\nfeature extraction. A semi-supervised training strategy is employed,\nincorporating 3D Morphable Model (3DMM) parameters from publicly available\ndatasets along with a differentiable renderer, enabling an end-to-end training\nprocess. Extensive experiments, including both comparative and ablation\nstudies, were conducted on two benchmark datasets, AFLW2000-3D and MICC\nFlorence, focusing on 3D face reconstruction and 3D face alignment tasks. The\neffectiveness of the proposed method was evaluated both quantitatively and\nqualitatively.", "AI": {"tldr": "提出了一种用于从野外单张2D图像中重建3D人脸模型的方法，通过广泛的实验验证其有效性。", "motivation": "3D人脸模型从2D图像恢复在计算机视觉领域受到了广泛关注，但由于缺少带有真实标记的数据集和复杂的真实环境，仍然存在很大的挑战。", "method": "提出了一种基于卷积神经网络的方法，即分层多级注意力网络（MLANet），用于从野外单张2D图像中重建3D人脸模型。该模型从单张图像中预测详细的面部几何形状、纹理、姿态和照明参数。具体来说，采用预训练的分层主干网络，并引入多层次注意力机制，在2D人脸图像特征提取的不同阶段发挥作用。", "result": "进行了广泛的实验，包括比较实验和消融实验，在两个基准数据集AFLW2000-3D和MICC Florence上进行，专注于3D人脸重建和3D人脸对齐任务。", "conclusion": "定性定量评估结果表明提出的方法是有效的。"}}
{"id": "2509.10040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10040", "abs": "https://arxiv.org/abs/2509.10040", "authors": ["Mohamed Basem", "Mohamed Younes", "Seif Ahmed", "Abdelrahman Moustafa"], "title": "!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment", "comment": "10 Pages , 8 figures , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained\nArabic readability assessment, achieving first place in six of six tracks. Our\napproach is a confidence-weighted ensemble of four complementary transformer\nmodels (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with\ndistinct loss functions to capture diverse readability signals. To tackle\nsevere class imbalance and data scarcity, we applied weighted training,\nadvanced preprocessing, SAMER corpus relabeling with our strongest model, and\nsynthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level\nsamples. A targeted post-processing step corrected prediction distribution\nskew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system\nreached 87.5 percent QWK at the sentence level and 87.4 percent at the document\nlevel, demonstrating the power of model and loss diversity, confidence-informed\nfusion, and intelligent augmentation for robust Arabic readability prediction.", "AI": {"tldr": "该系统通过集成四个互补的变压器模型并使用多种技术来应对类别不平衡和数据稀缺问题，从而在六个追踪项目中获得第一，并且在句子和文档级别达到了 87.5% 和 87.4% 的二次加权卡帕得分。", "motivation": "该研究是为了应对在阿拉伯语可读性评估任务中常见的数据稀缺和类别不均衡的问题，通过集成多个经过细调的变压器模型来提高预测准确性和鲁棒性。", "method": "我们的方法是一个带有置信度加权的集合，它包括四个互补的变压器模型（AraBERTv2、AraELECTRA、MARBERT 和 CAMeLBERT），每个模型通过不同的损失函数进行微调，以捕捉多样化的可读性信号。为了应对严重的类别不平衡和数据稀缺问题，我们采用了加权训练、先进的预处理、使用最强模型重新标记 SAMER 语料库以及通过 Gemini 2.5 Flash 生成合成数据，增加了大约 10,000 个罕见程度的样本。此外，我们还设计了一个针对性的后处理步骤来矫正预测分布的偏差，从而带来 6.3% 的二次加权卡帕（QWK）增益。", "result": "这个系统的性能达到了 87.5% 的句子级别和 87.4% 的文档级别的 QWK 分数，证明了其在阿拉伯语可读性评估上的成功。", "conclusion": "该系统显示了在阿拉伯语可读性预测中，模型和损失函数多样性、基于信心的融合以及智能增强技术的强大作用，可以在句子和文档级别上实现高度准确的预测。"}}
{"id": "2509.10026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10026", "abs": "https://arxiv.org/abs/2509.10026", "authors": ["Jing Huang", "Zhiya Tan", "Shutao Gong", "Fanwei Zeng", "Jianshu Li"], "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA", "comment": "12 Pages, 12 Figures, 2 Tables", "summary": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}", "AI": {"tldr": "LaV-CoT是第一个语言感知的视觉推理框架，通过多阶段推理流程和奖励优化方法，显著提升了多语言视觉问答任务的准确率和可解释性。", "motivation": "本文旨在解决现有方法过分依赖文本推理，对多语言多模态推理支持匮乏，不能充分应用于实际部署的问题。提出LaV-CoT框架，以支持更强大、更可解释的多语言多模态视觉推理。", "method": "LaV-CoT框架运用可解释的多阶段推理流程，包括带边界框的文本总结、语言识别、空间对象描述和逐步逻辑推理。通过迭代生成、修正和精炼，设计了全自动的数据整理方法，生成多语言推理注释。LaV-CoT采用两个阶段的训练模式，结合了监督微调和基于语言感知组成员相对策略优化，通过多方面奖励进行指导，以提高推理和泛化能力。", "result": "LaV-CoT在MMMB和Multilingual MMBench等公开数据集上取得最多9.5%的准确率提升，甚至超越规模大2倍的模型2.6%。此外，LaV-CoT在GPT-4o-0513和Gemini-2.5-flash等先进专有模型上也表现出更优性能。在线A/B测试进一步证实其在现实世界数据上的有效性。", "conclusion": "研究证实LaV-CoT框架在多语言多模态视觉问答任务中展现出优越的性能和可解释性，适用于工业部署，并促进了该领域的进一步发展。"}}
{"id": "2509.10078", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10078", "abs": "https://arxiv.org/abs/2509.10078", "authors": ["Dongmin Choi", "Woojung Song", "Jongwook Han", "Eun-Ju Lee", "Yohan Jo"], "title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models", "comment": "17 pages, 4 figures", "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication.", "AI": {"tldr": "研究建议不应使用既有心理问卷来评估LLMs，并比较了既有心理问卷和具生态效度的问卷在衡量大语言模型（LLMs）人格特质时的结果。", "motivation": "由于在大语言模型评估中使用人类设计的心理问卷的生态效度受到质疑，本研究旨在探讨既有心理问卷和具生态效度的问卷之间的结果差异及其可能提供的见解。", "method": "本研究进行了一项全面的比较分析，比较了既有心理问卷和具生态效度的问卷在衡量大语言模型（LLMs）人格特质时的结果。", "result": "研究结果显示，既有问卷可能导致（1）生成与用户查询上下文中表现出的LLMs的心理特征明显不同的资料，（2）评分项目不足，不能提供稳定测量，（3）生成LLMs有稳定构建的误导性印象，以及（4）对于受到人设提示的LLMs，生成夸大的资料。", "conclusion": "总体而言，本研究指出应谨慎使用既有心理问卷来衡量大语言模型（LLMs）人格特质，强调了采用生态效度更高的方法的重要性。"}}
{"id": "2509.10058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10058", "abs": "https://arxiv.org/abs/2509.10058", "authors": ["Sung-Lin Tsai", "Bo-Lun Huang", "Yu Ting Shen", "Cheng Yu Yeo", "Chiang Tseng", "Bo-Kai Ruan", "Wen-Sheng Lien", "Hong-Han Shuai"], "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation", "comment": "Accepted to ACM Multimedia 2025 (MM '25)", "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.", "AI": {"tldr": "提出无需训练的框架，使用LLM解决颜色描述模糊问题，提高T2I模型生成图像的颜色准确性。", "motivation": "为了精确渲染具有提示模糊性的颜色，提出了一种无需训练的框架，以解决现有模型在复杂颜色描述上的困难，这些模型往往产生与人类意图不符的图像。", "method": "通过利用大型语言模型（LLM）解析模糊的颜色描述，并在文本嵌入空间中直接指导颜色混合操作来提高颜色保真度。方法首先使用LLM解析文本提示中的模糊颜色术语，然后根据颜色术语在CIELAB颜色空间中的空间关系优化文本嵌入。", "result": "实验结果表明，该框架在不牺牲图像质量的情况下提高了颜色对齐效果，弥合了文本语义和视觉生成之间的差距。", "conclusion": "本研究提供了一种新的方法，能够在不进行额外训练或使用外部参考图像的情况下改善颜色准确性。"}}
{"id": "2509.10087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10087", "abs": "https://arxiv.org/abs/2509.10087", "authors": ["Mustapha Adamu", "Qi Zhang", "Huitong Pan", "Longin Jan Latecki", "Eduard C. Dragut"], "title": "Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery", "comment": "ACM SIGIR 2025 Workshop MANILA", "summary": "The growing complexity and volume of climate science literature make it\nincreasingly difficult for researchers to find relevant information across\nmodels, datasets, regions, and variables. This paper introduces a\ndomain-specific Knowledge Graph (KG) built from climate publications and\nbroader scientific texts, aimed at improving how climate knowledge is accessed\nand used. Unlike keyword based search, our KG supports structured, semantic\nqueries that help researchers discover precise connections such as which models\nhave been validated in specific regions or which datasets are commonly used\nwith certain teleconnection patterns. We demonstrate how the KG answers such\nquestions using Cypher queries, and outline its integration with large language\nmodels in RAG systems to improve transparency and reliability in\nclimate-related question answering. This work moves beyond KG construction to\nshow its real world value for climate researchers, model developers, and others\nwho rely on accurate, contextual scientific information.", "AI": {"tldr": "本文介绍了一个气候知识图谱，帮助研究人员改善获取气候知识的方式，展示了其实际应用价值。", "motivation": "随着气候科学文献复杂性与数据量的增加，研究人员越来越难以在模型、数据集、区域和变量之间找到相关的信息。", "method": "本文介绍了一个基于气候出版物和更广泛科学文本构建的领域特定知识图谱（KG），旨在改善气候知识的获取和使用方式。与基于关键词的搜索不同，该KG支持结构化语义查询，帮助研究人员发现精确的关联，如哪些模型已在特定区域进行了验证，或哪些数据集通常与某些遥相关模式一起使用。", "result": "该KG展示了通过Cypher查询回答这些问题的方式，并概述了其与RAG系统中大型语言模型的集成，以提高气候相关问答的透明度和可靠性。", "conclusion": "这项工作超越了构建知识图谱范围，展示了其对气候研究人员、模型开发者以及其他依赖准确且具有上下文的科学信息的人们的价值。"}}
{"id": "2509.10059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10059", "abs": "https://arxiv.org/abs/2509.10059", "authors": ["Yue Zhou", "Litong Feng", "Mengcheng Lan", "Xue Yang", "Qingyun Li", "Yiping Ke", "Xue Jiang", "Wayne Zhang"], "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration", "comment": "17 pages, 16 figures", "summary": "Mathematical reasoning is critical for tasks such as precise distance and\narea computations, trajectory estimations, and spatial analysis in unmanned\naerial vehicle (UAV) based remote sensing, yet current vision-language models\n(VLMs) have not been adequately tested in this domain. To address this gap, we\nintroduce AVI-Math, the first benchmark to rigorously evaluate multimodal\nmathematical reasoning in aerial vehicle imagery, moving beyond simple counting\ntasks to include domain-specific knowledge in areas such as geometry, logic,\nand algebra. The dataset comprises 3,773 high-quality vehicle-related questions\ncaptured from UAV views, covering 6 mathematical subjects and 20 topics. The\ndata, collected at varying altitudes and from multiple UAV angles, reflects\nreal-world UAV scenarios, ensuring the diversity and complexity of the\nconstructed mathematical problems. In this paper, we benchmark 14 prominent\nVLMs through a comprehensive evaluation and demonstrate that, despite their\nsuccess on previous multimodal benchmarks, these models struggle with the\nreasoning tasks in AVI-Math. Our detailed analysis highlights significant\nlimitations in the mathematical reasoning capabilities of current VLMs and\nsuggests avenues for future research. Furthermore, we explore the use of\nChain-of-Thought prompting and fine-tuning techniques, which show promise in\naddressing the reasoning challenges in AVI-Math. Our findings not only expose\nthe limitations of VLMs in mathematical reasoning but also offer valuable\ninsights for advancing UAV-based trustworthy VLMs in real-world applications.\nThe code, and datasets will be released at\nhttps://github.com/VisionXLab/avi-math", "AI": {"tldr": "研究提供了AVI-Math，一个评价无人飞行器视角下的多模态数学推理的基准数据集，通过评估多种视觉语言模型，识别其在需数学推理的任务中的不足，并探索可能的解决方法。", "motivation": "鉴于现有的视觉语言模型在无人飞行器远程感测的数学推理领域难以被充分测试，该研究旨在填补这一空白，尤其是针对几何学、逻辑和代数等领域的推理能力。", "method": "介绍了AVI-Math，这是一个专门为评价基于无人飞行器的多模态数学推理而设计的基准测试。数据集包含来自无人飞行器视角捕捉的3,773个高质量车辆相关问题，覆盖了6个数学主题和20个具体话题。", "result": "通过对14个主要的VLM模型进行评估，发现尽管这些模型在之前的多模态基准测试中表现良好，但在AVI-Math中的数学推理任务中却面临困难。", "conclusion": "研究揭示了现有的视觉语言模型在数学推理能力上的限制，并提出通过细调技术以及链式思考提示法来解决问题的潜力，为提高无人飞行器应用场景中可信赖的VLM模型提供了宝贵的见解。"}}
{"id": "2509.10095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10095", "abs": "https://arxiv.org/abs/2509.10095", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Ammar Mohammed"], "title": "Arabic Large Language Models for Medical Text Generation", "comment": "Published in 2025 4th International Conference on Computer\n  Technologies (ICCTech)", "summary": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments.", "AI": {"tldr": "该研究展示了通过微调大语言模型来生成阿拉伯语医学文本，对于解决HMS中的挑战具有潜力，提供了一种可扩展和适应全球卫生保健挑战的解决方案，尤其是语言和文化多元化的环境。", "motivation": "研究动机在于解决医院管理系统（HMS）面临的如过度拥挤、资源有限和紧急医疗服务不足的挑战，特别是缺乏提供准确实时医学建议的能力，尤其是在处理不规则输入和少数语言时。", "method": "该研究通过微调大语言模型（LLMs）来生成阿拉伯语医学文本，旨在为患者提供准确的医疗建议、诊断、药物推荐和治疗计划。研究方法包括收集社交媒体平台上的患者和医生之间的实际医疗对话数据，并进行预处理以适应多种阿拉伯方言。细调的模型包括Mistral-7B-Instruct-v0.2, LLaMA-2-7B, 和GPT-2 Medium。", "result": "评估结果显示，细调后的Mistral-7B模型表现优于其他模型，其BERT评分的精准度、召回率和F1得分分别为68.5%，69.08%和68.5%。基准测试和定性评估表明该系统能够根据非正式输入生成连贯和相关的医疗回复。", "conclusion": "这项研究表明，生成式人工智能在促进HMS方面具有潜力，为解决全球医疗服务中的挑战提供了可扩展和适应性强的解决方案，尤其是在语言和文化多元化的环境中。"}}
{"id": "2509.10080", "categories": ["cs.CV", "I.2.9; I.4.8"], "pdf": "https://arxiv.org/pdf/2509.10080", "abs": "https://arxiv.org/abs/2509.10080", "authors": ["Minsang Kong", "Myeongjun Kim", "Sang Gu Kang", "Sang Hun Lee"], "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals", "comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems\n  (under review)", "summary": "In autonomous driving, trajectory prediction is essential for ensuring safe\nand efficient navigation. To improve prediction accuracy, recent approaches\noften rely on pre-built high-definition (HD) maps or real-time local map\nconstruction modules to incorporate static environmental information. However,\npre-built HD maps are limited to specific regions and cannot adapt to transient\nchanges. In addition, local map construction modules, which recognize only\npredefined elements, may fail to capture critical scene details or introduce\nerrors that degrade prediction performance. To overcome these limitations, we\npropose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory\nprediction framework that operates directly in the bird's-eye view (BEV) space\nutilizing real-time sensor data without relying on any pre-built maps. The\nBEVTraj leverages deformable attention to efficiently extract relevant context\nfrom dense BEV features. Furthermore, we introduce a Sparse Goal Candidate\nProposal (SGCP) module, which enables full end-to-end prediction without\nrequiring any post-processing steps. Extensive experiments demonstrate that the\nBEVTraj achieves performance comparable to state-of-the-art HD map-based models\nwhile offering greater flexibility by eliminating the dependency on pre-built\nmaps. The source code is available at https://github.com/Kongminsang/bevtraj.", "AI": {"tldr": "提出了BEVTraj，一种仅依赖实时传感器数据进行轨迹预测的新框架，无需依赖预建地图。", "motivation": "现有的轨迹预测方法依赖于预建的高精度地图或实时局部地图构建模块，这些方法存在地区限制、无法适应瞬时变化以及可能引入预测错误的缺点。", "method": "BEVTraj 框架基于鸟瞰视图空间进行轨迹预测，使用可变形注意力机制从密集的BEV特征中高效提取相关上下文，并引入稀疏目标候选提案模块来实现端到端的预测。", "result": "实验表明，BEVTraj 达到了与基于高精度地图的最先进模型相当的性能，并提供了更大的灵活性，因为它没有依赖于预建地图。", "conclusion": "BEVTraj 提供了一种新型的在自动驾驶场景中替代预建地图的轨迹预测解决方案，体现了高度的实时性和灵活性。"}}
{"id": "2509.10108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10108", "abs": "https://arxiv.org/abs/2509.10108", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Khaled Shaban"], "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records", "comment": "Accepted in AICCSA 2025", "summary": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems.", "AI": {"tldr": "Synthetic data augmentation using advanced AI systems improved the scalability and generalization of models for developing medical chatbots in Arabic.", "motivation": "To address the scarcity of large-scale, high-quality annotated datasets for developing Arabic medical chatbots, resulting in limited scalability and generalization of models.", "method": "We propose a scalable synthetic data augmentation strategy using advanced generative AI systems like ChatGPT-4o and Gemini 2.5 Pro to generate 80,000 synthetic question-answer pairs and integrate them into the training pipeline.", "result": "The fine-tuned models showed improved performance using BERTScore metrics and qualitative assessments. An ablation study showed ChatGPT-4o data led to higher F1-scores and fewer hallucinations.", "conclusion": "Synthetic data augmentation is a viable solution for enhancing domain-specific language models in low-resource medical NLP, leading to more inclusive, scalable, and accurate Arabic healthcare chatbot systems."}}
