{"id": "2506.20747", "categories": ["cs.CL", "68T50, 68T37", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.20747", "abs": "https://arxiv.org/abs/2506.20747", "authors": ["Chen Shen", "Sajjadur Rahman", "Estevam Hruschka"], "title": "Towards Probabilistic Question Answering Over Tabular Data", "comment": null, "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning.", "AI": {"tldr": "The paper introduces LUCARIO, a framework that uses Bayesian Networks and large language models to answer probabilistic questions over large tabular data, demonstrating improved performance over existing methods.", "motivation": "The motivation is to address the shortcomings of current NL2SQL systems in handling probabilistic questions over tabular data that require reasoning under uncertainty.", "method": "Our method induces Bayesian Networks from tables, translates natural language queries into probabilistic queries, and uses large language models (LLMs) to generate final answers.", "result": "Empirical results demonstrate significant improvements over baselines.", "conclusion": "The study highlights the benefits of hybrid symbolic-neural reasoning for probabilistic QA over large tabular data."}}
{"id": "2506.20793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20793", "abs": "https://arxiv.org/abs/2506.20793", "authors": ["Victor Ojewale", "Inioluwa Deborah Raji", "Suresh Venkatasubramanian"], "title": "Multi-lingual Functional Evaluation for Large Language Models", "comment": null, "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations.", "AI": {"tldr": "构建了新多语言功能性基准测试，更准确地评估大模型的多语言能力，发现现有静态基准测试功能表现捕捉能力不足且模型鲁棒性在不同语言间有显著差异。", "motivation": "多语言能力在大规模语言模型中通常通过静态数据基准进行评估，但这些评估往往无法充分反映模型在多语言设置中的实际性能和鲁棒性。为了解决这一问题，构建了多语言功能性基准测试。", "method": "基于现有功能基准测试模板从英语翻译到包括法语、西班牙语、印地语、阿拉伯语和约鲁巴语在内的五种语言，以此构建跨语言功能性基准测试CL-GSM Symbolic和CL-IFEval，以更准确地评估大规模语言模型在多语言环境中的实际表现和鲁棒性。", "result": "研究结果显示，某些静态多语言基准测试功能性能的捕捉能力远低于新构建的功能性基准测试。例如，M-GSM和CL-GSM Symbolic在英语、法语和西班牙语中有24%、17%和18%的性能下降；而Belebele和CL-IFEval之间的性能下降为15%-24%，而M-MMLU和CL-IFEval之间的性能下降仅为0.5%-3%。此外，不同语言中的模型鲁棒性表现差异显著，以阿拉伯语和英语为例，这两种语言在反复评估中表现最为稳定。", "conclusion": "新构建的跨语言功能性基准测试能更精准地衡量大规模语言模型在多语言环境中的性能表现和鲁棒性，指出了不同静态多语言基准测试在功能性能捕捉上的不一致性。"}}
{"id": "2506.20803", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20803", "abs": "https://arxiv.org/abs/2506.20803", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "comment": "main paper is 14 pages", "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.", "AI": {"tldr": "研究发现，尽管LLM生成的想法在创新度方面具有潜力，但在实际执行结果上并不如人类专家的想法有效。", "motivation": "探讨AI生成的想法是否能导致更好的研究成果，而不是仅仅看起来新颖。", "method": "通过招募43位专家研究人员执行由专家撰写或LLM生成的想法来进行执行研究。每个专家花费超过100小时实现想法，并撰写一篇4页的短论文来记录实验，并由专家NLP研究者进行匿名评审。", "result": "执行研究后，LLM生成的想法的评分在所有评估指标（新颖性、兴奋度、有效性及总体）上显著下降，甚至在许多指标上，人类的想法得分高于LLM的想法。", "conclusion": "此执行研究揭示了当前LLM在生成真正有效的研究想法方面的局限性，以及在没有执行结果的情况下评估研究想法的挑战。"}}
{"id": "2506.20821", "categories": ["cs.CL", "cs.AI", "cs.CE", "68T50, 68T07 (Primary) 68P20, 91G15, 91G70, 68U10 (Secondary)", "I.2.7; I.2.10; H.3.3; H.2.8; I.5.4; J.1"], "pdf": "https://arxiv.org/pdf/2506.20821", "abs": "https://arxiv.org/abs/2506.20821", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "title": "MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering", "comment": "Preprint Copy", "summary": "Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span\nhundreds of pages and combine diverse modalities, including dense narrative\ntext, structured tables, and complex figures. Answering questions over such\ncontent often requires joint reasoning across modalities, which strains\ntraditional large language models (LLMs) and retrieval-augmented generation\n(RAG) pipelines due to token limitations, layout loss, and fragmented\ncross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation\nframework purpose-built for financial QA. MultiFinRAG first performs multimodal\nextraction by grouping table and figure images into batches and sending them to\na lightweight, quantized open-source multimodal LLM, which produces both\nstructured JSON outputs and concise textual summaries. These outputs, along\nwith narrative text, are embedded and indexed with modality-aware similarity\nthresholds for precise retrieval. A tiered fallback strategy then dynamically\nescalates from text-only to text+table+image contexts when necessary, enabling\ncross-modal reasoning while reducing irrelevant context. Despite running on\ncommodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy\nthan ChatGPT-4o (free-tier) on complex financial QA tasks involving text,\ntables, images, and combined multimodal reasoning.", "AI": {"tldr": "MultiFinRAG通过多模态提取策略和模态感知的精确检索技术，大幅提高了在财务问答任务上的精度表现。", "motivation": "传统的大型语言模型和增强检索生成管道在处理包含密集叙述文本、结构化表格和复杂图像的财务文档时存在代数限制、布局丢失和片段化跨模态上下文等问题。", "method": "MultiFinRAG采用多模态提取策略，首先将表格和图像分批处理，通过轻量级且量化后的开源多模态LLM生成结构化JSON输出和简洁的文本摘要。然后将这些输出与叙述性文本嵌入并索引，设置模态感知的相似性阈值以实现精确检索。此外，采用分层降级策略以动态地从文本到文本+表格+图像的上下文转换，从而减少无关上下文并促进跨模态推理。", "result": "在涉及文本、表格、图像和多模态推理的复杂财务问答任务上，MultiFinRAG相比ChatGPT-4o（免费版），在普通硬件上运行时，能获得高出19个百分点的准确率。", "conclusion": "MultiFinRAG是一个专门为财务问答设计的检索增强生成框架，能够解决传统大型语言模型和检索增强生成方法的局限性问题。"}}
{"id": "2506.20741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20741", "abs": "https://arxiv.org/abs/2506.20741", "authors": ["Qin Ren", "Yifan Wang", "Ruogu Fang", "Haibin Ling", "Chenyu You"], "title": "OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport", "comment": null, "summary": "Survival prediction using whole slide images (WSIs) can be formulated as a\nmultiple instance learning (MIL) problem. However, existing MIL methods often\nfail to explicitly capture pathological heterogeneity within WSIs, both\nglobally -- through long-tailed morphological distributions, and locally\nthrough -- tile-level prediction uncertainty. Optimal transport (OT) provides a\nprincipled way of modeling such heterogeneity by incorporating marginal\ndistribution constraints. Building on this insight, we propose OTSurv, a novel\nMIL framework from an optimal transport perspective. Specifically, OTSurv\nformulates survival predictions as a heterogeneity-aware OT problem with two\nconstraints: (1) global long-tail constraint that models prior morphological\ndistributions to avert both mode collapse and excessive uniformity by\nregulating transport mass allocation, and (2) local uncertainty-aware\nconstraint that prioritizes high-confidence patches while suppressing noise by\nprogressively raising the total transport mass. We then recast the initial OT\nproblem, augmented by these constraints, into an unbalanced OT formulation that\ncan be solved with an efficient, hardware-friendly matrix scaling algorithm.\nEmpirically, OTSurv sets new state-of-the-art results across six popular\nbenchmarks, achieving an absolute 3.6% improvement in average C-index. In\naddition, OTSurv achieves statistical significance in log-rank tests and offers\nhigh interpretability, making it a powerful tool for survival prediction in\ndigital pathology. Our codes are available at\nhttps://github.com/Y-Research-SBU/OTSurv.", "AI": {"tldr": "OTSurv利用最优传输方法解决病理图像中的异质性问题，提高了生存预测的准确性。", "motivation": "现有MIL方法难以捕捉病理图像中的异质性。OTSurv旨在通过全局和局部约束来解决这一问题，以更好地预测生存率。", "method": "OTSurv通过最优传输（OT）框架处理生存预测问题，引入了两个约束：全球长尾约束和局部不确定性约束，进而将问题转化为不平衡最优传输问题，使用矩阵缩放算法求解。", "result": "OTSurv在六个基准测试中表现优于现有方法，平均C指数提高了3.6%。", "conclusion": "OTSurv是一种高效的生存预测工具，在数字病理学中具有高可解释性。"}}
{"id": "2506.20822", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20822", "abs": "https://arxiv.org/abs/2506.20822", "authors": ["Quintin Myers", "Yanjun Gao"], "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes", "comment": "Under review", "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology.", "AI": {"tldr": "研究使用VBVQ评估了六个LLMs在检测暴力倾向上的能力，发现他们表面的文本生成表现与实际对暴力反应的偏好存在分歧，并且倾向于表现出与社会科学发现相矛盾的人口统计学差异。", "motivation": "大型语言模型(LLMs)被越来越多地提议用于检测和响应在线暴力内容，但它们对道德模糊现实情景的推理能力尚未得到充分研究，因此本研究旨在填补这一空白。", "method": "使用经过验证的社会科学研究工具——暴力行为情境问卷(VBVQ)来评估LLMs，引入基于角色的提示以改变美国境内的种族、年龄和地区身份来进行潜在偏见评估。在统一的零样本设置下评估了六个在不同地缘政治和组织背景下开发的LLMs。", "result": "研究表明LLMs的表层文本生成经常与其对暴力反应的内部偏好相悖，而且他们的暴力倾向随不同的人口统计学特征而变化，这经常与既定的犯罪学、社会科学和心理学发现相矛盾。", "conclusion": "尽管LLMs在表层文本生成上有一定表现，但其内部偏好和人口统计学特征对暴力反应倾向的影响仍需进一步探讨，揭示了现有模型在应对复杂社会情景上的局限性。"}}
{"id": "2506.20756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20756", "abs": "https://arxiv.org/abs/2506.20756", "authors": ["Haodong Li", "Chen Wang", "Jiahui Lei", "Kostas Daniilidis", "Lingjie Liu"], "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation", "comment": "Work done in Nov. 2024. Project page: https://stereodiff.github.io/", "summary": "Recent video depth estimation methods achieve great performance by following\nthe paradigm of image depth estimation, i.e., typically fine-tuning pre-trained\nvideo diffusion models with massive data. However, we argue that video depth\nestimation is not a naive extension of image depth estimation. The temporal\nconsistency requirements for dynamic and static regions in videos are\nfundamentally different. Consistent video depth in static regions, typically\nbackgrounds, can be more effectively achieved via stereo matching across all\nframes, which provides much stronger global 3D cues. While the consistency for\ndynamic regions still should be learned from large-scale video depth data to\nensure smooth transitions, due to the violation of triangulation constraints.\nBased on these insights, we introduce StereoDiff, a two-stage video depth\nestimator that synergizes stereo matching for mainly the static areas with\nvideo depth diffusion for maintaining consistent depth transitions in dynamic\nareas. We mathematically demonstrate how stereo matching and video depth\ndiffusion offer complementary strengths through frequency domain analysis,\nhighlighting the effectiveness of their synergy in capturing the advantages of\nboth. Experimental results on zero-shot, real-world, dynamic video depth\nbenchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,\nshowcasing its superior consistency and accuracy in video depth estimation.", "AI": {"tldr": "A new approach, StereoDiff, combines stereo matching and video depth diffusion to better handle static and dynamic regions in video depth estimation, achieving state-of-the-art performance.", "motivation": "To address the fundamental differences in temporal consistency requirements for static and dynamic regions in videos, which are not adequately addressed by simply extending image depth estimation methods.", "method": "StereoDiff, a two-stage video depth estimator, synergizes stereo matching for static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas.", "result": "StereoDiff shows superior consistency and accuracy in video depth estimation, achieving SoTA performance on zero-shot, real-world dynamic video depth benchmarks.", "conclusion": "The synergistic approach of combining stereo matching and video depth diffusion effectively captures the advantages of both techniques, leading to better performance in video depth estimation."}}
{"id": "2506.20876", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20876", "abs": "https://arxiv.org/abs/2506.20876", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "comment": null, "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process.", "AI": {"tldr": "The abstract discusses challenges and considerations for developing end-to-end fact-checking systems specifically in the medical domain, suggesting an interactive approach over autonomous systems.", "motivation": "To explore reasons why advanced fact-checking systems are not widely used in public health and medicine despite technological advancements, and to understand how clinical experts verify medical claims.", "method": "Presents a study examining how clinical experts verify real medical claims from social media by synthesizing scientific evidence.", "result": "Identifies fundamental challenges in applying end-to-end fact-checking to medicine, such as connecting claims to relevant evidence and dealing with ambiguous or subjective claims.", "conclusion": "Proposes that fact-checking in medicine should be treated as an interactive communication problem, rather than relying solely on end-to-end systems."}}
{"id": "2506.20757", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20757", "abs": "https://arxiv.org/abs/2506.20757", "authors": ["Zhiyuan Wu", "Yongqiang Zhao", "Shan Luo"], "title": "ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations", "comment": null, "summary": "Vision and touch are two fundamental sensory modalities for robots, offering\ncomplementary information that enhances perception and manipulation tasks.\nPrevious research has attempted to jointly learn visual-tactile representations\nto extract more meaningful information. However, these approaches often rely on\ndirect combination, such as feature addition and concatenation, for modality\nfusion, which tend to result in poor feature integration. In this paper, we\npropose ConViTac, a visual-tactile representation learning network designed to\nenhance the alignment of features during fusion using contrastive\nrepresentations. Our key contribution is a Contrastive Embedding Conditioning\n(CEC) mechanism that leverages a contrastive encoder pretrained through\nself-supervised contrastive learning to project visual and tactile inputs into\nunified latent embeddings. These embeddings are used to couple visual-tactile\nfeature fusion through cross-modal attention, aiming at aligning the unified\nrepresentations and enhancing performance on downstream tasks. We conduct\nextensive experiments to demonstrate the superiority of ConViTac in real world\nover current state-of-the-art methods and the effectiveness of our proposed CEC\nmechanism, which improves accuracy by up to 12.0% in material classification\nand grasping prediction tasks.", "AI": {"tldr": "The paper presents ConViTac, a network for visual-tactile representation learning using a novel Contrastive Embedding Conditioning mechanism that enables better aligned and integrated features, leading to superior performance over existing methods in robotic perception and manipulation tasks.", "motivation": "The motivation of the paper is to improve the integration of visual and tactile information in robotic perception and manipulation tasks, as previous methods often rely on direct feature combination which results in poor integration.", "method": "ConViTac is a visual-tactile representation learning network that introduces a Contrastive Embedding Conditioning (CEC) mechanism to enhance feature alignment during fusion. The CEC uses a contrastive encoder that has been pretrained through self-supervised contrastive learning to project visual and tactile inputs into unified latent embeddings. Visual-tactile feature fusion is then performed through cross-modal attention on these embeddings.", "result": "Experiments show that ConViTac outperforms current state-of-the-art methods in real-world applications and the CEC mechanism improves accuracy by up to 12.0% in material classification and grasping prediction tasks.", "conclusion": "The conclusion is that ConViTac, with its CEC mechanism, is an effective approach for fusing visual and tactile information, leading to improved performance in downstream tasks, such as material classification and grasping prediction, compared to previous methods."}}
{"id": "2506.20917", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20917", "abs": "https://arxiv.org/abs/2506.20917", "authors": ["Zhengyan Shi"], "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective", "comment": "PhD Thesis", "summary": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet\nadapting them efficiently and robustly to specific tasks remains challenging.\nAs their scale and complexity grow, fine-tuning LMs on labelled data often\nunderutilizes available unlabelled data, leads to overfitting on small\ntask-specific sets, and imposes significant computational costs. These\nlimitations hamper their application to the open-ended landscape of real-world\nlanguage tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream\napplications. First, we explore strategies for extracting task-relevant\nknowledge from unlabelled data, introducing a novel continued pre-training\ntechnique that outperforms state-of-the-art semi-supervised approaches. Next,\nwe present a parameter-efficient fine-tuning method that substantially reduces\nmemory and compute costs while maintaining competitive performance. We also\nintroduce improved supervised fine-tuning methods that enable LMs to better\nfollow instructions, especially when labelled data is scarce, enhancing their\nperformance across a range of NLP tasks, including open-ended generation.\nFinally, we develop new evaluation methods and benchmarks, such as multi-hop\nspatial reasoning tasks, to assess LM capabilities and adaptation more\ncomprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results\ndemonstrate that these approaches substantially improve LM robustness,\nefficiency, and generalization, making them more adaptable to a broad range of\napplications. These advances mark a significant step towards more robust and\nefficient LMs, bringing us closer to the goal of artificial general\nintelligence.", "AI": {"tldr": "论文提出了一系列方法以改进语言模型对下游应用的适应性，包括使用未标记数据的持续预训练技术、参数高效的微调方法、改进的监督微调方法以及开发新的评估基准，这些方法显著提高了语言模型的鲁棒性、效率和泛化能力。", "motivation": "动机在于解决语言模型在特定任务中的有效和鲁棒性适应问题。随着模型规模和复杂性的增加，仅依赖标记数据的微调方法往往未能充分利用未标记数据，导致在小规模特定任务数据上过拟合，并且需要大量的计算资源。这些问题限制了语言模型在实际语言任务中的应用。", "method": "这是我们论文的方法部分摘要：该论文首先探讨了从未标记数据中提取任务相关知识的策略，并引入了一种新的持续预训练技术，该技术优于现有的半监督方法。其次，提出了一种参数高效的微调方法，该方法显著减少了内存和计算成本，同时保持了竞争力的性能。还介绍了改进的监督微调方法，使语言模型能够更好地遵循指令，尤其是在标记数据稀缺的情况下，提高了它们在各种NLP任务（包括开放式生成）上的性能。最后，开发了新的评估方法和基准（如多跳空间推理任务），以更全面地评估语言模型的能力和适应性。", "result": "实验结果显示，这些方法在各种NLP任务中显著提高了语言模型的鲁棒性、效率和泛化性能，使其能够更好地适应广泛的应用场景。", "conclusion": "这项研究标志着向更加健壮和高效的语言模型迈进了一大步，推动了人工智能迈向通用智能的目标。"}}
{"id": "2506.20786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20786", "abs": "https://arxiv.org/abs/2506.20786", "authors": ["Connor Ludwig", "Khashayar Namdar", "Farzad Khalvati"], "title": "AI-Driven MRI-based Brain Tumour Segmentation Benchmarking", "comment": null, "summary": "Medical image segmentation has greatly aided medical diagnosis, with U-Net\nbased architectures and nnU-Net providing state-of-the-art performance. There\nhave been numerous general promptable models and medical variations introduced\nin recent years, but there is currently a lack of evaluation and comparison of\nthese models across a variety of prompt qualities on a common medical dataset.\nThis research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM\n2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS\n2023 adult glioma and pediatrics dataset across multiple prompt qualities for\nboth points and bounding boxes. Several of these models exhibit promising Dice\nscores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,\nrespectively when given extremely accurate bounding box prompts which exceeds\nnnU-Net's segmentation performance. However, nnU-Net remains the dominant\nmedical image segmentation network due to the impracticality of providing\nhighly accurate prompts to the models. The model and prompt evaluation, as well\nas the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and\nSAM-Med-3D on the pediatrics dataset. The improvements in point prompt\nperformance after fine-tuning are substantial and show promise for future\ninvestigation, but are unable to achieve better segmentation than bounding\nboxes or nnU-Net.", "AI": {"tldr": "该研究评估了多种分割模型在BraTS 2023数据集上的表现，发现在给定高质量提示时，SAM和SAM 2的性能优于nnU-Net，但提供高准确度的提示不现实。", "motivation": "近年来，已经引入了许多通用的可提示模型和医疗领域的变化，但在共同的医疗数据集上跨各种提示质量对这些模型进行评估和比较存在不足。该研究旨在填补这一空白，并提供一些见解。", "method": "该研究使用了Segment Anything Model (SAM), Segment Anything Model 2 (SAM 2), MedSAM, SAM-Med-3D, 和 nnU-Net在BraTS 2023成人胶质瘤和儿科数据集上进行了零样本推理，评估了不同质量的提示（点和边界框）。此外，还对SAM、SAM 2、MedSAM和SAM-Med-3D进行了细调，以进一步提高儿科数据集上的点提示性能。", "result": "研究发现在给定高质量的边界框提示时，SAM和SAM 2在某些Dice系数上达到了0.894和0.893，超过了nnU-Net的分割性能。细化后，点提示的性能有了显著提升，但仍未超过边界框提示或nnU-Net的分割性能。", "conclusion": "nnU-Net依然是领先的医学图像分割网络，因为提给其他模型提供精确的提示不切实际。尽管如此，一些模型如SAM和SAM 2在给定高度准确提示的情况下表现出色。"}}
{"id": "2506.20920", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20920", "abs": "https://arxiv.org/abs/2506.20920", "authors": ["Guilherme Penedo", "Hynek Kydlíček", "Vinko Sabolčec", "Bettina Messmer", "Negar Foroutan", "Amir Hossein Kargaran", "Colin Raffel", "Martin Jaggi", "Leandro Von Werra", "Thomas Wolf"], "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language", "comment": null, "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases.", "AI": {"tldr": "本文提出了一种新的预训练数据集整理流水线，可以适应多种语言，并通过1000多种语言的数据集提升了多语言模型的性能。", "motivation": "训练高性能的多语言大规模语言模型仍然是一个挑战，尤其是在针对多种语言调整过滤和去重流水线时。为此，本文旨在创建一个能够适用于多种语言的预训练数据集整理方法，以解决这个问题并提高模型性能。", "method": "此论文提出了一种新的预训练数据集整理流水线，基于FineWeb，可以自动适应任何语言。此外，还提出了一种简单而原理性的方法来重新平衡数据集，考虑了复制数量和质量。", "result": "实验表明，该流水线创建的非英语语料库能够产生比先前数据集更优秀的模型。通过使用几乎100个Common Crawl快照，作者扩大了他们的流水线，使用超过1000种语言创建了新的20TB（50亿文档）的多语言数据集FineWeb2，并发布了他们的流水线，训练，以及评估代码库。", "conclusion": "通过上述方法，作者解决了多语言模型训练中遇到的挑战，成功地创建了一个适应多种语言的预训练数据集整理流水线，并验证了该方法的有效性。此外，还提出了一个简单且原理性的方法来重新平衡数据集，进一步提升了模型性能。"}}
{"id": "2506.20795", "categories": ["cs.CV", "cs.HC", "cs.RO", "I.2.10; I.2.9; I.5.4; I.4.8; I.4.9; H.1.2"], "pdf": "https://arxiv.org/pdf/2506.20795", "abs": "https://arxiv.org/abs/2506.20795", "authors": ["Stephanie Käs", "Anton Burenko", "Louis Markert", "Onur Alp Culha", "Dennis Mack", "Timm Linder", "Bastian Leibe"], "title": "How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?", "comment": null, "summary": "Gestures enable non-verbal human-robot communication, especially in noisy\nenvironments like agile production. Traditional deep learning-based gesture\nrecognition relies on task-specific architectures using images, videos, or\nskeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)\nand Vision Language Models (VLMs) with their strong generalization abilities\noffer potential to reduce system complexity by replacing dedicated\ntask-specific modules. This study investigates adapting such models for\ndynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art\nVFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing\nskeleton-based approach). We introduce NUGGET, a dataset tailored for\nhuman-robot communication in intralogistics environments, to evaluate the\ndifferent gesture recognition approaches. In our experiments, HD-GCN achieves\nbest performance, but V-JEPA comes close with a simple, task-specific\nclassification head - thus paving a possible way towards reducing system\ncomplexity, by using it as a shared multi-task model. In contrast, Gemini\nstruggles to differentiate gestures based solely on textual descriptions in the\nzero-shot setting, highlighting the need of further research on suitable input\nrepresentations for gestures.", "AI": {"tldr": "研究探索VFMs和VLMs在全身体势识别中的应用，特别是在人机通信背景下。实验结果显示HD-GCN性能最佳，但V-JEPA作为一个较为通用的模型也表现良好，暗示了一种可能的系统简化途径。", "motivation": "动机在于探索使用VFMs和VLMs简化大规模自动化生产环境中人机通信的复杂性，特别关注于基于通用模型而非特定任务模型的解决方案。", "method": "Gesture识别技术在人机交互中的应用研究，特别是在嘈杂的生产环境中。传统的基于深度学习的gesture识别依赖于特定的任务架构，输入包括图像、视频或骨骼姿态估计。论文调查了Vision Foundation Models (VFMs)和Vision Language Models (VLMs)应用于动态全身体势识别的可行性，并对比了V-JEPA（一种先进的VFM）、Gemini Flash 2.0（一种多模态VLM）和HD-GCN（一种顶级的基于骨架的方法）。", "result": "实验结果表明HD-GCN表现出最佳性能，但是通过简单的任务特定分类头，V-JEPA表现接近，体现了减少系统复杂性的可能性。相反，Gemini在基于文本描述的zero-shot设置中难以区分手势，说明需要进一步研究适合手势识别的输入表示方式。", "conclusion": "研究结果提供了一种可能的途径，通过使用V-JEPA作为通用多任务模型来简化系统复杂性，但也指出对于基于文本描述的零样本手势识别需要更多的研究与改进。"}}
{"id": "2506.20923", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20923", "abs": "https://arxiv.org/abs/2506.20923", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note:\n  substantial text overlap with arXiv:2501.01028", "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters.", "AI": {"tldr": "KaLM-Embedding-V2 is a compact and versatile text embedding model with enhanced performance through improved architecture and training techniques, achieving competitive results against much larger models while using less parameters.", "motivation": "The paper aims to introduce a compact and versatile embedding model, KaLM-Embedding-V2, with improved performance through architectural upgrades and a comprehensive training methodology.", "method": "Our model, KaLM-Embedding-V2, removes the causal attention mask and adopts a fully bidirectional transformer with mean-pooling to produce embeddings. It uses a multi-stage training with pre-training and fine-tuning, includes a focal-style reweighting mechanism and online hard-negative mixing, and leverages extensive data from over 20 categories for pre-training and 100 for fine-tuning.", "result": "Evaluations on MTEB in Chinese and English indicated significant outperformance compared to models of similar size and competitive performance with larger models, establishing a new standard for a versatile and compact embedding model.", "conclusion": "KaLM-Embedding-V2 sets a new bar for small-size embedding models, achieving performance competitive with much larger models while using less than 1B parameters."}}
{"id": "2506.20832", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20832", "abs": "https://arxiv.org/abs/2506.20832", "authors": ["Cansu Korkmaz", "Ahmet Murat Tekalp", "Zafer Dogan"], "title": "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models", "comment": "14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on\n  Circuits and Systems for Video Technology", "summary": "Super-resolution (SR) is an ill-posed inverse problem with many feasible\nsolutions consistent with a given low-resolution image. On one hand, regressive\nSR models aim to balance fidelity and perceptual quality to yield a single\nsolution, but this trade-off often introduces artifacts that create ambiguity\nin information-critical applications such as recognizing digits or letters. On\nthe other hand, diffusion models generate a diverse set of SR images, but\nselecting the most trustworthy solution from this set remains a challenge. This\npaper introduces a robust, automated framework for identifying the most\ntrustworthy SR sample from a diffusion-generated set by leveraging the semantic\nreasoning capabilities of vision-language models (VLMs). Specifically, VLMs\nsuch as BLIP-2, GPT-4o, and their variants are prompted with structured queries\nto assess semantic correctness, visual quality, and artifact presence. The\ntop-ranked SR candidates are then ensembled to yield a single trustworthy\noutput in a cost-effective manner. To rigorously assess the validity of\nVLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid\nmetric that quantifies SR reliability based on three complementary components:\nsemantic similarity via CLIP embeddings, structural integrity using SSIM on\nedge maps, and artifact sensitivity through multi-level wavelet decomposition.\nWe empirically show that TWS correlates strongly with human preference in both\nambiguous and natural images, and that VLM-guided selections consistently yield\nhigh TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail\nto reflect information fidelity, our approach offers a principled, scalable,\nand generalizable solution for navigating the uncertainty of the diffusion SR\nspace. By aligning outputs with human expectations and semantic correctness,\nthis work sets a new benchmark for trustworthiness in generative SR.", "AI": {"tldr": "文章提出了一种结合视觉语言模型（VLMs）评估超分辨率图像的方法，通过提出新的信任度得分（TWS），解决了现有超分辨率模型存在的问题。", "motivation": "解决现有超分辨率方法存在的问题，包括回归模型引入的伪影以及扩散模型中挑选最可信解决方案的挑战。", "method": "该论文提出了一种结合视觉语言模型（VLMs）评估超分辨率图像语义正确性、视觉质量和伪影存在的框架。特别是，通过BLIP-2、GPT-4o及其变体，使用结构化查询评估超分辨率图像。结合排名最高的超分辨率候选图像，以产生一个值得信赖的输出，同时保证成本效益。", "result": "提出了一种新的信任度得分（TWS），这是一个混合指标，可以基于语义相似度（通过CLIP嵌入）、结构完整性（通过边缘映射上的SSIM）和伪影敏感度（通过多层次小波分解）量化超分辨率的可靠性。在模糊图像和自然图像中，TWS与人类偏好高度相关。", "conclusion": "通过与人类期望和语义正确性对齐，本工作为生成性超分辨率设定了新的可信度基准，提供了一个原则化、可扩展且通用的方法来解决扩散SR空间的不确定性。"}}
{"id": "2506.20989", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20989", "abs": "https://arxiv.org/abs/2506.20989", "authors": ["Eric Zhang", "Leshem Choshen", "Jacob Andreas"], "title": "Can Gradient Descent Simulate Prompting?", "comment": "14 pages, 2 figures", "summary": "There are two primary ways of incorporating new information into a language\nmodel (LM): changing its prompt or changing its parameters, e.g. via\nfine-tuning. Parameter updates incur no long-term storage cost for model\nchanges. However, for many model updates, prompting is significantly more\neffective: prompted models can generalize robustly from single examples and\ndraw logical inferences that do not occur under standard fine-tuning. Can\nmodels be modified so that fine-tuning does emulate prompting? This paper\ndescribes a method for meta-training LMs such that gradient updates emulate the\neffects of conditioning on new information. Our approach uses tools from\ngradient-based meta-learning but uses an LM's own prompted predictions as\ntargets, eliminating the need for ground-truth labels. Subsequent gradient\ndescent training recovers some (and occasionally all) of prompted model\nperformance -- showing improvement on the ``reversal curse'' tasks, and\nanswering questions about text passages after a single gradient update. These\nresults suggest that, with appropriate initialization, gradient descent can be\nsurprisingly expressive. Our results suggest new avenues for long-context\nmodeling and offer insight into the generalization capabilities of\ngradient-based learning.", "AI": {"tldr": "本文提出了一种新方法，使得语言模型的微调能够模仿基于提示的做法，这为优化模型的泛化能力和长上下文处理提供了新的思路。", "motivation": "研究动机在于探索是否可以通过某种方式修改模型，使得微调能够模仿提示的效果。即改变模型参数（如通过微调）是否可以达到提示的效果。", "method": "研究提出了一种元训练方法，使语言模型的梯度更新能够模仿基于提示的效果。这种方法采用基于梯度的元学习工具，使用模型的提示预测作为目标，不需要真实标签。", "result": "实验结果显示，后续的梯度下降训练可以恢复部分（有时甚至是全部）提示模型的性能，包括在\"反转诅咒\"任务上的改进，以及在单次梯度更新后回答文本段落的问题。", "conclusion": "研究表明，适当的初始化下，梯度下降学习可以非常有表现力。此外，该研究为长上下文建模提供了新的途径，并提供了关于梯度学习泛化能力的洞见。"}}
{"id": "2506.20841", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20841", "abs": "https://arxiv.org/abs/2506.20841", "authors": ["Ha Min Son", "Shahbaz Rezaei", "Xin Liu"], "title": "FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization", "comment": null, "summary": "Semi-supervised domain generalization (SSDG) aims to solve the problem of\ngeneralizing to out-of-distribution data when only a few labels are available.\nDue to label scarcity, applying domain generalization methods often\nunderperform. Consequently, existing SSDG methods combine semi-supervised\nlearning methods with various regularization terms. However, these methods do\nnot explicitly regularize to learn domains invariant representations across all\ndomains, which is a key goal for domain generalization. To address this, we\nintroduce FixCLR. Inspired by success in self-supervised learning, we change\ntwo crucial components to adapt contrastive learning for explicit domain\ninvariance regularization: utilization of class information from pseudo-labels\nand using only a repelling term. FixCLR can also be added on top of most\nexisting SSDG and semi-supervised methods for complementary performance\nimprovements. Our research includes extensive experiments that have not been\npreviously explored in SSDG studies. These experiments include benchmarking\ndifferent improvements to semi-supervised methods, evaluating the performance\nof pretrained versus non-pretrained models, and testing on datasets with many\ndomains. Overall, FixCLR proves to be an effective SSDG method, especially when\ncombined with other semi-supervised methods.", "AI": {"tldr": "A new SSDG method, FixCLR, is introduced to improve domain generalization by explicitly regularizing for domain invariance using contrastive learning adapted with pseudo-labels and a repelling term.", "motivation": "To improve the generalization of SSDG methods to out-of-distribution data, especially when only a few labels are available, and to achieve explicit domain invariance regularization which is lacking in current methods.", "method": "Semi-supervised domain generalization (SSDG) aims to generalize to out-of-distribution data with scarce labels by combining semi-supervised learning and regularization, but fails to explicitly regularize for domain invariance. FixCLR addresses this by adapting contrastive learning with class information from pseudo-labels and using a repelling term only.", "result": "FixCLR demonstrates complementary performance improvements when added to existing SSDG or semi-supervised methods and shows effectiveness through benchmarking different improvements, evaluating pretrained versus non-pretrained models, and testing on multi-domain datasets.", "conclusion": "FixCLR effectively achieves explicit domain invariance regularization, proving to be an effective SSDG method, especially when combined with other semi-supervised methods."}}
{"id": "2506.20993", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20993", "abs": "https://arxiv.org/abs/2506.20993", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "comment": "Under review", "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.", "AI": {"tldr": "本文扩展了MPI模型，引入了16PF模型，开发了SAC框架，能够更精细地控制LLMs的人格特质，使LLM的行为更加具有一致性和可操控性。通过实验发现，多维度控制的人格特质强度能更好地模拟人类行为。", "motivation": "现有的LLM模型在展示人类性格方面存在两个主要的局限性：依赖于只提供粗宽性格维度的大五人格框架，并缺乏对特质强度的控制机制。因此，为了弥补这个缺口并提高LLM在模拟人类性格表达方面的灵活性和精确度，本文提出了新的方法和模型。", "method": "本研究将原有的Machine Personality Inventory (MPI)模型扩展，用16人格因素（16PF）模型取代了之前的大五人格框架，从而精确地控制了十六种不同的个性特质。并且提出了一个名为Specific Attribute Control (SAC)的结构化框架，用于评估和动态激发LLMs的人格特质强度。SAC方法通过基于形容词的语义锚定来指导人格特质强度的表达，并使用了五个强度因素：频率、深度、阈值、努力和意愿的行为问题来进行人格特质的操控。", "result": "实验表明，将人格强度建模为连续谱相比于二元特质切换，能够提供更为一致和可控的性格表达。此外，我们观察到目标特质强度的变化系统地影响了与之紧密相关的其他特质，这种影响在心理学意义上是连贯的，暗示LLMs内化了多维度的人格结构。", "conclusion": "这项工作开辟了新的途径，为控制人类与机器之间的交互打开了新机会，使LLMs更具有人类特性，这对于医疗、教育、面试等领域的应用有着重要意义。"}}
{"id": "2506.20850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20850", "abs": "https://arxiv.org/abs/2506.20850", "authors": ["Yuting He", "Shuo Li"], "title": "Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision", "comment": "Accepted by ICCV 2025", "summary": "Contrastive learning (CL) has become a cornerstone of self-supervised\npretraining (SSP) in foundation models, however, extending CL to pixel-wise\nrepresentation, crucial for medical vision, remains an open problem. Standard\nCL formulates SSP as a binary optimization problem (binary CL) where the\nexcessive pursuit of feature dispersion leads to an over-dispersion problem,\nbreaking pixel-wise feature correlation thus disrupting the intra-class\ndistribution. Our vector CL reformulates CL as a vector regression problem,\nenabling dispersion quantification in pixel-wise pretraining via modeling\nfeature distances in regressing displacement vectors. To implement this novel\nparadigm, we propose the COntrast in VEctor Regression (COVER) framework. COVER\nestablishes an extendable vector-based self-learning, enforces a consistent\noptimization flow from vector regression to distance modeling, and leverages a\nvector pyramid architecture for granularity adaptation, thus preserving\npixel-wise feature correlations in SSP. Extensive experiments across 8 tasks,\nspanning 2 dimensions and 4 modalities, show that COVER significantly improves\npixel-wise SSP, advancing generalizable medical visual foundation models.", "AI": {"tldr": "向量对比学习框架（COVER）用于改善像素级自监督预训练，通过向量回归优化像素级特征相关性，实验结果表明该方法在多个任务中效果显著，推动了医学视觉基础模型的发展。", "motivation": "标准的对比学习应用于像素级表示时，因过度追求特征弥散性而导致像素间特征相关性的破坏，影响了类内分布，因此提出新的对比学习方法解决该问题。", "method": "向量对比学习（VECTOR CL）框架，将对比学习问题重新定义为向量回归问题，通过位移向量的回归来量化特征距离。具体的实现框架为COntrast in VEctor Regression (COVER)，具备可扩展的向量基础自我学习，并采用向量金字塔结构。", "result": "论文摘要主要探讨了对比学习（CL）在基础模型中自监督预训练（SSP）中的应用及其在像素级表示上的扩展问题。针对标准的二元对比学习可能导致像素级特征相关性的破坏，作者提出了一种向量对比学习的方法，即将对比学习重新定义为向量回归问题，通过回归位移向量来量化特征距离，进而保持像素级特征相关性。为此，作者提出了COntrast in VEctor Regression (COVER)框架，该框架通过可扩展的向量基础自我学习，确保从向量回归到距离模型的一致优化过程，并使用向量金字塔结构进行粒度调整。实验结果显示，COVER框架在多个维度和模态下的多项任务中，显著提高了像素级自监督预训练的表现，有助于发展具有泛化能力的医学视觉基础模型。", "conclusion": "通过实验验证，COVER框架显著提高了像素级自监督预训练的表现，为开发具有泛化能力的医学视觉基础模型提供了新的思路。"}}
{"id": "2506.21031", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21031", "abs": "https://arxiv.org/abs/2506.21031", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Mohammad Adnan", "Sakshi Deo", "Ali Imam Abidi", "Keshav Gupta"], "title": "Large Language Models Acing Chartered Accountancy", "comment": "Accepted for publication at MoStart 2025: International Conference on\n  Digital Transformation in Education and Applications of Artificial\n  Intelligence, Bosnia and Herzegovina, 2025", "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation.", "AI": {"tldr": "本文通过CA-Ben基准测试评估了六个大型语言模型在印度财务背景下的性能，并发现这些模型在法律推理方面表现出色，但在数值计算和法律解释方面仍有改进空间。", "motivation": "本文旨在填补印度复杂金融背景下LLM评估领域的空白，探讨这些模型如何有效捕捉和应用特定领域的财务知识。", "method": "本文介绍了CA-Ben基准测试，该测试由基于印度注册会计师协会（ICAI）考试的结构化问答数据集组成，旨在评估LLM在财务、法律和定量推理方面的能力。评估了六个主要的LLM，包括使用标准化协议评估的GPT 4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet和Microsoft Phi 4。", "result": "结果表明，不同模型的性能存在差异，Claude 3.5 Sonnet和GPT-4o在概念和法律推理方面表现尤为突出。然而，在数值计算和法律解释方面仍面临挑战。", "conclusion": "研究结果强调了当前LLM的优势和局限性，建议未来改进方法应集中在混合推理和检索增强生成上，特别是对于定量分析和法律解释的准确性。"}}
{"id": "2506.20867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20867", "abs": "https://arxiv.org/abs/2506.20867", "authors": ["Ryosuke Kawamura", "Hideaki Hayashi", "Shunsuke Otake", "Noriko Takemura", "Hajime Nagahara"], "title": "Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation", "comment": null, "summary": "Dynamic facial expression recognition (DFER) is a task that estimates\nemotions from facial expression video sequences. For practical applications,\naccurately recognizing ambiguous facial expressions -- frequently encountered\nin in-the-wild data -- is essential. In this study, we propose MIDAS, a data\naugmentation method designed to enhance DFER performance for ambiguous facial\nexpression data using soft labels representing probabilities of multiple\nemotion classes. MIDAS augments training data by convexly combining pairs of\nvideo frames and their corresponding emotion class labels. This approach\nextends mixup to soft-labeled video data, offering a simple yet highly\neffective method for handling ambiguity in DFER. To evaluate MIDAS, we\nconducted experiments on both the DFEW dataset and FERV39k-Plus, a newly\nconstructed dataset that assigns soft labels to an existing DFER dataset. The\nresults demonstrate that models trained with MIDAS-augmented data achieve\nsuperior performance compared to the state-of-the-art method trained on the\noriginal dataset.", "AI": {"tldr": "研究提出了一种新的数据增强方法MIDAS，为模糊面部表情数据的动态面部表情识别提供了有效解决方案，实验结果表明，模型在MIDAS增强的数据上训练能获得比使用原始数据训练的先进方法更好的性能。", "motivation": "研究旨在解决实际应用中准确识别野外数据中经常遇到的模糊面部表情，这对于动态面部表情识别任务是至关重要的。", "method": "MIDAS是一种数据增强方法，通过使用多个情感类别的概率软标签来增强模糊面部表情数据的动态面部表情识别（DFER）性能。MIDAS通过凸组合视频帧及其对应的情感类别标签来增强训练数据，这种方法扩展了mixup方法到软标签视频数据，是一种处理DFER中的模棱两可问题的简单而有效的方法。", "result": "实验在DFEW数据集以及一个新的FERV39k-Plus数据集（为现有DFER数据集分配软标签）上进行。结果表明，使用MIDAS增强的数据训练的模型性能优于使用原始数据训练的最先进方法。", "conclusion": "实验结果证明，使用MIDAS增强的数据进行训练的模型在DFER任务上达到了优越的性能，优于在原始数据上训练的最先进方法。"}}
{"id": "2506.21049", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21049", "abs": "https://arxiv.org/abs/2506.21049", "authors": ["Chunyuan Yuan", "Chong Zhang", "Zheng Fang", "Ming Pang", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Ching Law"], "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification", "comment": "Accepted by ACL 2025", "summary": "Query classification, including multiple subtasks such as intent and category\nprediction, is vital to e-commerce applications. E-commerce queries are usually\nshort and lack context, and the information between labels cannot be used,\nresulting in insufficient prior information for modeling. Most existing\nindustrial query classification methods rely on users' posterior click behavior\nto construct training samples, resulting in a Matthew vicious cycle.\nFurthermore, the subtasks of query classification lack a unified framework,\nleading to low efficiency for algorithm optimization.\n  In this paper, we propose a novel Semi-supervised Scalable Unified Framework\n(SSUF), containing multiple enhanced modules to unify the query classification\ntasks. The knowledge-enhanced module uses world knowledge to enhance query\nrepresentations and solve the problem of insufficient query information. The\nlabel-enhanced module uses label semantics and semi-supervised signals to\nreduce the dependence on posterior labels. The structure-enhanced module\nenhances the label representation based on the complex label relations. Each\nmodule is highly pluggable, and input features can be added or removed as\nneeded according to each subtask. We conduct extensive offline and online A/B\nexperiments, and the results show that SSUF significantly outperforms the\nstate-of-the-art models.", "AI": {"tldr": "本文提出了一种新的半监督可扩展统一框架(SSUF)，用以解决电子商务查询分类中的信息不足和标签依赖问题，并通过实验验证了其优越性。", "motivation": "查询分类，包括意图和类别预测等多个子任务，对电子商务应用至关重要。电子商务查询通常较短且缺乏上下文，导致标签之间的信息无法被充分利用，从而造成建模时的先验信息不足。现有的大多数工业查询分类方法依赖于用户的后点击行为来构建训练样本，这导致了一个马太效应的恶性循环。此外，查询分类的子任务缺乏统一框架，使得算法优化效率低下。", "method": "我们提出了一种新的半监督可扩展统一框架(SSUF)，该框架包含多个增强模块，用以统一查询分类任务。知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号减少对后标签的依赖。结构增强模块基于复杂的标签关系增强标签表示。每个模块的输入特征可根据每个子任务的需要灵活添加或移除。", "result": "我们进行了广泛的离线和在线A/B实验，结果显示，与当前的先进模型相比，SSUF模型具有显著的优势。", "conclusion": "通过广泛的离线和在线A/B实验，结果表明SSUF模型显著优于当前最先进的模型。"}}
{"id": "2506.20877", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.20877", "abs": "https://arxiv.org/abs/2506.20877", "authors": ["Calin Teodor Ioan"], "title": "THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion", "comment": null, "summary": "Monocular depth estimation methods traditionally train deep models to infer\ndepth directly from RGB pixels. This implicit learning often overlooks explicit\nmonocular cues that the human visual system relies on, such as occlusion\nboundaries, shading, and perspective. Rather than expecting a network to\ndiscover these cues unaided, we present ThirdEye, a cue-aware pipeline that\ndeliberately supplies each cue through specialised, pre-trained, and frozen\nnetworks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)\nequipped with a key-value working-memory module that weights them by\nreliability. An adaptive-bins transformer head then produces a high-resolution\ndisparity map. Because the cue experts are frozen, ThirdEye inherits large\namounts of external supervision while requiring only modest fine-tuning. This\nextended version provides additional architectural detail, neuroscientific\nmotivation, and an expanded experimental protocol; quantitative results will\nappear in a future revision.", "AI": {"tldr": "ThirdEye提出了一种新的单目深度估计方法，明确地提供和结合单目线索，以提高深度估计的准确性。", "motivation": "传统的单目深度估计方法通常是隐式学习，忽视了人类视觉系统依赖的明确的单目线索。ThirdEye方法希望通过提供专门的线索网络来改进这一点。", "method": "ThirdEye方法通过专门的、预训练的和冻结的网络来提供明确的单目线索，如遮挡边界、阴影和透视。这些线索在一个三阶段皮层层次（V1->V2->V3）中融合，并配备了一个根据可靠性加权这些线索的关键值工作记忆模块。最后，一个自适应bins的transformer头部生成高分辨率的视差图。", "result": "由于线索专家网络是冻结的，ThirdEye可以继承大量外部监督，只需适度微调。实验细节和效果将在未来修订中提供。", "conclusion": "通过集成明确的视觉线索，ThirdEye旨在提高单目深度估计性能，该方法利用了预先训练的专家网络，并在皮层层次结构中融合这些线索。"}}
{"id": "2506.21053", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21053", "abs": "https://arxiv.org/abs/2506.21053", "authors": ["Fuqiang Niu", "Genan Dai", "Yisha Lu", "Jiayu Liao", "Xiang Li", "Hu Huang", "Bowen Zhang"], "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection", "comment": null, "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.", "AI": {"tldr": "本文介绍了MT2-CSD，一个用于多目标多轮对话立场检测的大型数据集，并提出了Large Language Model增强的对话关系注意力网络（LLM-CRAN），实验表明LLM-CRAN在MT2-CSD数据集上的对话立场检测任务中显著优于基线模型。", "motivation": "传统立场检测方法主要针对单个实例进行，限制了其处理真实社交环境中多方面对话的能力，因此本文的动机是开发针对多目标多轮对话立场检测的更复杂和准确的模型。", "method": "提出了Large Language Model增强的对话关系注意力网络（LLM-CRAN），利用LLM的逻辑推理能力来提高对话理解能力。", "result": "实验结果显示，LLM-CRAN在多目标多轮对话立场检测任务上优于当前的几个基线模型。", "conclusion": "本文提出的方法LLM-CRAN在新发布的MT2-CSD数据集上的测试证明了其在多目标多轮对话立场检测上取得了显著的性能提升。"}}
