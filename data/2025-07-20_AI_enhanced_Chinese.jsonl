{"id": "2507.12547", "categories": ["cs.CL", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.12547", "abs": "https://arxiv.org/abs/2507.12547", "authors": ["Lionel Wong", "Katherine M. Collins", "Lance Ying", "Cedegao E. Zhang", "Adrian Weller", "Tobias Gersternberg", "Timothy O'Donnell", "Alexander K. Lew", "Jacob D. Andreas", "Joshua B. Tenenbaum", "Tyler Brooke-Wilson"], "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models", "comment": "Presented at CogSci 2025", "summary": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.", "AI": {"tldr": "提出一种结合分布式和符号表征的MSA架构，用于处理开放性推理任务，展示了优于传统语言模型的方法。", "motivation": "探讨人们如何在面对新情境时，综合广泛的背景知识和相关信息并进行连贯的推理。", "method": "作者提出了一个名为`模型合成架构`（MSA）的计算实现方法，该方法使用语言模型来实现全局相关性检索和模型合成，并使用概率程序来实现定制的、连贯的世界模型。", "result": "在Model Olympics数据集上，MSA方法在直接生成和链式思考生成模式下，都比仅使用语言模型的基准方法更好地捕捉了人类的判断。", "conclusion": "MSA可以以模仿人类处理全局相关变量能力的方式实现，为理解和复制人类在开放性领域中的推理提供了路径。"}}
{"id": "2507.12553", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12553", "abs": "https://arxiv.org/abs/2507.12553", "authors": ["Michael A. Lepori", "Jennifer Hu", "Ishita Dasgupta", "Roma Patel", "Thomas Serre", "Ellie Pavlick"], "title": "Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility", "comment": null, "summary": "Language models (LMs) are used for a diverse range of tasks, from question\nanswering to writing fantastical stories. In order to reliably accomplish these\ntasks, LMs must be able to discern the modal category of a sentence (i.e.,\nwhether it describes something that is possible, impossible, completely\nnonsensical, etc.). However, recent studies have called into question the\nability of LMs to categorize sentences according to modality (Michaelov et al.,\n2025; Kauf et al., 2023). In this work, we identify linear representations that\ndiscriminate between modal categories within a variety of LMs, or modal\ndifference vectors. Analysis of modal difference vectors reveals that LMs have\naccess to more reliable modal categorization judgments than previously\nreported. Furthermore, we find that modal difference vectors emerge in a\nconsistent order as models become more competent (i.e., through training steps,\nlayers, and parameter count). Notably, we find that modal difference vectors\nidentified within LM activations can be used to model fine-grained human\ncategorization behavior. This potentially provides a novel view into how human\nparticipants distinguish between modal categories, which we explore by\ncorrelating projections along modal difference vectors with human participants'\nratings of interpretable features. In summary, we derive new insights into LM\nmodal categorization using techniques from mechanistic interpretability, with\nthe potential to inform our understanding of modal categorization in humans.", "AI": {"tldr": "研究中通过识别模态差向量揭示了语言模型在模态分类中比以往认为的更为可靠的能力，并且这种向量的分析为理解人类的模态分类行为提供新的视角。", "motivation": "探讨语言模型在模态分类中的表现，纠正此前关于语言模型无法可靠地进行模态分类的说法。", "method": "通过分析线性表示来区分语言模型中的模态类别，这些线性表示被称为模态差向量。", "result": "发现模态差向量可以提供比之前研究更可靠的模态分类判断，并且随着模型变得更加专业，这些矢量在一致性顺序中出现。", "conclusion": "模态差向量不仅有助于理解语言模型的模态分类能力，还能帮助解释人类如何区分模态类别。"}}
{"id": "2507.12672", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12672", "abs": "https://arxiv.org/abs/2507.12672", "authors": ["Abu-Viskhan A. Umishov", "Vladislav A. Grigorian"], "title": "The first open machine translation system for the Chechen language", "comment": "7 pages", "summary": "We introduce the first open-source model for translation between the\nvulnerable Chechen language and Russian, and the dataset collected to train and\nevaluate it. We explore fine-tuning capabilities for including a new language\ninto a large language model system for multilingual translation NLLB-200. The\nBLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for\ntranslation from Russian to Chechen and reverse direction, respectively. The\nrelease of the translation models is accompanied by the distribution of\nparallel words, phrases and sentences corpora and multilingual sentence encoder\nadapted to the Chechen language.", "AI": {"tldr": "本文介绍了首个多语言翻译开源模型，专门用于车臣语与俄语之间的翻译，同时也评估了在NLLB-200模型上进行新语言微调的能力。", "motivation": "本研究旨在为脆弱的车臣语言提供开源翻译模型和数据集，同时也探索在已有的大型语言模型上进行新语言微调的方法。", "method": "我们介绍了首个用于车臣语与俄语之间翻译的开源模型及所收集的训练和评估数据集。我们探讨了将新语言纳入大型语言模型系统NLLB-200进行多语言翻译微调的能力。", "result": "评测结果显示，模型的BLEU和ChrF++得分分别为8.34/34.69（俄语到车臣语）和20.89/44.55（车臣语到俄语）。", "conclusion": "此次发布不仅包括了翻译模型，还包括了平行词汇、短语和句子语料库，以及适应车臣语的多语言句子编码器。"}}
{"id": "2507.12679", "categories": ["cs.CL", "q-bio.QM", "I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2507.12679", "abs": "https://arxiv.org/abs/2507.12679", "authors": ["Arthur J. Funnell", "Panayiotis Petousis", "Fabrice Harel-Canada", "Ruby Romero", "Alex A. T. Bui", "Adam Koncsol", "Hritika Chaturvedi", "Chelsea Shover", "David Goodman-Meza"], "title": "Improving Drug Identification in Overdose Death Surveillance using Large Language Models", "comment": "30 pages, 1 figure, 4 tables, 2 supplemental figures, 4 supplemental\n  tables, submitted to Journal of Forensic Sciences (JFS)", "summary": "The rising rate of drug-related deaths in the United States, largely driven\nby fentanyl, requires timely and accurate surveillance. However, critical\noverdose data are often buried in free-text coroner reports, leading to delays\nand information loss when coded into ICD (International Classification of\nDisease)-10 classifications. Natural language processing (NLP) models may\nautomate and enhance overdose surveillance, but prior applications have been\nlimited. A dataset of 35,433 death records from multiple U.S. jurisdictions in\n2020 was used for model training and internal testing. External validation was\nconducted using a novel separate dataset of 3,335 records from 2023-2024.\nMultiple NLP approaches were evaluated for classifying specific drug\ninvolvement from unstructured death certificate text. These included\ntraditional single- and multi-label classifiers, as well as fine-tuned\nencoder-only language models such as Bidirectional Encoder Representations from\nTransformers (BERT) and BioClinicalBERT, and contemporary decoder-only large\nlanguage models such as Qwen 3 and Llama 3. Model performance was assessed\nusing macro-averaged F1 scores, and 95% confidence intervals were calculated to\nquantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect\nperformance, with macro F1 scores >=0.998 on the internal test set. External\nvalidation confirmed robustness (macro F1=0.966), outperforming conventional\nmachine learning, general-domain BERT models, and various decoder-only large\nlanguage models. NLP models, particularly fine-tuned clinical variants like\nBioClinicalBERT, offer a highly accurate and scalable solution for overdose\ndeath classification from free-text reports. These methods can significantly\naccelerate surveillance workflows, overcoming the limitations of manual ICD-10\ncoding and supporting near real-time detection of emerging substance use\ntrends.", "AI": {"tldr": "研究评估了多种NLP模型对药物相关死亡证书文本的分类能力，发现微调后的BioClinicalBERT表现最佳，提供了一种高精度的过量服药死亡分类方法。", "motivation": "提升与药物有关的死亡事件监测的及时性和准确性，解决因为手动将数据编码到ICD-10分类而导致的延迟和信息丢失问题。", "method": "使用了多种自然语言处理方法来分类特定药物涉及情况，包括传统的单标签和多标签分类器，以及微调后的BioClinicalBERT和Qwen 3等大语言模型。", "result": "微调后的BioClinicalBERT模型在内部测试集上的宏观F1分数达到0.998，在外部验证集上也达到了0.966的宏观F1分数。", "conclusion": "NLP模型，尤其是微调后的BioClinicalBERT，能提供一种精准且可扩展的方案，用于分类药物过量死亡，从而加速公共卫生监测流程。"}}
{"id": "2507.12490", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12490", "abs": "https://arxiv.org/abs/2507.12490", "authors": ["Maximiliano Hormazábal Lagos", "Héctor Cerezo-Costas", "Dimosthenis Karatzas"], "title": "Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering", "comment": "This work has been accepted for presentation at the 16th Conference\n  and Labs of the Evaluation Forum (CLEF 2025) and will be published in the\n  proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available", "summary": "We introduce EaGERS, a fully training-free and model-agnostic pipeline that\n(1) generates natural language rationales via a vision language model, (2)\ngrounds these rationales to spatial sub-regions by computing multimodal\nembedding similarities over a configurable grid with majority voting, and (3)\nrestricts the generation of responses only from the relevant regions selected\nin the masked image. Experiments on the DocVQA dataset demonstrate that our\nbest configuration not only outperforms the base model on exact match accuracy\nand Average Normalized Levenshtein Similarity metrics but also enhances\ntransparency and reproducibility in DocVQA without additional model\nfine-tuning.", "AI": {"tldr": "本文提出了一种无需训练的方法EaGERS，来在没有任何额外训练的情况下增强DocVQA任务的表现，并提供更高透明度和可重复性。", "motivation": "作者旨在开发一个不需要训练且能够增加解释性的方法，用于从文档图像中提取信息的任务，即DocVQA任务。", "method": "本文提出了一种名为EaGERS的全训练免费且模型无关的框架，该框架通过视觉语言模型生成自然语言解释，通过计算多模态嵌入相似度并采用多数投票方法将这些解释定位到空间子区域，并仅从被选中的遮罩图像区域生成响应。", "result": "实验结果表明，该方法在DocVQA数据集上不仅提高了基础模型在精确匹配准确率和平均归一化Levenshtein相似度指标上的表现，还提高了DocVQA任务的透明性和可重复性，并且无需额外的模型微调。", "conclusion": "通过在DocVQA基准上对比性能，本文证明了EaGERS在提高任务准确性和增强方法透明度方面的有效性，而不需要额外的训练步骤。"}}
{"id": "2507.12695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12695", "abs": "https://arxiv.org/abs/2507.12695", "authors": ["S M Rafiuddin", "Sadia Kamal", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen"], "title": "AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis", "comment": "12 pages (including references), 2 figures (Fig. 1 overview, Fig. 2\n  hyperparameter sensitivity with two subplots), 6 tables (performance,\n  ablation, dataset stats, case studies, etc.), accepted at ASONAM 2025 (Social\n  Network Analysis and Mining)", "summary": "We introduce AdaptiSent, a new framework for Multimodal Aspect-Based\nSentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms\nto improve sentiment classification and aspect term extraction from both text\nand images. Our model integrates dynamic modality weighting and\ncontext-adaptive attention, enhancing the extraction of sentiment and\naspect-related information by focusing on how textual cues and visual context\ninteract. We tested our approach against several baselines, including\ntraditional text-based models and other multimodal methods. Results from\nstandard Twitter datasets show that AdaptiSent surpasses existing models in\nprecision, recall, and F1 score, and is particularly effective in identifying\nnuanced inter-modal relationships that are crucial for accurate sentiment and\naspect term extraction. This effectiveness comes from the model's ability to\nadjust its focus dynamically based on the context's relevance, improving the\ndepth and accuracy of sentiment analysis across various multimodal data sets.\nAdaptiSent sets a new standard for MABSA, significantly outperforming current\nmethods, especially in understanding complex multimodal information.", "AI": {"tldr": "AdaptiSent是一个用于多模态基于方面的意见分析的新框架，它利用自适应的跨模态注意力机制来提高从文本和图像中提取情感分类和方面词的能力。实验证明，AdaptiSent超越了现有方法，在精确度、召回率和F1分数方面有显著提高。", "motivation": "目标是解决多模态情感分析中的挑战，提升在处理复杂多模态信息时的理解和分析能力，特别是要提高情感分类和方面词提取的精度。", "method": "AdaptiSent通过集成动态模态权重和上下文自适应注意力机制，专注于文本线索和视觉上下文之间的相互作用来强化情感和方面信息的提取。", "result": "在标准的Twitter数据集上测试，AdaptiSent在多个指标上表现优于传统文本模型和其他多模态方法，尤其在识别微妙的跨模态关系上特别有效。", "conclusion": "AdaptiSent由于其动态调整注意力焦点的能力，在多种多模态数据集上的深度和准确性分析中取得了显著效果，为基于多模态的意见分析设定了新的标准。"}}
{"id": "2507.12508", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12508", "abs": "https://arxiv.org/abs/2507.12508", "authors": ["Yuncong Yang", "Jiageng Liu", "Zheyuan Zhang", "Siyuan Zhou", "Reuben Tan", "Jianwei Yang", "Yilun Du", "Chuang Gan"], "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning", "comment": "Project Page: https://umass-embodied-agi.github.io/MindJourney", "summary": "Spatial reasoning in 3D space is central to human cognition and indispensable\nfor embodied tasks such as navigation and manipulation. However,\nstate-of-the-art vision-language models (VLMs) struggle frequently with tasks\nas simple as anticipating how a scene will look after an egocentric motion:\nthey perceive 2D images but lack an internal model of 3D dynamics. We therefore\npropose MindJourney, a test-time scaling framework that grants a VLM with this\nmissing capability by coupling it to a controllable world model based on video\ndiffusion. The VLM iteratively sketches a concise camera trajectory, while the\nworld model synthesizes the corresponding view at each step. The VLM then\nreasons over this multi-view evidence gathered during the interactive\nexploration. Without any fine-tuning, our MindJourney achieves over an average\n8% performance boost on the representative spatial reasoning benchmark SAT,\nshowing that pairing VLMs with world models for test-time scaling offers a\nsimple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also\nimproves upon the test-time inference VLMs trained through reinforcement\nlearning, which demonstrates the potential of our method that utilizes world\nmodels for test-time scaling.", "AI": {"tldr": "MindJourney couples a vision-language model with a controllable world model to enhance 3D spatial reasoning, achieving significant performance improvements on the SAT benchmark without fine-tuning.", "motivation": "The motivation behind MindJourney is the lack of 3D spatial understanding in VLMs, which limits their performance in embodied tasks like navigation and manipulation. This innovation aims to augment VLMs with a capability for understanding 3D dynamics, thus improving their performance in spatial tasks.", "method": "Spatial reasoning in 3D space is often a challenge for vision-language models (VLMs) due to their 2D perception. The paper introduces MindJourney, a test-time scaling framework. This framework couples a VLM with a controllable world model based on video diffusion. The VLM sketches a camera trajectory and the world model generates corresponding views at each step, enabling the VLM to reason over multi-view evidence.", "result": "Without fine-tuning, MindJourney achieves a significant performance boost of over 8% average on the SAT spatial reasoning benchmark. It also outperforms test-time inference methods in VLMs trained through reinforcement learning, highlighting the robustness of 3D reasoning through test-time scaling.", "conclusion": "The paper concludes that combining VLMs with world models via test-time scaling can significantly improve performance in 3D reasoning tasks, showcasing the potential of this approach for enhancing VLMs without the need for additional training."}}
{"id": "2507.12705", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12705", "abs": "https://arxiv.org/abs/2507.12705", "authors": ["Potsawee Manakul", "Woody Haosheng Gan", "Michael J. Ryan", "Ali Sartaz Khan", "Warit Sirichotedumrong", "Kunat Pipatanakul", "William Held", "Diyi Yang"], "title": "AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation", "comment": null, "summary": "Current speech evaluation suffers from two critical limitations: the need and\ndifficulty of designing specialized systems targeting individual audio\ncharacteristics, and poor correlation between automatic evaluation methods and\nhuman preferences. This work presents a systematic study of Large Audio Model\n(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified\nevaluation framework that addresses both challenges. We systematically explore\nAudioJudge across audio characteristic detection tasks, including\npronunciation, speaking rate, speaker identification and speech quality, and\nsystem-level human preference simulation for automated benchmarking. We\ninvestigate different prompt engineering strategies, finding that audio\nconcatenation combined with in-context learning significantly improves\nperformance across both audio characteristic detection and human preference\nsimulation tasks. We further introduce a multi-aspect ensemble AudioJudge to\nenable general-purpose multi-aspect audio evaluation. This method decomposes\nspeech assessment into specialized judges for lexical content, speech quality,\nand paralinguistic features, achieving up to 0.91 Spearman correlation with\nhuman preferences on our system ranking benchmark. Robustness analysis reveals\nthat while LAMs maintain strong performance under acoustic noise, they exhibit\nsignificant verbosity and positional biases that require careful mitigation.", "AI": {"tldr": "本文探讨了一种统一的评估框架—AudioJudge，利用大型音频模型来提升音频特征检测和人类偏好模拟任务的性能，同时提出解决LAM的冗余和位置偏倚问题的方法。", "motivation": "研究的动机在于解决当前语音评估中两个关键问题：设计专门针对个体音频特征的系统的需要和难度，以及自动评估方法与人类偏好之间的差强人意的相关性。", "method": "研究采用了不同的提示工程策略，发现音频连接结合上下文学习可以显著提升音频特征检测和人类偏好模拟任务的表现。同时还介绍了将语音评估分解为专门针对词汇内容、语音质量和超音段特征的评估者的集成方法。", "result": "研究探索了AudioJudge在包括发音、说话速率、说话人识别和语音质量在内的多个音频特征检测任务，并通过系统级的人类偏好模拟进行自动化基准化测试。", "conclusion": "通过引入多方面集成的AudioJudge，该研究实现了高达0.91的斯皮尔曼相关性，与人类偏好在系统排名基准测试中的匹配度显著提高。尽管LAM在存在噪声条件下性能稳健，但需要解决冗余和位置偏倚的问题。"}}
{"id": "2507.12566", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12566", "abs": "https://arxiv.org/abs/2507.12566", "authors": ["Gen Luo", "Wenhan Dou", "Wenhao Li", "Zhaokai Wang", "Xue Yang", "Changyao Tian", "Hao Li", "Weiyun Wang", "Wenhai Wang", "Xizhou Zhu", "Yu Qiao", "Jifeng Dai"], "title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models", "comment": null, "summary": "This paper focuses on monolithic Multimodal Large Language Models (MLLMs),\nwhich integrate visual encoding and language decoding into a single model.\nExisting structures and pre-training strategies for monolithic MLLMs often\nsuffer from unstable optimization and catastrophic forgetting. To address these\nchallenges, our key idea is to embed a new visual parameter space into a\npre-trained LLM, enabling stable learning of visual knowledge from noisy data\nvia delta tuning. Based on this principle, we first introduce Mono-InternVL, an\nadvanced monolithic MLLM that incorporates a set of visual experts through a\nmultimodal mixture-of-experts architecture. In addition, we design an\ninnovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize\nits visual capabilities via progressive learning. Mono-InternVL achieves\ncompetitive performance against existing MLLMs but also leads to relatively\nexpensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper\nand stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++\nintroduces additional visual attention experts to Mono-InternVL-1.5 and\nre-organizes the pre-training process in an efficient manner. During inference,\nit includes a fused CUDA kernel to speed up its MoE operations. With these\ndesigns, Mono-InternVL-1.5 significantly reduces training and inference costs,\nwhile still maintaining competitive performance with Mono-InternVL. To evaluate\nour approach, we conduct extensive experiments across 15 benchmarks. Results\ndemonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out\nof 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared\nto its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves\nsimilar multimodal performance while reducing first-token latency by up to 69%.\nCode and models are released at https://github.com/OpenGVLab/Mono-InternVL.", "AI": {"tldr": "本文提出了一种处理单体多模态大语言模型优化不稳定问题的方法，并通过其改进版本实现了更低的数据成本和推理延迟。", "motivation": "本文旨在解决现有单体多模态大语言模型优化不稳定和灾难性遗忘的问题，同时降低数据成本，以实现更好的视觉能力和更高的运行效率。", "method": "本文提出了一种名为Mono-InternVL的单体多模态大语言模型，该模型在现有的大型语言模型基础上嵌入了新的视觉参数空间，通过增量调优学习来自嘈杂数据的视觉知识。为了进一步提高模型性能并降低成本，还引入了Mono-InternVL-1.5，它通过改进的内生视觉预训练（EViP++）和优化的推理过程实现了这些目标。", "result": "实验结果表明，Mono-InternVL在15个基准测试中的12个上优于现有的单体多模态大语言模型，例如在OCR-BENCH上相对Emu3提高了114个点的性能。与模块化对应版本相比，Mono-InternVL-1.5实现了相似的多模态性能，同时将首次响应延迟降低了69%。", "conclusion": "本文提出的方法Mono-InternVL及其改进版本Mono-InternVL-1.5在实现高效视觉知识学习的同时，显著降低了数据成本和推理延迟，并在多个基准测试中展示了竞争性能。"}}
{"id": "2507.12720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12720", "abs": "https://arxiv.org/abs/2507.12720", "authors": ["Abraham Toluase Owodunni", "Orevaoghene Ahia", "Sachin Kumar"], "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models", "comment": null, "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10\\% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens", "AI": {"tldr": "研究开发了一种更为灵活的语言模型FLEXITOKENS，实验证明其能够在多种任务上避免分词过度碎片化，并提升总体性能。", "motivation": "现有的语言模型通过简单的微调难以适应新的数据分布，问题主要在于其子词分词器的僵化，无法学习新的语言或脚本的分词方式。", "method": "开发了一种名为FLEXITOKENS的方法，利用预测输入字节序列边界的小模块，将输入编码成可变长度的片段，相比现有方法更为灵活。", "result": "通过开发可学习的字节级语言模型及其自适应分词器，实现了在多种任务和语料库下的性能提升，有效减少分词碎片化问题，相比传统子词分词器，性能提升可达10%。", "conclusion": "提出了一种名为FLEXITOKENS的训练目标，能够有效提升语言模型的适应性和性能，改进了传统方法中分词器固定的限制。"}}
{"id": "2507.12590", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12590", "abs": "https://arxiv.org/abs/2507.12590", "authors": ["Judy Long", "Tao Liu", "Sean Alexander Woznicki", "Miljana Marković", "Oskar Marko", "Molly Sears"], "title": "Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows", "comment": "A review article. 41 pages, 22 figures. Preprint", "summary": "Crop mapping involves identifying and classifying crop types using spatial\ndata, primarily derived from remote sensing imagery. This study presents the\nfirst comprehensive review of large-scale, pixel-wise crop mapping workflows,\nencompassing both conventional supervised methods and emerging transfer\nlearning approaches. To identify the optimal supervised crop mapping workflows,\nwe conducted systematic experiments, comparing six widely adopted satellite\nimage-based preprocessing methods, alongside eleven supervised pixel-wise\nclassification models. Additionally, we assessed the synergistic impact of\nvaried training sample sizes and variable combinations. Moreover, we identified\noptimal transfer learning techniques for different magnitudes of domain shift.\nThe evaluation of best methods was conducted across five diverse agricultural\nsites. Landsat 8 served as the primary satellite data source. Labels come from\nCDL trusted pixels and field surveys.\n  Our findings reveal three key insights. First, fine-scale interval\npreprocessing paired with Transformer models consistently delivered optimal\nperformance for both supervised and transferable workflows. RF offered rapid\ntraining and competitive performance in conventional supervised learning and\ndirect transfer to similar domains. Second, transfer learning techniques\nenhanced workflow adaptability, with UDA being effective for homogeneous crop\nclasses while fine-tuning remains robust across diverse scenarios. Finally,\nworkflow choice depends heavily on the availability of labeled samples. With a\nsufficient sample size, supervised training typically delivers more accurate\nand generalizable results. Below a certain threshold, transfer learning that\nmatches the level of domain shift is a viable alternative to achieve crop\nmapping. Repository:\nBest-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows", "AI": {"tldr": "该研究对大规模像素级农作物映射工作流程进行了全面回顾，涵盖了传统监督方法和新兴的迁移学习方法。通过系统实验，研究确定了最佳的预处理方法及分类模型，并展示了迁移学习技术在不同领域转移情况下的有效性。", "motivation": "推动大规模像素级农作物映射技术的进步，以便更准确地识别和分类作物类型。", "method": "系统地比较了六种卫星图像预处理方法以及十一种监督像素分类模型，还评估了不同训练样本尺寸和变量组合的影响。", "result": "精细间隔预处理及Transformer模型在监督和迁移工作流程中表现最佳。随机森林在传统监督学习和直接迁移相似域中表现出快速训练和竞争力。迁移学习显著提高了流程适应性。", "conclusion": "监督训练在有足够的标签样本时通常结果更准确和泛化能力强；样本数量不足时，匹配领域转移级别的迁移学习是一个可行的替代方案。"}}
{"id": "2507.12724", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12724", "abs": "https://arxiv.org/abs/2507.12724", "authors": ["Richard Sproat", "Tianyu Zhao", "Llion Jones"], "title": "TransEvalnia: Reasoning-based Evaluation and Ranking of Translations", "comment": null, "summary": "We present TransEvalnia, a prompting-based translation evaluation and ranking\nsystem that uses reasoning in performing its evaluations and ranking. This\nsystem presents fine-grained evaluations based on a subset of the\nMultidimensional Quality Metrics (https://themqm.org/), returns an assessment\nof which translation it deems the best, and provides numerical scores for the\nvarious dimensions and for the overall translation. We show that TransEvalnia\nperforms as well as or better than the state-of-the-art MT-Ranker (Moosa et al.\n2024) on our own English-Japanese data as well as several language pairs from\nvarious WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and\nQwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations\nreturned are deemed highly acceptable to human raters, and that the scores\nassigned to the translations by Sonnet, as well as other LLMs, correlate well\nwith scores assigned by the human raters. We also note the sensitivity of our\nsystem -- as well as MT-Ranker -- to the order in which the translations are\npresented, and we propose methods to address this position bias. All data,\nincluding the system's evaluation and reasoning, human assessments, as well as\ncode is released.", "AI": {"tldr": "TransEvalnia改进了翻译评估技术，优于或至少达到了当前最佳系统MT-Ranker的水平，并解决了位置偏见问题。", "motivation": "为了提供一个精细的翻译评估工具，可以与现有的最佳系统MT-Ranker进行竞争或超过其性能，并证明其评估结果能够被人类评分者高度接受。", "method": "基于提示的翻译评估和排名系统（TransEvalnia），该系统通过推理来进行评估和排名，使用了Multidimensional Quality Metrics的子集来进行评估，并返回其认为最佳翻译的评估及各个维度和整体翻译的数值分数。", "result": "TransEvalnia在英语-日语数据以及来自WMT共享任务的几种语言对上，表现优于或至少与MT-Ranker持平。使用Claude-3.5-Sonnet和Qwen-2.5-72B-Instruct作为评估大模型，显示出较高的人类评分者接受度，且其评分与人类评分高度相关。", "conclusion": "TransEvalnia系统提供了一种新的、有效的翻译评估方法，适用于多种语言对，并且提议的方法可解决系统敏感性问题。所有数据和代码均已公开。"}}
{"id": "2507.12591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12591", "abs": "https://arxiv.org/abs/2507.12591", "authors": ["Trong-Thang Pham", "Akash Awasthi", "Saba Khan", "Esteban Duran Marti", "Tien-Phat Nguyen", "Khoa Vo", "Minh Tran", "Ngoc Son Nguyen", "Cuong Tran Van", "Yuki Ikebe", "Anh Totti Nguyen", "Anh Nguyen", "Zhigang Deng", "Carol C. Wu", "Hien Van Nguyen", "Ngan Le"], "title": "CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling", "comment": "ICCV 2025", "summary": "Understanding radiologists' eye movement during Computed Tomography (CT)\nreading is crucial for developing effective interpretable computer-aided\ndiagnosis systems. However, CT research in this area has been limited by the\nlack of publicly available eye-tracking datasets and the three-dimensional\ncomplexity of CT volumes. To address these challenges, we present the first\npublicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we\nintroduce CT-Searcher, a novel 3D scanpath predictor designed specifically to\nprocess CT volumes and generate radiologist-like 3D fixation sequences,\novercoming the limitations of current scanpath predictors that only handle 2D\ninputs. Since deep learning models benefit from a pretraining step, we develop\na pipeline that converts existing 2D gaze datasets into 3D gaze data to\npretrain CT-Searcher. Through both qualitative and quantitative evaluations on\nCT-ScanGaze, we demonstrate the effectiveness of our approach and provide a\ncomprehensive assessment framework for 3D scanpath prediction in medical\nimaging.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.12732", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12732", "abs": "https://arxiv.org/abs/2507.12732", "authors": ["Fuya Nakamori", "Yin Jou Huang", "Fei Cheng"], "title": "Strategy Adaptation in Large Language Model Werewolf Agents", "comment": "7 pages, 2 figures", "summary": "This study proposes a method to improve the performance of Werewolf agents by\nswitching between predefined strategies based on the attitudes of other players\nand the context of conversations. While prior works of Werewolf agents using\nprompt engineering have employed methods where effective strategies are\nimplicitly defined, they cannot adapt to changing situations. In this research,\nwe propose a method that explicitly selects an appropriate strategy based on\nthe game context and the estimated roles of other players. We compare the\nstrategy adaptation Werewolf agents with baseline agents using implicit or\nfixed strategies and verify the effectiveness of our proposed method.", "AI": {"tldr": "A new method for Werewolf agents allows them to switch between predefined strategies based on the game context and player attitudes, resulting in better performance compared to agents with fixed or implicit strategies.", "motivation": "The motivation is to improve the performance of Werewolf agents by enabling them to adapt their strategies based on game context and estimated roles of other players, which previous methods could not do effectively.", "method": "The method involves switching between predefined strategies for Werewolf agents based on the attitudes of other players and context, contrasting with previous prompt engineering approaches which used implicit strategy definitions, unable to adapt to dynamic gaming situations.", "result": "The study verifies the effectiveness of the proposed method by comparing strategy-adapting Werewolf agents with baseline agents using implicit or fixed strategies.", "conclusion": "The conclusion is that the explicitly selected strategy based on game context and estimated roles of other players outperforms the implicit or fixed strategy approaches in Werewolf agents."}}
{"id": "2507.12602", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12602", "abs": "https://arxiv.org/abs/2507.12602", "authors": ["Said Ohamouddou", "Abdellatif El Afia", "Hanaa El Afia", "Raddouane Chiheb"], "title": "MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification", "comment": null, "summary": "Tree species classification from terrestrial LiDAR point clouds is\nchallenging because of the complex multi-scale geometric structures in forest\nenvironments. Existing approaches using multi-scale dynamic graph convolutional\nneural networks (MS-DGCNN) employ parallel multi-scale processing, which fails\nto capture the semantic relationships between the hierarchical levels of the\ntree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion\ndynamic graph convolutional network that uses semantically meaningful feature\nextraction at local, branch, and canopy scales with cross-scale information\npropagation. Our method employs scale-specific feature engineering, including\nstandard geometric features for the local scale, normalized relative vectors\nfor the branch scale, and distance information for the canopy scale. This\nhierarchical approach replaces uniform parallel processing with semantically\ndifferentiated representations that are aligned with the natural tree\nstructure. Under the same proposed tree species data augmentation strategy for\nall experiments, MS-DGCNN++ achieved an accuracy of 94.96 \\% on STPCTLS,\noutperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On\nFOR-species20K, it achieves 67.25\\% accuracy (6.1\\% improvement compared to\nMS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN\nand MS-DGCNN with overall accuracies of 93.15\\% on ModelNet40 and 94.05\\% on\nModelNet10. With lower parameters and reduced complexity compared to\nstate-of-the-art transformer approaches, our method is suitable for\nresource-constrained applications while maintaining a competitive accuracy.\nBeyond tree classification, the method generalizes to standard 3D object\nrecognition, establishing it as a versatile solution for diverse point cloud\nprocessing applications. The implementation code is publicly available at\nhttps://github.com/said-ohamouddou/MS-DGCNN2.", "AI": {"tldr": "本文介绍了MS-DGCNN++，一种分层多尺度融合动态图卷积网络方法，用于从地面LiDAR点云中进行树种分类，该方法在多种数据集上实现了高精度并优于现有的方法。", "motivation": "当前的方法使用多尺度动态图卷积网络（MS-DGCNN），但在捕捉树木结构层次间的语义关系方面存在不足。", "method": "通过多尺度具体特征工程，并在局部、枝干和树冠尺度上进行跨尺度信息传播，该论文提出了MS-DGCNN++，以更接近树自然结构的语义化表示替代了统一并行处理。", "result": "在STPCTLS数据集上，MS-DGCNN++达到了94.96%的分类精度，优于DGCNN、MS-DGCNN和最先进模型PPT；在FOR-species20K上也有不错的表现，准确率提升了6.1%；标准3D对象识别方面，该模型在ModelNet40上达到了93.15%，在ModelNet10上达到了94.05%，均优于DGCNN和MS-DGCNN。", "conclusion": "该方法不仅适用于树木分类，还可以扩展到标准3D对象识别，作为一个多种点云处理应用的多功能解决方案。"}}
{"id": "2507.12759", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12759", "abs": "https://arxiv.org/abs/2507.12759", "authors": ["Yunxiang Zhang", "Muhammad Khalifa", "Lechen Zhang", "Xin Liu", "Ayoung Lee", "Xinliang Frederick Zhang", "Farima Fatahi Bayat", "Lu Wang"], "title": "Logit Arithmetic Elicits Long Reasoning Capabilities Without Training", "comment": null, "summary": "Large reasoning models (LRMs) can do complex reasoning via long\nchain-of-thought (CoT) involving cognitive strategies such as backtracking and\nself-correction. Recent studies suggest that some models inherently possess\nthese long reasoning abilities, which may be unlocked via extra training. Our\nwork first investigates whether we can elicit such behavior without any\ntraining. To this end, we propose a decoding-time approach, ThinkLogit, which\nutilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for\nlong reasoning using a substantially smaller model as guider. We then show that\nwe can further boost performance by training the guider model with preference\noptimization over correct/incorrect reasoning pairs sampled from both the\ntarget and guider model -- a setup we refer to as ThinkLogit-DPO. Our\nexperiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative\nimprovement in pass@1 by 26% and 29%, respectively, over four mathematical\ndatasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model\n21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills\nacquired through reinforcement learning, improving pass@1 by 13% relative\ncompared to the Qwen2.5-32B base model. Our work presents a\ncomputationally-efficient method to elicit long reasoning in large models with\nminimal or no additional training.", "AI": {"tldr": "本文提出ThinkLogit及ThinkLogit-DPO方法提升大型语言模型的长链推理能力，实验显示相比Qwen2.5-32B模型显著提升通过率。", "motivation": "研究如何在不进行额外训练的情况下激发大型语言模型的长链推理能力，包括回溯和自我修正等认知策略。", "method": "ThinkLogit和ThinkLogit-DPO两种方法用于增强大型语言模型的长链推理能力。ThinkLogit使用较小的引导模型通过logits算术微调大型语言模型，而ThinkLogit-DPO通过偏好优化训练引导模型以进一步提升性能。", "result": "实验表明，ThinkLogit和ThinkLogit-DPO方法分别在四个数学数据集上相比Qwen2.5-32B模型提升了26%和29%的通过率。此外，它还能将通过强化学习获得的长期推理技能迁移到其他模型。", "conclusion": "提出了一个计算效率高的方法，可以在不进行或只需要最少额外训练的情况下，显著提高大型语言模型进行长链推理的能力。"}}
{"id": "2507.12617", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12617", "abs": "https://arxiv.org/abs/2507.12617", "authors": ["David Freire-Obregón", "Oliverio J. Santana", "Javier Lorenzo-Navarro", "Daniel Hernández-Sosa", "Modesto Castrillón-Santana"], "title": "Predicting Soccer Penalty Kick Direction Using Human Action Recognition", "comment": "Accepted at 23rd International Conference on Image Analysis and\n  Processing (ICIAP 2025)", "summary": "Action anticipation has become a prominent topic in Human Action Recognition\n(HAR). However, its application to real-world sports scenarios remains limited\nby the availability of suitable annotated datasets. This work presents a novel\ndataset of manually annotated soccer penalty kicks to predict shot direction\nbased on pre-kick player movements. We propose a deep learning classifier to\nbenchmark this dataset that integrates HAR-based feature embeddings with\ncontextual metadata. We evaluate twenty-two backbone models across seven\narchitecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D),\nachieving up to 63.9% accuracy in predicting shot direction (left or right),\noutperforming the real goalkeepers' decisions. These results demonstrate the\ndataset's value for anticipatory action recognition and validate our model's\npotential as a generalizable approach for sports-based predictive tasks.", "AI": {"tldr": "本研究发布了一个新的手动标注的足球点球数据集，用于预测射门方向。研究采用深度学习分类器进行评估，模型在预测射门方向上的精度高达63.9%，优于实际守门员的判断。", "motivation": "尽管人类行为识别中的动作预测在体育场景中的应用受到合适标注数据集的限制，本研究旨在通过构建一个新的足球点球数据集来突破这一限制。", "method": "本研究构建了一个手动标注的足球点球数据集，用于根据射门前的球员动作预测射门方向。提出了一种结合人类行为识别特征嵌入与上下文元数据的深度学习分类器来评估该数据集。", "result": "通过对七个架构家族中二十二个基础模型的评估，所提模型在预测射门方向（左或右）的准确率达到63.9%，优于实际守门员的表现。", "conclusion": "研究结果表明所构建的数据集对于预测性动作识别具有重要价值，并验证了所提模型作为体育领域预测任务通用化方法的潜力。"}}
{"id": "2507.12769", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.12769", "abs": "https://arxiv.org/abs/2507.12769", "authors": ["Keli Zheng", "Zerong Xie"], "title": "Synergy: End-to-end Concept Model", "comment": null, "summary": "In this paper, we present Synergy, a language model that bridges different\nlevels of abstraction in an end-to-end fashion through a learned routing\nmechanism. Focusing on low-level linguistic abstraction, we trained our model\nas a byte-level language model. Our model spontaneously learns to tokenize\nbytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)\ntokenizers while keeping comparable performance. By comparing with Llama3, we\nobserved an advantage of Synergy under the same model scale and training\ndataset size. Further studies show that the middle part (the higher abstraction\npart) of our model performs better when positional encodings are removed,\nsuggesting the emergence of position-independent concepts. These findings\ndemonstrate the feasibility of tokenizer-free architectures, paving the way for\nmore robust and flexible pipelines.", "AI": {"tldr": "Synergy语言模型通过学习的路由机制在端到端的方式中连接不同的语法抽象层次，无需传统的分词器而自动进行字节分词，性能优于Llama3。", "motivation": "研究的动机是为了创建一个能够无缝连接不同抽象层次的语言模型，并减少对预设分词方案的依赖。", "method": "提出了Synergy，一个通过学习路由机制在端到端的方式连接不同抽象层次的语言模型。模型以字节级语言模型的形式训练，自动学习对字节进行分词，生成的概念标记数量少于BBPE分词器，同时保持相似性能。", "result": "相比Llama3，在相同的模型规模和训练数据量下，Synergy表现出优势。研究发现模型中间部分（更高抽象层次）在移除位置编码后表现更佳，表明位置无关概念的出现。", "conclusion": "这些结果证明了无分词器架构的可行性，为更健壯和灵活的管道铺平了道路。"}}
{"id": "2507.12628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12628", "abs": "https://arxiv.org/abs/2507.12628", "authors": ["Sandipan Sarma", "Agney Talwarr", "Arijit Sur"], "title": "Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection", "comment": "10 pages, 6 figures", "summary": "Human-object interaction detection (HOID) refers to localizing interactive\nhuman-object pairs in images and identifying the interactions. Since there\ncould be an exponential number of object-action combinations, labeled data is\nlimited - leading to a long-tail distribution problem. Recently, zero-shot\nlearning emerged as a solution, with end-to-end transformer-based object\ndetectors adapted for HOID becoming successful frameworks. However, their\nprimary focus is designing improved decoders for learning entangled or\ndisentangled interpretations of interactions. We advocate that HOI-specific\ncues must be anticipated at the encoder stage itself to obtain a stronger scene\ninterpretation. Consequently, we build a top-down framework named Funnel-HOI\ninspired by the human tendency to grasp well-defined concepts first and then\nassociate them with abstract concepts during scene understanding. We first\nprobe an image for the presence of objects (well-defined concepts) and then\nprobe for actions (abstract concepts) associated with them. A novel asymmetric\nco-attention mechanism mines these cues utilizing multimodal information\n(incorporating zero-shot capabilities) and yields stronger interaction\nrepresentations at the encoder level. Furthermore, a novel loss is devised that\nconsiders objectaction relatedness and regulates misclassification penalty\nbetter than existing loss functions for guiding the interaction classifier.\nExtensive experiments on the HICO-DET and V-COCO datasets across\nfully-supervised and six zero-shot settings reveal our state-of-the-art\nperformance, with up to 12.4% and 8.4% gains for unseen and rare HOI\ncategories, respectively.", "AI": {"tldr": "本文提出Funnel-HOI框架，通过编码阶段的特征挖掘实现更好的人-物交互检测，尤其是在零样本学习场景中，取得了显著性能提升。", "motivation": "现有的HOI框架主要集中在改进解码器设计上，以学习纠缠或非纠缠的交互解释。作者认为，为了获得更强的场景解释能力，HOI特有的线索应该在编码阶段就被预测。因此，提出了本文的创新框架和方法。", "method": "提出了一种名为Funnel-HOI的自顶向下的框架，首先探测图像中的物体（明确定义的概念），然后再探查与它们相关的动作（抽象的概念）。使用新颖的不对称共同注意机制利用多模态信息（包含零样本学习能力）来挖掘这些线索，并在编码阶段产生更强的交互表示。同时还设计了一种新的损失函数，考虑了对象-动作相关性，并比现有损失函数更好地调节分类错误惩罚，以指导交互分类器。", "result": "在HICO-DET和V-COCO数据集上进行的广泛实验，涵盖了全监督和六种零样本设置，显示了本文方法达到了当前最先进的性能，分别为未见过和罕见的HOI类别提升了12.4%和8.4%。", "conclusion": "本研究展示了通过在HOI预测中引入编码阶段的HOI特定线索来获得更强的场景解释能力，并通过实验验证了方法的有效性。"}}
{"id": "2507.12782", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12782", "abs": "https://arxiv.org/abs/2507.12782", "authors": ["Thinh Hung Truong", "Karin Verspoor", "Trevor Cohn", "Timothy Baldwin"], "title": "Learning Robust Negation Text Representations", "comment": null, "summary": "Despite rapid adoption of autoregressive large language models, smaller text\nencoders still play an important role in text understanding tasks that require\nrich contextualized representations. Negation is an important semantic function\nthat is still not properly captured by such methods, affecting many downstream\napplications relying on text embeddings. We propose a strategy to improve\nnegation robustness of text encoders, by distilling data from large language\nmodels using diverse patterns of negation and hedging. We adopt a standard\ncontrastive learning strategy to finetune a strong BERT-based model, and\nobserve large improvement in negation understanding capabilities while\nmaintaining competitive performance on general benchmarks. In addition, we also\nshow that our method can be adapted to LLMs, leading to improved performance on\nnegation benchmarks.", "AI": {"tldr": "提出了一种改进文本编码器中否定稳健性的策略，通过使用多样化否定和保留模式从大型语言模型中蒸馏数据，极大地提高了否定理解能力。", "motivation": "虽然自回归大型语言模型快速普及，但在需要丰富上下文化表示的文本理解任务中，较小的文本编码器仍然扮演重要角色。然而，这些方法仍然未能恰当地捕捉否定，这影响到了许多依赖文本嵌入的下游应用。", "method": "通过多样化否定和保留模式从大型语言模型蒸馏数据来提高文本编码器的否定稳健性策略。采用标准对比学习策略微调强大的基于BERT的模型。", "result": "所提出的方法在提高否定理解能力的同时，保持了在通用基准测试上的竞争力。此外，该方法也可以适应大型语言模型，从而在否定基准测试上取得更好的性能。", "conclusion": "研究展示了一种能够提升文本编码器对于否定理解能力的方法，并且能够保持在通用基准测试上的竞争性能。方法还可用于大型语言模型，使得在否定基准测试上表现更高。"}}
{"id": "2507.12646", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12646", "abs": "https://arxiv.org/abs/2507.12646", "authors": ["Kaihua Chen", "Tarasha Khurana", "Deva Ramanan"], "title": "Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos", "comment": "Project page: https://cog-nvs.github.io/", "summary": "We explore novel-view synthesis for dynamic scenes from monocular videos.\nPrior approaches rely on costly test-time optimization of 4D representations or\ndo not preserve scene geometry when trained in a feed-forward manner. Our\napproach is based on three key insights: (1) covisible pixels (that are visible\nin both the input and target views) can be rendered by first reconstructing the\ndynamic 3D scene and rendering the reconstruction from the novel-views and (2)\nhidden pixels in novel views can be \"inpainted\" with feed-forward 2D video\ndiffusion models. Notably, our video inpainting diffusion model (CogNVS) can be\nself-supervised from 2D videos, allowing us to train it on a large corpus of\nin-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot\nto novel test videos via test-time finetuning. We empirically verify that\nCogNVS outperforms almost all prior art for novel-view synthesis of dynamic\nscenes from monocular videos.", "AI": {"tldr": "This paper presents a new approach, CogNVS, which synthesizes novel views of dynamic scenes from monocular videos using a combination of dynamic 3D reconstructions and a self-supervised 2D video diffusion model for inpainting.", "motivation": "The motivation behind this paper is to address the limitations of prior approaches, which either rely on costly test-time optimization of 4D representations or fail to preserve scene geometry when trained in a feed-forward manner for novel-view synthesis of dynamic scenes from monocular videos.", "method": "Our approach involves three key steps: firstly, it reconstructs the dynamic 3D scene using covisible pixels visible in both input and target views. Secondly, it uses a feed-forward 2D video diffusion model to 'inpaint' hidden pixels in novel views. Lastly, our video inpainting diffusion model, CogNVS, can be self-supervised from 2D videos and is applied zero-shot to novel test videos through test-time finetuning.", "result": "The empirical results show that CogNVS outperforms nearly all prior methods for synthesizing novel views of dynamic scenes from monocular videos.", "conclusion": "The conclusion is that the proposed method, CogNVS, achieves superior performance for novel-view synthesis of dynamic scenes from monocular videos compared to previous approaches and can be applied in a zero-shot manner."}}
{"id": "2507.12808", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.12808", "abs": "https://arxiv.org/abs/2507.12808", "authors": ["Andrew Shin", "Kunitake Kaneko"], "title": "Large Language Models' Internal Perception of Symbolic Music", "comment": null, "summary": "Large language models (LLMs) excel at modeling relationships between strings\nin natural language and have shown promise in extending to other symbolic\ndomains like coding or mathematics. However, the extent to which they\nimplicitly model symbolic music remains underexplored. This paper investigates\nhow LLMs represent musical concepts by generating symbolic music data from\ntextual prompts describing combinations of genres and styles, and evaluating\ntheir utility through recognition and generation tasks. We produce a dataset of\nLLM-generated MIDI files without relying on explicit musical training. We then\ntrain neural networks entirely on this LLM-generated MIDI dataset and perform\ngenre and style classification as well as melody completion, benchmarking their\nperformance against established models. Our results demonstrate that LLMs can\ninfer rudimentary musical structures and temporal relationships from text,\nhighlighting both their potential to implicitly encode musical patterns and\ntheir limitations due to a lack of explicit musical context, shedding light on\ntheir generative capabilities for symbolic music.", "AI": {"tldr": "研究发现，大型语言模型可以在没有明确音乐训练的情况下，根据文本提示生成MIDI文件，并且可以进行音乐风格和类型的分类及旋律补全任务，显示出其对音乐模式的潜在隐式编码能力，同时也指出了其由于缺乏明确音乐背景所存在的局限性。", "motivation": "研究旨在探索大型语言模型在没有显式音乐训练的情况下，是否能够隐式地理解和生成音乐，特别是在理解和生成符号音乐方面的能力。", "method": "通过使用大型语言模型生成描述不同音乐风格和流派组合的文本提示，生成MIDI文件，然后训练神经网络对其进行风格和类型分类及旋律补全任务。", "result": "结果表明大型语言模型可以从文本中推断出基本的音乐结构和时间关系，但是也存在一定的限制，这归因于其缺乏明确的音乐上下文。", "conclusion": "大型语言模型显示出潜在的隐式编码音乐模式的能力，对于生成音乐内容具有一定的能力，但同时也表明需要更多关于音乐的明确训练才能提升其准确性。"}}
{"id": "2507.12663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12663", "abs": "https://arxiv.org/abs/2507.12663", "authors": ["Inamullah", "Ernesto Elias Vidal Rosas", "Imran Razzak", "Shoaib Jameel"], "title": "Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort", "comment": null, "summary": "Cardiovascular disease (CVD) remains the leading global cause of mortality,\nyet current risk stratification methods often fail to detect early, subclinical\nchanges. Previous studies have generally not integrated retinal\nmicrovasculature characteristics with comprehensive serum lipidomic profiles as\npotential indicators of CVD risk. In this study, an innovative imaging omics\nframework was introduced, combining retinal microvascular traits derived\nthrough deep learning based image processing with serum lipidomic data to\nhighlight asymptomatic biomarkers of cardiovascular risk beyond the\nconventional lipid panel. This represents the first large scale, covariate\nadjusted and stratified correlation analysis conducted in a healthy population,\nwhich is essential for identifying early indicators of disease. Retinal\nphenotypes were quantified using automated image analysis tools, while serum\nlipid profiling was performed by Ultra High Performance Liquid Chromatography\nElectrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).\nStrong, age- and sex-independent correlations were established, particularly\nbetween average artery width, vessel density, and lipid subclasses such as\ntriacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These\nassociations suggest a converging mechanism of microvascular remodeling under\nmetabolic stress. By linking detailed\n  vascular structural phenotypes to specific lipid species, this study fills a\ncritical gap in the understanding of early CVD pathogenesis. This integration\nnot only offers a novel perspective on microvascular metabolic associations but\nalso presents a significant opportunity for the identification of robust,\nnon-invasive biomarkers. Ultimately, these findings may support improved early\ndetection, targeted prevention, and personalized approaches in cardiovascular\nhealthcare.", "AI": {"tldr": "研究通过结合深度学习提取的视网膜微血管特征与血清脂质组学数据，发现了心血管疾病（CVD）新的非侵入性早期生物标志物，有助于早期发现和个性化干预。", "motivation": "传统的风险分层方法通常无法发现早期亚临床的CVD改变，而本研究整合了视网膜微血管特点和全面的血清脂质组学。", "method": "采用创新的成像组学框架，结合深度学习图像处理提取的视网膜微血管特征与血清脂质组学数据，对健康人群进行了大规模、共变量调整和分层的相关性分析。", "result": "研究表明视网膜表型，例如平均动脉宽度，血管密度与脂质亚类（如三酰甘油、二酰甘油和神经酰胺）存在强相关性。", "conclusion": "研究填补了对早期CVD发病机制理解的空缺，为CVD的早期发现、定向预防和个性化医疗提供了重要机会。"}}
{"id": "2507.12838", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12838", "abs": "https://arxiv.org/abs/2507.12838", "authors": ["Xi Ai", "Mahardika Krisna Ihsani", "Min-Yen Kan"], "title": "Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?", "comment": null, "summary": "Cross-lingual consistency should be considered to assess cross-lingual\ntransferability, maintain the factuality of the model knowledge across\nlanguages, and preserve the parity of language model performance. We are thus\ninterested in analyzing, evaluating, and interpreting cross-lingual consistency\nfor factual knowledge. We examine code-mixed coreferential statements conveyed\nidentical knowledge across languages to study cross-lingual knowledge\nconsistency. We use some interpretability approaches to analyze the behavior of\na model in cross-lingual contexts, discovering that multilingual models show\ndifferent levels of consistency, subject to language families, linguistic\nfactors, and a bottleneck in cross-lingual consistency on a particular layer.\nIn addition, we evaluate common strategies aimed at improving multilingual\nperformance to observe whether these strategies can improve knowledge\nconsistency at the same time. While knowledge is not cross-lingual consistency\nin many cases, code-switching training and cross-lingual word alignment\nobjectives show the most promising results, emphasizing the noteworthiness of\ncross-lingual alignment supervision and code-switching training for both\nmultilingual performance and cross-lingual consistency enhancement.", "AI": {"tldr": "研究分析了跨语言知识一致性，在考察了跨语言环境下模型的行为后发现，虽然普遍提升多语言性能的方法对增强知识一致性作用有限，但跨语言对齐监督和代码切换训练可显著提高跨语言一致性和多语言性能。", "motivation": "跨语言一致性对于评估跨语言传输能力、维护模型知识的事实性以及保持语言模型性能的平等性至关重要。研究的目标是为了分析、评估和解释跨语言知识的一致性。", "method": "通过分析跨语言环境中模型的行为来研究跨语言知识的一致性，特别是考查了混杂语言的共指陈述如何在不同的语言中传达相同的知识。研究使用了一些可解释性方法来发现多语言模型在不同语言家族、语言因素以及特定层面上的一致性水平。", "result": "发现仅靠提高多语言性能的常见策略并不一定能改善知识一致性。然而，代码切换训练和跨语言词对齐的目标显示出最令人鼓舞的结果，表明跨语言对齐监督和代码切换训练对于提高跨语言一致性和多语言性能都有重要意义。", "conclusion": "研究结果强调了跨语言对齐监督和代码切换训练对于维持知识跨语言一致性和提高多语言性能的重要性。尽管在多数情况下知识一致性并未得到保证，但研究指出某些特定策略对于提升跨语言一致性有显著帮助。"}}
{"id": "2507.12675", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12675", "abs": "https://arxiv.org/abs/2507.12675", "authors": ["Christina Thrainer", "Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Christian Guetl", "Steven Sloan", "Kendall N. Niles", "Ken Pathak"], "title": "FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks", "comment": null, "summary": "Automated structural defect segmentation in civil infrastructure faces a\ncritical challenge: achieving high accuracy while maintaining computational\nefficiency for real-time deployment. This paper presents FORTRESS\n(Function-composition Optimized Real-Time Resilient Structural Segmentation), a\nnew architecture that balances accuracy and speed by using a special method\nthat combines depthwise separable convolutions with adaptive Kolmogorov-Arnold\nNetwork integration. FORTRESS incorporates three key innovations: a systematic\ndepthwise separable convolution framework achieving a 3.6x parameter reduction\nper layer, adaptive TiKAN integration that selectively applies function\ncomposition transformations only when computationally beneficial, and\nmulti-scale attention fusion combining spatial, channel, and KAN-enhanced\nfeatures across decoder levels. The architecture achieves remarkable efficiency\ngains with 91% parameter reduction (31M to 2.9M), 91% computational complexity\nreduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while\ndelivering superior segmentation performance. Evaluation on benchmark\ninfrastructure datasets demonstrates state-of-the-art results with an F1- score\nof 0.771 and a mean IoU of 0.677, significantly outperforming existing methods\nincluding U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves\nessential for optimal performance, establishing FORTRESS as a robust solution\nfor practical structural defect segmentation in resource-constrained\nenvironments where both accuracy and computational efficiency are paramount.\nComprehensive architectural specifications are provided in the Supplemental\nMaterial. Source code is available at URL:\nhttps://github.com/faeyelab/fortress-paper-code.", "AI": {"tldr": "FORTRESS是用于平衡基础设施自动化结构缺陷分割精度和速度的新架构，通过减少91%的参数和近91%的计算复杂度，同时将推理速度提高了3倍，达到了卓越的效率。", "motivation": "研究动机是解决自动化结构缺陷分割在基础设施中面临的挑战，即在实现高精度的同时保持计算效率，以实现实时部署。", "method": "FORTRESS采用特殊方法结合深度可分离卷积和自适应Kolmogorov-Arnold网络集成，该架构包含三项关键创新：系统化的深度可分离卷积框架，实现了每层3.6倍的参数减少，自适应TiKAN集成选择性地应用函数组合变换，以及跨解码器级别的多尺度注意力融合结合空间、通道和KAN增强特征。", "result": "在基础设施基准数据集上的评估显示，FORTRESS达到了最先进的结果，F1得分为0.771，平均IoU为0.677，显著优于现有的U-Net、SA-UNet和U-KAN方法。", "conclusion": "双优化策略对于最佳性能至关重要，FORTRESS作为一个强大解决方案，在资源受限环境中实现精确定位和计算效率并重的结构缺陷分割。"}}
{"id": "2507.12930", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12930", "abs": "https://arxiv.org/abs/2507.12930", "authors": ["Yihong Wang", "Zhonglin Jiang", "Ningyuan Xi", "Yue Zhao", "Qingqing Gu", "Xiyuan Chen", "Hao Wu", "Sheng Xu", "Hange Zhou", "Yong Chen", "Luo Ji"], "title": "Making Language Model a Hierarchical Classifier and Generator", "comment": null, "summary": "Decoder-only language models, such as GPT and LLaMA, generally decode on the\nlast layer. Motivated by human's hierarchical thinking capability, we propose\nthat a hierarchical decoder architecture could be built with different layers\ndecoding texts simultaneously. Due to limited time and computationally\nresources, we choose to adapt a pretrained language model into this form of\nhierarchical decoder. Language heads of the last layer are copied to different\nselected intermediate layers, and fine-tuned with different task inputs. By\nthorough experiments, we validate that these selective intermediate layers\ncould be adapted to speak meaningful and reasonable contents, and this paradigm\nof hierarchical decoder can obtain state-of-the-art performances on multiple\ntasks such as hierarchical text classification, classification-guided\ngeneration, and hierarchical text generation. This study suggests the\npossibility of a generalized hierarchical reasoner, pretraining from scratch.", "AI": {"tldr": "通过构建层次化解码器，改进预训练语言模型的性能，实现在多个任务上的优异效果。", "motivation": "模仿人类的层次化思维能力，提高语言模型的理解和生成能力。", "method": "将预训练模型的最后一层语言头复制到选定的中间层，并针对特定任务输入进行微调。", "result": "这项研究通过构建层次解码器架构，将预训练语言模型的不同层同时解码文本，来模拟人类的层次化思维能力。通过选择性地将最后一层的语言头复制到不同的中间层并进行微调，该方法在多个任务中表现出色，包括层次化文本分类、分类引导生成和层次化文本生成。研究结果表明，这种方法能够生成有意义的内容，并且可能为预训练通用层次化推理器提供可能性。", "conclusion": "该研究方法验证了层次化解码器的优势，并暗示了预训练通用层次化推理器的可能性。"}}
{"id": "2507.12714", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.12714", "abs": "https://arxiv.org/abs/2507.12714", "authors": ["Yang Yang", "Dongni Mao", "Hiroaki Santo", "Yasuyuki Matsushita", "Fumio Okura"], "title": "NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement", "comment": "IEEE/CVF International Conference on Computer Vision (ICCV 2025),\n  Project: https://neuraleaf-yang.github.io/", "summary": "We develop a neural parametric model for 3D leaves for plant modeling and\nreconstruction that are essential for agriculture and computer graphics. While\nneural parametric models are actively studied for humans and animals, plant\nleaves present unique challenges due to their diverse shapes and flexible\ndeformation. To this problem, we introduce a neural parametric model for\nleaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be\napproximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into\ntheir 2D base shapes and 3D deformations. This representation allows learning\nfrom rich sources of 2D leaf image datasets for the base shapes, and also has\nthe advantage of simultaneously learning textures aligned with the geometry. To\nmodel the 3D deformation, we propose a novel skeleton-free skinning model and\ncreate a newly captured 3D leaf dataset called DeformLeaf. We show that\nNeuraLeaf successfully generates a wide range of leaf shapes with deformation,\nresulting in accurate model fitting to 3D observations like depth maps and\npoint clouds. Our implementation and dataset are available at\nhttps://neuraleaf-yang.github.io/.", "AI": {"tldr": "本文开发了一种名为NeuraLeaf的神经参数模型，用于植物叶片的3D建模和重建，该模型能够对叶片的基础形状和变形进行学习，并有助于农业和计算机图形学的研究。", "motivation": "现有的关于人类和动物的神经参数模型研究较为丰富，但叶片由于其形状多样和灵活变形特性，需要一个专门的模型来处理。我们的目标是开发一种适用于植物叶片建模和重建的神经参数模型，以推动农业和计算机图形学领域的相关研究。", "method": "NeuraLeaf模型利用2D叶片图像数据集学习叶片的基础形状，并使用一种无骨架蒙皮方法来模拟3D变形，同时提出一个名为DeformLeaf的新3D叶片数据集用于模型训练和测试。", "result": "本文提出了一种用于3D叶片建模和重建的神经参数模型NeuraLeaf，这是农业和计算机图形学中的重要组成部分。NeuraLeaf利用叶片摊平后的形状可以近似为2D平面的特点，将叶片的几何形状分为2D基础形状和3D变形两个部分。该模型能够利用丰富的2D叶片图像数据集进行学习，并且可以同时学习与几何形状一致的纹理。为了解决3D变形问题，我们提出了一种无骨架蒙皮（skeleton-free skinning）模型，并创建了一个新的3D叶片数据集DeformLeaf。实验表明，NeuraLeaf能够成功生成多种不同形状的叶片，并且其生成的模型能够准确地拟合3D观测数据（如深度图和点云）。实验代码和数据集可以在https://neuraleaf-yang.github.io/获取。", "conclusion": "这种基于神经参数模型的新型叶片建模方法为农业研究和计算机图形学领域中的植物建模和重建提供了一种精确的技术手段。"}}
{"id": "2507.12981", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12981", "abs": "https://arxiv.org/abs/2507.12981", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Sáez", "Héctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "title": "MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps", "comment": "Accepted as an official challenge paper in the PRESTA: Questions and\n  Answers over Tabular Data shared task at IberLEF 2025, colocated with the\n  41st SEPLN Conference in Zaragoza, Spain", "summary": "This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas\ny Respuestas sobre Tablas en Espa\\~nol (Questions and Answers about Tables in\nSpanish). Our solution obtains answers to the questions by implementing Python\ncode generation with LLMs that is used to filter and process the table. This\nsolution evolves from the MRT implementation for the Semeval 2025 related task.\nThe process consists of multiple steps: analyzing and understanding the content\nof the table, selecting the useful columns, generating instructions in natural\nlanguage, translating these instructions to code, running it, and handling\npotential errors or exceptions. These steps use open-source LLMs and\nfine-grained optimized prompts for each step. With this approach, we achieved\nan accuracy score of 85\\% in the task.", "AI": {"tldr": "本文提出了一种利用LLM生成Python代码来处理表格数据以回答问题的方法，在IberLEF 2025任务PRESTA中取得了85%的准确性。", "motivation": "本文的方法目的是参与IberLEF 2025任务PRESTA，即关于西班牙语表格的问题与答案。", "method": "通过LLM实现Python代码生成，用以筛选和处理表格内容，从而获得问题的答案。这个方法是基于MRT在SemEval 2025相关任务中的实现而发展的。该过程包括多个步骤：分析和理解表格内容，选择有用的列，生成自然语言指令，将这些指令翻译成代码，运行代码，并处理潜在的错误或异常。这些步骤使用开源语言模型和针对每个步骤优化的细粒度提示。", "result": "采用这个方法，我们在任务中达到了85%的准确性分数。", "conclusion": "通过LLM生成Python代码处理表格数据的方法取得了85%的准确率，展示了一种有效解决表格数据中问答问题的技术路径。"}}
{"id": "2507.12727", "categories": ["cs.CV", "I.4"], "pdf": "https://arxiv.org/pdf/2507.12727", "abs": "https://arxiv.org/abs/2507.12727", "authors": ["Peijun Wang", "Jinhua Zhao"], "title": "SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery", "comment": null, "summary": "Small object detection remains a challenging problem in the field of object\ndetection. To address this challenge, we propose an enhanced YOLOv8-based\nmodel, SOD-YOLO. This model integrates an ASF mechanism in the neck to enhance\nmulti-scale feature fusion, adds a Small Object Detection Layer (named P2) to\nprovide higher-resolution feature maps for better small object detection, and\nemploys Soft-NMS to refine confidence scores and retain true positives.\nExperimental results demonstrate that SOD-YOLO significantly improves detection\nperformance, achieving a 36.1% increase in mAP$_{50:95}$ and 20.6% increase in\nmAP$_{50}$ on the VisDrone2019-DET dataset compared to the baseline model.\nThese enhancements make SOD-YOLO a practical and efficient solution for small\nobject detection in UAV imagery. Our source code, hyper-parameters, and model\nweights are available at https://github.com/iamwangxiaobai/SOD-YOLO.", "AI": {"tldr": "This paper introduces SOD-YOLO, an enhanced YOLOv8-based model, which shows substantial improvements in detecting small objects by integrating multi-scale features, a high-resolution detection layer, and refined NMS, resulting in better performance on the VisDrone2019-DET dataset.", "motivation": "The motivation behind this paper is the persistent challenge of achieving accurate small object detection in surveillance and UAV imagery, which traditional object detection models often struggle with. The aim is to improve detection performance specifically for small objects.", "method": "This paper addresses the challenge of small object detection by proposing a model called SOD-YOLO, which is an enhanced version of YOLOv8. It includes an Adaptive Scale Fusion (ASF) mechanism in the neck to boost multi-scale feature fusion, introduces a Small Object Detection Layer (P2) for higher-resolution feature maps, and uses Soft-NMS to refine confidence scores and ensure the retention of true positives.", "result": "The experimental results show a significant improvement in the detection performance of the proposed model. Compared to the baseline model, SOD-YOLO achieves a 36.1% increase in mAP$_{50:95}$ and a 20.6% increase in mAP$_{50}$ on the VisDrone2019-DET dataset, highlighting its effectiveness in small object detection.", "conclusion": "The conclusions drawn from the paper are that the proposed SOD-YOLO model significantly enhances the detection performance for small objects in UAV imagery. This improvement makes the model more practical and efficient for real-world applications involving small object detection, such as in UAV surveillance systems."}}
{"id": "2507.13076", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13076", "abs": "https://arxiv.org/abs/2507.13076", "authors": ["Quentin Goux", "Nadira Lammari"], "title": "Formalizing Attack Scenario Description: A Proposed Model", "comment": null, "summary": "Organizations face an ever-changing threat landscape. They must continuously\ndedicate significant efforts to protect their assets, making their adoption of\nincreased cybersecurity automation inevitable. However, process automation\nrequires formalization of input data. Through this paper, we address this need\nfor processes that use attack scenarios as input. Among these processes, one\ncan mention both the generation of scripts for attack simulation and training\npurposes, as well as the analysis of attacks. Therefore, the paper's main\nresearch contribution is a novel formal model that encompasses the attack's\ncontext description and its scenario. It is abstracted using UML class model.\nOnce the description of our model done, we will show how it could serve an\nupstream attack analysis process. We will show also its use for an automatic\ngeneration of attack scripts in the context of cybersecurity training. These\ntwo uses cases constitute the second contribution of this present research\nwork.", "AI": {"tldr": "The paper proposes a novel UML-based formal model for attack scenarios to support automation in cybersecurity processes, such as attack simulation and training script generation.", "motivation": "The motivation is to formalize input data for cybersecurity processes in an evolving threat landscape, facilitating process automation and improved security.", "method": "A UML class model formalizes the context and scenario of attacks, enabling its use in attack analysis and automatic script generation.", "result": "The model is capable of supporting upstream attack analysis and the automatic generation of attack scripts for training.", "conclusion": "The paper highlights the importance and feasibility of formalizing attack scenarios to enhance cybersecurity automation and effectiveness."}}
{"id": "2507.12730", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12730", "abs": "https://arxiv.org/abs/2507.12730", "authors": ["Homare Sueyoshi", "Kiyoshi Nishikawa", "Hitoshi Kiya"], "title": "A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique", "comment": "4 pages, 5 figures, 1 table. Accepted to GCCE 2025", "summary": "We propose a privacy-preserving semantic-segmentation method for applying\nperceptual encryption to images used for model training in addition to test\nimages. This method also provides almost the same accuracy as models without\nany encryption. The above performance is achieved using a domain-adaptation\ntechnique on the embedding structure of the Vision Transformer (ViT). The\neffectiveness of the proposed method was experimentally confirmed in terms of\nthe accuracy of semantic segmentation when using a powerful\nsemantic-segmentation model with ViT called Segmentation Transformer.", "AI": {"tldr": "A privacy-preserving semantic-segmentation method that uses perceptual encryption and domain-adaptation on ViT embedding structures, maintaining model accuracy as confirmed through experiments with the Segmentation Transformer.", "motivation": "The motivation is to preserve privacy while maintaining the accuracy of models used for semantic segmentation.", "method": "We propose a privacy-preserving semantic-segmentation method that applies perceptual encryption to images used for model training and test images. This method maintains high accuracy by employing a domain-adaptation technique on the embedding structure of the Vision Transformer (ViT).", "result": "The method was tested using the Segmentation Transformer, and it was found to significantly maintain the accuracy of semantic segmentation, demonstrating its effectiveness.", "conclusion": "The proposed method successfully achieves privacy preservation without sacrificing the accuracy of models used for semantic segmentation tasks."}}
{"id": "2507.13105", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13105", "abs": "https://arxiv.org/abs/2507.13105", "authors": ["Marc Brinner", "Sina Zarriess"], "title": "SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts", "comment": null, "summary": "We introduce SemCSE, an unsupervised method for learning semantic embeddings\nof scientific texts. Building on recent advances in contrastive learning for\ntext embeddings, our approach leverages LLM-generated summaries of scientific\nabstracts to train a model that positions semantically related summaries closer\ntogether in the embedding space. This resulting objective ensures that the\nmodel captures the true semantic content of a text, in contrast to traditional\ncitation-based approaches that do not necessarily reflect semantic similarity.\nTo validate this, we propose a novel benchmark designed to assess a model's\nability to understand and encode the semantic content of scientific texts,\ndemonstrating that our method enforces a stronger semantic separation within\nthe embedding space. Additionally, we evaluate SemCSE on the comprehensive\nSciRepEval benchmark for scientific text embeddings, where it achieves\nstate-of-the-art performance among models of its size, thus highlighting the\nbenefits of a semantically focused training approach.", "AI": {"tldr": "SemCSE is an unsupervised method for learning semantic embeddings of scientific texts using LLM-generated summaries to capture true semantic content, as shown by performance on various benchmarks.", "motivation": "The motivation behind this method is to improve upon traditional citation-based approaches that do not necessarily reflect the semantic similarity of texts. SemCSE aims to capture true semantic content of a text.", "method": "Our method is called SemCSE, an unsupervised approach for learning semantic embeddings of scientific texts. It uses LLM-generated summaries of scientific abstracts to train a model, positioning semantically similar summaries closer together in the embedding space.", "result": "The proposed method demonstrates strong semantic separation within the embedding space on a novel benchmark designed to assess semantic understanding of scientific texts. It also achieves state-of-the-art performance on the established SciRepEval benchmark.", "conclusion": "The conclusion from the results is that focusing on semantic content in the training of embeddings could significantly enhance their performance and semantic representation capability."}}
{"id": "2507.12739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12739", "abs": "https://arxiv.org/abs/2507.12739", "authors": ["Ijazul Haq", "Muhammad Saqib", "Yingjie Zhang"], "title": "Transformer-based Spatial Grounding: A Comprehensive Survey", "comment": null, "summary": "Spatial grounding, the process of associating natural language expressions\nwith corresponding image regions, has rapidly advanced due to the introduction\nof transformer-based models, significantly enhancing multimodal representation\nand cross-modal alignment. Despite this progress, the field lacks a\ncomprehensive synthesis of current methodologies, dataset usage, evaluation\nmetrics, and industrial applicability. This paper presents a systematic\nliterature review of transformer-based spatial grounding approaches from 2018\nto 2025. Our analysis identifies dominant model architectures, prevalent\ndatasets, and widely adopted evaluation metrics, alongside highlighting key\nmethodological trends and best practices. This study provides essential\ninsights and structured guidance for researchers and practitioners,\nfacilitating the development of robust, reliable, and industry-ready\ntransformer-based spatial grounding models.", "AI": {"tldr": "本文通过系统文献回顾，概述了2018年至2025年间基于变换器的空间定位的研究进展，提供了关键方法论趋势和最佳实践，为研究人员和从业者开发稳健的、可靠的以及面向工业的应用程序提供指导。", "motivation": "尽管基于变换器模型的空间定位技术在多模态表示和跨模态对齐方面取得了显著的进步，但缺乏对当前方法的综合概述，特别是关于数据集使用、评估指标和工业应用的实用性方面的信息。", "method": "本研究采用系统性的文献回顾方法，分析了2018年至2025年间基于变换器的空间定位方法，重点在于模型架构、数据集、评估指标以及方法论趋势和最佳实践的识别。", "result": "研究结果包括对主导模型架构、常用数据集和广泛采用的评估指标的识别，揭示了关键的方法论趋势和最佳实践。", "conclusion": "通过这项研究，研究者能够获得关于基于变换器的空间定位方法的重要洞察，得到结构化的指导，有助于开发出稳健且适用于工业环境的模型。"}}
{"id": "2507.13115", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13115", "abs": "https://arxiv.org/abs/2507.13115", "authors": ["Jaya Caporusso", "Matthew Purver", "Senja Pollak"], "title": "A Computational Framework to Identify Self-Aspects in Text", "comment": "Accepted to ACL SRW 2025", "summary": "This Ph.D. proposal introduces a plan to develop a computational framework to\nidentify Self-aspects in text. The Self is a multifaceted construct and it is\nreflected in language. While it is described across disciplines like cognitive\nscience and phenomenology, it remains underexplored in natural language\nprocessing (NLP). Many of the aspects of the Self align with psychological and\nother well-researched phenomena (e.g., those related to mental health),\nhighlighting the need for systematic NLP-based analysis. In line with this, we\nplan to introduce an ontology of Self-aspects and a gold-standard annotated\ndataset. Using this foundation, we will develop and evaluate conventional\ndiscriminative models, generative large language models, and embedding-based\nretrieval approaches against four main criteria: interpretability, ground-truth\nadherence, accuracy, and computational efficiency. Top-performing models will\nbe applied in case studies in mental health and empirical phenomenology.", "AI": {"tldr": "该博士论文提案旨在开发一个计算框架识别文本中的自我方面，引入自我方面的本体论和黄金标准标注数据集，评估不同类型的模型，并应用于实际案例研究。", "motivation": "自我是一个多面的概念，在语言中有所体现。虽然它在认知科学和现象学等领域有所描述，但在自然语言处理（NLP）中尚未得到充分探索。许多自我方面的内容与心理健康等领域已有研究的现象相一致，强调了系统性NLP分析的必要性。", "method": "本研究计划开发一个计算框架来识别文本中的自我方面。将引入自我方面的本体论和一个黄金标准的标注数据集作为基础。将开发并评估传统的判别模型、生成型大型语言模型以及基于嵌入的检索方法，并根据可解释性、真实数据一致性、准确性以及计算效率四项主要标准进行评价。", "result": "此研究将开发一个自我方面的计算识别框架，评估不同的模型，并应用于心理健康和经验现象学的案例研究中。", "conclusion": "该研究将推动NLP领域自我识别的研究，并为心理健康和经验现象学提供有价值的工具和见解。"}}
{"id": "2507.12755", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12755", "abs": "https://arxiv.org/abs/2507.12755", "authors": ["Yanchen Guan", "Haicheng Liao", "Chengyue Wang", "Bonan Wang", "Jiaxun Zhang", "Jia Hu", "Zhenning Li"], "title": "Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation", "comment": null, "summary": "Developing precise and computationally efficient traffic accident\nanticipation system is crucial for contemporary autonomous driving\ntechnologies, enabling timely intervention and loss prevention. In this paper,\nwe propose an accident anticipation framework employing a dual-branch\narchitecture that effectively integrates visual information from dashcam videos\nwith structured textual data derived from accident reports. Furthermore, we\nintroduce a feature aggregation method that facilitates seamless integration of\nmultimodal inputs through large models (GPT-4o, Long-CLIP), complemented by\ntargeted prompt engineering strategies to produce actionable feedback and\nstandardized accident archives. Comprehensive evaluations conducted on\nbenchmark datasets (DAD, CCD, and A3D) validate the superior predictive\naccuracy, enhanced responsiveness, reduced computational overhead, and improved\ninterpretability of our approach, thus establishing a new benchmark for\nstate-of-the-art performance in traffic accident anticipation.", "AI": {"tldr": "本论文提出了一种采用双分支架构的交通事故预测框架，该框架通过大型模型（GPT-4o, Long-CLIP）结合前视摄像头视频和事故报告中的结构化文本数据，实现了高效精确的交通事故预测，并通过实验验证了其最新技术水平的预测准确性、响应时间、计算效率和可解释性的优越性。", "motivation": "为了进一步推动当代自动驾驶技术的发展和损失预防，需要开发精确且计算高效的交通事故预测系统。", "method": "Structure", "result": "{\"tldr\": \"本论文提出了一种采用双分支架构的交通事故预测框架，该框架通过大型模型（GPT-4o, Long-CLIP）结合前视摄像头视频和事故报告中的结构化文本数据，实现了高效精确的交通事故预测，并通过实验验证了其最新技术水平的预测准确性、响应时间、计算效率和可解释性的优越性。\", \"motivation\": \"为了进一步推动当代自动驾驶技术的发展和损失预防，需要开发精确且计算高效的交通事故预测系统。\", \"method\": \"架构包括两个分支：一个用于处理前视摄像头视频的视觉信息，另一个用于处理事故报告的结构化文本数据。模型通过大型AI模型实现多模态输入的有效结合，并辅以针对性的提示工程策略。\", \"result\": \"在DAD, CCD 和 A3D等基准数据集上的综合评估验证了该方法在预测准确性、响应速度、计算效率和解释性方面的优越性。\", \"conclusion\": \"该研究成功建立了新的交通事故预测系统的最新水平标准，证明了双分支架构和大型模型的优越性能。\"}", "conclusion": "该研究成功建立了新的交通事故预测系统的最新水平标准，证明了双分支架构和大型模型的优越性能。"}}
{"id": "2507.13138", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13138", "abs": "https://arxiv.org/abs/2507.13138", "authors": ["Hadi Mohammadi", "Tina Shahedi", "Pablo Mosteiro", "Massimo Poesio", "Ayoub Bagheri", "Anastasia Giachanou"], "title": "Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation", "comment": null, "summary": "Understanding the sources of variability in annotations is crucial for\ndeveloping fair NLP systems, especially for tasks like sexism detection where\ndemographic bias is a concern. This study investigates the extent to which\nannotator demographic features influence labeling decisions compared to text\ncontent. Using a Generalized Linear Mixed Model, we quantify this inf luence,\nfinding that while statistically present, demographic factors account for a\nminor fraction ( 8%) of the observed variance, with tweet content being the\ndominant factor. We then assess the reliability of Generative AI (GenAI) models\nas annotators, specifically evaluating if guiding them with demographic\npersonas improves alignment with human judgments. Our results indicate that\nsimplistic persona prompting often fails to enhance, and sometimes degrades,\nperformance compared to baseline models. Furthermore, explainable AI (XAI)\ntechniques reveal that model predictions rely heavily on content-specific\ntokens related to sexism, rather than correlates of demographic\ncharacteristics. We argue that focusing on content-driven explanations and\nrobust annotation protocols offers a more reliable path towards fairness than\npotentially persona simulation.", "AI": {"tldr": "该研究通过广义线性混合模型分析发现，尽管人口统计特征对标注有一定影响，但影响甚微，主要决定因素仍然是文本内容。尝试引导AI模型使用人口统计特征进行注释整体未能提升效果。", "motivation": "研究的主要动机是探讨如何在自然语言处理系统中实现更公平的性别歧视检测任务，尤其是通过理解注释者的人口统计特征和文本内容对标注决策的影响。这种研究对于消除标注过程中的偏差、提高模型在现实情况中的公平性具有重要意义。", "method": "该研究使用了一种统计模型——广义线性混合模型来量化注释者的人口统计特征和文本内容对标注决策的相对影响。还评估了生成AI模型在被具体人口统计特征指导时的表现，并使用了解释性AI技术来深入分析模型预测的关键依赖因素。", "result": "{\"tldr\": \"该研究探讨了注释者的人口统计特征和文本内容对其标注决策的影响，发现文本内容是主要决定因素，人口统计特征解释的变异性极小。此外，研究评估了引导生成AI模型使用人口统计特征作为注释者的效果，发现这种方法通常没有提升效果，有时反而会降低性能。\", \"motivation\": \"理解标注中的变异性来源对于开发公平的自然语言处理系统至关重要，尤其是在像性别歧视检测这样的任务中，人口统计偏差是一个需要关注的问题。\", \"method\": \"使用广义线性混合模型量化人口统计特征与文本内容对标注决策的影响，并使用解释性AI技术揭示模型预测的依赖因素。\", \"result\": \"发现人口统计特征仅解释了一小部分观察到的变异性（8%），并且引导生成AI模型使用人口统计特征并没有提升其与人类判断的一致性，有时反而降低了其性能。\", \"conclusion\": \"研究建议，应重点依靠内容驱动的解释和稳健的标注协议来实现公平性，而不是模拟人口统计特征。\"}", "conclusion": "该研究指出，注释过程中的变异性和公平性问题主要源于文本内容，而非注释者的人口统计特征。此外，尝试通过引导AI模型使用人口统计特征来提升其注释效果的方法并不总是有效，有时反而降低了它们的表现。这些发现强调了在开发自然语言处理系统时应更多依赖于内容驱动的解释与标注协议来确保更好的公平性。"}}
{"id": "2507.12758", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12758", "abs": "https://arxiv.org/abs/2507.12758", "authors": ["Wangzheng Shi", "Yinglin Zheng", "Yuxin Lin", "Jianmin Bao", "Ming Zeng", "Dong Chen"], "title": "HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation", "comment": null, "summary": "Hair transfer is increasingly valuable across domains such as social media,\ngaming, advertising, and entertainment. While significant progress has been\nmade in single-image hair transfer, video-based hair transfer remains\nchallenging due to the need for temporal consistency, spatial fidelity, and\ndynamic adaptability. In this work, we propose HairShifter, a novel \"Anchor\nFrame + Animation\" framework that unifies high-quality image hair transfer with\nsmooth and coherent video animation. At its core, HairShifter integrates a\nImage Hair Transfer (IHT) module for precise per-frame transformation and a\nMulti-Scale Gated SPADE Decoder to ensure seamless spatial blending and\ntemporal coherence. Our method maintains hairstyle fidelity across frames while\npreserving non-hair regions. Extensive experiments demonstrate that HairShifter\nachieves state-of-the-art performance in video hairstyle transfer, combining\nsuperior visual quality, temporal consistency, and scalability. The code will\nbe publicly available. We believe this work will open new avenues for\nvideo-based hairstyle transfer and establish a robust baseline in this field.", "AI": {"tldr": "本研究介绍HairShifter框架，这是一种将高质量图像头发转移与平滑一致的视频动画结合的新方法。HairShifter解决了视频头发转移中的时间一致性和空间精确度问题，实现了卓越的视觉质量和时间一致性。", "motivation": "虽然单图像头发转移已取得显著进展，但在视频中进行头发转移难度较大，因为它需要在时间一致性、空间精确度和动态适应性之间找到平衡。", "method": "在本研究中，提出了名为HairShifter的新框架，该框架结合了单帧图像头发转移模块和多尺度门控SPADE解码器，以实现高质量的空间融合和时间一致性。", "result": "大量的实验表明，HairShifter在视频发型转移中实现了最先进的性能，结合了卓越的视觉质量、时间一致性和可扩展性。", "conclusion": "这项工作的目的是为基于视频的发型转移打开新的方向，并为此领域建立一个强大的基线。"}}
{"id": "2507.13164", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13164", "abs": "https://arxiv.org/abs/2507.13164", "authors": ["Emma Sharratt", "Annelien Smith", "Retief Louw", "Daleen Klop", "Febe de Wet", "Herman Kamper"], "title": "Feature-based analysis of oral narratives from Afrikaans and isiXhosa children", "comment": "SLaTE 2025 in Nijmegen, Netherlands", "summary": "Oral narrative skills are strong predictors of later literacy development.\nThis study examines the features of oral narratives from children who were\nidentified by experts as requiring intervention. Using simple machine learning\nmethods, we analyse recorded stories from four- and five-year-old Afrikaans-\nand isiXhosa-speaking children. Consistent with prior research, we identify\nlexical diversity (unique words) and length-based features (mean utterance\nlength) as indicators of typical development, but features like articulation\nrate prove less informative. Despite cross-linguistic variation in\npart-of-speech patterns, the use of specific verbs and auxiliaries associated\nwith goal-directed storytelling is correlated with a reduced likelihood of\nrequiring intervention. Our analysis of two linguistically distinct languages\nreveals both language-specific and shared predictors of narrative proficiency,\nwith implications for early assessment in multilingual contexts.", "AI": {"tldr": "这项研究分析了被认定需要干预的四至五岁儿童的口头叙事，发现在南非语和 isiXhosa 两种语言中，词汇多样性和基于长度的特征是正常的言语发展的标志；并且特定动词和助动词的使用可以降低需要干预的可能性，这对多语言环境中早期评估有重要意义。", "motivation": "研究动机在于探索口头叙事的特征，特别是那些被专家认定需要干预的儿童。", "method": "本文采用简单的机器学习方法分析了四至五岁说南非语和 isiXhosa 的儿童所讲述的故事录音。", "result": "研究发现词汇多样性（独特的词汇）和基于长度的特征（平均言话语句长度）是正常发展的指标，而发音速率则不太具有信息量。特定动词和助动词的使用与降低需要干预的可能性相关。", "conclusion": "本研究揭示了两种语言中的语言特有和共同的叙事能力预测指标，这对多语言环境中的早期评估具有重要意义。"}}
{"id": "2507.12760", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12760", "abs": "https://arxiv.org/abs/2507.12760", "authors": ["Ruicheng Zhang", "Haowei Guo", "Kanghui Tian", "Jun Zhou", "Mingliang Yan", "Zeyu Zhang", "Shen Zhao"], "title": "Unified Medical Image Segmentation with State Space Modeling Snake", "comment": "This paper has been accepted by ACM MM 2025", "summary": "Unified Medical Image Segmentation (UMIS) is critical for comprehensive\nanatomical assessment but faces challenges due to multi-scale structural\nheterogeneity. Conventional pixel-based approaches, lacking object-level\nanatomical insight and inter-organ relational modeling, struggle with\nmorphological complexity and feature conflicts, limiting their efficacy in\nUMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state\nspace modeling for UMIS. Mamba Snake frames multi-contour evolution as a\nhierarchical state space atlas, effectively modeling macroscopic inter-organ\ntopological relationships and microscopic contour refinements. We introduce a\nsnake-specific vision state space module, the Mamba Evolution Block (MEB),\nwhich leverages effective spatiotemporal information aggregation for adaptive\nrefinement of complex morphologies. Energy map shape priors further ensure\nrobust long-range contour evolution in heterogeneous data. Additionally, a\ndual-classification synergy mechanism is incorporated to concurrently optimize\ndetection and segmentation, mitigating under-segmentation of microstructures in\nUMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's\nsuperior performance, with an average Dice improvement of 3\\% over\nstate-of-the-art methods.", "AI": {"tldr": "Mamba Snake, a novel deep snake framework with state space modeling for UMIS, improves segmentation efficacy by incorporating Mamba Evolution Block, energy map shape priors, and a dual-classification synergy mechanism, showing 3% better Dice scores.", "motivation": "The motivation is to address the challenges in UMIS due to the multi-scale structural heterogeneity, which conventional pixel-based approaches cannot effectively handle due to the lack of object-level anatomical insight and inter-organ relational modeling.", "method": "Mamba Snake, a novel deep snake framework enhanced by state space modeling, is proposed for Unified Medical Image Segmentation (UMIS). It includes a snake-specific vision state space module called Mamba Evolution Block (MEB) for adaptive refinement of complex morphologies, and uses energy map shape priors and a dual-classification synergy mechanism to optimize detection and segmentation.", "result": "Extensive evaluations across five clinical datasets revealed Mamba Snake's superior performance in UMIS, demonstrating an average Dice improvement of 3% over state-of-the-art methods.", "conclusion": "The introduction of Mamba Snake, with its innovative approach using state space modeling and various optimization mechanisms, marks a significant advancement in UMIS, enhancing the accuracy and comprehensive assessment of anatomical structures in medical images."}}
{"id": "2507.13190", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13190", "abs": "https://arxiv.org/abs/2507.13190", "authors": ["Jisoo Lee", "Raeyoung Chang", "Dongwook Kwon", "Harmanpreet Singh", "Nikhil Verma"], "title": "GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems", "comment": "4 figures, 1 algorithm, 2 tables, 6 pages, under review at EMNLP\n  Industry track 2025", "summary": "Multi-agent systems built on language models have shown strong performance on\ncollaborative reasoning tasks. However, existing evaluations focus only on the\ncorrectness of the final output, overlooking how inefficient communication and\npoor coordination contribute to redundant reasoning and higher computational\ncosts. We introduce GEMMAS, a graph-based evaluation framework that analyzes\nthe internal collaboration process by modeling agent interactions as a directed\nacyclic graph. To capture collaboration quality, we propose two process-level\nmetrics: Information Diversity Score (IDS) to measure semantic variation in\ninter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant\nreasoning paths. We evaluate GEMMAS across five benchmarks and highlight\nresults on GSM8K, where systems with only a 2.1% difference in accuracy differ\nby 12.8% in IDS and 80% in UPR, revealing substantial variation in internal\ncollaboration. These findings demonstrate that outcome-only metrics are\ninsufficient for evaluating multi-agent performance and highlight the\nimportance of process-level diagnostics in designing more interpretable and\nresource-efficient collaborative AI systems.", "AI": {"tldr": "本研究提出了GEMMAS框架，用于评估多智能体系统内部的协作过程，揭示了仅依赖结果评估的不足，并强调了过程诊断的重要性。", "motivation": "现有的评估只关注最终输出的正确性，忽视了低效的通信和不良的协调如何导致重复推理和更高的计算成本。", "method": "引入GEMMAS，这是一种基于图的评估框架，通过将代理交互建模为有向无环图来分析内部协作过程。提出了两个过程级别的度量标准：信息多样性分数（IDS），用于测量代理间消息的语义变化；无用路径比率（UPR），用于量化冗余推理路径的数量。", "result": "在GSM8K基准测试中，两个系统准确率仅相差2.1%，但在IDS和UPR上的差异分别为12.8%和80%，说明内部协作的质量存在显著差异。", "conclusion": "结果显示，仅依赖结果的度量标准不足以评估多智能体系统的性能，强调了设计更可解释和资源高效的合作AI系统时过程级诊断的重要性。"}}
{"id": "2507.12761", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12761", "abs": "https://arxiv.org/abs/2507.12761", "authors": ["Hanlei Shi", "Leyuan Qu", "Yu Liu", "Di Gao", "Yuhua Zheng", "Taihao Li"], "title": "Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation", "comment": null, "summary": "Emotional talking-head generation has emerged as a pivotal research area at\nthe intersection of computer vision and multimodal artificial intelligence,\nwith its core value lying in enhancing human-computer interaction through\nimmersive and empathetic engagement.With the advancement of multimodal large\nlanguage models, the driving signals for emotional talking-head generation has\nshifted from audio and video to more flexible text. However, current\ntext-driven methods rely on predefined discrete emotion label texts,\noversimplifying the dynamic complexity of real facial muscle movements and thus\nfailing to achieve natural emotional expressiveness.This study proposes the\nThink-Before-Draw framework to address two key challenges: (1) In-depth\nsemantic parsing of emotions--by innovatively introducing Chain-of-Thought\n(CoT), abstract emotion labels are transformed into physiologically grounded\nfacial muscle movement descriptions, enabling the mapping from high-level\nsemantics to actionable motion features; and (2) Fine-grained expressiveness\noptimization--inspired by artists' portrait painting process, a progressive\nguidance denoising strategy is proposed, employing a \"global emotion\nlocalization--local muscle control\" mechanism to refine micro-expression\ndynamics in generated videos.Our experiments demonstrate that our approach\nachieves state-of-the-art performance on widely-used benchmarks, including MEAD\nand HDTF. Additionally, we collected a set of portrait images to evaluate our\nmodel's zero-shot generation capability.", "AI": {"tldr": "研究提出了一种名为Think-Before-Draw的框架，通过引入思维链方法将抽象的情感标签转化为面部肌肉运动描述，并使用逐级引导降噪策略对其进行了优化，从而提高了生成情感说话头像的自然性和精细度。实验表明该方法在MEAD和HDTF基准测试中达到了最先进的性能。", "motivation": "当前文本驱动的情感对话头像生成方法依赖于预定义的情感标签文本，这简化了真实的面部肌肉运动复杂性，未能实现自然的情感表达。为了克服这一限制并提高生成的情感对话头像的自然性，该研究提出了这一框架。", "method": "Think-Before-Draw框架通过引入思维链方法将抽象的情感标签转化为面部肌肉运动描述，并采用类似肖像画的过程提出了逐级引导降噪策略，通过“全局情感定位-局部肌肉控制”机制优化微表情的动态。", "result": "实验结果表明该方法在MEAD和HDTF基准上达到了最先进的性能水平，并通过一组肖像图片验证了此模型的零样本生成能力。", "conclusion": "该研究提出的方法通过深入语义解析情感和精细表达能力优化，提高了文本驱动情感对话头像生成的自然性和动态精细度。"}}
{"id": "2507.13205", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13205", "abs": "https://arxiv.org/abs/2507.13205", "authors": ["R. Louw", "E. Sharratt", "F. de Wet", "C. Jacobs", "A. Smith", "H. Kamper"], "title": "Automatically assessing oral narratives of Afrikaans and isiXhosa children", "comment": "Accepted to SLaTE 2025", "summary": "Developing narrative and comprehension skills in early childhood is critical\nfor later literacy. However, teachers in large preschool classrooms struggle to\naccurately identify students who require intervention. We present a system for\nautomatically assessing oral narratives of preschool children in Afrikaans and\nisiXhosa. The system uses automatic speech recognition followed by a machine\nlearning scoring model to predict narrative and comprehension scores. For\nscoring predicted transcripts, we compare a linear model to a large language\nmodel (LLM). The LLM-based system outperforms the linear model in most cases,\nbut the linear system is competitive despite its simplicity. The LLM-based\nsystem is comparable to a human expert in flagging children who require\nintervention. We lay the foundation for automatic oral assessments in\nclassrooms, giving teachers extra capacity to focus on personalised support for\nchildren's learning.", "AI": {"tldr": "研究开发了一个自动评估学前儿童口头叙述能力的系统，该系统在预测分数方面优于线性模型，并可与人类专家匹配以识别需要干预的儿童。", "motivation": "研究旨在帮助教师准确识别需要干预的学前儿童，解决大规模幼儿园教室中的教学难题。", "method": "该系统使用自动语音识别技术，然后通过机器学习评分模型来预测叙述和理解分数。该研究比较了线性模型和大型语言模型（LLM）的评分预测效果。", "result": "大型语言模型（LLM）系统在大多数情况下优于线性模型，能与人类专家一样准确地识别需要干预的儿童。", "conclusion": "研究为教室中的自动口头评估奠定了基础，使教师有更多能力提供个性化的学习支持。"}}
{"id": "2507.12762", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12762", "abs": "https://arxiv.org/abs/2507.12762", "authors": ["Yanchen Guan", "Haicheng Liao", "Chengyue Wang", "Xingcheng Liu", "Jiaxun Zhang", "Zhenning Li"], "title": "World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving", "comment": null, "summary": "Reliable anticipation of traffic accidents is essential for advancing\nautonomous driving systems. However, this objective is limited by two\nfundamental challenges: the scarcity of diverse, high-quality training data and\nthe frequent absence of crucial object-level cues due to environmental\ndisruptions or sensor deficiencies. To tackle these issues, we propose a\ncomprehensive framework combining generative scene augmentation with adaptive\ntemporal reasoning. Specifically, we develop a video generation pipeline that\nutilizes a world model guided by domain-informed prompts to create\nhigh-resolution, statistically consistent driving scenarios, particularly\nenriching the coverage of edge cases and complex interactions. In parallel, we\nconstruct a dynamic prediction model that encodes spatio-temporal relationships\nthrough strengthened graph convolutions and dilated temporal operators,\neffectively addressing data incompleteness and transient visual noise.\nFurthermore, we release a new benchmark dataset designed to better capture\ndiverse real-world driving risks. Extensive experiments on public and newly\nreleased datasets confirm that our framework enhances both the accuracy and\nlead time of accident anticipation, offering a robust solution to current data\nand modeling limitations in safety-critical autonomous driving applications.", "AI": {"tldr": "本文提出一种综合框架，通过生成增强学习和改进的时间推理技术，显著提升了自动驾驶系统的事故预测能力和时间，并发布了一个新的数据分析集。", "motivation": "当前的自动驾驶系统在交通事故预测方面面临两个根本挑战：高质量多样训练数据的稀缺性以及由于环境干扰或传感器缺陷导致的关键对象级别线索的缺失。本文旨在通过提出一种新的框架来解决这些问题。", "method": "本研究提出了一种结合生成场景增强与自适应时间推理的综合框架。首先，通过世界模型，基于领域引导提示，开发了一种视频生成流程，以创建高分辨率的、统计一致的驾驶场景，特别是丰富了边缘情况和复杂交互的覆盖。其次，构建了一个动态预测模型，通过加强的图卷积和膨胀的时间运算符编码时空关系，有效处理了数据不完整性和暂时的视觉噪声。此外，还发布了一个新的基准数据集，旨在更好地捕捉多样化的驾驶风险。", "result": "实验结果表明，本研究提出的框架在公共数据集和新发布数据集上大大提高了事故预测的准确性和预测时间。", "conclusion": "研究证明了所提出框架解决当前自动驾驶应用中数据和建模限制的能力，并提供了一种应对安全性关键应用的强健解决方案。"}}
{"id": "2507.13236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13236", "abs": "https://arxiv.org/abs/2507.13236", "authors": ["Xinyu Tang", "Zhihao Lv", "Xiaoxue Cheng", "Junyi Li", "Wayne Xin Zhao", "Zujie Wen", "Zhiqiang Zhang", "Jun Zhou"], "title": "Enhancing Cross-task Transfer of Large Language Models via Activation Steering", "comment": null, "summary": "Large language models (LLMs) have shown impressive abilities in leveraging\npretrained knowledge through prompting, but they often struggle with unseen\ntasks, particularly in data-scarce scenarios. While cross-task in-context\nlearning offers a direct solution for transferring knowledge across tasks, it\nstill faces critical challenges in terms of robustness, scalability, and\nefficiency. In this paper, we investigate whether cross-task transfer can be\nachieved via latent space steering without parameter updates or input\nexpansion. Through an analysis of activation patterns in the latent space of\nLLMs, we observe that the enhanced activations induced by in-context examples\nhave consistent patterns across different tasks. Inspired by these findings, we\npropose CAST, a novel Cross-task Activation Steering Transfer framework that\nenables effective transfer by manipulating the model's internal activation\nstates. Our approach first selects influential and diverse samples from\nhigh-resource tasks, then utilizes their contrastive representation-enhanced\nactivations to adapt LLMs to low-resource tasks. Extensive experiments across\nboth cross-domain and cross-lingual transfer settings show that our method\noutperforms competitive baselines and demonstrates superior scalability and\nlower computational costs.", "AI": {"tldr": "研究团队提出了一种名为CAST的新方法，通过分析和操作大型语言模型的潜在空间中的激活模式，来有效地实现跨任务的知识转移，并展示了在跨领域和跨语言场景中超过现有方法的效果。", "motivation": "大型语言模型（LLMs）虽然在通过提示来激发预训练知识方面表现出色，但在未见任务，特别是在数据稀缺的情况下，往往存在困难。尽管跨任务上下文学习为任务间的知识转移提供了直接解决方案，但该方法在鲁棒性、可扩展性和效率方面仍面临巨大挑战。", "method": "通过分析大型语言模型（LLMs）在任务之间进行跨任务传输时的潜在空间中的激活模式，研究团队观察到由同场景示例诱导的增强激活在不同任务中具有持续的模式。基于这些发现，他们提出了一种名为CAST的新方法，即跨任务激活引导传输框架，通过操作模型的内部激活状态来实现有效的传输。该方法首先从高资源任务中选择具有影响力且多样性的样本，然后利用这些样本的对比表示增强激活来适应低资源任务中的LLMs。", "result": "实验结果显示，该方法在跨领域和跨语言传输设置下表现优于竞争基线，展示了优良的可扩展性和更低的计算成本。", "conclusion": "研究结果表明，通过利用高资源任务的样本并通过操作模型内部的激活状态来适应低资源任务，可以在不进行参数更新或输入扩增的情况下实现高效、可靠的跨任务传输。这种方法相对于其他方法而言，不仅能提供更好的性能，还表现出更强的可扩展性和较低的计算代价。"}}
{"id": "2507.12763", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12763", "abs": "https://arxiv.org/abs/2507.12763", "authors": ["Heegyeong Kim", "Alice James", "Avishkar Seth", "Endrowednes Kuantama", "Jane Williamson", "Yimeng Feng", "Richard Han"], "title": "Continuous Marine Tracking via Autonomous UAV Handoff", "comment": "6 pages, 5 figures, to be published in DroNet '25: Proceedings of the\n  10th Workshop on Micro Aerial Vehicle Networks, Systems, and Applications", "summary": "This paper introduces an autonomous UAV vision system for continuous,\nreal-time tracking of marine animals, specifically sharks, in dynamic marine\nenvironments. The system integrates an onboard computer with a stabilised RGB-D\ncamera and a custom-trained OSTrack pipeline, enabling visual identification\nunder challenging lighting, occlusion, and sea-state conditions. A key\ninnovation is the inter-UAV handoff protocol, which enables seamless transfer\nof tracking responsibilities between drones, extending operational coverage\nbeyond single-drone battery limitations. Performance is evaluated on a curated\nshark dataset of 5,200 frames, achieving a tracking success rate of 81.9\\%\nduring real-time flight control at 100 Hz, and robustness to occlusion,\nillumination variation, and background clutter. We present a seamless UAV\nhandoff framework, where target transfer is attempted via high-confidence\nfeature matching, achieving 82.9\\% target coverage. These results confirm the\nviability of coordinated UAV operations for extended marine tracking and lay\nthe groundwork for scalable, autonomous monitoring.", "AI": {"tldr": "该研究介绍了一种自主无人机视觉系统，专门针对在动态海洋环境中连续实时跟踪鲨鱼。系统成功展示了在复杂条件下的跟踪能力和无人机间的任务交接，为扩展的海洋动物跟踪提供了可能。", "motivation": "研究动机在于解决动态海洋环境中对鲨鱼进行连续实时跟踪的挑战，并扩展无人机的运行范围。这项研究为可扩展的自主监测奠定了基础，并证实了协同无人机操作用于延长海洋跟踪的可行性。", "method": "介绍了一种用于在动态海洋环境中连续实时跟踪海洋动物（特别是鲨鱼）的自主UAV视觉系统。该系统集成了机载计算机、稳定化的RGB-D摄像头和一个定制训练的OSTrack管道，在具有挑战性的光照、遮挡和海况条件下实现视觉识别。关键创新是无人机之间的跟踪任务交接协议，它可以在单个无人机的电池限制之外扩展操作范围。", "result": "在精心挑选的鲨鱼数据集（共5200帧）上进行了性能评估，实际飞行控制期间的跟踪成功率达到了81.9%，抗遮挡、光照变化和背景杂波的鲁棒性也得到了验证。此外，通过高置信度特征匹配实现目标转移，目标覆盖率达到了82.9%。", "conclusion": "该研究为通过协同无人机操作进行延长的海洋跟踪提供了可能性，并为可扩展的自主监测奠定了基础。"}}
{"id": "2507.13238", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13238", "abs": "https://arxiv.org/abs/2507.13238", "authors": ["Ashray Gupta", "Rohan Joseph", "Sunny Rai"], "title": "HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models", "comment": null, "summary": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.", "AI": {"tldr": "本文提出了一个用于评估多语言大语言模型（LLM）在印地语中推理能力的新测试集（HATS），并引入了一种新的基于认知理论的类比推理思维链方法。实验显示，无论提示策略如何，英文提示都使模型表现最佳。", "motivation": "大语言模型（LLMs）在英语推理能力方面被广泛评估，但在印度语言中的推理能力研究不足，影响了我们对这些模型跨语言泛化能力的理解。为此，本论文致力于填补这一空白。", "method": "引入了一个新的印地语类比测试集（HATS），包含405个多选题，这些题目源自印度政府考试。通过各种提示策略评估了最先进的多语言大语言模型，并引入了一种基于认知理论的类比推理思维链方法，以提高模型在印地语类比题目上的性能。", "result": "实验表明，使用英文提示时，模型在印地语类比题目的表现最好，无论采用哪种提示策略。", "conclusion": "本论文通过引入HATS解决了缺乏关键资源来评估LLM在印地语中的推理能力的问题，提出的方法改善了模型在印地语类比题目上的表现。"}}
{"id": "2507.12768", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12768", "abs": "https://arxiv.org/abs/2507.12768", "authors": ["Hengkai Tan", "Yao Feng", "Xinyi Mao", "Shuhe Huang", "Guodong Liu", "Zhongkai Hao", "Hang Su", "Jun Zhu"], "title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation", "comment": null, "summary": "Vision-language-action (VLA) models have shown promise on task-conditioned\ncontrol in complex settings such as bimanual manipulation. However, the heavy\nreliance on task-specific human demonstrations limits their generalization and\nincurs high data acquisition costs. In this work, we present a new notion of\ntask-agnostic action paradigm that decouples action execution from\ntask-specific conditioning, enhancing scalability, efficiency, and\ncost-effectiveness. To address the data collection challenges posed by this\nparadigm -- such as low coverage density, behavioral redundancy, and safety\nrisks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a\nscalable self-supervised framework that accelerates collection by over $\n30\\times $ compared to human teleoperation. To further enable effective\nlearning from task-agnostic data, which often suffers from distribution\nmismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics\nmodel equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder\n(DAD). We additionally integrate a video-conditioned action validation module\nto verify the feasibility of learned policies across diverse manipulation\ntasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51%\nimprovement in test accuracy and achieves 30-40% higher success rates in\ndownstream tasks such as lifting, pick-and-place, and clicking, using\nreplay-based video validation. Project Page:\nhttps://embodiedfoundation.github.io/vidar_anypos", "AI": {"tldr": "该研究提出了一种新的不依赖任务的动作范式来解决数据收集难题，提高了复杂操作任务中的表现。", "motivation": "传统视觉语言动作模型在复杂场景中表现出色，但其大量依赖于特定任务的人类演示，限制了泛化能力，并且增加了数据收集成本。因此，该研究旨在通过提出一个新的不依赖任务的动作范式来解决这些问题，提高效率并降低成本。", "method": "该研究提出了一个新的不依赖任务的动作范式ATARA（Automated Task-Agnostic Random Actions），该范式可以加速数据收集过程，同时引入了AnyPos，一个配备有肢解耦估计和方向感知解码器的逆动力学模型，以及一个基于视频条件的动作验证模块以验证学习策略的可行性。", "result": "实验结果显示，AnyPos- ATARA流水线在测试准确性上提高了51％，在抓取、放置和点击等下游任务的成功率上提高了30-40％。", "conclusion": "通过该新范式及其工具的使用，可以高效地从不依赖任务的数据中学习，从而提高在多种操作任务中的表现。"}}
{"id": "2507.13255", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13255", "abs": "https://arxiv.org/abs/2507.13255", "authors": ["Lyucheng Wu", "Mengru Wang", "Ziwen Xu", "Tri Cao", "Nay Oo", "Bryan Hooi", "Shumin Deng"], "title": "Automating Steering for Safe Multimodal Large Language Models", "comment": "Working in progress. 22 pages (8+ for main); 25 figures; 1 table", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.", "AI": {"tldr": "该研究介绍了一种名为AutoSteer的推理时间干预技术，用于提高多模态大型语言模型的安全性，同时保持其一般能力。实验显示，AutoSteer显著降低了文本、视觉和跨模态威胁的攻击成功率。", "motivation": "由于多模态大型语言模型（MLLMs）在面对对抗性多模态输入时提出了新的安全问题，该研究旨在通过引入AutoSteer技术提高MLLMs在推理过程中的安全性，而无需对底层模型进行微调。", "method": "AutoSteer 技术包含三个核心组件：一种新型的安全意识评分（SAS），自适应安全探测器，以及轻量级拒绝头。这些组件协同工作，以在检测到安全风险时调节生成过程。", "result": "实验表明，AutoSteer在减少了文本、视觉和跨模态威胁的攻击成功率的同时，还保持了模型的一般能力。", "conclusion": "该研究提出的方法可作为一个实用、可解释且有效的框架，用于多模态AI系统的安全部署。"}}
