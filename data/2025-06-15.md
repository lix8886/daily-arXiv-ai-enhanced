<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 4]
- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
*Tian Lan,Yang-Hao Zhou,Zi-Ao Ma,Fanshu Sun,Rui-Qing Sun,Junyu Luo,Rong-Cheng Tu,Heyan Huang,Chen Xu,Zhijing Wu,Xian-Ling Mao*

Main category: cs.CL

论文概述：本文提出了一个跨模态的生成内容自动评估方法的全面回顾和统一分类法，适用于文本、图像和音频生成领域，旨在解决当前缺乏系统性评估框架的问题。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于解决自动评估生成内容质量的问题，尤其是在文本、图像和音频生成领域缺乏系统性的评估框架。

Method: 分析论文中的方法部分，本文提出了一个跨模态生成内容自动评估方法的全面回顾和统一分类法。

Result: 本文确立了一个用于跨模态生成内容的评估方法的全面回顾和统一分类，并将其应用扩展到了文本、图像和音频生成领域。

Conclusion: 研究结论是提出了一个适用于文本、图像和音频生成内容的评估框架，并确定了五种基础范式来描述这些领域的现有评估方法，为未来跨模态评估方法的研究提供了方向。

Abstract: Recent advances in deep learning have significantly enhanced generative AI
capabilities across text, images, and audio. However, automatically evaluating
the quality of these generated outputs presents ongoing challenges. Although
numerous automatic evaluation methods exist, current research lacks a
systematic framework that comprehensively organizes these methods across text,
visual, and audio modalities. To address this issue, we present a comprehensive
review and a unified taxonomy of automatic evaluation methods for generated
content across all three modalities; We identify five fundamental paradigms
that characterize existing evaluation approaches across these domains. Our
analysis begins by examining evaluation methods for text generation, where
techniques are most mature. We then extend this framework to image and audio
generation, demonstrating its broad applicability. Finally, we discuss
promising directions for future research in cross-modal evaluation
methodologies.

</details>


### [2] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
*Dingfeng Shi,Jingyi Cao,Qianben Chen,Weichen Sun,Weizhen Li,Hongxuan Lu,Fangchen Dong,Tianrui Qin,King Zhu,Minghao Yang,Jian Yang,Ge Zhang,Jiaheng Liu,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

本文提出了TaskCraft，一种自动生成智能任务的工作流，用于改进NLP和AI中的多步骤问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 高阶任务需要自主、工具使用和适应性推理等多步骤问题解决能力，它们越来越成为促进NLP和AI发展的核心。然而，现有的指令数据缺乏工具交互，而现有的高阶任务基准依赖于昂贵的人工标注，限制了其可扩展性。

Method: 引入了TaskCraft，这是一个可以自动生成难度可调、多工具协作且可以验证的智能任务的工作流。TaskCraft通过基于深度和宽度的扩展来扩展原子任务，创造出结构复杂和层次化的挑战。

Result: 实验证明这些任务可以改善生成工作流中的提示优化，并提高智能基础模型的监督微调效果。

Conclusion: 该研究提供了一个包含大约36,000个不同难度任务的大规模合成数据集，以支持未来的智能调优和评估研究。

Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool
use, and adaptive reasoning, are becoming increasingly central to the
advancement of NLP and AI. However, existing instruction data lacks tool
interaction, and current agentic benchmarks rely on costly human annotation,
limiting their scalability. We introduce \textsc{TaskCraft}, an automated
workflow for generating difficulty-scalable, multi-tool, and verifiable agentic
tasks with execution trajectories. TaskCraft expands atomic tasks using
depth-based and width-based extensions to create structurally and
hierarchically complex challenges. Empirical results show that these tasks
improve prompt optimization in the generation workflow and enhance supervised
fine-tuning of agentic foundation models. We present a large-scale synthetic
dataset of approximately 36,000 tasks with varying difficulty to support future
research on agent tuning and evaluation.

</details>


### [3] [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
*Christopher J. Agostino,Quan Le Thien,Molly Apsel,Denizhan Pak,Elina Lesyk,Ashabari Majumdar*

Main category: cs.CL

论文探讨了自然语言中的语义退化现象，指出其对于语言模型和NLP系统解释能力的限制。通过语义贝尔不等式测试，得出语言解释在模棱两可的上下文中表现出非经典的上下文性，意味着传统的基于频率的方法在分析自然语言时会丢失信息。作者建议采用贝叶斯重复采样的方法来更好描述语言意义。


<details>
  <summary>Details</summary>
Motivation: 探讨自然语言的语义退化现象对于大型语言模型（LLM）和其他现代NLP系统来说，构成了固有的局限性。因为它们操作在自然语言之内，同样受到语义退化带来的解释限制。

Method: 通过Kolmogorov复杂性理论论证，随着表达复杂度的增加，任何解释代理（人类或LLM驱动的AI）恢复单一预期含义的可能性会消失。为此，进行了语义贝尔不等式测试，使用不同LLM代理作为‘计算认知系统’来解释在不同上下文设置下的模棱两可的词对。

Result: 在多个独立实验中，平均CHSH期望值范围从1.2到2.8，有些运行结果（例如2.3-2.4）显著违反了经典界限(|S|≤2)。这表明在模棱两可是的语境下，语言解释可以表现出非经典的上下文性，与人类认知实验的结果一致。

Conclusion: 研究结果表明，由于语义退化现象的存在，自然语言的意义不是固定的，而是通过观察者依赖的解释行为实现的。因此，提出使用贝叶斯采样方法来更适合地表征语言意义。

Abstract: Semantic degeneracy represents a fundamental property of natural language
that extends beyond simple polysemy to encompass the combinatorial explosion of
potential interpretations that emerges as semantic expressions increase in
complexity. Large Language Models (LLMs) and other modern NLP systems face
inherent limitations precisely because they operate within natural language
itself, making them subject to the same interpretive constraints imposed by
semantic degeneracy. In this work, we argue using Kolmogorov complexity that as
an expression's complexity grows, the likelihood of any interpreting agent
(human or LLM-powered AI) recovering the single intended meaning vanishes. This
computational intractability suggests the classical view that linguistic forms
possess meaning in and of themselves is flawed. We alternatively posit that
meaning is instead actualized through an observer-dependent interpretive act.
To test this, we conducted a semantic Bell inequality test using diverse LLM
agents as ``computational cognitive systems'' to interpret ambiguous word pairs
under varied contextual settings. Across several independent experiments, we
found average CHSH expectation values ranging from 1.2 to 2.8, with several
runs yielding values (e.g., 2.3-2.4) that significantly violate the classical
boundary ($|S|\leq2$). This demonstrates that linguistic interpretation under
ambiguity can exhibit non-classical contextuality, consistent with results from
human cognition experiments. These results inherently imply that classical
frequentist-based analytical approaches for natural language are necessarily
lossy. Instead, we propose that Bayesian-style repeated sampling approaches can
provide more practically useful and appropriate characterizations of linguistic
meaning in context.

</details>


### [4] [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
*Christodoulos Constantinides,Shuxin Lin,Nianjun Zhou,Dhaval Patel*

Main category: cs.CL

本文提出了一种名为Chat-of-Thought的多代理系统，用于优化工业资产的FMEA文档生成过程。系统通过多角色驱动的讨论和高级AI技术提高了文档生成和验证的效率。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索工业设备监测的应用领域，突出关键挑战，并通过交互式、模板驱动的工作流程和上下文感知的代理协作，展示Chat-of-Thought系统在解决这些挑战方面的潜力。

Method: 本文提出了一种名为Chat-of-Thought的新颖多代理系统，旨在为工业资产生成故障模式与影响分析（FMEA）文档。该系统采用多个具有特定角色的大语言模型（LLM）代理，并利用高级AI技术和动态任务路由来优化FMEA表格的生成和验证。其中一个关键创新点是提出了基于动态多角色驱动讨论的思考对话（Chat of Thought）机制，使内容能够通过迭代方式得到完善。

Result: 虽然摘要中没有具体列出实验结果，但研究表明Chat-of-Thought系统能够通过其独特的多代理协作架构和思考对话机制在生成FMEA文档上取得显著进步。

Conclusion: 研究结论表明Chat-of-Thought系统在解决工业监测领域的挑战上有较大的潜力，尤其是在生成FMEA文档及通过上下文感知的代理协作完成复杂任务方面。

Abstract: This paper presents a novel multi-agent system called Chat-of-Thought,
designed to facilitate the generation of Failure Modes and Effects Analysis
(FMEA) documents for industrial assets. Chat-of-Thought employs multiple
collaborative Large Language Model (LLM)-based agents with specific roles,
leveraging advanced AI techniques and dynamic task routing to optimize the
generation and validation of FMEA tables. A key innovation in this system is
the introduction of a Chat of Thought, where dynamic, multi-persona-driven
discussions enable iterative refinement of content. This research explores the
application domain of industrial equipment monitoring, highlights key
challenges, and demonstrates the potential of Chat-of-Thought in addressing
these challenges through interactive, template-driven workflows and
context-aware agent collaboration.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [5] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

该研究介绍了一种从文本输入自动生成60秒电影的方法，使用了Stable Diffusion、GPT-2和混合音频管道技术，提供了专业的视频质量和故事连贯性。


<details>
  <summary>Details</summary>
Motivation: 鉴于生成式人工智能的进步，这项工作旨在通过结合先进的图像、叙述和音频技术，从文本输入创建具有高质量的动态视频，以支持创意、教育和工业应用。

Method: 提出的方法包括利用Stable Diffusion进行高质量图像合成，GPT-2构建叙述结构，以及结合作文语音合成和YouTube音乐的混合音频管道，通过线性帧插入、后期制作和音频视频同步在Google Colab上实现此过程。

Result: 实验表明该方法在视觉质量和故事连贯性方面表现出色，并已在简单的GUI和高级模式的不同分辨率和帧率下进行了测试和优化。

Conclusion: 该方法展示了生成文本到视频合成的强大能力，适用于各种应用，包括但不限于创意视频制作、教育资源开发和商业内容创作。

Abstract: Advances in generative artificial intelligence have altered multimedia
creation, allowing for automatic cinematic video synthesis from text inputs.
This work describes a method for creating 60-second cinematic movies
incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for
narrative structuring, and a hybrid audio pipeline using gTTS and
YouTube-sourced music. It uses a five-scene framework, which is augmented by
linear frame interpolation, cinematic post-processing (e.g., sharpening), and
audio-video synchronization to provide professional-quality results. It was
created in a GPU-accelerated Google Colab environment using Python 3.11. It has
a dual-mode Gradio interface (Simple and Advanced), which supports resolutions
of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA
memory management and error handling ensure reliability. The experiments
demonstrate outstanding visual quality, narrative coherence, and efficiency,
furthering text-to-video synthesis for creative, educational, and industrial
applications.

</details>


### [6] [LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning](https://arxiv.org/abs/2506.10082)
*Chenjian Gao,Lihe Ding,Xin Cai,Zhanpeng Huang,Zibin Wang,Tianfan Xue*

Main category: cs.CV

A new method uses mask-based LoRA tuning to adaptively edit videos, offering control over content propagation without changing the model's architecture.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide more flexibility in video editing beyond the limitations of existing methods, which often require large-scale pretraining and lack control over edits beyond the first frame.

Method: Our approach involves a mask-based LoRA tuning method that allows the adaptation of pre-trained Image-to-Video models for specific video edits, preserving background regions while allowing for controllable alterations.

Result: Experimental results demonstrate superior performance over current state-of-the-art methods.

Conclusion: The mask-based LoRA tuning method enables efficient and adaptable video editing, providing enhanced control over video content propagation.

Abstract: Video editing using diffusion models has achieved remarkable results in
generating high-quality edits for videos. However, current methods often rely
on large-scale pretraining, limiting flexibility for specific edits.
First-frame-guided editing provides control over the first frame, but lacks
flexibility over subsequent frames. To address this, we propose a mask-based
LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video
(I2V) models for flexible video editing. Our approach preserves background
regions while enabling controllable edits propagation. This solution offers
efficient and adaptable video editing without altering the model architecture.
To better steer this process, we incorporate additional references, such as
alternate viewpoints or representative scene states, which serve as visual
anchors for how content should unfold. We address the control challenge using a
mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model
to the editing context. The model must learn from two distinct sources: the
input video provides spatial structure and motion cues, while reference images
offer appearance guidance. A spatial mask enables region-specific learning by
dynamically modulating what the model attends to, ensuring that each area draws
from the appropriate source. Experimental results show our method achieves
superior video editing performance compared to state-of-the-art methods.

</details>


### [7] [DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding](https://arxiv.org/abs/2506.10084)
*Bin Guo,John H. L. Hansen*

Main category: cs.CV

DeepTraverse是基于经典搜索算法设计的新视觉架构，它通过递归探索和自适应校准来构建和优化特征。实验表明在保持或超过传统模型的性能的同时，提高特征分类的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉骨干网络通过一系列操作构造特征，并且缺乏明确的适应性和迭代修正的路径。研究者提出了一种新的方法，试图通过引入经典搜索算法的原则来改进这一点。

Method: 该研究提出了名为DeepTraverse的新视觉架构，灵感来自经典的搜索算法。DeepTraverse包括两个关键组成部分：递归探索模块，通过共享参数不断深化特征分析；自适应校准模块，根据不断变化的全局上下文动态调整特征的重要性。

Result: 实验结果显示，DeepTraverse在多样化的图像分类基准测试中取得了有竞争力的分类准确性，并且在特征区分度方面通常优于参数量类似甚至更多的传统模型。

Conclusion: 这项研究表明，通过集成算法性的先验知识，可以有效地构建出更高效、性能更好且结构更清晰的视觉骨干网络。

Abstract: Conventional vision backbones, despite their success, often construct
features through a largely uniform cascade of operations, offering limited
explicit pathways for adaptive, iterative refinement. This raises a compelling
question: can principles from classical search algorithms instill a more
algorithmic, structured, and logical processing flow within these networks,
leading to representations built through more interpretable, perhaps
reasoning-like decision processes? We introduce DeepTraverse, a novel vision
architecture directly inspired by algorithmic search strategies, enabling it to
learn features through a process of systematic elucidation and adaptive
refinement distinct from conventional approaches. DeepTraverse operationalizes
this via two key synergistic components: recursive exploration modules that
methodically deepen feature analysis along promising representational paths
with parameter sharing for efficiency, and adaptive calibration modules that
dynamically adjust feature salience based on evolving global context. The
resulting algorithmic interplay allows DeepTraverse to intelligently construct
and refine feature patterns. Comprehensive evaluations across a diverse suite
of image classification benchmarks show that DeepTraverse achieves highly
competitive classification accuracy and robust feature discrimination, often
outperforming conventional models with similar or larger parameter counts. Our
work demonstrates that integrating such algorithmic priors provides a
principled and effective strategy for building more efficient, performant, and
structured vision backbones.

</details>


### [8] [Test-Time Adaptation for Generalizable Task Progress Estimation](https://arxiv.org/abs/2506.10085)
*Christos Ziakas,Alessandra Russo*

Main category: cs.CV

本文提出了一种基于元学习的测试时适应方法，用于改进基于语义内容的进度估计，该方法在分布外任务中优于现有的上下文学习方法。


<details>
  <summary>Details</summary>
Motivation: 改进现有的进度估计模型的泛化能力，使其能够适应视觉和时间上下文的变化，从而提高在分布外场景中的性能。

Method: 我们提出了一种测试时适应方法，该方法可以使进度估计模型通过优化一个学习的自监督目标，在线适应测试轨迹的视觉和时间上下文。为此，我们引入了一种基于梯度的元学习策略，用于训练模型，使其能够从专家的视觉轨迹及其自然语言任务描述中学习，从而在测试时适应性改进基于语义内容而不是时间顺序的进度估计。

Result: 我们的测试时适应方法能够从单个训练环境泛化到多样化的分布外任务、环境和身体形态上，超越了使用自回归视觉-语言模型的当前最先进的上下文学习方法。

Conclusion: 提出的测试时适应方法不仅提高了模型对语义内容的理解，还展现了在多样化的未见过的任务和环境中的优越表现，证明了该方法的有效性和优越性。

Abstract: We propose a test-time adaptation method that enables a progress estimation
model to adapt online to the visual and temporal context of test trajectories
by optimizing a learned self-supervised objective. To this end, we introduce a
gradient-based meta-learning strategy to train the model on expert visual
trajectories and their natural language task descriptions, such that test-time
adaptation improves progress estimation relying on semantic content over
temporal order. Our test-time adaptation method generalizes from a single
training environment to diverse out-of-distribution tasks, environments, and
embodiments, outperforming the state-of-the-art in-context learning approach
using autoregressive vision-language models.

</details>


### [9] [EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models](https://arxiv.org/abs/2506.10100)
*Yantai Yang,Yuhao Wang,Zichen Wen,Luo Zhongwei,Chang Zou,Zhipeng Zhang,Chuan Wen,Linfeng Zhang*

Main category: cs.CV

EfficientVLA是一种用于视觉-语言-动作模型的高效推理加速框架，通过整合三种策略系统性地消除冗余，实现加速和减少计算量的目标。


<details>
  <summary>Details</summary>
Motivation: 针对视觉-语言-动作模型(VLA)高计算和内存需求的瓶颈问题，现有加速方法往往只针对孤立的效率问题，无法全面解决整个VLA流水线中的计算和内存瓶颈。

Method: 提出了一种名为EfficientVLA的加速框架，该框架通过三种策略系统地消除冗余：1. 根据层间冗余性分析对语言模块进行剪枝操作；2. 通过任务感知策略优化视觉处理路径，选择一组紧凑、多样化的视觉标记；3. 在迭代扩散动作头中缓存并复用关键中间特征，以减少时间上的计算冗余。

Result: 将该方法应用于标准VLA模型CogACT，获得了1.93倍的推理加速，FLOPs降低到28.9%，在SIMPLER基准测试中的成功率下降仅0.6%。

Conclusion: EfficientVLA框架证明了其能够有效地减少VLA模型中的计算和内存需求，同时保持较高成功率，有助于这些模型在实际中的部署。

Abstract: Vision-Language-Action (VLA) models, particularly diffusion-based
architectures, demonstrate transformative potential for embodied intelligence
but are severely hampered by high computational and memory demands stemming
from extensive inherent and inference-time redundancies. While existing
acceleration efforts often target isolated inefficiencies, such piecemeal
solutions typically fail to holistically address the varied computational and
memory bottlenecks across the entire VLA pipeline, thereby limiting practical
deployability. We introduce EfficientVLA, a structured and training-free
inference acceleration framework that systematically eliminates these barriers
by cohesively exploiting multifaceted redundancies. EfficientVLA
synergistically integrates three targeted strategies: (1) pruning of
functionally inconsequential layers from the language module, guided by an
analysis of inter-layer redundancies; (2) optimizing the visual processing
pathway through a task-aware strategy that selects a compact, diverse set of
visual tokens, balancing task-criticality with informational coverage; and (3)
alleviating temporal computational redundancy within the iterative
diffusion-based action head by strategically caching and reusing key
intermediate features. We apply our method to a standard VLA model CogACT,
yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%
success rate drop in the SIMPLER benchmark.

</details>


### [10] [A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild](https://arxiv.org/abs/2506.10117)
*Klim Kireev,Ana-Maria Creţu,Raphael Meier,Sarah Adel Bargal,Elissa Redmiles,Carmela Troncoso*

Main category: cs.CV

论文发布了一个基准测试数据集ICCWD，用于检测图像中的未成年人，并表明儿童检测是一个挑战性的任务。


<details>
  <summary>Details</summary>
Motivation: 平台和法律对未成年人（定义为18岁以下的个人）的内容与其他类型的内容的监管有所不同。鉴于需要评估的内容量巨大，通常使用基于机器学习的自动化工具来检测描绘未成年人的内容。该论文旨在填补现有数据集中没有用于在多模态环境中检测这些识别方法的空白。

Method: 该论文发布了名为Image-Caption Children in the Wild Dataset (ICCWD) 的数据集，用于基准测试检测未成年人描绘的工具。数据集包含10,000个图像-标题对，手动标注以指示图像中是否存在儿童。

Result: 使用该数据集对三种不同的检测器进行了基准测试，包括应用于图像的商业年龄估算系统。结果显示，儿童检测是一项具有挑战性的任务，最好的方法实现了75.3%的真正例率。

Conclusion: 该数据集的发布有助于设计更有效的未成年人检测方法，并适用于各种场景。

Abstract: Platforms and the law regulate digital content depicting minors (defined as
individuals under 18 years of age) differently from other types of content.
Given the sheer amount of content that needs to be assessed, machine
learning-based automation tools are commonly used to detect content depicting
minors. To our knowledge, no dataset or benchmark currently exists for
detecting these identification methods in a multi-modal environment. To fill
this gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an
image-caption dataset aimed at benchmarking tools that detect depictions of
minors. Our dataset is richer than previous child image datasets, containing
images of children in a variety of contexts, including fictional depictions and
partially visible bodies. ICCWD contains 10,000 image-caption pairs manually
labeled to indicate the presence or absence of a child in the image. To
demonstrate the possible utility of our dataset, we use it to benchmark three
different detectors, including a commercial age estimation system applied to
images. Our results suggest that child detection is a challenging task, with
the best method achieving a 75.3% true positive rate. We hope the release of
our dataset will aid in the design of better minor detection methods in a wide
range of scenarios.

</details>


### [11] [Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers](https://arxiv.org/abs/2506.10119)
*Natanael Lucena,Fábio S. da Silva,Ricardo Rios*

Main category: cs.CV

本文对比了卷积神经网络（CNN）和视觉变换器（ViT）在多分类银屑病及其类似疾病图像任务中的性能。模型在ImageNet上预训练，并适应特定数据集。ViT因其在较小模型中的优越性能而突出。双注意力视觉变换器-基础（DaViT-B）效果最佳，F1分数为96.4%，推荐为自动银屑病检测最有效的架构。


<details>
  <summary>Details</summary>
Motivation: 探讨卷积神经网络（CNN）与视觉变换器（ViT）在医学图像分类中的相对性能，特别是针对银屑病及其相似疾病的自动检测。

Method: 采用预训练在ImageNet上的CNN和ViT模型，并适应特定的皮肤病图像数据集。测试相对性能，比较各类模型在分类任务上的表现。

Result: 两种模型都取得了较高的预测指标，但ViT以其更小的模型尺寸实现了更优效果。其中，双注意力视觉变换器-基础（DaViT-B）达到了96.4%的F1分数，性能最佳。

Conclusion: 研究表明，ViT在医学图像分类任务中具有巨大潜力，尤其是在处理如银屑病这类皮肤病图像的自动检测中，展示了优于CNN的性能。

Abstract: This paper presents a comparison of the performance of Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying
images containing lesions of psoriasis and diseases similar to it. Models
pre-trained on ImageNet were adapted to a specific data set. Both achieved high
predictive metrics, but the ViTs stood out for their superior performance with
smaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the
best results, with an f1-score of 96.4%, and is recommended as the most
efficient architecture for automated psoriasis detection. This article
reinforces the potential of ViTs for medical image classification tasks.

</details>


### [12] [ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs](https://arxiv.org/abs/2506.10128)
*Xiyao Wang,Zhengyuan Yang,Chao Feng,Yongyuan Liang,Yuhang Zhou,Xiaoyu Liu,Ziyi Zang,Ming Li,Chung-Ching Lin,Kevin Lin,Linjie Li,Furong Huang,Lijuan Wang*

Main category: cs.CV

研究设计了一个名为ViCrit的新任务，以通过强化学习提高视觉语言模型的视觉感知能力。该任务在其设计上实现了感知难度和可验证性的平衡，并在多个视觉语言基准测试中显示了其有效性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 为了解决视觉感知任务在视觉语言模型中难以验证的问题，设计了一个新的任务以提高模型的视觉感知能力。

Method: 引入了ViCrit（视觉描述幻觉批评），一个RL代理任务，该任务训练视觉语言模型（VLM）定位段落中人工引入的细小视觉幻觉。通过修改人类编写的图像描述中的单词、属性、数量或空间关系，使模型在给定图像和修改后的描述的情况下找到错误部分。

Result: 使用ViCrit任务训练的模型在多种视觉语言基准测试中表现出显著的提升，这种提升不仅局限于自然图像，还扩展到了抽象图像推理和视觉数学上，展示了学习感知而非单纯记忆的潜力。

Conclusion: 细粒度的幻觉批评是一个有效且通用的目标，可以提高视觉语言模型中的视觉感知能力。

Abstract: Reinforcement learning (RL) has shown great effectiveness for fine-tuning
large language models (LLMs) using tasks that are challenging yet easily
verifiable, such as math reasoning or code generation. However, extending this
success to visual perception in vision-language models (VLMs) has been impeded
by the scarcity of vision-centric tasks that are simultaneously challenging and
unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption
Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle,
synthetic visual hallucination injected into paragraphs of human-written image
captions. Starting from a 200-word captions, we inject a single, subtle visual
description error-altering a few words on objects, attributes, counts, or
spatial relations-and task the model to pinpoint the corrupted span given the
image and the modified caption. This formulation preserves the full perceptual
difficulty while providing a binary, exact-match reward that is easy to compute
and unambiguous. Models trained with the ViCrit Task exhibit substantial gains
across a variety of VL benchmarks. Crucially, the improvements transfer beyond
natural-image training data to abstract image reasoning and visual math,
showing promises of learning to perceive rather than barely memorizing seen
objects. To facilitate evaluation, we further introduce ViCrit-Bench, a
category-balanced diagnostic benchmark that systematically probes perception
errors across diverse image domains and error types. Together, our results
demonstrate that fine-grained hallucination criticism is an effective and
generalizable objective for enhancing visual perception in VLMs.

</details>
