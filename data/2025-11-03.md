<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling](https://arxiv.org/abs/2510.26912)
*Hyunji Lee,Wenhao Yu,Hongming Zhang,Kaixin Ma,Jiyeon Kim,Dong Yu,Minjoon Seo*

Main category: cs.CL

> 研究了SSM与注意力机制结合的混合模型，发现顺序和并行集成在不同上下文长度中的表现差异，并提出一种基于数据的方法有效提升召回率。

<details>
  <summary>Details</summary>

**Motivation:** 对SSM和注意力机制结合的混合模型背后的架构设计选择理解不足，该研究旨在通过内存使用和整体性能的视角来分析这种混合架构。

**Method:** 分析了顺序和并行集成SSM和注意力层之间的区别，并引入了一种基于数据的方法，即通过添加同义句扩充数据集进行持续训练，进一步增强召回能力。

**Result:** 发现顺序混合模型在较短上下文中表现更好，而并行混合模型对于较长上下文更有效。提出的基于数据的方法能够进一步提升召回率，且泛化能力强，优于旨在提升召回率的架构修改。

**Conclusion:** 此研究提供了对混合SSM-注意力模型的深入理解，并为设计适用于各种应用场景的架构提供了实用指导。

**Abstract:** Hybrid models that combine state space models (SSMs) with attention
mechanisms have shown strong performance by leveraging the efficiency of SSMs
and the high recall ability of attention. However, the architectural design
choices behind these hybrid models remain insufficiently understood. In this
work, we analyze hybrid architectures through the lens of memory utilization
and overall performance, and propose a complementary method to further enhance
their effectiveness. We first examine the distinction between sequential and
parallel integration of SSM and attention layers. Our analysis reveals several
interesting findings, including that sequential hybrids perform better on
shorter contexts, whereas parallel hybrids are more effective for longer
contexts. We also introduce a data-centric approach of continually training on
datasets augmented with paraphrases, which further enhances recall while
preserving other capabilities. It generalizes well across different base models
and outperforms architectural modifications aimed at enhancing recall. Our
findings provide a deeper understanding of hybrid SSM-attention models and
offer practical guidance for designing architectures tailored to various use
cases. Our findings provide a deeper understanding of hybrid SSM-attention
models and offer practical guidance for designing architectures tailored to
various use cases.

</details>


### [2] [Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence](https://arxiv.org/abs/2510.26969)
*Lívia Dutra,Arthur Lorenzi,Laís Berno,Franciany Campos,Karoline Biscardi,Kenneth Brown,Marcelo Viridiano,Frederico Belcavello,Ely Matos,Olívia Guaranha,Erik Santos,Sofia Reinach,Tiago Timponi Torrent*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** We introduce a methodology for the identification of notifiable events in the
domain of healthcare. The methodology harnesses semantic frames to define
fine-grained patterns and search them in unstructured data, namely, open-text
fields in e-medical records. We apply the methodology to the problem of
underreporting of gender-based violence (GBV) in e-medical records produced
during patients' visits to primary care units. A total of eight patterns are
defined and searched on a corpus of 21 million sentences in Brazilian
Portuguese extracted from e-SUS APS. The results are manually evaluated by
linguists and the precision of each pattern measured. Our findings reveal that
the methodology effectively identifies reports of violence with a precision of
0.726, confirming its robustness. Designed as a transparent, efficient,
low-carbon, and language-agnostic pipeline, the approach can be easily adapted
to other health surveillance contexts, contributing to the broader, ethical,
and explainable use of NLP in public health systems.

</details>


### [3] [Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations](https://arxiv.org/abs/2510.26974)
*Jean-Philippe Corbeil,Asma Ben Abacha,Jerome Tremblay,Phillip Swazinna,Akila Jeeson Daniel,Miguel Del-Agua,Francois Beaulieu*

Main category: cs.CL

> 介绍了MEDIQA-OE 2025共享任务，该任务首次尝试从医患对话中提取医疗指令，旨在通过使用大量的语言模型减轻医生的文档工作量并改进患者护理。

<details>
  <summary>Details</summary>

**Motivation:** 当前临床文档越来越多地采用自动语音识别和摘要技术，然而将对话转换为可用于电子健康记录的医疗指令尚待探索，解决该问题可以显著减轻临床医生的文档负担和直接改进患者的治疗。

**Method:** 介绍了MEDIQA-OE 2025共享任务，涉及从医患对话中提取医疗指令的挑战。任务中使用了包括大型语言模型在内的多种方法。

**Result:** 六支团队参与了此共享任务，实验了广泛的策略，并使用了封闭权重和开放权重的大语言模型。该任务聚焦于从医患对话中提取医疗指令，这是一个首次被探索的挑战。通过这个任务，可以显著减轻临床医生的文档负担，直接改进对患者的治疗。

**Conclusion:** 随着临床文档中自动语音识别和摘要的日益使用，研究尚缺乏关于将对话转换为可用于电子健康记录的医疗指令的方法。MEDIQA-OE 2025挑战赛的举行为解决这个问题提供了创新的解决方案，为减轻医生负担和改进患者治疗提供了可能。

**Abstract:** Clinical documentation increasingly uses automatic speech recognition and
summarization, yet converting conversations into actionable medical orders for
Electronic Health Records remains unexplored. A solution to this problem can
significantly reduce the documentation burden of clinicians and directly impact
downstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first
challenge on extracting medical orders from doctor-patient conversations. Six
teams participated in the shared task and experimented with a broad range of
approaches, and both closed- and open-weight large language models (LLMs). In
this paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking,
and participants' solutions.

</details>


### [4] [Semantically-Aware LLM Agent to Enhance Privacy in Conversational AI Services](https://arxiv.org/abs/2510.27016)
*Jayden Serenari,Stephen Lee*

Main category: cs.CL

> 提出了LOPSIDED框架以保护用户在与大型语言模型交互时的个人隐私信息，实验证明该框架显著减少了语义效用错误，提升了隐私保护效果。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型在对话中可能存在个人隐私信息泄露风险，作者提出LOPSIDED框架来保护用户隐私数据，解决隐私泄露问题。

**Method:** LOPSIDED框架通过动态替换用户输入中的敏感个人隐私实体为语义一致的化名，从而保持对话的上下文完整性，同时在模型生成响应后自动去伪名化。

**Result:** 实验结果显示，与基线技术相比，LOPSIDED框架将语义效用错误减少了五倍，同时增强了隐私保护。

**Conclusion:** LOPSIDED框架在保护用户隐私的同时，保持了对话的质量，相比于以前的方法，降低了对话中的语义效用错误。

**Abstract:** With the increasing use of conversational AI systems, there is growing
concern over privacy leaks, especially when users share sensitive personal data
in interactions with Large Language Models (LLMs). Conversations shared with
these models may contain Personally Identifiable Information (PII), which, if
exposed, could lead to security breaches or identity theft. To address this
challenge, we present the Local Optimizations for Pseudonymization with
Semantic Integrity Directed Entity Detection (LOPSIDED) framework, a
semantically-aware privacy agent designed to safeguard sensitive PII data when
using remote LLMs. Unlike prior work that often degrade response quality, our
approach dynamically replaces sensitive PII entities in user prompts with
semantically consistent pseudonyms, preserving the contextual integrity of
conversations. Once the model generates its response, the pseudonyms are
automatically depseudonymized, ensuring the user receives an accurate,
privacy-preserving output. We evaluate our approach using real-world
conversations sourced from ShareGPT, which we further augment and annotate to
assess whether named entities are contextually relevant to the model's
response. Our results show that LOPSIDED reduces semantic utility errors by a
factor of 5 compared to baseline techniques, all while enhancing privacy.

</details>


### [5] [Kad: A Framework for Proxy-based Test-time Alignment with Knapsack Approximation Deferral](https://arxiv.org/abs/2510.27017)
*Ayoub Hammal,Pierre Zweigenbaum,Caio Corro*

Main category: cs.CL

> The paper proposes a proxy-based test-time alignment approach using a small aligned model, described by a token-specific cascading method to address the high computational costs of aligning large language models.

<details>
  <summary>Details</summary>

**Motivation:** to reduce the computational cost associated with aligning large language models as they scale in size, while ensuring they can still meet downstream task requirements and other desired properties.

**Method:** token-specific cascading method, involving a 0-1 knapsack problem for the deferral decision in using a guidance from a smaller, aligned model during the test time to align large language models.

**Result:** the proposed method shows benefits in both task performance and speculative decoding speed through experimental validation.

**Conclusion:** the approach effectively mitigates the computational burden while enhancing performance and speed.

**Abstract:** Several previous works concluded that the largest part of generation
capabilities of large language models (LLM) are learned (early) during
pre-training. However, LLMs still require further alignment to adhere to
downstream task requirements and stylistic preferences, among other desired
properties. As LLMs continue to scale in terms of size, the computational cost
of alignment procedures increase prohibitively. In this work, we propose a
novel approach to circumvent these costs via proxy-based test-time alignment,
i.e. using guidance from a small aligned model. Our approach can be described
as token-specific cascading method, where the token-specific deferral rule is
reduced to 0-1 knapsack problem. In this setting, we derive primal and dual
approximations of the optimal deferral decision. We experimentally show the
benefits of our method both in task performance and speculative decoding speed.

</details>


### [6] [Elastic Architecture Search for Efficient Language Models](https://arxiv.org/abs/2510.27037)
*Shang Wang*

Main category: cs.CL

> 本文提出了Elastic Language Model (ELM)，一种优化小型语言模型的NAS方法，通过灵活的搜索空间和新颖的知识蒸馏损失提升性能。实验表明其优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 面对大型预训练语言模型在计算和内存方面的巨大需求导致的经济和环境担忧，本研究旨在通过提出ELM来减少这些模型的资源消耗。

**Method:** 该论文介绍了一种称为Elastic Language Model (ELM)的新神经架构搜索（NAS）方法，用于优化小型语言模型。ELM将现有的NAS方法扩展为一个灵活的搜索空间，其中包含高效的Transformer模块以及用于调整维度和头数的动态模块。此外，ELM引入了一种新型的知识蒸馏损失，以保留每个模块的独特性并提高架构搜索过程中的辨别能力。

**Result:** 实验结果表明，通过ELM搜索到的语言模型在掩码语言建模和因果语言建模任务上显著优于现有方法。

**Conclusion:** ELM的新颖架构搜索方法和知识蒸馏技术能够有效提升小规模语言模型的性能，而这些改进得益于模型架构探索过程中的效率和灵活性增强。

**Abstract:** As large pre-trained language models become increasingly critical to natural
language understanding (NLU) tasks, their substantial computational and memory
requirements have raised significant economic and environmental concerns.
Addressing these challenges, this paper introduces the Elastic Language Model
(ELM), a novel neural architecture search (NAS) method optimized for compact
language models. ELM extends existing NAS approaches by introducing a flexible
search space with efficient transformer blocks and dynamic modules for
dimension and head number adjustment. These innovations enhance the efficiency
and flexibility of the search process, which facilitates more thorough and
effective exploration of model architectures. We also introduce novel knowledge
distillation losses that preserve the unique characteristics of each block, in
order to improve the discrimination between architectural choices during the
search process. Experiments on masked language modeling and causal language
modeling tasks demonstrate that models discovered by ELM significantly
outperform existing methods.

</details>


### [7] [Dataset Creation and Baseline Models for Sexism Detection in Hausa](https://arxiv.org/abs/2510.27038)
*Fatima Adam Muhammad,Shamsuddeen Muhammad Hassan,Isa Inuwa-Dutse*

Main category: cs.CL

> 本研究针对豪萨语，开发首个性别歧视检测数据集，并通过用户研究及传统机器学习和预训练语言模型方法评估性别歧视检测的有效性，揭示了文化细微差别方面的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 考虑到在线平台促进性别歧视的多种形式，迫切需要有效的性别歧视检测和缓解策略。虽然计算方法在资源丰富的语言中广泛应用于性别歧视检测，但在资源贫乏的语言中进展仍然有限。这些语言由于语言资源有限和文化差异，性别歧视的表现和感知方式受到影响。

**Method:** 本研究引入了首个豪萨语性别歧视检测数据集，该数据集通过社区参与、定性编码和数据增强开发。为了捕捉文化细微差别和语言代表性，进行了两阶段用户研究（n=66），包括母语者的参与，以探讨豪萨语中性别歧视的定义及在日常对话中的表达方式。此外，研究了传统机器学习分类器和预训练的多语言语言模型，并评估了小样本学习在豪萨语性别歧视检测中的有效性。

**Result:** 研究发现挑战在于捕捉文化细微差别，尤其是寻求澄清和惯用表达时，且在这类情况下存在许多假阳性。

**Conclusion:** 本研究强调豪萨语中性别歧视检测的复杂性，尤其是文化细节和惯用语的表现，表明在资源有限的语言中，如何有效和准确地检测性别歧视仍然是一个挑战。

**Abstract:** Sexism reinforces gender inequality and social exclusion by perpetuating
stereotypes, bias, and discriminatory norms. Noting how online platforms enable
various forms of sexism to thrive, there is a growing need for effective sexism
detection and mitigation strategies. While computational approaches to sexism
detection are widespread in high-resource languages, progress remains limited
in low-resource languages where limited linguistic resources and cultural
differences affect how sexism is expressed and perceived. This study introduces
the first Hausa sexism detection dataset, developed through community
engagement, qualitative coding, and data augmentation. For cultural nuances and
linguistic representation, we conducted a two-stage user study (n=66) involving
native speakers to explore how sexism is defined and articulated in everyday
discourse. We further experiment with both traditional machine learning
classifiers and pre-trained multilingual language models and evaluating the
effectiveness few-shot learning in detecting sexism in Hausa. Our findings
highlight challenges in capturing cultural nuance, particularly with
clarification-seeking and idiomatic expressions, and reveal a tendency for many
false positives in such cases.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench](https://arxiv.org/abs/2510.26865)
*Fenfen Lin,Yesheng Liu,Haiyu Xu,Chen Yue,Zheqi He,Mingxuan Zhao,Miguel Hu Chen,Jiakang Liu,JG Yao,Xi Yang*

Main category: cs.CV

> The paper introduces MeasureBench, a benchmark for visual measurement reading, and discusses the challenges faced by vision-language models (VLMs) in accurately reading measurements due to difficulties in indicator localization.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to highlight the difficulty VLMs face in reading measurements and to provide a benchmark for future research to address these challenges.

**Method:** The method involves creating MeasureBench, a benchmark using both real-world and synthesized images of measurements, and a data synthesis pipeline to generate diverse images.

**Result:** Results indicate that even advanced VLMs struggle with measurement reading, particularly in correctly identifying the positions of indicators.

**Conclusion:** The study concludes that current VLMs exhibit a fundamental limitation in precise spatial perception, suggesting future work should focus on improving fine-grained spatial grounding for VLMs.

**Abstract:** Reading measurement instruments is effortless for humans and requires
relatively little domain expertise, yet it remains surprisingly challenging for
current vision-language models (VLMs) as we find in preliminary evaluation. In
this work, we introduce MeasureBench, a benchmark on visual measurement reading
covering both real-world and synthesized images of various types of
measurements, along with an extensible pipeline for data synthesis. Our
pipeline procedurally generates a specified type of gauge with controllable
visual appearance, enabling scalable variation in key details such as pointers,
scales, fonts, lighting, and clutter. Evaluation on popular proprietary and
open-weight VLMs shows that even the strongest frontier VLMs struggle
measurement reading in general. A consistent failure mode is indicator
localization: models can read digits or labels but misidentify the key
positions of pointers or alignments, leading to big numeric errors despite
plausible textual reasoning. We have also conducted preliminary experiments
with reinforcement learning over synthetic data, and find encouraging results
on in-domain synthetic subset but less promising for real-world images. Our
analysis highlights a fundamental limitation of current VLMs in fine-grained
spatial grounding. We hope this resource can help future advances on visually
grounded numeracy and precise spatial perception of VLMs, bridging the gap
between recognizing numbers and measuring the world.

</details>


### [9] [PF-DAformer: Proximal Femur Segmentation via Domain Adaptive Transformer for Dual-Center QCT](https://arxiv.org/abs/2510.26903)
*Rochak Dhakal,Chen Zhao,Zixin Shi,Joyce H. Keyak,Tadashi S. Kaneko,Kuan-Jui Su,Hui Shen,Hong-Wen Deng,Weihua Zhou*

Main category: cs.CV

> 研究提出了一种针对多机构QCT图像分割的领域自适应变换框架，提高分割结果的稳定性和跨站点的可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 克服深网在不同数据集上训练后跨应用效果下降的问题，提高多中心骨质疏松研究中放射组学和结构有限元分析结果的可重复性。

**Method:** 本文提出了一个基于3D TransUNet骨干网络的领域自适应变换分割框架，用于处理多机构QCT图像中的领域偏移问题。框架集成了两种互补策略：通过梯度反转层（GRL）实现对抗性对齐，以及通过最大均值差异（MMD）实现统计对齐。

**Result:** 为处理因不同机构的扫描设备、重建设置和患者人口统计学差异导致的领域偏移问题提供了一个解决方案，并在一个包含1,024个QCT图像扫描和384个扫描的研究队列上进行了训练和验证。

**Conclusion:** 该方法在多机构QCT分割任务上提高了跨站点的稳定性与定量指标的可靠性，有助于实现不依赖于扫描仪的特征学习同时保持解剖细节。

**Abstract:** Quantitative computed tomography (QCT) plays a crucial role in assessing bone
strength and fracture risk by enabling volumetric analysis of bone density
distribution in the proximal femur. However, deploying automated segmentation
models in practice remains difficult because deep networks trained on one
dataset often fail when applied to another. This failure stems from domain
shift, where scanners, reconstruction settings, and patient demographics vary
across institutions, leading to unstable predictions and unreliable
quantitative metrics. Overcoming this barrier is essential for multi-center
osteoporosis research and for ensuring that radiomics and structural finite
element analysis results remain reproducible across sites. In this work, we
developed a domain-adaptive transformer segmentation framework tailored for
multi-institutional QCT. Our model is trained and validated on one of the
largest hip fracture related research cohorts to date, comprising 1,024 QCT
images scans from Tulane University and 384 scans from Rochester, Minnesota for
proximal femur segmentation. To address domain shift, we integrate two
complementary strategies within a 3D TransUNet backbone: adversarial alignment
via Gradient Reversal Layer (GRL), which discourages the network from encoding
site-specific cues, and statistical alignment via Maximum Mean Discrepancy
(MMD), which explicitly reduces distributional mismatches between institutions.
This dual mechanism balances invariance and fine-grained alignment, enabling
scanner-agnostic feature learning while preserving anatomical detail.

</details>


### [10] [DC4GS: Directional Consistency-Driven Adaptive Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2510.26921)
*Moonsoo Jeong,Dongbeen Kim,Minseong Kim,Sungkil Lee*

Main category: cs.CV

> 该研究提出了方向一致性（DC）驱动的自适应密度控制（ADC），用于优化3D高斯图像合成。相比现有的ADC方法，DC4GS减少了高达30%的原始物体数量并增强了重建精度。

<details>
  <summary>Details</summary>

**Motivation:** 传统的自适应密度控制（ADC）方法在处理复杂的3D结构时可能引起不必要的高斯图像元素（或'原始物'）分裂，导致计算成本上升和视觉上的次优结果。

**Method:** 该论文提出了方向一致性（DC）驱动的自适应密度控制（ADC），用于改进3D高斯图像合成（DC4GS）。它在传统的基于位置梯度幅度的ADC基础上，进一步融入了梯度的方向一致性，通过梯度的方向相关性来实现。这样可以更准确地捕捉局部结构的复杂性，避免不必要的分割。当需要分割时，方法利用方向一致性来定义最优的分割位置，使子原始物结构能够最优地顺应局部结构，而不是随机放置。

**Result:** 该方法能够在减少合成结构的原始物数量的同时，提高了重建的保真度。实验显示，相比较于现有的ADC方法，新方法能够将使用到的原始物数量减少多达30%。

**Conclusion:** 提出的方向一致性驱动的自适应密度控制方法不仅大幅减少了所需的3D高斯图像合成中的原始物体数量，而且也显著提高了重建模型的保真度。这项研究表明，梯度的方向一致性可用于提高3D图形单元分割中的结构一致性。

**Abstract:** We present a Directional Consistency (DC)-driven Adaptive Density Control
(ADC) for 3D Gaussian Splatting (DC4GS). Whereas the conventional ADC bases its
primitive splitting on the magnitudes of positional gradients, we further
incorporate the DC of the gradients into ADC, and realize it through the
angular coherence of the gradients. Our DC better captures local structural
complexities in ADC, avoiding redundant splitting. When splitting is required,
we again utilize the DC to define optimal split positions so that
sub-primitives best align with the local structures than the conventional
random placement. As a consequence, our DC4GS greatly reduces the number of
primitives (up to 30% in our experiments) than the existing ADC, and also
enhances reconstruction fidelity greatly.

</details>
