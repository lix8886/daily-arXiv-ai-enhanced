<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.CV](#cs.CV) [Total: 13]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [WearVox: An Egocentric Multichannel Voice Assistant Benchmark for Wearables](https://arxiv.org/abs/2601.02391)
*Zhaojiang Lin,Yong Xu,Kai Sun,Jing Zheng,Yin Huang,Surya Teja Appini,Krish Narang,Renjie Tao,Ishan Kapil Jain,Siddhant Arora,Ruizhi Li,Yiteng Huang,Kaushik Patnaik,Wenfang Xu,Suwon Shon,Yue Liu,Ahmed A Aly,Anuj Kumar,Florian Metze,Xin Luna Dong*

Main category: cs.CL

> 本文引入了WearVox，一个专门用于评估可穿戴设备中语音助手性能的新基准测试，包括多样化的真实场景和环境。研究表明，多通道音频输入对于改善语音助手环境噪声鲁棒性和背景对话辨别能力非常重要。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基准测试集中在干净或通用对话音频上，忽略了可穿戴设备带来声音挑战，如自我中心音频受运动和噪声影响、快速微交互、以及需要区分设备指引语音和背景对话。

**Method:** 提出了WearVox基准测试，一种专门用于评估可穿戴设备场景下语音助手性能的测试集，包括多通道自我中心音频记录，收集自AI眼镜，涵盖五种多样化任务和多种环境下的音效条件。

**Result:** 对主要的专有和开源语音LLM进行了基准测试，发现大多数模型在WearVox测试集上准确率在29%到59%之间，室外嘈杂音频上性能大幅度下降。进一步研究单通道和多通道音频输入模型，展示多通道音频输入显著增强了模型对环境噪声的鲁棒性和辨别设备指引语音的能力。

**Conclusion:** WearVox测试集的建立对推进可穿戴设备上语音AI研究非常重要，显示了空间音频线索对于环境感知语音助手的重要性。

**Abstract:** Wearable devices such as AI glasses are transforming voice assistants into always-available, hands-free collaborators that integrate seamlessly with daily life, but they also introduce challenges like egocentric audio affected by motion and noise, rapid micro-interactions, and the need to distinguish device-directed speech from background conversations. Existing benchmarks largely overlook these complexities, focusing instead on clean or generic conversational audio. To bridge this gap, we present WearVox, the first benchmark designed to rigorously evaluate voice assistants in realistic wearable scenarios. WearVox comprises 3,842 multi-channel, egocentric audio recordings collected via AI glasses across five diverse tasks including Search-Grounded QA, Closed-Book QA, Side-Talk Rejection, Tool Calling, and Speech Translation, spanning a wide range of indoor and outdoor environments and acoustic conditions. Each recording is accompanied by rich metadata, enabling nuanced analysis of model performance under real-world constraints. We benchmark leading proprietary and open-source speech Large Language Models (SLLMs) and find that most real-time SLLMs achieve accuracies on WearVox ranging from 29% to 59%, with substantial performance degradation on noisy outdoor audio, underscoring the difficulty and realism of the benchmark. Additionally, we conduct a case study with two new SLLMs that perform inference with single-channel and multi-channel audio, demonstrating that multi-channel audio inputs significantly enhance model robustness to environmental noise and improve discrimination between device-directed and background speech. Our results highlight the critical importance of spatial audio cues for context-aware voice assistants and establish WearVox as a comprehensive testbed for advancing wearable voice AI research.

</details>


### [2] [PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models](https://arxiv.org/abs/2601.02404)
*Inpyo Song,Eunji Jeon,Jangwon Lee*

Main category: cs.CL

> 介绍了PCEval，一个评估LLMs在物理计算中能力的首个自动基准，指出模型在代码生成和逻辑电路设计表现良好，但在面包板布局方面存在困难。

<details>
  <summary>Details</summary>

**Motivation:** 由于LLMs在硬件约束下的表现未被完全探索，尤其是涉及与物理硬件交互的场景，提出PCEval来填补这一空白。

**Method:** 通过评估框架，PCEval自动检验了13个领先模型在生成电路和生产兼容代码方面的表现，涵盖了从简单到复杂的项目层次。

**Result:** 实验揭示了LLMs在逻辑和代码生成方面的表现不错，但难以处理物理面包板布局，存在连接问题和电路错误。

**Conclusion:** PCEval提升了我们对AI在硬件依赖计算环境中作用的理解，并为开发更好的支持物理计算教育工具奠定了基础。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are considered-for instance, in physical computing, where software must interact with and control physical hardware -their effectiveness has not been fully explored. To address this gap, we introduce \textsc{PCEval} (Physical Computing Evaluation), the first benchmark in physical computing that enables a fully automatic evaluation of the capabilities of LLM in both the logical and physical aspects of the projects, without requiring human assessment. Our evaluation framework assesses LLMs in generating circuits and producing compatible code across varying levels of project complexity. Through comprehensive testing of 13 leading models, \textsc{PCEval} provides the first reproducible and automatically validated empirical assessment of LLMs' ability to reason about fundamental hardware implementation constraints within a simulation environment. Our findings reveal that while LLMs perform well in code generation and logical circuit design, they struggle significantly with physical breadboard layout creation, particularly in managing proper pin connections and avoiding circuit errors. \textsc{PCEval} advances our understanding of AI assistance in hardware-dependent computing environments and establishes a foundation for developing more effective tools to support physical computing education.

</details>


### [3] [Losses that Cook: Topological Optimal Transport for Structured Recipe Generation](https://arxiv.org/abs/2601.02531)
*Mattia Ottoborgo,Daniele Rege Cambrin,Paolo Garza*

Main category: cs.CL

> 该研究提出了一种新的拓扑损失函数和混合损失函数，显著提高了食谱生成的质量。

<details>
  <summary>Details</summary>

**Motivation:** 烹饪食谱是复杂的流程，需要流畅且事实准确的文本，以及准确的时间、温度和程序连贯性，以及正确的成分组合。标准的训练程序主要基于交叉熵，仅关注流利性。

**Method:** 通过RECIPE-NLG，我们研究了几种复合目标的使用，并提出了一种新的拓扑损失函数，该函数将配料列表表示为嵌入空间中的点云，最小化预测配料和理想配料之间的发散。

**Result:** 实验结果表明，我们的损失函数显著提高了配料和操作级别的评价指标。同时，Dice损失在时间和温度准确性方面表现优异，混合损失在数量和时间上提供了具有协同效应的平衡。人类偏好分析支持我们的发现，显示我们的模型在62%的情况下更受欢迎。

**Conclusion:** 研究展示了在食谱生成中使用复合损失函数的重要性，特别是新的拓扑损失函数和混合损失函数的应用，显著改善了生成食谱的质量。

**Abstract:** Cooking recipes are complex procedures that require not only a fluent and factual text, but also accurate timing, temperature, and procedural coherence, as well as the correct composition of ingredients. Standard training procedures are primarily based on cross-entropy and focus solely on fluency. Building on RECIPE-NLG, we investigate the use of several composite objectives and present a new topological loss that represents ingredient lists as point clouds in embedding space, minimizing the divergence between predicted and gold ingredients. Using both standard NLG metrics and recipe-specific metrics, we find that our loss significantly improves ingredient- and action-level metrics. Meanwhile, the Dice loss excels in time/temperature precision, and the mixed loss yields competitive trade-offs with synergistic gains in quantity and time. A human preference analysis supports our finding, showing our model is preferred in 62% of the cases.

</details>


### [4] [ModeX: Evaluator-Free Best-of-N Selection for Open-Ended Generation](https://arxiv.org/abs/2601.02535)
*Hyeong Kyu Choi,Sharon Li*

Main category: cs.CL

> 提出了一种无评估器的Best-of-N选择框架ModeX，通过构建相似性图并递归应用谱聚类来选择代表性质心，避免了外部评估器的需求，适用于开放式文本生成任务。

<details>
  <summary>Details</summary>

**Motivation:** 解决从多个随机生成结果中选择高质量输出的问题，特别是在没有标准答案的开放式任务中，避免现有方法对额外模型或字符串匹配等外部评估的依赖。

**Method:** 通过构建候选生成文本的相似性图并递归应用谱聚类来识别大多数生成文本中的含义共同点作为代表性输出，提出ModeX及更高效的简化版ModeX-Lite。

**Result:** 在包括文本摘要、代码生成和数学推理等开放式任务上，提出的模式优于标准的单路径和多路径基线模型。

**Conclusion:** 为开放式文本生成任务提供了一种计算高效的选择框架，提出了ModeX及其light版本作为解决方案。

**Abstract:** Selecting a single high-quality output from multiple stochastic generations remains a fundamental challenge for large language models (LLMs), particularly in open-ended tasks where no canonical answer exists. While Best-of-N and self-consistency methods show that aggregating multiple generations can improve performance, existing approaches typically rely on external evaluators, reward models, or exact string-match voting, limiting their applicability and efficiency. We propose Mode Extraction (ModeX), an evaluator-free Best-of-N selection framework that generalizes majority voting to open-ended text generation by identifying the modal output representing the dominant semantic consensus among generated texts. ModeX constructs a similarity graph over candidate generations and recursively applies spectral clustering to select a representative centroid, without requiring additional inference or auxiliary models. We further instantiate this selection principle as ModeX-Lite, an improved version of ModeX with early pruning for efficiency. Across open-ended tasks -- including text summarization, code generation, and mathematical reasoning -- our approaches consistently outperform standard single- and multi-path baselines, providing a computationally efficient solution for robust open-ended text generation. Code is released in https://github.com/deeplearning-wisc/ModeX.

</details>


### [5] [LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference](https://arxiv.org/abs/2601.02569)
*Hossein Rajabzadeh,Maryam Dialameh,Chul B. Park,Il-Min Kim,Hyock Ju Kwon*

Main category: cs.CL

> LoRA-Drop是一种用于加速LLMs（大型语言模型）解码过程的推理框架，能显著提升解码速度和减少KV缓存使用，同时保持较高的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 当前动态深度和跳过层方法虽然能在一定程度上减少解码成本，但往往依赖辅助路由机制或在跳过层未补偿的情况下导致准确性下降。因此，提出了LoRA-Drop方法，在不引入额外辅助机制的前提下，既能加速解码又不会牺牲太多准确性。

**Method:** LoRA-Drop方法通过应用一种称为时间计算调度的技术来加速解码过程。具体来说，该方法是基于固定的一组中间层，在大多数解码步骤中，选定的层重复上一个标记的隐藏状态，并应用低秩LoRA修正。同时，定期执行全模型的刷新步骤，以防止漂移问题。此外，LoRA-Drop不需要路由网络，并且兼容标准的KV缓存，而且还可通过在LoRA步骤中跳过可跳过层的KV更新，并定期刷新，从而减少KV缓存的足迹。

**Result:** 实验表明，LoRA-Drop在不同大小的模型上都能实现最高2.6倍的解码加速，并减少45-55%的KV缓存需求，同时与基线相比保持在0.5个百分点的准确性差异范围内。质量检测方面也证明了该方法在多种任务上存在一个可以保证质量同时带来效率提升的安全区域。

**Conclusion:** LoRA-Drop提供了一种简便的方法，可在不牺牲太多准确性的前提下实现大规模语言模型解码效率的显著提升，这为实现适应性容量推理提供了路径。此方法也已经开放了代码。

**Abstract:** Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \textbf{LLaMA2-7B}, \textbf{LLaMA3-8B}, \textbf{Qwen2.5-7B}, and \textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \textbf{2.6$\times$ faster decoding} and \textbf{45--55\% KV-cache reduction} while staying within \textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.

</details>


### [6] [Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency](https://arxiv.org/abs/2601.02574)
*Haoran Wang,Maryam Khalid,Qiong Wu,Jian Gao,Cheng Cao*

Main category: cs.CL

> PCC框架自适应地决定是使用LLM的内部知识还是通过检索外部证据来提高事实性准确性，通过评估LLM的不确定性来实现这一目标，实验验证了其有效性和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 大多数现有的事实核查方法会无差别地检索外部证据，这可能导致引入无关信息或忽视模型内部的知识。我们希望通过一个自适应的方法，使得模型能够根据其对自己的输出的信心来决定是使用内部知识还是检索外部证据。

**Method:** 我们提出了一个名为概率确定性和一致性（Probabilistic Certainty and Consistency, PCC）的框架，该框架通过联合建模大语言模型的概然确定性和推理一致性来估计事实性信心。这些信心信号使模型能够根据信心自适应地选择应答策略：在有信心时直接回答，在不确定或推理不一致性时触发针对性的检索，在高度模糊时进行深入搜索。

**Result:** 实验结果显示，提出的PCC框架在三个具有挑战性的基准上实现了更好的不确定性量化，比口头表达的信心更可靠，并且比现有的基于LLM的事实核查方法更优。此外，PCC在不同类型的LLM中具有很好的泛化能力。

**Conclusion:** PCC框架通过引入一种自适应的策略来决定是使用内部知识还是检索外部证据，提高了事实核查的准确性和效率，并且展示了在不同大语言模型上的泛化能力。

**Abstract:** Large language models (LLMs) are increasingly used in applications requiring factual accuracy, yet their outputs often contain hallucinated responses. While fact-checking can mitigate these errors, existing methods typically retrieve external evidence indiscriminately, overlooking the model's internal knowledge and potentially introducing irrelevant noise. Moreover, current systems lack targeted mechanisms to resolve specific uncertainties in the model's reasoning. Inspired by how humans fact-check, we argue that LLMs should adaptively decide whether to rely on internal knowledge or initiate retrieval based on their confidence in a given claim. We introduce Probabilistic Certainty and Consistency (PCC), a framework that estimates factual confidence by jointly modeling an LLM's probabilistic certainty and reasoning consistency. These confidence signals enable an adaptive verification strategy: the model answers directly when confident, triggers targeted retrieval when uncertain or inconsistent, and escalates to deep search when ambiguity is high. Our confidence-guided routing mechanism ensures that retrieval is invoked only when necessary, improving both efficiency and reliability. Extensive experiments across three challenging benchmarks show that PCC achieves better uncertainty quantification than verbalized confidence and consistently outperforms strong LLM-based fact-checking baselines. Furthermore, we demonstrate that PCC generalizes well across various LLMs.

</details>


### [7] [DataParasite Enables Scalable and Repurposable Online Data Curation](https://arxiv.org/abs/2601.02578)
*Mengyi Sun*

Main category: cs.CL

> 提出了DataParasite，一个开源、模块化的在线数据收集管道，降低了数据收集成本，提高了精度，为数据整理提供了新的解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 许多计算社会科学中的问题依赖于从异构在线来源组装的数据集，这个过程通常是劳动密集型的，成本高昂且难以复制。尽管大型语言模型近期取得了进展，现有的系统通常不透明、缺乏灵活性或者不适合科学数据整理。该项目旨在解决这些问题。

**Method:** 介绍了一种名为DataParasite的开源模块化网络数据收集管道。该管道通过轻量级配置文件将表格整理任务分解为独立的实体级别搜索，并使用通用的Python脚本执行。该管道可以使用自然语言指令用于不同的任务，包括那些没有预定义实体列表的任务。

**Result:** 在多个计算社会科学的典型任务中，包括教职员工的聘用历史、精英的死亡事件及政治生涯轨迹，该管道在降低数据收集成本一个数量级的同时，达到了高精度。

**Conclusion:** 通过降低在线数据组装的技术和劳动力障碍，DataParasite为计算社会科学及更广泛领域的规模化、透明且可重复使用的数据整理提供了一个实际基础。

**Abstract:** Many questions in computational social science rely on datasets assembled from heterogeneous online sources, a process that is often labor-intensive, costly, and difficult to reproduce. Recent advances in large language models enable agentic search and structured extraction from the web, but existing systems are frequently opaque, inflexible, or poorly suited to scientific data curation. Here we introduce DataParasite, an open-source, modular pipeline for scalable online data collection. DataParasite decomposes tabular curation tasks into independent, entity-level searches defined through lightweight configuration files and executed through a shared, task-agnostic python script. Crucially, the same pipeline can be repurposed to new tasks, including those without predefined entity lists, using only natural-language instructions. We evaluate the pipeline on multiple canonical tasks in computational social science, including faculty hiring histories, elite death events, and political career trajectories. Across tasks, DataParasite achieves high accuracy while reducing data-collection costs by an order of magnitude relative to manual curation. By lowering the technical and labor barriers to online data assembly, DataParasite provides a practical foundation for scalable, transparent, and reusable data curation in computational social science and beyond.

</details>


### [8] [Reconstructing Item Characteristic Curves using Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.02580)
*Christopher Ormerod*

Main category: cs.CL

> 研究表明，通过微调大型语言模型如Qwen-3和LoRA，可以有效模拟不同能力水平学生对题目作答，生成合成项目特性曲线，从而估计IRT参数，特别在题目区分度建模上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 传统方法依赖于昂贵的实地测试来收集学生表现数据，从而确定诸如难度和区分度之类的题目参数。这项研究的动机是提出一种更为经济有效的方法，使用大型语言模型来生成这些参数。

**Method:** 研究通过微调大型语言模型（如Qwen-3模型系列和LoRA）来模拟不同潜能力水平的学生回答多项选择题，以此隐式建模题目参数，如难度和区分度。通过生成基于学生能力的正确答案概率函数，研究者生成合成的项目特性曲线（ICCs），以估计项目的IRT参数。

**Result:** 该方法在六年级英语语言艺术题目数据集和BEA 2024共享任务数据集上的评估表明，这种仿真技术在某些情况下能够与或超过现有方法的性能。特别是，它在建模项目区分度方面表现良好。

**Conclusion:** 该研究提出了一种新的方法，通过模拟学生回答题目来估计IRT参数，并在采用大型语言模型的仿真技术评估中展现了优异性能，尤其是在题目区分度的建模上。

**Abstract:** Traditional methods for determining assessment item parameters, such as difficulty and discrimination, rely heavily on expensive field testing to collect student performance data for Item Response Theory (IRT) calibration. This study introduces a novel approach that implicitly models these psychometric properties by fine-tuning Large Language Models (LLMs) to simulate student responses across a spectrum of latent abilities. Leveraging the Qwen-3 dense model series and Low-Rank Adaptation (LoRA), we train models to generate responses to multiple choice questions conditioned on discrete ability descriptors. We reconstruct the probability of a correct response as a function of student ability, effectively generating synthetic Item Characteristic Curves (ICCs) to estimate IRT parameters. Evaluation on a dataset of Grade 6 English Language Arts (ELA) items and the BEA 2024 Shared Task dataset demonstrates that this method competes with or outperforms baseline approaches. This simulation-based technique seems particularly effective at modeling item discrimination.

</details>


### [9] [FlowPlan-G2P: A Structured Generation Framework for Transforming Scientific Papers into Patent Descriptions](https://arxiv.org/abs/2601.02589)
*Kris W Pan,Yongmin Yoo*

Main category: cs.CL

> 本文提出FlowPlan-G2P框架，用于改善科学论文转化成专利描述的过程，尤其是在增加逻辑连贯性和法律合规性方面取得了显著成效。

<details>
  <summary>Details</summary>

**Motivation:** 由于专利描述的编写需要深厚的技术和法律知识，将科学论文转化为专利描述具有挑战性，尤其是在结构化推理和法律约束方面。

**Method:** 提出FlowPlan-G2P框架，将任务分解为三个阶段：概念图诱导、段落和章节规划以及基于图的生成。

**Result:** 实验显示，与端到端的LLM基线相比，FlowPlan-G2P在逻辑连贯性和法律合规性方面有显著提升。

**Conclusion:** 该框架为论文到专利的生成建立了新的范式，并推进了专门领域的结构化文本生成。

**Abstract:** Over 3.5 million patents are filed annually, with drafting patent descriptions requiring deep technical and legal expertise. Transforming scientific papers into patent descriptions is particularly challenging due to their differing rhetorical styles and stringent legal requirements. Unlike black-box text-to-text approaches that struggle to model structural reasoning and legal constraints, we propose FlowPlan-G2P, a novel framework that mirrors the cognitive workflow of expert drafters by reformulating this task into three stages: (1) Concept Graph Induction, extracting technical entities and relationships into a directed graph via expert-like reasoning; (2) Paragraph and Section Planning, reorganizing the graph into coherent clusters aligned with canonical patent sections; and (3) Graph-Conditioned Generation, producing legally compliant paragraphs using section-specific subgraphs and tailored prompts. Experiments demonstrate that FlowPlan-G2P significantly improves logical coherence and legal compliance over end-to-end LLM baselines. Our framework establishes a new paradigm for paper-to-patent generation and advances structured text generation for specialized domains.

</details>


### [10] [Scalable Construction of a Lung Cancer Knowledge Base: Profiling Semantic Reasoning in LLMs](https://arxiv.org/abs/2601.02604)
*Cesar Felipe Martínez Cisneros,Jesús Ulises Quiroz Bautista,Claudia Anahí Guzmán Solano,Bogdan Kaleb García Rivera,Iván García Pacheco,Yalbi Itzel Balderas Martínez,Kolawole John Adebayoc,Ignacio Arroyo Fernández*

Main category: cs.CL

> The paper outlines a pipeline for creating a lung cancer knowledge base using OpenIE methods, which enhances the fine-tuning of T5 models for biomedical NLP, showing improved performance and interpretability.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to develop more effective and precise Natural Language Processing methods for the oncology field, leveraging large language models and structured knowledge bases derived from open information extraction.

**Method:** This study utilizes a pipeline that involves the use of the MeSH thesaurus for identifying medical concepts, filtering PubMed literature with permissive licenses, extracting triplets using OpenIE, and enriching them with NER to create a lung cancer knowledge base.

**Result:** The study demonstrates improved performance and semantic coherence of T5 models fine-tuned on the dataset with a comparative assessment using ROUGE and BERTScore.

**Conclusion:** The results indicate that OpenIE-derived resources can serve as scalable and low-cost solutions for enhancing biomedical natural language processing.

**Abstract:** The integration of Large Language Models (LLMs) into biomedical research offers new opportunities for domainspecific reasoning and knowledge representation. However, their performance depends heavily on the semantic quality of training data. In oncology, where precision and interpretability are vital, scalable methods for constructing structured knowledge bases are essential for effective fine-tuning. This study presents a pipeline for developing a lung cancer knowledge base using Open Information Extraction (OpenIE). The process includes: (1) identifying medical concepts with the MeSH thesaurus; (2) filtering open-access PubMed literature with permissive licenses (CC0); (3) extracting (subject, relation, object) triplets using OpenIE method; and (4) enriching triplet sets with Named Entity Recognition (NER) to ensure biomedical relevance. The resulting triplet sets provide a domain-specific, large-scale, and noise-aware resource for fine-tuning LLMs. We evaluated T5 models finetuned on this dataset through Supervised Semantic Fine-Tuning. Comparative assessments with ROUGE and BERTScore show significantly improved performance and semantic coherence, demonstrating the potential of OpenIE-derived resources as scalable, low-cost solutions for enhancing biomedical NLP.

</details>


### [11] [Improved Evidence Extraction for Document Inconsistency Detection with LLMs](https://arxiv.org/abs/2601.02627)
*Nelvin Tan,Yaowen Zhang,James Asikin Cheung,Fusheng Liu,Yu-Ching Shih,Dong Yang*

Main category: cs.CL

> 研究提出了一个新的框架和证据提取指标，以改进大型语言模型（LLMs）在文档不一致性检测方面的性能，特别是专注于提供不一致的句子证据方面。提出的方法优于直接的提示方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型（LLMs）在许多领域变得非常有用，但关于基于LLM的方法在文档不一致性检测方面的研究相对较少。特别是针对提供不一致句子的证据方面。

**Method:** 引入了新的全面的证据提取指标和一个带有约束过滤的红acted-重试框架，这显著提高了基于LLM的文档不一致检测效果，优于直接提示方法。

**Result:** 通过实验结果支持了所提出的框架和指标的有效性。

**Conclusion:** 新的证据提取指标和红acted-重试框架显著改善了基于LLM的文档不一致性检测性能。

**Abstract:** Large language models (LLMs) are becoming useful in many domains due to their impressive abilities that arise from large training datasets and large model sizes. However, research on LLM-based approaches to document inconsistency detection is relatively limited. There are two key aspects of document inconsistency detection: (i) classification of whether there exists any inconsistency, and (ii) providing evidence of the inconsistent sentences. We focus on the latter, and introduce new comprehensive evidence-extraction metrics and a redact-and-retry framework with constrained filtering that substantially improves LLM-based document inconsistency detection over direct prompting. We back our claims with promising experimental results.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [12] [Self-Supervised Masked Autoencoders with Dense-Unet for Coronary Calcium Removal in limited CT Data](https://arxiv.org/abs/2601.02392)
*Mo Chen*

Main category: cs.CV

> 本论文介绍了一种针对冠状动脉钙化伪影去除的自监督学习框架Dense-MAE，该框架通过预训练策略提高了3D重建的精度，特别是在标注数据稀缺的情况下。实验表明，这种方法显著改善了伪影去除效果。

<details>
  <summary>Details</summary>

**Motivation:** 冠状动脉钙化在CTA中产生显著的伪影，极大地干扰了诊断管腔狭窄。然而，由于标注数据稀缺，利用深度卷积神经网络（如Dense-Unet）进行伪影去除的方法受到了限制。因此，研究者们希望通过自监督学习方法来提高这类任务的性能。

**Method:** 提出了一种新颖的自监督学习框架Dense-MAE，用于体素医学数据。该框架采用预训练策略，随机屏蔽3D血管腔室的补丁并训练Dense-Unet进行重建，从而迫使编码器学习动脉拓扑的高级潜在特征，而无需人类标注。这种方法特别适用于标注数据稀缺的医疗领域场景。

**Result:** 实验结果显示，利用基于MAE的预训练权重初始化钙去除网络，相比于从零开始训练，显著提高了填充精度和狭窄度估计，特别是在标注数据不足的情况下。

**Conclusion:** 通过引入基于3D点云的掩码自编码器技术，Dense-MAE框架成功地解决了标注数据有限的条件下钙化伪影的去除问题。

**Abstract:** Coronary calcification creates blooming artifacts in Computed Tomography Angiography (CTA), severely hampering the diagnosis of lumen stenosis. While Deep Convolutional Neural Networks (DCNNs) like Dense-Unet have shown promise in removing these artifacts via inpainting, they often require large labeled datasets which are scarce in the medical domain. Inspired by recent advancements in Masked Autoencoders (MAE) for 3D point clouds, we propose \textbf{Dense-MAE}, a novel self-supervised learning framework for volumetric medical data. We introduce a pre-training strategy that randomly masks 3D patches of the vessel lumen and trains the Dense-Unet to reconstruct the missing geometry. This forces the encoder to learn high-level latent features of arterial topology without human annotation. Experimental results on clinical CTA datasets demonstrate that initializing the Calcium Removal network with our MAE-based weights significantly improves inpainting accuracy and stenosis estimation compared to training from scratch, specifically in few-shot scenarios.

</details>


### [13] [MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion](https://arxiv.org/abs/2601.02414)
*Jichao Zhu,Jun Yu*

Main category: cs.CV

> The paper introduces MIAR, a method for multimodal emotion recognition (MER) that addresses the problem of distributional differences among modalities and varying contributions by using feature interaction and contrastive learning.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of previous MER methods, which are insensitive to distributional differences among modalities and varying contributions, and lack robust generalization capabilities.

**Method:** The method MIAR uses feature interaction to generate feature tokens representing global representations of modalities’ interactions. It aligns these different modalities using contrastive learning and normalization strategies.

**Result:** Experiments on CMU-MOSI and CMU-MOSEI datasets show that MIAR outperforms state-of-the-art methods in multimodal emotion recognition.

**Conclusion:** MIAR provides an effective solution to the challenges faced by conventional multimodal emotion recognition methods by integrating cross-modal interaction and alignment.

**Abstract:** Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.

</details>


### [14] [Multimodal Sentiment Analysis based on Multi-channel and Symmetric Mutual Promotion Feature Fusion](https://arxiv.org/abs/2601.02415)
*Wangyuan Zhu,Jun Yu*

Main category: cs.CV

> This paper proposes a method for multimodal sentiment analysis that improves intra-modal feature representation and inter-modal feature fusion using symmetric mutual promotion and attention mechanisms, demonstrating its effectiveness on benchmark datasets.

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in multimodal sentiment analysis, specifically limited features from single modalities and the lack of consideration for inter-modal feature differences.

**Method:** Structure

**Result:** The proposed method shows effectiveness and superiority on benchmark datasets.

**Conclusion:** The proposed method enhances inter-modal interactions and intra-modal feature representation, providing a solution for more accurate human emotional state recognition in multimodal sentiment analysis.

**Abstract:** Multimodal sentiment analysis is a key technology in the fields of human-computer interaction and affective computing. Accurately recognizing human emotional states is crucial for facilitating smooth communication between humans and machines. Despite some progress in multimodal sentiment analysis research, numerous challenges remain. The first challenge is the limited and insufficiently rich features extracted from single modality data. Secondly, most studies focus only on the consistency of inter-modal feature information, neglecting the differences between features, resulting in inadequate feature information fusion. In this paper, we first extract multi-channel features to obtain more comprehensive feature information. We employ dual-channel features in both the visual and auditory modalities to enhance intra-modal feature representation. Secondly, we propose a symmetric mutual promotion (SMP) inter-modal feature fusion method. This method combines symmetric cross-modal attention mechanisms and self-attention mechanisms, where the cross-modal attention mechanism captures useful information from other modalities, and the self-attention mechanism models contextual information. This approach promotes the exchange of useful information between modalities, thereby strengthening inter-modal interactions. Furthermore, we integrate intra-modal features and inter-modal fused features, fully leveraging the complementarity of inter-modal feature information while considering feature information differences. Experiments conducted on two benchmark datasets demonstrate the effectiveness and superiority of our proposed method.

</details>


### [15] [Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning](https://arxiv.org/abs/2601.02422)
*Wenting Lu,Didi Zhu,Tao Shen,Donglin Zhu,Ayong Ye,Chao Wu*

Main category: cs.CV

> 本文提出了CoCoT框架，该框架在跨模态场景中解决了过度依赖单一粗粒度图像区域和语义断裂的问题。通过构建CoCoT-70K数据集，这一框架在多个挑战性基准测试上显著提高了视觉推理准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的Chain-of-Thought方法存在过度依赖单一粗粒度图像区域和跨模态场景中各推理步骤间的语义断裂两个主要问题。本研究旨在解决这两个问题，以提高多模态推理的效果。

**Method:** 本文提出了CoCoT框架，该框架包括两个创新点：a) 动态多区域定位，根据问题自适应地检测出最相关的图像区域；b) 关系感知推理，通过迭代对齐视觉线索，实现多区域合作，构建连贯且逻辑性的思维链路。

**Result:** 通过实验表明，CoCoT显著提升了复杂的视觉推理性能，在LLaVA-1.5和Qwen2-VL上的六个具有挑战性的基准测试中，平均准确率分别提高了15.4%和4.0%。

**Conclusion:** 本研究表明，基于动态多区域定位和关系感知推理的CoCoT框架在改善多模态场景下的视觉推理性能方面非常有效，并构建了一个高质量的数据集CoCoT-70K，有助于进一步的研究。

**Abstract:** Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) framework, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively aligning visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual reasoning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.

</details>


### [16] [NitroGen: An Open Foundation Model for Generalist Gaming Agents](https://arxiv.org/abs/2601.02427)
*Loïc Magne,Anas Awadalla,Guanzhi Wang,Yinzhen Xu,Joshua Belofsky,Fengyuan Hu,Joohwan Kim,Ludwig Schmidt,Georgia Gkioxari,Jan Kautz,Yisong Yue,Yejin Choi,Yuke Zhu,Linxi "Jim" Fan*

Main category: cs.CV

> NitroGen是一个经过大规模训练的游戏代理模型，展示出在不同游戏类型下的强大性能，并能有效迁移至未见过的游戏，提升了跨游戏的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 研究目标是为了推进通用型具身代理的研究，特别是通过结合大规模数据集、统一模型和具体游戏环境来提升游戏代理的能力。

**Method:** 我们提出NitroGen，一个用于通用游戏代理的视觉-动作基础模型，该模型在超过1,000个游戏的40,000小时游戏视频上进行训练。这一模型包含了三个关键要素：1)通过从公开的游戏视频中自动提取玩家行为生成的互联网级视频-动作数据集，2)可以衡量跨游戏泛化能力的多游戏基准环境，3)通过大规模行为克隆训练的统一视觉-动作模型。

**Result:** NitroGen在各种领域展现出强大的性能，包括3D动作游戏中的战斗场景、2D平台游戏中的高精度控制，以及在程序生成的游戏世界中的探索。它在未知游戏中能有效迁移，与从零训练的模型相比，任务成功率提高了高达52%。

**Conclusion:** 我们发布了数据集、评估套件和模型权重，以促进对通用型具身代理的研究。

**Abstract:** We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.

</details>


### [17] [TAP-ViTs: Task-Adaptive Pruning for On-Device Deployment of Vision Transformers](https://arxiv.org/abs/2601.02437)
*Zhibo Wang,Zuoyuan Zhang,Xiaoyi Pang,Qile Zhang,Xuanyi Hao,Shuguo Zhuo,Peng Sun*

Main category: cs.CV

> 本文提出了TAP-ViTs，一种能够生成设备特定剪枝ViT模型的任务自适应剪枝框架，在保持性能的同时降低了ViT的计算和内存需求。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服现有方法在资源受限的移动和边缘设备上部署Vision Transformers (ViTs)时存在的计算和内存需求过高问题，同时解决现有剪枝方法忽略设备异构性和依赖设备本地数据进行微调的局限性。

**Method:** TAP-ViTs采用基于GMM的指标数据集构建机制来推断设备级别的任务特性，并提出了一种基于双重粒度重要性评估的剪枝策略，能够在顾及每个设备计算预算的同时实现细粒度的任务感知剪枝。

**Result:** 实验结果表明，TAP-ViTs在多种ViT主干网络和数据集上进行测试时，相比于现有最先进的剪枝方法，在相似的压缩比下表现更优。

**Conclusion:** 研究提出了一种能够在尊重隐私的前提下，生成针对各个设备具体的剪枝ViT模型的方法，从而解决了现有方法无法在隐私保护的移动计算场景中实现任务定制化ViT剪枝的问题。

**Abstract:** Vision Transformers (ViTs) have demonstrated strong performance across a wide range of vision tasks, yet their substantial computational and memory demands hinder efficient deployment on resource-constrained mobile and edge devices. Pruning has emerged as a promising direction for reducing ViT complexity. However, existing approaches either (i) produce a single pruned model shared across all devices, ignoring device heterogeneity, or (ii) rely on fine-tuning with device-local data, which is often infeasible due to limited on-device resources and strict privacy constraints. As a result, current methods fall short of enabling task-customized ViT pruning in privacy-preserving mobile computing settings. This paper introduces TAP-ViTs, a novel task-adaptive pruning framework that generates device-specific pruned ViT models without requiring access to any raw local data. Specifically, to infer device-level task characteristics under privacy constraints, we propose a Gaussian Mixture Model (GMM)-based metric dataset construction mechanism. Each device fits a lightweight GMM to approximate its private data distribution and uploads only the GMM parameters. Using these parameters, the cloud selects distribution-consistent samples from public data to construct a task-representative metric dataset for each device. Based on this proxy dataset, we further develop a dual-granularity importance evaluation-based pruning strategy that jointly measures composite neuron importance and adaptive layer importance, enabling fine-grained, task-aware pruning tailored to each device's computational budget. Extensive experiments across multiple ViT backbones and datasets demonstrate that TAP-ViTs consistently outperforms state-of-the-art pruning methods under comparable compression ratios.

</details>


### [18] [Understanding Pure Textual Reasoning for Blind Image Quality Assessment](https://arxiv.org/abs/2601.02441)
*Yuan Li,Shin'ya Nishida*

Main category: cs.CV

> 研究通过三种范式（Chain-of-Thought, Self-Consistency, Autoencoder）评估文本信息对盲图像质量评估（BIQA）的影响，发现自我一致性范式在缩小图像和文本预测之间的差距方面显著有效。

<details>
  <summary>Details</summary>

**Motivation:** 探讨文本信息如何以及在多大程度上能够代表与分数相关的图像内容，并改善BIQA的表现。

**Method:** 比较现有的BIQA模型与设计的三种学习图像-文本-分数关系的范式，通过实验测量。

**Result:** 只有文本信息时，现有模型的分数预测表现显著下降；自我一致性范式大幅缩小了图像和文本预测之间的差距，而其他范式表现较弱。

**Conclusion:** 研究提供的见解有助于增进对文本推理在BIQA以及更高层次视觉任务中的理解。

**Abstract:** Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.

</details>


### [19] [Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative](https://arxiv.org/abs/2601.02443)
*Li Wang,Xi Chen,XiangWen Deng,HuaHui Yi,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

> 本文通过消融研究，探索了多模态大规模语言模型在膝关节骨关节炎分类任务中的表现。结果表明，单独训练的视觉编码器优于整个MLLMs系统，且数据平衡和质量比数量更加重要。建议优化视觉编码器和精心策划数据集以开发临床应用程序。

<details>
  <summary>Details</summary>

**Motivation:** 多模态语言模型在医学视觉问答和报告生成方面表现出色，但这些生成和解释能力并不能可靠地转移到疾病特定分类任务中。本文着眼于被忽视的膝关节骨关节炎影像分类问题，此病全球病患估计有3-4亿人。

**Method:** 本文通过对膝关节骨关节炎分类的多模态大规模语言模型（MLLMs）系统的消融研究，系统性地评估了视觉编码器、连接器和大规模语言模型（LLM）的作用，探讨了各种训练策略的影响。

**Result:** 研究结果显示，单独训练的视觉编码器在分类任务上的表现优于完整的MLLMs系统，且LLM在微调后没有提供显著提升。此外，少量样本下的LoRA微调（500张类平衡图像）效果优于大量样本下的不平衡数据训练（5,778张图像）。

**Conclusion:** 本研究得出，对于特定领域的医学分类任务，大规模语言模型更适合充当解释者和报告生成器，而非主分类器。因此，MLLM架构可能不适合那些需要高确定性的医学图像诊断任务。建议优先优化视觉编码器并精心选择数据集，以便开发出可应用于临床的系统。

**Abstract:** Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.

</details>


### [20] [A Spatio-Temporal Deep Learning Approach For High-Resolution Gridded Monsoon Prediction](https://arxiv.org/abs/2601.02445)
*Parashjyoti Borah,Sanghamitra Sarkar,Ranjan Phukan*

Main category: cs.CV

> 本文提出了一种基于深度学习的方法，用于预测印度夏季季风的高分辨率网格化降雨模式，解决了传统预报方法在空间细节上的不足。

<details>
  <summary>Details</summary>

**Motivation:** 传统的长期预报集中于预测单个空间平均季节值，缺少区域水资源管理所需的细节。为了弥补这一不足，引入了这种新框架。

**Method:** 本文介绍了一种新颖的深度学习框架，将网格化季风预测重新定义为一个时空计算机视觉任务。通过将季风前的多变量大气和海洋场视为一系列多通道图像，形成类似视频的输入张量，采用卷积神经网络（CNN）结构来学习从五个月（1月至5月）前的状态到随后季风季节的高分辨率网格化降雨模式之间的复杂映射。

**Result:** 该框架能够为季风季节的每一个月（6月至9月）以及整个季节的平均降雨模式产生区别的预测结果，展示其在短期内和季节性展望中的实用价值。

**Conclusion:** 新框架证明了其在印度夏季季风预测中的应用价值，为区域内资源管理提供了重要的空间分辨率。

**Abstract:** The Indian Summer Monsoon (ISM) is a critical climate phenomenon, fundamentally impacting the agriculture, economy, and water security of over a billion people. Traditional long-range forecasting, whether statistical or dynamical, has predominantly focused on predicting a single, spatially-averaged seasonal value, lacking the spatial detail essential for regional-level resource management. To address this gap, we introduce a novel deep learning framework that reframes gridded monsoon prediction as a spatio-temporal computer vision task. We treat multi-variable, pre-monsoon atmospheric and oceanic fields as a sequence of multi-channel images, effectively creating a video-like input tensor. Using 85 years of ERA5 reanalysis data for predictors and IMD rainfall data for targets, we employ a Convolutional Neural Network (CNN)-based architecture to learn the complex mapping from the five-month pre-monsoon period (January-May) to a high-resolution gridded rainfall pattern for the subsequent monsoon season. Our framework successfully produces distinct forecasts for each of the four monsoon months (June-September) as well as the total seasonal average, demonstrating its utility for both intra-seasonal and seasonal outlooks.

</details>


### [21] [Don't Mind the Gaps: Implicit Neural Representations for Resolution-Agnostic Retinal OCT Analysis](https://arxiv.org/abs/2601.02447)
*Bennet Kahrs,Julia Andresen,Fenja Falta,Monty Santarossa,Heinz Handels,Timo Kepp*

Main category: cs.CV

> 通过使用INRs，本文提出了两种用于视网膜OCT体积密集3D分析的框架，改善了视网膜形状表示并允许分辨率独立分析。这解决了以往方法因二维方法、各向异性问题及分辨率依赖带来的弊端。

<details>
  <summary>Details</summary>

**Motivation:** 大多数基于学习的方法采用二维方法来替代体积分析，以规避由于各向异性而引发的问题。然而这些方法导致了对相邻B扫描生成不一致的结果的风险。另外，通常使用的卷积神经网络受限于训练数据的分辨率，使其不能用于不同成像协议获取的图像。为了克服这些问题，作者利用了INRs能将体素化数据存储为连续表示的特性，使其可以应用于各向异性数据。

**Method:** 我们提出两种利用INRs（隐式神经表示）特性的框架来进行视网膜OCT体积的密集三维分析。第一种方法通过纳入来自平铺视网膜模式的附加信息来执行B扫描之间的插值，有助于在B扫描之间保留相关结构。第二种方法创建一个分辨率无关的视网膜图谱，使无需严格的数据要求即可进行一般分析。两种方法都利用了可泛化的INRs，通过基于人群的训练改善视网膜形状表示，并允许对未见病例进行预测。

**Result:** 提出的两种方法可以增强对视网膜形态的表示，并允许对未观察过的病例进行预测。这提供了一个分辨率独立的框架，从而可以分析具有大跨距B扫描的OCT图像，增加了对视网膜结构和病理学进行体积评价的可能性。

**Conclusion:** 使用INRs的两种框架能克服现有方法在视网膜OCT体积分析中存在的瓶颈，提供了分辨率无关的视网膜图像密集三维分析，这为视网膜结构和病态的体积评估开辟了新的可能性。

**Abstract:** Routine clinical imaging of the retina using optical coherence tomography (OCT) is performed with large slice spacing, resulting in highly anisotropic images and a sparsely scanned retina. Most learning-based methods circumvent the problems arising from the anisotropy by using 2D approaches rather than performing volumetric analyses. These approaches inherently bear the risk of generating inconsistent results for neighboring B-scans. For example, 2D retinal layer segmentations can have irregular surfaces in 3D. Furthermore, the typically used convolutional neural networks are bound to the resolution of the training data, which prevents their usage for images acquired with a different imaging protocol. Implicit neural representations (INRs) have recently emerged as a tool to store voxelized data as a continuous representation. Using coordinates as input, INRs are resolution-agnostic, which allows them to be applied to anisotropic data. In this paper, we propose two frameworks that make use of this characteristic of INRs for dense 3D analyses of retinal OCT volumes. 1) We perform inter-B-scan interpolation by incorporating additional information from en-face modalities, that help retain relevant structures between B-scans. 2) We create a resolution-agnostic retinal atlas that enables general analysis without strict requirements for the data. Both methods leverage generalizable INRs, improving retinal shape representation through population-based training and allowing predictions for unseen cases. Our resolution-independent frameworks facilitate the analysis of OCT images with large B-scan distances, opening up possibilities for the volumetric evaluation of retinal structures and pathologies.

</details>


### [22] [PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding](https://arxiv.org/abs/2601.02457)
*Souhail Hadgi,Bingchen Gong,Ramana Sundararaman,Emery Pierson,Lei Li,Peter Wonka,Maks Ovsjanikov*

Main category: cs.CV

> 介绍了一种新的仅编码器的3D模型，能直接从点云生成部分级别的语言对齐特征，显著提升了3D部分分割的性能和效率。

<details>
  <summary>Details</summary>

**Motivation:** 当前的3D基础模型在全局任务（检索、分类）中表现出色，但在局部部分级别的推理上表现不佳。此外，当前办法依赖多视图渲染和大量语言模型（LLM）提示工程，未能充分利用3D几何形状的内在属性。

**Method:** 我们引入了一种仅编码器的3D模型，该模型能直接从点云中生成语言对齐的patch-level特征。预训练方法基于现有生成带有部分注释的3D形状的数据引擎，通过将多视图SAM区域与视觉和语言模型（VLM）的描述配对实现。训练分为两个阶段：（1）从视觉编码器（如DINOv2）中蒸馏稠密2D特征到3D图块；（2）通过多正对比目标对这些图块嵌入和部分级文本嵌入进行对齐。

**Result:** 我们的3D编码器实现了零样本3D部分分割，具有快速单次推理的优点，无需测试时进行多视图渲染，并在多个3D部分分割基准测试中显著优于之前的渲染方法和前馈方法。

**Conclusion:** 研究提出了一种新的基于点云的3D模型，可以直接生成语言对齐的特征，提高了局部部分级别的推理能力，并提高了性能和计算效率。

**Abstract:** Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/

</details>


### [23] [CT Scans As Video: Efficient Intracranial Hemorrhage Detection Using Multi-Object Tracking](https://arxiv.org/abs/2601.02521)
*Amirreza Parvahan,Mohammad Hoseyni,Javad Khoramdel,Amirhossein Nikoofard*

Main category: cs.CV

> 本文介绍了一种面向边缘设备医学图像分析的轻量级计算机视觉框架，通过处理体积CT图像作为视频流简化了计算需求，提高了颅内出血检测的精确度，并可应用于资源有限环境中的实时患者优先排序。

<details>
  <summary>Details</summary>

**Motivation:** 研发此框架的动机在于减少体积医学成像在边缘设备自动化分析所面临的高内存和计算需求的压力，从而实现更加高效的2D检测与3D情境需求的结合。

**Method:** 本文提出了一种将体积CT数据重新表述为顺序视频流的轻量级计算机视觉框架，以解决3D卷积神经网络在边缘设备上进行体积医学成像自动化分析时面临的高内存和计算需求问题。通过对Hemorica数据集进行颅内出血检测任务的评估，使用了YOLO架构的不同版本（v8，v10，v11和v12）的小型配置进行了基准测试，选择具有最高mAP@50的版本作为切片级骨干网络，并引入了ByteTrack算法来确保Z轴上解剖学的一致性。此外，本文提出了一种混合推理策略和时空一致性过滤器来解决视频跟踪器初始化产生的延迟问题，从而有效地区分真实的病理情况和瞬时预测噪声。

**Result:** 实验结果显示，该框架在独立测试数据上将检测精度从基线2D检测器的0.703提高到了0.779，同时保持了高灵敏度。

**Conclusion:** 通过这种代替3D上下文推理的方法，在显著降低计算成本的同时，为在资源紧张环境下实现实时患者优先排序提供了一种可扩展的解决方案。

**Abstract:** Automated analysis of volumetric medical imaging on edge devices is severely constrained by the high memory and computational demands of 3D Convolutional Neural Networks (CNNs). This paper develops a lightweight computer vision framework that reconciles the efficiency of 2D detection with the necessity of 3D context by reformulating volumetric Computer Tomography (CT) data as sequential video streams. This video-viewpoint paradigm is applied to the time-sensitive task of Intracranial Hemorrhage (ICH) detection using the Hemorica dataset. To ensure operational efficiency, we benchmarked multiple generations of the YOLO architecture (v8, v10, v11 and v12) in their Nano configurations, selecting the version with the highest mAP@50 to serve as the slice-level backbone. A ByteTrack algorithm is then introduced to enforce anatomical consistency across the $z$-axis. To address the initialization lag inherent in video trackers, a hybrid inference strategy and a spatiotemporal consistency filter are proposed to distinguish true pathology from transient prediction noise. Experimental results on independent test data demonstrate that the proposed framework serves as a rigorous temporal validator, increasing detection Precision from 0.703 to 0.779 compared to the baseline 2D detector, while maintaining high sensitivity. By approximating 3D contextual reasoning at a fraction of the computational cost, this method provides a scalable solution for real-time patient prioritization in resource-constrained environments, such as mobile stroke units and IoT-enabled remote clinics.

</details>


### [24] [MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark](https://arxiv.org/abs/2601.02536)
*Shaden Shaar,Bradon Thymes,Sirawut Chaixanien,Claire Cardie,Bharath Hariharan*

Main category: cs.CV

> 本文引入了基于电影概要视频的开放性多模态视频QA基准测试MovieRecapsQA，以解决现有方法无法有效捕捉多模态推理的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视频QA基准很难捕捉多模态推理，并且大多是封闭式的，因为在评估开放式答案方面存在困难。本文的动机在于填补这一研究空白，创建开放性多模态视频QA基准测试。

**Method:** 本文提出了一种基于电影概要视频的新型开放性多模态视频QA基准测试MovieRecapsQA。该测试使用两种模态（即，概要视频及概要总结文本）去总结电影的关键事件，并生成了约8200个问题答案对（QA）。同时，提供相关的验证答案所需的事实，以此实现无参考验证。

**Result:** 通过对七种最先进的多模态语言模型（MLLMs）进行性能评估，发现视觉问题仍是最具挑战性的，而模型倾向于默认使用文本输入，从视频中提取准确的事实信息对所有模型都是困难的，但在仅依赖视频的问题上，专有模型与开源模型性能相当。

**Conclusion:** MovieRecapsQA作为一个新的开放性多模态视频QA基准测试，可以提供视频和文本输入的明确文本上下文，为模型性能的评估提供了新的标准。

**Abstract:** Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary "facts" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [25] [Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis](https://arxiv.org/abs/2601.02409)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh*

Main category: eess.IV

> 本文提出EGxFSL和xGAL框架，解决医学图像分析中的数据稀缺和模型可解释性问题，通过实验表明该方法在几种不同的医学图像数据集上表现优于基线模型。

<details>
  <summary>Details</summary>

**Motivation:** 医学图像分析面临着标注数据不足和模型不可解释性的挑战，这阻碍了临床AI系统的部署。少样本学习（FSL）解决了数据限制问题，但预测结果不够透明。主动学习（AL）方法优化数据获取，但忽视了获取样本的可解释性。

**Method:** 我们提出了一种双框架解决方案：专家引导的可解释少样本学习（EGxFSL）和可解释性引导的主动学习（xGAL）。EGxFSL整合了放射科医生定义的兴趣区域作为空间监督，通过Grad-CAM基础的Dice损失，联合优化原型分类实现可解释的少样本学习。xGAL引入了迭代样本采集，优先考虑预测不确定性与注意力偏差，形成一个闭环框架，其中可解释性指导着训练和样本选择的协同作用。

**Result:** 在BraTS（MRI）、VinDr-CXR（胸部X线）和SIIM-COVID-19（胸部X线）数据集上，我们分别实现了92%、76%和62%的准确率，与非引导基线相比在所有数据集上均有更好的表现。在数据极度受限的情况下，xGAL通过680个样本实现了76%的准确率，而随机采样仅获得57%的准确率。Grad-CAM的可视化展示了引导模型专注于具有诊断相关性的区域，同时跨模式有效性在乳腺超声数据上得到验证。

**Conclusion:** 通过EGxFSL和xGAL框架，我们有效地解决了少样本学习和主动学习在医学图像分析中的限制，实验结果验证了该方法在准确率上的显著提升，并展示了跨模态适应性。

**Abstract:** Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\%, 76\%, and 62\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\% accuracy with only 680 samples versus 57\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.

</details>
