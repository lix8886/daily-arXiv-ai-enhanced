<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA](https://arxiv.org/abs/2512.22208)
*Pu Zhao,Xuan Shen,Zhenglun Kong,Yixin Shen,Sung-En Chang,Arash Akbari,Timothy Rupprecht,Lei Lu,Enfu Nan,Changdi Yang,Yumei He,Weiyan Shi,Xingchen Xu,Yu Huang,Wei Jiang,Wei Wang,Yue Chen,Yong He,Yanzhi Wang*

Main category: cs.CL

> 本文介绍了名为Moxin 7B的完全开源大型语言模型，该模型提供了在训练、数据集和实现细节上的完全透明性。文章开发了三种面向不同任务（视觉语言、视觉语言-动作和中文处理）的变体模型，并展示了这些模型在多个评估中的优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 随着专有大型语言模型如GPT-4和GPT-o1在AI社区中的流行和优秀性能得到了广泛关注，同时开源的大型语言模型如LLaMA和Mistral因为容易定制和部署而迅速普及。本文强调了完全透明和开放的重要性，以促进健康的开源生态系统。

**Method:** 本文介绍了Moxin 7B，这是一个完全开源的大语言模型，它遵循模型开放框架，不仅分享模型权重，还完全透明地呈现了训练、数据集和实现细节，从而促进了更加包容和协作的研究环境。为了进一步增强Moxin在不同任务中的能力，开发了三种变体：面向视觉语言任务的Moxin-VLM，面向视觉语言和动作任务的Moxin-VLA，以及面向中文能力的Moxin-Chinese。

**Result:** 实验结果表明，模型在各种评估中表现出色。

**Conclusion:** 作者们公开了模型、数据和代码，以支持进一步的研究和应用。

**Abstract:** Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.

</details>


### [2] [Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces](https://arxiv.org/abs/2512.22227)
*Sophie Zhao*

Main category: cs.CL

> 该研究展示了变压器模型编码的句子嵌入中存在与人类可理解的认知或心理属性对齐的层级结构。研究使用线性和浅非线性探测器可以可靠地恢复这些注释，并且观察到的结构不仅仅是由表面词统计引起的。

<details>
  <summary>Details</summary>

**Motivation:** 探索变压器模型的嵌入空间是否编码了与人类认知或心理属性对齐的等级结构。

**Method:** Structure

**Result:** {"tldr": "该研究展示了变压器模型编码的句子嵌入中存在与人类可理解的认知或心理属性对齐的层级结构。研究使用线性和浅非线性探测器可以可靠地恢复这些注释，并且观察到的结构不仅仅是由表面词统计引起的。", "motivation": "探索变压器模型的嵌入空间是否编码了与人类认知或心理属性对齐的等级结构。", "method": "构建了一个包含480个自然语言句子的数据集，这些句子被注释有连续的等级能量得分和七个有序认知类别中的离散级别标签。使用固定句子嵌入，通过线性和浅非线性探测器评估这些注释的可恢复性。", "result": "连续得分和级别标签可以通过探测器可靠地恢复，且浅非线性探测器比线性探测器提供了更好的性能。质性分析揭示了嵌入空间中的平滑低到高梯度及相邻级别的混淆。", "conclusion": "结果表明，变压器嵌入空间显示了一个与人类定义的认知属性对齐的层级几何组织结构。"}

**Conclusion:** 结果表明，变压器嵌入空间显示了一个与人类定义的认知属性对齐的层级几何组织结构。

**Abstract:** Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores and discrete tier labels spanning seven ordered cognitive categories. Using fixed sentence embeddings from multiple transformer models, we evaluate the recoverability of these annotations via linear and shallow nonlinear probes. Across models, both continuous scores and tier labels are reliably decodable, with shallow nonlinear probes providing consistent performance gains over linear probes. Lexical TF-IDF baselines perform substantially worse, indicating that the observed structure is not attributable to surface word statistics alone. Nonparametric permutation tests further confirm that probe performance exceeds chance under label-randomization nulls. Qualitative analyses using UMAP visualizations and confusion matrices reveal smooth low-to-high gradients and predominantly adjacent-tier confusions in embedding space. Taken together, these results provide evidence that transformer embedding spaces exhibit a hierarchical geometric organization aligned with human-defined cognitive attributes, while remaining agnostic to claims of internal awareness or phenomenology.

</details>


### [3] [SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents](https://arxiv.org/abs/2512.22322)
*Shaofei Cai,Yulei Qin,Haojia Lin,Zihan Xu,Gang Li,Yuchen Shi,Zongyi Li,Yong Mao,Siqi Cai,Xiaoyu Tan,Yitao Liang,Ke Li,Xing Sun*

Main category: cs.CL

> SmartSnap introduces a self-verifying agent paradigm for agentic reinforcement learning that uses snapshots to achieve scalable and reliable task verification, leading to significant performance improvements.

<details>
  <summary>Details</summary>

**Motivation:** To address the scalability issues of verification in agentic reinforcement learning by shifting from passive post-hoc verification to proactive self-verification through carefully selected snapshots.

**Method:** SmartSnap, a self-verifying agent paradigm that uses curated snapshot evidences guided by the 3C Principles (Completeness, Conciseness, and Creativity) to enable scalable task verification in agentic reinforcement learning.

**Result:** Experiments show performance gains up to 26.08% and 16.66% for 8B and 30B models, and competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

**Conclusion:** The SmartSnap paradigm facilitates the creation of scalable, efficient, and self-verifying agents in complex GUI tasks using minimal snapshot evidences for task validation.

**Abstract:** Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

</details>


### [4] [The Syntax of qulk-clauses in Yemeni Ibbi Arabic: A Minimalist Approach](https://arxiv.org/abs/2512.22376)
*Zubaida Mohammed Albadani,Mohammed Q. Shormani*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This study investigates the syntax of qulk-clauses in Yemeni Ibbi Arabic (YIA) within the Minimalist Program. The construction qulk-clause, a morphologically fused form meaning 'I said,' introduces embedded declarative interrogative, and imperative clauses, often eithout complementizer. The central proposal of this paper is that qulk-clauses are biclausal structures in which qulk functions a clause-embedding predicate sec;ecting a dull CP complement. By applying core minimalist operations, viz., Merge, Move, Agree, and Spell-out, the study provides a layered syntactic analysis of qulk-clauses, for illustrating how their derivation proceeds through standard computational steps and post-syntactic processes such as Morphological Merger. The proposal also accounts for dialect-specific features like bipartite negation, cliticization, and CP embedding. The findings offer theoretical contributions to generative syntax, specifically minimalism. The study concludes raising theoretical questions concerning extending the analysis to the addressee-clause kil-k 'you said'. It also provides insights into the possibility of the universality of minimalism.

</details>


### [5] [Towards Efficient Post-Training via Fourier-Driven Adapter Architectures](https://arxiv.org/abs/2512.22378)
*Donggyun Bae,Jongil Park*

Main category: cs.CL

> 研究介绍了一种名为傅里叶激活适配器（FAA）的新框架，通过加入随机傅里叶特征使得模型在微调过程中可以进行频率感知的语义信息调制，实验验证了这种方法在多个基准测试中的良好效果和低计算开销。

<details>
  <summary>Details</summary>

**Motivation:** 设计的目的是让模型在适应过程中选择性地强调信息频段，同时保持冻结主干的表示能力，从而实现参数高效微调。

**Method:** 提出了一种名为傅里叶激活适配器（FAA）的新颖框架，用于大型预训练语言模型的参数高效微调。通过在轻量级适配器模块中加入随机傅里叶特征，FAA 将中间表示分解为互补的低频和高频成分，实现了对语义信息的频率感知调制。

**Result:** 在 GLUE、E2E NLG 和指令微调基准上的广泛实验表明，FAA 一致实现了与现有参数高效微调方法相当或更优的性能，同时维持较低的计算和内存开销。

**Conclusion:** 消融研究进一步证实了频率感知激活和自适应加权机制的有效性，突显了 FAA 作为大型语言模型后期训练的稳健和高效方法的地位。

**Abstract:** We propose a novel framework, termed Fourier-Activated Adapter (FAA), for parameter-efficient fine-tuning of large pre-trained language models. By incorporating random Fourier features into lightweight adapter modules, FAA decomposes intermediate representations into complementary low- and high-frequency components, enabling frequency-aware modulation of semantic information. This design allows the model to selectively emphasize informative frequency bands during adaptation while preserving the representational capacity of the frozen backbone. Extensive experiments on GLUE, E2E NLG, and instruction-tuning benchmarks demonstrate that FAA consistently achieves competitive or superior performance compared to existing parameter-efficient fine-tuning methods, while maintaining low computational and memory overhead. Ablation studies further verify the effectiveness of frequency-aware activation and adaptive weighting mechanisms, highlighting FAA as a robust and efficient approach for post-training large language models.

</details>


### [6] [LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition](https://arxiv.org/abs/2512.22385)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

> 本文提出了一种LLM引导样例选择框架，以解决最先进的HAR方法在选择具有代表性的少量穿戴传感器活动样本时的局限性，通过结合语义推理及几何信息，该框架在UCI-HAR数据集上表现出优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决最先进的HAR方法中对大量标记数据集的依赖以及仅依靠几何样品选择的问题，特别是在区分如行走、上楼行走和下楼行走这类相似的穿戴传感器活动方面。

**Method:** 该方法引入了由LLM生成的知识先验，捕捉特征重要性、类间混淆性和样例预算乘数，以此指导样例得分与选择，并结合边距验证线索、PageRank中心性、枢纽惩罚和设施定位优化，获得精简且信息丰富的样例集合。

**Result:** 该框架在严格的少量样本条件下，使用UCI-HAR数据集进行评估，实现了88.78%的宏F1分数，超越了诸如随机抽样、herding和k-center等经典方法。结果表明，当与结构和几何线索结合时，源自LLM的语义先验为穿戴传感器HAR中具有代表性的传感器样本选择奠定了更坚实的基础。

**Conclusion:** 结论是，融合了LLM生成的知识先验的样品选择框架，在选择代表性的穿戴传感器少样本HAR样本时，能够提供更强有力的支持。

**Abstract:** In this paper, we propose an LLM-Guided Exemplar Selection framework to address a key limitation in state-of-the-art Human Activity Recognition (HAR) methods: their reliance on large labeled datasets and purely geometric exemplar selection, which often fail to distinguish similar weara-ble sensor activities such as walking, walking upstairs, and walking downstairs. Our method incorporates semantic reasoning via an LLM-generated knowledge prior that captures feature importance, inter-class confusability, and exemplar budget multipliers, and uses it to guide exemplar scoring and selection. These priors are combined with margin-based validation cues, PageRank centrality, hubness penalization, and facility-location optimization to obtain a compact and informative set of exemplars. Evaluated on the UCI-HAR dataset under strict few-shot conditions, the framework achieves a macro F1-score of 88.78%, outperforming classical approaches such as random sampling, herding, and $k$-center. The results show that LLM-derived semantic priors, when integrated with structural and geometric cues, provide a stronger foundation for selecting representative sensor exemplars in few-shot wearable-sensor HAR.

</details>


### [7] [Hallucination Detection and Evaluation of Large Language Model](https://arxiv.org/abs/2512.22416)
*Chenggong Zhang,Haopeng Wang*

Main category: cs.CL

> Integrate HHEM for efficient and accurate detection of hallucinations in LLMs, showing improvements in speed and accuracy, and find that larger models have fewer hallucinations.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of hallucinations in LLMs which can generate misleading content, and to improve upon existing methods that have high computational costs.

**Method:** We integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based framework for improved efficiency and accuracy in detecting hallucinations in LLMs. We also introduce segment-based retrieval to improve detection in summarization tasks.

**Result:** HHEM reduces evaluation time significantly from 8 hours to 10 minutes and achieves the highest accuracy (82.2%) and TPR (78.9%) in non-fabrication checking in several LLMs. Larger models tend to have fewer hallucinations, while medium-sized models show higher instability.

**Conclusion:** Structured evaluation frameworks balancing computational efficiency and robust factual validation are essential for enhancing the reliability of content generated by LLMs.

**Abstract:** Hallucinations in Large Language Models (LLMs) pose a significant challenge, generating misleading or unverifiable content that undermines trust and reliability. Existing evaluation methods, such as KnowHalu, employ multi-stage verification but suffer from high computational costs. To address this, we integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based framework that operates independently of LLM-based judgments, significantly improving efficiency while maintaining high detection accuracy. We conduct a comparative analysis of hallucination detection methods across various LLMs, evaluating True Positive Rate (TPR), True Negative Rate (TNR), and Accuracy on question-answering (QA) and summarization tasks. Our results show that HHEM reduces evaluation time from 8 hours to 10 minutes, while HHEM with non-fabrication checking achieves the highest accuracy \(82.2\%\) and TPR \(78.9\%\). However, HHEM struggles with localized hallucinations in summarization tasks. To address this, we introduce segment-based retrieval, improving detection by verifying smaller text components. Additionally, our cumulative distribution function (CDF) analysis indicates that larger models (7B-9B parameters) generally exhibit fewer hallucinations, while intermediate-sized models show higher instability. These findings highlight the need for structured evaluation frameworks that balance computational efficiency with robust factual validation, enhancing the reliability of LLM-generated content.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

> 本文通过研究文本到视频的扩散模型的动作和外观编辑之间的关系，系统地描述了动作-外观边界的概念，并提出了一个简化的方法来实现动作转移。

<details>
  <summary>Details</summary>

**Motivation:** 现有的实践者常常利用一个经验法则，即在早期时间步长，主要定义动作和布局，而后期的时间步长则精炼外观，但这一行为尚未被系统地描述和理解。

**Method:** 文本到视频的扩散模型通过逐步去噪来合成时间动作和空间外观。作者通过研究在指定时间段内注入新条件时，外观编辑与动作保留之间的权衡来代理视频扩散时间段中的动作编码。通过一种大型定量研究的方式表征这种代理方式，将动作与外观进行定量划分。

**Result:** 通过对不同架构的持续识别，发现了一个早期阶段，此阶段以动作为主导，以及一个后期阶段，此阶段以外观为主导。由此得出了一个在时间步长中的操作性动作-外观边界。

**Conclusion:** 通过限制训练和推理到动作为主导的阶段来简化当前的一次性动作定制范式，并且无需辅助去偏模块或特殊目标，就能实现强大的动作转移。这一分析将一个广泛使用的经验法则转化为一种时空分离原则，并且提出了一个时段约束的配方，可以作为现成的集成纳入现有的动作转移和编辑方法中。

**Abstract:** Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

</details>


### [9] [Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment](https://arxiv.org/abs/2512.22177)
*Dawnena Key*

Main category: cs.CV

> 该论文描述了一种使用3D CNN和LSTM结合的方法实现的手语识别系统，能够在网络摄像头视频流中实时识别ASL手势，系统在多个数据集上训练，并部署在AWS及OAK-D摄像头上。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在为全球的聋哑和重听个体提供一种实时的ASL识别解决方案，以克服沟通障碍。

**Method:** 该论文提出了一个使用结合3D卷积神经网络(3D CNN)与长短时记忆(LSTM)网络的混合深度学习架构的实时美式手语(ASL)识别系统。系统处理网络摄像头视频流，识别单词级的ASL手势，来应对全球7000多万聋哑和重听个体的沟通障碍。架构利用3D卷积从视频帧中捕捉时空特征，随后用LSTM层模拟手语手势中的序列依赖。

**Result:** 该系统在训练的多个数据集上实现了0.71到0.99的F1分数，显示了识别不同手语类别的广泛能力。

**Conclusion:** 该手语识别系统在多种评估指标下显示出强大的识别能力，并为实际应用场景提供了部署指导，有助于提高聋哑和重听个体的沟通能力。

**Abstract:** This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.

</details>


### [10] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

> 本文介绍了一种将AI与局部线性嵌入（LLE）集成以改善医疗数据处理的方法，特别是在医疗计费和转录服务方面。通过实际实验，展示了这种方法在提高数据处理准确性及操作效率方面的显著效果。

<details>
  <summary>Details</summary>

**Motivation:** 随着AI在医疗领域的迅速发展，本文旨在探索AI尤其是AI与LLE结合如何提高医疗计费和转录服务的效率和准确性，以减少人为错误并优化操作流程。

**Method:** 本文提出了一种AI增强的LLE模型来处理高维医疗数据，以提高医疗计费系统和转录服务的精度和效率。通过一个全面的数学模型详细描述了这种方法。

**Result:** 实验结果显示，AI增强的LLE模型在医疗数据处理的精确度和操作效率上有了显著提升。

**Conclusion:** 本文证明了AI增强的LLE在医疗数据分析中的潜力，并为未来更广泛的医疗应用研究奠定了基础。

**Abstract:** The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

</details>


### [11] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

> VISTA通过显式的知识瓶颈分离感知与推理，显著提升了视觉语言模型的鲁棒性，同时减少了对虚假关联的依赖。

<details>
  <summary>Details</summary>

**Motivation:** 端到端视觉语言模型（VLMs）通常利用与其视觉因果证据无关的虚假关联来回答视觉问题，并且在微调时可能变得更倾向于依赖这些非因果因素。为了解决这个问题，提出了VISTA框架。

**Method:** 我们介绍了VISTA框架，它通过显式的知识瓶颈将感知与推理模块化分离。在这个框架中，一个冻结的VLM传感器被限制在短小且客观的感知查询中，而一个仅基于文本的LLM推理器则负责分解问题，规划查询，以及以自然语言聚合视觉事实。这种受控的接口定义了一个适用于强化学习的奖励对齐环境，用于训练无偏的视觉推理。

**Result:** 使用Qwen2.5-VL和Llama3.2-Vision传感器，并通过GRPO训练仅使用641个精心策划的多步骤问题，VISTA在SpuriVerse上显著提升了对现实世界虚假关联的鲁棒性（分别提高了+16.29%和+6.77%），同时在MMVP和平衡的SeedBench子集上保持竞争力。此外，VISTA能够跨未知VLM传感器转移且能识别和恢复VLM感知失败。

**Conclusion:** 总的来说，VISTA的推理轨迹比端到端VLM基线更中立，依赖虚假属性较少，且更明确地以视觉证据为依据。

**Abstract:** End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

</details>


### [12] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

> Introduces SAMM2D for aneurysm detection with strong pretraining, achieving higher performance without data augmentation.

<details>
  <summary>Details</summary>

**Motivation:** To improve the detection of aneurysms, addressing the challenges of subtle morphology, class imbalance, and lack of annotated data.

**Method:** A dual-encoder framework named SAMM2D was introduced, which does not require data augmentation and uses a strong pretrained backbone.

**Result:** Achieved an AUC of 0.686 with a 32% improvement over the clinical baseline. The unaugmented baseline model outperformed all augmented variants.

**Conclusion:** Clinical meaningful focus on relevant vascular regions, suggesting a greater benefit from strong pretraining than from complex augmentation pipelines.

**Abstract:** Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

</details>


### [13] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

> 论文提出了HookMIL，一种在病理学全片图像分析中效果好、计算效率高且可解释性强的多重实例学习框架。

<details>
  <summary>Details</summary>

**Motivation:** 传统的多重实例学习方法在处理全片切片图像时容易丢失重要信息，而基于变换器的方法虽然更具有表达能力，但计算复杂度高且存在冗余计算。 HookMIL旨在解决这些问题，提供了更有效的上下文信息处理方式。

**Method:** 该论文提出了一种名为HookMIL的新型多重实例学习框架，它结合了紧凑的可学习hook token来进行结构化的上下文聚合。这些tokens可以从视觉特征、文本嵌入或空间特征中初始化，从而利用多模态信息来加速收敛并提高表示质量。在训练过程中，tokens通过具有线性复杂度的双向注意力机制与实例进行交互。为了促进token的差异化，引入了一种Hook多样性损失，并设计了一种token间的通信机制以减少冗余。

**Result:** 在四个公开的病理学数据集上的广泛实验表明，HookMIL达到了最先进的性能，同时在计算效率和可解释性上也有提升。

**Conclusion:** HookMIL框架通过使用可学习的hook token及新颖的设计，不仅提高了多重实例学习方法在病理学全片图像分析中的性能，还在计算效率和模型解释性上有所改善。

**Abstract:** Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

</details>


### [14] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

> Develop a fast hybrid pipeline Tiny-YOLOSAM combining YOLOv12 and TinySAM for efficient and high-quality segmentation on COCO val2017.

<details>
  <summary>Details</summary>

**Motivation:** To address the computational inefficiency of TinySAM and improve segmentation coverage and speed in latency-sensitive scenarios.

**Method:** The paper introduces Tiny-YOLOSAM, a hybrid approach that leverages YOLOv12 for generating bounding box prompts and TinySAM for segmenting salient objects, complemented by sparse point prompts for uncovered areas.

**Result:** The proposed method improve AR from 16.4% to 77.1% and mIoU from 19.2% to 67.8% on COCO val2017, while reducing runtime from 49.20s/image to 10.39s/image.

**Conclusion:** The combination of a detector for initial prompts and a segmenter with targeted sparse sampling is shown to be effective for achieving better segmentation quality and efficiency.

**Abstract:** The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

</details>


### [15] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

> 本文提出了一种新的多模式解释模型，用于糖尿病性视网膜病变的检测和分类，能够提供自然语言解释，并且在OCT和眼底图像中提供视觉可解释性，以解决现有诊断模型的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 糖尿病性视网膜病变（DR）是导致视力丧失的主要原因，需要早期检测来保护视力。然而，由于缺乏医生资源，DR往往无法得到诊断。虽然基于AI的模型目前依赖病变分割来提高解释性，但手动标注病变对临床医生来说是不切实际的。为了克服这些问题，本文旨在开发一种能够用自然语言识别单个DR病灶的定量检测系统，以提供临床诊断、治疗和研究的多功能应用。

**Method:** 本文介绍了一种新的多模式可解释性模型，该模型利用具有少量样本学习的视觉语言模型（VLM），模拟眼科医生的推理过程，分析视网膜区域内的病变分布，针对眼底图像进行分类。该模型生成Grad-CAM热图，展示OCT和眼底图像中各个神经元的权重，突出显示分类糖尿病性视网膜病变严重程度的贡献区域。

**Result:** 该创新方法利用3000张眼底图像和1000张OCT图像的数据集，解决了当前糖尿病视网膜病变诊断中的关键限制，为改善患者结果提供了一种实用和全面的工具。

**Conclusion:** 本文中的模型和方法通过提供一种模拟眼科医生思考过程的、具有高度视觉解释能力和多模态的模型，填补了现有糖尿病视网膜病变诊断系统的关键空白。这为普及和改善治疗提供了新的可能。

**Abstract:** Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

</details>


### [16] [TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting](https://arxiv.org/abs/2512.22203)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

> 本文提出TCFormer，一种轻量化、弱监督的Transformer人群计数框架，参数量较小，实现高精度人群计数，在多数据集上验证了其在参数效率和计数准确性之间的平衡。

<details>
  <summary>Details</summary>

**Motivation:** 人群计数通常依赖于劳动密集型的点级标注和计算密集型的基本模型，限制了其规模和在资源受限环境中的部署。为了解决这些挑战而提出该方法。

**Method:** 该论文提出了一种名为TCFormer的轻量级、弱监督Transformer-based人群计数框架。首先，采用了强大的视觉Transformer作为特征提取器，以较小的内存占用提供语义丰富的人群特征。其次，设计了可学习密度加权平均模块以补偿空间监督的缺失，该模块根据预测的密度得分动态重新加权局部标记。此外，引入了一个密度级别的分类损失，将人群密度离散化为不同的等级，从而增强模型对不同人群密度级别的分类能力。

**Result:** TCFormer在四个基准数据集（包括ShanghaiTech A/B、UCF-QNRF和NWPU）上的广泛实验表明，该方法在参数效率和计数准确性之间取得了优越的平衡。

**Conclusion:** TCFormer框架可以在边缘设备上实现高精度的人群计数任务，且参数量仅为500万，展示了其在有限计算资源下的高效性和准确性。

**Abstract:** Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.

</details>
