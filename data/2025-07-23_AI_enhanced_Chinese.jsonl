{"id": "2507.15863", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15863", "abs": "https://arxiv.org/abs/2507.15863", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "title": "eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs", "comment": "8 pages;1 figure;5 tables", "summary": "We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge)\nModule, a secure and scalable Retrieval-Augmented Generation pipeline designed\nspecifically for enterprise document question answering. Designed and\nimplemented by eSapiens, the system ingests heterogeneous content (PDF, Office,\nweb), splits it into 1,000-token overlapping chunks, and indexes them in a\nhybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via\ncombined vector+BM25 search, reranked with Cohere, and answered by an LLM using\nCO-STAR prompt engineering. A LangGraph verifier enforces citation overlap,\nregenerating answers until every claim is grounded. On four LegalBench subsets,\n1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank\nboosts Precision@10 by approximately 7 pp; the verifier raises TRACe\nUtilization above 0.50 and limits unsupported statements to less than 3%. All\ncomponents run in containers, enforce end-to-end TLS 1.3 and AES-256. These\nresults demonstrate that the DEREK module delivers accurate, traceable, and\nproduction-ready document QA with minimal operational overhead. The module is\ndesigned to meet enterprise demands for secure, auditable, and context-faithful\nretrieval, providing a reliable baseline for high-stakes domains such as legal\nand finance.", "AI": {"tldr": "DEREK模块是一个安全和可扩展的Retrieval-Augmented Generation系统，专门用于企业文档问答，从多方面参数提高了系统性能，保证了答案的准确性和可信度。", "motivation": "开发DEREK模块是为了满足企业对安全、可审计且忠实于上下文的检索系统的需求，特别适用于法律和金融等高风险领域。", "method": "企业文档问答的Deep Extraction & Reasoning Engine for Knowledge（DEREK）模块，专门用于处理异构内容（如PDF、Office文件和网络内容），通过1000-token重叠块进行索引，并使用混合HNSW+BM25存储。用户查询经过GPT-4o优化，并通过向量+BM25搜索检索，Cohere进行重新排序，LLM使用CO-STAR提示工程回答问题。LangGraph验证器通过确保每个声明都基于引用，直到满足条件为止，来保证答案的可信度。", "result": "在四个LegalBench子集上测试表明，1000-token块帧提高了Recall@50约1个百分点，而混合搜索+重新排序提升了Precision@10约7个百分点。验证器提高了TRACe利用率到0.5以上，并将未经支持的陈述限制在3%以下。", "conclusion": "DEREK模块能够在最小的操作开销下，提供准确、可追溯和生产就绪的文档问答系统，满足企业对安全、可审计和忠实于上下文的检索系统的需求，在高风险领域提供了可靠的基线。"}}
{"id": "2507.15864", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15864", "abs": "https://arxiv.org/abs/2507.15864", "authors": ["Guowen Yuan", "Tien-Hsuan Wu", "Lianghao Xia", "Ben Kao"], "title": "Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity", "comment": null, "summary": "We study the problem of named entity recognition (NER) based on demonstration\nlearning in low-resource scenarios. We identify two issues in demonstration\nconstruction and model training. Firstly, existing methods for selecting\ndemonstration examples primarily rely on semantic similarity; We show that\nfeature similarity can provide significant performance improvement. Secondly,\nwe show that the NER tagger's ability to reference demonstration examples is\ngenerally inadequate. We propose a demonstration and training approach that\neffectively addresses these issues. For the first issue, we propose to select\nexamples by dual similarity, which comprises both semantic similarity and\nfeature similarity. For the second issue, we propose to train an NER model with\nadversarial demonstration such that the model is forced to refer to the\ndemonstrations when performing the tagging task. We conduct comprehensive\nexperiments in low-resource NER tasks, and the results demonstrate that our\nmethod outperforms a range of methods.", "AI": {"tldr": "研究基于演示学习的低资源场景下的命名实体识别问题，提出结合双重相似性选择示例及对抗性示例训练模型来改善性能。", "motivation": "现有的演示示例选择方法主要依赖于语义相似性，这限制了模型在低资源场景中的性能，并且模型参考示例的能力不足。", "method": "我们提出了一个结合语义相似性和特征相似性的双重相似性方法来选择示例，并通过对抗性示例训练NER模型，以解决模型参考示例能力不足的问题。", "result": "我们在低资源NER任务中进行了全面的实验，结果表明我们的方法优于多种现有方法。", "conclusion": "通过使用提出的演示示例选择方法和对抗性示例训练技术，在低资源的命名实体识别任务中取得了优于其他方法的性能。"}}
{"id": "2507.15868", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15868", "abs": "https://arxiv.org/abs/2507.15868", "authors": ["Altynbek Ismailov", "Salia Asanova"], "title": "Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models", "comment": null, "summary": "Large language models (LLMs) now write code in settings where misreading a\nsingle word can break safety or cost money, yet we still expect them to\noverlook stray typos. To probe where useful robustness ends and harmful\ninsensitivity begins, we compile 50 LeetCode problems and craft three minimal\nprompt perturbations that should vary in importance: (i) progressive\nunderspecification deleting 10 % of words per step; (ii) lexical flip swapping\na pivotal quantifier (\"max\" to \"min\"); and (iii) jargon inflation replacing a\ncommon noun with an obscure technical synonym. Six frontier models, including\nthree \"reasoning-tuned\" versions, solve each mutated prompt, and their Python\noutputs are checked against the original test suites to reveal whether they\nreused the baseline solution or adapted. Among 11 853 generations we observe a\nsharp double asymmetry. Models remain correct in 85 % of cases even after 90 %\nof the prompt is missing, showing over-robustness to underspecification, yet\nonly 54 % react to a single quantifier flip that reverses the task, with\nreasoning-tuned variants even less sensitive than their bases. Jargon edits lie\nin between, passing through 56 %. Current LLMs thus blur the line between\nharmless noise and meaning - changing edits, often treating both as ignorable.\nMasking salient anchors such as function names can force re - evaluation. We\nadvocate evaluation and training protocols that reward differential\nsensitivity: stay steady under benign noise but adapt - or refuse - when\nsemantics truly change.", "AI": {"tldr": "研究对前沿大型语言模型进行测试，发现其在处理提示扰动时存在过度鲁棒性和对关键变化的迟钝，主张评估和训练中应奖励差异敏感性。", "motivation": "研究动机在于探讨大型语言模型在面对输入提示扰动时的鲁棒性和敏感性之间的界限。", "method": "该研究通过编制50个LeetCode问题并设计三种最小提示扰动来测试大型语言模型的鲁棒性。这些扰动分别是逐步欠规范（每步删除10%的单词）、词汇翻转（如将“max”翻转为“min”）以及术语膨胀（用晦涩的技术同义词替换一个常见的名词）。", "result": "在总共11,853次生成结果中发现，模型在面对线索减少时显示出过度鲁棒性，但对于关键变化过于迟钝。", "conclusion": "研究发现，尽管模型在90%的提示缺失时仍有85%的情况下保持正确，但对于单一的量化词翻转这种任务反转，只有54%的模型能作出反应，显示出过度鲁棒性及对关键变化的忽视。研究倡导在模型评估和训练中实施奖励差异敏感性的协议。"}}
{"id": "2507.16002", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16002", "abs": "https://arxiv.org/abs/2507.16002", "authors": ["Sumit Singh", "Rohit Mishra", "Uma Shanker Tiwary"], "title": "Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation", "comment": null, "summary": "One major challenge in natural language processing is named entity\nrecognition (NER), which identifies and categorises named entities in textual\ninput. In order to improve NER, this study investigates a Hindi NER technique\nthat makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and\nGenerative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf\n(Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments\nthe data with retrieved data from external relevant contexts, notably from\nWikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA.\nHowever, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER\ngeneration. Our investigation shows that the mentioned language models (LMs)\nwith Retrieval Augmentation (RA) outperform baseline methods that don't\nincorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69\nand 0.495, respectively, without RA and increase to 0.70 and 0.71,\nrespectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B\nby a significant margin. On the other hand the generative models which are not\nfine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA\nwell; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval\ncontext. The findings show that RA significantly improves performance,\nespecially for low-context data. This study adds significant knowledge about\nhow best to use data augmentation methods and pretrained models to enhance NER\nperformance, particularly in languages with limited resources.", "AI": {"tldr": "研究通过使用印地语预训练模型、生成模型和检索增强技术提高印地语NER的性能，发现检索增强特别受生成模型GPT3.5-turbo欢迎，而对大型语言模型的效果提升有限。", "motivation": "动机在于改进印地语命名实体识别（NER）技术。", "method": "研究调查了一种用于印地语命名实体识别（NER）的技术，利用了印地语特定的预训练编解码器（MuRIL和XLM-R），生成模型（如Llama-2-7B-chat-hf, Llama-2-70B-chat-hf, Llama-3-70B-Instruct, GPT3.5-turbo）以及从维基百科提取的相关外部上下文数据进行数据增强。", "result": "研究发现，使用检索增强（RA）的语言模型在大多数情况下优于不使用RA的基线方法。具体来说，MuRIL和XLM-R在没有RA时的宏观F1得分分别是0.69和0.495，在有RA的情况下分别增加到0.70和0.71。调整后的Llama2-7B显著优于未调整的Llama2-7B版本。而对于未进行微调的生成模型，在有额外数据的情况下，性能也有所提高。GPT3.5-turbo很好地采用了RA，但Llama2-70B和llama3-70B并未对RA做出同等程度的适应。", "conclusion": "研究结果表明，RA显着提升了性能，特别是对于低语境数据。这项研究表明，如何有效地利用数据增强技术和预训练模型来提高NER性能，尤其是在资源有限的语言中。"}}
{"id": "2507.15878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15878", "abs": "https://arxiv.org/abs/2507.15878", "authors": ["Bin Han", "Jonathan Gratch"], "title": "Salience Adjustment for Context-Based Emotion Recognition", "comment": null, "summary": "Emotion recognition in dynamic social contexts requires an understanding of\nthe complex interaction between facial expressions and situational cues. This\npaper presents a salience-adjusted framework for context-aware emotion\nrecognition with Bayesian Cue Integration (BCI) and Visual-Language Models\n(VLMs) to dynamically weight facial and contextual information based on the\nexpressivity of facial cues. We evaluate this approach using human annotations\nand automatic emotion recognition systems in prisoner's dilemma scenarios,\nwhich are designed to evoke emotional reactions. Our findings demonstrate that\nincorporating salience adjustment enhances emotion recognition performance,\noffering promising directions for future research to extend this framework to\nbroader social contexts and multimodal applications.", "AI": {"tldr": "论文提出了一种调整显著性的框架，利用贝叶斯提示整合和视觉-语言模型在动态社交环境下进行情绪识别，测试结果表明这种方法可以提升情绪识别性能。", "motivation": "本研究的动机在于理解和解决情绪识别中面部表情和情境线索相互作用的复杂性，特别是在动态社交背景下情绪识别的挑战。", "method": "本论文提出了一个基于贝叶斯提示整合（BCI）和视觉-语言模型（VLMs）的调整显著性框架，用于在动态社交环境中识别情绪。该框架能够根据面部线索的表达性动态地加权面部和情境信息。", "result": "通过使用人类标注和自动情绪识别系统在囚徒困境场景中的测试，发现引入显著性调整可以提高情绪识别的效果。", "conclusion": "研究结果表明，通过整合显著性调整，可以提升情绪识别性能，为未来的社交环境的扩展研究和多模态应用提供了良好的方向。"}}
{"id": "2507.16003", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16003", "abs": "https://arxiv.org/abs/2507.16003", "authors": ["Benoit Dherin", "Michael Munn", "Hanna Mazzawi", "Michael Wunder", "Javier Gonzalvo"], "title": "Learning without training: The implicit dynamics of in-context learning", "comment": null, "summary": "One of the most striking features of Large Language Models (LLM) is their\nability to learn in context. Namely at inference time an LLM is able to learn\nnew patterns without any additional weight update when these patterns are\npresented in the form of examples in the prompt, even if these patterns were\nnot seen during training. The mechanisms through which this can happen are\nstill largely unknown. In this work, we show that the stacking of a\nself-attention layer with an MLP, allows the transformer block to implicitly\nmodify the weights of the MLP layer according to the context. We argue through\ntheory and experimentation that this simple mechanism may be the reason why\nLLMs can learn in context and not only during training. Specifically, we show\nunder mild simplifying assumptions how a transformer block implicitly\ntransforms a context into a low-rank weight-update of the MLP layer.", "AI": {"tldr": "Transformer blocks modify MLP weights according to context, possibly explaining LLMs' contextual learning without additional training.", "motivation": "To understand the mechanisms behind LLMs' ability to learn in context without additional training.", "method": "Theoretical analysis and experimentation on how a transformer block with a self-attention layer and an MLP layer implicitly transforms the context into a low-rank weight-update for the MLP layer.", "result": "Demonstrates through theory and experiments that a transformer block can implicitly adjust MLP weights based on context under certain assumptions.", "conclusion": "Transformer blocks can implicitly modify MLP weights according to context, potentially explaining LLMs' ability to learn in context without additional training."}}
{"id": "2507.15882", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15882", "abs": "https://arxiv.org/abs/2507.15882", "authors": ["Goeric Huybrechts", "Srikanth Ronanki", "Sai Muralidhar Jayanthi", "Jack Fitzgerald", "Srinivasan Veeravanallur"], "title": "Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark", "comment": null, "summary": "The proliferation of multimodal Large Language Models has significantly\nadvanced the ability to analyze and understand complex data inputs from\ndifferent modalities. However, the processing of long documents remains\nunder-explored, largely due to a lack of suitable benchmarks. To address this,\nwe introduce Document Haystack, a comprehensive benchmark designed to evaluate\nthe performance of Vision Language Models (VLMs) on long, visually complex\ndocuments. Document Haystack features documents ranging from 5 to 200 pages and\nstrategically inserts pure text or multimodal text+image \"needles\" at various\ndepths within the documents to challenge VLMs' retrieval capabilities.\nComprising 400 document variants and a total of 8,250 questions, it is\nsupported by an objective, automated evaluation framework. We detail the\nconstruction and characteristics of the Document Haystack dataset, present\nresults from prominent VLMs and discuss potential research avenues in this\narea.", "AI": {"tldr": "我们提出了Document Haystack，这是一个评估视觉语言模型处理长文档能力的新基准测试。它包括5到200页的文档，支撑了一种客观、自动的评估框架，并探讨了该领域的研究前景。", "motivation": "当前，多模态大型语言模型在处理长文档方面仍然研究较少，缺乏合适的基准测试。为了解决这个问题，本研究提出了Document Haystack基准测试。", "method": "我们介绍了一个名为Document Haystack的基准测试，用于评估视觉语言模型处理长且视觉复杂的文档的能力。该基准测试包含从5页到200页不等的文档，并在文档的不同深度处插入纯文本或文本+图像的“针”，以挑战VLMs的检索能力。", "result": "介绍了Document Haystack数据集的构建和特性，并展示了前沿视觉语言模型的初步结果，同时探讨了这一领域潜在的研究方向。", "conclusion": "Document Haystack作为一个新的基准测试，展示了其在评估视觉语言模型处理长文档能力方面的价值，并为未来的研究提供了指导。"}}
{"id": "2507.16007", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16007", "abs": "https://arxiv.org/abs/2507.16007", "authors": ["Hannah Rashkin", "Elizabeth Clark", "Fantine Huot", "Mirella Lapata"], "title": "Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback", "comment": "ACL 2025 main conference", "summary": "Can LLMs provide support to creative writers by giving meaningful writing\nfeedback? In this paper, we explore the challenges and limitations of\nmodel-generated writing feedback by defining a new task, dataset, and\nevaluation frameworks. To study model performance in a controlled manner, we\npresent a novel test set of 1,300 stories that we corrupted to intentionally\nintroduce writing issues. We study the performance of commonly used LLMs in\nthis task with both automatic and human evaluation metrics. Our analysis shows\nthat current models have strong out-of-the-box behavior in many respects --\nproviding specific and mostly accurate writing feedback. However, models often\nfail to identify the biggest writing issue in the story and to correctly decide\nwhen to offer critical vs. positive feedback.", "AI": {"tldr": "本文探讨了LLMs在写作反馈任务中的性能，发现模型能够提供有效且准确的反馈，但在识别主要问题和选择合适反馈类型方面存在问题。", "motivation": "本文旨在探讨大型语言模型（LLMs）是否能为创意作家提供有意义的写作反馈，定义了新的任务、数据集和评估框架，以研究模型在写作反馈任务中的挑战和局限性。", "method": "通过创建一个包含1,300篇故意引入写作问题的故事的数据集，采用自动和人工评估指标来评估常用LLMs在该任务中的表现。", "result": "研究显示，当前模型在许多方面表现出强大且直接有效的行为，能够提供具体且大多准确的写作反馈。然而，模型经常无法识别故事中的主要写作问题，并且在决定提供批评性反馈还是正面反馈时常常作出错误的判断。", "conclusion": "虽然LLMs能够提供准确并具体的写作反馈，但在识别故事中的主要问题和做出正确的反馈类型判断上还存在一些局限性。"}}
{"id": "2507.15888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15888", "abs": "https://arxiv.org/abs/2507.15888", "authors": ["Leonardo Santiago Benitez Pereira", "Arathy Jeevan"], "title": "PAT++: a cautionary tale about generative visual augmentation for Object Re-identification", "comment": null, "summary": "Generative data augmentation has demonstrated gains in several vision tasks,\nbut its impact on object re-identification - where preserving fine-grained\nvisual details is essential - remains largely unexplored. In this work, we\nassess the effectiveness of identity-preserving image generation for object\nre-identification. Our novel pipeline, named PAT++, incorporates Diffusion\nSelf-Distillation into the well-established Part-Aware Transformer. Using the\nUrban Elements ReID Challenge dataset, we conduct extensive experiments with\ngenerated images used for both model training and query expansion. Our results\nshow consistent performance degradation, driven by domain shifts and failure to\nretain identity-defining features. These findings challenge assumptions about\nthe transferability of generative models to fine-grained recognition tasks and\nexpose key limitations in current approaches to visual augmentation for\nidentity-preserving applications.", "AI": {"tldr": "研究评估了身份保留图像生成在对象重新识别中的有效性，提出PAT++管道，实验表明性能下降，这揭示了生成模型的局限性。", "motivation": "虽然生成性数据增强已经在多个视觉任务中展现了其优势，但其在对象重新识别（其中保持细粒度的视觉细节至关重要）方面的影响仍然鲜为人知。", "method": "该研究提出了一种新的管道PAT++, 它结合了Diffusion Self-Distillation与已建立的Part-Aware Transformer方法，用于对象重新识别任务。", "result": "通过使用Urban Elements ReID Challenge数据集进行广泛实验，结果表明使用生成图像进行模型训练和查询扩展导致了性能下降，这归因于域转换和未能保留标识定义特征。", "conclusion": "研究结果挑战了关于生成模型在细粒度识别任务中的可转移性的假设，并暴露了当前视觉增强方法在身份保持应用中的关键限制。"}}
{"id": "2507.16011", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16011", "abs": "https://arxiv.org/abs/2507.16011", "authors": ["Hellina Hailu Nigatu", "Min Li", "Maartje ter Hoeve", "Saloni Potdar", "Sarah Chasins"], "title": "mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages", "comment": "Accepted to Findings of ACL 2025", "summary": "Knowledge Graphs represent real-world entities and the relationships between\nthem. Multilingual Knowledge Graph Construction (mKGC) refers to the task of\nautomatically constructing or predicting missing entities and links for\nknowledge graphs in a multilingual setting. In this work, we reformulate the\nmKGC task as a Question Answering (QA) task and introduce mRAKL: a\nRetrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve\nthis by using the head entity and linking relation in a question, and having\nour model predict the tail entity as an answer. Our experiments focus primarily\non two low-resourced languages: Tigrinya and Amharic. We experiment with using\nhigher-resourced languages Arabic and English for cross-lingual transfer. With\na BM25 retriever, we find that the RAG-based approach improves performance over\na no-context setting. Further, our ablation studies show that with an idealized\nretrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points\nfor Tigrinya and Amharic, respectively.", "AI": {"tldr": "本文将多语言知识图谱构建任务定义为问答任务，并使用mRAKL系统通过头实体和关系预测尾实体。实验表明，mRAKL在提格雷尼亚语和阿姆哈拉语上的准确性分别提高了4.92和8.79个百分点。", "motivation": "本文的动机是改进多语言环境中知识图谱的自动构建或预测缺失实体和链接的方法。研究集中在低资源语言(提格雷尼亚语和阿姆哈拉语)上，同时探索使用高资源语言(阿拉伯语和英语)进行跨语言传输。", "method": "将多语言知识图谱构建(mKGC)任务重新定义为问答(QA)任务，并引入基于检索增强生成(RAG)的系统mRAKL来执行mKGC。系统通过将头实体和连接关系作为问题来让模型预测尾实体作为答案。", "result": "通过BM25检索器，基于RAG的方法与无上下文方法相比提高了性能。理想化检索系统下的消融研究显示，mRAKL系统在Tigrinya和Amharic上的准确性分别提高了4.92和8.79个百分点。", "conclusion": "将mKGC问题转化为问答问题，并通过引入mRAKL系统实现，实验结果显示该方法有效提升了在低资源语言中的多语言知识图谱构建性能。"}}
{"id": "2507.15911", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15911", "abs": "https://arxiv.org/abs/2507.15911", "authors": ["Liuchi Xu", "Kang Liu", "Jinshuai Liu", "Lu Wang", "Lisheng Xu", "Jun Cheng"], "title": "Local Dense Logit Relations for Enhanced Knowledge Distillation", "comment": "Accepted by ICCV2025", "summary": "State-of-the-art logit distillation methods exhibit versatility, simplicity,\nand efficiency. Despite the advances, existing studies have yet to delve\nthoroughly into fine-grained relationships within logit knowledge. In this\npaper, we propose Local Dense Relational Logit Distillation (LDRLD), a novel\nmethod that captures inter-class relationships through recursively decoupling\nand recombining logit information, thereby providing more detailed and clearer\ninsights for student learning. To further optimize the performance, we\nintroduce an Adaptive Decay Weight (ADW) strategy, which can dynamically adjust\nthe weights for critical category pairs using Inverse Rank Weighting (IRW) and\nExponential Rank Decay (ERD). Specifically, IRW assigns weights inversely\nproportional to the rank differences between pairs, while ERD adaptively\ncontrols weight decay based on total ranking scores of category pairs.\nFurthermore, after the recursive decoupling, we distill the remaining\nnon-target knowledge to ensure knowledge completeness and enhance performance.\nUltimately, our method improves the student's performance by transferring\nfine-grained knowledge and emphasizing the most critical relationships.\nExtensive experiments on datasets such as CIFAR-100, ImageNet-1K, and\nTiny-ImageNet demonstrate that our method compares favorably with\nstate-of-the-art logit-based distillation approaches. The code will be made\npublicly available.", "AI": {"tldr": "A new logit distillation method named LDRLD is proposed, enhancing the student model's performance by capturing detailed logit information and critical inter-class relationships dynamically.", "motivation": "To delve deeper into fine-grained logit knowledge relationships and improve student model learning.", "method": "Local Dense Relational Logit Distillation (LDRLD) captures inter-class relationships through recursively decoupling and recombining logit information. An Adaptive Decay Weight (ADW) strategy is introduced to dynamically adjust weights for inter-class pairs using Inverse Rank Weighting (IRW) and Exponential Rank Decay (ERD).", "result": "Extensive experiments on CIFAR-100, ImageNet-1K, and Tiny-ImageNet show that the method outperforms state-of-the-art logit-based distillation approaches.", "conclusion": "The proposed LDRLD method improves student performance by transferring detailed logit knowledge and focusing on critical relationships, outperforming existing methods on various datasets."}}
{"id": "2507.16054", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16054", "abs": "https://arxiv.org/abs/2507.16054", "authors": ["Simon Baeuerle", "Max Radyschevski", "Ulrike Pado"], "title": "AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering", "comment": null, "summary": "In large organisations, knowledge is mainly shared in meetings, which takes\nup significant amounts of work time. Additionally, frequent in-person meetings\nproduce inconsistent documentation -- official minutes, personal notes,\npresentations may or may not exist. Shared information therefore becomes hard\nto retrieve outside of the meeting, necessitating lengthy updates and\nhigh-frequency meeting schedules.\n  Generative Artificial Intelligence (genAI) models like Large Language Models\n(LLMs) exhibit an impressive performance on spoken and written language\nprocessing. This motivates a practical usage of genAI for knowledge management\nin engineering departments: using genAI for transcribing meetings and\nintegrating heterogeneous additional information sources into an easily usable\nformat for ad-hoc searches.\n  We implement an end-to-end pipeline to automate the entire meeting\ndocumentation workflow in a proof-of-concept state: meetings are recorded and\nminutes are created by genAI. These are further made easily searchable through\na chatbot interface. The core of our work is to test this genAI-based software\ntooling in a real-world engineering department and collect extensive survey\ndata on both ethical and technical aspects. Direct feedback from this\nreal-world setup points out both opportunities and risks: a) users agree that\nthe effort for meetings could be significantly reduced with the help of genAI\nmodels, b) technical aspects are largely solved already, c) organizational\naspects are crucial for a successful ethical usage of such a system.", "AI": {"tldr": "研究展示了如何使用生成式AI技术来改进会议记录和知识管理流程，虽然技术上可行，但仍需关注伦理和组织方面的问题。", "motivation": "由于大型组织中的信息共享主要集中在会议中，并且会议的记录存在文档不一致的问题，这里利用genAI如大型语言模型（LLMs）的出色语言处理能力来改进知识管理。", "method": "本研究提出了一种基于生成式人工智能（genAI）的端到端会议文档工作流自动化方案。会议记录并使用genAI生成会议纪要，并通过聊天机器人界面进行搜索。", "result": "在工程部门进行了基于genAI的软件工具测试，并收集了有关技术与伦理方面的详尽调查数据。用户反馈表明genAI可显著降低会议时间，同时技术上的问题已大部分解决，但伦理使用方面对组织层面的规范提出了较高要求。", "conclusion": "基于genAI的会议管理工具在技术上是可行的，但在真正的应用中需要额外考虑伦理和组织因素。"}}
{"id": "2507.15915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15915", "abs": "https://arxiv.org/abs/2507.15915", "authors": ["Mohammad Asifur Rahim", "Muhammad Nazmul Arefin", "Md. Mizanur Rahman", "Md Ali Hossain", "Ahmed Moustafa"], "title": "An empirical study for the early detection of Mpox from skin lesion images using pretrained CNN models leveraging XAI technique", "comment": null, "summary": "Context: Mpox is a zoonotic disease caused by the Mpox virus, which shares\nsimilarities with other skin conditions, making accurate early diagnosis\nchallenging. Artificial intelligence (AI), especially Deep Learning (DL), has a\nstrong tool for medical image analysis; however, pre-trained models like CNNs\nand XAI techniques for mpox detection is underexplored. Objective: This study\naims to evaluate the effectiveness of pre-trained CNN models (VGG16, VGG19,\nInceptionV3, MobileNetV2) for the early detection of monkeypox using binary and\nmulti-class datasets. It also seeks to enhance model interpretability using\nGrad-CAM an XAI technique. Method: Two datasets, MSLD and MSLD v2.0, were used\nfor training and validation. Transfer learning techniques were applied to\nfine-tune pre-trained CNN models by freezing initial layers and adding custom\nlayers for adapting the final features for mpox detection task and avoid\noverfitting. Models performance were evaluated using metrics such as accuracy,\nprecision, recall, F1-score and ROC. Grad-CAM was utilized for visualizing\ncritical features. Results: InceptionV3 demonstrated the best performance on\nthe binary dataset with an accuracy of 95%, while MobileNetV2 outperformed on\nthe multi-class dataset with an accuracy of 93%. Grad-CAM successfully\nhighlighted key image regions. Despite high accuracy, some models showed\noverfitting tendencies, as videnced by discrepancies between training and\nvalidation losses. Conclusion: This study underscores the potential of\npre-trained CNN models in monkeypox detection and the value of XAI techniques.\nFuture work should address dataset limitations, incorporate multimodal data,\nand explore additional interpretability techniques to improve diagnostic\nreliability and model transparency", "AI": {"tldr": "通过使用预训练的CNN模型和迁移学习，研究探讨了猴痘早期检测的有效性，并通过Grad-CAM展示了模型的关键特征提取能力。", "motivation": "研究旨在评估预训练的CNN模型在猴痘早期检测中的有效性，并通过Grad-CAM技术增强模型的可解释性。", "method": "此研究使用了迁移学习技术微调了几个预训练的CNN模型（VGG16, VGG19, InceptionV3, MobileNetV2），用于猴痘的早期检测。通过冻结模型的初始层并添加自定义层来避免过拟合，并使用了MSLD和MSLD v2.0两个数据集进行训练和验证。此外，采用Grad-CAM技术来提高模型的可解释性。", "result": "研究结果显示，InceptionV3在二分类数据集中表现最佳，准确率达到95%；MobileNetV2在多分类数据集中表现最优，准确率达到了93%。Grad-CAM成功地突出显示了关键的图像区域。然而，某些模型显示出过拟合的倾向，表现为训练损失与验证损失之间存在差异。", "conclusion": "研究强调了预训练的CNN模型在猴痘检测中的潜力，以及XAI技术的价值。未来的工作应该改进数据集的限制，纳入多模态数据，并探索其他可解释性技术以提高诊断的可靠性和模型的透明度。"}}
{"id": "2507.16075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16075", "abs": "https://arxiv.org/abs/2507.16075", "authors": ["Rujun Han", "Yanfei Chen", "Zoey CuiZhu", "Lesly Miculicich", "Guan Sun", "Yuanjun Bi", "Weiming Wen", "Hui Wan", "Chunfeng Wen", "Solène Maître", "George Lee", "Vishy Tirumalashetty", "Emily Xue", "Zizhao Zhang", "Salem Haykal", "Burak Gokturk", "Tomas Pfister", "Chen-Yu Lee"], "title": "Deep Researcher with Test-Time Diffusion", "comment": null, "summary": "Deep research agents, powered by Large Language Models (LLMs), are rapidly\nadvancing; yet, their performance often plateaus when generating complex,\nlong-form research reports using generic test-time scaling algorithms. Drawing\ninspiration from the iterative nature of human research, which involves cycles\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\nResearcher (TTD-DR). This novel framework conceptualizes research report\ngeneration as a diffusion process. TTD-DR initiates this process with a\npreliminary draft, an updatable skeleton that serves as an evolving foundation\nto guide the research direction. The draft is then iteratively refined through\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\nthat incorporates external information at each step. The core process is\nfurther enhanced by a self-evolutionary algorithm applied to each component of\nthe agentic workflow, ensuring the generation of high-quality context for the\ndiffusion process. This draft-centric design makes the report writing process\nmore timely and coherent while reducing information loss during the iterative\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\nresults on a wide array of benchmarks that require intensive search and\nmulti-hop reasoning, significantly outperforming existing deep research agents.", "AI": {"tldr": "The paper proposes TTD-DR, a novel framework for generating high-quality research reports by focusing on iterative refinement through a diffusion process and enhancing the quality of context with an evolutionary algorithm.", "motivation": "The motivation behind TTD-DR is to address the performance plateau issue of deep research agents when generating complex, long-form research reports using generic test-time scaling algorithms by copying the iterative nature of human research.", "method": "This novel framework, TTD-DR, conceptualizes research report generation as a diffusion process that begins with a preliminary draft and gets iteratively refined through a denoising process, informed by a retrieval mechanism that incorporates external information at each step. The self-evolutionary algorithm enhances each component of the agentic workflow, ensuring the generation of high-quality context.", "result": "TTD-DR achieves state-of-the-art results on a wide array of benchmarks that require intensive search and multi-hop reasoning, significantly outperforming existing deep research agents.", "conclusion": "The draft-centric design of TTD-DR facilitates more timely and coherent report writing processes, reducing information loss during the iterative search process and demonstrating excellence in benchmarks demanding intensive search and multi-hop reasoning."}}
{"id": "2507.15961", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15961", "abs": "https://arxiv.org/abs/2507.15961", "authors": ["Ahmed Aman Ibrahim", "Hamad Mansour Alawar", "Abdulnasser Abbas Zehi", "Ahmed Mohammad Alkendi", "Bilal Shafi Ashfaq Ahmed Mirza", "Shan Ullah", "Ismail Lujain Jaleel", "Hassan Ugail"], "title": "A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications", "comment": null, "summary": "Face image quality plays a critical role in determining the accuracy and\nreliability of face verification systems, particularly in real-time screening\napplications such as surveillance, identity verification, and access control.\nLow-quality face images, often caused by factors such as motion blur, poor\nlighting conditions, occlusions, and extreme pose variations, significantly\ndegrade the performance of face recognition models, leading to higher false\nrejection and false acceptance rates. In this work, we propose a lightweight\nyet effective framework for automatic face quality assessment, which aims to\npre-filter low-quality face images before they are passed to the verification\npipeline. Our approach utilises normalised facial landmarks in conjunction with\na Random Forest Regression classifier to assess image quality, achieving an\naccuracy of 96.67\\%. By integrating this quality assessment module into the\nface verification process, we observe a substantial improvement in performance,\nincluding a comfortable 99.7\\% reduction in the false rejection rate and\nenhanced cosine similarity scores when paired with the ArcFace face\nverification model. To validate our approach, we have conducted experiments on\na real-world dataset collected comprising over 600 subjects captured from CCTV\nfootage in unconstrained environments within Dubai Police. Our results\ndemonstrate that the proposed framework effectively mitigates the impact of\npoor-quality face images, outperforming existing face quality assessment\ntechniques while maintaining computational efficiency. Moreover, the framework\nspecifically addresses two critical challenges in real-time screening:\nvariations in face resolution and pose deviations, both of which are prevalent\nin practical surveillance scenarios.", "AI": {"tldr": "本文提出了一种轻量且有效的自动面部图像质量评估框架，旨在预筛选低质量面部图像。利用标准化面部特征点和随机森林回归分类器，实现了96.67%的质量评估精度，大幅降低了误拒率并改善了ArcFace面验证模型的余弦相似度得分。在包含600多名受试者的实际数据集上验证了方法的有效性，有效缓解了低质量图像的影响，同时保持计算效率。该框架特别解决了实时筛选中的面部分辨率变化和姿态偏差问题。", "motivation": "面部图像质量对面部验证系统的准确性和可靠性至关重要，尤其是在实时监控和身份验证等应用中。低质量的面部图像，如运动模糊、光照条件差、部分遮挡和极端姿态变化，会导致错误接受率和拒绝率上升，严重影响面部识别模型的表现。为了改善这种情况，作者提出了一种新的自动面部质量评估框架。", "method": "作者的方法利用标准化的面部特征点和随机森林回归分类器，先对输入的面部图像进行质量评估，在低质量的面像被传递到验证流水线之前进行预筛选。", "result": "实验表明，该模型实现了96.67%的质量评估精度，并且在与ArcFace模型结合使用时带来了虚假拒绝率大幅下降等性能改进。在包含实际环境中采集的600多名参与者的数据集上的实验验证了这一方法的有效性。", "conclusion": "该框架在保持计算效率的同时，有效处理了低质量面部图像，大大减少了误拒率，并提高了余弦相似度得分。特别是，解决了实际监控场景中的面部分辨率变化和姿态偏差问题。这种方法优于现有面部图像质量评估技术。"}}
{"id": "2507.16076", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16076", "abs": "https://arxiv.org/abs/2507.16076", "authors": ["Marlene Lutz", "Indira Sen", "Georg Ahnert", "Elisa Rogers", "Markus Strohmaier"], "title": "The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models", "comment": null, "summary": "Persona prompting is increasingly used in large language models (LLMs) to\nsimulate views of various sociodemographic groups. However, how a persona\nprompt is formulated can significantly affect outcomes, raising concerns about\nthe fidelity of such simulations. Using five open-source LLMs, we\nsystematically examine how different persona prompt strategies, specifically\nrole adoption formats and demographic priming strategies, influence LLM\nsimulations across 15 intersectional demographic groups in both open- and\nclosed-ended tasks. Our findings show that LLMs struggle to simulate\nmarginalized groups, particularly nonbinary, Hispanic, and Middle Eastern\nidentities, but that the choice of demographic priming and role adoption\nstrategy significantly impacts their portrayal. Specifically, we find that\nprompting in an interview-style format and name-based priming can help reduce\nstereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B\noutperform larger ones such as Llama-3.3-70B. Our findings offer actionable\nguidance for designing sociodemographic persona prompts in LLM-based simulation\nstudies.", "AI": {"tldr": "本研究旨在探讨不同的人设提示策略如何影响大型语言模型在模拟不同社会人口群体时的表现，发现适当的策略可以提升模型表现，甚至小模型可能更优。研究结果对于改善大型语言模型在模拟中的表现提供了指导。", "motivation": "研究背景在于，人设提示在大型语言模型中被广泛用来模拟视图，但是格式的不同会影响结果，从而影响了这种模拟的真实度。通过研究，希望探索如何更加准确地实施这种模拟。", "method": "研究通过使用五种开放源代码的语言模型(LLMs)，系统地检查了不同的人设提示策略，特别是在角色采用格式和人口统计提示策略方面，这些因素如何影响LLMs在15个人口统计交叉群体中开放和封闭任务的模拟效果。", "result": "研究发现，对于边缘群体（特别是非二元性、西班牙裔和中东身份）来说，LLMs在模拟方面存在困难。然而，选择合适的人口统计提示与角色采用策略可以显著改善这些问题。具体来说，以面试风格进行提示和基于姓名的提示可以有效地减少刻板印象并提高一致性。此外，意外地发现，较小的模型如OLMo-2-7B在模拟效果上优于较大的模型如Llama-3.3-70B。", "conclusion": "这项研究提供了关于如何在基于LLM的模拟研究中设计社会人口学特征人设提示的实用指南。"}}
{"id": "2507.16010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16010", "abs": "https://arxiv.org/abs/2507.16010", "authors": ["Zheng Wang", "Xianbing Sun", "Shengyi Wu", "Jiahui Zhan", "Jianlou Si", "Chi Zhang", "Liqing Zhang", "Jianfu Zhang"], "title": "FW-VTON: Flattening-and-Warping for Person-to-Person Virtual Try-on", "comment": null, "summary": "Traditional virtual try-on methods primarily focus on the garment-to-person\ntry-on task, which requires flat garment representations. In contrast, this\npaper introduces a novel approach to the person-to-person try-on task. Unlike\nthe garment-to-person try-on task, the person-to-person task only involves two\ninput images: one depicting the target person and the other showing the garment\nworn by a different individual. The goal is to generate a realistic combination\nof the target person with the desired garment. To this end, we propose\nFlattening-and-Warping Virtual Try-On (\\textbf{FW-VTON}), a method that\noperates in three stages: (1) extracting the flattened garment image from the\nsource image; (2) warping the garment to align with the target pose; and (3)\nintegrating the warped garment seamlessly onto the target person. To overcome\nthe challenges posed by the lack of high-quality datasets for this task, we\nintroduce a new dataset specifically designed for person-to-person try-on\nscenarios. Experimental evaluations demonstrate that FW-VTON achieves\nstate-of-the-art performance, with superior results in both qualitative and\nquantitative assessments, and also excels in garment extraction subtasks.", "AI": {"tldr": "本文提出了一种新的虚拟试穿方法Flattening-and-Warping Virtual Try-On (FW-VTON)，适用于人物到人物的试穿任务，该方法在实验评估中表现出色，取得了最先进的性能。", "motivation": "传统的虚拟试穿方法主要专注于衣物到人物的试穿任务，这需要使用平面化的衣物表示。相比之下，本文引入了一种新的任务，即人物到人物的试穿任务。这项任务仅涉及两个输入图像：一个描绘目标人物，另一个展示不同人物身上的衣物。", "method": "Flattening-and-Warping Virtual Try-On (FW-VTON)方法，包括三个阶段：1) 从源图像中提取平面化的衣物图像；2) 对衣物进行变形以匹配目标姿势；3) 将变形后的衣物无缝整合到目标人物上。", "result": "实验评估表明FW-VTON达到了最先进的性能，其在定性和定量评估方面均取得了优越的结果，并且在衣物提取子任务中也表现出色。", "conclusion": "FW-VTON方法通过三个阶段处理人物到人物的试穿任务，并且实验表明其性能优于现有方法。"}}
{"id": "2507.16083", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16083", "abs": "https://arxiv.org/abs/2507.16083", "authors": ["Ondrej Bohdal", "Mete Ozay", "Jijoong Moon", "Kyeng-Hun Lee", "Hyeonmok Ko", "Umberto Michieli"], "title": "Efficient Compositional Multi-tasking for On-device Large Language Models", "comment": null, "summary": "Adapter parameters provide a mechanism to modify the behavior of machine\nlearning models and have gained significant popularity in the context of large\nlanguage models (LLMs) and generative AI. These parameters can be merged to\nsupport multiple tasks via a process known as task merging. However, prior work\non merging in LLMs, particularly in natural language processing, has been\nlimited to scenarios where each test example addresses only a single task. In\nthis paper, we focus on on-device settings and study the problem of text-based\ncompositional multi-tasking, where each test example involves the simultaneous\nexecution of multiple tasks. For instance, generating a translated summary of a\nlong text requires solving both translation and summarization tasks\nconcurrently. To facilitate research in this setting, we propose a benchmark\ncomprising four practically relevant compositional tasks. We also present an\nefficient method (Learnable Calibration) tailored for on-device applications,\nwhere computational resources are limited, emphasizing the need for solutions\nthat are both resource-efficient and high-performing. Our contributions lay the\ngroundwork for advancing the capabilities of LLMs in real-world multi-tasking\nscenarios, expanding their applicability to complex, resource-constrained use\ncases.", "AI": {"tldr": "本文探讨了基于文本的组合多任务问题，并提出了一种名为可学习校准的有效方法，特别适用于计算资源有限的设备端应用。", "motivation": "本文的动机是在设备端环境下研究基于文本的组合多任务问题，特别是在每个测试示例涉及多个任务的同时执行。", "method": "本文提出了一种称为可学习校准（Learnable Calibration）的有效方法，特别适用于设备端应用，强调了在计算资源有限的情况下，解决方案不仅需要高效还要高性能。", "result": "本研究的成果包括建立了一个由四个实际相关的组合任务组成的基准，推动了大语言模型在多任务情景下的应用。", "conclusion": "本文的研究奠定了在多任务场景中推进大语言模型能力的基础，扩展其在复杂、资源受限使用场景中的适用性。"}}
{"id": "2507.16015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16015", "abs": "https://arxiv.org/abs/2507.16015", "authors": ["Matteo Dunnhofer", "Zaira Manigrasso", "Christian Micheloni"], "title": "Is Tracking really more challenging in First Person Egocentric Vision?", "comment": "2025 IEEE/CVF International Conference on Computer Vision (ICCV)", "summary": "Visual object tracking and segmentation are becoming fundamental tasks for\nunderstanding human activities in egocentric vision. Recent research has\nbenchmarked state-of-the-art methods and concluded that first person egocentric\nvision presents challenges compared to previously studied domains. However,\nthese claims are based on evaluations conducted across significantly different\nscenarios. Many of the challenging characteristics attributed to egocentric\nvision are also present in third person videos of human-object activities. This\nraises a critical question: how much of the observed performance drop stems\nfrom the unique first person viewpoint inherent to egocentric vision versus the\ndomain of human-object activities? To address this question, we introduce a new\nbenchmark study designed to disentangle such factors. Our evaluation strategy\nenables a more precise separation of challenges related to the first person\nperspective from those linked to the broader domain of human-object activity\nunderstanding. By doing so, we provide deeper insights into the true sources of\ndifficulty in egocentric tracking and segmentation, facilitating more targeted\nadvancements on this task.", "AI": {"tldr": "研究引入了新的基准测试，以便更精确地分离第一人称视觉视角的挑战与更广泛的人类-物体活动理解领域中的挑战，以推动该任务更针对性的发展。", "motivation": "近期的研究表明，第一人称视角的视觉理解在物体跟踪和分割任务中提出了独特的挑战。然而，这些挑战是否仅由第一人称视角造成，或与人类-物体活动的本质相关尚不清楚。", "method": "引入了一个新的基准测试研究，旨在区分第一人称视角与人类-物体活动领域带来的挑战。", "result": "该评估策略能够更精确地分离出与第一人称视角相关的问题，以及与人类-物体活动理解领域相关的问题。", "conclusion": "通过区分这两方面的困难，本研究为识别第一人称视觉跟踪和分割任务中的真实困难提供了更深入的见解，从而促进更有针对性的进步。"}}
{"id": "2507.16183", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16183", "abs": "https://arxiv.org/abs/2507.16183", "authors": ["Azizul Hakim Fayaz", "MD. Shorif Uddin", "Rayhan Uddin Bhuiyan", "Zakia Sultana", "Md. Samiul Islam", "Bidyarthi Paul", "Tashreef Muhammad", "Shahriar Manzoor"], "title": "BIDWESH: A Bangla Regional Based Hate Speech Detection Dataset", "comment": null, "summary": "Hate speech on digital platforms has become a growing concern globally,\nespecially in linguistically diverse countries like Bangladesh, where regional\ndialects play a major role in everyday communication. Despite progress in hate\nspeech detection for standard Bangla, Existing datasets and systems fail to\naddress the informal and culturally rich expressions found in dialects such as\nBarishal, Noakhali, and Chittagong. This oversight results in limited detection\ncapability and biased moderation, leaving large sections of harmful content\nunaccounted for. To address this gap, this study introduces BIDWESH, the first\nmulti-dialectal Bangla hate speech dataset, constructed by translating and\nannotating 9,183 instances from the BD-SHS corpus into three major regional\ndialects. Each entry was manually verified and labeled for hate presence, type\n(slander, gender, religion, call to violence), and target group (individual,\nmale, female, group), ensuring linguistic and contextual accuracy. The\nresulting dataset provides a linguistically rich, balanced, and inclusive\nresource for advancing hate speech detection in Bangla. BIDWESH lays the\ngroundwork for the development of dialect-sensitive NLP tools and contributes\nsignificantly to equitable and context-aware content moderation in low-resource\nlanguage settings.", "AI": {"tldr": "该研究提出了BIDWESH，这是首个用于多方言孟加拉语仇恨言论检测的语料库，它填补了标准孟加拉语之外，特别是在区域方言中仇恨言论检测能力不足的空白。", "motivation": "当前的标准孟加拉语仇恨言论检测数据集和系统无法处理方言中的非正式和文化丰富的表达，导致检测能力有限和有偏见的管理，许多有害内容未被识别。本研究正是为了弥补这一空白。", "method": "为了填补标准孟加拉语之外，特别是区域方言如Barishal, Noakhali和Chittagong的仇恨言论检测的空白，本研究构建了BIDWESH。BIDWESH是第一个多方言的孟加拉语仇恨言论数据集，通过将BD-SHS语料库中的9,183个实例翻译和标注到这三个主要区域方言中。每个条目都经过人工核查和标记，涵盖了仇恨存在的确认、类型（诽谤、性别、宗教、暴力号召）和目标群体（个人、男性、女性、群体），确保了语言和语境的准确无误。", "result": "BIDWESH为推动孟加拉语仇恨言论检测提供了语言丰富、平衡和包容的资源。", "conclusion": "BIDWESH为开发敏感于方言的自然语言处理工具奠定了基础，为资源较少语言环境中的公平和情境感知内容管理做出了重要贡献。"}}
{"id": "2507.16018", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16018", "abs": "https://arxiv.org/abs/2507.16018", "authors": ["Andrew Lu", "Wentinn Liao", "Liuhui Wang", "Huzheng Yang", "Jianbo Shi"], "title": "Artifacts and Attention Sinks: Structured Approximations for Efficient Vision Transformers", "comment": null, "summary": "Vision transformers have emerged as a powerful tool across a wide range of\napplications, yet their inner workings remain only partially understood. In\nthis work, we examine the phenomenon of massive tokens - tokens with\nexceptionally high activation norms that act as attention sinks - and artifact\ntokens that emerge as a byproduct during inference. Our analysis reveals that\nthese tokens mutually suppress one another through the attention mechanism,\nplaying a critical role in regulating information flow within the network.\nLeveraging these insights, we introduce Fast Nystr\\\"om Attention (FNA), a\ntraining-free method that approximates self-attention in linear time and space\nby exploiting the structured patterns formed by massive and artifact tokens.\nAdditionally, we propose a masking strategy to mitigate noise from these\ntokens, yielding modest performance gains at virtually no cost. We evaluate our\napproach on popular pretrained vision backbones and demonstrate competitive\nperformance on retrieval, classification, segmentation, and visual question\nanswering (VQA), all while reducing computational overhead.", "AI": {"tldr": "研究视觉变压器中的标记现象，提出了一种无需训练的注意力机制方法 Fast Nystr\"om 注意力，并提出屏蔽策略来减少干扰，提高效果。", "motivation": "理解视觉变压器的操作机制并提高其性能，减少计算开销。", "method": "通过分析视觉变压器中的 '巨大标记' 和 '工件标记' 来提出新的注意力机制方法 Fast Nystr\"om 注意力（FNA），该方法不需训练即可近似自注意力机制，同时提出了一种屏蔽策略来减少这些标记带来的干扰。", "result": "实验表明，Fast Nystr\"om 注意力方法和屏蔽策略可以在保持良好性能的同时大幅度减少计算资源的消耗。", "conclusion": "提出的 Fast Nystr\"om 注意力方法和屏蔽策略，可以有效地减少计算开销，同时在检索、分类、分割和视觉问题回答（VQA）等多个任务上保持竞争力。"}}
{"id": "2507.16196", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16196", "abs": "https://arxiv.org/abs/2507.16196", "authors": ["Jared Moore", "Ned Cooper", "Rasmus Overmark", "Beba Cibralic", "Nick Haber", "Cameron R. Jones"], "title": "Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task", "comment": "To appear in COLM, 2025", "summary": "Recent evidence suggests Large Language Models (LLMs) display Theory of Mind\n(ToM) abilities. Most ToM experiments place participants in a spectatorial\nrole, wherein they predict and interpret other agents' behavior. However, human\nToM also contributes to dynamically planning action and strategically\nintervening on others' mental states. We present MindGames: a novel `planning\ntheory of mind' (PToM) task which requires agents to infer an interlocutor's\nbeliefs and desires to persuade them to alter their behavior. Unlike previous\nevaluations, we explicitly evaluate use cases of ToM. We find that humans\nsignificantly outperform o1-preview (an LLM) at our PToM task (11% higher;\n$p=0.006$). We hypothesize this is because humans have an implicit causal model\nof other agents (e.g., they know, as our task requires, to ask about people's\npreferences). In contrast, o1-preview outperforms humans in a baseline\ncondition which requires a similar amount of planning but minimal mental state\ninferences (e.g., o1-preview is better than humans at planning when already\ngiven someone's preferences). These results suggest a significant gap between\nhuman-like social reasoning and LLM abilities.", "AI": {"tldr": "该研究通过MindGames实验发现，尽管在给定他人偏好的情况下大语言模型比人类在规划方面表现更好，但在需要理解他人心智状态的任务上，人类显著优于这些模型。", "motivation": "研究动机在于识别和评估语言模型在模拟社交推理能力方面的局限性，特别是它们在理解和预测他人行为方面的能力。", "method": "该研究提出了一个名为MindGames的新任务，用于评估代理在理解和影响他人的信念和欲望方面的能力，这被称为'规划理论心智'（PToM）任务。", "result": "研究发现人类在PToM任务上显著优于o1-preview（一种大语言模型），成功率高出约11%，p值为0.006。", "conclusion": "研究的结论是，当前的大型语言模型在模仿人类的社会推理能力上存在显著差距，尤其是在需要理解和干预他人心理状态的情境中。"}}
{"id": "2507.16038", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16038", "abs": "https://arxiv.org/abs/2507.16038", "authors": ["Rahul Venkatesh", "Klemen Kotar", "Lilian Naing Chen", "Seungwoo Kim", "Luca Thomas Wheeler", "Jared Watrous", "Ashley Xu", "Gia Ancone", "Wanhee Lee", "Honglin Chen", "Daniel Bear", "Stefan Stojanov", "Daniel Yamins"], "title": "Discovering and using Spelke segments", "comment": "Project page at: https://neuroailab.github.io/spelke_net", "summary": "Segments in computer vision are often defined by semantic considerations and\nare highly dependent on category-specific conventions. In contrast,\ndevelopmental psychology suggests that humans perceive the world in terms of\nSpelke objects--groupings of physical things that reliably move together when\nacted on by physical forces. Spelke objects thus operate on category-agnostic\ncausal motion relationships which potentially better support tasks like\nmanipulation and planning. In this paper, we first benchmark the Spelke object\nconcept, introducing the SpelkeBench dataset that contains a wide variety of\nwell-defined Spelke segments in natural images. Next, to extract Spelke\nsegments from images algorithmically, we build SpelkeNet, a class of visual\nworld models trained to predict distributions over future motions. SpelkeNet\nsupports estimation of two key concepts for Spelke object discovery: (1) the\nmotion affordance map, identifying regions likely to move under a poke, and (2)\nthe expected-displacement map, capturing how the rest of the scene will move.\nThese concepts are used for \"statistical counterfactual probing\", where diverse\n\"virtual pokes\" are applied on regions of high motion-affordance, and the\nresultant expected displacement maps are used define Spelke segments as\nstatistical aggregates of correlated motion statistics. We find that SpelkeNet\noutperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench.\nFinally, we show that the Spelke concept is practically useful for downstream\napplications, yielding superior performance on the 3DEditBench benchmark for\nphysical object manipulation when used in a variety of off-the-shelf object\nmanipulation models.", "AI": {"tldr": "本文介绍了一个新的数据集SpelkeBench和一个新模型SpelkeNet，用来识别图像中的Spelke物体，并展示了这种方法在物理对象操作任务中的实用性。", "motivation": "研究发现人类倾向于根据Spelke物体（物理上协同移动的物体组）感知世界，而不是基于类别的特定惯例。这可能更适合于操作和规划等任务。", "method": "构建了SpelkeNet，这是一种视觉世界模型，旨在预测未来的运动分布，从而估计两种关键概念：运动能力图和预期位移图，用于发现Spelke物体。", "result": "SpelkeNet在SpelkeBench上优于监督基线模型如SegmentAnything（SAM），并且在使用各种现成的对象操作模型时，在3DEditBench基准上表现出色。", "conclusion": "Spelke物体概念不仅在理论上有重要意义，而且对于实际任务，特别是物理对象操作应用中的下游任务，具有实际用途。"}}
{"id": "2507.16199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16199", "abs": "https://arxiv.org/abs/2507.16199", "authors": ["Zipeng Ling", "Yuehao Tang", "Shuliang Liu", "Junqi Yang", "Shenghong Fu", "Yao Wan", "Kejia Huang", "Zhichao Hou", "Xuming Hu"], "title": "WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability", "comment": null, "summary": "Large Language Models (LLMs) frequently output the label \\emph{Unknown}, yet\ncurrent evaluations focus almost exclusively on whether such answers are\n\\emph{honest} rather than why they arise. This blurs two distinct cases: (i) an\ninput that is genuinely indeterminate and (ii) a solvable problem that the\nmodel fails to resolve. We call this phenomenon \\emph{Vague Perception}. And\nthus we introduce a framework that quantifies the proportion of \\emph{Unknown}\nresponses attributable to model incapacity and tests whether guided stimulation\ncan convert them into either correct (\\emph{Known}) or intrinsically\nindeterminate outcomes. By separating these sources of uncertainty, our method\nprovides a clearer picture of LLM reasoning limits and their potential for\nimprovement. As we get a theoretical accuracy of reasoning task on different\nLLMs, we apply different methods to test whether the model can reach the\naccuracy given a baseline framework. Our work is meaningful in exploring the\ntrue reasoning ability of LLMs and providing a new perspective on solving the\n\\emph{Vague Perception} phenomenon.", "AI": {"tldr": "本文提出了一种新的评估框架，用于区分LLM产生'未知'回复的真实原因，并通过框架测试和引导策略来探索模型的实际推理能力和提升空间。", "motivation": "该论文的动机在于，当前的评估主要集中在LLM的'未知'回复是否诚实上，而不是探索这些回复为何出现。作者们认为这两个方面并不相同，一种是输入问题本质上未确定，另一种是问题可以解决但模型未能解决。", "method": "我们引入了一个新的框架，旨在量化'未知'回复中模型能力不足所占的比例，并测试了通过引导刺激是否能够将这些回复转化为正确的'已知'或本质上不确定的答案。通过区分不确定性的来源，我们的方法能够更清晰地展示出LLM的推理极限及其改进潜力。", "result": "通过理论分析和推理任务上不同LLMs的表现，研究人员发现可以通过他们设定的基准框架和不同方法测试模型是否能达到预测的准确性。这为研究LLM的实际推理能力提供了新的视角和解决方案思路。", "conclusion": "研究结论强调了此框架的意义在于探索了LLMs真正的推理能力，并提供了理解和解决'模糊感知'现象的新视角。通过这种方法，研究人员得以更清晰地了解模型的限制和提升潜力。"}}
{"id": "2507.16052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16052", "abs": "https://arxiv.org/abs/2507.16052", "authors": ["Yuyang Luo", "Xiaosen Wang", "Zhijin Ge", "Yingzhe He"], "title": "Disrupting Semantic and Abstract Features for Better Adversarial Transferability", "comment": null, "summary": "Adversarial examples pose significant threats to deep neural networks (DNNs),\nand their property of transferability in the black-box setting has led to the\nemergence of transfer-based attacks, making it feasible to target real-world\napplications employing DNNs. Among them, feature-level attacks, where\nintermediate features are perturbed based on feature importance weight matrix\ncomputed from transformed images, have gained popularity. In this work, we find\nthat existing feature-level attacks primarily manipulate the semantic\ninformation to derive the weight matrix. Inspired by several works that find\nCNNs tend to focus more on high-frequency components (a.k.a. abstract features,\ne.g., texture, edge, etc.), we validate that transforming images in the\nhigh-frequency space also improves transferability. Based on this finding, we\npropose a balanced approach called Semantic and Abstract FEatures disRuption\n(SAFER). Specifically, SAFER conducts BLOCKMIX on the input image and SELF-MIX\non the frequency spectrum when computing the weight matrix to highlight crucial\nfeatures. By using such a weight matrix, we can direct the attacker to disrupt\nboth semantic and abstract features, leading to improved transferability.\nExtensive experiments on the ImageNet dataset also demonstrate the\neffectiveness of our method in boosting adversarial transferability.", "AI": {"tldr": "本文提出了一种新的对抗样本生成方法SAFER，通过在语义信息和抽象特征（如纹理和边缘）之间取得平衡，提高对抗样本的迁移性。", "motivation": "现有特征级别的对抗攻击主要集中在语义信息上，但CNN更倾向于关注高频组成部分。因此，本文希望通过同时扰乱语义和抽象特征来提升对抗样本的迁移性。", "method": "Structure", "result": "{\"tldr\": \"本文提出了一种新的对抗样本生成方法SAFER，通过在语义信息和抽象特征（如纹理和边缘）之间取得平衡，提高对抗样本的迁移性。\", \"motivation\": \"现有特征级别的对抗攻击主要集中在语义信息上，但CNN更倾向于关注高频组成部分。因此，本文希望通过同时扰乱语义和抽象特征来提升对抗样本的迁移性。\", \"method\": \"SAFER方法采用BLOCKMIX处理输入图像，并利用SELF-MIX在频率谱上执行操作来计算权重矩阵，以突出关键特征。\", \"result\": \"在ImageNet数据集上进行了大量实验，证明了SAFER方法能够显著提高对抗样本的迁移性。\", \"conclusion\": \"研究表明，同时扰乱图像中的语义和抽象特征可以有效提高对抗样本的迁移性，这对保障深度神经网络的安全性具有重要意义。\"}", "conclusion": "研究表明，同时扰乱图像中的语义和抽象特征可以有效提高对抗样本的迁移性，这对保障深度神经网络的安全性具有重要意义。"}}
{"id": "2507.16217", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16217", "abs": "https://arxiv.org/abs/2507.16217", "authors": ["Shahriar Golchin", "Yanfei Chen", "Rujun Han", "Manan Gandhi", "Tianli Yu", "Swaroop Mishra", "Mihai Surdeanu", "Rishabh Agarwal", "Chen-Yu Lee", "Tomas Pfister"], "title": "Towards Compute-Optimal Many-Shot In-Context Learning", "comment": "Final version; accepted at COLM 2025", "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.", "AI": {"tldr": "The paper enhances ICL performance using targeted demonstration selection strategies that are cost-effective.", "motivation": "The motivation behind the paper is to address the high inference costs associated with many-shot ICL and improve performance by refining the selection of training demonstrations without overburdening computational resources.", "method": "The paper proposes two demonstration selection strategies for many-shot in-context learning (ICL) to improve performance and reduce computational overhead. The first strategy combines similar demonstrations with a larger set of random, cached demonstrations. The second strategy further improves this by using demonstrations selected based on centroids derived from test sample representations using k-means clustering.", "result": "Experiments with the Gemini Pro and Flash models across various datasets demonstrate consistent outperformance of random selection and achieve or surpass the best-performing selection methods while enabling caching and significantly reducing inference costs (up to ten times).", "conclusion": "The strategies for demonstration selection proposed in the paper provide a means to balance performance gains with computational efficiency, effectively reducing inference costs while maintaining or improving model performance in many-shot ICL scenarios."}}
{"id": "2507.16095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16095", "abs": "https://arxiv.org/abs/2507.16095", "authors": ["Parul Gupta", "Abhinav Dhall", "Thanh-Toan Do"], "title": "Improving Personalized Image Generation through Social Context Feedback", "comment": null, "summary": "Personalized image generation, where reference images of one or more subjects\nare used to generate their image according to a scene description, has gathered\nsignificant interest in the community. However, such generated images suffer\nfrom three major limitations -- complex activities, such as $<$man, pushing,\nmotorcycle$>$ are not generated properly with incorrect human poses, reference\nhuman identities are not preserved, and generated human gaze patterns are\nunnatural/inconsistent with the scene description. In this work, we propose to\novercome these shortcomings through feedback-based fine-tuning of existing\npersonalized generation methods, wherein, state-of-art detectors of pose,\nhuman-object-interaction, human facial recognition and human gaze-point\nestimation are used to refine the diffusion model. We also propose\ntimestep-based inculcation of different feedback modules, depending upon\nwhether the signal is low-level (such as human pose), or high-level (such as\ngaze point). The images generated in this manner show an improvement in the\ngenerated interactions, facial identities and image quality over three\nbenchmark datasets.", "AI": {"tldr": "论文提出了一种基于反馈机制的个性化图像生成方法，改进了现有方法在复杂活动、身份保真度和注视点方面的不足，提高了生成图像的质量。", "motivation": "解决个性化图像生成中复杂活动不准确、参考人物身份不保真及人眼注视模式不合理的问题。", "method": "采用基于反馈的微调方法来改进现有个性化图像生成技术，使用姿态检测、人-物交互检测、面部识别和人眼注视点估计等先进检测器来优化扩散模型。此外，根据不同信号层次，在不同的时间步骤引入不同的反馈模块。", "result": "实验结果表明，提出的生成方法在人与物交互、面部识别和图像质量上优于基准数据集上的现有方法。", "conclusion": "本研究通过反馈机制增强了个性化图像生成的精度和质量，验证了该方法在三种基准数据集上的效能。"}}
{"id": "2507.16248", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16248", "abs": "https://arxiv.org/abs/2507.16248", "authors": ["Run Sun", "Zuo Bai", "Wentao Zhang", "Yuxiang Zhang", "Li Zhao", "Shan Sun", "Zhengwen Qiu"], "title": "FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents", "comment": null, "summary": "Recently, AI agents are rapidly evolving in intelligence and widely used in\nprofessional research applications, such as STEM, software development,\nfinance, etc. Among these AI agents, deep research agent is a key category as\nit can perform long-horizon tasks and solve problems of greater complexity.\nHowever, there are few evaluation frameworks and benchmarks that systematically\nand automatically investigate the capabilities of these research agents.\nFurthermore, financial research problems have distinct complexity and subtlety.\nTo fill in the gap, we propose FinResearchBench, which is a logic tree based\nAgent-as-a-Judge and targets specifically for the financial research agents. It\nprovides a comprehensive and automatic assessment of the research agents across\n7 key types of tasks in the financial research domain. The contributions of\nthis work are two-folded: (1) the first and innovative Agent-as-a-Judge system\nthat extracts the logic tree of the research outcome and uses it as the\nintermediate information to present a comprehensive, reliable and robust\nevaluation; (2) finance oriented that it covers 70 typical financial research\nquestions, spreading across 7 frequently encountered types of tasks in the\ndomain.", "AI": {"tldr": "本文提出了一个针对金融研究AI代理的评估系统FinResearchBench，它基于逻辑树，提供全面可靠的自动评估，并包含70个典型的金融研究问题。", "motivation": "现有的评估框架和基准无法系统和自动地评估研究型AI代理的能力，特别是针对金融研究这种具有独特复杂性和细微差别的领域。为了填补这个空白，本研究提出了FinResearchBench。", "method": "本研究提出了FinResearchBench，这是一种基于逻辑树的Agent-as-a-Judge系统，专门针对金融研究中的AI代理进行评估。该系统能够全面自动地评估代理在金融研究中7个关键类型任务的表现。", "result": "该系统提供了对金融研究代理能力的全面、可靠和自动评估。", "conclusion": "本研究贡献了两个方面的创新：(1) 首次提出并创新地使用Agent-as-a-Judge系统，通过提取研究结果的逻辑树来实现全面、可靠和稳健的评估；(2) 针对金融研究领域的特点，覆盖了70个典型的金融研究问题，跨越7个最常见的任务类型。"}}
{"id": "2507.16114", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16114", "abs": "https://arxiv.org/abs/2507.16114", "authors": ["An D. Le", "Hung Nguyen", "Sungbal Seo", "You-Suk Bae", "Truong Q. Nguyen"], "title": "Stop-band Energy Constraint for Orthogonal Tunable Wavelet Units in Convolutional Neural Networks for Computer Vision problems", "comment": null, "summary": "This work introduces a stop-band energy constraint for filters in orthogonal\ntunable wavelet units with a lattice structure, aimed at improving image\nclassification and anomaly detection in CNNs, especially on texture-rich\ndatasets. Integrated into ResNet-18, the method enhances convolution, pooling,\nand downsampling operations, yielding accuracy gains of 2.48% on CIFAR-10 and\n13.56% on the Describable Textures dataset. Similar improvements are observed\nin ResNet-34. On the MVTec hazelnut anomaly detection task, the proposed method\nachieves competitive results in both segmentation and detection, outperforming\nexisting approaches.", "AI": {"tldr": "A novel stop-band energy constraint for filters in CNNs improves image classification and anomaly detection, especially on texture-rich datasets.", "motivation": "The aim is to improve image classification and anomaly detection performance, particularly on texture-rich datasets, by modifying the CNNs' convolution, pooling, and downsampling operations.", "method": "The paper introduces a stop-band energy constraint for filters in orthogonal tunable wavelet units with a lattice structure to enhance image classification and anomaly detection in CNNs.", "result": "The method improves accuracy by 2.48% on CIFAR-10 and 13.56% on the Describable Textures dataset when integrated into ResNet-18. Similar improvements are seen in ResNet-34. It also shows competitive results in segmentation and detection on the MVTec hazelnut anomaly detection task.", "conclusion": "The stop-band energy constrained wavelet units effectively enhance performance in tasks such as image classification and anomaly detection, demonstrating significant improvements over existing methods."}}
{"id": "2507.16252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16252", "abs": "https://arxiv.org/abs/2507.16252", "authors": ["Hyunji Nam", "Omer Gottesman", "Amy Zhang", "Dean Foster", "Emma Brunskill", "Lyle Ungar"], "title": "Efficient RL for optimizing conversation level outcomes with an LLM-based tutor", "comment": "9 pages", "summary": "Large language models (LLMs) built on existing reinforcement learning with\nhuman feedback (RLHF) frameworks typically optimize responses based on\nimmediate turn-level human preferences. However, this approach falls short in\nmulti-turn dialogue settings, such as online math tutoring. We propose a method\nto enhance LLM-based tutors by representing the dialogue history with a\nlower-dimensional latent state representation of a student and optimizing a\nlong-term policy to determine high-level actions based on the latent state. The\ngoal is to better align the tutor's behavior with the long-term objective of\nguiding the student towards solving a target math problem on their own. Our\nmodel is lightweight, requiring less computational resources than prior work of\ntraining the tutor policy end-to-end to directly output the tutor's next\nutterance. Our experiment results demonstrate that these modifications lead to\nimproved long-term outcomes compared to prompting in LLM-simulated tutoring\ntasks.", "AI": {"tldr": "本研究提出了一种改进LLM为基础的辅导系统的方法，通过低维度潜在状态表示和长期策略优化，以改善多轮对话场景下的表现。实验表明该方法优于基于提示的方法，并具有较低的计算需求。", "motivation": "现有的基于人类反馈强化学习框架构建的语言模型通常根据即时的人类偏好来优化响应，这种方法在多轮对话场景（如在线数学辅导）中效果不佳", "method": "通过使用学生对话历史的低维度潜在状态表示，优化长期策略，以基于潜在状态确定高层次动作，从而改进LLM基础的辅导系统", "result": "实验结果表明，这些改进在LLM模拟的辅导任务中相比提示法（prompting）能够带来更好的长期表现", "conclusion": "本文提出的方法可以更好地使辅导系统的行为与引导学生自学解决问题这一长期目标保持一致，并且模型较轻量，计算资源需求较少"}}
{"id": "2507.16116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16116", "abs": "https://arxiv.org/abs/2507.16116", "authors": ["Yaofang Liu", "Yumeng Ren", "Aitor Artola", "Yuxuan Hu", "Xiaodong Cun", "Xiaotong Zhao", "Alan Zhao", "Raymond H. Chan", "Suiyun Zhang", "Rui Liu", "Dandan Tu", "Jean-Michel Morel"], "title": "PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation", "comment": "Code is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen", "summary": "The rapid advancement of video diffusion models has been hindered by\nfundamental limitations in temporal modeling, particularly the rigid\nsynchronization of frame evolution imposed by conventional scalar timestep\nvariables. While task-specific adaptations and autoregressive models have\nsought to address these challenges, they remain constrained by computational\ninefficiency, catastrophic forgetting, or narrow applicability. In this work,\nwe present Pusa, a groundbreaking paradigm that leverages vectorized timestep\nadaptation (VTA) to enable fine-grained temporal control within a unified video\ndiffusion framework. Besides, VTA is a non-destructive adaptation, which means\nit fully preserves the capabilities of the base model. By finetuning the SOTA\nWan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --\nsurpassing the performance of Wan-I2V-14B with $\\leq$ 1/200 of the training\ncost (\\$500 vs. $\\geq$ \\$100,000) and $\\leq$ 1/2500 of the dataset size (4K vs.\n$\\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V)\ngeneration, achieving a VBench-I2V total score of 87.32\\% (vs. 86.86\\% of\nWan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as\nstart-end frames and video extension -- all without task-specific training.\nMeanwhile, Pusa can still perform text-to-video generation. Mechanistic\nanalyses reveal that our approach preserves the foundation model's generative\npriors while surgically injecting temporal dynamics, avoiding the combinatorial\nexplosion inherent to vectorized timesteps. This work establishes a scalable,\nefficient, and versatile paradigm for next-generation video synthesis,\ndemocratizing high-fidelity video generation for research and industry alike.\nCode is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen", "AI": {"tldr": "Pusa是通过向量化时间步调整（VTA）实现视频生成的技术，其在SOTA模型上微调后，不仅在其主要任务上表现优越，而且还能解锁更多功能，同时大大减少了训练成本和数据量需求。", "motivation": "传统的标量时间步变量在视频扩散模型中的刚性同步限制了视频扩散模型的快速发展。虽然针对特定任务的调整和自回归模型试图解决这些问题，但仍受计算效率低下、灾难性遗忘以及适用性窄的限制。", "method": "该研究提出了一种名为Pusa的新范式，通过向量化时间步调整（VTA）来实现视频扩散模型中的精细时间控制。VTA是一种非破坏性调整，意味着它完全保留了基础模型的能力。通过对SOTA Wan2.1-T2V-14B模型进行VTA微调，实现了前所未有的效率。", "result": "Pusa不仅为图像到视频（I2V）生成设定了新标准，达到了VBench-I2V的总分87.32%（优于Wan-I2V-14B的86.86%），还解锁了多种零样本多任务能力，如开始-结束帧和视频扩展。此外，Pusa还能进行文本到视频的生成。", "conclusion": "本工作建立了一种可扩展、高效、多功能的下一代视频合成范式，使高质量视频生成在研究和行业中都变得更加普及。代码开源于https://github.com/Yaofang-Liu/Pusa-VidGen。"}}
{"id": "2507.16263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16263", "abs": "https://arxiv.org/abs/2507.16263", "authors": ["Yujian Sun", "Tian Li"], "title": "iShumei-Chinchunmei at SemEval-2025 Task 4: A balanced forgetting and retention multi-task framework using effective unlearning loss", "comment": null, "summary": "As the Large Language Model (LLM) gains widespread adoption, increasing\nattention has been given to the challenge of making LLM forget non-compliant\ndata memorized during its pre-training. Machine Unlearning focuses on\nefficiently erasing sensitive information from LLM under limited computational\nresources. To advance research in this area, SemEval 2025 Task 4: \"Unlearning\nSensitive Content from Large Language Models\" introduces three unlearning\ndatasets and establishes a benchmark by evaluating both forgetting\neffectiveness and the preservation of standard capabilities. In this work, we\npropose a more controllable forgetting loss, Effective Unlearning Loss, and\nexplore its integration with various techniques to achieve more efficient and\ncontrolled unlearning. Our system ultimately ranked 5th on the competition\nleaderboard.", "AI": {"tldr": "研究了如何更有效地从大型语言模型（LLM）中删除敏感内容同时保留其标准能力，并提出了有效的遗忘损失方法，在比赛中排名第五。", "motivation": "随着大型语言模型（LLM）的广泛应用，如何让LLM忘记在预训练过程中记忆的非合规数据成为一个亟待解决的问题。该研究旨在提高在有限计算资源下有效地从LLM中擦除敏感信息的技术。", "method": "提出了一种更可控的遗忘损失函数——有效遗忘损失，并探索了将其与各种技术相结合以实现更高效和可控遗忘的方法。", "result": "系统在比赛中排名第五。研究展示了有效遗忘损失方法在删除敏感信息方面的潜力。", "conclusion": "研究引入了有效遗忘损失方法来控制大型语言模型的遗忘过程，并证明了其在高效和可控遗忘方面的有效性。"}}
{"id": "2507.16119", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16119", "abs": "https://arxiv.org/abs/2507.16119", "authors": ["An D. Le", "Hung Nguyen", "Melanie Tran", "Jesse Most", "Dirk-Uwe G. Bartsch", "William R Freeman", "Shyamanga Borooah", "Truong Q. Nguyen", "Cheolhong An"], "title": "Universal Wavelet Units in 3D Retinal Layer Segmentation", "comment": null, "summary": "This paper presents the first study to apply tunable wavelet units (UwUs) for\n3D retinal layer segmentation from Optical Coherence Tomography (OCT) volumes.\nTo overcome the limitations of conventional max-pooling, we integrate three\nwavelet-based downsampling modules, OrthLattUwU, BiorthLattUwU, and\nLS-BiorthLattUwU, into a motion-corrected MGU-Net architecture. These modules\nuse learnable lattice filter banks to preserve both low- and high-frequency\nfeatures, enhancing spatial detail and structural consistency. Evaluated on the\nJacobs Retina Center (JRC) OCT dataset, our framework shows significant\nimprovement in accuracy and Dice score, particularly with LS-BiorthLattUwU,\nhighlighting the benefits of tunable wavelet filters in volumetric medical\nimage segmentation.", "AI": {"tldr": "本文提出了一种新的方法，使用可调小波单元进行OCT卷的数据中的3D视网膜层分割，利用改进的波型下采样模块增强了图像的细节保留和结构一致性，提高了分割精度。", "motivation": "这是首次将可调小波单元应用于3D视网膜层分割中的研究。目的在于通过改进的波型下采样模块增强细部结构的一致性和精确度，克服传统池化方法的局限。", "method": "本研究将可调小波单元（UwUs）应用于3D视网膜层分割的光学相干断层扫描（OCT）数据中。为克服传统最大池化方法的限制，采用了三种基于小波的下采样模块，分别是OrthLattUwU、BiorthLattUwU和LS-BiorthLattUwU，集成了运动校正的MGU-Net架构。这些模块通过可学习的格滤波器组来保持低频和高频特征，增强空间细节和结构一致性。", "result": "在Jacobs视网膜中心（JRC）OCT数据集上进行了实验，结果表明，本框架在准确性、Dice分数上有了显著提高，特别是采用LS-BiorthLattUwU的情况下。", "conclusion": "研究结果证明了可调小波滤波器在体积医学图像分割中的优势。"}}
