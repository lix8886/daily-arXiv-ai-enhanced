{"id": "2508.10066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10066", "abs": "https://arxiv.org/abs/2508.10066", "authors": ["Javier Rodenas", "Eduardo Aguilar", "Petia Radeva"], "title": "Stochastic-based Patch Filtering for Few-Shot Learning", "comment": "CVPR Workshop MetaFood 2025", "summary": "Food images present unique challenges for few-shot learning models due to\ntheir visual complexity and variability. For instance, a pasta dish might\nappear with various garnishes on different plates and in diverse lighting\nconditions and camera perspectives. This problem leads to losing focus on the\nmost important elements when comparing the query with support images, resulting\nin misclassification. To address this issue, we propose Stochastic-based Patch\nFiltering for Few-Shot Learning (SPFF) to attend to the patch embeddings that\nshow greater correlation with the class representation. The key concept of SPFF\ninvolves the stochastic filtering of patch embeddings, where patches less\nsimilar to the class-aware embedding are more likely to be discarded. With\npatch embedding filtered according to the probability of appearance, we use a\nsimilarity matrix that quantifies the relationship between the query image and\nits respective support images. Through a qualitative analysis, we demonstrate\nthat SPFF effectively focuses on patches where class-specific food features are\nmost prominent while successfully filtering out non-relevant patches. We\nvalidate our approach through extensive experiments on few-shot classification\nbenchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing\nSoA methods.", "AI": {"tldr": "SPFF is proposed to enhance few-shot learning for classifying food images by focusing on relevant features and filtering out non-relevant information.", "motivation": "The motivation is to solve the problem that few-shot learning models face when classifying food images due to the high visual variability and complexity of food images.", "method": "Stochastic-based Patch Filtering for Few-Shot Learning (SPFF) is proposed, which focuses on filtering out patch embeddings that are less similar to the class-aware embedding, thus helping the model concentrate on relevant food features and achieve better classification.", "result": "Experiments on benchmarks like Food-101, VireoFood-172, and UECFood-256 show that SPFF outperforms state-of-the-art methods for few-shot learning in food image classification.", "conclusion": "The conclusion is that SPFF can effectively focus on important class-specific food features and filter out irrelevant patches, leading to improved few-shot classification accuracy."}}
{"id": "2508.10104", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10104", "abs": "https://arxiv.org/abs/2508.10104", "authors": ["Oriane Siméoni", "Huy V. Vo", "Maximilian Seitzer", "Federico Baldassarre", "Maxime Oquab", "Cijo Jose", "Vasil Khalidov", "Marc Szafraniec", "Seungeun Yi", "Michaël Ramamonjisoa", "Francisco Massa", "Daniel Haziza", "Luca Wehrstedt", "Jianyuan Wang", "Timothée Darcet", "Théo Moutakanni", "Leonel Sentana", "Claire Roberts", "Andrea Vedaldi", "Jamie Tolan", "John Brandt", "Camille Couprie", "Julien Mairal", "Hervé Jégou", "Patrick Labatut", "Piotr Bojanowski"], "title": "DINOv3", "comment": null, "summary": "Self-supervised learning holds the promise of eliminating the need for manual\ndata annotation, enabling models to scale effortlessly to massive datasets and\nlarger architectures. By not being tailored to specific tasks or domains, this\ntraining paradigm has the potential to learn visual representations from\ndiverse sources, ranging from natural to aerial images -- using a single\nalgorithm. This technical report introduces DINOv3, a major milestone toward\nrealizing this vision by leveraging simple yet effective strategies. First, we\nleverage the benefit of scaling both dataset and model size by careful data\npreparation, design, and optimization. Second, we introduce a new method called\nGram anchoring, which effectively addresses the known yet unsolved issue of\ndense feature maps degrading during long training schedules. Finally, we apply\npost-hoc strategies that further enhance our models' flexibility with respect\nto resolution, model size, and alignment with text. As a result, we present a\nversatile vision foundation model that outperforms the specialized state of the\nart across a broad range of settings, without fine-tuning. DINOv3 produces\nhigh-quality dense features that achieve outstanding performance on various\nvision tasks, significantly surpassing previous self- and weakly-supervised\nfoundation models. We also share the DINOv3 suite of vision models, designed to\nadvance the state of the art on a wide spectrum of tasks and data by providing\nscalable solutions for diverse resource constraints and deployment scenarios.", "AI": {"tldr": "DINOv3, a self-supervised learning model, achieves outstanding performance across a broad range of vision tasks by leveraging effective strategies and outperforms specialized state of the art without fine-tuning", "motivation": "to realize the vision of eliminating the need for manual data annotation and enabling models to scale effortlessly to massive datasets and larger architectures", "method": "self-supervised learning to eliminate manual data annotation, using DINOv3 with strategies including scaling dataset and model size, Gram anchoring, and post-hoc resolution strategies", "result": "DINOv3 outperforms specialized state of the art across a broad range of settings without fine-tuning, achieving outstanding performance on various vision tasks and surpassing previous self- and weakly-supervised foundation models", "conclusion": "DINOv3 is a versatile vision foundation model that advances the state of the art on a wide spectrum of tasks and data, offering scalable solutions for diverse resource constraints and deployment scenarios"}}
{"id": "2508.10110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10110", "abs": "https://arxiv.org/abs/2508.10110", "authors": ["Sushrut Patwardhan", "Raghavendra Ramachandra", "Sushma Venkatesh"], "title": "Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model", "comment": null, "summary": "Morphing attack detection has become an essential component of face\nrecognition systems for ensuring a reliable verification scenario. In this\npaper, we present a multimodal learning approach that can provide a textual\ndescription of morphing attack detection. We first show that zero-shot\nevaluation of the proposed framework using Contrastive Language-Image\nPretraining (CLIP) can yield not only generalizable morphing attack detection,\nbut also predict the most relevant text snippet. We present an extensive\nanalysis of ten different textual prompts that include both short and long\ntextual prompts. These prompts are engineered by considering the human\nunderstandable textual snippet. Extensive experiments were performed on a face\nmorphing dataset that was developed using a publicly available face biometric\ndataset. We present an evaluation of SOTA pre-trained neural networks together\nwith the proposed framework in the zero-shot evaluation of five different\nmorphing generation techniques that are captured in three different mediums.", "AI": {"tldr": "研究提出了一种基于多模式学习的换脸攻击检测方法，并通过零样本评估验证了其有效性和可解释性。", "motivation": "换脸攻击检测已经成为面部识别系统中确保可靠验证场景的关键组成部分。因此，本研究旨在提供一种可以产生人类可理解文本描述的检测方法。", "method": "本文提出了一种多模式学习方法，能够为换脸攻击检测提供文本描述。该方法利用对比语言-图像预训练（CLIP）实现零样本评估，并且可以预测最相关的文本片段。", "result": "在使用公开的面部生物识别数据集构建的面部换脸数据集上进行了广泛的实验。结果表明，该方法不仅能够实现通用的换脸攻击检测，还能在零样本评估中对五种不同的换脸生成技术进行有效评价。", "conclusion": "实验展示了所提框架在零样本评估中对不同换脸技术的有效性，表明其在面部识别系统中具有潜在的应用价值。"}}
{"id": "2508.10113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10113", "abs": "https://arxiv.org/abs/2508.10113", "authors": ["Kaixin Peng", "Mengyang Zhao", "Haiyang Yu", "Teng Fu", "Bin Li"], "title": "Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs", "comment": null, "summary": "As the oldest mature writing system, Oracle Bone Script (OBS) has long posed\nsignificant challenges for archaeological decipherment due to its rarity,\nabstractness, and pictographic diversity. Current deep learning-based methods\nhave made exciting progress on the OBS decipherment task, but existing\napproaches often ignore the intricate connections between glyphs and the\nsemantics of OBS. This results in limited generalization and interpretability,\nespecially when addressing zero-shot settings and undeciphered OBS. To this\nend, we propose an interpretable OBS decipherment method based on Large\nVision-Language Models, which synergistically combines radical analysis and\npictograph-semantic understanding to bridge the gap between glyphs and meanings\nof OBS. Specifically, we propose a progressive training strategy that guides\nthe model from radical recognition and analysis to pictographic analysis and\nmutual analysis, thus enabling reasoning from glyph to meaning. We also design\na Radical-Pictographic Dual Matching mechanism informed by the analysis\nresults, significantly enhancing the model's zero-shot decipherment\nperformance. To facilitate model training, we propose the Pictographic\nDecipherment OBS Dataset, which comprises 47,157 Chinese characters annotated\nwith OBS images and pictographic analysis texts. Experimental results on public\nbenchmarks demonstrate that our approach achieves state-of-the-art Top-10\naccuracy and superior zero-shot decipherment capabilities. More importantly,\nour model delivers logical analysis processes, possibly providing\narchaeologically valuable reference results for undeciphered OBS, and thus has\npotential applications in digital humanities and historical research. The\ndataset and code will be released in https://github.com/PKXX1943/PD-OBS.", "AI": {"tldr": "本文基于大型视觉语言模型提出了一种可解释的甲骨文释读方法，结合部首和象形分析，设计了部首-象形双重匹配机制，并实现了优越的零样本释读性能。", "motivation": "现有的基于深度学习的方法在释读甲骨文方面取得了进展，但往往忽略了形符与甲骨文语义之间的复杂关联，导致泛化能力有限且解释性不足，特别是在解决零样本问题和未释读甲骨文时。", "method": "提出了一种基于大型视觉语言模型的可解释甲骨文释读方法，该方法结合了部首分析和图像语义理解，以在甲骨文中架起字符与意义之间的桥梁。具体来说，提出了一个渐进训练策略，指导模型从部首识别分析到象形分析和相互分析，从而实现从字符到意义的推理。同时设计了基于分析结果的部首-象形双重匹配机制，显著增强了模型的零样本释读性能。", "result": "在公开基准测试中，该方法实现了最先进的Top-10准确率和优越的零样本释读能力。", "conclusion": "我们的模型提供了逻辑分析过程，可能为未释读甲骨文提供考古研究有价值的参考结果，从而在数字人文和历史研究中具有潜在应用。"}}
{"id": "2508.09991", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09991", "abs": "https://arxiv.org/abs/2508.09991", "authors": ["Lovedeep Gondara", "Gregory Arbour", "Raymond Ng", "Jonathan Simkin", "Shebnum Devji"], "title": "Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry", "comment": null, "summary": "Automating data extraction from clinical documents offers significant\npotential to improve efficiency in healthcare settings, yet deploying Natural\nLanguage Processing (NLP) solutions presents practical challenges. Drawing upon\nour experience implementing various NLP models for information extraction and\nclassification tasks at the British Columbia Cancer Registry (BCCR), this paper\nshares key lessons learned throughout the project lifecycle. We emphasize the\ncritical importance of defining problems based on clear business objectives\nrather than solely technical accuracy, adopting an iterative approach to\ndevelopment, and fostering deep interdisciplinary collaboration and co-design\ninvolving domain experts, end-users, and ML specialists from inception. Further\ninsights highlight the need for pragmatic model selection (including hybrid\napproaches and simpler methods where appropriate), rigorous attention to data\nquality (representativeness, drift, annotation), robust error mitigation\nstrategies involving human-in-the-loop validation and ongoing audits, and\nbuilding organizational AI literacy. These practical considerations,\ngeneralizable beyond cancer registries, provide guidance for healthcare\norganizations seeking to successfully implement AI/NLP solutions to enhance\ndata management processes and ultimately improve patient care and public health\noutcomes.", "AI": {"tldr": "文章总结了在BCCR实施NLP解决方案的实践经验，强调了基于清晰的业务目标定义问题、采用迭代开发方法、跨学科协作、务实的模型选择、对数据质量的重视、稳健的错误缓解策略及建立组织AI素养的重要性。", "motivation": "该研究的动机在于通过分享实际部署NLP解决方案的经验教训，为希望成功实施AI/NLP解决方案的医疗机构提供指导。这些解决方案旨在提高医疗活动中数据管理的效率，最终改善患者护理和公共卫生结果。", "method": "该文章通过分享在不列颠哥伦比亚癌症登记处（BCCR）部署NLP解决方案的经验，强调了问题定义、迭代开发过程、跨学科合作的重要性。文章还讨论了模型选择的务实方法，数据质量的重要性，以及建立组织的AI素养等实际考量。", "result": "该研究的结果显示，通过跨学科团队的深度合作、严格的数据质量管理及合理的模型选择，可以成功实施NLP解决方案以改善医疗数据管理。这些经验在癌症登记之外的医疗机构也有借鉴价值。", "conclusion": "结论强调，医疗机构在部署AI/NLP解决方案时，需不仅仅关注技术准确性，还要重视业务目标的定义、过程中的迭代开发以及跨学科的团队合作，以确保技术能够成功且有效地被应用。"}}
{"id": "2508.10132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10132", "abs": "https://arxiv.org/abs/2508.10132", "authors": ["Arianna Bunnell", "Devon Cataldi", "Yannik Glaser", "Thomas K. Wolfgruber", "Steven Heymsfield", "Alan B. Zonderman", "Thomas L. Kelly", "Peter Sadowski", "John A. Shepherd"], "title": "Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging", "comment": "Preprint of manuscript accepted to the ShapeMI workshop at MICCAI\n  2025", "summary": "Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost\nwhole-body imaging modality, widely used for body composition assessment. We\ndevelop and validate a deep learning method for automatic fiducial point\nplacement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method\nachieves 99.5% percentage correct keypoints in an external testing dataset. To\ndemonstrate the value for shape and appearance modeling (SAM), our method is\nused to place keypoints on 35,928 scans for five different TBDXA imaging modes,\nthen associations with health markers are tested in two cohorts not used for\nSAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature\ndistributions associated with health biomarkers are shown to corroborate\nexisting evidence and generate new hypotheses on body composition and shape's\nrelationship to various frailty, metabolic, inflammation, and cardiometabolic\nhealth markers. Evaluation scripts, model weights, automatic point file\ngeneration code, and triangulation files are available at\nhttps://github.com/hawaii-ai/dxa-pointplacement.", "AI": {"tldr": "研究开发了一种基于深度学习的自动方法来放置全身DEXA图像的关键点，并证明了该方法在提高形状和外观模型准确性及与健康标志物关联分析中的效用。", "motivation": "全身DEXA成像是一种广泛用于身体组成评估的相对低成本的全身成像方式，本研究旨在通过开发自动方法提高其在形状和外观建模中对健康标志物关联分析的效率和准确度。", "method": "通过开发和验证了一种基于深度学习的方法，自动在1,683个手动标注的全身DEXA扫描图像上放置标志点，并在不同的DEXA成像模式的35,928个扫描图像上应用此方法，以验证其在形状和外观建模(SAM)中的价值。", "result": "该方法在外部测试数据集上实现了99.5%的正确关键点识别率，并且在未用于SAM模型生成的两个队列中进行了与健康标志物的关联测试。结果表明，与健康生物标志物相关的SAM特征分布不仅证实了现有的证据，而且还产生了关于身体组成和形状与易损性、代谢、炎症和心血管代谢健康标志物之间关系的新假设。", "conclusion": "研究证实了该自动化标志点放置方法在提高全身DEXA图像分析精度方面的有效性，同时为身体组成和形状与健康标志物之间的关系提供了新的视角。"}}
{"id": "2508.09993", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09993", "abs": "https://arxiv.org/abs/2508.09993", "authors": ["Hugo Massaroli", "Leonardo Iara", "Emmanuel Iarussi", "Viviana Siless"], "title": "A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in realworld\napplications, yet concerns about their fairness persist especially in\nhighstakes domains like criminal justice, education, healthcare, and finance.\nThis paper introduces transparent evaluation protocol for benchmarking the\nfairness of opensource LLMs using smart contracts on the Internet Computer\nProtocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,\nimmutable, and reproducible evaluations by executing onchain HTTP requests to\nhosted Hugging Face endpoints and storing datasets, prompts, and metrics\ndirectly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the\nPISA dataset for academic performance prediction (OECD, 2018), a dataset\nsuitable for fairness evaluation using statistical parity and equal opportunity\nmetrics (Hardt et al., 2016). We also evaluate structured Context Association\nMetrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure\nsocial bias in contextual associations. We further extend our analysis with a\nmultilingual evaluation across English, Spanish, and Portuguese using the\nKaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic\ndisparities. All code and results are open source, enabling community audits\nand longitudinal fairness tracking across model versions.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.10133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10133", "abs": "https://arxiv.org/abs/2508.10133", "authors": ["Thanh-Dat Truong", "Christophe Bobda", "Nitin Agarwal", "Khoa Luu"], "title": "MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning", "comment": null, "summary": "Multimodal learning has gained much success in recent years. However, current\nmultimodal fusion methods adopt the attention mechanism of Transformers to\nimplicitly learn the underlying correlation of multimodal features. As a\nresult, the multimodal model cannot capture the essential features of each\nmodality, making it difficult to comprehend complex structures and correlations\nof multimodal inputs. This paper introduces a novel Multimodal Attention-based\nNormalizing Flow (MANGO) approach\\footnote{The source code of this work will be\npublicly available.} to developing explicit, interpretable, and tractable\nmultimodal fusion learning. In particular, we propose a new Invertible\nCross-Attention (ICA) layer to develop the Normalizing Flow-based Model for\nmultimodal data. To efficiently capture the complex, underlying correlations in\nmultimodal data in our proposed invertible cross-attention layer, we propose\nthree new cross-attention mechanisms: Modality-to-Modality Cross-Attention\n(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality\nCross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based\nNormalizing Flow to enable the scalability of our proposed method to\nhigh-dimensional multimodal data. Our experimental results on three different\nmultimodal learning tasks, i.e., semantic segmentation, image-to-image\ntranslation, and movie genre classification, have illustrated the\nstate-of-the-art (SoTA) performance of the proposed approach.", "AI": {"tldr": "本文提出了一种新的基于归一化流的多模态融合方法MANGO，通过三种新的交叉注意力机制在多模态数据处理中实现了显式、可解释且可操作的学习，实验结果验证了其优越性能。", "motivation": "由于当前的多模态融合方法采用Transformer的注意力机制来隐式地学习多模态特征之间的潜在关联，使得模型难以捕捉到每个模态的基本特征以及多模态输入之间的复杂结构和关联。因此，本文旨在开发一种显式、可解释且可操作的多模态融合学习方法。", "method": "本文提出了一个新的基于归一化流的多模态融合学习方法，即Multimodal Attention-based Normalizing Flow (MANGO)。具体来说，我们提出了一种新的可逆交叉注意力层（ICA），并在该层中采用了三种新的交叉注意力机制：模态到模态交叉注意力（MMCA）、跨模态交叉注意力（IMCA）和可学习的跨模态交叉注意力（LICA），以更有效地捕捉多模态数据中的复杂潜在关联。", "result": "实验结果表明，所提出的方法在三种多模态学习任务（语义分割、图像到图像翻译和电影类型分类）上达到了最先进的性能。", "conclusion": "本文提出的MANGO方法在三种多模态学习任务上展示了最先进的性能，证明了其在多模态数据处理中的有效性和优越性。"}}
{"id": "2508.09997", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.09997", "abs": "https://arxiv.org/abs/2508.09997", "authors": ["Johannes Schneider", "Béatrice S. Hasler", "Michaela Varrone", "Fabian Hoya", "Thomas Schroffenegger", "Dana-Kristin Mah", "Karl Peböck"], "title": "Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling", "comment": "Accepted at the International Conference on Computer-Human\n  Interaction Research and Applications (CHIRA), 2025", "summary": "We analyze anonymous interaction data of minors in class-rooms spanning\nseveral months, schools, and subjects employing a novel, simple topic modeling\napproach. Specifically, we categorize more than 17,000 messages generated by\nstudents, teachers, and ChatGPT in two dimensions: content (such as nature and\npeople) and tasks (such as writing and explaining). Our hierarchical\ncategorization done separately for each dimension includes exemplary prompts,\nand provides both a high-level overview as well as tangible insights. Prior\nworks mostly lack a content or thematic categorization. While task\ncategorizations are more prevalent in education, most have not been supported\nby real-world data for K-12. In turn, it is not surprising that our analysis\nyielded a number of novel applications. In deriving these insights, we found\nthat many of the well-established classical and emerging computational methods,\ni.e., topic modeling, for analysis of large amounts of texts underperform,\nleading us to directly apply state-of-the-art LLMs with adequate pre-processing\nto achieve hierarchical topic structures with better human alignment through\nexplicit instructions than prior approaches. Our findings support fellow\nresearchers, teachers and students in enriching the usage of GenAI, while our\ndiscussion also highlights a number of concerns and open questions for future\nresearch.", "AI": {"tldr": "研究使用新的主题建模方法，对教室中未成年人的互动进行分类分析，发现现有计算方法不足，并提出使用先进语言模型改善主题结构，增强对生成式AI的应用。", "motivation": "分析教室里未成年人的匿名交互数据，以往的研究大多缺乏内容或主题分类，而我们的研究填补了这一空白，特别是在K-12教育中得到真实世界数据支持的任务分类方面。", "method": "采用了一种新的、简单的主题建模方法，分析了数月间学生们、教师们和ChatGPT生成的超过17000条信息，分别在内容（如自然和人物）和任务（如写作和解释）两个维度进行分类。", "result": "得到了一个分层分类，包括每个维度的示例提示，提供了高层次的概述和实际见解。发现许多经典和新兴的计算方法在分析大量文本时表现不佳，而使用先进的LLM并进行适当的预处理可以更好地实现层次主题结构并与人类对齐。", "conclusion": "研究支持其他研究者、教师和学生增强对生成式AI的使用，并讨论了一些未来研究的关注点和开放性问题。"}}
{"id": "2508.10156", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.10156", "abs": "https://arxiv.org/abs/2508.10156", "authors": ["Nitin Rai", "Nathan S. Boyd", "Gary E. Vallad", "Arnold W. Schumann"], "title": "Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model", "comment": null, "summary": "The current advancements in generative artificial intelligence (GenAI) models\nhave paved the way for new possibilities for generating high-resolution\nsynthetic images, thereby offering a promising alternative to traditional image\nacquisition for training computer vision models in agriculture. In the context\nof crop disease diagnosis, GenAI models are being used to create synthetic\nimages of various diseases, potentially facilitating model creation and\nreducing the dependency on resource-intensive in-field data collection.\nHowever, limited research has been conducted on evaluating the effectiveness of\nintegrating real with synthetic images to improve disease classification\nperformance. Therefore, this study aims to investigate whether combining a\nlimited number of real images with synthetic images can enhance the prediction\naccuracy of an EfficientNetV2-L model for classifying watermelon\n\\textit{(Citrullus lanatus)} diseases. The training dataset was divided into\nfive treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1\nreal-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to\nimprove variability and model generalization). All treatments were trained\nusing a custom EfficientNetV2-L architecture with enhanced fine-tuning and\ntransfer learning techniques. Models trained on H2, H3, and H4 treatments\ndemonstrated high precision, recall, and F1-score metrics. Additionally, the\nweighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying\nthat the addition of a small number of real images with a considerable volume\nof synthetic images improved model performance and generalizability. Overall,\nthis validates the findings that synthetic images alone cannot adequately\nsubstitute for real images; instead, both must be used in a hybrid manner to\nmaximize model performance for crop disease classification.", "AI": {"tldr": "研究考察了使用EfficientNetV2-L模型结合真实与合成图像提高西瓜病害分类准确性的效果，结果表明适量结合真实图像和大量合成图像提高了模型性能和泛化性，综合得分从0.65提升到1.00。", "motivation": "鉴于合成图像生成技术的进步，尤其是在农业中用于检测作物病害，这项研究旨在评估结合真实与合成图像来提高病害分类性能的有效性。", "method": "研究将训练数据集分为五组进行处理：仅使用真实图像，仅使用合成图像，1:1的真实与合成图像比例，1:10的真实与合成图像比例，以及1:10比例外加随机图像。所有处理均采用自定义的EfficientNetV2-L架构进行训练，同时附加增强的微调和迁移学习技术。", "result": "实验结果显示，使用1:1，1:10，以及1:10加随机图像的处理组展示了较高的精确度，召回率和F1得分。综合得分（加权F1得分）从仅使用真实图像的0.65增加到了1:10比例组及随机图像组的1.00。", "conclusion": "尽管合成图像在提高模型性能方面发挥了作用，但单独使用合成图像并不像结合真实图像时那样有效，结合使用两者可以达到最佳模型性能。"}}
{"id": "2508.09998", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09998", "abs": "https://arxiv.org/abs/2508.09998", "authors": ["Lucie-Aimée Kaffee", "Giada Pistilli", "Yacine Jernite"], "title": "INTIMA: A Benchmark for Human-AI Companionship Behavior", "comment": null, "summary": "AI companionship, where users develop emotional bonds with AI systems, has\nemerged as a significant pattern with positive but also concerning\nimplications. We introduce Interactions and Machine Attachment Benchmark\n(INTIMA), a benchmark for evaluating companionship behaviors in language\nmodels. Drawing from psychological theories and user data, we develop a\ntaxonomy of 31 behaviors across four categories and 368 targeted prompts.\nResponses to these prompts are evaluated as companionship-reinforcing,\nboundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,\nand Claude-4 reveals that companionship-reinforcing behaviors remain much more\ncommon across all models, though we observe marked differences between models.\nDifferent commercial providers prioritize different categories within the more\nsensitive parts of the benchmark, which is concerning since both appropriate\nboundary-setting and emotional support matter for user well-being. These\nfindings highlight the need for more consistent approaches to handling\nemotionally charged interactions.", "AI": {"tldr": "INTIMA被用于评估AI陪伴系统的行为模式，发现虽然所有模型中陪伴强化行为较为普遍，但不同模型在敏感部分的表现各异，需关注情感健康的边界设置和情感支持。", "motivation": "此研究动机在于评估AI陪伴系统的用户情感绑定，识别AI陪伴中存在的积极和消极影响，并提出一个评估标准INTIMA。", "method": "介绍了INTIMA（INTeractions and Machine Attachment Benchmark），这是一个用于评估语言模型中陪伴行为的基准。INTIMA基于心理学理论和用户数据，开发了一个包含四个类别共31种行为的分类体系，并设计了368个针对性提示语，用于评估模型的陪伴强化行为、边界保持行为或中性行为。", "result": "应用INTIMA对Gemma-3，Phi-4，o3-mini和Claude-4这四种模型进行测试，发现所有模型中的陪伴强化性行为更为普遍，但不同模型在敏感部分的优先级有所不同。", "conclusion": "研究结果表明，在处理情感相关的交互时，需要采用更加一致的方法来确保用户的情绪健康，而不是仅仅关注陪伴强化。"}}
{"id": "2508.10171", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.10171", "abs": "https://arxiv.org/abs/2508.10171", "authors": ["Aaditya Baranwal", "Abdul Mueez", "Jason Voelker", "Guneet Bhatia", "Shruti Vyas"], "title": "SynSpill: Improved Industrial Spill Detection With Synthetic Data", "comment": "Accepted at ICCV (VISION'25 Workshop) 2025", "summary": "Large-scale Vision-Language Models (VLMs) have transformed general-purpose\nvisual recognition through strong zero-shot capabilities. However, their\nperformance degrades significantly in niche, safety-critical domains such as\nindustrial spill detection, where hazardous events are rare, sensitive, and\ndifficult to annotate. This scarcity -- driven by privacy concerns, data\nsensitivity, and the infrequency of real incidents -- renders conventional\nfine-tuning of detectors infeasible for most industrial settings.\n  We address this challenge by introducing a scalable framework centered on a\nhigh-quality synthetic data generation pipeline. We demonstrate that this\nsynthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of\nVLMs and substantially boosts the performance of state-of-the-art object\ndetectors such as YOLO and DETR. Notably, in the absence of synthetic data\n(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than\nthese detectors. When SynSpill is used, both VLMs and detectors achieve marked\nimprovements, with their performance becoming comparable.\n  Our results underscore that high-fidelity synthetic data is a powerful means\nto bridge the domain gap in safety-critical applications. The combination of\nsynthetic generation and lightweight adaptation offers a cost-effective,\nscalable pathway for deploying vision systems in industrial environments where\nreal data is scarce/impractical to obtain.\n  Project Page: https://synspill.vercel.app", "AI": {"tldr": "提出了一个基于高质量合成数据生成管道的可扩展框架，使得VLMs能够有效进行参数高效微调并提高目标检测器的性能。", "motivation": "大规模视觉语言模型（VLMs）已经通过强大的零样本学习能力改变了通用视觉识别。不过，它们在如工业泄漏检测这样少样本且敏感领域表现不佳，由于隐私问题、数据敏感性和实际事件的罕见性，传统微调方法在这里不可行。", "method": "通过构建高质量的合成数据生成管道来应对挑战，展示了这种合成数据能够有效进行参数高效微调（PEFT），并且大幅提高了Yolo和DETR等先进目标检测器的性能。", "result": "结果显示，即使没有合成数据，VLMs在未见泄漏场景下的泛化能力仍优于这些检测器，而使用SynSpill数据，VLMs和检测器的性能都有显著提高。", "conclusion": "这些结果表明，高保真合成数据是弥合安全关键应用领域差距的有效工具。合成生成和轻量级适应的结合为在难以获取真实数据的工业环境部署视觉系统提供了一种性价比高、可扩展的方法。"}}
{"id": "2508.09999", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09999", "abs": "https://arxiv.org/abs/2508.09999", "authors": ["Yuzhuo Xiao", "Zeyu Han", "Yuhan Wang", "Huaizu Jiang"], "title": "XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs", "comment": "For associated code and dataset, see https://github.com/neu-vi/XFacta", "summary": "The rapid spread of multimodal misinformation on social media calls for more\neffective and robust detection methods. Recent advances leveraging multimodal\nlarge language models (MLLMs) have shown the potential in addressing this\nchallenge. However, it remains unclear exactly where the bottleneck of existing\napproaches lies (evidence retrieval v.s. reasoning), hindering the further\nadvances in this field. On the dataset side, existing benchmarks either contain\noutdated events, leading to evaluation bias due to discrepancies with\ncontemporary social media scenarios as MLLMs can simply memorize these events,\nor artificially synthetic, failing to reflect real-world misinformation\npatterns. Additionally, it lacks comprehensive analyses of MLLM-based model\ndesign strategies. To address these issues, we introduce XFacta, a\ncontemporary, real-world dataset that is better suited for evaluating\nMLLM-based detectors. We systematically evaluate various MLLM-based\nmisinformation detection strategies, assessing models across different\narchitectures and scales, as well as benchmarking against existing detection\nmethods. Building on these analyses, we further enable a semi-automatic\ndetection-in-the-loop framework that continuously updates XFacta with new\ncontent to maintain its contemporary relevance. Our analysis provides valuable\ninsights and practices for advancing the field of multimodal misinformation\ndetection. The code and data have been released.", "AI": {"tldr": "The paper tackles the issue of multimodal misinformation on social media by introducing a new real-world dataset, XFacta, and evaluating various MLLM-based detection strategies, offering insights for the field.", "motivation": "There is a need for better detection of multimodal misinformation on social media due to the rapid spread and existing bottlenecks in current approaches. The paper addresses this by proposing a new dataset and evaluating strategies.", "method": "The paper introduces XFacta, a contemporary, real-world dataset for evaluating MLLM-based detectors and systematically evaluates various MLLM-based misinformation detection strategies. It also provides a semi-automatic detection-in-the-loop framework.", "result": "The paper systematically evaluates different MLLM architectures and scales in the context of misinformation detection, and introduces a dynamic data set maintenance framework.", "conclusion": "The analysis provides valuable insights into the effectiveness of various MLLM-based methods and architectures, which can guide the advancement of multimodal misinformation detection methods."}}
{"id": "2508.10227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10227", "abs": "https://arxiv.org/abs/2508.10227", "authors": ["Yuning Huang", "Jiahao Pang", "Fengqing Zhu", "Dong Tian"], "title": "EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting", "comment": null, "summary": "As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)\ndemonstrates fast training/rendering with superior visual quality. The two\ntasks of 3DGS, Gaussian creation and view rendering, are typically separated\nover time or devices, and thus storage/transmission and finally compression of\n3DGS Gaussians become necessary. We begin with a correlation and statistical\nanalysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals\nthat spherical harmonic AC attributes precisely follow Laplace distributions,\nwhile mixtures of Gaussian distributions can approximate rotation, scaling, and\nopacity. Additionally, harmonic AC attributes manifest weak correlations with\nother attributes except for inherited correlations from a color space. A\nfactorized and parameterized entropy coding method, EntropyGS, is hereinafter\nproposed. During encoding, distribution parameters of each Gaussian attribute\nare estimated to assist their entropy coding. The quantization for entropy\ncoding is adaptively performed according to Gaussian attribute types. EntropyGS\ndemonstrates about 30x rate reduction on benchmark datasets while maintaining\nsimilar rendering quality compared to input 3DGS data, with a fast encoding and\ndecoding time.", "AI": {"tldr": "本文提出了EntropyGS，一种针对3DGS的高斯属性参数化熵编码方法，实现了大约30倍的数据压缩，同时保持了高质量的渲染效果并且具有快速的编码和解码速度。", "motivation": "由于3DGS的高斯创建和视图渲染任务往往分时间或设备进行，因此需要对3DGS高斯进行存储、传输和压缩。研究通过统计分析发现，高斯属性存在特殊分布特点，比如球面谐波AC属性符合Laplace分布，而旋转、缩放和不透明度可以用高斯混合分布来近似。基于这些发现，研究旨在提出一种高效的压缩方法。", "method": "本研究提出了一个名为EntropyGS的参数化熵编码方法，用于编码3D Gaussian Splatting（3DGS）的高斯参数。编码过程中，会根据每种高斯属性的分布参数进行熵编码，量化过程会根据属性类型自适应进行。", "result": "实验表明，与原始3DGS数据相比，EntropyGS能够在基准数据集上实现大约30倍的压缩率，同时保持相似的渲染质量，具有较快的编码和解码速度。", "conclusion": "实验结果证明，EntropyGS是一种有效的3DGS高斯数据压缩方法，能够大幅缩减数据量，而且保持了高效的渲染质量。"}}
{"id": "2508.10000", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10000", "abs": "https://arxiv.org/abs/2508.10000", "authors": ["Chenhao Xue", "Yuanzhe Jin", "Adrian Carrasco-Revilla", "Joyraj Chakraborty", "Min Chen"], "title": "AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification", "comment": null, "summary": "When developing text classification models for real world applications, one\nmajor challenge is the difficulty to collect sufficient data for all text\nclasses. In this work, we address this challenge by utilizing large language\nmodels (LLMs) to generate synthetic data and using such data to improve the\nperformance of the models without waiting for more real data to be collected\nand labelled. As an LLM generates different synthetic data in response to\ndifferent input examples, we formulate an automated workflow, which searches\nfor input examples that lead to more ``effective'' synthetic data for improving\nthe model concerned. We study three search strategies with an extensive set of\nexperiments, and use experiment results to inform an ensemble algorithm that\nselects a search strategy according to the characteristics of a class. Our\nfurther experiments demonstrate that this ensemble approach is more effective\nthan each individual strategy in our automated workflow for improving\nclassification models using LLMs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.10232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10232", "abs": "https://arxiv.org/abs/2508.10232", "authors": ["Paul H. Acosta", "Pingjun Chen", "Simon P. Castillo", "Maria Esther Salvatierra", "Yinyin Yuan", "Xiaoxi Pan"], "title": "CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics", "comment": null, "summary": "Xenium, a new spatial transcriptomics platform, enables\nsubcellular-resolution profiling of complex tumor tissues. Despite the rich\nmorphological information in histology images, extracting robust cell-level\nfeatures and integrating them with spatial transcriptomics data remains a\ncritical challenge. We introduce CellSymphony, a flexible multimodal framework\nthat leverages foundation model-derived embeddings from both Xenium\ntranscriptomic profiles and histology images at true single-cell resolution. By\nlearning joint representations that fuse spatial gene expression with\nmorphological context, CellSymphony achieves accurate cell type annotation and\nuncovers distinct microenvironmental niches across three cancer types. This\nwork highlights the potential of foundation models and multimodal fusion for\ndeciphering the physiological and phenotypic orchestration of cells within\ncomplex tissue ecosystems.", "AI": {"tldr": "CellSymphony is introduced as a multimodal framework that uses embeddings derived from both Xenium spatial transcriptomic profiles and histology images at single-cell resolution to achieve accurate cell type annotation and uncover distinct microenvironmental niches in complex tumor tissues.", "motivation": "The motivation for this research is to address the difficulty in extracting robust cell-level features from histology images and integrating them with spatial transcriptomics data at true single-cell resolution.", "method": "The method involves developing CellSymphony, a flexible framework that leverages foundation model-derived embeddings to create joint representations that combine spatial gene expression with morphological context.", "result": "The results demonstrate that CellSymphony accurately annotates cell types and identifies distinct microenvironmental niches across three types of cancer.", "conclusion": "The conclusion is that the use of foundation models and multimodal fusion can effectively decipher the physiological and phenotypic organization of cells in complex tissue environments."}}
{"id": "2508.10001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10001", "abs": "https://arxiv.org/abs/2508.10001", "authors": ["Rakesh Thakur", "Sneha Sharma", "Gauri Chopra"], "title": "HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish", "comment": null, "summary": "Fact-checking in code-mixed, low-resource languages such as Hinglish remains\nan underexplored challenge in natural language processing. Existing\nfact-verification systems largely focus on high-resource, monolingual settings\nand fail to generalize to real-world political discourse in linguistically\ndiverse regions like India. Given the widespread use of Hinglish by public\nfigures, particularly political figures, and the growing influence of social\nmedia on public opinion, there's a critical need for robust, multilingual and\ncontext-aware fact-checking tools. To address this gap a novel benchmark HiFACT\ndataset is introduced with 1,500 realworld factual claims made by 28 Indian\nstate Chief Ministers in Hinglish, under a highly code-mixed low-resource\nsetting. Each claim is annotated with textual evidence and veracity labels. To\nevaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking\nmodel is proposed that combines multilingual contextual encoding,\nclaim-evidence semantic alignment, evidence graph construction, graph neural\nreasoning, and natural language explanation generation. Experimental results\nshow that HiFACTMix outperformed accuracy in comparison to state of art\nmultilingual baselines models and provides faithful justifications for its\nverdicts. This work opens a new direction for multilingual, code-mixed, and\npolitically grounded fact verification research.", "AI": {"tldr": "A novel dataset (HiFACT) and a graph-aware model (HiFACTMix) are introduced for fact-checking code-mixed Hinglish, outperforming existing multilingual models in accuracy and explanatory power.", "motivation": "The research aims to address the underexplored challenge of fact-checking in code-mixed, low-resource languages, such as Hinglish, which is particularly crucial in regions with diverse languages and heavy political discourse, like India.", "method": "The paper proposes a novel graph-aware, retrieval-augmented fact-checking model that integrates multilingual contextual encoding, semantic alignment of claims and evidence, construction of evidence graphs, graph neural reasoning, and natural language explanation generation.", "result": "HiFACTMix, the proposed model, has demonstrated superior accuracy on the HiFACT dataset when compared to current multilingual state-of-the-art models, and also provides reliable explanations for its verdicts.", "conclusion": "This study opens new avenues for research in multilingual, code-mixed, and politically focused fact-checking systems, suggesting that the proposed model can serve as a viable solution for combating misinformation in linguistically diverse settings."}}
{"id": "2508.10256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10256", "abs": "https://arxiv.org/abs/2508.10256", "authors": ["Xinan Zhang", "Haolin Wang", "Yung-An Hsieh", "Zhongyu Yang", "Anthony Yezzi", "Yi-Chang Tsai"], "title": "Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets", "comment": null, "summary": "Crack detection plays a crucial role in civil infrastructures, including\ninspection of pavements, buildings, etc., and deep learning has significantly\nadvanced this field in recent years. While numerous technical and review papers\nexist in this domain, emerging trends are reshaping the landscape. These shifts\ninclude transitions in learning paradigms (from fully supervised learning to\nsemi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation\nand fine-tuning foundation models), improvements in generalizability (from\nsingle-dataset performance to cross-dataset evaluation), and diversification in\ndataset reacquisition (from RGB images to specialized sensor-based data). In\nthis review, we systematically analyze these trends and highlight\nrepresentative works. Additionally, we introduce a new dataset collected with\n3D laser scans, 3DCrack, to support future research and conduct extensive\nbenchmarking experiments to establish baselines for commonly used deep learning\nmethodologies, including recent foundation models. Our findings provide\ninsights into the evolving methodologies and future directions in deep\nlearning-based crack detection. Project page:\nhttps://github.com/nantonzhang/Awesome-Crack-Detection", "AI": {"tldr": "这篇论文综述了深度学习在裂缝检测领域的最新趋势，包括新的学习范式、模型泛化能力和数据集多样性的改进，并引入了一个新的3D激光扫描数据集3DCrack来进行基准测试。", "motivation": "随着深度学习技术的发展，裂缝检测方法也在不断进步。然而，现有研究缺乏对这些趋势的系统性分析。本文旨在填补这一空白，提供最新的技术趋势分析。", "method": "本文采用系统性方法分析深度学习在裂缝检测领域的最新技术趋势，包括新的学习范式、模型泛化能力和数据集多样性改进。", "result": "通过分析各种趋势，本文高亮了一些代表性的工作，并引入了3DCrack数据集来支持未来的研究。通过广泛的基准测试实验，我们建立了深度学习方法的基线。", "conclusion": "本文总结了深度学习在裂缝检测领域的发展趋势，提供了新的基准数据集3DCrack，为未来的研究提供了宝贵的信息。"}}
{"id": "2508.10003", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10003", "abs": "https://arxiv.org/abs/2508.10003", "authors": ["Austin C. Kozlowski", "Callin Dai", "Andrei Boutyline"], "title": "Semantic Structure in Large Language Model Embeddings", "comment": null, "summary": "Psychological research consistently finds that human ratings of words across\ndiverse semantic scales can be reduced to a low-dimensional form with\nrelatively little information loss. We find that the semantic associations\nencoded in the embedding matrices of large language models (LLMs) exhibit a\nsimilar structure. We show that the projections of words on semantic directions\ndefined by antonym pairs (e.g. kind - cruel) correlate highly with human\nratings, and further find that these projections effectively reduce to a\n3-dimensional subspace within LLM embeddings, closely resembling the patterns\nderived from human survey responses. Moreover, we find that shifting tokens\nalong one semantic direction causes off-target effects on geometrically aligned\nfeatures proportional to their cosine similarity. These findings suggest that\nsemantic features are entangled within LLMs similarly to how they are\ninterconnected in human language, and a great deal of semantic information,\ndespite its apparent complexity, is surprisingly low-dimensional. Furthermore,\naccounting for this semantic structure may prove essential for avoiding\nunintended consequences when steering features.", "AI": {"tldr": "研究发现大型语言模型（LLMs）展现的语义结构与人类评分相似，表明语义信息是低维度的，并通过调整这些语义结构可以减少操纵时的无意后果。", "motivation": "研究动机是探索人类语言中的语义特征如何在大型语言模型（LLMs）中体现，以及这些语义特征是如何相互关联的。通过这种方式，研究希望能揭示语义信息的低维度特性，并且探讨在操纵特征时如何避免无意造成的后果。", "method": "通过对比大型语言模型（LLMs）嵌入矩阵中的语义关联与人类在各种语义尺度上对词语的评分，研究者进行了分析。他们使用反义词对定义的语义方向（例如善良-残忍）的词语投影与人类评分的相关性来展示这一点，并发现这些投影在LLMs嵌入中有效减少到一个三维子空间，类似于人类调查响应中得出的模式。此外，他们还发现沿着一个语义方向偏移标记物会引起成比例于它们余弦相似性的几何对齐特征的脱靶效应。", "result": "结果表明，LLMs中的语义特征与人类语言中的那样是相互纠缠的，尽管语义信息看起来很复杂，但实际上它是一个惊人的低维度。此外，考虑到这种语义结构对于在操纵特征时避免无意的后果是至关重要的。", "conclusion": "研究结论指出，语义特征在LLMs中的呈现方式与它们在人类语言中的紧密相连的呈现方式相似，这表明了语义信息的低维度特性。此外，这类语义信息的结构对于改善特征操纵的效果和限制意外后果是重要的。"}}
{"id": "2508.10264", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10264", "abs": "https://arxiv.org/abs/2508.10264", "authors": ["Haonan Ge", "Yiwei Wang", "Ming-Hsuan Yang", "Yujun Cai"], "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown strong performance across\nmultimodal tasks. However, they often produce hallucinations -- text that is\ninconsistent with visual input, due to the limited ability to verify\ninformation in different regions of the image. To address this, we propose\nMulti-Region Fusion Decoding (MRFD), a training-free decoding method that\nimproves factual grounding by modeling inter-region consistency. MRFD\nidentifies salient regions using cross-attention, generates initial responses\nfor each, and computes reliability weights based on Jensen-Shannon Divergence\n(JSD) among the responses. These weights guide a consistency-aware fusion of\nper-region predictions, using region-aware prompts inspired by Chain-of-Thought\nreasoning. Experiments across multiple LVLMs and benchmarks show that MRFD\nsignificantly reduces hallucinations and improves response factuality without\nrequiring model updates.", "AI": {"tldr": "提出多区域融合解码（MRFD）方法解决大型视觉-语言模型产生的幻觉问题，增强模型事实准确性。", "motivation": "大型视觉-语言模型在处理多模态任务时可能存在信息验证能力的不足，因此常常会产生与视觉输入不一致的文本。MRFD旨在解决这一问题，通过改进模型的区域间一致性来增强其事实准确性的表现。", "method": "MRFD通过交叉注意力识别图像中的重要区域，并为每个区域生成初始响应。通过计算响应间的Jensen-Shannon散度（JSD）来确定每个响应的可靠性权重。利用这些权重，MRFD采用一种基于区域标记的提示方法，对各个区域的预测进行一致性导向的融合。", "result": "大型视觉-语言模型（LVLMs）在多模态任务中表现出色，但它们常常产生与视觉输入不一致的文本（幻觉），原因是这些模型在验证图像不同区域的信息方面能力有限。为此，我们提出了一种无训练的解码方法——多区域融合解码（MRFD），通过建模区域间的相互一致性来提升事实依据性。MRFD通过交叉注意力识别显著区域，为每个区域生成初始响应，并基于响应之间的Jensen-Shannon散度（JSD）计算可靠性权重。这些权重指引了一种基于区域认知提示的连贯性感知的区域预测融合，这种方法受到了思维链推理的启发。实验结果显示，MRFD能够显著减少幻觉并提升响应的事实性，而且无需更新模型。", "conclusion": "MRFD作为一个不需模型再训练的解码方法，在不更新模型的前提下能够有效减少幻想并提高响应的准确性，适用于多种大型视觉-语言模型和基准测试。"}}
{"id": "2508.10004", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10004", "abs": "https://arxiv.org/abs/2508.10004", "authors": ["Andrés Carvallo", "Denis Parra", "Peter Brusilovsky", "Hernan Valdivieso", "Gabriel Rada", "Ivania Donoso", "Vladimir Araujo"], "title": "User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents", "comment": null, "summary": "The attention mechanism is a core component of the Transformer architecture.\nBeyond improving performance, attention has been proposed as a mechanism for\nexplainability via attention weights, which are associated with input features\n(e.g., tokens in a document). In this context, larger attention weights may\nimply more relevant features for the model's prediction. In evidence-based\nmedicine, such explanations could support physicians' understanding and\ninteraction with AI systems used to categorize biomedical literature. However,\nthere is still no consensus on whether attention weights provide helpful\nexplanations. Moreover, little research has explored how visualizing attention\naffects its usefulness as an explanation aid. To bridge this gap, we conducted\na user study to evaluate whether attention-based explanations support users in\nbiomedical document classification and whether there is a preferred way to\nvisualize them. The study involved medical experts from various disciplines who\nclassified articles based on study design (e.g., systematic reviews, broad\nsynthesis, randomized and non-randomized trials). Our findings show that the\nTransformer model (XLNet) classified documents accurately; however, the\nattention weights were not perceived as particularly helpful for explaining the\npredictions. However, this perception varied significantly depending on how\nattention was visualized. Contrary to Munzner's principle of visual\neffectiveness, which favors precise encodings like bar length, users preferred\nmore intuitive formats, such as text brightness or background color. While our\nresults do not confirm the overall utility of attention weights for\nexplanation, they suggest that their perceived helpfulness is influenced by how\nthey are visually presented.", "AI": {"tldr": "尽管Transformer中的注意力机制能提高预测准确率，但注意力权重在医学文献分类中的解释性并不被认为特别有用。然而，注意力的可视化方式显著影响了其被感知的有用性，人们更偏好直观的格式如文字亮度或背景色，而非精确编码如条形长度。", "motivation": "在基于证据的医学中，通过注意力权重提高AI系统的解释性对医生理解分类是有帮助的。研究尚无共识注意力权重是否提供有用解释，也没有研究可视化注意力的影响。研究的目的就是评估注意力解释的影响及可视化偏好。", "method": "进行了用户研究，包括来自不同学科的医学专家分类基于研究设计的文章。", "result": "结果发现XLNet模型能准确分类文档，但注意力权重作为解释未被认为特别有用，可视化方式影响了其被感知的有用性。", "conclusion": "尽管注意力权重的总体解释性未确认，但结果表明其被感知的有用性受可视化的显著影响。"}}
{"id": "2508.10268", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10268", "abs": "https://arxiv.org/abs/2508.10268", "authors": ["Yujie Zhao", "Jiabei Zeng", "Shiguang Shan"], "title": "Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones", "comment": "Accepted for British Machine Vision Conference (BMVC) 2025", "summary": "Although appearance-based point-of-gaze (PoG) estimation has improved, the\nestimators still struggle to generalize across individuals due to personal\ndifferences. Therefore, person-specific calibration is required for accurate\nPoG estimation. However, calibrated PoG estimators are often sensitive to head\npose variations. To address this, we investigate the key factors influencing\ncalibrated estimators and explore pose-robust calibration strategies.\nSpecifically, we first construct a benchmark, MobilePoG, which includes facial\nimages from 32 individuals focusing on designated points under either fixed or\ncontinuously changing head poses. Using this benchmark, we systematically\nanalyze how the diversity of calibration points and head poses influences\nestimation accuracy. Our experiments show that introducing a wider range of\nhead poses during calibration improves the estimator's ability to handle pose\nvariation. Building on this insight, we propose a dynamic calibration strategy\nin which users fixate on calibration points while moving their phones. This\nstrategy naturally introduces head pose variation during a user-friendly and\nefficient calibration process, ultimately producing a better calibrated PoG\nestimator that is less sensitive to head pose variations than those using\nconventional calibration strategies. Codes and datasets are available at our\nproject page.", "AI": {"tldr": "This paper investigates the factors affecting calibrated point-of-gaze estimators and develops a strategy for pose-robust calibration. A systematic analysis of a custom benchmark, MobilePoG, which includes images from 32 individuals under various head poses, was conducted to inform the development of a new dynamic calibration approach that is more robust to head pose variations.", "motivation": "The motivation behind this study is to address the issues with current appearance-based point-of-gaze (PoG) estimators, which struggle to generalize across different individuals and are sensitive to variations in head pose. The study aims to explore key factors influencing calibrated PoG estimators and to develop more robust calibration strategies that can handle these variations.", "method": "The method involves constructing a benchmark called MobilePoG, which includes facial images taken from 32 different individuals focusing on specific points under either fixed or continuously changing head poses. The authors systematically analyzed the benchmark to understand the impact of diversity in calibration points and head poses on estimation accuracy. Based on their findings, they proposed a dynamic calibration strategy where users perform calibration by moving their phones while fixing their gaze on calibration points, naturally introducing head pose variations into the calibration process.", "result": "The analysis showed that introducing a broader range of head poses during calibration significantly improves the estimator's performance. The authors proposed a dynamic calibration strategy where users perform calibration by moving their phones while looking at calibration points, which increases head pose variation and leads to better calibrated PoG estimators that are less affected by head pose changes.", "conclusion": "The study concludes that a dynamic calibration strategy where users move their phones during calibration can lead to PoG estimators that are more robust to head pose variations, addressing key issues with current calibration methods. The proposed method is user-friendly, efficient and improves the overall accuracy of PoG estimators compared to conventional calibration techniques."}}
{"id": "2508.10005", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10005", "abs": "https://arxiv.org/abs/2508.10005", "authors": ["Chengliang Zhou", "Mei Wang", "Ting Zhang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "title": "From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nmathematical problem-solving. However, the transition from providing answers to\ngenerating high-quality educational questions presents significant challenges\nthat remain underexplored. To advance Educational Question Generation (EQG) and\nfacilitate LLMs in generating pedagogically valuable and educationally\neffective questions, we introduce EQGBench, a comprehensive benchmark\nspecifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench\nestablishes a five-dimensional evaluation framework supported by a dataset of\n900 evaluation samples spanning three fundamental middle school disciplines:\nmathematics, physics, and chemistry. The dataset incorporates user queries with\nvarying knowledge points, difficulty gradients, and question type\nspecifications to simulate realistic educational scenarios. Through systematic\nevaluation of 46 mainstream large models, we reveal significant room for\ndevelopment in generating questions that reflect educational value and foster\nstudents' comprehensive abilities.", "AI": {"tldr": "介绍了EQGBench，一个专门用于评估LLMs在中文教育问题生成方面表现的全面基准测试。该基准测试涵盖了初中三个学科：数学、物理和化学，包含900个评估样本，系统评估了46个主流大型模型，发现这些模型在生成反映教育价值和培养学生综合能力的问题上还有很大提升空间。", "motivation": "提升教育问题生成的质量，使大型语言模型能够生成具有教育价值的问题。", "method": "创建了一个包含900个评估样本的数据集，覆盖数学、物理和化学三个学科，包含用户查询、知识点、难度梯度和问题类型，以此来模拟真实的教育场景。", "result": "系统评估了46个主流大型模型，发现这些模型在生成反映教育价值和培养综合能力的问题上还有很大的提升空间。", "conclusion": "EQGBench能有效评估大型语言模型在中文教育问题生成方面的能力，但模型在生成反映教育价值和培养学生综合能力的问题上还有很大的改进空间。"}}
{"id": "2508.10280", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10280", "abs": "https://arxiv.org/abs/2508.10280", "authors": ["Danyi Gao"], "title": "High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance", "comment": null, "summary": "This paper addresses the performance bottlenecks of existing text-driven\nimage generation methods in terms of semantic alignment accuracy and structural\nconsistency. A high-fidelity image generation method is proposed by integrating\ntext-image contrastive constraints with structural guidance mechanisms. The\napproach introduces a contrastive learning module that builds strong\ncross-modal alignment constraints to improve semantic matching between text and\nimage. At the same time, structural priors such as semantic layout maps or edge\nsketches are used to guide the generator in spatial-level structural modeling.\nThis enhances the layout completeness and detail fidelity of the generated\nimages. Within the overall framework, the model jointly optimizes contrastive\nloss, structural consistency loss, and semantic preservation loss. A\nmulti-objective supervision mechanism is adopted to improve the semantic\nconsistency and controllability of the generated content. Systematic\nexperiments are conducted on the COCO-2014 dataset. Sensitivity analyses are\nperformed on embedding dimensions, text length, and structural guidance\nstrength. Quantitative metrics confirm the superior performance of the proposed\nmethod in terms of CLIP Score, FID, and SSIM. The results show that the method\neffectively bridges the gap between semantic alignment and structural fidelity\nwithout increasing computational complexity. It demonstrates a strong ability\nto generate semantically clear and structurally complete images, offering a\nviable technical path for joint text-image modeling and image generation.", "AI": {"tldr": "本文提出了一种新的文本驱动图像生成方法，通过对比学习和结构引导增强了图像的语义匹配和结构保真度，并在实验中证实了其在多种性能指标上的优势。", "motivation": "论文旨在解决现有基于文本生成图像方法在语义对齐准确率和结构一致性方面的性能瓶颈。", "method": "提出了一个集成文本-图像对比约束和结构引导机制的图像生成方法，引入了对比学习模块，同时使用语义布局图或边缘草图作为结构先验，以提高空间层次的结构建模能力。通过多目标监督机制共同优化对比损失、结构一致性损失和语义保持损失。", "result": "该论文通过集成文本-图像对比约束和结构引导机制，提出了一种高保真的图像生成方法。引入了对比学习模块，增强了文本与图像之间的语义匹配。同时，使用语义布局图或边缘草图等结构先验知识，提高了空间层次的结构建模能力。该方法在整体框架中，通过多目标监督机制优化了对比损失、结构一致性损失和语义保持损失，实验结果证实，该方法在CLIP评分、FID和SSIM等定量指标上表现优异，成功地在不增加计算复杂度的前提下实现了语义对齐和结构保真的桥梁，展示了生成语义清晰、结构完整图像的强大能力。", "conclusion": "实验结果表明，该方法在多项性能指标上表现优异，能有效生成具有高语义清晰度和结构完整度的图像。这种方法为联合文本-图像建模和图像生成提供了一条可行的技术路径。"}}
{"id": "2508.10007", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.10007", "abs": "https://arxiv.org/abs/2508.10007", "authors": ["Y. Lyu", "D. Combs", "D. Neumann", "Y. C. Leong"], "title": "Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models", "comment": "We have no known conflict of interest", "summary": "Hostile attribution bias is the tendency to interpret social interactions as\nintentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)\nis commonly used to measure hostile attribution bias, and includes open-ended\nquestions where participants describe the perceived intentions behind a\nnegative social situation and how they would respond. While these questions\nprovide insights into the contents of hostile attributions, they require\ntime-intensive scoring by human raters. In this study, we assessed whether\nlarge language models can automate the scoring of AIHQ open-ended responses. We\nused a previously collected dataset in which individuals with traumatic brain\ninjury (TBI) and healthy controls (HC) completed the AIHQ and had their\nopen-ended responses rated by trained human raters. We used half of these\nresponses to fine-tune the two models on human-generated ratings, and tested\nthe fine-tuned models on the remaining half of AIHQ responses. Results showed\nthat model-generated ratings aligned with human ratings for both attributions\nof hostility and aggression responses, with fine-tuned models showing higher\nalignment. This alignment was consistent across ambiguous, intentional, and\naccidental scenario types, and replicated previous findings on group\ndifferences in attributions of hostility and aggression responses between TBI\nand HC groups. The fine-tuned models also generalized well to an independent\nnonclinical dataset. To support broader adoption, we provide an accessible\nscoring interface that includes both local and cloud-based options. Together,\nour findings suggest that large language models can streamline AIHQ scoring in\nboth research and clinical contexts, revealing their potential to facilitate\npsychological assessments across different populations.", "AI": {"tldr": "本研究评估了大型语言模型是否可通过微调来自动生成AIHQ开放式回答的评分，并表明这些模型可在各类情境下与人类评分者的一致性较高，从而为心理评估提供便捷工具。", "motivation": "本研究旨在评估大型语言模型是否可以用来自动生成AIHQ开放式回答的评分，以减少人类评分者的时间成本。", "method": "本研究使用了一个先前收集的数据集，其中包含创伤性脑损伤（TBI）患者和健康对照组（HC）完成的AIHQ问卷及其由训练有素的人类评分者评分的开放式回答。研究者用这些回答的一半对两个模型进行了微调，并在剩余的一半回答上测试了微调后的模型。", "result": "结果显示，对于敌意归因和攻击性反应的评分，模型生成的评分与人类评分者的一致性较高。这种一致性在模糊、故意和意外场景类型之间是一致的，同时也验证了TBI组和HC组之间归因敌意和攻击反应评分差别。微调的模型在独立的非临床数据集上也表现良好。", "conclusion": "研究结果表明，大型语言模型可以简化AIHQ评分，无论是在研究还是临床环境中，这为跨不同人群的心理评估提供了便利。"}}
{"id": "2508.10281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10281", "abs": "https://arxiv.org/abs/2508.10281", "authors": ["Ryota Tanaka", "Tomohiro Suzuki", "Keisuke Fujii"], "title": "VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation", "comment": null, "summary": "Understanding human actions from videos plays a critical role across various\ndomains, including sports analytics. In figure skating, accurately recognizing\nthe type and timing of jumps a skater performs is essential for objective\nperformance evaluation. However, this task typically requires expert-level\nknowledge due to the fine-grained and complex nature of jump procedures. While\nrecent approaches have attempted to automate this task using Temporal Action\nSegmentation (TAS), there are two major limitations to TAS for figure skating:\nthe annotated data is insufficient, and existing methods do not account for the\ninherent three-dimensional aspects and procedural structure of jump actions. In\nthis work, we propose a new TAS framework for figure skating jumps that\nexplicitly incorporates both the three-dimensional nature and the semantic\nprocedure of jump movements. First, we propose a novel View-Invariant, Figure\nSkating-Specific pose representation learning approach (VIFSS) that combines\ncontrastive learning as pre-training and action classification as fine-tuning.\nFor view-invariant contrastive pre-training, we construct FS-Jump3D, the first\npublicly available 3D pose dataset specialized for figure skating jumps.\nSecond, we introduce a fine-grained annotation scheme that marks the ``entry\n(preparation)'' and ``landing'' phases, enabling TAS models to learn the\nprocedural structure of jumps. Extensive experiments demonstrate the\neffectiveness of our framework. Our method achieves over 92% F1@50 on\nelement-level TAS, which requires recognizing both jump types and rotation\nlevels. Furthermore, we show that view-invariant contrastive pre-training is\nparticularly effective when fine-tuning data is limited, highlighting the\npracticality of our approach in real-world scenarios.", "AI": {"tldr": "We propose a novel TAS framework for figure skating jumps that addresses the limitations of existing methods by incorporating the 3D nature and semantic procedure of jump movements and achieving an F1@50 score over 92% on element-level TAS.", "motivation": "The motivation for our work arises from the limitations of existing Temporal Action Segmentation (TAS) methods for figure skating jumps, which face issues with insufficient annotated data and a failure to consider the 3D and procedural aspects of jump actions.", "method": "Our method consists of two key components: the View-Invariant, Figure Skating-Specific (VIFSS) pose representation learning approach, which uses contrastive learning as pre-training and action classification for fine-tuning; and a fine-grained annotation scheme that captures the procedural structure of jumps, specifically marking 'entry (preparation)' and 'landing' phases.", "result": "Our approach achieves an F1@50 score over 92% on element-level TAS, demonstrating its effectiveness. Moreover, view-invariant contrastive pre-training shows particular effectiveness under data scarcity, highlighting the practicality for real-world applications.", "conclusion": "This work presents a new TAS framework tailored for figure skating jumps, effectively addressing key challenges in jump timing and type recognition. The approach not only significantly improves performance but also shows practical potential in data-limited scenarios, thereby advancing the field of video action understanding in specialized domains."}}
{"id": "2508.10008", "categories": ["cs.CL", "cs.LG", "68", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.10008", "abs": "https://arxiv.org/abs/2508.10008", "authors": ["Antonio Leandro Martins Candido", "Jose Everardo Bessa Maia"], "title": "Multidimensional classification of posts for online course discussion forum curation", "comment": "8 pages, 1 figure", "summary": "The automatic curation of discussion forums in online courses requires\nconstant updates, making frequent retraining of Large Language Models (LLMs) a\nresource-intensive process. To circumvent the need for costly fine-tuning, this\npaper proposes and evaluates the use of Bayesian fusion. The approach combines\nthe multidimensional classification scores of a pre-trained generic LLM with\nthose of a classifier trained on local data. The performance comparison\ndemonstrated that the proposed fusion improves the results compared to each\nclassifier individually, and is competitive with the LLM fine-tuning approach", "AI": {"tldr": "本文提出了一种使用贝叶斯融合方法减少在线课程讨论论坛整理中大语言模型频繁再训练需求的技术，并展示了其优于单独使用分类器，并与微调方法性能相当。", "motivation": "在线课程的讨论论坛自动整理需要持续更新，这使得大语言模型的频繁再训练成为一个耗资源的过程。为避免昂贵的微调，本文提出并评估了贝叶斯融合方法的应用。", "method": "使用贝叶斯融合方法结合预训练通用语言模型和基于本地数据训练的分类器的多维分类分数，以减少在线课程讨论论坛自动整理过程中对大语言模型频繁再训练的需求。", "result": "实验结果表明，所提出的融合方法比单独使用每个分类器的性能更好，并且与大语言模型的微调方法具有竞争力。", "conclusion": "贝叶斯融合方法可以有效减少大语言模型在自动整理在线课程讨论论坛中的再训练成本，并能取得与微调方法相当的性能。"}}
{"id": "2508.10287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10287", "abs": "https://arxiv.org/abs/2508.10287", "authors": ["Simindokht Jahangard", "Mehrzad Mohammadi", "Yi Shen", "Zhixi Cai", "Hamid Rezatofighi"], "title": "JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) and large language models\n(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI\nagents like robots. However, existing visual reasoning benchmarks often suffer\nfrom several limitations: they lack a clear definition of reasoning complexity,\noffer have no control to generate questions over varying difficulty and task\ncustomization, and fail to provide structured, step-by-step reasoning\nannotations (workflows). To bridge these gaps, we formalize reasoning\ncomplexity, introduce an adaptive query engine that generates customizable\nquestions of varying complexity with detailed intermediate annotations, and\nextend the JRDB dataset with human-object interaction and geometric\nrelationship annotations to create JRDB-Reasoning, a benchmark tailored for\nvisual reasoning in human-crowded environments. Our engine and benchmark enable\nfine-grained evaluation of visual reasoning frameworks and dynamic assessment\nof visual-language models across reasoning levels.", "AI": {"tldr": "本文通过定义推理复杂性并构建自适应查询引擎解决了现有可视化推理基准测试的不足，并扩展了JRDB数据集，用于评估视觉推理在人类密集环境中的性能。", "motivation": "现有可视化推理基准存在定义推理复杂性不明确、无法生成不同难度的问题、缺乏结构化步骤注释等缺陷。", "method": "本文提出了一个自适应查询引擎，该引擎能够生成不同复杂度和任务定制化的可视化问题，并且提供了详细的中间注释。同时，扩展了JRDB数据集，增加了人-物交互和几何关系注释，构建了JRDB-Reasoning基准测试。", "result": "引入的自适应查询引擎和扩展的JRDB数据集可以帮助评估视觉推理框架，并动态评估可视-语言模型在不同推理水平上的表现。", "conclusion": "本文解决了现有可视推理基准的缺陷，提供了一个自适应查询引擎和特定基准测试，用于在人类密集的环境中评估视觉推理。"}}
{"id": "2508.10009", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10009", "abs": "https://arxiv.org/abs/2508.10009", "authors": ["Hojun Jin", "Eunsoo Hong", "Ziwon Hyung", "Sungjun Lim", "Seungjin Lee", "Keunseok Cho"], "title": "Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts", "comment": "Accepted to Interspeech 2025", "summary": "Hard-parameter sharing is a common strategy to train a single model jointly\nacross diverse tasks. However, this often leads to task interference, impeding\noverall model performance. To address the issue, we propose a simple yet\neffective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of\nExperts models, S-MoE eliminates the need for training gating functions by\nutilizing special guiding tokens to route each task to its designated expert.\nBy assigning each task to a separate feedforward network, S-MoE overcomes the\nlimitations of hard-parameter sharing. We further apply S-MoE to a\nspeech-to-text model, enabling the model to process mixed-bandwidth input while\njointly performing automatic speech recognition (ASR) and speech translation\n(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,\nachieving a 6.35% relative improvement in Word Error Rate (WER) when applied to\nboth the encoder and decoder.", "AI": {"tldr": "本文提出了一种新的模型，S-MoE，该模型能够解决共用参数训练时的各个任务之间相互干扰的问题，同时表明它在语音到文本的应用中能够改善性能。", "motivation": "解决传统参数共享策略下任务干扰问题，提高模型在多任务场景下的性能。", "method": "提出了一种名为监督专家混合模型（S-MoE）的方法，它通过使用特殊的引导标记将每个任务引导到其指定的专家，从而消除了传统专家混合模型中对门控函数训练的需求。", "result": "实验结果表明，S-MoE在应用于编码器和解码器时能够实现6.35%的相对字错率（WER）改进，证明了其有效性。", "conclusion": "通过实验结果证明S-MoE可以在处理多种带宽输入的同时，有效提高语音识别和语音翻译的性能。"}}
{"id": "2508.10294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10294", "abs": "https://arxiv.org/abs/2508.10294", "authors": ["Tao Huang", "Hongbo Pan", "Nanxi Zhou", "Shun Zhou"], "title": "A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method", "comment": null, "summary": "High-accuracy matching of multimodal optical images is the basis of geometric\nprocessing. However, the image matching accuracy is usually degraded by the\nnonlinear radiation and geometric deformation differences caused by different\nspectral responses. To address these problems, we proposed a phase consistency\nweighted least absolute deviation (PCWLAD) sub-pixel template matching method\nto improve the matching accuracy of multimodal optical images. This method\nconsists of two main steps: coarse matching with the structural similarity\nindex measure (SSIM) and fine matching with WLAD. In the coarse matching step,\nPCs are calculated without a noise filter to preserve the original structural\ndetails, and template matching is performed using the SSIM. In the fine\nmatching step, we applied the radiometric and geometric transformation models\nbetween two multimodal PC templates based on the coarse matching. Furthermore,\nmutual structure filtering is adopted in the model to mitigate the impact of\nnoise within the corresponding templates on the structural consistency, and the\nWLAD criterion is used to estimate the sub-pixel offset. To evaluate the\nperformance of PCWLAD, we created three types of image datasets: visible to\ninfrared Landsat images, visible to near-infrared close-range images, and\nvisible to infrared uncrewed aerial vehicle (UAV) images. PCWLAD outperformed\nexisting state-of-the-art eight methods in terms of correct matching rate (CMR)\nand root mean square error (RMSE) and reached an average matching accuracy of\napproximately 0.4 pixels across all three datasets. Our software and datasets\nare publicly available at https://github.com/huangtaocsu/PCWLAD.", "AI": {"tldr": "本文提出了一种新的子像素模板匹配方法PCWLAD，通过相位一致性加权最小绝对偏差来提高多模态光学图像的匹配精度，并验证了其优越性。", "motivation": "多模态光学图象匹配的精度容易受到非线性辐射和几何变形的影响而降低，此问题亟需解决。", "method": "提出的方法包含两个主要步骤：SSIM粗匹配和WLAD细匹配。粗匹配时使用无滤波器来计算PCs以便保持原始结构细节，细匹配时采用互结构滤波来减少噪声影响，并使用WLAD准则来估计亚像素偏移。", "result": "相比于八个现有的最先进的方法，PCWLAD在所有三种类型的数据集上具有更高的匹配正确率和更低的均方根误差（RMSE），达到了约0.4像素的平均匹配精度。", "conclusion": "PCWLAD能够有效提高多模态光学图像匹配的精度，并且软件及数据集已公开，可作为后续研究的基础。"}}
{"id": "2508.10010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10010", "abs": "https://arxiv.org/abs/2508.10010", "authors": ["Ayana Hussain", "Patrick Zhao", "Nicholas Vincent"], "title": "An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs", "comment": null, "summary": "Large Language Models (LLMs) are a double-edged sword capable of generating\nharmful misinformation -- inadvertently, or when prompted by \"jailbreak\"\nattacks that attempt to produce malicious outputs. LLMs could, with additional\nresearch, be used to detect and prevent the spread of misinformation. In this\npaper, we investigate the efficacy and characteristics of LLM-produced\njailbreak attacks that cause other models to produce harmful medical\nmisinformation. We also study how misinformation generated by jailbroken LLMs\ncompares to typical misinformation found on social media, and how effectively\nit can be detected using standard machine learning approaches. Specifically, we\nclosely examine 109 distinct attacks against three target LLMs and compare the\nattack prompts to in-the-wild health-related LLM queries. We also examine the\nresulting jailbreak responses, comparing the generated misinformation to\nhealth-related misinformation on Reddit. Our findings add more evidence that\nLLMs can be effectively used to detect misinformation from both other LLMs and\nfrom people, and support a body of work suggesting that with careful design,\nLLMs can contribute to a healthier overall information ecosystem.", "AI": {"tldr": "研究发现，通过精心设计，大型语言模型不仅可以用作检测并防止误信息传播的工具，而且可以有效识别来自其他模型甚至人的误信息。", "motivation": "鉴于大型语言模型能够生成有害的误信息，无论是无意生成还是被越狱攻击生成的恶意内容。研究希望通过额外的研究，用大型语言模型来检测并防止误信息的传播。", "method": "研究了大型语言模型（LLMs）在产生有害医疗误信息的越狱攻击方面的有效性及特征。调查了由越狱LLMs生成的误信息与社交媒体上的典型误信息的对比，以及使用标准机器学习方法检测的效率。具体分析了109种不同的攻击方法针对三个目标LLMs的情况。", "result": "发现LLMs可以有效地检测来自其他LLM以及人类的误信息。支持现有研究，表明经过精心设计，LLMs可能贡献于创建更健康的信息生态。", "conclusion": "研究提供了进一步证据，表明经过适当设计，大型语言模型可以在检测和减轻误信息传播中发挥正面作用，有助于构建更健康的信息生态。"}}
{"id": "2508.10297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10297", "abs": "https://arxiv.org/abs/2508.10297", "authors": ["Yiyi Ma", "Yuanzhi Liang", "Xiu Li", "Chi Zhang", "Xuelong Li"], "title": "InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild", "comment": "Accepted by ICCV2025", "summary": "We present Interleaved Learning for Motion Synthesis (InterSyn), a novel\nframework that targets the generation of realistic interaction motions by\nlearning from integrated motions that consider both solo and multi-person\ndynamics. Unlike previous methods that treat these components separately,\nInterSyn employs an interleaved learning strategy to capture the natural,\ndynamic interactions and nuanced coordination inherent in real-world scenarios.\nOur framework comprises two key modules: the Interleaved Interaction Synthesis\n(INS) module, which jointly models solo and interactive behaviors in a unified\nparadigm from a first-person perspective to support multiple character\ninteractions, and the Relative Coordination Refinement (REC) module, which\nrefines mutual dynamics and ensures synchronized motions among characters.\nExperimental results show that the motion sequences generated by InterSyn\nexhibit higher text-to-motion alignment and improved diversity compared with\nrecent methods, setting a new benchmark for robust and natural motion\nsynthesis. Additionally, our code will be open-sourced in the future to promote\nfurther research and development in this area.", "AI": {"tldr": "InterSyn框架通过交错学习策略，结合单人和多人动态，生成更真实的交互动作。实验结果表明，与近期方法相比，其生成的动作序列在文本到动作的对齐度和多样性方面表现出色，设定了新的基准。代码将在未来开源。", "motivation": "现有的方法通常分别处理单人和多人动态，导致生成的交互动作不够自然。因此，提出了InterSyn框架来解决这一问题。", "method": "该框架包含两个关键模块：交错交互合成模块(INS)和相对协调细化模块(REC)。INS模块统一建模单人和交互行为，而REC模块细化相互作用动态和确保角色之间的同步动作。", "result": "实验结果表明，InterSyn生成的动作序列在文本到动作的对齐度和多样性方面优于近期方法。", "conclusion": "InterSyn通过交错学习策略，能够生成更加自然且多样化的交互动作，并在此领域设定了一个新的基准。"}}
{"id": "2508.10011", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10011", "abs": "https://arxiv.org/abs/2508.10011", "authors": ["Yuta Nagamori", "Mikoto Kosai", "Yuji Kawai", "Haruka Marumo", "Misaki Shibuya", "Tatsuya Negishi", "Masaki Imanishi", "Yasumasa Ikeda", "Koichiro Tsuchiya", "Asuka Sawai", "Licht Miyamoto"], "title": "Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan", "comment": null, "summary": "Generative artificial intelligence (AI) based on large language models\n(LLMs), such as ChatGPT, has demonstrated remarkable progress across various\nprofessional fields, including medicine and education. However, their\nperformance in nutritional education, especially in Japanese national licensure\nexamination for registered dietitians, remains underexplored. This study aimed\nto evaluate the potential of current LLM-based generative AI models as study\naids for nutrition students. Questions from the Japanese national examination\nfor registered dietitians were used as prompts for ChatGPT and three Bing\nmodels (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question\nwas entered into independent sessions, and model responses were analyzed for\naccuracy, consistency, and response time. Additional prompt engineering,\nincluding role assignment, was tested to assess potential performance\nimprovements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the\npassing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did\nnot. Bing-Precise and Bing-Creative generally outperformed others across\nsubject fields except Nutrition Education, where all models underperformed.\nNone of the models consistently provided the same correct responses across\nrepeated attempts, highlighting limitations in answer stability. ChatGPT showed\ngreater consistency in response patterns but lower accuracy. Prompt engineering\nhad minimal effect, except for modest improvement when correct answers and\nexplanations were explicitly provided. While some generative AI models\nmarginally exceeded the passing threshold, overall accuracy and answer\nconsistency remained suboptimal. Moreover, all the models demonstrated notable\nlimitations in answer consistency and robustness. Further advancements are\nneeded to ensure reliable and stable AI-based study aids for dietitian\nlicensure preparation.", "AI": {"tldr": "研究对比了ChatGPT和三种Bing模型在应对日本注册营养师国家考试问题时的表现，结果显示Bing-Precise和Bing-Creative表现较好，但所有模型在一致性上仍有不足，未来需要进一步改进以达到可靠的自学辅助工具标准。", "motivation": "目的是评估基于大型语言模型的生成式AI在营养学学生学习中的潜力，特别是在准备日本注册营养师国家考试方面。", "method": "使用日本注册营养师国家考试的问题作为Prompt，对比ChatGPT和三种Bing模型的表现，考虑准确性、一致性和响应时间，并测试提示工程技术的影响。", "result": "Bing-Precise和Bing-Creative通过了60%的门槛，其他模型没有；但在营养教育领域，所有模型都表现不佳；没有模型在重复测试中一直提供相同正确答案。", "conclusion": "虽然某些生成式AI模型勉强超过了通过门槛，但整体准确性和答案一致性还不理想；未来需要更多改进以确保可靠稳定的AI自学辅助工具。"}}
{"id": "2508.10309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10309", "abs": "https://arxiv.org/abs/2508.10309", "authors": ["Wenjie Zhao", "Jia Li", "Yunhui Guo"], "title": "From Pixel to Mask: A Survey of Out-of-Distribution Segmentation", "comment": null, "summary": "Out-of-distribution (OoD) detection and segmentation have attracted growing\nattention as concerns about AI security rise. Conventional OoD detection\nmethods identify the existence of OoD objects but lack spatial localization,\nlimiting their usefulness in downstream tasks. OoD segmentation addresses this\nlimitation by localizing anomalous objects at pixel-level granularity. This\ncapability is crucial for safety-critical applications such as autonomous\ndriving, where perception modules must not only detect but also precisely\nsegment OoD objects, enabling targeted control actions and enhancing overall\nsystem robustness. In this survey, we group current OoD segmentation approaches\ninto four categories: (i) test-time OoD segmentation, (ii) outlier exposure for\nsupervised training, (iii) reconstruction-based methods, (iv) and approaches\nthat leverage powerful models. We systematically review recent advances in OoD\nsegmentation for autonomous-driving scenarios, identify emerging challenges,\nand discuss promising future research directions.", "AI": {"tldr": "本文系统地回顾了自动驾驶场景中OoD分割的最新进展，识别了新兴的挑战，并讨论了有前景的未来研究方向。", "motivation": "随着AI安全性的关注日益增加，出分布(OoD)检测和分割技术受到了越来越多的关注。传统的OoD检测方法只能识别存在OoD对象，但缺乏空间定位，这限制了它们在下游任务中的实用性。OoD分割方法通过以像素级的精度来进行异常对象的空间定位，这对于自动驾驶等安全关键应用至关重要。", "method": "该论文将当前的OoD分割方法分为四类：(i) 测试时OoD分割，(ii) 异常值暴露用于监督训练，(iii) 基于重建的方法，(iv) 利用强大的模型的方法。", "result": "系统性地回顾了OoD分割技术，并且对自动驾驶场景中的应用做出了详尽的分析。", "conclusion": "通过将OoD分割技术分为四个类别，即测试时OoD分割、异常值暴露、基于重建的方法以及利用强大模型的方法，该论文为了解自动驾驶场景下的OoD分割技术和面临的挑战提供了一个结构化的视角。"}}
{"id": "2508.10012", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10012", "abs": "https://arxiv.org/abs/2508.10012", "authors": ["Dehao Tao", "Guangjie Liu", "Weizheng", "Yongfeng Huang", "Minghu jiang"], "title": "Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs", "comment": null, "summary": "While Large Language Models (LLMs) exhibit strong linguistic capabilities,\ntheir reliance on static knowledge and opaque reasoning processes limits their\nperformance in knowledge intensive tasks. Knowledge graphs (KGs) offer a\npromising solution, but current exploration methods face a fundamental trade\noff: question guided approaches incur redundant exploration due to granularity\nmismatches, while clue guided methods fail to effectively leverage contextual\ninformation for complex scenarios. To address these limitations, we propose\nGuidance Graph guided Knowledge Exploration (GG Explore), a novel framework\nthat introduces an intermediate Guidance Graph to bridge unstructured queries\nand structured knowledge retrieval. The Guidance Graph defines the retrieval\nspace by abstracting the target knowledge' s structure while preserving broader\nsemantic context, enabling precise and efficient exploration. Building upon the\nGuidance Graph, we develop: (1) Structural Alignment that filters incompatible\ncandidates without LLM overhead, and (2) Context Aware Pruning that enforces\nsemantic consistency with graph constraints. Extensive experiments show our\nmethod achieves superior efficiency and outperforms SOTA, especially on complex\ntasks, while maintaining strong performance with smaller LLMs, demonstrating\npractical value.", "AI": {"tldr": "本文提出了一种新型的知识探索框架，通过引入一个中间指导图来优化知识图谱的检索过程，实现了更高的效率和更优的性能。", "motivation": "大型语言模型（LLMs）因其静态知识依赖和不易理解的推理过程，在知识密集型任务中的表现受限。知识图谱（KGs）虽然是一个有前景的解决方案，但当前的探索方法在问题导向和线索导向方面存在根本的权衡问题。", "method": "本研究提出了Guidance Graph指导的知识探索（GG Explore）框架，引入了一个中间的指导图来连接非结构化查询和结构化知识检索。指导图通过抽象目标知识的结构并保持更广泛的语义上下文，实现了精准和高效的探索。基于指导图，研究开发了以下方法：(1) 结构对齐，用于在不增加大型语言模型开销的情况下过滤不兼容的候选；(2) 上下文感知剪枝，用于通过图约束强制语义一致性。", "result": "实验结果表明，该方法在效率上表现出色，并超越了现有的最佳实践，尤其是在复杂任务上，同时也保持了在使用较小LLMs时的强劲性能，展示了其实用价值。", "conclusion": "研究展示了GG Explore框架在知识密集型任务中的优势，尤其是在利用知识图谱辅助大型语言模型时。该框架能够通过更精准的探索方式提高系统性能，尤其是对于复杂查询场景。"}}
{"id": "2508.10316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10316", "abs": "https://arxiv.org/abs/2508.10316", "authors": ["Yuanzhi Liang", "Yijie Fang", "Rui Li", "Ziqi Ni", "Ruijie Su", "Chi Zhang", "Xuelong Li"], "title": "Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances", "comment": "Ongoing work", "summary": "Generative models have made significant progress in synthesizing visual\ncontent, including images, videos, and 3D/4D structures. However, they are\ntypically trained with surrogate objectives such as likelihood or\nreconstruction loss, which often misalign with perceptual quality, semantic\naccuracy, or physical realism. Reinforcement learning (RL) offers a principled\nframework for optimizing non-differentiable, preference-driven, and temporally\nstructured objectives. Recent advances demonstrate its effectiveness in\nenhancing controllability, consistency, and human alignment across generative\ntasks. This survey provides a systematic overview of RL-based methods for\nvisual content generation. We review the evolution of RL from classical control\nto its role as a general-purpose optimization tool, and examine its integration\ninto image, video, and 3D/4D generation. Across these domains, RL serves not\nonly as a fine-tuning mechanism but also as a structural component for aligning\ngeneration with complex, high-level goals. We conclude with open challenges and\nfuture research directions at the intersection of RL and generative modeling.", "AI": {"tldr": "论文概述了基于强化学习（RL）的视觉内容生成方法的系统性综述，强调RL作为微调机制和结构组件，在连接生成任务与复杂目标中的作用，并讨论了未来的挑战和方向。", "motivation": "论文动机在于强调当前生成模型虽然在视觉内容合成上取得了显著进步，但其训练的替代目标（如似然性和重构损失）与感知质量、语义准确性和物理真实感的对齐度不高，而强化学习提供了一种优化非微分、偏好驱动和时间结构目标的框架。", "method": "分析论文的方法部分涉及到将强化学习作为优化工具整合到生成模型中，该方法涵盖了图像、视频和3D/4D生成任务，不仅用于微调，还作为结构组件来对齐复杂的高级目标。", "result": "综述了RL从经典控制到成为通用优化工具的演变过程，以及其在图像、视频和3D/4D生成任务中的融合，展示了RL作为调优和结构部件在这一领域中如何运作。", "conclusion": "论文总结了RL在生成模型中的应用，提出了在该领域许多开放挑战和未来研究方向。"}}
{"id": "2508.10013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10013", "abs": "https://arxiv.org/abs/2508.10013", "authors": ["Linqing Chen", "Hanmeng Zhong", "Wentao Wu", "Weilei Wang"], "title": "Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis", "comment": null, "summary": "Large language model (LLM) training faces a critical bottleneck: the scarcity\nof high-quality, reasoning-intensive question-answer pairs, especially from\nsparse, domain-specific sources like PubMed papers or legal documents. Existing\nmethods rely on surface patterns, fundamentally failing to generate\ncontrollable, complex multi-hop reasoning questions that test genuine\nunderstanding-essential for advancing LLM training paradigms. We present\n\\textbf{Semantic Bridge}, the first universal framework for controllably\ngenerating sophisticated multi-hop reasoning questions from arbitrary sources.\nOur breakthrough innovation is \\textit{semantic graph weaving}-three\ncomplementary bridging mechanisms (entity bridging for role-varying shared\nentities, predicate chain bridging for temporal/causal/logical sequences, and\ncausal bridging for explicit reasoning chains)-that systematically construct\ncomplex pathways across documents, with fine-grained control over complexity\nand types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to\n9.5% better round-trip quality, enabling production-ready controllable QA\ngeneration. Extensive evaluation demonstrates performance across both\ngeneral-purpose datasets (Wikipedia) and specialized domains (biomedicine) It\nyields consistent 18.3%-25.4% gains over baselines across four languages\n(English, Chinese, French, German). Question pairs generated from 200 sources\noutperform 600 native human annotation examples with 67% fewer materials. Human\nevaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%\nimproved pattern coverage. Semantic Bridge establishes a new paradigm for LLM\ntraining data synthesis, enabling controllable generation of targeted reasoning\nquestions from sparse sources. We will release our core code and semantic\nbridge model.", "AI": {"tldr": "Introducing Semantic Bridge for generating complex reasoning questions for training Language Models with better quality and efficiency.", "motivation": "The scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources, is a critical bottleneck in the training of large language models.", "method": "Semantic Bridge, a universal framework for generating complex multi-hop reasoning questions, using a technique called semantic graph weaving, which involves three bridging mechanisms: entity bridging, predicate chain bridging, and causal bridging.", "result": "Achieved up to 9.5% better round-trip quality in AMR pipeline, yielding 18.3%-25.4% gains over baselines across four languages. Question pairs created from 200 sources outperform native human annotation examples with 67% fewer materials, demonstrating 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage.", "conclusion": "Semantic Bridge sets a new paradigm for the synthesis of large language model training data, enabling the controllable generation of targeted reasoning questions from sparse sources."}}
{"id": "2508.10339", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10339", "abs": "https://arxiv.org/abs/2508.10339", "authors": ["Andrew Bai", "Justin Cui", "Ruochen Wang", "Cho-Jui Hsieh"], "title": "Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models", "comment": "11 pages, 1 figure", "summary": "Vision-language instruction tuning achieves two main purposes: learning\nvisual concepts and learning visual skills. In this paper, we found that\nvision-language benchmarks fall into the dichotomy of mainly benefiting from\ntraining on instructions with similar skills or visual concepts. Inspired by\nthe discovery, we designed a simple targeted training data selection method to\noptimize the performance of a given benchmark. We first extract the\nconcepts/skills from the benchmark, determine whether the benchmark\npredominantly benefits from similar concepts or skills, and finally select\ninstructions with the most matching concepts/skills. Experiments on 10+\nbenchmarks validate the effectiveness of our targeted data selection method,\nshowing +0.9\\% over the best existing baseline averaged over all benchmarks and\n+1.5\\% on the skill-focused subset. Our findings underscore the importance of\nrecognizing the inherent trade-off within instruction selection, which requires\nbalancing the acquisition of conceptual knowledge against visual skill.", "AI": {"tldr": "本篇论文提出了一种有针对性的数据选择方法，通过分析基准测试中的概念和技能，以优化视觉-语言模型的表现。实验结果显示，该方法在多个基准测试上表现优于现有的最佳基线方案。", "motivation": "研究动机在于发现视觉-语言基准测试主要受益于相似技能或视觉概念的训练，从而提出了一种优化性能的方法。", "method": "我们的方法包括从基准测试中提取概念/技能，确定基准测试主要受益于相似的概念还是技能，然后选择匹配的概念/技能的指令。这是一种简单的有针对性的训练数据选择方法，旨在优化特定基准测试的表现。", "result": "实验结果验证了所提出的有针对性的数据选择方法在10个以上基准测试中的有效性，平均比现有最佳基线方案提高了0.9%，在技能聚焦的子集上提高了1.5%。", "conclusion": "论文结论指出，教学指令选择中存在概念知识掌握与视觉技能之间的内在权衡，强调了认识这一权衡的重要性。"}}
{"id": "2508.10014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10014", "abs": "https://arxiv.org/abs/2508.10014", "authors": ["Lingfeng Zhou", "Jialing Zhang", "Jin Gao", "Mohan Jiang", "Dequan Wang"], "title": "PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?", "comment": "Accepted by COLM 2025", "summary": "Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,\nwhich may fail to reflect how humans perceive role fidelity. A key prerequisite\nfor human-aligned evaluation is role identification, the ability to recognize\nwho is speaking based on dialogue context. We argue that any meaningful\njudgment of role-playing quality (how well a character is played) fundamentally\ndepends on first correctly attributing words and actions to the correct persona\n(who is speaking). We present PersonaEval, the first benchmark designed to test\nwhether LLM evaluators can reliably identify human roles. PersonaEval uses\nhuman-authored dialogues from novels, scripts, and video transcripts,\nchallenging models to determine the correct persona according to the\nconversation context. Our experiments, including a human study, show that even\nthe best-performing LLMs reach only around 69% accuracy, well below the level\nneeded for reliable evaluation. In contrast, human participants perform near\nceiling with 90.8% accuracy, highlighting that current LLM evaluators are still\nnot human enough to effectively judge role-play scenarios. To better understand\nthis gap, we examine training-time adaptation and test-time compute, suggesting\nthat reliable evaluation requires more than task-specific tuning, but depends\non strong, human-like reasoning abilities in LLM evaluators. We release our\nbenchmark at https://github.com/maple-zhou/PersonaEval.", "AI": {"tldr": "本研究提出了PersonaEval作为基准评估模型在角色辨识上的能力，通过对比，指出现有的LLMs在角色辨识上远远不及人类，强调了需要更多类似人类判断能力的重要性。", "motivation": "当前的角色扮演研究往往依赖未经验证的LLM作为评判者的方案，这可能无法真实反映人类对角色真实性的感知。我们主张，任何有意义的角色扮演质量评判（角色被扮演的好坏），从根本上取决于首先正确归因话语和行为到正确的角色（谁在讲话）。", "method": "我们提出了PersonaEval，这是一个用来测试LLM评估者是否能准确识别人类角色的基准测试。PersonaEval使用来自小说、剧本和视频脚本的人类编写对话，挑战模型根据对话上下文确定正确角色的能力。", "result": "我们的实验，包括一项人类研究，表明即使是最优秀的LLM只能达到大约69%的准确性，这远低于可靠的评估水平。相比之下，人类参与者的表现接近90.8%，凸显了现有的LLM评估者仍不具备有效地评判角色扮演情景所需的人类水平的判断能力。", "conclusion": "通过训练时间和测试时计算的考察，我们发现可靠的评估不仅需要特定任务的调优，还需要LLM评估者的强大、类似人类的推理能力。我们将在https://github.com/maple-zhou/PersonaEval发布我们的基准测试。"}}
{"id": "2508.10351", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10351", "abs": "https://arxiv.org/abs/2508.10351", "authors": ["Zhentai Zhang", "Danyi Weng", "Guibin Zhang", "Xiang Chen", "Kaixing Long", "Jian Geng", "Yanmeng Lu", "Lei Zhang", "Zhitao Zhou", "Lei Cao"], "title": "Glo-DMU: A Deep Morphometry Framework of Ultrastructural Characterization in Glomerular Electron Microscopic Images", "comment": "15 pages, 6 figures", "summary": "Complex and diverse ultrastructural features can indicate the type,\nprogression, and prognosis of kidney diseases. Recently, computational\npathology combined with deep learning methods has shown tremendous potential in\nadvancing automatic morphological analysis of glomerular ultrastructure.\nHowever, current research predominantly focuses on the recognition of\nindividual ultrastructure, which makes it challenging to meet practical\ndiagnostic needs. In this study, we propose the glomerular morphometry\nframework of ultrastructural characterization (Glo-DMU), which is grounded on\nthree deep models: the ultrastructure segmentation model, the glomerular\nfiltration barrier region classification model, and the electron-dense deposits\ndetection model. Following the conventional protocol of renal biopsy diagnosis,\nthis framework simultaneously quantifies the three most widely used\nultrastructural features: the thickness of glomerular basement membrane, the\ndegree of foot process effacement, and the location of electron-dense deposits.\nWe evaluated the 115 patients with 9 renal pathological types in real-world\ndiagnostic scenarios, demonstrating good consistency between automatic\nquantification results and morphological descriptions in the pathological\nreports. Glo-DMU possesses the characteristics of full automation, high\nprecision, and high throughput, quantifying multiple ultrastructural features\nsimultaneously, and providing an efficient tool for assisting renal\npathologists.", "AI": {"tldr": "研究提出了Glo-DMU框架，使用三个深度学习模型对肾小球超微结构进行自动量化分析，展示了良好的临床一致性，为肾生物活检提供了一种高效辅助工具。", "motivation": "现有的研究主要集中在超微结构的单独识别，这使得满足实际诊断需求具有挑战性。该研究旨在通过自动化的、高精度的和高通量的方法，同时量化多个超微结构特征，为肾病理学家提供有效的辅助工具。", "method": "研究提出了Glo-DMU框架，基于三个深度模型：超微结构分割模型，肾小球滤过屏障区域分类模型和电子致密沉积物检测模型，以量化肾小球超微结构的三个广泛使用的特征：肾小球基底膜厚度，足突融合程度和电子致密沉积物的位置。", "result": "在对9种肾病病理类型的115名患者进行了评估，展示了自动化量化结果与病理报告中的形态描述之间良好的一致性。", "conclusion": "研究表明，Glo-DMU框架在现实世界的诊断场景中能提供自动量化结果与病理报告中形态描述之间良好的一致性。"}}
{"id": "2508.10015", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10015", "abs": "https://arxiv.org/abs/2508.10015", "authors": ["Enzhi Wang", "Qicheng Li", "Shiwan Zhao", "Aobo Kong", "Jiaming Zhou", "Xi Yang", "Yequan Wang", "Yonghua Lin", "Yong Qin"], "title": "RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis", "comment": "9 pages", "summary": "In recent years, large language models (LLMs) have achieved remarkable\nadvancements in multimodal processing, including end-to-end speech-based\nlanguage models that enable natural interactions and perform specific tasks in\ntask-oriented dialogue (TOD) systems. However, existing TOD datasets are\npredominantly text-based, lacking real speech signals that are essential for\nevaluating the robustness of speech-based LLMs. Moreover, existing speech TOD\ndatasets are primarily English and lack critical aspects such as speech\ndisfluencies and speaker variations. To address these gaps, we introduce\nRealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal\nTOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired\nspeech-text annotations. RealTalk-CN captures diverse dialogue scenarios with\nannotated spontaneous speech disfluencies, ensuring comprehensive coverage of\nreal-world complexities in speech dialogue. In addition, we propose a novel\ncross-modal chat task that authentically simulates real-world user\ninteractions, allowing dynamic switching between speech and text modalities.\nOur evaluation covers robustness to speech disfluencies, sensitivity to speaker\ncharacteristics, and cross-domain performance. Extensive experiments validate\nthe effectiveness of RealTalk-CN, establishing a strong foundation for Chinese\nspeech-based LLMs research.", "AI": {"tldr": "提出了RealTalk-CN数据集，包含5.4k个对话和150小时的中文语音文本，用于提高中文语音导向对话系统的性能。", "motivation": "现有的任务导向对话系统数据集主要基于文本，缺乏真实的语音信号；对语音的健壮性评估不足；语音数据集主要以英语为准，缺乏语音口吃和说话者变化等关键特征。", "method": "介绍了一个名为RealTalk-CN的新数据集，此数据集是第一个中文多轮、多领域语音-文本双模态任务导向对话数据集，包含5.4k个对话，60K个话语和150小时的语音文本配对注释，涵盖了真实世界的复杂语音对话情境，并提出了一种新的跨模态聊天任务来模拟真实的用户交互。", "result": "评估包括对语音不流畅的健壮性、对说话者特征的敏感度以及跨领域的性能表现。广泛的实验验证了RealTalk-CN的有效性，为中文语音导向的大型语言模型研究提供了坚实的基础。", "conclusion": "RealTalk-CN数据集展示的有效性使其成为中文语音导向大型语言模型研究的坚实基础。"}}
{"id": "2508.10356", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10356", "abs": "https://arxiv.org/abs/2508.10356", "authors": ["Hylke Westerdijk", "Ben Blankenborg", "Khondoker Ittehadul Islam"], "title": "Improving OCR for Historical Texts of Multiple Languages", "comment": null, "summary": "This paper presents our methodology and findings from three tasks across\nOptical Character Recognition (OCR) and Document Layout Analysis using advanced\ndeep learning techniques. First, for the historical Hebrew fragments of the\nDead Sea Scrolls, we enhanced our dataset through extensive data augmentation\nand employed the Kraken and TrOCR models to improve character recognition. In\nour analysis of 16th to 18th-century meeting resolutions task, we utilized a\nConvolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for\nsemantic segmentation with a Bidirectional LSTM, incorporating confidence-based\npseudolabeling to refine our model. Finally, for modern English handwriting\nrecognition task, we applied a CRNN with a ResNet34 encoder, trained using the\nConnectionist Temporal Classification (CTC) loss function to effectively\ncapture sequential dependencies. This report offers valuable insights and\nsuggests potential directions for future research.", "AI": {"tldr": "该研究采用了先进的深度学习技术，针对历史文献的光学字符识别（OCR）和文档布局分析进行了研究，具体包括希伯来古文献、16-18世纪会议决议和现代英文手写文档三个任务。在这三个任务中，研究采用了不同的深度学习模型和技术进行分析，包括数据增强、Kraken和TrOCR模型、CRNN结合DeepLabV3+及双向LSTM、以及CTC损失函数等技术。", "motivation": "研究旨在提高历史文本的OCR和文档布局分析的准确性，推动该领域的技术进步。选择这三个任务是为了验证所提出方法在不同文本和历史背景下的有效性。", "method": "研究方法包括数据增强技术的应用，使用Kraken和TrOCR模型处理希伯来古文献，CRNN结合DeepLabV3+和双向LSTM的模型处理16-18世纪会议决议，以及使用带有ResNet34编码器的CRNN和CTC损失函数来处理现代英文手写文档。", "result": "研究成功提高了一系列文档和文本的OCR识别率和文档布局分析的准确性，并提供了对每个任务的深度学习模型和技术应用的效果分析。", "conclusion": "研究提出的方法显著提升了文献OCR和布局分析的准确性，为未来的研究提供了新的视角和技术路径。"}}
{"id": "2508.10016", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10016", "abs": "https://arxiv.org/abs/2508.10016", "authors": ["Tianyu Xie", "Yuhang Wu", "Yongdong Luo", "Jiayi Ji", "Xiawu Zheng"], "title": "Training-Free Multimodal Large Language Model Orchestration", "comment": null, "summary": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into\na unified multimodal input-output system directly. In previous work, training\nhas been considered as an inevitable component due to challenges in modal\nalignment, Text-to-Speech efficiency and other integration issues. In this\npaper, we introduce Multimodal Large Language Model Orchestration, an effective\napproach for creating interactive multimodal AI systems without additional\ntraining. MLLM Orchestration leverages the inherent reasoning capabilities of\nlarge language models to coordinate specialized models through explicit\nworkflows, enabling natural multimodal interactions while maintaining\nmodularity, improving interpretability, and significantly enhancing\ncomputational efficiency. Our orchestration framework is built upon three key\ninnovations: (1) a central controller LLM that analyzes user inputs and\ndynamically routes tasks to appropriate specialized models through carefully\ndesigned agents; (2) a parallel Text-to-Speech architecture that enables true\nfull-duplex interaction with seamless interruption handling and natural\nconversational flow; and (3) a cross-modal memory integration system that\nmaintains coherent context across modalities through intelligent information\nsynthesis and retrieval, selectively avoiding unnecessary modality calls in\ncertain scenarios to improve response speed. Extensive evaluations demonstrate\nthat MLLM Orchestration achieves comprehensive multimodal capabilities without\nadditional training, performance improvements of up to 7.8% over traditional\njointly-trained approaches on standard benchmarks, reduced latency by 10.3%,\nand significantly enhanced interpretability through explicit orchestration\nprocesses.", "AI": {"tldr": "本文提出多模态大语言模型编排（MLLM Orchestration）的方法，通过大型语言模型协调专业化模型实现无需额外训练的交互式多模态AI系统。", "motivation": "现有的不同多模态大语言模型无法直接集成进统一的多模态输入输出系统中。通常训练被认为是不可避免的一个环节，因为需要解决模态对齐、文字转语音效率和其他集成挑战。该论文旨在提出不需要额外训练即可实现多模态AI系统的方法。", "method": "通过多模态大语言模型编排（MLLM Orchestration）来创建交互式多模态AI系统，无需额外训练。该方法利用大型语言模型的内在推理能力，通过显式工作流协调专业化模型，从而实现自然的多模态交互，同时保持模块化，提高可解释性，并显著增强计算效率。", "result": "MLLM Orchestration 达到了全面的多模态能力，无需额外训练。相比于传统的联合训练方法，在标准基准测试上的性能提高了7.8%，延迟减少10.3%，并且通过显式编排过程显著增强了可解释性。", "conclusion": "通过实验评估证明，多模态大语言模型编排（MLLM Orchestration）方法能够在无需额外训练的情况下，实现多模态AI系统的创建，同时带来性能的提升和延迟的减少，以及显著的解释性提高。"}}
{"id": "2508.10359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10359", "abs": "https://arxiv.org/abs/2508.10359", "authors": ["Hao Wang", "Hongkui Zheng", "Kai He", "Abolfazl Razi"], "title": "AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging", "comment": null, "summary": "Scanning transmission electron microscopy (STEM) plays a critical role in\nmodern materials science, enabling direct imaging of atomic structures and\ntheir evolution under external interferences. However, interpreting\ntime-resolved STEM data remains challenging due to two entangled degradation\neffects: spatial drift caused by mechanical and thermal instabilities, and\nbeam-induced signal loss resulting from radiation damage. These factors distort\nboth geometry and intensity in complex, temporally correlated ways, making it\ndifficult for existing methods to explicitly separate their effects or model\nmaterial dynamics at atomic resolution. In this work, we present AtomDiffuser,\na time-aware degradation modeling framework that disentangles sample drift and\nradiometric attenuation by predicting an affine transformation and a spatially\nvarying decay map between any two STEM frames. Unlike traditional denoising or\nregistration pipelines, our method leverages degradation as a physically\nheuristic, temporally conditioned process, enabling interpretable structural\nevolutions across time. Trained on synthetic degradation processes,\nAtomDiffuser also generalizes well to real-world cryo-STEM data. It further\nsupports high-resolution degradation inference and drift alignment, offering\ntools for visualizing and quantifying degradation patterns that correlate with\nradiation-induced atomic instabilities.", "AI": {"tldr": "本文介绍了一种新框架AtomDiffuser，通过时间感知退化模型分离STEM数据中的空间漂移和辐射度量衰减，适用于冷冻STEM数据，有利于材料动态的分析。", "motivation": "在现代材料科学中，扫描透射电子显微镜(STEM)对于直接成像原子结构及其在外干扰下的演变具有关键作用。然而，由于空间漂移和辐射损伤引起的时间分辨STEM数据的解释仍然具有挑战性。这些因素以复杂且时间上相互关联的方式扭曲了几何和强度，使得现有方法很难显式分离它们的影响，或者在原子分辨率下建模材料动态。", "method": "提出了一种名为AtomDiffuser的时间感知退化建模框架，该框架通过预测任意两个STEM帧之间的仿射变换和空间变化衰减图来解耦样品漂移和辐射度量衰减。", "result": "该方法利用退化作为一种物理上启发式且时间调节的过程，使得跨时间的可解释结构演变成为可能。训练在合成退化过程中，AtomDiffuser也能够很好地泛化到现实世界的冷冻STEM数据。它还支持高分辨率退化推断和漂移对齐，提供工具来可视化和量化与辐射诱导的原子不稳定性相关联的退化模式。", "conclusion": "AtomDiffuser提供了一种新的方式来处理和解释复杂的时间分辨STEM数据，通过直观地建模退化过程，可以帮助研究人员更好地理解和量化材料动态。"}}
{"id": "2508.10018", "categories": ["cs.CL", "cs.AI", "math.AT"], "pdf": "https://arxiv.org/pdf/2508.10018", "abs": "https://arxiv.org/abs/2508.10018", "authors": ["Sridhar Mahadevan"], "title": "A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models", "comment": "26 pages. arXiv admin note: text overlap with arXiv:2402.18732", "summary": "Natural language is replete with superficially different statements, such as\n``Charles Darwin wrote\" and ``Charles Darwin is the author of\", which carry the\nsame meaning. Large language models (LLMs) should generate the same next-token\nprobabilities in such cases, but usually do not. Empirical workarounds have\nbeen explored, such as using k-NN estimates of sentence similarity to produce\nsmoothed estimates. In this paper, we tackle this problem more abstractly,\nintroducing a categorical homotopy framework for LLMs. We introduce an LLM\nMarkov category to represent probability distributions in language generated by\nan LLM, where the probability of a sentence, such as ``Charles Darwin wrote\" is\ndefined by an arrow in a Markov category. However, this approach runs into\ndifficulties as language is full of equivalent rephrases, and each generates a\nnon-isomorphic arrow in the LLM Markov category. To address this fundamental\nproblem, we use categorical homotopy techniques to capture ``weak equivalences\"\nin an LLM Markov category. We present a detailed overview of application of\ncategorical homotopy to LLMs, from higher algebraic K-theory to model\ncategories, building on powerful theoretical results developed over the past\nhalf a century.", "AI": {"tldr": "本文提出了一个范畴同伦框架来解决大型语言模型在处理同义句时概率分布不一致的问题。", "motivation": "解决大型语言模型在面对意义相同但表述不同的句子时,不能生成相同下一个词的概率的问题", "method": "引入了LLM马尔可夫范畴来表示大型语言模型生成的语言的概率分布,提出使用范畴同伦技术来捕捉‘弱等价’", "result": "提出了一个处理大型语言模型同义句概率分布问题的抽象框架，并详细阐述了范畴同伦在大型语言模型中的应用", "conclusion": "通过范畴同伦技术，可以捕捉到大型语言模型中的弱等价关系，为解决同义句概率分布问题提供了一个新的视角"}}
{"id": "2508.10367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10367", "abs": "https://arxiv.org/abs/2508.10367", "authors": ["Pablo Hernández-Cámara", "Alexandra Gomez-Villa", "Jose Manuel Jaén-Lorites", "Jorge Vila-Tomás", "Jesus Malo", "Valero Laparra"], "title": "Contrast Sensitivity Function of Multimodal Vision-Language Models", "comment": null, "summary": "Assessing the alignment of multimodal vision-language models~(VLMs) with\nhuman perception is essential to understand how they perceive low-level visual\nfeatures. A key characteristic of human vision is the contrast sensitivity\nfunction (CSF), which describes sensitivity to spatial frequency at\nlow-contrasts. Here, we introduce a novel behavioral psychophysics-inspired\nmethod to estimate the CSF of chat-based VLMs by directly prompting them to\njudge pattern visibility at different contrasts for each frequency. This\nmethodology is closer to the real experiments in psychophysics than the\npreviously reported. Using band-pass filtered noise images and a diverse set of\nprompts, we assess model responses across multiple architectures. We find that\nwhile some models approximate human-like CSF shape or magnitude, none fully\nreplicate both. Notably, prompt phrasing has a large effect on the responses,\nraising concerns about prompt stability. Our results provide a new framework\nfor probing visual sensitivity in multimodal models and reveal key gaps between\ntheir visual representations and human perception.", "AI": {"tldr": "论文提出了一种新的方法，用于评估多模态视觉语言模型接近人类视觉对比度敏感性的程度，发现这些模型的反应与人类感知存在差异，提示表达方式对此有显著影响。", "motivation": "评估多模态视觉语言模型的低级别视觉特征与人类感知的对齐程度是至关重要的。本文提出的方法更加接近心理物理学中的真实实验。", "method": "使用带通滤波噪声图像和多样化的提示，研究多个架构的模型反应，通过直接提示模型在不同对比度下判断模式可见性来估计其CSF。", "result": "本文提出了一种新的基于行为心理物理学的方法来评估面向聊天的多模态视觉语言模型在不同对比度下的空间频率敏感度函数(CSF)，发现虽然某些模型能近似模拟人类CSF的形状或大小，但没有一个模型能完全复制。文章揭示了多模态模型的视觉表现与人类视觉感知之间的关键差异，并指出了提示语句对模型反应有显著影响，引发了对于提示稳定性的关注。", "conclusion": "揭示了在多模态视觉模型中探测视觉敏感性的新框架，并指出模型的视觉表现与人类感知之间存在关键差距，尤其是提示的表达方式对模型的反应有重大影响。"}}
{"id": "2508.10019", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10019", "abs": "https://arxiv.org/abs/2508.10019", "authors": ["Li Wang", "Changhao Zhang", "Zengqi Xiu", "Kai Lu", "Xin Yu", "Kui Zhang", "Wenjun Wu"], "title": "Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning", "comment": null, "summary": "Despite recent advances in the reasoning capabilities of Large Language\nModels (LLMs), improving the reasoning ability of Small Language Models (SLMs,\ne.g., $\\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity\nand variability of natural language: essentially equivalent problems often\nappear in diverse surface forms, often obscured by redundant or distracting\ndetails. This imposes a dual burden on SLMs: they must first extract the core\nproblem from complex linguistic input, and then perform reasoning based on that\nunderstanding. The resulting vast and noisy problem space hinders optimization,\nparticularly for models with limited capacity. To address this, we propose a\nnew framework that decouples understanding from reasoning by mapping natural\nlanguage problems into a canonical problem space-a semantically simplified yet\nexpressive domain. This enables SLMs to focus on reasoning over standardized\ninputs, free from linguistic variability. Within this framework, we introduce\nDURIT (Decoupled Understanding from Reasoning via Iterative Training), a\nthree-step algorithm that iteratively: (1) mapping natural language problems\nvia reinforcement learning, (2) aligns reasoning trajectories through\nself-distillation, and (3) trains reasoning policies in the problem space. The\nmapper and reasoner are co-trained in an alternating loop throughout this\nprocess. Experiments show that DURIT substantially improves SLMs' performance\non both in-domain and out-of-domain mathematical and logical reasoning tasks.\nBeyond improving reasoning capabilities, DURIT also improves the robustness of\nreasoning, validating decoupling understanding from reasoning as an effective\nstrategy for strengthening SLMs.", "AI": {"tldr": "为了解决自然语言问题表面形式多样化导致的理解与推理双重负担问题，提出了一种将理解和推理分离的方法，显著提高了小语言模型的推理能力。", "motivation": "虽然大规模语言模型的推理能力有所提升，但仍面临小规模语言模型推理能力提升挑战。尤其是在面对自然语言问题的复杂性和多样性时，小规模语言模型难以提取问题核心并进行准确推理。", "method": "本文提出了一种新的框架，该框架通过将自然语言问题映射到一个规范化的语义简化但表达丰富的领域（即规范问题空间）来解耦理解和推理，从而缓解小语言模型面临的理解与推理上的双重负担。在该框架下，提出了一种名为DURIT的三步骤迭代训练算法，分别通过强化学习映射自然语言问题、通过自蒸馏对齐推理轨迹以及在问题空间中训练推理策略。", "result": "实验表明，DURIT显著提高了小语言模型在数学和逻辑推理任务的领域内和领域外的表现，并增强了其推理的鲁棒性。", "conclusion": "该方法验证了解耦理解和推理作为提高小语言模型推理能力的有效策略。"}}
{"id": "2508.10382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10382", "abs": "https://arxiv.org/abs/2508.10382", "authors": ["Hyundo Lee", "Suhyung Choi", "Byoung-Tak Zhang", "Inwoo Hwang"], "title": "Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models", "comment": null, "summary": "Image generation models trained on large datasets can synthesize high-quality\nimages but often produce spatially inconsistent and distorted images due to\nlimited information about the underlying structures and spatial layouts. In\nthis work, we leverage intrinsic scene properties (e.g., depth, segmentation\nmaps) that provide rich information about the underlying scene, unlike prior\napproaches that solely rely on image-text pairs or use intrinsics as\nconditional inputs. Our approach aims to co-generate both images and their\ncorresponding intrinsics, enabling the model to implicitly capture the\nunderlying scene structure and generate more spatially consistent and realistic\nimages. Specifically, we first extract rich intrinsic scene properties from a\nlarge image dataset with pre-trained estimators, eliminating the need for\nadditional scene information or explicit 3D representations. We then aggregate\nvarious intrinsic scene properties into a single latent variable using an\nautoencoder. Building upon pre-trained large-scale Latent Diffusion Models\n(LDMs), our method simultaneously denoises the image and intrinsic domains by\ncarefully sharing mutual information so that the image and intrinsic reflect\neach other without degrading image quality. Experimental results demonstrate\nthat our method corrects spatial inconsistencies and produces a more natural\nlayout of scenes while maintaining the fidelity and textual alignment of the\nbase model (e.g., Stable Diffusion).", "AI": {"tldr": "我们提出了一种新方法，通过同时生成图像和其对应的内在场景特性来生成更一致和现实的图像。我们的方法在实验中展现出改善空间一致性的同时，保持了图像质量和文本一致性。", "motivation": "图像生成模型在大型数据集上训练时可以合成高质量的图像，但由于缺乏关于潜在结构和空间布局的信息，它们经常产生空间不一致和扭曲的图像。我们的动机是利用比先前方法更多的场景内在属性信息（如深度，分割图），超越仅依赖图像-文本对或使用内在属性作为条件输入的方法。", "method": "我们提出的方法旨在同时生成图像及其对应的内在场景特性，以隐式捕捉潜在的场景结构，并生成更空间一致和逼真的图像。首先，我们使用预训练的估计器从大型图像数据集中提取丰富的内在场景属性，消除了对额外场景信息或显式3D表示的需求。然后，我们使用自编码器将各种内在场景特性聚合到单一的潜在变量中。基于预训练的大规模潜在扩散模型（LDMs），我们的方法通过仔细共享相互信息，同时消除图像和内在领域的噪声，使得图像和内在特性相互反映而不降低图像质量。", "result": "实验结果表明，我们的方法能够纠正空间不一致性，并产生更自然的场景布局，同时保持基础模型（如Stable Diffusion）的保真度和文本一致性。", "conclusion": "通过提出的方法，我们成功生成了更空间一致和逼真的图像，消除了仅依赖图像-文本对或使用内在属性作为条件输入的方法所带来的空间不一致性问题。"}}
{"id": "2508.10020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10020", "abs": "https://arxiv.org/abs/2508.10020", "authors": ["Chuan Li", "Qianyi Zhao", "Fengran Mo", "Cen Chen"], "title": "FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models", "comment": null, "summary": "Efficiently enhancing the reasoning capabilities of large language models\n(LLMs) in federated learning environments remains challenging, particularly\nwhen balancing performance gains with strict computational, communication, and\nprivacy constraints. This challenge is especially acute in healthcare, where\ndecisions-spanning clinical, operational, and patient-facing contexts-demand\nnot only accurate outputs but also interpretable, traceable rationales to\nensure safety, accountability, and regulatory compliance. Conventional\nfederated tuning approaches on LLM fail to address this need: they optimize\nprimarily for answer correctness while neglecting rationale quality, leaving\nCoT capabilities dependent on models' innate pre-training abilities. Moreover,\nexisting methods for improving rationales typically rely on privacy-violating\nknowledge distillation from centralized models. Additionally, the communication\noverhead in traditional federated fine-tuning on LLMs remains substantial. We\naddresses this gap by proposing FedCoT, a novel framework specifically designed\nto enhance reasoning in federated settings. FedCoT leverages a lightweight\nchain-of-thought enhancement mechanism: local models generate multiple\nreasoning paths, and a compact discriminator dynamically selects the most\npromising one. This approach improves reasoning accuracy and robustness while\nproviding valuable interpretability, which is particularly critical for medical\napplications. To manage client heterogeneity efficiently, we adopt an improved\naggregation approach building upon advanced LoRA module stacking, incorporating\nclient classifier-awareness to achieve noise-free aggregation across diverse\nclients. Comprehensive experiments on medical reasoning tasks demonstrate that\nFedCoT significantly boosts client-side reasoning performance under stringent\nresource budgets while fully preserving data privacy.", "AI": {"tldr": "本文提出FedCoT框架，通过轻量级链式思考增强机制提升联邦学习环境下大语言模型的推理能力，同时保证数据隐私和解释性。", "motivation": "改进大语言模型在联邦学习环境中的推理能力，尤其是在医疗领域，需要确保结果的准确性和可解释性，同时满足性能增益与计算、通信和隐私约束之间的平衡。", "method": "FedCoT框架采用了轻量级的链式思考增强机制，本地模型生成多个推理路径，紧凑的判别器动态选择最有前景的一种。同时，通过改进的聚合方法来处理客户端之间的异质性。", "result": "实验表明，在严格的资源预算下，FedCoT显著提高了客户端推理性能，同时全面保护了数据隐私。", "conclusion": "FedCoT框架成功提升了联邦学习大语言模型的推理准确性和鲁棒性，同时提供有价值的解释性，特别是在医疗应用中。"}}
{"id": "2508.10383", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10383", "abs": "https://arxiv.org/abs/2508.10383", "authors": ["Yechan Kim", "Dongho Yoon", "Younkwan Lee", "Unse Fatima", "Hong Kook Kim", "Songjae Lee", "Sanga Park", "Jeong Ho Park", "Seonjong Kang", "Moongu Jeon"], "title": "Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise", "comment": null, "summary": "While previous studies on image segmentation focus on handling severe (or\nexplicit) label noise, real-world datasets also exhibit subtle (or implicit)\nlabel imperfections. These arise from inherent challenges, such as ambiguous\nobject boundaries and annotator variability. Although not explicitly present,\nsuch mild and latent noise can still impair model performance. Typical data\naugmentation methods, which apply identical transformations to the image and\nits label, risk amplifying these subtle imperfections and limiting the model's\ngeneralization capacity. In this paper, we introduce NSegment+, a novel\naugmentation framework that decouples image and label transformations to\naddress such realistic noise for semantic segmentation. By introducing\ncontrolled elastic deformations only to segmentation labels while preserving\nthe original images, our method encourages models to focus on learning robust\nrepresentations of object structures despite minor label inconsistencies.\nExtensive experiments demonstrate that NSegment+ consistently improves\nperformance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in\naverage on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even\nwithout bells and whistles, highlighting the importance of addressing implicit\nlabel noise. These gains can be further amplified when combined with other\ntraining tricks, including CutMix and Label Smoothing.", "AI": {"tldr": "NSegment+ is a new data augmentation technique for semantic segmentation that separately modifies segmentation labels with elastic deformations to tackle hidden label imperfections, thereby improving model robustness and performance.", "motivation": "The motivation behind this paper is to address subtle and implicit label imperfections that are not handled effectively by conventional augmentation techniques in real-world image segmentation tasks. These imperfections can degrade model performance and are typically caused by ambiguous object boundaries and annotator variability.", "method": "The paper introduces NSegment+, a new data augmentation framework for semantic segmentation tasks. NSegment+ decouples the transformation of images and labels, applying elastic deformations to labels while maintaining original images to tackle implicit label noise.", "result": "Experiments demonstrate that NSegment+ can improve performance by achieving mIoU gains of up to 2.29, 2.38, 1.75, and 3.39 on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC datasets, respectively. The technique's effectiveness is further enhanced when combined with other training techniques.", "conclusion": "NSegment+ effectively enhances the robustness of segmentation models by mitigating the impact of implicit label noise, leading to significant performance improvements across multiple datasets. This approach underscores the importance of addressing subtle label imperfections in real-world datasets."}}
{"id": "2508.10021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10021", "abs": "https://arxiv.org/abs/2508.10021", "authors": ["Egor Fadeev", "Dzhambulat Mollaev", "Aleksei Shestov", "Dima Korolev", "Omar Zoloev", "Ivan Kireev", "Andrey Savchenko", "Maksim Makarenko"], "title": "LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients", "comment": null, "summary": "Learning clients embeddings from sequences of their historic communications\nis central to financial applications. While large language models (LLMs) offer\ngeneral world knowledge, their direct use on long event sequences is\ncomputationally expensive and impractical in real-world pipelines. In this\npaper, we propose LATTE, a contrastive learning framework that aligns raw event\nembeddings with semantic embeddings from frozen LLMs. Behavioral features are\nsummarized into short prompts, embedded by the LLM, and used as supervision via\ncontrastive loss. The proposed approach significantly reduces inference cost\nand input size compared to conventional processing of complete sequence by LLM.\nWe experimentally show that our method outperforms state-of-the-art techniques\nfor learning event sequence representations on real-world financial datasets\nwhile remaining deployable in latency-sensitive environments.", "AI": {"tldr": "LATTE, a contrastive learning technique, effectively reduces computational cost while maintaining high performance in learning client embeddings from long event sequences in financial datasets.", "motivation": "To address the computational inefficiency and impracticality of using large language models directly on long event sequences for financial applications.", "method": "LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs, by summarizing behavioral features into short prompts embedded by the LLM and used as supervision via contrastive loss.", "result": "The proposed method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets.", "conclusion": "LATTE is effective and deployable in latency-sensitive environments, providing a computationally efficient alternative to processing complete sequences with LLMs."}}
{"id": "2508.10397", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10397", "abs": "https://arxiv.org/abs/2508.10397", "authors": ["Haibin Sun", "Xinghui Song"], "title": "PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection", "comment": "11 pages, 6 figures", "summary": "Driver distraction detection is essential for improving traffic safety and\nreducing road accidents. However, existing models often suffer from degraded\ngeneralization when deployed in real-world scenarios. This limitation primarily\narises from the few-shot learning challenge caused by the high cost of data\nannotation in practical environments, as well as the substantial domain shift\nbetween training datasets and target deployment conditions. To address these\nissues, we propose a Pose-driven Quality-controlled Data Augmentation Framework\n(PQ-DAF) that leverages a vision-language model for sample filtering to\ncost-effectively expand training data and enhance cross-domain robustness.\nSpecifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to\naccurately capture key driver pose features and synthesize diverse training\nexamples. A sample quality assessment module, built upon the CogVLM\nvision-language model, is then introduced to filter out low-quality synthetic\nsamples based on a confidence threshold, ensuring the reliability of the\naugmented dataset. Extensive experiments demonstrate that PQ-DAF substantially\nimproves performance in few-shot driver distraction detection, achieving\nsignificant gains in model generalization under data-scarce conditions.", "AI": {"tldr": "本文提出PQ-DAF模型解决驾驶员注意力分散检测中数据标注成本高和领域偏移问题，通过合成高质量数据和样本过滤提高检测性能。", "motivation": "为了解决驾驶员注意力分散检测模型在实际环境中数据标注成本高的难题，以及训练数据与目标部署条件之间存在显著领域偏移的问题。", "method": "提出了一种基于姿态驱动的质量控制数据增强框架 (PQ-DAF)，利用视觉语言模型进行样本过滤，以经济地扩展训练数据并增强跨域鲁棒性。具体来说，采用了渐进性条件扩散模型（PCDMs）来准确捕捉驾驶员姿势特征并合成多样化的训练样本。通过基于CogVLM视觉-语言模型构建样本质量评估模块，在置信度阈值基础上过滤出低质量的合成样本，确保增强数据集的有效性。", "result": "实验结果表明，PQ-DAF显著提高了驾驶员注意力分散检测在样本量较少情况下的性能，大幅提升了模型在数据稀缺条件下的泛化能力。", "conclusion": "PQ-DAF在数据稀缺条件下实现了显著的性能增益，证明了其在驾驶员注意力分散检测中的有效性和泛化能力。"}}
{"id": "2508.10022", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10022", "abs": "https://arxiv.org/abs/2508.10022", "authors": ["Yuanchang Ye"], "title": "Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control", "comment": null, "summary": "This study introduces a significance testing-enhanced conformal prediction\n(CP) framework to improve trustworthiness of large language models (LLMs) in\nmultiple-choice question answering (MCQA). While LLMs have been increasingly\ndeployed in disciplinary QA scenarios, hallucination and nonfactual generation\nsubstantially compromise response reliability. Although CP provides\nstatistically rigorous marginal coverage guarantees for prediction sets, and\nsignificance testing offers established statistical rigor, their synergistic\nintegration remains unexplored. To mitigate hallucination and factual\ninaccuracies, our framework integrates $p$-value computation with conformity\nscoring through self-consistency resampling of MCQA responses. This approach\ncalculates option frequencies to address LLMs' black-box nature, subsequently\nconstructing prediction sets via null hypothesis testing ($\\mathcal{H}_0$) with\nempirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks\nusing off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves\nuser-specified empirical miscoverage rates; (2) Test-set average prediction set\nsize (APSS) decreases monotonically with increasing risk levels ($\\alpha$),\nvalidating APSS as an effective uncertainty metric. This work establishes a\nprincipled statistical framework for trustworthy LLM deployment in high-stakes\nQA applications.", "AI": {"tldr": "The study introduces a significance testing-enhanced conformal prediction framework to improve the reliability of LLMs in multiple-choice question answering.", "motivation": "The motivation is to enhance the trustworthiness and factual accuracy of LLMs in MCQA scenarios by mitigating hallucination through a statistically rigorous approach.", "method": "This study integrates significance testing into the conformal prediction (CP) framework, specifically by using $p$-value computation with self-consistency resampling to address hallucination and factual inaccuracies in LLMs' multiple-choice question answering (MCQA).", "result": "On MMLU and MMLU-Pro benchmarks, the enhanced CP framework was able to achieve user-specified empirical miscoverage rates and shows a decreasing average prediction set size with increasing risk levels, indicating it is an effective uncertainty metric.", "conclusion": "The work presents a principled statistical framework to ensure the trustworthy deployment of LLMs in high-stakes QA scenarios, evidenced by its performance on standardized benchmarks."}}
{"id": "2508.10407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10407", "abs": "https://arxiv.org/abs/2508.10407", "authors": ["Eunseo Koh", "Seunghoo Hong", "Tae-Young Kim", "Simon S. Woo", "Jae-Pil Heo"], "title": "Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-Image (T2I) diffusion models have made significant progress in\ngenerating diverse high-quality images from textual prompts. However, these\nmodels still face challenges in suppressing content that is strongly entangled\nwith specific words. For example, when generating an image of ``Charlie\nChaplin\", a ``mustache\" consistently appears even if explicitly instructed not\nto include it, as the concept of ``mustache\" is strongly entangled with\n``Charlie Chaplin\". To address this issue, we propose a novel approach to\ndirectly suppress such entangled content within the text embedding space of\ndiffusion models. Our method introduces a delta vector that modifies the text\nembedding to weaken the influence of undesired content in the generated image,\nand we further demonstrate that this delta vector can be easily obtained\nthrough a zero-shot approach. Furthermore, we propose a Selective Suppression\nwith Delta Vector (SSDV) method to adapt delta vector into the cross-attention\nmechanism, enabling more effective suppression of unwanted content in regions\nwhere it would otherwise be generated. Additionally, we enabled more precise\nsuppression in personalized T2I models by optimizing delta vector, which\nprevious baselines were unable to achieve. Extensive experimental results\ndemonstrate that our approach significantly outperforms existing methods, both\nin terms of quantitative and qualitative metrics.", "AI": {"tldr": "The paper introduces a method to suppress undesired components in text-to-image generation through adjusting text embeddings using a delta vector approach.", "motivation": "To address the issue where text-to-image models generate strong entangled content (like a mustache appearing with Charlie Chaplin) even when instructed otherwise.", "method": "Introduces a delta vector to modify text embeddings for weakening the influence of undesired elements and a Selective Suppression with Delta Vector method to adapt this adjustment into cross-attention.", "result": "The proposed model achieves significant improvements over existing methods, as evidenced by both quantitative and qualitative evaluations.", "conclusion": "The approach effectively suppresses undesired content in generated images by modifying the text embedding space, increasing control over image generation."}}
{"id": "2508.10024", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.0"], "pdf": "https://arxiv.org/pdf/2508.10024", "abs": "https://arxiv.org/abs/2508.10024", "authors": ["J. Pablo Muñoz", "Jinjie Yuan"], "title": "RTTC: Reward-Guided Collaborative Test-Time Compute", "comment": null, "summary": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation.", "AI": {"tldr": "介绍了一种自适应选择最佳TTC策略的新框架RTTC，实现跨域和任务的高精度增强。", "motivation": "现有TTC策略的盲目应用导致了计算开销的显著增加，因为最优的适应策略会根据查询的变化而变化。", "method": "提出了一种名为RTTC的新框架，该框架通过预训练奖励模型自适应地选择每个查询的最佳TTC策略，以在不同的域和任务中最大化下游准确性。RTTC在分布式服务器-客户端架构中运行，仅在必要时从远程知识库中检索相关样本，并在客户端设备上应用RAG或轻量级微调。此外，提出了一种查询状态缓存技术，以减少冗余计算。", "result": "在多个LLM和基准测试中的广泛实验表明，RTTC在准确性方面始终优于传统的RAG或TTT方法。", "conclusion": "实验结果验证了适应性、奖励引导式的TTC选择的重要性，证明了RTTC在大规模高性能语言模型适应中的应用潜力。"}}
{"id": "2508.10411", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10411", "abs": "https://arxiv.org/abs/2508.10411", "authors": ["Chaesong Park", "Eunbin Seo", "Jihyeon Hwang", "Jongwoo Lim"], "title": "SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection", "comment": "10 pages, 4 figures, 5 tables", "summary": "In this paper, we introduce SC-Lane, a novel slope-aware and temporally\nconsistent heightmap estimation framework for 3D lane detection. Unlike\nprevious approaches that rely on fixed slope anchors, SC-Lane adaptively\ndetermines the fusion of slope-specific height features, improving robustness\nto diverse road geometries. To achieve this, we propose a Slope-Aware Adaptive\nFeature module that dynamically predicts the appropriate weights from image\ncues for integrating multi-slope representations into a unified heightmap.\nAdditionally, a Height Consistency Module enforces temporal coherence, ensuring\nstable and accurate height estimation across consecutive frames, which is\ncrucial for real-world driving scenarios. To evaluate the effectiveness of\nSC-Lane, we employ three standardized metrics-Mean Absolute Error(MAE), Root\nMean Squared Error (RMSE), and threshold-based accuracy-which, although common\nin surface and depth estimation, have been underutilized for road height\nassessment. Using the LiDAR-derived heightmap dataset introduced in prior work\n[20], we benchmark our method under these metrics, thereby establishing a\nrigorous standard for future comparisons. Extensive experiments on the OpenLane\nbenchmark demonstrate that SC-Lane significantly improves both height\nestimation and 3D lane detection, achieving state-of-the-art performance with\nan F-score of 64.3%, outperforming existing methods by a notable margin. For\ndetailed results and a demonstration video, please refer to our project\npage:https://parkchaesong.github.io/sclane/", "AI": {"tldr": "SC-Lane 提出了一种新型的基于斜率感知且在时间上一致的高度图估计框架，用来进行3D车道检测，通过自适应确定融合斜率特定的高度特征，该方法在多个标准度量下实现了最先进的性能，得分为64.3%的F值，超越了现有方法。", "motivation": "SC-Lane 解决了以前方法中依赖固定斜率锚点的问题，改善了对多样性道路几何形状的鲁棒性，并通过一个自适应特征模块动态预测图像线索的适当权重，整合多斜率表征，另外还使用了一致性模块确保时间连续帧之间的稳定准确的高度估计，这些对于实际驾驶场景至关重要。", "method": "SC-Lane 使用的Slope-Aware Adaptive Feature module能够自适应从图像信息中预测合适的权重来整合多斜率表示，而Height Consistency Module保证了不同帧之间的高度信息的一致性，除了基本方法，还在OpenLane基准测试上进行了性能测试，具体使用了三个标准指标：均值绝对误差(MAE)、均方根误差(RMSE)和基于阈值的准确性。", "result": "在OpenLane基准测试中，SC-Lane在高度估测和3D车道检测方面都表现出色，取得了64.3%的F值得分，超越了其他现有方法，显示了其显著的优越性。", "conclusion": "SC-Lane 通过引入自适应斜率感知和时间一致性，显著改进了3D车道检测，特别是在高度估测方面。这种新技术设立了未来研究和对比的高标准，其表现和效果可以在其项目页面中得到更详细的观察和分析。"}}
{"id": "2508.10025", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10025", "abs": "https://arxiv.org/abs/2508.10025", "authors": ["Silvia García-Méndez", "Francisco de Arriba-Pérez"], "title": "Detecting and explaining postpartum depression in real-time with generative artificial intelligence", "comment": null, "summary": "Among the many challenges mothers undergo after childbirth, postpartum\ndepression (PPD) is a severe condition that significantly impacts their mental\nand physical well-being. Consequently, the rapid detection of ppd and their\nassociated risk factors is critical for in-time assessment and intervention\nthrough specialized prevention procedures. Accordingly, this work addresses the\nneed to help practitioners make decisions with the latest technological\nadvancements to enable real-time screening and treatment recommendations.\nMainly, our work contributes to an intelligent PPD screening system that\ncombines Natural Language Processing, Machine Learning (ML), and Large Language\nModels (LLMs) towards an affordable, real-time, and non-invasive free speech\nanalysis. Moreover, it addresses the black box problem since the predictions\nare described to the end users thanks to the combination of LLMs with\ninterpretable ml models (i.e., tree-based algorithms) using feature importance\nand natural language. The results obtained are 90 % on ppd detection for all\nevaluation metrics, outperforming the competing solutions in the literature.\nUltimately, our solution contributes to the rapid detection of PPD and their\nassociated risk factors, critical for in-time and proper assessment and\nintervention.", "AI": {"tldr": "本文提出了一种结合NLP、ML和LLM的智能产后抑郁筛查系统，能实时高效地筛查产后抑郁及其风险因素，并提高了预测结果的可解释性；在产后抑郁检测上的准确率达到了90%，优于现有方法。", "motivation": "产后抑郁是一种严重影响女性心理健康和身体状况的疾病。快速检测产后抑郁及其相关风险因素对于及时的评估和干预至关重要，因此本研究提出了一种智能筛查系统。", "method": "研究中提到的方法为结合自然语言处理、机器学习（ML）和大型语言模型（LLM），进行实时、低成本、非侵入式的语音分析，并通过解释性的ML模型（如基于树的算法）与LLM结合，利用特征重要性和自然语言描述预测结果以解决黑箱问题。", "result": "该研究通过结合自然语言处理、机器学习和大型语言模型，提出了一种智能产后抑郁筛查系统，实现了在时间和成本上的优化，同时解决了机器学习的黑箱问题，提高了预测的解释性。该系统在产后抑郁检测上的准确率达到90%，优于现有文献中的方法。最终，该系统有助于产后抑郁和相关风险因素的快速检测，对于及时有效的干预具有重要意义。", "conclusion": "该研究成功地构建了一个能够提高产后抑郁检测效率和准确性的系统，既克服了黑箱效应，又突破了成本和时空上的障碍，对于实现及时有效的干预具有重要贡献。"}}
{"id": "2508.10424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10424", "abs": "https://arxiv.org/abs/2508.10424", "authors": ["Shanyuan Liu", "Jian Zhu", "Junda Lu", "Yue Gong", "Liuzhuozheng Li", "Bo Cheng", "Yuhang Ma", "Liebucha Wu", "Xiaoyu Wu", "Dawei Leng", "Yuhui Yin"], "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer", "comment": null, "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.", "AI": {"tldr": "This paper introduces NanoControl, a highly efficient method for controllable text-to-image generation using Diffusion Transformers (DiTs), which significantly reduces parameter and computational costs compared to existing methods.", "motivation": "To address the challenges of parameter overhead and increased computational costs associated with the ControlNet paradigm in controllable text-to-image generation using DiT models.", "method": "NanoControl employs Flux as the backbone network, uses a LoRA-style control module to learn control signals from raw conditioning inputs, and introduces KV-Context Augmentation for efficient fusion of conditional features.", "result": "The model achieves state-of-the-art performance in controllable text-to-image generation with minimal increases in parameter count and computational cost.", "conclusion": "NanoControl provides a more efficient solution for controllable text-to-image synthesis, reducing computational overhead while maintaining superior generation quality and achieving improved controllability."}}
{"id": "2508.10026", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10026", "abs": "https://arxiv.org/abs/2508.10026", "authors": ["Kai Zhao", "Yanjun Zhao", "Jiaming Song", "Shien He", "Lusheng Zhang", "Qiang Zhang", "Tianjiao Li"], "title": "SABER: Switchable and Balanced Training for Efficient LLM Reasoning", "comment": null, "summary": "Large language models (LLMs) empowered by chain-of-thought reasoning have\nachieved impressive accuracy on complex tasks but suffer from excessive\ninference costs and latency when applied uniformly to all problems. We propose\nSABER (Switchable and Balanced Training for Efficient LLM Reasoning), a\nreinforcement learning framework that endows LLMs with user-controllable,\ntoken-budgeted reasoning. SABER first profiles each training example's\nbase-model thinking token usage and assigns it to one of the predefined budget\ntiers. During fine-tuning, the model is guided by system prompts and\nlength-aware rewards to respect its assigned budget. In parallel, we\nincorporate no-think examples to ensure the model remains reliable even when\nexplicit reasoning is turned off. SABER further supports four discrete\ninference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling\nflexible trade-offs between latency and reasoning depth. Extensive evaluations\non math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning\n(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight\nbudgets, graceful degradation, and effective cross-scale and cross-domain\ngeneralization. In particular, SABER-FastThink cuts reasoning length by 65.4%\nand yields a 3.6% accuracy gain compared with the base model on the MATH\nbenchmark.", "AI": {"tldr": "SABER 是一种用于 LLMs 的强化学习框架，能够通过系统提示和激励措施来控制推理的令牌预算，从而在减少延迟的同时保持推理的准确性。此框架适用于多种任务，并在多个数据集上取得了成功。", "motivation": "虽然大型语言模型（LLMs）在复杂任务中通过链式思维推理实现了显著的准确性，但它们在应用到所有问题上时会遭受过高的推理成本和延迟。SABER 的动机是解决这一问题，提供更加高效、成本效益更好的 LLMs 推理方式。", "method": "SABER (Switchable and Balanced Training for Efficient LLM Reasoning) 是一个强化学习框架，它使大型语言模型具备了用户控制的、基于令牌预算的推理能力。首先，每一个训练样本的基础模型思路令牌使用情况被分析并分配到预定义的预算层级中。在微调过程中，模型通过系统提示和长度意识奖励来遵守其分配的预算。与此同时，加入了无需推理的样本以确保模型在禁用明确推理的情况下仍能可靠工作。最后，SABER 支持四种离散推理模式 - NoThink，FastThink，CoreThink 和 DeepThink，这些模式允许模型在延迟和推理深度之间进行灵活的权衡。", "result": "评估显示，SABER 在数学推理 (MATH, GSM8K)、代码生成 (MBPP) 和逻辑推理 (LiveBench-Reasoning) 上，在严格的预算下实现了高精度，以及有效的跨规模和跨领域泛化。特别地，在 MATH 数据集上，SABER-FastThink 将推理长度减少了 65.4%，并且与基础模型相比，准确率提高了 3.6%。", "conclusion": "通过引入 SABER 框架，研究证明了可以以较低的推理成本降低大型语言模型中的延迟，同时保持甚至提高准确性。这一框架还展示了在不同任务上的可泛化性和灵活性。"}}
{"id": "2508.10427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10427", "abs": "https://arxiv.org/abs/2508.10427", "authors": ["Keishi Ishihara", "Kento Sasaki", "Tsubasa Takahashi", "Daiki Shiono", "Yu Yamaguchi"], "title": "STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes", "comment": "Project Page: https://turingmotors.github.io/stride-qa/", "summary": "Vision-Language Models (VLMs) have been applied to autonomous driving to\nsupport decision-making in complex real-world scenarios. However, their\ntraining on static, web-sourced image-text pairs fundamentally limits the\nprecise spatiotemporal reasoning required to understand and predict dynamic\ntraffic scenes. We address this critical gap with STRIDE-QA, a large-scale\nvisual question answering (VQA) dataset for physically grounded reasoning from\nan ego-centric perspective. Constructed from 100 hours of multi-sensor driving\ndata in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the\nlargest VQA dataset for spatiotemporal reasoning in urban driving, offering 16\nmillion QA pairs over 285K frames. Grounded by dense, automatically generated\nannotations including 3D bounding boxes, segmentation masks, and multi-object\ntracks, the dataset uniquely supports both object-centric and ego-centric\nreasoning through three novel QA tasks that require spatial localization and\ntemporal prediction. Our benchmarks demonstrate that existing VLMs struggle\nsignificantly, achieving near-zero scores on prediction consistency. In\ncontrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,\nachieving 55% success in spatial localization and 28% consistency in future\nmotion prediction, compared to near-zero scores from general-purpose VLMs.\nTherefore, STRIDE-QA establishes a comprehensive foundation for developing more\nreliable VLMs for safety-critical autonomous systems.", "AI": {"tldr": "STRIDE-QA 数据集旨在改善视觉-语言模型在自动驾驶中的时空推理能力，通过涵盖现实驾驶挑战的数据集提高现有模型性能。", "motivation": "视觉-语言模型在训练过程中依赖于静态、来源于网络的图像-文本对，这限制了它们对复杂动态交通场景的理解和预测能力。为了解决这一问题，STRIDE-QA旨在填补关键空白，通过提供大规模的视觉问答数据集来支持自车视角下的物理推理。", "method": "STRIDE-QA 是一个大规模的视觉问答数据集，用于自动驾驶中的时空推理。该数据集从东京的多传感器驾驶数据中构建，包含100小时的数据，涵盖多样且具有挑战性的情况。它提供了1600万个问答对，覆盖285K帧。通过三个新颖的问答任务，该数据集支持物体为中心和自车为中心的推理，这些任务要求空间定位和时间预测。", "result": "基准测试表明，现有的视觉-语言模型在STRIDE-QA上表现不佳，预测一致性得分接近零。相比之下，经过STRIDE-QA微调的视觉-语言模型在空间定位和未来运动预测方面表现出显著的性能提升，分别达到了55%的成功率和28%的预测一致性。", "conclusion": "STRIDE-QA数据集为开发更可靠的应用于安全关键的自动驾驶系统的视觉-语言模型提供了全面的基础。"}}
{"id": "2508.10027", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10027", "abs": "https://arxiv.org/abs/2508.10027", "authors": ["Ali Zolnour", "Hossein Azadmaleki", "Yasaman Haghbin", "Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sina Rashidi", "Masoud Khani", "AmirSajjad Taleban", "Samin Mahdizadeh Sani", "Maryam Dadkhah", "James M. Noble", "Suzanne Bakken", "Yadollah Yaghoobzadeh", "Abdol-Hossein Vahabie", "Masoud Rouhizadeh", "Maryam Zolnoori"], "title": "LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data", "comment": null, "summary": "Alzheimer's disease and related dementias (ADRD) affect approximately five\nmillion older adults in the U.S., yet over half remain undiagnosed.\nSpeech-based natural language processing (NLP) offers a promising, scalable\napproach to detect early cognitive decline through linguistic markers.\n  To develop and evaluate a screening pipeline that (i) fuses transformer\nembeddings with handcrafted linguistic features, (ii) tests data augmentation\nusing synthetic speech generated by large language models (LLMs), and (iii)\nbenchmarks unimodal and multimodal LLM classifiers for ADRD detection.\n  Transcripts from the DementiaBank \"cookie-theft\" task (n = 237) were used.\nTen transformer models were evaluated under three fine-tuning strategies. A\nfusion model combined embeddings from the top-performing transformer with 110\nlexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,\nMinistral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic\nspeech, which was used to augment training data. Three multimodal models\n(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in\nzero-shot and fine-tuned settings.\n  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or\ntransformer-only baselines. Augmenting training data with 2x MedAlpaca-7B\nsynthetic speech increased F1 to 85.7. Fine-tuning significantly improved\nunimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current\nmultimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =\n66.0). Performance gains aligned with the distributional similarity between\nsynthetic and real speech.\n  Integrating transformer embeddings with linguistic features enhances ADRD\ndetection from speech. Clinically tuned LLMs effectively support both\nclassification and data augmentation, while further advancement is needed in\nmultimodal modeling.", "AI": {"tldr": "本研究提出了一种结合变压器嵌入和语言学特征的检测框架，用以改善阿尔茨海默病和其他痴呆症的早期诊断，并通过引入合成语音的数据增强技术，取得了比传统方法更高的准确率。", "motivation": "鉴于阿尔茨海默病和其他相关痴呆症（ADRD）影响了美国大约500万老年人，但其中一半以上仍未被诊断出来，研究者希望通过基于口语的自然语言处理（NLP）技术，利用语言标记来检测早期认知下降，提供了一种有前景的，可扩展的方法。", "method": "该研究将变压器模型的嵌入与手工设计的语义特征结合起来，开发了一个检测阿尔茨海默病和其他相关痴呆症的筛查流水线。研究还测试了通过大规模语言模型(如LLM)生成的合成语音来增强数据的方法。此外，研究还对单模态和多模态LLM分类器进行了基准测试，用于阿尔茨海默病相关的痴呆症检测。", "result": "融合模型取得了F1值为83.3（AUC为89.5），优于仅使用语言学特征或变压器模型的基线。通过使用MedAlpaca-7B生成的2倍合成语音来增强训练数据，F1提升到了85.7。对于单模态LLM分类器，微调显著提高了性能，例如MedAlpaca的F1值从47.3提高到了78.5。多模态模型表现较低，GPT-4o和Qwen的F1值分别为70.2和66.0。表现的改进与合成语音和实际语音之间的分布相似性相吻合。", "conclusion": "将变压器嵌入和语言特征相结合，可以提高从语音中检测阿尔茨海默病以及其他痴呆症的效果。临床调优的LLM对于分类和数据增强起到了支持作用，但需要在多模态建模方面继续改进。"}}
{"id": "2508.10432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10432", "abs": "https://arxiv.org/abs/2508.10432", "authors": ["Baichen Liu", "Qi Lyu", "Xudong Wang", "Jiahua Dong", "Lianqing Liu", "Zhi Han"], "title": "CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation", "comment": null, "summary": "Continual video instance segmentation demands both the plasticity to absorb\nnew object categories and the stability to retain previously learned ones, all\nwhile preserving temporal consistency across frames. In this work, we introduce\nContrastive Residual Injection and Semantic Prompting (CRISP), an earlier\nattempt tailored to address the instance-wise, category-wise, and task-wise\nconfusion in continual video instance segmentation. For instance-wise learning,\nwe model instance tracking and construct instance correlation loss, which\nemphasizes the correlation with the prior query space while strengthening the\nspecificity of the current task query. For category-wise learning, we build an\nadaptive residual semantic prompt (ARSP) learning framework, which constructs a\nlearnable semantic residual prompt pool generated by category text and uses an\nadjustive query-prompt matching mechanism to build a mapping relationship\nbetween the query of the current task and the semantic residual prompt.\nMeanwhile, a semantic consistency loss based on the contrastive learning is\nintroduced to maintain semantic coherence between object queries and residual\nprompts during incremental training. For task-wise learning, to ensure the\ncorrelation at the inter-task level within the query space, we introduce a\nconcise yet powerful initialization strategy for incremental prompts. Extensive\nexperiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that\nCRISP significantly outperforms existing continual segmentation methods in the\nlong-term continual video instance segmentation task, avoiding catastrophic\nforgetting and effectively improving segmentation and classification\nperformance. The code is available at https://github.com/01upup10/CRISP.", "AI": {"tldr": "CRISP是一种改进持续视频实例分割技术的方法，通过建立实例级、类别级和任务级学习，维持分割和分类性能，实验结果证明其优于其他方法。", "motivation": "视频实例分割需要系统具备同时吸收新的物体类别和保留以前学习到的类别信息的能力，同时保持帧间的时序一致性。现有方法在长期任务上容易遗忘以前的学习，无法有效平衡新旧信息学习，因此本研究旨在通过CRISP方法提高长期持续视频实例分割的表现，减少灾难性遗忘，提高分割和分类性能。", "method": "本研究提出了Contrastive Residual Injection and Semantic Prompting (CRISP)方法，旨在解决持续视频实例分割中的实例级别、类别级别和任务级别的混淆问题。对于实例级别学习，通过实例跟踪建模和细化实例相关损失函数，加强了当前任务查询的特异性。对于类别级别学习，提出了自适应残差语义提示(ARSP)学习框架，构建了一个可学习的语义残差提示池，并引入对比学习建立语义一致性损失来维持训练期间对象查询与残差提示之间的语义一致性。对于任务级别学习，提供了一个简洁但有效的增量提示初始化策略，以保持多个任务间查询空间的关联性。", "result": "实验结果显示，CRISP在YouTube-VIS-2019和YouTube-VIS-2021数据集上显著超过了现有的持续分割方法，在长期持续视频实例分割任务上避免了灾难性的遗忘，有效地提高了分割和分类表现。", "conclusion": "CRISP方法在长期持续视频实例分割任务上表现突出，通过学习实例、类别的相关性并加强任务间的联系，能有效避免遗忘，提高性能。"}}
{"id": "2508.10028", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10028", "abs": "https://arxiv.org/abs/2508.10028", "authors": ["Xiao Fu", "Hossein A. Rahmani", "Bin Wu", "Jerome Ramos", "Emine Yilmaz", "Aldo Lipani"], "title": "PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs", "comment": "7 pages", "summary": "Personalised text generation is essential for user-centric information\nsystems, yet most evaluation methods overlook the individuality of users. We\nintroduce \\textbf{PREF}, a \\textbf{P}ersonalised \\textbf{R}eference-free\n\\textbf{E}valuation \\textbf{F}ramework that jointly measures general output\nquality and user-specific alignment without requiring gold personalised\nreferences. PREF operates in a three-step pipeline: (1) a coverage stage uses a\nlarge language model (LLM) to generate a comprehensive, query-specific\nguideline covering universal criteria such as factuality, coherence, and\ncompleteness; (2) a preference stage re-ranks and selectively augments these\nfactors using the target user's profile, stated or inferred preferences, and\ncontext, producing a personalised evaluation rubric; and (3) a scoring stage\napplies an LLM judge to rate candidate answers against this rubric, ensuring\nbaseline adequacy while capturing subjective priorities. This separation of\ncoverage from preference improves robustness, transparency, and reusability,\nand allows smaller models to approximate the personalised quality of larger\nones. Experiments on the PrefEval benchmark, including implicit\npreference-following tasks, show that PREF achieves higher accuracy, better\ncalibration, and closer alignment with human judgments than strong baselines.\nBy enabling scalable, interpretable, and user-aligned evaluation, PREF lays the\ngroundwork for more reliable assessment and development of personalised\nlanguage generation systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
