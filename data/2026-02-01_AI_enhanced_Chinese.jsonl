{"id": "2601.20881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20881", "abs": "https://arxiv.org/abs/2601.20881", "authors": ["Matteo Rossi"], "title": "MA-LipNet: Multi-Dimensional Attention Networks for Robust Lipreading", "comment": null, "summary": "Lipreading, the technology of decoding spoken content from silent videos of lip movements, holds significant application value in fields such as public security. However, due to the subtle nature of articulatory gestures, existing lipreading methods often suffer from limited feature discriminability and poor generalization capabilities. To address these challenges, this paper delves into the purification of visual features from temporal, spatial, and channel dimensions. We propose a novel method named Multi-Attention Lipreading Network(MA-LipNet). The core of MA-LipNet lies in its sequential application of three dedicated attention modules. Firstly, a \\textit{Channel Attention (CA)} module is employed to adaptively recalibrate channel-wise features, thereby mitigating interference from less informative channels. Subsequently, two spatio-temporal attention modules with distinct granularities-\\textit{Joint Spatial-Temporal Attention (JSTA)} and \\textit{Separate Spatial-Temporal Attention (SSTA)}-are leveraged to suppress the influence of irrelevant pixels and video frames. The JSTA module performs a coarse-grained filtering by computing a unified weight map across the spatio-temporal dimensions, while the SSTA module conducts a more fine-grained refinement by separately modeling temporal and spatial attentions. Extensive experiments conducted on the CMLR and GRID datasets demonstrate that MA-LipNet significantly reduces the Character Error Rate (CER) and Word Error Rate (WER), validating its effectiveness and superiority over several state-of-the-art methods. Our work highlights the importance of multi-dimensional feature refinement for robust visual speech recognition.", "AI": {"tldr": "The paper proposes MA-LipNet, a lipreading technology that uses a novel method of multi-attention mechanisms to improve feature discriminability and generalization capabilities by refining visual features across multiple dimensions.", "motivation": "To improve the feature discriminability and generalization capabilities of lipreading technologies, which are currently limited due to the subtlety of lip movements.", "method": "Proposes MA-LipNet, which employs three attention modules: Channel Attention (CA), Joint Spatial-Temporal Attention (JSTA), and Separate Spatial-Temporal Attention (SSTA) to refine visual features temporally, spatially, and channel-wise.", "result": "Experiments on CMLR and GRID datasets showed that MA-LipNet achieved lower CER and WER, outperforming state-of-the-art methods.", "conclusion": "The work highlights the effectiveness of multi-dimensional feature refinement in enhancing the accuracy and robustness of lipreading technologies."}}
{"id": "2601.20911", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20911", "abs": "https://arxiv.org/abs/2601.20911", "authors": ["Haochen Zhang", "Animesh Sinha", "Felix Juefei-Xu", "Haoyu Ma", "Kunpeng Li", "Zhipeng Fan", "Meng Dong", "Xiaoliang Dai", "Tingbo Hou", "Peizhao Zhang", "Zecheng He"], "title": "Non-Markov Multi-Round Conversational Image Generation with History-Conditioned MLLMs", "comment": "19 pages, 19 figures, plan for TIP", "summary": "Conversational image generation requires a model to follow user instructions across multiple rounds of interaction, grounded in interleaved text and images that accumulate as chat history. While recent multimodal large language models (MLLMs) can generate and edit images, most existing multi-turn benchmarks and training recipes are effectively Markov: the next output depends primarily on the most recent image, enabling shortcut solutions that ignore long-range history. In this work we formalize and target the more challenging non-Markov setting, where a user may refer back to earlier states, undo changes, or reference entities introduced several rounds ago. We present (i) non-Markov multi-round data construction strategies, including rollback-style editing that forces retrieval of earlier visual states and name-based multi-round personalization that binds names to appearances across rounds; (ii) a history-conditioned training and inference framework with token-level caching to prevent multi-round identity drift; and (iii) enabling improvements for high-fidelity image reconstruction and editable personalization, including a reconstruction-based DiT detokenizer and a multi-stage fine-tuning curriculum. We demonstrate that explicitly training for non-Markov interactions yields substantial improvements in multi-round consistency and instruction compliance, while maintaining strong single-round editing and personalization.", "AI": {"tldr": "研究探讨了非马尔可夫设置的多轮次图像生成问题，通过改进数据构建、训练框架和图像重建技术，显著增强了长期一致性和指令遵从性。", "motivation": "当前的多轮次基准测试和训练方案主要集中在马尔可夫模型上，忽视了长期历史记录的重要性。本研究旨在处理更复杂的非马尔可夫场景，增强长范围历史引用的能力。", "method": "本研究提出了非马尔可夫多回合数据构建策略，包括回滚式编辑和基于名称的多轮个性化处理。研究还涉及基于历史条件训练和推理框架，以及高保真图像重建和可编辑个性化的改进。", "result": "实验结果表明，针对非马尔可夫交互的显式训练显著提升了多轮一致性及指令遵从性，同时保持了单轮编辑和个性化处理的高水平。", "conclusion": "研究成功展示了在非马尔可夫交互环境中进行训练可以带来显著的性能提升，尤其是在多轮次一致性方面的改进。"}}
{"id": "2601.20990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20990", "abs": "https://arxiv.org/abs/2601.20990", "authors": ["Xuehua Ye", "Hongxu Yang", "Adam J. Schwarz"], "title": "Text controllable PET denoising", "comment": "SPIE Medical Imaging 2026", "summary": "Positron Emission Tomography (PET) imaging is a vital tool in medical diagnostics, offering detailed insights into molecular processes within the human body. However, PET images often suffer from complicated noise, which can obscure critical diagnostic information. The quality of the PET image is impacted by various factors including scanner hardware, image reconstruction, tracer properties, dose/count level, and acquisition time. In this study, we propose a novel text-guided denoising method capable of enhancing PET images across a wide range of count levels within a single model. The model utilized the features from a pretrained CLIP model with a U-Net based denoising model. Experimental results demonstrate that the proposed model leads significant improvements in both qualitative and quantitative assessments. The flexibility of the model shows the potential for helping more complicated denoising demands or reducing the acquisition time.", "AI": {"tldr": "研究提出了一种新的基于文本指导的降噪方法，能够提升PET成像质量，在单一模型中适应广泛的计数水平。", "motivation": "由于PET图像常常受到复杂噪声的影响，这可能掩盖关键的诊断信息。本研究的动机是改善PET图像的质量，克服现有方法的限制。", "method": "该研究利用了预训练CLIP模型的特征与基于U-Net的降噪模型结合的方法。", "result": "实验结果显示，所提出的模型在定性和定量评估中都带来了显著的改进。模型的灵活性表明，它有潜力帮助应对更复杂的降噪需求，或减少采集时间。", "conclusion": "这项研究提出了一种新型的基于文本引导的降噪方法，可以在单一模型中增强广泛的计数水平下的PET图像质量，具有广泛的潜在应用前景。"}}
{"id": "2601.20995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20995", "abs": "https://arxiv.org/abs/2601.20995", "authors": ["Hongxu Yang", "Levente Lippenszky", "Edina Timko", "Lehel Ferenczi", "Gopal Avinash"], "title": "Low performing pixel correction in computed tomography with unrolled network and synthetic data training", "comment": "ISBI 2026 accepted", "summary": "Low performance pixels (LPP) in Computed Tomography (CT) detectors would lead to ring and streak artifacts in the reconstructed images, making them clinically unusable. In recent years, several solutions have been proposed to correct LPP artifacts, either in the image domain or in the sinogram domain using supervised deep learning methods. However, these methods require dedicated datasets for training, which are expensive to collect. Moreover, existing approaches focus solely either on image-space or sinogram-space correction, ignoring the intrinsic correlations from the forward operation of the CT geometry. In this work, we propose an unrolled dual-domain method based on synthetic data to correct LPP artifacts. Specifically, the intrinsic correlations of LPP between the sinogram and image domains are leveraged through synthetic data generated from natural images, enabling the trained model to correct artifacts without requiring any real-world clinical data. In experiments simulating 1-2% detectors defect near the isocenter, the proposed method outperformed the state-of-the-art approaches by a large margin. The results indicate that our solution can correct LPP artifacts without the cost of data collection for model training, and it is adaptable to different scanner settings for software-based applications.", "AI": {"tldr": "proposed an unrolled dual-domain method using synthetic data to correct CT low performance pixel artifacts without the need for real-world datasets", "motivation": "low performance pixels (LPP) in CT detectors cause clinically unusable ring and streak artifacts in images, and existing solutions require expensive data sets or work in only one domain", "method": "unrolled dual-domain method based on synthetic data to correct LPP artifacts", "result": "outperformed state-of-the-art approaches by a large margin in experiments simulating 1-2% detector defects near the isocenter", "conclusion": "the proposed method can effectively correct LPP artifacts without expensive data collection for training and is adaptable to different scanner settings"}}
{"id": "2601.20975", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.20975", "abs": "https://arxiv.org/abs/2601.20975", "authors": ["Nikita Gupta", "Riju Chatterjee", "Lukas Haas", "Connie Tao", "Andrew Wang", "Chang Liu", "Hidekazu Oiwa", "Elena Gribovskaya", "Jan Ackermann", "John Blitzer", "Sasha Goldshtein", "Dipanjan Das"], "title": "DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents", "comment": "DeepSearchQA can be found at https://www.kaggle.com/benchmarks/google/dsqa/leaderboard", "summary": "We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.", "AI": {"tldr": "DeepSearchQA是一个包含900个提示的跨17个不同领域的多步信息检索任务基准。它旨在评估代理系统从不同来源系统收集和整合信息、去重和实体解析以及在开放式搜索空间中判断停止标准的能力。研究表明，最先进的代理架构在保持高召回率的同时也面临精度上的挑战。", "motivation": "作者试图建立一个更复杂的基准，用于评估代理系统在执行多步骤、信息碎片化搜索任务中的综合能力。传统基准无法充分评估代理的复杂搜索计划生成和执行能力。", "method": "DeepSearchQA设计了一系列基于因果链结构的任务，这些任务依赖于一系列复杂搜索步骤的完成，并通过开放式网络获取信息，保证答案集客观可验证。", "result": "在最先进的代理架构评估中发现，即便最先进模型也难以在保持高召回率同时达成高精度。主要失败模式为过早停止搜索或过度补偿导致低置信度的答案增多。", "conclusion": "研究结果突显了现有代理设计中的空间，以及DeepSearchQA作为检测工具在推动未来研究和设计更强大、更深入的研究能力方面的重要作用。"}}
{"id": "2601.21022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21022", "abs": "https://arxiv.org/abs/2601.21022", "authors": ["Andrea Camilloni", "Chiara Micoli", "Nita Mulliqi", "Erik Everett Palm", "Thorgerdur Palsdottir", "Kelvin Szolnoky", "Xiaoyi Ji", "Sol Erika Boman", "Andrea Discacciati", "Henrik Grönberg", "Lars Egevad", "Tobias Nordström", "Kimmo Kartasalo", "Martin Eklund"], "title": "AI-based Prediction of Biochemical Recurrence from Biopsy and Prostatectomy Samples", "comment": "39 pages, 6 tables, 11 figures", "summary": "Biochemical recurrence (BCR) after radical prostatectomy (RP) is a surrogate marker for aggressive prostate cancer with adverse outcomes, yet current prognostic tools remain imprecise. We trained an AI-based model on diagnostic prostate biopsy slides from the STHLM3 cohort (n = 676) to predict patient-specific risk of BCR, using foundation models and attention-based multiple instance learning. Generalizability was assessed across three external RP cohorts: LEOPARD (n = 508), CHIMERA (n = 95), and TCGA-PRAD (n = 379). The image-based approach achieved 5-year time-dependent AUCs of 0.64, 0.70, and 0.70, respectively. Integrating clinical variables added complementary prognostic value and enabled statistically significant risk stratification. Compared with guideline-based CAPRA-S, AI incrementally improved postoperative prognostication. These findings suggest biopsy-trained histopathology AI can generalize across specimen types to support preoperative and postoperative decision making, but the added value of AI-based multimodal approaches over simpler predictive models should be critically scrutinized in further studies.", "AI": {"tldr": "研究团队通过AI模型提高了前列腺癌术后BCR预测的准确性，展示了AI在术前术后决策中的潜力，但强调了未来需要进一步研究的必要性。", "motivation": "当前的预测工具仍不够精确，BCR是根治性前列腺切除术后具有不良预后的侵袭性前列腺癌的替代指标，因此研究的动机是开发一种更精确的BCR风险预测工具。", "method": "使用基于AI的模型对STHLM3队列中的诊断前列腺活检切片进行训练，以预测患者的BCR风险，采用了基础模型和基于注意力的多实例学习方法。在三个外部RP队列（LEOPARD，CHIMERA，TCGA-PRAD）中测试了一般性。", "result": "基于图像的方法在LEOPARD，CHIMERA，TCGA-PRAD队列中的5年时间依赖AUC分别为0.64，0.70，0.70。将临床变量进行整合增加了预测价值，并允许统计学上显著的风险分层。与CAPRA-S指南相比，AI增量改善了术后预后评估。", "conclusion": "这些研究结果表明，基于活检训练的组织病理学AI可以泛化至多种样本类型，支持术前术后决策，但是AI多模态方法相对于简单预测模型的价值需要在进一步研究中被批判性地仔细评估。"}}
{"id": "2601.20992", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.20992", "abs": "https://arxiv.org/abs/2601.20992", "authors": ["Oleg Sedukhin", "Andrey Kostin"], "title": "asr_eval: Algorithms and tools for multi-reference and streaming speech recognition evaluation", "comment": null, "summary": "We propose several improvements to the speech recognition evaluation. First, we propose a string alignment algorithm that supports both multi-reference labeling, arbitrary-length insertions and better word alignment. This is especially useful for non-Latin languages, those with rich word formation, to label cluttered or longform speech. Secondly, we collect a novel test set DiverseSpeech-Ru of longform in-the-wild Russian speech with careful multi-reference labeling. We also perform multi-reference relabeling of popular Russian tests set and study fine-tuning dynamics on its corresponding train set. We demonstrate that the model often adopts to dataset-specific labeling, causing an illusion of metric improvement. Based on the improved word alignment, we develop tools to evaluate streaming speech recognition and to align multiple transcriptions to compare them visually. Additionally, we provide uniform wrappers for many offline and streaming speech recognition models. Our code will be made publicly available.", "AI": {"tldr": "改进了语音识别评估方法，并创建了一个新的测试集，展示了模型适应特定标签的现象，开发了评估工具。", "motivation": "优化非拉丁语言和词汇丰富的语言的语音识别评估，以及长文本或杂乱语音的标签。", "method": "提出了多项改进的语音识别评估方法，包括一个多参考点标签的字符串对齐算法，支持任意长度的插入和更好的单词对齐。还创建了一个新的测试集DiverseSpeech-Ru，包含精心多参考标签的长形俄罗斯语音，并重新标注了流行的俄语测试集。", "result": "展示了模型倾向于适应数据集特定标签的现象，开发的工具支持流式语音识别评估和多个转录的视觉对比。", "conclusion": "改进的评估方法揭示了模型实际上是对适应数据集的标签，而非性能确实提升，工具和代码将公开发布。"}}
{"id": "2601.21066", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.21066", "abs": "https://arxiv.org/abs/2601.21066", "authors": ["Kealan Dunnett", "Reza Arablouei", "Dimity Miller", "Volkan Dedeoglu", "Raja Jurdak"], "title": "BadDet+: Robust Backdoor Attacks for Object Detection", "comment": null, "summary": "Backdoor attacks pose a severe threat to deep learning, yet their impact on object detection remains poorly understood compared to image classification. While attacks have been proposed, we identify critical weaknesses in existing detection-based methods, specifically their reliance on unrealistic assumptions and a lack of physical validation. To bridge this gap, we introduce BadDet+, a penalty-based framework that unifies Region Misclassification Attacks (RMA) and Object Disappearance Attacks (ODA). The core mechanism utilizes a log-barrier penalty to suppress true-class predictions for triggered inputs, resulting in (i) position and scale invariance, and (ii) enhanced physical robustness. On real-world benchmarks, BadDet+ achieves superior synthetic-to-physical transfer compared to existing RMA and ODA baselines while preserving clean performance. Theoretical analysis confirms the proposed penalty acts within a trigger-specific feature subspace, reliably inducing attacks without degrading standard inference. These results highlight significant vulnerabilities in object detection and the necessity for specialized defenses.", "AI": {"tldr": "The paper introduces BadDet+, a framework that enhances the robustness of backdoor attacks in object detection tasks by addressing limitations in existing methods.", "motivation": "To address the lack of physical validation and unrealistic assumptions in existing backdoor detection-based methods and to bridge the gap in understanding their impact on object detection.", "method": "BadDet+ uses a log-barrier penalty to suppress true-class predictions for triggered inputs, thereby achieving position and scale invariance and enhancing physical robustness.", "result": "BadDet+ outperforms existing baselines in terms of synthetic-to-physical transfer on real-world benchmarks while maintaining clean performance.", "conclusion": "The research highlights the significant vulnerabilities in object detection systems and emphasizes the necessity for tailored defenses against backdoor attacks."}}
{"id": "2601.21000", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21000", "abs": "https://arxiv.org/abs/2601.21000", "authors": ["Muhammad Ali Shafique", "Areej Mehboob", "Layba Fiaz", "Muhammad Usman Qadeer", "Hamza Farooq"], "title": "UrduBench: An Urdu Reasoning Benchmark using Contextually Ensembled Translations with Human-in-the-Loop", "comment": null, "summary": "Recent advances in large language models (LLMs) have led to strong reasoning capabilities; however, evaluating such models in low-resource languages remains challenging due to the lack of standardized benchmarks. In particular, Urdu reasoning evaluation has been limited by the sensitivity of machine translation and an emphasis on general language tasks rather than reasoning benchmarks. In this paper, we propose a contextually ensembled translation framework with human-in-the-loop validation that leverages multiple translation systems to develop Urdu reasoning benchmarks while preserving contextual and structural integrity. Using this framework, we translate widely adopted reasoning and question-answering benchmarks, including MGSM, MATH-500, CommonSenseQA, and OpenBookQA, into Urdu, collectively referred to as UrduBench, and conduct a comprehensive evaluation of both reasoning-oriented and instruction-tuned LLMs across multiple prompting strategies. Our analysis reveals performance differences across (1) four datasets, (2) five task difficulty levels, (3) diverse model architectures, (4) multiple model scaling settings, and (5) language consistency tests. We find that multi-step and symbolic reasoning tasks pose significant challenges in Urdu, and that stable language alignment is a critical prerequisite for robust reasoning. Overall, our work establishes a scalable methodology for standardized reasoning evaluation in Urdu and provides empirical insights into multilingual reasoning failures. This experimental setup is also broadly applicable to other low-resource languages. The code and datasets will be publicly released.", "AI": {"tldr": "这项研究提出了一个翻译和评估乌尔都语推理任务的新框架，发现乌尔都语中多步和符号推理任务具有挑战性，并提供了该评估方法和结果，为其他低资源语言的推理能力评估提供了参考。", "motivation": "由于缺乏标准化的基准测试，大型语言模型（LLMs）在低资源语言中的推理能力评估较为困难。尤其是乌尔都语推理评估方面，机器翻译的敏感性和对通用语言任务的强调而非推理基准限制了发展。因此，提出了一种新的框架来开发乌尔都语推理基准。", "method": "提出了一种具有人在环验证的上下文集成翻译框架，该框架利用多个翻译系统来开发乌尔都语推理基准，同时保持了上下文和结构的完整性。使用该框架将广泛采用的推理和问答基准转换成乌尔都语，包括MGSM、MATH-500、CommonSenseQA和OpenBookQA等。", "result": "该研究对推理导向和指令微调的LLMs进行了全面的评估，结果揭示了性能差异，包括四个数据集、五个任务难度级别、多种模型架构、多种模型缩放设置以及语言一致性测试。尤其是，多步和符号推理任务在乌尔都语中提出了重大挑战。", "conclusion": "本工作建立了一种可扩展的方法来标准化乌尔都语的推理评估，并提供了多语言推理失败的经验性见解。该实验设置也广泛适用于其他低资源语言。代码和数据集将公开发布。"}}
{"id": "2601.21078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21078", "abs": "https://arxiv.org/abs/2601.21078", "authors": ["Jiaqi Li", "Guangming Wang", "Shuntian Zheng", "Minzhe Ni", "Xiaoman Lu", "Guanghui Ye", "Yu Guan"], "title": "Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization", "comment": null, "summary": "Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.", "AI": {"tldr": "ActionVLM提出了一种新的方法来减少TAL中的模态偏差，保留视觉作为主要信号，同时适应性地利用语言作为补充，从而提升性能。", "motivation": "由于现有方法倾向于过分强调语言先验，导致视觉性能被削弱，因此需要解决在TAL任务中由于模态偏差引起的问题。", "method": "ActionVLM，一种视觉语言聚合框架，旨在系统性地减轻TAL中的模态偏差。该方法包括（i）一个去偏差重加权模块，评估语言优势并动态调整语言模态权重；（ii）一种残差聚合策略，将语言作为补充精炼而非主要驱动因素。", "result": "实验结果表明，ActionVLM在THUMOS14数据集上的表现优于现有最佳方法，提升了最高3.2%的mAP。", "conclusion": "ActionVLM通过减少模态偏差和过度自信问题，同时增强了时间推理能力，从而提高了TAL任务的性能。"}}
{"id": "2601.21084", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.21084", "abs": "https://arxiv.org/abs/2601.21084", "authors": ["Amit Meghanani", "Thomas Hain"], "title": "Position-invariant Fine-tuning of Speech Enhancement Models with Self-supervised Speech Representations", "comment": "Accepted to ICASSP 2026", "summary": "Integrating front-end speech enhancement (SE) models with self-supervised learning (SSL)-based speech models is effective for downstream tasks in noisy conditions. SE models are commonly fine-tuned using SSL representations with mean squared error (MSE) loss between enhanced and clean speech. However, MSE is prone to exploiting positional embeddings in SSL models, allowing the objective to be minimised through positional correlations instead of content-related information. This work frames the problem as a general limitation of self-supervised representation fine-tuning and investigates it through representation-guided SE. Two strategies are considered: (1) zero-padding, previously explored in SSL pre-training but here examined in the fine-tuning setting, and (2) speed perturbations with a soft-DTW loss. Experiments show that the soft-DTW-based approach achieves faster convergence and improved downstream performance, underscoring the importance of position-invariant fine-tuning in SSL-based speech modelling.", "AI": {"tldr": "The study investigates the problem of positional embedding exploitation in self-supervised speech model fine-tuning and proposes two methods using representation-guided speech enhancement, with the soft-DTW-based method proving more effective", "motivation": "address the issue that MSE loss in fine-tuning could exploit positional embeddings in self-supervised learning models, leading to optimization of positional correlations rather than content-related information", "method": "representation-guided speech enhancement with two strategies: zero-padding and speed perturbations with soft-DTW loss", "result": "Experiments show that the soft-DTW strategy provides faster convergence and better performance in downstream tasks", "conclusion": "Position-invariant fine-tuning is crucial for the effectiveness of speech models based on self-supervised learning"}}
{"id": "2601.21081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.21081", "abs": "https://arxiv.org/abs/2601.21081", "authors": ["Yu Huo", "Siyu Zhang", "Kun Zeng", "Haoyue Liu", "Owen Lee", "Junlin Chen", "Yuquan Lu", "Yifu Guo", "Yaodong Liang", "Xiaoying Tang"], "title": "Shape of Thought: Progressive Object Assembly via Visual Chain-of-Thought", "comment": "The code is available at https://anonymous.4open.science/r/16FE/", "summary": "Multimodal models for text-to-image generation have achieved strong visual fidelity, yet they remain brittle under compositional structural constraints-notably generative numeracy, attribute binding, and part-level relations. To address these challenges, we propose Shape-of-Thought (SoT), a visual CoT framework that enables progressive shape assembly via coherent 2D projections without external engines at inference time. SoT trains a unified multimodal autoregressive model to generate interleaved textual plans and rendered intermediate states, helping the model capture shape-assembly logic without producing explicit geometric representations. To support this paradigm, we introduce SoT-26K, a large-scale dataset of grounded assembly traces derived from part-based CAD hierarchies, and T2S-CompBench, a benchmark for evaluating structural integrity and trace faithfulness. Fine-tuning on SoT-26K achieves 88.4% on component numeracy and 84.8% on structural topology, outperforming text-only baselines by around 20%. SoT establishes a new paradigm for transparent, process-supervised compositional generation. The code is available at https://anonymous.4open.science/r/16FE/. The SoT-26K dataset will be released upon acceptance.", "AI": {"tldr": "Shape-of-Thought (SoT)是一种新提出的视觉CoT框架，用于改善文本到图像生成模型在处理复杂形状和结构时的性能。通过连贯的2D投影逐步组装形状，SoT在SoT-26K数据集上的表现优于传统方法。", "motivation": "现有的多模态文本到图像生成模型在面对组合结构约束时性能不佳，特别是生成数值、属性绑定和部分级关系方面存在问题。", "method": "提出了一种名为Shape-of-Thought (SoT)的视觉CoT框架，该框架能够在没有外部引擎的情况下通过连贯的2D投影逐步组装形状。SoT训练了一个统一的多模态自回归模型，用于生成交织的文本计划和渲染的中间状态，帮助模型捕捉形状组装逻辑，而无需生成显式的几何表示。", "result": "在SoT-26K数据集上进行微调后，该模型在组件数值上达到了88.4%，在结构拓扑上达到了84.8%，相比仅使用文本的基线方法提高了大约20%。", "conclusion": "SoT建立了一种新的透明、过程监督的组合生成范式，证明了在处理复杂形状和结构时的有效性。"}}
