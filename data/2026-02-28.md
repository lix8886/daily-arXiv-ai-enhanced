<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decoder-based Sense Knowledge Distillation](https://arxiv.org/abs/2602.22351)
*Qitong Wang,Mohammed J. Zaki,Georgios Kollias,Vasileios Kalantzis*

Main category: cs.CL

> 提出了一种新的框架DSKD，用于在不需在推理阶段进行词典查询的情况下将词汇知识整合到解码器模型的知识蒸馏中，从而提高了生成模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）在学习上下文化嵌入时往往会忽略结构化的词汇知识，如词义和关系。先前的工作表明，将词典整合到编码器模型的知识蒸馏中可以提高性能，但将其应用于作为生成模型的解码器却具有挑战性。

**Method:** 我们提出了一种称为基于解码器的词义知识蒸馏（DSKD）的框架，该框架可以在不需在推理时查找词典的情况下将词法资源集成到解码器式LLMs的训练中。

**Result:** 广泛的实验表明，DSKD显著提高了解码器的知识蒸馏性能，使生成模型能够继承结构化语义，同时保持高效的训练。

**Conclusion:** 该框架通过整合词法资源，为在解码器式LLMs中进行知识蒸馏提供了一种有效的方法，不需在推理时进行词典查询，有助于提高模型的性能。

**Abstract:** Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.

</details>


### [2] [Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts](https://arxiv.org/abs/2602.22359)
*Arno Simons*

Main category: cs.CL

> 本研究通过对2x3平横设计中的不同提示支架和框架对GPT-5进行测试，探究大型语言模型在解释性引文分析中的适用性。结果表明，尽管模型在表面分类具有稳定性，但所使用的提示支架会影响模型生成的解读和词汇选择。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是为了测试大型语言模型（如GPT-5）是否可以通过对单个复杂案例进行深入解释来支持引文上下文分析（CCA），而不是通过对类型标签进行拓展。本文强调了对提示灵敏度分析作为方法论问题的重要性。

**Method:** 该研究采用了两种阶段的GPT-5流水线来分析引文背景：首先基于引文文本进行表面分类和预期评估，然后使用引用文献和被引用文献的全文进行跨文档解释性重构。研究者对不同的提示支架和框架进行了2x3平衡设计下多变量测试，以分析其对模型输出的影响。

**Result:** 研究结果显示，在90个重构中，模型共产生了450种不同的假设。通过仔细阅读和归纳编码，研究发现21种反复出现的解释性动作，并且线性概率模型估计了提示选择如何改变它们的频率和词汇集合。GPT-5在表面阶段非常稳定，能够持续将引用分类为补充性。在解释性重构阶段，模型生成了一种可能替代空间的结构，但提示支架和实例的加入重新分配了注意力和词汇，有时会导致紧张的阅读。与Gilbert相比，GPT-5检测到相同文本的关键点，但更频繁地将其归类为源流和定位，而不是警告。

**Conclusion:** 该研究详细介绍了在将大型语言模型作为解释性引文背景分析的指导共同分析师时，其应用机会与风险。研究结果表明，提示支架和框架的系统性倾斜会影响模型强调的可能解读和词汇选择。该研究对理解大型语言模型在复杂文本分析中的作用提供了新的视角。

**Abstract:** This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as "supplementary". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.

</details>


### [3] [Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework](https://arxiv.org/abs/2602.22391)
*Rakib Ullah,Mominul islam,Md Sanjid Hossain,Md Ismail Hossain*

Main category: cs.CL

> 研究介绍了一个新的孟加拉语梗数据集Bn-HIB及其多模态协同注意力融合模型MCFM，该模型显著提升了有害内容识别的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决低资源语言（如孟加拉语）中识别梗内容所面临的挑战，通过创建一个专注孟加拉语的有标注的数据集，并开发一种能够有效处理此问题的新模型。

**Method:** 该研究引入了Bn-HIB数据集，这是第一个区分孟加拉语梗中煽动性和直接仇恨言论的数据集。此外，提出了一种称为MCFM（多模态协同注意力融合模型）的架构，该模型能够分别分析和融合梗中的视觉和文本元素，以实现更准确的分类。

**Result:** 实验表明，MCFM模型在Bn-HIB数据集上的表现显著优于几个最先进的模型，显示出其在该任务中的有效性。

**Conclusion:** 该研究填补了针对低资源语言孟加拉语梗内容有害性识别研究的空白，且提出的MCFM模型展现了在识别该类型内容方面的潜力。

**Abstract:** Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Enabling clinical use of foundation models in histopathology](https://arxiv.org/abs/2602.22347)
*Audun L. Henriksen,Ole-Johan Skrede,Lisa van der Schee,Enric Domingo,Sepp De Raedt,Ilyá Kostolomov,Jennifer Hay,Karolina Cyll,Wanja Kildal,Joakim Kalsnes,Robert W. Williams,Manohar Pradhan,John Arne Nesheim,Hanne A. Askautrud,Maria X. Isaksen,Karmele Saez de Gordoa,Miriam Cuatrecasas,Joanne Edwards,TransSCOT group,Arild Nesbakken,Neil A. Shepherd,Ian Tomlinson,Daniel-Christoph Wagner,Rachel S. Kerr,Tarjei Sveinsgjerd Hveem,Knut Liestøl,Yoshiaki Nakamura,Marco Novelli,Masaaki Miyo,Sebastian Foersch,David N. Church,Miangela M. Lacle,David J. Kerr,Andreas Kleppe*

Main category: cs.CV

> 引入新颖的稳健性损失函数训练任务特定模型，显著提高了模型的稳健性和预测准确性，尤其在识别生物相关特征方面效果突出。

<details>
  <summary>Details</summary>

**Motivation:** 减少预分析和扫描器特定带来的偏差，提高模型的泛用性和预测准确性。

**Method:** 引入新颖的稳健性损失函数以减少下游任务模型训练中的技术偏差。

**Result:** 在27,042张WSI和6155位患者的数据集上实验，显示出稳健性和预测精度的显著提高。

**Conclusion:** 该方法成功解决了计算病理学中基础模型的鲁棒性问题，且无需重新训练基础模型，适用于常规临床实践。

**Abstract:** Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.

</details>


### [5] [Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search](https://arxiv.org/abs/2602.22361)
*Liping Meng,Fan Nie,Yunyun Zhang,Chao Han*

Main category: cs.CV

> MNAS-Unet, a medical image segmentation framework that uses MCTS and NAS, offers improved efficiency and accuracy in architecture search compared to NAS-Unet and other models, while using fewer resources.

<details>
  <summary>Details</summary>

**Motivation:** To develop a more efficient and accurate medical image segmentation framework that can operate under practical resource constraints.

**Method:** MNAS-Unet combines MCTS and NAS to dynamically explore network architectures, optimizes DownSC and UpSC unit structures, and achieves high segmentation accuracy with fewer parameters and reduced GPU memory usage.

**Result:** MNAS-Unet significantly outperforms NAS-Unet and state-of-the-art models in segmentation accuracy and resource efficiency across several medical image datasets.

**Conclusion:** MNAS-Unet improves search efficiency and segmentation accuracy while reducing the computational budget and resource usage, enhancing practical applicability for medical image analysis.

**Abstract:** This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.

</details>


### [6] [AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction](https://arxiv.org/abs/2602.22376)
*Hanyang Liu,Rongjun Qin*

Main category: cs.CV

> AeroDGS is a physics-guided 4D Gaussian splatting framework proposed to enhance monocular aerial reconstruction by addressing depth ambiguity and unstable motion estimation issues in dynamic scenes captured by UAVs.

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of monocular dynamic scene reconstruction in aerial environments with challenges such as large spatial range, dynamic objects of varying size, and motion disparity.

**Method:** The AeroDGS framework includes a Monocular Geometry Lifting module for 3D geometry reconstruction and a Physics-Guided Optimization module to refine motion estimation by applying physical constraints.

**Result:** The method outperforms other existing techniques in both synthetic and real-world UAV datasets, providing better reconstruction quality in dynamic aerial settings.

**Conclusion:** AeroDGS significantly improves 4D scene reconstruction in monocular aerial sequences by addressing the issue of monocular depth ambiguity and unstable motion estimation, demonstrating superior performance in dynamic environments.

**Abstract:** Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.

</details>


### [7] [Enhancing Renal Tumor Malignancy Prediction: Deep Learning with Automatic 3D CT Organ Focused Attention](https://arxiv.org/abs/2602.22381)
*Zhengkang Fan,Chengkun Sun,Russell Terry,Jie Xu,Longin Jan Latecki*

Main category: cs.CV

> 研究开发了一种新的深度学习框架，利用OFA损失函数来提高肾肿瘤恶性程度预测的准确性，无需手动分割，并在多个数据集上表现出优于传统方法的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的成像技术不具备准确预测肾肿瘤恶性程度的准确性，而传统的深度学习方法依赖于手动分割来隔离肿瘤区域，以减少噪声，这既耗时又昂贵。本研究旨在提高预测准确性，同时减少人工干预的需求。

**Method:** 本研究开发了一种利用器官聚焦注意力（OFA）损失函数的深度学习框架，该函数可以调整图像块的注意力，使得器官块仅关注其他器官块，从而在进行恶性肿瘤预测时不需要对三维肾脏CT图像进行分割。

**Result:** 该框架在UF集成数据存储库（IDR）的私有数据集上实现了0.685的AUC和0.872的F1分数，在公开可用的KiTS21数据集上实现了0.760的AUC和0.852的F1分数。

**Conclusion:** 研究结果表明，不依赖于手动分割的方法可以提高肾肿瘤恶性预测的准确性，这为临床诊断提供了一种更有效和可靠的方法。

**Abstract:** Accurate prediction of malignancy in renal tumors is crucial for informing clinical decisions and optimizing treatment strategies. However, existing imaging modalities lack the necessary accuracy to reliably predict malignancy before surgical intervention. While deep learning has shown promise in malignancy prediction using 3D CT images, traditional approaches often rely on manual segmentation to isolate the tumor region and reduce noise, which enhances predictive performance. Manual segmentation, however, is labor-intensive, costly, and dependent on expert knowledge. In this study, a deep learning framework was developed utilizing an Organ Focused Attention (OFA) loss function to modify the attention of image patches so that organ patches attend only to other organ patches. Hence, no segmentation of 3D renal CT images is required at deployment time for malignancy prediction. The proposed framework achieved an AUC of 0.685 and an F1-score of 0.872 on a private dataset from the UF Integrated Data Repository (IDR), and an AUC of 0.760 and an F1-score of 0.852 on the publicly available KiTS21 dataset. These results surpass the performance of conventional models that rely on segmentation-based cropping for noise reduction, demonstrating the frameworks ability to enhance predictive accuracy without explicit segmentation input. The findings suggest that this approach offers a more efficient and reliable method for malignancy prediction, thereby enhancing clinical decision-making in renal cancer diagnosis.

</details>


### [8] [Vision Transformers Need More Than Registers](https://arxiv.org/abs/2602.22394)
*Cheng Shi,Yizhou Yu,Sibei Yang*

Main category: cs.CV

> 本文分析了ViT中的artifacts，发现其源于一种惰性聚合行为，并提出了一种针对这一现象的解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在大规模数据上预训练的ViTs可以提供适用于各种下游任务的通用表示，但这些模型中广泛存在的artifacts现象尚未得到充分解释。为了探究其根本机制，本文展开了系统性研究。

**Method:** 通过系统性分析，本文揭示了ViT模型中的artifacts起源于一种惰性聚合行为，即ViT利用语义无关的背景patch作为捷径来表示全局语义，这种行为是由全局注意力和粗粒度语义监督驱动的。

**Result:** 本文提出的解决方案通过有选择地将patch特征整合到CLS token中，减少了背景主导的捷径影响，并在12个基准测试中，在标签、文本和自我监督下的性能上均有提升。

**Conclusion:** 本文的研究为理解ViT的行为提供了一个新的视角，并通过实验证明了解决方案的有效性。

**Abstract:** Vision Transformers (ViTs), when pre-trained on large-scale data, provide general-purpose representations for diverse downstream tasks. However, artifacts in ViTs are widely observed across different supervision paradigms and downstream tasks. Through systematic analysis of artifacts in ViTs, we find that their fundamental mechanisms have yet to be sufficiently elucidated. In this paper, through systematic analysis, we conclude that these artifacts originate from a lazy aggregation behavior: ViT uses semantically irrelevant background patches as shortcuts to represent global semantics, driven by global attention and Coarse-grained semantic supervision. Our solution selectively integrates patch features into the CLS token, reducing the influence of background-dominated shortcuts and consistently improving performance across 12 benchmarks under label-, text-, and self-supervision. We hope this work offers a new perspective on ViT behavior.

</details>


### [9] [CLIP Is Shortsighted: Paying Attention Beyond the First Sentence](https://arxiv.org/abs/2602.22419)
*Marc-Antoine Lavoie,Anas Mahmoud,Aldo Zaimi,Arsene Fansi Tchango,Steven L. Waslander*

Main category: cs.CV

> DeBias-CLIP resolves CLIP models' bias towards simple descriptions by removing summary sentences and applying techniques to distribute attention on long captions, leading to improved performance in text-to-image retrieval tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation was to address the bias in CLIP models, which tend to focus on encoding simple descriptions and are less effective with complex scenes and detailed descriptions due to biases in the training data, such as a reliance on initial descriptions and summaries.

**Method:** The paper introduces DeBias-CLIP, a method that removes the summary sentence during the training phase and employs sentence sub-sampling and text token padding to distribute the supervision across all token positions in long captions, addressing the bias of models concentrating on initial tokens.

**Result:** DeBias-CLIP achieves state-of-the-art performance in long-text retrieval, improves on short-text retrieval, and shows reduced sensitivity to sentence order, outperforming existing methods like Long-CLIP without any increase in trainable parameters.

**Conclusion:** The conclusion is that DeBias-CLIP is an effective method for mitigating training biases in CLIP models through distribution of training supervision across all token positions, improving alignment and performance without additional parameters.

**Abstract:** CLIP models learn transferable multi-modal features via image-text contrastive learning on internet-scale data. They are widely used in zero-shot classification, multi-modal retrieval, text-to-image diffusion, and as image encoders in large vision-language models. However, CLIP's pretraining is dominated by images paired with short captions, biasing the model toward encoding simple descriptions of salient objects and leading to coarse alignment on complex scenes and dense descriptions. While recent work mitigates this by fine-tuning on small-scale long-caption datasets, we identify an important common bias: both human- and LLM-generated long captions typically begin with a one-sentence summary followed by a detailed description. We show that this acts as a shortcut during training, concentrating attention on the opening sentence and early tokens and weakening alignment over the rest of the caption. To resolve this, we introduce DeBias-CLIP, which removes the summary sentence during training and applies sentence sub-sampling and text token padding to distribute supervision across all token positions. DeBias-CLIP achieves state-of-the-art long-text retrieval, improves short-text retrieval, and is less sensitive to sentence order permutations. It is a drop-in replacement for Long-CLIP with no additional trainable parameters.

</details>


### [10] [SimpleOCR: Rendering Visualized Questions to Teach MLLMs to Read](https://arxiv.org/abs/2602.22426)
*Yibo Peng,Peng Xia,Ding Zhong,Kaide Zeng,Siwei Han,Yiyang Zhou,Jiaqi Liu,Ruiyi Zhang,Huaxiu Yao*

Main category: cs.CV

> 研究揭示了Multimodal Large Language Models存在视觉依赖问题，提出了SimpleOCR策略，改进了模型性能，实现了数据高效利用。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于诊断多模态大型语言模型在处理嵌入图像中的文本时，是真正“阅读”图像中的文本，还是仅仅依赖于文本提示中的参数捷径。

**Method:** 本研究提出了名为SimpleOCR的策略，通过将训练样本转化为Visualized-Question（VQ）格式并随机化样式，来消除基于文本的捷径，促使模型激活并优化其从图像中提取文本的路径。

**Result:** 实验表明，SimpleOCR策略在无需修改架构的情况下实现了稳健的性能提升。在四个代表性的OOD基准测试中，SimpleOCR超越了基础模型5.4%，以及基于原始图像的GRPO方法2.7%。此外，与最近基于RL的方法相比，SimpleOCR仅使用30倍较少的样本量（8.5K）即可达到更好的性能。

**Conclusion:** 研究表明多模态大型语言模型存在“模态懒惰”的问题。SimpleOCR策略展示了其在提高模型性能的同时，保持了极高的数据效率，并能够与其他先进的RL策略无缝结合，以实现互补的改进。

**Abstract:** Despite the rapid advancements in Multimodal Large Language Models (MLLMs), a critical question regarding their visual grounding mechanism remains unanswered: do these models genuinely ``read'' text embedded in images, or do they merely rely on parametric shortcuts in the text prompt? In this work, we diagnose this issue by introducing the Visualized-Question (VQ) setting, where text queries are rendered directly onto images to structurally mandate visual engagement. Our diagnostic experiments on Qwen2.5-VL reveal a startling capability-utilization gap: despite possessing strong OCR capabilities, models suffer a performance degradation of up to 12.7% in the VQ setting, exposing a deep-seated ``modality laziness.'' To bridge this gap, we propose SimpleOCR, a plug-and-play training strategy that imposes a structural constraint on the learning process. By transforming training samples into the VQ format with randomized styles, SimpleOCR effectively invalidates text-based shortcuts, compelling the model to activate and optimize its visual text extraction pathways. Empirically, SimpleOCR yields robust gains without architectural modifications. On four representative OOD benchmarks, it surpasses the base model by 5.4% and GRPO based on original images by 2.7%, while exhibiting extreme data efficiency, achieving superior performance with 30x fewer samples (8.5K) than recent RL-based methods. Furthermore, its plug-and-play nature allows seamless integration with advanced RL strategies like NoisyRollout to yield complementary improvements. Code is available at https://github.com/aiming-lab/SimpleOCR.

</details>
