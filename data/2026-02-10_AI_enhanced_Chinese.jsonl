{"id": "2602.06973", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06973", "abs": "https://arxiv.org/abs/2602.06973", "authors": ["Lucky Susanto", "Musa Izzanardi Wijanarko", "Khumaisa Nur'aini", "Farid Adilazuarda", "Alham Fikri Aji", "Derry Tanti Wijaya"], "title": "Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models", "comment": "Submitted to ARR January", "summary": "While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.", "AI": {"tldr": "研究了基于视觉的语言模型是否摆脱了分词器约束的问题。结果表明，重新引入文本分词器会使问题再现。", "motivation": "研究视觉呈现是否真的将模型从分词器约束中解耦。", "method": "通过研究印尼四种低资源本地语言（爪哇语、巴厘语、巽他语和蓝普戎语），这些语言拥有非拉丁字母的独特书写系统，我们评估了在DualGPT架构中脚本-分词器对齐的影响。", "result": "尽管使用了视觉呈现，但重新引入文本分词器会导致分词器错配问题再现，这正是基于像素的语言建模试图解决的问题。即使OOV和生育率较低，Llama 2分词器的表现仍然显著低于自定义分词器，改进达到了30.15 chrF++。", "conclusion": "文本分词器仍然是实现公平模型的重要障碍，这为未来的多模态变体提供了警告。"}}
{"id": "2602.06975", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06975", "abs": "https://arxiv.org/abs/2602.06975", "authors": ["R. James Cotton", "Thomas Leonard"], "title": "BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents", "comment": null, "summary": "Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.", "AI": {"tldr": "研究开发了BiomechAgent，使无编程技能的临床医生可以使用自然语言进行数据查询、可视化和分析，无需编写代码。实验结果表明BiomechAgent在数据检索和可视化方面有很好的表现，但在使用本地模型时性能降低。", "motivation": "使临床医生等非编程专家能够更加便捷地分析无标记运动捕捉数据，从而进行定量的运动分析。", "method": "通过自然语言查询数据库、生成可视化图表并解释数据，而不需要用户编写代码。BiomechAgent使用了领域特定的生物力学指令和经过验证的专业工具，提高了在步态事件检测等复杂空间时间分析中的准确率。同时，研究还对比了本地开放权重模型与前沿云LLM的性能差异。", "result": "BiomechAgent在数据检索和可视化任务中表现出强大的准确率，并展示了初步的临床推理能力。然而，研究发现本地模型在大多数领域中表现不佳，尤其是在复杂时空分析方面。", "conclusion": "BiomechAgent通过综合利用生物力学知识和专业工具，在一定程度上提高了无标记运动捕捉数据的可用性和易用性。"}}
{"id": "2602.06976", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.06976", "abs": "https://arxiv.org/abs/2602.06976", "authors": ["Chen Shen", "Wei Cheng", "Jingyue Yang", "Huan Zhang", "Yuhan Wu", "Wei Hu"], "title": "Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks", "comment": null, "summary": "The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.", "AI": {"tldr": "本文提出了一种名为ILA-agent的框架，该框架允许大型语言模型（LLMs）通过与有限的外部资源进行动态交互来掌握一种不熟悉的编程语言，并通过Cangjie-bench基准测试验证了其表现。", "motivation": "大型语言模型（LLMs）在编码任务中的表现通常依赖于其广泛的预训练语料库，而这一表现往往在遇到不熟悉的编程语言时会下降。本文旨在通过开发Inference-time Language Acquisition (ILA)技术来解决这一问题，使模型能够在推理时学习新语言。", "method": "本文提出了一种名为ILA-agent的框架，该框架允许大型语言模型（LLMs）通过与有限的外部资源进行动态交互来掌握一种不熟悉的编程语言。ILA-agent模仿人类行为，如探索、应用和验证语言知识，通过与官方文档和执行环境的结构化互动进行学习。", "result": "实验结果表明，ILA-agent显著优于检索增强基线，并在各种语言模型上测试了其性能。分析还指出了在不同的编程任务中模型行为的新兴模式及性能差距。", "conclusion": "ILA-agent框架展示了大型语言模型能够通过少量外部资源的学习，在面对不熟悉编程语言时也能有效掌握新知识，从而在代码生成、代码翻译和程序修复任务中表现出优越的性能。"}}
{"id": "2602.07120", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07120", "abs": "https://arxiv.org/abs/2602.07120", "authors": ["Jacqueline He", "Jonathan Hayase", "Wen-tau Yih", "Sewoong Oh", "Luke Zettlemoyer", "Pang Wei Koh"], "title": "Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model", "comment": "51 pages, 12 figures, 16 tables. Code is publicly available at https://github.com/jacqueline-he/anchored-decoding", "summary": "Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.", "AI": {"tldr": "Introduces Anchored Decoding to suppress verbatim copying in language models, reducing copyright risks while maintaining utility, tested on six models.", "motivation": "Modern language models tend to reproduce verbatim spans from their training data, posing issues of consent, compensation for creators, and compliance risks for developers when sources are sensitive or copyright-protected.", "method": "Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying, which decodes from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. It adaptively allocates a user-chosen information budget and enforces per-step constraints yielding a sequence-level guarantee, enabling a risk-utility trade-off. A byte-level variant of the method, called Anchored$_{\\mathrm{Byte}}$ Decoding, is also introduced for cross-vocabulary fusion.", "result": "Anchored and Anchored$_{\\mathrm{Byte}}$ Decoding preserve near-original fluency and factuality while reducing up to 75% of the measurable copying gap between the risky baseline and a safe reference across six model pairs.", "conclusion": "Anchored Decoding and Anchored$_{\\mathrm{Byte}}$ Decoding define a new Pareto frontier in balancing utility and risk in language models, with a modest inference overhead."}}
{"id": "2602.07006", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07006", "abs": "https://arxiv.org/abs/2602.07006", "authors": ["Alokesh Manna", "Neil Spencer", "Dipak K. Dey"], "title": "Scalable spatial point process models for forensic footwear analysis", "comment": null, "summary": "Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.", "AI": {"tldr": "研究开发一种层次贝叶斯模型以更好地量化鞋印中事故特征的稀有性，提升了法医鞋印分析的精确度和可靠性。", "motivation": "动机在于，虽然可以通过分析鞋印上的事故特征来进一步缩小嫌疑人范围，但对于量化的罕见性这项任务，现有方法能力有限。本研究希望通过改进的方法来更准确地评估鞋印证据的强度。", "method": "本研究开发了一种层次贝叶斯模型来解决量化事故特征模式稀有性的问题。研究方法的主要进步在于两方面：首先，以潜在的高斯模型框架表述方法，使推断可以通过整合嵌套拉普拉斯近似方法有效扩展到大量注释鞋印；其次，引入空间变化系数模型鞋底花纹与事故特征位置之间的关系。", "result": "相较于现有的方法，我们的方法在预留数据上的性能表现更优，这提升了法医鞋印分析的准确性与可靠性。", "conclusion": "研究得出结论，通过开发和应用层次贝叶斯模型，能够更有效地确定鞋印上的事故特征的位置和关系，从而增强法医鞋印分析的准确性和可靠性。"}}
{"id": "2602.07160", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07160", "abs": "https://arxiv.org/abs/2602.07160", "authors": ["Jiecheng Lu", "Shihao Yang"], "title": "Free Energy Mixer", "comment": "Camera-ready version. Accepted at ICLR 2026", "summary": "Standard attention stores keys/values losslessly but reads them via a per-head convex average, blocking channel-wise selection. We propose the Free Energy Mixer (FEM): a free-energy (log-sum-exp) read that applies a value-driven, per-channel log-linear tilt to a fast prior (e.g., from queries/keys in standard attention) over indices. Unlike methods that attempt to improve and enrich the $(q,k)$ scoring distribution, FEM treats it as a prior and yields a value-aware posterior read at unchanged complexity, smoothly moving from averaging to per-channel selection as the learnable inverse temperature increases, while still preserving parallelism and the original asymptotic complexity ($O(T^2)$ for softmax; $O(T)$ for linearizable variants). We instantiate a two-level gated FEM that is plug-and-play with standard and linear attention, linear RNNs and SSMs. It consistently outperforms strong baselines on NLP, vision, and time-series at matched parameter budgets.", "AI": {"tldr": "This paper introduces the Free Energy Mixer (FEM) which improves upon standard attention by treating the $(q,k)$ distribution as a prior and applying a value-driven posterior read that can smoothly transition between averaging and per-channel selection, resulting in performance improvements while maintaining complexity and parallelism.", "motivation": "The motivation is to address the issue in standard attention where keys/values are stored without loss but accessed through a per-head convex average, which prevents channel-wise selection. FEM aims to improve the $(q,k)$ scoring distribution by treating it as a prior and yields a more flexible and value-aware posterior read.", "method": "We propose the Free Energy Mixer (FEM), which uses a free-energy (log-sum-exp) read with a value-driven, per-channel log-linear tilt, operating on a fast prior such as queries/keys in standard attention. This method allows for a value-aware posterior read and can move from averaging to per-channel selection by increasing the learnable inverse temperature.", "result": "The two-level gated FEM outperforms strong baselines in NLP, vision, and time-series tasks when parameter budgets are matched.", "conclusion": "FEM offers a plug-and-play improvement to standard attention mechanisms, maintaining parallelism and original complexity while implementing a smoother transition to channel-wise selection and outperforming standard approaches in various tasks."}}
{"id": "2602.07008", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07008", "abs": "https://arxiv.org/abs/2602.07008", "authors": ["Ruoyu Chen", "Shangquan Sun", "Xiaoqing Guo", "Sanyi Zhang", "Kangwei Liu", "Shiming Liu", "Zhangcheng Wang", "Qunli Zhang", "Hua Zhang", "Xiaochun Cao"], "title": "Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making", "comment": null, "summary": "Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.", "AI": {"tldr": "该研究提出了一种将人类先验知识融入模型决策过程的方法，通过惩罚偏离预期证据的情况，提高了模型决策准确性与合理性。", "motivation": "传统监督学习往往只能提供分类级别的标签，允许模型通过捷径相关性而非预期证据来实现高准确性。人类的先验知识可以帮助限制这种行为，但将模型行为对齐到这些先验知识一直是个挑战。", "method": "我们提出了一种基于属性的人类先验对齐方法。通过将人类先验编码为模型应依赖的输入区域（例如，边界框），并利用一种高保真的子集选择属性方法来暴露模型在训练过程中的决策证据。当属性区域显著偏离先验区域时，我们对偏离先验的证据施加惩罚，鼓励模型的属性向预期区域靠拢。", "result": "我们在图像分类任务和基于多模态大模型的GUI代理模型的点击决策任务上验证了我们的方法。结果表明，在传统分类和自回归生成设置中，人类先验对齐方法都能提高任务准确性，并增强模型决策的合理性。", "conclusion": "我们的方法证明，通过结合替代性先验知识并用以约束模型决策，可以使机器学习模型不仅预测准确，还能提供合理的决策依据，在多种任务上取得了更好的效果和合理性。"}}
{"id": "2602.07164", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07164", "abs": "https://arxiv.org/abs/2602.07164", "authors": ["Ruimeng Ye", "Zihan Wang", "Zinan Ling", "Yang Xiao", "Manling Li", "Xiaolong Ma", "Bo Hui"], "title": "Your Language Model Secretly Contains Personality Subnetworks", "comment": "ICLR 2026", "summary": "Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.", "AI": {"tldr": "研究发现大型语言模型自身参数空间中存在与特定人格特征相关的子网络，通过轻量级的掩码策略和对比剪枝方法，能够在不需要额外训练的情况下实现人格特征的分离与个性化，这种方法比依赖外部知识的方法效果更好且更有效。", "motivation": "研究动机在于探讨大型语言模型是否可以通过其已有的参数空间，而不需要额外的外部知识或参数，就能表达出不同的行为特性。", "method": "本研究利用少量校准数据集，识别与不同人格特征相关的不同激活特征，并基于这些统计特征开发了一种掩码策略，以隔离轻量级人格子网络。另外，提出了对比剪枝策略来增强对立人格特征的分离。", "result": "研究结果表明，大型语言模型参数空间本身就包含针对不同人格特征的子网络，这种轻量级人格子网络在多样化的评估设置中表现出比需要外部知识的方法更强的人格一致性，且更为高效。", "conclusion": "该工作指出，大型语言模型内嵌了多样性的、类似人类的行为，这为可控制和可解释的大语言模型个性化提供了新的视角。"}}
{"id": "2602.07011", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07011", "abs": "https://arxiv.org/abs/2602.07011", "authors": ["Zhuonan Wang", "Zhenxuan Fan", "Siwen Tan", "Yu Zhong", "Yuqian Yuan", "Haoyuan Li", "Hao Jiang", "Wenqiao Zhang", "Feifei Shao", "Hongwei Wang", "Jun Xiao"], "title": "MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation", "comment": "9 pages, 5 figures", "summary": "As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.", "AI": {"tldr": "This paper introduces MAU-Set, a multi-domain dataset for industrial anomaly detection, and MAU-GPT, a domain-adapted multimodal model with enhanced defect recognition capabilities.", "motivation": "To overcome the limitations of current industrial quality control methods, which are constrained by limited dataset coverage and poor generalization across various anomaly types.", "method": "The authors have created a comprehensive dataset (MAU-Set) with a hierarchical task structure for different levels of anomaly understanding tasks. They also developed MAU-GPT, a large multimodal model with a novel AMoE-LoRA mechanism to improve anomaly-aware and generalist expert adaptation.", "result": "MAU-GPT demonstrated superior performance over existing state-of-the-art methods in industrial anomaly detection across multiple domains.", "conclusion": "The results suggest that MAU-GPT has significant potential for improving the scalability and automation of industrial quality control systems."}}
{"id": "2602.07176", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07176", "abs": "https://arxiv.org/abs/2602.07176", "authors": ["Mohamed El Hajji", "Tarek Ait Baha", "Aicha Dakir", "Hammou Fadili", "Youssef Es-Saady"], "title": "Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI", "comment": "19 pages, 15 figures", "summary": "Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.", "AI": {"tldr": "本文提出了一种开源教育平台Open TutorAI，它基于先进的人工智能技术，提供个性化辅导，增强学习参与度和情感存在感，支持自我调节学习。", "motivation": "当前教育聊天机器人系统缺乏语境适应性、实时响应性和教学灵活性，这限制了学习者参与度并减弱了教学效果。因此，需要一个开放集成的平台，结合AI和沉浸式技术来支持个性化和有意义的学习体验。", "method": "本文介绍了Open TutorAI，这是一个基于大型语言模型(LLMs)和生成技术的开源教育平台，旨在提供动态和个性化的辅导。平台利用自然语言处理和可定制的3D头像，促进多元化的学习互动。", "result": "该平台集成了组织内容的工具、嵌入式反馈系统以及供学习者、教育者和家长使用的特定接口，支持适应性支持，响应个体学习者概况，且无需技术专业知识。", "conclusion": "Open TutorAI结合了模块化架构、生成AI和学习者分析，在开放框架内促进了下一代智能辅导系统的发展。"}}
{"id": "2602.07012", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07012", "abs": "https://arxiv.org/abs/2602.07012", "authors": ["Zhonghua Wang", "Lie Ju", "Sijia Li", "Wei Feng", "Sijin Zhou", "Ming Hu", "Jianhao Xiong", "Xiaoying Tang", "Yifan Peng", "Mingquan Lin", "Yaodong Ding", "Yong Zeng", "Wenbin Wei", "Li Dong", "Zongyuan Ge"], "title": "A General Model for Retinal Segmentation and Quantification", "comment": null, "summary": "Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.", "AI": {"tldr": "RetSAM框架通过多阶段策略训练，在大量视网膜图像基础上，实现了稳健的多目标分割、标准化生物标志物提取，超越现有方法，在多个公共数据集上表现优异，并支持大规模眼科学研究。", "motivation": "视网膜成像无创快速，能够提供量化的结构和血管信号，用以评估眼部和全身健康，但由于多标签数据集的匮乏和缺乏统一的分割到量化流水线，这种分析难以大规模进行。", "method": "RetSAM是一个针对眼底成像的视网膜分割和量化框架，通过多阶段策略使用私有和公开眼底图像训练，支持三类任务，分割五种解剖结构、四种视网膜表型模式和20种以上病灶类型。", "result": "RetSAM在17个公开数据集上实现了优于先前方法的表现，平均DSC提高了3.9个百分点，在具有挑战性的多任务基准测试中甚至提高了15个百分点，能很好地推广到不同人群、成像设备和临床环境中。", "conclusion": "RetSAM将眼底图像转化为标准化、可解读的定量表型，支持大规模眼科学研究和转化。"}}
{"id": "2602.07181", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07181", "abs": "https://arxiv.org/abs/2602.07181", "authors": ["Tianyu Zhao", "Siqi Li", "Yasser Shoukry", "Salma Elmalaki"], "title": "Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs", "comment": null, "summary": "User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.", "AI": {"tldr": "研究发现，通过对用户人格特征进行分析，选择与人格特征一致的偏好可以显著提高个性化问答的质量，从29.25%提高到76%。", "motivation": "研究动机是探索更稳定的方法来利用用户偏好信号，改善大型语言模型（LLM）的个性化回答，解决偏好信号不稳定的问题。", "method": "通过实验观察到稳定的人格特质影响日常偏好，因此研究人格作为偏好陈述背后的潜在信号，并推出了PACIFIC框架。", "result": "当选择与用户推测人格相符的偏好而不是随机选择时，回答准确性从29.25%提高到了76%。", "conclusion": "提出了一个可以使LLM模型自动检索人格一致的偏好并在回答生成中整合的框架。"}}
{"id": "2602.07013", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07013", "abs": "https://arxiv.org/abs/2602.07013", "authors": ["Jiaxi Yang", "Shicheng Liu", "Yuchen Yang", "Dongwon Lee"], "title": "Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models", "comment": null, "summary": "With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \\textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \\textbf{C}onfigurable \\textbf{R}efusal in \\textbf{VLM}s (\\textbf{CR-VLM}), a robust and efficient approach for {\\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.", "AI": {"tldr": "本文提出了CR-VLM，一种在视觉语言模型中实现可配置拒绝的鲁棒且高效的方法。", "motivation": "由于现有的拒绝策略大多是“一刀切”，不能适应多样化的用户需求和上下文约束，导致拒绝的不足或过度。本文开发了CR-VLM，以解决上述挑战。", "method": "CR-VLM包含三个集成组件：(1)通过教师强制机制提取可配置的拒绝向量来放大拒绝信号；(2)引入一个门控机制来缓解过度拒绝，同时保留对范围内的查询的接受；(3)设计反事实视觉增强模块，使视觉表示与拒绝要求对齐。", "result": "全面的实验显示，CR-VLM在多个数据集和各种视觉语言模型中实现了有效、高效且稳健的可配置拒绝，为视觉语言模型中的用户自适应安全对齐提供了一个可扩展的路径。", "conclusion": "通过CR-VLM，论文证明了在视觉语言模型中实现有效、高效且稳健的可配置拒绝是可能的，这为未来用户自适应的安全对齐提供了解决方案。"}}
{"id": "2602.07190", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07190", "abs": "https://arxiv.org/abs/2602.07190", "authors": ["Anagha Kulkarni", "Parin Rajesh Jhaveri", "Prasha Shrestha", "Yu Tong Han", "Reza Amini", "Behrouz Madahian"], "title": "Long-Context Long-Form Question Answering for Legal Domain", "comment": "EACL 2026", "summary": "Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.", "AI": {"tldr": "为了解决法律文件中长上下文问答的挑战，本文提出了一种问答系统，该系统改进了词汇检索、文档布局解析和答案生成的技术，并通过专家知识库进行了性能评估。", "motivation": "法律文件具有复杂的文档布局，包含多个嵌套部分、冗长的脚注，并使用特殊的语言机制如复杂的句法和领域专用词汇来确保精确性和权威性。这些法律文件的固有特性使得问答挑战重重，特别是在答案跨多个页面（即需要长上下文）并且要求全面（即长形式答案）的情况下。", "method": "提出了一种问答系统，该系统能够（a）分解领域特定的词汇以便更好地从源文档中检索，（b）解析复杂的文档布局，同时隔离部分并适当链接它们，（c）使用精确的领域特定词汇生成综合答案。我们还引入了一个覆盖率度量指标，用来将性能划分为以召回为基础的覆盖率类别，使人类用户能够轻松评估召回率。", "result": "我们通过综合实验和消融研究，证明了所提出系统的实用性和优越性。", "conclusion": "实验结果和消融研究表明，该系统在应对法律文件中的问答挑战方面表现出色，特别在处理长上下文和长篇幅答案的需求方面。"}}
{"id": "2602.07014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07014", "abs": "https://arxiv.org/abs/2602.07014", "authors": ["Qingyu Wu", "Yuxuan Han", "Haijun Li", "Zhao Xu", "Jianshan Zhao", "Xu Jin", "Longyue Wang", "Weihua Luo"], "title": "Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation", "comment": null, "summary": "In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.", "AI": {"tldr": "本文介绍了Vectra，一个首创的、无参考的、基于多模态大规模语言模型（MLLM）的电子商务中基于图像的机器翻译（IIMT）视觉质量评估框架，包含评分体系、数据集和模型三部分。实验表明，Vectra与人类排名的相关性达到了最先进的水平。", "motivation": "现有技术在处理密集上下文的产品图像和多模态缺陷时无法提供解释性，因此本文提出了Vectra来满足电子商务中IIMT的视觉质量评估需求。", "method": " Vectra包含三个部分：1) Vectra评分体系，一个将视觉质量分解为14个可解释维度的评分系统，并且能够量化缺陷区域比（DAR）以减少注释的不确定性；2) Vectra数据集，由110万张现实世界产品图像通过多样性的抽取方式构建而成；3) Vectra模型，一个40亿参数的MLLM，能够生成定量评分和诊断性推理。", "result": "实验结果显示，Vectra在评分性能上超过了包括GPT-5和Gemini-3在内的领先MLLM。", "conclusion": "该研究为电子商务中的IIMT任务提供了一种新的视觉质量评估方法，并通过实验验证了其有效性。"}}
