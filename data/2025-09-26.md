<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 16]
- [cs.CV](#cs.CV) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

> 本文提出了一种新的框架，用于通过修改外交事件文本描述来改善公众情绪。通过训练语言模型预测公众反应，并采用反事实生成算法来修改文本，该框架能将公众情绪正面转换的成功率达到了70%。

<details>
  <summary>Details</summary>

**Motivation:** 传统的方法，例如大规模调查或手动分析媒体内容，用于衡量公众情绪通常是费时、劳动密集型的，并且缺乏前瞻性分析的能力。因此，需要一种新的方法来快速有效地进行公众情绪分析和预测。

**Method:** 我们提出了一种新的框架，能够识别特定修改以改变外交事件叙事，从而将公众情绪从消极转变为中性或积极。该框架首先训练了一个语言模型来预测公众对外交事件的反应，为此构建了一个包含外交事件描述及其相关公众讨论的数据集。其次，依据交流理论和领域专家的指导，预先确定了几种文本特征进行修改，确保任何更改改变了事件的叙事框架，同时保留其核心事实。此外，我们开发了一种反事实生成算法，它使用大型语言模型系统地生成原始文本的修改版本。

**Result:** 实验结果表明，该框架能将公众情绪转变为更加有利的状态的成功率为70%。

**Conclusion:** 这一框架可以作为外交官、政策制定者和传播专家的实用工具，提供数据驱动的见解，以优化外交倡议或事件报道，促进更理想化的公众情绪。

**Abstract:** Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [2] [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
*Shreya G. Upadhyay,Carlos Busso,Chi-Chun Lee*

Main category: cs.CL

> 本文提出了一种通过构建基于图的说话人社区并利用双空间锚定技术来提升跨语言情感识别性能的方法，实验结果表明该方法有效提高了情感识别的跨语言泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前跨语言语音情感识别存在由于语言间发音变异性及说话人表达风格差异导致的挑战。

**Method:** 方法是基于图的聚类构建共享说话人特征的社区，并在说话人和音素空间应用双空间锚定技术。

**Result:** 在MSP-Podcast与BIIC-Podcast语料库上的评估结果显示，提出的方法比竞争基准方法具有更好的跨语言泛化性能。

**Conclusion:** 研究表明，该方法能够有效地提高跨语言情感表达的一致性和情感转移性能，为解决跨语言情感识别问题提供了一种有效方案。

**Abstract:** Cross-lingual speech emotion recognition (SER) remains a challenging task due
to differences in phonetic variability and speaker-specific expressive styles
across languages. Effectively capturing emotion under such diverse conditions
requires a framework that can align the externalization of emotions across
different speakers and languages. To address this problem, we propose a
speaker-style aware phoneme anchoring framework that aligns emotional
expression at the phonetic and speaker levels. Our method builds
emotion-specific speaker communities via graph-based clustering to capture
shared speaker traits. Using these groups, we apply dual-space anchoring in
speaker and phonetic spaces to enable better emotion transfer across languages.
Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)
corpora demonstrate improved generalization over competitive baselines and
provide valuable insights into the commonalities in cross-lingual emotion
representation.

</details>


### [3] [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
*Nithin Somasekharan,Ling Yue,Yadi Cao,Weichao Li,Patrick Emami,Pochinapeddi Sai Bhargav,Anurag Acharya,Xingyu Xie,Shaowu Pan*

Main category: cs.CL

> 研究提出了CFDLLMBench，一个旨在全面评估大型语言模型在计算流体动力学（CFD）领域性能的基准测试套件。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在自然语言处理任务中表现出色，但对于复杂物理系统自动化数值实验的能力却鲜有探索。鉴于CFD作为过去几十年计算科学的主要工作，它提供了一个独特的测试平台来评估大型语言模型的科学能力。

**Method:** 本研究引入了CFDLLMBench基准测试套件，该套件包含三个互补组件——CFDQuery、CFDCodeBench和FoamBench，旨在全面评估模型在CFD领域的三个关键能力：研究生层次的CFD知识、CFD的数值和物理推理，以及上下文相关的工作流实现能力。

**Result:** 基准测测试结合了详细的任务分类与严谨的评估框架，旨在提供可重复的结果，并量化大型语言模型在代码可执行性、解精度和数值收敛行为上的表现。

**Conclusion:** CFDLLMBench为开发和评估全自动化复杂物理系统数值实验的大型语言模型奠定了坚实的基础。

**Abstract:** Large Language Models (LLMs) have demonstrated strong performance across
general NLP tasks, but their utility in automating numerical experiments of
complex physical system -- a critical and labor-intensive component -- remains
underexplored. As the major workhorse of computational science over the past
decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging
testbed for evaluating the scientific capabilities of LLMs. We introduce
CFDLLMBench, a benchmark suite comprising three complementary components --
CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM
performance across three key competencies: graduate-level CFD knowledge,
numerical and physical reasoning of CFD, and context-dependent implementation
of CFD workflows. Grounded in real-world CFD practices, our benchmark combines
a detailed task taxonomy with a rigorous evaluation framework to deliver
reproducible results and quantify LLM performance across code executability,
solution accuracy, and numerical convergence behavior. CFDLLMBench establishes
a solid foundation for the development and evaluation of LLM-driven automation
of numerical experiments for complex physical systems. Code and data are
available at https://github.com/NREL-Theseus/cfdllmbench/.

</details>


### [4] [Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text](https://arxiv.org/abs/2509.20375)
*Sharanya Parimanoharan,Ruwan D. Nawarathna*

Main category: cs.CL

> 研究比较了多种机器学习方法在识别AI生成文本上的效果，发现DistilBERT最优，集成方法未超越个体模型。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大型语言模型（如ChatGPT）带来的学术诚信、知识产权和传播错误信息等紧迫问题，此研究致力于可靠地检测AI生成的文本，以保障人类创作的真实性，并在数字通信中建立信任。

**Method:** 研究采用了经典机器学习方法（如套用了传统词袋模型、词性标注、TF-IDF特征的逻辑回归）和基于变压器的方法（如套用了N-grams的BERT、DistilBERT、带轻量级自定义分类器的BERT、基于LSTM的N-gram模型）来区分ChatGPT-3.5生成的文本与人类撰写的文本。

**Result:** 结果显示，DistilBERT整体表现最佳，逻辑回归和自定义BERT模型也有稳健均衡的表现；相比之下，基于BERT和LSTM的N-gram方法表现落后。最佳三模型的最大投票集成未能超越DistilBERT，这强调了单个基于变压器的方法的重要性。

**Conclusion:** 此研究全面评估了几种AI文本检测方法的优势和不足，为构建更强大的基于变压器框架奠定了基础，此类框架需要结合更大、更丰富的数据集以跟上持续改进的AI生成模型的步伐。

**Abstract:** The rapid adoption of large language models (LLMs) such as ChatGPT has
blurred the line between human and AI-generated texts, raising urgent questions
about academic integrity, intellectual property, and the spread of
misinformation. Thus, reliable AI-text detection is needed for fair assessment
to safeguard human authenticity and cultivate trust in digital communication.
In this study, we investigate how well current machine learning (ML) approaches
can distinguish ChatGPT-3.5-generated texts from human-written texts employing
a labeled data set of 250 pairs of abstracts from a wide range of research
topics. We test and compare both classical (Logistic Regression armed with
classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT
augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,
and LSTM-based N-gram models) ML detection techniques. As we aim to assess each
model's performance in detecting AI-generated research texts, we also aim to
test whether an ensemble of these models can outperform any single detector.
Results show DistilBERT achieves the overall best performance, while Logistic
Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and
BERT-N-gram approaches lag. The max voting ensemble of the three best models
fails to surpass DistilBERT itself, highlighting the primacy of a single
transformer-based representation over mere model diversity. By comprehensively
assessing the strengths and weaknesses of these AI-text detection approaches,
this work lays a foundation for more robust transformer frameworks with larger,
richer datasets to keep pace with ever-improving generative AI models.

</details>


### [5] [ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models](https://arxiv.org/abs/2509.20376)
*Haoxuan Li,Zhen Wen,Qiqi Jiang,Chenxiao Li,Yuwei Wu,Yuchen Yang,Yiyao Wang,Xiuqi Huang,Minfeng Zhu,Wei Chen*

Main category: cs.CL

> ConceptViz是一种可视分析系统，通过探索LLMs中的概念来解决LLM特性和人类可理解概念之间的对齐问题。

<details>
  <summary>Details</summary>

**Motivation:** 虽然稀疏自动编码器已经作为提取LLMs中可解释特性的有前途的技术出现，但是这些特性并不与人类可理解的概念对齐，这使得它们的解释困难且劳动密集。为了弥合SAE特征与人类概念之间的差距。

**Method:** ConceptViz是一种可视分析系统，旨在探索大语言模型（LLMs）中的概念。它实现了一个新颖的识别、解释和验证管道，允许用户用感兴趣的条款查询稀疏自动编码器（SAEs），互动探索概念与特性之间的对应，验证通过模型行为确认这些对应。

**Result:** 通过两个使用场景和一个用户研究，证明了ConceptViz的有效性，它可以通过流线型发现和验证在LLMs有意义的概念表示来提高解释性研究。

**Conclusion:** ConceptViz增强了解释性研究，使研究人员可以建立更准确的关于LLM特性的心理模型，促进了LLM特性和人类概念之间对齐的理解。代码和用户指南公开可用。

**Abstract:** Large language models (LLMs) have achieved remarkable performance across a
wide range of natural language tasks. Understanding how LLMs internally
represent knowledge remains a significant challenge. Despite Sparse
Autoencoders (SAEs) have emerged as a promising technique for extracting
interpretable features from LLMs, SAE features do not inherently align with
human-understandable concepts, making their interpretation cumbersome and
labor-intensive. To bridge the gap between SAE features and human concepts, we
present ConceptViz, a visual analytics system designed for exploring concepts
in LLMs. ConceptViz implements a novel dentification => Interpretation =>
Validation pipeline, enabling users to query SAEs using concepts of interest,
interactively explore concept-to-feature alignments, and validate the
correspondences through model behavior verification. We demonstrate the
effectiveness of ConceptViz through two usage scenarios and a user study. Our
results show that ConceptViz enhances interpretability research by streamlining
the discovery and validation of meaningful concept representations in LLMs,
ultimately aiding researchers in building more accurate mental models of LLM
features. Our code and user guide are publicly available at
https://github.com/Happy-Hippo209/ConceptViz.

</details>


### [6] [SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20377)
*Tomoaki Isoda*

Main category: cs.CL

> 研究提出了一种使用模型自知识来改进RAG方法的新技术SKILL-RAG，提高了生成质量并减少了输入文档数量。

<details>
  <summary>Details</summary>

**Motivation:** 由于检索系统可能会返回不相关的内容，将其纳入模型中可能导致幻觉。因此，识别和过滤出无用的检索内容是提高RAG性能的关键挑战。

**Method:** 提出了一种名为SKILL-RAG的新方法，该方法利用模型的自知识来确定哪些检索到的文档对回答给定的查询是有效的。设计了一个基于强化学习的训练框架，以明确地从模型中提取自知识，并使用句子级别的细粒度来过滤掉无关内容，同时保留有用的知识。

**Result:** 实验结果表明，SKILL-RAG不仅提高了生成质量，还显著减少了输入文档的数量，这验证了自知识在指导高质量检索选择方面的重要性。

**Conclusion:** 基于强化学习的SKILL-RAG方法通过利用模型的自知识来提高检索增强生成任务的性能，展示了在不需要大量输入文档的情况下提升生成质量的能力。

**Abstract:** Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive tasks in
recent years. However, since retrieval systems may return irrelevant content,
incorporating such information into the model often leads to hallucinations.
Thus, identifying and filtering out unhelpful retrieved content is a key
challenge for improving RAG performance.To better integrate the internal
knowledge of the model with external knowledge from retrieval, it is essential
to understand what the model "knows" and "does not know" (which is also called
"self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge
Induced Learning and Filtering for RAG), a novel method that leverages the
model's self-knowledge to determine which retrieved documents are beneficial
for answering a given query. We design a reinforcement learning-based training
framework to explicitly elicit self-knowledge from the model and employs
sentence-level granularity to filter out irrelevant content while preserving
useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several
question answering benchmarks. Experimental results demonstrate that SKILL-RAG
not only improves generation quality but also significantly reduces the number
of input documents, validating the importance of self-knowledge in guiding the
selection of high-quality retrievals.

</details>


### [7] [Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation](https://arxiv.org/abs/2509.20378)
*Sirui Wang,Andong Chen,Tiejun Zhao*

Main category: cs.CL

> 本文介绍了Emo-FiLM，一种能够处理细粒度情感动态变化的文本转语音系统，实验结果显示其在全局和局部情感控制方面均优于现有技术。

<details>
  <summary>Details</summary>

**Motivation:** 传统的文本转语音系统通常依赖于预定义标签、参考音频或自然语言提示对整句进行情感控制，但这种方法无法捕捉到句内情感的瞬息变化。为了弥补这一不足，提出了Emo-FiLM框架。

**Method:** Emo-FiLM, 一个基于LLM的TTS的细粒度情感建模框架，通过将emotion2vec的帧级特征与词汇对齐以获取词汇级情感注释，并通过特征级线性调制（FiLM）层直接调制文本嵌入，实现词汇级情感控制。

**Result:** 实验表明，Emo-FiLM在全局和细粒度任务上均优于现有方法，证明了其在表达性语音合成中的有效性和通用性。

**Conclusion:** Emo-FiLM成功解决了现有系统无法捕捉句子内部情感变化的局限性，显著提升了情感文本转语音的质量和自然度。

**Abstract:** Emotional text-to-speech (E-TTS) is central to creating natural and
trustworthy human-computer interaction. Existing systems typically rely on
sentence-level control through predefined labels, reference audio, or natural
language prompts. While effective for global emotion expression, these
approaches fail to capture dynamic shifts within a sentence. To address this
limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework
for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to
words to obtain word-level emotion annotations, and maps them through a
Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion
control by directly modulating text embeddings. To support evaluation, we
construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed
annotations of emotional transitions. Experiments show that Emo-FiLM
outperforms existing approaches on both global and fine-grained tasks,
demonstrating its effectiveness and generality for expressive speech synthesis.

</details>


### [8] [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
*Jianyu Wen,Jingyun Wang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Ying Zhang*

Main category: cs.CL

> 提出了一种基于用户模拟器的培训推理框架(USB-Rec)，用于提高LLMs在会话推荐中的性能。该框架包括偏好优化数据集构建策略和自我增强策略。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于大型语言模型(LLMs)的方法主要集中在如何利用LLMs的总结和分析能力，而忽视了训练问题。本文旨在解决这一问题，提高LLMs在会话推荐系统中的性能。

**Method:** Structure

**Result:** 在各种数据集上的广泛实验表明，所提出的方法始终优于之前的方法。

**Conclusion:** 通过综合的培训推理框架，本文改进了LLMs在会话推荐系统中性能的方法是可取的。

**Abstract:** Recently, Large Language Models (LLMs) have been widely employed in
Conversational Recommender Systems (CRSs). Unlike traditional language model
approaches that focus on training, all existing LLMs-based approaches are
mainly centered around how to leverage the summarization and analysis
capabilities of LLMs while ignoring the issue of training. Therefore, in this
work, we propose an integrated training-inference framework,
User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs
in conversational recommendation at the model level. Firstly, we design a
LLM-based Preference Optimization (PO) dataset construction strategy for RL
training, which helps the LLMs understand the strategies and methods in
conversational recommendation. Secondly, we propose a Self-Enhancement Strategy
(SES) at the inference stage to further exploit the conversational
recommendation potential obtained from RL training. Extensive experiments on
various datasets demonstrate that our method consistently outperforms previous
state-of-the-art methods.

</details>


### [9] [Document Summarization with Conformal Importance Guarantees](https://arxiv.org/abs/2509.20461)
*Bruce Kuwahara,Chen-Yuan Lin,Xiao Shi Huang,Kin Kwan Leung,Jullian Arta Yapeter,Ilya Stanevich,Felipe Perez,Jesse C. Cresswell*

Main category: cs.CL

> 研究介绍了一种名为Conformal Importance Summarization的框架，这种框架使用符合性预测来创建保持重要性摘要，可以在高风险领域提供必要的覆盖保证。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大语言模型在自动摘要系统中取得了快速进步，但在医疗保健、法律和金融等高风险领域，它们仍缺乏可靠保证关键内容被包含在内。为了解决这一问题，提出了此框架。

**Method:** 本研究介绍了一种名为Conformal Importance Summarization的框架，该框架使用了符合性预测，可以在抽取文档摘要时提供严格的、无分布的覆盖保证。通过校准句子重要性评分的阈值，使得提取概括能够达到用户指定的关键内容覆盖和召回率。此方法与模型无关，仅需少量校准集，且可以无缝集成到现有的黑盒大语言模型中。

**Result:** 在已有的摘要基准测试中，Conformal Importance Summarization实现了理论上保证的信息覆盖率。

**Conclusion:** 本研究展示了Conformal Importance Summarization框架与现有技术结合，可以实现可靠的、可控的自动摘要，为在关键应用中部署AI自动摘要工具铺平了道路。

**Abstract:** Automatic summarization systems have advanced rapidly with large language
models (LLMs), yet they still lack reliable guarantees on inclusion of critical
content in high-stakes domains like healthcare, law, and finance. In this work,
we introduce Conformal Importance Summarization, the first framework for
importance-preserving summary generation which uses conformal prediction to
provide rigorous, distribution-free coverage guarantees. By calibrating
thresholds on sentence-level importance scores, we enable extractive document
summarization with user-specified coverage and recall rates over critical
content. Our method is model-agnostic, requires only a small calibration set,
and seamlessly integrates with existing black-box LLMs. Experiments on
established summarization benchmarks demonstrate that Conformal Importance
Summarization achieves the theoretically assured information coverage rate. Our
work suggests that Conformal Importance Summarization can be combined with
existing techniques to achieve reliable, controllable automatic summarization,
paving the way for safer deployment of AI summarization tools in critical
applications. Code is available at
https://github.com/layer6ai-labs/conformal-importance-summarization.

</details>


### [10] [ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos](https://arxiv.org/abs/2509.20467)
*Henrik Vatndal,Vinay Setty*

Main category: cs.CL

> 研究开发了ShortCheck，一个能够自动识别需要核查的短视频的系统，旨在提高事实核查效率。

<details>
  <summary>Details</summary>

**Motivation:** 由于短视频平台（如TikTok）内容的多模态、动态和杂乱特性，它们为错误信息检测带来了独特的挑战。

**Method:** ShortCheck, 一个模块化、仅推理的管道，包含语音转录、OCR、对象和深度伪造检测、视频到文本摘要和声明验证等功能，以识别值得核查的短视频，帮助人工事实核查员。

**Result:** 该系统在一个多语言设置的两个手动注释数据集上进行了验证，其F1加权评分超过70%。

**Conclusion:** ShortCheck系统展示了在识别和验证值得核查的短视频方面的潜力，为处理短视频平台上的虚假信息提供了有力的工具。

**Abstract:** Short-form video platforms like TikTok present unique challenges for
misinformation detection due to their multimodal, dynamic, and noisy content.
We present ShortCheck, a modular, inference-only pipeline with a user-friendly
interface that automatically identifies checkworthy short-form videos to help
human fact-checkers. The system integrates speech transcription, OCR, object
and deepfake detection, video-to-text summarization, and claim verification.
ShortCheck is validated by evaluating it on two manually annotated datasets
with TikTok videos in a multilingual setting. The pipeline achieves promising
results with F1-weighted score over 70\%.

</details>


### [11] [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
*Xiao Wang,Jia Wang,Yijie Wang,Pengtao Dang,Sha Cao,Chi Zhang*

Main category: cs.CL

> 本文提出了MARS系统，它通过减少代理之间的交互，提高了推理质量，并在保持准确性的同时减少了计算资源的消耗。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服现有的多代理辩论方法在计算上的高开销，同时提高推理质量。

**Method:** 本文提出了一种基于角色的合作框架MARS，其中作者代理生成初始解决方案，审稿人代理独立提供决策和意见，元审稿人集成反馈以做出最终决策并指导进一步修订。

**Result:** 实验结果显示，MARS在多个基准上达到了与MAD相当的准确性，同时将token使用量和推理时间减少了大约一半。

**Conclusion:** MARS是一种有效的方法，可以在减少计算成本的同时提高多代理系统的推理质量。

**Abstract:** Large language models (LLMs) have achieved impressive results in natural
language understanding, yet their reasoning capabilities remain limited when
operating as single agents. Multi-Agent Debate (MAD) has been proposed to
address this limitation by enabling collaborative reasoning among multiple
models in a round-table debate manner. While effective, MAD introduces
substantial computational overhead due to the number of agents involved and the
frequent communication required. In this paper, we propose MARS (Multi-Agent
Review System), a role-based collaboration framework inspired by the review
process. In MARS, an author agent generates an initial solution, reviewer
agents provide decisions and comments independently, and a meta-reviewer
integrates the feedback to make the final decision and guide further revision.
This design enhances reasoning quality while avoiding costly
reviewer-to-reviewer interactions, thereby controlling token consumption and
inference time. We compared MARS with both MAD and other state-of-the-art
reasoning strategies across multiple benchmarks. Extensive experiments with
different LLMs show that MARS matches the accuracy of MAD while reducing both
token usage and inference time by approximately 50\%. Code is available at
https://github.com/xwang97/MARS.

</details>


### [12] [SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages](https://arxiv.org/abs/2509.20557)
*Hannah Liu,Junghyun Min,Ethan Yue Heng Cheung,Shou-Yi Hung,Syed Mekael Wasti,Runtong Liang,Shiyao Qian,Shizhao Zheng,Elsie Chan,Ka Ieng Charlotte Lo,Wing Yu Yip,Richard Tzong-Han Tsai,En-Shiun Annie Lee*

Main category: cs.CL

> 本文介绍了SiniticMTError数据集，该数据集对英语到三门不同中文方言的机器翻译错误进行全面标注，以支持低资源语言翻译研究和错误检测能力提升。

<details>
  <summary>Details</summary>

**Motivation:** 尽管近年来机器翻译取得了显著进展，但对于缺乏大规模训练数据和语言资源的低资源语言，进展仍然有限。因此，本文旨在通过引入SiniticMTError数据集来改善这一状况。

**Method:** 本文介绍了SiniticMTError数据集的创建方法，该数据集针对英语到粤语、吴语和普通话的机器翻译错误进行标注，包括错误跨度、错误类型和错误严重程度。

**Result:** 该数据集为机器翻译社区提供了资源，可用于微调具有错误检测能力的模型，支持翻译质量评估、错误感知生成和低资源语言评估的研究。

**Conclusion:** 通过母语者的严格标注过程，本文提供了关于注释者之间的一致性、迭代反馈以及错误类型和严重程度模式的分析。

**Abstract:** Despite major advances in machine translation (MT) in recent years, progress
remains limited for many low-resource languages that lack large-scale training
data and linguistic resources. Cantonese and Wu Chinese are two Sinitic
examples, although each enjoys more than 80 million speakers around the world.
In this paper, we introduce SiniticMTError, a novel dataset that builds on
existing parallel corpora to provide error span, error type, and error severity
annotations in machine-translated examples from English to Mandarin, Cantonese,
and Wu Chinese. Our dataset serves as a resource for the MT community to
utilize in fine-tuning models with error detection capabilities, supporting
research on translation quality estimation, error-aware generation, and
low-resource language evaluation. We report our rigorous annotation process by
native speakers, with analyses on inter-annotator agreement, iterative
feedback, and patterns in error type and severity.

</details>


### [13] [SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations](https://arxiv.org/abs/2509.20567)
*Ayan Sar,Pranav Singh Puri,Sumit Aich,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

> SwasthLLM是一个用于医疗诊断的跨语言、零样本学习框架，旨在增强不同语言中的疾病分类任务性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于低资源语言的标注医疗数据稀缺以及人群间语言差异性，自动疾病诊断仍然是一个具有挑战性的任务。本文旨在提出一种统一、零样本、跨语言和多任务学习框架，以解决这一问题。

**Method:** SwasthLLM采用多语言XLM-RoBERTa编码器，结合语言敏感的注意力机制和疾病分类头。为了使跨语言的语义表示一致，引入了暹罗对比学习模块，并使用翻译一致性模块和对比投影头来增强语言不变的表示学习。模型通过多任务学习策略进行训练，同时优化疾病分类、翻译对齐和对比学习目标。此外，还采用了元学习（MAML）方法来使模型具备快速适应新语言或任务的能力。

**Result:** SwasthLLM展示了高诊断性能，并在多种语言间表现出了强大的零样本推广能力。

**Conclusion:** SwastLLM在监督设置中实现了97.22%的测试准确率和97.17%的F1得分。在零样本场景中，该模型在印地语和孟加拉语医学文本上的准确率分别为92.78%和73.33%，展示了在低资源环境中的强推广能力。

**Abstract:** In multilingual healthcare environments, automatic disease diagnosis from
clinical text remains a challenging task due to the scarcity of annotated
medical data in low-resource languages and the linguistic variability across
populations. This paper proposes SwasthLLM, a unified, zero-shot,
cross-lingual, and multi-task learning framework for medical diagnosis that
operates effectively across English, Hindi, and Bengali without requiring
language-specific fine-tuning. At its core, SwasthLLM leverages the
multilingual XLM-RoBERTa encoder augmented with a language-aware attention
mechanism and a disease classification head, enabling the model to extract
medically relevant information regardless of the language structure. To align
semantic representations across languages, a Siamese contrastive learning
module is introduced, ensuring that equivalent medical texts in different
languages produce similar embeddings. Further, a translation consistency module
and a contrastive projection head reinforce language-invariant representation
learning. SwasthLLM is trained using a multi-task learning strategy, jointly
optimizing disease classification, translation alignment, and contrastive
learning objectives. Additionally, we employ Model-Agnostic Meta-Learning
(MAML) to equip the model with rapid adaptation capabilities for unseen
languages or tasks with minimal data. Our phased training pipeline emphasizes
robust representation alignment before task-specific fine-tuning. Extensive
evaluation shows that SwasthLLM achieves high diagnostic performance, with a
test accuracy of 97.22% and an F1-score of 97.17% in supervised settings.
Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and
73.33% accuracy on Bengali medical text, demonstrating strong generalization in
low-resource contexts.

</details>


### [14] [Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures](https://arxiv.org/abs/2509.20577)
*Sampurna Roy,Ayan Sar,Anurag Kaushish,Kanav Gupta,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

> 提出了深度专业化专家混合（DS-MoE）框架，通过动态组合不同深度的专家模块来优化推理过程，提高了计算效率、推理质量和可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的变压器架构对所有输入应用相同的处理深度，这导致了资源浪费并限制了推理质量。

**Method:** 提出了深度专业化专家混合（DS-MoE）框架，使用优化的不同推理深度的专家模块，并通过学习路由网络动态组合这些专家模块来适应输入的复杂性。

**Result:** 在800GB的The Pile数据集上进行训练和评估，DS-MoE框架相比统一深度的变压器实现了最高16%的计算节省，推理速度提高了35%，并且复杂多步骤推理基准测试准确度提高了2.8%。

**Conclusion:** DS-MoE框架证明了通过深度专业化模块化处理可以同时提高计算效率、推理质量和可解释性，是适应性神经架构的一个重要进步。

**Abstract:** Contemporary transformer architectures apply identical processing depth to
all inputs, creating inefficiencies and limiting reasoning quality. Simple
factual queries are subjected to the same multilayered computation as complex
logical problems, wasting resources while constraining deep inference. To
overcome this, we came up with a concept of Dynamic Reasoning Chains through
Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends
the Mixture of Experts paradigm from width-based to depth specialised
computation. DS-MoE introduces expert modules optimised for distinct reasoning
depths, shallow pattern recognition, compositional reasoning, logical
inference, memory integration, and meta-cognitive supervision. A learned
routing network dynamically assembles custom reasoning chains, activating only
the necessary experts to match input complexity. The dataset on which we
trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse
domains such as scientific papers, legal texts, programming code, and web
content, enabling systematic assessment across reasoning depths. Experimental
results demonstrate that DS-MoE achieves up to 16 per cent computational
savings and 35 per cent faster inference compared to uniform-depth
transformers, while delivering 2.8 per cent higher accuracy on complex
multi-step reasoning benchmarks. Furthermore, routing decisions yield
interpretable reasoning chains, enhancing transparency and scalability. These
findings establish DS-MoE as a significant advancement in adaptive neural
architectures, demonstrating that depth-specialised modular processing can
simultaneously improve efficiency, reasoning quality, and interpretability in
large-scale language models.

</details>


### [15] [Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding](https://arxiv.org/abs/2509.20581)
*Ayan Sar,Sampurna Roy,Kanav Gupta,Anurag Kaushish,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

> 本文提出了一种新型架构HRT，它通过模拟多分辨率处理来更有效地捕捉语言的层级结构，从而在多个自然语言任务基准上优于标准的 Transformer 模型。

<details>
  <summary>Details</summary>

**Motivation:** 当前的 Transformer 架构在处理文本时将其视为平坦的令牌序列，从根本上误解了人类语言的层级特性。这导致了二次计算成本，弱组合推广能力，以及对话语层次建模不足。

**Method:** 提出了一种名为分层解析变换器（HRT）的新神经架构，该架构受小波启发，能够同时在多个分辨率上处理语言，从字符到话语级别单元。HRT 构建了多分辨率注意力机制，实现了自下而上的组合化和自上而下的语境化。通过在尺度间使用指数序列约简，HRT 达到了 O(nlogn) 的计算复杂度，从而在效率上显著优于标准的 Transformer。

**Result:** HRT 在 GLUE、SuperGLUE、Long Range Arena 和 WikiText-103 等多样化基准测试中实现了卓越性能，平均优于标准 Transformer 基准3.8%至6.1%，同时减少了42%的内存使用和37%的推理延迟。

**Conclusion:** HRT 作为首个将计算结构与人类语言的层级组织对齐的架构，证明了受小波启发的多尺度处理不仅在理论上减少了计算资源的需求，而且在实际语言理解任务中也带来了显著改善。

**Abstract:** Transformer architectures have achieved state-of-the-art performance across
natural language tasks, yet they fundamentally misrepresent the hierarchical
nature of human language by processing text as flat token sequences. This
results in quadratic computational cost, weak computational cost, weak
compositional generalization, and inadequate discourse-level modeling. We
propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired
neural architecture that processes language simultaneously across multiple
resolutions, from characters to discourse-level units. HRT constructs a
multi-resolution attention, enabling bottom-up composition and top-down
contextualization. By employing exponential sequence reduction across scales,
HRT achieves O(nlogn) complexity, offering significant efficiency improvements
over standard transformers. We evaluated HRT on a diverse suite of benchmarks,
including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results
demonstrated that HRT outperforms standard transformer baselines by an average
of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while
reducing memory usage by 42% and inference latency by 37% compared to BERT and
GPT style models of similar parameter count. Ablation studies confirm the
effectiveness of cross-resolution attention and scale-specialized modules,
showing that each contributes independently to both efficiency and accuracy.
Our findings establish HRT as the first architecture to align computational
structure with the hierarchical organization of human language, demonstrating
that multi-scale, wavelet-inspired processing yields both theoretical
efficiency gains and practical improvements in language understanding.

</details>


### [16] [FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models](https://arxiv.org/abs/2509.20624)
*Amin Karimi Monsefi,Nikhil Bhendawade,Manuel Rafael Ciosici,Dominic Culver,Yizhe Zhang,Irina Belousova*

Main category: cs.CL

> FS-DFM is a method designed to speed up discrete diffusion sampling in language generation models, enabling fast yet accurate language generation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of autoregressive language models (ARMs) and standard discrete diffusion models in terms of throughput and latency for long sequences. ARMs generate tokens serially, limiting their efficiency, while discrete diffusion typically requires many model evaluations, making the process time-consuming.

**Method:** FS-DFM (Few-Step Discrete Flow-Matching) is introduced, which aims to make language generation faster without compromising quality. It achieves this by treating the number of sampling steps as a variable and training the model to be consistent across different step budgets, ensuring that a single step can achieve what multiple steps would otherwise accomplish. Additionally, the method uses a reliable update rule to guide probability updates accurately and avoids overshooting by using teacher guidance from long-run trajectories.

**Result:** FS-DFM achieves comparable perplexity to a 1,024-step discrete-flow baseline at generating 1,024 tokens, while using only 8 sampling steps with a similarly sized model. This significantly accelerates the sampling process, improving overall latency and throughput.

**Conclusion:** FS-DFM, through its innovative approach to sampling and model training, successfully balances speed with quality, making it a promising advancement in the realm of language generation models.

**Abstract:** Autoregressive language models (ARMs) deliver strong likelihoods, but are
inherently serial: they generate one token per forward pass, which limits
throughput and inflates latency for long sequences. Diffusion Language Models
(DLMs) parallelize across positions and thus appear promising for language
generation, yet standard discrete diffusion typically needs hundreds to
thousands of model evaluations to reach high quality, trading serial depth for
iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A
discrete flow-matching model designed for speed without sacrificing quality.
The core idea is simple: make the number of sampling steps an explicit
parameter and train the model to be consistent across step budgets, so one big
move lands where many small moves would. We pair this with a reliable update
rule that moves probability in the right direction without overshooting, and
with strong teacher guidance distilled from long-run trajectories. Together,
these choices make few-step sampling stable, accurate, and easy to control. On
language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity
parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens
using a similar-size model, delivering up to 128 times faster sampling and
corresponding latency/throughput gains.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [17] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

> 论文探索了一种基于NTP（下一个词概率）的轻量级方法以实现高效的幻觉检测，这种方法利用传统机器学习模型量化模型不确定性，并证明了高不确定性与幻觉有强相关性。实验表明此方法能够在不依赖强大VLM的情况下达到相似的检测效果。

<details>
  <summary>Details</summary>

**Motivation:** 传统的幻觉检测方法使用同一个或不同的VLM来评估生成的输出，这在计算上消耗大并增加模型延迟。本研究旨在探索高效的实时幻觉检测方法，以增强VLM的可靠性。

**Method:** 通过训练传统的机器学习模型处理视觉语言模型（VLM）的下一个词概率（NTPs）信号来实现实时幻觉检测。我们假设高不确定性（即低NTP值）与幻觉有很强的关系。

**Result:** 研究结果显示，基于NTP的特征是幻觉预测的有力指标，简单的机器学习模型能够达到与强大的VLM相媲美的性能。通过结合语言NTPs，能提高幻觉检测性能，并且将VLM的幻觉预测分数整合到NTP模型中，比单独使用VLM或NTP有更好效果。

**Conclusion:** 本研究为提高VLM的可靠性铺平了道路，提供了简单且轻量级的解决方案。通过利用NTPs和语言NTPs，我们可以创建高效的幻觉检测机制以改进VLM的准确性和可靠性。

**Abstract:** Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [18] [Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification](https://arxiv.org/abs/2509.20420)
*Elias N. Zois,Moises Diaz,Salem Said,Miguel A. Ferrer*

Main category: cs.CV

> A method using Riemannian Gaussian mixtures to generate synthetic SPD data for training metric learning classifiers achieves low error rates in writer-independent offline signature verification.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenge of writer-independent offline handwritten signature verification, where the model needs to generalize across unseen individuals. The method aims to leverage synthetic data generation in Riemannian space to improve the classifier's performance without relying heavily on real-world datasets for training.

**Method:** The paper introduces a quasi-synthetic data generation framework based on Riemannian geometry of SPD matrices to generate synthetic positive and negative SPD populations. These are used in a metric learning framework to train a classifier for offline handwritten signature verification.

**Result:** Experiments on two datasets with different writing styles show that the proposed method yields low error rates, indicating the effectiveness of using synthetic data for training signature verification systems.

**Conclusion:** The conclusion is that using quasi-synthetic data generated in Riemannian space shows potential for improving the accuracy of writer-independent offline handwritten signature verification, with reduced dependence on real-world datasets for training.

**Abstract:** Offline handwritten signature verification remains a challenging task,
particularly in writer-independent settings where models must generalize across
unseen individuals. Recent developments have highlighted the advantage of
geometrically inspired representations, such as covariance descriptors on
Riemannian manifolds. However, past or present, handcrafted or data-driven
methods usually depend on real-world signature datasets for classifier
training. We introduce a quasi-synthetic data generation framework leveraging
the Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small
set of genuine samples in the SPD space is the seed to a Riemannian Gaussian
Mixture which identifies Riemannian centers as synthetic writers and variances
as their properties. Riemannian Gaussian sampling on each center generates
positive as well as negative synthetic SPD populations. A metric learning
framework utilizes pairs of similar and dissimilar SPD points, subsequently
testing it over on real-world datasets. Experiments conducted on two popular
signature datasets, encompassing Western and Asian writing styles, demonstrate
the efficacy of the proposed approach under both intra- and cross- dataset
evaluation protocols. The results indicate that our quasi-synthetic approach
achieves low error rates, highlighting the potential of generating synthetic
data in Riemannian spaces for writer-independent signature verification
systems.

</details>


### [19] [Seedream 4.0: Toward Next-generation Multimodal Image Generation](https://arxiv.org/abs/2509.20427)
*Team Seedream,Yunpeng Chen,Yu Gao,Lixue Gong,Meng Guo,Qiushan Guo,Zhiyao Guo,Xiaoxia Hou,Weilin Huang,Yixuan Huang,Xiaowen Jian,Huafeng Kuang,Zhichao Lai,Fanshi Li,Liang Li,Xiaochen Lian,Chao Liao,Liyang Liu,Wei Liu,Yanzuo Lu,Zhengxiong Luo,Tongtong Ou,Guang Shi,Yichun Shi,Shiqi Sun,Yu Tian,Zhi Tian,Peng Wang,Rui Wang,Xun Wang,Ye Wang,Guofeng Wu,Jie Wu,Wenxu Wu,Yonghui Wu,Xin Xia,Xuefeng Xiao,Shuang Xu,Xin Yan,Ceyuan Yang,Jianchao Yang,Zhonghua Zhai,Chenlin Zhang,Heng Zhang,Qi Zhang,Xinyu Zhang,Yuwei Zhang,Shijia Zhao,Wenliang Zhao,Wenjia Zhu*

Main category: cs.CV

> 本文介绍了Seedream 4.0，一种高效、高性能的多模态图像生成系统，该系统可以统一处理文本到图像的合成、图像编辑和多图像合成任务。Seedream 4.0 在大型数据集上进行了预训练，并通过各种优化技术实现了快速生成高分辨率图像的能力。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在开发一个高效且高性能的多模态图像生成系统，它可以统一文本到图像合成、图像编辑以及多图像合成。

**Method:** 我们开发了一种高效的扩散变换器，并结合了强大的变分自编码器（VAE），以显著减少图像标记的数量。这使得我们的模型能够高效训练并快速生成原生高分辨率图像（例如，1K-4K）。我们还在数十亿个文本-图像对上对模型进行了预训练，并通过优化策略确保了大规模训练的稳定性和泛化能力。此外，通过引入仔细微调过的视觉语言模型（VLM），我们可以同时进行文本到图像合成和图像编辑任务的多模态后训练。

**Result:** Seedream 4.0 在文本到图像合成和多模态图像编辑任务上实现了最新的技术水平，尤其是在复杂的图像编辑和上下文感知推理任务中表现卓越。

**Conclusion:** Seedream 4.0 延伸了传统的文本到图像系统，使之成为一个更加交互和多维的创意工具，为创造力和专业应用领域的发展设定了新的标准。

**Abstract:** We introduce Seedream 4.0, an efficient and high-performance multimodal image
generation system that unifies text-to-image (T2I) synthesis, image editing,
and multi-image composition within a single framework. We develop a highly
efficient diffusion transformer with a powerful VAE which also can reduce the
number of image tokens considerably. This allows for efficient training of our
model, and enables it to fast generate native high-resolution images (e.g.,
1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning
diverse taxonomies and knowledge-centric concepts. Comprehensive data
collection across hundreds of vertical scenarios, coupled with optimized
strategies, ensures stable and large-scale training, with strong
generalization. By incorporating a carefully fine-tuned VLM model, we perform
multi-modal post-training for training both T2I and image editing tasks
jointly. For inference acceleration, we integrate adversarial distillation,
distribution matching, and quantization, as well as speculative decoding. It
achieves an inference time of up to 1.8 seconds for generating a 2K image
(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream
4.0 can achieve state-of-the-art results on both T2I and multimodal image
editing. In particular, it demonstrates exceptional multimodal capabilities in
complex tasks, including precise image editing and in-context reasoning, and
also allows for multi-image reference, and can generate multiple output images.
This extends traditional T2I systems into an more interactive and
multidimensional creative tool, pushing the boundary of generative AI for both
creativity and professional applications. Seedream 4.0 is now accessible on
https://www.volcengine.com/experience/ark?launch=seedream.

</details>


### [20] [A Contrastive Learning Framework for Breast Cancer Detection](https://arxiv.org/abs/2509.20474)
*Samia Saeed,Khuram Naveed*

Main category: cs.CV

> 研究提出了一种新的半监督对比学习方法，显著提高了基于ResNet-50的乳腺癌检测模型的性能，实现了96.7%的检测准确率。

<details>
  <summary>Details</summary>

**Motivation:** 尽管深度学习方法在乳腺癌检测方面具有巨大潜力，但由于大型标记数据集的缺乏，它们的准确性经常受到影响。我们的研究旨在解决这个问题。

**Method:** 我们的研究引入了一种对比学习（CL）框架，该框架在小规模标记数据集上表现出色。我们使用未标记的乳腺钼靶图像数据，通过半监督的CL方法训练Resnet-50。我们采用了各种数据增强和变换手段来提高模型性能，并最终在一小部分标记数据上对模型进行调优。

**Result:** 我们的方法在基准数据集INbreast和MIAS上达到了96.7%的乳腺癌检测准确率，超越了现有的最好结果。

**Conclusion:** 研究表明，通过使用对比学习框架和数据增强技术，可以显著提高乳腺癌检测的准确率，特别是在数据有限的情况下。

**Abstract:** Breast cancer, the second leading cause of cancer-related deaths globally,
accounts for a quarter of all cancer cases [1]. To lower this death rate, it is
crucial to detect tumors early, as early-stage detection significantly improves
treatment outcomes. Advances in non-invasive imaging techniques have made early
detection possible through computer-aided detection (CAD) systems which rely on
traditional image analysis to identify malignancies. However, there is a
growing shift towards deep learning methods due to their superior
effectiveness. Despite their potential, deep learning methods often struggle
with accuracy due to the limited availability of large-labeled datasets for
training. To address this issue, our study introduces a Contrastive Learning
(CL) framework, which excels with smaller labeled datasets. In this regard, we
train Resnet-50 in semi supervised CL approach using similarity index on a
large amount of unlabeled mammogram data. In this regard, we use various
augmentation and transformations which help improve the performance of our
approach. Finally, we tune our model on a small set of labelled data that
outperforms the existing state of the art. Specifically, we observed a 96.7%
accuracy in detecting breast cancer on benchmark datasets INbreast and MIAS.

</details>


### [21] [Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data](https://arxiv.org/abs/2509.20479)
*Simon Baeuerle,Pratik Khanna,Nils Friederich,Angelo Jovin Yamachui Sitcheu,Damir Shakirov,Andreas Steimer,Ralf Mikut*

Main category: cs.CV

> 研究了基础模型在工业自动质量检查中的应用潜力，发现它们在公共数据集上表现良好，但在真实工业数据上表现不佳。

<details>
  <summary>Details</summary>

**Motivation:** 研究基础模型（FMs）在系列制造过程中自动质量检查的潜力，以及它们在不同产品上使用时的跨域和跨数据集泛化能力，旨在减少繁琐的标注任务。

**Method:** 分析文本内容，提取关键信息。

**Result:** 尽管基础模型在公共基准数据集上表现出色，但它们在实际的工业图像数据上表现不佳。

**Conclusion:** 基础模型在没有标注数据的情况下实现了跨域和跨数据集的泛化，但在实际的工业应用中表现不如同类型模型在标准数据集上的表现。这表明基础模型在实际工业数据中的适应性和可靠性需要进一步提高。

**Abstract:** Foundation Models (FMs) have shown impressive performance on various text and
image processing tasks. They can generalize across domains and datasets in a
zero-shot setting. This could make them suitable for automated quality
inspection during series manufacturing, where various types of images are being
evaluated for many different products. Replacing tedious labeling tasks with a
simple text prompt to describe anomalies and utilizing the same models across
many products would save significant efforts during model setup and
implementation. This is a strong advantage over supervised Artificial
Intelligence (AI) models, which are trained for individual applications and
require labeled training data. We test multiple recent FMs on both custom
real-world industrial image data and public image data. We show that all of
those models fail on our real-world data, while the very same models perform
well on public benchmark datasets.

</details>


### [22] [Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision](https://arxiv.org/abs/2509.20481)
*Jing Li,Oskar Bartosz,Chengyu Wang,Michal Wnuczynski,Dilshan Godaliyadda,Michael Polley*

Main category: cs.CV

> Introduces a universal Neural Space for pre-computing generalizable visual features across multiple tasks, leading to improved efficiency and hardware compatibility in AI vision pipelines.

<details>
  <summary>Details</summary>

**Motivation:** To improve efficiency in multi-task applications by reducing the need for separate latent domains for each task and to support a wider range of hardware through a lightweight CNN-based backbone.

**Method:** Proposes a universal Neural Space (NS) using an encoder-decoder framework for pre-computing features across various vision and imaging tasks, focusing on transformation-aware, generalizable representations to enable efficient multi-task pipelines with shared feature spaces.

**Result:** The method demonstrates efficient performance across imaging and vision tasks, such as demosaicing, denoising, depth estimation, and semantic segmentation, while reducing redundancy and improving generalization across domain shifts.

**Conclusion:** The universal Neural Space with a shared feature space improves efficiency and widens hardware compatibility in multi-task vision pipelines.

**Abstract:** The majority of AI models in imaging and vision are customized to perform on
specific high-precision task. However, this strategy is inefficient for
applications with a series of modular tasks, since each requires a mapping into
a disparate latent domain. To address this inefficiency, we proposed a
universal Neural Space (NS), where an encoder-decoder framework pre-computes
features across vision and imaging tasks. Our encoder learns transformation
aware, generalizable representations, which enable multiple downstream AI
modules to share the same feature space. This architecture reduces redundancy,
improves generalization across domain shift, and establishes a foundation for
effecient multi-task vision pipelines. Furthermore, as opposed to larger
transformer backbones, our backbone is lightweight and CNN-based, allowing for
wider across hardware. We furthur demonstrate that imaging and vision modules,
such as demosaicing, denoising, depth estimation and semantic segmentation can
be performed efficiently in the NS.

</details>


### [23] [Data-Efficient Stream-Based Active Distillation for Scalable Edge Model Deployment](https://arxiv.org/abs/2509.20484)
*Dani Manjah,Tim Bary,Benoît Gérin,Benoît Macq,Christophe de Vleeschouwer*

Main category: cs.CV

> 研究探讨了如何在保持传输成本较低的情况下，通过选择最具代表性的图像进行训练，以提高边缘设备模型的质量。

<details>
  <summary>Details</summary>

**Motivation:** 随着环境不断变化，边缘摄像机系统需要频繁更新模型。处理此需求的一种方法是使用中央服务器上的复杂教师模型来标记数据，为计算能力有限的边缘设备训练更小的模型。

**Method:** 采用高置信度流策略结合多样性选择方法，从数据集中选取训练所需的图像。

**Result:** 实验表明，对于类似的训练负载（即迭代次数），上述策略可以以最小的数据集查询数量生成高质量的模型。

**Conclusion:** 高置信度流策略结合多样性选择方法，能够在保持传输成本低的同时提高模型质量。

**Abstract:** Edge camera-based systems are continuously expanding, facing ever-evolving
environments that require regular model updates. In practice, complex teacher
models are run on a central server to annotate data, which is then used to
train smaller models tailored to the edge devices with limited computational
power. This work explores how to select the most useful images for training to
maximize model quality while keeping transmission costs low. Our work shows
that, for a similar training load (i.e., iterations), a high-confidence
stream-based strategy coupled with a diversity-based approach produces a
high-quality model with minimal dataset queries.

</details>


### [24] [InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On](https://arxiv.org/abs/2509.20524)
*Julien Han,Shuwen Qiu,Qi Li,Xingzi Xu,Mehmet Saygin Seyfioglu,Kavosh Asadi,Karim Bouyarmane*

Main category: cs.CV

> InstructVTON通过结合视觉语言模型和图像分割模型实现自动化的二值掩码生成，进而控制虚拟试衣过程中的衣服样式，简化用户操作并提升试衣效果。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于二值掩码的虚拟试衣模型存在生成理想的试衣效果困难的问题，而且掩码的生成需要一定的背景知识，并且可能是特定模型依赖的，在某些情况下甚至是不可能的任务。

**Method:** 提出InstructVTON，一种基于指令随从的交互式虚拟试衣系统，该系统允许用户通过自然语言控制生成结果的精细和复杂的风格调整，适用于单个或多个衣物。这种方法将虚拟试衣问题定义为基于图像指导或约束的图像修复任务。

**Result:** 实验表明，InstructVTON能够实现与现有虚拟试衣模型的互操作，以达到具有样式控制能力的最先进的结果。

**Conclusion:** InstructVTON简化了最终用户的体验，不仅去除了精确绘制掩码的必要性，还通过自动化执行多轮图像生成简化了试服装景，这些场景仅靠基于掩码的虚拟试衣模型是无法达到的。

**Abstract:** We present InstructVTON, an instruction-following interactive virtual try-on
system that allows fine-grained and complex styling control of the resulting
generation, guided by natural language, on single or multiple garments. A
computationally efficient and scalable formulation of virtual try-on formulates
the problem as an image-guided or image-conditioned inpainting task. These
inpainting-based virtual try-on models commonly use a binary mask to control
the generation layout. Producing a mask that yields desirable result is
difficult, requires background knowledge, might be model dependent, and in some
cases impossible with the masking-based approach (e.g. trying on a long-sleeve
shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt
with sleeves down, where the mask will necessarily cover the entire sleeve).
InstructVTON leverages Vision Language Models (VLMs) and image segmentation
models for automated binary mask generation. These masks are generated based on
user-provided images and free-text style instructions. InstructVTON simplifies
the end-user experience by removing the necessity of a precisely drawn mask,
and by automating execution of multiple rounds of image generation for try-on
scenarios that cannot be achieved with masking-based virtual try-on models
alone. We show that InstructVTON is interoperable with existing virtual try-on
models to achieve state-of-the-art results with styling control.

</details>


### [25] [Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition](https://arxiv.org/abs/2509.20537)
*Dana A Abdullah,Dana Rasul Hamad,Bishar Rasheed Ibrahim,Sirwan Abdulwahid Aula,Aso Khaleel Ameen,Sabat Salih Hamadamin*

Main category: cs.CV

> DeepAFRNet 提出了一种新的深度学习模型来识别修改过的指纹，显著提高了此类样本的识别准确率，并在一个包含三难度级别的数据集上进行了测试。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于对手可以修改指纹以逃避检测，这项研究旨在开发一种能够有效识别修改后指纹的系统，以提高实际应用中的安全性。

**Method:** DeepAFRNet 是一种深度学习识别模型，使用 VGG16 的骨干网络来提取高维特征，并利用余弦相似度来比较特征嵌入。

**Result:** 对于三个难度级别（简单，中等，困难），在严格的阈值下，DeepAFRNet 分别达到了 96.7%，98.76% 和 99.54% 的准确率。然而，阈值的放松会导致识别率显著下降。

**Conclusion:** DeepAFRNet 通过对真实修改数据样本进行测试并报告分层指标的方法，解决了先前基于合成篡改或有限验证协议的研究的局限性，并表明了其在实际部署中的潜在应用价值。

**Abstract:** Altered fingerprint recognition (AFR) is challenging for biometric
verification in applications such as border control, forensics, and fiscal
admission. Adversaries can deliberately modify ridge patterns to evade
detection, so robust recognition of altered prints is essential. We present
DeepAFRNet, a deep learning recognition model that matches and recognizes
distorted fingerprint samples. The approach uses a VGG16 backbone to extract
high-dimensional features and cosine similarity to compare embeddings. We
evaluate on the SOCOFing Real-Altered subset with three difficulty levels
(Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of
96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A
threshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72
sharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent,
underscoring the importance of threshold selection in biometric systems. By
using real altered samples and reporting per-level metrics, DeepAFRNet
addresses limitations of prior work based on synthetic alterations or limited
verification protocols, and indicates readiness for real-world deployments
where both security and recognition resilience are critical.

</details>


### [26] [Large Pre-Trained Models for Bimanual Manipulation in 3D](https://arxiv.org/abs/2509.20579)
*Hanna Yurchyk,Wei-Di Chang,Gregory Dudek,David Meger*

Main category: cs.CV

> The research integrates attention maps from a pre-trained Vision Transformer (DINOv2) into voxel representations to improve the performance of a bimanual robotic manipulation policy, achieving significant performance gains on the RLBench benchmark.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the capabilities of bimanual robotic manipulation through advanced visual attention mechanisms, leading to better semantic understanding in the policy’s decision-making process.

**Method:** The method involves extracting attention maps from DINOv2 as pixel-level saliency scores over RGB images, then transferring these maps into a 3D voxel grid to be used as semantic cues in a behavior cloning policy.

**Result:** The result is a notable improvement: the attention-guided approach improved the policy's performance by 8.2% on average and 21.9% relatively across all tasks in the RLBench bimanual benchmark.

**Conclusion:** The conclusion is that integrating visual attention from a pre-trained Vision Transformer effectively enhances the performance of bimanual robotic policies by providing useful semantic information in voxel-based representations.

**Abstract:** We investigate the integration of attention maps from a pre-trained Vision
Transformer into voxel representations to enhance bimanual robotic
manipulation. Specifically, we extract attention maps from DINOv2, a
self-supervised ViT model, and interpret them as pixel-level saliency scores
over RGB images. These maps are lifted into a 3D voxel grid, resulting in
voxel-level semantic cues that are incorporated into a behavior cloning policy.
When integrated into a state-of-the-art voxel-based policy, our
attention-guided featurization yields an average absolute improvement of 8.2%
and a relative gain of 21.9% across all tasks in the RLBench bimanual
benchmark.

</details>


### [27] [A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management](https://arxiv.org/abs/2509.20580)
*Xinyang Mu,Yuzhen Lu,Boyang Deng*

Main category: cs.CV

> 本研究对比分析了YOLO和RT-DETR系列的实时对象检测模型在蓝莓检测任务上的表现，并通过半监督学习进一步优化了模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 蓝莓检测在自然环境下由于光照、遮挡和运动模糊等因素变得具有挑战性，本研究旨在通过深度学习方法来改进检测效果。

**Method:** 研究通过一个新构建的蓝莓检测数据集评估了36种YOLO和RT-DETR模型变体的检测性能。数据集包括661幅树冠图像和85,879个标记实例，并使用了半监督学习技术进一步优化模型。

**Result:** YOLOv12m和RT-DETRv2-X分别以93.3%和93.6%的mAP@50值表现最佳，而半监督学习使检测性能在不同模型上有了-1.4%到2.9%的提高。

**Conclusion:** 实验证明，更复杂的模型如YOLOv12m和RT-DETRv2-X在蓝莓检测任务上具有较高的准确性，且通过半监督学习可以进一步提高检测性能。这些发现为实际应用中模型选择提供了依据。

**Abstract:** Blueberry detection in natural environments remains challenging due to
variable lighting, occlusions, and motion blur due to environmental factors and
imaging devices. Deep learning-based object detectors promise to address these
challenges, but they demand a large-scale, diverse dataset that captures the
real-world complexities. Moreover, deploying these models in practical
scenarios often requires the right accuracy/speed/memory trade-off in model
selection. This study presents a novel comparative benchmark analysis of
advanced real-time object detectors, including YOLO (You Only Look Once)
(v8-v12) and RT-DETR (Real-Time Detection Transformers) (v1-v2) families,
consisting of 36 model variants, evaluated on a newly curated dataset for
blueberry detection. This dataset comprises 661 canopy images collected with
smartphones during the 2022-2023 seasons, consisting of 85,879 labelled
instances (including 36,256 ripe and 49,623 unripe blueberries) across a wide
range of lighting conditions, occlusions, and fruit maturity stages. Among the
YOLO models, YOLOv12m achieved the best accuracy with a mAP@50 of 93.3%, while
RT-DETRv2-X obtained a mAP@50 of 93.6%, the highest among all the RT-DETR
variants. The inference time varied with the model scale and complexity, and
the mid-sized models appeared to offer a good accuracy-speed balance. To
further enhance detection performance, all the models were fine-tuned using
Unbiased Mean Teacher-based semi-supervised learning (SSL) on a separate set of
1,035 unlabeled images acquired by a ground-based machine vision platform in
2024. This resulted in accuracy gains ranging from -1.4% to 2.9%, with
RT-DETR-v2-X achieving the best mAP@50 of 94.8%. More in-depth research into
SSL is needed to better leverage cross-domain unlabeled data. Both the dataset
and software programs of this study are made publicly available to support
further research.

</details>


### [28] [Region-of-Interest Augmentation for Mammography Classification under Patient-Level Cross-Validation](https://arxiv.org/abs/2509.20585)
*Farbod Bigdeli,Mohsen Mohammadagha,Ali Bigdeli*

Main category: cs.CV

> 研究通过Mini-DDSM数据集引入了一种轻量级的兴趣区域(ROI)增强策略，以提高基于深度学习的乳腺癌筛查的性能，该策略在训练阶段显示了一定的提升效果，但在不同折中性能差异较大。

<details>
  <summary>Details</summary>

**Motivation:** 受限于低分辨率数据集和小样本规模，研究旨在通过一种轻量级ROI增强策略提高基于深度学习的乳腺摄影解释的性能，特别是在约束环境下。

**Method:** 研究采用Mini-DDSM数据集，并在训练阶段引入了一个ROI增强策略，即以一定概率将完整图像替换为随机采样的ROI裁剪，通过这种方式增加训练的变异性。

**Result:** 实验结果表明，在不同折中，该ROI增强策略在ROC-AUC上的平均增益较为有限，PR-AUC则保持平坦甚至略有下降。

**Conclusion:** 这项研究表明，即使在没有增加标签或修改现有架构的情况下，简单的基于数据的ROI策略也能在约束环境下提高乳腺摄影的分类性能。

**Abstract:** Breast cancer screening with mammography remains central to early detection
and mortality reduction. Deep learning has shown strong potential for
automating mammogram interpretation, yet limited-resolution datasets and small
sample sizes continue to restrict performance. We revisit the Mini-DDSM dataset
(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest
(ROI) augmentation strategy. During training, full images are probabilistically
replaced with random ROI crops sampled from a precomputed, label-free
bounding-box bank, with optional jitter to increase variability. We evaluate
under strict patient-level cross-validation and report ROC-AUC, PR-AUC, and
training-time efficiency metrics (throughput and GPU memory). Because ROI
augmentation is training-only, inference-time cost remains unchanged. On
Mini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest
average ROC-AUC gains, with performance varying across folds; PR-AUC is flat to
slightly lower. These results demonstrate that simple, data-centric ROI
strategies can enhance mammography classification in constrained settings
without requiring additional labels or architectural modifications.

</details>


### [29] [Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections](https://arxiv.org/abs/2509.20607)
*Jing Wu,Zirui Wang,Iro Laina,Victor Adrian Prisacariu*

Main category: cs.CV

> 本文提出了一种新的3D重建方法，可以利用镜子反射从单张图像生成多视图信息，进而构建虚拟相机并生成虚拟视图，适用于静态和动态场景。

<details>
  <summary>Details</summary>

**Motivation:** 镜子反射提供了一种从单张图像中获取立体信息的方法，可以从单张图像中构建多视图立体设置，从而简化成像过程，使其与强大的前馈重建模型兼容，适用于泛化和稳健的3D重建。

**Method:** 通过利用镜子反射属性，设计一个将反射视为辅助视图并构建有效虚拟相机的变换过程，使能够在像素域直接生成虚拟视图，保持与现实世界的成像过程一致。同时提出对称感知损失来优化姿态估计。该框架还能扩展到动态场景，提供高效的逐帧几何恢复。

**Result:** 进行了在真实数据和合成数据上的广泛实验，展示了该方法的有效性。合成数据集包含了16个Blender场景及其地面真实的点云和相机姿态。

**Conclusion:** 利用镜子反射从单张图像中获取多视图信息的方法能够简化3D重建过程，并且在真实和合成数据上进行了大量实验验证了方法的有效性。

**Abstract:** Mirror reflections are common in everyday environments and can provide stereo
information within a single capture, as the real and reflected virtual views
are visible simultaneously. We exploit this property by treating the reflection
as an auxiliary view and designing a transformation that constructs a
physically valid virtual camera, allowing direct pixel-domain generation of the
virtual view while adhering to the real-world imaging process. This enables a
multi-view stereo setup from a single image, simplifying the imaging process,
making it compatible with powerful feed-forward reconstruction models for
generalizable and robust 3D reconstruction. To further exploit the geometric
symmetry introduced by mirrors, we propose a symmetric-aware loss to refine
pose estimation. Our framework also naturally extends to dynamic scenes, where
each frame contains a mirror reflection, enabling efficient per-frame geometry
recovery. For quantitative evaluation, we provide a fully customizable
synthetic dataset of 16 Blender scenes, each with ground-truth point clouds and
camera poses. Extensive experiments on real-world data and synthetic data are
conducted to illustrate the effectiveness of our method.

</details>


### [30] [Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery](https://arxiv.org/abs/2509.20628)
*Yiming Xiao,Archit Gupta,Miguel Esparza,Yu-Hsuan Ho,Antonia Sebastian,Hannah Weas,Rose Houck,Ali Mostafavi*

Main category: cs.CV

> 研究提出FacadeTrack框架，结合街道全景视频与地块信息，评估建筑物灾害后的占用状态，能够提供高精度的可居住性评估。

<details>
  <summary>Details</summary>

**Motivation:** 为了在灾害后快速评估建筑物的可居住性，作者研究了一种结合高层和街道视图优势的方法，以提供快速、全面且精准的占用评估。

**Method:** FacadeTrack是一种街道级别的、语言引导框架，它将全景视频与地块链接，矫正视角到立面，并提取出可解释的属性（例如入口阻塞、临时覆盖物、局部残骸），这些属性驱动两种决策策略：一种是透明的一阶段规则，另一种是将感知与保守推理分离的两阶段设计。

**Result:** 在两次飓风Helene后的调查中，两阶段方法达到了0.927的精度、0.781的召回率，以及0.848的F1分数，相比之下，一阶段基线方法达到了0.943的精度、0.728的召回率，以及0.822的F1分数。

**Conclusion:** 该研究提出的方法不仅可以提高占用评估的准确性，还可以揭示错误出现的位置和原因，从而提高质量控制。这种方法适用于地理空间和紧急管理工作流程。

**Abstract:** Building-level occupancy after disasters is vital for triage, inspections,
utility re-energization, and equitable resource allocation. Overhead imagery
provides rapid coverage but often misses facade and access cues that determine
habitability, while street-view imagery captures those details but is sparse
and difficult to align with parcels. We present FacadeTrack, a street-level,
language-guided framework that links panoramic video to parcels, rectifies
views to facades, and elicits interpretable attributes (for example, entry
blockage, temporary coverings, localized debris) that drive two decision
strategies: a transparent one-stage rule and a two-stage design that separates
perception from conservative reasoning. Evaluated across two post-Hurricane
Helene surveys, the two-stage approach achieves a precision of 0.927, a recall
of 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a
precision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond
accuracy, intermediate attributes and spatial diagnostics reveal where and why
residual errors occur, enabling targeted quality control. The pipeline provides
auditable, scalable occupancy assessments suitable for integration into
geospatial and emergency-management workflows.

</details>


### [31] [Human Semantic Representations of Social Interactions from Moving Shapes](https://arxiv.org/abs/2509.20673)
*Yiling Yun,Hongjing Lu*

Main category: cs.CV

> 研究发现人类在识别动画中的社交互动时不仅依赖视觉特征，还使用语义表征，尤其是在基于动词的语义模型中表现最好。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探索人类在识别简单动画中的社交互动时除视觉特征外所使用的表征方式。

**Method:** 通过两个研究来探讨人类在识别简单动画中的社交互动时所使用的语义表征，并将其与视觉特征进行比较。研究1直接要求人类参与者根据移动形状的印象对动画进行标注。研究2测量了27种社交互动的表征几何，并通过人类相似性判断与基于视觉特征、标签和动画描述的语义嵌入的模型预测进行比较。

**Result:** 研究发现语义模型为解释人类判断提供了补充信息，特别是基于动词的嵌入从描述中提取出的信息最好地解释了人类的相似性判断。

**Conclusion:** 这些结果表明，简单动画中的社交感知反映出了社交互动的语义结构，将视觉和抽象表征联系了起来。

**Abstract:** Humans are social creatures who readily recognize various social interactions
from simple display of moving shapes. While previous research has often focused
on visual features, we examine what semantic representations that humans employ
to complement visual features. In Study 1, we directly asked human participants
to label the animations based on their impression of moving shapes. We found
that human responses were distributed. In Study 2, we measured the
representational geometry of 27 social interactions through human similarity
judgments and compared it with model predictions based on visual features,
labels, and semantic embeddings from animation descriptions. We found that
semantic models provided complementary information to visual features in
explaining human judgments. Among the semantic models, verb-based embeddings
extracted from descriptions account for human similarity judgments the best.
These results suggest that social perception in simple displays reflects the
semantic structure of social interactions, bridging visual and abstract
representations.

</details>


### [32] [Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance](https://arxiv.org/abs/2509.20684)
*Xiaowei Wang,Di Wang,Ke Li,Yifeng Wang,Chengjian Wang,Libin Sun,Zhihong Wu,Yiming Zhang,Quan Wang*

Main category: cs.CV

> EGS, a novel framework for cross-view geo-localization, uses an E(2)-Steerable CNN and a graph with a virtual super-node to enhance cross-domain generalization and establish global-local consistency.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of robustness under severe appearance variations and the establishment of reliable correspondences between global scene-level semantics and fine-grained local details in cross-view geo-localization.

**Method:** The method introduces an E(2)-Steerable CNN encoder to extract stable features under rotation and viewpoint shifts, and constructs a graph with a virtual super-node connected to all local nodes to facilitate global semantics aggregation and redistribution.

**Result:** Experiments on University-1652 and SUES-200 benchmarks show that EGS achieves significant performance improvements.

**Conclusion:** EGS sets a new state-of-the-art in cross-domain cross-view geo-localization.

**Abstract:** Cross-view geo-localization (CVGL) aims to match images of the same location
captured from drastically different viewpoints. Despite recent progress,
existing methods still face two key challenges: (1) achieving robustness under
severe appearance variations induced by diverse UAV orientations and fields of
view, which hinders cross-domain generalization, and (2) establishing reliable
correspondences that capture both global scene-level semantics and fine-grained
local details. In this paper, we propose EGS, a novel CVGL framework designed
to enhance cross-domain generalization. Specifically, we introduce an
E(2)-Steerable CNN encoder to extract stable and reliable features under
rotation and viewpoint shifts. Furthermore, we construct a graph with a virtual
super-node that connects to all local nodes, enabling global semantics to be
aggregated and redistributed to local regions, thereby enforcing global-local
consistency. Extensive experiments on the University-1652 and SUES-200
benchmarks demonstrate that EGS consistently achieves substantial performance
gains and establishes a new state of the art in cross-domain CVGL.

</details>


### [33] [DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection](https://arxiv.org/abs/2509.20701)
*Jiayi Zuo,Songwei Pei,Qian Li*

Main category: cs.CV

> 提出Dual-Path Edge Network解决红外小目标检测中高噪声和低对比度问题，结合结构语义和边缘细化提高检测精度。

<details>
  <summary>Details</summary>

**Motivation:** 本文提出Dual-Path Edge Network是为了应对红外小目标检测中的关键挑战，即在高噪声和低对比度条件下准确提取目标边缘。

**Method:** 我们提出了一种名为Dual-Path Edge Network的新方法，通过将边缘增强和语义建模分成两个互补的处理路径来解决这一挑战。第一条路径使用双向交互模块，结合局部自我注意力和全局自我注意力来捕捉多尺度局部和全局特征依赖关系。第二条路径引入多边缘精修器，利用级联的Taylor有限差分算子在多尺度上增强边缘细节，并通过注意力驱动的门控机制实现精确的边缘定位和特征增强。

**Result:** 本研究没有直接展示具体的实验结果，但表明提出的方法在捕获精细边缘细节和增强不同大小目标的特征时效果显著，有效抑制了噪声。

**Conclusion:** 我们的方法在红外小目标检测和定位方面提供了一个有前景的解决方案，将结构语义和边缘细化统一在一个框架内。

**Abstract:** Infrared small target detection is crucial for remote sensing applications
like disaster warning and maritime surveillance. However, due to the lack of
distinctive texture and morphological features, infrared small targets are
highly susceptible to blending into cluttered and noisy backgrounds. A
fundamental challenge in designing deep models for this task lies in the
inherent conflict between capturing high-resolution spatial details for minute
targets and extracting robust semantic context for larger targets, often
leading to feature misalignment and suboptimal performance. Existing methods
often rely on fixed gradient operators or simplistic attention mechanisms,
which are inadequate for accurately extracting target edges under low contrast
and high noise. In this paper, we propose a novel Dual-Path Edge Network that
explicitly addresses this challenge by decoupling edge enhancement and semantic
modeling into two complementary processing paths. The first path employs a
Bidirectional Interaction Module, which uses both Local Self-Attention and
Global Self-Attention to capture multi-scale local and global feature
dependencies. The global attention mechanism, based on a Transformer
architecture, integrates long-range semantic relationships and contextual
information, ensuring robust scene understanding. The second path introduces
the Multi-Edge Refiner, which enhances fine-grained edge details using cascaded
Taylor finite difference operators at multiple scales. This mathematical
approach, along with an attention-driven gating mechanism, enables precise edge
localization and feature enhancement for targets of varying sizes, while
effectively suppressing noise. Our method provides a promising solution for
precise infrared small target detection and localization, combining structural
semantics and edge refinement in a unified framework.

</details>


### [34] [Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset](https://arxiv.org/abs/2509.20715)
*Ruixu Zhang,Yuran Wang,Xinyi Hu,Chaoyu Mai,Wenxuan Liu,Danni Xu,Xian Zhong,Zheng Wang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Intention recognition has traditionally focused on individual intentions,
overlooking the complexities of collective intentions in group settings. To
address this limitation, we introduce the concept of group intention, which
represents shared goals emerging through the actions of multiple individuals,
and Group Intention Forecasting (GIF), a novel task that forecasts when group
intentions will occur by analyzing individual actions and interactions before
the collective goal becomes apparent. To investigate GIF in a specific
scenario, we propose SHOT, the first large-scale dataset for GIF, consisting of
1,979 basketball video clips captured from 5 camera views and annotated with 6
types of individual attributes. SHOT is designed with 3 key characteristics:
multi-individual information, multi-view adaptability, and multi-level
intention, making it well-suited for studying emerging group intentions.
Furthermore, we introduce GIFT (Group Intention ForecasTer), a framework that
extracts fine-grained individual features and models evolving group dynamics to
forecast intention emergence. Experimental results confirm the effectiveness of
SHOT and GIFT, establishing a strong foundation for future research in group
intention forecasting. The dataset is available at
https://xinyi-hu.github.io/SHOT_DATASET.

</details>


### [35] [Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection](https://arxiv.org/abs/2509.20745)
*Yu Guo,Shengfeng He,Yuxu Lu,Haonan An,Yihang Tao,Huilin Zhu,Jingxian Liu,Yuguang Fang*

Main category: cs.CV

> 论文介绍了Neptune-X框架，通过合成数据和样本选择提高了海事目标检测的性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决海事领域的两个主要挑战：标注数据的稀缺性和在不同海事属性之间泛化能力差的问题，特别是现有数据集训练的模型在开放海域等场景下的表现欠佳。

**Method:** 提出了Neptune-X框架，该框架通过多模态条件生成模型X-to-Maritime生成各种各样的现实海事场景，并利用双向物体-水注意力模块捕捉物体与其水环境之间的边界互动。此外，提出了基于任务相关性的主动采样策略以提高下游任务性能，并构建了专用于生成式海事学习的Maritime Generation Dataset数据集。

**Result:** 实验结果表明，该方法在海事场景合成中建立了新的基准，并显著提升了检测准确性，尤其是在以前代表性不足的场景中。

**Conclusion:** Neptune-X框架通过结合生成的数据和任务相关的样本选择，增强了训练的有效性，并在海事目标检测任务中展现了显著的性能提升。

**Abstract:** Maritime object detection is essential for navigation safety, surveillance,
and autonomous operations, yet constrained by two key challenges: the scarcity
of annotated maritime data and poor generalization across various maritime
attributes (e.g., object category, viewpoint, location, and imaging
environment). % In particular, models trained on existing datasets often
underperform in underrepresented scenarios such as open-sea environments. To
address these challenges, we propose Neptune-X, a data-centric
generative-selection framework that enhances training effectiveness by
leveraging synthetic data generation with task-aware sample selection. From the
generation perspective, we develop X-to-Maritime, a multi-modality-conditioned
generative model that synthesizes diverse and realistic maritime scenes. A key
component is the Bidirectional Object-Water Attention module, which captures
boundary interactions between objects and their aquatic surroundings to improve
visual fidelity. To further improve downstream tasking performance, we propose
Attribute-correlated Active Sampling, which dynamically selects synthetic
samples based on their task relevance. To support robust benchmarking, we
construct the Maritime Generation Dataset, the first dataset tailored for
generative maritime learning, encompassing a wide range of semantic conditions.
Extensive experiments demonstrate that our approach sets a new benchmark in
maritime scene synthesis, significantly improving detection accuracy,
particularly in challenging and previously underrepresented settings.The code
is available at https://github.com/gy65896/Neptune-X.

</details>
