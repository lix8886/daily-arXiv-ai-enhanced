<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [cs.CV](#cs.CV) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

> 该研究通过系统回顾多语言环境中偏见评估与缓解的方法，发现了一些方法论上的不足，并提出了未来研究的方向，以提高该领域的包容性和文化适应性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于预训练的多语言模型与处理英语文本的模型表现出相同的社会偏见，本研究旨在分析已有的研究成果，以揭示领域的研究方法选择上的不足，如对某些语言的偏好及多语言缓解实验的稀缺等。

**Method:** 本研究通过系统回顾扩展到多语言和非英语环境中的偏见评估和缓解方法的相关研究，以考察这些研究在语言多样性、文化意识以及评估指标和缓解技术选择方面的表现。

**Result:** 研究揭示了该领域在方法选择上的不足，并归纳了在跨语言和跨文化环境中使用偏见基准时遇到的常见问题及所采取的解决方案。

**Conclusion:** 研究建议未来的研究应强调多语言偏见文献的包容性、跨文化适用性和与最先进的自然语言处理技术的结合，以推动该领域的进一步发展。

**Abstract:** Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [2] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

> 研究探讨了使用语言模型自动化生成多项选择题的方法，特别重视结构化提示策略和模型微调的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索使用语言模型自动生成多项选择题以评估形态学知识，目的是降低人工考试开发的成本和不一致性。

**Method:** 该研究采用两阶段方法，首先比对了一个微调后的中型模型（Gemma, 2B）和一个较大的未经微调模型（GPT-3.5, 175B）的性能；其次评估了七种结构化提示策略，包括零样本、少量样本、链式思考、角色扮演、顺序设计和它们的组合。

**Result:** 研究表明，结构化提示策略尤其是结合了链式思考和顺序设计的策略，显著提高了Gemma模型的输出效果。Gemma模型通常生成了更多符合构建和教学要求的问题，而提示设计在中型模型性能中起到关键作用。

**Conclusion:** 该研究展示了，在数据有限的情况下，通过结构化提示和高效微调可以提升中型模型的自动试题生成能力。同时强调了结合自动化评估指标、专家判断和大型模型模拟来确保与评估目标一致性的价值。

**Abstract:** This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [3] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

> 本文提出了一个将SystemC TLM模型集成到基于FMI的协同模拟工作流程中的全开源方法，解决了时间同步和数据交换的关键技术问题，并通过案例研究展示了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 随着网络物理系统复杂性的增加，特别是汽车应用中的高效建模和跨域协同仿真技术的需求增加。然而，SystemC TLM在与其他工程领域模型的互操作性方面存在局限性，因此呈现出集成挑战。

**Method:** 本文提出了一种全开源的方法，将SystemC事务级建模(TLM)组件封装为功能模拟单元(FMUs)，以适应FMI 3.0共模拟工作流程，从而实现异构模拟环境的标准化集成。

**Result:** 通过引入一个轻量级的开源工具链，解决关键的技术挑战如时间同步和数据交换问题，通过代表性的案例研究展示了集成的可行性和有效性。

**Conclusion:** 该方法成功地展示了SystemC TLM和FMI之间的无缝集成，并通过开放源代码工具链克服了集成障碍。

**Abstract:** The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [4] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

> 本文针对紧凑型语言模型强化学习中的挑战，提出了基于知识蒸馏的策略优化方法DGPO，实验表明该方法有效且可应用于资源有限的场景。

<details>
  <summary>Details</summary>

**Motivation:** 紧凑型语言模型由于推理能力差，在强化学习中很难表现出诸如搜索和规划等agency RAG行为。本文旨在通过新型训练方法DGPO解决这些问题。

**Method:** 本文提出了Distillation-Guided Policy Optimization (DGPO)方法，通过从教师演示中冷启动初始化，并在策略优化过程中提供持续的教师指导来解决紧凑型语言模型（如0.5B参数）在强化学习中的推理能力不足的问题。

**Result:** 实验结果表明，DGPO使得紧凑型模型能够实现复杂的agency搜索行为，并在某些情况下甚至超过了更大的教师模型。

**Conclusion:** DGPO方法使在计算资源受限的环境中实现agency RAG变得可行。

**Abstract:** Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [5] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

> 本文提出了GUARD方法，用于将政府发布的道德准则转化为具体的测试问题以评估语言模型的合规性，能够自动检测大型语言模型是否遵守指导方针，并通过创建实际情境来识别潜在的违规情形。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型变得更重要，它们产生有害响应的潜在风险引起了社会和监管方面的担忧。然而，政府发布的道德准则通常是高层次的要求，因此在如何转化为具体测试问题以验证LLM合规性方面存在空缺。

**Method:** GUARD方法是通过自动化生成基于政府发布的准则的违规问题来实现针对大型语言模型的测试，通过这种方法，可以检测模型响应是否遵守指导方针。如果响应直接违反指导方针，GUARD会报告不一致之处。对于没有直接违反指导方针的响应，GUARD-JD通过创建可能引发不道德或违规回应的情境，以识别绕过内置安全机制的潜在场景。

**Result:** GUARD方法在七种大型语言模型上进行了验证，包括Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, 和Claude-3.7，通过测试它们在三种政府发布的指导方针下的合规性，显示出了该方法的有效性。GUARD-JD还能将越狱检测转移到视觉语言模型中。

**Conclusion:** 本方法通过生成具体测试案例来评估语言模型遵守政府发布的道德准则的情况，从而达成合规报告，有效地帮助语言模型开发人员测试并提高模型的合规性。

**Abstract:** As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [6] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

> 研究展示了JERR框架，这是一个用于增强大语言模型长上下文理解和透明度的新框架，实验结果表明其在多个评估指标上表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 尽管取得了显著进步，大语言模型（LLMs）在处理长上下文任务时仍面临内存限制和缺乏透明性等问题，以及产生幻觉的倾向。为了解决这些问题，提出JERR框架。

**Method:** JERR框架通过图结构化推理来增强大语言模型（LLMs）的长上下文理解。它包含三个关键组件：摘要提取、图构建和关系推理。首先，通过策略性的碎片化文本，模型可以更高效地总结和理解信息。其次，构建有向无环图（DAG）以解决冗余问题，确保逻辑一致性和清晰度。最后，引入蒙特卡洛树搜索（MCTS）帮助模型导航复杂的推理路径，确保更准确和可解释的输出。

**Result:** 实验结果表明，JERR在ROUGE和F1度量上一致优于所有基线，在LLM-Rater评估中获得最高分。

**Conclusion:** JERR框架为LLMs处理延长上下文和复杂推理任务提供了新颖解决方案，提升了可靠性和透明度。

**Abstract:** Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [7] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

> 本文提出使用NP难图问题对LLMs进行后训练，开发了一种包括监督微调和强化学习的两阶段框架，以提高推理深度和效率，实验结果表明，旗舰模型在多个领域表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 当前，建立Long CoT行为主要依赖于高质量、成本高昂和人类编纂的数据集，本研究旨在探索可扩展的其他替代方法，尤其是通过NP难图问题作为合成训练语料。

**Method:** 本论文提出了一个两阶段的后训练框架来改进大型语言模型的长链思考能力，第一阶段是通过拒绝采样的NP难图问题实例进行长链思考监督微调，以提高推理深度；第二阶段使用带有细粒度奖励设计的强化学习来提升推理效率。

**Result:** 实验表明，旗舰模型Graph-R1-7B在数学、编程、STEM和逻辑等多个领域展现了强大的泛化能力，并在NP难图问题的准确性和推理效率上超过了QwQ-32B。

**Conclusion:** 该研究为改进大型语言模型在Long CoT推理方面的能力提供了一个有效且可扩展的方法，打开了语言模型后训练的新领域。

**Abstract:** Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [8] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

> 为弥补传统性格评估方法忽视对话历史影响的不足，研究提出首个上下文感知性格评估框架（CAPE），并阐明了上下文对大语言模型LLMs响应一致性的增强效果和性格变化的影响。

<details>
  <summary>Details</summary>

**Motivation:** 现有的性格评估研究采用上下文无关的方法，这忽略了现实世界应用中的对话历史如何影响响应。CAPE框架旨在弥补这一差距。

**Method:** 提出首个针对大语言模型（LLMs）的上下文感知性格评估（CAPE）框架，该框架结合了之前的对话交互。为了全面分析上下文的影响，引入了新的度量标准来量化LLMs响应的一致性。

**Result:** 实验结果表明，对话历史通过上下文学习增强了响应的一致性，但也可能导致性格转变，尤其是在GPT-3.5-Turbo和GPT-4-Turbo中。此外，GPT模型对于问题顺序非常稳健，而Gemini-1.5-Flash和Llama-8B则显示出显著的敏感性。

**Conclusion:** 将框架应用于角色扮演代理（RPAs）的实验发现，基于上下文的性格变化提升了响应一致性，并与人类判断更加一致。

**Abstract:** Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [9] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

> 研究展示了通过条件熵衡量推理步骤的不确定性来预测推理步骤对最终答案正确性的影响，发现条件熵递减与正确答案相关，而递增则容易导致错误。为设计更高效的推理系统提供了一定的方法。

<details>
  <summary>Details</summary>

**Motivation:** 大多数大型语言模型（LLMs）依赖于生成中间推理步骤来提高准确性，但很少有工作研究推理实用性对于最终答案正确性的贡献。鉴于自回归生成的随机性，生成更多上下文并不能保证对答案的信心增加。

**Method:** 通过使用Qwen2.5-32B和GPT-4在MATH数据集上生成推理链，并利用Qwen3-8B来量化这些链对于最终正确性的有用性，测量在每个推理步骤中答案跨度Y的不确定性，用逐步扩展上下文的条件熵进行衡量。

**Result:** 实验结果表明，条件熵随着时间步骤递减与正确答案有很强的相关性，而平缓或增加的熵通常会导致错误的答案。此外，错误的推理路径往往比正确的长，表明较长的推理并不一定带来更好的结果。

**Conclusion:** 这些发现为设计能够早期识别并避免无效推理的高效推理流水线奠定了基础。

**Abstract:** Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [10] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

> 我们开发了UI-Bench，一个评估AI文本到应用工具视觉优越性的基准测试，通过专家评估进行排名，并纳入了可信赖的估算方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前AI文本到应用的工具承诺能在几分钟内生成高质量的应用程序和网站，但没有公开的基准严格验证这些说法。为了填补这一空白，我们开发了UI-Bench。

**Method:** 我们引入了UI-Bench，这是第一个大规模基准测试，通过专家两两比较来评估不同AI文本到应用工具的视觉优越性。

**Result:** UI-Bench跨越了10种工具，30个提示，300个生成的网站，以及4000多个专家的判断，使用TrueSkill衍生模型对系统进行排名，并提供了校准的信心区间。

**Conclusion:** UI-Bench建立了一个可以复制的标准，以推进AI驱动的网页设计。我们发布了完整的提示集，开源评估框架，以及一个公共排行榜。

**Abstract:** AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [11] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

> 研究引入了DentalBench，这是用于评估LLMs在牙科领域表现的首个双语基准。实验指出，在执行知识密集型和术语密集型任务时，领域适应性对模型性能至关重要。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大语言模型和医疗LLMs在一般医疗基准测试中表现出色，但在需要更深领域专业知识的特定医疗领域中的表现仍未得到充分研究，主要是因为缺乏针对特定领域评估的资源。

**Method:** 本研究引入了DentalBench，这是一个旨在评估和提升大语言模型（LLMs）在牙科领域表现的首个全面的双语基准。DentalBench包含两个主要部分：DentalQA，一个包含36,597个问题的英中问答基准，涵盖4项任务和16个牙科子领域；以及DentalCorpus，一个包含3.37亿个令牌的大型高质量语料库，用于牙科领域的适应，支持监督微调（SFT）和检索增强生成（RAG）。

**Result:** 对14个LLMs进行了评估，涵盖了专有、开源和特定医疗模型，并揭示了不同类型任务和不同语言中的显著性能差距。进一步使用Qwen-2.5-3B进行的实验表明，领域适应显著提升了模型在知识密集型和术语密集型任务上的表现。

**Conclusion:** 实验结果强调了专业领域基准对于开发值得信赖且有效的、专为医疗应用定制的LLMs的重要性。

**Abstract:** Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [12] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

> 本文提出了KG-CQR框架，利用知识图谱增强检索阶段的复杂查询上下文表示，无须额外的训练即可应用于各种规模的语言模型，并在多项实验中显示出优于现有基线模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在通过提出KG-CQR框架，利用基于语料库的知识图谱增强复杂查询的上下文表示，以改进检索增强生成系统中的检索阶段，相较于现有方法，该框架侧重于通过结构化关系表示来丰富查询。

**Method:** KG-CQR框架通过从知识图谱中抽取和补全相关子图来增强复杂查询的上下文表示，这包括子图抽取、子图补全和上下文生成三个模块，作为一个模型不可知的管道，适用于各种规模的语言模型且无需额外训练。

**Result:** 实验结果表明，KG-CQR在RAGBench和MultiHop-RAG数据集上显示出更优性能，mAP提升了4-6%，Recall@25提升了2-3%，并在多跳问答等复杂RAG任务中也显示出一致的性能优势。

**Conclusion:** 该研究证明了KG-CQR框架能有效增强检索增强生成系统的性能，尤其是在复杂查询和多跳问答任务上。

**Abstract:** The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [13] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

> 开发了一个针对民用航空维修领域的基准测试，以评估和改进大语言模型在特定领域的性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了填补专用评估工具在民用航空维修领域的大语言模型方面的空白，从而识别知识缺口和复杂推理能力的不足。

**Method:** 提出并开发了一个专门针对民用航空维修领域的工业级基准测试，旨在评估大语言模型在此特定领域的能力。

**Result:** 通过实验探索和分析，展示了该基准测试在评估这一领域模型性能方面的有效性。

**Conclusion:** 此工作填补了当前大语言模型评估中主要集中在数学和编码推理任务的空白，并开源了该评估基准和代码，以促进进一步的研究和发展。

**Abstract:** Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [14] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

> 提出CHAIR-DPO方法，利用CHAIR指标和直接偏好优化技术减少多模态大型语言模型的幻觉现象，结果表明该方法在多个基准测试中有效。

<details>
  <summary>Details</summary>

**Motivation:** 解决多模态大型语言模型（MLLM）在生成答案时倾向于产生幻觉的问题，即生成的答案在视觉输入中未被反映。

**Method:** 通过利用CHAIR指标区别生成答案，并利用直接偏好优化（DPO）微调现成的多模态大型语言模型，以减少模型生成答案时的幻觉现象。

**Result:** 在多个幻觉基准测试中，CHAIR-DPO方法显著减少了幻觉答案的数量，显示了利用CHAIR奖励微调MLLM的有效性。

**Conclusion:** CHAIR-DPO方法能有效减少多模态大型语言模型的幻觉现象，证明了利用CHAIR指标进行模型微调的有效性。

**Abstract:** Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [15] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

> Researchers integrate the multi-modal architecture of Stable Diffusion (SD) into an image forensic framework to improve forgery localization, achieving up to 12% better performance on benchmark datasets and showing strong results on real-world data.

<details>
  <summary>Details</summary>

**Motivation:** The rapid advancement in image manipulation technologies challenges existing forgery localization methods, which depend heavily on annotated data. Our paper aims to overcome these limitations by integrating the capabilities of SD into image forensic frameworks.

**Method:** We leverage the multi-modal architecture of Stable Diffusion (SD), specifically SD3, to enhance image forgery localization by treating forgery residuals as an explicit modality in the latent space. This approach theoretically allows SD to condition on forgery-related information, thus generating forgery localization results inherently.

**Result:** Experiments show our framework achieves up to 12% performance improvements over state-of-the-art models on benchmark datasets. The method also performs well on real-world data unseen during training.

**Conclusion:** Our proposed method significantly enhances forgery localization accuracy and efficiency by leveraging the multimodal SD framework, showing promising results even on unseen real-world data.

**Abstract:** Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [16] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [17] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

> 本研究提出了一种集成了监督学习、自监督学习和信号重建目标的统一Vision Transformer框架，解决了现有AMR方法中依赖大量标注数据和复杂训练流程的问题，并在RML2018.01A数据集上验证了其在低标签率下的优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 自动调制识别（AMR）对于认知无线电、频谱监测和安全无线通信至关重要。然而，现有的解决方案通常依赖于大规模标注数据集或分阶段的训练流程，限制了它在实际应用中的扩展性和泛化性能。

**Method:** 我们提出了一种统一的Vision Transformer (ViT)框架，该框架结合了监督学习、自监督学习以及信号重建目标。模型包括一个ViT编码器、一个轻量级卷积解码器和一个线性分类器；通过重建分支将增强后的信号映射回原始信号，使得编码器可以锚定到精细的I/Q结构上。这种方法在预训练阶段促进了鲁棒的、判别特征的学习，细调阶段的部分标签监督使得在有限标签下也能实现有效的分类。

**Result:** 在RML2018.01A数据集上，我们提出的方法在低标签率情况下优于监督CNN和ViT基线，在仅有15-20%的标签数据时接近ResNet的准确性，并在不同的信噪比水平下保持了强劲的表现。

**Conclusion:** 总体而言，提出的框架为AMR提供了一种简单、通用且少标签有效的解决方案。

**Abstract:** Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [18] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

> InfinityHuman是一种用于高分辨率、长时长的音频驱动人类动画生成的新框架，解决了现有技术在准确性、一致性及手部动作表现上的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法在生成高分辨率、长时长视频时面临挑战，包括一致性外观和自然手部动作的生成。这些方法通常会累积误差，导致身份漂移、色彩偏移和场景不稳定，同时手部动作不佳，导致显著的失真和与音频的错位。该工作的动机正是解决这些问题。

**Method:** _INFINITYHUMAN_采用了一种从粗到细的框架，首先生成与音频同步的表示，然后通过姿势引导的细化器逐步将其细化为高分辨率、长时长的视频。为了减少偏移并改进唇同步，该细化器使用稳定的姿势和初始帧作为视觉锚点。此外，为了提高语义准确性和手势真实性，引入了一种基于高质量手部运动数据训练的手部特定奖励机制。

**Result:** 实验表明_INFINITYHUMAN_在视频质量、身份保持、手部精度和唇同步方面达到了最先进的性能。消融研究表明每个模块的有效性。

**Conclusion:** InfinityHuman展示了一个创新方法来提高长视频片段的生成质量，不仅提高了整体视频的准确性，尤其是在手部动作和唇同步方面的表现。此方法具有重要的实用价值，并在实验中验证了其有效性。

**Abstract:** Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [19] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

> This paper presents SalViT360 and SalViT360-AV models designed for predicting saliency in 360-degree videos using spatial audio and visual cues, outperforming existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the challenges of predicting visual saliency in 360-degree videos, which include spherical distortion and integration of spatial audio. This is driven by the need for comprehensive datasets for predicting saliency in such environments.

**Method:** Our study proposes two models: SalViT360, which is a vision-transformer framework with spherical geometry-aware attention layers, and SalViT360-AV, which adds transformer adapters for audio data. The study also curates a new dataset, YT360-EyeTracking.

**Result:** The models SalViT360 and SalViT360-AV were tested on multiple datasets, including the proposed YT360-EyeTracking dataset, and showed superior performance in predicting saliency compared to existing methods.

**Conclusion:** The integration of audio cues is essential for accurately predicting saliency in 360-degree videos, as demonstrated by the outperformance of SalViT360-AV over SalViT360 alone.

**Abstract:** Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [20] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

> 研究提出了一种新的方法，利用视觉-语言模型来解释视觉模型的样本级和数据集级行为，以增强模型的可解释性并帮助发现模型的失效情况。

<details>
  <summary>Details</summary>

**Motivation:** 视觉模型的发展往往侧重于提升性能指标，如准确性、IoU和mAP，而较少关注模型的可解释性。现有的可解释人工智能（xAI）方法大多局限于对样本的解释，而缺乏对视觉模型在大范围数据集上表现行为的解释。这种对视觉模型更广泛行为的理解对于防止偏见判断和识别模型趋势很关键。

**Method:** 本研究提出了一个管道，用于在样本级和数据集级解释视觉模型。该管道使用视觉-语言模型，旨在发现失败案例并深入了解视觉模型的行为。

**Result:** 该管道成功地发现了模型的失败案例，并且让研究人员能够以最少的额外努力了解模型的行为。

**Conclusion:** 本研究的管道为视觉模型的可解释性提供了一个有效的框架，它将视觉模型的发展和xAI分析融合在一起，推进了图像分析领域。

**Abstract:** The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [21] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

> ATMS-KD is a new framework that enhances lightweight CNN models for agricultural use, surpassing current methods in Rosa damascena maturity classification accuracy and speed.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to develop lightweight CNN models suitable for resource-limited agricultural environments, specifically for the classification of Rosa damascena maturity under diverse environmental conditions.

**Method:** The method employs ATMS-KD, a framework that integrates adaptive temperature scheduling and mixed-sample augmentation to transfer knowledge from a large MobileNetV3 model to smaller residual CNN models.

**Result:** The results show significant improvements in validation accuracy (over 96.7%) and inference efficiency compared to direct training methods and existing knowledge distillation techniques.

**Conclusion:** The conclusion indicates that the ATMS-KD framework successfully facilitates effective knowledge transfer and outperforms other methods in terms of accuracy and speed, making it a promising approach for agricultural computer vision tasks.

**Abstract:** This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [22] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

> 研究提出了一种结合视觉-语言表示的框架，用于快速可靠地认证先进材料，特别是在增材制造工艺中。该框架通过深度语义分割和混合多模态模型实现，并能在未见过的样本上进行有效的零样本分类。

<details>
  <summary>Details</summary>

**Motivation:** 快速可靠地对先进材料进行评估是工业制造中的一个瓶颈，尤其是对于通过非传统增材制造工艺生产的异质结构。为了克服这一瓶颈，引入了一种结合专家知识与视觉-语言表示的新框架。

**Method:** 该研究提出了一种新的框架，通过定制和混合视觉-语言表示（VLRs），将微结构信息学与一系列专家表征知识联系起来。该框架通过深度语义分割，结合预训练的多模态模型（如CLIP和FLAVA），将视觉微结构数据和文本专家评估信息编码为共享表示形式。开发了一种基于相似性的定制表示方法，它包含从专家注释图像及其相关文本描述中获得的正、负参考，从而实现对之前未见过的微结构的零样本分类。

**Result:** 在通过增材制造方式制备的金属基复合材料数据集上进行验证，该框架可分辨合格与不合格样本，表现出较高的区分能力。COMPARATIVE分析显示，FLAVA模型提供了更高的视觉敏感度，而CLIP模型与文本标准的一致性更强。此外，通过Z-score归一化调整了原始单模态和跨模态相似性得分，使得混合视觉-语言框架中的对齐和分类更加有效。

**Conclusion:** 提出的框架通过增强数据与专家知识之间语义互操作性，提高了资格认证过程中的可追溯性和可解释性，实现了人类在回路决策，不需要特定任务的模型重训练。该成果有助于工程信息学中可扩展和领域适应性认证策略的发展。

**Abstract:** Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [23] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [24] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

> 介绍了一种无训练的反馈机制，用于改进视觉基础模型的开放词汇分割任务。通过将输出的预测反馈到中间注意力以提升空间一致性，框架无需额外训练即可提升现有模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** CLIP虽然表现出优秀的视觉-文本对齐能力，但在开放词汇分割任务上表现不佳，原因是定位效果差。前人尝试通过修改中间注意力来增强空间一致性，但因为后续操作如投影等因素，这种一致性未能有效地传递到最终输出。此外，中间注意力缺乏与文本表示的直接交互，因此限制了CLIP的潜力。

**Method:** 提出了一种无训练、反馈驱动的自适应框架，该框架可将基于输出的补丁级对应关系反馈回中间注意力。通过设计关键模块，包括注意力隔离、基于置信度的稀疏适应裁剪和适应集成，有效地将输出一致性的线索反馈回来。

**Result:** 在四种最先进的方法中，使用三种不同的backbone（ViT-B, ViT-L, ViT-H）进行了验证，且在三种不同的注意力类型（Q-K, self-self，代理加入MAE, SAM和DINO）上进行了验证，展示了在八个基准测试中的稳定性能提升。

**Conclusion:** 所提出的框架作为一种插件模块，增强了内部表示和最终预测之间的语义一致性，验证表明，该方法在多种注意力机制和基准测试中均表现出高水平的性能提升。

**Abstract:** CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [25] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

> 研究描述了一个探查框架，用来系统分析多模态大规模语言模型如何处理视觉和文本输入，揭示了分层功能角色并提供了统一的视角和分析路径。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大规模语言模型（MLLMs）在广泛的视觉-语言任务中表现出色，但其内部处理动态仍不为人知。这项工作旨在探索这一领域。

**Method:** 介绍了一个探查框架，系统地分析了MLLMs如何跨层处理视觉和文本输入。通过训练线性分类器，从每个层中提取的标记嵌入来预测细粒度的视觉类别（例如，狗的品种）。另外，使用三种类型的受控提示变化来评估这些探针：（1）测试对表面级别变化敏感性的词汇变体，（2）通过修改提示中的视觉概念来翻转预期答案的语义否定变体，（3）保留推理但改变答案格式的输出格式变体。

**Result:** 应用该框架于LLaVA-1.5、LLaVA-Next-LLaMA-3和Qwen2-VL，识别出一致的分阶段结构。

**Conclusion:** 发现了可以跨模型推广的分阶段结构，即早期层执行视觉定位，中期层支持词汇整合和语义推理，最终层准备任务特定输出。此外，虽然全局阶段结构在视觉标记化、指令调优数据和预训练语料库的变异情况下保持稳定，但特定层每个阶段的分配有明显的变化。

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [26] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

> 本文提出了一种新的监督字典学习方法，用于估计复杂视觉场景的概念特定组件，并通过该方法优化了一个具有组结构的字典。最终展示了在视觉语言模型中提供更精确图像检索的能力。

<details>
  <summary>Details</summary>

**Motivation:** 我们假设嵌入空间可以通过分解嵌入来分离复杂场景中各个内容信息，将其分解为处于不同子空间中的多个特定概念成分向量。基于这一点，我们想探究能否通过字典学习方法来证明这一假设，并提供更精确的图像检索功能。

**Method:** 我们提出了一种有监督的字典学习方法来估计一个线性合成模型，该模型由字典中的原子向量组的稀疏、非负组合构成，这些原子向量组的活动模式与多标签信息相匹配。每个特定概念的组件是与特定标签关联的原子向量组的非负组合。我们通过一种具有保证收敛性的交替优化方法来优化这种带有分组结构的字典。

**Result:** 我们展示了通过我们的稀疏线性概念子空间（SLiCS），能够提供更精确的概念过滤图像检索，并且适用于高度压缩的自编码器嵌入物TiTok和来自自我监督学习的DINOv2的潜在嵌入空间。实验结果显示，对于所有嵌入，概念过滤的图像检索精度都得到了提升。

**Conclusion:** 研究结果表明，我们的方法可以提供分离开来的嵌入空间，这些嵌入空间使概念过滤图像检索（以及基于图像转提示的有条件生成）变得更加精确。这些成果展示出SLiCS在处理多标签信息和提供精确图像检索方面的能力。

**Abstract:** Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [27] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

> 摘要：开发了MedFoundationHub，用于安全使用医疗视觉语言模型，并通过专家评估，发现了模型的一些局限性。

<details>
  <summary>Details</summary>

**Motivation:** 动机：医疗视觉语言模型虽然在临床应用中有巨大潜力，但也带来了一些安全问题，例如隐私保护和数据泄露等。因此，开发了一个工具包来解决这些问题，并评估这些模型的表现。

**Method:** Content分析：介绍了MedFoundationHub工具包，该工具包可以帮助医生和工程师安全使用医疗视觉语言模型（VLMs），同时确保隐私保护。评估了五种最先进的VLM模型在临床场景中的表现，通过专家评估了其在结肠和肾病例上的表现。

**Result:** 结果：评估结果显示，模型存在一些问题，包括偏离目标的答案、模棱两可的推理逻辑和病理学术语的不一致。

**Conclusion:** 结论：虽然医疗视觉语言模型在临床应用上有潜力，但仍存在许多挑战，如隐私保护和模型准确性问题。

**Abstract:** Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [28] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

> The paper introduces BIM, which uses BI-Scan and MS-Scan mechanisms to enable efficient and effective multi-task dense prediction, demonstrating better results on challenging benchmarks than state-of-the-art methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this work is to improve cross-task interaction in multi-task dense prediction without incurring excessive computational cost, addressing the trade-off between interaction completeness and efficiency in existing methods.

**Method:** The paper proposes a Bidirectional Interaction Mamba (BIM), which includes a Bidirectional Interaction Scan (BI-Scan) and a Multi-Scale Scan (MS-Scan) mechanism to facilitate multi-task dense prediction. BI-Scan constructs task-specific representations as bidirectional sequences to efficiently preserve cross-task information, while MS-Scan achieves multi-granularity scene modeling, enhancing feature interactions.

**Result:** Experiments on NYUD-V2 and PASCAL-Context benchmarks show the effectiveness of the proposed BIM, demonstrating its superiority over state-of-the-art methods.

**Conclusion:** The proposed BIM, incorporating BI-Scan and MS-Scan mechanisms, effectively addresses the challenge of achieving efficient yet comprehensive cross-task interaction in multi-task dense prediction.

**Abstract:** Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [29] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

> 本研究创新性地提出了一个音频引导的视觉编辑框架，能够处理复杂的编辑任务且无需额外训练，解决了现有方法的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于音频引导的视觉编辑方法通常需要在特定数据集上进行训练以对齐音频和文本，限制了它们在现实世界场景中的泛化能力。因此，开发一个能在无需额外训练的情况下处理复杂编辑任务的框架成为一种需求。

**Method:** 本研究提出了一个基于音频引导的视觉编辑框架，该框架能够处理复杂的编辑任务，而无需额外的训练。通过利用预训练的多模态编码器和缓解音频编码器空间与扩散模型提示编码器空间之间的差异，本研究成功地将多样的音频信息整合进视觉编辑任务中。此外，本研究还提出了一种通过分离噪声分支和自适应补丁选择来处理复杂多模态编辑提示的方法。

**Result:** 实验结果表明，该框架在包括复杂的编辑任务上表现出色，尤其在仅靠文本无法完成的场景下显示出优势。

**Conclusion:** 本研究提出的新框架通过利用音频引导，成功扩展了视觉编辑的边界，展示了在处理复杂任务上的潜力。

**Abstract:** Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>
