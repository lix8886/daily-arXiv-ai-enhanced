{"id": "2507.17842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17842", "abs": "https://arxiv.org/abs/2507.17842", "authors": ["Yimeng Zhang", "Tian Wang", "Jiri Gesi", "Ziyi Wang", "Yuxuan Lu", "Jiacheng Lin", "Sinong Zhan", "Vianne Gao", "Ruochen Jiao", "Junze Liu", "Kun Qian", "Yuxin Tang", "Ran Xue", "Houyu Zhang", "Qingjun Cui", "Yufan Guo", "Dakuo Wang"], "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in\ngenerating 'believable human-like' behavior in web environments. Prior work has\nexplored augmenting training data with LLM-synthesized rationales and applying\nsupervised fine-tuning (SFT) to enhance reasoning ability, which in turn can\nimprove downstream action prediction. However, the performance of such\napproaches remains inherently bounded by the reasoning capabilities of the\nmodel used to generate the rationales. In this paper, we introduce Shop-R1, a\nnovel reinforcement learning (RL) framework aimed at enhancing the reasoning\nability of LLMs for simulation of real human behavior in online shopping\nenvironments Specifically, Shop-R1 decomposes the human behavior simulation\ntask into two stages: rationale generation and action prediction, each guided\nby distinct reward signals. For rationale generation, we leverage internal\nmodel signals (e.g., logit distributions) to guide the reasoning process in a\nself-supervised manner. For action prediction, we propose a hierarchical reward\nstructure with difficulty-aware scaling to prevent reward hacking and enable\nfine-grained reward assignment. This design evaluates both high-level action\ntypes and the correctness of fine-grained sub-action details (attributes and\nvalues), rewarding outputs proportionally to their difficulty. Experimental\nresults show that our method achieves a relative improvement of over 65%\ncompared to the baseline.", "AI": {"tldr": "Introduces Shop-R1, a reinforcement learning framework that significantly improves the reasoning capability of LLMs when simulating human behavior in online shopping scenarios.", "motivation": "To improve the reasoning ability of LLMs in simulating human behavior in online shopping environments, exceeding the limitations set by current approaches which rely on the reasoning capabilities of the models they utilize.", "method": "Shop-R1, a reinforcement learning framework for enhancing reasoning ability of LLMs in simulating human behavior in online shopping. It decomposes tasks into rationale generation and action prediction, each with distinct reward signals. For rationale generation, it uses self-supervised learning with internal model signals. For action prediction, it introduces a hierarchical reward structure.", "result": "The method demonstrates a relative improvement of over 65% compared to baseline approaches.", "conclusion": "The reinforcement learning framework, Shop-R1, improves LLMs' ability to simulate human-like behavior through a bifurcated task of reasoning and action prediction with specialized reward structures."}}
{"id": "2507.17849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17849", "abs": "https://arxiv.org/abs/2507.17849", "authors": ["Zhangyue Yin", "Qiushi Sun", "Zhiyuan Zeng", "Qinyuan Cheng", "Xipeng Qiu", "Xuanjing Huang"], "title": "Dynamic and Generalizable Process Reward Modeling", "comment": "Accepted by ACL 2025 Main", "summary": "Process Reward Models (PRMs) are crucial for guiding Large Language Models\n(LLMs) in complex scenarios by providing dense reward signals. However,\nexisting PRMs primarily rely on heuristic approaches, which struggle with\ncross-domain generalization. While LLM-as-judge has been proposed to provide\ngeneralized rewards, current research has focused mainly on feedback results,\noverlooking the meaningful guidance embedded within the text. Additionally,\nstatic and coarse-grained evaluation criteria struggle to adapt to complex\nprocess supervision. To tackle these challenges, we propose Dynamic and\nGeneralizable Process Reward Modeling (DG-PRM), which features a reward tree to\ncapture and store fine-grained, multi-dimensional reward criteria. DG-PRM\ndynamically selects reward signals for step-wise reward scoring. To handle\nmultifaceted reward signals, we pioneeringly adopt Pareto dominance estimation\nto identify discriminative positive and negative pairs. Experimental results\nshow that DG-PRM achieves stunning performance on prevailing benchmarks,\nsignificantly boosting model performance across tasks with dense rewards.\nFurther analysis reveals that DG-PRM adapts well to out-of-distribution\nscenarios, demonstrating exceptional generalizability.", "AI": {"tldr": "本文提出了动态通用过程奖励模型（DG-PRM），通过奖励树捕捉细粒度、多维度奖励标准，并采用帕累托优势估计方法处理多方面奖励信号，实验表明该模型在各种基准测试和任务中表现出色，显示出优秀的泛化能力。", "motivation": "现有的过程奖励模型依赖于启发式方法，这在跨领域普遍化上表现不佳；尽管有LLM作为评判者的提出，但研究主要集中在反馈结果上，忽视了文本中的有价值指导；静态、宏观评估标准难以适应复杂的过程监督。为了应对这些挑战，本文提出了DG-PRM模型来捕捉细粒度的、多维度的奖励标准。", "method": "DG-PRM通过奖励树捕捉细粒度、多维度奖励标准，并在每一步根据需要动态选择奖励信号。此外，采用帕累托优势估计来处理多面奖励信号，识别具有区分性的正负样本对。", "result": "{", "conclusion": "实验结果表明，DG-PRM在主流基准测试上取得了显著性能，显著提高了具有密集奖励的任务模型性能，并且在分布外的场景中具有很好的适应性，展示了优秀的泛化能力。"}}
{"id": "2507.17896", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.17896", "abs": "https://arxiv.org/abs/2507.17896", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL", "comment": null, "summary": "Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity.", "AI": {"tldr": "VeriMinder is an interactive system designed to help users avoid cognitive biases in data analysis questions by introducing a contextual semantic mapping framework, an operationalized analytical framework, and an LLM-powered prompt generation system. It significantly improved the quality of analysis in user testing.", "motivation": "The paper addresses the need to help users without a background in statistical analysis to formulate bias-free analytical questions using natural language interfaces to databases.", "method": "VeriMinder introduces a contextual semantic mapping framework, an analytical framework operationalizing the Hard-to-Vary principle, and an optimized LLM-powered prompt generation system.", "result": "User testing showed that 82.5% of participants found VeriMinder improved the quality of their analysis, and it outperformed alternative approaches by at least 20% in metrics like concreteness, comprehensiveness, and accuracy.", "conclusion": "VeriMinder effectively aids users in avoiding cognitive biases during data analysis, with its web application implementation available as open-source software to foster further research and adoption."}}
{"id": "2507.17918", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17918", "abs": "https://arxiv.org/abs/2507.17918", "authors": ["Nhan Phan", "Anusha Porwal", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tamás Grósz", "Mikko Kurimo"], "title": "One Whisper to Grade Them All", "comment": "Accepted to SLaTE 2025 workshop", "summary": "We present an efficient end-to-end approach for holistic Automatic Speaking\nAssessment (ASA) of multi-part second-language tests, developed for the 2025\nSpeak & Improve Challenge. Our system's main novelty is the ability to process\nall four spoken responses with a single Whisper-small encoder, combine all\ninformation via a lightweight aggregator, and predict the final score. This\narchitecture removes the need for transcription and per-part models, cuts\ninference time, and makes ASA practical for large-scale Computer-Assisted\nLanguage Learning systems.\n  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming\nthe text-based baseline (0.44) while using at most 168M parameters (about 70%\nof Whisper-small). Furthermore, we propose a data sampling strategy, allowing\nthe model to train on only 44.8% of the speakers in the corpus and still reach\n0.383 RMSE, demonstrating improved performance on imbalanced classes and strong\ndata efficiency.", "AI": {"tldr": "本文提出了一种基于单个Whisper-small编码器的多部分语音测试评分系统，该系统减少了推理时间，并证明了其在效率和性能上的优越性。", "motivation": "本文的动机是为了开发一种更高效、更快速的语音评估系统，减少计算资源的消耗，并提高在大规模语言学习系统中的实用性。", "method": "本文提出了一种高效的一体化多部分第二语言测试自动语音评估(ASA)的方法，利用一个较小的Whisper编码器处理所有四种口语回答，并通过一个轻量级聚合器组合所有信息来预测最终评分。这种方法不需要转录和单部分模型，减少了推理时间，使ASA在大规模计算机辅助语言学习系统中变得实用。", "result": "本文所提出的方法达到了0.384的均方根误差(RMSE)，优于文本基线的0.44误差，使用了最多168M参数(约Whisper-small的70%)，并证明了一种新的数据采样策略的效用，在仅使用44.8%的语料库说话者的情况下仍然能达到0.383 RMSE。", "conclusion": "该研究证明了提出的自动语音评估系统在减少计算资源消耗和提高大规模语言学习系统实用性方面的有效性，同时表明新的数据采样策略能够在不平衡类别中提高性能和数据效率。"}}
{"id": "2507.17801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17801", "abs": "https://arxiv.org/abs/2507.17801", "authors": ["Yi Xin", "Juncheng Yan", "Qi Qin", "Zhen Li", "Dongyang Liu", "Shicheng Li", "Victor Shea-Jay Huang", "Yupeng Zhou", "Renrui Zhang", "Le Zhuo", "Tiancheng Han", "Xiaoqing Sun", "Siqi Luo", "Mengmeng Wang", "Bin Fu", "Yuewen Cao", "Hongsheng Li", "Guangtao Zhai", "Xiaohong Liu", "Yu Qiao", "Peng Gao"], "title": "Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling", "comment": "Tech Report, 23 pages, 11 figures, 7 tables", "summary": "We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model\nthat revisits and revitalizes the autoregressive paradigm for high-quality\nimage generation and beyond. Unlike existing approaches that rely on pretrained\ncomponents or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from\nscratch, enabling unrestricted architectural design and licensing freedom. It\nachieves generation quality on par with state-of-the-art diffusion models such\nas DALL-E 3 and SANA, while preserving the inherent flexibility and\ncompositionality of autoregressive modeling. Our unified tokenization scheme\nallows the model to seamlessly handle a wide spectrum of tasks-including\nsubject-driven generation, image editing, controllable synthesis, and dense\nprediction-within a single generative framework. To further boost usability, we\nincorporate efficient decoding strategies like inference-time scaling and\nspeculative Jacobi sampling to improve quality and speed, respectively.\nExtensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)\ndemonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses\ndiffusion-based models. Moreover, we confirm its multi-task capabilities on the\nGraph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally\nwell. These results position Lumina-mGPT 2.0 as a strong, flexible foundation\nmodel for unified multimodal generation. We have released our training details,\ncode, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.", "AI": {"tldr": "Lumina-mGPT 2.0 是一个完全从零开始训练的自回归模型，用于高质量图像生成，展现出与顶级扩散模型相匹敌甚至更高的性能。", "motivation": "该研究旨在重新审视并激活自回归模型在高质量图像生成领域的应用，并提供一个不依赖预训练组件或混合架构的统一生成框架。", "method": "Lumina-mGPT 2.0 是从零开始训练的独立解码器型自回归模型，支持多种图像生成任务，并在推理时引入了高效的解码策略，如推理时缩放策略和投机性 Jacobi 采样策略，以提升生成质量和速度。", "result": "Lumina-mGPT 2.0 在标准的文本到图像基准，如 GenEval 和 DPG 上表现出了与现有最先进的扩散模型相匹配甚至超过的表现，并在 Graph200K 多任务基准测试中表现出色。", "conclusion": "Lumina-mGPT 2.0 成为了一个强大且灵活的基础模型，适用于统一的多模态生成任务。"}}
{"id": "2507.17944", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17944", "abs": "https://arxiv.org/abs/2507.17944", "authors": ["Hulayyil Alshammari", "Praveen Rao"], "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text", "comment": null, "summary": "Large language models (LLMs) have rapidly transformed the creation of written\nmaterials. LLMs have led to questions about writing integrity, thereby driving\nthe creation of artificial intelligence (AI) detection technologies.\nAdversarial attacks, such as standard and humanized paraphrasing, inhibit\ndetectors' ability to detect machine-generated text. Previous studies have\nmainly focused on ChatGPT and other well-known LLMs and have shown varying\naccuracy across detectors. However, there is a clear gap in the literature\nabout DeepSeek, a recently published LLM. Therefore, in this work, we\ninvestigate whether six generally accessible AI detection tools -- AI Text\nClassifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can\nconsistently recognize text generated by DeepSeek. The detectors were exposed\nto the aforementioned adversarial attacks. We also considered DeepSeek as a\ndetector by performing few-shot prompting and chain-of-thought reasoning (CoT)\nfor classifying AI and human-written text. We collected 49 human-authored\nquestion-answer pairs from before the LLM era and generated matching responses\nusing DeepSeek-v3, producing 49 AI-generated samples. Then, we applied\nadversarial techniques such as paraphrasing and humanizing to add 196 more\nsamples. These were used to challenge detector robustness and assess accuracy\nimpact. While QuillBot and Copyleaks showed near-perfect performance on\noriginal and paraphrased DeepSeek text, others -- particularly AI Text\nClassifier and GPT-2 -- showed inconsistent results. The most effective attack\nwas humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and\n52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best\nfive-shot result misclassifying only one of 49 samples (AI recall 96%, human\nrecall 100%).", "AI": {"tldr": "研究评估了六个AI检测工具在检测DeepSeek生成文本方面的有效性，特别是在对抗攻击如人性化改写时的表现。结果显示，QuillBot和Copyleaks在原文检测中表现最优，但在对抗攻击下准确率显著下降。DeepSeek在少量样本提示和链式推理下进行检测模型时显示出较高的准确性。", "motivation": "研究动机在于探讨当代AI检测工具在检测新发布的大型语言模型（LLM）——DeepSeek生成的文本能力。鉴于现有研究主要集中在ChatGPT和其它知名LLMs而忽视了DeepSeek，这项工作填补了在这一领域的空白。", "method": "本研究通过使用六个常用的AI检测工具（AI文本分类器、内容检测AI、Copyleaks、QuillBot、GPT-2和GPTZero）来检测DeepSeek生成的文本，分析了它们在对抗攻击（包括标准和人性化的改写）中的表现。另外，研究还将DeepSeek作为一种检测器，采用少量样本提示和链式思考推理(CoT)来分类AI和人类撰写的文章。", "result": "QuillBot和Copyleaks在检测DeepSeek生成的原文和同义改写文本方面表现接近完美，其他工具特别是AI文本分类器和GPT-2则存在明显的一致性问题。最有效的攻击是人性化的处理，导致Copyleaks、QuillBot和GPTZero的准确性分别下降至71%、58%和52%。使用少量样本提示和CoT的DeepSeek检测器，其中五样本提示法的最佳结果只将49个样本之一误分类（AI召回率96%，人类召回率100%）。", "conclusion": "研究检视了现有的AI检测工具在对抗攻击下的效能，尤其是在检测DeepSeek生成文本的能力方面。结果表明，尽管一些工具如QuillBot和Copyleaks在检测DeepSeek生成的文本方面表现较好，但它们对人性化的改写攻击较为敏感，准确率显著降低。因此，为了提高检测的准确性，还需要提升现有检测工具的鲁棒性和抗攻击能力。此外，研究还提出，通过少量样本提示和链式思维推理方法，DeepSeek本身在检测任务中也展现出了很高的准确性。"}}
{"id": "2507.17844", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17844", "abs": "https://arxiv.org/abs/2507.17844", "authors": ["Sai Varun Kodathala", "Yashwanth Reddy Vutukoori", "Rakesh Vunnam"], "title": "SV3.3B: A Sports Video Understanding Model for Action Recognition", "comment": "8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025", "summary": "This paper addresses the challenge of automated sports video analysis, which\nhas traditionally been limited by computationally intensive models requiring\nserver-side processing and lacking fine-grained understanding of athletic\nmovements. Current approaches struggle to capture the nuanced biomechanical\ntransitions essential for meaningful sports analysis, often missing critical\nphases like preparation, execution, and follow-through that occur within\nseconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B\nparameter video understanding model that combines novel temporal motion\ndifference sampling with self-supervised learning for efficient on-device\ndeployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction\nmechanism that intelligently identifies the 16 most representative frames from\nsports sequences, followed by a V-DWT-JEPA2 encoder pretrained through\nmask-denoising objectives and an LLM decoder fine-tuned for sports action\ndescription generation. Evaluated on a subset of the NSVA basketball dataset,\nSV3.3B achieves superior performance across both traditional text generation\nmetrics and sports-specific evaluation criteria, outperforming larger\nclosed-source models including GPT-4o variants while maintaining significantly\nlower computational requirements. Our model demonstrates exceptional capability\nin generating technically detailed and analytically rich sports descriptions,\nachieving 29.2% improvement over GPT-4o in ground truth validation metrics,\nwith substantial improvements in information density, action complexity, and\nmeasurement precision metrics essential for comprehensive athletic analysis.\nModel Available at https://huggingface.co/sportsvision/SV3.3B.", "AI": {"tldr": "本文提出了SV3.3B，一种轻量级的33亿参数视频理解模型，改进了体育视频分析过程中对运动员动作的细致理解，该模型速度快、计算需求低，生成的体育描述详细且具有分析深度。", "motivation": "本文旨在解决自动化体育视频分析的挑战，这种方法通常受到计算密集型模型的限制，这种模型需要服务器侧处理，并且缺乏对运动员动作的细致理解。", "method": "SV3.3B模型结合了新颖的时间运动差异采样与自监督学习，用于有效的设备部署。该方法采用基于DWT-VGG16-LDA的关键帧提取机制，从体育序列中智能识别出16个最具代表性的关键帧，然后通过预先通过掩码去噪目标训练的V-DWT-JEPA2编码器和为体育动作描述生成微调的LLM解码器。", "result": "在NSVA篮球数据集的一个子集上进行评估，SV3.3B在传统文本生成指标和体育特定评估标准上均表现出色，优于包括GPT-4o变体在内的更大规模的封闭源模型，同时保持了显著较低的计算要求。", "conclusion": "该模型在综合运动分析的关键指标上优于更大规模的模型，演示出生成技术上详细、分析性丰富的体育描述的卓越能力。"}}
{"id": "2507.17951", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17951", "abs": "https://arxiv.org/abs/2507.17951", "authors": ["Sohaib Imran", "Ihor Kendiukhov", "Matthew Broerman", "Aditya Thomas", "Riccardo Campanella", "Rob Lamb", "Peter M. Atkinson"], "title": "Are LLM Belief Updates Consistent with Bayes' Theorem?", "comment": "Accepted at the ICML 2025 Workshop on Assessing World Models", "summary": "Do larger and more capable language models learn to update their \"beliefs\"\nabout propositions more consistently with Bayes' theorem when presented with\nevidence in-context? To test this, we formulate a Bayesian Coherence\nCoefficient (BCC) metric and generate a dataset with which to measure the BCC.\nWe measure BCC for multiple pre-trained-only language models across five model\nfamilies, comparing against the number of model parameters, the amount of\ntraining data, and model scores on common benchmarks. Our results provide\nevidence for our hypothesis that larger and more capable pre-trained language\nmodels assign credences that are more coherent with Bayes' theorem. These\nresults have important implications for our understanding and governance of\nLLMs.", "AI": {"tldr": "研究发现，更大的语言模型在一连串证据下，能够更一致地更新其对命题的信念，符合贝叶斯定理。", "motivation": "研究更大的语言模型是否能够更一致地按照贝叶斯定理更新其对命题的“信念”。", "method": "我们制定了一个贝叶斯一致系数（BCC）指标，并生成一个数据集来度量BCC。我们对五个模型家族中的多个仅预训练的语言模型进行了BCC测量，比较了模型参数的数量、训练数据量以及在常见基准测试上的模型得分。", "result": "结果表明，更大的语言模型赋值的一致性更高，更符合贝叶斯定理。", "conclusion": "这些结果对我们理解和治理大语言模型具有重要意义。"}}
{"id": "2507.17853", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17853", "abs": "https://arxiv.org/abs/2507.17853", "authors": ["Lifeng Chen", "Jiner Wang", "Zihao Pan", "Beier Zhu", "Xiaofeng Yang", "Chi Zhang"], "title": "Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models", "comment": null, "summary": "Recent advances in text-to-image (T2I) generation have led to impressive\nvisual results. However, these models still face significant challenges when\nhandling complex prompt, particularly those involving multiple subjects with\ndistinct attributes. Inspired by the human drawing process, which first\noutlines the composition and then incrementally adds details, we propose\nDetail++, a training-free framework that introduces a novel Progressive Detail\nInjection (PDI) strategy to address this limitation. Specifically, we decompose\na complex prompt into a sequence of simplified sub-prompts, guiding the\ngeneration process in stages. This staged generation leverages the inherent\nlayout-controlling capacity of self-attention to first ensure global\ncomposition, followed by precise refinement. To achieve accurate binding\nbetween attributes and corresponding subjects, we exploit cross-attention\nmechanisms and further introduce a Centroid Alignment Loss at test time to\nreduce binding noise and enhance attribute consistency. Extensive experiments\non T2I-CompBench and a newly constructed style composition benchmark\ndemonstrate that Detail++ significantly outperforms existing methods,\nparticularly in scenarios involving multiple objects and complex stylistic\nconditions.", "AI": {"tldr": "提出了一种新的文本到图像生成框架Detail++，该框架通过渐进式细节注入策略来改进复杂提示的处理。实验表明，相较于现有方法，Detail++在处理涉及多个对象和复杂样式条件的场景时表现更佳。", "motivation": "现有的文本到图像生成模型在处理复杂提示时，特别是在涉及多个具有不同属性的对象时，仍面临挑战。为解决这一问题，提出了Detail++框架。", "method": "采用了一种称为Detail++的无训练框架，该框架引入了一种新的渐进式细节注入策略来改进文本到图像生成。具体来说，Detail++将复杂的提示分解为一系列简化的子提示，分阶段指导生成过程。通过利用自注意力机制来控制布局，首先确保全局布局，然后进行精确的细化。为了实现属性和对应对象之间的准确绑定，还利用了交叉注意力机制，并在测试时引入了质心对齐损失来降低绑定噪音，增强属性一致性。", "result": "实验结果表明，Detail++在涉及多个对象和复杂样式条件的场景中，显著优于现有方法。", "conclusion": "Detail++通过渐进式细节注入策略改进了文本到图像生成，特别是在处理复杂样式条件和多个对象的场景中，表现优于现有方法。"}}
{"id": "2507.17974", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17974", "abs": "https://arxiv.org/abs/2507.17974", "authors": ["Fitsum Gaim", "Jong C. Park"], "title": "Natural Language Processing for Tigrinya: Current State and Future Directions", "comment": null, "summary": "Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable.\\footnote{Tigrinya NLP Anthology:\nhttps://github.com/fgaim/tigrinya-nlp-anthology.", "AI": {"tldr": "论文系统性回顾了提格雷尼亚语的自然语言处理研究，并指出从规则系统到现代神经架构的发展轨迹。", "motivation": "提格雷尼亚语作为使用广泛的语言之一，在自然语言处理研究中却受到了忽视。这项工作的动机在于总结提格雷尼亚语的NLP研究进展，并指导未来的研究。", "method": "此论文通过对超过40项研究的系统性回顾，分析了从2011年到2025年关于提格雷尼亚语自然语言处理的研究状态。", "result": "研究表明，提格雷尼亚语自然语言处理的研究已经从基础的规则系统发展到了现代的神经网络模型，并指出资源创设对研究进展的关键作用。", "conclusion": "该研究强调提格雷尼亚语的形态学复杂性和资源稀缺性是主要挑战，并提出了包括形态学感知建模和跨语言迁移等有前景的研究方向。"}}
{"id": "2507.17859", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17859", "abs": "https://arxiv.org/abs/2507.17859", "authors": ["Muayad Abujabal", "Lyes Saad Saoud", "Irfan Hussain"], "title": "FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains", "comment": null, "summary": "Accurate fish detection in underwater imagery is essential for ecological\nmonitoring, aquaculture automation, and robotic perception. However, practical\ndeployment remains limited by fragmented datasets, heterogeneous imaging\nconditions, and inconsistent evaluation protocols. To address these gaps, we\npresent \\textit{FishDet-M}, the largest unified benchmark for fish detection,\ncomprising 13 publicly available datasets spanning diverse aquatic environments\nincluding marine, brackish, occluded, and aquarium scenes. All data are\nharmonized using COCO-style annotations with both bounding boxes and\nsegmentation masks, enabling consistent and scalable cross-domain evaluation.\nWe systematically benchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.\nEvaluations are conducted using standard metrics including mAP, mAP@50, and\nmAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and\ninference profiling in terms of latency and parameter count. The results\nhighlight the varying detection performance across models trained on FishDet-M,\nas well as the trade-off between accuracy and efficiency across models of\ndifferent architectures. To support adaptive deployment, we introduce a\nCLIP-based model selection framework that leverages vision-language alignment\nto dynamically identify the most semantically appropriate detector for each\ninput image. This zero-shot selection strategy achieves high performance\nwithout requiring ensemble computation, offering a scalable solution for\nreal-time applications. FishDet-M establishes a standardized and reproducible\nplatform for evaluating object detection in complex aquatic scenes. All\ndatasets, pretrained models, and evaluation tools are publicly available to\nfacilitate future research in underwater computer vision and intelligent marine\nsystems.", "AI": {"tldr": "FishDet-M is introduced as a comprehensive benchmark and evaluation tool for underwater fish detection, enhancing the accuracy and efficiency of algorithms in diverse aquatic settings.", "motivation": "To overcome limitations such as fragmented datasets, inconsistent imaging conditions, and evaluation protocols in underwater fish detection for ecological monitoring, aquaculture automation, and robotic perception.", "method": "FishDet-M, a large unified benchmark for fish detection, which includes 13 datasets across various aquatic environments. 28 object detection models were evaluated using metrics like mAP, including scale-specific AP and inference profiling.", "result": "The study highlights varying detection performances among tested models and the trade-offs between accuracy and efficiency. It also introduces a CLIP-based model selection framework for real-time applications.", "conclusion": "FishDet-M provides a standardized platform for evaluating fish detection models in complex aquatic scenes, with all resources publicly available for further research and development in underwater computer vision and marine intelligence systems."}}
{"id": "2507.18013", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.18013", "abs": "https://arxiv.org/abs/2507.18013", "authors": ["Zihan Wang", "Xinzhang Liu", "Yitong Yao", "Chao Wang", "Yu Zhao", "Zhihao Yang", "Wenmin Deng", "Kaipeng Jia", "Jiaxin Peng", "Yuyao Huang", "Sishi Xiong", "Zhuo Jiang", "Kaidong Yu", "Xiaohui Hu", "Fubei Yao", "Ruiyu Fang", "Zhuoru Jiang", "Ruiting Song", "Qiyi Xie", "Rui Xue", "Xuewei He", "Yanlei Xue", "Zhu Yuan", "Zhaoxi Zhang", "Zilu Huang", "Shiquan Wang", "Xin Wang", "Hanming Wu", "Mingyuan Wang", "Xufeng Zhan", "Yuhan Sun", "Zhaohu Xing", "Yuhao Jiang", "Bingkai Yang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "Technical Report of TeleChat2, TeleChat2.5 and T1", "comment": "32 pages, 5 figures", "summary": "We introduce the latest series of TeleChat models: \\textbf{TeleChat2},\n\\textbf{TeleChat2.5}, and \\textbf{T1}, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith \\textbf{TeleChat2}, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. \\textbf{TeleChat2.5} and \\textbf{T1} expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The \\textbf{T1} variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, \\textbf{TeleChat2.5} prioritizes speed, delivering rapid\ninference. Both flagship models of \\textbf{T1} and \\textbf{TeleChat2.5} are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, \\textbf{T1-115B} outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release \\textbf{TeleChat2},\n\\textbf{TeleChat2.5} and \\textbf{T1}, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications.", "AI": {"tldr": "本文介绍了TeleChat系列的新成员：TeleChat2、TeleChat2.5和T1，通过改进的训练策略，而不是改变模型架构，显著提升了这些模型的性能，尤其是在代码生成和数学推理上。", "motivation": "此论文的动机是为了通过优化训练策略而不是改变模型架构，从而提升语言模型的性能，特别是针对代码生成和数学推理等特定任务。", "method": "该系列模型的主要改进在于训练策略的优化，而不是模型结构的改变。TeleChat2通过高质量和多样化的10万亿个token进行预训练，随后进行了监督微调(Supervised Fine-Tuning, SFT)和直接偏好优化(Direct Preference Optimization, DPO)。TeleChat2.5和T1则引入了以特定领域数据为源的持续预训练，并结合强化学习(Reinforcement Learning, RL)，以提升代码生成和数学推理能力。T1模型还能支持复杂的长链式推理(Chain-of-Thought, CoT)，而TeleChat2.5则侧重于加快推理速度。", "result": "TeleChat2、TeleChat2.5和T1模型在代码生成和数学推理任务上展示了显著的性能提升，特别是在复杂推理任务上。这些模型的性能已经超越了某些私有模型。", "conclusion": "这些模型展示了相较于原版TeleChat在推理能力和常规任务性能上的显著提升，特别是在数学和编程任务上的表现超越了包括OpenAI的o1-mini和GPT-4o在内的私有模型。"}}
{"id": "2507.17860", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17860", "abs": "https://arxiv.org/abs/2507.17860", "authors": ["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"], "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "comment": null, "summary": "Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.", "AI": {"tldr": "研究利用LightningDiT模型评估公共黑色素瘤分类器的公平性，发现使用逼真合成数据评价公平性很有前景，但需解决训练数据差异带来的挑战。", "motivation": "面对潜在的不公平性和偏见问题，评估和改进这类系统的公平性至关重要。评估数据集需要充分代表不同的个人身份信息（PII）（性别、年龄、种族）和其他少数群体。", "method": "利用前沿的生成式AI（GenAI）LightningDiT模型，评估公开可用的黑色素瘤分类器的公平性。", "result": "结果表明，使用高度逼真的合成数据进行公平性评估是一个有前途的方向。然而，当我们使用的黑色素瘤检测模型训练数据与合成图像的数据集不一致时，验证公平性变得更加困难。", "conclusion": "虽然存在挑战，但我们提议该方法为使用合成数据来衡量和提高医疗影像GenAI系统的公平性提供了一条宝贵的新的途径。"}}
{"id": "2507.18028", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18028", "abs": "https://arxiv.org/abs/2507.18028", "authors": ["Weizhi Fei", "Hao Shi", "Jing Xu", "Jingchen Peng", "Jiazheng Li", "Jingzhao Zhang", "Bo Bai", "Wei Han", "Zhenyuan Chen", "Xueyan Niu"], "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database", "comment": null, "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).", "AI": {"tldr": "NeuralDB, a neural Key-Value database with a gated retrieval module, improves large-scale factual editing of LLMs without compromising their general abilities.", "motivation": "To effectively edit large numbers of facts in LLMs without degrading their general abilities or leading to forgetting of edited facts during scale-up.", "method": "NeuralDB, a framework explicitly representing edited facts as a neural Key-Value database with a non-linear gated retrieval module, to address the issue of preserving general abilities of LLMs during large-scale fact editing.", "result": "NeuralDB shows superior performance in editing 10,000 facts, excelling in efficacy, generalization, specificity, fluency, and consistency. It also maintains this effectiveness when scaled to 100,000 facts.", "conclusion": "NeuralDB successfully enhances the capability of large-scale factual editing in LLMs while preserving their general abilities, marking a significant advancement in the field."}}
{"id": "2507.17892", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17892", "abs": "https://arxiv.org/abs/2507.17892", "authors": ["Hanzhou Liu", "Binghan Li", "Chengkai Liu", "Mi Lu"], "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration", "comment": null, "summary": "Transformers, with their self-attention mechanisms for modeling long-range\ndependencies, have become a dominant paradigm in image restoration tasks.\nHowever, the high computational cost of self-attention limits scalability to\nhigh-resolution images, making efficiency-quality trade-offs a key research\nfocus. To address this, Restormer employs channel-wise self-attention, which\ncomputes attention across channels instead of spatial dimensions. While\neffective, this approach may overlook localized artifacts that are crucial for\nhigh-quality image restoration. To bridge this gap, we explore Dilated\nNeighborhood Attention (DiNA) as a promising alternative, inspired by its\nsuccess in high-level vision tasks. DiNA balances global context and local\nprecision by integrating sliding-window attention with mixed dilation factors,\neffectively expanding the receptive field without excessive overhead. However,\nour preliminary experiments indicate that directly applying this global-local\ndesign to the classic deblurring task hinders accurate visual restoration,\nprimarily due to the constrained global context understanding within local\nattention. To address this, we introduce a channel-aware module that\ncomplements local attention, effectively integrating global context without\nsacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based\narchitecture specifically designed for image restoration, achieves competitive\nresults across multiple benchmarks, offering a high-quality solution for\ndiverse low-level computer vision problems.", "AI": {"tldr": "本文提出DiNAT-IR模型，通过稀疏注意力结合局部注意力模块和通道感知模块，实现了在图像修复任务中高效且高质量的处理。", "motivation": "鉴于Transformer中自注意力机制虽能捕捉长距离依赖性，但由于计算成本高昂，限制了其在高分辨率图像修复任务中的应用。为了平衡效率与质量，本文旨在改进图像修复任务中的注意力机制。", "method": "本文提出了一种名为DiNAT-IR的基于Transformer的架构，用于图像修复。该方法结合了稀疏注意力机制和局部注意力模块，通过通道感知模块集成全局上下文信息以改进像素级别的精度。", "result": "实验结果显示，DiNAT-IR在多个基准测试中达到了有竞争力的结果，为多种低级计算机视觉问题提供了高质量的解决方案。", "conclusion": "通过对通道感知模块的引入，DiNAT-IR提升了图像修复任务中的全局上下文理解和局部细腻性的结合，从而达到了高分辨率图像修复的高效和高质量。"}}
{"id": "2507.18043", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18043", "abs": "https://arxiv.org/abs/2507.18043", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs", "comment": "21 pages. Code: https://github.com/duykhuongnguyen/GrAInS", "summary": "Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.", "AI": {"tldr": "GrAInS是一种针对语言模型和视觉语言模型的推理时间引导方法，通过gradient-based归因方式构建引导向量，实验证明其有效且效率高。", "motivation": "大多数现有的推理时间引导方法依赖于固定的全局干预向量，忽视了个体输入token的因果影响，未能充分利用模型logits的指导性梯度，特别是在多模态设置下，视觉和文本输入贡献不均匀。", "method": "GrAInS通过对比、基于梯度的归因来识别对输出贡献最大（偏好和非偏好）的token，从而构建方向性引导向量，捕捉从不理想行为到理想行为的语义变化。在推理阶段，GrAInS根据token级别的归因信号调整Transformer层的隐藏激活，并进行归一化处理以保持表征尺度不变。", "result": "实证研究表明，GrAInS在TruthfulQA上使用Llama-3.1-8B时准确率提高了13.22%，使用LLaVA-1.6-7B时，在MMHal-Bench上的幻觉率从0.624降低到了0.514，并在SPA-VL上提升了8.11%的对齐获胜率，同时保持了模型的流畅性和通用能力。", "conclusion": "GrAInS提供了一种轻量级的、对语言模型和视觉语言模型的推理时间引导方法，它能够在不重新训练或提供辅助监督的情况下，实现细粒度、可解释和模块化的模型行为控制。"}}
{"id": "2507.17957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17957", "abs": "https://arxiv.org/abs/2507.17957", "authors": ["Md. Al-Masrur Khan", "Durgakant Pushp", "Lantao Liu"], "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation", "comment": null, "summary": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is\ntrained on labeled source domain data (e.g., synthetic images) and adapted to\nan unlabeled target domain (e.g., real-world images) without access to target\nannotations. Existing UDA-SS methods often struggle to balance fine-grained\nlocal details with global contextual information, leading to segmentation\nerrors in complex regions. To address this, we introduce the Adaptive Feature\nRefinement (AFR) module, which enhances segmentation accuracy by refining\nhighresolution features using semantic priors from low-resolution logits. AFR\nalso integrates high-frequency components, which capture fine-grained\nstructures and provide crucial boundary information, improving object\ndelineation. Additionally, AFR adaptively balances local and global information\nthrough uncertaintydriven attention, reducing misclassifications. Its\nlightweight design allows seamless integration into HRDA-based UDA methods,\nleading to state-of-the-art segmentation performance. Our approach improves\nexisting UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on\nSynthia-->Cityscapes. The implementation of our framework is available at:\nhttps://github.com/Masrur02/AFRDA", "AI": {"tldr": "本文提出了一种自适应特征精炼（AFR）模块，用于无监督领域适应语义分割（UDA-SS），提高了复杂区域的分割准确性。", "motivation": "现有的UDA-SS方法往往难以平衡细粒度的局部细节与全局上下文信息，导致在复杂区域出现分割错误。", "method": "AFR模块通过使用来自低分辨率对数的语义先验来增强高分辨率特征，提高分割精度。AFR还集成了捕获细粒度结构并提供重要边界信息的高频成分，改善了物体的边界描绘。AFR通过不确定性驱动的注意力机制平衡局部和全局信息，减少误分类。它的轻量级设计使其能够无缝集成到基于HRDA的UDA方法中。", "result": "AFR方法在GTA V到Cityscapes的迁移上提升了1.05%的mIoU，在Synthia到Cityscapes的迁移上提升了1.04%的mIoU。", "conclusion": "AFR模块的集成改进了现有的UDA-SS方法，提高了分割精度，且其轻量级设计便于与其他UDA方法结合使用。"}}
{"id": "2507.18044", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18044", "abs": "https://arxiv.org/abs/2507.18044", "authors": ["Hoyeon Lee", "Sejung Son", "Ye-Eun Kang", "Jong-Hwan Kim"], "title": "Synthetic Data Generation for Phrase Break Prediction with Large Language Model", "comment": "Accepted at Interspeech 2025", "summary": "Current approaches to phrase break prediction address crucial prosodic\naspects of text-to-speech systems but heavily rely on vast human annotations\nfrom audio or text, incurring significant manual effort and cost. Inherent\nvariability in the speech domain, driven by phonetic factors, further\ncomplicates acquiring consistent, high-quality data. Recently, large language\nmodels (LLMs) have shown success in addressing data challenges in NLP by\ngenerating tailored synthetic data while reducing manual annotation needs.\nMotivated by this, we explore leveraging LLM to generate synthetic phrase break\nannotations, addressing the challenges of both manual annotation and\nspeech-related tasks by comparing with traditional annotations and assessing\neffectiveness across multiple languages. Our findings suggest that LLM-based\nsynthetic data generation effectively mitigates data challenges in phrase break\nprediction and highlights the potential of LLMs as a viable solution for the\nspeech domain.", "AI": {"tldr": "本文探索使用大型语言模型生成合成短语断点注释，以减少人工标注需求，发现该方法在多种语言中有效，表明LLM在语音领域具有巨大潜力。", "motivation": "受到大型语言模型在解决NLP数据挑战方面的成功启发，本文旨在探索使用LLMs生成合成短语断点注释，以减少人工标注的需求，提高语音相关任务的质量。", "method": "本文提出的方法是利用大型语言模型（LLMs）生成合成的短语断点注释，以应对传统的文本转语音系统中依赖大量人工标注的问题，并通过与传统注释进行比较来评估这种方法在多种语言中的有效性。", "result": "研究结果表明，基于LLMs的合成数据生成有效地解决了短语断点预测中的数据挑战，并凸显了LLMs作为语音领域可行解决方案的潜力。", "conclusion": "结论是LLM生成的合成数据在缓解短语断点预测的数据挑战方面有效，并可能成为语音领域的一个可行的解决方案。"}}
{"id": "2507.17959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17959", "abs": "https://arxiv.org/abs/2507.17959", "authors": ["Ali Abedi", "Sadaf Safa", "Tracey J. F. Colella", "Shehroz S. Khan"], "title": "OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments", "comment": "14 pages, 3 figures, 7 tables", "summary": "Engagement in virtual learning is essential for participant satisfaction,\nperformance, and adherence, particularly in online education and virtual\nrehabilitation, where interactive communication plays a key role. Yet,\naccurately measuring engagement in virtual group settings remains a challenge.\nThere is increasing interest in using artificial intelligence (AI) for\nlarge-scale, real-world, automated engagement recognition. While engagement has\nbeen widely studied in younger academic populations, research and datasets\nfocused on older adults in virtual and telehealth learning settings remain\nlimited. Existing methods often neglect contextual relevance and the\nlongitudinal nature of engagement across sessions. This paper introduces OPEN\n(Older adult Patient ENgagement), a novel dataset supporting AI-driven\nengagement recognition. It was collected from eleven older adults participating\nin weekly virtual group learning sessions over six weeks as part of cardiac\nrehabilitation, producing over 35 hours of data, making it the largest dataset\nof its kind. To protect privacy, raw video is withheld; instead, the released\ndata include facial, hand, and body joint landmarks, along with affective and\nbehavioral features extracted from video. Annotations include binary engagement\nstates, affective and behavioral labels, and context-type indicators, such as\nwhether the instructor addressed the group or an individual. The dataset offers\nversions with 5-, 10-, 30-second, and variable-length samples. To demonstrate\nutility, multiple machine learning and deep learning models were trained,\nachieving engagement recognition accuracy of up to 81 percent. OPEN provides a\nscalable foundation for personalized engagement modeling in aging populations\nand contributes to broader engagement recognition research.", "AI": {"tldr": "本文介绍了一个名为OPEN的数据集，用于改善虚拟学习环境中，特别是老年人在远程心脏康复中的参与度识别。OPEN数据集具有独特的情感特征和行为特征标签以及上下文类型指标，为AI驱动的参与度模型提供了一个可扩展的基础。", "motivation": "动机在于填补针对虚拟和远程学习中老年人参与度研究和数据集的不足。传统方法忽视了参与度的上下文相关性和跨会议的纵向特性，而研究准确的虚拟学习小组环境的参与度测量则一直是具有挑战性的。", "method": "此论文介绍了OPEN数据集的创建，该数据集用于AI驱动的参与度识别。数据集来源于11位老年人在六周的每周虚拟学习小组会议中的表现，总计超过35小时的数据，是此类最大的数据集。数据集包括面部、手部、身体关节的地标以及从视频中提取的情感和行为特征。标签包括二进制参与状态、情感和行为标签，以及上下文类型指示器。", "result": "为了证明数据集的实用性，使用了多种机器学习和深度学习模型训练，达到了高达81%的参与度识别准确率。", "conclusion": "OPEN数据集为老龄化人群量身定制的参与度建模提供了一个可扩展的基础，并有助于更广泛的参与度识别研究。"}}
{"id": "2507.18055", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18055", "abs": "https://arxiv.org/abs/2507.18055", "authors": ["Tevin Atwal", "Chan Nam Tieu", "Yefeng Yuan", "Zhan Shi", "Yuhong Liu", "Liang Cheng"], "title": "Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs", "comment": null, "summary": "The increasing use of synthetic data generated by Large Language Models\n(LLMs) presents both opportunities and challenges in data-driven applications.\nWhile synthetic data provides a cost-effective, scalable alternative to\nreal-world data to facilitate model training, its diversity and privacy risks\nremain underexplored. Focusing on text-based synthetic data, we propose a\ncomprehensive set of metrics to quantitatively assess the diversity (i.e.,\nlinguistic expression, sentiment, and user perspective), and privacy (i.e.,\nre-identification risk and stylistic outliers) of synthetic datasets generated\nby several state-of-the-art LLMs. Experiment results reveal significant\nlimitations in LLMs' capabilities in generating diverse and privacy-preserving\nsynthetic data. Guided by the evaluation results, a prompt-based approach is\nproposed to enhance the diversity of synthetic reviews while preserving\nreviewer privacy.", "AI": {"tldr": "文章关注文本合成数据的多样性和隐私性评估，发现当前大语言模型生成合成数据的局限性，并提出改进方案。", "motivation": "鉴于大型语言模型生成的合成数据在数据驱动应用中带来的成本效益和可扩展性的机会，以及其多样性和隐私风险尚未得到充分研究的挑战。", "method": "提出了一套全面的指标来定量评估由几个最先进的大语言模型生成的合成数据集的多样性和隐私性。这些指标涵盖语言表达、情感和用户视角的多样性，以及再识别风险和风格异常的隐私性。", "result": "实验结果显示了这些模型在生成多样且保护隐私的合成数据方面存在显著限制。", "conclusion": "基于评估结果，提出了一种基于提示的方法，以增强合成评论的多样性同时保护评论者的隐私。"}}
{"id": "2507.17987", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17987", "abs": "https://arxiv.org/abs/2507.17987", "authors": ["Arsen Yermukan", "Pedro Machado", "Feliciano Domingos", "Isibor Kennedy Ihianle", "Jordan J. Bird", "Stefano S. K. Kaburu", "Samantha J. Ward"], "title": "Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring", "comment": null, "summary": "Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is\ntime-consuming and prone to errors. This project introduces an automated system\nfor real-time video analysis, using You Only Look Once (YOLO) object detection\nmodels to identify two key behaviours: basking and hunting. We trained five\nYOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of\n1200 images, encompassing bearded dragons (600), heating lamps (500), and\ncrickets (100). YOLOv8s was selected as the optimal model due to its superior\nbalance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes\nvideo footage by extracting per-frame object coordinates, applying temporal\ninterpolation for continuity, and using rule-based logic to classify specific\nbehaviours. Basking detection proved reliable. However, hunting detection was\nless accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).\nFuture improvements will focus on enhancing cricket detection through expanded\ndatasets or specialised small-object detectors. This automated system offers a\nscalable solution for monitoring reptile behaviour in controlled environments,\nsignificantly improving research efficiency and data quality.", "AI": {"tldr": "提出了一个自动化的实时视频分析系统，用于检测鬃狮蜥的行为，通过训练YOLO模型，系统能够可靠地检测到部分行为，但也存在一些准确性上的挑战。", "motivation": "传统的鬃狮蜥行为监测方法耗时且容易出错，因此引入了一种自动系统来实现实时视频分析，使得研究效率和数据质量得到显著提升。", "method": "使用YOLO对象检测模型来识别鬃狮蜥的关键行为，包括晒太阳和狩猎。训练了五种YOLO变体（v5, v7, v8, v11, v12）在自定义数据集上，该数据集包含1200张鬃狮蜥、加热灯和蟋蟀的图像。通过提取每帧对象的坐标、应用时间插值以及基于规则的逻辑来分类特定行为。", "result": "YOLOv8s被选为最优模型，因为它的准确性和速度达到了最佳平衡。系统在检测晒太阳行为时表现出很高的可靠性，但狩猎行为的检测准确性较低，主要原因是蟋蟀检测的准确性低（mAP@0.5 = 0.392）。", "conclusion": "该自动化系统为在受控环境中监测爬行类行为提供了一种可扩展的解决方案，显著提高了研究效率和数据质量，但是对于较小目标如蟋蟀的检测准确性还有待改善。"}}
{"id": "2507.18061", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18061", "abs": "https://arxiv.org/abs/2507.18061", "authors": ["Zehan Li", "Hongjie Chen", "Yuxin Zhang", "Jing Zhou", "Xuening Wang", "Hang Lv", "Mengjie Du", "Yaodong Song", "Jie Lian", "Jian Kang", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios", "comment": null, "summary": "Spoken language models (SLMs) have seen rapid progress in recent years, along\nwith the development of numerous benchmarks for evaluating their performance.\nHowever, most existing benchmarks primarily focus on evaluating whether SLMs\ncan perform complex tasks comparable to those tackled by large language models\n(LLMs), often failing to align with how users naturally interact in real-world\nconversational scenarios. In this paper, we propose TELEVAL, a dynamic\nbenchmark specifically designed to evaluate SLMs' effectiveness as\nconversational agents in realistic Chinese interactive settings. TELEVAL\ndefines three evaluation dimensions: Explicit Semantics, Paralinguistic and\nImplicit Semantics, and System Abilities. It adopts a dialogue format\nconsistent with real-world usage and evaluates text and audio outputs\nseparately. TELEVAL particularly focuses on the model's ability to extract\nimplicit cues from user speech and respond appropriately without additional\ninstructions. Our experiments demonstrate that despite recent progress,\nexisting SLMs still have considerable room for improvement in natural\nconversational tasks. We hope that TELEVAL can serve as a user-centered\nevaluation framework that directly reflects the user experience and contributes\nto the development of more capable dialogue-oriented SLMs.", "AI": {"tldr": "TELEVAL被设计用来评估SLMs在中文对话环境下的表现，针对现有SLMs在自然对话任务中的不足，以期推动相关技术的发展。", "motivation": "大多数现有的基准主要集中在评估SLMs是否能够执行与大型语言模型（LLMs）相当的复杂任务，但往往未能与用户在实际对话场景中的自然互动方式保持一致。为了解决这个问题，提出了TELEVAL。", "method": "我们提出了TELEVAL，一个针对中文互动环境下评估语音语言模型（SLMs）作为对话代理有效性的动态基准。TELEVAL定义了三个评估维度：显式语义、副语言和隐式语义、以及系统能力。它采用了与真实世界使用一致的对话格式，并分别评估文本和音频输出。TELEVAL特别关注模型从用户语音中提取隐式线索并适当响应的能力，无需额外指令。", "result": "实验表明，尽管最近有所进展，但现有的SLMs在自然对话任务上仍有很大的改进空间。", "conclusion": "我们希望TELEVAL能够作为一个以用户为中心的评估框架，直接反映用户体验，并有助于开发更强大的对话导向SLMs。"}}
{"id": "2507.17995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17995", "abs": "https://arxiv.org/abs/2507.17995", "authors": ["Huy Nguyen", "Kien Nguyen", "Akila Pemasiri", "Akmal Jahan", "Clinton Fookes", "Sridha Sridharan"], "title": "AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID", "comment": "Accepted atIEEE International Joint Conference on Biometrics (IJCB)\n  2025", "summary": "Person re-identification (Re-ID) across visible and infrared modalities is\ncrucial for 24-hour surveillance systems, but existing datasets primarily focus\non ground-level perspectives. While ground-based IR systems offer nighttime\ncapabilities, they suffer from occlusions, limited coverage, and vulnerability\nto obstructions--problems that aerial perspectives uniquely solve. To address\nthese limitations, we introduce AG-VPReID.VIR, the first aerial-ground\ncross-modality video-based person Re-ID dataset. This dataset captures 1,837\nidentities across 4,861 tracklets (124,855 frames) using both UAV-mounted and\nfixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents\nunique challenges including cross-viewpoint variations, modality discrepancies,\nand temporal dynamics. Additionally, we propose TCC-VPReID, a novel\nthree-stream architecture designed to address the joint challenges of\ncross-platform and cross-modality person Re-ID. Our approach bridges the domain\ngaps between aerial-ground perspectives and RGB-IR modalities, through\nstyle-robust feature learning, memory-based cross-view adaptation, and\nintermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR\npresents distinctive challenges compared to existing datasets, with our\nTCC-VPReID framework achieving significant performance gains across multiple\nevaluation protocols. Dataset and code are available at\nhttps://github.com/agvpreid25/AG-VPReID.VIR.", "AI": {"tldr": "论文提出了AG-VPReID.VIR数据集以及TCC-VPReID方法，旨在解决人物跨模态再识别问题，特别是在空中-地面视角下的问题。", "motivation": "现有的人物再识别数据集主要关注地面视角，这导致了遮挡、覆盖范围有限和易受障碍物影响的问题。为解决这些问题，该论文引入了一种新的数据集和一种新的方法。", "method": "引入了AG-VPReID.VIR，这是第一个空中-地面跨模态视频人物再识别数据集，并提出了TCC-VPReID，一种新的三流架构，旨在解决跨平台和跨模态人物再识别的挑战。", "result": "通过风格鲁棒特征学习、基于记忆的跨视图适配和引导时间建模，TCC-VPReID框架在AG-VPReID.VIR数据集上取得了显著的性能提升。", "conclusion": "实验表明，AG-VPReID.VIR提出了与现有数据集不同的挑战，并且TCC-VPReID框架在多种评估协议下实现了显著的性能提升。"}}
{"id": "2507.18076", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18076", "abs": "https://arxiv.org/abs/2507.18076", "authors": ["Haomin Qi", "Zihan Dai", "Chengbo Huang"], "title": "Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints", "comment": "10 pages, 2 figures and 1 table", "summary": "Fine-tuning large language models (LLMs) remains a computational bottleneck\ndue to their scale and memory demands. This paper presents a comprehensive\nevaluation of parameter-efficient fine-tuning (PEFT) techniques, including\nLoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that\ndynamically integrates BOFT's orthogonal stability with LoRA-GA's\ngradient-aligned rapid convergence. By computing per-layer adaptive updates\nguided by gradient norms, the hybrid method achieves superior convergence\nefficiency and generalization across diverse tasks. We also explore, for the\nfirst time, the adaptation of unitary RNN (uRNN) principles to\ntransformer-based LLMs, enhancing gradient stability through structured unitary\nconstraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,\nand HumanEval -- using models ranging from 7B to 405B parameters demonstrate\nthat our hybrid method consistently outperforms individual PEFT baselines,\napproaching full fine-tuning accuracy while reducing resource consumption by up\nto 2.1 times in training time and 50 percent in memory usage. These findings\nestablish the hybrid approach as a practical and scalable fine-tuning solution\nfor real-world deployment of LLMs under resource constraints.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.17996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17996", "abs": "https://arxiv.org/abs/2507.17996", "authors": ["Emma A. M. Stanley", "Raghav Mehta", "Mélanie Roschewitz", "Nils D. Forkert", "Ben Glocker"], "title": "Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification", "comment": "Accepted at MICCAI Workshop on Fairness of AI in Medical Imaging\n  (FAIMI) 2025", "summary": "Systematic mislabelling affecting specific subgroups (i.e., label bias) in\nmedical imaging datasets represents an understudied issue concerning the\nfairness of medical AI systems. In this work, we investigated how size and\nseparability of subgroups affected by label bias influence the learned features\nand performance of a deep learning model. Therefore, we trained deep learning\nmodels for binary tissue density classification using the EMory BrEast imaging\nDataset (EMBED), where label bias affected separable subgroups (based on\nimaging manufacturer) or non-separable \"pseudo-subgroups\". We found that\nsimulated subgroup label bias led to prominent shifts in the learned feature\nrepresentations of the models. Importantly, these shifts within the feature\nspace were dependent on both the relative size and the separability of the\nsubgroup affected by label bias. We also observed notable differences in\nsubgroup performance depending on whether a validation set with clean labels\nwas used to define the classification threshold for the model. For instance,\nwith label bias affecting the majority separable subgroup, the true positive\nrate for that subgroup fell from 0.898, when the validation set had clean\nlabels, to 0.518, when the validation set had biased labels. Our work\nrepresents a key contribution toward understanding the consequences of label\nbias on subgroup fairness in medical imaging AI.", "AI": {"tldr": "本研究发现模拟子群标签偏差导致模型学习特征表示发生了显著的变化，这些变化在特征空间中依赖于受标签偏差影响的子群的相对大小和可分离性。当验证集使用有偏的标签时，主要可分离子群的真实阳性率从0.898下降到0.518。", "motivation": "研究规模和可分离性受标签偏见影响的子组如何影响深度学习模型的学习特征和性能。", "method": "通过使用EMory BrEast影像数据集（EMBED），在受标签偏见影响的可分离子组（基于成像制造商）或非可分离‘伪子组’上训练深度学习模型，对二元组织密度分类进行了研究。", "result": "在模拟子群标签偏见时，模型学习到的特征表示发生了显著变化，这些变化与受标签偏差影响子群的相对大小和可分离性相关。此外，使用带有干净标签或有偏标签的验证集对模型分类阈值的定义有显著影响。", "conclusion": "这项研究对理解标签偏差在医学影像AI的子组公平性方面的影响至关重要。"}}
{"id": "2507.18103", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18103", "abs": "https://arxiv.org/abs/2507.18103", "authors": ["Riley Carlson", "John Bauer", "Christopher D. Manning"], "title": "A New Pair of GloVes", "comment": null, "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.", "AI": {"tldr": "本研究对新近开发的2024年GloVe模型进行了详细记录与评估，结果显示新模型在体现文化与语言变化及特定NER任务上性能提升，弥补了2014年模型在数据记录不详和性能限制上的缺陷。", "motivation": "尽管2014年版的GloVe模型已被广泛使用，鉴于语言和世界的变化，研究者认为更新的模型可能带来更好的效果。此外，2014年模型的数据版本与预处理记录不够详细，新模型进行了详细记录以弥补这一不足。", "method": "本论文介绍并评估了2024年的新的英语GloVe（全局向量词表示）模型。实验使用了来自Wikipedia、Gigaword和Dolma子集的数据进行训练。通过词汇比较、直接测试及命名实体识别任务评估了这些新模型。", "result": "评估结果显示，2024年的GloVe模型包含了更多的文化与语言相关的词汇，并且在结构任务如类比和相似性上表现相当。在一些新的、时间依赖性较强的NER数据集上，特别是非西方的新闻数据上，新模型的性能有了提升。", "conclusion": "2024年更新的GloVe模型不仅捕捉到了最新的语言演变趋势，还改善了对部分数据集的处理性能，特别是在与时间相关的命名实体识别任务上表现得更为出色。"}}
{"id": "2507.17998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17998", "abs": "https://arxiv.org/abs/2507.17998", "authors": ["Jaeho Shin", "Hyeonjae Gil", "Junwoo Jang", "Maani Ghaffari", "Ayoung Kim"], "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold", "comment": null, "summary": "Affine Grassmannian has been favored for expressing proximity between lines\nand planes due to its theoretical exactness in measuring distances among\nfeatures. Despite this advantage, the existing method can only measure the\nproximity without yielding the distance as an explicit function of rigid body\ntransformation. Thus, an optimizable distance function on the manifold has\nremained underdeveloped, stifling its application in registration problems.\nThis paper is the first to explicitly derive an optimizable cost function\nbetween two Grassmannian features with respect to rigid body transformation\n($\\mathbf{R}$ and $\\mathbf{t}$). Specifically, we present a rigorous\nmathematical proof demonstrating that the bases of high-dimensional linear\nsubspaces can serve as an explicit representation of the cost. Finally, we\npropose an optimizable cost function based on the transformed bases that can be\napplied to the registration problem of any affine subspace. Compared to vector\nparameter-based approaches, our method is able to find a globally optimal\nsolution by directly minimizing the geodesic distance which is agnostic to\nrepresentation ambiguity. The resulting cost function and its extension to the\ninlier-set maximizing \\ac{BnB} solver have been demonstrated to improve the\nconvergence of existing solutions or outperform them in various computer vision\ntasks. The code is available on\nhttps://github.com/joomeok/GrassmannRegistration.", "AI": {"tldr": "本文首次明确地推导了两个格拉斯曼特征关于刚体转换（$\\mathbf{R}$ 和 $\\mathbf{t}$）的可优化成本函数。利用高维线性子空间的基础作为成本的显式表示，提出了一个可应用于任何仿射子空间注册问题的可优化成本函数。该方法通过直接最小化测地距离，能够在参数表示的歧义性之外找到一个全局最优解。", "motivation": "虽然仿射格拉斯曼流形在表达直线和平面之间的相近性方面具有理论上的精确度，但现有的方法只能衡量相近性而不能明确给出关于刚体转换的显式距离函数，这限制了它在注册问题中的应用。为了克服这个缺陷，作者们希望开发一种可优化的成本函数，以便于在注册问题中应用仿射子空间。", "method": "<tool_call>\ndelete\n", "result": "<tool_call>\ndelete\n<tool_call>\ndelete\n<tool_call>\ndelete\n<tool_call>\ndelete\n<tool_call>\ndelete\n<tool_call>\ndelete\n<tool_call>\ndelete\n", "conclusion": "该文章展示的方法不仅改善了现有解决方案的收敛性，还在多种计算机视觉任务中超过了它们。代码可以在https://github.com/joomeok/GrassmannRegistration上找到。"}}
{"id": "2507.18119", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18119", "abs": "https://arxiv.org/abs/2507.18119", "authors": ["Hongjie Chen", "Zehan Li", "Yaodong Song", "Wenming Deng", "Yitong Yao", "Yuxin Zhang", "Hang Lv", "Xuechao Zhu", "Jian Kang", "Jie Lian", "Jie Li", "Chao Wang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness", "comment": null, "summary": "Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken\ninteractions. However, most existing models treat speech merely as a vehicle\nfor linguistic content, often overlooking the rich paralinguistic and speaker\ncharacteristic cues embedded in human speech, such as dialect, age, emotion,\nand non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel\nspoken language model with paralinguistic and speaker characteristic awareness,\ndesigned to extend spoken language modeling beyond text semantics. GOAT-SLM\nadopts a dual-modality head architecture that decouples linguistic modeling\nfrom acoustic realization, enabling robust language understanding while\nsupporting expressive and adaptive speech generation. To enhance model\nefficiency and versatility, we propose a modular, staged training strategy that\nprogressively aligns linguistic, paralinguistic, and speaker characteristic\ninformation using large-scale speech-text corpora. Experimental results on\nTELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM\nachieves well-balanced performance across both semantic and non-semantic tasks,\nand outperforms existing open-source models in handling emotion, dialectal\nvariation, and age-sensitive interactions. This work highlights the importance\nof modeling beyond linguistic content and advances the development of more\nnatural, adaptive, and socially aware spoken language systems.", "AI": {"tldr": "GOAT-SLM是一个创新的口语语言模型，能够理解和生成表达性强且适应性强的语音，并在多维度评估基准TELEVAL上表现出色。", "motivation": "现有的大多数语音语言模型仅将语音视为语言内容的载体，忽视了人类语音中嵌入丰富的副语言和说话人特征信息，如方言、年龄、情感以及非语音发声。这项研究旨在通过引入GOAT-SLM模型，扩展语音语言模型在语义之外的应用。", "method": "GOAT-SLM采用双模态头部架构，将语义建模与语音实现解耦，以实现鲁棒的语言理解和表达性强且适应性强的语音生成。同时提出了一种模块化的、分阶段的训练策略，以逐步对大规模语音-文本语料库中的语义、副语言和说话人特征信息进行对齐，提高模型效率和灵活性。", "result": "实验结果表明，GOAT-SLM在TELEVAL，一个多维评估基准上实现了语义和非语义任务的良好平衡性能，并且在处理情感、方言变化以及年龄敏感互动方面优于现有的开源模型。", "conclusion": "此研究强调了在语言模型中考虑超出语言内容建模的重要性和对未来开发自然、适应性强以及具有社会意识的口语系统的推进。"}}
{"id": "2507.18009", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18009", "abs": "https://arxiv.org/abs/2507.18009", "authors": ["Jake R. Patock", "Nicole Catherine Lewis", "Kevin McCoy", "Christina Gomez", "Canling Chen", "Lorenzo Luzi"], "title": "GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures", "comment": "12 pages, 2 figures", "summary": "State-of-the-art (SOTA) image and text generation models are multimodal\nmodels that have many similarities to large language models (LLMs). Despite\nachieving strong performances, leading foundational multimodal model\narchitectures frequently lag behind the architectural sophistication of\ncontemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner\n(CoCa) model that incorporates Gaussian error gated linear units, root mean\nsquared normalization, and rotary positional embedding into the textual\ndecoders and the vision transformer (ViT) encoder. Each architectural\nmodification has been shown to improve model performance in LLMs, but has yet\nto be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model\nwith the same modified textual decoders but with CoCa's original ViT encoder.\nWe used standard pretraining and fine-tuning workflows to benchmark the models\non contrastive and generative tasks. Our GRR-CoCa significantly outperformed\nBaseline CoCa on the pretraining dataset and three diverse fine-tuning\ndatasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in\nperplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were\n13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We\nshow that GRR-CoCa's modified architecture improves performance and\ngeneralization across vision-language domains.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "{\n  \"tldr\": \"GRR-CoCa, an improved Contrastive Captioner (CoCa) model, incorporates several architectural enhancements previously applied in large language models (LLMs) and shows significant improvements in both pretraining and fine-tuning across vision-language tasks.\", \n  \"motivation\": \"The motivation behind this paper is to address the architectural gap between large language models (LLMs) and multimodal models like CoCa, aiming to improve multi-modal model performance by incorporating sophisticated architectural components.\", \n  \"method\": \"The researchers introduce GRR-CoCa, which integrates Gaussian error gated linear units, root mean squared normalization, and rotary positional embedding into the textual decoders and the vision transformer (ViT) encoder. They compare GRR-CoCa with a model (Baseline CoCa) that shares the same modified decoders but retains the original CoCa ViT encoder.\", \n  \"result\": \"GRR-CoCa showed substantial performance gains over Baseline CoCa across all tested tasks, particularly in pretraining (27.25% in contrastive loss, 3.71% in perplexity, and 7.15% in CoCa loss) and fine-tuning (13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss).\", \n  \"conclusion\": \"The conclusion of the paper indicates that GRR-CoCa's enhanced architecture significantly boosts performance and generalization in vision-language domains, showcasing the effectiveness of incorporating advanced architectural components from LLMs in multi-modal models.\"]}", "conclusion": ""}}
{"id": "2507.18140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18140", "abs": "https://arxiv.org/abs/2507.18140", "authors": ["Xiaoyuan Li", "Moxin Li", "Wenjie Wang", "Rui Men", "Yichang Zhang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "title": "MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning", "comment": "Under Review", "summary": "Recent progress in Multi-modal Large Language Models (MLLMs) has enabled\nstep-by-step multi-modal mathematical reasoning by performing visual operations\nbased on the textual instructions. A promising approach uses code as an\nintermediate representation to precisely express and manipulate the images in\nthe reasoning steps. However, existing evaluations focus mainly on text-only\nreasoning outputs, leaving the MLLM's ability to perform accurate visual\noperations via code largely unexplored. This work takes a first step toward\naddressing that gap by evaluating MLLM's code-based capabilities in multi-modal\nmathematical reasoning.Specifically, our framework focuses on two key\nevaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's\nability to accurately understand and construct visualizations from scratch. (2)\nMulti-modal Code Editing (MCE) assesses the model's capacity for fine-grained\noperations, which include three types: Deletion, Modification and Annotation.\nTo evaluate the above tasks, we incorporate a dataset that covers the five most\npopular types of mathematical figures, including geometric diagrams, function\nplots, and three types of statistical charts, to provide a comprehensive and\neffective measurement of existing MLLMs. Our experimental evaluation involves\nnine mainstream MLLMs, and the results reveal that existing models still lag\nsignificantly behind human performance in performing fine-grained visual\noperations.", "AI": {"tldr": "The paper presents a framework for evaluating the effectiveness of Multi-modal Large Language Models (MLLMs) in multi-modal mathematical reasoning, especially in their ability to perform visual operations via code.", "motivation": "The motivation is to fill the gap in evaluating MLLM's ability to perform accurate visual operations via code in multi-modal mathematical reasoning, as current evaluations focus mainly on text-only reasoning outputs.", "method": "Content involves introducing a framework that evaluates the capabilities of Multi-modal Large Language Models (MLLMs) in multi-modal mathematical reasoning through two evaluation aspects: Multi-modal Code Generation (MCG) and Multi-modal Code Editing (MCE).", "result": "The framework evaluates the model's ability to accurately understand and construct visualizations and perform intricate operations on them, using a dataset that covers five popular types of mathematical figures. The results highlight the current models' limitations in these tasks.", "conclusion": "The experimental results, which involve nine mainstream MLLMs, reveal that existing models still lag significantly behind human performance in performing fine-grained visual operations."}}
{"id": "2507.18015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18015", "abs": "https://arxiv.org/abs/2507.18015", "authors": ["Yuezun Li", "Delong Zhu", "Xinjie Cui", "Siwei Lyu"], "title": "Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics", "comment": "https://github.com/OUC-VAS/Celeb-DF-PP", "summary": "The rapid advancement of AI technologies has significantly increased the\ndiversity of DeepFake videos circulating online, posing a pressing challenge\nfor \\textit{generalizable forensics}, \\ie, detecting a wide range of unseen\nDeepFake types using a single model. Addressing this challenge requires\ndatasets that are not only large-scale but also rich in forgery diversity.\nHowever, most existing datasets, despite their scale, include only a limited\nvariety of forgery types, making them insufficient for developing generalizable\ndetection methods. Therefore, we build upon our earlier Celeb-DF dataset and\nintroduce {Celeb-DF++}, a new large-scale and challenging video DeepFake\nbenchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers\nthree commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment\n(FR), and Talking-face (TF). Each scenario contains a substantial number of\nhigh-quality forged videos, generated using a total of 22 various recent\nDeepFake methods. These methods differ in terms of architectures, generation\npipelines, and targeted facial regions, covering the most prevalent DeepFake\ncases witnessed in the wild. We also introduce evaluation protocols for\nmeasuring the generalizability of 24 recent detection methods, highlighting the\nlimitations of existing detection methods and the difficulty of our new\ndataset.", "AI": {"tldr": "Celeb-DF++是一个新的大规模视频DeepFake数据集，覆盖了三种常见伪造类型，并包含22种不同深度伪造方法生成的高精度伪造视频。本文细化了对现有检测方法的局限性调查和数据集复杂性评估。", "motivation": "随着AI技术的快速发展，DeepFake视频在网络上的多样性显著增加。需要一个大且多样化的数据集来发展通用的DeepFake检测方法，以应对各种未见过的伪造类型。现有的大多数数据集规模虽大，但伪造类型有限，不足以支持通用检测方法的开发。", "method": "本文基于早期的Celeb-DF数据集创建了一个新的大规模视频DeepFake基准数据集Celeb-DF++。该数据集采用了22种不同的DeepFake生成方法，这些方法在架构、生成流程和目标面部区域上都有所不同，从而覆盖了常见的DeepFake案例。", "result": "本文提出了Celeb-DF++数据集，以应对多样化的DeepFake视频检测挑战。该数据集涵盖了三种常见的伪造场景：换脸(FS)、面部重新演绎(FR)和说话面孔(TF)，并使用22种不同的DeepFake生成方法生产高精度伪造视频。通过引入评估协议来衡量24种最新检测方法的泛化能力，突出了现有检测方法的局限性和新数据集的难度。", "conclusion": "通过引入评价协议来评估24种最近的检测方法的泛化性，本文揭示了现有识别方法的局限性和数据集的新挑战。因此，Celeb-DF++不仅提供了一个高质量的伪造视频资源，还提供了一个新的测试平台来评估DeepFake检测算法。"}}
{"id": "2507.18143", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18143", "abs": "https://arxiv.org/abs/2507.18143", "authors": ["Gonzalo Cardenal Antolin", "Jacques Fellay", "Bashkim Jaha", "Roger Kouyos", "Niko Beerenwinkel", "Diane Duroux"], "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support", "comment": null, "summary": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care.", "AI": {"tldr": "本研究通过HIVMedQA评估框架评估了大型语言模型在HIV管理中的应用，发现Gemini 2.5 Pro表现最佳，且医疗专业型模型并不总是优于通用型模型，模型大小也不能可靠预测性能，强调了定向开发的重要性。", "motivation": "研究大型语言模型在HIV管理中的应用，因为它是一个复杂疾病管理的典型案例，包括多样化的治疗选项、共病症和治疗依从性挑战，亟需评估其在临床环境中的准确性和潜在风险。", "method": "本研究通过HIVMedQA评估框架评估了大型语言模型在HIV管理中的表现。该框架包括七个通用型和三个医疗专业型大型语言模型，使用提示工程提升性能，评估维度包括问题理解、推理、知识回忆、偏见、潜在伤害和事实准确性。", "result": "Gemini 2.5 Pro在大多数维度上表现最佳，而医疗专业型模型并不总是优于通用型模型，模型大小也不是性能的可靠预测因素。推理和问题理解比事实回忆更具挑战性，观察到认知偏见如近期效应和现状偏见。", "conclusion": "研究结果表明需要针对开发和评估进行定向改进，以确保大型语言模型在临床护理中的安全和有效性。"}}
{"id": "2507.18023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18023", "abs": "https://arxiv.org/abs/2507.18023", "authors": ["Jun Zhou", "Dinghao Li", "Nannan Li", "Mingjie Wang"], "title": "High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details", "comment": null, "summary": "Recent advancements in multi-view 3D reconstruction and novel-view synthesis,\nparticularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting\n(3DGS), have greatly enhanced the fidelity and efficiency of 3D content\ncreation. However, inpainting 3D scenes remains a challenging task due to the\ninherent irregularity of 3D structures and the critical need for maintaining\nmulti-view consistency. In this work, we propose a novel 3D Gaussian inpainting\nframework that reconstructs complete 3D scenes by leveraging sparse inpainted\nviews. Our framework incorporates an automatic Mask Refinement Process and\nregion-wise Uncertainty-guided Optimization. Specifically, we refine the\ninpainting mask using a series of operations, including Gaussian scene\nfiltering and back-projection, enabling more accurate localization of occluded\nregions and realistic boundary restoration. Furthermore, our Uncertainty-guided\nFine-grained Optimization strategy, which estimates the importance of each\nregion across multi-view images during training, alleviates multi-view\ninconsistencies and enhances the fidelity of fine details in the inpainted\nresults. Comprehensive experiments conducted on diverse datasets demonstrate\nthat our approach outperforms existing state-of-the-art methods in both visual\nquality and view consistency.", "AI": {"tldr": "研究提出了一个利用稀疏插补视图来重建完整3D场景的新3D高斯插补框架，该框架包含自动Mask细化过程和区域不确定导向优化，以改进隐藏区域的定位和多视角的一致性，实验表明其效果优于现有方法。", "motivation": "尽管在多视角3D重建和新颖视角合成方面取得了进展，特别是通过神经辐射场（NeRF）和3D高斯点阵（3DGS），但3D场景插补仍然是一个挑战，因为它涉及3D结构的固有不规则性和保持多视角一致性的需求。", "method": "我们提出了一个新颖的3D高斯插补框架，通过利用稀疏的插补视图来重建完整的3D场景。该框架包括一个自动的Mask Refinement Process和基于区域的Uncertainty-guided Optimization。具体而言，我们使用高斯场景过滤和反投影等一系列操作来细化插补Mask，增强隐蔽区域的精确定位和真实边界的修复。此外，我们的Uncertainty-guided Fine-grained Optimization策略在训练过程中估计每个区域在多视角图像中的重要性，从而缓解了多视角的不一致问题，并提高了插补结果的精细细节的保真度。", "result": "在多样数据集上进行的综合性实验表明，我们的方法在视觉质量和视角一致性方面都优于现有的最先进的方法。", "conclusion": "所提出的方法为3D场景插补提供了一个强大而准确的解决方案，改进了多视角一致性和细节的保真度。"}}
