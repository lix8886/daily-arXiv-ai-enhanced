{"id": "2602.11156", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.11156", "abs": "https://arxiv.org/abs/2602.11156", "authors": ["Sungmoon Kim", "Hyuna Jeon", "Dahye Kim", "Mingyu Kim", "Dong-Kyu Chae", "Jiwoong Kim"], "title": "HybridRAG: A Practical LLM-based ChatBot Framework based on Pre-Generated Q&A over Raw Unstructured Documents", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for grounding Large Language Model (LLM)-based chatbot responses on external knowledge. However, existing RAG studies typically assume well-structured textual sources (e.g. Wikipedia or curated datasets) and perform retrieval and generation at query time, which can limit their applicability in real-world chatbot scenarios. In this paper, we present HybridRAG, a novel and practical RAG framework towards more accurate and faster chatbot responses. First, HybridRAG ingests raw, unstructured PDF documents containing complex layouts (text, tables, figures) via Optical Character Recognition (OCR) and layout analysis, and convert them into hierarchical text chunks. Then, it pre-generates a plausible question-answer (QA) knowledge base from the organized chunks using an LLM. At query time, user questions are matched against this QA bank to retrieve immediate answers when possible, and only if no suitable QA match is found does our framework fall back to an on-the-fly response generation. Experiments on OHRBench demonstrate that our HybridRAG provides higher answer quality and lower latency compared to a standard RAG baseline. We believe that HybridRAG could be a practical solution for real-world chatbot applications that must handle large volumes of unstructured documents and lots of users under limited computational resources.", "AI": {"tldr": "本文提出了HybridRAG框架，通过预先生成QA库并快速匹配用户查询，提高了聊天机器人响应的质量和速度。实验表明，相比标准RAG方法，HybridRAG在质量和延迟方面表现更好。", "motivation": "HybridRAG提出了一个新颖且实用的检索增强生成框架，针对现有RAG方法通常假设结构良好的文本来源以及查询时进行检索和生成的局限，致力于提高聊天机器人响应的准确性和速度，适用于无法处理大量非结构化文档和大量用户场景下的计算资源有限的实际聊天机器人应用。", "method": "HybridRAG采用光学字符识别(OCR)和布局分析技术来处理包含复杂布局（如文本、表格和插图）的原始非结构化PDF文档，并将它们转换为分层文本块。接下来，通过大型语言模型(LLM)从组织好的文本块中预先生成一个可能的问题-答案(QA)知识库。在查询时，用户的问题将与这个QA库匹配以寻找即时答案，只有在没有合适的QA匹配时，系统才会退回到即时响应生成。", "result": "通过OHRBench实验证明，与标准RAG基线相比，HybridRAG提供了更高的答案质量和更低的延迟。", "conclusion": "相信HybridRAG可以作为处理大量非结构化文档和大量用户场景下、计算资源有限的实际聊天机器人应用的实用解决方案。"}}
{"id": "2602.11157", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11157", "abs": "https://arxiv.org/abs/2602.11157", "authors": ["Max Zhang", "Derek Liu", "Kai Zhang", "Joshua Franco", "Haihao Liu"], "title": "Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety", "comment": "9 pages, Poster presented at Socially Responsible and Trustworthy Foundation Models at NeurIPS 2025 Workshop", "summary": "Large language models (LLMs) are increasingly deployed worldwide, yet their safety alignment remains predominantly English-centric. This allows for vulnerabilities in non-English contexts, especially with low-resource languages. We introduce a novel application of knowledge distillation (KD) in the context of multilingual jailbreak prevention, examining its efficacy. We distill the refusal behaviors of a proprietary teacher model (OpenAI o1-mini) with Low-Rank Adaptation (LoRA) into three open-source student models: Meta-Llama-3-8B-Instruct, Gemma-2-2B-IT, and Qwen3-8B, using ~28,000 multilingual jailbreak prompts from XSafety via black-box response-based, parameter-efficient fine-tuning (PEFT). Evaluation on the MultiJail benchmark reveals a counterintuitive behavior: standard fine-tuning on the teacher's ``safe'' refusal data inadvertently increases Jailbreak Success Rate (JSR) for all student models, up to 16.6 percentage points. Our experiments reveal a divergent generalization to unseen languages during distillation, with varying outcomes depending on the base model. By removing a primary source of safety degradation, nuanced `boundary' refusals, we mitigate or even reverse safety declines in student models, although reductions in reasoning performance (GSM8K) persist. Overall, our exploratory study highlights the challenges and potential of KD as a technique for multilingual safety alignment, offering a foundation for future research in this direction.", "AI": {"tldr": "研究表明，在多语言模型微调过程中，直接使用教师模型的安全拒绝数据进行标准微调，反向增加了逃逸成功率，展示了知识蒸馏技术用于多语言安全对齐的挑战性和潜在可能性。", "motivation": "鉴于大型语言模型（LLMs）在全球范围内越来越广泛地使用，而其安全校准多集中于英语语言环境，导致非英语环境存在漏洞，尤其是低资源语言。本文引入了一种知识蒸馏的新应用，以多语言逃逸预防为中心，评估其有效性。", "method": "本研究采用了知识蒸馏（KD）技术，在多语言逃逸预防的背景下，使用Low-Rank Adaptation（LoRA）将专有教师模型（OpenAI o1-mini）的拒绝行为迁移到三个开源学生模型：Meta-Llama-3-8B-Instruct、Gemma-2-2B-IT和Qwen3-8B。采用黑盒响应式、参数高效微调（PEFT），使用来自XSafety的大约28,000个多语言逃逸提示。", "result": "实验结果显示，标准微调在教师模型的“安全”拒绝数据上意外导致所有学生模型的逃逸成功率上升，从8.4%到25%不等。去除细腻的‘边界’拒绝行为之后，有些模型的安全性能得到了缓解乃至改善。", "conclusion": "该探索性研究表明了在多语言背景下，使用知识蒸馏技术进行安全对齐的挑战与潜力。实验中发现去除安全降解的主要来源——细腻的‘边界’拒绝行为后，可以削弱或逆转学生模型的安全下降，尽管推理表现仍有所下降。"}}
{"id": "2602.11162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.11162", "abs": "https://arxiv.org/abs/2602.11162", "authors": ["Yuping Lin", "Zitao Li", "Yue Xing", "Pengfei He", "Yingqian Cui", "Yaliang Li", "Bolin Ding", "Jingren Zhou", "Jiliang Tang"], "title": "Retrieval Heads are Dynamic", "comment": null, "summary": "Recent studies have identified \"retrieval heads\" in Large Language Models (LLMs) responsible for extracting information from input contexts. However, prior works largely rely on static statistics aggregated across datasets, identifying heads that perform retrieval on average. This perspective overlooks the fine-grained temporal dynamics of autoregressive generation. In this paper, we investigate retrieval heads from a dynamic perspective. Through extensive analysis, we establish three core claims: (1) Dynamism: Retrieval heads vary dynamically across timesteps; (2) Irreplaceability: Dynamic retrieval heads are specific at each timestep and cannot be effectively replaced by static retrieval heads; and (3) Correlation: The model's hidden state encodes a predictive signal for future retrieval head patterns, indicating an internal planning mechanism. We validate these findings on the Needle-in-a-Haystack task and a multi-hop QA task, and quantify the differences on the utility of dynamic and static retrieval heads in a Dynamic Retrieval-Augmented Generation framework. Our study provides new insights into the internal mechanisms of LLMs.", "AI": {"tldr": "本研究通过分析，发现了大语言模型中检索头部的动态性、不可替代性和相关性，并在实验中验证了这些发现。", "motivation": "此前的研究主要依赖于跨数据集的静态统计，这忽略了自回归生成的细粒度时间动态。本研究旨在从动态视角探讨检索头部。", "method": "通过广泛分析，我们确立了三个核心观点：(1) 动态性：检索头部在时间步上动态变化；(2) 不可替代性：动态检索头部在每个时间步都是特定的，不能被静态检索头部有效替代；(3) 相关性：模型的隐藏状态编码了对未来检索头部模式的预测信号，表明存在内部规划机制。", "result": "通过在Needle-in-a-Haystack任务和多跳问答任务中的验证，我们量化了动态和静态检索头部在使用上的差异。", "conclusion": "本研究为我们理解大语言模型内部机制提供了新的视角。"}}
{"id": "2602.11163", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11163", "abs": "https://arxiv.org/abs/2602.11163", "authors": ["Muhammad Haris", "Hans Höft", "Markus M. Becker", "Markus Stocker"], "title": "Nested Named Entity Recognition in Plasma Physics Research Articles", "comment": null, "summary": "Named Entity Recognition (NER) is an important task in natural language processing that aims to identify and extract key entities from unstructured text. We present a novel application of NER in plasma physics research articles and address the challenges of extracting specialized entities from scientific text in this domain. Research articles in plasma physics often contain highly complex and context-rich content that must be extracted to enable, e.g., advanced search. We propose a lightweight approach based on encoder-transformers and conditional random fields to extract (nested) named entities from plasma physics research articles. First, we annotate a plasma physics corpus with 16 classes specifically designed for the nested NER task. Second, we evaluate an entity-specific model specialization approach, where independent BERT-CRF models are trained to recognize individual entity types in plasma physics text. Third, we integrate an optimization process to systematically fine-tune hyperparameters and enhance model performance. Our work contributes to the advancement of entity recognition in plasma physics and also provides a foundation to support researchers in navigating and analyzing scientific literature.", "AI": {"tldr": "本文提出了一种新的在等离子体物理学研究文章中进行命名实体识别的方法，使用基于BERT和CRF的模型和优化过程以提高性能。", "motivation": "解决从等离子体物理学研究文章中提取专业实体的挑战，这些文章通常包含复杂且内容丰富的文本，需要被提取以实现例如高级搜索等功能。", "method": "提出了一种基于编码器-变压器和条件随机场的轻量级方法，用于从等离子体物理学研究文章中提取嵌套的命名实体。首先，我们使用16个特定于嵌套命名实体识别任务的类别对等离子体物理学语料库进行了注释。其次，我们评估了实体特定模型专业化方法，其中独立的BERT-CRF模型被训练来识别等离子体物理学文本中的单个实体类型。第三，我们整合了一个优化过程，以系统地微调超参数并增强模型性能。", "result": "工作对等离子体物理学中的实体识别有所贡献，并为支持研究人员导航和分析科学文献奠定了基础。", "conclusion": "通过该方法，我们提高了等离子体物理学研究文章中实体识别的准确性和有效性，有助于提高科学文献的检索和分析能力。"}}
{"id": "2602.11214", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.11214", "abs": "https://arxiv.org/abs/2602.11214", "authors": ["Manuel Hetzel", "Kerim Turacan", "Hannes Reichert", "Konrad Doll", "Bernhard Sick"], "title": "DD-MDN: Human Trajectory Forecasting with Diffusion-Based Dual Mixture Density Networks and Uncertainty Self-Calibration", "comment": null, "summary": "Human Trajectory Forecasting (HTF) predicts future human movements from past trajectories and environmental context, with applications in Autonomous Driving, Smart Surveillance, and Human-Robot Interaction. While prior work has focused on accuracy, social interaction modeling, and diversity, little attention has been paid to uncertainty modeling, calibration, and forecasts from short observation periods, which are crucial for downstream tasks such as path planning and collision avoidance. We propose DD-MDN, an end-to-end probabilistic HTF model that combines high positional accuracy, calibrated uncertainty, and robustness to short observations. Using a few-shot denoising diffusion backbone and a dual mixture density network, our method learns self-calibrated residence areas and probability-ranked anchor paths, from which diverse trajectory hypotheses are derived, without predefined anchors or endpoints. Experiments on the ETH/UCY, SDD, inD, and IMPTC datasets demonstrate state-of-the-art accuracy, robustness at short observation intervals, and reliable uncertainty modeling. The code is available at: https://github.com/kav-institute/ddmdn.", "AI": {"tldr": "DD-MDN improves human trajectory forecasting with high accuracy and uncertainty modeling, especially for short observation periods, across multiple datasets.", "motivation": "To address the lack of uncertainty modeling and the need for robustness at short observation periods in human trajectory forecasting, which are critical for applications like autonomous driving and surveillance.", "method": "DD-MDN is an end-to-end probabilistic human trajectory forecasting model that uses a few-shot denoising diffusion backbone and a dual mixture density network to generate diverse trajectory hypotheses without predefined anchors or endpoints.", "result": "Experiments on the ETH/UCY, SDD, inD, and IMPTC datasets show state-of-the-art accuracy, robustness at short observation intervals, and reliable uncertainty modeling.", "conclusion": "DD-MDN improves upon traditional HTF models by offering high accuracy, calibrated uncertainty, and robustness, particularly from short observation intervals, applicable to downstream tasks such as path planning and collision avoidance."}}
{"id": "2602.11165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11165", "abs": "https://arxiv.org/abs/2602.11165", "authors": ["Pushwitha Krishnappa", "Amit Das", "Vinija Jain", "Tathagata Mukherjee", "Aman Chadha"], "title": "Assessing LLM Reliability on Temporally Recent Open-Domain Questions", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed for open-domain question answering, yet their alignment with human perspectives on temporally recent information remains underexplored. We introduce RECOM (Reddit Evaluation for Correspondence of Models), a benchmark dataset of 15,000 recent Reddit questions from September 2025 paired with community-derived reference answers. We investigate how four open-source LLMs (Llama3.1-8B, Mistral-7B, Gemma-2-9B, and GPT-OSS-20B) respond to these questions, evaluating alignment using lexical metrics (BLEU, ROUGE), semantic similarity (BERTScore, MoverScore, cosine similarity), and logical inference (NLI). Our central finding is a striking semantic-lexical paradox: all models achieve over 99% cosine similarity with references despite less than 8% BLEU-1 overlap, a 90+ percentage point gap indicating that models preserve meaning through extensive paraphrasing rather than lexical reproduction. MoverScore (51-53%) confirms this pattern, occupying an intermediate position that reflects the optimal transport cost of semantic alignment. Furthermore, model scale does not predict performance: Mistral-7B (7B parameters) outperforms GPT-OSS-20B (20B parameters) across all metrics. NLI analysis reveals that contradiction rates remain below 7%, suggesting models rarely generate content that directly conflicts with human consensus. These findings challenge the reliability of lexical metrics for evaluating abstractive generation and argue for multi-dimensional evaluation frameworks that capture semantic fidelity beyond surface-level text matching. The RECOM dataset is publicly available at https://anonymous.4open.science/r/recom-D4B0", "AI": {"tldr": "研究引入了RECOM数据集，评估了四个开源语言模型与人类最近信息的理解是否对齐，发现模型能通过大量改写而保存语义内容，挑战了传统词汇度量的有效性。", "motivation": "研究动机在于探索语言模型在处理时新信息时与人类视角的对齐情况，特别是对最新信息的回答。", "method": "该研究通过RECOM基准数据集，评估了四个开源语言模型（Llama3.1-8B、Mistral-7B、Gemma-2-9B 和 GPT-OSS-20B）对Reddit社区中最新问题的回答能力。评估方法包括词汇度量、语义相似度以及逻辑推理分析。", "result": "研究发现，尽管所有模型与参考答案的词汇重叠率非常低（不足8%的BLEU-1重叠），但它们却能实现超过99%的余弦相似度，这展示了模型通过大量改写而不是简单复制来保留意义的能力。此外，模型规模并不直接决定性能。", "conclusion": "此研究挑战了用词汇度量评估抽象生成内容的可靠性，并呼吁使用多维框架来捕捉超越表面文字匹配的语义保真度。"}}
{"id": "2602.11236", "categories": ["cs.CV", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.11236", "abs": "https://arxiv.org/abs/2602.11236", "authors": ["Yandan Yang", "Shuang Zeng", "Tong Lin", "Xinyuan Chang", "Dekang Qi", "Junjin Xiao", "Haoyun Liu", "Ronghan Chen", "Yuzhi Chen", "Dongjie Huo", "Feng Xiong", "Xing Wei", "Zhiheng Ma", "Mu Xu"], "title": "ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning", "comment": "Project website: https://amap-cvlab.github.io/ABot-Manipulation/ . Code: https://github.com/amap-cvlab/ABot-Manipulation . 22 pages, 10 figures, 10 tables", "summary": "Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ''one-brain, many-forms'' paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.", "AI": {"tldr": "提出ABot-M0框架和Action Manifold Learning（AML），通过系统化数据整理、优化训练策略及引入低维动作流形假设，提高了机器人动作预测的速度和稳定性，在跨多平台和任务的多功能机器人代理领域取得了显著成效。", "motivation": "解决跨不同硬件构建通用机器人代理的挑战，当前该领域面临的数据碎片化、表现形式不一致和训练目标不一致等问题，阻碍了进步。", "method": "介绍了ABot-M0框架，该框架包括一个系统化的数据整理管道，同时优化模型架构和训练策略，将异构原始数据转换为统一、有效的表示形式。通过联合优化，统一预训练提高了跨平台和任务的知识迁移和泛化能力。引入了动作流形假设与动作流形学习(AML)，使用DiT骨干来预测干净、连续的动作序列，提高解码速度和策略稳定性。此外，提出了模块化感知机制，增强了空间理解能力。", "result": "建立了一个大规模的多功能机器人数据集UniACT-dataset，包含超过6百万条轨迹和9500小时的数据，涵盖了不同的机器人形态和任务场景，实验表明各个组件可以独立运作并且具有叠加效果。", "conclusion": "ABot-M0框架和Action Manifold Learning方法在处理多功能机器人代理任务时展现了良好的性能和稳定性，未来将公开所有代码和流水线，以支持可重复性和未来研究。"}}
{"id": "2602.11166", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11166", "abs": "https://arxiv.org/abs/2602.11166", "authors": ["Xu Hu", "Yifan Zhang", "Songtao Wei", "Chen Zhao", "Qiannan Li", "Bingzhe Li", "Feng Chen"], "title": "Small Updates, Big Doubts: Does Parameter-Efficient Fine-tuning Enhance Hallucination Detection ?", "comment": "18 pages, 13 figures, 8 tables", "summary": "Parameter-efficient fine-tuning (PEFT) methods are widely used to adapt large language models (LLMs) to downstream tasks and are often assumed to improve factual correctness. However, how the parameter-efficient fine-tuning methods affect hallucination behavior remains insufficiently understood, especially on QA datasets. In this work, we systematically investigate the impact of PEFT on hallucination detection through a comprehensive empirical study across three open-weight LLM backbones and three fact-seeking QA benchmarks. For each model, we evaluate performance using seven unsupervised hallucination detection methods spanning three complementary approaches: semantic consistency based detectors, confidence based detectors, and entropy based detectors. This multifaceted evaluation enables us to characterize how PEFT reshapes uncertainty across different detection paradigms. In conclusion, our experimental results show that PEFT consistently strengthens hallucination detection ability, substantially improving AUROC across a wide range of hallucination detectors. Besides, further analyses using linear probes and representation diagnostics indicate that PEFT methods primarily reshapes how uncertainty is encoded and surfaced, comparing with injecting new factual knowledge into the models.", "AI": {"tldr": "研究了参数高效的微调（PEFT）方法对幻觉检测的影响，证明PEFT增强了幻觉检测能力，通过各种幻觉检测方法和模型分析，发现PEFT主要改变了不确定性编码方式，而不仅仅是注入新的事实知识。", "motivation": "参数高效的微调（PEFT）方法广泛用于将大型语言模型（LLMs）适应于下游任务，通常被认为可以提高事实的准确性。然而，PEFT方法如何影响幻觉行为仍然理解不足，尤其是在QA数据集中。本研究旨在系统地探讨PEFT方法对幻觉检测的影响。", "method": "我们使用了七种无监督的幻觉检测方法，包括基于语义一致性、基于置信度和基于熵的方法，来评估三种开放权重语言模型在三种事实查找QA基准上的幻觉检测能力。", "result": "实验结果显示，PEFT方法一致提高了幻觉检测能力，并显著改善了各种幻觉检测器的AUROC。线性探针和表示诊断分析进一步表明PEFT调整了不确定性编码方式。", "conclusion": "本研究表明，PEFT可以提高幻觉检测能力，更改了模型中不确定性编码和表现的方式，而不仅仅是加入了新事实知识。"}}
{"id": "2602.11239", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11239", "abs": "https://arxiv.org/abs/2602.11239", "authors": ["Samanta Ghosh", "Jannatul Adan Mahi", "Shayan Abrar", "Md Parvez Mia", "Asaduzzaman Rayhan", "Abdul Awal Yasir", "Asaduzzaman Hridoy"], "title": "Toward Reliable Tea Leaf Disease Diagnosis Using Deep Learning Model: Enhancing Robustness With Explainable AI and Adversarial Training", "comment": "6 pages,9 figures, 2025 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE)", "summary": "Tea is a valuable asset for the economy of Bangladesh. So, tea cultivation plays an important role to boost the economy. These valuable plants are vulnerable to various kinds of leaf infections which may cause less production and low quality. It is not so easy to detect these diseases manually. It may take time and there could be some errors in the detection.Therefore, the purpose of the study is to develop an automated deep learning model for tea leaf disease classification based on the teaLeafBD dataset so that anyone can detect the diseases more easily and efficiently. There are 5,278 high-resolution images in this dataset. The images are classified into seven categories. Six of them represents various diseases and the rest one represents healthy leaves. The proposed pipeline contains data preprocessing, data splitting, adversarial training, augmentation, model training, evaluation, and comprehension made possible with Explainable AI strategies. DenseNet201 and EfficientNetB3 were employed to perform the classification task. To prepare the model more robustly, we applied adversarial training so it can operate effectively even with noisy or disturbed inputs. In addition, Grad-CAM visualization was executed to analyze the model's predictions by identifying the most influential regions of each image. Our experimental outcomes revealed that EfficientNetB3 achieved the highest classification accuracy of 93%, while DenseNet201 reached 91%. The outcomes prove that the effectiveness of the proposed approach can accurately detect tea leaf diseases and provide a practical solution for advanced agricultural management.", "AI": {"tldr": "研究开发了一种用于茶叶叶病分类的深度学习模型，提高了疾病检测的准确性和效率，能够提供一种解决农业管理问题的实际方案。", "motivation": "茶产业对孟加拉国的经济发展起着重要作用。由于茶叶植物容易感染各种叶病，这些疾病可能会导致产量减少和质量下降。人工检测发现了比较困难且容易出现错误。因此，该研究的目的是开发一个自动化深度学习系统用于茶树叶片疾病的分类，以便更便捷和高效地检测这些疾病。", "method": "开发了一种基于茶树叶片疾病的自动化深度学习模型。该模型使用了tealLeafBD数据集，其中包括了5278张高分辨率图像，这些图像被分类为七类，其中有六类是不同的疾病，一类是健康的叶子。该模型包括了数据预处理，数据分割，对抗训练，数据增强，模型训练，评估，并且使用了可解释人工智能策略。实验中使用了DenseNet201和EfficientNetB3两种模型进行分类任务。为了增强模型的健壮性，采用了对抗训练使其即使在面对噪声或干扰输入时也能有效工作。此外，还在模型预测中应用了Grad-CAM可视化以识别图像中最具影响力的区域。", "result": "实验结果表明，EfficientNetB3获得了最高的分类准确率93%，而DenseNet201则达到了91%。这些结果证明了所提出的方法能够准确地检测茶树叶片的疾病，并且可以为先进的农业管理提供切实可行的解决方案。", "conclusion": "实验结果表明，提出的深度学习模型能够有效地检测茶树叶片的疾病，这对农业管理有着重要的意义，同时也为疾病检测提供了一种新的、可靠的解决方案。"}}
{"id": "2602.11167", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11167", "abs": "https://arxiv.org/abs/2602.11167", "authors": ["Nathan Mao", "Varun Kaushik", "Shreya Shivkumar", "Parham Sharafoleslami", "Kevin Zhu", "Sunishchal Dev"], "title": "Visualizing and Benchmarking LLM Factual Hallucination Tendencies via Internal State Analysis and Clustering", "comment": null, "summary": "Large Language Models (LLMs) often hallucinate, generating nonsensical or false information that can be especially harmful in sensitive fields such as medicine or law. To study this phenomenon systematically, we introduce FalseCite, a curated dataset designed to capture and benchmark hallucinated responses induced by misleading or fabricated citations. Running GPT-4o-mini, Falcon-7B, and Mistral 7-B through FalseCite, we observed a noticeable increase in hallucination activity for false claims with deceptive citations, especially in GPT-4o-mini. Using the responses from FalseCite, we can also analyze the internal states of hallucinating models, visualizing and clustering the hidden state vectors. From this analysis, we noticed that the hidden state vectors, regardless of hallucination or non-hallucination, tend to trace out a distinct horn-like shape. Our work underscores FalseCite's potential as a foundation for evaluating and mitigating hallucinations in future LLM research.", "AI": {"tldr": "The paper introduces FalseCite, a dataset designed to identify and benchmark the hallucination caused by misleading or fabricated citations in LLMs. It also investigates the internal state vectors of the models during hallucination, showing a particular pattern in hidden states.", "motivation": "To systematically study and benchmark the tendency of LLMs to generate nonsensical or false information, especially in sensitive contexts, by introducing FalseCite.", "method": "FalseCite was used to test several LLMs for their responses to false citations, and the internal hidden state vectors of the models were analyzed during the process.", "result": "The paper found that LLMs, particularly GPT-4o-mini, show increased hallucination activity when responding to false citations. Additionally, the hidden state vectors of hallucinating models tend to form a distinct horn-shaped pattern.", "conclusion": "FalseCite can serve as an important benchmark dataset for future research aimed at understanding and mitigating the problem of hallucinations in LLMs."}}
