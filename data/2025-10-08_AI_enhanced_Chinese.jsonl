{"id": "2510.05266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05266", "abs": "https://arxiv.org/abs/2510.05266", "authors": ["Christina Thrainer", "Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Christian Guetl", "Steven Sloan", "Kendall N. Niles", "Ken Pathak"], "title": "Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation", "comment": null, "summary": "Few-shot semantic segmentation is vital for deep learning-based\ninfrastructure inspection applications, where labeled training examples are\nscarce and expensive. Although existing deep learning frameworks perform well,\nthe need for extensive labeled datasets and the inability to learn new defect\ncategories with little data are problematic. We present our Enhanced Feature\nPyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert\nand sewer defect categories using a prototypical learning framework. Our\napproach has three main contributions: (1) adaptive E-FPN encoder using\nInceptionSepConv blocks and depth-wise separable convolutions for efficient\nmulti-scale feature extraction; (2) prototypical learning with masked average\npooling for powerful prototype generation from small support examples; and (3)\nattention-based feature representation through global self-attention, local\nself-attention and cross-attention. Comprehensive experimentation on\nchallenging infrastructure inspection datasets illustrates that the method\nachieves excellent few-shot performance, with the best configuration being\n8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way\nclassification testing. The self-attention method had the most significant\nperformance improvements, providing 2.57% F1-score and 2.9% mIoU gain over\nbaselines. Our framework addresses the critical need to rapidly respond to new\ndefect types in infrastructure inspection systems with limited new training\ndata that lead to more efficient and economical maintenance plans for critical\ninfrastructure systems.", "AI": {"tldr": "我们的研究通过改进的特征金字塔网络（E-FPN）和原型学习框架解决了基础设施检测中的少样本语义分割难题，特别是在涵洞和下水道缺陷类别上，表现出色。", "motivation": "当前深度学习框架虽然表现良好，但仍需大量标注数据，且在利用少量数据学习新的缺陷类别方面存在问题。我们的工作针对基础设施检测应用中的少样本语义分割问题，特别是在标记训练样本稀缺且昂贵的情况下。", "method": "我们的Enhanced Feature Pyramid Network (E-FPN)框架使用原型学习方法进行涵洞和下水道缺陷类别的少样本语义分割。该方法的主要贡献包括：1) 使用InceptionSepConv块和深度可分离卷积的自适应E-FPN编码器进行高效的多尺度特征提取；2) 使用带掩码平均池化的原型学习，从少量支持样本创建强大的原型；3) 通过全局自注意力、局部自注意力和交叉注意力实现注意力特征表示。", "result": "该方法在具有挑战性的基础设施检测数据集上的实验显示了优秀的少样本性能，最佳配置下（8路5样本训练），在2类分类测试中的F1得分为82.55%，mIoU达到72.26%，特别是在自注意力方法中性能有显著改善。", "conclusion": "我们的框架解决了基础设施检测系统中快速响应新类型缺陷的需求，特别是在新的训练数据有限的情况下，有助于制定更有效和具有成本效益的关键基础设施维护方案。"}}
{"id": "2510.05296", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.05296", "abs": "https://arxiv.org/abs/2510.05296", "authors": ["Zahra Maleki", "Amirhossein Akbari", "Amirhossein Binesh", "Babak Khalaj"], "title": "SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography", "comment": null, "summary": "Remote photoplethysmography (rPPG) is an innovative method for monitoring\nheart rate and vital signs by using a simple camera to record a person, as long\nas any part of their skin is visible. This low-cost, contactless approach helps\nin remote patient monitoring, emotion analysis, smart vehicle utilization, and\nmore. Over the years, various techniques have been proposed to improve the\naccuracy of this technology, especially given its sensitivity to lighting and\nmovement. In the unsupervised pipeline, it is necessary to first select skin\nregions from the video to extract the rPPG signal from the skin color changes.\nWe introduce a novel skin segmentation technique that prioritizes skin regions\nto enhance the quality of the extracted signal. It can detect areas of skin all\nover the body, making it more resistant to movement, while removing areas such\nas the mouth, eyes, and hair that may cause interference. Our model is\nevaluated on publicly available datasets, and we also present a new dataset,\ncalled SYNC-rPPG, to better represent real-world conditions. The results\nindicate that our model demonstrates a prior ability to capture heartbeats in\nchallenging conditions, such as talking and head rotation, and maintain the\nmean absolute error (MAE) between predicted and actual heart rates, while other\nmethods fail to do so. In addition, we demonstrate high accuracy in detecting a\ndiverse range of skin tones, making this technique a promising option for\nreal-world applications.", "AI": {"tldr": "本文提出了一种改进的rPPG方法，通过新的皮肤分割技术来提高准确性，特别是在运动和不同肤色的情况下。", "motivation": "传统的rPPG技术易受到光线和运动的影响。为了提高其准确性，特别是提高其在无监督管道中的表现，本文旨在改善皮肤区域的选择。", "method": "提出了一种新的皮肤分割技术，该技术优先考虑皮肤区域以提高提取信号的质量。该方法可以检测全身的皮肤区域，更抗运动干扰，并去除可能引起干扰的区域如嘴巴、眼睛和头发。", "result": "实验结果表明，本模型能够在嘈杂条件下如说话和头部旋转时捕获心跳，并保持预测的心率和实际心率之间的平均绝对误差，而其他方法在此类条件下则表现不佳。此外，该模型还能高精度地检测各种肤色。", "conclusion": "该技术展示了在种种挑战条件下也能保持高精度，预示着其在实际应用中的巨大潜力。"}}
{"id": "2510.05315", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05315", "abs": "https://arxiv.org/abs/2510.05315", "authors": ["Yousef Yeganeh", "Maximilian Frantzen", "Michael Lee", "Kun-Hsing Yu", "Nassir Navab", "Azade Farshad"], "title": "DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology", "comment": null, "summary": "While Whole Slide Imaging (WSI) scanners remain the gold standard for\ndigitizing pathology samples, their high cost limits accessibility in many\nhealthcare settings. Other low-cost solutions also face critical limitations:\nautomated microscopes struggle with consistent focus across varying tissue\nmorphology, traditional auto-focus methods require time-consuming focal stacks,\nand existing deep-learning approaches either need multiple input images or lack\ngeneralization capability across tissue types and staining protocols. We\nintroduce a novel automated microscopic system powered by DeepAf, a novel\nauto-focus framework that uniquely combines spatial and spectral features\nthrough a hybrid architecture for single-shot focus prediction. The proposed\nnetwork automatically regresses the distance to the optimal focal point using\nthe extracted spatiospectral features and adjusts the control parameters for\noptimal image outcomes. Our system transforms conventional microscopes into\nefficient slide scanners, reducing focusing time by 80% compared to stack-based\nmethods while achieving focus accuracy of 0.18 {\\mu}m on the same-lab samples,\nmatching the performance of dual-image methods (0.19 {\\mu}m) with half the\ninput requirements. DeepAf demonstrates robust cross-lab generalization with\nonly 0.72% false focus predictions and 90% of predictions within the depth of\nfield. Through an extensive clinical study of 536 brain tissue samples, our\nsystem achieves 0.90 AUC in cancer classification at 4x magnification, a\nsignificant achievement at lower magnification than typical 20x WSI scans. This\nresults in a comprehensive hardware-software design enabling accessible,\nreal-time digital pathology in resource-constrained settings while maintaining\ndiagnostic accuracy.", "AI": {"tldr": "This paper presents DeepAf, a novel auto-focus framework for transforming conventional microscopes into efficient slide scanners, achieving high accuracy and rapid focusing with low input requirements.", "motivation": "High costs and accessibility challenges of Whole Slide Imaging (WSI) scanners and limitations of existing low-cost alternatives motivated the development of a more accessible and efficient solution for digital pathology.", "method": "We introduce DeepAf, a novel auto-focus framework that uses a hybrid architecture to combine spatial and spectral features for predicting focus distance in a single-shot. This system transforms conventional microscopes into efficient slide scanners.", "result": "The system reduces focusing time by 80% compared to stack-based methods, achieving focus accuracy of 0.18 μm on the same-lab samples. In a clinical study of 536 brain tissue samples, the system achieves 0.90 AUC in cancer classification at 4x magnification, surpassing typical performance at higher magnification. DeepAf exhibits robust cross-lab generalization.", "conclusion": "DeepAf provides a comprehensive design that offers accessible, real-time, and accurate digital pathology solutions, especially in settings with limited resources, while maintaining diagnostic accuracy."}}
{"id": "2510.05326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05326", "abs": "https://arxiv.org/abs/2510.05326", "authors": ["Jalal Ahmmed", "Faruk Ahmed", "Rashedul Hasan Shohan", "Md. Mahabub Rana", "Mahdi Hasan"], "title": "Fine-Tuned CNN-Based Approach for Multi-Class Mango Leaf Disease Detection", "comment": "Double column 6 pages, 10 figures, ieee conference style", "summary": "Mango is an important fruit crop in South Asia, but its cultivation is\nfrequently hampered by leaf diseases that greatly impact yield and quality.\nThis research examines the performance of five pre-trained convolutional neural\nnetworks, DenseNet201, InceptionV3, ResNet152V2, SeResNet152, and Xception, for\nmulti-class identification of mango leaf diseases across eight classes using a\ntransfer learning strategy with fine-tuning. The models were assessed through\nstandard evaluation metrics, such as accuracy, precision, recall, F1-score, and\nconfusion matrices. Among the architectures tested, DenseNet201 delivered the\nbest results, achieving 99.33% accuracy with consistently strong metrics for\nindividual classes, particularly excelling in identifying Cutting Weevil and\nBacterial Canker. Moreover, ResNet152V2 and SeResNet152 provided strong\noutcomes, whereas InceptionV3 and Xception exhibited lower performance in\nvisually similar categories like Sooty Mould and Powdery Mildew. The training\nand validation plots demonstrated stable convergence for the highest-performing\nmodels. The capability of fine-tuned transfer learning models, for precise and\ndependable multi-class mango leaf disease detection in intelligent agricultural\napplications.", "AI": {"tldr": "本研究评估了几种预训练的卷积神经网络用于芒果叶病害多类别识别的效能，DenseNet201表现最佳。研究强调了迁移学习和微调策略在精准可靠的智能农业应用中的重要性。", "motivation": "芒果是南亚重要的水果作物，但其种植常受叶病害影响，这严重影响了产量和质量。因此，本研究旨在开发一种基于深度学习的多类别叶病害识别方法，以提高芒果的生产效率和产品质量。", "method": "该研究使用了五种预训练的卷积神经网络（DenseNet201, InceptionV3, ResNet152V2, SeResNet152, 和 Xception）来识别八类芒果叶病害。通过迁移学习和微调策略对这些模型进行评估，使用了包括准确率、精确率、召回率、F1分数和混淆矩阵在内的标准评估指标。", "result": "DenseNet201在所有测试的模型中表现最佳，准确率达到99.33%，特别是对于Cutting Weevil和Bacterial Canker的识别非常出色。ResNet152V2和SeResNet152也表现良好，相比之下，InceptionV3和Xception在一些视觉上相似的病害类别如Sooty Mould和Powdery Mildew上的表现较差。", "conclusion": "研究结果表明，通过迁移学习和微调策略训练的深度学习模型能够有效用于多类芒果叶病害的精准识别，这对智能农业应用具有重要意义。"}}
{"id": "2510.05110", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05110", "abs": "https://arxiv.org/abs/2510.05110", "authors": ["Arezoo Saedi", "Afsaneh Fatemi", "Mohammad Ali Nematbakhsh", "Sophie Rosset", "Anne Vilnat"], "title": "Collaborative and Proactive Management of Task-Oriented Conversations", "comment": null, "summary": "Task oriented dialogue systems (TOD) complete particular tasks based on user\npreferences across natural language interactions. Considering the impressive\nperformance of large language models (LLMs) in natural language processing\n(NLP) tasks, most of the latest TODs are centered on LLMs. While proactive\nplanning is crucial for task completion, many existing TODs overlook effective\ngoal-aware planning. This paper creates a model for managing task-oriented\nconversations, conceptualized centered on the information state approach to\ndialogue management. The created model incorporated constructive intermediate\ninformation in planning. Initially, predefined slots and text part\ninformational components are created to model user preferences. Investigating\nintermediate information, critical circumstances are identified. Informational\ncomponents corresponding to these circumstances are created. Possible\nconfigurations for these informational components lead to limited information\nstates. Then, dialogue moves, which indicate movement between these information\nstates and the procedures that must be performed in the movements, are created.\nEventually, the update strategy is constructed. The created model is\nimplemented leveraging in-context learning of LLMs. In this model, database\nqueries are created centered on indicated predefined slots and the order of\nretrieved entities is indicated centered on text part. This mechanism enables\npassing the whole corresponding entities to the preferences in the order of\ncongruency. Evaluations exploiting the complete test conversations of MultiWOZ,\nwith no more than a domain in a conversation, illustrate maximal inform and\nsuccess, and improvement compared with previous methods.", "AI": {"tldr": "论文提出了一个基于信息系统的方法来规划任务导向的对话，利用预定义插槽模型用户偏好的方法，通过数据库查询顺序传递相应实体。实验显示这种方法在MultiWOZ数据集上取得最优的表现。", "motivation": "该论文指出当前大多数任务导向型对话系统依赖大型语言模型，但忽视了基于目标的规划。因此，论文希望创建一个包含建设性中间信息的规划对话管理模型。", "method": "该论文建立了一个基于信息状态方法的对话管理模型来处理任务导向型对话。初始阶段，定义了用户偏好的插槽和文本部分信息。通过分析中间信息，识别关键情况并创建相应信息组件。这些组件的各种配置生成有限信息状态。随后，创建对话步骤以指示在这些信息状态之间的移动及需执行的过程。最后，构建更新策略。该模型利用大型语言模型的上下文学习能力实现，通过预定义插槽创建数据库查询并按照文本部分指示检索实体的顺序，这使得能按一致性顺序传递所有相关实体给偏好。", "result": "该模型在MultiWOZ数据集上的测试对话取得了最大的信息完整性和成功性，并且相比于之前的方法有了改进。", "conclusion": "论文的结论是其提出的模型利用了大型语言模型的上下文学习优势，并通过特定的数据库查询优化了实体传递，提高了任务导向对话系统的性能。"}}
{"id": "2510.05356", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05356", "abs": "https://arxiv.org/abs/2510.05356", "authors": ["Kostas Triaridis", "Alexandros Graikos", "Aggelina Chatziagapi", "Grigorios G. Chrysos", "Dimitris Samaras"], "title": "Mitigating Diffusion Model Hallucinations with Dynamic Guidance", "comment": null, "summary": "Diffusion models, despite their impressive demos, often produce hallucinatory\nsamples with structural inconsistencies that lie outside of the support of the\ntrue data distribution. Such hallucinations can be attributed to excessive\nsmoothing between modes of the data distribution. However, semantic\ninterpolations are often desirable and can lead to generation diversity, thus\nwe believe a more nuanced solution is required. In this work, we introduce\nDynamic Guidance, which tackles this issue. Dynamic Guidance mitigates\nhallucinations by selectively sharpening the score function only along the\npre-determined directions known to cause artifacts, while preserving valid\nsemantic variations. To our knowledge, this is the first approach that\naddresses hallucinations at generation time rather than through post-hoc\nfiltering. Dynamic Guidance substantially reduces hallucinations on both\ncontrolled and natural image datasets, significantly outperforming baselines.", "AI": {"tldr": "本文提出了动态引导方法来减少扩散模型生成中的幻觉现象，同时保证语义多样性。", "motivation": "尽管扩散模型的演示效果令人印象深刻，但它们通常会产生结构不一致的样本，这些样本超出了真实数据分布的支持范围。这些幻觉往往归因于数据分布模式之间的过度平滑。然而，语义插值通常是可取的，并且可以导致生成多样性。", "method": "通过引入动态引导(Dynamic Guidance)，该方法选择性地仅沿预知会产生伪影的方向锐化得分函数，从而减少幻觉的产生，同时保持有效的语义变异。", "result": "动态引导显著减少了受控和自然图像数据集上的幻觉，明显优于基线方法。", "conclusion": "这是首个在生成阶段而非通过事后过滤来解决幻觉问题的方法。"}}
{"id": "2510.05113", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05113", "abs": "https://arxiv.org/abs/2510.05113", "authors": ["Nisheeth Joshi", "Pragya Katyayan", "Palak Arora"], "title": "Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati Machine Translation System", "comment": "8 Pages, 4 Tables, 4 Figures", "summary": "Machine Translation (MT) Evaluation is an integral part of the MT development\nlife cycle. Without analyzing the outputs of MT engines, it is impossible to\nevaluate the performance of an MT system. Through experiments, it has been\nidentified that what works for English and other European languages does not\nwork well with Indian languages. Thus, In this paper, we have introduced a\nreference-based MT evaluation metric for Gujarati which is based on supervised\nlearning. We have trained two versions of the metric which uses 25 features for\ntraining. Among the two models, one model is trained using 6 hidden layers with\n500 epochs while the other model is trained using 10 hidden layers with 500\nepochs. To test the performance of the metric, we collected 1000 MT outputs of\nseven MT systems. These MT engine outputs were compared with 1 human reference\ntranslation. While comparing the developed metrics with other available\nmetrics, it was found that the metrics produced better human correlations.", "AI": {"tldr": "本文提出了一种基于监督学习的针对古吉拉特语机器翻译的评价指标，该指标在实验中表现出优于其他现有指标的能力。", "motivation": "由于实验表明适用于英语和其他欧洲语言的评价方法在印度语言上表现不佳，因此本文旨在开发针对古吉拉特语的机器翻译评价指标。", "method": "本文介绍了基于监督学习的参考文本机器翻译评价指标，该指标专门用于古吉拉特语。研究中训练了两个版本的评价指标，每个版本使用25个特征进行训练。其中一个模型使用6个隐藏层并在500个周期内进行训练，另一个模型使用10个隐藏层并在500个周期内进行训练。", "result": "通过收集7个机器翻译系统的1000个输出，并与一个人类参考翻译进行比较，测试了该评价指标的性能。结果显示，开发的评价指标比现有一些指标更能产生更好的人类相关性。", "conclusion": "研究表明，基于监督学习的参考文本机器翻译评价指标对于古吉拉特语可以产生较高的人类相关的评价结果。"}}
{"id": "2510.05367", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05367", "abs": "https://arxiv.org/abs/2510.05367", "authors": ["Yang Xiao", "Gen Li", "Kaiyuan Deng", "Yushu Wu", "Zheng Zhan", "Yanzhi Wang", "Xiaolong Ma", "Bo Hui"], "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation", "comment": null, "summary": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache .", "AI": {"tldr": "This paper proposes three strategies for reducing memory consumption in the denoising and decoding stages of diffusion model inference, leading to faster inference speed and lower memory usage without significantly degrading quality.", "motivation": "The motivation behind this paper is to address the issue of substantial memory surges in the denoising and decoding stages of diffusion model inference, which are often observed in cache-based acceleration methods.", "method": "In this paper, the authors decompose the inference process into encoding, denoising, and decoding stages. They analyze the characteristics of these stages and propose three strategies for reducing memory consumption: Asynchronous Cache Swapping, Feature chunk, and Slicing latents to decode.", "result": "The proposed approach achieves faster inference speed and lower memory usage, with quality degradation remaining within an acceptable range.", "conclusion": "The paper concludes that the strategies proposed for stage-specific acceleration can effectively reduce memory consumption while maintaining a balance between speed and quality."}}
{"id": "2510.05116", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05116", "abs": "https://arxiv.org/abs/2510.05116", "authors": ["Bowen Xu"], "title": "Hallucination is Inevitable for LLMs with the Open World Assumption", "comment": null, "summary": "Large Language Models (LLMs) exhibit impressive linguistic competence but\nalso produce inaccurate or fabricated outputs, often called ``hallucinations''.\nEngineering approaches usually regard hallucination as a defect to be\nminimized, while formal analyses have argued for its theoretical inevitability.\nYet both perspectives remain incomplete when considering the conditions\nrequired for artificial general intelligence (AGI). This paper reframes\n``hallucination'' as a manifestation of the generalization problem. Under the\nClosed World assumption, where training and test distributions are consistent,\nhallucinations may be mitigated. Under the Open World assumption, however,\nwhere the environment is unbounded, hallucinations become inevitable. This\npaper further develops a classification of hallucination, distinguishing cases\nthat may be corrected from those that appear unavoidable under open-world\nconditions. On this basis, it suggests that ``hallucination'' should be\napproached not merely as an engineering defect but as a structural feature to\nbe tolerated and made compatible with human intelligence.", "AI": {"tldr": "论文将语言模型产生的不准确或虚构输出（即'幻觉'）重新定义为一种泛化问题的表现形式，并提出了在开放世界假设下，幻觉是不可避免的，建议将其视为不应仅从工程缺陷角度处理的结构特征。", "motivation": "探讨在追求通用人工智能的背景下，如何更全面地理解语言模型中的幻觉问题，而非单纯将其视为需要最小化的缺陷。", "method": "通过将幻觉问题与封闭世界和开放世界的假设条件相关联，分类幻觉的不同情况，区分哪些幻觉可能纠正，哪些在开放世界条件下似乎是不可避免的。", "result": "论文提出了将幻觉定义为泛化问题的传统假设之外的观点，并且建议应将幻觉视为一种应兼容于人类智能的结构特征。", "conclusion": "在开放世界条件下，幻觉是不可避免的，因此不应仅从工程缺陷的角度来对待幻觉，而应看作是一个结构特征，以期与人类智能相兼容。"}}
{"id": "2510.05408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05408", "abs": "https://arxiv.org/abs/2510.05408", "authors": ["Kebin Contreras", "Luis Toscano-Palomino", "Mauro Dalla Mura", "Jorge Bacca"], "title": "See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models", "comment": null, "summary": "Recovering the past from present observations is an intriguing challenge with\npotential applications in forensics and scene analysis. Thermal imaging,\noperating in the infrared range, provides access to otherwise invisible\ninformation. Since humans are typically warmer (37 C -98.6 F) than their\nsurroundings, interactions such as sitting, touching, or leaning leave residual\nheat traces. These fading imprints serve as passive temporal codes, allowing\nfor the inference of recent events that exceed the capabilities of RGB cameras.\nThis work proposes a time-reversed reconstruction framework that uses paired\nRGB and thermal images to recover scene states from a few seconds earlier. The\nproposed approach couples Visual-Language Models (VLMs) with a constrained\ndiffusion process, where one VLM generates scene descriptions and another\nguides image reconstruction, ensuring semantic and structural consistency. The\nmethod is evaluated in three controlled scenarios, demonstrating the\nfeasibility of reconstructing plausible past frames up to 120 seconds earlier,\nproviding a first step toward time-reversed imaging from thermal traces.", "AI": {"tldr": "研究提出了一种利用视觉语言模型和受限扩散过程结合RGB和热图像来恢复几秒前场景状态的方法，实现了时间反转成像的可行性。", "motivation": "这项工作旨在通过当前的观测恢复过去的场景状态，特别是在取证和场景分析中有潜在应用价值。通过结合RGB和热成像信息，该研究提供了一种新的方法来推断超过普通RGB相机能力的近期事件。", "method": "该研究提出了一种结合视觉语言模型（VLMs）和受限扩散过程的时间反转重建框架，其中一个VLM生成场景描述，另一个引导图像重建，以确保语义和结构的一致性。", "result": "该方法在三个受控场景中进行了评估，展示了重建最多120秒之前可信过去的帧的可行性。", "conclusion": "这项研究提供了一个从热痕迹中实现时间反转成像的第一步。"}}
{"id": "2510.05121", "categories": ["cs.CL", "cs.CE", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05121", "abs": "https://arxiv.org/abs/2510.05121", "authors": ["Durgesh Nandini", "Rebekka Koch", "Mirco Schoenfeld"], "title": "Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models", "comment": null, "summary": "This study investigates the effectiveness of Large Language Models (LLMs) for\nthe extraction of structured knowledge in the form of Subject-Predicate-Object\ntriples. We apply the setup for the domain of Economics application. The\nfindings can be applied to a wide range of scenarios, including the creation of\neconomic trade knowledge graphs from natural language legal trade agreement\ntexts. As a use case, we apply the model to regional trade agreement texts to\nextract trade-related information triples. In particular, we explore the\nzero-shot, one-shot and few-shot prompting techniques, incorporating positive\nand negative examples, and evaluate their performance based on quantitative and\nqualitative metrics. Specifically, we used Llama 3.1 model to process the\nunstructured regional trade agreement texts and extract triples. We discuss key\ninsights, challenges, and potential future directions, emphasizing the\nsignificance of language models in economic applications.", "AI": {"tldr": "研究使用Llama 3.1模型从自然语言的贸易协定文本中提取经济贸易信息三元组，应用了零样本、单样本和少样本技术，评估了其在经济应用中的有效性。", "motivation": "研究的动机在于探索大型语言模型在经济领域抽取结构化知识的有效性，尤其是从自然语言的贸易协定文本中创建经济贸易知识图谱的潜力。", "method": "研究使用Llama 3.1模型，应用零样本、单样本和少样本提示技术，处理无结构的区域贸易协定文本，提取相关信息三元组。通过对正反例的考虑，进行定量和定性评估。", "result": "该研究旨在探索大型语言模型(LLMs)在经济领域的应用中抽取结构化知识的有效性，尤其是从自然语言的贸易协定文本中提取经济贸易知识图谱。研究中，使用了Llama 3.1模型处理无结构的区域贸易协定文本，抽取贸易相关信息三元组。研究讨论了零样本、单样本和少样本提示技术的性能，同时考虑了正反例的影响，并基于定量和定性指标进行了评估。研究强调了语言模型在经济应用中的重要性。", "conclusion": "研究强调了语言模型在经济应用中的重要性，并讨论了抽取贸易相关信息三元组的挑战和未来可能的研究方向。"}}
{"id": "2510.05411", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05411", "abs": "https://arxiv.org/abs/2510.05411", "authors": ["Bruno Korbar", "Andrew Zisserman"], "title": "Personalizing Retrieval using Joint Embeddings or \"the Return of Fluffy\"", "comment": "Published as an oral in CBMI2025", "summary": "The goal of this paper is to be able to retrieve images using a compound\nquery that combines object instance information from an image, with a natural\ntext description of what that object is doing or where it is. For example, to\nretrieve an image of \"Fluffy the unicorn (specified by an image) on someone's\nhead\". To achieve this we design a mapping network that can \"translate\" from a\nlocal image embedding (of the object instance) to a text token, such that the\ncombination of the token and a natural language query is suitable for CLIP\nstyle text encoding, and image retrieval. Generating a text token in this\nmanner involves a simple training procedure, that only needs to be performed\nonce for each object instance. We show that our approach of using a trainable\nmapping network, termed pi-map, together with frozen CLIP text and image\nencoders, improves the state of the art on two benchmarks designed to assess\npersonalized retrieval.", "AI": {"tldr": "提出了一种新的方法，使用映射网络将图像对象实例信息转换成文本标记，结合自然语言查询进行更好的图像检索。", "motivation": "本文的目标是能够使用结合图像对象实例信息和对该对象所做之事或所在之处的自然文本描述的复合查询来检索图像。", "method": "我们设计了一个映射网络，可以将局部图像嵌入（对象实例）“翻译”为文本标记，使该标记与自然语言查询相结合，适用于CLIP风格的文本编码和图像检索。通过一个简单的训练步骤，为每个对象实例生成该文本标记。", "result": "我们的方法显示，在两个旨在评估个性化检索的基准测试中，使用可训练的映射网络（称为pi-map）与冻结的CLIP文本和图像编码器的结合，优于当前最先进的方法。", "conclusion": "通过设计的pi-map映射网络，实现了使用复合查询进行图像检索，该网络与已有CLIP模型中的文本和图像编码器结合，展示了在个性化检索任务中的优势。"}}
{"id": "2510.05122", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05122", "abs": "https://arxiv.org/abs/2510.05122", "authors": ["Jie Zhu", "Yuanchen Zhou", "Shuo Jiang", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang", "Fang Kong"], "title": "CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation", "comment": "Preprint", "summary": "Emotional Support Conversation (ESC) plays a vital role in alleviating\npsychological stress and providing emotional value through dialogue. While\nrecent studies have largely focused on data augmentation and synthetic corpus\nconstruction, they often overlook the deeper cognitive reasoning processes that\nunderpin effective emotional support. To address this gap, we propose\n\\textbf{CARE}, a novel framework that strengthens reasoning in ESC without\nrelying on large-scale synthetic data. CARE leverages the original ESC training\nset to guide models in generating logically coherent and supportive responses,\nthereby explicitly enhancing cognitive reasoning. Building on this foundation,\nwe further employ reinforcement learning to refine and reinforce the reasoning\nprocess. Experimental results demonstrate that CARE significantly improves both\nthe logical soundness and supportive quality of responses, advancing the\ndevelopment of empathetic, cognitively robust, and human-like emotional support\nsystems.", "AI": {"tldr": "提出CARE框架，旨在加强情感支持对话中的逻辑连贯性和支持性，改善认知推理过程，而不需要依赖大规模合成数据。", "motivation": "尽管最近的研究主要集中在数据增强和合成语料库的构建上，但它们往往忽视了构成有效情感支持背后更深层次的认知推理过程。", "method": "通过CARE框架，利用原始的情感支持对话训练集指导模型生成逻辑连贯和支持性的响应，以增强认知推理过程。在此基础上，使用强化学习进一步优化和加强推理过程。", "result": "实验结果表明，CARE大大提高了响应的逻辑合理性和支持性质量，推动了同理心、认知上更强大的和类似人类的情感支持系统的发展。", "conclusion": "CARE框架证明了通过利用原始情感支持对话训练集并结合强化学习可以显著提升情感支持系统的逻辑连贯性和支持性质量。"}}
{"id": "2510.05488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05488", "abs": "https://arxiv.org/abs/2510.05488", "authors": ["Peizhi Yan", "Rabab Ward", "Qiang Tang", "Shan Du"], "title": "ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time\nrendering of 3D head avatars. Existing 3DGS-based avatars typically rely on\ntens of thousands of 3D Gaussian points (Gaussians), with the number of\nGaussians fixed after training. However, many practical applications require\nadjustable levels of detail (LOD) to balance rendering efficiency and visual\nquality. In this work, we propose \"ArchitectHead\", the first framework for\ncreating 3D Gaussian head avatars that support continuous control over LOD. Our\nkey idea is to parameterize the Gaussians in a 2D UV feature space and propose\na UV feature field composed of multi-level learnable feature maps to encode\ntheir latent features. A lightweight neural network-based decoder then\ntransforms these latent features into 3D Gaussian attributes for rendering.\nArchitectHead controls the number of Gaussians by dynamically resampling\nfeature maps from the UV feature field at the desired resolutions. This method\nenables efficient and continuous control of LOD without retraining.\nExperimental results show that ArchitectHead achieves state-of-the-art (SOTA)\nquality in self and cross-identity reenactment tasks at the highest LOD, while\nmaintaining near SOTA performance at lower LODs. At the lowest LOD, our method\nuses only 6.2\\% of the Gaussians while the quality degrades moderately (L1 Loss\n+7.9\\%, PSNR --0.97\\%, SSIM --0.6\\%, LPIPS Loss +24.1\\%), and the rendering\nspeed nearly doubles.", "AI": {"tldr": "本文提出了一个可以使3D高斯点头像模型支持连续可控LOD效果的框架ArchitectHead，实验验证了该框架在高LOD时的高性能和在低LOD时的效率提升。", "motivation": "尽管现有的3D高斯喷涂（3DGS）能够实时渲染逼真的3D人物头像，但它们通常需要数万个固定数量的3D高斯点，无法灵活调整细节层次。由于实际应用中需要在渲染效率与视觉质量之间找到平衡点，故本文目的在于使头像模型支持连续控制LOD的能力。", "method": "本文提出了一种名为ArchitectHead的框架，该框架可以在2D UV特征空间中参数化3D高斯点，并通过一个多级可学习特征图组成的UV特征场来编码这些高斯点的潜在特征。接着，通过一个轻量级的神经网络解码器将这些潜在特征转换为用于渲染的3D高斯属性。通过动态地在不同的分辨率下重新采样特征图，ArchitectHead可以实现对细节层次（LOD）的连续可控调整，且无需重新训练。", "result": "实验结果显示，ArchitectHead在最高LOD的自我重现和跨身份重新演绎任务中达到了最先进的质量水平，并在较低的LOD下保持接近最先进的性能。", "conclusion": "总的来说，通过ArchitectHead框架，可以高效连续地控制LOD，且在最低LOD下仅使用6.2%的高斯点，同时只中度降低质量，渲染速度几乎翻倍。"}}
{"id": "2510.05124", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.05124", "abs": "https://arxiv.org/abs/2510.05124", "authors": ["Mingjin Li", "Yu Liu", "Huayi Liu", "Xiang Ye", "Chao Jiang", "Hongguang Zhang"], "title": "MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation", "comment": "work in progress", "summary": "We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for\ngenerating persuasive multi-turn dialogues via agent self-play. MADS employs\nthree coordinated agents: User Agents simulating diverse persona-driven\nbehaviors, a Dialog Agent executing task-oriented persuasion strategies and an\nOptimization Agent evaluating and refining dialogue outcomes. We further\nvalidate its effectiveness through users' Chain-of-Attitude (CoA) modeling and\ndedicated LLMs' persuasion assessment. This approach enables low-cost\ngeneration of training data without human annotation, addressing key industry\nchallenges such as lack of user data, cold-start evaluation difficulties, and\nprompt inefficiency. Applied to a real-world marketing scenario, MADS\nsignificantly improved the persuasion capacity of small LLMs, increasing the\norganic traffic conversion rate by 22.4\\% (from 1.83\\% to 2.24\\%) ,\ndemonstrating clear business value.", "AI": {"tldr": "MADS, a scalable framework for generating persuasive multi-turn dialogues through agent self-play, effectively simulates diverse user personas and employs task-oriented persuasion strategies, leading to significant improvements in organic traffic conversion in real-world marketing scenarios.", "motivation": "To address the challenges in generating training data without human annotation and to improve the persuasive capacity of small LLMs.", "method": "MADS uses three agents to simulate dialogues: User Agents for diverse persona-driven behaviors, a Dialog Agent for persuasion strategies, and an Optimization Agent for refining dialogue outcomes.", "result": "In a real-world marketing scenario, MADS improved the organic traffic conversion rate by 22.4%, showing clear business value.", "conclusion": "The MADS framework is effective in enhancing the persuasive capacity of small LLMs, demonstrating potential for significant business benefits without the need for extensive human annotation."}}
{"id": "2510.05506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05506", "abs": "https://arxiv.org/abs/2510.05506", "authors": ["James Dickens"], "title": "Human Action Recognition from Point Clouds over Time", "comment": null, "summary": "Recent research into human action recognition (HAR) has focused predominantly\non skeletal action recognition and video-based methods. With the increasing\navailability of consumer-grade depth sensors and Lidar instruments, there is a\ngrowing opportunity to leverage dense 3D data for action recognition, to\ndevelop a third way. This paper presents a novel approach for recognizing\nactions from 3D videos by introducing a pipeline that segments human point\nclouds from the background of a scene, tracks individuals over time, and\nperforms body part segmentation. The method supports point clouds from both\ndepth sensors and monocular depth estimation. At the core of the proposed HAR\nframework is a novel backbone for 3D action recognition, which combines\npoint-based techniques with sparse convolutional networks applied to\nvoxel-mapped point cloud sequences. Experiments incorporate auxiliary point\nfeatures including surface normals, color, infrared intensity, and body part\nparsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D\n120 dataset demonstrates that the method is competitive with existing skeletal\naction recognition algorithms. Moreover, combining both sensor-based and\nestimated depth inputs in an ensemble setup, this approach achieves 89.3%\naccuracy when different human subjects are considered for training and testing,\noutperforming previous point cloud action recognition methods.", "AI": {"tldr": "A new approach for 3D action recognition from point cloud data is introduced, involving segmentation, tracking, and a novel backbone with sparse convolutions. The method shows strong performance on the NTU RGB-D 120 dataset.", "motivation": "The motivation is to leverage dense 3D data from depth sensors and Lidar for action recognition, offering an alternative to traditional methods like skeletal and video-based approaches.", "method": "This paper proposes a new method for 3D action recognition from point cloud data, which includes segmenting human point clouds, tracking over time, body part segmentation, and using a novel backbone combining point-based techniques with sparse convolutions on voxel-mapped sequences.", "result": "The method was tested on the NTU RGB-D 120 dataset and shown to be competitive with current skeletal action recognition methods, achieving 89.3% accuracy when combining sensor-based and estimated depth data.", "conclusion": "The paper concludes that the new 3D action recognition method is effective, achieving high accuracy and outperforming previous point cloud-based methods with the incorporation of multiple auxiliary features."}}
{"id": "2510.05125", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05125", "abs": "https://arxiv.org/abs/2510.05125", "authors": ["Reza Shirkavand", "Xiaokai Wei", "Chen Wang", "Zheng Hui", "Heng Huang", "Michelle Gong"], "title": "Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation", "comment": null, "summary": "While collaborative filtering delivers predictive accuracy and efficiency,\nand Large Language Models (LLMs) enable expressive and generalizable reasoning,\nmodern recommendation systems must bring these strengths together. Growing user\nexpectations, such as natural-language queries and transparent explanations,\nfurther highlight the need for a unified approach. However, doing so is\nnontrivial. Collaborative signals are often token-efficient but semantically\nopaque, while LLMs are semantically rich but struggle to model implicit user\npreferences when trained only on textual inputs. This paper introduces Item-ID\n+ Oral-language Mixture-of-Experts Language Model (IDIOMoE), which treats item\ninteraction histories as a native dialect within the language space, enabling\ncollaborative signals to be understood in the same way as natural language. By\nsplitting the Feed Forward Network of each block of a pretrained LLM into a\nseparate text expert and an item expert with token-type gating, our method\navoids destructive interference between text and catalog modalities. IDIOMoE\ndemonstrates strong recommendation performance across both public and\nproprietary datasets, while preserving the text understanding of the pretrained\nmodel.", "AI": {"tldr": "本论文提出了IDIOMoE模型，通过结合协作过滤与大型语言模型的优势，解决了推荐系统中自然语言处理和协作信号解释的挑战，实验证明模型表现优异。", "motivation": "面对用户日益增长的自然语言查询和解释透明度的需求，本论文动机在于整合协作过滤和大型语言模型的优势，以提供更好的推荐体验和服务。", "method": "本论文采用了Mixture-of-Experts (MoE)方法，将项目的交互历史视为一种特殊的语言表达形式，用单独的文本专家和项目专家处理，通过令牌类型门控机制避免了不同模态处理的冲突。", "result": "本论文通过引入Item-ID + Oral-language Mixture-of-Experts Language Model (IDIOMoE)，来整合协作过滤和大型语言模型的优势，旨在解决现代推荐系统中用户期望自然语言查询和透明解释的需求。IDIOMoE将项目交互历史视为语言空间中的原生方言，使协作信号能以自然语言的形式被理解。通过将预训练LLM每个块的前馈网络分割成单独的文本专家和项目专家，并结合令牌类型门控，方法避免了文本和目录模式之间的破坏性干扰。实验结果显示，IDIOMoE在公开和专有数据集上均表现出强大的推荐性能，同时保持了预训练模型的文本理解能力。", "conclusion": "本论文证明了IDIOMoE模型能有效整合协作过滤和大型语言模型的优势，提高推荐系统的性能，同时满足用户对自然语言查询和透明解释的期望。"}}
{"id": "2510.05509", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05509", "abs": "https://arxiv.org/abs/2510.05509", "authors": ["Shinnosuke Saito", "Takashi Matsubara"], "title": "Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models", "comment": null, "summary": "Diffusion models are powerful deep generative models (DGMs) that generate\nhigh-fidelity, diverse content. However, unlike classical DGMs, they lack an\nexplicit, tractable low-dimensional latent space that parameterizes the data\nmanifold. This absence limits manifold-aware analysis and operations, such as\ninterpolation and editing. Existing interpolation methods for diffusion models\ntypically follow paths through high-density regions, which are not necessarily\naligned with the data manifold and can yield perceptually unnatural\ntransitions. To exploit the data manifold learned by diffusion models, we\npropose a novel Riemannian metric on the noise space, inspired by recent\nfindings that the Jacobian of the score function captures the tangent spaces to\nthe local data manifold. This metric encourages geodesics in the noise space to\nstay within or run parallel to the learned data manifold. Experiments on image\ninterpolation show that our metric produces perceptually more natural and\nfaithful transitions than existing density-based and naive baselines.", "AI": {"tldr": "该论文针对扩散模型缺乏显式低维度潜在空间的问题，提出了一种在噪声空间中的黎曼度量方式，提升了扩散模型在插值任务中的表现，使之能够生成更加自然和忠实过渡的图像。", "motivation": "该论文的研究动机是解决扩散模型缺乏显式的低维度潜在空间参数化数据流形的缺点。而现有的扩散模型插值方法通常通过高密度区域的路径，这种方法不一定是沿着数据流形的，会导致感知上的不自然过渡。通过拥抱扩散模型所学习的数据流形，该研究旨在通过新的方法改善插值过渡。", "method": "该论文的方法是提出了一种新颖的黎曼度量方式，在噪声空间中应用该度量方式能促使测地线保持在或者平行于已学习的数据流形上。该方法是受到最近发现的分数函数的雅可比矩阵能捕捉局部数据流形的切空间的启发。", "result": "该论文的实验结果表明，他们提出的黎曼度量在噪声空间中生成的图像插值过渡比现有的基于密度的方法和简单的基线方法更加自然和准确。", "conclusion": "该论文得出结论，他们所提出的在噪声空间中的黎曼度量方法能够使得扩散模型更好地捕捉到数据流形的信息，并在图像插值任务中产生更加自然和忠实的过渡。"}}
{"id": "2510.05126", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05126", "abs": "https://arxiv.org/abs/2510.05126", "authors": ["Mark Steyvers", "Catarina Belem", "Padhraic Smyth"], "title": "Improving Metacognition and Uncertainty Communication in Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly used in decision-making\ncontexts, but when they present answers without signaling low confidence, users\nmay unknowingly act on erroneous outputs. While prior work shows that LLMs\nmaintain internal uncertainty signals, their explicit verbalized confidence is\ntypically miscalibrated and poorly discriminates between correct and incorrect\nanswers. Across two types of LLMs, we investigate whether supervised finetuning\ncan improve models' ability to communicate uncertainty and whether such\nimprovements generalize across tasks and domains. We finetune the LLMs on\ndatasets spanning general knowledge, mathematics, and open-ended trivia, and\nevaluate two metacognitive tasks: (1) single-question confidence estimation,\nwhere the model assigns a numeric certainty to its answer, and (2) pairwise\nconfidence comparison, where the model selects which of two answers it is more\nlikely to have correct. We assess generalization to unseen domains, including\nmedical and legal reasoning. Results show that finetuning improves calibration\n(alignment between stated confidence and accuracy) and discrimination (higher\nconfidence for correct vs. incorrect responses) within and across domains,\nwhile leaving accuracy unchanged. However, improvements are task-specific:\ntraining on single-question calibration does not transfer to pairwise\ncomparison, and vice versa. In contrast, multitask finetuning on both forms of\nmetacognition yields broader gains, producing lower calibration error and\nstronger discrimination in out-of-domain evaluations. These results show that\nwhile uncertainty communication in LLMs is trainable and generalizable,\ndifferent metacognitive skills do not naturally reinforce one another and must\nbe developed together through multitask training.", "AI": {"tldr": "研究通过监督微调改进 LLMs 传达不确定性的能力，提高校准和区分度，多任务微调产生更广泛收益，但改进具有任务特定性。", "motivation": "大型语言模型（LLMs）在决策情境中得到越来越广泛的应用，但是当模型以不表达低置信度的方式呈现答案时，用户可能不知道地根据错误的答案采取行动。尽管先前研究表明 LLM 维持内部不确定性的信号，但是它们明确的置信度表达通常是不准确的，不能很好地区分正确和错误的答案。因此，对 LLMs 的改进方法进行了研究，以增强模型传达不确定性并评估改进是否泛化。", "method": "通过在涵盖一般知识、数学和开放性 trivia 的数据集上对两种类型的大型语言模型 (LLMs) 进行监督微调，探讨是否可以提高模型传达不确定性的能力，并评估这些改进是否能在不同任务和领域之间泛化。研究包括两个元认知任务：(1) 单个问题信心估计，模型为其答案分配一个数值的信心；(2) 一对信心比较，模型选择两个答案中哪个更可能正确。", "result": "微调提高了校准（陈述的信心与准确性之间的校准）和区分度（正确与不正确的响应的信心更高）在域内和跨域中的表现，同时准确度保持不变。但是，改进具有任务特定性：在单个问题校准训练上没有转移到成对比较中，反之亦然。相比之下，两项元认知形式的多任务微调产生了更广泛的收益，在域外评估中产生了更低的校准误差和更强的区分度。", "conclusion": "研究表明，尽管 LLMs 中不确定性传达是可训练和可泛化的，但不同的元认知技能并不会自然地相互加强，必须通过多任务训练来一起开发。"}}
{"id": "2510.05532", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05532", "abs": "https://arxiv.org/abs/2510.05532", "authors": ["Sam Sartor", "Pieter Peers"], "title": "Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation", "comment": null, "summary": "Large pretrained diffusion models can provide strong priors beneficial for\nmany graphics applications. However, generative applications such as neural\nrendering and inverse methods such as SVBRDF estimation and intrinsic image\ndecomposition require additional input or output channels. Current solutions\nfor channel expansion are often application specific and these solutions can be\ndifficult to adapt to different diffusion models or new tasks. This paper\nintroduces Teamwork: a flexible and efficient unified solution for jointly\nincreasing the number of input and output channels as well as adapting a\npretrained diffusion model to new tasks. Teamwork achieves channel expansion\nwithout altering the pretrained diffusion model architecture by coordinating\nand adapting multiple instances of the base diffusion model (\\ie, teammates).\nWe employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address\nboth adaptation and coordination between the different teammates. Furthermore\nTeamwork supports dynamic (de)activation of teammates. We demonstrate the\nflexibility and efficiency of Teamwork on a variety of generative and inverse\ngraphics tasks such as inpainting, single image SVBRDF estimation, intrinsic\ndecomposition, neural shading, and intrinsic image synthesis.", "AI": {"tldr": "本论文主要提出了Teamwork，一种灵活和高效的解决方案，它可以通过协调和适应多个基础扩散模型实例来扩展通道数量，用于多种图像处理任务，支持队友的动态（去）激活。", "motivation": "大型预训练扩散模型可以提供用于许多图形应用的强大先验。然而，对于生成性应用（如神经渲染和逆方法，例如SVBRDF估计和内在图像分解）来说，需要额外的输入或输出通道。当前的通道扩展解决方案往往是针对具体应用的，这些解决方案可能难以适应不同的扩散模型或新任务。因此，研究动机在于提出一个灵活且高效的统一解决方案来解决这些问题。", "method": "本研究引入了Teamwork：一种灵活且高效的统一解决方案，用于同时增加输入和输出通道数量，并针对新任务调整预训练扩散模型。Teamwork通过协调和适应多个基础扩散模型实例（即队友）实现通道扩展，而不改变预训练扩散模型的架构。研究采用了低秩适应（LoRA）的新型变体来解决适应和不同队友之间的协调问题。此外，Teamwork支持队友的动态（去）激活。", "result": "本研究通过多个生成和逆图形任务（如图像修复、单图像SVBRDF估计、内在图像分解、神经着色和内在图像合成）来证明了Teamwork的灵活性和高效率。", "conclusion": "实验结果显示，Teamwork在多种生成和逆图形任务（如图像修复、单图像SVBRDF估计、内在图像分解、神经着色和内在图像合成）上展示了其灵活性和高效率。"}}
{"id": "2510.05128", "categories": ["cs.CL", "cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.05128", "abs": "https://arxiv.org/abs/2510.05128", "authors": ["Si-Ioi Ng", "Pranav S. Ambadi", "Kimberly D. Mueller", "Julie Liss", "Visar Berisha"], "title": "Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models", "comment": null, "summary": "Current methods for automated assessment of cognitive-linguistic impairment\nvia picture description often neglect the visual narrative path - the sequence\nand locations of elements a speaker described in the picture. Analyses of\nspatio-semantic features capture this path using content information units\n(CIUs), but manual tagging or dictionary-based mapping is labor-intensive. This\nstudy proposes a BERT-based pipeline, fine tuned with binary cross-entropy and\npairwise ranking loss, for automated CIU extraction and ordering from the\nCookie Theft picture description. Evaluated by 5-fold cross-validation, it\nachieves 93% median precision, 96% median recall in CIU detection, and 24%\nsequence error rates. The proposed method extracts features that exhibit strong\nPearson correlations with ground truth, surpassing the dictionary-based\nbaseline in external validation. These features also perform comparably to\nthose derived from manual annotations in evaluating group differences via\nANCOVA. The pipeline is shown to effectively characterize visual narrative\npaths for cognitive impairment assessment, with the implementation and models\nopen-sourced to public.", "AI": {"tldr": "A BERT-based pipeline for automated CIU extraction and ordering outperforms dictionary-based methods in assessing cognitive-linguistic impairments via picture description analysis.", "motivation": "To address the limitations of existing methods that overlook the visual narrative path in cognitive-linguistic impairment assessment, the paper aims to offer an automated system for CIU extraction and ordering.", "method": "Content uses a BERT-based pipeline fine-tuned with binary cross-entropy and pairwise ranking loss for automated CIU extraction and ordering from picture descriptions, specifically the Cookie Theft picture.", "result": "The method achieves high precision and recall in CIU detection, low sequence error rates, and shows strong correlations with ground truth, performing comparably to manual annotations in group difference assessments.", "conclusion": "The study demonstrates the effectiveness of the proposed pipeline in characterizing visual narrative paths for cognitive impairment assessment. The tool is made publicly available."}}
{"id": "2510.05538", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05538", "abs": "https://arxiv.org/abs/2510.05538", "authors": ["Owen Henkel", "Bill Roberts", "Doug Jaffe", "Laurence Holt"], "title": "Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) raise the\nquestion of their potential for grading, analyzing, and offering feedback on\nhandwritten student classwork. This capability would be particularly beneficial\nin elementary and middle-school mathematics education, where most work remains\nhandwritten, because seeing students' full working of a problem provides\nvaluable insights into their learning processes, but is extremely\ntime-consuming to grade. We present two experiments investigating MLLM\nperformance on handwritten student mathematics classwork. Experiment A examines\n288 handwritten responses from Ghanaian middle school students solving\narithmetic problems with objective answers. In this context, models achieved\nnear-human accuracy (95%, k = 0.90) but exhibited occasional errors that human\neducators would be unlikely to make. Experiment B evaluates 150 mathematical\nillustrations from American elementary students, where the drawings are the\nanswer to the question. These tasks lack single objective answers and require\nsophisticated visual interpretation as well as pedagogical judgment in order to\nanalyze and evaluate them. We attempted to separate MLLMs' visual capabilities\nfrom their pedagogical abilities by first asking them to grade the student\nillustrations directly, and then by augmenting the image with a detailed human\ndescription of the illustration. We found that when the models had to analyze\nthe student illustrations directly, they struggled, achieving only k = 0.20\nwith ground truth scores, but when given human descriptions, their agreement\nlevels improved dramatically to k = 0.47, which was in line with human-to-human\nagreement levels. This gap suggests MLLMs can \"see\" and interpret arithmetic\nwork relatively well, but still struggle to \"see\" student mathematical\nillustrations.", "AI": {"tldr": "研究探讨了多模态大型语言模型在评估手写数学作业和插图方面的表现。模型在评估算术题答案上有接近人类的准确率，但在直接受换单纯插图进行评估时遇到困难。辅助人类描述插图后，模型表现有所提升。", "motivation": "随着多模态大型语言模型（MLLMs）的最新进展，研究它们在评估、分析并提供学生手写作业反馈方面的潜力变得十分重要。特别是在小学和初中数学教育中，这种能力尤为有益，因为看到学生问题的解答步骤可以提供有价值的洞察，尽管这需要大量的时间来批改。", "method": "研究通过两个实验探讨了多模态大型语言模型（MLLMs）在评估和分析手写学生作业方面的应用。实验A分析了来自加纳中学生解决算术题的288份手写答案。实验B评估了来自美国小学生的150份数学插图，这些插图作为题目的答案，并且需要复杂的视觉解释和教学判断来分析评价。研究尝试将MLLMs的视觉能力与其教学能力分开，通过直接评估学生插图以及用详细的人类描述辅助MLLMs评估。", "result": "实验A中模型在判断算术题的答案上的准确率为95%，但仍存在一些人类教育者很少会犯的错误。实验B中，直接分析插图时模型的准确率较低，为0.20，当辅助人类描述后，准确率显著提高到0.47，与人与人之间的评分水平相当。", "conclusion": "研究结果表明，MLLMs在解释算术作业方面做得相对较好，但在识别和评估学生数学插图方面仍有挑战。"}}
{"id": "2510.05129", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05129", "abs": "https://arxiv.org/abs/2510.05129", "authors": ["Qingshu Xu", "Hong Jiao", "Tianyi Zhou", "Ming Li", "Nan Zhang", "Sydney Peters", "Yanbin Fu"], "title": "Automated Alignment of Math Items to Content Standards in Large-Scale Assessments Using Language Models", "comment": null, "summary": "Accurate alignment of items to content standards is critical for valid score\ninterpretation in large-scale assessments. This study evaluates three automated\nparadigms for aligning items with four domain and nineteen skill labels. First,\nwe extracted embeddings and trained multiple classical supervised machine\nlearning models, and further investigated the impact of dimensionality\nreduction on model performance. Second, we fine-tuned eight BERT model and its\nvariants for both domain and skill alignment. Third, we explored ensemble\nlearning with majority voting and stacking with multiple meta-models. The\nDeBERTa-v3-base achieved the highest weighted-average F1 score of 0.950 for\ndomain alignment while the RoBERTa-large yielded the highest F1 score of 0.869\nfor skill alignment. Ensemble models did not surpass the best-performing\nlanguage models. Dimension reduction enhanced linear classifiers based on\nembeddings but did not perform better than language models. This study\ndemonstrated different methods in automated item alignment to content\nstandards.}", "AI": {"tldr": "研究使用多种语言模型和经典机器学习模型评估自动对齐项目与内容标准的效果，发现DeBERTa-v3-base和RoBERTa-large在领域和技能对齐上表现最佳。", "motivation": "准确地对齐项目与内容标准对于大规模评估中的有效得分解释至关重要。", "method": "本研究评估了三种自动对齐项目与内容标准的方法：1）提取嵌入并训练多种经典监督机器学习模型，并进一步研究了降维对模型性能的影响；2）对八个BERT模型及其变体进行微调以实现领域和技能对齐；3）探索基于多数投票和堆叠的集成学习方法。", "result": "研究结果显示，对于领域对齐，DeBERTa-v3-base模型取得了最高的加权平均F1分数为0.950；而对于技能对齐，RoBERTa-large模型获得了最高的F1分数为0.869。集成模型表现不如最佳的语言模型，而降维虽然提升了基于嵌入的线性分类器性能，但其表现仍然落后于语言模型。", "conclusion": "这项研究展示了不同方法在自动化项目与内容标准对齐的应用。"}}
{"id": "2510.05558", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05558", "abs": "https://arxiv.org/abs/2510.05558", "authors": ["Christopher Hoang", "Mengye Ren"], "title": "Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics", "comment": "Project page: https://agenticlearning.ai/midway-network/", "summary": "Object recognition and motion understanding are key components of perception\nthat complement each other. While self-supervised learning methods have shown\npromise in their ability to learn from unlabeled data, they have primarily\nfocused on obtaining rich representations for either recognition or motion\nrather than both in tandem. On the other hand, latent dynamics modeling has\nbeen used in decision making to learn latent representations of observations\nand their transformations over time for control and planning tasks. In this\nwork, we present Midway Network, a new self-supervised learning architecture\nthat is the first to learn strong visual representations for both object\nrecognition and motion understanding solely from natural videos, by extending\nlatent dynamics modeling to this domain. Midway Network leverages a midway\ntop-down path to infer motion latents between video frames, as well as a dense\nforward prediction objective and hierarchical structure to tackle the complex,\nmulti-object scenes of natural videos. We demonstrate that after pretraining on\ntwo large-scale natural video datasets, Midway Network achieves strong\nperformance on both semantic segmentation and optical flow tasks relative to\nprior self-supervised learning methods. We also show that Midway Network's\nlearned dynamics can capture high-level correspondence via a novel analysis\nmethod based on forward feature perturbation.", "AI": {"tldr": "Midway Network 是一种新的自我监督学习架构，首次仅通过自然视频学习对象识别和运动理解的强大视觉表示，通过扩展潜在动态建模到这个领域实现这一目标，它在语义分割和光流任务中表现出色。", "motivation": "尽管自我监督学习方法已经表明它们可以从无标签数据中学习，但主要集中在获取丰富的识别或运动表示，而不是两者兼备。因此，Midway Network 的目标是仅通过自然视频，同时学习强大的对象识别和运动理解视觉表示。", "method": "通过扩展潜在动态建模到这个领域，Midway Network 采用了中间的自顶向下路径推断视频帧之间的运动潜变量，并使用密集的前向预测目标和分层结构来处理自然视频中的复杂多目标场景。", "result": "研究结果表明，Midway Network 在两个大型自然视频数据集上预先训练之后，在语义分割和光流任务中相对于先前的自我监督学习方法表现出强大的性能。", "conclusion": "Midway Network 的学习动态能够通过一种基于前向特征扰动的新分析方法捕捉高层次的对应关系。"}}
{"id": "2510.05130", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05130", "abs": "https://arxiv.org/abs/2510.05130", "authors": ["Shaoyi Zheng", "Canyu Zhang", "Tianyi Zhou", "Shengjie Wang"], "title": "Submodular Context Partitioning and Compression for In-Context Learning-short paper", "comment": null, "summary": "In-context learning (ICL) enables efficient few-shot learning in large\nlanguage models (LLMs) without training, but suffers from the quadratic input\ncomplexity of transformers, limiting the maximum number of exemplars. While\nvarious efficient ICL approaches partition the context into blocks to process\n(e.g., ensembling, compression, cross-attention), they often ignore the\ninformation redundancy or under-representation caused by different partition\nstrategies, leading to suboptimal performance. To tackle this problem, we\npropose Sub-CP, a block-aware context selection framework that leverages\nsubmodular objectives to control block diversity. Sub-CP supports a flexible\nspectrum of selection strategies, allowing each block to range from globally\ndiverse to locally coherent. This allows fine-grained control over semantic\nstructure while enabling precomputation. Extensive experiments across diverse\ntasks on multiple datasets show that Sub-CP consistently improves performance\nacross model scales.", "AI": {"tldr": "The paper addresses inefficiencies in in-context learning for large language models by proposing Sub-CP, a framework that enhances performance through diverse and coherent block partitioning strategies.", "motivation": "The motivation is to improve in-context learning (ICL) for large language models (LLMs) by addressing the limitations of existing methods related to input complexity and block partitioning.", "method": "Sub-CP is a block-aware context selection framework that uses submodular objectives to control block diversity, allowing for a flexible range of selection strategies from globally diverse to locally coherent.", "result": "Extensive experiments on various tasks and datasets show that Sub-CP consistently improves performance across different model scales.", "conclusion": "Sub-CP effectively addresses the performance issues associated with information redundancy and under-representation caused by different partition strategies in existing in-context learning methods."}}
{"id": "2510.05560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05560", "abs": "https://arxiv.org/abs/2510.05560", "authors": ["Hongchi Xia", "Chih-Hao Lin", "Hao-Yu Hsu", "Quentin Leboutet", "Katelyn Gao", "Michael Paulitsch", "Benjamin Ummenhofer", "Shenlong Wang"], "title": "HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video", "comment": "Project page: https://xiahongchi.github.io/HoloScene", "summary": "Digitizing the physical world into accurate simulation-ready virtual\nenvironments offers significant opportunities in a variety of fields such as\naugmented and virtual reality, gaming, and robotics. However, current 3D\nreconstruction and scene-understanding methods commonly fall short in one or\nmore critical aspects, such as geometry completeness, object interactivity,\nphysical plausibility, photorealistic rendering, or realistic physical\nproperties for reliable dynamic simulation. To address these limitations, we\nintroduce HoloScene, a novel interactive 3D reconstruction framework that\nsimultaneously achieves these requirements. HoloScene leverages a comprehensive\ninteractive scene-graph representation, encoding object geometry, appearance,\nand physical properties alongside hierarchical and inter-object relationships.\nReconstruction is formulated as an energy-based optimization problem,\nintegrating observational data, physical constraints, and generative priors\ninto a unified, coherent objective. Optimization is efficiently performed via a\nhybrid approach combining sampling-based exploration with gradient-based\nrefinement. The resulting digital twins exhibit complete and precise geometry,\nphysical stability, and realistic rendering from novel viewpoints. Evaluations\nconducted on multiple benchmark datasets demonstrate superior performance,\nwhile practical use-cases in interactive gaming and real-time digital-twin\nmanipulation illustrate HoloScene's broad applicability and effectiveness.\nProject page: https://xiahongchi.github.io/HoloScene.", "AI": {"tldr": "HoloScene是一种新型的交互式3D重建框架，它通过优化目标函数来实现几何、物理和渲染的高效重建，适用于多种应用场合。", "motivation": "当前的3D重建和场景理解方法在几何完整性、对象交互性、物理合理性等多个方面通常存在不足。为了解决这些问题，提出了HoloScene。", "method": "HoloScene采用了一种基于能量优化的方法来进行3D重建，该方法将观察数据、物理约束和生成性先验集成到一个统一的目标函数中，通过结合基于采样的探索和基于梯度的细化来高效地进行优化。", "result": "实验结果表明，在多个基准数据集上的评估显示了HoloScene的优越性能，同时在互动游戏和实时数字孪生操作中的应用也展示了其广泛适用性和有效性。", "conclusion": "HoloScene可以创建出几何完整精准、物理稳定、视点变化下渲染逼真的数字孪生，体现了该框架对于复杂重建问题的处理能力。"}}
{"id": "2510.05131", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05131", "abs": "https://arxiv.org/abs/2510.05131", "authors": ["Bowen Wei"], "title": "Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery", "comment": null, "summary": "Head Start programs utilizing GoEngage face significant challenges when new\nor rotating staff attempt to locate appropriate Tasks (modules) on the platform\nhomepage. These difficulties arise from domain-specific jargon (e.g., IFPA,\nDRDP), system-specific nomenclature (e.g., Application Pool), and the inherent\nlimitations of lexical search in handling typos and varied word ordering. We\npropose a pragmatic hybrid semantic search system that synergistically combines\nlightweight typo-tolerant lexical retrieval, embedding-based vector similarity,\nand constrained large language model (LLM) re-ranking. Our approach leverages\nthe organization's existing Task Repository and Knowledge Base infrastructure\nwhile ensuring trustworthiness through low false-positive rates, evolvability\nto accommodate terminological changes, and economic efficiency via intelligent\ncaching, shortlist generation, and graceful degradation mechanisms. We provide\na comprehensive framework detailing required resources, a phased implementation\nstrategy with concrete milestones, an offline evaluation protocol utilizing\ncurated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online\nmeasurement methodology incorporating query success metrics, zero-result rates,\nand dwell-time proxies.", "AI": {"tldr": "提出了一种混合语义搜索系统，以解决Head Start项目中新手员工面临的任务搜索难题，该系统通过结合多种技术，提高了搜索的准确性、可信度和经济效益。", "motivation": "解决了Head Start项目中新手或轮岗员工在平台上寻找适当任务模块时，因领域特定术语和系统命名法导致的搜索困难。", "method": "提出了一种实用的混合语义搜索系统，该系统通过轻量级的容错词汇检索、基于嵌入向量的相似度以及受限的大语言模型（LLM）重排序进行协同工作。", "result": "详细框架包括所需资源、分阶段实施策略、具体的里程碑、使用精编测试用例的离线评估协议（如Hit@K, Precision@K, Recall@K, MRR）以及包含查询成功率、零结果率和驻留时间代理的在线测量方法。", "conclusion": "该方法利用现有的任务库和知识库基础设施，确保了可信度，具有适应术语变化的可进化性，并通过智能缓存、短名单生成和优雅降级机制实现了经济效益。"}}
{"id": "2510.05586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05586", "abs": "https://arxiv.org/abs/2510.05586", "authors": ["Bin Kang", "Bin Chen", "Junjie Wang", "Yulin Li", "Junzhi Zhao", "Zhuotao Tian"], "title": "CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval", "comment": "ACMMM2025(oral)", "summary": "Existing Visual Language Models (VLMs) suffer structural limitations where a\nfew low contribution tokens may excessively capture global semantics,\ndominating the information aggregation process and suppressing the\ndiscriminative features in text-driven image retrieval tasks. To address this,\nwe introduce \\textbf{CalibCLIP}, a training-free method designed to calibrate\nthe suppressive effect of dominant tokens. Specifically, in the visual space,\nwe propose the Contrastive Visual Enhancer (CVE), which decouples visual\nfeatures into target and low information regions. Subsequently, it identifies\ndominant tokens and dynamically suppresses their representations.In the textual\nspace, we introduce the Discriminative Concept Calibrator (DCC), which aims to\ndifferentiate between general and discriminative concepts within the text\nquery. By mitigating the challenges posed by generic concepts and improving the\nrepresentations of discriminative concepts, DCC strengthens the differentiation\namong similar samples. Finally, extensive experiments demonstrate consistent\nimprovements across seven benchmarks spanning three image retrieval tasks,\nunderscoring the effectiveness of CalibCLIP. Code is available at:\nhttps://github.com/kangbin98/CalibCLIP", "AI": {"tldr": "本文提出了一种无需训练的CalibCLIP方法，通过对比视觉增强器（CVE）和辨别性概念校准器（DCC）解决了视觉语言模型（VLMs）中少数低贡献Token过度捕获全局语义的问题，实验表明该方法在不同图像检索任务中的基准测试上表现优异。", "motivation": "现有视觉语言模型（VLMs）由于结构限制，少数低贡献Token过度影响全局语义，主导信息聚合过程，抑制文本驱动图像检索任务中的辨别性特征。", "method": "针对现有视觉语言模型（VLMs）中少数低贡献Token过度捕获全局语义的问题，本文提出了CalibCLIP，一种无需训练的校准方法。具体来说，在视觉空间中，提出了对比视觉增强器（CVE），将视觉特征分解为目标区域和低信息区域，识别出主导Token，并动态抑制其表达。在文本空间中，引入了辨别性概念校准器（DCC），旨在区分文本查询中的通用概念和辨别性概念，削弱通用概念的挑战，强化辨别性概念的表达，从而增强相似样本间的差异化。", "result": "实验结果证明，CalibCLIP在七个跨三种图像检索任务的基准测试中均表现出一致的改进效果。", "conclusion": "代码已开源，证明了CalibCLIP的有效性。"}}
{"id": "2510.05132", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05132", "abs": "https://arxiv.org/abs/2510.05132", "authors": ["Sheng Jia", "Xiao Wang", "Shiva Prasad Kasiviswanathan"], "title": "Training Large Language Models To Reason In Parallel With Global Forking Tokens", "comment": null, "summary": "Although LLMs have demonstrated improved performance by scaling parallel\ntest-time compute, doing so relies on generating reasoning paths that are both\ndiverse and accurate. For challenging problems, the forking tokens that trigger\ndiverse yet correct reasoning modes are typically deep in the sampling tree.\nConsequently, common strategies to encourage diversity, such as temperature\nscaling, encounter a worsened trade-off between diversity and accuracy.\nMotivated by this challenge, we treat parallel reasoning as a\nset-of-next-token-prediction problem, and incorporate a set-based global loss\ninto Supervised Fine-Tuning (SFT) using self-supervised bipartite matching\nbetween our global forking tokens and unique reasoning traces. We observe that,\nwhile naive fine-tuning with multiple reasoning traces collapses these unique\nreasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT),\npreserves these modes and produces emergent global forking tokens. Experiments\non multiple reasoning benchmarks show that our SSFT consistently outperforms\nSFT under both Pass@1 and Cons@k metrics.", "AI": {"tldr": "论文提出了一种新的微调方法（SSFT），通过集合形式的全局损失和二分匹配技术，有效地提高了在生成多样化且准确的推理路径上的性能。", "motivation": "动机是解决在处理具有挑战性问题时，现有方法在促进多样性的同时降低了准确性的困境。特别是在采样树的深处，触发多样化且正确推理路径的分支标记通常难以被及时发现。", "method": "采用了集合形式的全局损失(Supervised Fine-Tuning, SFT)的方法，通过自我监督的二分匹配，解决挑战性问题中的多种可能推理路径的生成问题。这种方法称为集合监督微调(Set Supervised Fine-Tuning, SSFT)。", "result": "实验结果表明，与传统的SFT方法相比，SSFT方法在多种推理基准测试下的Pass@1和Cons@k指标上都有显著的提升。", "conclusion": "结论表明，SSFT相较于传统的SFT方法能够更好地在保持推理路径多样性的同时提高预测准确性，实现了性能上的提升。"}}
{"id": "2510.05593", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05593", "abs": "https://arxiv.org/abs/2510.05593", "authors": ["Zeqi Gu", "Markos Georgopoulos", "Xiaoliang Dai", "Marjan Ghazvininejad", "Chu Wang", "Felix Juefei-Xu", "Kunpeng Li", "Yujun Shi", "Zecheng He", "Zijian He", "Jiawei Zhou", "Abe Davis", "Jialiang Wang"], "title": "Improving Chain-of-Thought Efficiency for Autoregressive Image Generation", "comment": null, "summary": "Autoregressive multimodal large language models have recently gained\npopularity for image generation, driven by advances in foundation models. To\nenhance alignment and detail, newer approaches employ chain-of-thought (CoT)\nreasoning, expanding user inputs into elaborated prompts prior to image\nsynthesis. However, this strategy can introduce unnecessary redundancy -- a\nphenomenon we call visual overthinking -- which increases computational costs\nand can introduce details that contradict the original prompt. In this work, we\nexplore how to generate more concise CoT sequences for more efficient image\ngeneration. We introduce ShortCoTI, a lightweight optimization framework that\nencourages more concise CoT while preserving output image quality. ShortCoTI\nrewards more concise prompts with an adaptive function that scales according to\nan estimated difficulty for each task. Incorporating this reward into a\nreinforcement learning paradigm reduces prompt reasoning length by 54% while\nmaintaining or slightly improving quality metrics across multiple benchmarks\n(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates\nverbose explanations and repetitive refinements, producing reasoning prompts\nthat are both concise and semantically rich. As a result, ShortCoTI improves\ncomputational efficiency without compromising the fidelity or visual appeal of\ngenerated images.", "AI": {"tldr": "The paper introduces ShortCoTI, an optimization framework that reduces unnecessary redundancy in image generation, thus improving computational efficiency without compromising image quality.", "motivation": "The motivation behind this paper is to address the issue of visual overthinking, which introduces unnecessary redundancy, increases computational costs, and can introduce details that contradict the original prompt in image generation.", "method": "ShortCoTI is introduced as a lightweight optimization framework that encourages more concise chain-of-thought (CoT) sequences while preserving output image quality in autoregressive multimodal large language models used for image generation.", "result": "Incorporating ShortCoTI into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval).", "conclusion": "ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images by eliminating verbose explanations and repetitive refinements in reasoning prompts."}}
{"id": "2510.05133", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05133", "abs": "https://arxiv.org/abs/2510.05133", "authors": ["Y. Du", "G. Wu", "G. Tang", "W. Wang", "Q. Fan"], "title": "Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios", "comment": "17 pages. Technical report", "summary": "Synthetic data generated by large language models has become integral to\nmodern NLP training pipelines, from bootstrapping reasoning capabilities to\naugmenting instruction-following datasets. While recent work demonstrates\nsuccessful applications maintaining high external data ratios, systematic\nunderstanding of how synthetic data proportion affects model behavior across\ndifferent scales remains limited. This paper presents a controlled empirical\nstudy examining model performance, calibration, and output characteristics when\ntrained on varying synthetic-to-external data ratios. Using the Pythia model\nsuite (410M-12B parameters) across five diverse tasks, we evaluate models after\none to three training iterations with synthetic data proportions ranging from\n0-50\\%. Our key findings include: models maintain stable performance with up to\n20\\% synthetic data, but degradation accelerates beyond 30\\%; larger models\n(6.9B-12B) show greater robustness to synthetic data than smaller models\n(410M-1.4B); calibration degradation precedes accuracy loss, providing an early\nwarning signal; and task characteristics matter, with reasoning tasks degrading\nfaster than retrieval tasks under synthetic data training. Importantly, we find\nthat current best practices, such as those employed in STaR and Self-Instruct\nsystems that maintain greater than 80\\% external data, operate well within safe\nregimes identified by our experiments. We provide practical guidance for\npractitioners on synthetic data budgets based on model scale and task\nrequirements, alongside detailed comparison with concurrent work including\nShumailov et al.'s model collapse findings.", "AI": {"tldr": "研究通过一系列实验探讨了合成数据比例对语言模型的影响，发现模型在一定比例的合成数据下表现稳定，但过高的合成数据比例会加速性能下降，研究为合成数据应用提供了指导。", "motivation": "尽管之前的工作展示了在保持较高外部数据比例的情况下合成数据的成功应用，但是关于合成数据比例如何影响模型行为的系统性理解仍然不足，尤其是在不同规模下的影响。", "method": "本研究使用Pythia模型套件（参数从410M至12B）在五个多样化的任务上进行实验，评估模型在1至3次训练过程中，随着合成数据比例（0%-50%）的变化，模型性能、校准度和输出特征的变化情况。", "result": "研究发现：模型在合成数据比例不超过20%时性能保持稳定，但超过30%后性能急剧下降；大型模型（6.9B-12B参数）比小型模型（410M-1.4B参数）更能抵抗合成数据的影响；校准度的下降先于准确性损失，可作为早期预警信号；任务的特性很重要，推理任务在合成数据训练下比检索任务更快衰减。", "conclusion": "当前最佳实践，如STaR和Self-Instruct系统维持超过80%的外部数据，在我们的实验确认的安全范围内运作良好。根据模型规模和任务要求提供实际的合成数据预算指导。"}}
{"id": "2510.05609", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05609", "abs": "https://arxiv.org/abs/2510.05609", "authors": ["Junwen Chen", "Peilin Xiong", "Keiji Yanai"], "title": "HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection", "comment": null, "summary": "Recent Human-object interaction detection (HOID) methods highly require prior\nknowledge from VLMs to enhance the interaction recognition capabilities. The\ntraining strategies and model architectures for connecting the knowledge from\nVLMs to the HOI instance representations from the object detector are\nchallenging, and the whole framework is complex for further development or\napplication. On the other hand, the inherent reasoning abilities of MLLMs on\nhuman-object interaction detection are under-explored. Inspired by the recent\nsuccess of training MLLMs with reinforcement learning (RL) methods, we propose\nHOI-R1 and first explore the potential of the language model on the HOID task\nwithout any additional detection modules. We introduce an HOI reasoning process\nand HOID reward functions to solve the HOID task by pure text. The results on\nthe HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline\nwith great generalization ability. The source code is available at\nhttps://github.com/cjw2021/HOI-R1.", "AI": {"tldr": "本文提出HOI-R1方法，基于强化学习无需额外检测模块解决了HOI检测任务，在HICO-DET数据集上达到了基线方法两倍的准确率。", "motivation": "当前HOI检测方法高度依赖VLMs的知识，但在将这些知识与目标检测模块的HOI实例表示连接的训练策略和模型架构方面颇为复杂。同时，多语种语言模型在HOI检测任务中的固有的推理能力还未得到充分探索。因此，本文旨在利用强化学习方法训练MLLMs来探索解决HOI任务的新方法。", "method": "本文提出了一种新的方法HOI-R1，首次探索了语言模型在HOI检测任务上的潜力，无需额外的检测模块。通过引入HOI推理过程和HOI检测奖励函数，该方法仅使用文本解决了HOI检测的问题。", "result": "实验结果表明，HOI-R1在HICO-DET数据集上达到了基线方法两倍的准确率，并具有很好的泛化能力。", "conclusion": "HOI-R1展示了一种新的方法，可以直接通过文本处理解决HOI检测任务，且无需复杂的模块连接，显示出巨大的潜力和发展空间。"}}
{"id": "2510.05135", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05135", "abs": "https://arxiv.org/abs/2510.05135", "authors": ["Vanya Bannihatti Kumar", "Divyanshu Goyal", "Akhil Eppa", "Neel Bhandari"], "title": "Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment", "comment": null, "summary": "Modern large language models (LLMs) excel at objective tasks such as\nevaluating mathematical reasoning and factual accuracy, yet they falter when\nfaced with the nuanced, subjective nature of assessing creativity. In this\nwork, we propose a novel curiosity-driven LLM-as-a-judge for evaluating\ncreative writing which is personlized to each individual's creative judgments.\nWe use the Torrance Test of Creative Thinking(TTCW) benchmark introduced in\nChakrabarty et al. (2024), which has stories annotated by expert humans across\nvarious subjective dimensions like Originality, to test our hypothesis. We show\nthat our method enables models across various sizes, to learn the nuanced\ncreative judgments of different individuals, by showing improvements over\nbaseline supervised finetuning(SFT) method across various evaluation metrics\nlike Pearson correlation, Cohen's and F1 values. Our method is especially\nuseful in subjective evaluations where not all the annotators agree with each\nother.", "AI": {"tldr": "我们提出了一种好奇心驱动的LLM评估创意写作的方法，该方法比监督微调的方法在个性化评估创意判断方面表现更好。", "motivation": "尽管现代大型语言模型在客观任务上表现出色，但它们在评估创造力等主观任务上面临挑战。我们希望通过个性化的方法来提高模型在评估创意写作方面的表现。", "method": "我们提出了一种新颖的好奇心驱动的LLM作为评判者，用于个性化评估每个人的创意写作判断。我们使用Chakrabarty et al. (2024)提出的Torrance创造力思维测试(TTCW)基准进行实验，该基准包含由专家人类根据原创性等主观维度标注的故事。", "result": "实验结果显示，我们的方法相较于基线监督微调方法，在各种评估指标（Pearson相关性、Cohen's kappa和F1值）上均有提升，特别是对于主观评估，当不同标注者意见不一致时，这种方法尤为有用。", "conclusion": "这一研究证明了好奇心驱动的LLM在主观、个性化的创意写作评估任务上的有效性，对促进AI在创造力评估领域的应用具有潜力。"}}
{"id": "2510.05610", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05610", "abs": "https://arxiv.org/abs/2510.05610", "authors": ["Jiaqi Liu", "Tao Huang", "Chang Xu"], "title": "Efficient Conditional Generation on Scale-based Visual Autoregressive Models", "comment": null, "summary": "Recent advances in autoregressive (AR) models have demonstrated their\npotential to rival diffusion models in image synthesis. However, for complex\nspatially-conditioned generation, current AR approaches rely on fine-tuning the\npre-trained model, leading to significant training costs. In this paper, we\npropose the Efficient Control Model (ECM), a plug-and-play framework featuring\na lightweight control module that introduces control signals via a distributed\narchitecture. This architecture consists of context-aware attention layers that\nrefine conditional features using real-time generated tokens, and a shared\ngated feed-forward network (FFN) designed to maximize the utilization of its\nlimited capacity and ensure coherent control feature learning. Furthermore,\nrecognizing the critical role of early-stage generation in determining semantic\nstructure, we introduce an early-centric sampling strategy that prioritizes\nlearning early control sequences. This approach reduces computational cost by\nlowering the number of training tokens per iteration, while a complementary\ntemperature scheduling during inference compensates for the resulting\ninsufficient training of late-stage tokens. Extensive experiments on\nscale-based AR models validate that our method achieves high-fidelity and\ndiverse control over image generation, surpassing existing baselines while\nsignificantly improving both training and inference efficiency.", "AI": {"tldr": "提出了一种能够在不显著增加训练成本的前提下，实现复杂图像生成控制的高效控制模型（ECM），相较现有基线方法，实现了更高的训练和推理效率。", "motivation": "针对现有自回归（AR）模型在复杂的空间条件生成中需要精细调整预训练模型，导致高昂的训练成本的问题，提出了新型框架来优化训练效率和生成控制。", "method": "提出了一种称为高效控制模型（ECM）的即插即用框架。该框架具有一个轻量级的控制模块，通过分布式架构引入控制信号。这种架构包括上下文感知注意力层，利用实时生成的标记微调条件特征，并设计了一个共享门控前馈网络（FFN），旨在最大化其有限容量的利用率，并确保连贯的控制特征学习。此外，为了优先学习早期控制序列，提出了一种以早期为中心的采样策略，减少了每次迭代的训练标记数量，而推理过程中的温度计划则补偿了后期标记训练不足的问题。", "result": "实验验证了在尺度基AR模型上，方法能够在降低训练标记数量的同时，实现高保真度和多样化的图像生成控制，优于现有基线方法，并显著提高了训练和推理效率。", "conclusion": "该研究展示了一种可行的方法，通过优化的控制模块和策略，在控制空间条件生成上的训练效率和效果方面，自回归模型有望与扩散模型相媲美，同时减少了计算成本、提升了控制能力。"}}
{"id": "2510.05136", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05136", "abs": "https://arxiv.org/abs/2510.05136", "authors": ["Luka Terčon", "Kaja Dobrovoljc"], "title": "Linguistic Characteristics of AI-Generated Text: A Survey", "comment": "26 pages, 5 figures", "summary": "Large language models (LLMs) are solidifying their position in the modern\nworld as effective tools for the automatic generation of text. Their use is\nquickly becoming commonplace in fields such as education, healthcare, and\nscientific research. There is a growing need to study the linguistic features\npresent in AI-generated text, as the increasing presence of such texts has\nprofound implications in various disciplines such as corpus linguistics,\ncomputational linguistics, and natural language processing. Many observations\nhave already been made, however a broader synthesis of the findings made so far\nis required to provide a better understanding of the topic. The present survey\npaper aims to provide such a synthesis of extant research. We categorize the\nexisting works along several dimensions, including the levels of linguistic\ndescription, the models included, the genres analyzed, the languages analyzed,\nand the approach to prompting. Additionally, the same scheme is used to present\nthe findings made so far and expose the current trends followed by researchers.\nAmong the most-often reported findings is the observation that AI-generated\ntext is more likely to contain a more formal and impersonal style, signaled by\nthe increased presence of nouns, determiners, and adpositions and the lower\nreliance on adjectives and adverbs. AI-generated text is also more likely to\nfeature a lower lexical diversity, a smaller vocabulary size, and repetitive\ntext. Current research, however, remains heavily concentrated on English data\nand mostly on text generated by the GPT model family, highlighting the need for\nbroader cross-linguistic and cross-model investigation. In most cases authors\nalso fail to address the issue of prompt sensitivity, leaving much room for\nfuture studies that employ multiple prompt wordings in the text generation\nphase.", "AI": {"tldr": "本文综述了AI生成文本的语言特征研究，总结了现存趋势，强调了需要进行更广泛的跨语言和跨模型调查以及对提示敏感性的研究。", "motivation": "鉴于AI生成文本在教育、医疗和科学研究等领域的广泛应用，研究其语言特征变得越来越重要，尤其是在语料库语言学、计算语言学和自然语言处理等领域。这种广泛存在的AI生成文本需要一个更系统的总结和理解。", "method": "本文通过多个维度分类现有研究工作，并总结目前研究的趋势。维度包括语言描述的层次、模型类型、分析的体裁、分析的语言和提示方法。", "result": "现有的研究发现，AI生成的文本更倾向于使用正式、非个人化的风格，特征是名词、限定词和介词的增加使用，而形容词和副词的使用较少。同时，AI生成文本的词汇多样性较低，词汇量较少且重复性较高。", "conclusion": "研究显示，大多数研究集中于英语数据和GPT模型系列生成的文本，需要更多的跨语言和跨模型的研究。此外，很少有研究探讨提示敏感性的问题，未来的研究应考虑使用多种提示词进行文本生成。"}}
{"id": "2510.05613", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05613", "abs": "https://arxiv.org/abs/2510.05613", "authors": ["Ziqiao Meng", "Qichao Wang", "Zhiyang Dou", "Zixing Song", "Zhipeng Zhou", "Irwin King", "Peilin Zhao"], "title": "PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction", "comment": null, "summary": "Autoregressive point cloud generation has long lagged behind diffusion-based\napproaches in quality. The performance gap stems from the fact that\nautoregressive models impose an artificial ordering on inherently unordered\npoint sets, forcing shape generation to proceed as a sequence of local\npredictions. This sequential bias emphasizes short-range continuity but\nundermines the model's capacity to capture long-range dependencies, hindering\nits ability to enforce global structural properties such as symmetry,\nconsistent topology, and large-scale geometric regularities. Inspired by the\nlevel-of-detail (LOD) principle in shape modeling, we propose PointNSP, a\ncoarse-to-fine generative framework that preserves global shape structure at\nlow resolutions and progressively refines fine-grained geometry at higher\nscales through a next-scale prediction paradigm. This multi-scale factorization\naligns the autoregressive objective with the permutation-invariant nature of\npoint sets, enabling rich intra-scale interactions while avoiding brittle fixed\norderings. Experiments on ShapeNet show that PointNSP establishes\nstate-of-the-art (SOTA) generation quality for the first time within the\nautoregressive paradigm. In addition, it surpasses strong diffusion-based\nbaselines in parameter, training, and inference efficiency. Finally, in dense\ngeneration with 8,192 points, PointNSP's advantages become even more\npronounced, underscoring its scalability potential.", "AI": {"tldr": "PointNSP解决了自回归点云生成中的顺序偏见问题，通过一种从粗到细的生成方法，在多尺度下实现了高质量的点云生成，达到新的技术高峰。", "motivation": "自回归点云生成方法由于强加了固有的人造顺序，与无序的点集不符，导致在生成质量上长期落后于扩散方法。这种顺序偏见强调了短程连续性，但削弱了模型捕捉长程依赖的能力，影响全局结构属性的生成。", "method": "提出了PointNSP框架，这是一个从粗到细的生成模型，它在低分辨率下保持全局形状结构，并通过下一尺度预测逐步在更高尺度上细化精细几何。这个多尺度分解使得自回归目标与点集的排列不变性相一致，同时避免了固定顺序的脆弱性。", "result": "实验表明，PointNSP在ShapeNet数据集上首次建立自回归范式的生成质量最新水准。它在参数、训练和推断效率上超过了强大的扩散基准模型。此外，在生成8,192个点的密集点云时，PointNSP的优势更为显著。", "conclusion": "通过提出PointNSP，改进了自回归模型生成点云的质量，使其首次达到了某一领域的最先进水平，同时还在效率方面超过了某些扩散模型。"}}
{"id": "2510.05137", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05137", "abs": "https://arxiv.org/abs/2510.05137", "authors": ["Maojia Song", "Renhang Liu", "Xinyu Wang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Soujanya Poria", "Jingren Zhou"], "title": "Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics", "comment": null, "summary": "RAG (Retrieval-Augmented Generation) systems and web agents are increasingly\nevaluated on multi-hop deep search tasks, yet current practice suffers from two\nmajor limitations. First, most benchmarks leak the reasoning path in the\nquestion text, allowing models to follow surface cues rather than discover\nreasoning chains autonomously. Second, evaluation is typically reduced to a\nsingle pass rate, which collapses diverse behaviours into one score and\nobscures whether failures stem from inadequate search, poor knowledge use, or\ninappropriate refusal. To address these issues, we present WebDetective, a\nbenchmark of hint-free multi-hop questions paired with a controlled Wikipedia\nsandbox that ensures full traceability of model actions, and a holistic\nevaluation framework that separates search sufficiency, knowledge utilisation,\nand refusal behaviour. Our evaluation of 25 state-of-the-art models reveals\nsystematic weaknesses across all architectures: models struggle with knowledge\nutilisation despite having sufficient evidence and demonstrate near-absent\nappropriate refusal when evidence is lacking. These patterns expose a\nfundamental gap: today's systems excel at executing given reasoning paths but\nfail when required to discover them. We develop an agentic workflow,\nEvidenceLoop, that explicitly targets the challenges our benchmark identifies,\nincorporating verification loops and systematic evidence tracking that improve\nboth search and synthesis capabilities. This baseline demonstrates that\nWebDetective's diagnostic framework can guide concrete architectural\nimprovements, establishing our benchmark as a critical tool for developing\ngenuinely autonomous reasoning systems rather than pattern-following agents.", "AI": {"tldr": "提出了WebDetective基准测试框架，以解决RAG系统和网络代理在多跳搜索任务中所面临的提示泄露和单一评估的问题，通过对25种先进模型的评估发现了知识利用和适当拒绝方面的系统性弱点，并通过EvidenceLoop工作流程来改进这些系统的搜索和推理能力。", "motivation": "当前评估实践中存在两个主要问题：提示泄露推理路径，以及通过单一通过率进行评估导致的行为多样性被忽视。这导致了评估的不准确和系统能力发展受限，因此需要一个更为全面和多样化的评估框架来推进自主推理系统的开发。", "method": "通过开发名为WebDetective的基准测试和EvidenceLoop的代理工作流程来解决当前RAG系统和网络代理在多跳深度搜索任务中的两个主要限制：问题文本中的推理路径泄露和评估方法的单一化。WebDetective包含无提示的多跳问题及一个受控的维基百科沙盒，确保模型行为的全面可追溯性，并提供了一个将搜索充分性、知识利用和拒绝行为分开的全面评估框架。EvidenceLoop则专注于解决这些挑战，通过包含验证循环和系统证据跟踪来提升搜索和综合能力。", "result": "通过WebDetective对25个先进模型的评估揭示了系统在知识利用和适当拒绝行为上的系统性弱点，暴露了当前系统执行给定推理路径能力较强，但发现推理路径的能力较弱的问题。", "conclusion": "WebDetective的诊断框架可以引导具体的技术改进，被视为开发真正自主推理系统的重要工具，而不是仅仅跟随模式的代理。EvidenceLoop展现出改善搜索和综合能力的潜力。"}}
{"id": "2510.05615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05615", "abs": "https://arxiv.org/abs/2510.05615", "authors": ["Guangrong Wan", "Jun liu", "Tang tang", "Lianghao Shi", "Wenjun Luo", "TingTing Xu"], "title": "TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for Automated Tear Film Break-Up Segmentation", "comment": null, "summary": "Tear film break-up (TFBU) analysis is critical for diagnosing dry eye\nsyndrome, but automated TFBU segmentation remains challenging due to the lack\nof annotated datasets and integrated solutions. This paper introduces the Tear\nFilm Multi-task (TFM) Dataset, the first comprehensive dataset for multi-task\ntear film analysis, comprising 15 high-resolution videos (totaling 6,247\nframes) annotated with three vision tasks: frame-level classification ('clear',\n'closed', 'broken', 'blur'), Placido Ring detection, and pixel-wise TFBU area\nsegmentation. Leveraging this dataset, we first propose TF-Net, a novel and\nefficient baseline segmentation model. TF-Net incorporates a MobileOne-mini\nbackbone with re-parameterization techniques and an enhanced feature pyramid\nnetwork to achieve a favorable balance between accuracy and computational\nefficiency for real-time clinical applications. We further establish benchmark\nperformance on the TFM segmentation subset by comparing TF-Net against several\nstate-of-the-art medical image segmentation models. Furthermore, we design\nTF-Collab, a novel integrated real-time pipeline that synergistically leverages\nmodels trained on all three tasks of the TFM dataset. By sequentially\norchestrating frame classification for BUT determination, pupil region\nlocalization for input standardization, and TFBU segmentation, TF-Collab fully\nautomates the analysis. Experimental results demonstrate the effectiveness of\nthe proposed TF-Net and TF-Collab, providing a foundation for future research\nin ocular surface diagnostics. Our code and the TFM datasets are available at\nhttps://github.com/glory-wan/TF-Net", "AI": {"tldr": "本文提出TFM数据集和TF-Net模型，用于泪膜破裂的自动分析，进一步实现了一整套自动化分析流程TF-Collab，以提升干眼症诊断的准确性和效率。", "motivation": "针对自动泪膜破裂分析的困难，由于缺乏注释数据集和综合解决方案，文章旨在开发一种新的数据集和模型来提高诊断干眼综合征的效率和准确性。", "method": "本文介绍了TFM数据集，这是首个用于多任务泪膜分析的综合数据集。基于该数据集，作者提出了TF-Net，一个高效的基础分割模型，它通过改进特征金字塔网络来平衡准确性与计算效率。此外，还设计了TF-Collab，一个结合所有三项任务的实时集成管道，进一步自动化分析流程。", "result": "基准性能评估显示，TF-Net在TFM分割子集上相对于多种最先进的医学图像分割模型具备竞争优势。实验结果证明了TF-Net和TF-Collab的有效性，成功自动化泪膜分析流程。", "conclusion": "实验结果证明所提出的TF-Net和TF-Collab的有效性，为眼表诊断未来的研究奠定了基础。"}}
{"id": "2510.05138", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05138", "abs": "https://arxiv.org/abs/2510.05138", "authors": ["Gregory Hok Tjoan Go", "Khang Ly", "Anders Søgaard", "Amin Tabatabaei", "Maarten de Rijke", "Xinyi Chen"], "title": "LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation", "comment": null, "summary": "The rapid growth of scientific publications has made it increasingly\ndifficult to keep literature reviews comprehensive and up-to-date. Though prior\nwork has focused on automating retrieval and screening, the writing phase of\nsystematic reviews remains largely under-explored, especially with regard to\nreadability and factual accuracy. To address this, we present LiRA (Literature\nReview Agents), a multi-agent collaborative workflow which emulates the human\nliterature review process. LiRA utilizes specialized agents for content\noutlining, subsection writing, editing, and reviewing, producing cohesive and\ncomprehensive review articles. Evaluated on SciReviewGen and a proprietary\nScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey\nand MASS-Survey in writing and citation quality, while maintaining competitive\nsimilarity to human-written reviews. We further evaluate LiRA in real-world\nscenarios using document retrieval and assess its robustness to reviewer model\nvariation. Our findings highlight the potential of agentic LLM workflows, even\nwithout domain-specific tuning, to improve the reliability and usability of\nautomated scientific writing.", "AI": {"tldr": "研究团队开发了LiRA系统，以提高系统性文献综述的可靠性和可操作性，特别是在写作质量和引用质量方面表现出色。", "motivation": "由于科学出版物的快速增长，难以维持文献综述的全面性和时效性。之前的很多工作主要集中在自动化检索和筛选上，但文献综述的写作阶段，尤其是在可读性和事实准确性方面，探讨较少。", "method": "提出了一种多代理协作工作流LiRA，用于模拟人类文献综述过程，包括内容设计、子部分写作、编辑和审阅等专有代理的协作。", "result": "LiRA在SciReviewGen和一个专有的ScienceDirect数据集上的评估结果表明，其在写作和引用质量方面超过了现有的基准（如AutoSurvey和MASS-Survey），并且在与人类写作的相似性方面保持了竞争力。", "conclusion": "研究结果凸显了代理大语言模型工作流在提升自动化科学写作可靠性与可用性方面的潜力，即使在没有特定领域调优的情况下也是如此。"}}
{"id": "2510.05617", "categories": ["cs.CV", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05617", "abs": "https://arxiv.org/abs/2510.05617", "authors": ["Ibrahim Salihu Yusuf", "Iffanice Houndayi", "Rym Oualha", "Mohamed Aziz Cherif", "Kobby Panford-Quainoo", "Arnu Pretorius"], "title": "InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment", "comment": null, "summary": "Open-access multispectral imagery from missions like Landsat 8-9 and\nSentinel-2 has fueled the development of geospatial foundation models (GFMs)\nfor humanitarian and environmental applications. Yet, their deployment remains\nlimited by (i) the absence of automated geospatial data pipelines and (ii) the\nlarge size of fine-tuned models. Existing GFMs lack workflows for processing\nraw satellite imagery, and downstream adaptations often retain the full\ncomplexity of the original encoder.\n  We present InstaGeo, an open-source, end-to-end framework that addresses\nthese challenges by integrating: (1) automated data curation to transform raw\nimagery into model-ready datasets; (2) task-specific model distillation to\nderive compact, compute-efficient models; and (3) seamless deployment as\ninteractive web-map applications. Using InstaGeo, we reproduced datasets from\nthree published studies and trained models with marginal mIoU differences of\n-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for\ndesert locust prediction. The distilled models are up to 8x smaller than\nstandard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal\naccuracy loss.\n  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger\ncrop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp\nimprovement over prior baselines. Moreover, InstaGeo enables users to progress\nfrom raw data to model deployment within a single working day.\n  By unifying data preparation, model compression, and deployment, InstaGeo\ntransforms research-grade GFMs into practical, low-carbon tools for real-time,\nlarge-scale Earth observation. This approach shifts geospatial AI toward data\nquality and application-driven innovation. Source code, datasets, and model\ncheckpoints are available at:\nhttps://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git", "AI": {"tldr": "The paper presents InstaGeo, an open-source, end-to-end framework that overcomes limitations in deploying geospatial foundation models by integrating automated data curation, model distillation, and seamless deployment.", "motivation": "The motivation behind the paper is to address the challenges of automated geospatial data pipelines and the large size of fine-tuned models, which limit the deployment of geospatial foundation models (GFMs) for humanitarian and environmental applications.", "method": "The paper introduces InstaGeo, an end-to-end framework that integrates automated data curation, task-specific model distillation, and seamless deployment. It transforms raw satellite imagery into model-ready datasets and produces compact models with minimal accuracy loss, suitable for real-time Earth observation.", "result": "The distilled models in InstaGeo are up to 8x smaller than standard fine-tuned counterparts, reducing FLOPs and CO2 emissions. Reproduced datasets for flood mapping, crop segmentation, and desert locust prediction show only marginal drops in accuracy. A new crop segmentation dataset achieves a state-of-the-art mIoU of 60.65%.", "conclusion": "InstaGeo transforms research-grade GFMs into practical, low-carbon tools by unifying data preparation, model compression, and deployment. The approach shifts geospatial AI toward innovation driven by data quality and application needs, enabling the processing from raw data to model deployment in just one working day."}}
{"id": "2510.05139", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05139", "abs": "https://arxiv.org/abs/2510.05139", "authors": ["Hamed Jelodar", "Mohammad Meymani", "Parisa Hamedi", "Tochukwu Emmanuel Nwankwo", "Samita Bai", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description", "comment": null, "summary": "Natural Language Description (NLD) is a Natural Language Processing (NLP)\ntask that requires models to generate structured and meaningful outputs from\nnatural language inputs. In this work, we propose NLD-LLM, a systematic NLP\nframework to evaluate the performance of language models to generate accurate\nand concise source code descriptions. This framework incorporates a diverse set\nof transformer models, including Qwen, DeepSeek, Phi, LLaMA, and Mistral,\nspanning various sizes, architectures, and training approaches. Central to\nNLD-LLM is a comprehensive prompt design strategy that includes standardized\nformatting, clear task guidance, and NLD prompting, ensuring fair and\nconsistent evaluation. Additionally, we apply an iterative refinement process\nto improve output's quality and assess the model's adaptability. Using semantic\nand structural metrics, our analysis demonstrates that prompt engineering\nsignificantly impacts the effectiveness of the model such that smaller models\noften performing competitively when supported by well-crafted prompts.", "AI": {"tldr": "本文提出NLD-LLM框架，用于评估语言模型生成源代码描述的能力，强调提示工程在提高模型性能方面的重要性。", "motivation": "NLD是一项NLP任务，需要模型从自然语言输入生成结构化和有意义的输出。本研究旨在系统评估语言模型在生成源代码描述方面的性能。", "method": "提出了NLD-LLM，一个系统性的NLP框架，用于评估语言模型生成准确且简洁的源代码描述的能力。框架包含多种Transformer模型，并采用全面的提示设计策略，包括标准格式、清晰的任务指南和NLD提示。此外，还采用了迭代改进过程以提高输出质量并评估模型的适应性。", "result": "分析表明，提示工程显著影响模型的有效性，适当的设计可以使得较小的模型在性能上与较大的模型相当。", "conclusion": "研究展示了在NLD任务中，通过精心设计的提示策略，可以提高模型输出的质量，甚至让较小的模型在性能上表现出色。"}}
{"id": "2510.05633", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.05633", "abs": "https://arxiv.org/abs/2510.05633", "authors": ["Sara Mandelli", "Diego Vila-Portela", "David Vázquez-Padín", "Paolo Bestagini", "Fernando Pérez-González"], "title": "Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection", "comment": null, "summary": "Over the years, the forensics community has proposed several deep\nlearning-based detectors to mitigate the risks of generative AI. Recently,\nfrequency-domain artifacts (particularly periodic peaks in the magnitude\nspectrum), have received significant attention, as they have been often\nconsidered a strong indicator of synthetic image generation. However,\nstate-of-the-art detectors are typically used as black-boxes, and it still\nremains unclear whether they truly rely on these peaks. This limits their\ninterpretability and trust. In this work, we conduct a systematic study to\naddress this question. We propose a strategy to remove spectral peaks from\nimages and analyze the impact of this operation on several detectors. In\naddition, we introduce a simple linear detector that relies exclusively on\nfrequency peaks, providing a fully interpretable baseline free from the\nconfounding influence of deep learning. Our findings reveal that most detectors\nare not fundamentally dependent on spectral peaks, challenging a widespread\nassumption in the field and paving the way for more transparent and reliable\nforensic tools.", "AI": {"tldr": "本文研究了图像检测器是否依赖频谱峰值，并发现大多数检测器并不依赖频谱峰值，从而提出了一个简单线性检测器作为基线，以提高可解释性。", "motivation": "目前最先进的检测器通常作为黑箱使用，不清楚它们是否真正依赖这些峰值。这限制了它们的可解释性和信任度。为了应对这一问题，我们进行了系统性的研究。", "method": "我们提出了一种策略来从图像中移除频谱峰值，并分析了这一操作对多个检测器的影响。此外，我们引入了一种仅依赖频率峰值的简单线性检测器，作为完全可解释的基线，不受深度学习混淆的影响。", "result": "我们的研究结果显示，大多数检测器并非从根本上依赖于频谱峰值，这挑战了该领域的广泛假设，并为开发更加透明和可靠的取证工具铺平了道路。", "conclusion": "我们的研究挑战了检测器依赖频谱峰值的假设，并提出了一种新的检测器，这对于开发更透明和可信赖的图像检测工具具有重要意义。"}}
{"id": "2510.05141", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05141", "abs": "https://arxiv.org/abs/2510.05141", "authors": ["Byung-Doh Oh", "Tal Linzen"], "title": "To model human linguistic prediction, make LLMs less superhuman", "comment": null, "summary": "When people listen to or read a sentence, they actively make predictions\nabout upcoming words: words that are less predictable are generally read more\nslowly than predictable ones. The success of large language models (LLMs),\nwhich, like humans, make predictions about upcoming words, has motivated\nexploring the use of these models as cognitive models of human linguistic\nprediction. Surprisingly, in the last few years, as language models have become\nbetter at predicting the next word, their ability to predict human reading\nbehavior has declined. This is because LLMs are able to predict upcoming words\nmuch better than people can, leading them to predict lower processing\ndifficulty in reading than observed in human experiments; in other words,\nmainstream LLMs are 'superhuman' as models of language comprehension. In this\nposition paper, we argue that LLMs' superhumanness is primarily driven by two\nfactors: compared to humans, LLMs have much stronger long-term memory for facts\nand training examples, and they have much better short-term memory for previous\nwords in the text. We advocate for creating models that have human-like\nlong-term and short-term memory, and outline some possible directions for\nachieving this goal. Finally, we argue that currently available human data is\ninsufficient to measure progress towards this goal, and outline human\nexperiments that can address this gap.", "AI": {"tldr": "文章分析了大型语言模型（LLMs）比人类更优秀地预测下一个词的能力如何影响其作为人类语言预测认知模型的有效性，并提出需要开发具有人类般记忆能力的语言模型。", "motivation": "探讨大型语言模型在预测人类阅读行为上的不足，发现模型过度强于人类的语言预测能力导致不能准确预测人类处理语言时的难度，并提出解决这一问题的可能方向。", "method": "通过分析大型语言模型（LLMs）与人类语言理解的对比，文章探究了LLMs作为人类语言预测认知模型的潜力和局限性。作者认为，LLMs的'超人类'特性主要归因于它们比人类具有更强的事实和训练实例长期记忆，以及更好的文本中先前词汇的短期记忆能力。", "result": "发现LLMs的预测能力远超人类，导致它们无法准确预测人类阅读行为，反而预测出较低的阅读处理难度。", "conclusion": "需要创建具有人类般记忆能力的语言模型，并指出当前的人类数据不足以衡量进展，建议进行进一步的人类实验以填补这一空白。"}}
{"id": "2510.05643", "categories": ["cs.CV", "68T10,", "I.2.10; I.4.7"], "pdf": "https://arxiv.org/pdf/2510.05643", "abs": "https://arxiv.org/abs/2510.05643", "authors": ["Shozo Saeki", "Minoru Kawahara", "Hirohisa Aman"], "title": "Combined Hyperbolic and Euclidean Soft Triple Loss Beyond the Single Space Deep Metric Learning", "comment": "12 pages, 4 figures", "summary": "Deep metric learning (DML) aims to learn a neural network mapping data to an\nembedding space, which can represent semantic similarity between data points.\nHyperbolic space is attractive for DML since it can represent richer\nstructures, such as tree structures. DML in hyperbolic space is based on\npair-based loss or unsupervised regularization loss. On the other hand,\nsupervised proxy-based losses in hyperbolic space have not been reported yet\ndue to some issues in applying proxy-based losses in a hyperbolic space.\nHowever, proxy-based losses are attractive for large-scale datasets since they\nhave less training complexity. To address these, this paper proposes the\nCombined Hyperbolic and Euclidean Soft Triple (CHEST) loss. CHEST loss is\ncomposed of the proxy-based losses in hyperbolic and Euclidean spaces and the\nregularization loss based on hyperbolic hierarchical clustering. We find that\nthe combination of hyperbolic and Euclidean spaces improves DML accuracy and\nlearning stability for both spaces. Finally, we evaluate the CHEST loss on four\nbenchmark datasets, achieving a new state-of-the-art performance.", "AI": {"tldr": "本文提出 CHEST 损失，以改进基于代理的损失在双曲空间中的应用，提升深度度量学习的性能，导致在四个基准数据集上表现优于现有方法。", "motivation": "研发 CHEST 损失的原因在于弥补基于代理的损失在双曲空间应用上的空白，并改进大规模数据集的度量学习，以减少训练复杂度。", "method": "CHEST loss 结合了双曲空间和欧式空间的基于代理的损失以及基于双曲层次聚类的正则化损失，旨在改善深度度量学习的准确性与学习稳定性。", "result": "在四个基准数据集上的评估中，CHEST 损失达到了新的最先进性能。", "conclusion": "结合双曲和欧式空间的 CHEST 损失能够有效提高深度度量学习的准确性和学习稳定性。"}}
{"id": "2510.05142", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.05142", "abs": "https://arxiv.org/abs/2510.05142", "authors": ["Xin Wang", "Anshu Raj", "Matthew Luebbe", "Haiming Wen", "Shuozhi Xu", "Kun Lu"], "title": "Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models", "comment": "27 pages, 4 figures, 7 tables", "summary": "Data-driven materials discovery requires large-scale experimental datasets,\nyet most of the information remains trapped in unstructured literature.\nExisting extraction efforts often focus on a limited set of features and have\nnot addressed the integrated composition-processing-microstructure-property\nrelationships essential for understanding materials behavior, thereby posing\nchallenges for building comprehensive databases. To address this gap, we\npropose a multi-stage information extraction pipeline powered by large language\nmodels, which captures 47 features spanning composition, processing,\nmicrostructure, and properties exclusively from experimentally reported\nmaterials. The pipeline integrates iterative extraction with source tracking to\nenhance both accuracy and reliability. Evaluations at the feature level\n(independent attributes) and tuple level (interdependent features) yielded F1\nscores around 0.96. Compared with single-pass extraction without source\ntracking, our approach improved F1 scores of microstructure category by 10.0%\n(feature level) and 13.7% (tuple level), and reduced missed materials from 49\nto 13 out of 396 materials in 100 articles on precipitate-containing\nmulti-principal element alloys (miss rate reduced from 12.4% to 3.3%). The\npipeline enables scalable and efficient literature mining, producing databases\nwith high precision, minimal omissions, and zero false positives. These\ndatasets provide trustworthy inputs for machine learning and materials\ninformatics, while the modular design generalizes to diverse material classes,\nenabling comprehensive materials information extraction.", "AI": {"tldr": "研究提出了一种多阶段信息提取管道，利用大型语言模型从实验报告的材料中精确提取多种特性，具备高精确率和可靠性的特征提取能力，并且能高效地从文献中挖掘数据，为机器学习和材料信息学提供了可靠的数据库。", "motivation": "数据驱动的材料发现需要大规模的实验数据集，但大多数信息仍然被困在非结构化的文献中。现有的提取工作往往只关注有限的特性，并没有解决复合-加工-微观结构-性能关系的综合问题，这对理解材料行为至关重要，因而对建立综合数据库构成了挑战。", "method": "我们提出了一种多阶段信息提取管道，由大型语言模型驱动，能够从实验报告的材料中独占捕获47个特性，这些特性涵盖了组成、处理、微观结构和属性。该管道通过迭代提取和源跟踪来提高准确性和可靠性。", "result": "在特征级别（独立属性）和元组级别（互依特性）的评估中，获得了接近0.96的F1分数。与没有源跟踪的单次抽取相比，我们方法在微观结构类别的F1分数提高了10.0%（特征级别）和13.7%（元组级别），并且在100篇关于含沉淀物的多元主元素合金的文章中，遗漏的材料从49个减少到13个（错漏率从12.4%降低到3.3%）。", "conclusion": "此管道支持大型文献信息的可扩展且高效的挖掘，产生了高精度、遗漏少且无错误正例的数据库。这些数据库为机器学习和材料信息学提供了可靠的输入，同时模块化设计适用于多种材料类别，实现了综合材料信息提取。"}}
{"id": "2510.05649", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05649", "abs": "https://arxiv.org/abs/2510.05649", "authors": ["Saja Al-Dabet", "Sherzod Turaev", "Nazar Zaki", "Arif O. Khan", "Luai Eldweik"], "title": "Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation", "comment": null, "summary": "Ocular-induced abnormal head posture (AHP) is a compensatory mechanism that\narises from ocular misalignment conditions, such as strabismus, enabling\npatients to reduce diplopia and preserve binocular vision. Early diagnosis\nminimizes morbidity and secondary complications such as facial asymmetry;\nhowever, current clinical assessments remain largely subjective and are further\ncomplicated by incomplete medical records. This study addresses both challenges\nthrough two complementary deep learning frameworks. First, AHP-CADNet is a\nmulti-level attention fusion framework for automated diagnosis that integrates\nocular landmarks, head pose features, and structured clinical attributes to\ngenerate interpretable predictions. Second, a curriculum learning-based\nimputation framework is designed to mitigate missing data by progressively\nleveraging structured variables and unstructured clinical notes to enhance\ndiagnostic robustness under realistic data conditions. Evaluation on the\nPoseGaze-AHP dataset demonstrates robust diagnostic performance. AHP-CADNet\nachieves 96.9-99.0 percent accuracy across classification tasks and low\nprediction errors for continuous variables, with MAE ranging from 0.103 to\n0.199 and R2 exceeding 0.93. The imputation framework maintains high accuracy\nacross all clinical variables (93.46-99.78 percent with PubMedBERT), with\nclinical dependency modeling yielding significant improvements (p < 0.001).\nThese findings confirm the effectiveness of both frameworks for automated\ndiagnosis and recovery from missing data in clinical settings.", "AI": {"tldr": "文章提出了两种深度学习框架来改善AHP的自动化诊断和处理数据缺失问题，实验结果表现出色，证实了其临床应用价值。", "motivation": "眼部引起的异常头部姿势（AHP）是眼球偏斜等眼部不正位状况的一种代偿机制，通过早期诊断最小化疾病严重性和相关并发症如面部不对称。然而，当前的临床评估仍然主要是主观的，不完整的医疗记录使得评估更加复杂。", "method": "提出了两个互补的深度学习框架：AHP-CADNet（用于自动化诊断的多级注意力融合框架）和基于课程学习的数据补全框架（用于减少数据缺失的影响）", "result": "在PoseGaze-AHP数据集上的评估显示了强大的诊断性能。AHP-CADNet在分类任务中取得了96.9-99.0%的准确率，并且对于连续变量的预测误差较低，MAE范围在0.103到0.199之间，R2值超过0.93。补全框架在所有临床变量中保持了93.46-99.78%的准确性，引入临床变量依赖模型带来了显著改进（p < 0.001）。", "conclusion": "这些研究结果证实了这两个框架在临床环境中自动诊断和数据缺失恢复方面的有效性。"}}
{"id": "2510.05144", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05144", "abs": "https://arxiv.org/abs/2510.05144", "authors": ["Muskaan Chopra", "Lorenz Sparrenberg", "Rafet Sifa"], "title": "SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation", "comment": null, "summary": "Critical Error Detection (CED) in machine translation aims to determine\nwhether a translation is safe to use or contains unacceptable deviations in\nmeaning. While the WMT21 English-German CED dataset provided the first\nbenchmark, it is limited in scale, label balance, domain coverage, and temporal\nfreshness. We present SynCED-EnDe, a new resource consisting of 1,000\ngold-labeled and 8,000 silver-labeled sentence pairs, balanced 50/50 between\nerror and non-error cases. SynCED-EnDe draws from diverse 2024-2025 sources\n(StackExchange, GOV.UK) and introduces explicit error subclasses, structured\ntrigger flags, and fine-grained auxiliary judgments (obviousness, severity,\nlocalization complexity, contextual dependency, adequacy deviation). These\nenrichments enable systematic analyses of error risk and intricacy beyond\nbinary detection. The dataset is permanently hosted on GitHub and Hugging Face,\naccompanied by documentation, annotation guidelines, and baseline scripts.\nBenchmark experiments with XLM-R and related encoders show substantial\nperformance gains over WMT21 due to balanced labels and refined annotations. We\nenvision SynCED-EnDe as a community resource to advance safe deployment of MT\nin information retrieval and conversational assistants, particularly in\nemerging contexts such as wearable AI devices.", "AI": {"tldr": "本文介绍了SynCED-EnDe数据集，解决了现有数据集的局限性，并通过实验验证了其在错误检测方面的优异性能。", "motivation": "动机在于现有的WMT21数据集在规模、标签平衡性、领域覆盖率和时效性上存在不足，新的数据集旨在解决这些问题并推进机器翻译的安全应用。", "method": "该研究提出了SynCED-EnDe，一个新的英语-德语翻译错误检测数据集，包含1000个黄金标签和8000个银标签的句子对，并引入了具体的错误子类、结构触发标志和细粒度辅助判断。", "result": "基准实验显示，通过XLM-R和其他相关编码器，新数据集在错误检测方面比WMT21数据集表现更好。", "conclusion": "SynCED-EnDe数据集旨在作为社区资源，推进机器翻译在信息检索和对话助手中的安全部署，特别是在可穿戴AI设备等新环境中的应用。"}}
{"id": "2510.05650", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.05650", "abs": "https://arxiv.org/abs/2510.05650", "authors": ["Yiping Ma", "Shiyu Hu", "Buyuan Zhu", "Yipei Wang", "Yaxuan Kang", "Shiqing Liu", "Kang Hao Cheong"], "title": "EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario", "comment": "Preprint, Under review", "summary": "Reproducing cognitive development, group interaction, and long-term evolution\nin virtual classrooms remains a core challenge for educational AI, as real\nclassrooms integrate open-ended cognition, dynamic social interaction,\naffective factors, and multi-session development rarely captured together.\nExisting approaches mostly focus on short-term or single-agent settings,\nlimiting systematic study of classroom complexity and cross-task reuse. We\npresent EduVerse, the first user-defined multi-agent simulation space that\nsupports environment, agent, and session customization. A distinctive\nhuman-in-the-loop interface further allows real users to join the space. Built\non a layered CIE (Cognition-Interaction-Evolution) architecture, EduVerse\nensures individual consistency, authentic interaction, and longitudinal\nadaptation in cognition, emotion, and behavior-reproducing realistic classroom\ndynamics with seamless human-agent integration. We validate EduVerse in\nmiddle-school Chinese classes across three text genres, environments, and\nmultiple sessions. Results show: (1) Instructional alignment: simulated IRF\nrates (0.28-0.64) closely match real classrooms (0.37-0.49), indicating\npedagogical realism; (2) Group interaction and role differentiation: network\ndensity (0.27-0.40) with about one-third of peer links realized, while\nhuman-agent tasks indicate a balance between individual variability and\ninstructional stability; (3) Cross-session evolution: the positive transition\nrate R+ increase by 11.7% on average, capturing longitudinal shifts in\nbehavior, emotion, and cognition and revealing structured learning\ntrajectories. Overall, EduVerse balances realism, reproducibility, and\ninterpretability, providing a scalable platform for educational AI. The system\nwill be open-sourced to foster cross-disciplinary research.", "AI": {"tldr": "EduVerse is a novel AI tool for simulating educational environments by integrating human and AI agents in realistic classroom settings, offering capabilities for studying cognitive, social, and evolutionary dynamics in education.", "motivation": "The motivation behind EduVerse is to address the core challenge in educational AI of reproducing the complexity of real classrooms, including cognitive development, group interaction, and long-term evolution, which existing methods often fail to capture comprehensively.", "method": "EduVerse, a user-defined multi-agent simulation space, is built on a layered CIE (Cognition-Interaction-Evolution) architecture, enabling the simulation of open-ended cognition, dynamic social interaction, affective factors, and multi-session development in a classroom environment. With a human-in-the-loop interface, real users can participate in the simulation and interact with AI agents.", "result": "EduVerse was validated in middle-school Chinese classes, demonstrating pedagogical realism with simulated IRF rates closely matching those of real classrooms, group interaction and role differentiation consistent with real classrooms, and positive transition rate R+ increases reflective of longitudinal student development.", "conclusion": "EduVerse offers a scalable platform for educational AI that balances realism, reproducibility, and interpretability, allowing for a more comprehensive study of classroom dynamics and fostering cross-disciplinary research through open-sourcing."}}
{"id": "2510.05148", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05148", "abs": "https://arxiv.org/abs/2510.05148", "authors": ["Qi Li", "Runpeng Yu", "Haiquan Lu", "Xinchao Wang"], "title": "Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs", "comment": null, "summary": "Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a\ncompetitive paradigm for non-autoregressive language modeling. Their\ndistinctive decoding mechanism enables faster inference speed and strong\nperformance in code generation and mathematical tasks. In this work, we show\nthat the decoding mechanism of dLLMs not only enhances model utility but also\ncan be used as a powerful tool for model attribution. A key challenge in this\nproblem lies in the diversity of attribution scenarios, including\ndistinguishing between different models as well as between different\ncheckpoints or backups of the same model. To ensure broad applicability, we\nidentify two fundamental problems: what information to extract from the\ndecoding trajectory, and how to utilize it effectively. We first observe that\nrelying directly on per-step model confidence yields poor performance. This is\nmainly due to the bidirectional decoding nature of dLLMs: each newly decoded\ntoken influences the confidence of other decoded tokens, making model\nconfidence highly redundant and washing out structural signal regarding\ndecoding order or dependencies. To overcome this, we propose a novel\ninformation extraction scheme called the Directed Decoding Map (DDM), which\ncaptures structural relationships between decoding steps and better reveals\nmodel-specific behaviors. Furthermore, to make full use of the extracted\nstructural information during attribution, we propose Gaussian-Trajectory\nAttribution (GTA), where we fit a cell-wise Gaussian distribution at each\ndecoding position for each target model, and define the likelihood of a\ntrajectory as the attribution score: if a trajectory exhibits higher\nlog-likelihood under the distribution of a specific model, it is more likely to\nhave been generated by that model. Extensive experiments under different\nsettings validate the utility of our methods.", "AI": {"tldr": "研究开发了 Directed Decoding Map (DDM) 以解决离散扩散大语言模型 (dLLMs) 在不同模型或同一模型不同检查点之间的归因问题，并提出 Gaussian-Trajectory Attribution (GTA) 方法来有效利用提取出的结构信息。", "motivation": "离散扩散大语言模型 (dLLMs) 由于其解码机制的独特性，能够提升模型在代码生成和数学任务上的性能。然而，在模型归属评价任务中，dLLMs 解码结果多样性的挑战显而易见，研究动机在于找到一种方式来有效提取和利用解码轨迹中的信息，以区分不同的模型实例。", "method": "研究首先提出 DDM 来捕捉解码步骤间的结构性关系，然后使用 GTA 在各个解码位置上为每个目标模型拟合一个单元格高斯分布，并将轨迹的似然性作为归因得分。", "result": "实验结果验证了所提方法的有效性，在多种设置下都获得了很好的表现。", "conclusion": "本文提出的 DDM 和 GTA 方法，有效解决了dLLM在归因任务上的挑战，通过捕捉和利用解码过程中的结构信息，提升了不同模型实例间的归因准确性。"}}
