<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.CV](#cs.CV) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EvalCards: A Framework for Standardized Evaluation Reporting](https://arxiv.org/abs/2511.21695)
*Ruchira Dhar,Danae Sanchez Villegas,Antonia Karamolegkou,Alice Schiavone,Yifei Yuan,Xinyi Chen,Jiaang Li,Stella Frank,Laura De Grazia,Monorama Swain,Stephanie Brandl,Daniel Hershcovich,Anders Søgaard,Desmond Elliott*

Main category: cs.CL

> The paper identifies three shortcomings in current NLP evaluation reporting practices and introduces Evaluation Disclosure Cards (EvalCards) to enhance transparency and meet governance requirements.

<details>
  <summary>Details</summary>

**Motivation:** The increasing release of open-access models in the NLP field has highlighted the need for more transparent evaluation reporting practices.

**Method:** The authors survey recent work on evaluation and documentation in NLP to identify and address shortcomings in current reporting practices.

**Result:** The authors introduce Evaluation Disclosure Cards (EvalCards) to enhance transparency in evaluation reporting for both researchers and practitioners.

**Conclusion:** EvalCards can serve as a practical foundation for meeting the emerging governance requirements in the NLP field.

**Abstract:** Evaluation has long been a central concern in NLP, and transparent reporting practices are more critical than ever in today's landscape of rapidly released open-access models. Drawing on a survey of recent work on evaluation and documentation, we identify three persistent shortcomings in current reporting practices: reproducibility, accessibility, and governance. We argue that existing standardization efforts remain insufficient and introduce Evaluation Disclosure Cards (EvalCards) as a path forward. EvalCards are designed to enhance transparency for both researchers and practitioners while providing a practical foundation to meet emerging governance requirements.

</details>


### [2] [Cacheback: Speculative Decoding With Nothing But Cache](https://arxiv.org/abs/2511.21699)
*Zhiyao Ma,In Gim,Lin Zhong*

Main category: cs.CL

> Cacheback Decoding是一种无训练需求的推测解码方法，利用LRU缓存表中的token n-grams生成草稿序列，尽管设计简单，但其性能处于同类方法的前沿。

<details>
  <summary>Details</summary>

**Motivation:** Cacheback Decoding旨在通过利用语言的局部性来加速大规模语言模型的推理过程。

**Method:** Cacheback Decoding采用了一种无需训练且模型无关的推测解码方法，利用LRU缓存表中的token n-grams来生成草稿序列。

**Result:** Cacheback Decoding展示了在同类方法中处于前沿的性能，并且能够快速适应新的领域。

**Conclusion:** Cacheback Decoding的简约设计为其提供了易于集成的优点，并在不牺牲性能的情况下打开了新的应用方向。

**Abstract:** We present Cacheback Decoding, a training-free and model-agnostic speculative decoding method that exploits the locality in language to accelerate Large Language Model (LLM) inference. Cacheback leverages only Least Recently Used (LRU) cache tables of token n-grams to generate draft sequences. Cacheback achieves state-of-the-art performance among comparable methods despite its minimalist design, and its simplicity allows easy integration into existing systems. Cacheback also shows potential for fast adaptation to new domains.

</details>


### [3] [JELV: A Judge of Edit-Level Validity for Evaluation and Automated Reference Expansion in Grammatical Error Correction](https://arxiv.org/abs/2511.21700)
*Yuhao Zhan,Yuqing Zhang,Jing Yuan,Qixiang Ma,Zhiqi Yang,Yu Gu,Zemin Liu,Fei Wu*

Main category: cs.CL

> 本文介绍了一种新的自动评估框架JELV，用于改进语法错误修正系统的评估和泛化能力，通过扩展参考样式和提升评估指标与人类判断的匹配度，实现了模型性能的整体提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有的语法错误修正系统受限于参考多样性不足，导致评估低估和模型泛化能力受限。为了改进评估并增强模型泛化，亟需一种新方法来解决这个问题。

**Method:** 我们提出了一种称为Judge of Edit-Level Validity (JELV)的自动化框架，用于验证语法错误修正的质量。该框架根据语法性、忠实性和流畅性标准来评估修正编辑的有效性。它有两种实现方式：一个使用多轮LLM-as-Judges流水线，与人类注释者有90%的一致性；另一个是蒸馏的DeBERTa分类器，对于有效编辑的精度为85%。

**Result:** 应用JELV重新分类评估中的误判假阳性，并通过整合假阳性解耦和流畅度评分，提出了一种综合评估指标。该指标与人类判断具有最高关联性，并通过筛选LLM生成的备选修正方案，扩展了数据集，提升GEC模型的性能。

**Conclusion:** JELV提供了一种可扩展的解决方案，能够增强参考多样性、改进评估质量和增强模型泛化能力。

**Abstract:** Existing Grammatical Error Correction (GEC) systems suffer from limited reference diversity, leading to underestimated evaluation and restricted model generalization. To address this issue, we introduce the Judge of Edit-Level Validity (JELV), an automated framework to validate correction edits from grammaticality, faithfulness, and fluency. Using our proposed human-annotated Pair-wise Edit-level Validity Dataset (PEVData) as benchmark, JELV offers two implementations: a multi-turn LLM-as-Judges pipeline achieving 90% agreement with human annotators, and a distilled DeBERTa classifier with 85% precision on valid edits. We then apply JELV to reclassify misjudged false positives in evaluation and derive a comprehensive evaluation metric by integrating false positive decoupling and fluency scoring, resulting in state-of-the-art correlation with human judgments. We also apply JELV to filter LLM-generated correction candidates, expanding the BEA19's single-reference dataset containing 38,692 source sentences. Retraining top GEC systems on this expanded dataset yields measurable performance gains. JELV provides a scalable solution for enhancing reference diversity and strengthening both evaluation and model generalization.

</details>


### [4] [47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations](https://arxiv.org/abs/2511.21701)
*Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song,Ziqian Bi*

Main category: cs.CL

> 本文对27个最先进的大语言模型在中文医疗考试题上的表现进行了全面评估，涵盖了七个医学专科，并区分了主治医师和高级医师的不同水平。评估数据显示不同模型的表现差异显著，最大的问题在于模型大小与其性能之间没有一致的关联。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型的发展，它们在医学领域的潜在应用引起了广泛关注。为了更好地理解这些模型在复杂医疗环境中的应用潜力，本文建立了一个评估框架。

**Method:** 使用一个包含2,800个精心策划的题目数据集，覆盖了七个医学专业，并根据主治医师和高级医师的不同水平设置了难度，对27个最先进的大语言模型进行了评估。

**Result:** 评估显示不同模型的表现差异显著，Mixtral-8x7B表现最佳，准确率达到了74.25%，而模型大小与其性能之间没有一致的关联。

**Conclusion:** 本研究提供了一些关键见解，说明了这些大型语言模型在医学教育和临床决策支持系统中的部署潜力，同时也强调了当前技术中的局限性。

**Abstract:** The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.

</details>


### [5] [CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2511.21702)
*Dong Liu,Yanxuan Yu,Ben Lengerich*

Main category: cs.CL

> CSV-Decode提出了一个针对大型语言模型解码瓶颈的新方法，通过构建子词汇表来实现快速解码，同时保证了解码的正确性和效率。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在推理过程中的计算瓶颈主要是由于针对大词汇表的昂贵输出层计算。

**Method:** CSV-Decode方法采用几何上限来为每个解码步骤构建小型子词汇表，以便在维护双重正确性保证的同时实现有效的稀疏计算。方法首先离线聚类词汇表嵌入，并使用质心加半径的界限来识别哪些标记可以安全地从计算中省略。

**Result:** 实验结果表明，与全词汇表解码相比，该方法实现了显著的速度提升，同时保证了分布保证和低回退率。

**Conclusion:** 该研究提供了一个完整的系统实现，包括稀疏GEMV内核、多GPU分片和CUDA图优化，并且实验验证了其有效性。

**Abstract:** Large language models face significant computational bottlenecks during inference due to the expensive output layer computation over large vocabularies. We present CSV-Decode, a novel approach that uses geometric upper bounds to construct small sub-vocabularies for each decoding step, enabling efficient sparse computation while maintaining dual correctness guarantees: exact top-$k$ certification and $\varepsilon$-certified softmax approximations. Our method clusters vocabulary embeddings offline and uses centroid-plus-radius bounds to identify which tokens can be safely omitted from computation. We provide a complete system implementation with sparse GEMV kernels, multi-GPU sharding, and CUDA Graph optimization. Experimental results demonstrate significant speedup over full vocabulary decoding while maintaining distributional guarantees and low fallback rates. Our code implementation available at \href{https://github.com/FastLM/CSV-Decode}{https://github.com/FastLM/CSV-Decode}.

</details>


### [6] [Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational Geometry](https://arxiv.org/abs/2511.21703)
*Siyaxolisa Kabane*

Main category: cs.CL

> 研究了LLM与非LLM编码器生成的密集文本嵌入的泛化性能差异，并探讨了SLERP模型融合在缓解过度专业化方面的作用。实验发现，基于LLM的模型在捕捉高阶组合模式方面表现更好，但有过度专业化问题；SLERP模型融合能恢复基础模型结构，提升了聚类性能和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 研究了当嵌入骨干模型为大语言模型（LLM）与非LLM编码器时，密集文本嵌入泛化性能的差异，并探讨了SLERP模型融合是否能减轻因任务特定适应而引入的过度专业化。

**Method:** 我们设计了一套受控实验，用于比较非LLM编码器和LLM编码器生成的密集文本嵌入在泛化性能上的差异，实验中特别关注了通过球形线性插值（SLERP）模型融合在减轻任务特定适应（如LoRA）导致的过度专业化方面的作用。实验涉及四种模型类别：(1) 从零开始训练或为嵌入进行微调的非LLM编码器；(2) 使用参数高效方法（LoRA）调整的LLM编码器；(3) 使用LoRA调整的LLM编码器，然后通过模型组合合并到基础权重中；(4) 同样使用LoRA调整的LLM，通过SLERP按检查点或阶段融合。我们用聚类指标（如轮廓系数和戴维斯-布尔德林指数）来评估表示质量，并用k-means标签来分析嵌入是否编码了超出测试范围的额外信息。

**Result:** 实验证实了基于LLM的骨干模型生成的嵌入更好捕捉到了高阶、组合的数值模式，但这些模型易受到适配器影响，导致平衡泛化能力下降；而SLERP融合可一致恢复基础模型结构，同时保持大部分任务收益，相较于模型组合或未融合的模型，在聚类可分离性和鲁棒性方面取得了更优的权衡。

**Conclusion:** SLERP融合对于LLM嵌入在保留任务收益的同时恢复基础模型结构非常有效，提高了模型的聚类可分离性和泛化鲁棒性。

**Abstract:** We investigate the generalization properties of dense text embeddings when the embedding backbone is a large language model (LLM) versus when it is a non-LLM encoder, and we study the extent to which spherical linear interpolation (SLERP) model-merging mitigates over-specialization introduced by task-specific adaptation (e.g., LoRA). To make the comparison concrete and domain-agnostic, we design a controlled suite of experiments in which models embed short numerical sequences and are evaluated on their ability to cluster and classify those sequences according to well-defined number-theoretic properties. Our experimental protocol compares four families of models: (1) non-LLM encoders trained from scratch or fine-tuned for embeddings, (2) LLM-based encoders adapted with parameter-efficient methods (LoRA), (3) LLM-based encoders with LoRA followed by model souping merging into the base weights, and (4) the same LoRA-adapted LLMs merged using SLERP across checkpoints or stages. We evaluate representational quality with clustering indices (Silhouette and Davies Bouldin). We additionally analyze the use of kmeans labels to see if the embeddings encode any other information besides the one we are testing for. Empirically, we find that LLM-based backbones produce embeddings that better capture higher-order, compositional numeric patterns, but are prone to adapter dominance that degrades balanced generalization; SLERP merging consistently recovers base-model structure while retaining most task gains, yielding superior tradeoffs in clustering separability, and robustness compared to model souping or models that were not merged.

</details>


### [7] [On the Cross-lingual Transferability of Pre-trained wav2vec2-based Models](https://arxiv.org/abs/2511.21704)
*Jonatas Grosman,Cassio Almeida,Guilherme Schardong,Hélio Lopes*

Main category: cs.CL

> 研究调查了基于wav2vec 2.0的预训练模型在跨语言任务中的表现，并发现影响最终性能的不仅仅是预训练数据的量，还有数据的多样性。此外，语言之间的相似性也影响着跨语言知识迁移的效果。

<details>
  <summary>Details</summary>

**Motivation:** 尽管已有研究指出，预训练数据对基于wav2vec 2.0的模型性能有影响，但很少有研究深入探讨这些预训练模型在不同语言间转移知识的能力。本研究旨在填补这一空白，了解如何最好地利用当前的预训练模型。

**Method:** Structure

**Result:** {
  "tldr": "研究调查了基于wav2vec 2.0的预训练模型在跨语言任务中的表现，并发现影响最终性能的不仅仅是预训练数据的量，还有数据的多样性。此外，语言之间的相似性也影响着跨语言知识迁移的效果。",
  "motivation": "尽管已有研究指出，预训练数据对基于wav2vec 2.0的模型性能有影响，但很少有研究深入探讨这些预训练模型在不同语言间转移知识的能力。本研究旨在填补这一空白，了解如何最好地利用当前的预训练模型。",
  "method": "研究者进行了一系列基于15个大型预训练模型的跨语言语音识别任务微调实验，共覆盖18种语言，分析了预训练数据大小和多样性对下游任务表现的影响。",
  "result": "实验结果表明，模型在处理印欧语系语言时表现优于非印欧语系语言。当预训练语言与下游任务语言相比较为相同时，跨语言知识迁移的效果更加显著。",
  "conclusion": "这些发现有助于科学界利用现有的基于wav2vec 2.0的预训练模型，并促进未来新模型的预训练工作，特别需要注意预训练数据的多样性和任务语言的相似性。")

**Conclusion:** 这些发现有助于科学界利用现有的基于wav2vec 2.0的预训练模型，并促进未来新模型的预训练工作，特别需要注意预训练数据的多样性和任务语言的相似性。

**Abstract:** Using representations provided by a large pre-trained model has become the primary strategy for achieving state-of-the-art results in a wide range of tasks. A recently proposed large pre-trained model, wav2vec 2.0, was seminal for several other works on pre-training large models on speech data. Many models are being pre-trained using the same architecture as wav2vec 2.0 and are getting state-of-the-art in various speech-related tasks. Previous work has demonstrated that the data used during the pre-training of these wav2vec2-based models can impact the model's performance in downstream tasks, and this should be taken into consideration before utilizing these models. However, few works have proposed investigating further how the transfer knowledge of these pre-trained models behaves in different languages, even when the target language differs from the one used during the model's pre-training. Our work aims to investigate the cross-lingual transferability of these wav2vec2-based models. We performed several fine-tuning experiments on the speech recognition task in 18 languages using 15 large pre-trained models. The results of our experiments showed us that the size of data used during the pre-training of these models is not as important to the final performance as the diversity. We noticed that the performance of Indo-European languages is superior to non-Indo-European languages in the evaluated models. We have observed a positive cross-lingual transfer of knowledge using monolingual models, which was evident in all the languages we used, but more pronounced when the language used during pre-training was more similar to the downstream task language. With these findings, we aim to assist the scientific community in utilizing existing wav2vec2-based pre-trained models, as well as facilitate the pre-training of new ones.

</details>


### [8] [Insight-A: Attribution-aware for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.21705)
*Junjie Wu,Yumeng Fu,Chen Gong,Guohong Fu*

Main category: cs.CL

> 文章提出了Insight-A来解决由AI生成内容带来的多模态虚假信息问题，通过跨属性提示和自动归因偏见消除提示的方法，实现了对伪造来源的归因，并提出了新的检测范式。

<details>
  <summary>Details</summary>

**Motivation:** 现有的标准提示方法使用多模态大语言模型（MLLMs）识别新兴的虚假信息，忽略了这种信息的来源归属。因此，提出了Insight-A，旨在通过MLLM见解探索归属，以检测多模式的虚假信息。

**Method:** 文中提出了Insight-A方法，该方法企图解决现有技术忽视驳斥信息来源的缺陷。它主要有两个方面：第一，利用跨属性提示（CAP）将虚假信息归因于伪造来源，通过学习感知和推理之间的复杂关联。第二，采用具有层级推理的有效流水线来检测横跨不同模态的失真，并使用自动归因偏见消除提示（ADP）减少人为标注提示的主观性。此外还设计了图像标注（IC）实现视觉细节的增加，以提高跨模态一致性检查。

**Result:** 实验结果展示了该提案的优越性，并为AIGC时代的跨模态虚假信息检测提供了新的范式。

**Conclusion:** 通过引入CAP和ADP以及图像标注技术，Insight-A方法能够有效检测多模态不良信息，在AIGC时代提出了新的解决范式。

**Abstract:** AI-generated content (AIGC) technology has emerged as a prevalent alternative to create multimodal misinformation on social media platforms, posing unprecedented threats to societal safety. However, standard prompting leverages multimodal large language models (MLLMs) to identify the emerging misinformation, which ignores the misinformation attribution. To this end, we present Insight-A, exploring attribution with MLLM insights for detecting multimodal misinformation. Insight-A makes two efforts: I) attribute misinformation to forgery sources, and II) an effective pipeline with hierarchical reasoning that detects distortions across modalities. Specifically, to attribute misinformation to forgery traces based on generation patterns, we devise cross-attribution prompting (CAP) to model the sophisticated correlations between perception and reasoning. Meanwhile, to reduce the subjectivity of human-annotated prompts, automatic attribution-debiased prompting (ADP) is used for task adaptation on MLLMs. Additionally, we design image captioning (IC) to achieve visual details for enhancing cross-modal consistency checking. Extensive experiments demonstrate the superiority of our proposal and provide a new paradigm for multimodal misinformation detection in the era of AIGC.

</details>


### [9] [A General Highly Accurate Online Planning Method Integrating Large Language Models into Nested Rollout Policy Adaptation for Dialogue Tasks](https://arxiv.org/abs/2511.21706)
*Hui Wang,Fafa Zhang,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.CL

> 本文提出了NRPA-GD方法，利用LLM模拟对话行为，通过嵌套蒙特卡洛仿真及策略自适应优化，在对话过程中动态调整策略，避免了特定模型训练。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于复杂提示工程的方法效果依赖于人工经验，而结合策略网络和预训练模型的方法则难以适应新对话场景且训练成本高昂。因此，提出NRPA-GD方法以解决这些问题。

**Method:** NRPA-GD方法利用大型语言模型（LLM）同时模拟用户和系统的对话行为，通过构建完整的评估机制和嵌套蒙特卡洛仿真及策略自适应优化框架，在对话过程中动态调整策略。此方法完全避免了特定模型的训练。

**Result:** 实验结果表明，NRPA-GD在四个典型的目标导向对话数据集上超越了现有的提示工程方法和特定预训练模型方法，甚至在仅使用一个6亿参数的LLM时超越了ChatGPT和预训练策略模型。

**Conclusion:** 研究进一步表明，利用规划方法在大型语言模型上解决实际规划任务的优势和新颖性。

**Abstract:** In goal-oriented dialogue tasks, the main challenge is to steer the interaction towards a given goal within a limited number of turns. Existing approaches either rely on elaborate prompt engineering, whose effectiveness is heavily dependent on human experience, or integrate policy networks and pre-trained policy models, which are usually difficult to adapt to new dialogue scenarios and costly to train. Therefore, in this paper, we present Nested Rollout Policy Adaptation for Goal-oriented Dialogue (NRPA-GD), a novel dialogue policy planning method that completely avoids specific model training by utilizing a Large Language Model (LLM) to simulate behaviors of user and system at the same time. Specifically, NRPA-GD constructs a complete evaluation mechanism for dialogue trajectories and employs an optimization framework of nested Monte Carlo simulation and policy self-adaptation to dynamically adjust policies during the dialogue process. The experimental results on four typical goal-oriented dialogue datasets show that NRPA-GD outperforms both existing prompt engineering and specifically pre-trained model-based methods. Impressively, NRPA-GD surpasses ChatGPT and pre-trained policy models with only a 0.6-billion-parameter LLM. The proposed approach further demonstrates the advantages and novelty of employing planning methods on LLMs to solve practical planning tasks.

</details>


### [10] [Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?](https://arxiv.org/abs/2511.21708)
*Matteo Spreafico,Ludovica Tassini,Camilla Sancricca,Cinzia Cappiello*

Main category: cs.CL

> 大型语言模型在数据准备任务中的表现进行了研究，与传统工具相比，使用了定制的质量模型进行评估。

<details>
  <summary>Details</summary>

**Motivation:** 探索大型语言模型在数据准备这类关键但往往劳动密集型任务中的能力。

**Method:** 使用了通用及针对表格数据微调的大型语言模型，通过对低质量数据集的操作来测量其数据配置文件和清理方面的表现。

**Result:** 通过定制的质量模型以及用户研究对比了大型语言模型与传统数据准备工具的能力。

**Conclusion:** 尚未明确得出结论，但通过此研究，我们获得了关于从业者期望的见解。

**Abstract:** Large language models have recently demonstrated their exceptional capabilities in supporting and automating various tasks. Among the tasks worth exploring for testing large language model capabilities, we considered data preparation, a critical yet often labor-intensive step in data-driven processes. This paper investigates whether large language models can effectively support users in selecting and automating data preparation tasks. To this aim, we considered both general-purpose and fine-tuned tabular large language models. We prompted these models with poor-quality datasets and measured their ability to perform tasks such as data profiling and cleaning. We also compare the support provided by large language models with that offered by traditional data preparation tools. To evaluate the capabilities of large language models, we developed a custom-designed quality model that has been validated through a user study to gain insights into practitioners' expectations.

</details>


### [11] [Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach](https://arxiv.org/abs/2511.21709)
*Blessed Guda,Lawrence Francis,Gabrial Zencha Ashungafac,Carlee Joe-Wong,Moise Busogi*

Main category: cs.CL

> 本文提出了一种新的无监督的置换偏差度量（PBM）方法和一种高效的多数投票方法（BaQCKV），并且使用LoRA-1进行微调来缓解MCQ任务中的选择偏差，实验表明这些方法提高了模型的一致性和准确性，同时降低了计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 现有选择偏差度量方法需要答案标签，而对于不同答案排列下模型预测的一致性捕捉不够；现有的偏置缓解策略如多数投票计算成本过于高昂，校准方法需要验证集且不具有良好的泛化能力。因此，本研究旨在填补这一空白，提供一种无监督、计算成本低且有效缓解选择偏差的方法。

**Method:** 此论文提出一种新的无监督且无需标签的置换偏差度量（PBM）方法，用于直接量化模型预测在不同答案排列下的不一致性，并提供了一种更精确的度量选择偏差的方式。此外，还提出了一种高效的多数投票方法，称为批处理问题上下文KV缓存（BaQCKV），它可以在显著降低计算成本的同时保持偏置缓解的有效性。最后，提出了基于LoRA-1的无监督微调策略，该策略基于所提出的度量和BaQCKV来缓解选择偏差。

**Result:** 实验显示，所提方法减少了偏置，提高了准确性的一致性，同时最小化了计算成本。

**Conclusion:** 研究提出了有效的方法来缓解MCQ评估框架中的选择偏差问题，这对于提升大型语言模型的评估可靠性具有重要意义。

**Abstract:** Multiple Choice Question (MCQ) answering is a widely used method for evaluating the performance of Large Language Models (LLMs). However, LLMs often exhibit selection bias in MCQ tasks, where their choices are influenced by factors like answer position or option symbols rather than the content. This bias undermines the reliability of MCQ as an evaluation framework. Most existing selection bias metrics require answer labels and measure divergences between prediction and answer distributions, but do not fully capture the consistency of a model's predictions across different orderings of answer choices. Existing selection bias mitigation strategies have notable limitations: majority voting, though effective, is computationally prohibitive; calibration-based methods require validation sets and often fail to generalize across datasets. To address these gaps, we propose three key contributions: (1) a new unsupervised label-free Permutation Bias Metric (PBM) that directly quantifies inconsistencies in model predictions across answer permutations, providing a more precise measure of selection bias, (2) an efficient majority voting approach called Batch Question-Context KV caching (BaQCKV), to significantly reduce computational costs while preserving bias mitigation effectiveness, and (3) an unsupervised Low-Rank Adaptation (LoRA-1) fine-tuning strategy based on our proposed metric and the BaQCKV that mitigates selection bias, providing a computationally efficient alternative that maintains model generalizability. Experiments across multiple MCQ benchmarks demonstrate that our approaches reduce bias, increasing consistency in accuracy while minimizing computational costs.

</details>


### [12] [Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation](https://arxiv.org/abs/2511.21711)
*Fatima Kazi*

Main category: cs.CL

> 研究分析了大型语言模型（如ChatGPT）的偏见问题，通过特定基准测试并采用多角度方法检测显性和隐性偏见，提出了一系列缓解策略，发现微调模型在处理某些类型偏见时表现优异，但仍然存在过度依赖关键词的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是在人工智能特别是大型语言模型快速发展背景下，探讨这些模型可能存在的社会、伦理、文化、宗教等偏见，并试图通过评价和缓解策略来减少这些偏见，确保输出更加公平准确。

**Method:** 该研究使用诸如StereoSet和CrowSPairs这样的偏见特定基准测试了包括BERT、GPT 3.5和ADA在内的多种生成模型，采用三管齐下的策略来检测偏见。

**Result:** 研究表明，微调的模型在处理种族偏见上优于处理性别偏见，并且在一些情况下表现出色，但仍存在依赖关键字的问题。另一方面，借助包含细调、不同提示方法和偏见基准数据增强在内的学习策略改善模型性能，显著提高了隐性偏见测试中的表现，提升最高可达20%。

**Conclusion:** 研究结论是有必要对人工智能尤其是大型语言模型中的偏见问题进行更加深入的审视，同时说明实施适当的缓解策略可以提高模型性能，减少偏见输出，但仍需注意模型理解准确性和真实性的局限性。

**Abstract:** Large Language models (LLMs), such as ChatGPT, have gained popularity in recent years with the advancement of Natural Language Processing (NLP), with use cases spanning many disciplines and daily lives as well. LLMs inherit explicit and implicit biases from the datasets they were trained on; these biases can include social, ethical, cultural, religious, and other prejudices and stereotypes. It is important to comprehensively examine such shortcomings by identifying the existence and extent of such biases, recognizing the origin, and attempting to mitigate such biased outputs to ensure fair outputs to reduce harmful stereotypes and misinformation. This study inspects and highlights the need to address biases in LLMs amid growing generative Artificial Intelligence (AI). We utilize bias-specific benchmarks such StereoSet and CrowSPairs to evaluate the existence of various biases in many different generative models such as BERT, GPT 3.5, and ADA. To detect both explicit and implicit biases, we adopt a three-pronged approach for thorough and inclusive analysis. Results indicate fine-tuned models struggle with gender biases but excel at identifying and avoiding racial biases. Our findings also illustrated that despite some cases of success, LLMs often over-rely on keywords in prompts and its outputs. This demonstrates the incapability of LLMs to attempt to truly understand the accuracy and authenticity of its outputs. Finally, in an attempt to bolster model performance, we applied an enhancement learning strategy involving fine-tuning, models using different prompting techniques, and data augmentation of the bias benchmarks. We found fine-tuned models to exhibit promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.

</details>


### [13] [EulerESG: Automating ESG Disclosure Analysis with LLMs](https://arxiv.org/abs/2511.21712)
*Yi Ding,Xushuo Tang,Zhengyi Yang,Wenqian Zhang,Simin Wu,Yuxin Huang,Lingjing Lan,Weiyuan Li,Yin Chen,Mingchen Ju,Wenke Yang,Thong Hoang,Mykhailo Klymenko,Xiwei Zu,Wenjie Zhang*

Main category: cs.CL

> EulerESG是一个LLM驱动的系统，用于自动化ESG披露分析，具有对ESG框架的明确意识。

<details>
  <summary>Details</summary>

**Motivation:** 现有的ESG报告分析工具要么依赖于脆弱的基于规则的提取，要么将ESG报告视为通用文本，没有明确建模底层报告标准。该项目旨在开发一个LLM驱动的系统，自动化ESG披露分析，同时明确考虑ESG框架。

**Method:** EulerESG结合双通道检索和基于LLM的披露分析，以及一个用于探索、基准测试和解释的交互式仪表板和聊天机器人。

**Result:** 通过四个全球知名企业和十二个SASB子行业的应用，EulerESG可以以高达0.95的平均准确率自动填充标准化的指标表，同时在端到端运行中保持实用。

**Conclusion:** EulerESG系统能够实现对ESG披露的高度准确的自动分析，并提供一个用户友好的交互式界面进行探索和验证。

**Abstract:** Environmental, Social, and Governance (ESG) reports have become central to how companies communicate climate risk, social impact, and governance practices, yet they are still published primarily as long, heterogeneous PDF documents. This makes it difficult to systematically answer seemingly simple questions. Existing tools either rely on brittle rule-based extraction or treat ESG reports as generic text, without explicitly modelling the underlying reporting standards. We present \textbf{EulerESG}, an LLM-powered system for automating ESG disclosure analysis with explicit awareness of ESG frameworks. EulerESG combines (i) dual-channel retrieval and LLM-driven disclosure analysis over ESG reports, and (ii) an interactive dashboard and chatbot for exploration, benchmarking, and explanation. Using four globally recognised companies and twelve SASB sub-industries, we show that EulerESG can automatically populate standard-aligned metric tables with high fidelity (up to 0.95 average accuracy) while remaining practical in end-to-end runtime, and we compare several recent LLM models in this setting. The full implementation, together with a demonstration video, is publicly available at https://github.com/UNSW-database/EulerESG.

</details>


### [14] [GPS: General Per-Sample Prompter](https://arxiv.org/abs/2511.21714)
*Pawel Batorski,Paul Swoboda*

Main category: cs.CL

> 研究提出了一种无需针对任何特定任务进行调整，也能生成定制化提示的通用方法GPS，显著提高了模型在不同任务上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 当前的自动提示方法在面对新任务时需要大量的训练数据集，依赖耗时的优化循环，并且通常产生单一的任务级别提示，不能随着个别输入问题的变化而自适应。因此，提出了GPS方法来克服这些限制。

**Method:** 提出了一种名为GPS的通用且针对每个样本的提示方法，该方法无需特定任务的调整，可以为每个未见过的输入生成定制的提示，从而提高不同任务的表现。该方法中的提示生成器通过强化学习在一系列训练任务上进行训练，并包含了新的正则化方法以有效地适应每个样本的提示。此外，采用了最小贝叶斯风险解码以稳定推断。

**Result:** 实验结果表明，在不针对特定任务进行训练的情况下，GPS在文本简化任务上获得了第二好的结果，在总结任务上获得了第三好的结果，并在分类任务上达到了与基线相同的结果。在同领域提示任务上的GSM8K上，GPS达到了最优结果。

**Conclusion:** 该研究展示了一种新颖且有效的自动提示范例的潜力：生成适应性强且针对输入具体的提示，同时不进行大量的优化且无需访问特定任务的训练集。

**Abstract:** LLMs are sensitive to prompting, with task performance often hinging on subtle, sometimes imperceptible variations in phrasing. As a result, crafting effective prompts manually remains challenging and time-consuming. Recent automatic prompting methods mitigate this difficulty but face three key limitations: (i) for each new task, they require large datasets to train good prompts;(ii) they rely on costly optimization loops that may take hours; (iii)they typically produce a single task-level prompt that does not adapt to the individual input problem to be solved.
  We propose GPS, the first general-purpose, per-sample prompting method. Without any task-specific tuning, GPS generates a tailored prompt for each unseen input, improving performance across diverse tasks. The prompter is trained with reinforcement learning on a suite of training tasks and includes a novel regularization for effectively adapting to per-sample prompting. Finally, we employ Minimum Bayes Risk decoding to stabilize inference.
  Empirically, GPS demonstrates competitive performance: we attain second best results among baselines on text simplification, third best results on summarization and on-par results on classification, while not training on any of these tasks, in contrast to the baselines. For in-domain prompting, we obtain sota on GSM8K. Our work shows the potential of a novel and effective paradigm for automatic prompting: generating adaptive, input-specific prompts without extensive optimization and without access to a task-specific training set. Our code is available at https://github.com/Batorskq/GPS.

</details>


### [15] [An Optimized Machine Learning Classifier for Detecting Fake Reviews Using Extracted Features](https://arxiv.org/abs/2511.21716)
*Shabbir Anees,Anshuman,Ayush Chaurasia,Prathmesh Bogar*

Main category: cs.CL

> A machine-learning-based method for identifying AI-generated online reviews, achieving high accuracy through advanced text processing, multi-modal feature extraction, Harris Hawks Optimization for feature selection, and ensemble learning, with emphasis on data privacy for cloud-based systems.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to address the issue of fraudulent reviews written by AI and appearing as if they were written by humans, which undermines the trust in online purchases.

**Method:** Our method integrates advanced text preprocessing, multi-modal feature extraction, Harris Hawks Optimization (HHO) for feature selection, and a stacking ensemble classifier.

**Result:** The system achieved 95.40% accuracy, 92.81% precision, 95.01% recall, and a 93.90% F1-Score on a dataset of 40,432 OR and CG reviews.

**Conclusion:** The combination of ensemble learning and bio-inspired optimization is a promising approach for recognizing AI-generated text in online reviews.

**Abstract:** It is well known that fraudulent reviews cast doubt on the legitimacy and dependability of online purchases. The most recent development that leads customers towards darkness is the appearance of human reviews in computer-generated (CG) ones. In this work, we present an advanced machine-learning-based system that analyses these reviews produced by AI with remarkable precision. Our method integrates advanced text preprocessing, multi-modal feature extraction, Harris Hawks Optimization (HHO) for feature selection, and a stacking ensemble classifier. We implemented this methodology on a public dataset of 40,432 Original (OR) and Computer-Generated (CG) reviews. From an initial set of 13,539 features, HHO selected the most applicable 1,368 features, achieving an 89.9% dimensionality reduction. Our final stacking model achieved 95.40% accuracy, 92.81% precision, 95.01% recall, and a 93.90% F1-Score, which demonstrates that the combination of ensemble learning and bio-inspired optimisation is an effective method for machine-generated text recognition. Because large-scale review analytics commonly run on cloud platforms, privacy-preserving techniques such as differential approaches and secure outsourcing are essential to protect user data in these systems.

</details>


### [16] [CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution](https://arxiv.org/abs/2511.21717)
*Baoliang Tian,Yuxuan Si,Jilong Wang,Lingyao Li,Zhongyuan Bao,Zineng Zhou,Tao Wang,Sixu Li,Ziyao Xu,Mingze Wang,Zhouzhuo Zhang,Zhihao Wang,Yike Yun,Ke Tian,Ning Yang,Minghui Qiu*

Main category: cs.CL

> 研究介绍了CrossCheck-Bench，这是一个用于评估多模态输入中的矛盾检测能力的基准。实验结果揭示了在多模态推理中持续存在的瓶颈，并为构建能够进行稳健的跨模态验证的模型提出了新方向。

<details>
  <summary>Details</summary>

**Motivation:** 多模态大型语言模型主要通过配对的图像-文本对进行训练和评估，这使得它们检测和解决现实世界的不一致性能力很大程度上未被探索。在开放域应用中，视觉和文本线索经常存在冲突，要求模型执行表面级别对齐之外的结构化推理。

**Method:** 我们介绍了一个名为CrossCheck-Bench的诊断基准，用于评估多模态输入中的矛盾检测能力。该基准涵盖了三个层次的推理复杂性，定义了七个解决跨模态不一致的基本能力。该数据集包含15000个来自真实世界材料的问题-答案对，其中包含合成注入的矛盾。

**Result:** 对13个最先进的视觉-语言模型的评估表明，任务从感知匹配转移到逻辑矛盾检测时，性能普遍下降。大多数模型在孤立的实体识别上表现良好，但在必须综合多个线索进行冲突推理时失败。采用符号推理与基于视觉处理相结合的方法可以实现更稳定改进。

**Conclusion:** 这些结果突出了多模态推理中的持续瓶颈，并建议了构建能够进行稳健跨模态验证模型的新方向。

**Abstract:** Multimodal Large Language Models are primarily trained and evaluated on aligned image-text pairs, which leaves their ability to detect and resolve real-world inconsistencies largely unexplored. In open-domain applications visual and textual cues often conflict, requiring models to perform structured reasoning beyond surface-level alignment. We introduce CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The benchmark adopts a hierarchical task framework covering three levels of reasoning complexity and defines seven atomic capabilities essential for resolving cross-modal inconsistencies. CrossCheck-Bench includes 15k question-answer pairs sourced from real-world artifacts with synthetically injected contradictions. The dataset is constructed through a multi-stage annotation pipeline involving more than 450 expert hours to ensure semantic validity and calibrated difficulty across perception, integration, and reasoning. We evaluate 13 state-of-the-art vision-language models and observe a consistent performance drop as tasks shift from perceptual matching to logical contradiction detection. Most models perform well on isolated entity recognition but fail when multiple clues must be synthesized for conflict reasoning. Capability-level analysis further reveals uneven skill acquisition, especially in tasks requiring multi-step inference or rule-based validation. Additional probing shows that conventional prompting strategies such as Chain-of-Thought and Set-of-Mark yield only marginal gains. By contrast, methods that interleave symbolic reasoning with grounded visual processing achieve more stable improvements. These results highlight a persistent bottleneck in multimodal reasoning and suggest new directions for building models capable of robust cross-modal verification.

</details>


### [17] [When Harmless Words Harm: A New Threat to LLM Safety via Conceptual Triggers](https://arxiv.org/abs/2511.21718)
*Zhaoxin Zhang,Borui Chen,Yiming Hu,Youyang Qu,Tianqing Zhu,Longxiang Gao*

Main category: cs.CL

> 本文介绍了MICM，一种新的越狱方法，它可以在不触发安全过滤器的情况下，引导大语言模型的输出朝向特定的价值立场，实验结果表明这种新方法在成功率及减少拒绝率上表现出优越性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大语言模型（LLM）越狱研究主要集中在绕过安全机制以生成明显有害输出的技术上，往往忽视了利用模型的抽象概括能力来进行攻击的方法，这种方法可以通过微妙地操作嵌入在模型输出中的隐式社会价值观来进行，从而诱导出令人反感的内容。这种盲点使得当前的对齐策略存在漏洞。

**Method:** 本文提出了一种新型的模型无关的越狱方法MICM，该方法针对大语言模型（LLM）在输出中反映出的总体价值结构。MICM基于概念形态理论，通过固定提示模板中的预定义短语集合编码特定的精细概念配置，这些短语作为概念触发器，引导模型输出朝向特定的价值立场，同时不触发传统安全过滤器。

**Result:** 实验结果显示MICM在五个先进的LLM上均表现出高成功率和低拒率，甚至还包括了GPT-4o、Deepseek-R1 和 Qwen3-8B。这表明商业LLM的安全机制对潜在的价值对齐仍可能存在漏洞。

**Conclusion:** 研究揭示了商业大语言模型中一个关键的脆弱性：其安全机制仍然容易受到潜在的价值对齐操作的影响。这表明需要重新考量当前的安全过滤器策略。

**Abstract:** Recent research on large language model (LLM) jailbreaks has primarily focused on techniques that bypass safety mechanisms to elicit overtly harmful outputs. However, such efforts often overlook attacks that exploit the model's capacity for abstract generalization, creating a critical blind spot in current alignment strategies. This gap enables adversaries to induce objectionable content by subtly manipulating the implicit social values embedded in model outputs. In this paper, we introduce MICM, a novel, model-agnostic jailbreak method that targets the aggregate value structure reflected in LLM responses. Drawing on conceptual morphology theory, MICM encodes specific configurations of nuanced concepts into a fixed prompt template through a predefined set of phrases. These phrases act as conceptual triggers, steering model outputs toward a specific value stance without triggering conventional safety filters. We evaluate MICM across five advanced LLMs, including GPT-4o, Deepseek-R1, and Qwen3-8B. Experimental results show that MICM consistently outperforms state-of-the-art jailbreak techniques, achieving high success rates with minimal rejection. Our findings reveal a critical vulnerability in commercial LLMs: their safety mechanisms remain susceptible to covert manipulation of underlying value alignment.

</details>


### [18] [PeerCoPilot: A Language Model-Powered Assistant for Behavioral Health Organizations](https://arxiv.org/abs/2511.21721)
*Gao Mo,Naveen Raman,Megan Chai,Cindy Peng,Shannon Pagdon,Nev Jones,Hong Shen,Peggy Swarbrick,Fei Fang*

Main category: cs.CL

> PeerCoPilot是一款利用大型语言模型（LLM）的助手，设计用于帮助同行心理健康服务提供者改善服务。它展现了比基线LLM更加可靠且具体的信息提供能力，且得到了同行工作者们的积极反馈。

<details>
  <summary>Details</summary>

**Motivation:** 同行健康组织面临着资金和人员的限制，难以满足所有使用者的需求。因此，我们引入了PeerCoPilot来帮助同行健康组织的服务提供者处理日常任务。

**Method:** PeerCoPilot采用了一个检索增强生成管道，该管道基于包含超过1,300个经过审查资源的大型数据库，旨在帮助同行提供者创建保健计划，构建逐步目标，并定位以支持这些目标的组织资源。

**Result:** 进行了15位同行提供者和6位服务对象的人类评估，结果有超过90%的用户支持使用PeerCoPilot。此外，PeerCoPilot提供了比基线语言模型更可靠、更具体的信息。PeerCoPilot目前已经由CSPNJ内的一组5-10位同行提供者在使用，并且我们正在积极扩大PeerCoPilot的使用范围。

**Conclusion:** PeerCoPilot是一个有效的人工智能助手，它能在同行心理健康服务提供者的工作中起到关键作用，帮助他们更好地服务用户，计划保健，同时也证明了其在提供可靠和具体信息方面的优越性。

**Abstract:** Behavioral health conditions, which include mental health and substance use disorders, are the leading disease burden in the United States. Peer-run behavioral health organizations (PROs) critically assist individuals facing these conditions by combining mental health services with assistance for needs such as income, employment, and housing. However, limited funds and staffing make it difficult for PROs to address all service user needs. To assist peer providers at PROs with their day-to-day tasks, we introduce PeerCoPilot, a large language model (LLM)-powered assistant that helps peer providers create wellness plans, construct step-by-step goals, and locate organizational resources to support these goals. PeerCoPilot ensures information reliability through a retrieval-augmented generation pipeline backed by a large database of over 1,300 vetted resources. We conducted human evaluations with 15 peer providers and 6 service users and found that over 90% of users supported using PeerCoPilot. Moreover, we demonstrated that PeerCoPilot provides more reliable and specific information than a baseline LLM. PeerCoPilot is now used by a group of 5-10 peer providers at CSPNJ, a large behavioral health organization serving over 10,000 service users, and we are actively expanding PeerCoPilot's use.

</details>


### [19] [German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies](https://arxiv.org/abs/2511.21722)
*Jens Rupprecht,Leon Fröhling,Claudia Wagner,Markus Strohmaier*

Main category: cs.CL

> 本文介绍了GGP集合，这是一个基于ALLBUS调查的全面且具有代表性的德语人格提示集合。证明了使用GGP引导的LLMs在模拟人口反应方面优于当前最先进的分类器，特别是在数据稀缺的情况下。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的经过精心策划、实证支持的人格集合稀少，限制了此类模拟的准确性和代表性，因此作者开发了GGP以增强LLMs在模拟人类观点时的准确性。

**Method:** 本文介绍了德国通用形象集（GGP）的创建，该集基于德国通用社会调查（ALLBUS），旨在为大型语言模型（LLMs）提供模拟德国人口反应的人格提示。

**Result:** 研究结果表明，在各种话题上，使用GGP引导的LLMs可以更好地模拟调查响应分布，尤其是在数据稀缺的情况下。此外，GGP对于研究基于LLM的社会模拟中与人口反应一致的人格提示选择提供了有价值的资源。

**Conclusion:** GGP提供了一个有价值的资源，用于基于LLM的社会模拟研究，促进了与人口反应一致的人格提示选择的系统性探索。

**Abstract:** The use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). The GGP and its persona prompts are designed to be easily plugged into prompts for all types of LLMs and tasks, steering models to generate responses aligned with the underlying German population. We evaluate GGP by prompting various LLMs to simulate survey response distributions across diverse topics, demonstrating that GGP-guided LLMs outperform state-of-the-art classifiers, particularly under data scarcity. Furthermore, we analyze how the representativity and attribute selection within persona prompts affect alignment with population responses. Our findings suggest that GGP provides a potentially valuable resource for research on LLM-based social simulations that enables more systematic explorations of population-aligned persona prompting in NLP and social science research.

</details>


### [20] [AD-CDO: A Lightweight Ontology for Representing Eligibility Criteria in Alzheimer's Disease Clinical Trials](https://arxiv.org/abs/2511.21724)
*Zenan Sun,Rashmie Abeysinghe,Xiaojin Li,Xinyue Hu,Licong Cui,Guo-Qiang Zhang,Jiang Bian,Cui Tao*

Main category: cs.CL

> 该研究介绍了用于阿尔茨海默病临床试验的AD-CDO语义丰富型轻量级本体，通过提取高频概念并标准化关键的合格标准概念，达到63%的概念覆盖率，且具有良好的实用性。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是为解决阿尔茨海默病临床试验中关键合格标准概念的表示和标准化问题。

**Method:** 从超过1,500个阿尔茨海默病临床试验中提取高频概念并将其组织成7个语义类别，采用Jenks自然断点法来平衡覆盖率和可控性。

**Result:** 优化后的AD-CDO实现了对提取的试验概念63%以上的覆盖率，并在两个实际用例中展示了其实用性。

**Conclusion:** AD-CDO提供了一个与标准词汇表对齐的灵活基础，用于支持阿尔茨海默病临床试验的本体驱动研究。

**Abstract:** Objective
  This study introduces the Alzheimer's Disease Common Data Element Ontology for Clinical Trials (AD-CDO), a lightweight, semantically enriched ontology designed to represent and standardize key eligibility criteria concepts in Alzheimer's disease (AD) clinical trials.
  Materials and Methods
  We extracted high-frequency concepts from more than 1,500 AD clinical trials on ClinicalTrials.gov and organized them into seven semantic categories: Disease, Medication, Diagnostic Test, Procedure, Social Determinants of Health, Rating Criteria, and Fertility. Each concept was annotated with standard biomedical vocabularies, including the UMLS, OMOP Standardized Vocabularies, DrugBank, NDC, and NLM VSAC value sets. To balance coverage and manageability, we applied the Jenks Natural Breaks method to identify an optimal set of representative concepts.
  Results
  The optimized AD-CDO achieved over 63% coverage of extracted trial concepts while maintaining interpretability and compactness. The ontology effectively captured the most frequent and clinically meaningful entities used in AD eligibility criteria. We demonstrated AD-CDO's practical utility through two use cases: (a) an ontology-driven trial simulation system for formal modeling and virtual execution of clinical trials, and (b) an entity normalization task mapping raw clinical text to ontology-aligned terms, enabling consistency and integration with EHR data.
  Discussion
  AD-CDO bridges the gap between broad biomedical ontologies and task-specific trial modeling needs. It supports multiple downstream applications, including phenotyping algorithm development, cohort identification, and structured data integration.
  Conclusion
  By harmonizing essential eligibility entities and aligning them with standardized vocabularies, AD-CDO provides a versatile foundation for ontology-driven AD clinical trial research.

</details>


### [21] [PromptTailor: Multi-turn Intent-Aligned Prompt Synthesis for Lightweight LLMs](https://arxiv.org/abs/2511.21725)
*Yizhou Xu,Janet Davis*

Main category: cs.CL

> PromptTailor系统提升了轻量级语言模型在开放生成任务中的性能，通过扩展用户指令并保持用户意图，该系统在精确性和效率方面均表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 轻量级语言模型由于其在设备上运行和保护隐私方面的吸引力而备受关注，但它们的输出质量高度依赖于提示的质量。非专家用户通常缺乏持续创建高质量提示的知识或时间，这让他们依赖于提示优化工具。然而，确保优化后的提示真正符合用户的原始意图和偏好是一个关键挑战。

**Method:** 引入了PromptTailor系统，该系统用于生成可控的开放文本提示，通过意图对齐的提示合成来提高模型输出的质量。PromptTailor将用户的最小指令扩展为丰富的领域相关提示，同时保持用户所述的偏好。该系统是一个量化的Llama3-8B模型，使用轻量级LoRA适配器在12,300个提示优化对话上进行了微调，这些对话跨越了41个日常领域，从三个更强的LLM中提炼而来。该适配器可以附加到任何Llama3-8B基础模型上，使得可以在边缘部署。

**Result:** 在人类和LLM评估者对多个目标模型和优化基线进行的评估中，PromptTailor的偏好率高于思路链提示方法，并且与最先进的提示优化方法持平或超越，同时需要较少的模型调用（例如，3次而不是9次）。

**Conclusion:** 这些结果显示，一个简洁的学生模型，在强大的教师指导下，可以学会有效的提示生成策略，这既提高了响应的质量，又保持了与用户意图的一致性。

**Abstract:** Lightweight language models remain attractive for on-device and privacy-sensitive applications, but their responses are highly sensitive to prompt quality. For open-ended generation, non-expert users often lack the knowledge or time to consistently craft high-quality prompts, leading them to rely on prompt optimization tools. However, a key challenge is ensuring the optimized prompts genuinely align with users' original intents and preferences. We introduce PromptTailor, a system for controllable prompt generation for open-ended text that improves model output quality by intent-aligned prompt synthesis. PromptTailor expands minimal user instructions into rich, domain-aware prompts while preserving the user's stated preferences. The system is a quantized Llama3-8B model fine-tuned with a lightweight LoRA adapter on 12,300 prompt-refinement dialogues spanning 41 everyday domains, distilled from three stronger LLMs. The adapter attaches to any Llama3-8B base, enabling edge deployment. In human and LLM-judge evaluations across multiple target models and optimization baselines, PromptTailor yields higher preference rates than chain-of-thought prompting and matches or surpasses state-of-the-art prompt optimization methods while requiring fewer model calls (e.g., 3 vs. 9). These results show that a compact student, guided by powerful teachers, can learn effective prompt-generation strategies that enhance response quality while maintaining alignment with user intent.

</details>


### [22] [Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks](https://arxiv.org/abs/2511.21726)
*Yicong Zheng,Kevin L. McKee,Thomas Miconi,Zacharie Bugaud,Mick van Gelderen,Jed McCaleb*

Main category: cs.CL

> 本研究证明了直接应用于原始数据的简单搜索方法比当前长期上下文记忆任务中的目标无关和带偏见的压缩算法更优。

<details>
  <summary>Details</summary>

**Motivation:** 现有的记忆框架和基准更多关注优化记忆压缩算法以提高特定任务的性能，这导致了人类偏见的增加，而本研究着眼于开发一种通用解决方案，以应对不同的数据分布。

**Method:** 本研究提出了SUMER（通过经验回放进行未压缩记忆搜索）方法，这是一种具有可验证奖励的端到端的强化学习代理，它学习使用搜索工具来收集信息并回答目标问题。

**Result:** 在LoCoMo数据集上，采用Qwen2.5-7B-Instruct的SUMER方法优于所有其他有偏见的记忆压缩方法和完整的上下文基准，性能提高了43%，达到了最先进的水平。

**Conclusion:** 研究结果表明，在处理长期上下文记忆任务时，直接搜索原始数据而非压缩数据的方法表现更优，这支持了动态和自主可扩展的新范式和基准的发展。

**Abstract:** How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.

</details>


### [23] [Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue](https://arxiv.org/abs/2511.21728)
*Lin Yu,Xiaofei Han,Yifei Kang,Chiung-Yi Tseng,Danyang Zhang,Ziqian Bi,Zhimo Han*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.

</details>


### [24] [Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems](https://arxiv.org/abs/2511.21729)
*Jithin Krishnan*

Main category: cs.CL

> 研究表明将各种增强技术协同整合在检索增强生成系统中可以大幅度减少放弃率，而不增加幻觉率。这是通过理解组件间的相互作用以及引入标准化的指标和标签来实现的。

<details>
  <summary>Details</summary>

**Motivation:** 构建一个可靠的检索增强生成(RAG)系统需要理解各组件是如何交互的，而不仅仅是添加强大的组件。

**Method:** 通过50个查询（15个可回答的，10个边缘案例，25个对抗性的）的消融研究，探讨了增强功能如混合检索、集成验证和自适应阈值设置的效果。

**Result:** 单独使用这些增强功能几乎没有任何好处，但它们一起使用可以将放弃率减少95%（从40%减少到2%），而不增加幻觉率。识别出衡量挑战：不同的验证策略可能会行为安全但分配不一致的标签（例如，“放弃”与“不支持”），造成实际上是标签关联幻觉率的假象。

**Conclusion:** 研究结果表明，协同整合比任何单一组件的强度更重要，标准化的指标和标签是正确解释性能所必需的，即使检索质量很高，也需要适应性校准来防止过于自信的过度回答。

**Abstract:** Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, "abstained" versus "unsupported"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.

</details>


### [25] [A Benchmark for Procedural Memory Retrieval in Language Agents](https://arxiv.org/abs/2511.21730)
*Ishant Kohar,Aswanth Krishnan*

Main category: cs.CL

> 我们提出了一个专门测试程序性记忆检索能力的基准，结果表明基于嵌入的方法在熟悉上下文中表现良好，但在新颖上下文中表现下降显著，而LLM生成的程序性抽象在不同上下文中展示了可靠的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的AI代理在熟悉的设定中表现出色，但在面对具有未见过词汇表的新任务时表现急剧下降，这是程序性记忆系统的一个核心限制。

**Method:** 我们使用ALFWorld构建了两个专家轨迹和LLM生成轨迹的数据集，并评估了六种检索方法，采用了系统分层的查询策略。

**Result:** 实验暴露了一个清晰的泛化悬崖，算法嵌入方法在熟悉环境中的表现良好，但在新环境中的表现显著下降，而LLM生成的程序性抽象则展示了可靠跨上下文传输的能力。

**Conclusion:** 当前研究提供了一个诊断框架，分离出真实的程序理解与表面级别的记忆，并提供了发展可靠通用化的检索系统的工具。

**Abstract:** Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).

</details>


### [26] [Identifying Quantum Structure in AI Language: Evidence for Evolutionary Convergence of Human and Artificial Cognition](https://arxiv.org/abs/2511.21731)
*Diederik Aerts,Jonito Aerts Arguëlles,Lester Beltran,Suzette Geriente,Roberto Leporini,Massimiliano Sassoli de Bianchi,Sandro Sozzo*

Main category: cs.CL

> 研究发现大型语言模型（如ChatGPT和Gemini）在概念组合的认知测试中表现出类量子特性和玻色-爱因斯坦统计。这些特征在人类认知测试和信息检索中也有类似发现，表明可能存在认知-语言领域量子结构的系统性出现。研究提出了一个统一框架来解释这种量子意义组织现象，并推测语言模型的认知和语言可能正在快速地与人类的生物进化认知和语言趋同。

<details>
  <summary>Details</summary>

**Motivation:** 探究大型语言模型（LLMs）中的认知过程与人类认知过程之间的关系，以及二者都表现出的量子物理特性，旨在理解复杂概念表达背后的潜在机制。

**Method:** 通过让ChatGPT和Gemini这两种大型语言模型参与概念组合的认知测试，发现并分析测试结果是否违反贝尔不等式，以及其词汇分布是否遵循玻色-爱因斯坦统计。

**Result:** 结果显示LLMs在认知测试中的表现违反了贝尔不等式，显现出“量子纠缠”现象；并且词汇分布呈现玻色-爱因斯坦统计特性，暗示了量子结构在认知-语言领域的系统性出现。

**Conclusion:** 论文提出人类和LLMs在认知和语言上的趋同，推测量子结构在人类和LLMs的认知与语言领域普遍存在，并提出了一个统一框架解释这种趋同现象。

**Abstract:** We present the results of cognitive tests on conceptual combinations, performed using specific Large Language Models (LLMs) as test subjects. In the first test, performed with ChatGPT and Gemini, we show that Bell's inequalities are significantly violated, which indicates the presence of 'quantum entanglement' in the tested concepts. In the second test, also performed using ChatGPT and Gemini, we instead identify the presence of 'Bose-Einstein statistics', rather than the intuitively expected 'Maxwell-Boltzmann statistics', in the distribution of the words contained in large-size texts. Interestingly, these findings mirror the results previously obtained in both cognitive tests with human participants and information retrieval tests on large corpora. Taken together, they point to the 'systematic emergence of quantum structures in conceptual-linguistic domains', regardless of whether the cognitive agent is human or artificial. Although LLMs are classified as neural networks for historical reasons, we believe that a more essential form of knowledge organization takes place in the distributive semantic structure of vector spaces built on top of the neural network. It is this meaning-bearing structure that lends itself to a phenomenon of evolutionary convergence between human cognition and language, slowly established through biological evolution, and LLM cognition and language, emerging much more rapidly as a result of self-learning and training. We analyze various aspects and examples that contain evidence supporting the above hypothesis. We also advance a unifying framework that explains the pervasive quantum organization of meaning that we identify.

</details>


### [27] [HUMORCHAIN: Theory-Guided Multi-Stage Reasoning for Interpretable Multimodal Humor Generation](https://arxiv.org/abs/2511.21732)
*Jiajun Zhang,Shijia Luo,Ruikang Zhang,Qi Su*

Main category: cs.CL

> 本文提出了HUMORCHAIN框架，通过将幽默理论中的认知结构嵌入到多模态幽默生成过程中，解决了现有数据驱动方法在生成幽默图像描述时出现的问题。实验表明，该方法在人类幽默偏好和语义多样性方面超过了现有的最佳基线方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的数据驱动方法在生成幽默图像描述时，往往缺乏对幽默背后认知机制的捕捉，导致生成的描述虽然流畅但缺乏真正幽默或认知深度。

**Method:** HUMORCHAIN是一个基于理论的多阶段推理框架，它结合了视觉语义解析、基于幽默和心理学的推理以及一个微调的幽默评估鉴别器，形成一个可解释和可控的认知推理链。

**Result:** 实验在Meme-Image-No-Text、Oogiri-GO和OxfordTVG-HIC数据集上表明，HUMORCHAIN相比于状态-of-the-art基线方法，在人类幽默偏好、Elo/BT评分和语义多样性方面表现出色。

**Conclusion:** 理论驱动的结构化推理可以使大型语言模型生成符合人类感知的幽默，本研究是首个将幽默理论中的认知结构显式嵌入到多模态幽默生成中的工作。

**Abstract:** Humor, as both a creative human activity and a social binding mechanism, has long posed a major challenge for AI generation. Although producing humor requires complex cognitive reasoning and social understanding, theories of humor suggest that it follows learnable patterns and structures, making it theoretically possible for generative models to acquire them implicitly. In recent years, multimodal humor has become a prevalent form of online communication, especially among Gen Z, highlighting the need for AI systems capable of integrating visual understanding with humorous language generation. However, existing data-driven approaches lack explicit modeling or theoretical grounding of humor, often producing literal descriptions that fail to capture its underlying cognitive mechanisms, resulting in the generated image descriptions that are fluent but lack genuine humor or cognitive depth. To address this limitation, we propose HUMORCHAIN (HUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioning), a theory-guided multi-stage reasoning framework. It integrates visual semantic parsing, humor- and psychology-based reasoning, and a fine-tuned discriminator for humor evaluation, forming an interpretable and controllable cognitive reasoning chain. To the best of our knowledge, this is the first work to explicitly embed cognitive structures from humor theories into multimodal humor generation, enabling a structured reasoning process from visual understanding to humor creation. Experiments on Meme-Image-No-Text, Oogiri-GO, and OxfordTVG-HIC datasets show that HUMORCHAIN outperforms state-of-the-art baselines in human humor preference, Elo/BT scores, and semantic diversity, demonstrating that theory-driven structured reasoning enables large language models to generate humor aligned with human perception.

</details>


### [28] [RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models](https://arxiv.org/abs/2511.21733)
*Dayan Pan,Jingyuan Wang,Yilong Zhou,Jiawei Cheng,Pengyue Jia,Xiangyu Zhao*

Main category: cs.CL

> Introduces RoSA, a novel PEFT method, focusing on targeted adaptation for more efficient model fine-tuning, especially concerning the influence of RoPE on model performance.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of existing PEFT methods by adapting parameters in a targeted manner based on the roles of model components and the heterogeneous importance across layers, specifically focusing on the impact of Rotary Position Embeddings (RoPE).

**Method:** Parameter-Efficient Fine-Tuning (PEFT) method named RoPE-aware Selective Adaptation (RoSA), which includes a RoPE-aware Attention Enhancement (RoAE) module and a Dynamic Layer Selection (DLS) strategy.

**Result:** RoSA outperforms existing mainstream PEFT methods on fifteen commonsense and arithmetic benchmarks with comparable trainable parameters.

**Conclusion:** RoSA achieves more targeted and efficient fine-tuning by combining dimension-wise enhancement with layer-wise adaptation.

**Abstract:** Fine-tuning large language models is essential for task-specific adaptation, yet it remains computationally prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a solution, but current approaches typically ignore the distinct roles of model components and the heterogeneous importance across layers, thereby limiting adaptation efficiency. Motivated by the observation that Rotary Position Embeddings (RoPE) induce critical activations in the low-frequency dimensions of attention states, we propose RoPE-aware Selective Adaptation (RoSA), a novel PEFT framework that allocates trainable parameters in a more targeted and effective manner. RoSA comprises a RoPE-aware Attention Enhancement (RoAE) module, which selectively enhances the low-frequency components of RoPE-influenced attention states, and a Dynamic Layer Selection (DLS) strategy that adaptively identifies and updates the most critical layers based on LayerNorm gradient norms. By combining dimension-wise enhancement with layer-wise adaptation, RoSA achieves more targeted and efficient fine-tuning. Extensive experiments on fifteen commonsense and arithmetic benchmarks demonstrate that RoSA outperforms existing mainstream PEFT methods under comparable trainable parameters. The code is available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/RoSA.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [29] [SO-Bench: A Structural Output Evaluation of Multimodal LLMs](https://arxiv.org/abs/2511.21750)
*Di Feng,Kaixin Ma,Feng Nan,Haofeng Chen,Bohan Zhai,David Griffiths,Mingfei Gao,Zhe Gan,Eshan Verma,Yinfei Yang,Zhifeng Chen,Afshin Dehghan*

Main category: cs.CV

> 提出了SO-Bench基准测试系统，用于评估多模态大型语言模型在视觉领域中结构化输出的能力，并展示了模型在该领域存在的不足以及通过训练改善这些能力的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在文本领域中结构化生成方面取得了近期进步，但仍然是一个多模态大型语言模型在实际应用中需要其输出不仅要正确，还要符合预定义的数据模式，而当前没有系统地评估模式导向的信息提取和基于视觉输入的推理能力的基准。

**Method:** 通过设计一个名为SO-Bench的全面基准测试，研究多模态大型语言模型在视觉结构化输出方面的能力。该基准测试涵盖了四个视觉领域，包括UI界面、自然图像、文档和图表，基于超过6500个多样化JSON模式和1800个精心策划的图像-模式对，并进行了人工质量验证。

**Result:** 在开源模型和前沿专有模型上的基准测试实验表明，在预测准确且符合模式的输出方面存在持久的差距，突显了对更好多模态结构化推理的需求。进一步的训练实验显著改善了模型的结构化输出能力。

**Conclusion:** 研究表明当前的多模态大型语言模型在多模态结构化推理方面存在不足，特别是在预测准确且符合模式的输出方面。提出的研究计划将基准测试提供给社区。

**Abstract:** Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.

</details>


### [30] [Saddle-Free Guidance: Improved On-Manifold Sampling without Labels or Additional Training](https://arxiv.org/abs/2511.21863)
*Eric Yeats,Darryl Hannan,Wilson Fearn,Timothy Doster,Henry Kvinge,Scott Mahan*

Main category: cs.CV

> SFG uses positive curvature in saddle regions for guiding score-based models, achieving superior results in image generation without requiring additional training or labeled data.

<details>
  <summary>Details</summary>

**Motivation:** The primary motivation for SFG is to address the limitations of existing guidance methods, such as the need for labeled data or additional model training. SFG aims to provide a simpler and more efficient alternative that can be applied widely.

**Method:** The paper introduces Saddle-Free Guidance (SFG), a technique that leverages the positive curvature of log density estimates in saddle regions to guide score-based generative models. This method maintains an estimate of maximal positive curvature to guide the generation process, avoiding the need for additional model training or labeled data.

**Result:** SFG was found to achieve state-of-the-art FID and FD-DINOv2 metrics in unconditional ImageNet-512 generation. When combined with Auto-Guidance, SFG improved the overall performance in FD-DINOv2 score. Additionally, SFG increased the diversity of output images while maintaining high quality.

**Conclusion:** The experiments show that SFG is an effective and computationally efficient method for guiding score-based models. It outperforms traditional methods in terms of image diversity and quality without the need for extra training or labeled data, marking a notable advancement in the field.

**Abstract:** Score-based generative models require guidance in order to generate plausible, on-manifold samples. The most popular guidance method, Classifier-Free Guidance (CFG), is only applicable in settings with labeled data and requires training an additional unconditional score-based model. More recently, Auto-Guidance adopts a smaller, less capable version of the original model to guide generation. While each method effectively promotes the fidelity of generated data, each requires labeled data or the training of additional models, making it challenging to guide score-based models when (labeled) training data are not available or training new models is not feasible.
  We make the surprising discovery that the positive curvature of log density estimates in saddle regions provides strong guidance for score-based models. Motivated by this, we develop saddle-free guidance (SFG) which maintains estimates of maximal positive curvature of the log density to guide individual score-based models. SFG has the same computational cost of classifier-free guidance, does not require additional training, and works with off-the-shelf diffusion and flow matching models. Our experiments indicate that SFG achieves state-of-the-art FID and FD-DINOv2 metrics in single-model unconditional ImageNet-512 generation. When SFG is combined with Auto-Guidance, its unconditional samples achieve general state-of-the-art in FD-DINOv2 score. Our experiments with FLUX.1-dev and Stable Diffusion v3.5 indicate that SFG boosts the diversity of output images compared to CFG while maintaining excellent prompt adherence and image fidelity.

</details>


### [31] [UniArt: Unified 3D Representation for Generating 3D Articulated Objects with Open-Set Articulation](https://arxiv.org/abs/2511.21887)
*Bu Jin,Weize Li,Songen Gu,Yupeng Zheng,Yuhang Zheng,Zhengyi Zhou,Yao Yao*

Main category: cs.CV

> UniArt提供了一种端到端的方式，直接从图像生成高质量的关节3D模型。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于人工构造关节3D模型成本高昂且难以扩缩，UniArt旨在提高这种资产生成的效率和质量。

**Method:** UniArt采用扩散模型直接从单张图像生成完整的3D对象，引入了一种可逆的关节到体素嵌入方法，实现了关节特征与几何结构的空间对齐，并解决了开放集关节类型预测问题。

**Result:** 在PartNet-Mobility基准测试上，UniArt展示了领先的网格质量和关节准确性。

**Conclusion:** UniArt作为一种一体化的扩散模型框架，实现了高效、高质量的3D关节物体的生成，并具备对新关节类别的泛化能力。

**Abstract:** Articulated 3D objects play a vital role in realistic simulation and embodied robotics, yet manually constructing such assets remains costly and difficult to scale. In this paper, we present UniArt, a diffusion-based framework that directly synthesizes fully articulated 3D objects from a single image in an end-to-end manner. Unlike prior multi-stage techniques, UniArt establishes a unified latent representation that jointly encodes geometry, texture, part segmentation, and kinematic parameters. We introduce a reversible joint-to-voxel embedding, which spatially aligns articulation features with volumetric geometry, enabling the model to learn coherent motion behaviors alongside structural formation. Furthermore, we formulate articulation type prediction as an open-set problem, removing the need for fixed joint semantics and allowing generalization to novel joint categories and unseen object types. Experiments on the PartNet-Mobility benchmark demonstrate that UniArt achieves state-of-the-art mesh quality and articulation accuracy.

</details>


### [32] [PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images](https://arxiv.org/abs/2511.21902)
*Kunpeng Zhang,Hanwen Xu,Sheng Wang*

Main category: cs.CV

> 提出了名为PathReasoning的多模态推理代理，通过多轮推理和改进，在WSIs中高效地定位诊断相关区域，优于其他ROI选择方法，并且生成的报告比标准GPT-4o更准确。

<details>
  <summary>Details</summary>

**Motivation:** 全文图像（WSIs）在癌症诊断、预后和治疗反应方面起着关键作用，但由于其像素数量庞大，解码肿瘤微环境具有挑战性且耗时。

**Method:** PathReasoning通过模拟病理学家在WSIs中进行导航的方法，进行了多轮推理和改进。从随机采样的候选区域开始，审查当前选择，推理视觉观察与临床问题的相关性，并建议新的探索区域，逐步引导注意力到诊断相关的区域。

**Result:** PathReasoning在子类划分和纵向分析任务中的AUROC值分别比其他ROI选择方法高出6.7%和3.1%，并且生成的报告准确性比标准GPT-4o高出10%。

**Conclusion:** PathReasoning可以有效定位问题相关的区域，并构建可解释的推理链，支持有效的幻灯片审查、连贯的诊断解释、全面的报告和数字病理中的证据追踪。

**Abstract:** Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed "PathReasoning", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.

</details>


### [33] [Adaptive Parameter Optimization for Robust Remote Photoplethysmography](https://arxiv.org/abs/2511.21903)
*Cecilia G. Morales,Fanurs Chi En Teh,Kai Li,Pushpak Agrawal,Artur Dubrawski*

Main category: cs.CV

> The paper presents a training-free PRISM algorithm that optimizes photometric detrending and color mixing adaptively, achieving state-of-the-art performance in unsupervised rPPG methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the adaptability of rPPG methods to diverse deployment environments by overcoming the limitation of fixed parameters optimized for specific conditions.

**Method:** This paper introduces the PRISM algorithm, which uses online parameter adaptation based on signal quality assessment to jointly optimize photometric detrending and color mixing. It is a training-free method.

**Result:** PRISM achieves state-of-the-art performance among unsupervised methods with MAE of 0.77 bpm on PURE, 0.66 bpm on UBFC-rPPG, and accuracy of 97.3% and 97.5% respectively at a 5 bpm threshold, performing equivalently to leading supervised methods.

**Conclusion:** The conclusion is that adaptive time series optimization significantly improves rPPG performance across diverse conditions without requiring training.

**Abstract:** Remote photoplethysmography (rPPG) enables contactless vital sign monitoring using standard RGB cameras. However, existing methods rely on fixed parameters optimized for particular lighting conditions and camera setups, limiting adaptability to diverse deployment environments. This paper introduces the Projection-based Robust Signal Mixing (PRISM) algorithm, a training-free method that jointly optimizes photometric detrending and color mixing through online parameter adaptation based on signal quality assessment. PRISM achieves state-of-the-art performance among unsupervised methods, with MAE of 0.77 bpm on PURE and 0.66 bpm on UBFC-rPPG, and accuracy of 97.3\% and 97.5\% respectively at a 5 bpm threshold. Statistical analysis confirms PRISM performs equivalently to leading supervised methods ($p > 0.2$), while maintaining real-time CPU performance without training. This validates that adaptive time series optimization significantly improves rPPG across diverse conditions.

</details>


### [34] [Interpretable Multimodal Cancer Prototyping with Whole Slide Images and Incompletely Paired Genomics](https://arxiv.org/abs/2511.21937)
*Yupei Zhang,Yating Huang,Wanming Hu,Lequan Yu,Hujun Yin,Chao Li*

Main category: cs.CV

> 本文提出了一种灵活的多模态原型框架，用于整合全切片图像和不完整的基因组数据，以提升精准肿瘤学的效果。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于表型和基因型异质性限制了单模态表示的质量，使得有效的多模态整合变得困难，且大多数现有方法忽略了在现实的临床场景中基因组数据可能缺失的问题，本文提出了相应的解决方案。

**Method:** 方法包括：1）通过文本提示和原型加权的生物原型化；2）通过样本和分布对齐的多视角对齐；3）双图融合，捕捉模态共享信息和模态特定信息；4）语义基因组填补，处理缺失数据。

**Result:** 实验显示，相比于其他最先进的方法，本文提出的方法在多个下游任务中表现出了持续的优势。

**Conclusion:** 该研究促进了精准肿瘤学的发展，其代码可以公开获取。

**Abstract:** Multimodal approaches that integrate histology and genomics hold strong potential for precision oncology. However, phenotypic and genotypic heterogeneity limits the quality of intra-modal representations and hinders effective inter-modal integration. Furthermore, most existing methods overlook real-world clinical scenarios where genomics may be partially missing or entirely unavailable. We propose a flexible multimodal prototyping framework to integrate whole slide images and incomplete genomics for precision oncology. Our approach has four key components: 1) Biological Prototyping using text prompting and prototype-wise weighting; 2) Multiview Alignment through sample- and distribution-wise alignments; 3) Bipartite Fusion to capture both shared and modality-specific information for multimodal fusion; and 4) Semantic Genomics Imputation to handle missing data. Extensive experiments demonstrate the consistent superiority of the proposed method compared to other state-of-the-art approaches on multiple downstream tasks. The code is available at https://github.com/helenypzhang/Interpretable-Multimodal-Prototyping.

</details>


### [35] [AmodalGen3D: Generative Amodal 3D Object Reconstruction from Sparse Unposed Views](https://arxiv.org/abs/2511.21945)
*Junwei Zhou,Yu-Wing Tai*

Main category: cs.CV

> AmodalGen3D, a novel 3D reconstruction framework, uses advanced generative techniques to infer complete object geometry and appearance from sparse inputs, showing superior performance in highly occluded scenes.

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenge of reconstructing complete and geometrically coherent 3D objects from sparse, unposed, and partially occluded views, which is common in real-world scenarios but difficult for existing methods.

**Method:** We introduce AmodalGen3D, a generative framework for 3D object reconstruction from sparse, unposed, and partially occluded views. It uses 2D amodal completion priors with multi-view stereo geometry conditioning, featuring a View-Wise Cross Attention mechanism for feature fusion and a Stereo-Conditioned Cross Attention module for inferring unobserved structures.

**Result:** Experiments show AmodalGen3D achieves higher fidelity and completeness in reconstructing objects under heavy occlusion conditions compared to traditional methods.

**Conclusion:** AmodalGen3D addresses the critical need for robust object-level 3D reconstruction techniques, especially in robotics, AR/VR, and embodied AI applications.

**Abstract:** Reconstructing 3D objects from a few unposed and partially occluded views is a common yet challenging problem in real-world scenarios, where many object surfaces are never directly observed. Traditional multi-view or inpainting-based approaches struggle under such conditions, often yielding incomplete or geometrically inconsistent reconstructions. We introduce AmodalGen3D, a generative framework for amodal 3D object reconstruction that infers complete, occlusion-free geometry and appearance from arbitrary sparse inputs. The model integrates 2D amodal completion priors with multi-view stereo geometry conditioning, supported by a View-Wise Cross Attention mechanism for sparse-view feature fusion and a Stereo-Conditioned Cross Attention module for unobserved structure inference. By jointly modeling visible and hidden regions, AmodalGen3D faithfully reconstructs 3D objects that are consistent with sparse-view constraints while plausibly hallucinating unseen parts. Experiments on both synthetic and real-world datasets demonstrate that AmodalGen3D achieves superior fidelity and completeness under occlusion-heavy sparse-view settings, addressing a pressing need for object-level 3D scene reconstruction in robotics, AR/VR, and embodied AI applications.

</details>


### [36] [TAPVid-360: Tracking Any Point in 360 from Narrow Field of View Video](https://arxiv.org/abs/2511.21946)
*Finlay G. C. Hudson,James A. D. Gardner,William A. P. Smith*

Main category: cs.CV

> The paper introduces TAPVid-360, a novel task that focuses on predicting the 3D direction of scene points across video sequences, even when out of the field of view. It presents a new dataset, TAPVid360-10k, and a baseline method that shows improvements over existing approaches in the TAP and TAPVid 3D tasks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the capability of artificial vision systems to maintain panoramic understanding and track objects persistently across scenes, rather than processing frames egocentrically as is currently common.

**Method:** The paper exploits 360-degree videos to create a dataset for supervision, allowing the system to train on predicting the 3D direction of points queried in a scene, even when these points are out of the narrow field of view of the videos. The baseline method adapts CoTracker v3 to predict per-point rotations for updating the direction.

**Result:** The introduced TAPVid-360 task and dataset improve upon existing Track Any Point (TAP) and TAPVid methods by providing a way to predict 3D directions continuously across video sequences, overcoming the limitation of tracking only within the field of view.

**Conclusion:** The paper demonstrates the effectiveness of the TAPVid-360 task and the associated dataset in fostering allocentric scene understanding capabilities in artificial vision systems, representing a significant improvement over current methods that struggle with tracking points outside of the field of view.

**Abstract:** Humans excel at constructing panoramic mental models of their surroundings, maintaining object permanence and inferring scene structure beyond visible regions. In contrast, current artificial vision systems struggle with persistent, panoramic understanding, often processing scenes egocentrically on a frame-by-frame basis. This limitation is pronounced in the Track Any Point (TAP) task, where existing methods fail to track 2D points outside the field of view. To address this, we introduce TAPVid-360, a novel task that requires predicting the 3D direction to queried scene points across a video sequence, even when far outside the narrow field of view of the observed video. This task fosters learning allocentric scene representations without needing dynamic 4D ground truth scene models for training. Instead, we exploit 360 videos as a source of supervision, resampling them into narrow field-of-view perspectives while computing ground truth directions by tracking points across the full panorama using a 2D pipeline. We introduce a new dataset and benchmark, TAPVid360-10k comprising 10k perspective videos with ground truth directional point tracking. Our baseline adapts CoTracker v3 to predict per-point rotations for direction updates, outperforming existing TAP and TAPVid 3D methods.

</details>


### [37] [WalkCLIP: Multimodal Learning for Urban Walkability Prediction](https://arxiv.org/abs/2511.21947)
*Shilong Xiang,JangHyeon Lee,Min Namgung,Yao-Yi Chiang*

Main category: cs.CV

> WalkCLIP是一种多模态框架，结合了卫星数据、街景图像和人口动力学信息来评估城市步行性，优于单一数据源的方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的步行性评估依赖于调查和实地考察，成本高昂且难以扩展。单一的数据源无法全面反映步行环境。因此，提出了WalkCLIP以综合多种数据源进行步行性的预测。

**Method:** 介绍了WalkCLIP，这是一个多模态框架，它结合了卫星数据、街景图像和人口动力学数据来评估城市步行性。WalkCLIP通过GPT-4o生成的图像描述学习视觉-语言表示，并利用空间聚合模块融合周围环境信息，最后结合人口动力学基础模型来预测步行性。

**Result:** 在Minneapolis-Saint Paul的4,660个地点测试，WalkCLIP在预测精确度和空间匹配度上超越了单一模式和多模式的基础模型。

**Conclusion:** 研究结果表明，结合视觉和行为信号可以可靠地预测步行环境。

**Abstract:** Urban walkability is a cornerstone of public health, sustainability, and quality of life. Traditional walkability assessments rely on surveys and field audits, which are costly and difficult to scale. Recent studies have used satellite imagery, street view imagery, or population indicators to estimate walkability, but these single-source approaches capture only one dimension of the walking environment. Satellite data describe the built environment from above, but overlook the pedestrian perspective. Street view imagery captures conditions at the ground level, but lacks broader spatial context. Population dynamics reveal patterns of human activity but not the visual form of the environment. We introduce WalkCLIP, a multimodal framework that integrates these complementary viewpoints to predict urban walkability. WalkCLIP learns walkability-aware vision-language representations from GPT-4o generated image captions, refines these representations with a spatial aggregation module that incorporates neighborhood context, and fuses the resulting features with representations from a population dynamics foundation model. Evaluated at 4,660 locations throughout Minneapolis-Saint Paul, WalkCLIP outperforms unimodal and multimodal baselines in both predictive accuracy and spatial alignment. These results show that the integration of visual and behavioral signals yields reliable predictions of the walking environment.

</details>


### [38] [DeepGI: Explainable Deep Learning for Gastrointestinal Image Classification](https://arxiv.org/abs/2511.21959)
*Walid Houmaidi,Mohamed Hadadi,Youssef Sabiri,Yousra Chtouki*

Main category: cs.CV

> 本篇论文通过深入分析一种新型胃肠医学影像数据集（包含4,000张内窥镜图像，涵盖四个重要疾病类别：憩室、肿瘤、腹膜炎和输尿管疾病），利用先进的深度学习技术来应对内窥镜成像中的挑战，并通过Grad-CAM可视化解释AI模型的预测。整体实验结果表明：VGG16和MobileNetV2模型在测试中均达到96.5%的准确率，而Xception则达到94.24%，揭示了在复杂真实场景下的准确和可解释性的医疗图像分析潜能。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在应对内窥镜成像中的常见挑战，如变化的照明条件、不稳定的相机角度和频繁的成像缺陷等问题，并通过高级深度学习技术提供基准和比较见解，以推进胃肠计算机辅助诊断技术。

**Method:** 论文采用多种深度学习模型进行比较分析，包括VGG16、MobileNetV2和Xception，同时使用Grad-CAM技术来提供模型预测的可解释性。

**Result:** 结果表明，VGG16和MobileNetV2模型在测试中均达到了96.5%的准确率，Xception模型的准确率为94.24%，表明这些模型在处理复杂的真实内窥镜图像时具有较高的准确性和可靠性。

**Conclusion:** 本研究为胃肠疾病的自动分类提供了强有力的基准和基准模型，并通过Grad-CAM可视化技术提高了模型预测的可解释性，强调了多样性、临床相关数据集和模型解释在医疗AI研究中的重要性。

**Abstract:** This paper presents a comprehensive comparative model analysis on a novel gastrointestinal medical imaging dataset, comprised of 4,000 endoscopic images spanning four critical disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging state-of-the-art deep learning techniques, the study confronts common endoscopic challenges such as variable lighting, fluctuating camera angles, and frequent imaging artifacts. The best performing models, VGG16 and MobileNetV2, each achieved a test accuracy of 96.5%, while Xception reached 94.24%, establishing robust benchmarks and baselines for automated disease classification. In addition to strong classification performance, the approach includes explainable AI via Grad-CAM visualization, enabling identification of image regions most influential to model predictions and enhancing clinical interpretability. Experimental results demonstrate the potential for robust, accurate, and interpretable medical image analysis even in complex real-world conditions. This work contributes original benchmarks, comparative insights, and visual explanations, advancing the landscape of gastrointestinal computer-aided diagnosis and underscoring the importance of diverse, clinically relevant datasets and model explainability in medical AI research.

</details>


### [39] [PAT3D: Physics-Augmented Text-to-3D Scene Generation](https://arxiv.org/abs/2511.21978)
*Guying Lin,Kemeng Huang,Michael Liu,Ruihan Gao,Hanke Chen,Lyuhao Chen,Beijia Lu,Taku Komura,Yuan Liu,Jun-Yan Zhu,Minchen Li*

Main category: cs.CV

> PAT3D是一个将文本转换为物理上合理且无交叠的高质量3D场景的框架，该框架利用视觉-语言模型和物理模拟来实现。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是创建一个能够生成既在物理上合理又与输入文本在语义上一致的高质量3D场景的框架，以用于场景编辑和机器人操纵等下游任务。

**Method:** 我们介绍了一种名为PAT3D的框架，它是第一个将视觉-语言模型与物理基础模拟集成的文本到3D场景生成框架。PAT3D可以生成3D物体，推断它们的空间关系，并组织成一个层次化的场景树，然后再转换为模拟的初始条件。通过使用可微的刚体模拟器保证了在重力作用下物体之间的相互作用，使场景达到无交叠的静态平衡。为了进一步提高场景质量，我们引入了一个循环模拟优化过程，保证了物理稳定性和无交叠情况，同时提高了与输入提示的语义一致性。

**Result:** 实验表明，PAT3D在物理合理性，语义一致性以及视觉质量方面显着优于现有的方法。

**Conclusion:** PAT3D不仅生成高质量的3D场景，而且还能够生成可供下游任务使用的可模拟3D场景，比如场景编辑和机器人操作。

**Abstract:** We introduce PAT3D, the first physics-augmented text-to-3D scene generation framework that integrates vision-language models with physics-based simulation to produce physically plausible, simulation-ready, and intersection-free 3D scenes. Given a text prompt, PAT3D generates 3D objects, infers their spatial relations, and organizes them into a hierarchical scene tree, which is then converted into initial conditions for simulation. A differentiable rigid-body simulator ensures realistic object interactions under gravity, driving the scene toward static equilibrium without interpenetrations. To further enhance scene quality, we introduce a simulation-in-the-loop optimization procedure that guarantees physical stability and non-intersection, while improving semantic consistency with the input prompt. Experiments demonstrate that PAT3D substantially outperforms prior approaches in physical plausibility, semantic consistency, and visual quality. Beyond high-quality generation, PAT3D uniquely enables simulation-ready 3D scenes for downstream tasks such as scene editing and robotic manipulation. Code and data will be released upon acceptance.

</details>


### [40] [DialBench: Towards Accurate Reading Recognition of Pointer Meter using Large Foundation Models](https://arxiv.org/abs/2511.21982)
*Futian Wang,Chaoliu Weng,Xiao Wang,Zhen Chen,Zhicheng Zhao,Jin Tang*

Main category: cs.CV

> 本论文提出了一项新的大规模指针表盘读数基准数据集RPM-10K，以及一种基于物理关系注入的视觉语言模型MRLM，旨在解决指针表盘读数识别的关键挑战。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有方法在解决反光、遮挡、动态视角和指针与刻度标记过于接近等问题时较为脆弱，且缺乏大规模数据集支持，本论文旨在创建一个新数据集并提出一种新型模型以解决这些问题。

**Method:** 本论文提出了一种新颖的视觉语言模型MRLM，该模型基于物理关系注入，显式地编码指针与刻度之间的几何和因果关系，通过交叉注意力融合和自适应专家选择来解释刻度盘配置并生成精确的数值读数。

**Result:** 通过广泛的实验验证了所提出的框架在新基准数据集上的有效性。

**Conclusion:** 研究结果表明，所提出的模型能够有效识别指针表盘读数，并提升了智能电网系统中的准确性和可靠性。为了促进进一步研究，数据集和源代码将公开发布。

**Abstract:** The precise reading recognition of pointer meters plays a key role in smart power systems, but existing approaches remain fragile due to challenges like reflections, occlusions, dynamic viewing angles, and overly between thin pointers and scale markings. Up to now, this area still lacks large-scale datasets to support the development of robust algorithms. To address these challenges, this paper first presents a new large-scale benchmark dataset for dial reading, termed RPM-10K, which contains 10730 meter images that fully reflect the aforementioned key challenges. Built upon the dataset, we propose a novel vision-language model for pointer meter reading recognition, termed MRLM, based on physical relation injection. Instead of exhaustively learning image-level correlations, MRLM explicitly encodes the geometric and causal relationships between the pointer and the scale, aligning perception with physical reasoning in the spirit of world-model perspectives. Through cross-attentional fusion and adaptive expert selection, the model learns to interpret dial configurations and generate precise numeric readings. Extensive experiments fully validated the effectiveness of our proposed framework on the newly proposed benchmark dataset. Both the dataset and source code will be released on https://github.com/Event-AHU/DialBench

</details>


### [41] [PPBoost: Progressive Prompt Boosting for Text-Driven Medical Image Segmentation](https://arxiv.org/abs/2511.21984)
*Xuchen Li,Hengrui Gu,Mohan Zhang,Qin Liu,Zhen Tan,Xinyuan Zhu,Huixue Zhou,Tianlong Chen,Kaixiong Zhou*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Text-prompted foundation models for medical image segmentation offer an intuitive way to delineate anatomical structures from natural language queries, but their predictions often lack spatial precision and degrade under domain shift. In contrast, visual-prompted models achieve strong segmentation performance across diverse modalities by leveraging spatial cues of precise bounding-box (bbox) prompts to guide the segmentation of target lesions. However, it is costly and challenging to obtain the precise visual prompts in clinical practice. We propose PPBoost (Progressive Prompt-Boosting), a framework that bridges these limitations by transforming weak text-derived signals into strong, spatially grounded visual prompts, operating under a strict zero-shot regime with no image- or pixel-level segmentation labels. PPBoost first uses a vision-language model to produce initial pseudo-bboxes conditioned on the textual object descriptions and applies an uncertainty-aware criterion to filter unreliable predictions. The retained image-bboxes pairs are then leveraged to train a pseudo-labeled detector, producing the high-quality bboxes for the query images. During inference, PPBoost further refines the generated bboxes by appropriately expanding them to tightly cover the target anatomical structures. The enhanced spatially-grounding bbox prompts guide existing segmentation models to generate final dense masks, effectively amplifying weak text cues into strong spatial guidance. Across three datasets spanning diverse modalities and anatomies, PPBoost consistently improves Dice and Normalized Surface Distance over text- and visual-prompted baselines and, notably, surpasses few-shot segmentation models without using labeled data. PPBoost can generalize to multiple typical visual segmentation model backbones.

</details>


### [42] [Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?](https://arxiv.org/abs/2511.21998)
*Apratim Bhattacharyya,Bicheng Xu,Sanjay Haresh,Reza Pourreza,Litian Liu,Sunny Panchal,Pulkit Madan,Leonid Sigal,Roland Memisevic*

Main category: cs.CV

> 本文提出了Qualcomm Interactive Cooking基准和数据集，旨在评估和开发能够提供实时交互指导的多模态大型语言模型，并引入了LiveMamba作为这种模型的实例。

<details>
  <summary>Details</summary>

**Motivation:** 为了开发未来能够提供实时、互动的分步指导的AI助手，当前的多模态大型语言模型需要在检测指令的成功执行及识别和警告用户错误方面实现突破，因此需要能够异步反应视频流变化的模型。

**Method:** 介绍了Qualcomm Interactive Cooking基准和数据集，该数据集基于CaptainCook4D构建，包含了用户在执行任务时的错误。该数据集和基准包含密集注释的定时指令和反馈消息，特别是错误警报精确地与视频中的视觉事件时间戳对齐。还引入了针对交互式指导的实时多模态LLM——LiveMamba。

**Result:** 评估了现有的多模态大语言模型在Qualcomm Interactive Cooking基准上的表现，并展示了LiveMamba在实时互动指导上的优势。

**Conclusion:** 本文首次提供了一个专注于开发和评估实时情景指导下教练体的基准，并且引入了LiveMamba作为这类模型的基准。

**Abstract:** Multi-modal Large Language Models (LLM) have advanced conversational abilities but struggle with providing live, interactive step-by-step guidance, a key capability for future AI assistants. Effective guidance requires not only delivering instructions but also detecting their successful execution, as well as identifying and alerting users to mistakes, all of which has to happen in real-time. This requires models that are not turn-based, but that can react asynchronously to a video stream, as well as video data showing users performing tasks including mistakes and their corrections. To this end, we introduce Qualcomm Interactive Cooking, a new benchmark and dataset built upon CaptainCook4D, which contains user mistakes during task execution. Our dataset and benchmark features densely annotated, timed instructions and feedback messages, specifically including mistake alerts precisely timestamped to their visual occurrence in the video. We evaluate state-of-the-art multi-modal LLMs on the Qualcomm Interactive Cooking benchmark and introduce LiveMamba, a streaming multi-modal LLM designed for interactive instructional guidance. This work provides the first dedicated benchmark and a strong baseline for developing and evaluating on live, situated coaching.

</details>


### [43] [StreamFlow: Theory, Algorithm, and Implementation for High-Efficiency Rectified Flow Generation](https://arxiv.org/abs/2511.22009)
*Sen Fang,Hongbin Zhong,Yalin Feng,Dimitris N. Metaxas*

Main category: cs.CV

> 研究为Rectified Flow提出了一个在理论、设计、策略上的加速管道，增强了现有的生成模型性能，实验中将512*512图像的生成速度提高了611%。

<details>
  <summary>Details</summary>

**Motivation:** 当前技术如Rectified Flow和Flow Matching虽然提升了生成模型的性能，但由于理论和设计上的差异，现有加速方法无法直接使用，因此需要开发新的加速策略。

**Method:** 研究提出一个加速管道，包括批量处理和新的速度场、异构时间步批量处理的矢量化以及动态TensorRT编译。

**Result:** 本研究针对Rectified Flow和Flow Matching等新技术在生成模型上的性能提升，提出了一个完整的加速管道，从理论、设计和策略方面进行改进。新方法包括批量处理和新的速度场、异构时间步批量处理的矢量化以及动态TensorRT编译，显著加快了基于流模型的加速。实验表明，相对现有方法只有18%的加速，新方法可以将512*512图像生成速度提升到611%，远超当前的非通用加速方法。

**Conclusion:** 我们的新加速方法显著提高了基于流模型的生成效率，特别是在512*512图像生成速度上，显示出其在现有技术上的优越性。

**Abstract:** New technologies such as Rectified Flow and Flow Matching have significantly improved the performance of generative models in the past two years, especially in terms of control accuracy, generation quality, and generation efficiency. However, due to some differences in its theory, design, and existing diffusion models, the existing acceleration methods cannot be directly applied to the Rectified Flow model. In this article, we have comprehensively implemented an overall acceleration pipeline from the aspects of theory, design, and reasoning strategies. This pipeline uses new methods such as batch processing with a new velocity field, vectorization of heterogeneous time-step batch processing, and dynamic TensorRT compilation for the new methods to comprehensively accelerate related models based on flow models. Currently, the existing public methods usually achieve an acceleration of 18%, while experiments have proved that our new method can accelerate the 512*512 image generation speed to up to 611%, which is far beyond the current non-generalized acceleration methods.

</details>


### [44] [MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis](https://arxiv.org/abs/2511.22018)
*Chunzheng Zhu,Yangfang Lin,Shen Chen,Yijun Wang,Jianxin Lin*

Main category: cs.CV

> MedEyes 是一种新的强化学习框架，模拟临床医生的诊断推理过程，通过结合专家指导和自主发现，提高了医学VQA基准的性能。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于当前的视觉语言模型虽然在逻辑链条上显示出一定的推理能力，但其诊断路径往往缺乏临床准确性，MedEyes旨在通过模拟临床医生的逐步聚焦和迭代推理，提升诊断的准确性。

**Method:** MedEyes通过Gaze-guided Reasoning Navigator (GRN) 和 Confidence Value Sampler (CVS) 模拟临床医生的诊断过程，使用双通道探索策略来进行系统性的异常定位和详细的区域分析，同时实现专家模仿和自主发现的平衡。

**Result:** 实验结果表明，MedEyes在多个医学问答基准上取得了平均8.5%的性能提升。

**Conclusion:** MedEyes验证了其在构建具有解释性的医学AI系统方面的潜力。

**Abstract:** Accurate medical diagnosis often involves progressive visual focusing and iterative reasoning, characteristics commonly observed in clinical workflows. While recent vision-language models demonstrate promising chain-of-thought (CoT) reasoning capabilities via reinforcement learning with verifiable rewards (RLVR), their purely on-policy learning paradigm tends to reinforce superficially coherent but clinically inaccurate reasoning paths. We propose MedEyes, a novel reinforcement learning framework that dynamically models clinician-style diagnostic reasoning by progressively attending to and interpreting relevant medical image regions. By incorporating off-policy expert guidance, MedEyes converts expert visual search trajectories into structured external behavioral signals, guiding the model toward clinically aligned visual reasoning. We design the Gaze-guided Reasoning Navigator (GRN) to emulate the diagnostic process through a dual-mode exploration strategy, scanning for systematic abnormality localization and drilling for detailed regional analysis. To balance expert imitation and autonomous discovery, we introduce the Confidence Value Sampler (CVS), which employs nucleus sampling and adaptive termination to create diverse yet credible exploration paths. Finally, the dual-stream GRPO optimization framework decouples on-policy and off-policy learning signals, mitigating reward assimilation and entropy collapse. Experiments demonstrate that MedEyes achieves an average performance improvement of +8.5\% across multiple medical VQA benchmarks, validating MedEyes's potential in building interpretable medical AI systems.

</details>


### [45] [Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models](https://arxiv.org/abs/2511.22019)
*Zhenxiang Lin,Maryam Haghighat,Will Browne,Dimity Miller*

Main category: cs.CV

> This paper presents a post-hoc uncertainty estimation method for VLMs that enhances error detection without fine-tuning and demonstrates robust performance across various datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the issue of high confidence misclassifications in vision-language models (VLMs), which limits their reliability in safety-critical applications.

**Method:** Our method measures visual feature consistency within a class using feature projection and multivariate Gaussians to create class-specific probabilistic embeddings, which helps detect erroneous predictions in VLMs without the need for fine-tuning.

**Result:** Experiments on ImageNet, Flowers102, Food101, EuroSAT, and DTD demonstrate the method's effectiveness in detecting erroneous predictions across datasets.

**Conclusion:** The proposed method performs state-of-the-art in error detection, showing significant improvements over existing baselines, and is effective even with a small number of training images.

**Abstract:** Vision-language models (VLMs), such as CLIP, have gained popularity for their strong open vocabulary classification performance, but they are prone to assigning high confidence scores to misclassifications, limiting their reliability in safety-critical applications. We introduce a training-free, post-hoc uncertainty estimation method for contrastive VLMs that can be used to detect erroneous predictions. The key to our approach is to measure visual feature consistency within a class, using feature projection combined with multivariate Gaussians to create class-specific probabilistic embeddings. Our method is VLM-agnostic, requires no fine-tuning, demonstrates robustness to distribution shift, and works effectively with as few as 10 training images per class. Extensive experiments on ImageNet, Flowers102, Food101, EuroSAT and DTD show state-of-the-art error detection performance, significantly outperforming both deterministic and probabilistic VLM baselines. Code is available at https://github.com/zhenxianglin/ICPE.

</details>


### [46] [Layover or Direct Flight: Rethinking Audio-Guided Image Segmentation](https://arxiv.org/abs/2511.22025)
*Joel Alberto Santos,Zongwei Wu,Xavier Alameda-Pineda,Radu Timofte*

Main category: cs.CV

> 本研究探索了在不依赖文本的情况下直接从语音进行视觉场景中的对象定位的可行性，并证明这种方法在某些情况下甚至优于传统的依赖于文本的方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管近期在利用文本作为中间表达进行对象定位方面取得了进展，但研究者质疑这种依赖语音转录的方法的效率和鲁棒性。本研究旨在探索直接进行语音视觉对齐的可能。

**Method:** 本研究重点探讨了直接从语音进行对象定位的可行性，而非依赖于文本转录。研究通过采用单词语音指令作为任务简化手段，并引入了一个新的以音频为基础的对象定位数据集，包含多种对象和不同口音。

**Result:** 研究结果表明，直接从语音进行对象定位不仅可行，而且在处理语言变异性方面比文本转录方法更优秀，呈现出更高的鲁棒性。

**Conclusion:** 本研究提倡重新关注直接的语音到视觉对象定位技术，并指出这种方法对构建更加高效和鲁棒的多模态理解系统具有重要意义。

**Abstract:** Understanding human instructions is essential for enabling smooth human-robot interaction. In this work, we focus on object grounding, i.e., localizing an object of interest in a visual scene (e.g., an image) based on verbal human instructions. Despite recent progress, a dominant research trend relies on using text as an intermediate representation. These approaches typically transcribe speech to text, extract relevant object keywords, and perform grounding using models pretrained on large text-vision datasets. However, we question both the efficiency and robustness of such transcription-based pipelines. Specifically, we ask: Can we achieve direct audio-visual alignment without relying on text? To explore this possibility, we simplify the task by focusing on grounding from single-word spoken instructions. We introduce a new audio-based grounding dataset that covers a wide variety of objects and diverse human accents. We then adapt and benchmark several models from the closely audio-visual field. Our results demonstrate that direct grounding from audio is not only feasible but, in some cases, even outperforms transcription-based methods, especially in terms of robustness to linguistic variability. Our findings encourage a renewed interest in direct audio grounding and pave the way for more robust and efficient multimodal understanding systems.

</details>


### [47] [PAGen: Phase-guided Amplitude Generation for Domain-adaptive Object Detection](https://arxiv.org/abs/2511.22029)
*Shuchen Du,Shuo Lei,Feiran Li,Jiacheng Li,Daisuke Iso*

Main category: cs.CV

> This paper presents a simple unsupervised domain adaptation method that adapts image styles in the frequency domain to reduce domain discrepancy, achieving better performance with fewer complications than state-of-the-art methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to present a simple yet effective unsupervised domain adaptation method that avoids overly complex adversarial training strategies or elaborate architectural designs common in state-of-the-art approaches.

**Method:** The proposed method learns to adapt image styles in the frequency domain to reduce the discrepancy between source and target domains, introducing only a lightweight pre-processing module during training that is discarded at inference time.

**Result:** The method achieves substantial performance gains on multiple DAOD benchmarks when tested with ground-truth annotations easily accessible in source domains and difficult to obtain in target domains.

**Conclusion:** The proposed approach validates its effectiveness and practicality in domain-adaptive object detection tasks by significantly improving performance on multiple benchmarks with minimal additional overhead.

**Abstract:** Unsupervised domain adaptation (UDA) greatly facilitates the deployment of neural networks across diverse environments. However, most state-of-the-art approaches are overly complex, relying on challenging adversarial training strategies, or on elaborate architectural designs with auxiliary models for feature distillation and pseudo-label generation. In this work, we present a simple yet effective UDA method that learns to adapt image styles in the frequency domain to reduce the discrepancy between source and target domains. The proposed approach introduces only a lightweight pre-processing module during training and entirely discards it at inference time, thus incurring no additional computational overhead. We validate our method on domain-adaptive object detection (DAOD) tasks, where ground-truth annotations are easily accessible in source domains (e.g., normal-weather or synthetic conditions) but challenging to obtain in target domains (e.g., adverse weather or low-light scenes). Extensive experiments demonstrate that our method achieves substantial performance gains on multiple benchmarks, highlighting its practicality and effectiveness.

</details>


### [48] [SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model](https://arxiv.org/abs/2511.22039)
*Jiayuan Du,Yiming Zhao,Zhenglong Guo,Yong Pan,Wenbo Hou,Zhihui Hao,Kun Zhan,Qijun Chen*

Main category: cs.CV

> 本文提出一种新的轨迹条件下的未来3D场景占用预测架构，该架构通过稀疏占据表示的变压器模型直接处理图像特征，超越现有方法并取得显著性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 相比于依赖变分自动编码器（VAEs）生成离散占据令牌的方法，这些方法受到表示能力的限制，本论文提出的方法可以实现多帧未来占据的直接生成。通过避免离散令牌化与BEV表示结构局限性，以实现更高的性能。

**Method:** 本论文提出了一种基于稀疏占用表示的注意力机制变压器架构，以直接从原始图像特征进行端到端的未来3D场景占用预测。该方法避开了中间无人机视角（BEV）投影及其明确的几何先验，提高了对时空依赖关系的捕捉能力。

**Result:** 在nuScenes基准上，对于1-3秒的占用预测，这种方法优于现有方法，并且在任意未来轨迹条件下的场景动态理解方面表现出高度的准确性。

**Conclusion:** 最终结果显示，本文提出的方法在nuScenes基准上取得了最先进的表现，并且在任意未来轨迹条件下展现出了强大的场景动力学理解能力。

**Abstract:** This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.

</details>


### [49] [ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion](https://arxiv.org/abs/2511.22048)
*Junoh Kang,Donghun Ryu,Bohyung Han*

Main category: cs.CV

> The paper introduces ICM to regularize output towards a manifold conditioned on sparse structural information for enhancing super-resolution quality, showing significant improvements in perceptual quality.

<details>
  <summary>Details</summary>

**Motivation:** To address the conceptual misalignment with Real-ISR, and flawed generative prior of image reconstruction with color distortions and blurred edges.

**Method:** ICM (image-conditioned manifold regularization) is proposed, which regularizes the output towards a manifold conditioned on sparse structural information—colormap and Canny edges, to enhance super-resolution quality.

**Result:** Experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality.

**Conclusion:** The proposed method, ICM, provides a task-aligned and stable regularization signal, which significantly enhances the performance of super-resolution, especially in perceptual quality, thus demonstrating its effectiveness in real-world applications.

**Abstract:** Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.

</details>


### [50] [TPCNet: Triple physical constraints for Low-light Image Enhancement](https://arxiv.org/abs/2511.22052)
*Jing-Yi Shi,Ming-Fei Li,Ling-An Wu*

Main category: cs.CV

> 本文提出了TPCNet，通过一种新的物理约束模型——三重物理约束（TPCs）理论，在特征空间中构建光照、反射和检测之间的约束关系，提升了低光图像增强的效果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的可解释深度学习算法很大程度上依赖Retinex理论，但忽略了反射过程中的镜面反射，导致模型的泛化能力有限。

**Method:** 基于Kubelka-Munk理论，本文提出了一种新的物理约束关系—三重物理约束（TPCs）理论。该理论保留了反射系数，并在特征空间中构建了光照、反射和检测之间的约束关系。基于此理论，提出了TPC网络（TPCNet）。

**Result:** 通过对10个数据集的定量和定性基准测试和消融实验，证明了TPCNet在提高性能指标和视觉质量方面优于其他最先进的方法，且无需引入新的参数。

**Conclusion:** 实验结果表明，基于TPCs理论构建的TPCNet具有高效性和泛化能力，在多个数据集上表现出色。

**Abstract:** Low-light image enhancement is an essential computer vision task to improve image contrast and to decrease the effects of color bias and noise. Many existing interpretable deep-learning algorithms exploit the Retinex theory as the basis of model design. However, previous Retinex-based algorithms, that consider reflected objects as ideal Lambertian ignore specular reflection in the modeling process and construct the physical constraints in image space, limiting generalization of the model. To address this issue, we preserve the specular reflection coefficient and reformulate the original physical constraints in the imaging process based on the Kubelka-Munk theory, thereby constructing constraint relationship between illumination, reflection, and detection, the so-called triple physical constraints (TPCs)theory. Based on this theory, the physical constraints are constructed in the feature space of the model to obtain the TPC network (TPCNet). Comprehensive quantitative and qualitative benchmark and ablation experiments confirm that these constraints effectively improve the performance metrics and visual quality without introducing new parameters, and demonstrate that our TPCNet outperforms other state-of-the-art methods on 10 datasets.

</details>


### [51] [OralGPT-Omni: A Versatile Dental Multimodal Large Language Model](https://arxiv.org/abs/2511.22055)
*Jing Hao,Yuci Liang,Lizhuo Lin,Yuxuan Fan,Wenkai Zhou,Kaixin Guo,Zanting Ye,Yanpeng Sun,Xinyu Zhang,Yanqi Yang,Qiankun Li,Hao Tang,James Kit-Hon Tsoi,Linlin Shen,Kuo Feng Hung*

Main category: cs.CV

> 本研究提出了专门针对牙科影像分析的多模态大型语言模型OralGPT-Omni及其训练方法，通过构建牙科专用的数据集和训练范式，该模型在统一的多模态基准测试中大幅超越了现有模型。

<details>
  <summary>Details</summary>

**Motivation:** 由于牙科领域缺乏特定的数据和专家标注，多模态大型语言模型在牙科领域的应用尚未得到充分探索。本研究旨在通过专门针对牙科的MLLM解决这一问题，提升模型在牙科影像分析中的性能。

**Method:** 我们提出了第一个专门针对牙科的多模态大型语言模型（MLLM）——OralGPT-Omni，用于全面且值得信赖的牙科影像分析。通过构建TRACE-CoT数据集，模拟牙科放射学家的决策过程，该数据集提供了临床推理链。此外，我们提出了一种四阶段训练范式，极大提升了模型对牙科影像的理解和分析能力。同时，我们推出了第一个统一的牙科影像分析多模态基准测试——MMOral-Uni。

**Result:** 我们的模型OralGPT-Omni在MMOral-Uni基准测试中得分51.84，在MMOral-OPG基准测试中得分45.31，相较于GPT-5的表现有了显著的提升。

**Conclusion:** 这项工作促进了智能牙科的发展，为未来的牙科影像分析研究铺平了道路，展示了多模态大型语言模型在牙科学中的巨大潜力。

**Abstract:** Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.

</details>


### [52] [DNA: Dual-branch Network with Adaptation for Open-Set Online Handwriting Generation](https://arxiv.org/abs/2511.22064)
*Tsai-Ling Huang,Nhat-Tuong Do-Tran,Ngoc-Hoang-Lam Le,Hong-Han Shuai,Ching-Chun Huang*

Main category: cs.CV

> This paper introduces a new method for online handwriting generation (OHG) that generates unseen characters and styles by using a Dual-branch Network with Adaptation (DNA), which adapts to the style and decomposes symbols into structure and texture details.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of existing online handwriting generation methods in generating unseen characters, particularly in glyph-based languages like Chinese, which limits their real-world applicability.

**Method:** The paper proposes a Dual-branch Network with Adaptation (DNA), which includes an adaptive style branch to learn stroke attributes and an adaptive content branch designed to generalize to unseen characters by decomposing character content into structural information and texture details.

**Result:** The DNA model achieves state-of-the-art performance in the unseen online handwriting generation (OHG) setting, as demonstrated by extensive experiments.

**Conclusion:** The conclusion is that the proposed DNA model effectively handles the generation of unseen characters and styles, thus enhancing the real-world applicability of online handwriting generation methods.

**Abstract:** Online handwriting generation (OHG) enhances handwriting recognition models by synthesizing diverse, human-like samples. However, existing OHG methods struggle to generate unseen characters, particularly in glyph-based languages like Chinese, limiting their real-world applicability. In this paper, we introduce our method for OHG, where the writer's style and the characters generated during testing are unseen during training. To tackle this challenge, we propose a Dual-branch Network with Adaptation (DNA), which comprises an adaptive style branch and an adaptive content branch. The style branch learns stroke attributes such as writing direction, spacing, placement, and flow to generate realistic handwriting. Meanwhile, the content branch is designed to generalize effectively to unseen characters by decomposing character content into structural information and texture details, extracted via local and global encoders, respectively. Extensive experiments demonstrate that our DNA model is well-suited for the unseen OHG setting, achieving state-of-the-art performance.

</details>


### [53] [WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation](https://arxiv.org/abs/2511.22098)
*Quanjian Song,Yiren Song,Kelly Peng,Yuan Gao,Mike Zheng Shou*

Main category: cs.CV

> 本文提出WorldWander框架，用于解决视频生成中第一人称和第三人称视角转换问题，通过对齐视角和编码位置，实现实时同步，实验表明该框架表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 近年来，视频扩散模型在真实感和可控性方面已经取得了显著进步，但是不同视角之间无缝转换，如第一人称视角（egocentric）和第三人称视角（exocentric）之间，仍然少有人探索。而连接这两种视角对于电影制作、具身AI和世界模型来说是至关重要的。

**Method:** WorldWander采用先进的视频扩散变压器，结合上下文视角对齐和协作位置编码，高效地实现了跨视图同步。另外，为了支持这项任务，研究团队还整理了一个大规模数据集EgoExo-8K，其中包含来自合成和现实世界场景中的同步第一人称和第三人称视频三元组。

**Result:** 实验表明，WorldWander在视角同步、角色一致性以及泛化能力方面表现优异，为第一人称到第三人称的视频转换设定了一个新的基准。

**Conclusion:** 研究提出了一种专门针对视频生成中第一人称和第三人称世界转换的上下文学习框架WorldWander，实验结果证实了其在多方面的优越性，并设定了新的基准。

**Abstract:** Video diffusion models have recently achieved remarkable progress in realism and controllability. However, achieving seamless video translation across different perspectives, such as first-person (egocentric) and third-person (exocentric), remains underexplored. Bridging these perspectives is crucial for filmmaking, embodied AI, and world models. Motivated by this, we present WorldWander, an in-context learning framework tailored for translating between egocentric and exocentric worlds in video generation. Building upon advanced video diffusion transformers, WorldWander integrates (i) In-Context Perspective Alignment and (ii) Collaborative Position Encoding to efficiently model cross-view synchronization. To further support our task, we curate EgoExo-8K, a large-scale dataset containing synchronized egocentric-exocentric triplets from both synthetic and real-world scenarios. Experiments demonstrate that WorldWander achieves superior perspective synchronization, character consistency, and generalization, setting a new benchmark for egocentric-exocentric video translation.

</details>
