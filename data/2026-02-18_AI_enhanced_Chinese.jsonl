{"id": "2602.15072", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15072", "abs": "https://arxiv.org/abs/2602.15072", "authors": ["Abdul Joseph Fofanah", "Lian Wen", "Alpha Alimamy Kamara", "Zhongyi Zhang", "David Chen", "Albert Patrick Sankoh"], "title": "GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation", "comment": null, "summary": "Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.", "AI": {"tldr": "本文提出GRAFNet来解决肠道息肉分割的挑战，包括形态多样性和视觉相似性等，并在五个公共数据集上展示出优秀的分割效果和泛化能力。", "motivation": "克服现有深度学习方法在息肉分割中的单向处理、弱多尺度融合和缺乏解剖学约束，减少误报和漏检。", "method": "GRAFNet由三个模块组成：指导性不对称注意力模块（GAAM）、多尺度视网膜模块（MSRM）和指导性皮层注意力反馈模块（GCAFM），这些模块整合在一个息肉编解码模块（PEDM）中。", "result": "在五个公开数据集上结果显著优于现有技术，Dice指数提升3-8%，泛化能力提高10-20%。", "conclusion": "GRAFNet通过引入神经计算原则，实现了AI准确性与临床可信赖性之间的平衡。"}}
{"id": "2602.15124", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15124", "abs": "https://arxiv.org/abs/2602.15124", "authors": ["Shiyu Xuan", "Dongkai Wang", "Zechao Li", "Jinhui Tang"], "title": "Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition", "comment": "ICLR 2026", "summary": "Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.", "AI": {"tldr": "摘要总结了该论文提出了一种针对零样本人类-物体交互检测的分割框架，将目标检测与交互识别分离，并引入了多模式大语言模型以进行零样本交互识别。该论文还提出了一种确定性生成方法以及设计了空间感知池化模块和一次通过确定性匹配方法。", "motivation": "动机在于通过将交互识别从目标检测中分离出来并且使用更复杂的多模式语言模型来提高在零样本设置下的交互识别性能，并通过新颖的模型设计进一步提升性能与效率。", "method": "Structure", "result": "{\n  \"tldr\": \"The paper proposes a decoupled framework for zero-shot human-object interaction (HOI) detection, which separates object detection from interaction recognition (IR) and uses multi-modal large language models (MLLMs) for zero-shot IR. It introduces a deterministic generation method and designs a spatial-aware pooling module as well as a one-pass deterministic matching method.\",\n  \"motivation\": \"The motivation is to improve interaction recognition in zero-shot settings by decoupling it from object detection and using more sophisticated multi-modal language models, while also enhancing performance and efficiency through novel model designs.\",\n  \"method\": \"The method involves a decoupled approach for object detection and interaction recognition, utilizing multi-modal large language models for recognition. It includes deterministic generation through visual question answering, spatial-aware pooling, and a one-pass matching method for efficiency.\",\n  \"result\": \"Experiments show superior zero-shot performance, strong cross-dataset generalization, and the capability to integrate any object detectors without retraining.\",\n  \"conclusion\": \"The findings suggest that the proposed framework effectively enhances zero-shot interaction recognition for HOI detection.\",\n  \"schema\": \"tldr,motivation,method,result,conclusion,type=json\"}\n}", "conclusion": "结果表明，提出的框架有效提高了零样本人类-物体交互检测中的交互识别性能。"}}
{"id": "2602.15138", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15138", "abs": "https://arxiv.org/abs/2602.15138", "authors": ["Marcus Jenkins", "Jasenka Mazibrada", "Bogdan Leahu", "Michal Mackiewicz"], "title": "MB-DSMIL-CL-PL: Scalable Weakly Supervised Ovarian Cancer Subtype Classification and Localisation Using Contrastive and Prototype Learning with Frozen Patch Features", "comment": null, "summary": "The study of histopathological subtypes is valuable for the personalisation of effective treatment strategies for ovarian cancer. However, increasing diagnostic workloads present a challenge for UK pathology departments, leading to the rise in AI approaches. While traditional approaches in this field have relied on pre-computed, frozen image features, recent advances have shifted towards end-to-end feature extraction, providing an improvement in accuracy but at the expense of significantly reduced scalability during training and time-consuming experimentation. In this paper, we propose a new approach for subtype classification and localisation in ovarian cancer histopathology images using contrastive and prototype learning with pre-computed, frozen features via feature-space augmentations. Compared to DSMIL, our method achieves an improvement of 70.4\\% and 15.3\\% in F1 score for instance- and slide-level classification, respectively, along with AUC gains of 16.9\\% for instance localisation and 2.3\\% for slide classification, while maintaining the use of frozen patch features.", "AI": {"tldr": "本文提出了一种新的方法，利用对比学习和原型学习在卵巢癌组织学图像中进行亚型分类和定位。这种方法在保持特征冻结的同时大大提升了性能。", "motivation": "传统的卵巢癌亚型研究依赖于预计算的冻结图像特征，而新方法旨在通过端到端特征提取提高准确性，尽管这种方法减少了训练时的可扩展性且实验耗时。因此，本研究试图在保持特征块冻结的情况下，提高分类和定位性能。", "method": "我们提出了一种新的方法，利用对比学习和原型学习以及特征空间的增强来分类和定位卵巢癌组织学图像中的亚型。这种方法在保持使用冻结的特征块的同时，依然实现了显著的性能提升。", "result": "与现有方法DSMIL相比，我们的方法在实例级和幻灯片级分类F1评分上分别提高了70.4%和15.3%，并且在实例定位和幻灯片分类的AUC上分别提高了16.9%和2.3%。", "conclusion": "该研究表明，使用对比学习和原型学习技术的进步可以在保持特征冻结的条件下显著提升卵巢癌组织学图像的亚型分类和定位性能。"}}
{"id": "2602.15154", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15154", "abs": "https://arxiv.org/abs/2602.15154", "authors": ["Praditha Alwis", "Soumyadeep Chandra", "Deepak Ravikumar", "Kaushik Roy"], "title": "Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories", "comment": "8 pages, 5 figures, 6 tables", "summary": "High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.", "AI": {"tldr": "The paper proposes using Cumulative Sample Loss (CSL) to detect annotation errors in video datasets by analyzing loss patterns across training epochs, flagging frames with persistently high CSL.", "motivation": "The motivation is to address the issue of annotation errors in video datasets, specifically mislabeling and disordering, which can significantly impact the performance of models trained on these datasets.", "method": "The method involves calculating the Cumulative Sample Loss (CSL) for each frame of a video, which is the average loss across various model checkpoints during training. Frames with persistently high or irregular CSL values are flagged for potential annotation errors.", "result": "Experiments on EgoPER and Cholec80 datasets demonstrate the effectiveness of the method in detecting subtle inconsistencies such as mislabeling and frame disordering with strong performance.", "conclusion": "The proposed method is model-agnostic, does not require ground truth on annotation errors, and is shown to be effective in flagging mislabeling and temporal errors, providing a valuable tool for improving the quality of video datasets used in machine learning."}}
{"id": "2602.15034", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15034", "abs": "https://arxiv.org/abs/2602.15034", "authors": ["Houping Yue", "Zixiang Di", "Mei Jiang", "Bingdong Li", "Hao Hao", "Yu Song", "Bo Jiang", "Aimin Zhou"], "title": "EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research", "comment": "13 pages, 4 figures", "summary": "While Large Language Models (LLMs) are reshaping the paradigm of AI for Social Science (AI4SS), rigorously evaluating their capabilities in scholarly writing remains a major challenge. Existing benchmarks largely emphasize single-shot, monolithic generation and thus lack the fine-grained assessments required to reflect complex academic research workflows. To fill this gap, we introduce EduResearchBench, the first comprehensive evaluation platform dedicated to educational academic writing. EduResearchBench is built upon our Hierarchical Atomic Task Decomposition (HATD) framework, which decomposes an end-to-end research workflow into six specialized research modules (e.g., Quantitative Analysis, Qualitative Research, and Policy Research) spanning 24 fine-grained atomic tasks. This taxonomy enables an automated evaluation pipeline that mitigates a key limitation of holistic scoring, where aggregate scores often obscure specific capability bottlenecks, and instead provides fine-grained, diagnostic feedback on concrete deficiencies. Moreover, recognizing the high cognitive load inherent in scholarly writing, we propose a curriculum learning strategy that progressively builds competence from foundational skills to complex methodological reasoning and argumentation. Leveraging 55K raw academic samples, we curate 11K high-quality instruction pairs to train EduWrite, a specialized educational scholarly writing model. Experiments show that EduWrite (30B) substantially outperforms larger general-purpose models (72B) on multiple core metrics, demonstrating that in vertical domains, data quality density and hierarchically staged training curricula are more decisive than parameter scale.", "AI": {"tldr": "The paper presents EduResearchBench, an evaluation platform for scholarly writing using the HATD framework, and the EduWrite model, which shows that data quality and structured training are more crucial than model size.", "motivation": "The motivation is to address the challenge of rigorously evaluating Large Language Models (LLMs) in scholarly writing, given that current benchmarks do not provide fine-grained assessments necessary for complex academic research workflows.", "method": "The paper introduces EduResearchBench, an evaluation platform for educational academic writing, based on the Hierarchical Atomic Task Decomposition (HATD) framework which breaks down research workflows into 24 fine-grained tasks across six modules, enabling detailed diagnostic feedback. It also describes a curriculum learning strategy to train a specialized model, EduWrite, using 11K high-quality instruction pairs out of 55K academic samples.", "result": "Experiments demonstrated that the specialized EduWrite model (30B parameters) outperformed larger general-purpose models (72B parameters) on various core metrics.", "conclusion": "The study concludes that in niche domains, the quality and density of training data, alongside a well-designed hierarchical training curriculum, are more critical factors for model performance compared to the scale of model parameters."}}
{"id": "2602.15167", "categories": ["cs.CV", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.15167", "abs": "https://arxiv.org/abs/2602.15167", "authors": ["Xiaoyi Wen", "Fei Jiang"], "title": "Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift", "comment": null, "summary": "Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.", "AI": {"tldr": "本文提出了一种新的深度学习框架，通过在高分辨率的CFD仿真与4D Flow MRI样本上训练和微调来提升超分辨率性能和领域泛化能力，尤其是在处理临床数据方面表现出色。", "motivation": "传统的超分辨率方法通常依赖于成对的数据集，这些数据集包括降采样的图像和原始的高分辨率图像，但这种方法在实际的临床环境中存在局限性，因为低分辨率数据往往来源于与简单降采样不同的获取机制，导致模型泛化能力差。", "method": "提出了一种基于分布的深度学习框架，旨在增强模型的鲁棒性和领域泛化能力。该方法首先在高分辨率的计算流体动力学(CFD)仿真及其降采样版本上训练模型，然后使用少量配对的4D Flow MRI和CFD样本进行微调。", "result": "该方法显著优于传统的深度学习方法，这在实际数据应用中得到了证实，证明了基于分布的学习框架在解决领域转移问题和提高超分辨率性能方面的有效性。", "conclusion": "通过提出一个基于分布的深度学习框架来改善模型在临床真实场景中的超分辨率性能，解决了传统方法由于领域转移而产生的泛化能力差的问题。"}}
{"id": "2602.15038", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15038", "abs": "https://arxiv.org/abs/2602.15038", "authors": ["Mihir Panchal", "Deeksha Varshney", "Mamta", "Asif Ekbal"], "title": "Indic-TunedLens: Interpreting Multilingual Models in Indian Languages", "comment": "19th Conference of the European Chapter of the Association for Computational Linguistics (EACL) Thirteenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial) 2026", "summary": "Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at https://huggingface.co/spaces/AnonymousAccountACL/IndicTunedLens. Our code is available at https://github.com/AnonymousAccountACL/IndicTunedLens.", "AI": {"tldr": "介绍了一种专为印度语言设计的可解释性框架Indic-TunedLens，解决了跨语言解释性的难题，提高了多语言大模型在印度语言上的解释能力。", "motivation": "多语言大模型在印度等语言多样性丰富的地区被广泛部署，然而大多数解释性工具有限且主要针对英语。现有的方法在解码非英语语言的中间表示时存在不足。", "method": "提出Indic-TunedLens框架，通过学习共享的仿射变换来调整每个目标语言的隐藏状态，使其与目标输出分布对齐。", "result": "通过MMLU基准在10种印度语言上评估，新方法显著优于现有解释技术，特别是对于形态丰富、资源较少的语言。", "conclusion": "该研究为多语言变换器中的层级语义编码提供了重要见解。"}}
{"id": "2602.15181", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.15181", "abs": "https://arxiv.org/abs/2602.15181", "authors": ["Yunxiao Zhang", "William Stone", "Suryansh Kumar"], "title": "Time-Archival Camera Virtualization for Sports and Visual Performances", "comment": "Project Page: https://yunxiaozhangjack.com/tacv/; Under minor revision in Journal of Computer Vision and Image Understanding (CVIU); Special Issue: Computer Vision for Sports and Winter Sports. Outcome of a master and bachelor student project completed in Visual and Spatial AI Lab at TAMU", "summary": "Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...", "AI": {"tldr": "本文提出了一种基于神经体积渲染的方法，通过神经表征学习，来改善现有方法在处理快速、非刚体动态场景时的效果，并具备时间存档功能，可用于体育赛事和现场表演等领域。", "motivation": "解决现有相机虚拟化方法在处理快节奏、大范围非刚体场景中未能提供连贯且逼真的渲染效果的问题，尤其是那些涉及多个独立运动实体的场景。", "method": "提出了使用神经体积渲染方法，将动态场景建模为多视角静态相机视图下的刚性变换，并在此基础上进行神经表征学习，提供高质量的渲染效果。", "result": "方法能够实现实时视图合成，支持时间存档，允许用户在任何过去的动态场景实例中进行视图合成，以实现回顾性渲染、分析及档案存储的功能。", "conclusion": "该方法通过神经网络学习动态场景的神经体积表示，解决了现有方法对快速、复杂动态场景处理时的不足，适用于体育赛事广播、现场表演等领域。"}}
{"id": "2602.15139", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15139", "abs": "https://arxiv.org/abs/2602.15139", "authors": ["Tahir Hussain", "Saddam Hussain Khan"], "title": "CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding", "comment": "24 Pages, 9 Tables, 7 Figures", "summary": "Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.", "AI": {"tldr": "针对经典伊斯兰文本的挑战，本文提出名为CGRA DeBERTa的新框架，以提高神学问答系统的准确性。通过在特定领域应用技术和机制改进了训练模型，实现了97.85的EM评分，优于其他模型。", "motivation": "由于经典伊斯兰文本中特定领域的语义、长上下文依赖性和概念敏感推理等挑战，对准确的问答（QA）系统的需求亟待解决。", "method": "提出了一种名为CGRA DeBERTa的概念引导残差领域增强变换器框架，该框架基于定制化的DeBERTa变换器骨干网络，结合了轻量级LoRA适应性和概念感知门控机制，以增强对圣训集的神学问答（QA）准确性。定制化的DeBERTa嵌入块学习全局和位置上下文，同时神学概念引导残差块从一个精心策划的伊斯兰概念字典中整合12个核心术语的神学先验。此外，概念门控机制通过重要加权注意力选择性地放大语义关键标记，应用不同的缩放倍数从1.04到3.00。", "result": "利用由Sahih alBukhari和Sahih Muslim文本构成的42591个QA对的特别构建数据集训练CGRA模型后，该模型在EM评分上超过了BERT（75.87）和DeBERTa（89.77），达到97.85，而推理成本仅增加了约8。定性评估指出更好的提取和区别以及神学精准性。", "conclusion": "研究展示了提供的Hadith QA系统是高效、可解释和准确的，能够提供具有必要神学细微差别的教育资源。"}}
{"id": "2602.15257", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15257", "abs": "https://arxiv.org/abs/2602.15257", "authors": ["Austin Veselka"], "title": "How to Train Your Long-Context Visual Document Model", "comment": null, "summary": "We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.", "AI": {"tldr": "研究了24B和32B参数的长期上下文视觉语言模型，实现了MMLongBenchDoc基准测试的最佳性能，发现了新训练方法的有效性并发布了改进的MMLBD-C数据集。", "motivation": "我们的动机是弥补现有研究中训练配方和数据管道不可复现的空白，通过系统化研究和广泛的长上下文评估来填补这一空白。", "method": "我们研究了长期上下文视觉语言模型的训练，涉及24B和32B参数的模型。我们系统地研究了继续预训练、监督微调和偏好优化等方法，并通过长上下文评估和消融研究来验证这些方法的有效性。", "result": "我们在MMLongBenchDoc基准测试上达到了该参数规模下的最佳性能。发现与评价上下文长度匹配的训练上下文长度表现优于更长的训练上下文长度，页面索引训练和评估提供了简单高效的长文档性能提升，以及合成数据管道有助于通过继续预训练和监督微调实现自我改进。", "conclusion": "我们证明了视觉和文本之间的长上下文能力可以互相迁移，并发布了MMLBD-C数据集以提高基准质量。"}}
