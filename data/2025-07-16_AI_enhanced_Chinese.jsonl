{"id": "2507.10689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10689", "abs": "https://arxiv.org/abs/2507.10689", "authors": ["Tongshun Zhang", "Pingping Liu", "Yubing Lu", "Mengen Cai", "Zijian Zhang", "Zhe Zhang", "Qiuzhan Zhou"], "title": "CWNet: Causal Wavelet Network for Low-Light Image Enhancement", "comment": "Accepted by ICCV 2025", "summary": "Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on\nuniform brightness adjustment, often neglecting instance-level semantic\ninformation and the inherent characteristics of different features. To address\nthese limitations, we propose CWNet (Causal Wavelet Network), a novel\narchitecture that leverages wavelet transforms for causal reasoning.\nSpecifically, our approach comprises two key components: 1) Inspired by the\nconcept of intervention in causality, we adopt a causal reasoning perspective\nto reveal the underlying causal relationships in low-light enhancement. From a\nglobal perspective, we employ a metric learning strategy to ensure causal\nembeddings adhere to causal principles, separating them from non-causal\nconfounding factors while focusing on the invariance of causal factors. At the\nlocal level, we introduce an instance-level CLIP semantic loss to precisely\nmaintain causal factor consistency. 2) Based on our causal analysis, we present\na wavelet transform-based backbone network that effectively optimizes the\nrecovery of frequency information, ensuring precise enhancement tailored to the\nspecific attributes of wavelet transforms. Extensive experiments demonstrate\nthat CWNet significantly outperforms current state-of-the-art methods across\nmultiple datasets, showcasing its robust performance across diverse scenes.\nCode is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.", "AI": {"tldr": "CWNet 提出了一种新的架构，结合因果推理和小波变换，以改善传统的低光图像增强方法，显著提升了图像增强效果。", "motivation": "传统的低光图像增强方法主要集中在亮度的均匀调整，忽视了实例级别的语义信息和不同特征的内在特性。", "method": "CWNet 采用了基于因果推理和小波变换的网络架构，包含两个关键组件：因果推理方法和基于小波变换的骨干网络。", "result": "实验结果显示，CWNet 在多个数据集上显著优于目前最先进的方法，表现出强大的跨场景性能表现。", "conclusion": "通过全球视角的因果嵌入学习和局部语义损失，CWNet 有效保留了因果因素一致性，并实现针对具体小波变换属性的精准增强。"}}
{"id": "2507.10737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10737", "abs": "https://arxiv.org/abs/2507.10737", "authors": ["Jiayuan Chen", "Thai-Hoang Pham", "Yuanlong Wang", "Ping Zhang"], "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines", "comment": "ICCV 2025", "summary": "High-throughput screening techniques, such as microscopy imaging of cellular\nresponses to genetic and chemical perturbations, play a crucial role in drug\ndiscovery and biomedical research. However, robust perturbation screening for\n\\textit{de novo} cell lines remains challenging due to the significant\nmorphological and biological heterogeneity across cell lines. To address this,\nwe propose a novel framework that integrates external biological knowledge into\nexisting pretraining strategies to enhance microscopy image profiling models.\nOur approach explicitly disentangles perturbation-specific and cell\nline-specific representations using external biological information.\nSpecifically, we construct a knowledge graph leveraging protein interaction\ndata from STRING and Hetionet databases to guide models toward\nperturbation-specific features during pretraining. Additionally, we incorporate\ntranscriptomic features from single-cell foundation models to capture cell\nline-specific representations. By learning these disentangled features, our\nmethod improves the generalization of imaging models to \\textit{de novo} cell\nlines. We evaluate our framework on the RxRx database through one-shot\nfine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from\nthe RxRx19a dataset. Experimental results demonstrate that our method enhances\nmicroscopy image profiling for \\textit{de novo} cell lines, highlighting its\neffectiveness in real-world phenotype-based drug discovery applications.", "AI": {"tldr": "提出了一种新的框架，整合外部生物知识来改善显微镜成像模型处理从头细胞系时的表现，实验表明该方法在药物发现中有效。", "motivation": "高通量筛选技术对药物发现和生物医学研究至关重要，但是针对从头细胞系的鲁棒性扰动筛选由于不同细胞系之间存在显著的形态和生物学异质性，仍然是一个挑战。", "method": "我们提出了一种新框架，该框架将外部生物知识与现有预训练策略结合，以增强显微镜成像模型的配置文件化效果。我们的方法明确分离了扰动特定和细胞系特定表示，利用STRING和Hetionet数据库中的蛋白质相互作用数据构建知识图谱，以指导模型在预训练期间聚焦于扰动特定特征。此外，我们整合了来自单细胞基础模型的转录组特征，以捕捉细胞系特定表示。", "result": "我们在RxRx数据库上进行了一次性的微调（针对RxRx1细胞系）和几次微调（针对RxRx19a数据集中的细胞系），实验结果表明，我们的方法提高了显微镜成像模型对从头细胞系的泛化能力，展示了其在基于表型的药物发现应用中的有效性。", "conclusion": "我们的方法提高了显微镜成像模型对从头细胞系的适应能力，对于基于表型的药物发现应用具有重要意义。"}}
{"id": "2507.10755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10755", "abs": "https://arxiv.org/abs/2507.10755", "authors": ["Rina Khan", "Catherine Stinson"], "title": "Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias", "comment": null, "summary": "Facial expression recognition (FER) algorithms classify facial expressions\ninto emotions such as happy, sad, or angry. An evaluative challenge facing FER\nalgorithms is the fall in performance when detecting spontaneous expressions\ncompared to posed expressions. An ethical (and evaluative) challenge facing FER\nalgorithms is that they tend to perform poorly for people of some races and\nskin colors. These challenges are linked to the data collection practices\nemployed in the creation of FER datasets. In this study, we audit two\nstate-of-the-art FER datasets. We take random samples from each dataset and\nexamine whether images are spontaneous or posed. In doing so, we propose a\nmethodology for identifying spontaneous or posed images. We discover a\nsignificant number of images that were posed in the datasets purporting to\nconsist of in-the-wild images. Since performance of FER models vary between\nspontaneous and posed images, the performance of models trained on these\ndatasets will not represent the true performance if such models were to be\ndeployed in in-the-wild applications. We also observe the skin color of\nindividuals in the samples, and test three models trained on each of the\ndatasets to predict facial expressions of people from various races and skin\ntones. We find that the FER models audited were more likely to predict people\nlabeled as not white or determined to have dark skin as showing a negative\nemotion such as anger or sadness even when they were smiling. This bias makes\nsuch models prone to perpetuate harm in real life applications.", "AI": {"tldr": "The study audits two FER datasets, revealing issues with posed images in supposed in-the-wild datasets, and racial biases in model predictions towards darker skin tones.", "motivation": "To evaluate the performance and ethical implications of facial expression recognition models and datasets by auditing the source of their data and testing their biases.", "method": "Random samples from two FER datasets are examined to determine if images are spontaneous or posed, and models are tested on their ability to predict emotions for various races and skin tones.", "result": "A significant number of images in the datasets are posed, and the models show bias towards predicting negative emotions for individuals with darker skin tones.", "conclusion": "The performance of FER models varies greatly between spontaneous and posed images, and these models have racial biases perpetuating harm if applied in the wild."}}
{"id": "2507.10770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10770", "abs": "https://arxiv.org/abs/2507.10770", "authors": ["Ionuţ Grigore", "Călin-Adrian Popa", "Claudiu Leoveanu-Condrei"], "title": "FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching", "comment": null, "summary": "The extraction and matching of interest points are fundamental to many\ngeometric computer vision tasks. Traditionally, matching is performed by\nassigning descriptors to interest points and identifying correspondences based\non descriptor similarity. This work introduces a technique where interest\npoints are inherently associated during detection, eliminating the need for\ncomputing, storing, transmitting, or matching descriptors. Although the\nmatching accuracy is marginally lower than that of conventional approaches, our\nmethod completely eliminates the need for descriptors, leading to a drastic\nreduction in memory usage for localization systems. We assess its effectiveness\nby comparing it against both classical handcrafted methods and modern learned\napproaches.", "AI": {"tldr": "A new interest point matching technique that eliminates the need for descriptors, reducing memory usage.", "motivation": "The motivation is to reduce memory usage in localization systems by completely eliminating the need for descriptors, despite the marginal decrease in matching accuracy compared to conventional methods.", "method": "The paper introduces a technique for the extraction and matching of interest points that inherently associates points during detection, eliminating the need for descriptors.", "result": "The method was assessed by comparing it against both classical handcrafted and modern learned approaches.", "conclusion": "This approach leads to a drastic reduction in memory usage, offering a viable alternative to traditional methods."}}
{"id": "2507.10577", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.10577", "abs": "https://arxiv.org/abs/2507.10577", "authors": ["Logé Cécile", "Ghori Rehan"], "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions", "comment": null, "summary": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.", "AI": {"tldr": "本文介绍了一种基于AI的系统来对抗误导性信息，该系统可以检查YouTube视频中的声明并在评论区生成有说服力的评论。实验结果证实了该系统的事实核查能力和改善在线环境的潜力。", "motivation": "本文旨在解决当今数字世界中迅速蔓延的错误信息问题。通过开发一种AI驱动的系统来检查YouTube视频中的声明并在评论区与用户互动，作者希望挑战误导性的叙述，从而帮助创建更加知情的在线环境。", "method": "本文提出了一种基于AI的系统，该系统不仅可以检查YouTube视频中的陈述的真实性，还可以通过与用户在评论区互动来挑战误导性的叙述。该系统由两个主要组件组成：Truth Sleuth和Trend Bender。Truth Sleuth使用检索增强生成(RAG)方法评估视频中的陈述，并生成详细的报告。Trend Bender根据这些报告生成有见地和有说服力的评论，以促进有意义的讨论。", "result": "实验结果显示，该系统的事实核查代理具有高准确性，并展示了AI驱动干预在对抗错误信息方面的潜力，同时也证实了它在促进更知情的在线空间方面的潜力。", "conclusion": "研究表明，AI系统在事实核查和与用户互动方面有很高的潜力，这对于创建更加知情和有包容性的在线社群是非常重要的。"}}
{"id": "2507.10775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10775", "abs": "https://arxiv.org/abs/2507.10775", "authors": ["Jeffrey Joan Sam", "Janhavi Sathe", "Nikhil Chigali", "Naman Gupta", "Radhey Ruparel", "Yicheng Jiang", "Janmajay Singh", "James W. Berck", "Arko Barman"], "title": "A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers", "comment": null, "summary": "Spacecraft deployed in outer space are routinely subjected to various forms\nof damage due to exposure to hazardous environments. In addition, there are\nsignificant risks to the subsequent process of in-space repairs through human\nextravehicular activity or robotic manipulation, incurring substantial\noperational costs. Recent developments in image segmentation could enable the\ndevelopment of reliable and cost-effective autonomous inspection systems. While\nthese models often require large amounts of training data to achieve\nsatisfactory results, publicly available annotated spacecraft segmentation data\nare very scarce. Here, we present a new dataset of nearly 64k annotated\nspacecraft images that was created using real spacecraft models, superimposed\non a mixture of real and synthetic backgrounds generated using NASA's TTALOS\npipeline. To mimic camera distortions and noise in real-world image\nacquisition, we also added different types of noise and distortion to the\nimages. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to\ngenerate performance benchmarks for the dataset under well-defined hardware and\ninference time constraints to mimic real-world image segmentation challenges\nfor real-time onboard applications in space on NASA's inspector spacecraft. The\nresulting models, when tested under these constraints, achieved a Dice score of\n0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.\nThe dataset and models for performance benchmark are available at\nhttps://github.com/RiceD2KLab/SWiM.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10580", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10580", "abs": "https://arxiv.org/abs/2507.10580", "authors": ["Vimaleswar A", "Prabhu Nandan Sahu", "Nilesh Kumar Sahu", "Haroon R Lone"], "title": "An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation", "comment": null, "summary": "Mental health plays a crucial role in the overall well-being of an\nindividual. In recent years, digital platforms have been increasingly used to\nexpand mental health and emotional support. However, there are persistent\nchallenges related to limited user accessibility, internet connectivity, and\ndata privacy, which highlight the need for an offline, smartphone-based\nsolution. To address these challenges, we propose EmoSApp (Emotional Support\nApp): an entirely offline, smartphone-based conversational app designed for\nmental health and emotional support. The system leverages Large Language Models\n(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and\nExecutorch for resource-constrained devices, allowing all inferences to occur\non the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned\nthe LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of\n14,582 mental-health QA pairs, along with the multi-turn conversational data.\n  Through qualitative human evaluation with the student population, we\ndemonstrate that EmoSApp has the ability to respond coherently, empathetically,\nmaintain interactive dialogue, and provide relevant suggestions to user's\nmental health problems. Additionally, quantitative evaluations on nine standard\ncommonsense and reasoning benchmarks demonstrate the efficacy of our\nfine-tuned, quantized model in low-resource settings. By prioritizing on-device\ndeployment and specialized domain adaptation, EmoSApp serves as a blueprint for\nfuture innovations in portable, secure, and highly tailored AI-driven mental\nhealth solutions.", "AI": {"tldr": "EmoSApp是一个完全离线的智能手机心理支持应用，利用大型语言模型为用户提供心理健康支持，克服了在线心理健康平台面临的问题。", "motivation": "由于心理健康对于个体的整体福祉至关重要，且数字平台面临用户访问性、互联网连接和数据隐私等方面的挑战，因此迫切需要开发一种离线、基于智能手机的心理健康和情感支持解决方案。", "method": "本文提出了一种名为EmoSApp的离线智能手机应用，利用大型语言模型(LLM)，特别针对资源有限的设备进行了微调、量化和部署。通过在包含14,582个心理健康问答对的定制数据集上进行微调，EmoSApp能够提供连贯、富有同情心、互动性强且相关信息的对话支持。", "result": "定性的人类评估显示EmoSApp能连贯、富有同情心地对话，并能提供相关建议；定量评估则表明在资源有限的环境中，经过微调和量化的模型依然具有高效性。", "conclusion": "通过定性和定量评估证明了EmoSApp在提供心理健康支持方面的能力。这表明，EmoSApp作为一种携带便捷、安全且高度定制的AI驱动心理健康解决方案，为未来的创新提供了蓝图。"}}
{"id": "2507.10778", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10778", "abs": "https://arxiv.org/abs/2507.10778", "authors": ["Hsiang-Wei Huang", "Jen-Hao Cheng", "Kuang-Ming Chen", "Cheng-Yen Yang", "Bahaa Alattar", "Yi-Ru Lin", "Pyongkun Kim", "Sangwon Kim", "Kwangju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "title": "Warehouse Spatial Question Answering with LLM Agent", "comment": "1st Place Solution of the 9th AI City Challenge Track 3", "summary": "Spatial understanding has been a challenging task for existing Multi-modal\nLarge Language Models~(MLLMs). Previous methods leverage large-scale MLLM\nfinetuning to enhance MLLM's spatial understanding ability. In this paper, we\npresent a data-efficient approach. We propose a LLM agent system with strong\nand advanced spatial reasoning ability, which can be used to solve the\nchallenging spatial question answering task in complex indoor warehouse\nscenarios. Our system integrates multiple tools that allow the LLM agent to\nconduct spatial reasoning and API tools interaction to answer the given\ncomplicated spatial question. Extensive evaluations on the 2025 AI City\nChallenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that\nour system achieves high accuracy and efficiency in tasks such as object\nretrieval, counting, and distance estimation. The code is available at:\nhttps://github.com/hsiangwei0903/SpatialAgent", "AI": {"tldr": "Developed a system that enhances spatial understanding using a combination of LLMs and specialized tools, demonstrating superior performance in warehouse scenario tasks.", "motivation": "The motivation is to present an efficient method to improve the spatial understanding ability of Multi-modal Large Language Models in challenging environments without relying on extensive finetuning.", "method": "We propose a LLM agent system integrating multiple tools for spatial reasoning and API interaction, aiming to enhance spatial understanding in complex indoor warehouse scenarios.", "result": "Extensive evaluations on the AI City Challenge dataset show that our system achieves high accuracy and efficiency in tasks involving object retrieval, counting, and distance estimation.", "conclusion": "The proposed system successfully addresses complex spatial reasoning tasks within a warehouse setting, providing a robust solution that surpasses the spatial capabilities of existing Multi-modal Large Language Models."}}
{"id": "2507.10582", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.10582", "abs": "https://arxiv.org/abs/2507.10582", "authors": ["Anders Ledberg", "Anna Thalén"], "title": "Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis", "comment": null, "summary": "Unstructured text from legal, medical, and administrative sources offers a\nrich but underutilized resource for research in public health and the social\nsciences. However, large-scale analysis is hampered by two key challenges: the\npresence of sensitive, personally identifiable information, and significant\nheterogeneity in structure and language. We present a modular toolchain that\nprepares such text data for embedding-based analysis, relying entirely on\nopen-weight models that run on local hardware, requiring only a\nworkstation-level GPU and supporting privacy-sensitive research.\n  The toolchain employs large language model (LLM) prompting to standardize,\nsummarize, and, when needed, translate texts to English for greater\ncomparability. Anonymization is achieved via LLM-based redaction, supplemented\nwith named entity recognition and rule-based methods to minimize the risk of\ndisclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court\ndecisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.\nEach document is processed into an anonymized, standardized summary and\ntransformed into a document-level embedding. Validation, including manual\nreview, automated scanning, and predictive evaluation shows the toolchain\neffectively removes identifying information while retaining semantic content.\nAs an illustrative application, we train a predictive model using embedding\nvectors derived from a small set of manually labeled summaries, demonstrating\nthe toolchain's capacity for semi-automated content analysis at scale.\n  By enabling structured, privacy-conscious analysis of sensitive documents,\nour toolchain opens new possibilities for large-scale research in domains where\ntextual data was previously inaccessible due to privacy and heterogeneity\nconstraints.", "AI": {"tldr": "本文提出了一种工具链，旨在通过自动化的标准化、摘要和匿名化处理，使得在保护隐私的前提下进行无结构文本数据的大规模分析成为可能，为先前由于隐私和异质性约束而无法访问的文本数据研究打开新途径。", "motivation": "无结构文本（如法律、医疗和行政来源）为公共卫生和社会科学研究提供了丰富但未充分利用的资源，但大规模分析受个人敏感信息和结构语言显著异质性的阻碍，本研究旨在解决这些问题。", "method": "我们提出了一种模块化的工具链，用于准备基于嵌入分析的无结构文本数据。该工具链依赖完全开放权重的模型运行于本地硬件，只需工作站级别的GPU，并支持隐私敏感性的研究。该工具链运用大型语言模型（LLM）提示来标准化、摘要化文本，并在需要时将其翻译成英文以提高可比性。通过LLM的红行动作、命名实体识别和基于规则的方法来实现匿名化，减少暴露风险。", "result": "通过手动检查、自动化扫描和预测评估验证证明，工具链有效地删除了识别信息，同时保留了语义内容。我们使用从少量手动标签摘要中得出的嵌入向量训练预测模型，证明了工具链在大规模半自动化内容分析中的能力。", "conclusion": "通过启用结构化、隐私意识分析的敏感文档，我们的工具链为以前因隐私和异质性约束而无法访问文本数据的领域的大规模研究打开了新的可能性。"}}
{"id": "2507.10800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10800", "abs": "https://arxiv.org/abs/2507.10800", "authors": ["Ali Hojjat", "Janek Haberer", "Soren Pirk", "Olaf Landsiedel"], "title": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference", "comment": "Under Review", "summary": "Vision Transformers deliver state-of-the-art performance, yet their fixed\ncomputational budget prevents scalable deployment across heterogeneous\nhardware. Recent nested Transformer architectures mitigate this by embedding\nnested subnetworks within a single model to enable scalable inference. However,\nthese models allocate the same amount of compute to all inputs, regardless of\ntheir complexity, which leads to inefficiencies. To address this, we introduce\nThinkingViT, a nested ViT architecture that employs progressive thinking stages\nto dynamically adjust inference computation based on input difficulty.\nThinkingViT initiates inference by activating a small subset of the most\nimportant attention heads and terminates early if predictions reach sufficient\ncertainty. Otherwise, it activates additional attention heads and re-evaluates\nthe input. At the core of ThinkingViT is our Token Recycling mechanism, which\nconditions each subsequent inference stage on the embeddings from the previous\nstage, enabling progressive improvement. Due to its backbone-preserving design,\nThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show\nthat ThinkingViT surpasses nested baselines by up to 2.0 percentage points\n(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs\non ImageNet-1K. The source code is available at\nhttps://github.com/ds-kiel/ThinkingViT.", "AI": {"tldr": "ThinkingViT is a nested Vision Transformer architecture that utilizes a Token Recycling mechanism to dynamically adjust computation based on the complexity of the input, improving efficiency and accuracy on ImageNet-1K.", "motivation": "To enhance the efficiency and scalability of Vision Transformers across various hardware types by dynamically adjusting the amount of computation according to the difficulty of the input, thus avoiding unnecessary computational expenditures.", "method": "Proposes a nested ViT architecture (ThinkingViT) with progressive thinking stages and a Token Recycling mechanism. It starts with a limited number of attention heads, expands for complex inputs, and terminates when prediction certainty is high.", "result": "On ImageNet-1K, ThinkingViT demonstrates a 2.0 percentage point increase in accuracy compared to nested baselines at the same throughput and up to 2.9 percentage points at equal GMACs.", "conclusion": "ThinkingViT presents a backbone-preserving upgrade to vanilla Vision Transformers, significantly boosting efficiency and accuracy by intelligently allocating computational resources based on input complexity."}}
{"id": "2507.10585", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10585", "abs": "https://arxiv.org/abs/2507.10585", "authors": ["Isar Nejadgholi", "Mona Omidyeganeh", "Marc-Antoine Drouin", "Jonathan Boisvert"], "title": "A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations", "comment": "Presented at the Workshop of Technical AI Governance, 5 pages 2\n  figures", "summary": "Effective AI governance requires structured approaches for stakeholders to\naccess and verify AI system behavior. With the rise of large language models,\nNatural Language Explanations (NLEs) are now key to articulating model\nbehavior, which necessitates a focused examination of their characteristics and\ngovernance implications. We draw on Explainable AI (XAI) literature to create\nan updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:\n(1) Context, including task, data, audience, and goals; (2) Generation and\nPresentation, covering generation methods, inputs, interactivity, outputs, and\nforms; and (3) Evaluation, focusing on content, presentation, and user-centered\nproperties, as well as the setting of the evaluation. This taxonomy provides a\nframework for researchers, auditors, and policymakers to characterize, design,\nand enhance NLEs for transparent AI systems.", "AI": {"tldr": "本文提出了一种针对基于提示的自然语言解释（NLE）的更新的XAI分类法，旨在提供一个框架，以便研究者、审核员和政策制定者能够对透明的AI系统进行表征、设计和增强NLEs。", "motivation": "有效的AI治理需要结构化的方法让利益相关者可以访问和验证AI系统的行为。随着大型语言模型的兴起，自然语言解释(NLEs)现在成为表达模型行为的关键，这需要对NLEs的特征和治理影响进行专门研究。", "method": "我们基于可解释AI(XAI)文献，创建了一个更新的XAI分类法，该分类法专门针对基于提示的自然语言解释(NLE)，涵盖了三个维度：(1)上下文，包括任务、数据、受众和目标；(2)生成与呈现，涵盖生成方法、输入、交互性、输出和形式；以及(3)评估，重点是内容、呈现和以用户为中心的特性，以及评估的设置。", "result": "创建了一个专门针对基于提示的自然语言解释(NLEs)的XAI分类法，该分类法提供了透明AI系统的表征、设计和增强的框架。", "conclusion": "这个分类法提供了一个框架，帮助研究人员、审计员和政策制定者对透明的AI系统进行表征、设计和增强NLEs。"}}
{"id": "2507.10844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10844", "abs": "https://arxiv.org/abs/2507.10844", "authors": ["Furkan Mumcu", "Michael J. Jones", "Anoop Cherian", "Yasin Yilmaz"], "title": "LLM-Guided Agentic Object Detection for Open-World Understanding", "comment": null, "summary": "Object detection traditionally relies on fixed category sets, requiring\ncostly re-training to handle novel objects. While Open-World and\nOpen-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD\nlacks semantic labels for unknowns, and OVOD depends on user prompts, limiting\nautonomy. We propose an LLM-guided agentic object detection (LAOD) framework\nthat enables fully label-free, zero-shot detection by prompting a Large\nLanguage Model (LLM) to generate scene-specific object names. These are passed\nto an open-vocabulary detector for localization, allowing the system to adapt\nits goals dynamically. We introduce two new metrics, Class-Agnostic Average\nPrecision (CAAP) and Semantic Naming Average Precision (SNAP), to separately\nevaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD\nvalidate our approach, showing strong performance in detecting and naming novel\nobjects. Our method offers enhanced autonomy and adaptability for open-world\nunderstanding.", "AI": {"tldr": "We propose an LLM-guided framework for open-world object detection that enhances autonomy and adaptability, outperforming traditional methods by generating names for unknown objects without retraining.", "motivation": "Object detection traditionally relies on fixed category sets, requiring costly re-training to handle novel objects. While Open-World and Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting autonomy.", "method": "We propose an LLM-guided agentic object detection (LAOD) framework that enables fully label-free, zero-shot detection by prompting a Large Language Model (LLM) to generate scene-specific object names.", "result": "Experiments on LVIS, COCO, and COCO-OOD validate our approach, showing strong performance in detecting and naming novel objects.", "conclusion": "Our method offers enhanced autonomy and adaptability for open-world understanding."}}
{"id": "2507.10586", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10586", "abs": "https://arxiv.org/abs/2507.10586", "authors": ["Kaushik Dwivedi", "Padmanabh Patanjali Mishra"], "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable fluency across a\nrange of natural language tasks, yet remain vulnerable to hallucinations -\nfactual inaccuracies that undermine trust in real world deployment. We present\nAutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that\ntackles hallucination in large language models through lightweight LoRA-based\nadapters and KL-regularized training. Our pipeline integrates automated prompt\nrewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in\nretrieved evidence. A hallucination detection module, using both\nclassifier-based and self-evaluation techniques, assigns confidence scores to\ngenerated outputs, triggering an optional feedback correction loop. This loop\nenforces factual alignment via contrastive KL loss and adapter fine tuning. We\ndemonstrate that AutoRAG-LoRA significantly reduces the factual drift while\npreserving the efficiency and modularity of the model.", "AI": {"tldr": "提出AutoRAG-LoRA框架，通过综合利用多种技术手段减少大语言模型的幻觉现象，提高模型在实际应用中的可信度。", "motivation": "解决大语言模型在生成过程中出现的事实不准确问题，这会降低模型在实际部署中的信任度。", "method": "AutoRAG-LoRA框架综合利用轻量级LoRA适配器、KL正则化训练以及自动化提示重写、混合检索和低秩适配器微调等技术，将生成的回答与检索到的证据相结合，从而减少大语言模型的幻觉现象。", "result": "AutoRAG-LoRA显著减少了模型的幻觉现象，同时保持了模型的效率和模块化特性。", "conclusion": "该方法通过引入轻量级适配器和KL正则化训练等技术，实现在不过度影响生成效率的前提下减少大语言模型的幻觉。"}}
{"id": "2507.10846", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10846", "abs": "https://arxiv.org/abs/2507.10846", "authors": ["Casey Wall", "Longwei Wang", "Rodrigue Rizk", "KC Santosh"], "title": "Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization", "comment": "15 pages, 10 figures, 7 tables. Submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence", "summary": "Interpreting the decision-making process of Convolutional Neural Networks\n(CNNs) is critical for deploying models in high-stakes domains.\nGradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method\nfor visual explanations, yet it typically focuses on the final convolutional\nlayer or na\\\"ively averages across layers, strategies that can obscure\nimportant semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a\nnovel, human-tunable extension of Grad-CAM that generates robust and coherent\nsaliency maps by aggregating information across all convolutional layers. To\nmitigate the influence of noisy or extreme attribution values, Winsor-CAM\napplies Winsorization, a percentile-based outlier attenuation technique. A\nuser-controllable threshold allows for semantic-level tuning, enabling flexible\nexploration of model behavior across representational hierarchies. Evaluations\non standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the\nPASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable\nheatmaps and achieves superior performance in localization metrics, including\nintersection-over-union and center-of-mass alignment, when compared to Grad-CAM\nand uniform layer-averaging baselines. Winsor-CAM advances the goal of\ntrustworthy AI by offering interpretable, multi-layer insights with\nhuman-in-the-loop control.", "AI": {"tldr": "本文提出了一种新颖的、可调的Grad-CAM扩展方法——Winsor-CAM，它生成健壮且连贯的显著性图，并在多个标准指标和模型上验证了其在可解释性和定位性能上的优越性。", "motivation": "解释卷积神经网络（CNNs）的决策过程对于在高风险领域部署模型至关重要，而广泛使用的Grad-CAM方法通常关注于最终卷积层或简单地跨层平均，这可能会遮蔽重要的语义线索或放大无关的噪声。", "method": "通过应用Winsorization（一种基于百分位数的异常值衰减技术），Winsor-CAM方法聚合了所有卷积层的信息，生成健壮且连贯的显著性图，以减少噪声或极端属性值的影响。用户可控制的阈值允许进行语义级别的调整，可以在表征层次结构中灵活地探索模型行为。", "result": "在使用PASCAL VOC 2012数据集评估中，Winsor-CAM与Grad-CAM和均匀层平均基准相比，在定位指标（如交并比和质心对齐）上表现更好，产生了更具解释性的热图。", "conclusion": "Winsor-CAM通过提供具有人类参与控制的可解释、多层次洞察，推进了可信AI的目标。"}}
{"id": "2507.10587", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10587", "abs": "https://arxiv.org/abs/2507.10587", "authors": ["Dennis Ulmer", "Alexandra Lorson", "Ivan Titov", "Christian Hardmeier"], "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing", "comment": null, "summary": "Human users increasingly rely on natural language interactions with large\nlanguage models (LLMs) in order to receive help on a large variety of tasks and\nproblems. However, the trustworthiness and perceived legitimacy of LLMs is\nundermined by the fact that their output is frequently stated in very confident\nterms, even when its accuracy is questionable. Therefore, there is a need to\nsignal the confidence of the language model to a user in order to reap the\nbenefits of human-machine collaboration and mitigate potential harms.\nVerbalized uncertainty is the expression of confidence with linguistic means,\nan approach that integrates perfectly into language-based interfaces.\nNevertheless, most recent research in natural language processing (NLP)\noverlooks the nuances surrounding human uncertainty communication and the data\nbiases that influence machine uncertainty communication. We argue for\nanthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty\ncommunication requires a degree of linguistic authenticity and personalization\nto the user, which could be achieved by emulating human communication. We\npresent a thorough overview over the research in human uncertainty\ncommunication, survey ongoing research, and perform additional analyses to\ndemonstrate so-far overlooked biases in verbalized uncertainty. We conclude by\npointing out unique factors in human-machine communication of uncertainty and\ndeconstruct anthropomimetic uncertainty into future research directions for\nNLP.", "AI": {"tldr": "本文讨论了在自然语言处理中，通过语言手段表达语言模型的不确定性，提出了拟人化的不确定性方法，以增强人类对机器的信任。", "motivation": "由于大型语言模型的输出常常在准确性受到质疑的情况下表达得很自信，这影响了用户对这些模型的信任。因此，有必要通过语言手段表达语言模型的信心，并将这种信心传达给用户，以从人机协作中获益，并减少潜在的危害。", "method": "本文通过综述人类不确定性交流的研究，调查了正在进行的研究，并进行了进一步的分析，展示了迄今为止被忽视的口头化不确定性偏差。", "result": "研究表明在人类不确定性交流中存在数据偏差，机器的不确定性交流受到这些偏差的影响。", "conclusion": "文章总结了人类与机器之间不确定性交流的独特因素，并将拟人化不确定性分解为NLP未来研究方向。"}}
{"id": "2507.10855", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10855", "abs": "https://arxiv.org/abs/2507.10855", "authors": ["Wei Chen", "Jingxi Yu", "Zichen Miao", "Qiang Qiu"], "title": "Sparse Fine-Tuning of Transformers for Generative Tasks", "comment": "Accepted by International Conference on Computer Vision 2025", "summary": "Large pre-trained transformers have revolutionized artificial intelligence\nacross various domains, and fine-tuning remains the dominant approach for\nadapting these models to downstream tasks due to the cost of training from\nscratch. However, in existing fine-tuning methods, the updated representations\nare formed as a dense combination of modified parameters, making it challenging\nto interpret their contributions and understand how the model adapts to new\ntasks. In this work, we introduce a fine-tuning framework inspired by sparse\ncoding, where fine-tuned features are represented as a sparse combination of\nbasic elements, i.e., feature dictionary atoms. The feature dictionary atoms\nfunction as fundamental building blocks of the representation, and tuning atoms\nallows for seamless adaptation to downstream tasks. Sparse coefficients then\nserve as indicators of atom importance, identifying the contribution of each\natom to the updated representation. Leveraging the atom selection capability of\nsparse coefficients, we first demonstrate that our method enhances image\nediting performance by improving text alignment through the removal of\nunimportant feature dictionary atoms. Additionally, we validate the\neffectiveness of our approach in the text-to-image concept customization task,\nwhere our method efficiently constructs the target concept using a sparse\ncombination of feature dictionary atoms, outperforming various baseline\nfine-tuning methods.", "AI": {"tldr": "A sparse coding inspired fine-tuning framework is introduced to improve interpretability and adaptability of pretrained models for various tasks, showing better performance than existing methods.", "motivation": "To address the challenge of understanding model adaptation in fine-tuning by using sparse representation, enhancing interpretability.", "method": "Sparse coding inspired fine-tuning framework, where features are represented as a sparse combination of dictionary atoms for better interpretability and adaptation.", "result": "Improved performance in image editing with text alignment and text-to-image concept customization.", "conclusion": "The sparse coding method offers a better way to understand how pretrained models adapt to new tasks and performs well in image editing and concept customization tasks, outperforming baseline methods."}}
{"id": "2507.10596", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10596", "abs": "https://arxiv.org/abs/2507.10596", "authors": ["Yogachandran Rahulamathavan", "Misbah Farooq", "Varuna De Silva"], "title": "PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification", "comment": null, "summary": "Large Language Models (LLMs) excel in text classification, but their\ncomplexity hinders interpretability, making it difficult to understand the\nreasoning behind their predictions. Explainable AI (XAI) methods like LIME and\nSHAP offer local explanations by identifying influential words, but they rely\non computationally expensive perturbations. These methods typically generate\nthousands of perturbed sentences and perform inferences on each, incurring a\nsubstantial computational burden, especially with LLMs. To address this, we\npropose \\underline{P}erturbation-free \\underline{L}ocal \\underline{Ex}planation\n(PLEX), a novel method that leverages the contextual embeddings extracted from\nthe LLM and a ``Siamese network\" style neural network trained to align with\nfeature importance scores. This one-off training eliminates the need for\nsubsequent perturbations, enabling efficient explanations for any new sentence.\nWe demonstrate PLEX's effectiveness on four different classification tasks\n(sentiment, fake news, fake COVID-19 news and depression), showing more than\n92\\% agreement with LIME and SHAP. Our evaluation using a ``stress test\"\nreveals that PLEX accurately identifies influential words, leading to a similar\ndecline in classification accuracy as observed with LIME and SHAP when these\nwords are removed. Notably, in some cases, PLEX demonstrates superior\nperformance in capturing the impact of key features. PLEX dramatically\naccelerates explanation, reducing time and computational overhead by two and\nfour orders of magnitude, respectively. This work offers a promising solution\nfor explainable LLM-based text classification.", "AI": {"tldr": "PLEX是一种基于从LLM提取的上下文嵌入和「Siamese网络」风格的神经网络的新方法，旨在提供更高效且计算成本更低的局部解释。", "motivation": "大型语言模型（LLMs）在文本分类任务中表现出色，但其复杂性阻碍了模型的可解释性。传统解释性AI方法如LIME和SHAP虽然提供局部解释，但依赖于计算成本高昂的扰动操作。为了缓解这一问题，提出了一个新的方法。", "method": "PLEX方法利用从LLM提取的上下文嵌入和类似「Siamese网络」的神经网络来对特征重要性评分进行对齐，从而避免了在每次解释时需要进行扰动的步骤。这种方法仅需一次性的训练过程即可对任意新的句子提供高效的解释。", "result": "PLEX在四个分类任务（情感分析、假新闻检测、虚假新冠疫情新闻检测和抑郁分析）上得到了超过92%的与LIME和SHAP的一致性。此外，通过「压力测试」，PLEX证明了它准确识别重要词的能力，并且在某些情况下，比LIME和SHAP表现更优，减少了两到四个数量级的解释时间和计算开销。", "conclusion": "PLEX为LLM驱动的可解释文本分类提供了一种有效和高效的解决方案，显著降低了计算时间和计算成本，同时保持了高精度的数据解释。"}}
{"id": "2507.10864", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10864", "abs": "https://arxiv.org/abs/2507.10864", "authors": ["Saadat Behzadi", "Danial Sharifrazi", "Bita Mesbahzadeh", "Javad Hassannataj Joloudarid", "Roohallah Alizadehsani"], "title": "A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n", "comment": null, "summary": "Objectives: Timely and accurate detection of colorectal polyps plays a\ncrucial role in diagnosing and preventing colorectal cancer, a major cause of\nmortality worldwide. This study introduces a new, lightweight, and efficient\nframework for polyp detection that combines the Local Outlier Factor (LOF)\nalgorithm for filtering noisy data with the YOLO-v11n deep learning model.\n  Study design: An experimental study leveraging deep learning and outlier\nremoval techniques across multiple public datasets.\n  Methods: The proposed approach was tested on five diverse and publicly\navailable datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.\nSince these datasets originally lacked bounding box annotations, we converted\ntheir segmentation masks into suitable detection labels. To enhance the\nrobustness and generalizability of our model, we apply 5-fold cross-validation\nand remove anomalous samples using the LOF method configured with 30 neighbors\nand a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a\nfast and resource-efficient object detection architecture optimized for\nreal-time applications. We train the model using a combination of modern\naugmentation strategies to improve detection accuracy under diverse conditions.\n  Results: Our approach significantly improves polyp localization performance,\nachieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5\nof 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,\nour model demonstrates enhanced accuracy and efficiency.\n  Conclusions: These results suggest that the proposed method is well-suited\nfor real-time colonoscopy support in clinical settings. Overall, the study\nunderscores how crucial data preprocessing and model efficiency are when\ndesigning effective AI systems for medical imaging.", "AI": {"tldr": "研究提出了一种结合局部离群因子(LOF)算法和YOLO-v11n深度学习模型的轻量级结肠息肉检测框架，并在五个公开数据集上进行了测试，结果显示该方法在息肉检测任务上表现出高精度和效率。", "motivation": "及时和准确地检测结肠息肉对于诊断和预防结肠癌至关重要，而结肠癌是全球主要死亡原因之一。因此，研究旨在提出一种轻量级且高效的息肉检测框架，以支持临床结肠镜检查。", "method": "研究在五个不同的公开数据集：CVC-ColonDB、CVC-ClinicDB、Kvasir-SEG、ETIS和EndoScene上测试了一种融合了局部离群因子(LOF)算法和YOLO-v11n深度学习模型的方法。为了增强模型的鲁棒性和泛化能力，研究采用了5折交叉验证法和LOF方法来移除异常样本，将分割描边转换成适用于检测的目标标签，并使用了多种现代图像增广策略进行训练。", "result": "该方法显著提高了息肉的定位性能，达到了95.83%的精确率、91.85%的查全率、93.48%的F1分数、0.5时的MAP值为96.48%，以及0.5:0.95区间内的mAP值为77.75%。相比于先前基于YOLO的方法，该模型展示了更高的检测准确性和效率。", "conclusion": "研究结果表明，所提出的框架在临床场景中的结肠镜检查实时支持方面具有巨大的应用潜力。此外，该研究强调了数据预处理和模型效率在设计有效的医学影像AI系统中的重要性。"}}
{"id": "2507.10599", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10599", "abs": "https://arxiv.org/abs/2507.10599", "authors": ["Bo Zhao", "Maya Okawa", "Eric J. Bigelow", "Rose Yu", "Tomer Ullman", "Ekdeep Singh Lubana", "Hidenori Tanaka"], "title": "Emergence of Hierarchical Emotion Organization in Large Language Models", "comment": null, "summary": "As large language models (LLMs) increasingly power conversational agents,\nunderstanding how they model users' emotional states is critical for ethical\ndeployment. Inspired by emotion wheels -- a psychological framework that argues\nemotions organize hierarchically -- we analyze probabilistic dependencies\nbetween emotional states in model outputs. We find that LLMs naturally form\nhierarchical emotion trees that align with human psychological models, and\nlarger models develop more complex hierarchies. We also uncover systematic\nbiases in emotion recognition across socioeconomic personas, with compounding\nmisclassifications for intersectional, underrepresented groups. Human studies\nreveal striking parallels, suggesting that LLMs internalize aspects of social\nperception. Beyond highlighting emergent emotional reasoning in LLMs, our\nresults hint at the potential of using cognitively-grounded theories for\ndeveloping better model evaluations.", "AI": {"tldr": "研究发现大型语言模型在情绪识别上存在系统偏差，尤其对一些交叉身份的群体辨识有较大误差，并强调了需以认知模型优化模型评价。", "motivation": "随着大型语言模型越来越多地被用于会话代理，理解这些模型是如何建模用户的情绪状态变得至关重要，这关系到伦理部署的问题。同时，研究还希望能够深入了解模型内部情绪推理机制及其相关偏差，以及探索认知理论在改进模型评估中的潜在应用。", "method": "该研究受情绪轮模型（一种心理学框架）的启发，分析了模型输出中情绪状态之间的概率依赖关系。通过比较模型输出和真实人类的数据，探讨了LLMs如何形成分层情绪树并与人类心理模型相比较。此外，还进行了人类研究，以揭示LLMs内部化社会感知的方面及其误分类的特征。", "result": "研究揭示了大型语言模型(LLMs)如何自然形成与人类心理模型相吻合的分层情绪树，并且较大的模型会发展出更复杂的层次结构。此外，研究还发现模型在识别不同社会经济身份的情绪时存在系统性偏差，特别是对于交叉和代表性不足群体的误分类现象更为严重。这些发现不仅展示了LLMs中情绪推理的出现，还暗示了可以利用认知理论来改进模型评估的潜力。", "conclusion": "大型语言模型能够自然地模拟分层情绪树，反映出类似人类的情感组织方式，模型越大，其情绪层次结构越复杂。但是，模型在识别不同社会经济背景用户情绪时存在系统性偏差，尤其是在识别交叉身份群体时，这种偏差更为明显，这提示我们应进一步优化模型以减少偏见，并且可以借助认知理论来改进模型评估。"}}
{"id": "2507.10881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10881", "abs": "https://arxiv.org/abs/2507.10881", "authors": ["Roman Naeem", "David Hagerman", "Jennifer Alvén", "Lennart Svensson", "Fredrik Kahl"], "title": "Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes", "comment": "Submitted Version. Accepted at MICCAI 2025", "summary": "Tubular tree structures, such as blood vessels and airways, are essential in\nhuman anatomy and accurately tracking them while preserving their topology is\ncrucial for various downstream tasks. Trexplorer is a recurrent model designed\nfor centerline tracking in 3D medical images but it struggles with predicting\nduplicate branches and terminating tracking prematurely. To address these\nissues, we present Trexplorer Super, an enhanced version that notably improves\nperformance through novel advancements. However, evaluating centerline tracking\nmodels is challenging due to the lack of public datasets. To enable thorough\nevaluation, we develop three centerline datasets, one synthetic and two real,\neach with increasing difficulty. Using these datasets, we conduct a\ncomprehensive evaluation of existing state-of-the-art (SOTA) models and compare\nthem with our approach. Trexplorer Super outperforms previous SOTA models on\nevery dataset. Our results also highlight that strong performance on synthetic\ndata does not necessarily translate to real datasets. The code and datasets are\navailable at https://github.com/RomStriker/Trexplorer-Super.", "AI": {"tldr": "本文介绍了一种改进的中心线追踪模型Trexplorer Super，它可以更准确地追踪复杂结构并展示了优于现有模型的表现。同时也强调了在评估算法性能时真实数据的重要性。", "motivation": "Tubular tree结构，如血管和气道，是人体解剖学中的重要部分。准确追踪这些结构并保持其拓扑结构对于很多下游任务至关重要。现有的Trexplorer算法在预测重复分支和过早终止的问题上效果不佳。", "method": "本文提出了Trexplorer Super，这是Trexplorer算法的改进版，主要用于改善中心线追踪过程中预测重复分支和过早终止的问题，通过一些创新进步来提高性能。", "result": "通过使用开发的三个中心线数据集（一个合成数据集，两个真实数据集）对现有的SOTA模型进行全面评估，结果表明Trexplorer Super在所有数据集上都超过了先前的SOTA模型。此外，研究还发现，在合成数据上表现出色并不一定意味着在真实数据上也能表现出色。", "conclusion": "研究证明了Trexplorer Super在追踪人体内部的重要结构时能够提供比现有SOTA模型更好的追踪效果，同时也揭示了在合成数据上表现出色的模型不一定能在真实数据集上得到同样好的性能。"}}
{"id": "2507.10743", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10743", "abs": "https://arxiv.org/abs/2507.10743", "authors": ["Nickolas Freeman", "Thanh Nguyen", "Gregory Bott", "Jason Parton", "Collin Francel"], "title": "Language Models for Adult Service Website Text Analysis", "comment": "32 pages, 12 figures, 1 table", "summary": "Sex trafficking refers to the use of force, fraud, or coercion to compel an\nindividual to perform in commercial sex acts against their will. Adult service\nwebsites (ASWs) have and continue to be linked to sex trafficking, offering a\nplatform for traffickers to advertise their victims. Thus, organizations\ninvolved in the fight against sex trafficking often use ASW data when\nattempting to identify potential sex trafficking victims. A critical challenge\nin transforming ASW data into actionable insight is text analysis. Previous\nresearch using ASW data has shown that ASW ad text is important for linking\nads. However, working with this text is challenging due to its extensive use of\nemojis, poor grammar, and deliberate obfuscation to evade law enforcement\nscrutiny. We conduct a comprehensive study of language modeling approaches for\nthis application area, including simple information retrieval methods,\npre-trained transformers, and custom transformer models. We demonstrate that\ncharacteristics of ASW text data allow efficient custom transformer models to\nbe trained with relatively small GPU resources and used efficiently for\ninference on consumer hardware. Our custom models outperform fine-tuned\nvariants of well-known encoder-only transformer models, including BERT-base,\nRoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We\ndemonstrate the use of our best-performing custom configuration on three tasks\nrelated to ASW data analysis: (i) decomposing the giant component in a graph\nrepresentation of ASW data, (ii) clustering ASW ad text, and (iii) using the\nlearned token embeddings to understand the use of emojis in the illicit context\nwe study. The models we develop represent a significant advancement in ASW text\nanalysis, which can be leveraged in a variety of downstream applications and\nresearch.", "AI": {"tldr": "本文通过研究语言模型方法改进成人服务网站（ASWs）文本分析，展示了定制变压器模型在准确率、召回率、F1 分数和 ROC AUC 方面优于其他模型。", "motivation": "本文动机在于解决成人服务网站（ASWs）文本数据中的挑战，提高识别潜在性 trafficking 受害者的能力。", "method": "本文研究了成人服务网站（ASWs）文本数据分析的语言模型方法，包括简单信息检索方法、预训练变压器模型和定制变压器模型。", "result": "本文结果显示定制变压器模型在关键任务中优于其他预训练模型，如图中巨大组件分解、广告文本聚类和非法环境中表情符号使用的理解。", "conclusion": "本文的发展代表了成人服务网站（ASWs）文本分析的显著进步，该模型可在多种下游应用和研究中发挥作用。"}}
{"id": "2507.10893", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2507.10893", "abs": "https://arxiv.org/abs/2507.10893", "authors": ["Minjong Cheon", "Eunhan Goo", "Su-Hyeon Shin", "Muhammad Ahmed", "Hyungjun Kim"], "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency", "comment": "26pages, 9 Figures", "summary": "Recently, AI-based weather forecast models have achieved impressive advances.\nThese models have reached accuracy levels comparable to traditional NWP\nsystems, marking a significant milestone in data-driven weather prediction.\nHowever, they mostly leverage Transformer-based architectures, which often\nleads to high training complexity and resource demands due to the massive\nparameter sizes. In this study, we introduce a modernized CNN-based model for\nglobal weather forecasting that delivers competitive accuracy while\nsignificantly reducing computational requirements. To present a systematic\nmodernization roadmap, we highlight key architectural enhancements across\nmultiple design scales from an earlier CNN-based approach. KAI-a incorporates a\nscale-invariant architecture and InceptionNeXt-based blocks within a\ngeophysically-aware design, tailored to the structure of Earth system data.\nTrained on the ERA5 daily dataset with 67 atmospheric variables, the model\ncontains about 7 million parameters and completes training in just 12 hours on\na single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the\nperformance of state-of-the-art models in medium-range weather forecasting,\nwhile offering a significantly lightweight design. Furthermore, case studies on\nthe 2018 European heatwave and the East Asian summer monsoon demonstrate\nKAI-a's robust skill in capturing extreme events, reinforcing its practical\nutility.", "AI": {"tldr": "本文介绍了一种基于CNN的新型全球天气预报模型KAI-a，该模型在保持高准确性的前提下大幅降低了计算成本，适合用于捕捉极端天气事件。", "motivation": "由于许多现有的天气预报模型基于Transformer架构，导致训练复杂度和资源需求高，因此本文旨在提出一种能够在保证预测精度的同时显著减少计算要求的CNN模型。", "method": "本文提出了一种基于现代CNN的全球天气预报模型，该模型在其设计中融入了尺度不变架构和基于InceptionNeXt的模块，并考虑了地球系统数据的特性。该模型在一个NVIDIA L40s GPU上仅需12小时即可完成训练，参数量约为7百万。", "result": "实验表明，KAI-a在中短期天气预报中达到了现有最优模型的性能，同时具有轻量级设计。此外，KAI-a在2018年欧洲热浪和东亚夏季风的案例研究中表现出色，能够精准捕捉极端天气事件。", "conclusion": "KAI-a证明了在资源高效利用的情况下，依然可以获得高质量的天气预报性能，特别适用于极端天气事件的捕捉，展示了其实际应用的潜力。"}}
{"id": "2507.10772", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10772", "abs": "https://arxiv.org/abs/2507.10772", "authors": ["Michal Podstawski"], "title": "Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs", "comment": null, "summary": "Labeled property graphs often contain rich textual attributes that can\nenhance analytical tasks when properly leveraged. This work explores the use of\npretrained text embedding models to enable efficient semantic analysis in such\ngraphs. By embedding textual node and edge properties, we support downstream\ntasks including node classification and relation prediction with improved\ncontextual understanding. Our approach integrates language model embeddings\ninto the graph pipeline without altering its structure, demonstrating that\ntextual semantics can significantly enhance the accuracy and interpretability\nof property graph analysis.", "AI": {"tldr": "本研究探索了预训练的文本嵌入模型在含丰富文本属性的标记属性图中的应用，提升了图分析的准确性和可解释性。", "motivation": "研究动机在于，通过有效利用属性图中的文本属性，可以增强分析任务的效果。", "method": "本研究采用预训练的文本嵌入模型来增强具有丰富文本属性的标记属性图的语义分析。通过对节点和边的文本属性进行嵌入处理，支持后续节点分类和关系预测任务，从而提高上下文理解能力。", "result": "方法将语言模型嵌入集成到图数据处理管道中，而不改变图的结构，从而显著提升属性图分析的准确性和可解释性。", "conclusion": "结论表明，文本语义可以在不改变图结构的情况下，显著增强属性图分析的准确性与可解释性。"}}
{"id": "2507.10895", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10895", "abs": "https://arxiv.org/abs/2507.10895", "authors": ["Xiaocong Zeng", "Craig Michoski", "Yan Pang", "Dongyang Kuang"], "title": "Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition", "comment": null, "summary": "In this work, we address the often-overlooked issue of Timescale Dependent\nLabel Inconsistency (TsDLI) in training neural network models for EEG-based\nhuman emotion recognition. To mitigate TsDLI and enhance model generalization\nand explainability, we propose two novel regularization strategies: Local\nVariation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods\nincorporate classical mathematical principles--specifically, functions of\nbounded variation and commute-time distances--within a graph theoretic\nframework. Complementing our regularizers, we introduce a suite of new\nevaluation metrics that better capture the alignment between temporally local\npredictions and their associated global emotion labels. We validate our\napproach through comprehensive experiments on two widely used EEG emotion\ndatasets, DREAMER and DEAP, across a range of neural architectures including\nLSTM and transformer-based models. Performance is assessed using five distinct\nmetrics encompassing both quantitative accuracy and qualitative consistency.\nResults consistently show that our proposed methods outperform state-of-the-art\nbaselines, delivering superior aggregate performance and offering a principled\ntrade-off between interpretability and predictive power under label\ninconsistency. Notably, LVL achieves the best aggregate rank across all\nbenchmarked backbones and metrics, while LGCL frequently ranks the second,\nhighlighting the effectiveness of our framework.", "AI": {"tldr": "本文针对基于EEG的情感识别训练中神经网络模型遇到的时间尺度依赖性标签不一致问题，提出了两种新策略：LVL和LGCL，通过图论框架和经典数学原理，取得了优于现有方法的实验结果。", "motivation": "本文旨在解决神经网络模型在基于EEG的人类情感识别训练中的一个常常被忽视的问题：时间尺度依赖性标签不一致（TsDLI）。", "method": "两种新的正则化策略——局部变化损失（LVL）和局部-全局一致性损失（LGCL）被提出，以缓解由于时间尺度依赖性标签不一致（TsDLI）引起的问题。这两种方法都结合了有界变函数和通勤时间距离的经典数学原理，在图论框架内使用。", "result": "通过在广泛使用的EEG情感数据集DREAMER和DEAP上进行全面的实验，验证了所提出方法的有效性。实验结果表明，所提出的方法在多种神经架构中均优于现有最先进的基线方法，实现了更好的总体性能。", "conclusion": "所提出的方法提供了一种在标签不一致性背景下，在可解释性和预测能力之间达成原则性权衡的方法。"}}
{"id": "2507.10787", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10787", "abs": "https://arxiv.org/abs/2507.10787", "authors": ["Yilun Zhao", "Chengye Wang", "Chuhan Li", "Arman Cohan"], "title": "Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers", "comment": "ACL 2025 Findings", "summary": "This paper introduces MISS-QA, the first benchmark specifically designed to\nevaluate the ability of models to interpret schematic diagrams within\nscientific literature. MISS-QA comprises 1,500 expert-annotated examples over\n465 scientific papers. In this benchmark, models are tasked with interpreting\nschematic diagrams that illustrate research overviews and answering\ncorresponding information-seeking questions based on the broader context of the\npaper. We assess the performance of 18 frontier multimodal foundation models,\nincluding o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant\nperformance gap between these models and human experts on MISS-QA. Our analysis\nof model performance on unanswerable questions and our detailed error analysis\nfurther highlight the strengths and limitations of current models, offering key\ninsights to enhance models in comprehending multimodal scientific literature.", "AI": {"tldr": "研究引入了MISS-QA基准测试来评估模型解释科学文献图表的能力，并评估了18个多模态模型，结果显示模型与人类专家之间存在显著差距。", "motivation": "研究的动机是评估和改进现有模型在解释科学文献图表中的表现，特别是在多模态理解和信息查询方面的能力。", "method": "该研究使用了专门设计的MISS-QA基准测试，其中包括大量标注的例子和多种多模态模型的性能评估。", "result": "该研究引入了MISS-QA，这是一个专门用于评估模型解释科学文献中图表能力的基准。MISS-QA包含1,500个经过专家标注的例子，涵盖了465篇科学论文。在这个基准测试中，模型的任务是解释展示研究概览的图表并回答相关的信息查询问题。研究评估了18个前沿的多模态基础模型，包括o4-mini、Gemini-2.5-Flash和Qwen2.5-VL。结果显示这些模型与人类专家在MISS-QA上的表现存在显著差距。对无法回答的问题和详细的错误分析进一步突显了当前模型的优势和局限性，为提高模型理解多模态科学文献的能力提供了关键见解。", "conclusion": "研究发现现有的多模态模型在解释科学文献中的图表方面仍然存在显著的性能差距，人类专家的表现明显优于模型。研究还揭示了模型在处理特定类型任务时的优势和局限性。"}}
{"id": "2507.10935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10935", "abs": "https://arxiv.org/abs/2507.10935", "authors": ["Shaowen Tong", "Zimin Xia", "Alexandre Alahi", "Xuming He", "Yujiao Shi"], "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization", "comment": "accepted by ICCV2025", "summary": "Cross-view localization, the task of estimating a camera's\n3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with\nsatellite images, is crucial for large-scale outdoor applications like\nautonomous navigation and augmented reality. Existing methods often rely on\nfully supervised learning, which requires costly ground-truth pose annotations.\nIn this work, we propose GeoDistill, a Geometry guided weakly supervised self\ndistillation framework that uses teacher-student learning with Field-of-View\n(FoV)-based masking to enhance local feature learning for robust cross-view\nlocalization. In GeoDistill, the teacher model localizes a panoramic image,\nwhile the student model predicts locations from a limited FoV counterpart\ncreated by FoV-based masking. By aligning the student's predictions with those\nof the teacher, the student focuses on key features like lane lines and ignores\ntextureless regions, such as roads. This results in more accurate predictions\nand reduced uncertainty, regardless of whether the query images are panoramas\nor limited FoV images. Our experiments show that GeoDistill significantly\nimproves localization performance across different frameworks. Additionally, we\nintroduce a novel orientation estimation network that predicts relative\norientation without requiring precise planar position ground truth. GeoDistill\nprovides a scalable and efficient solution for real-world cross-view\nlocalization challenges. Code and model can be found at\nhttps://github.com/tongshw/GeoDistill.", "AI": {"tldr": "GeoDistill是一种几何引导的弱监督自蒸馏框架，通过FoV-based masking实现跨视角定位，提高定位精度并减少不确定性，适用于全景图和有限视角图像。", "motivation": "现有的方法通常依赖于需要昂贵的地面真实姿态注释的全监督学习。GeoDistill旨在解决这一问题，通过自蒸馏框架来提高跨视角定位的性能。", "method": "GeoDistill框架通过教师-学生学习的方式，并利用Field-of-View (FoV)-based masking方法来增强局部特征学习，以实现稳健的跨视角定位。教师模型定位全景图像，而学生模型预测通过FoV-based masking生成的有限视角图像的位置。", "result": "实验结果表明，GeoDistill在不同的框架中显著提高了定位性能。同时，还引入了一种新的方向估计网络，可以在无需精确平面位置真实值情况下预测相对方向。", "conclusion": "GeoDistill提供了一种可扩展且高效的解决实际跨视角定位挑战的方法。"}}
{"id": "2507.10810", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.10810", "abs": "https://arxiv.org/abs/2507.10810", "authors": ["David M. Markowitz", "Samuel Hardman Taylor"], "title": "Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler", "comment": null, "summary": "In this paper, we explored how online hate is motivated by receiving social\napproval from others. We specifically examined two central tenets of Walther's\n(2024) social approval theory of online hate: (H1a) more signals of social\napproval on hate messages predicts more subsequent hate messages, and (H1b) as\nsocial approval increases, hate speech messages become more extreme. Using over\n110 million posts from Parler (2018-2021), we observed that the number of\nupvotes a person received on a hate speech post was unassociated with the\namount of hate speech in their next post and posts during the next week, month,\nthree months, and six months. Between-person effects revealed an average\nnegative relationship between social approval and hate speech production at the\npost level, but this relationship was mixed at other time intervals. Social\napproval reinforcement mechanisms of online hate may operate differently on\nniche social media platforms.", "AI": {"tldr": "The study tested the link between social approval and online hate by examining posts from Parler and found mixed results, indicating different dynamics on niche platforms.", "motivation": "To investigate how receiving social approval influences the creation and escalation of online hate messages based on Walther's (2024) social approval theory.", "method": "We analyzed over 110 million posts from Parler (2018-2021) to test the relationship between social approval and online hate speech.", "result": "The number of upvotes on hate speech posts did not predict an increase in hate speech in future posts. The relationship between social approval and hate speech was mixed over different time intervals.", "conclusion": "Social approval reinforcement mechanisms of online hate may function differently on niche social media platforms like Parler compared to the predictions of Walther's (2024) social approval theory."}}
{"id": "2507.10938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10938", "abs": "https://arxiv.org/abs/2507.10938", "authors": ["Zhengyi Xu", "Haoran Wu", "Wen Jiang", "Jie Geng"], "title": "Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing", "comment": null, "summary": "Semantic change detection (SCD) extends the binary change detection task to\nprovide not only the change locations but also the detailed \"from-to\"\ncategories in multi-temporal remote sensing data. Such detailed semantic\ninsights into changes offer considerable advantages for a wide array of\napplications. However, since SCD involves the simultaneous optimization of\nmultiple tasks, the model is prone to negative transfer due to task-specific\nlearning difficulties and conflicting gradient flows. To address this issue, we\npropose Graph Aggregation Prototype Learning for Semantic Change Detection in\nremote sensing(GAPL-SCD). In this framework, a multi-task joint optimization\nmethod is designed to optimize the primary task of semantic segmentation and\nchange detection, along with the auxiliary task of graph aggregation prototype\nlearning. Adaptive weight allocation and gradient rotation methods are used to\nalleviate the conflict between training tasks and improve multi-task learning\ncapabilities. Specifically, the graph aggregation prototype learning module\nconstructs an interaction graph using high-level features. Prototypes serve as\nclass proxies, enabling category-level domain alignment across time points and\nreducing interference from irrelevant changes. Additionally, the proposed\nself-query multi-level feature interaction and bi-temporal feature fusion\nmodules further enhance multi-scale feature representation, improving\nperformance in complex scenes. Experimental results on the SECOND and\nLandsat-SCD datasets demonstrate that our method achieves state-of-the-art\nperformance, with significant improvements in accuracy and robustness for SCD\ntask.", "AI": {"tldr": "A method called GAPL-SCD is developed to enhance semantic change detection in remote sensing data by optimizing multiple tasks and using graph aggregation to improve category-level domain alignment, achieving state-of-the-art performance in accuracy and robustness.", "motivation": "The motivation is to overcome the challenge of negative transfer in multi-task learning for semantic change detection in remote sensing data, providing a more robust and accurate method for detecting and categorizing changes in a detailed manner.", "method": "Graph Aggregation Prototype Learning for Semantic Change Detection (GAPL-SCD) is proposed to address the issue of negative transfer in multi-task learning for SCD. It involves optimizing semantic segmentation and change detection alongside a graph aggregation prototype learning module, which constructs an interaction graph and uses prototypes for category-level domain alignment.", "result": "Experimental results on the SECOND and Landsat-SCD datasets show state-of-the-art performance, with significant improvements in accuracy and robustness for the SCD task.", "conclusion": "The proposed method, GAPL-SCD, improves multi-task learning capabilities in semantic change detection by effectively combining semantic segmentation with graph aggregation prototype learning, leading to better performance in complex scenes."}}
{"id": "2507.10852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10852", "abs": "https://arxiv.org/abs/2507.10852", "authors": ["Yiran Hu", "Zongyue Xue", "Haitao Li", "Siyuan Zheng", "Qingjing Chen", "Shaochun Wang", "Xihan Zhang", "Ning Zheng", "Yun Liu", "Qingyao Ai", "Yiqun Liu", "Charles L. A. Clarke", "Weixing Shen"], "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in high-stakes fields\nwhere their decisions impact rights and equity. However, LLMs' judicial\nfairness and implications for social justice remain underexplored. When LLMs\nact as judges, the ability to fairly resolve judicial issues is a prerequisite\nto ensure their trustworthiness. Based on theories of judicial fairness, we\nconstruct a comprehensive framework to measure LLM fairness, leading to a\nselection of 65 labels and 161 corresponding values. Applying this framework to\nthe judicial system, we compile an extensive dataset, JudiFair, comprising\n177,100 unique case facts. To achieve robust statistical inference, we develop\nthree evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and\nintroduce a method to assess the overall fairness of multiple LLMs across\nvarious labels. Through experiments with 16 LLMs, we uncover pervasive\ninconsistency, bias, and imbalanced inaccuracy across models, underscoring\nsevere LLM judicial unfairness. Particularly, LLMs display notably more\npronounced biases on demographic labels, with slightly less bias on substance\nlabels compared to procedure ones. Interestingly, increased inconsistency\ncorrelates with reduced biases, but more accurate predictions exacerbate\nbiases. While we find that adjusting the temperature parameter can influence\nLLM fairness, model size, release date, and country of origin do not exhibit\nsignificant effects on judicial fairness. Accordingly, we introduce a publicly\navailable toolkit containing all datasets and code, designed to support future\nresearch in evaluating and improving LLM fairness.", "AI": {"tldr": "研究构建了一套评估LLM公平性的框架和数据集JudiFair，揭示了LLM在司法判断中的公平性问题，公布了支持未来研究的工具包。", "motivation": "LLM在影响权利和公平的高风险领域应用越来越广泛，但是其司法公平性及对社会正义的影响尚未得到充分研究。本文旨在解决这个问题。", "method": "基于司法公平理论，构建了一个全面的框架来衡量LLM的公平性，选择了65个标签及161个对应值，针对司法系统收集了包含177,100个独特案件事实的数据集JudiFair，并开发了3个评估指标：不一致度、偏见和不平衡不准确性。", "result": "实验揭示了LLM在司法判断中的普遍不一致、偏见和不平衡不准确性，且在对人口统计标签上的偏见尤其明显，而相较于程序标签，物质标签上的偏见稍小。温度参数的调整对LLM的公平性有影响，但模型大小、发布时间、国家起源等对司法公平性没有显著影响。", "conclusion": "提出了一个全面的评估框架和数据集来衡量和改善LLM的司法公平性，并指出温度参数调整可能改善其公平性，同时也强调了未来需要更多研究来进一步优化LLM。"}}
{"id": "2507.10943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10943", "abs": "https://arxiv.org/abs/2507.10943", "authors": ["Yushun Fang", "Lu Liu", "Xiang Gao", "Qiang Hu", "Ning Cao", "Jianghe Cui", "Gang Chen", "Xiaoyun Zhang"], "title": "Robust ID-Specific Face Restoration via Alignment Learning", "comment": "17 pages, 8 figures", "summary": "The latest developments in Face Restoration have yielded significant\nadvancements in visual quality through the utilization of diverse diffusion\npriors. Nevertheless, the uncertainty of face identity introduced by\nidentity-obscure inputs and stochastic generative processes remains unresolved.\nTo address this challenge, we present Robust ID-Specific Face Restoration\n(RIDFR), a novel ID-specific face restoration framework based on diffusion\nmodels. Specifically, RIDFR leverages a pre-trained diffusion model in\nconjunction with two parallel conditioning modules. The Content Injection\nModule inputs the severely degraded image, while the Identity Injection Module\nintegrates the specific identity from a given image. Subsequently, RIDFR\nincorporates Alignment Learning, which aligns the restoration results from\nmultiple references with the same identity in order to suppress the\ninterference of ID-irrelevant face semantics (e.g. pose, expression, make-up,\nhair style). Experiments demonstrate that our framework outperforms the\nstate-of-the-art methods, reconstructing high-quality ID-specific results with\nhigh identity fidelity and demonstrating strong robustness.", "AI": {"tldr": "RIDFR is a novel ID-specific face restoration framework using diffusion models that improves identity fidelity and robustness in face restoration.", "motivation": "To address the uncertainty of face identity in face restoration due to identity-obscure inputs and stochastic generative processes.", "method": "Uses a pre-trained diffusion model with two parallel conditioning modules for content and identity injection, and Alignment Learning to suppress ID-irrelevant face semantics.", "result": "Experiments show high-quality ID-specific face restoration with high identity fidelity and strong robustness, outperforming state-of-the-art methods.", "conclusion": "RIDFR demonstrates significant improvements in face restoration, providing a robust solution for preserving specific identities in severely degraded images."}}
{"id": "2507.10918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10918", "abs": "https://arxiv.org/abs/2507.10918", "authors": ["Ikumi Numaya", "Shoji Moriya", "Shiki Sato", "Reina Akama", "Jun Suzuki"], "title": "How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations", "comment": "Accepted to SIGDIAL 2025 (long)", "summary": "Recent advancements in dialogue generation have broadened the scope of\nhuman-bot interactions, enabling not only contextually appropriate responses\nbut also the analysis of human affect and sensitivity. While prior work has\nsuggested that stylistic similarity between user and system may enhance user\nimpressions, the distinction between subjective and objective similarity is\noften overlooked. To investigate this issue, we introduce a novel dataset that\nincludes users' preferences, subjective stylistic similarity based on users'\nown perceptions, and objective stylistic similarity annotated by third party\nevaluators in open-domain dialogue settings. Analysis using the constructed\ndataset reveals a strong positive correlation between subjective stylistic\nsimilarity and user preference. Furthermore, our analysis suggests an important\nfinding: users' subjective stylistic similarity differs from third party\nobjective similarity. This underscores the importance of distinguishing between\nsubjective and objective evaluations and understanding the distinct aspects\neach captures when analyzing the relationship between stylistic similarity and\nuser preferences. The dataset presented in this paper is available online.", "AI": {"tldr": "研究发现用户偏好的对话风格主观相似性与第三方评定的客观相似性存在差异，强调了在分析风格相似性与用户偏好时区分这两种评估的重要性。", "motivation": "探讨风格相似性（主观与客观）与用户偏好的关系，以及它们在人机对话中的作用。", "method": "构建了一个包含用户偏好、用户自我感知的主观风格相似性和第三方评定的客观风格相似性的数据集，进行了相关分析。", "result": "分析结果发现了用户对主观风格相似性的偏好与第三方评估的客观相似性存在差异。", "conclusion": "这一发现突显了在分析风格相似性与用户偏好时区分主观和客观评估的重要性。"}}
{"id": "2507.10969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10969", "abs": "https://arxiv.org/abs/2507.10969", "authors": ["Palash Ray", "Mahuya Sasmal", "Asish Bera"], "title": "Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data", "comment": null, "summary": "Sports action classification representing complex body postures and\nplayer-object interactions is an emerging area in image-based sports analysis.\nSome works have contributed to automated sports action recognition using\nmachine learning techniques over the past decades. However, sufficient image\ndatasets representing women sports actions with enough intra- and inter-class\nvariations are not available to the researchers. To overcome this limitation,\nthis work presents a new dataset named WomenSports for women sports\nclassification using small-scale training data. This dataset includes a variety\nof sports activities, covering wide variations in movements, environments, and\ninteractions among players. In addition, this study proposes a convolutional\nneural network (CNN) for deep feature extraction. A channel attention scheme\nupon local contextual regions is applied to refine and enhance feature\nrepresentation. The experiments are carried out on three different sports\ndatasets and one dance dataset for generalizing the proposed algorithm, and the\nperformances on these datasets are noteworthy. The deep learning method\nachieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed\nWomenSports dataset, which is publicly available for research at Mendeley Data.", "AI": {"tldr": "本研究创建了WomenSports数据集，并提出了一种利用通道注意力机制的CNN模型来改进女性体育动作分类。在该数据集上，达到了89.15%的分类准确率。", "motivation": "为了克服现有研究中缺乏足够表示女性体育动作且包含足够类间和类内变异性图像数据集的限制，本研究旨在开发新的WomenSports数据集来实现基于小样本训练数据的女性体育动作分类。", "method": "本研究提出了一种用于深度特征提取的卷积神经网络(CNN)，并通过局部上下文区域的通道注意力机制来优化和增强特征表示。", "result": "实验在三个不同的体育数据集和一个舞蹈数据集上进行，结果表明该算法具有良好的泛化能力。使用ResNet-50在提出的WomenSports数据集上达到了89.15%的Top-1分类准确率。", "conclusion": "提出的WomenSports数据集和基于CNN的通道注意力方案显示出在女性体育动作分类方面的有效性和先进性。"}}
{"id": "2507.10920", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10920", "abs": "https://arxiv.org/abs/2507.10920", "authors": ["Seungho Choi"], "title": "HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training", "comment": null, "summary": "Large language models (LLMs) often show poor performance in low-resource\nlanguages like Korean, partly due to unique linguistic challenges such as\nhomophonous Sino-Korean words that are indistinguishable in Hangul script. To\naddress this semantic ambiguity, we propose HanjaBridge, a novel\nmeaning-injection technique integrated into a continual pre-training (CPT)\nframework. Instead of deterministically mapping a word to a single Hanja\n(Chinese character), HanjaBridge presents the model with all possible Hanja\ncandidates for a given homograph, encouraging the model to learn contextual\ndisambiguation. This process is paired with token-level knowledge distillation\nto prevent catastrophic forgetting. Experimental results show that HanjaBridge\nsignificantly improves Korean language understanding, achieving a 21\\% relative\nimprovement on the KoBALT benchmark. Notably, by reinforcing semantic alignment\nbetween Korean and Chinese through shared Hanja, we observe a strong positive\ncross-lingual transfer. Furthermore, these gains persist even when Hanja\naugmentation is omitted at inference time, ensuring practical efficiency with\nno additional run-time cost.", "AI": {"tldr": "为了解决低资源语言韩语中的语义模糊问题，本文提出了一种名为HanjaBridge的新技术，通过在持续预训练框架中提供所有可能的汉字候选来增强模型的上下文消歧能力。实验表明，这种技术显著提高了韩语模型的理解能力和跨语言迁移效果。", "motivation": "解决大型语言模型在像韩语这样的低资源语言中表现不佳的问题，尤其是由于音同形异的韩汉词汇在韩文书写系统中无法区分而造成的语义模糊问题。", "method": "提出HanjaBridge，这是一种新的意义注入技术，集成在持续预训练(CPT)框架中。HanjaBridge不是将一个词确定地映射到单一的汉字，而是为给定的多义词展示所有可能的汉字候选，鼓励模型学习上下文消歧。此过程与基于标记的知识蒸馏相结合，以防止灾难性 forgetting.", "result": "实验结果显示，HanjaBridge显著提高了韩语理解能力，在KoBALT基准测试中实现了21%的相对改进。通过加强韩语和汉语之间通过共享汉字的语义对齐，发现有强烈的跨语言积极迁移。这些改进在推理时即使忽略汉字增强也得以持续，确保了实际应用中的高效性，且没有任何额外的运行时间成本。", "conclusion": "通过集成HanjaBridge技术，可以有效解决低资源语言韩语中的语义模糊问题，提高韩语模型的理解和跨语言迁移能力。"}}
{"id": "2507.10977", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10977", "abs": "https://arxiv.org/abs/2507.10977", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "Human-object interaction (HOI) detection is essential for accurately\nlocalizing and characterizing interactions between humans and objects,\nproviding a comprehensive understanding of complex visual scenes across various\ndomains. However, existing HOI detectors often struggle to deliver reliable\npredictions efficiently, relying on resource-intensive training methods and\ninefficient architectures. To address these challenges, we conceptualize a\nwavelet attention-like backbone and a novel ray-based encoder architecture\ntailored for HOI detection. Our wavelet backbone addresses the limitations of\nexpressing middle-order interactions by aggregating discriminative features\nfrom the low- and high-order interactions extracted from diverse convolutional\nfilters. Concurrently, the ray-based encoder facilitates multi-scale attention\nby optimizing the focus of the decoder on relevant regions of interest and\nmitigating computational overhead. As a result of harnessing the attenuated\nintensity of learnable ray origins, our decoder aligns query embeddings with\nemphasized regions of interest for accurate predictions. Experimental results\non benchmark datasets, including ImageNet and HICO-DET, showcase the potential\nof our proposed architecture. The code is publicly available at\n[https://github.com/henry-pay/RayEncoder].", "AI": {"tldr": "The paper presents an efficient HOI detection method using a wavelet attention backbone and ray-based encoder, achieving better performance on standard datasets.", "motivation": "The motivation is to improve the efficiency and accuracy of HOI detection by addressing the limitations of existing methods that are inefficient and resource-intensive.", "method": "The paper introduces a wavelet attention-like backbone and a ray-based encoder architecture for HOI detection. The wavelet backbone aggregates discriminative features from interactions of different orders, while the ray-based encoder optimizes attention on relevant regions to reduce computational load.", "result": "The proposed architecture demonstrates promising results on benchmark datasets like ImageNet and HICO-DET.", "conclusion": "The wavelet attention and ray-based encoder show potential in enhancing HOI detection performance while reducing computational costs."}}
{"id": "2507.10957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10957", "abs": "https://arxiv.org/abs/2507.10957", "authors": ["Kalit Inani", "Keshav Kabra", "Vijay Marupudi", "Sashank Varma"], "title": "Modeling Understanding of Story-Based Analogies Using Large Language Models", "comment": "To appear at CogSci 2025", "summary": "Recent advancements in Large Language Models (LLMs) have brought them closer\nto matching human cognition across a variety of tasks. How well do these models\nalign with human performance in detecting and mapping analogies? Prior research\nhas shown that LLMs can extract similarities from analogy problems but lack\nrobust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the\ncurrent study focused on a story-based analogical mapping task and conducted a\nfine-grained evaluation of LLM reasoning abilities compared to human\nperformance. First, it explored the semantic representation of analogies in\nLLMs, using sentence embeddings to assess whether they capture the similarity\nbetween the source and target texts of an analogy, and the dissimilarity\nbetween the source and distractor texts. Second, it investigated the\neffectiveness of explicitly prompting LLMs to explain analogies. Throughout, we\nexamine whether LLMs exhibit similar performance profiles to those observed in\nhumans by evaluating their reasoning at the level of individual analogies, and\nnot just at the level of overall accuracy (as prior studies have done). Our\nexperiments include evaluating the impact of model size (8B vs. 70B parameters)\nand performance variation across state-of-the-art model architectures such as\nGPT-4 and LLaMA3. This work advances our understanding of the analogical\nreasoning abilities of LLMs and their potential as models of human reasoning.", "AI": {"tldr": "研究通过故事为基础的类比映射任务，评估大型语言模型的类比推理能力，特别是模型大小和架构对性能的影响。结果表明，LLMs在类比推理方面具有一定的人类推理模型潜力。", "motivation": "该研究旨在探索大型语言模型与人类性能在检测和映射类比方面的对齐程度。先前的研究表明，尽管LLMs能够从类比问题中提取相似性，但它们缺乏稳健的人类样化推理能力。", "method": "该研究采用了故事为基础的类比映射任务，通过句子嵌入评估大型语言模型对类比源文本和目标文本相似性的捕捉能力，以及它们对源文本和干扰文本之间差异性的理解。此外，还探讨了通过显式提示改进LLMs解释类比能力的效果，并评估了模型大小(8B vs. 70B参数)和不同先进模型架构（如GPT-4和LLaMA3）之间的表现差异。", "result": "通过对句子嵌入的评估，该研究发现LLMs在捕捉类比中的相似性和捕捉源文本与干扰文本之间的差异方面具有一定的能力。此外，研究评估了大型语言模型在不同模型大小和架构条件下的表现差异。", "conclusion": "该研究推进了我们对大型语言模型类比推理能力的理解，并展示了它们作为人类推理模型的潜在能力。尽管存在一些限制，但LLMs在类比推理方面展示了与人类类似的性能特征。"}}
{"id": "2507.10978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10978", "abs": "https://arxiv.org/abs/2507.10978", "authors": ["Ayush Gupta", "Siyuan Huang", "Rama Chellappa"], "title": "Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction", "comment": "Accepted at IJCB 2025", "summary": "Gait is becoming popular as a method of person re-identification because of\nits ability to identify people at a distance. However, most current works in\ngait recognition do not address the practical problem of occlusions. Among\nthose which do, some require paired tuples of occluded and holistic sequences,\nwhich are impractical to collect in the real world. Further, these approaches\nwork on occlusions but fail to retain performance on holistic inputs. To\naddress these challenges, we propose RG-Gait, a method for residual correction\nfor occluded gait recognition with holistic retention. We model the problem as\na residual learning task, conceptualizing the occluded gait signature as a\nresidual deviation from the holistic gait representation. Our proposed network\nadaptively integrates the learned residual, significantly improving performance\non occluded gait sequences without compromising the holistic recognition\naccuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR\ndatasets and show that learning the residual can be an effective technique to\ntackle occluded gait recognition with holistic retention.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10958", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10958", "abs": "https://arxiv.org/abs/2507.10958", "authors": ["Anthony Miyaguchi", "David Guecha", "Yuwen Chiu", "Sidharth Gaur"], "title": "DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models", "comment": null, "summary": "This Working Note summarizes the participation of the DS@GT team in two eRisk\n2025 challenges. For the Pilot Task on conversational depression detection with\nlarge language-models (LLMs), we adopted a prompt-engineering strategy in which\ndiverse LLMs conducted BDI-II-based assessments and produced structured JSON\noutputs. Because ground-truth labels were unavailable, we evaluated cross-model\nagreement and internal consistency. Our prompt design methodology aligned model\noutputs with BDI-II criteria and enabled the analysis of conversational cues\nthat influenced the prediction of symptoms. Our best submission, second on the\nofficial leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.", "AI": {"tldr": "本文总结了DS@GT团队在eRisk 2025挑战中的参与情况，特别是在对话抑郁症的LLMs方面取得了较好的成绩。", "motivation": "由于没有可用的真实标签，我们评估了跨模型的一致性和内部一致性。", "method": "采用了prompt工程策略，不同的大语言模型（LLMs）进行了BDI-II基于的评估并生成了结构化的JSON输出。", "result": "我们在官方排行榜上获得了第二名，其指标为DCHR = 0.50，ADODL = 0.89，ASHR = 0.27。", "conclusion": "我们的prompt设计方法使模型输出与BDI-II标准保持一致，并能够分析影响症状预测的对话线索。"}}
{"id": "2507.10999", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10999", "abs": "https://arxiv.org/abs/2507.10999", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "The resurgence of convolutional neural networks (CNNs) in visual recognition\ntasks, exemplified by ConvNeXt, has demonstrated their capability to rival\ntransformer-based architectures through advanced training methodologies and\nViT-inspired design principles. However, both CNNs and transformers exhibit a\nsimplicity bias, favoring straightforward features over complex structural\nrepresentations. Furthermore, modern CNNs often integrate MLP-like blocks akin\nto those in transformers, but these blocks suffer from significant information\nredundancies, necessitating high expansion ratios to sustain competitive\nperformance. To address these limitations, we propose SpaRTAN, a lightweight\narchitectural design that enhances spatial and channel-wise information\nprocessing. SpaRTAN employs kernels with varying receptive fields, controlled\nby kernel size and dilation factor, to capture discriminative multi-order\nspatial features effectively. A wave-based channel aggregation module further\nmodulates and reinforces pixel interactions, mitigating channel-wise\nredundancies. Combining the two modules, the proposed network can efficiently\ngather and dynamically contextualize discriminative features. Experimental\nresults in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable\nparameter efficiency while maintaining competitive performance. In particular,\non the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M\nparameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver\nstrong performance through an efficient design. On the COCO benchmark, it\nachieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M\nparameters. The code is publicly available at\n[https://github.com/henry-pay/SpaRTAN].", "AI": {"tldr": "SpaRTAN, a novel, efficient CNN architecture, outperforms existing models by effectively integrating varying receptive fields and wave-based channel aggregation, reducing redundancy and enhancing spatial and channel-wise information.", "motivation": "The motivation is to address the simplicity bias and information redundancy issues found in modern CNNs and transformer architectures, while improving their efficiency and performance.", "method": "The method involves a lightweight architecture named SpaRTAN, which employs kernels with varying receptive fields and a wave-based channel aggregation module to enhance information processing and reduce redundancy.", "result": "On the ImageNet-1k benchmark, SpaRTAN achieved 77.7% accuracy with 3.8M parameters and 1.0 GFLOPs. On the COCO benchmark, it achieved 50.0% AP, outperforming previous models with fewer parameters.", "conclusion": "The study concludes that SPAARTN efficiently gathers and contextualizes discriminative features, achieving remarkable performance with fewer parameters compared to existing architectures, demonstrating the potential of its design approach."}}
{"id": "2507.10972", "categories": ["cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.10972", "abs": "https://arxiv.org/abs/2507.10972", "authors": ["Zhaoyi An", "Rei Kawakami"], "title": "Teach Me Sign: Stepwise Prompting LLM for Sign Language Production", "comment": "Accepted by IEEE ICIP 2025", "summary": "Large language models, with their strong reasoning ability and rich\nknowledge, have brought revolution to many tasks of AI, but their impact on\nsign language generation remains limited due to its complexity and unique\nrules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign\nlanguage as another natural language. By fine-tuning an LLM, we enable it to\nlearn the correspondence between text and sign language, and facilitate\ngeneration. Considering the differences between sign and spoken language, we\nemploy a stepwise prompting strategy to extract the inherent sign language\nknowledge within the LLM, thereby supporting the learning and generation\nprocess. Experimental results on How2Sign and Phoenix14T datasets demonstrate\nthat our approach effectively leverages both the sign language knowledge and\nreasoning capabilities of LLM to align the different distribution and\ngrammatical rules between sign and spoken language.", "AI": {"tldr": "提出TEAch Me Sign（TEAM-Sign）通过微调LLM学习文本与手语间的对应关系，采用逐步提示策略进行手语知识的提取与生成，实验显示可以有效处理手语和口语间的规则差异。", "motivation": "尽管大型语言模型在许多AI任务上带来了革命性的变化，但它们对手语生成的影响仍然有限，主要原因在于手语的复杂性和独特规则。", "method": "通过微调大型语言模型（LLM），将其视为另一种自然语言，从而实现文本与手语间的对应关系学习与生成。考虑到手语和口语之间的差异，采用逐步提示策略提取LLM中的内在手语知识，以支持学习和生成过程。", "result": "在How2Sign和Phoenix14T数据集上的实验结果显示，我们的方法能有效利用LLM中的手语知识和推理能力，以协调手语与口语间不同的分布和语法规则。", "conclusion": "我们的方法在手语生成任务上展现出了显著效果，特别是在涉及手语与口语之间差异的情况下。"}}
{"id": "2507.11003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11003", "abs": "https://arxiv.org/abs/2507.11003", "authors": ["Yuhu Bai", "Jiangning Zhang", "Yunkang Cao", "Guangyuan Lu", "Qingdong He", "Xiangtai Li", "Guanzhong Tian"], "title": "Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection", "comment": null, "summary": "With the advent of vision-language models (e.g., CLIP) in zero- and few-shot\nsettings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in\nrecent research, where the rare classes are essential and expected in many\napplications. This study introduces \\textbf{FiSeCLIP} for ZSAD with\ntraining-free \\textbf{CLIP}, combining the feature matching with the\ncross-modal alignment. Testing with the entire dataset is impractical, while\nbatch-based testing better aligns with real industrial needs, and images within\na batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes\nother images in the same batch as reference information for the current image.\nHowever, the lack of labels for these references can introduce ambiguity, we\napply text information to \\textbf{fi}lter out noisy features. In addition, we\nfurther explore CLIP's inherent potential to restore its local\n\\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection\ntasks to enable a more accurate filtering process. Our approach exhibits\nsuperior performance for both anomaly classification and segmentation on\nanomaly detection benchmarks, building a stronger baseline for the direction,\ne.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by\n+4.6\\%$\\uparrow$/+5.7\\%$\\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.", "AI": {"tldr": "FiSeCLIP enhances CLIP's capability for ZSAD by utilizing batch image referencing and text-based noise filtering, showing better performance in anomaly detection tasks compared to existing methods.", "motivation": "The motivation behind FiSeCLIP is to develop a more effective zero-shot anomaly detection mechanism that leverages the strengths of CLIP while addressing its limitations in handling rare classes and noisy data, making it applicable to industrial needs.", "method": "The method FiSeCLIP integrates CLIP's feature matching with cross-modal alignment, using other images in the same batch as reference points to address the challenges in zero-shot anomaly detection (ZSAD). To refine filters and enhance accuracy, it employs text information to filter out noise and recovers CLIP's local semantic correlation.", "result": "FiSeCLIP outperforms the state-of-the-art (SOTA) AdaCLIP on the MVTec-AD benchmark, demonstrating superior performance improvements of +4.6% and +5.7% in segmentation metrics (AU-ROC/$F_1$-max).", "conclusion": "The paper concludes that FiSeCLIP represents a significant advancement in zero-shot anomaly detection, providing a strong baseline for future research and applications."}}
{"id": "2507.10996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10996", "abs": "https://arxiv.org/abs/2507.10996", "authors": ["Lin Tian", "Johanne R. Trippas", "Marian-Andrei Rizoiu"], "title": "Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection", "comment": "12 pages, 5 tables, CLEF 2025", "summary": "This paper presents our approach to EXIST 2025 Task 1, addressing text-based\nsexism detection in English and Spanish tweets through hierarchical Low-Rank\nAdaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter\nrouting that explicitly models label dependencies across three hierarchically\nstructured subtasks: binary sexism identification, source intention detection,\nand multilabel sexism categorization. Unlike conventional LoRA applications\nthat target only attention layers, we apply adaptation to all linear\ntransformations, enhancing the model's capacity to capture task-specific\npatterns. In contrast to complex data processing and ensemble approaches, we\nshow that straightforward parameter-efficient fine-tuning achieves strong\nperformance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each\nsubtask using unified multilingual training that leverages Llama 3.1's native\nbilingual capabilities. The method requires minimal preprocessing and uses\nstandard supervised learning. Our multilingual training strategy eliminates the\nneed for separate language-specific models, achieving 1.7-2.4\\% F1 improvements\nthrough cross-lingual transfer. With only 1.67\\% trainable parameters compared\nto full fine-tuning, our approach reduces training time by 75\\% and model\nstorage by 98\\%, while achieving competitive performance across all subtasks\n(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,\n0.6519 for multilabel categorization).", "AI": {"tldr": "The paper describes a hierarchical Low-Rank Adaptation (LoRA) method for efficient sexism detection in English and Spanish tweets, achieving strong performance with minimal trainable parameters.", "motivation": "To develop an efficient and scalable approach for sexism detection in tweets across multiple languages, aiming to improve upon traditional methods that involve complex data processing and ensembles.", "method": "Our method introduces conditional adapter routing for hierarchical task adaptation using LoRA, targeting all linear transformations of Llama 3.1 8B, and applying unified multilingual training for English and Spanish sexism detection.", "result": "Achieved competitive performance with 1.67% trainable parameters, reducing training time by 75% and model storage by 98%, with F1 improvements through cross-lingual transfer.", "conclusion": "The hierarchical LoRA approach provides an effective and efficient method for sexism detection in tweets, demonstrating that simple parameter-efficient fine-tuning can be effective without complex data preprocessing or ensembles."}}
{"id": "2507.11015", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11015", "abs": "https://arxiv.org/abs/2507.11015", "authors": ["Zeyi Hou", "Zeqiang Wei", "Ruixin Yan", "Ning Lang", "Xiuzhuang Zhou"], "title": "Semantically Informed Salient Regions Guided Radiology Report Generation", "comment": null, "summary": "Recent advances in automated radiology report generation from chest X-rays\nusing deep learning algorithms have the potential to significantly reduce the\narduous workload of radiologists. However, due to the inherent massive data\nbias in radiology images, where abnormalities are typically subtle and sparsely\ndistributed, existing methods often produce fluent yet medically inaccurate\nreports, limiting their applicability in clinical practice. To address this\nissue effectively, we propose a Semantically Informed Salient Regions-guided\n(SISRNet) report generation method. Specifically, our approach explicitly\nidentifies salient regions with medically critical characteristics using\nfine-grained cross-modal semantics. Then, SISRNet systematically focuses on\nthese high-information regions during both image modeling and report\ngeneration, effectively capturing subtle abnormal findings, mitigating the\nnegative impact of data bias, and ultimately generating clinically accurate\nreports. Compared to its peers, SISRNet demonstrates superior performance on\nwidely used IU-Xray and MIMIC-CXR datasets.", "AI": {"tldr": "该研究提出了一种名为SISRNet的新方法，通过识别具有医学关键特征的显著区域，用于生成准确的胸部X光报告，从而解决现有方法中由于数据偏差导致的误报问题。在标准数据集上表现优于其它方法。", "motivation": "现有自动放射学报告生成方法存在因放射学图像中的大量数据偏差导致的医疗准确性问题，提出新方法来解决这一挑战。", "method": "Structure", "result": "{\"tldr\": \"该研究提出了一种名为SISRNet的新方法，通过识别具有医学关键特征的显著区域，用于生成准确的胸部X光报告，从而解决现有方法中由于数据偏差导致的误报问题。在标准数据集上表现优于其它方法。\", \"motivation\": \"现有自动放射学报告生成方法存在因放射学图像中的大量数据偏差导致的医疗准确性问题，提出新方法来解决这一挑战。\", \"method\": \"SISRNet通过细粒度的跨模态语义，明确识别出具有医学关键特征的显著区域，并在这两个阶段中专注于这些高信息量的区域。\", \"result\": \"相比其他方法，SISRNet在常用的IU-Xray和MIMIC-CXR数据集上展现出更优越的性能。\", \"conclusion\": \"SISRNet通过聚焦于高信息量区域，有效捕捉细微异常发现，生成具有临床准确性的报告，解决数据偏差问题。\"}", "conclusion": "SISRNet通过聚焦于高信息量区域，有效捕捉细微异常发现，生成具有临床准确性的报告，解决数据偏差问题。"}}
{"id": "2507.11004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11004", "abs": "https://arxiv.org/abs/2507.11004", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "title": "Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification", "comment": "ACL 2025 Workshop (FEVER)", "summary": "This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task\nat the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the\nbest-performing open-source model from the previous year's challenge. It\nimproves evidence quality through document summarization and answer\nreformulation, optimizes veracity prediction via post-training quantization\nunder computational constraints, and enhances overall system performance by\nintegrating updated language model (LM) backbones. HerO 2 ranked second on the\nleaderboard while achieving the shortest runtime among the top three systems,\ndemonstrating both high efficiency and strong potential for real-world fact\nverification. The code is available at https://github.com/ssu-humane/HerO2.", "AI": {"tldr": "论文介绍了一个名为HerO 2的系统，该系统在计算约束下通过文档摘要、答案重构和量化优化提升了事实验证性能，效率高且效果好。", "motivation": "为了提升去年挑战中最好的开源模型HerO的性能，研究团队开发了HerO 2，旨在改善证据质量、优化真实性预测以及提升系统效率。", "method": "HerO 2系统通过文档摘要和答案重构提升证据质量，通过量化优化模型以提高在计算约束下的真实性预测性能，并通过集成更新的语言模型来提高整体系统性能。", "result": "HerO 2在排行榜上位列第二，并且在前三个系统中的运行时间最短，展示了在现实世界的事实验证中的高效率和强大潜力。", "conclusion": "HerO 2展示了高效性和在实际应用中的潜力，证明了通过文档摘要、量化优化和集成更新的语言模型可以提升事实验证系统的性能。"}}
{"id": "2507.11025", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11025", "abs": "https://arxiv.org/abs/2507.11025", "authors": ["Sung Ho Kang", "Hyun-Cheol Park"], "title": "Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion", "comment": null, "summary": "We present a novel framework for CBCT-to-MDCT translation, grounded in the\nSchrodinger Bridge (SB) formulation, which integrates GAN-derived priors with\nhuman-guided conditional diffusion. Unlike conventional GANs or diffusion\nmodels, our approach explicitly enforces boundary consistency between CBCT\ninputs and pseudo targets, ensuring both anatomical fidelity and perceptual\ncontrollability. Binary human feedback is incorporated via classifier-free\nguidance (CFG), effectively steering the generative process toward clinically\npreferred outcomes. Through iterative refinement and tournament-based\npreference selection, the model internalizes human preferences without relying\non a reward model. Subtraction image visualizations reveal that the proposed\nmethod selectively attenuates shade artifacts in key anatomical regions while\npreserving fine structural detail. Quantitative evaluations further demonstrate\nsuperior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical\ndatasets -- outperforming prior GAN- and fine-tuning-based feedback methods --\nwhile requiring only 10 sampling steps. These findings underscore the\neffectiveness and efficiency of our framework for real-time, preference-aligned\nmedical image translation.", "AI": {"tldr": "A novel CBCT-to-MDCT translation framework based on the Schrodinger Bridge formulation outperforms existing methods, providing superior anatomical fidelity and fine control.", "motivation": "The motivation for this paper is to improve the quality and controllability of medical image translation from CBCT to MDCT, ensuring both anatomical fidelity and perceptual controllability.", "method": "We present a novel framework for CBCT-to-MDCT translation using the Schrodinger Bridge formulation that incorporates GAN-derived priors and human-guided conditional diffusion. This method enforces boundary consistency between CBCT inputs and pseudo targets and uses binary human feedback via classifier-free guidance.", "result": "The proposed method effectively selectively attenuates shade artifacts in key anatomical regions while preserving fine structural detail. It also outperforms prior GAN- and fine-tuning-based feedback methods on RMSE, SSIM, LPIPS, and Dice metrics.", "conclusion": "The paper concludes that the proposed framework demonstrates superior performance for real-time, preference-aligned medical image translation, with only 10 sampling steps required."}}
{"id": "2507.11049", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11049", "abs": "https://arxiv.org/abs/2507.11049", "authors": ["Dahyun Lee", "Jonghyeon Choi", "Jiyoung Han", "Kunwoo Park"], "title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection", "comment": "Preprint. 24 pages", "summary": "As online news consumption grows, personalized recommendation systems have\nbecome integral to digital journalism. However, these systems risk reinforcing\nfilter bubbles and political polarization by failing to incorporate diverse\nperspectives. Stance detection -- identifying a text's position on a target --\ncan help mitigate this by enabling viewpoint-aware recommendations and\ndata-driven analyses of media bias. Yet, existing stance detection research\nremains largely limited to short texts and high-resource languages. To address\nthese gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for\narticle-level stance detection, comprising 2,000 news articles with\narticle-level and 19,650 segment-level stance annotations across 47 societal\nissues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided\n\\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that\nemploys a language model agent to predict the stances of key structural\nsegments (e.g., leads, quotes), which are then aggregated to infer the overall\narticle stance. Experiments show that \\textsc{JoA-ICL} outperforms existing\nstance detection methods, highlighting the benefits of segment-level agency in\ncapturing the overall position of long-form news articles. Two case studies\nfurther demonstrate its broader utility in promoting viewpoint diversity in\nnews recommendations and uncovering patterns of media bias.", "AI": {"tldr": "研究提出了JoA-ICL框架，用于处理长篇新闻文章的立场检测问题，并展示了其在促进新闻推荐观点多样性和揭示媒体偏见方面的应用价值。", "motivation": "随着在线新闻消费的增长，个性化推荐系统已成为数字新闻报道不可或缺的一部分。然而，这些系统存在加剧过滤气泡和政治极化的风险，因为它们未能纳入多元视角。为了缓解这些问题，该研究通过引入K-News-Stance数据集来填补现有立场检测研究的空白，这个数据集包含了针对2000篇新闻文章的立场标注。", "method": "本研究提出了一个名为JoA-ICL的框架，该框架利用语言模型代理来预测文章中关键结构部分（例如引言、引语）的观点，并将其聚合以推断整篇文章的观点。该框架用于处理长形式新闻文章中的立场检测问题。", "result": "实验表明，JoA-ICL在立场检测方面优于现有的方法，强调了分段代理在捕获长篇新闻文章整体立场上的优势。实验结果还通过两个案例研究进一步证明其在促进新闻推荐中的观点多样性及揭示媒体偏见方面的广泛应用价值。", "conclusion": "JoA-ICL在立场检测方面的优越性能及其在促进观点多样性和揭示媒体偏见方面的应用，表明该框架在长篇新闻文章中捕获整体立场的有效性。"}}
{"id": "2507.11030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11030", "abs": "https://arxiv.org/abs/2507.11030", "authors": ["Sunghyun Park", "Jungsoo Lee", "Shubhankar Borse", "Munawar Hayat", "Sungha Choi", "Kyuwoong Hwang", "Fatih Porikli"], "title": "Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation", "comment": "Accepted to ICCV 2025; 15 pages", "summary": "While open-vocabulary semantic segmentation (OVSS) can segment an image into\nsemantic regions based on arbitrarily given text descriptions even for classes\nunseen during training, it fails to understand personal texts (e.g., `my mug\ncup') for segmenting regions of specific interest to users. This paper\naddresses challenges like recognizing `my mug cup' among `multiple mug cups'.\nTo overcome this challenge, we introduce a novel task termed\n\\textit{personalized open-vocabulary semantic segmentation} and propose a text\nprompt tuning-based plug-in method designed to recognize personal visual\nconcepts using a few pairs of images and masks, while maintaining the\nperformance of the original OVSS. Based on the observation that reducing false\npredictions is essential when applying text prompt tuning to this task, our\nproposed method employs `negative mask proposal' that captures visual concepts\nother than the personalized concept. We further improve the performance by\nenriching the representation of text prompts by injecting visual embeddings of\nthe personal concept into them. This approach enhances personalized OVSS\nwithout compromising the original OVSS performance. We demonstrate the\nsuperiority of our method on our newly established benchmarks for this task,\nincluding FSS$^\\text{per}$, CUB$^\\text{per}$, and ADE$^\\text{per}$.", "AI": {"tldr": "本研究解决了OVSS无法准确识别个性化用户文本描述的问题，提出了个性化OVSS方法，通过负掩码提议和视觉嵌入增强了方法的性能，在新基准测试中表现优异。", "motivation": "开放词汇语义分割（OVSS）虽能分割基于任意文本描述的图像，但难以识别个性化文本（例如，用户特定的“我的马克杯”），尤其是当面对多个类似对象时。该研究动机在于解决这类个性化识别的挑战。", "method": "本研究提出了一种基于文本提示调优的插件方法，旨在通过少量图像和掩码对识别个性化视觉概念，同时保持原有开放词汇语义分割（OVSS）的性能。该方法通过引入“负掩码提议”来减少错误预测，并通过将个性化概念的视觉嵌入注入到文本提示中来丰富其表示，从而提高了个性化OVSS的性能。", "result": "实验表明，该方法在建立的新基准测试（包括FSS$^{\text{per}}$、CUB$^{\text{per}}$和ADE$^{\text{per}}$）中表现出色。", "conclusion": "研究结论是，通过引入“负掩码提议”和注入视觉嵌入到文本提示中，能够有效提升个性化OVSS的性能，同时保持传统OVSS的功能。实验验证了方法的有效性。"}}
{"id": "2507.11052", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11052", "abs": "https://arxiv.org/abs/2507.11052", "authors": ["Haowei Yang", "Ziyu Shen", "Junli Shao", "Luyao Men", "Xinyue Han", "Jing Dong"], "title": "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP", "comment": null, "summary": "Timely identification and accurate risk stratification of cardiovascular\ndisease (CVD) remain essential for reducing global mortality. While existing\nprediction models primarily leverage structured data, unstructured clinical\nnotes contain valuable early indicators. This study introduces a novel\nLLM-augmented clinical NLP pipeline that employs domain-adapted large language\nmodels for symptom extraction, contextual reasoning, and correlation from\nfree-text reports. Our approach integrates cardiovascular-specific fine-tuning,\nprompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III\nand CARDIO-NLP datasets demonstrate improved performance in precision, recall,\nF1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by\ncardiologists. Challenges such as contextual hallucination, which occurs when\nplausible information contracts with provided source, and temporal ambiguity,\nwhich is related with models struggling with chronological ordering of events\nare addressed using prompt engineering and hybrid rule-based verification. This\nwork underscores the potential of LLMs in clinical decision support systems\n(CDSS), advancing early warning systems and enhancing the translation of\npatient narratives into actionable risk assessments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.11035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11035", "abs": "https://arxiv.org/abs/2507.11035", "authors": ["Lirong Zheng", "Yanshan Li", "Rui Yu", "Kaihao Zhang"], "title": "Efficient Dual-domain Image Dehazing with Haze Prior Perception", "comment": "12 pages", "summary": "Transformer-based models exhibit strong global modeling capabilities in\nsingle-image dehazing, but their high computational cost limits real-time\napplicability. Existing methods predominantly rely on spatial-domain features\nto capture long-range dependencies, which are computationally expensive and\noften inadequate under complex haze conditions. While some approaches introduce\nfrequency-domain cues, the weak coupling between spatial and frequency branches\nlimits the overall performance. To overcome these limitations, we propose the\nDark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel\ndual-domain framework that performs physically guided degradation alignment\nacross spatial and frequency domains. At its core, the DGFDBlock comprises two\nkey modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a\npixel-level haze confidence map from dark channel priors to adaptively enhance\nhaze-relevant frequency components, thereby achieving global degradation-aware\nspectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which\nfuses multi-scale features through diverse convolutional kernels and hybrid\ngating mechanisms to recover fine structural details. Additionally, a Prior\nCorrection Guidance Branch (PCGB) incorporates a closed-loop feedback\nmechanism, enabling iterative refinement of the prior by intermediate dehazed\nfeatures and significantly improving haze localization accuracy, especially in\nchallenging outdoor scenes. Extensive experiments on four benchmark haze\ndatasets demonstrate that DGFDNet achieves state-of-the-art performance with\nsuperior robustness and real-time efficiency. Code is available at:\nhttps://github.com/Dilizlr/DGFDNet.", "AI": {"tldr": "DGFDNet is proposed to improve single-image dehazing by integrating spatial and frequency domain strategies, achieving strong performance with high efficiency.", "motivation": "The motivation is to develop a method for single-image dehazing that can effectively model long-range dependencies, handle complex haze conditions, and achieve real-time applicability while maintaining high performance.", "method": "Transformer-based models are criticized for their high computational cost in single-image dehazing. DGFDNet addresses this by introducing a dual-domain approach, using a Haze-Aware Frequency Modulator (HAFM) to generate a haze confidence map and adjust frequency components. It also includes a Multi-level Gating Aggregation Module (MGAM) for fusing features, and a Prior Correction Guidance Branch (PCGB) for iterative improvement of haze localization.", "result": "Extensive experiments on four benchmark datasets show that DGFDNet outperforms existing methods, achieving superior robustness and real-time efficiency.", "conclusion": "DGFDNet demonstrates state-of-the-art performance on benchmark haze datasets, with superior robustness and real-time efficiency, effectively overcoming limitations of spatial and frequency domain approaches."}}
{"id": "2507.11084", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11084", "abs": "https://arxiv.org/abs/2507.11084", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha"], "title": "Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach", "comment": "This paper has been accepted and presented at the IEEE ECAI 2025. The\n  final version will be available in the IEEE Xplore Digital Library", "summary": "The July Revolution in Bangladesh marked a significant student-led mass\nuprising, uniting people across the nation to demand justice, accountability,\nand systemic reform. Social media platforms played a pivotal role in amplifying\npublic sentiment and shaping discourse during this historic mass uprising. In\nthis study, we present a hybrid transformer-based sentiment analysis framework\nto decode public opinion expressed in social media comments during and after\nthe revolution. We used a brand new dataset of 4,200 Bangla comments collected\nfrom social media. The framework employs advanced transformer-based feature\nextraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the\nproposed hybrid XMB-BERT, to capture nuanced patterns in textual data.\nPrinciple Component Analysis (PCA) were utilized for dimensionality reduction\nto enhance computational efficiency. We explored eleven traditional and\nadvanced machine learning classifiers for identifying sentiments. The proposed\nhybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of\n83.7% and outperform other model classifier combinations. This study\nunderscores the potential of machine learning techniques to analyze social\nsentiment in low-resource languages like Bangla.", "AI": {"tldr": "该研究提出了一种基于变压器的混合情感分析框架，针对孟加拉语社会媒体评论进行情感分析，采用了包括XMB-BERT在内的多种方法，结合投票分类器实现了83.7%的高精度。", "motivation": "研究动机是在社会媒体上放大公众情绪和塑造话语的历史性群众运动中，特别是在孟加拉国七月革命期间和之后，分析公共意见。", "method": "该研究提出了一种基于变压器的混合情感分析框架，使用包括BanglaBERT、mBERT、XLM-RoBERTa以及提出的混合XMB-BERT在内的先进变压器特征抽取技术来捕获文本数据中的细微模式。同时采用了主成分分析（PCA）以提高计算效率，探讨了11种经典和先进的机器学习分类器用于情感识别。", "result": "提出的混合XMB-BERT结合投票分类器取得了83.7%的高精度，优于其他模型。", "conclusion": "这项研究表明了机器学习技术（特别是提出的混合XMB-BERT模型）分析低资源语言（如孟加拉语）中的社会情绪的潜力。"}}
{"id": "2507.11037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11037", "abs": "https://arxiv.org/abs/2507.11037", "authors": ["Jie-Wen Li", "Zi-Han Ye", "Qingyuan Zhou", "Jiayi Song", "Ying He", "Ben Fei", "Wen-Ming Chen"], "title": "A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion", "comment": "15 pages, 10 figures, 2 tables", "summary": "The kinematics analysis of foot-ankle complex during gait is essential for\nadvancing biomechanical research and clinical assessment. Collecting accurate\nsurface geometry data from the foot and ankle during dynamic gait conditions is\ninherently challenging due to swing foot occlusions and viewing limitations.\nThus, this paper introduces FootGait3D, a novel multi-view dataset of\nhigh-resolution ankle-foot surface point clouds captured during natural gait.\nDifferent from existing gait datasets that typically target whole-body or\nlower-limb motion, FootGait3D focuses specifically on the detailed modeling of\nthe ankle-foot region, offering a finer granularity of motion data. To address\nthis, FootGait3D consists of 8,403 point cloud frames collected from 46\nsubjects using a custom five-camera depth sensing system. Each frame includes a\ncomplete 5-view reconstruction of the foot and ankle (serving as ground truth)\nalong with partial point clouds obtained from only four, three, or two views.\nThis structured variation enables rigorous evaluation of 3D point cloud\ncompletion methods under varying occlusion levels and viewpoints. Our dataset\nis designed for shape completion tasks, facilitating the benchmarking of\nstate-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and\nmulti-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the\nchallenge of recovering the full foot geometry from occluded inputs. FootGait3D\nhas significant potential to advance research in biomechanics and multi-segment\nfoot modeling, offering a valuable testbed for clinical gait analysis,\nprosthetic design, and robotics applications requiring detailed 3D models of\nthe foot during motion. The dataset is now available at\nhttps://huggingface.co/datasets/ljw285/FootGait3D.", "AI": {"tldr": "本文介绍了一个新的多视角高分辨率三维足-踝点云数据集FootGait3D，用于在自然步态情况下评估三维点云建模方法，并为生物力学和机器人应用提供重要工具。", "motivation": "足踝复合体在步态中的运动学分析对于推进生物力学研究和临床评估至关重要，但采集动态步态下足踝区域的准确表面几何数据极具挑战性。此数据集旨在解决这一问题，提供细粒度的足踝运动数据。", "method": "本文介绍了FootGait3D，一个专注于足踝区域动态步态下的高分辨率三维点云数据集。数据集由8,403帧点云画面构成，采集自46名受试者，使用了自定义的五相机深度感知系统。每帧包括完整的五个视角重建画面（作为真实值）以及四种部分视角的点云。", "result": "FootGait3D提供了一个有结构的变异，可以评估在不同遮挡水平和视角下的三维点云完成方法。数据集适用于形状补全任务，可以用于现有方法的基准测试。", "conclusion": "这项研究的成果包含了一个新颖的数据集，该数据集有望推动足踝复合体步态的生物机械研究，为步态分析、假肢设计和需要精细三维模型的机器人应用建立了一个宝贵的测试平台。"}}
{"id": "2507.11086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11086", "abs": "https://arxiv.org/abs/2507.11086", "authors": ["Andres Azqueta-Gavaldón", "Joaquin Ramos Cosgrove"], "title": "Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification", "comment": null, "summary": "The growing prevalence of cross-border financial activities in global markets\nhas underscored the necessity of accurately identifying and classifying foreign\nentities. This practice is essential within the Spanish financial system for\nensuring robust risk management, regulatory adherence, and the prevention of\nfinancial misconduct. This process involves a labor-intensive entity-matching\ntask, where entities need to be validated against available reference sources.\nChallenges arise from linguistic variations, special characters, outdated\nnames, and changes in legal forms, complicating traditional matching algorithms\nlike Jaccard, cosine, and Levenshtein distances. These methods struggle with\ncontextual nuances and semantic relationships, leading to mismatches. To\naddress these limitations, we explore Large Language Models (LLMs) as a\nflexible alternative. LLMs leverage extensive training to interpret context,\nhandle abbreviations, and adapt to legal transitions. We evaluate traditional\nmethods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft\nCopilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.\nResults show traditional methods achieve accuracies over 92% but suffer high\nfalse positive rates (20-40%). Interface-based LLMs outperform, achieving\naccuracies above 93%, F1 scores exceeding 96%, and lower false positives\n(40-80%).", "AI": {"tldr": "The paper explores the use of Large Language Models (LLMs) to enhance entity-matching accuracy in cross-border financial activities, demonstrating improved performance over traditional methods.", "motivation": "The motivation is to improve the accuracy and reduce false positives in entity-matching tasks, which are critical for risk management, regulatory compliance, and fraud prevention in the Spanish financial system.", "method": "Content involves comparing traditional entity-matching algorithms (Jaccard, cosine, Levenshtein) with Large Language Models (LLMs) for improving the accuracy of entity-matching tasks in the context of cross-border financial activities.", "result": "The study finds that traditional methods have over 92% accuracy but high false positive rates (20-40%). Interface-based LLMs perform the best with accuracies above 93%, F1 scores over 96%, and false positive rates of 40-80%.", "conclusion": "The conclusion is that interface-based LLMs significantly outperform traditional methods in entity-matching tasks, offering a more accurate and reliable solution for the financial sector, particularly for foreign entity identification and classification."}}
{"id": "2507.11040", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11040", "abs": "https://arxiv.org/abs/2507.11040", "authors": ["Nicolas Drapier", "Aladine Chetouani", "Aurélien Chateigner"], "title": "Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery", "comment": "11 pages, 9 figures", "summary": "We present GLOD, a transformer-first architecture for object detection in\nhigh-resolution satellite imagery. GLOD replaces CNN backbones with a Swin\nTransformer for end-to-end feature extraction, combined with novel UpConvMixer\nblocks for robust upsampling and Fusion Blocks for multi-scale feature\nintegration. Our approach achieves 32.95\\% on xView, outperforming SOTA methods\nby 11.46\\%. Key innovations include asymmetric fusion with CBAM attention and a\nmulti-path head design capturing objects across scales. The architecture is\noptimized for satellite imagery challenges, leveraging spatial priors while\nmaintaining computational efficiency.", "AI": {"tldr": "GLOD是一个用于高分辨率卫星图像的基于Transformer的对象检测模型，相比现有的方法有显著的性能提升。", "motivation": "研究动机在于提高高分辨率卫星图像中物体检测的准确性和效率。", "method": "GLOD采用基于Transformer的架构，用Swin Transformer取代传统的CNN主干网，结合新型的UpConvMixer模块进行稳健的上采样，并使用Fusion Blocks进行多尺度特征整合。", "result": "实验结果表明，在xView数据集上，GLOD达到了32.95%的性能，相较于现有最优方法提高了11.46%。", "conclusion": "该架构通过引入非对称融合与CBAM注意力机制及多路径头设计，特别是在捕捉不同尺度的物体方面具有创新性，且在维护计算效率的同时利用了空间先验知识。"}}
{"id": "2507.11097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11097", "abs": "https://arxiv.org/abs/2507.11097", "authors": ["Zichen Wen", "Jiashu Qu", "Dongrui Liu", "Zhiyuan Liu", "Ruixi Wu", "Yicun Yang", "Xiangqi Jin", "Haoyun Xu", "Xuyang Liu", "Weijia Li", "Chaochao Lu", "Jing Shao", "Conghui He", "Linfeng Zhang"], "title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs", "comment": "21 pages, 9 figures, work in progress", "summary": "Diffusion-based large language models (dLLMs) have recently emerged as a\npowerful alternative to autoregressive LLMs, offering faster inference and\ngreater interactivity via parallel decoding and bidirectional modeling.\nHowever, despite strong performance in code generation and text infilling, we\nidentify a fundamental safety concern: existing alignment mechanisms fail to\nsafeguard dLLMs against context-aware, masked-input adversarial prompts,\nexposing novel vulnerabilities. To this end, we present DIJA, the first\nsystematic study and jailbreak attack framework that exploits unique safety\nweaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial\ninterleaved mask-text prompts that exploit the text generation mechanisms of\ndLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional\nmodeling drives the model to produce contextually consistent outputs for masked\nspans, even when harmful, while parallel decoding limits model dynamic\nfiltering and rejection sampling of unsafe content. This causes standard\nalignment mechanisms to fail, enabling harmful completions in alignment-tuned\ndLLMs, even when harmful behaviors or unsafe instructions are directly exposed\nin the prompt. Through comprehensive experiments, we demonstrate that DIJA\nsignificantly outperforms existing jailbreak methods, exposing a previously\noverlooked threat surface in dLLM architectures. Notably, our method achieves\nup to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior\nbaseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and\nby 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of\nharmful content in the jailbreak prompt. Our findings underscore the urgent\nneed for rethinking safety alignment in this emerging class of language models.\nCode is available at https://github.com/ZichenWen1/DIJA.", "AI": {"tldr": "DIJA is a framework that exploits dLLMs safety weaknesses by crafting adversarial prompts, demonstrating significant vulnerabilities that current alignment methods cannot adequately address.", "motivation": "To highlight the safety concerns associated with diffusion-based large language models (dLLMs) when dealing with context-aware adversarial prompts, leading to the development of a new systematic attack framework.", "method": "DIJA constructs specific adversarial prompts using interleaved mask-text inputs that capitalize on bidirectional modeling and parallel decoding mechanisms of dLLMs, thereby bypassing standard safety alignments.", "result": "DIJA achieved high attack success rates, notably surpassing previous methods in both evaluator-based and keyword-based assessment measures.", "conclusion": "The study indicates a critical need for re-examining the safety mechanisms in dLLMs, as current methods are ineffective against the developed adversarial attack framework."}}
{"id": "2507.11055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11055", "abs": "https://arxiv.org/abs/2507.11055", "authors": ["Shuchang Ye", "Usman Naseem", "Mingyuan Meng", "Jinman Kim"], "title": "Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation", "comment": "Accepted to ICCV 2025", "summary": "Medical language-guided segmentation, integrating textual clinical reports as\nauxiliary guidance to enhance image segmentation, has demonstrated significant\nimprovements over unimodal approaches. However, its inherent reliance on paired\nimage-text input, which we refer to as ``textual reliance\", presents two\nfundamental limitations: 1) many medical segmentation datasets lack paired\nreports, leaving a substantial portion of image-only data underutilized for\ntraining; and 2) inference is limited to retrospective analysis of cases with\npaired reports, limiting its applicability in most clinical scenarios where\nsegmentation typically precedes reporting. To address these limitations, we\npropose ProLearn, the first Prototype-driven Learning framework for\nlanguage-guided segmentation that fundamentally alleviates textual reliance. At\nits core, in ProLearn, we introduce a novel Prototype-driven Semantic\nApproximation (PSA) module to enable approximation of semantic guidance from\ntextual input. PSA initializes a discrete and compact prototype space by\ndistilling segmentation-relevant semantics from textual reports. Once\ninitialized, it supports a query-and-respond mechanism which approximates\nsemantic guidance for images without textual input, thereby alleviating textual\nreliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG\ndemonstrate that ProLearn outperforms state-of-the-art language-guided methods\nwhen limited text is available.", "AI": {"tldr": "本论文提出了ProLearn框架，该框架通过原型驱动语义近似(PSA)模块减轻了对文本报告的依赖性，显示出了在有限文本条件下超过现有语言引导分割方法的性能。", "motivation": "现有文献引导的分割方法依赖于图像-文本配对输入，存在两个限制：一是很多医疗分割数据集缺乏配对报告，导致大量图像数据未被充分利用；二是推断限制在有配对报告的病例的回顾性分析中，限制了其在大多数临床场景中的可适用性。为此，本论文提出了新型原型驱动学习框架。", "method": "Medical language-guided segmentation方法通过结合文本临床报告作为辅助指导来提高图像分割性能。本研究提出了一种名为ProLearn的新型原型驱动学习框架，该框架能够减轻对文本的依赖。核心组件是原型驱动语义近似(PSA)模块，该模块通过从文本报告中提取与分割相关的语义，来初始化离散紧凑的原型空间，为没有文本输入的图像提供语义指导。", "result": "通过在QaTa-COV19，MosMedData+和Kvasir-SEG数据集上的广泛实验表明，ProLearn在有限文本可用时优于现有的语言引导方法。", "conclusion": "ProLearn框架通过引入PSA模块，显著降低了对带有文本报告图像的依赖性，促进了医疗图像分割技术的进步，使其更加适应实际临床环境。"}}
{"id": "2507.11112", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11112", "abs": "https://arxiv.org/abs/2507.11112", "authors": ["Sanhanat Sivapiromrat", "Caiqi Zhang", "Marco Basaldella", "Nigel Collier"], "title": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs", "comment": null, "summary": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning.", "AI": {"tldr": "The paper introduces a framework to study data poisoning attacks in Large Language Models (LLMs) with multiple backdoor triggers that can coexist without interference. It also offers a post hoc recovery method that targets specific model components for retraining to defend against such attacks.", "motivation": "The motivation is to understand and mitigate the vulnerability of LLMs to data poisoning attacks, particularly the coexistence and interaction of multiple backdoor triggers within the same model.", "method": "The method involves creating a framework to study multiple distinct backdoor triggers within LLMs and demonstrating their ability to coexist without mutual interference. A post hoc recovery method selectively retrains model components based on layer-wise weight differences to remove trigger behavior.", "result": "The results show that multiple triggers can coexist in a model and exhibit robust activation even under token substitution and separation. The proposed recovery method successfully eliminates trigger behavior with minimal parameter updates.", "conclusion": "The paper concludes by highlighting the vulnerability of LLMs to multi-trigger poisoning attacks and proposes an effective and practical defense mechanism to mitigate the threat."}}
{"id": "2507.11061", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11061", "abs": "https://arxiv.org/abs/2507.11061", "authors": ["Hayeon Kim", "Ji Ha Jang", "Se Young Chun"], "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling", "comment": null, "summary": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing.", "AI": {"tldr": "该论文提出了RoMaP，一种用于3D高斯表示局部精确编辑的框架，通过引入3D面具生成模块和正则化SDS损失来解决现有方法的局限性，从而达成更精准和丰富的局部编辑。", "motivation": "旨在解决现有3D神经表示和实例级编辑模型在实现精准局部3D编辑中遇到的挑战，尤其是高斯模型在局部编辑上的局限性。", "method": "提出RoMaP框架，包括使用球谐系数预测3D部分分割的3D-Geometry Aware Label Prediction（3D-GALP）模块和结合额外正则化的SDS损失。", "result": "实验表明，RoMaP在重建和生成的高斯场景与对象上的局部3D编辑中达到最先进水平，定性和定量实验结果均证明了这一点。", "conclusion": "RoMaP框架允许用户进行更加稳健和灵活的3D高斯模型的部分级编辑。"}}
{"id": "2507.11114", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11114", "abs": "https://arxiv.org/abs/2507.11114", "authors": ["Seif Ahmed", "Mohamed T. Younes", "Abdelrahman Moustafa", "Abdelrahman Allam", "Hamza Moustafa"], "title": "MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models", "comment": null, "summary": "We present a robust ensemble-based system for multilingual multimodal\nreasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach\nintegrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption\nrefinement and consistency checks, and Gemini 2.5 Pro as a reasoner which\nhandles final answer selection, all coordinated through carefully engineered\nfew-shot and zero-shot prompts. We conducted an extensive ablation study,\ntraining several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,\nMistral) on an English dataset and its multilingual augmented version.\nAdditionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for\ncomparison and found it to substantially outperform the trained models. Prompt\ndesign also proved critical: enforcing concise, language-normalized formats and\nprohibiting explanatory text boosted model accuracy on the English validation\nset from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)\nachieved first place overall in the multilingual track with 81.4% accuracy, and\nled 11 out of 13 individual language tracks, with top results such as 95.07%\nfor Croatian and 92.12% for Italian. These findings highlight that lightweight\nOCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual\naugmentation, can outperform heavier end-to-end models in high-stakes,\nmultilingual educational settings.", "AI": {"tldr": "研究团队提出了一种基于集成系统的多语言多模态推理系统，用于处理ImageCLEF 2025 EXAMS V挑战中的问题。系统集成了多个Gemini模型，通过精心设计的提示策略在多种语言任务中表现出色，尤其在多语言轨道中取得了81.4%的准确率，显示了轻量级OCR-VLM集成和精确提示策略的优势。", "motivation": "此研究针对的是在多语言情况下进行有效且精确推理的挑战，尤其是涉及到教育设置中的高风险任务。多语言推理要求系统能够理解多模态输入并给出准确的回答。", "method": "该系统采用Gemini 2.5 Flash提供描述，Gemini 1.5 Pro进行改进和一致性检查，以及Gemini 2.5 Pro作为推理器，指导最终答案的选择。此外，通过精心设计的提示策略优化了模型性能。", "result": "系统在官方排行榜中以81.4%的准确率获得了多语言赛道的第一名，并且在13种语言中领先了11个。具体表现如克罗地亚语的95.07%和意大利语的92.12%。", "conclusion": "研究得出轻量级OCR-VLM集成结合精确提示策略和跨语言数据增强不仅可以克服多语言推理中的挑战，而且能够在复杂的教育环境中与更重的端到端模型竞争中胜出。"}}
{"id": "2507.11075", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2507.11075", "abs": "https://arxiv.org/abs/2507.11075", "authors": ["Chang Peng", "Yifei Zhou", "Huifeng Xi", "Shiqing Huang", "Chuangye Chen", "Jianming Yang", "Bao Yang", "Zhenyu Jiang"], "title": "Joint angle model based learning to refine kinematic human pose estimation", "comment": null, "summary": "Marker-free human pose estimation (HPE) has found increasing applications in\nvarious fields. Current HPE suffers from occasional errors in keypoint\nrecognition and random fluctuation in keypoint trajectories when analyzing\nkinematic human poses. The performance of existing deep learning-based models\nfor HPE refinement is considerably limited by inaccurate training datasets in\nwhich the keypoints are manually annotated. This paper proposed a novel method\nto overcome the difficulty through joint angle-based modeling. The key\ntechniques include: (i) A joint angle-based model of human pose, which is\nrobust to describe kinematic human poses; (ii) Approximating temporal variation\nof joint angles through high order Fourier series to get reliable \"ground\ntruth\"; (iii) A bidirectional recurrent network is designed as a\npost-processing module to refine the estimation of well-established HRNet.\nTrained with the high-quality dataset constructed using our method, the network\ndemonstrates outstanding performance to correct wrongly recognized joints and\nsmooth their spatiotemporal trajectories. Tests show that joint angle-based\nrefinement (JAR) outperforms the state-of-the-art HPE refinement network in\nchallenging cases like figure skating and breaking.", "AI": {"tldr": "本文提出了一种新颖的基于关节角度的建模方法来改进人体姿态估计，使用该方法进行训练的数据集使算法在识别错误修正与时空轨迹平滑方面表现出色。实验证明在像花样滑冰这样的复杂场景下，此种方法优于现有的最优姿态估计算法。", "motivation": "现有的人体姿态估计算法往往会因关键点识别偶尔出错以及关键点轨迹随机波动而受到限制。尤其现有深度学习模型的训练数据集中关键点手动标注的不准确性也极大限制了其性能。", "method": "该论文提出了一种基于关节角度建模的新方法。关键技术包括：(i) 一种用于描述运动中人体姿态的关节角度模型；(ii) 通过高阶傅里叶级数拟合关节角度的时间变化以获得可靠的“真实值”；(iii) 设计双向循环网络作为后处理模块来优化已有的HRNet的估算结果。", "result": "利用本方法构建的高质量数据集训练，该网络在修正错误识别的关键点和平滑其时空轨迹方面表现出色。测试表明，在像花样滑冰这样的具有挑战性的情况下，关节角度优化(JAR)优于最先进的HPE优化网络。", "conclusion": "该研究提出了一种新的基于关节角度的方法来改进人体姿态估计算法，并通过实验验证了其在困难场景下的有效性。"}}
{"id": "2507.11128", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2507.11128", "abs": "https://arxiv.org/abs/2507.11128", "authors": ["Dimitri Staufer"], "title": "What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests", "comment": "16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable\n  Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto,\n  Portugal", "summary": "Large Language Models (LLMs) can memorize and reveal personal information,\nraising concerns regarding compliance with the EU's GDPR, particularly the\nRight to Be Forgotten (RTBF). Existing machine unlearning methods assume the\ndata to forget is already known but do not address how to identify which\nindividual-fact associations are stored in the model. Privacy auditing\ntechniques typically operate at the population level or target a small set of\nidentifiers, limiting applicability to individual-level data inquiries. We\nintroduce WikiMem, a dataset of over 5,000 natural language canaries covering\n243 human-related properties from Wikidata, and a model-agnostic metric to\nquantify human-fact associations in LLMs. Our approach ranks ground-truth\nvalues against counterfactuals using calibrated negative log-likelihood across\nparaphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B\nparameters), showing that memorization correlates with subject web presence and\nmodel scale. We provide a foundation for identifying memorized personal data in\nLLMs at the individual level, enabling the dynamic construction of forget sets\nfor machine unlearning and RTBF requests.", "AI": {"tldr": "本文提出WikiMem数据集及一种新的度量标准，用于评估LLM中人类事实的关联性，这为实现大规模语言模型的个别数据遗忘请求提供了潜在方案。", "motivation": "旨在解决大型语言模型中个人资料的识别及记忆问题，尤其是针对欧盟GDPR中的被遗忘权。现有的机器遗忘方法假设需要遗忘的数据是已知的，并不能识别模型中存储的个人事实关联，而传统的隐私审计技术也存在局限性。", "method": "我们引入了WikiMem数据集，该数据集包含超过5,000个自然语言的防护措施，并涵盖了来自Wikidata的243个人类相关的属性。我们还提出了一种模型无关的度量标准，用于量化LLM中的人类事实关联。通过使用校准的负对数似然来对不同事实的真值进行排序以应对各种不同措辞的问题。", "result": "我们在15种不同参数规模的LLM中对200个个体进行了评估，发现记忆与个体在网上的存在以及模型规模相关。", "conclusion": "这一方法可为识别LLM中个人内存数据提供基础，从而为机器遗忘和被遗忘权请求提供了动态构造遗忘集的可能性。"}}
{"id": "2507.11077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11077", "abs": "https://arxiv.org/abs/2507.11077", "authors": ["Weizhao Ma", "Dong Zhou", "Yuhui Hu", "Zipeng He"], "title": "GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft", "comment": null, "summary": "Monocular pose estimation of non-cooperative spacecraft is significant for\non-orbit service (OOS) tasks, such as satellite maintenance, space debris\nremoval, and station assembly. Considering the high demands on pose estimation\naccuracy, mainstream monocular pose estimation methods typically consist of\nkeypoint detectors and PnP solver. However, current keypoint detectors remain\nvulnerable to structural symmetry and partial occlusion of non-cooperative\nspacecraft. To this end, we propose a graph-based keypoints network for the\nmonocular pose estimation of non-cooperative spacecraft, GKNet, which leverages\nthe geometric constraint of keypoints graph. In order to better validate\nkeypoint detectors, we present a moderate-scale dataset for the spacecraft\nkeypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000\nsimulated images, and corresponding high-precise keypoint annotations.\nExtensive experiments and an ablation study have demonstrated the high accuracy\nand effectiveness of our GKNet, compared to the state-of-the-art spacecraft\nkeypoint detectors. The code for GKNet and the SKD dataset is available at\nhttps://github.com/Dongzhou-1996/GKNet.", "AI": {"tldr": "本文提出了一种新的、基于图的、用于单目估计非合作航天器姿态的方法，即GKNet，该方法使用了空间几何约束，并提供了一个航天器关键点检测的中等规模数据集SKD来评估其性能。", "motivation": "非合作航天器的单目姿态估计对于在轨服务（如卫星维护、空间碎片清除和站组装）任务至关重要。现有的关键点检测器在面对非合作航天器的结构对称性和部分遮挡时依然脆弱。我们的目标是提高这类姿态估计的准确性。", "method": "我们的方法是一种基于图的、用于非合作航天器单目姿态估计算法，称为GKNet，它利用了关键点图的几何约束。为了更好地验证关键点检测器，我们还提出了一种适度规模的航天器关键点检测数据集，名为SKD，包括3个航天器目标、90,000张模拟图像及其对应的精确关键点注释。", "result": "广泛的实验和消融研究表明，相较于最先进的航天器关键点检测器，我们的GKNet在准确性和有效性上表现优异。", "conclusion": "研究表明，GKNet和SKD数据集提供了一个有效的解决方案，可用于提高非合作航天器的单目姿态估计的准确性。代码和数据集已公开。"}}
{"id": "2507.11198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11198", "abs": "https://arxiv.org/abs/2507.11198", "authors": ["Conrad Borchers", "Bahar Shahrokhian", "Francesco Balzan", "Elham Tajik", "Sreecharan Sankaranarayanan", "Sebastian Simon"], "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding", "comment": "Manuscript submitted for review", "summary": "Large Language Models (LLMs) enable new possibilities for qualitative\nresearch at scale, including coding and data annotation. While multi-agent\nsystems (MAS) can emulate human coding workflows, their benefits over\nsingle-agent coding remain poorly understood. We conducted an experimental\nstudy of how agent persona and temperature shape consensus-building and coding\naccuracy of dialog segments based on a codebook with 8 codes. Our open-source\nMAS mirrors deductive human coding through structured agent discussion and\nconsensus arbitration. Using six open-source LLMs (with 3 to 32 billion\nparameters) and 18 experimental configurations, we analyze over 77,000 coding\ndecisions against a gold-standard dataset of human-annotated transcripts from\nonline math tutoring sessions. Temperature significantly impacted whether and\nwhen consensus was reached across all six LLMs. MAS with multiple personas\n(including neutral, assertive, or empathetic), significantly delayed consensus\nin four out of six LLMs compared to uniform personas. In three of those LLMs,\nhigher temperatures significantly diminished the effects of multiple personas\non consensus. However, neither temperature nor persona pairing lead to robust\nimprovements in coding accuracy. Single agents matched or outperformed MAS\nconsensus in most conditions. Only one model (OpenHermesV2:7B) and code\ncategory showed above-chance gains from MAS deliberation when temperature was\n0.5 or lower and especially when the agents included at least one assertive\npersona. Qualitative analysis of MAS collaboration for these configurations\nsuggests that MAS may nonetheless aid in narrowing ambiguous code applications\nthat could improve codebooks and human-AI coding. We contribute new insight\ninto the limits of LLM-based qualitative methods, challenging the notion that\ndiverse MAS personas lead to better outcomes. We open-source our MAS and\nexperimentation code.", "AI": {"tldr": "研究结果表明，温度显著地影响了所有六种大语言模型中共识的达成和达成的时间长短。存在多个角色的多代理系统比统一角色的多代理系统在四分之六个模型中更延迟达成共识。只有在一个拥有70亿参数的大语言模型（OpenHermesV2:7B）和某一种编码类别中，多代理系统的商议在特定条件下能带来高于机会水平的改善。", "motivation": "研究动机在于探索多代理系统（MAS）对编码工作流的模拟，以及其相对于单代理编码的好处。目前对于后者的优势还不甚明了，因此这项研究旨在填补这一知识空白。", "method": "本研究通过实验研究了代理角色和温度如何影响基于对话片段的共识构建和编码准确性，这些对话片段基于一个包含8个编码的代码簿。研究使用了六个开源的大语言模型（参数数量从30亿到320亿不等）和18种实验配置，分析了超过77,000个编码决策。", "result": "结果表明，温度显著地影响了共识的达成和时间长短。多角色的MAS在四个大语言模型中比单角色MAS更延迟达成共识，但在三个模型中，较高温度减弱了角色多样性对共识效果的影响。MAS在大多数情况下未能在编码准确性上有显著改善，与单代理系统持平或低于单代理系统。", "conclusion": "研究结果挑战了多样化的MAS角色能带来更好结果的观念。即便如此，对于特定配置的MAS合作可能有助于缩小编码应用中的模糊性，从而改进代码簿和人机编码质量。整体而言，该研究提供了关于基于大语言模型的定性方法的局限性的新见解。"}}
{"id": "2507.11081", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.2"], "pdf": "https://arxiv.org/pdf/2507.11081", "abs": "https://arxiv.org/abs/2507.11081", "authors": ["Chang Peng", "Bao Yang", "Meiqi Li", "Ge Zhang", "Hui Sun", "Zhenyu Jiang"], "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification", "comment": null, "summary": "Ground penetrating radar (GPR) has become a rapid and non-destructive\nsolution for road subsurface distress (RSD) detection. However, RSD recognition\nfrom GPR images is labor-intensive and heavily relies on inspectors' expertise.\nDeep learning offers the possibility for automatic RSD recognition, but its\ncurrent performance is limited by two factors: Scarcity of high-quality dataset\nfor network training and insufficient capability of network to distinguish RSD.\nIn this study, a rigorously validated 3D GPR dataset containing 2134 samples of\ndiverse types was constructed through field scanning. Based on the finding that\nthe YOLO model trained with one of the three scans of GPR images exhibits\nvarying sensitivity to specific type of RSD, we proposed a novel\ncross-verification strategy with outstanding accuracy in RSD recognition,\nachieving recall over 98.6% in field tests. The approach, integrated into an\nonline RSD detection system, can reduce the labor of inspection by around 90%.", "AI": {"tldr": "本文提出一种新的针对地面穿透雷达（GPR）图像中RSD识别的交叉验证策略，该策略基于高质量3D GPR数据集和深度学习技术，在实际应用中大幅减少检测工作量并提高准确性。", "motivation": "解决现有RSD识别劳动强度大、对专家依赖度高以及深度学习中缺少高质量训练数据集和网络对RSD区分能力不足的问题。", "method": "本研究构建了一个严格验证的3D GPR数据集，包含2134个多样本，并提出了一种基于YOLO模型的交叉验证策略，该策略在特定类型道路路面下层病害（RSD）识别上具有不同敏感度，从而提高了识别准确性。", "result": "通过实施这种识别策略，在实地测试中实现了超过98.6%的召回率。", "conclusion": "该方法集成到一个在线RSD检测系统中，可将检查工作量减少90%左右。"}}
{"id": "2507.11216", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11216", "abs": "https://arxiv.org/abs/2507.11216", "authors": ["Valle Ruiz-Fernández", "Mario Mina", "Júlia Falcão", "Luis Vasquez-Reina", "Anna Sallés", "Aitor Gonzalez-Agirre", "Olatz Perez-de-Viñaspre"], "title": "EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering", "comment": null, "summary": "Previous literature has largely shown that Large Language Models (LLMs)\nperpetuate social biases learnt from their pre-training data. Given the notable\nlack of resources for social bias evaluation in languages other than English,\nand for social contexts outside of the United States, this paper introduces the\nSpanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and\nCaBBQ). Based on the original BBQ, these two parallel datasets are designed to\nassess social bias across 10 categories using a multiple-choice QA setting, now\nadapted to the Spanish and Catalan languages and to the social context of\nSpain. We report evaluation results on different LLMs, factoring in model\nfamily, size and variant. Our results show that models tend to fail to choose\nthe correct answer in ambiguous scenarios, and that high QA accuracy often\ncorrelates with greater reliance on social biases.", "AI": {"tldr": "该论文引入了EsBBQ和CaBBQ，即西班牙语和加泰罗尼亚语偏见基准，用于评估未经英语和美国社会环境以外的语言模型的社会偏见。", "motivation": "目前关于语言模型社会偏见的资源在英语之外的语言和美国之外的社会环境里匮乏。", "method": "基于原始BBQ（偏见基准），设计适用于西班牙语和加泰罗尼亚语，评估跨10个社会类别偏见的平行数据集。", "result": "结果显示，较大的语言模型在具有社会偏见的模棱两可的情境中倾向于表现不佳，且高QA准确率经常与更大程度上依赖社会偏见相关。", "conclusion": "这项工作说明了评估非英语语言模型中社会偏见的重要性和方法。"}}
{"id": "2507.11085", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11085", "abs": "https://arxiv.org/abs/2507.11085", "authors": ["Tianchi Xu"], "title": "Atmos-Bench: 3D Atmospheric Structures for Climate Insight", "comment": null, "summary": "Atmospheric structure, represented by backscatter coefficients (BC) recovered\nfrom satellite LiDAR attenuated backscatter (ATB), provides a volumetric view\nof clouds, aerosols, and molecules, playing a critical role in human\nactivities, climate understanding, and extreme weather forecasting. Existing\nmethods often rely on auxiliary inputs and simplified physics-based\napproximations, and lack a standardized 3D benchmark for fair evaluation.\nHowever, such approaches may introduce additional uncertainties and\ninsufficiently capture realistic radiative transfer and atmospheric\nscattering-absorption effects. To bridge these gaps, we present Atmos-Bench:\nthe first 3D atmospheric benchmark, along with a novel FourCastX:\nFrequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)\ngenerates 921,600 image slices from 3D scattering volumes simulated at 532 nm\nand 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean\ntime steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC\nphysical constraints into the model architecture, promoting energy consistency\nduring restoration; (c) achieves consistent improvements on the Atmos-Bench\ndataset across both 355 nm and 532 nm bands, outperforming state-of-the-art\nbaseline models without relying on auxiliary inputs. Atmos-Bench establishes a\nnew standard for satellite-based 3D atmospheric structure recovery and paves\nthe way for deeper climate insight.", "AI": {"tldr": "本文提出了一个新的3D大气恢复方法FourCastX，它基于频率增强的空间-时间专家混合网络，解决了现有方法依赖辅助输入和简化物理近似的不足，建立了Atmos-Bench作为3D大气基准。", "motivation": "现有的方法依赖于辅助输入和简化的基于物理的近似，缺乏标准化的3D基准进行公平评估。然而，这些方法可能会引入额外的不确定性，并且无法充分捕捉真实的辐射传输和大气散射吸收效应。", "method": "提出了Atmos-Bench：第一个3D大气基准，以及一种新的FourCastX：频率增强空间-时间专家混合网络，该网络（a）从通过将WRF与增强的COSP模拟器耦合在532纳米和355纳米波长下模拟的3D散射体积中生成921,600张图像切片，产生高质体素参考；（b）将ATB-BC物理约束嵌入模型架构中，促进恢复过程中的能量一致性。", "result": "<tool_call>\ncmpleted the analysis and retrieved the information.\n<tool_call>\",", "conclusion": "FourCastX在355纳米和532纳米波段上的Atmos-Bench数据集上均取得了持续的改进，优于现有最先进的基线模型，并且不需要依赖辅助输入。Atmos-Bench为基于卫星的3D大气结构恢复建立了新标准，开启了更深层次的气候见解。"}}
{"id": "2507.11222", "categories": ["cs.CL", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11222", "abs": "https://arxiv.org/abs/2507.11222", "authors": ["Fares Wael", "Youssef Maklad", "Ali Hamdi", "Wael Elsersy"], "title": "An Agentic Flow for Finite State Machine Extraction using Prompt Chaining", "comment": null, "summary": "Finite-State Machines (FSMs) are critical for modeling the operational logic\nof network protocols, enabling verification, analysis, and vulnerability\ndiscovery. However, existing FSM extraction techniques face limitations such as\nscalability, incomplete coverage, and ambiguity in natural language\nspecifications. In this paper, we propose FlowFSM, a novel agentic framework\nthat leverages Large Language Models (LLMs) combined with prompt chaining and\nchain-of-thought reasoning to extract accurate FSMs from raw RFC documents.\nFlowFSM systematically processes protocol specifications, identifies state\ntransitions, and constructs structured rule-books by chaining agent outputs.\nExperimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM\nachieves high extraction precision while minimizing hallucinated transitions,\nshowing promising results. Our findings highlight the potential of agent-based\nLLM systems in the advancement of protocol analysis and FSM inference for\ncybersecurity and reverse engineering applications.", "AI": {"tldr": "本研究提出了FlowFSM，利用大型语言模型和代理输出链接技术从协议的原始文档中准确提取FSMs，解决了现有技术在FSM提取中的问题。", "motivation": "论文指出现有的FSM抽取技术存在可扩展性差、覆盖率低、自然语言规范模糊等问题，因此提出了一种新的方法来解决这些问题。这种方法旨在改善网络协议的状态机建模，使其在验证、分析和发现漏洞方面更加准确和有效。", "method": "本论文提出了一种新的框架FlowFSM，该框架利用大型语言模型（LLMs）、提示链和链式思维推理技术，从原始RFC文档中准确提取有限状态机（FSMs）。FlowFSM系统地处理协议规范，识别状态转换，并通过链接代理输出来构建结构化的规约书。", "result": "实验评估表明，FlowFSM在FTP和RTSP协议上实现了高精度的FSM提取，减少了幻觉状态转移，显示了其在协议分析中的潜力。", "conclusion": "该研究证实了基于代理的LLM系统在协议分析和FSM推断中的应用潜力，特别是在网络安全和逆向工程领域。"}}
{"id": "2507.11099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11099", "abs": "https://arxiv.org/abs/2507.11099", "authors": ["Qiyang Wan", "Chengzhi Gao", "Ruiping Wang", "Xilin Chen"], "title": "A Survey on Interpretability in Visual Recognition", "comment": "20 pages, 7 figures, 2 tables. Under review", "summary": "In recent years, visual recognition methods have advanced significantly,\nfinding applications across diverse fields. While researchers seek to\nunderstand the mechanisms behind the success of these models, there is also a\ngrowing impetus to deploy them in critical areas like autonomous driving and\nmedical diagnostics to better diagnose failures, which promotes the development\nof interpretability research. This paper systematically reviews existing\nresearch on the interpretability of visual recognition models and proposes a\ntaxonomy of methods from a human-centered perspective. The proposed taxonomy\ncategorizes interpretable recognition methods based on Intent, Object,\nPresentation, and Methodology, thereby establishing a systematic and coherent\nset of grouping criteria for these XAI methods. Additionally, we summarize the\nrequirements for evaluation metrics and explore new opportunities enabled by\nrecent technologies, such as large multimodal models. We aim to organize\nexisting research in this domain and inspire future investigations into the\ninterpretability of visual recognition models.", "AI": {"tldr": "This paper reviews the interpretability of visual recognition models, proposes a human-centered taxonomy and discusses new opportunities for research.", "motivation": "To better understand the mechanisms of visual recognition models and to diagnose failures in critical applications, this paper aims to review existing research and inspire future work.", "method": "Structure", "result": "A taxonomy of methods for interpretable visual recognition models based on Intent, Object, Presentation, and Methodology is proposed, along with evaluation criteria and new research opportunities.", "conclusion": "The taxonomy and proposed criteria provide a structured approach to understand and evaluate interpretability methods, aimed at advancing the field of visual recognition models."}}
{"id": "2507.11230", "categories": ["cs.CL", "68T50"], "pdf": "https://arxiv.org/pdf/2507.11230", "abs": "https://arxiv.org/abs/2507.11230", "authors": ["Lyzander Marciano Andrylie", "Inaya Rahmanisa", "Mahardika Krisna Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages", "comment": null, "summary": "Understanding the multilingual mechanisms of large language models (LLMs)\nprovides insight into how they process different languages, yet this remains\nchallenging. Existing studies often focus on individual neurons, but their\npolysemantic nature makes it difficult to isolate language-specific units from\ncross-lingual representations. To address this, we explore sparse autoencoders\n(SAEs) for their ability to learn monosemantic features that represent concrete\nand abstract concepts across languages in LLMs. While some of these features\nare language-independent, the presence of language-specific features remains\nunderexplored. In this work, we introduce SAE-LAPE, a method based on feature\nactivation probability, to identify language-specific features within the\nfeed-forward network. We find that many such features predominantly appear in\nthe middle to final layers of the model and are interpretable. These features\ninfluence the model's multilingual performance and language output and can be\nused for language identification with performance comparable to fastText along\nwith more interpretability. Our code is available at\nhttps://github.com/LyzanderAndrylie/language-specific-features .", "AI": {"tldr": "研究提出了一种基于稀疏自编码器的SAE-LAPE方法，用于识别大语言模型中的语言特异性特征，这些特征在模型中到末层出现，并可用于语言识别，性能类似fastText但更易解释。", "motivation": "理解大语言模型中的多语言机制有助于洞察它们如何处理不同的语言，但现有的研究由于神经元的多义性难以将语言特异单元与跨语言表征区分开。", "method": "采用稀疏自编码器（SAEs）学习多语种大语言模型（LLMs）中具有一致性的具体和抽象概念特征。特别是引入了基于特征激活概率的SAE-LAPE方法，在前馈网络中识别语言特异性特征。", "result": "发现了大量语言特异性特征主要出现在模型的中到末层，并且这些特征可以用于语言识别，其性能与fastText相近，但更具可解释性。", "conclusion": "通过使用SAE-LAPE方法，可以识别并利用大语言模型中的语言特异性特征，这些特征对于模型的多语言性能和输出具有重要影响。"}}
{"id": "2507.11102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11102", "abs": "https://arxiv.org/abs/2507.11102", "authors": ["Jie Yang", "Wang Zeng", "Sheng Jin", "Lumin Xu", "Wentao Liu", "Chen Qian", "Zhen Li", "Ruimao Zhang"], "title": "KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model", "comment": "Extended Version of KptLLM. arXiv admin note: text overlap with\n  arXiv:2411.01846", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has revolutionized\nimage understanding by bridging textual and visual modalities. However, these\nmodels often struggle with capturing fine-grained semantic information, such as\nthe precise identification and analysis of object keypoints. Keypoints, as\nstructure-aware, pixel-level, and compact representations of objects,\nparticularly articulated ones, play a crucial role in applications such as\nfine-grained image analysis, object retrieval, and behavior recognition. In\nthis paper, we propose KptLLM++, a novel multimodal large language model that\nspecifically designed for generic keypoint comprehension through the\nintegration of diverse input modalities guided by user-defined instructions. By\nunifying keypoint detection across varied contexts, KptLLM++ establishes itself\nas an advanced interface, fostering more effective human-AI collaboration. The\nmodel is built upon a novel identify-then-detect paradigm, which first\ninterprets keypoint semantics and subsequently localizes their precise\npositions through a structured chain-of-thought reasoning mechanism. To push\nthe boundaries of performance, we have scaled up the training dataset to over\n500K samples, encompassing diverse objects, keypoint categories, image styles,\nand scenarios with complex occlusions. This extensive scaling enables KptLLM++\nto unlock its potential, achieving remarkable accuracy and generalization.\nComprehensive experiments on multiple keypoint detection benchmarks demonstrate\nits state-of-the-art performance, underscoring its potential as a unified\nsolution for fine-grained image understanding and its transformative\nimplications for human-AI interaction.", "AI": {"tldr": "本文提出了KptLLM++，一种专为通用关键点理解设计的多模态大型语言模型，通过结合多样化的输入模式和用户定义的指令，实现了对关键点位置的精确定位，显著提高了细粒度图像理解的性能。", "motivation": "现有的多模态大型语言模型在捕捉细粒度语义信息，特别是对关键点对象的精确识别和分析上有局限性，因此提出KptLLM++来提升关键点检测的效率和准确性。", "method": "KptLLM++采用识别-检测范式，首先理解和解释关键点的语义，然后通过结构化推理机制精确定位关键点位置，并通过规模扩大的训练数据集提高模型性能。", "result": "在多个关键点检测基准实验中展示了KptLLM++的优越性能，显著提升了细粒度图像理解和复杂环境下的对象识别准确性。", "conclusion": "KptLLM++通过整合不同模式输入和扩展训练数据资源，证明了其作为细粒度图像理解统一解决方案的能力，对人机交互模式具有变革性的影响。"}}
{"id": "2507.11273", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11273", "abs": "https://arxiv.org/abs/2507.11273", "authors": ["Luohe Shi", "Zuchao Li", "Lefei Zhang", "Guoming Liu", "Baoyuan Qi", "Hai Zhao"], "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding", "comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.", "AI": {"tldr": "研究提出KV-Latent范式，通过降采样Key-Value向量维度到潜在空间，改进了Rotary Positional Embedding的频率采样机制，减少KV缓存足迹，提高推理速度，实验结果证明了该方法的有效性。", "motivation": "尽管基于Transformer Decoders的语言模型在对话生成AI中占主导地位，但在推理过程中，Key-Value (KV)缓存的逐渐增加成为主要的效率瓶颈，尤其是在内存消耗和数据传输带宽方面。为了解决这些问题，提出了本研究。", "method": "本研究提出了一种称为KV-Latent的新范式，通过将Key-Value向量维度降采样到潜在空间，显著减少KV缓存的足迹，并提高推理速度。同时改进了在低维向量上应用的Rotary Positional Embedding的频率采样机制，以增强其稳定性。", "result": "实验结果，包括具有Grouped Query Attention的模型和没有它的模型，都取得了令人满意的结果。此外，还进行了对比实验，研究单独减少Key和Value组件对模型性能的影响。", "conclusion": "这种方法允许构建更有效的语言模型系统，开启了在KV缓存节省和高效大规模语言模型的新可能性。"}}
