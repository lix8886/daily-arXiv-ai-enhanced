{"id": "2601.09806", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09806", "abs": "https://arxiv.org/abs/2601.09806", "authors": ["Shahrzad Sayyafzadeh", "Hongmei Chi", "Shonda Bernadin"], "title": "Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification", "comment": "This manuscript is a preprint. A revised version of this work has been accepted for publication in the Springer Nature book Artificial Intelligence-Driven Forensics. This version includes one additional figure for completeness", "summary": "This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems, with applications in forensic analysis and security testing. We utilize FGSM to generate adversarial noise targeting an identity classifier and employ a diffusion model with reverse diffusion to enhance imperceptibility through Gaussian smoothing and adaptive brightness correction, thereby facilitating synthetic adversarial patch evasion. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person's identity for adversarial images, supporting forensic interpretation and documentation for identity evasion and recognition attacks. The pipeline evaluates changes in identity classification, captioning results, and vulnerabilities in facial identity verification and expression recognition under adversarial conditions. We further demonstrate effective detection and analysis of adversarial patches and adversarial samples using perceptual hashing and segmentation, achieving an SSIM of 0.95.", "AI": {"tldr": "论文提出了一种对面部生物识别系统进行对抗补丁攻击的方法，通过反向扩散扩散模型优化补丁，保持视觉自然度同时避开识别系统，构建基于ViT-GPT2模型的描述系统，并展示检测和分析对抗样本的有效方法，SSIM值达到0.95。", "motivation": "该研究旨在开发一个端到端的管道，用于生成、优化和评估对抗补丁，以破坏面部生物识别系统，应用于法医分析和安全测试。", "method": "该论文使用了FGSM生成针对身份分类器的对抗样本噪音，并通过反向扩散的扩散模型进行高斯平滑和自适应亮度校正，以增强不可感知性。然后将优化后的对抗补丁应用到面部图像中，测试其在保持自然视觉特征的同时避开识别系统的有效性。此外，使用Vision Transformer (ViT)-GPT2模型生成描述对抗图像中人物身份的字幕，支持身份逃逸和识别攻击的取证解释和记录。", "result": "研究展示了检测和分析对抗样本的有效方法，使用感知哈希和分割技术实现0.95的SSIM值。", "conclusion": "该研究成功展示了在一个端到端的框架中生成和优化对抗补丁的能力，可以用于评估人脸识别系统的安全性，并提供有效的对抗样本检测方法。"}}
{"id": "2601.09812", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.09812", "abs": "https://arxiv.org/abs/2601.09812", "authors": ["Carlo Sgaravatti", "Riccardo Pieroni", "Matteo Corno", "Sergio M. Savaresi", "Luca Magri", "Giacomo Boracchi"], "title": "LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving", "comment": "35 pages, 14 figures. Published at Pattern Recognition", "summary": "Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.", "AI": {"tldr": "LCF3D combines RGB images and LiDAR point clouds to improve 3D object detection, achieving better performance especially for pedestrians and cyclists.", "motivation": "The motivation is to improve 3D object detection accuracy, especially in autonomous driving scenarios, by effectively combining different types of sensor data (RGB and LiDAR).", "method": "Our method proposes LCF3D, a sensor fusion framework combining 2D object detection on RGB images with 3D object detection on LiDAR point clouds. It includes late fusion to reduce LiDAR False Positives and cascade fusion to recover missed objects from LiDAR.", "result": "Our approach achieves significant improvements over LiDAR-based methods, especially for challenging categories such as pedestrians, cyclists in the KITTI dataset, and motorcycles, bicycles in the nuScenes dataset.", "conclusion": "LCF3D is an effective sensor fusion framework that enhances 3D object detection by integrating RGB and LiDAR data, showing strong results in autonomous driving applications and proving beneficial for domain generalization."}}
{"id": "2601.09814", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09814", "abs": "https://arxiv.org/abs/2601.09814", "authors": ["Adil O. Khadidos", "Aziida Nanyonga", "Alaa O. Khadidos", "Olfat M. Mirza", "Mustafa Tahsin Yilmaz"], "title": "Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images", "comment": null, "summary": "Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.", "AI": {"tldr": "EfficientNet-B0 outperformed DenseNet121 in the detection of pediatric pneumonia from chest X-ray images, demonstrating higher accuracy, F1-score, and MCC.", "motivation": "To improve diagnosis accuracy and efficiency of pediatric pneumonia through deep learning model analysis of chest X-rays.", "method": "Two CNN architectures, DenseNet121 and EfficientNet-B0, fine-tuned with ImageNet weights on a dataset of 5,863 pediatric X-rays, using accuracy, F1-score, MCC, and recall for performance evaluation, and Grad-CAM and LIME for model explainability.", "result": "EfficientNet-B0 achieved 84.6% accuracy, 0.8899 F1-score, and 0.6849 MCC, outperforming DenseNet121 which scored 79.7% accuracy, 0.8597 F1-score, and 0.5852 MCC.", "conclusion": "EfficientNet-B0 is a strong candidate for clinical deployment due to its balanced and efficient performance, with explainability techniques enhancing the decision-making transparency of AI-assisted pediatric pneumonia diagnosis."}}
{"id": "2601.09823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09823", "abs": "https://arxiv.org/abs/2601.09823", "authors": ["Subhajit Sanyal", "Srinivas Soumitri Miriyala", "Akshay Janardan Bankar", "Sravanth Kodavanti", "Harshit", "Abhishek Ameta", "Shreyas Pandith", "Amit Satish Unde"], "title": "NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration", "comment": "Submitted to CVPR 2026", "summary": "Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.", "AI": {"tldr": "NanoSD是一个从Stable Diffusion 1.5中提炼出来的、适合边缘设备部署的高效轻量化扩散基础模型家族，它克服了现有轻量化方法的不足，并在多种图像处理任务上实现了State-of-the-art（SOTA）性能。", "motivation": "强大的生成先验虽然对于图像修复非常有价值，但现有的轻量级变体通过压缩去噪U-Net或减少扩散轨迹来实现简化方式，这会破坏潜在流型并限制了通用性。因此，作者提出了NanoSD，以解决现有方法的不足，使其能更好地应用于边缘设备。", "method": "通过网络手术、基于特征的生成性蒸馏以及对U-Net和VAE编解码器的结构化架构扩展联合应用的方式从Stable Diffusion 1.5中提炼出来的NanoSD，是一个帕累托最优扩散基础模型家族。这种方法在保持生成先验的同时产生了具有不同精度、延迟和大小的模型。", "result": "NanoSD实现了在移动类神经处理单元（NPUs）上以高感知质量进行实时推断，最少20ms的延迟，参数规模在130M到315M之间。", "conclusion": "NanoSD建立了一个适合边缘设备上实时视觉生成和修复的通用型扩散基础模型系列。"}}
{"id": "2601.09713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09713", "abs": "https://arxiv.org/abs/2601.09713", "authors": ["Jinqiang Wang", "Huansheng Ning", "Jianguo Ding", "Tao Zhu", "Liming Chen", "Chris Nugent"], "title": "LLM-Driven Preference Data Synthesis for Proactive Prediction of the Next User Utterance in Human-Machine Dialogue", "comment": "19 pages", "summary": "Proactively predicting a users next utterance in human-machine dialogue can streamline interaction and improve user experience. Existing commercial API-based solutions are subject to privacy concerns while deploying general-purpose LLMs locally remains computationally expensive. As such, training a compact, task-specific LLM provides a practical alternative. Although user simulator methods can predict a user's next utterance, they mainly imitate their speaking style rather than advancing the dialogue. Preference data synthesis has been investigated to generate data for proactive next utterance prediction and help align LLMs with user preferences. Yet existing methods lack the ability to explicitly model the intent reasoning that leads to the user's next utterance and to define and synthesize preference and non-preference reasoning processes for predicting the user's next utterance.To address these challenges, we propose ProUtt, an LLM-driven preference data synthesis method for proactive next utterance prediction. ProUtt converts dialogue history into an intent tree and explicitly models intent reasoning trajectories by predicting the next plausible path from both exploitation and exploration perspectives. It then constructs preference and non-preference reasoning processes by perturbing or revising intent tree paths at different future turns. Extensive evaluations using LLM-as-a-judge and human judgments demonstrate that ProUtt consistently outperforms existing data synthesis methods, user simulators, and commercial LLM APIs across four benchmark datasets. We release both the code and the synthesized datasets to facilitate future research.", "AI": {"tldr": "提出了ProUtt方法，通过显式建模意图推理过程来预测用户的下一个发言，优于现有方法。", "motivation": "现有解决方案存在隐私顾虑或计算成本高，ProUtt旨在通过任务特定的LLM简化交互并提高用户体验。", "method": "ProUtt将对话历史转换为意图树，预测可能的意图路径，并通过扰动或修改意图树路径来构建偏好和非偏好推理过程以预测下一个用户发言。", "result": "实验表明，ProUtt在多个基准数据集上优于现有数据合成、用户模拟器和商用API。", "conclusion": "ProUtt是一个有效的对话预测工具，可以生成与用户偏好对齐的数据，有助于未来研究。"}}
{"id": "2601.09828", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.09828", "abs": "https://arxiv.org/abs/2601.09828", "authors": ["Xiaoxu Ma", "Runhao Li", "Hanwen Liu", "Xiangbo Zhang", "Zhenyu Weng"], "title": "UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval", "comment": null, "summary": "Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.", "AI": {"tldr": "UniHash, a dual-branch framework for deep hashing, unifies pointwise and pairwise paradigms to improve retrieval performance across both known and novel categories.", "motivation": "Most existing deep hashing methods are limited to a single paradigm, leading to suboptimal performance on either seen or unseen categories. UniHash aims to combine the strengths of both paradigms for balanced retrieval performance.", "method": "UniHash consists of a center-based branch (pointwise paradigm) and a pairwise branch. It introduces a novel hash code learning method with a mutual learning loss and an SM-MoH module for inter-branch knowledge sharing.", "result": "Theoretical and experimental results on CIFAR-10, MSCOCO, and ImageNet show UniHash achieves state-of-the-art performance in image retrieval for both seen and unseen categories.", "conclusion": "UniHash is effective in achieving balanced retrieval performance across seen and unseen categories by unifying pointwise and pairwise paradigms in a dual-branch framework with bidirectional knowledge transfer."}}
{"id": "2601.09714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09714", "abs": "https://arxiv.org/abs/2601.09714", "authors": ["Devesh Saraogi", "Rohit Singhee", "Dhruv Kumar"], "title": "Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines", "comment": "Under Review", "summary": "The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.", "AI": {"tldr": "研究结果显示，基于分解和长上下文的工作流平均新颖度评分为4.17/5，而反思迭代方法则较低，为2.33/5。这表明，精心设计的多阶段智能工作流可以推动AI辅助科研构思的发展。", "motivation": "探讨大型语言模型在科研生态系统中的应用是否能产生更多新颖的科研思路，以及与传统一次性提示方法相比，多步骤智能工作流的优越性。", "method": "采用多步骤智能工作流（包括迭代推理、进化搜索和递归分解）来生成更为新颖和可行的研究计划，并评估了五种推理架构的性能，包括反思迭代优化、Sakana AI v2 进化算法、Google Co-Scientist 多代理框架、GPT Deep Research (GPT-5.1) 递归分解和Gemini~3 Pro 多模态长上下文管道。", "result": "不同研究领域的工作流性能有所不同，但高得分的工作流保持了可行性的同时并没有损害创新力。", "conclusion": "研究表明，精心设计的多阶段代理工作流可以推动AI辅助的科学想法创建。"}}
{"id": "2601.09851", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09851", "abs": "https://arxiv.org/abs/2601.09851", "authors": ["Po-han Li", "Shenghui Chen", "Ufuk Topcu", "Sandeep Chinchali"], "title": "ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning", "comment": null, "summary": "Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\\%$ in VQA accuracy without increasing processing load.", "AI": {"tldr": "本文提出ViSIL评分来量化多模态视频摘要中的信息丢失情况，通过VLM推理实现评测，证明了这种新指标在VQA任务中的表现优于传统摘要。", "motivation": "传统的评估指标如BLEU或ROUGE无法量化不同模式之间信息覆盖面的情况，例如难以对比一段文本和一系列关键帧之间的信息。因此需要一种新的评估方法。", "method": "提出了一种名为Video Summary Information Loss (ViSIL)的评分方法，该方法是一个基于信息论的框架，量化了视频信息未被摘要捕捉的部分，通过视觉-语言模型（VLM）的推理来测量信息损失。", "result": "结果表明，ViSIL评分与视频问答（VQA）任务中的人类表现和视觉-语言模型（VLM）性能之间存在统计学上的显著相关性。它还能优化信息损失和处理速度之间的权衡，在不增加处理负担的情况下，比文本摘要的VQA准确率提高了7%。", "conclusion": "ViSIL作为一个统一的评测指标，能够直接比较不同结构的多模态摘要格式，并创造出帕累托最优边界，显示出优越的性能。"}}
{"id": "2601.09715", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09715", "abs": "https://arxiv.org/abs/2601.09715", "authors": ["Adam Bradley", "John Hastings", "Khandaker Mamun Ahmed"], "title": "Introducing Axlerod: An LLM-based Chatbot for Assisting Independent Insurance Agents", "comment": "6 pages, 2 figures, 1 table", "summary": "The insurance industry is undergoing a paradigm shift through the adoption of artificial intelligence (AI) technologies, particularly in the realm of intelligent conversational agents. Chatbots have evolved into sophisticated AI-driven systems capable of automating complex workflows, including policy recommendation and claims triage, while simultaneously enabling dynamic, context-aware user engagement. This paper presents the design, implementation, and empirical evaluation of Axlerod, an AI-powered conversational interface designed to improve the operational efficiency of independent insurance agents. Leveraging natural language processing (NLP), retrieval-augmented generation (RAG), and domain-specific knowledge integration, Axlerod demonstrates robust capabilities in parsing user intent, accessing structured policy databases, and delivering real-time, contextually relevant responses. Experimental results underscore Axlerod's effectiveness, achieving an overall accuracy of 93.18% in policy retrieval tasks while reducing the average search time by 2.42 seconds. This work contributes to the growing body of research on enterprise-grade AI applications in insurtech, with a particular focus on agent-assistive rather than consumer-facing architectures.", "AI": {"tldr": "本论文展示了Axlerod的设计与实现，它是一个AI驱动的保险行业客户代理系统，具有良好的政策检索和用户互动能力，提高了保险代理的工作效率。", "motivation": "随着人工智能技术，特别是在智能会话代理领域的应用，保险行业正在经历一场范式转变，该论文的动机是开发一个能够通过自动化复杂工作流程，例如政策推荐和保险申请初审的AI驱动的系统，以提高保险代理的工作效率。", "method": "该论文设计并实现了一个名为Axlerod的AI驱动的会话界面，它利用自然语言处理（NLP）、检索增强生成（RAG）和领域特定知识的整合来解析用户意图，访问结构化的政策数据库并提供实时的相关回复，旨在提高独立保险代理的操作效率。", "result": "实验结果显示Axlerod在政策检索任务上的总体准确率为93.18%，平均搜索时间减少了2.42秒。", "conclusion": "研究表明AI技术在保险行业的有效应用以及提高独立保险代理的运营效率的潜力，为保险科技中的企业级AI应用程序提供了贡献，尤其在代理辅助架构方面。"}}
{"id": "2601.09859", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09859", "abs": "https://arxiv.org/abs/2601.09859", "authors": ["Anant Mehta", "Xiyuan Wei", "Xingyu Chen", "Tianbao Yang"], "title": "Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP", "comment": "Submitted to ICLR 2026", "summary": "CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.", "AI": {"tldr": "TuneCLIP, a self-supervised fine-tuning method, improves the performance of open-weight CLIP models across various tasks by mitigating performance degradation with a warm-up and fine-tuning stage.", "motivation": "To improve the general performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets, overcoming the issue of performance degradation observed with standard training protocols.", "method": "TuneCLIP, a self-supervised fine-tuning framework with two key components: (1) a warm-up stage for recovering optimization statistics, (2) a fine-tuning stage of optimizing a new contrastive loss.", "result": "TuneCLIP consistently improves performance across model architectures and scales, achieving gains of up to +2.5% on ImageNet and +1.2% on the DataComp benchmark.", "conclusion": "TuneCLIP sets a new strong baseline for efficient post-pretraining adaptation, achieving significant performance gains without the need for extensive retraining from scratch."}}
{"id": "2601.09716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09716", "abs": "https://arxiv.org/abs/2601.09716", "authors": ["Derguene Mbaye", "Tatiana D. P. Mbengue", "Madoune R. Seye", "Moussa Diallo", "Mamadou L. Ndiaye", "Dimitri S. Adjanohoun", "Cheikh S. Wade", "Djiby Sow", "Jean-Claude B. Munyaka", "Jerome Chenal"], "title": "Opportunities and Challenges of Natural Language Processing for Low-Resource Senegalese Languages in Social Science Research", "comment": null, "summary": "Natural Language Processing (NLP) is rapidly transforming research methodologies across disciplines, yet African languages remain largely underrepresented in this technological shift. This paper provides the first comprehensive overview of NLP progress and challenges for the six national languages officially recognized by the Senegalese Constitution: Wolof, Pulaar, Sereer, Joola, Mandingue, and Soninke. We synthesize linguistic, sociotechnical, and infrastructural factors that shape their digital readiness and identify gaps in data, tools, and benchmarks. Building on existing initiatives and research works, we analyze ongoing efforts in text normalization, machine translation, and speech processing. We also provide a centralized GitHub repository that compiles publicly accessible resources for a range of NLP tasks across these languages, designed to facilitate collaboration and reproducibility. A special focus is devoted to the application of NLP to the social sciences, where multilingual transcription, translation, and retrieval pipelines can significantly enhance the efficiency and inclusiveness of field research. The paper concludes by outlining a roadmap toward sustainable, community-centered NLP ecosystems for Senegalese languages, emphasizing ethical data governance, open resources, and interdisciplinary collaboration.", "AI": {"tldr": "这篇论文概述了塞内加尔六种官方语言在自然语言处理方面的进展和挑战，并提出了可持续发展的社区导向型NLP生态系统的路线图。", "motivation": "非洲语言在NLP技术变革中代表性不足，论文旨在填补塞内加尔六种语言在数据、工具和基准方面的空白，促进社会科学研究的效率和包容性。", "method": "分析并总结了塞内加尔宪法认可的六种官方语言（沃尔夫、富拉尼、塞雷尔、朱拉、曼丁哥和松尼凯克）在自然语言处理方面的进展与挑战，论文集成了影响这些语言数字化准备的语言、社会技术和基础设施因素。", "result": "分析了在文本规范化、机器翻译和语音处理方面的努力，建立了一个综合的GitHub存储库，汇编了这些语言的NLP任务资源，促进了合作和可复现性。", "conclusion": "论文提出了一条通往可持续发展的社区导向型NLP生态系统的路线图，强调了数据治理、开放资源和跨学科合作的重要性。"}}
{"id": "2601.09866", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09866", "abs": "https://arxiv.org/abs/2601.09866", "authors": ["Kiarie Ndegwa", "Andreas Gros", "Tony Chang", "David Diaz", "Vincent A. Landau", "Nathan E. Rutenbeck", "Luke J. Zachmann", "Guy Bayes", "Scott Conway"], "title": "VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching", "comment": "12 pages, 8 figures, 2 tables", "summary": "We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.", "AI": {"tldr": "VibrantSR 是一种新的生成式超分辨率框架，它利用Sentinel-2季节合成影像估算0.5米分辨率的冠层高度模型，能够实现跨大陆范围的森林监测和碳核算，具有更高的成本效益和采集频率优势。", "motivation": "研究的动机在于，当前方法如基于航空影像的超分辨率技术受限于采集频率低和成本高昂的问题，因此，提出了一种新的基于Sentinel-2季节合成影像的高分辨率冠层高度模型估算方法，以实现更频繁和成本效益更高的森林监测和碳核算。", "method": "我们提出了VibrantSR，这是一种生成式的超分辨率框架，它可以从10米分辨率的Sentinel-2影像中估算出0.5米分辨率的冠层高度模型（CHM）。与基于航空影像的方法不同，该方法受限于不频繁且不规则的采集计划，VibrantSR利用了全球可获取的Sentinel-2季节合成影像，从而能够实现季节到年度频率的一致监测。", "result": "在对美国西部22个EPA三级生态区进行了空间分离验证后，VibrantSR在树冠高度大于等于2米的情况下，实现了4.39米的平均绝对误差（MAE），优于Meta（4.83米）、LANDFIRE（5.96米）和ETH（7.05米）等基于卫星的技术。尽管基于航空影像的VibrantVS具有更高的精度优势（2.71米的MAE），但VibrantSR能够在不依赖成本高昂且时间不规律的航空采集的情况下，实现跨大陆范围的森林监测和碳核算。", "conclusion": "虽然基于航空影像的方法在精度上可能仍有优势，但VibrantSR更为经济高效地实现了高分辨率冠层高度模型的估算，使得跨大陆范围的森林监测和碳核算成为可能。"}}
{"id": "2601.09717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09717", "abs": "https://arxiv.org/abs/2601.09717", "authors": ["Yiwei Yan", "Hao Li", "Hua He", "Gong Kai", "Zhengyi Yang", "Guanfeng Liu"], "title": "SALP-CG: Standard-Aligned LLM Pipeline for Classifying and Grading Large Volumes of Online Conversational Health Data", "comment": null, "summary": "Online medical consultations generate large volumes of conversational health data that often embed protected health information, requiring robust methods to classify data categories and assign risk levels in line with policies and practice. However, existing approaches lack unified standards and reliable automated methods to fulfill sensitivity classification for such conversational health data. This study presents a large language model-based extraction pipeline, SALP-CG, for classifying and grading privacy risks in online conversational health data. We concluded health-data classification and grading rules in accordance with GB/T 39725-2020. Combining few-shot guidance, JSON Schema constrained decoding, and deterministic high-risk rules, the backend-agnostic extraction pipeline achieves strong category compliance and reliable sensitivity across diverse LLMs. On the MedDialog-CN benchmark, models yields robust entity counts, high schema compliance, and accurate sensitivity grading, while the strongest model attains micro-F1=0.900 for maximum-level prediction. The category landscape stratified by sensitivity shows that Level 2-3 items dominate, enabling re-identification when combined; Level 4-5 items are less frequent but carry outsize harm. SALP-CG reliably helps classify categories and grading sensitivity in online conversational health data across LLMs, offering a practical method for health data governance. Code is available at https://github.com/dommii1218/SALP-CG.", "AI": {"tldr": "本研究提出了一种新的方法SALP-CG，用于对在线医疗对话中的隐私风险进行分类和分级，展示了较高的准确率。", "motivation": "在线医疗咨询产生了大量的包含受保护的健康信息的对话健康数据，然而现有的方法缺乏统一的标准和可靠的自动化方法对这类对话健康数据进行敏感度分类。", "method": "此研究提出了一种基于大型语言模型的提取管道SALP-CG，用于对在线医疗对话中的隐私风险进行分类和分级。该管道结合了少量示例指导、JSON Schema约束解码以及确定性的高风险规则，能够在多种大型语言模型中实现类别的合规性和敏感度的可靠性。", "result": "在MedDialog-CN基准测试中，模型展示出稳健的实体计数、高模式合规性和准确的敏感度分级。最强的模型在最大级别预测上达到了0.900的微F1值。", "conclusion": "SALP-CG能够可靠地帮助对在线医疗对话中的数据进行分类和敏感度分级，为健康数据治理提供了实用的方法。"}}
{"id": "2601.09879", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09879", "abs": "https://arxiv.org/abs/2601.09879", "authors": ["Yang Xing", "Jiong Wu", "Savas Ozdemir", "Ying Zhang", "Yang Yang", "Wei Shao", "Kuang Gong"], "title": "MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation", "comment": null, "summary": "Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.", "AI": {"tldr": "提出了MedVL-SAM2，一种统一的3D多模态医学模型，同时支持报告生成、视觉问题解答和多范式分割。通过多阶段训练结合语言理解和分割目标，实现了精确的空间定位和高级视觉推理。", "motivation": "医学视觉语言模型在图像级文本相关任务上已有良好表现，但在3D医学模型中实现细粒度视觉定位和体积空间推理仍具挑战性，且难以在一个通用框架中统一这些能力。", "method": "MedVL-SAM2通过整合图像级推理和像素级感知，支持报告生成、VQA和多范式分割。通过多阶段训练流程，首先在大规模3D CT图像-文本对上进行预训练，然后在综合3D CT分割数据集上联合优化语言理解和分割目标。", "result": "实验结果展示了该模型在报告生成、VQA和多3D分割任务上达到前沿性能，并且能够实现可靠的3D视觉定位、可控的交互分割和稳健的跨模态推理。", "conclusion": "结合高阶语义推理和精确3D定位在统一的3D医学视觉语言模型中是可行的。"}}
{"id": "2601.09718", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09718", "abs": "https://arxiv.org/abs/2601.09718", "authors": ["Jing-Yi Zeng", "Guan-Hua Huang"], "title": "StatLLaMA: A multi-stage training framework for building a domain-optimized statistical language model", "comment": "31 pages, 3 figures", "summary": "This study investigates how to efficiently build a domain-specialized large language model (LLM) for statistics using the lightweight LLaMA-3.2-3B family as the foundation model (FM). We systematically compare three multi-stage training pipelines, starting from a base FM with no instruction-following capability, a base FM augmented with post-hoc instruction tuning, and an instruction-tuned FM with strong general reasoning abilities across continual pretraining, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) preference alignment, and downstream task adaptation. Results show that pipelines beginning with a base FM fail to develop meaningful statistical reasoning, even after extensive instruction tuning, SFT, or RLHF alignment. In contrast, starting from LLaMA-3.2-3B-Instruct enables effective domain specialization. A comprehensive evaluation of SFT variants reveals clear trade-offs between domain expertise and general reasoning ability. We further demonstrate that direct preference optimization provides stable and effective RLHF preference alignment. Finally, we show that downstream fine-tuning must be performed with extremely low intensity to avoid catastrophic forgetting in highly optimized models. The final model, StatLLaMA, achieves strong and balanced performance on benchmarks of mathematical reasoning, common-sense reasoning, and statistical expertise, offering a practical blueprint for developing resource-efficient statistical LLMs. The code is available at https://github.com/HuangDLab/StatLLaMA.", "AI": {"tldr": "研究了如何使用轻量级LLaMA-3.2-3B族作为基础模型来高效构建特定领域的大语言模型(统计学领域)。比较三种多阶段训练管道，并分析了结果和下游任务调整。最终模型StatLLaMA在统计学专业以及其他理性推理方面表现出色，提供了一个开发资源高效统计LLM的实用蓝图。", "motivation": "探索在统计学领域构建大型语言模型的方法，旨在提高效率同时保持性能。", "method": "系统比较三种多阶段训练管道：从无指令跟随能力的基础模型开始、从增强后指令调整的基础模型开始、以及从指令优化基础模型开始。使用连续预训练、监督精细调整、人类反馈的强化学习，以及下游任务适应。", "result": "结果显示，直接从无指令基础模型开始的管道无法实现有效的统计推理。而从LLaMA-3.2-3B指令调整模型开始则可以有效实现领域专业化。SFT变体的综合评估表明领域专业知识和一般推理能力之间存在权衡。直接偏好优化提供稳定的RLHF偏好校准。下游细调须在极低强度下进行以避免灾难性遗忘。", "conclusion": "最终模型StatLLaMA在数学推理、常识推理和统计学专业方面表现均衡。表明开发资源高效的统计LLM是可行的。"}}
{"id": "2601.09881", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09881", "abs": "https://arxiv.org/abs/2601.09881", "authors": ["Weili Nie", "Julius Berner", "Nanye Ma", "Chao Liu", "Saining Xie", "Arash Vahdat"], "title": "Transition Matching Distillation for Fast Video Generation", "comment": null, "summary": "Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd", "AI": {"tldr": "The paper introduces Transition Matching Distillation (TMD), a method to distill large video diffusion models into efficient few-step generators for real-time applications, by matching multi-step denoising trajectories with lightweight, few-step probability transitions.", "motivation": "The inefficiency of multi-step sampling processes in large video diffusion models limits their use in real-time interactive applications. TMD aims to make these models efficient without sacrificing visual quality.", "method": "TMD distills video diffusion models by decomposing them into a main backbone for capturing semantic representations and a flow head for lightweight inner flow updates, modeling each transition as a conditional flow through distribution matching distillation.", "result": "Experiments demonstrate that TMD offers a strong trade-off between generation speed and visual quality, outperforming existing distilled models under comparable inference costs.", "conclusion": "TMD is presented as a novel framework for efficiently distilling video diffusion models into faster, yet high-quality few-step generators for real-time applications."}}
