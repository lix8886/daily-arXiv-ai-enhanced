<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decoder-based Sense Knowledge Distillation](https://arxiv.org/abs/2602.22351)
*Qitong Wang,Mohammed J. Zaki,Georgios Kollias,Vasileios Kalantzis*

Main category: cs.CL

> 本文提出了DSKD框架，它在训练基于解码器的LLMs时整合词法资源，无需在推理时进行字典查找，从而提高了解码器的知识蒸馏性能。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型(Large language models, LLMs)学习获得的上下文嵌入可以捕捉丰富的语义信息，但通常忽视了如词义和关系等结构化的词法知识。之前的工作表明，结合词典可以改善编码器模型的知识蒸馏，但对于生成模型来说，将其应用到解码器仍然是具有挑战性的。

**Method:** 通过引入Decoder-based Sense Knowledge Distillation (DSKD)框架，该框架在训练解码器模型时整合词法资源，但推理时不需要字典查询。

**Result:** 广泛的实验表明，DSKD显著提高了解码器的知识蒸馏性能，使生成模型能够继承结构化的语义，同时保持高效的训练。

**Conclusion:** 实验结果显示，通过DSKD框架，生成模型能够有效继承结构化的语义知识，且保持了高效的训练过程。

**Abstract:** Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.

</details>


### [2] [Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts](https://arxiv.org/abs/2602.22359)
*Arno Simons*

Main category: cs.CL

> 本文通过两阶段GPT-5管道测试大语言模型支持解释性引用文本分析的能力，结果表明提示支架影响模型解释范围。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是测试大型语言模型在解释性引用文本分析中的支持作用，强调提示敏感性分析作为一种方法论问题。

**Method:** 该研究采用了两阶段的GPT-5管道，首先对引用文本进行表面分类和预期通过，然后对引用和被引用的完整文本进行跨文档解释性重建。研究设计了平衡的2x3实验来改变提示支架和框架。

**Result:** 研究结果表明，GPT-5的表面通过高度稳定，但在解释性重建中，提示支架和框架会系统地影响模型所突出的可能解释和词汇。

**Conclusion:** 该研究揭示了使用大型语言模型作为引导共析者进行可窥视和可争议的解释性引用文本分析的机会和风险。

**Abstract:** This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as "supplementary". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.

</details>


### [3] [Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework](https://arxiv.org/abs/2602.22391)
*Rakib Ullah,Mominul islam,Md Sanjid Hossain,Md Ismail Hossain*

Main category: cs.CL

> 为了解决低资源语言 Bengali 中检测梗中的有害和煽动性内容的问题，研究人员提出了 Bn-HIB 数据集和 MCFM 模型。Bn-HIB 是第一个在 Bengali 梗中区分煽动性内容和直接仇恨言论的数据集，而 MCFM 模型则在数据集上显示出有效性。

<details>
  <summary>Details</summary>

**Motivation:** 由于检测低资源语言如 Bengali 中隐晦且具有文化特异性的梗特别具有挑战性，现有研究主要集中在高资源语言上，作者希望填补这一研究空白。

**Method:** 提出了 MCFM（多模态协同注意力融合模型），该模型能够相互分析梗的视觉和文本特征，并融合这些特征以提高分类准确性。

**Result:** 互联网梗已成为社交媒体上的一种主要表达形式，包括在讲 Bengali 的社区中。虽然梗通常是幽默的，但也可能被用来传播针对个人和团体的不友好、有害和煽动性内容。由于其讽刺性、隐晦性和文化特异性，检测这种内容非常具有挑战性。这一问题在像 Bengali 这样的低资源语言中尤为严重，因为现有的研究主要集中在高资源语言上。为了解决这个关键的研究空白，我们介绍了一个新型数据集 Bn-HIB（Bangla Hate Inflammatory Benign），包含 3,247 个手工标注的 Bengali 梗，分类为善意、仇恨和煽动性内容，特别是在 Bengali 梗中区分煽动性内容和直接仇恨言论的第一份数据集。此外，我们提出了 MCFM（多模态协同注意力融合模型），这是一个简单而有效的架构，能够相互分析梗的视觉和文本元素。MCFM 使用协同注意力机制来识别并融合每个模态的关键特征，从而实现更准确的分类。实验表明，MCFM 在 Bn-HIB 数据集上显著优于几种最先进的模型，证明其在这一具有挑战性的任务中的有效性。警告：这份材料可能包含一些令人不安的内容。请观众自行斟酌。

**Conclusion:** 实验结果显示，MCFM 模型在 Bn-HIB 数据集上显著优于其他几种最先进的模型，证明了该模型在检测 Bengali 梗中煽动性和仇恨内容方面具有有效性。

**Abstract:** Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.

</details>


### [4] [SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context](https://arxiv.org/abs/2602.22404)
*Aishwarya Verma,Laud Ammah,Olivia Nercy Ndlovu Lucas,Andrew Zaldivar,Vinodkumar Prabhakaran,Sunipa Dev*

Main category: cs.CL

> 本文针对撒哈拉以南非洲国家的自然语言处理资源稀缺问题，开发出了一个包含英语和15种本土语言刻板印象的数据集，提升了该地区在AI模型安全性评估方面的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的刻板印象资源库在全球范围内的覆盖不足，而本研究旨在通过针对性的扩展，解决自然语言处理资源中撒哈拉以南非洲地区代表性严重不足的问题。

**Method:** 该研究通过电话调查，采用社区参与的方法，用当地语言进行语料收集，针对加纳、肯尼亚、尼日利亚和南非四个撒哈拉以南非洲国家建立了多语言刻板印象资源库。

**Result:** 该研究建立了一个包含3,534个英语刻板印象和3,206个本土语言刻板印象的数据集，覆盖了15种本土语言。

**Conclusion:** 本研究提出了一个可重复的方法，该方法对撒哈拉以南非洲地区复杂的语言多样性和传统口语较为敏感，确保了刻板印象资源库的广泛覆盖。

**Abstract:** Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.

</details>


### [5] [Causality $\neq$ Invariance: Function and Concept Vectors in LLMs](https://arxiv.org/abs/2602.22424)
*Gustaw Opiełka,Hannes Rosenbusch,Claire E. Stevenson*

Main category: cs.CL

> 研究发现，虽然大型语言模型的确含有抽象概念的表示，但这些表示与驱动上下文学习性能的FVs不同，CVs更适应跨输入格式和语言的情况，证明了LLMs拥有较为稳定的抽象概念表示方式。

<details>
  <summary>Details</summary>

**Motivation:** 探讨大型语言模型是否以抽象方式表示概念，即独立于输入格式。

**Method:** 研究考察了在不同输入格式（如开放式问题与多项选择题）下，大型语言模型(LLM)中的功能向量(FVs)和概念向量(CVs)的表现。FVs是通过上下文学习任务生成的紧凑表示，而CVs通过表示相似性分析(RSA)挑选出的注意力头组成，以更稳定地编码概念。

**Result:** 发现FVs并不是完全不变的，当从不同的输入格式中提取时，即使是针对相同的概念，它们也几乎是正交的。相比之下，CVs在不同的输入格式和语言中表现出了更好的泛化能力。

**Conclusion:** LLMs包含抽象的概念表示，但这些与驱动ICL性能的功能向量FVs不同。概念向量CVs在不同输入格式和语言之间具有较好的泛化能力，显示了LLMs进行抽象概念表示的不同机制。

**Abstract:** Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.

</details>


### [6] [A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection](https://arxiv.org/abs/2602.22449)
*Mirza Raquib,Asif Pervez Polok,Kedar Nath Biswas,Rahat Uddin Azad,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CL

> 本文针对孟加拉语网络欺凌的多标签检测提出了一种结合Transformer和LSTM的融合架构，并通过多种采样策略和评价指标来评价模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前多数研究使用单标签分类方法，而忽视了多标签分类的必要性。特别是在资源匮乏的低资源语言如孟加拉语中，建立准确且具有一般性的模型更具挑战性。因此，本文旨在解决这些局限性，并应对孟加拉语网络欺凌检测的需求。

**Method:** 本文提出了一种结合BanglaBERT-Large和双层堆叠LSTM的融合架构，旨在同时捕捉上下文信息和序列依赖，通过不同的采样策略来解决类别不平衡问题。

**Result:** 使用多个评估指标（如准确率、精确率、召回率、F1分数等）进行评估，并采用5折交叉验证来检测架构的泛化能力。

**Conclusion:** 该混合架构能够有效改善孟加拉语中网络欺凌多标签检测的性能，具有良好的泛化能力。

**Abstract:** Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each comment contains only one type of abuse. In reality, a single comment may include overlapping forms such as threats, hate speech, and harassment. Therefore, multilabel detection is both realistic and essential. However, multilabel cyberbullying detection has received limited attention, especially in low-resource languages like Bangla, where robust pre-trained models are scarce. Developing a generalized model with moderate accuracy remains challenging. Transformers offer strong contextual understanding but may miss sequential dependencies, while LSTM models capture temporal flow but lack semantic depth. To address these limitations, we propose a fusion architecture that combines BanglaBERT-Large with a two-layer stacked LSTM. We analyze their behavior to jointly model context and sequence. The model is fine-tuned and evaluated on a publicly available multilabel Bangla cyberbullying dataset covering cyberbully, sexual harassment, threat, and spam. We apply different sampling strategies to address class imbalance. Evaluation uses multiple metrics, including accuracy, precision, recall, F1-score, Hamming loss, Cohens kappa, and AUC-ROC. We employ 5-fold cross-validation to assess the generalization of the architecture.

</details>


### [7] [Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads](https://arxiv.org/abs/2602.22453)
*Shaswat Patel,Vishvesh Trivedi,Yue Han,Yihuai Hong,Eunsol Choi*

Main category: cs.CL

> 本文研究了多语言环境中的检索头，并进一步在跨语言环境中发现检索-转换头（RTH），表明 RTH 对多语言大语言模型的连贯推理更为重要。屏蔽 RTH 会导致更大的性能下降。

<details>
  <summary>Details</summary>

**Motivation:** 近期研究发现 Transformer 中的注意力头中有一部分为检索头，负责从上下文中检索信息。我们进一步研究了多语言环境中的检索头，发现多语言模型中的检索头经常在多种语言中共享。

**Method:** 我们首先在多语言环境中研究检索头，并进一步扩展到跨语言场景。通过实验，我们确定了控制特定目标语言输出转换的检索-转换头（RTH）。

**Result:** 我们的实验表明，RTH 与检索头是不同的，并且在多语言大语言模型的连贯推理中更为重要。在四个多语言基准测试和两种模型家族上，我们发现屏蔽 RTH 会导致比屏蔽检索头更大的性能下降。

**Conclusion:** 我们的工作通过孤立负责映射到目标语言的注意力头来加深对多语言大语言模型的理解。

**Abstract:** Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Enabling clinical use of foundation models in histopathology](https://arxiv.org/abs/2602.22347)
*Audun L. Henriksen,Ole-Johan Skrede,Lisa van der Schee,Enric Domingo,Sepp De Raedt,Ilyá Kostolomov,Jennifer Hay,Karolina Cyll,Wanja Kildal,Joakim Kalsnes,Robert W. Williams,Manohar Pradhan,John Arne Nesheim,Hanne A. Askautrud,Maria X. Isaksen,Karmele Saez de Gordoa,Miriam Cuatrecasas,Joanne Edwards,TransSCOT group,Arild Nesbakken,Neil A. Shepherd,Ian Tomlinson,Daniel-Christoph Wagner,Rachel S. Kerr,Tarjei Sveinsgjerd Hveem,Knut Liestøl,Yoshiaki Nakamura,Marco Novelli,Masaaki Miyo,Sebastian Foersch,David N. Church,Miangela M. Lacle,David J. Kerr,Andreas Kleppe*

Main category: cs.CV

> 研究发现，在训练特定任务模型时引入新的稳健性损失方法，能有效降低技术变异性的影响，提高模型在计算病理学中预测的准确性与稳健性。

<details>
  <summary>Details</summary>

**Motivation:** 目前的基础模型在捕获生物相关特征的同时，也捕获了前分析和扫描仪特异性变化，这些变化可能会偏置任务特定模型的预测。因此，需要开发一种方法来减少这些技术变化的影响。

**Method:** 本研究提出在训练特定任务模型时引入新颖的稳健性损失，以减少对技术变异性的敏感性。研究设计了一个全面的实验框架，使用了来自6155名患者的27,042个全幻灯片图像（WSIs），基于八个流行的基础模型提取的特征来训练数千个模型。

**Result:** 实验结果表明，使用该方法不仅可以显著提高模型的稳健性，而且通过专注于生物相关特征，预测准确性也得到了提高。

**Conclusion:** 该方法成功减轻了基础模型在计算病理学中的稳健性问题，为开发适用于临床实践的真实世界数据的稳健计算病理学模型提供了可能，且无需重新训练基础模型。

**Abstract:** Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.

</details>


### [9] [Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search](https://arxiv.org/abs/2602.22361)
*Liping Meng,Fan Nie,Yunyun Zhang,Chao Han*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.

</details>


### [10] [AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction](https://arxiv.org/abs/2602.22376)
*Hanyang Liu,Rongjun Qin*

Main category: cs.CV

> 引入AeroDGS框架，通过单目几何提升模块和基于物理的优化模块，解决了单目航拍视频在动态建模中的深度模糊和不稳定问题，结果表明该方法在动态空中环境中优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法在单一视角的空中条件下进行三维重建时面临巨大的挑战，如宽阔的空间范围和动态对象的有限空间足迹及较大的运动差异，导致严重的深度模糊和不稳定的运动估计。为此，研究旨在解决这些问题。

**Method:** AeroDGS采用单目几何提升模块从单个航拍序列中重建可靠静态和动态几何结构，提供动态估计的稳定基础。此外，提出基于物理引导的优化模块，结合可微分地面支撑、竖直稳定性和平滑轨迹先验，将模棱两可的图像提示转化为物理上一致的运动。框架共同优化静态背景和动态实体，具有稳定几何结构和连贯的时间演化。

**Result:** 在合成和真实无人机场景上的实验表明，AeroDGS比现有方法在动态空中环境中获得了更高的重建保真度。

**Conclusion:** AeroDGS方法有效地提升了动态航空环境中单目重建的精度，在真实和合成的无人机场景中表现出色，展示了其在面对深度模糊和不稳定运动估计问题时的优势。

**Abstract:** Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.

</details>
