{"id": "2509.18113", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18113", "abs": "https://arxiv.org/abs/2509.18113", "authors": ["Xin Hu", "Yue Kang", "Guanzi Yao", "Tianze Kang", "Mengjie Wang", "Heyao Liu"], "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs", "comment": null, "summary": "This study addresses the generalization limitations commonly observed in\nlarge language models under multi-task and cross-domain settings. Unlike prior\nmethods such as SPoT, which depends on fixed prompt templates, our study\nintroduces a unified multi-task learning framework with dynamic prompt\nscheduling mechanism. By introducing a prompt pool and a task-aware scheduling\nstrategy, the method dynamically combines and aligns prompts for different\ntasks. This enhances the model's ability to capture semantic differences across\ntasks. During prompt fusion, the model uses task embeddings and a gating\nmechanism to finely control the prompt signals. This ensures alignment between\nprompt content and task-specific demands. At the same time, it builds flexible\nsharing pathways across tasks. In addition, the proposed optimization objective\ncenters on joint multi-task learning. It incorporates an automatic learning\nstrategy for scheduling weights, which effectively mitigates task interference\nand negative transfer. To evaluate the effectiveness of the method, a series of\nsensitivity experiments were conducted. These experiments examined the impact\nof prompt temperature parameters and task number variation. The results confirm\nthe advantages of the proposed mechanism in maintaining model stability and\nenhancing transferability. Experimental findings show that the prompt\nscheduling method significantly improves performance on a range of language\nunderstanding and knowledge reasoning tasks. These results fully demonstrate\nits applicability and effectiveness in unified multi-task modeling and\ncross-domain adaptation.", "AI": {"tldr": "a novel dynamic prompt scheduling framework for multi-task learning in large language models enhances model's ability to capture semantic differences across tasks, improving generalization and stability.", "motivation": "to overcome generalization limitations in large language models when applied in multi-task and cross-domain contexts, specifically addressing limitations of prior methods like SPoT which uses fixed prompt templates.", "method": "unified multi-task learning framework with dynamic prompt scheduling mechanism, incorporating task embeddings, a gating mechanism, and an adaptive learning strategy for scheduling weights.", "result": "the method effectively enhances model generalization abilities across tasks, maintaining stability and improving performance on language understanding and knowledge reasoning tasks.", "conclusion": "the proposed method significantly improves model performance across multi-task and cross-domain settings, demonstrating its applicability in unified multi-task modeling and cross-domain adaptation."}}
{"id": "2509.18122", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18122", "abs": "https://arxiv.org/abs/2509.18122", "authors": ["Yue Zhang", "Jiaxin Zhang", "Qiuyu Ren", "Tahsin Saffat", "Xiaoxuan Liu", "Zitong Yang", "Banghua Zhu", "Yi Ma"], "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models", "comment": "120 pages (including appendix)", "summary": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of\n\\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a\nbenchmark that evaluates LLMs' mathematical abilities across twelve core skill\ndimensions, grouped into three domains: knowledge and understanding, problem\nsolving and communication, and meta-skills and creativity. By categorizing\nproblems according to cognitive skills and designing tasks that isolate\nspecific abilities, GAUSS constructs comprehensive, fine-grained, and\ninterpretable profiles of models' mathematical abilities. These profiles\nfaithfully represent their underlying mathematical intelligence. To exemplify\nhow to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of\n\\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its\ndifferences relative to \\textsc{o4-mini-high}, thereby underscoring the value\nof multidimensional, skill-based evaluation.", "AI": {"tldr": "GAUSS是一个多维度数学能力评估基准测试，通过细致分类和特定任务评估评估对象的能力概况，展示了GPT-5-thinking和o4-mini-high之间的能力差异，强调了此类评估的重要性。", "motivation": "开发GAUSS基准测试的目的是为了更全面和细致地评估LLMs的数学能力，以便更真实地反映它们的数学智能。", "method": "引入了GAUSS（GAUSS（通用数学底层技能评估））基准测试，评估LLMs在十二个核心技能维度上的数学能力，分为三个域：知识与理解、问题解决与沟通及元技能和创造力。通过将问题按认知技能分类并设计任务来分离特定能力，GAUSS构建全面、细致且可解释的模型数学能力概况。", "result": "展示了如何使用GAUSS基准测试来导出GPT-5-thinking的能力概况，揭示了其优点和缺点以及与o4-mini-high的区别。", "conclusion": "强调了多维度技能评估的价值。"}}
{"id": "2509.18156", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18156", "abs": "https://arxiv.org/abs/2509.18156", "authors": ["Haoyu Wang", "Fengze Liu", "Jiayao Zhang", "Dan Roth", "Kyle Richardson"], "title": "Event Causality Identification with Synthetic Control", "comment": null, "summary": "Event causality identification (ECI), a process that extracts causal\nrelations between events from text, is crucial for distinguishing causation\nfrom correlation. Traditional approaches to ECI have primarily utilized\nlinguistic patterns and multi-hop relational inference, risking false causality\nidentification due to informal usage of causality and specious graphical\ninference. In this paper, we adopt the Rubin Causal Model to identify event\ncausality: given two temporally ordered events, we see the first event as the\ntreatment and the second one as the observed outcome. Determining their\ncausality involves manipulating the treatment and estimating the resultant\nchange in the likelihood of the outcome. Given that it is only possible to\nimplement manipulation conceptually in the text domain, as a work-around, we\ntry to find a twin for the protagonist from existing corpora. This twin should\nhave identical life experiences with the protagonist before the treatment but\nundergoes an intervention of treatment. However, the practical difficulty of\nlocating such a match limits its feasibility. Addressing this issue, we use the\nsynthetic control method to generate such a twin' from relevant historical\ndata, leveraging text embedding synthesis and inversion techniques. This\napproach allows us to identify causal relations more robustly than previous\nmethods, including GPT-4, which is demonstrated on a causality benchmark,\nCOPES-hard.", "AI": {"tldr": "新方法基于鲁宾因果模型，采用合成控制及高级文本技术，解决了传统方法易误判的缺陷，在因果性识别上优于GPT-4。", "motivation": "传统事件因果识别方法依赖于语言模式和多跳推理，容易导致虚假因果识别。为了提高识别的准确性，提出了新的方法。", "method": "该论文采用了鲁宾因果模型来识别事件因果关系，通过合成控制方法生成对照组，并利用文本嵌入综合和逆向工程技术来克服实际难题，从而更稳健地识别因果关系。", "result": "在COPES-hard因果性基准上，该方法比包括GPT-4在内的先前方法表现得更好。", "conclusion": "使用合成控制方法和先进的文本处理技术能够更好地识别事件因果关系，提升了因果识别的准确性。"}}
{"id": "2509.18158", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18158", "abs": "https://arxiv.org/abs/2509.18158", "authors": ["Seungyoun Yi", "Minsoo Khang", "Sungrae Park"], "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization", "comment": "9 pages, 4 figures. To appear in EMNLP 2025 Main Conference (Oral\n  Presentation)", "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM)\nperformance by refining prompts for specific tasks. However, prior APO methods\ntypically focus only on user prompts, rely on unstructured feedback, and\nrequire large sample sizes and long iteration cycles-making them costly and\nbrittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a\nnovel framework that jointly optimizes both system and user prompts through\nprincipled, low-overhead refinement. ZERA scores prompts using eight\ngeneralizable criteria with automatically inferred weights, and revises prompts\nbased on these structured critiques. This enables fast convergence to\nhigh-quality prompts using minimal examples and short iteration cycles. We\nevaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,\nsummarization, and code generation tasks. Experimental results demonstrate\nconsistent improvements over strong baselines. Further ablation studies\nhighlight the contribution of each component to more effective prompt\nconstruction. Our implementation including all prompts is publicly available at\nhttps://github.com/younatics/zera-agent.", "AI": {"tldr": "ZERA 是一个旨在优化大型语言模型（LLM）中系统和用户提示的框架，它通过结构化反馈和快速迭代，使用少量样例实现高质量提示的快速收敛，从而改进模型性能。", "motivation": "现有的自动提示优化（APO）方法通常只关注用户提示，依赖于非结构化的反馈，需要大量的样本和漫长的迭代周期，使其成本高昂且不稳定。", "method": "ZERA (Zero-init Instruction Evolving Refinement Agent) 提出了一种新的框架，该框架通过原理性的低开销精炼同时优化系统和用户提示。ZERA 使用八个可泛化的标准为提示评分，并基于这些结构化的批评对提示进行修改。", "result": "在五个大型语言模型和九个不同数据集上的实验结果表明，ZERA 在推理、总结和代码生成任务中相对于强大基线有显著的改进。进一步的消融研究表明，每个组件对更有效的提示构建的贡献。", "conclusion": "ZERA 的研究工作展示了其在多个数据集和任务中的有效性，并表明联合优化系统和用户提示是提高 LLM 性能的有效途径。"}}
{"id": "2509.18159", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18159", "abs": "https://arxiv.org/abs/2509.18159", "authors": ["Akwasi Asare", "Ulas Bagci"], "title": "PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset", "comment": null, "summary": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related\nmorbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as\ncritical precursors according to the World Health Organization (WHO). Early and\naccurate segmentation of polyps during colonoscopy is essential for reducing\nCRC progression, yet manual delineation is labor-intensive and prone to\nobserver variability. Deep learning methods have demonstrated strong potential\nfor automated polyp analysis, but their limited interpretability remains a\nbarrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an\nexplainable deep learning framework that integrates the U-Net architecture with\nGradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp\nsegmentation. The model was trained and evaluated on the Kvasir-SEG dataset of\n1000 annotated endoscopic images. Experimental results demonstrate robust\nsegmentation performance, achieving a mean Intersection over Union (IoU) of\n0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)\non training and validation sets. Grad-CAM visualizations further confirmed that\npredictions were guided by clinically relevant regions, enhancing transparency\nand trust in the model's decisions. By coupling high segmentation accuracy with\ninterpretability, PolypSeg-GradCAM represents a step toward reliable,\ntrustworthy AI-assisted colonoscopy and improved early colorectal cancer\nprevention.", "AI": {"tldr": "本研究提出了一种新的可解释深度学习框架PolypSeg-GradCAM，实现了息肉的准确分割，并通过Grad-CAM增强了模型决策的透明度。", "motivation": "研究动机在于提高早期和准确的息肉分割，这是结直肠癌预防的重要步骤。现有的手动分割既耗时又易出错，而深度学习方法虽然有潜力但解释性差，限制了其临床应用。", "method": "本研究介绍了PolypSeg-GradCAM，这是一个结合了U-Net架构和梯度加权类激活映射（Grad-CAM）的可解释深度学习框架，旨在实现透明的息肉分割。", "result": "实验结果显示，该模型在测试集上达到平均IoU为0.9257，且在训练和验证数据集中骰子系数（F-score > 0.96）表现始终较高。Grad-CAM可视化进一步表明，模型的预测由临床上相关区域指导。", "conclusion": "通过结合高分割精度和解释性，PolypSeg-GradCAM是一个朝着可靠、可信赖的AI辅助结肠镜检查迈进的重要步骤。"}}
{"id": "2509.18163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18163", "abs": "https://arxiv.org/abs/2509.18163", "authors": ["Haodong Zhao", "Chenyan Zhao", "Yansi Li", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning", "comment": "Work in progress", "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to\ntheir application in complex, knowledge-intensive domains. In real-world\nscenarios, LLMs are often augmented with external information that can be\nhelpful, irrelevant, or even misleading. This paper investigates the causal\nimpact of such auxiliary information on the reasoning process of LLMs with\nexplicit step-by-step thinking capabilities. We introduce SciAux, a new dataset\nderived from ScienceQA, to systematically test the robustness of the model\nagainst these types of information. Our findings reveal a critical\nvulnerability: the model's deliberative \"thinking mode\" is a double-edged\nsword. While helpful context improves accuracy, misleading information causes a\ncatastrophic drop in performance, which is amplified by the thinking process.\nInstead of conferring robustness, thinking reinforces the degree of error when\nprovided with misinformation. This highlights that the challenge is not merely\nto make models \"think\", but to endow them with the critical faculty to evaluate\nthe information upon which their reasoning is based. The SciAux dataset is\navailable at https://huggingface.co/datasets/billhdzhao/SciAux.", "AI": {"tldr": "研究揭示，即使是有逐步思考能力的LLMs，也会因为外部误导信息而出现性能骤降的问题。SciAux数据集的建立，旨在更系统性地研究这类模型的鲁棒性。", "motivation": "现有的研究表明，辅助信息对于提升LLMs的推理能力是至关重要的。然而，在实际应用中这些信息可能是误导性的，这对LLMs的推理性能构成了威胁。研究动机在于探索外部信息对具备逐步思考能力的LLMs推理过程的因果影响。", "method": "研究人员采用了一个新的数据集SciAux，基于ScienceQA构建，旨在系统性测试LLMs对不同类型辅助信息的鲁棒性。通过对比研究有用信息和误导信息对模型推理性能的影响来展开实验。", "result": "该论文研究了大型语言模型（LLMs）在面对真实世界情境时，借助外部信息进行推理的能力。外部信息可能是有用的、无关的或误导性的。研究引入了名为SciAux的新数据集来系统测试模型面对不同类型信息的鲁棒性。研究表明，模型的“思考模式”虽然能提高有用信息的存在情况下的准确性，但同时也会因为误解信息而显著降低性能。这表明，难点不仅仅在于使模型会“思考”，更在于赋予它们评估所依据信息的能力。SciAux数据集可以在https://huggingface.co/datasets/billhdzhao/SciAux中获取。", "conclusion": "研究揭示了LLMs在面对误导信息时的显著脆弱性，即使是经过精心设计的“思考过程”也会加剧这种错误。这一发现强调，除了使模型具有思考能力，还需要提升模型的批判性思维能力来评估所依赖的信息。"}}
{"id": "2509.18160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18160", "abs": "https://arxiv.org/abs/2509.18160", "authors": ["Akwasi Asare", "Isaac Baffour Senkyire", "Emmanuel Freeman", "Simon Hilary Ayinedenaba Aluze-Ele", "Kelvin Kwao"], "title": "PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis", "comment": null, "summary": "Diabetic retinopathy is a leading cause of vision loss among adults and a\nmajor global health challenge, particularly in underserved regions. This study\npresents PerceptronCARE, a deep learning-based teleophthalmology application\ndesigned for automated diabetic retinopathy detection using retinal images. The\nsystem was developed and evaluated using multiple convolutional neural\nnetworks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine\nthe optimal balance between accuracy and computational efficiency. The final\nmodel classifies disease severity with an accuracy of 85.4%, enabling real-time\nscreening in clinical and telemedicine settings. PerceptronCARE integrates\ncloud-based scalability, secure patient data management, and a multi-user\nframework, facilitating early diagnosis, improving doctor-patient interactions,\nand reducing healthcare costs. This study highlights the potential of AI-driven\ntelemedicine solutions in expanding access to diabetic retinopathy screening,\nparticularly in remote and resource-constrained environments.", "AI": {"tldr": "The paper presents PerceptronCARE, a deep learning-based teleophthalmology application for automated diabetic retinopathy detection with 85.4% accuracy, designed for use in clinical and telemedicine settings.", "motivation": "The motivation is to address the significant global health issue of diabetic retinopathy, especially in underserved regions lacking adequate healthcare resources.", "method": "The method involves developing and evaluating PerceptronCARE using multiple convolutional neural networks (ResNet-18, EfficientNet-B0, and SqueezeNet) for accuracy and computational efficiency.", "result": "The result is an automated diabetic retinopathy detection system with 85.4% accuracy, designed for real-time screening and integration with cloud-based scalability and secure patient data management.", "conclusion": "The conclusion is that AI-driven teleophthalmology solutions like PerceptronCARE can help expand access to diabetic retinopathy screening in remote and resource-limited settings."}}
{"id": "2509.18167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18167", "abs": "https://arxiv.org/abs/2509.18167", "authors": ["Junlin Wang", "Zehao Wu", "Shaowei Lu", "Yanlan Li", "Xinghao Huang"], "title": "SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework", "comment": "5 pages,2 figures, IRAC under review", "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess external knowledge sources, but the effectiveness of RAG relies on the\ncoordination between the retriever and the generator. Since these components\nare developed independently, their interaction is often suboptimal: the\nretriever may return irrelevant or redundant documents, while the generator may\nfail to fully leverage retrieved evidence. In this work, we propose a\nprocess-supervised multi-agent framework to bridge the gap between retriever\nand generator. The framework introduces two lightweight agents: a Decision\nMaker, which determines when to continue retrieval or stop for answer\ngeneration, and a Knowledge Selector, which filters retrieved documents to\nretain only the most useful evidence. To provide fine-grained supervision, we\nemploy an LLM-as-a-Judge that evaluates each intermediate action with\nprocess-level rewards, ensuring more accurate credit assignment than relying\nsolely on final answer correctness. We further adopt a tree-structured rollout\nstrategy to explore diverse reasoning paths, and train both agents with\nProximal Policy Optimization (PPO) in an end-to-end manner. Experiments on\nsingle-hop and multi-hop question answering benchmarks show that our approach\nachieves higher accuracy, more stable convergence, and produces more\ninterpretable reasoning trajectories compared with standard RAG baselines.\nImportantly, the proposed framework is modular and plug-and-play, requiring no\nmodification to the retriever or generator, making it practical for real-world\nRAG applications.", "AI": {"tldr": "本文提出了一种多智能体框架来改进RAG中检索器和生成器的协调，引入了两个智能体来优化检索和生成过程，并使用大型语言模型和树状展开策略来提供监督和探索多种推理路径，实验证明其有效性。", "motivation": "论文旨在解决检索增强生成（RAG）中检索器与生成器不协调的问题。当前存在的问题是检索器可能返回不相关的文档，而生成器可能未能充分利用检索到的证据，因此，作者希望改进这一现状。", "method": "本研究提出了一种基于过程监督的多智能体框架，以弥合检索器和生成器之间的差距。框架引入了两个轻量级智能体：决策者（决定继续检索还是停止以生成答案）和知识选择器（筛选检索到的文档，保留最有用的证据）。为了提供细粒度的监督，使用了一个作为评判者的大型语言模型，它根据过程级别的奖励评估每个中间动作，确保比仅依赖最终答案正确性更准确的归因。研究还采用了树状展开策略来探索多样化的推理路径，并通过端到端的方式使用近端策略优化（PPO）来训练这两个智能体。", "result": "实验结果显示，与标准的RAG基线相比，该方法在单跳和多跳问题回答基准上实现了更高的准确性、更稳定的收敛性，并生成了更可解释的推理轨迹。", "conclusion": "研究提出的框架具有模块化和即插即用的特性，无需对检索器或生成器进行任何修改，使其在实际的RAG应用中有实用性。"}}
{"id": "2509.18165", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18165", "abs": "https://arxiv.org/abs/2509.18165", "authors": ["Xiuding Cai", "Yaoyao Zhu", "Linjie Fu", "Dong Miao", "Yu Yao"], "title": "Self Identity Mapping", "comment": "Early accepted by Neural Networks 2025", "summary": "Regularization is essential in deep learning to enhance generalization and\nmitigate overfitting. However, conventional techniques often rely on\nheuristics, making them less reliable or effective across diverse settings. We\npropose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic\nregularization framework that leverages an inverse mapping mechanism to enhance\nrepresentation learning. By reconstructing the input from its transformed\noutput, SIM reduces information loss during forward propagation and facilitates\nsmoother gradient flow. To address computational inefficiencies, We instantiate\nSIM as $ \\rho\\text{SIM} $ by incorporating patch-level feature sampling and\nprojection-based method to reconstruct latent features, effectively lowering\ncomplexity. As a model-agnostic, task-agnostic regularizer, SIM can be\nseamlessly integrated as a plug-and-play module, making it applicable to\ndifferent network architectures and tasks.\n  We extensively evaluate $\\rho\\text{SIM}$ across three tasks: image\nclassification, few-shot prompt learning, and domain generalization.\nExperimental results show consistent improvements over baseline methods,\nhighlighting $\\rho\\text{SIM}$'s ability to enhance representation learning\nacross various tasks. We also demonstrate that $\\rho\\text{SIM}$ is orthogonal\nto existing regularization methods, boosting their effectiveness. Moreover, our\nresults confirm that $\\rho\\text{SIM}$ effectively preserves semantic\ninformation and enhances performance in dense-to-dense tasks, such as semantic\nsegmentation and image translation, as well as in non-visual domains including\naudio classification and time series anomaly detection. The code is publicly\navailable at https://github.com/XiudingCai/SIM-pytorch.", "AI": {"tldr": "本文提出了一个新的正则化框架，称为自我身份映射（SIM），尤其是它的实例ρSIM，通过逆映射机制从变换后的输出中重构输入来提高表示学习和泛化能力。SIM方法作为一种即插即用模块，有助于提升性能。", "motivation": "为了解决传统正则化方法在不同设置下可能不够有效或可靠的问题，本文提出了一个新的正则化框架SIM，尤其是其变体ρSIM，以改善表示学习并有效缓解过拟合问题。", "method": "Structure", "result": "<tool_call>\n{{\"name\": \"Structure\", \"arguments\": {\"tldr\": \"The paper proposes a new regularization framework called Self Identity Mapping (SIM), especially its instantiation as \\u03c1SIM, which helps enhance representation learning and generalization across diverse deep learning tasks by reconstructing the input from its transformed output using a plug-and-play approach. SIM methods improve performance and are complementary to existing regularization techniques.\", \"motivation\": \"The conventional regularization techniques used in deep learning are often based on heuristics and they may not be effective or reliable across different settings. The goal is to propose a novel regularization method, SIM, to improve representation learning and mitigate overfitting issues effectively.\", \"method\": \"The SIM framework uses an inverse mapping mechanism to reconstruct the input, reducing information loss during forward propagation and enabling smoother gradient flow. To improve computational efficiency, the authors introduced \\u03c1SIM, where features are sampled at the patch-level and reconstructed using a projection-based method.\", \"result\": \"Experiments across image classification, few-shot prompt learning, domain generalization, dense-to-dense tasks, and non-visual tasks (audio classification, time series anomaly detection) show that \\u03c1SIM effectively enhances representation learning and model performances consistently. \\u03c1SIM is also orthogonal to existing regularization techniques, which can further improve overall model performance.\", \"conclusion\": \"\\u03c1SIM is a versatile, plug-and-play regularization technique that can be integrated into various network architectures and tasks, demonstrating consistent improvements over baseline methods and the ability to enhance performance without increasing complexity significantly.\"}}}\n</tool_call>", "conclusion": "ρSIM是一种多功能的即插即用正则化技术，可以集成到各种网络架构和任务中，无论基准方法如何，都显示出性能提升且复杂性增加不大。"}}
{"id": "2509.18175", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18175", "abs": "https://arxiv.org/abs/2509.18175", "authors": ["Aditi Debsharma", "Bhushan Jagyasi", "Surajit Sen", "Priyanka Pandey", "Devicharith Dovari", "Yuvaraj V. C", "Rosalin Parida", "Gopali Contractor"], "title": "ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers", "comment": "7 pages, 6 Figures, 4 Tables, 18 References", "summary": "Emotion Recognition in Conversation has been seen to be widely applicable in\ncall center analytics, opinion mining, finance, retail, healthcare, and other\nindustries. In a call center scenario, the role of the call center agent is not\njust confined to receiving calls but to also provide good customer experience\nby pacifying the frustration or anger of the customers. This can be achieved by\nmaintaining neutral and positive emotion from the agent. As in any\nconversation, the emotion of one speaker is usually dependent on the emotion of\nother speaker. Hence the positive emotion of an agent, accompanied with the\nright resolution will help in enhancing customer experience. This can change an\nunhappy customer to a happy one. Imparting the right resolution at right time\nbecomes easier if the agent has the insight of the emotion of future\nutterances. To predict the emotions of the future utterances we propose a novel\narchitecture, Emotion Recognition and Forecasting in Conversation. Our proposed\nERFC architecture considers multi modalities, different attributes of emotion,\ncontext and the interdependencies of the utterances of the speakers in the\nconversation. Our intensive experiments on the IEMOCAP dataset have shown the\nfeasibility of the proposed ERFC. This approach can provide a tremendous\nbusiness value for the applications like call center, where the happiness of\ncustomer is utmost important.", "AI": {"tldr": "本文提出了一种名为ERFC的架构，用于预测对话中的未来言语情绪。在客服中心等需要提升客户满意度的应用场景中，该方法显示出巨大的商业价值。", "motivation": "在客服中心场景下，客服不仅需要接听电话，还需要通过安抚客户的不满或愤怒，提供良好的客户体验。客户的情绪往往依赖于另一说话者的情绪，因此，客服保持中立和积极的情绪将有助于提升客户体验。预测未来言语情绪有助于在适当的时间提供正确的解决方案，增强客户满意度。", "method": "本文提出了一种名为ERFC的新型架构，用于预测对话中的未来言语情绪。ERFC架构考虑了多模态、情绪的不同属性、背景以及对话中说话者言语之间的相互依赖性。", "result": "在IEMOCAP数据集上的实验显示，所提出的ERFC架构具有可行性。", "conclusion": "这种方法可以为需要提高客户满意度的关键应用程序（如客服中心）提供巨大的商业价值。"}}
{"id": "2509.18170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18170", "abs": "https://arxiv.org/abs/2509.18170", "authors": ["Zhanting Zhou", "Jinbo Wang", "Zeqin Wu", "Fengli Zhang"], "title": "MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion", "comment": null, "summary": "We study gradient inversion in the challenging single round averaged gradient\nSAG regime where per sample cues are entangled within a single batch mean\ngradient. We introduce MAGIA a momentum based adaptive correction on gradient\ninversion attack a novel label inference free framework that senses latent per\nimage signals by probing random data subsets. MAGIA objective integrates two\ncore innovations 1 a closed form combinatorial rescaling that creates a\nprovably tighter optimization bound and 2 a momentum based mixing of whole\nbatch and subset losses to ensure reconstruction robustness. Extensive\nexperiments demonstrate that MAGIA significantly outperforms advanced methods\nachieving high fidelity multi image reconstruction in large batch scenarios\nwhere prior works fail. This is all accomplished with a computational footprint\ncomparable to standard solvers and without requiring any auxiliary information.", "AI": {"tldr": "本次研究提出了一种新颖的、不需要辅助信息的高保真梯度反转方法MAGIA，它能够在复杂背景中实现高质量的图像重建，显著优于现有先进方法。", "motivation": "我们旨在研究挑战性的单轮平均梯度SAG（Single Averaged Gradient）制度中的梯度反转问题，在这一制度中，每样本线索在单批平均梯度中变得纠缠交织。", "method": "我们提出了一种名为MAGIA的方法，它是一种基于动量的适应性校正梯度反转攻击框架。该方法能够在无需标签信息的情况下，通过探测随机数据子集来感知潜在的每张图像信号。MAGIA的目标是结合两个核心创新：1. 一个封闭形式的组合缩放，创建了一个可证明更紧密的优化界限；2. 通过动量基础混合整批量和子集损失确保重建鲁棒性。", "result": "广泛实验表明，MAGIA在大规模批次场景中显著优于先进的方法，在高保真多图像重建上取得了优异性能，而之前的工作在此类场景中失效。", "conclusion": "MAGIA方法在计算负担与标准求解器相当的情况下实现了高性能，且不需要任何辅助信息。"}}
{"id": "2509.18293", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18293", "abs": "https://arxiv.org/abs/2509.18293", "authors": ["Jay Patel", "Hrudayangam Mehta", "Jeremy Blackburn"], "title": "Evaluating Large Language Models for Detecting Antisemitism", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Detecting hateful content is a challenging and important problem. Automated\ntools, like machine-learning models, can help, but they require continuous\ntraining to adapt to the ever-changing landscape of social media. In this work,\nwe evaluate eight open-source LLMs' capability to detect antisemitic content,\nspecifically leveraging in-context definition as a policy guideline. We explore\nvarious prompting techniques and design a new CoT-like prompt, Guided-CoT.\nGuided-CoT handles the in-context policy well, increasing performance across\nall evaluated models, regardless of decoding configuration, model sizes, or\nreasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.\nAdditionally, we examine LLM errors and introduce metrics to quantify semantic\ndivergence in model-generated rationales, revealing notable differences and\nparadoxical behaviors among LLMs. Our experiments highlight the differences\nobserved across LLMs' utility, explainability, and reliability.", "AI": {"tldr": "本研究通过评估多种LLM检测反犹太主义内容的能力，特别是在利用上下文定义作为政策指南时的表现，发现了新的提示方法Guided-CoT提升了模型性能。Llama 3.1 70B表现优异。此外，提出了一些新的衡量模型行为一致性及可靠性的方法。", "motivation": "自动检测仇恨内容，尤其是反犹太主义内容，是一个具有挑战性和重要性的课题。通过利用机器学习模型等自动工具，可以辅助人们应对这个问题。考虑到社交媒体环境的不断变化，模型需要持续训练以保持准确性。", "method": "研究评估了八个开源大型语言模型在检测反犹太主义内容的能力，特别是在利用上下文定义作为政策指南的情况下。研究者们探索了各种提示策略，并设计了一种新的类似CoT的提示方法，称为Guided-CoT。", "result": "Guided-CoT能够很好地处理上下文中的政策，提高了所评估的所有模型的性能，无论解码配置、模型大小或推理能力如何。特别是，Llama 3.1 70B的表现优于微调后的GPT-3.5。此外，研究还考察了LLM的错误，并引入了量化模型生成理由的语义发散度的指标，揭示了LLM之间的显著差异和矛盾行为。", "conclusion": "实验结果表明，不同LLM在实用性、可解释性和可靠性方面存在显著差异。研究强调了在进行仇恨内容检测时需要综合考虑这些差异的重要性。"}}
{"id": "2509.18174", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18174", "abs": "https://arxiv.org/abs/2509.18174", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motasim Hamed", "Ahmad Bastati", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR", "comment": null, "summary": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine- tuned\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\ncombining synthetic and real-world documents, Baseer is trained using a\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\nsystems. Our experiments show that Baseer significantly outperforms existing\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\na new state-of-the-art in the domain of Arabic document OCR. Our results\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\nand establish a strong baseline for high-accuracy OCR on morphologically rich\nlanguages like Arabic.", "AI": {"tldr": "本文介绍了Baseer模型，这是一个专门为阿拉伯文书本OCR优化的视觉语言模型。通过使用合成与实际文件的大型数据集进行微调，Baseer优于现有的开源和商业解决方案，为阿拉伯文书本OCR建立了新的标准。", "motivation": "阿拉伯文书本OCR因语言的连笔脚本、多样字体、变音符号以及从右向左的书写方式而仍具有挑战性。尽管现代多模态大型语言模型（MLLMs）已经提升了高资源语言的文档理解能力，但它们在阿拉伯语上的性能依然有限。因此，本文通过引入Baseer模型旨在改善阿拉伯文书本OCR的技术。", "method": "本文介绍了Baseer，这是一种专门为阿拉伯文书本OCR任务调整的视觉语言模型。通过结合合成和现实世界的大型数据集，Baseer采用了解码器的微调策略，以适应预训练的MLLM同时保持通用的视觉特征。", "result": "实验结果表明，Baseer显著优于现有的开源解决方案和商业解决方案，实现了0.25的WER（词差误率）并为阿拉伯文书本OCR领域建立了新的最佳实践。", "conclusion": "本文的研究结果强调了将通用的MLLM应用于特定领域的调整所带来的优势，并为诸如阿拉伯语这类语素丰富的语言的高精度OCR奠定了一个强大的基线。"}}
{"id": "2509.18314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18314", "abs": "https://arxiv.org/abs/2509.18314", "authors": ["Hieu Tran", "Zonghai Yao", "Hong Yu"], "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs", "comment": "15 pages", "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over\nlong sequences makes token-level credit assignment the key bottleneck. We study\nthe verifiable-reward setting, where the final answer is checkable and multiple\nresponses can be drawn per prompt. Reasoning tasks in math and medical QA align\nwith this setup, where only a few decision tokens significantly impact the\noutcome. PPO offers token-level advantages with a learned value model, but it\nis complex to train both the actor and critic models simultaneously, and it is\nnot easily generalizable, as the token-level values from the critic model can\nmake training prone to overfitting. GRPO is critic-free and supports verifiable\nrewards, but spreads a single sequence-level return across tokens and ignores\nbranching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that\nconverts a group of responses into a prefix tree and computes\n\\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes.\nBuilt on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated\n\\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a\ncritic-free algorithm that augments the group-relative outcome signal of GRPO\nwith \\emph{branch-gated} temporal-difference corrections derived from the tree.\nAt non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO\nreduces to GRPO; at branching tokens, it supplies precise token-level credit\nwithout a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,\nTEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and\nout-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and\nreaches higher validation accuracy with roughly the same wall-clock time.", "AI": {"tldr": "提出了一种名为TEMPO的新算法，基于Prefix-to-Tree方法，它改进了过去的强化学习技术，特别是提高了在数学和医疗问答等场景下的性能。", "motivation": "解决强化学习在语言模型（LLM）推理中的瓶颈问题，特别是稀疏延迟奖励下的令牌级信用分配问题，提高这些模型在数学和医疗问答等任务上的性能。", "method": "引入了Prefix-to-Tree (P2T) 和 TEMPO 方法，前者将响应组转换成前缀树并计算非参数前缀值，后者则利用树估计均值前缀值优化策略，逐步提供精确的令牌级奖励而无需学习价值网络或额外的判断/教师模型。", "result": "实验结果表明，TEMPO在数学问题解决（如MATH、GSM-HARD、AMC23）、医疗问答（MedQA, MedMCQA, MMLU-Medical）等基准测试中实现了优于PPO和GRPO的性能。", "conclusion": "TEMPO算法在一分布和出分布的基准测试中，相较于PPO和GRPO算法，具有更高的验证准确性，并且在几乎相同的时间内实现了改进。"}}
{"id": "2509.18176", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18176", "abs": "https://arxiv.org/abs/2509.18176", "authors": ["Wendong Yao", "Saeed Azadnejad", "Binhua Huang", "Shane Donohue", "Soumyabrata Dev"], "title": "A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland", "comment": "This paper is submitted to IEEE Transactions on Geoscience and Remote\n  Sensing", "summary": "Monitoring ground displacement is crucial for urban infrastructure stability\nand mitigating geological hazards. However, forecasting future deformation from\nsparse Interferometric Synthetic Aperture Radar (InSAR) time-series data\nremains a significant challenge. This paper introduces a novel deep learning\nframework that transforms these sparse point measurements into a dense\nspatio-temporal tensor. This methodological shift allows, for the first time,\nthe direct application of advanced computer vision architectures to this\nforecasting problem. We design and implement a hybrid Convolutional Neural\nNetwork and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to\nsimultaneously learn spatial patterns and temporal dependencies from the\ngenerated data tensor. The model's performance is benchmarked against powerful\nmachine learning baselines, Light Gradient Boosting Machine and LASSO\nregression, using Sentinel-1 data from eastern Ireland. Results demonstrate\nthat the proposed architecture provides significantly more accurate and\nspatially coherent forecasts, establishing a new performance benchmark for this\ntask. Furthermore, an interpretability analysis reveals that baseline models\noften default to simplistic persistence patterns, highlighting the necessity of\nour integrated spatio-temporal approach to capture the complex dynamics of\nground deformation. Our findings confirm the efficacy and potential of\nspatio-temporal deep learning for high-resolution deformation forecasting.", "AI": {"tldr": "研究提出了一种转换稀疏InSAR时序数据的新方法，并使用CNN-LSTM模型提供了更精确和空间一致的地面变形预测。", "motivation": "监测地面位移对于城市基础设施稳定性和减轻地质灾害至关重要，但从稀疏的InSAR时序数据中预报未来的变形仍然是一项重大挑战。", "method": "本研究提出了一种新的深度学习框架，能够将稀疏的InSAR时序数据转换成密集的时空张量。文中设计并实现了一个专门用于从生成的数据张量中学习空间模式和时间依赖性的混合卷积神经网络和长短时记忆模型（CNN-LSTM模型）。", "result": "实验结果表明，所提架构在预测精度和空间一致性方面显著优于基线方法——LightGBM和LASSO回归，确立了新的性能基准。解释性分析发现，基线模型通常倾向于简单的持续模式，强调了我们的时空一体化方法识别地面变形复杂动态的重要性。", "conclusion": "研究表明，时空深度学习对于高分辨率变形预报的效能和潜力得到了验证。"}}
{"id": "2509.18316", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18316", "abs": "https://arxiv.org/abs/2509.18316", "authors": ["Saksham Khatwani", "He Cheng", "Majid Afshar", "Dmitriy Dligach", "Yanjun Gao"], "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning", "comment": null, "summary": "Large language models (LLMs) show promise for diagnostic reasoning but often\nlack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as\nthe Unified Medical Language System (UMLS), offer structured biomedical\nknowledge that can support trustworthy reasoning. Prior approaches typically\nintegrate KGs via retrieval augmented generation or fine tuning, inserting KG\ncontent into prompts rather than enabling structured reasoning. We explore an\nalternative paradigm: treating the LLM as a reward model of KG reasoning paths,\nwhere the model learns to judge whether a candidate path leads to correct\ndiagnosis for a given patient input. This approach is inspired by recent work\nthat leverages reward training to enhance model reasoning abilities, and\ngrounded in computational theory, which suggests that verifying a solution is\noften easier than generating one from scratch. It also parallels physicians'\ndiagnostic assessment, where they judge which sequences of findings and\nintermediate conditions most plausibly support a diagnosis. We first\nsystematically evaluate five task formulation for knowledge path judging and\neight training paradigm. Second, we test whether the path judging abilities\ngeneralize to downstream diagnostic tasks, including diagnosis summarization\nand medical question answering. Experiments with three open source\ninstruct-tuned LLMs reveal both promise and brittleness: while specific reward\noptimization and distillation lead to strong path-judging performance, the\ntransferability to downstream tasks remain weak. Our finding provides the first\nsystematic assessment of \"reward model style\" reasoning over clinical KGs,\noffering insights into how structured, reward-based supervision influences\ndiagnostic reasoning in GenAI systems for healthcare.", "AI": {"tldr": "本文探讨了将大型语言模型（LLMs）作为知识图谱（KG）推理路径的奖励模型的方法，尽管证明了此方法在路径判断上的潜力，但其在下游任务中的应用表现仍不稳定。", "motivation": "现有的方法通常通过检索增强生成式方法或微调集成KG，将KG内容插入提示中，而不是启用结构化推理。本文旨在探索替代方案，以提高诊断推理的可靠性和基于知识的推理。", "method": "本研究提出了一种新的方法，将LLM作为KG推理路径的奖励模型，训练模型评估给定患者的候选路径是否能引向正确的诊断。这一方法借鉴了先前通过奖励训练增强模型推理能力的工作，并与医生使用发现序列和中间状态来评估诊断的可能性相类似。", "result": "实验结果表明，具体的奖励优化和知识蒸馏可以显著提高路径评估性能，但将其转移到下游任务上的效果仍较弱。", "conclusion": "这项研究提供了一个对“奖励模型式”推理在临床KG中应用的首次系统性评估，为AI在医疗诊断推理中的应用提供了见解。"}}
{"id": "2509.18177", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18177", "abs": "https://arxiv.org/abs/2509.18177", "authors": ["George Corrêa de Araújo", "Helena de Almeida Maia", "Helio Pedrini"], "title": "A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts", "comment": "WIP", "summary": "In this paper, we present the Scrapbook framework, a novel methodology\ndesigned to generate extensive datasets for probing the learned concepts of\nartificial intelligence (AI) models. The framework focuses on fundamental\nconcepts such as object recognition, absolute and relative positions, and\nattribute identification. By generating datasets with a large number of\nquestions about individual concepts and a wide linguistic variation, the\nScrapbook framework aims to validate the model's understanding of these basic\nelements before tackling more complex tasks. Our experimental findings reveal\nthat, while contemporary models demonstrate proficiency in recognizing and\nenumerating objects, they encounter challenges in comprehending positional\ninformation and addressing inquiries with additional constraints. Specifically,\nthe MobileVLM-V2 model showed significant answer disagreements and plausible\nwrong answers, while other models exhibited a bias toward affirmative answers\nand struggled with questions involving geometric shapes and positional\ninformation, indicating areas for improvement in understanding and consistency.\nThe proposed framework offers a valuable instrument for generating diverse and\ncomprehensive datasets, which can be utilized to systematically assess and\nenhance the performance of AI models.", "AI": {"tldr": "论文提出了一种新的数据集生成框架Scrapbook，用于探查AI模型对基本概念的理解。实验表明现有模型存在理解位置信息的挑战，Scrapbook可帮助系统性地评估和提升模型性能。", "motivation": "动机是通过数据集生成来评估AI模型在学习和理解基本元素（如对象识别，位置关系，属性识别等）上的表现，并据此提升它们处理复杂任务的能力。", "method": "提出了一种名为Scrapbook的框架，旨在生成用于探查AI模型学习概念的大量数据集。此框架重点关注对象识别、绝对和相对位置以及属性识别等基本概念。通过生成包含大量关于个别概念和广泛语言变化的问题的数据集，Scrapbook框架旨在验证模型对这些基本元素的理解从而更好地处理更复杂的任务。", "result": "实验结果表明，尽管现有模型在识别和枚举对象方面表现出色，但它们在理解和回答涉及位置信息和附加约束的问题时遇到困难。特别是MobileVLM-V2模型展示了显著的答案不一致和看似合理的错误答案，其他模型偏向于肯定答案，并在涉及几何形状和位置信息的问题上遇到挑战。", "conclusion": "Scrapbook框架为生成多样且全面的数据集提供了有价值的工具，可用于系统地评估和改进AI模型的表现。"}}
{"id": "2509.18344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18344", "abs": "https://arxiv.org/abs/2509.18344", "authors": ["Pei-Shuo Wang", "Jian-Jia Chen", "Chun-Che Yang", "Chi-Chih Chang", "Ning-Chi Huang", "Mohamed S. Abdelfattah", "Kai-Chiang Wu"], "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding", "comment": "Accepted by NeurIPS 2025", "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18179", "abs": "https://arxiv.org/abs/2509.18179", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes", "comment": "13 pages, 7 Figures", "summary": "With the increasing integration of multimodal AI systems in creative\nworkflows, understanding information loss in vision-language-vision pipelines\nhas become important for evaluating system limitations. However, the\ndegradation that occurs when visual content passes through textual\nintermediation remains poorly quantified. In this work, we provide empirical\nanalysis of the describe-then-generate bottleneck, where natural language\nserves as an intermediate representation for visual information. We generated\n150 image pairs through the describe-then-generate pipeline and applied\nexisting metrics (LPIPS, SSIM, and color distance) to measure information\npreservation across perceptual, structural, and chromatic dimensions. Our\nevaluation reveals that 99.3% of samples exhibit substantial perceptual\ndegradation and 91.5% demonstrate significant structural information loss,\nproviding empirical evidence that the describe-then-generate bottleneck\nrepresents a measurable and consistent limitation in contemporary multimodal\nsystems.", "AI": {"tldr": "The study empirically analyzes the information loss in a vision-language-vision pipeline and demonstrates significant perceptual and structural degradation via various quantifiable metrics.", "motivation": "To understand the information loss in vision-language-vision pipelines, especially when visual content is converted to text and back to visuals.", "method": "We generated 150 image pairs using a describe-then-generate pipeline and evaluated the perceptual, structural, and chromatic preservation using LPIPS, SSIM, and color distance metrics.", "result": "99.3% of samples showed perceptual degradation, and 91.5% exhibited significant structural information loss.", "conclusion": "The empirical analysis provides measurable evidence of significant limitations in the describe-then-generate process within multimodal systems."}}
{"id": "2509.18360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18360", "abs": "https://arxiv.org/abs/2509.18360", "authors": ["Chutong Meng", "Philipp Koehn"], "title": "Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents", "comment": "Accepted by EMNLP 2025 (main)", "summary": "We present Speech Vecalign, a parallel speech document alignment method that\nmonotonically aligns speech segment embeddings and does not depend on text\ntranscriptions. Compared to the baseline method Global Mining, a variant of\nspeech mining, Speech Vecalign produces longer speech-to-speech alignments. It\nalso demonstrates greater robustness than Local Mining, another speech mining\nvariant, as it produces less noise. We applied Speech Vecalign to 3,000 hours\nof unlabeled parallel English-German (En-De) speech documents from VoxPopuli,\nyielding about 1,000 hours of high-quality alignments. We then trained En-De\nspeech-to-speech translation models on the aligned data. Speech Vecalign\nimproves the En-to-De and De-to-En performance over Global Mining by 0.37 and\n0.18 ASR-BLEU, respectively. Moreover, our models match or outperform\nSpeechMatrix model performance, despite using 8 times fewer raw speech\ndocuments.", "AI": {"tldr": "The paper introduces Speech Vecalign, a method for aligning speech segments without relying on text transcriptions, which outperforms other methods like Global and Local Mining in terms of alignment length and noise reduction, respectively.", "motivation": "To develop a novel speech alignment method that does not depend on text transcriptions, improving upon the noise and alignment length issues of previous methods.", "method": "Structure", "result": "Speech Vecalign was applied to 3,000 hours of English-German speech documents, yielding 1,000 hours of high-quality alignments, and improved speech-to-speech translation performance over Global Mining by 0.37 and 0.18 ASR-BLEU for En-to-De and De-to-En, respectively.", "conclusion": "Speech Vecalign is a robust method for aligning speech documents without text transcriptions, producing better quality alignments with less raw data compared to existing methods like SpeechMatrix."}}
{"id": "2509.18182", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18182", "abs": "https://arxiv.org/abs/2509.18182", "authors": ["Isabelle Tingzon", "Yoji Toriumi", "Caroline Gevaert"], "title": "AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines", "comment": "Accepted at the 2nd Workshop on Computer Vision for Developing\n  Countries (CV4DC) at ICCV 2025", "summary": "Detailed structural building information is used to estimate potential damage\nfrom hazard events like cyclones, floods, and landslides, making them critical\nfor urban resilience planning and disaster risk reduction. However, such\ninformation is often unavailable in many small island developing states (SIDS)\nin climate-vulnerable regions like the Caribbean. To address this data gap, we\npresent an AI-driven workflow to automatically infer rooftop attributes from\nhigh-resolution satellite imagery, with Saint Vincent and the Grenadines as our\ncase study. Here, we compare the utility of geospatial foundation models\ncombined with shallow classifiers against fine-tuned deep learning models for\nrooftop classification. Furthermore, we assess the impact of incorporating\nadditional training data from neighboring SIDS to improve model performance.\nOur best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof\nmaterial classification, respectively. Combined with local capacity building,\nour work aims to provide SIDS with novel capabilities to harness AI and Earth\nObservation (EO) data to enable more efficient, evidence-based urban\ngovernance.", "AI": {"tldr": "提出了一种AI驱动的工作流，从高分辨率卫星图像中自动推断屋顶属性，提高小岛屿发展中国家在灾害风险管理中的城市韧性规划能力。", "motivation": "解决小岛屿发展中国家在城市韧性规划和灾害风险管理中的建筑结构信息不足问题。", "method": "通过高分辨率卫星图像自动推断屋顶属性的AI驱动工作流，比较了地理空间基础模型与浅层分类器结合的方法以及微调的深度学习模型在屋顶分类中的实用性。此外，还评估了纳入来自邻近小岛屿发展中国家的额外训练数据以提高模型性能的影响。", "result": "在屋顶坡度和屋顶材料分类中，最佳模型分别达到了0.88和0.83的F1评分，结合当地能力构建，为小岛屿发展中国家提供了利用AI和地球观测数据进行更有效、基于证据的城市治理的新能力。", "conclusion": "提出的方法为小岛屿发展中国家提供了利用AI和地球观测数据进行城市规划和灾害风险管理的新途径。"}}
{"id": "2509.18377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18377", "abs": "https://arxiv.org/abs/2509.18377", "authors": ["Xinlu He", "Yiwen Guan", "Badrivishal Paurana", "Zilin Dai", "Jacob Whitehill"], "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback", "comment": null, "summary": "Most automatic speech processing systems operate in \"open loop\" mode without\nuser feedback about who said what; yet, human-in-the-loop workflows can\npotentially enable higher accuracy. We propose an LLM-assisted speaker\ndiarization correction system that lets users fix speaker attribution errors in\nreal time. The pipeline performs streaming ASR and diarization, uses an LLM to\ndeliver concise summaries to the users, and accepts brief verbal feedback that\nis immediately incorporated without disrupting interactions. Moreover, we\ndevelop techniques to make the workflow more effective: First, a\nsplit-when-merged (SWM) technique detects and splits multi-speaker segments\nthat the ASR erroneously attributes to just a single speaker. Second, online\nspeaker enrollments are collected based on users' diarization corrections, thus\nhelping to prevent speaker diarization errors from occurring in the future.\nLLM-driven simulations on the AMI test set indicate that our system\nsubstantially reduces DER by 9.92% and speaker confusion error by 44.23%. We\nfurther analyze correction efficacy under different settings, including summary\nvs full transcript display, the number of online enrollments limitation, and\ncorrection frequency.", "AI": {"tldr": "本文提出了一种可让用户实时修正演讲者划分错误的系统，利用LLM技术提供简明摘要与即时反馈，并开发了提高系统准确性的两种技术，显著减少了误差率。", "motivation": "大多数自动语音处理系统在“开环”模式下运行，没有用户的反馈；但用户体验可以潜在地提高准确性。", "method": "本文提出了一种基于LLM的演讲者划分纠正系统，该系统允许用户实时更正演讲者归属错误。系统流程包括实时语音识别和演讲者划分，利用LLM向用户提供简明摘要，并接受简短口头反馈以即时修正错误。\n此外，本文还开发了两种技术来提高工作流程的有效性：第一，'拆分时合并'（SWM）技术检测并拆分语音识别错误归属到单一演讲者的多演讲者段落；第二，在线收集演讲者样本，基于用户的反馈防止未来的演讲者划分错误。\n", "result": "LLM驱动的模拟测试显示，该系统大幅减少了AM测试集中的DER（9.92%）和演讲者混淆错误（44.23%）。", "conclusion": "该系统有效地提高了实时语音识别及演讲者划分的准确性，尤其是在有用户反馈的情况下。进一步分析了不同设定下的校正效果，包括摘要与全文本显示、在线登记限制和更正频率。"}}
{"id": "2509.18183", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18183", "abs": "https://arxiv.org/abs/2509.18183", "authors": ["Jinyue Bian", "Zhaoxing Zhang", "Zhengyu Liang", "Shiwei Zheng", "Shengtao Zhang", "Rong Shen", "Chen Yang", "Anzhou Hou"], "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation", "comment": null, "summary": "The Visual-Language-Action (VLA) models can follow text instructions\naccording to visual observations of the surrounding environment. This ability\nto map multimodal inputs to actions is derived from the training of the VLA\nmodel on extensive standard demonstrations. These visual observations captured\nby third-personal global and in-wrist local cameras are inevitably varied in\nnumber and perspective across different environments, resulting in significant\ndifferences in the visual features. This perspective heterogeneity constrains\nthe generality of VLA models. In light of this, we first propose the\nlightweight module VLA-LPAF to foster the perspective adaptivity of VLA models\nusing only 2D data. VLA-LPAF is finetuned using images from a single view and\nfuses other multiview observations in the latent space, which effectively and\nefficiently bridge the gap caused by perspective inconsistency. We instantiate\nour VLA-LPAF framework with the VLA model RoboFlamingo to construct\nRoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves\naround 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a\ncustomized simulation benchmark. We also demonstrate the developed viewadaptive\ncharacteristics of the proposed RoboFlamingo-LPAF through real-world tasks.", "AI": {"tldr": "A lightweight module (VLA-LPAF) is introduced to enhance the perspective adaptability of Visual-Language-Action (VLA) models using 2D data, notably improving the VLA model RoboFlamingo's performance across different benchmarks, showcasing improved adaptiveness in diverse visual environments.", "motivation": "The VLA models, which map multimodal inputs to actions, are constrained by the perspective heterogeneity across different visual observations. The authors aim to improve the generalization of these models.", "method": "The paper proposes a lightweight module, VLA-LPAF, to enhance the perspective adaptability of Visual-Language-Action (VLA) models using 2D data only. VLA-LPAF is fine-tuned using images from a single viewpoint and incorporates other multiple-view observations in the latent space to effectively bridge the perspective inconsistency gap.", "result": "Experimental results indicate that with the proposed VLA-LPAF module, particularly applied to the RoboFlamingo model (RoboFlamingo-LPAF), task success rates increased significantly across various benchmarks: an average 8% on CALVIN, 15% on LIBERO, and 30% on a customized simulation benchmark. The improved perspective adaptiveness of the model was also demonstrated in real-world tasks.", "conclusion": "The introduced VLA-LPAF framework successfully enhances the perspective adaptability of VLA models, leading to substantial performance improvements. This has significant implications for improving the generalization and robustness of VLA models across varied environments."}}
{"id": "2509.18395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18395", "abs": "https://arxiv.org/abs/2509.18395", "authors": ["Minki Hong", "Jangho Choi", "Jihie Kim"], "title": "NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery", "comment": "39 pages, 17 figures, EMNLP 2025 Main Conference", "summary": "Social norms govern culturally appropriate behavior in communication,\nenabling dialogue systems to produce responses that are not only coherent but\nalso socially acceptable. We present NormGenesis, a multicultural framework for\ngenerating and annotating socially grounded dialogues across English, Chinese,\nand Korean. To model the dynamics of social interaction beyond static norm\nclassification, we propose a novel dialogue type, Violation-to-Resolution\n(V2R), which models the progression of conversations following norm violations\nthrough recognition and socially appropriate repair. To improve pragmatic\nconsistency in underrepresented languages, we implement an exemplar-based\niterative refinement early in the dialogue synthesis process. This design\nintroduces alignment with linguistic, emotional, and sociocultural expectations\nbefore full dialogue generation begins. Using this framework, we construct a\ndataset of 10,800 multi-turn dialogues annotated at the turn level for norm\nadherence, speaker intent, and emotional response. Human and LLM-based\nevaluations demonstrate that NormGenesis significantly outperforms existing\ndatasets in refinement quality, dialogue naturalness, and generalization\nperformance. We show that models trained on our V2R-augmented data exhibit\nimproved pragmatic competence in ethically sensitive contexts. Our work\nestablishes a new benchmark for culturally adaptive dialogue modeling and\nprovides a scalable methodology for norm-aware generation across linguistically\nand culturally diverse languages.", "AI": {"tldr": "本文介绍了一个多文化框架NormGenesis，用于跨语言生成和标注社会相关的对话，并提出一种新的对话类型Violation-to-Resolution（V2R），以模拟违反规范后的对话进展。基于这一框架构建了一个大规模对话数据集并展示了其在精炼质量和对话自然度等方面的优越效果。", "motivation": "动机在于填补未被充分表示的语言中的语用一致性不足，并且扩大现有的对话系统所涉及的社会规范范围，使对话不仅在内容上连贯，也在社交层面上适当。", "method": "我们介绍了一个名为NormGenesis的多文化框架，用于跨英语、中文和韩语生成和标注社会相关的对话。为了超越静态规范分类来建模社交互动的动态，我们提出了一种新的对话类型：违规到解决（Violation-to-Resolution，V2R），用以模拟违反规范后的交互进展，包括识别并作出社交上的适当补救。此外，为了在未充分代表的语言中改进语用一致性，我们在对话生成早期实施了基于样例的迭代精炼策略。这种方法在全面生成对话之前与语言学、情感和社会文化期望进行对齐。", "result": "使用这个框架，我们构建了一个包含10,800个多轮次对话的数据集，这些对话是在回合级别进行注释的，注释内容涉及规范遵守、说话者意图和情感反应。人类和LLM（可能是大语言模型）的评估表明，NormGenesis在精炼质量、对话自然度和泛化性能上显著优于现有数据集。在涉及伦理敏感性的情境中，使用我们V2R数据增强后的模型展现了改进的语用能力。", "conclusion": "这项研究确立了一个在文化适应性对话建模方面的新基准，并提供了一种跨多种语言及多样文化的规范意识生成方法。"}}
{"id": "2509.18184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18184", "abs": "https://arxiv.org/abs/2509.18184", "authors": ["Yifeng Cheng", "Alois Knoll", "Hu Cao"], "title": "URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation", "comment": "This work is accepted by Visual Intelligence Journal", "summary": "Event cameras provide high temporal resolution, high dynamic range, and low\nlatency, offering significant advantages over conventional frame-based cameras.\nIn this work, we introduce an uncertainty-aware refinement network called URNet\nfor event-based stereo depth estimation. Our approach features a local-global\nrefinement module that effectively captures fine-grained local details and\nlong-range global context. Additionally, we introduce a Kullback-Leibler (KL)\ndivergence-based uncertainty modeling method to enhance prediction reliability.\nExtensive experiments on the DSEC dataset demonstrate that URNet consistently\noutperforms state-of-the-art (SOTA) methods in both qualitative and\nquantitative evaluations.", "AI": {"tldr": "提出URNet网络，通过KL散度不确定性建模，实现基于事件相机的立体深度估计，在DSEC数据集上表现出色。", "motivation": "事件相机相较于传统的帧式相机具有高时间分辨率、高动态范围和低延迟等显著优势。本研究旨在利用这些优势提出一种新的立体深度估计方法。", "method": "本研究引入了一种称为URNet的不确定性感知优化网络，用于基于事件的立体深度估计。该方法具有局部-全局优化模块，有效捕捉精细的局部细节和长范围的整体上下文，并引入了一种基于Kullback-Leibler (KL)散度的不确定性建模方法，以增强预测的可靠性。", "result": "在DSEC数据集上的广泛实验表明，URNet在定性和定量评估中均优于现有最优方法。", "conclusion": "研究证明，URNet是一种有效的基于事件的立体深度估计方法，能够在各种条件下的性能表现超越现有最优方法。"}}
{"id": "2509.18401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18401", "abs": "https://arxiv.org/abs/2509.18401", "authors": ["Armin Tourajmehr", "Mohammad Reza Modarres", "Yadollah Yaghoobzadeh"], "title": "Evaluating the Creativity of LLMs in Persian Literary Text Generation", "comment": null, "summary": "Large language models (LLMs) have demonstrated notable creative abilities in\ngenerating literary texts, including poetry and short stories. However, prior\nresearch has primarily centered on English, with limited exploration of\nnon-English literary traditions and without standardized methods for assessing\ncreativity. In this paper, we evaluate the capacity of LLMs to generate Persian\nliterary text enriched with culturally relevant expressions. We build a dataset\nof user-generated Persian literary spanning 20 diverse topics and assess model\noutputs along four creativity dimensions-originality, fluency, flexibility, and\nelaboration-by adapting the Torrance Tests of Creative Thinking. To reduce\nevaluation costs, we adopt an LLM as a judge for automated scoring and validate\nits reliability against human judgments using intraclass correlation\ncoefficients, observing strong agreement. In addition, we analyze the models'\nability to understand and employ four core literary devices: simile, metaphor,\nhyperbole, and antithesis. Our results highlight both the strengths and\nlimitations of LLMs in Persian literary text generation, underscoring the need\nfor further refinement.", "AI": {"tldr": "研究评估了大规模语言模型生成波斯文学文本的能力，通过改进创造力评估方法，发现模型在特定文学手法的应用上有优势和局限。", "motivation": "尽管大规模语言模型在生成文学文本方面表现突出，但过去的大多数研究集中在英语上，对非英语文学传统探索较少，也没有标准化的评估创造力的方法。此项研究旨在填补这一空白。", "method": "通过构建涵盖20个不同主题的用户生成的波斯文学数据集来评估大规模语言模型（LLMs）生成富含文化相关表达的波斯文学文本的能力。使用改进后的Torrance创造力思考测试衡量原创性、流畅性、灵活性和详尽性的四个创造力维度。通过采用LLM作为裁判进行自动化评分并使用类内相关系数验证其相对于人类评判的可靠性，以降低评估成本。", "result": "建立了评估模型理解和运用四种核心文学手法（明喻、隐喻、夸张和对立）的能力。结果突显了LLMs在波斯文学文本生成中的优势和局限性。", "conclusion": "研究结果强调了进一步完善LLMs在波斯文学文本生成任务上的必要性。"}}
{"id": "2509.18185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18185", "abs": "https://arxiv.org/abs/2509.18185", "authors": ["Giammarco La Barbera", "Enzo Bonnot", "Thomas Isla", "Juan Pablo de la Plata", "Joy-Rose Dunoyer de Segonzac", "Jennifer Attali", "Cécile Lozach", "Alexandre Bellucci", "Louis Marcellin", "Laure Fournier", "Sabine Sarnacki", "Pietro Gori", "Isabelle Bloch"], "title": "Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases", "comment": "Computer-Aided Pelvic Imaging for Female Health (CAPI) - Workshop\n  MICCAI 2025", "summary": "Endometriosis often leads to chronic pelvic pain and possible nerve\ninvolvement, yet imaging the peripheral nerves remains a challenge. We\nintroduce Visionerves, a novel hybrid AI framework for peripheral nervous\nsystem recognition from multi-gradient DWI and morphological MRI data. Unlike\nconventional tractography, Visionerves encodes anatomical knowledge through\nfuzzy spatial relationships, removing the need for selection of manual ROIs.\nThe pipeline comprises two phases: (A) automatic segmentation of anatomical\nstructures using a deep learning model, and (B) tractography and nerve\nrecognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in\n10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated\nsubstantial improvements over standard tractography, with Dice score\nimprovements of up to 25% and spatial errors reduced to less than 5 mm. This\nautomatic and reproducible approach enables detailed nerve analysis and paves\nthe way for non-invasive diagnosis of endometriosis-related neuropathy, as well\nas other conditions with nerve involvement.", "AI": {"tldr": "研究开发了一种名为Visionerves的新型AI框架，用于识别周围神经，特别针对子宫内膜异位症的神经病变，显示出优于传统方法的准确性和空间定位能力。", "motivation": "周围神经系统的成像一直是一个挑战，尤其是对于子宫内膜异位症导致的神经受累情况。该研究的目的是开发一种新型AI框架，以改善周围神经系统的识别，从而促进子宫内膜异位症相关神经病变的非侵入性诊断及其他涉及神经的病症。", "method": "介绍了一种名为Visionerves的新型混合AI框架，用于从多梯度弥散加权成像(DWI)和形态学MRI数据中识别周围神经系统。此方法采用深度学习模型进行自动解剖结构分割，并通过符号空间推理来完成神经追踪和识别。", "result": "在10名子宫内膜异位症（确诊或疑似）患者中应用Visionerves，相对于标准追踪方法，其Dice得分提高了高达25%，空间误差降低至5毫米以内。", "conclusion": "Visionerves提供了一种自动且可重复的方法，实现了详细的神经分析，并为子宫内膜异位症相关神经病变的非侵入性诊断奠定了基础。"}}
{"id": "2509.18439", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18439", "abs": "https://arxiv.org/abs/2509.18439", "authors": ["Oscar J. Ponce-Ponte", "David Toro-Tobon", "Luis F. Figueroa", "Michael Gionfriddo", "Megan Branda", "Victor M. Montori", "Saturnino Luz", "Juan P. Brito"], "title": "Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations", "comment": "53 pages, 1 figure, 4 tables, 5 supplementary figures, 13\n  supplementary tables", "summary": "Shared decision-making (SDM) is necessary to achieve patient-centred care.\nCurrently no methodology exists to automatically measure SDM at scale. This\nstudy aimed to develop an automated approach to measure SDM by using language\nmodelling and the conversational alignment (CA) score. A total of 157\nvideo-recorded patient-doctor conversations from a randomized multi-centre\ntrial evaluating SDM decision aids for anticoagulation in atrial fibrillations\nwere transcribed and segmented into 42,559 sentences. Context-response pairs\nand negative sampling were employed to train deep learning (DL) models and\nfine-tuned BERT models via the next sentence prediction (NSP) task. Each\ntop-performing model was used to calculate four types of CA scores. A\nrandom-effects analysis by clinician, adjusting for age, sex, race, and trial\narm, assessed the association between CA scores and SDM outcomes: the\nDecisional Conflict Scale (DCS) and the Observing Patient Involvement in\nDecision-Making 12 (OPTION12) scores. p-values were corrected for multiple\ncomparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,\nmean age 70 SD 10.8), clinicians on average spoke more words than patients\n(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1\nof 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1\nwith 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)\nscores generated with the DL without stylebook were associated with OPTION12.\nThe Max CA score generated with the fine-tuned BERTbase (110M) was associated\nwith the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an\nimpact the association between CA scores and SDM. This study introduces an\nautomated, scalable methodology to measure SDM in patient-doctor conversations\nthrough explainable CA scores, with potential to evaluate SDM strategies at\nscale.", "AI": {"tldr": "通过对病人与医生的对话进行语言建模和微调BERT模型，研究开发了一种自动测量共同决策的方法，并通过交谈一致性评分关联决策冲突量表和患者参与决策评分。", "motivation": "目前尚无可以大规模自动测量SDM的方法。这项研究侧重于开发一种自动测量SDM的方法，并通过可解释的CA评分来评估SDM策略。", "method": "本研究旨在通过使用语言建模和交谈一致性（CA）评分来开发一种自动测量共同决策（SDM）的方法。研究收集了157个病人与医生对话的视频记录，并将其分词成42,559句话。通过使用上下文-响应对和负样本，研究训练了深度学习模型，并通过下一句预测任务对BERT模型进行了微调。每种表现最好的模型计算了四种类别的CA评分。", "result": "研究发现，没有使用风格手册策略的深度学习模型的查全率@1为0.227，而经过微调的BERTbase(110M)模型达到了最高的查全率@1，为0.640。使用没有风格手册的深度学习模型生成的AbsMax（18.36 SE7.74 p=0.025）和Max CA（21.02 SE7.63 p=0.012）评分与OPTION12评分有关。使用经过微调的BERTbase(110M)模型生成的Max CA评分与DCS评分有显著关联（-27.61 SE12.63 p=0.037）。BERT模型的大小对CA评分和SDM之间的关联没有影响。", "conclusion": "本研究提出了一种自动化的、可扩展的方法，用于通过可解释的CA评分来测量病人-医生对话中的共同决策（SDM）。这种方法有望对大规模的SDM策略进行评估。"}}
{"id": "2509.18187", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18187", "abs": "https://arxiv.org/abs/2509.18187", "authors": ["Muhammad Naveed", "Nazia Perwaiz", "Sidra Sultana", "Mohaira Ahmad", "Muhammad Moazam Fraz"], "title": "V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling", "comment": null, "summary": "Road traffic accidents remain a major public health challenge, particularly\nin countries with heterogeneous road conditions, mixed traffic flow, and\nvariable driving discipline, such as Pakistan. Reliable detection of unsafe\ndriving behaviours is a prerequisite for improving road safety, enabling\nadvanced driver assistance systems (ADAS), and supporting data driven decisions\nin insurance and fleet management. Most of existing datasets originate from the\ndeveloped countries with limited representation of the behavioural diversity\nobserved in emerging economies and the driver's face recording voilates the\nprivacy preservation. We present V-SenseDrive, the first privacy-preserving\nmultimodal driver behaviour dataset collected entirely within the Pakistani\ndriving environment. V-SenseDrive combines smartphone based inertial and GPS\nsensor data with synchronized road facing video to record three target driving\nbehaviours (normal, aggressive, and risky) on multiple types of roads,\nincluding urban arterials, secondary roads, and motorways. Data was gathered\nusing a custom Android application designed to capture high frequency\naccelerometer, gyroscope, and GPS streams alongside continuous video, with all\nsources precisely time aligned to enable multimodal analysis. The focus of this\nwork is on the data acquisition process, covering participant selection,\ndriving scenarios, environmental considerations, and sensor video\nsynchronization techniques. The dataset is structured into raw, processed, and\nsemantic layers, ensuring adaptability for future research in driver behaviour\nclassification, traffic safety analysis, and ADAS development. By representing\nreal world driving in Pakistan, V-SenseDrive fills a critical gap in the global\nlandscape of driver behaviour datasets and lays the groundwork for context\naware intelligent transportation solutions.", "AI": {"tldr": "研究开发了V-SenseDrive，这是一个隐私保护的多模态驾驶行为数据集，记录了巴基斯坦驾驶环境中三种驾驶行为，并通过自定义APP收集了手机惯性和GPS传感器数据与面向道路的视频。", "motivation": "现有的驾驶行为数据集大多来自发达国家，缺乏对新兴经济体中的驾驶行为多样性的代表性，且驾驶员面部记录侵犯隐私。本研究旨在填补这一空白。", "method": "本研究通过自定义的Android应用程序收集数据，该程序能够同步记录高频率的加速度计、陀螺仪和GPS数据以及连续的视频，并确保各种数据源的时间对齐，以支持多模态分析。数据采集涵盖了多种道路类型的正常驾驶、激进驾驶和危险驾驶行为。", "result": "本研究创建了V-SenseDrive数据集，这是首个隐私保护的多模态驾驶行为数据集，专门针对巴基斯坦的驾驶环境。数据集涵盖驾驶行为分类、交通安全分析和智能驾驶辅助系统（ADAS）开发的研究。", "conclusion": "V-SenseDrive数据集填补了全球驾驶行为数据集中的空白，为基于上下文的智能交通解决方案奠定了基础，推动了未来研究的发展。"}}
{"id": "2509.18458", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68T05, 68T20, 68T27 (Secondary)", "I.2.7; I.2.6; I.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.18458", "abs": "https://arxiv.org/abs/2509.18458", "authors": ["Daniel Kaiser", "Arnoldo Frigessi", "Ali Ramezani-Kebrya", "Benjamin Ricaud"], "title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density", "comment": "29 pages (main: 12 + supplemental material: 17), 6 figures, 4 tables,\n  Code: https://github.com/kaiserdan/cogniload, Data:\n  https://huggingface.co/datasets/cogniloadteam/cogniload", "summary": "Current benchmarks for long-context reasoning in Large Language Models (LLMs)\noften blur critical factors like intrinsic task complexity, distractor\ninterference, and task length. To enable more precise failure analysis, we\nintroduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load\nTheory (CLT). CogniLoad generates natural-language logic puzzles with\nindependently tunable parameters that reflect CLT's core dimensions: intrinsic\ndifficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$)\nregulates extraneous load; and task length ($N$) serves as an operational proxy\nfor conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,\nCogniLoad reveals distinct performance sensitivities, identifying task length\nas a dominant constraint and uncovering varied tolerances to intrinsic\ncomplexity and U-shaped responses to distractor ratios. By offering systematic,\nfactorial control over these cognitive load dimensions, CogniLoad provides a\nreproducible, scalable, and diagnostically rich tool for dissecting LLM\nreasoning limitations and guiding future model development.", "AI": {"tldr": "介绍了一种新基准CogniLoad，它基于认知负荷理论，生成可独立调节参数的自然语言逻辑谜题，用于评估和剖析大规模语言模型的推理限制。", "motivation": "分析大型语言模型（LLMs）在处理长上下文推理问题时的性能和失败原因，当前的基准通常混淆了诸如内在任务复杂性、干扰物干扰性和任务长度等因素。为此，提出了CogniLoad基准，以便更精确地进行失败分析。", "method": "介绍了一种名为CogniLoad的新合成基准，基于认知负荷理论（CLT）。CogniLoad生成自然语言逻辑谜题，具有可以独立调节的参数，以反映CLT的核心维度：内在难度（d）控制内在负荷；干扰物与信号比（ρ）调节外在负荷；任务长度（N）作为需要相关负荷的条件的操作性代理。", "result": "评估了22个最先进的推理LLMs，结果显示。任务长度是主要限制因素，容忍内在复杂性的能力各异，而且对于干扰物比例呈U型敏感性。", "conclusion": "通过系统、因子化的控制这些认知负荷维度，CogniLoad提供了一种重复性高、扩展性强且诊断性丰富的工具，用于剖析LLM推理限制，并指导未来模型开发。"}}
{"id": "2509.18189", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18189", "abs": "https://arxiv.org/abs/2509.18189", "authors": ["Daxiang Dong", "Mingming Zheng", "Dong Xu", "Bairong Zhuang", "Wenyu Zhang", "Chunhua Luo", "Haoran Wang", "Zijian Zhao", "Jie Li", "Yuxuan Li", "Hanjun Zhong", "Mengyue Liu", "Jieting Chen", "Shupeng Li", "Lun Tian", "Yaping Feng", "Xin Li", "Donggang Jiang", "Yong Chen", "Yehua Xu", "Duohao Qin", "Chen Feng", "Dan Wang", "Henghua Zhang", "Jingjing Ha", "Jinhui He", "Yanfeng Zhai", "Chengxin Zheng", "Jiayi Mao", "Jiacheng Chen", "Ruchang Yao", "Ziye Yuan", "Jianmin Wu", "Guangjun Xie", "Dou Shen"], "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models", "comment": "12 pages", "summary": "We present Qianfan-VL, a series of multimodal large language models ranging\nfrom 3B to 70B parameters, achieving state-of-the-art performance through\ninnovative domain enhancement techniques. Our approach employs multi-stage\nprogressive training and high-precision data synthesis pipelines, which prove\nto be critical technologies for enhancing domain-specific capabilities while\nmaintaining strong general performance. Qianfan-VL achieves comparable results\nto leading open-source models on general benchmarks, with state-of-the-art\nperformance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and\nMMStar. The domain enhancement strategy delivers significant advantages in OCR\nand document understanding, validated on both public benchmarks (OCRBench 873,\nDocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B\nvariants incorporate long chain-of-thought capabilities, demonstrating superior\nperformance on mathematical reasoning (MathVista 78.6%) and logical inference\ntasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating\nthe capability of large-scale AI infrastructure to train SOTA-level multimodal\nmodels with over 90% scaling efficiency on 5000 chips for a single task. This\nwork establishes an effective methodology for developing domain-enhanced\nmultimodal models suitable for diverse enterprise deployment scenarios.", "AI": {"tldr": "本研究介绍了Qianfan-VL系列多模态大语言模型，通过领域增强技术在多任务上达到先进性能，证明了大规模AI基础设施的有效性。", "motivation": "研究目的是创建一种有效的方法来开发具有领域增强功能的多模态模型，能够在保持强大的通用性能的同时，增强特定领域的功能。", "method": "本研究采用多阶段渐进式训练和高精度数据合成管道，通过创新的领域增强技术，开发了Qianfan-VL系列多模态大语言模型。这些模型参数范围从30亿到700亿，适用于多样化的企业部署场景。80亿和700亿参数的版本模型还集成了长链思维能力，展示了在数学推理和逻辑推理任务上的优越性能。", "result": "Qianfan-VL在通用基准测试中取得了与领先的开源模型可比的结果，并在CCBench、SEEDBench IMG、ScienceQA和MMStar等基准测试中达到先进水平。在OCR和文档理解方面，在公开基准测试（如OCRBench 873、DocVQA 94.75%）和内部评估中也显示出了显著的优势。", "conclusion": "本研究展示了通过Baidu的Kunlun P800芯片训练大规模多模态模型的能力，且在单一任务上使用5000个芯片时，具有超过90%的扩展效率。这确立了一种有效的方法来开发适合于多样化企业场景的领域增强型多模态模型。"}}
{"id": "2509.18467", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18467", "abs": "https://arxiv.org/abs/2509.18467", "authors": ["Zeyu Liu", "Souvik Kundu", "Lianghao Jiang", "Anni Li", "Srikanth Ronanki", "Sravan Bodapati", "Gourav Datta", "Peter A. Beerel"], "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling", "comment": "17 pages, 8 figures", "summary": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources.", "AI": {"tldr": "本文提出了 LAWCAT 方法，用于将预训练 transformer 的能力高效地转移到高性能的线性注意力模型中，显著提升了模型在长上下文中的表现，同时减少了训练数据和计算资源的需求。", "motivation": "尽管 transformer 架构在各个领域实现了最先进的性能，但它们对于序列长度的二次计算复杂性仍然是一个关键瓶颈，特别是在对延迟敏感的长上下文应用中。LAWCAT 的动机在于克服这些限制，提供一种更高效的方法来处理长上下文问题。", "method": "LAWCAT (Linear Attention with Convolution Across Time) 是一种新型的线性化框架，旨在高效地将预训练的 transformer 的能力转移到一个高性能的线性注意力架构中。该框架集成了因果 Conv1D 层以增强局部依赖性建模，并使用归一化门控线性注意力来改进在不同上下文长度下的泛化能力。", "result": "LAWCAT 在使用只有 1K 长度的序列对 Mistral-7B 进行蒸馏后，能够在超过 22K 个 token 的情况下达到超过 90% 的 passkey 检索准确率。此外，Llama3.2-1B 的 LAWCAT 变体在 S-NIAH 1&2&3 任务和 BABILong 基准（QA2&QA3，0K-16K 上下文长度）中取得了具有竞争力的表现，相比预先训练模型，需要的预训练 tokens 减少了不到 0.1%，并且对于超过 8K tokens 的序列，LAWCAT 的预填充速度比 FlashAttention-2 更快。", "conclusion": "LAWCAT 为长上下文线性模型提供了高效途径，具备高性能可部署到边缘端，减少了对大量长序列训练数据和计算资源的依赖。"}}
{"id": "2509.18190", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18190", "abs": "https://arxiv.org/abs/2509.18190", "authors": ["Junseong Shin", "Seungwoo Chung", "Yunjeong Yang", "Tae Hyun Kim"], "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing", "comment": null, "summary": "Dehazing involves removing haze or fog from images to restore clarity and\nimprove visibility by estimating atmospheric scattering effects. While deep\nlearning methods show promise, the lack of paired real-world training data and\nthe resulting domain gap hinder generalization to real-world scenarios. In this\ncontext, physics-grounded learning becomes crucial; however, traditional\nmethods based on the Atmospheric Scattering Model (ASM) often fall short in\nhandling real-world complexities and diverse haze patterns. To solve this\nproblem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM\nas an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),\nHazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,\nenhancing real-world dehazing performance with only a single inference step.\nAdditionally, we introduce a non-homogeneous haze generation method using\nMarkov Chain Brownian Motion (MCBM) to address the scarcity of paired\nreal-world data. By simulating realistic haze patterns through MCBM, we enhance\nthe adaptability of HazeFlow to diverse real-world scenarios. Through extensive\nexperiments, we demonstrate that HazeFlow achieves state-of-the-art performance\nacross various real-world dehazing benchmark datasets.", "AI": {"tldr": "The paper proposes HazeFlow, an ODE-based framework inspired by Rectified Flow that reformulates the Atmospheric Scattering Model to enhance dehazing performance in real-world scenarios. It uses a non-homogeneous haze generation method based on Markov Chain Brownian Motion to improve generalization.", "motivation": "The motivation is to address the challenges posed by the lack of paired real-world training data and improve the generalization of dehazing methods to real-world scenarios, especially in handling diverse haze patterns that traditional methods based on the Atmospheric Scattering Model cannot effectively manage.", "method": "The method involves reformulating the Atmospheric Scattering Model as an ordinary differential equation (ODE) and learning an optimal ODE trajectory to map hazy images to clean ones. Additionally, it uses Markov Chain Brownian Motion to simulate non-homogeneous haze patterns, addressing data scarcity issues.", "result": "Through extensive experiments, the paper demonstrates that HazeFlow achieves state-of-the-art performance on various real-world dehazing benchmark datasets, proving its effectiveness and adaptability in real-world scenarios.", "conclusion": "HazeFlow is concluded as an effective and efficient method for dehazing, providing better generalization to real-world scenarios compared to existing methods. Its unique approach to modeling the dehazing process as a trajectory of an ODE, combined with enhanced data simulation techniques, is deemed promising for further advancements in the field."}}
{"id": "2509.18487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18487", "abs": "https://arxiv.org/abs/2509.18487", "authors": ["Ben Finkelshtein", "Silviu Cucerzan", "Sujay Kumar Jauhar", "Ryen White"], "title": "Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference", "comment": null, "summary": "Large language models (LLMs) are increasingly used for text-rich graph\nmachine learning tasks such as node classification in high-impact domains like\nfraud detection and recommendation systems. Yet, despite a surge of interest,\nthe field lacks a principled understanding of the capabilities of LLMs in their\ninteraction with graph data. In this work, we conduct a large-scale, controlled\nevaluation across several key axes of variability to systematically assess the\nstrengths and weaknesses of LLM-based graph reasoning methods in text-based\napplications. The axes include the LLM-graph interaction mode, comparing\nprompting, tool-use, and code generation; dataset domains, spanning citation,\nweb-link, e-commerce, and social networks; structural regimes contrasting\nhomophilic and heterophilic graphs; feature characteristics involving both\nshort- and long-text node attributes; and model configurations with varying LLM\nsizes and reasoning capabilities. We further analyze dependencies by\nmethodically truncating features, deleting edges, and removing labels to\nquantify reliance on input types. Our findings provide practical and actionable\nguidance. (1) LLMs as code generators achieve the strongest overall performance\non graph data, with especially large gains on long-text or high-degree graphs\nwhere prompting quickly exceeds the token budget. (2) All interaction\nstrategies remain effective on heterophilic graphs, challenging the assumption\nthat LLM-based methods collapse under low homophily. (3) Code generation is\nable to flexibly adapt its reliance between structure, features, or labels to\nleverage the most informative input type. Together, these findings provide a\ncomprehensive view of the strengths and limitations of current LLM-graph\ninteraction modes and highlight key design principles for future approaches.", "AI": {"tldr": "本文通过系统性评估，揭示了LLMs在图数据分析中的应用效果，并指出代码生成在处理长文本或高节点度图上的优势，以及在异质性图中的有效性。", "motivation": "尽管对LLMs的兴趣激增，但在LLM与图数据交互领域的研究缺乏系统理解。此研究旨在填补这一空白，提供关于LLMs在图数据中的性能和应用的深入理解。", "method": "本文通过大规模、受控的评估，系统地评估了基于大规模语言模型（LLMs）的图推理方法在基于文本的应用中的优势和劣势。评估的维度包括LLM与图数据的交互模式（包括提示、工具使用和代码生成），数据集领域（涵盖引文、网络链接、电子商务和社会网络），结构类型（对比同质性和异质性图），特征特性（包括短文本和长文本节点属性）和模型配置（包括不同规模和推理能力的LLM）。通过系统地截断特征、删除边和移除标签，以量化对不同输入类型的依赖性。", "result": "研究发现，(1)作为代码生成器的LLMs在图数据上的整体性能最强，特别是在长文本或者高节点度的图上，提示方法很快超出token预算。(2)所有交互策略在异质性图上仍有效，挑战了LLM方法在低同质性图下失效的假设。(3)代码生成可以通过灵活调整对结构、特征或标签的依赖性来利用最有信息价值的输入类型。", "conclusion": "本研究提供了关于当前LLMs与图数据交互模式的全面视角和设计原则，强调了未来研究中需要考虑的关键因素。"}}
{"id": "2509.18193", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18193", "abs": "https://arxiv.org/abs/2509.18193", "authors": ["Omar H. Khater", "Abdul Jabbar Siddiqui", "Aiman El-Maleh", "M. Shamim Hossain"], "title": "TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection", "comment": null, "summary": "Deploying deep learning models in agriculture is difficult because edge\ndevices have limited resources, but this work presents a compressed version of\nEcoWeedNet using structured channel pruning, quantization-aware training (QAT),\nand acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the\nchallenges of pruning complex architectures with residual shortcuts, attention\nmechanisms, concatenations, and CSP blocks, the model size was reduced by up to\n68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at\nFP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the\npruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n\n(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%\nmAP50, proving it to be both efficient and effective for precision agriculture.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.18514", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18514", "abs": "https://arxiv.org/abs/2509.18514", "authors": ["Mohamad Elzohbi", "Richard Zhao"], "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition", "comment": "Accepted for the Third Arabic Natural Language Processing Conference\n  (ArabicNLP 2025)", "summary": "This paper presents a methodology for inserting phrases in Arabic poems to\nconform to a specific rhythm using ByT5, a byte-level multilingual\ntransformer-based model. Our work discusses a rule-based grapheme-to-beat\ntransformation tailored for extracting the rhythm from fully diacritized Arabic\nscript. Our approach employs a conditional denoising objective to fine-tune\nByT5, where the model reconstructs masked words to match a target rhythm. We\nadopt a curriculum learning strategy, pre-training on a general Arabic dataset\nbefore fine-tuning on poetic dataset, and explore cross-lingual transfer from\nEnglish to Arabic. Experimental results demonstrate that our models achieve\nhigh rhythmic alignment while maintaining semantic coherence. The proposed\nmodel has the potential to be used in co-creative applications in the process\nof composing classical Arabic poems.", "AI": {"tldr": "本论文提出了一种方法，利用基于字节级多语言变压器模型ByT5，在阿拉伯诗歌中插入符合特定节奏的短语。", "motivation": "研究目的是开发一种算法，能够自动在阿拉伯诗歌中插入符合特定节奏的短语，丰富诗歌创作过程。", "method": "使用基于规则的音节到节奏的转换方法来提取完全标音的阿拉伯文本中的节奏，并利用条件去噪目标来微调ByT5模型，使其能够重建被掩码的词语以符合目标节奏。同时采用课程学习策略，先使用通用阿拉伯数据集进行预训练，再使用诗歌数据集微调，并探索了从英语到阿拉伯语的跨语言迁移。", "result": "实验结果表明，所提出的模型能够在保持语义连贯性的同时，达成高节奏对齐度。", "conclusion": "研究提出的方法有潜力用于古典阿拉伯诗歌创作过程中的共创应用。"}}
{"id": "2509.18284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18284", "abs": "https://arxiv.org/abs/2509.18284", "authors": ["Yi Gu", "Kuniaki Saito", "Jiaxin Ma"], "title": "Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction", "comment": "MICCAI 2025", "summary": "As medical diagnoses increasingly leverage multimodal data, machine learning\nmodels are expected to effectively fuse heterogeneous information while\nremaining robust to missing modalities. In this work, we propose a novel\nmultimodal learning framework that integrates enhanced modalities dropout and\ncontrastive learning to address real-world limitations such as modality\nimbalance and missingness. Our approach introduces learnable modality tokens\nfor improving missingness-aware fusion of modalities and augments conventional\nunimodal contrastive objectives with fused multimodal representations. We\nvalidate our framework on large-scale clinical datasets for disease detection\nand prediction tasks, encompassing both visual and tabular modalities.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance, particularly in challenging and practical scenarios where only a\nsingle modality is available. Furthermore, we show its adaptability through\nsuccessful integration with a recent CT foundation model. Our findings\nhighlight the effectiveness, efficiency, and generalizability of our approach\nfor multimodal learning, offering a scalable, low-cost solution with\nsignificant potential for real-world clinical applications. The code is\navailable at https://github.com/omron-sinicx/medical-modality-dropout.", "AI": {"tldr": "本研究提出了一种新的多模态学习框架，该框架通过增强的模态dropout技术及对比学习来解决模态失衡和缺失问题，并在临床数据上展示了其卓越的性能和广泛的适用性。", "motivation": "随着医学诊断越来越多地利用多模态数据，机器学习模型被期望能够有效地融合异构信息并保持对缺失模态的鲁棒性。", "method": "本研究提出了一种新的多模态学习框架，它结合了增强的模态dropout技术和对比学习，以解决现实世界中的模态失衡和缺失问题。该方法引入了可学习的模态令牌来改进模态缺失感知融合，并使用融合的多模态表示增强传统的单模态对比目标函数。", "result": "实验结果显示，该方法在大规模临床数据集上的疾病检测和预测任务中达到了最先进的性能，尤其是在仅有单一模态可用的具有挑战性和实用性的场景中。此外，该方法还通过成功集成最近的CT基础模型展现了其适应性。", "conclusion": "研究结果强调了我们方法在多模态学习中的有效性、效率和通用性，为现实世界的临床应用提供了一种可扩展且低成本的解决方案。"}}
{"id": "2509.18535", "categories": ["cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18535", "abs": "https://arxiv.org/abs/2509.18535", "authors": ["Mo Mu", "Dianqiao Lei", "Chang Li"], "title": "Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector", "comment": null, "summary": "The widespread adoption of ChatGPT has raised concerns about its misuse,\nhighlighting the need for robust detection of AI-generated text. Current\nword-level detectors are vulnerable to paraphrasing or simple prompts (PSP),\nsuffer from biases induced by ChatGPT's word-level patterns (CWP) and training\ndata content, degrade on modified text, and often require large models or\nonline LLM interaction. To tackle these issues, we introduce a novel task to\ndetect both original and PSP-modified AI-generated texts, and propose a\nlightweight framework that classifies texts based on their internal structure,\nwhich remains invariant under word-level changes. Our approach encodes sentence\nembeddings from pre-trained language models and models their relationships via\nattention. We employ contrastive learning to mitigate embedding biases from\nautoregressive generation and incorporate a causal graph with counterfactual\nmethods to isolate structural features from topic-related biases. Experiments\non two curated datasets, including abstract comparisons and revised life FAQs,\nvalidate the effectiveness of our method.", "AI": {"tldr": "提出了一种新的任务，旨在检测原始AI生成文本和简单提示修改的文本，利用轻量级框架通过文本内部结构进行分类，实验结果验证了方法的有效性。", "motivation": "当前基于词级别的检测器容易受到改写或简单提示的影响，且容易受ChatGPT词级别模式和训练数据内容的偏见影响，对修改后的文本效果不佳。", "method": "提出了一种基于文本内部结构的轻量级框架，该框架编码来自预训练语言模型的句子嵌入并通过注意力机制建模它们之间的关系。同时利用对比学习减轻自回归生成的嵌入偏见，并通过因果图结合反事实方法，从话题相关偏见中分离出结构特征。", "result": "实验结果在两个精选数据集上显示了方法的有效性，包括摘要比较和修改后的生命FAQ。", "conclusion": "研究表明，基于内部结构的文本分类方法能够有效检测并区分AI生成的文本及其修改版本，提供了一种更鲁棒的文本生成检测方法。"}}
{"id": "2509.18308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18308", "abs": "https://arxiv.org/abs/2509.18308", "authors": ["Yixin Zhang", "Ryan Chamberlain", "Lawrance Ngo", "Kevin Kramer", "Maciej A. Mazurowski"], "title": "Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model", "comment": "submitted to WACV 2026 application track, model weights available at:\n  https://github.com/mazurowski-lab/PulmonaryEmbolismSegmentation", "summary": "In this study, we curated a densely annotated in-house dataset comprising 490\nCTPA scans. Using this dataset, we systematically evaluated nine widely used\nsegmentation architectures from both the CNN and Vision Transformer (ViT)\nfamilies, initialized with either pretrained or random weights, under a unified\ntesting framework as a performance audit. Our study leads to several important\nobservations: (1) 3D U-Net with a ResNet encoder remains a highly effective\narchitecture for PE segmentation; (2) 3D models are particularly well-suited to\nthis task given the morphological characteristics of emboli; (3) CNN-based\nmodels generally yield superior performance compared to their ViT-based\ncounterparts in PE segmentation; (4) classification-based pretraining, even on\nlarge PE datasets, can adversely impact segmentation performance compared to\ntraining from scratch, suggesting that PE classification and segmentation may\nrely on different sets of discriminative features; (5) different model\narchitectures show a highly consistent pattern of segmentation performance when\ntrained on the same data; and (6) while central and large emboli can be\nsegmented with satisfactory accuracy, distal emboli remain challenging due to\nboth task complexity and the scarcity of high-quality datasets. Besides these\nfindings, our best-performing model achieves a mean Dice score of 0.7131 for\nsegmentation. It detects 181 emboli with 49 false positives and 28 false\nnegatives from 60 in-house testing scans. Its generalizability is further\nvalidated on public datasets.", "AI": {"tldr": "本研究使用了包含490个CTPA扫描的密集注释数据集，系统评估了九种广泛使用的分割架构。研究发现3D U-Net表现最佳，3D模型适用于PE分割，CNN优于ViT模型，分类预训练可能影响分割性能，模型在相同数据集上表现出一致的性能模式，但远端栓子的分割仍然是难题。最佳模型在分割上的平均Dice得分为0.7131。", "motivation": "研究动机是评估不同分割架构在肺栓塞（PE）分割中的表现，并找出最有效的模型。", "method": "使用包含490个CTPA扫描的密集注释数据集，系统测试了基于CNN和ViT的九种分割架构。", "result": "3D U-Net表现最佳，CNN模型优于基于ViT模型，分类预训练可能对分割表现有害，模型性能一致，但是远端栓子的分割是难题。", "conclusion": "3D U-Net是一个高效的架构，适用于PE分割的任务。CNN的性能优于ViT，分类预训练可能不利于分割任务；不同的模型在该任务表现相似；核心和大型栓子容易分割，远端栓子仍然是难点。"}}
{"id": "2509.18536", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18536", "abs": "https://arxiv.org/abs/2509.18536", "authors": ["Jin Young Kim", "Ji Won Yoon"], "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs", "comment": "Published as a main conference paper at EMNLP 2025", "summary": "Recently, inference-time reasoning strategies have further improved the\naccuracy of large language models (LLMs), but their effectiveness on smaller\nmodels remains unclear. Based on the observation that conventional approaches\noften fail to improve performance in this context, we propose\n\\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering\n(CCQA), a novel reasoning method that can be effectively applied to SLMs.\nInspired by cycle consistency, CCQA generates a question from each reasoning\npath and answer, evaluates each by its similarity to the original question, and\nthen selects the candidate solution with the highest similarity score as the\nfinal response. Since conventional SLMs struggle to generate accurate questions\nfrom their own reasoning paths and answers, we employ a lightweight Flan-T5\nmodel specialized for question generation to support this process efficiently.\nFrom the experimental results, it is verified that CCQA consistently\noutperforms existing state-of-the-art (SOTA) methods across eight models on\nmathematical and commonsense reasoning benchmarks. Furthermore, our method\nestablishes a new practical baseline for efficient reasoning in SLMs. Source\ncode can be found at https://github.com/scai-research/ccqa_official.", "AI": {"tldr": "The paper introduces CCQA, a new reasoning method for improving the performance of smaller language models in various reasoning tasks, demonstrating superior performance to state-of-the-art approaches.", "motivation": "The motivation is to enhance the performance of smaller language models (SLMs) using a reasoning strategy and to address the observed limitation of conventional approaches that fail to improve SLM performance.", "method": "The paper proposes CCQA, a novel reasoning method inspired by cycle consistency tailored for small language models. CCQA uses a lightweight Flan-T5 model to generate questions, evaluates them based on similarity to the original question, and selects the best answer.", "result": "The experimental results show that CCQA outperforms existing state-of-the-art (SOTA) methods on mathematical and commonsense reasoning benchmarks across eight models.", "conclusion": "The paper concludes that CCQA is effective in improving reasoning performance in SLMs and establishes a new practical baseline for efficient reasoning."}}
{"id": "2509.18309", "categories": ["cs.CV", "cs.LG", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.18309", "abs": "https://arxiv.org/abs/2509.18309", "authors": ["Alessa Carbo", "Eric Nalisnick"], "title": "Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach", "comment": null, "summary": "Handshapes serve a fundamental phonological role in signed languages, with\nAmerican Sign Language employing approximately 50 distinct shapes.\nHowever,computational approaches rarely model handshapes explicitly, limiting\nboth recognition accuracy and linguistic analysis.We introduce a novel graph\nneural network that separates temporal dynamics from static handshape\nconfigurations. Our approach combines anatomically-informed graph structures\nwith contrastive learning to address key challenges in handshape recognition,\nincluding subtle interclass distinctions and temporal variations. We establish\nthe first benchmark for structured handshape recognition in signing sequences,\nachieving 46% accuracy across 37 handshape classes (with baseline methods\nachieving 25%).", "AI": {"tldr": "提出一种新的图神经网络方法，提高了对有声手语中手形的识别精度。", "motivation": "手形在有声手语中扮演了基本的音位角色。然而，计算方法很少明确建模手形，这限制了识别精度和语言分析。为了提高识别精度和促进语言分析，引入了新的方法。", "method": "引入了一种新的图神经网络，该网络将手形的时态动态与静态配置分离。该方法结合了基于解剖学的图结构和对比学习来解决手形识别中的关键问题，包括类间细微差别和时态变化。", "result": "建立了首个针对有声手语序列中结构化手形识别的基准，在37种手形类别中实现了46%的识别精度（基线方法仅为25%）。", "conclusion": "提出的新方法在手形识别上取得了显著的进展，为研究有声手语提供了新的工具。"}}
