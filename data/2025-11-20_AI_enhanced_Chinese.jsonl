{"id": "2511.14772", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14772", "abs": "https://arxiv.org/abs/2511.14772", "authors": ["Zhuoyi Yang", "Xu Guo", "Tong Zhang", "Huijuan Xu", "Boyang Li"], "title": "Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective", "comment": null, "summary": "With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research", "AI": {"tldr": "本文调查了通过分配额外推理时间计算资源来提高预训练大型语言模型预测准确性的技术，通过统一各类方法如串想法、分支解决合并法和树形思考法，为未来的研究方向提供了建议。", "motivation": "动机在于研究如何在推理阶段通过分配额外的计算资源来提升大语言模型的预测准确性，并统一分析各种方法。", "method": "法文中提到，通过在推理过程中分配额外的计算资源来提高预训练大语言模型的预测准确性。文中特别强调了问题如何被分解为子问题，以及这些子问题的拓扑组织结构（串行、并行或树形）。", "result": "文章将Chain-of-Thought, Branch-Solve-Merge, 和Tree-of-Thought等不同的方法统一在同一个框架下进行分析。", "conclusion": "文章总结了现有技术的优缺点，并为未来的研究方向提出了建议。"}}
{"id": "2511.14773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14773", "abs": "https://arxiv.org/abs/2511.14773", "authors": ["Joey David"], "title": "Temporal Predictors of Outcome in Reasoning Language Models", "comment": "4 pages, 4 figures", "summary": "The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.", "AI": {"tldr": "研究表明，大型语言模型在推理的早期阶段对最终结果有内部承诺，这对理解模型的工作方式及如何控制其推理过程具有重要意义。", "motivation": "研究大型语言模型在推理早期对最终结果的内部承诺时间点，以及这对模型解释性和推理时间控制的意义。", "method": "通过在前t个推理令牌后的隐藏状态上训练线性分类器来研究大型语言模型（LLM）在推理早期阶段是否已经对最终结果做出了内部承诺。", "result": "发现正确性可以在仅几个令牌之后就被高度预测，即使需要更长的输出以到达确定的答案。对于更难的问题，预测准确性下降表明一个选择性偏差：困难的问题在较长的推理链中更被选中。", "conclusion": "研究结果表明，对于推理模型，成功与否的内部评估在仅几个令牌之后就会出现，这对模型的解释性和推理时间控制有潜在影响。"}}
{"id": "2511.14774", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14774", "abs": "https://arxiv.org/abs/2511.14774", "authors": ["Pei-Fu Guo", "Yun-Da Tsai", "Chun-Chia Hsu", "Kai-Xin Chen", "Ya-An Tsai", "Kai-Wei Chang", "Nanyun Peng", "Mi-Yen Yeh", "Shou-De Lin"], "title": "LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs", "comment": null, "summary": "Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.", "AI": {"tldr": "该论文介绍了一个名为LiveCLKTBench的工具，用于评估跨语言知识转移的有效性，研究表明跨语言知识转移受语言距离影响，大型模型能改善这种转移但收益渐减。", "motivation": "评估大型语言模型中的跨语言知识转移具挑战性，因为目标语言中的正确答案可能来源于真实的知识转移或者模型预训练时的先前暴露。", "method": "我们提出了LiveCLKTBench，一个自动化的生成管道，设计用于分离和衡量跨语言知识转移。该管道从现实世界领域中识别自包含和时间敏感的知识实体，基于时间发生过滤，并验证与模型的知识。这些有效实体的文档被用于生成事实问题，并翻译成多种语言以评估跨语言边界的知识转移能力。", "result": "使用LiveCLKTBench，我们评估了几种LLMs在五种语言上的表现，发现跨语言转移受语言距离的影响且不同语言方向上的转移往往是不对称的。虽然较大的模型能够改善知识转移，但收益随着模型大小的增加而递减，且在各领域中有所不同。", "conclusion": "这些研究结果为跨语言转移提供了新见解，并展示了LiveCLKTBench作为未来研究中可靠的基准的价值。"}}
{"id": "2511.14776", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14776", "abs": "https://arxiv.org/abs/2511.14776", "authors": ["Snigdha Pandya", "Rohan Nagale", "Kenji Sahay", "Anna Lin", "Shikhar Shiromani", "Kevin Zhu", "Dev Sunishchal"], "title": "COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation", "comment": "9 pages, 6 figures including algorithmns, 2 tables", "summary": "Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.", "AI": {"tldr": "COMPASS框架通过直接嵌入到了解码过程中的基于模型反馈循环，量化了对上下文的依赖，并通过PID控制器动态调节注意力单元，改善了事实一致性，减少上下文偏差率。", "motivation": "理解和引导大语言模型在上下文知识和参数知识之间注意力分配的问题，以改善可信赖的应用部署和模型机制的科学解释。", "method": "提出了COMPASS（基于上下文调节的PID注意力引导系统），一个轻量级的、可解释的控制框架，它直接在解码过程中嵌入基于模型的反馈循环。", "result": "在多个基准（如HotpotQA、XSum、HaluEval和RAGTruth）上，COMPASS系统一致地减少了上下文偏差率（绝对减少2.8到5.8个百分点），同时也揭示了不同注意力头如何贡献于证据对齐。", "conclusion": "这些结果强调了基于反馈的可解释性是理解和改进大语言模型行为的重要途径。"}}
{"id": "2511.14848", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14848", "abs": "https://arxiv.org/abs/2511.14848", "authors": ["Yarin Bekor", "Gal Michael Harari", "Or Perel", "Or Litany"], "title": "Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video", "comment": "SIGGRAPH Asia 2025", "summary": "We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/", "AI": {"tldr": "该论文提出了一种名为Gaussian See, Gaussian Do的新方法，能够实现无需骨骼绑定、跨类别的语义三维运动传输，使用锚点的视角感知运动嵌入机制保证了跨视角的一致性和加速了收敛。", "motivation": "该研究的动机是在不需要复杂骨骼系统的情况下，实现不同类别对象之间的语义三维运动传输，通过引入锚点的视角感知运动嵌入机制来确保跨视角一致性和加速收敛。", "method": "该方法提出了一个名为Gaussian See, Gaussian Do的新方法，用于从多视角视频中进行语义三维运动传输。这种方法无需骨骼绑定，可以在不同类别对象之间进行有意义的语义对应运动传输。", "result": "该方法在确立的第一个语义三维运动传输基准上，展示了比适应基线更好的运动保真度和结构一致性。", "conclusion": "结论表明，该方法在实现不同对象类别之间的运动传输上具有较高的可靠性和改进，为语义3D运动传输领域引入了首个基准。"}}
{"id": "2511.14779", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14779", "abs": "https://arxiv.org/abs/2511.14779", "authors": ["Julio Cesar Galdino", "Sidney Evaldo Leal", "Leticia Gabriella De Souza", "Rodrigo de Freitas Lima", "Antonio Nelson Fornari Mendes Moreira", "Arnaldo Candido Junior", "Miguel Oliveira", "Edresson Casanova", "Sandra M. Aluísio"], "title": "The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech", "comment": null, "summary": "Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.", "AI": {"tldr": "研究评估了韵律分割标注对Brazilian Portuguese语音合成质量的影响，表明手动韵律分割产生的语音更自然，所有资源都公开提供。", "motivation": "尽管语音合成系统已经在生成自然而可懂的语音方面取得了显著进展，但带有明确韵律分割的数据集构建及其对自发语音合成的影响尚未得到充分探索。", "method": "通过评估在巴西葡萄牙语中手动和自动韵律分割标注对FastSpeech 2模型合成语音质量的影响来研究韵律分割标注的效果。", "result": "实验结果显示，使用韵律分割训练生成的语音更清晰、更自然。自动分割趋向于创建更规则的段落，而手动韵律分割带来了更大的变异性，更接近自然韵律。", "conclusion": "研究证明了韵律分割在自发语音合成中的重要性，并提供了所有数据集、源代码和训练模型，以便支持未来研究的可重复性和进一步探索。"}}
{"id": "2511.14860", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14860", "abs": "https://arxiv.org/abs/2511.14860", "authors": ["Aashish Ghimire", "Jun Zeng", "Roshan Paudel", "Nikhil Kumar Tomar", "Deepak Ranjan Nayak", "Harshith Reddy Nalla", "Vivek Jha", "Glenda Reynolds", "Debesh Jha"], "title": "When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation", "comment": "8 pages, 4 figures", "summary": "Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures. Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. Our code is available at: https://github.com/JunZengz/dental-caries-segmentation.", "AI": {"tldr": "The study benchmarks various state-of-the-art deep learning architectures for dental caries segmentation in panoramic radiographs, finding that the CNN-based DoubleU-Net outperforms more complex architectures.", "motivation": "To address the challenges of identifying and segmenting dental caries in panoramic radiographs, which are crucial for early diagnosis and treatment.", "method": "Twelve state-of-the-art architectures, including CNNs, transformers, and state-space mamba methods, were evaluated on a comprehensive dataset for dental caries segmentation.", "result": "Contrary to expectations, simpler CNN-based models like DoubleU-Net performed the best, achieving high dice coefficient, mIoU, and precision metrics.", "conclusion": "The study highlights the importance of aligning architecture complexity with task requirements, as more complex models did not outperform simpler ones in this domain-specific task."}}
{"id": "2511.14783", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.14783", "abs": "https://arxiv.org/abs/2511.14783", "authors": ["Bingquan Zhang", "Xiaoxiao Liu", "Yuchi Wang", "Lei Zhou", "Qianqian Xie", "Benyou Wang"], "title": "Human or LLM as Standardized Patients? A Comparative Study for Medical Education", "comment": "10 pages, 9 figures, 8 table", "summary": "Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.", "AI": {"tldr": "介绍了一个多代理框架EasyMED，用于模拟标准化病人的临床技能培训，通过SPBench基准进行系统评估，效果与人类SP相当且更具成本效益。", "motivation": "现有的标准病人（SP）对于临床技能培训至关重要，但成本高昂、灵活性差且难以扩展。现有的基于大语言模型（LLM）的SP模拟器承诺成本较低，但表现出不一致的行为，并且缺乏与人类SP的严格比较。", "method": "我们提出了EasyMED，这是一个多代理框架，包括一个用于真实对话的患者代理，一个用于事实一致性的辅助代理，以及一个提供可操作反馈的评估代理。为了支持系统的评估，我们引入了SPBench，这是一个涵盖14个专业领域和8个专家定义的评估标准的真实SP-医生互动基准。", "result": "实验表明，EasyMED在教学成果上与人类SP相当，同时为低基线的学生提供了更大的技能提升，并提供了提高的灵活性、心理安全性和成本效率。", "conclusion": "EasyMED不仅能匹配人类SP的学习成果，还能在技能提升、灵活性、心理安全性和成本效率方面提供改进。"}}
{"id": "2511.14870", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14870", "abs": "https://arxiv.org/abs/2511.14870", "authors": ["Fuyang Zhang", "Pradeep Kumar Jayaraman", "Xiang Xu", "Yasutaka Furukawa"], "title": "B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?", "comment": "Project page: https://zhangfuyang.github.io/brdf/", "summary": "This paper presents a novel geometric representation for CAD Boundary Representation (B-Rep) based on volumetric distance functions, dubbed B-Rep Distance Functions (BR-DF). BR-DF encodes the surface mesh geometry of a CAD model as signed distance function (SDF). B-Rep vertices, edges, faces and their topology information are encoded as per-face unsigned distance functions (UDFs). An extension of the Marching Cubes algorithm converts BR-DF directly into watertight CAD B-Rep model (strictly speaking a faceted B-Rep model). A surprising characteristic of BR-DF is that this conversion process never fails. Leveraging the volumetric nature of BR-DF, we propose a multi-branch latent diffusion with 3D U-Net backbone for jointly generating the SDF and per-face UDFs of a BR-DF model. Our approach achieves comparable CAD generation performance against SOTA methods while reaching the unprecedented 100% success rate in producing (faceted) B-Rep models.", "AI": {"tldr": "A novel BR-DF method for CAD model representations allows for a highly reliable conversion to watertight B-Rep models with a 100% success rate, using a volumetric distance function encoding.", "motivation": "The motivation behind this work is to improve the reliability and performance of CAD model generation, addressing common challenges like failure rates in conversion processes of traditional methods.", "method": "This paper introduces B-Rep Distance Functions (BR-DF) to represent CAD models, using distance functions to encode surface geometry and topology. It extends the Marching Cubes algorithm to convert BR-DF into a watertight B-Rep model and proposes a 3D U-Net based multi-branch latent diffusion for generating SDF and UDFs.", "result": "The proposed method achieves a 100% success rate in generating (faceted) B-Rep models, showing high reliability and performance.", "conclusion": "The paper concludes that BR-DF, combined with the proposed generation model, offers a robust and efficient solution for CAD model generation, significantly outperforming state-of-the-art methods in terms of success rate."}}
{"id": "2511.14796", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14796", "abs": "https://arxiv.org/abs/2511.14796", "authors": ["Adel Hidri", "Suleiman Ali Alsaif", "Muteeb Alahmari", "Eman AlShehri", "Minyar Sassi Hidri"], "title": "Opinion Mining and Analysis Using Hybrid Deep Neural Networks", "comment": "22 pages, 4 figures, 11 tables", "summary": "Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.", "AI": {"tldr": "本文提出了一种混合的BGRU-LSTM模型，用于增强情感分析的效果，相较于传统深度学习模型，该模型改善了语境微妙性处理、可扩展性和类别不平衡问题，并在实验中展示了优越的性能和泛化能力。", "motivation": "由于社交媒体和电子商务的影响日益增长，理解客户态度已成为决策中的关键组成部分。现存的方法，如基于词汇的方法和传统机器学习技术，对于处理语境微妙性和可扩展性是不足的。因此，提出了一种新的混合模型。", "method": "本研究提出了一种结合双向门控循环单元（BGRU）和长短时记忆（LSTM）层的混合深度神经网络模型，旨在改善情感分析中的语境微妙性处理、可扩展性和类别不平衡问题。", "result": "通过使用IMDB电影评论和亚马逊产品评估等基准数据集上的全面实验，提出的混合BGRU-LSTM（HBGRU-LSTM）模型在测试准确度上达到了95%，超过LSTM（93.06%）、CNN+LSTM（93.31%）和GRU+LSTM（92.20%）等传统深度学习框架的性能。此外，通过平衡数据集，该模型在负面情感上的召回率从86%提高到了96%，显著下降了误分类损失。", "conclusion": "提出的HBGRU-LSTM架构提升了情感分析的性能，特别是在处理语境微妙性和类不平衡方面非常有效，展示了更高的召回率和更小的误分类损失，表明其更强的泛化能力和稳健性。"}}
{"id": "2511.14884", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14884", "abs": "https://arxiv.org/abs/2511.14884", "authors": ["Antonio Ruiz", "Tao Wu", "Andrew Melnik", "Qing Cheng", "Xuqin Wang", "Lu Liu", "Yongliang Wang", "Yanfeng Zhang", "Helge Ritter"], "title": "GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis", "comment": null, "summary": "Methods that synthesize indoor 3D scenes from text prompts have wide-ranging applications in film production, interior design, video games, virtual reality, and synthetic data generation for training embodied agents. Existing approaches typically either train generative models from scratch or leverage vision-language models (VLMs). While VLMs achieve strong performance, particularly for complex or open-ended prompts, smaller task-specific models remain necessary for deployment on resource-constrained devices such as extended reality (XR) glasses or mobile phones. However, many generative approaches that train from scratch overlook the inherent graph structure of indoor scenes, which can limit scene coherence and realism. Conversely, methods that incorporate scene graphs either demand a user-provided semantic graph, which is generally inconvenient and restrictive, or rely on ground-truth relationship annotations, limiting their capacity to capture more varied object interactions. To address these challenges, we introduce GeoSceneGraph, a method that synthesizes 3D scenes from text prompts by leveraging the graph structure and geometric symmetries of 3D scenes, without relying on predefined relationship classes. Despite not using ground-truth relationships, GeoSceneGraph achieves performance comparable to methods that do. Our model is built on equivariant graph neural networks (EGNNs), but existing EGNN approaches are typically limited to low-dimensional conditioning and are not designed to handle complex modalities such as text. We propose a simple and effective strategy for conditioning EGNNs on text features, and we validate our design through ablation studies.", "AI": {"tldr": "The paper introduces GeoSceneGraph, a method for synthesizing 3D scenes from text prompts that leverages the graph structure and geometric symmetries of scenes, achieving competitive performance without using predefined relationship classes. It uses equivariant graph neural networks (EGNNs) and a novel strategy for text conditioning.", "motivation": "The motivation behind GeoSceneGraph is to improve the synthesis of indoor 3D scenes from text prompts, addressing the limitations of existing approaches that either ignore the inherent graph structure of indoor scenes or impose restrictions on the user or require ground-truth annotations.", "method": "Structure", "result": "GeoSceneGraph achieves performance comparable to methods that rely on ground-truth relationships, despite not using such relationships, successfully synthesizing coherent and realistic 3D scenes from text prompts.", "conclusion": "The paper concludes that GeoSceneGraph is a promising approach for generating 3D indoor scenes from text, combining the benefits of graph structure and geometric symmetry without the need for predefined relationship classes, and it effectively conditions EGNNs on text features."}}
{"id": "2511.14868", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14868", "abs": "https://arxiv.org/abs/2511.14868", "authors": ["Xueying Ding", "Xingyue Huang", "Mingxuan Ju", "Liam Collins", "Yozen Liu", "Leman Akoglu", "Neil Shah", "Tong Zhao"], "title": "Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings", "comment": null, "summary": "Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.", "AI": {"tldr": "论文提出了HTP方法，解决大语言模型嵌入中的注意力限制和信息压缩问题，特别是在长文档的处理中实现了性能的提升。", "motivation": "大语言模型生成强大的文本嵌入，但它们的因果注意机制限制了从后期到早期标记的信息流动，损害了表示质量。虽然最近的方法试图通过添加单个摘要令牌来解决此问题，但它们过度压缩信息，从而在长文档上损害性能。", "method": "HTP（分层令牌前置）方法，通过将输入分割成块并在后续块前添加块级摘要令牌来解决注意力级别的压缩问题。为了缓解读出级别的过度压缩，用均值池化替代了最后一个令牌池化。", "result": "HTP在长上下文场景下显示出显着的性能提升。该方法被证明可以提高零样本和微调模型的性能。", "conclusion": "HTP在11个检索数据集和30个通用嵌入基准测试中实现了持续的性能提升，特别是在长上下文设置中。这是一个简单的、架构无关的方法，可以提升零样本和微调模型的性能，为长文档嵌入提供了可扩展的改进途径。"}}
{"id": "2511.14897", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14897", "abs": "https://arxiv.org/abs/2511.14897", "authors": ["Pranav Indrakanti", "Ivor Simpson"], "title": "HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation", "comment": "Submitted to ISBI 2026", "summary": "We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.", "AI": {"tldr": "A physics-inspired unsupervised model for synthesizing MRI images from HF to ULF and vice-versa, showing significant improvement in contrast and robustness in sensitivity experiments.", "motivation": "To enable more accurate bidirectional MRI image synthesis between HF and ULF environments without requiring supervised training data, by leveraging the physical principles that dictate MRI image contrasts and developing a robust forward model and super-resolution approach.", "method": "We propose an unsupervised method to synthesize bidirectional MRI images between High-Field (HF) and Ultra-Low Field (ULF) using a model grounded in the physics of MRI contrast changes. The forward model simulates HF to ULF transformation by estimating SNR values, and an Implicit Neural Representation (INR) network is used for the Super-Resolution task to synthesize HF images.", "result": "The synthesis method improved WM-GM contrast by 52% in synthetic ULF-like images and 37% in 64mT images. The forward model demonstrated robustness to variations in target contrast, noise, and initial seeding.", "conclusion": "The proposed physics-inspired unsupervised model effectively synthesizes MRI images for bidirectional transformations and shows robustness under various conditions."}}
{"id": "2511.15005", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15005", "abs": "https://arxiv.org/abs/2511.15005", "authors": ["Moses Kiprono"], "title": "Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation", "comment": "10 pages, theoretical/mathematical LLM research, no figures, intended for peer-reviewed journal", "summary": "Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.", "AI": {"tldr": "论文提出了一个数学基础的框架，用于理解、度量并减轻大型语言模型中的幻觉现象。", "motivation": "大型语言模型（LLMs）虽然强大，但在输出方面容易出现幻觉，即输出看似合理但实际上不正确或缺乏支持的事实信息。", "method": "该论文采用概率模型、信息论、三角信号分析和贝叶斯不确定性估计等数学工具，分析了LLMs中错误的累积机制，并提出了改进的不确定性度量，如语义和相位感知变体。", "result": "提出了对比解码、检索增强的置信度、事实对齐和保留等原则性的缓解策略。", "conclusion": "该研究为理解和减轻LLMs中的幻觉现象提供了一个统一的视角，促成了更安全、更可靠的LLMs的发展。"}}
{"id": "2511.14899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14899", "abs": "https://arxiv.org/abs/2511.14899", "authors": ["Daniel Gilo", "Or Litany"], "title": "InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization", "comment": null, "summary": "We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.", "AI": {"tldr": "我们提出了InstructMix2Mix框架，该框架通过利用多视图扩散模型的数据驱动3D先验知识，解决了现有方法在多视图图像编辑中的问题，提高了编辑一致性和质量。", "motivation": "现有的基于每个场景神经场或时间注意力机制的方法，在从稀疏输入视图进行多视图图像编辑时，常常会产生伪影和不连贯的编辑结果。我们的目标是在保持所有视图一致性的前提下，根据文本指令修改场景。", "method": "我们提出了InstructMix2Mix（I-Mix2Mix）框架，该框架将2D扩散模型的编辑能力提炼到一个预训练的多视图扩散模型中，利用其数据驱动的3D先验知识来保持跨视图的一致性。主要贡献在于用多视图扩散学生（multi-view diffusion student）替换Score Distillation Sampling（SDS）中的常规神经场汇编器（neural field consolidator），并进行了适应性修改：时间步长上的增量学生更新、特殊教师噪声调度程序以防止退化、以及注意力机制的修改，以增强跨视图一致性且不增加额外成本。", "result": "实验结果表明，I-Mix2Mix在提高多视图一致性的同时，保持了高质量的每帧编辑效果。", "conclusion": "I-Mix2Mix框架在保持跨视图一致性的多视图图像编辑任务中展示出显著改进，同时维持高质量的编辑结果。"}}
{"id": "2511.15163", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15163", "abs": "https://arxiv.org/abs/2511.15163", "authors": ["Yang Wu", "Rujing Yao", "Tong Zhang", "Yufei Shi", "Zhuoren Jiang", "Zhushan Li", "Xiaozhong Liu"], "title": "Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs", "comment": "AAAI 2026 Workshop", "summary": "Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.", "AI": {"tldr": "本文提出了一种新的学生意识辅导框架TASA，该框架整合了人格、记忆和遗忘动态，能动态更新学生掌握状态并生成适当的辅导问题和解释，从而提高了学习效果。", "motivation": "大多数现有的方法未能捕捉到学生的知识如何在他们各自的熟练度、概念差距和遗忘模式中动态变化。特别是在数学辅导中，有效的教学需要精细的支架，这些支架需要精确地调整到每个学生的掌握水平和认知保留。", "method": "提出TASA（根据学生能力教学）框架，该框架整合了人格、记忆和遗忘动态，以实现个性化数学学习。TASA通过维持结构化的学生人格（捕捉熟练度分布）和事件记忆（记录先前学习交互）来实现这一点。TASA结合了连续的遗忘曲线和知识追踪，动态地更新每个学生的掌握状态，并生成适当难度和上下文的问题及解释。", "result": "实证结果表明，与代表性的基线方法相比，TASA实现了更好的学习效果和更适应的教学行为，强调了在基于LLM的辅导系统中建模时间遗忘和学习者轮廓的重要性。", "conclusion": "TASA框架通过整合人格、记忆和遗忘动态，实现了更适应的学生数学辅导，并证明了在基于LLM的辅导系统中建模时间遗忘和学习者轮廓的重要性。"}}
{"id": "2511.14900", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14900", "abs": "https://arxiv.org/abs/2511.14900", "authors": ["Zehao Liu", "Wejieying Ren", "Jipeng Zhang", "Tianxiang Zhao", "Jingxi Zhu", "Xiaoting Li", "Vasant G. Honavar"], "title": "Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis", "comment": null, "summary": "The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.\n  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.", "AI": {"tldr": "提出SkinR1模型，结合深度教材推理与强化学习，解决皮肤病诊断中的数据异质性、缺乏可靠诊断逻辑和模型泛化能力不足的问题，展现优越的诊断准确性。", "motivation": "解决现有皮肤病视觉语言模型在数据异质性、缺乏诊断逻辑和泛化能力方面的限制，提高模型的临床可信度和实用性。", "method": "通过结合教材推理生成器和强化学习，设计一个统一的端到端框架。生成器合成高保真的推理轨迹，为SFT提供专家级监督，并利用层次结构的RL提升模型在大规模稀疏数据上的泛化能力。", "result": "实验显示，SkinR1在多个皮肤病数据集上具有诊断准确性，消融研究表明SFT的重要性，增强了推理能力。", "conclusion": "SkinR1通过整合深度推理与强化学习，在皮肤病诊断中解决了关键挑战，展现出优越的临床潜力。"}}
{"id": "2511.15183", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15183", "abs": "https://arxiv.org/abs/2511.15183", "authors": ["Rishikant Chigrupaatii", "Ponnada Sai Tulasi Kanishka", "Lalit Chandra Routhu", "Martin Patel Sama Supratheek Reddy", "Divyam Gupta", "Dasari Srikar", "Krishna Teja Kuchimanchi", "Rajiv Misra", "Rohun Tripathi"], "title": "HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples", "comment": null, "summary": "With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.", "AI": {"tldr": "研究构建了一个用于评估印度语系视觉语言模型的框架，发现模型从英语转换到印地语或泰卢固语时性能显著下降。", "motivation": "随着多语言视觉语言模型（VLMs）的重要性日益显现，对这些模型进行可靠评估的方法变得至关重要，从而推动低资源语言中更公平的人工智能发展。当前的多语言VLM评估存在四个主要问题：依赖未经验证的自动翻译、任务/领域覆盖范围狭窄、样本量有限，以及缺乏文化和本地化的问答。为了弥补这些缺陷，本文提出了一种在印度语系中评估VLM的可扩展框架，并将其与英语性能进行比较。", "method": "本文提出了一种半自动化数据集创建框架，结合了反向翻译、过滤和人工验证。此框架用于生成HinTel-AlignBench基准，该基准从多样化的来源选取数据，在印地语和泰卢固语中与英文样本相匹配。", "result": "使用此框架，作者生成了一个最全面的面向印地语和泰卢固语的视觉语言基准，包括英译数据集（如VQAv2, RealWorldQA, CLEVR-Math）和原创的印度语数据集（如STEM专题的JEE和文化根基的VAANI）。包含大约4000个问题/答案对。", "conclusion": "研究表明，在四种英语任务中的语言切换到印度语言后，所有模型的性能平均下降了，其中印地语任务下降了8.3分，泰卢固语任务下降了5.5分。研究还对常见失败模式进行了分类，以此强调在多语言和多模态理解中的具体改进领域。"}}
{"id": "2511.14901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14901", "abs": "https://arxiv.org/abs/2511.14901", "authors": ["Zhenshi Li", "Weikang Yu", "Dilxat Muhtar", "Xueliang Zhang", "Pengfeng Xiao", "Pedram Ghamisi", "Xiao Xiang Zhu"], "title": "FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding", "comment": null, "summary": "As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.", "AI": {"tldr": "通过构建多粒度遥感图像-文本数据集和提出FarSLIP预训练框架，极大地提高了遥感领域细粒度视觉-语言对齐的性能。", "motivation": "在增强遥感图像领域细粒度区域文本对齐方面遇到的阻碍以及识别捕捉细节的限制，促使构建了多粒度遥感图像-文本数据集 MGRS-200k，以充分利用多粒度监督信息。接着，提出了FarSLIP作为新的前沿方法，它引入了一种细粒度的遥感语言-图像预训练框架，旨在提供更好的视觉-文本对齐能力。", "method": "Structure", "result": "{\n  \"tldr\": \"FarSLIP 提出了一种新的预训练框架，用于改进遥感图像中细粒度的视觉-语言对齐，通过引入多粒度遥感图像-文本数据集 MGRS-200k 和特定的预训练策略，提高了对地物识别的准确性。\", \n  \"motivation\": \"传统的 CLIP 模型存在空间感知能力有限的问题，尤其是在遥感图像处理领域。为了充分利用多粒度监督信息，提高遥感图像中细粒度区域文本对齐的性能，提出了一个新的框架。\", \n  \"method\": \"FarSLIP 采用了一种新的 patch-to-patch 自蒸馏方法，并结合简单的 CLS-token 基于区域类别的对齐策略，解决了 CLIP 模型在细粒度对齐策略上遇到的问题。\", \n  \"result\": \"FarSLIP 在遥感图像的细粒度语义分割任务上创造了新的记录，并在图像级任务，如零样本分类和图像-文本检索方面也表现优异。\", \n  \"conclusion\": \"通过 FarSLIP 框架，遥感领域的细粒度视觉-语言对齐得到了显著提升，推动了这个领域的发展。\"}", "conclusion": "FarSLIP 在细粒度视觉-语言对齐上显著提升，并开创了遥感视觉任务的新境界，因此对于空间感知领域具有重要意义。"}}
{"id": "2511.15210", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15210", "abs": "https://arxiv.org/abs/2511.15210", "authors": ["Vladislav Pedashenko", "Laida Kushnareva", "Yana Khassan Nibal", "Eduard Tulchinskii", "Kristian Kuznetsov", "Vladislav Zharchinskii", "Yury Maximov", "Irina Piontkovskaya"], "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story", "comment": null, "summary": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.", "AI": {"tldr": "本研究探讨了内在维度（ID）在现代大语言模型中的作用，通过多种方法研究得出ID不依赖于预测质量的结论，并发现了科学写作相较于其他类型写作对LLM来说容易得多。", "motivation": "该论文通过研究语言模型中的内在维度（ID）来填补现有研究空白。ID是一个重要的工具，被用于现代大语言模型分析，但其文本决定因素尚未有充分探讨。", "method": "研究通过交叉编码分析、语言特征和稀疏自动编码器（SAE），将ID与可解释的文本属性联系起来。", "result": "该研究有三个关键发现：1) ID与基于熵的度量相互独立，揭示了额外的几何复杂性；2) ID在不同文体中表现出稳健的分层，科学文体的ID较低，奇幻/观点写作的ID较高；3) 使用SAE，科学信号（形式化的语气、报告模板）降低ID，而人类化信号（个性化、情感、叙事）增加ID。", "conclusion": "研究表明科学写作对现代LLM来说较为容易，而小说、观点和情感内容需要更多的表示自由度。研究成果为ID的使用和基于ID结果的解释提供了实际指导。"}}
{"id": "2511.14907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14907", "abs": "https://arxiv.org/abs/2511.14907", "authors": ["Xiangde Luo", "Jinxi Xiang", "Yuanfeng Ji", "Ruijiang Li"], "title": "nnMIL: A generalizable multiple instance learning framework for computational pathology", "comment": "A conceptual evaluation work; more studies are in progress; examples are here (https://github.com/Luoxd1996/nnMIL)", "summary": "Computational pathology holds substantial promise for improving diagnosis and guiding treatment decisions. Recent pathology foundation models enable the extraction of rich patch-level representations from large-scale whole-slide images (WSIs), but current approaches for aggregating these features into slide-level predictions remain constrained by design limitations that hinder generalizability and reliability. Here, we developed nnMIL, a simple yet broadly applicable multiple-instance learning framework that connects patch-level foundation models to robust slide-level clinical inference. nnMIL introduces random sampling at both the patch and feature levels, enabling large-batch optimization, task-aware sampling strategies, and efficient and scalable training across datasets and model architectures. A lightweight aggregator performs sliding-window inference to generate ensemble slide-level predictions and supports principled uncertainty estimation. Across 40,000 WSIs encompassing 35 clinical tasks and four pathology foundation models, nnMIL consistently outperformed existing MIL methods for disease diagnosis, histologic subtyping, molecular biomarker detection, and pan- cancer prognosis prediction. It further demonstrated strong cross-model generalization, reliable uncertainty quantification, and robust survival stratification in multiple external cohorts. In conclusion, nnMIL offers a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the development and deployment of reliable AI systems in real-world settings.", "AI": {"tldr": "Developed nnMIL for more reliable and generalizable aggregation of image patches into clinical predictions in computational pathology, showing superior performance across various tasks.", "motivation": "To enhance the generalizability and reliability of slide-level clinical inference by optimizing how patch-level features from pathology WSIs are aggregated.", "method": "nnMIL, a multiple-instance learning framework designed to address limitations in current approaches for aggregating patch-level features into slide-level predictions in computational pathology.", "result": "nnMIL outperformed existing methods across 35 clinical tasks using 40,000 WSIs and demonstrated strong cross-model generalization and uncertainty quantification.", "conclusion": "nnMIL provides a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the deployment of reliable AI systems."}}
{"id": "2511.15211", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15211", "abs": "https://arxiv.org/abs/2511.15211", "authors": ["Xinli Tao", "Xin Dong", "Xuezhong Zhou"], "title": "OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition", "comment": "12 pages, 4 figures, 4 tables", "summary": "Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.", "AI": {"tldr": "The paper presents OEMA, a zero-shot clinical NER framework utilizing multi-agent collaboration, which achieves near-supervised performance on clinical datasets like MTSamples and VAERS.", "motivation": "To overcome the challenges of high annotation costs for supervised models and the limitations of existing zero-shot NER approaches, such as granularity issues in example selection and integrating prompts with self-improvement.", "method": "OEMA, a zero-shot clinical NER framework that utilizes multi-agent collaboration, consists of three components: a self-annotator for generating examples, a discriminator for filtering them with SNOMED CT, and a predictor for accurate inference.", "result": "OEMA showed state-of-the-art exact-match performance on clinical datasets and matched the performance of supervised BioClinicalBERT under related-match criteria, surpassing CRF.", "conclusion": "OEMA addresses key challenges in zero-shot NER through ontology-guided reasoning and multi-agent collaboration, demonstrating strong potential for clinical NLP applications."}}
{"id": "2511.14918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14918", "abs": "https://arxiv.org/abs/2511.14918", "authors": ["Zefan Yang", "Ge Wang", "James Hendler", "Mannudeep K. Kalra", "Pingkun Yan"], "title": "X-WIN: Building Chest Radiograph World Model via Predictive Sensing", "comment": null, "summary": "Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.", "AI": {"tldr": "A CXR world model named X-WIN is introduced to distill volumetric knowledge from CT scans, improving the representation of 3D anatomies on 2D CXR images. This enhances disease diagnosis capabilities, outperforming existing models on several tasks.", "motivation": "The paper aims to overcome the limitations of 2D CXR images in representing 3D anatomical structures, which complicates disease diagnosis and representation learning. By developing X-WIN, the authors seek to enhance the diagnostic capabilities of CXR images through the use of 3D anatomical knowledge.", "method": "To address the limitations of 2D CXR images, the authors propose a novel CXR world model called X-WIN. This model distills volumetric knowledge from chest CTs by predicting 2D projections of these volumes in latent space. An affinity-guided contrastive alignment loss is used to capture correlated information across projections from the same volume. Masked image modeling and a domain classifier are employed to improve model adaptability by incorporating real CXRs into the training.", "result": "Experiments demonstrate that X-WIN outperforms existing foundation models on various downstream tasks using linear probing and few-shot fine-tuning. Additionally, X-WIN shows the capability to render 2D projections for the reconstruction of 3D CT volumes.", "conclusion": "The proposed X-WIN model improves the ability of CXR images to represent 3D anatomical structures, thereby aiding in disease diagnosis. The model's performance on various tasks and its capability to reconstruct 3D CT volumes from 2D projections confirm its effectiveness."}}
{"id": "2511.15244", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15244", "abs": "https://arxiv.org/abs/2511.15244", "authors": ["Fanfan Liu", "Haibo Qiu"], "title": "Context Cascade Compression: Exploring the Upper Limits of Text Compression", "comment": null, "summary": "Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression", "AI": {"tldr": "本研究提出了一种名为C3（Context Cascade Compression）的压缩方法，该方法通过级联两个LLMs实现了高比率的文本压缩和解码，取得了超出光学压缩方法的显著效果和准确率。", "motivation": "为了应对长上下文任务中百万级标记输入带来的计算和内存挑战，以及探索文本压缩的上限，受到DeepSeek-OCR研究的启发，我们引入了C3来进一步提高文本压缩比和解码准确性。", "method": "本研究引入了上下文级联压缩（C3），使用两个不同大小的大型语言模型（LLMs）级联来处理文本压缩和解码任务。较小的LLM作为第一阶段，通过将长上下文压缩成一组潜藏标记（例如，长度为32或64）来实现文本压缩。较大的LLM作为第二阶段，负责解码被压缩的上下文。", "result": "实验显示，当压缩比为20倍时，我们的模型实现了98%的解码准确率，而DeepSeek-OCR则约为60%。进一步将压缩比提升至40倍时，其准确率依然保持在93%左右。", "conclusion": "C3在上下文压缩领域展现出优越的性能和应用潜力，其纯文本管道处理方法为未来的光学字符压缩研究设定了压缩比的上限。"}}
{"id": "2511.14927", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.14927", "abs": "https://arxiv.org/abs/2511.14927", "authors": ["Kaiyuan Hu", "Yili Jin", "Junhua Liu", "Xize Duan", "Hong Kang", "Xue Liu"], "title": "CPSL: Representing Volumetric Video via Content-Promoted Scene Layers", "comment": null, "summary": "Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.\n  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.", "AI": {"tldr": "本研究提出了一种新的2.5D视频表示方法——内容增强场景层（CPSL），通过指导深度和内容重要性分解帧为几何一致的层，实现了视差校正的视图合成且大幅度降低了成本。这种方法在保持高质量的感知效果的同时，降低了存储和渲染成本，支持实时播放。", "motivation": "现有体视频表示在实现沉浸式和交互式视觉体验上的成本较高，限制了它们的可扩展性和实时通信的可行性。本研究旨在解决这一问题，使得常规2D内容可以享受到体视频的感知益处。", "method": "提出内容增强场景层（CPSL）方法，用以解决现有体视频表示（不管是显式的点云形式还是隐式的神经场形式）在捕获、计算及渲染方面成本高导致的可扩展性问题。CPSL通过指导深度和内容重要性，将每个帧分解成多个几何一致的层，并通过软alpha边缘带和边缘深度缓存共同保存遮挡顺序和边界连续性。这种方法无需昂贵的3D重建，实现了视差校正的视图合成，支持使用标准视频编解码器进行实时播放，同时还降低了存储和渲染成本。", "result": "CPSL在多个基准测试中实现了优于分层基线和神经场基线的感知质量和边界保真度，同时将存储和渲染成本降低了数倍。", "conclusion": "该研究提供的方法为从2D视频到可扩展的2.5D沉浸式媒体提供了一条实际途径，同时解决了现有体视频表示方法遇到的成本问题。"}}
{"id": "2511.15260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15260", "abs": "https://arxiv.org/abs/2511.15260", "authors": ["Sowmya Vajjala"], "title": "IndicGEC: Powerful Models, or a Measurement Mirage?", "comment": "Technical report", "summary": "In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.", "AI": {"tldr": "本文通过使用不同大小的语言模型在BHASHA-Task 1任务五种印度语言的语法纠正实验中取得良好成绩，并讨论了数据质量和评估指标的相关问题。", "motivation": "我们的动机是探讨不同大小语言模型的潜力，特别是在印度语言中的语法错误纠正能力，同时评估现有的数据质量和度量标准。", "method": "我们采用了零样本或少量样本提示（zero/few-shot prompting）不同大小的语言模型（从4B到大型专有模型）的方法进行语法错误纠正。", "result": "我们在BHASHA-Task 1任务中对五种印度语言进行了研究，取得了排名第四的泰卢固语和排名第二的印地语的GLEU得分分别为83.78和84.31。我们在其他三种语言——泰米尔语、马拉雅拉姆语和孟加拉语上扩展实验，并更详细地探讨了数据质量及评估指标。", "conclusion": "我们的研究主要揭示了小型语言模型在这一任务中的潜力，并提出了关于如何创建适合印度语言脚本的良好数据集和适当度量标准的担忧。"}}
{"id": "2511.14945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14945", "abs": "https://arxiv.org/abs/2511.14945", "authors": ["Fan Yang", "Quanting Xie", "Atsunori Moteki", "Shoichi Masui", "Shan Jiang", "Yonatan Bisk", "Graham Neubig"], "title": "Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities", "comment": "accepted to WACV 2026", "summary": "Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.", "AI": {"tldr": "研究创建了一个包含580个多模态人类活动序列的数据集，分析长期周期性工作流，并提出了一种轻量级无训练基准模型。", "motivation": "解决长期周期性工作流研究不足的问题，为该领域提供新的数据集和研究方法。", "method": "Structure", "result": "{\n  \"tldr\": \"研究团队创建了一个包含580个多模态人类活动序列的数据集，用于分析长期周期性工作流。提供了三项评估任务和一个轻量级无训练基准，显示出在检测、跟踪和异常检测任务上的优势。\",\n  \"motivation\": \"长期周期性工作流的研究相对不足，该项目旨在填补这一研究空白，为相关研究提供数据集和评估方法。\",\n  \"method\": \"创建了一个包含多种人类活动的工作流数据集，并提出了一种轻量级无训练的基准模型来分析这些工作流。\",\n  \"result\": \"该基准模型在所有评估任务上都超过了现有的方法，并且在实际应用中展示了不需要标注和重新训练的优势。\",\n  \"conclusion\": \"项目提供了一个新的研究方向和数据集，对周期性工作流的检测、跟踪和异常检测具有显著的实用价值。\"]", "conclusion": "新基准模型展示出在实际应用上的实用优势，无需标注和重新训练，提供开创性的数据和评估框架。"}}
{"id": "2511.15291", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15291", "abs": "https://arxiv.org/abs/2511.15291", "authors": ["Randa Zarnoufi"], "title": "MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews", "comment": null, "summary": "Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.", "AI": {"tldr": "本文提出了一种针对阿拉伯方言情感分析的方法，特别关注酒店领域的评论，通过SetFit框架提升了少样本学习的有效性。", "motivation": "研究阿拉伯方言情感分析面临的挑战，包括语言多样性及标注数据稀缺的问题。", "method": "采用SetFit（Sentence Transformer微调）框架，这是一种数据高效的少样本学习技术。", "result": "在官方评估集中实现了73%的F1值，在26个参赛团队中排名第12。", "conclusion": "展示了少样本学习在处理如酒店评价这类专业领域中细膩的方言阿拉伯文本中的潜力。"}}
{"id": "2511.14948", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14948", "abs": "https://arxiv.org/abs/2511.14948", "authors": ["Jaro Meyer", "Frédéric Giraud", "Joschua Wüthrich", "Marc Pollefeys", "Philipp Fürnstahl", "Lilian Calvet"], "title": "RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems", "comment": "16 pages, 6 figures", "summary": "Accurate spatiotemporal alignment of multi-view video streams is essential for a wide range of dynamic-scene applications such as multi-view 3D reconstruction, pose estimation, and scene understanding. However, synchronizing multiple cameras remains a significant challenge, especially in heterogeneous setups combining professional and consumer-grade devices, visible and infrared sensors, or systems with and without audio, where common hardware synchronization capabilities are often unavailable. This limitation is particularly evident in real-world environments, where controlled capture conditions are not feasible. In this work, we present a low-cost, general-purpose synchronization method that achieves millisecond-level temporal alignment across diverse camera systems while supporting both visible (RGB) and infrared (IR) modalities. The proposed solution employs a custom-built \\textit{LED Clock} that encodes time through red and infrared LEDs, allowing visual decoding of the exposure window (start and end times) from recorded frames for millisecond-level synchronization. We benchmark our method against hardware synchronization and achieve a residual error of 1.34~ms RMSE across multiple recordings. In further experiments, our method outperforms light-, audio-, and timecode-based synchronization approaches and directly improves downstream computer vision tasks, including multi-view pose estimation and 3D reconstruction. Finally, we validate the system in large-scale surgical recordings involving over 25 heterogeneous cameras spanning both IR and RGB modalities. This solution simplifies and streamlines the synchronization pipeline and expands access to advanced vision-based sensing in unconstrained environments, including industrial and clinical applications.", "AI": {"tldr": "This work introduces a cheap, versatile synchronization approach using an LED Clock to align video streams from various camera systems, achieving high accuracy and enhancing performance in downstream tasks.", "motivation": "The motivation is to develop a low-cost, general-purpose synchronization method that addresses the challenge of aligning multi-view video streams in heterogeneous setups, which is essential for various dynamic-scene applications and real-world environments.", "method": "The paper proposes a synchronization method using a custom-built LED Clock that encodes time with red and infrared LEDs, allowing for visual decoding of exposure window times from frames to achieve millisecond-level synchronization across diverse camera systems.", "result": "The method achieves a residual error of 1.34 ms RMSE and outperforms other synchronization approaches. It shows superior performance in downstream computer vision tasks like multi-view pose estimation and 3D reconstruction.", "conclusion": "The proposed method simplifies and streamlines the synchronization process, expands access to sophisticated vision-based sensing in various applications, and performs well in large-scale surgical recordings with over 25 heterogeneous cameras."}}
{"id": "2511.15304", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15304", "abs": "https://arxiv.org/abs/2511.15304", "authors": ["Piercosma Bisconti", "Matteo Prandi", "Federico Pierucci", "Francesco Giarrusso", "Marcantonio Bracale", "Marcello Galisai", "Vincenzo Suriani", "Olga Sorokoletova", "Federico Sartore", "Daniele Nardi"], "title": "Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models", "comment": null, "summary": "We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.", "AI": {"tldr": "诗歌作为一种单轮破解技巧可以普遍应用于大型语言模型的安全性测试，并显示出当前对齐方法和评估协议的基本限制，尤其对于不同家族和安全培训方法的模型存在系统性漏洞。", "motivation": "研究动机是探索诗歌作为一种单轮破解技巧，能否普遍应用于大型语言模型的安全性测试，并揭示当前对齐方法和评估协议的基本限制。", "method": "本文通过使用精心设计的诗歌提示来测试25个前沿的专有和开放权重的语言模型，以评估其对潜在有害内容的防御能力。作者首先将1,200个MLCommons有害提示转换成诗歌形式，然后使用一组开放式权重的评判模型和人工验证的分层子集来评估结果。", "result": "研究结果表明，诗歌形式的提示能大幅度提高攻击成功率，最高可达非诗歌基准线的18倍，平均成功率为62%（手工艺诗歌）和43%（元提示转换），这显示出一种系统性漏洞，可能影响不同模型家族和安全培训方法。", "conclusion": "结论指出，仅仅通过风格变化，就可以规避当前的安全防护机制，这表明当前的对齐方法和评估协议存在着根本性的局限性。"}}
{"id": "2511.14952", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14952", "abs": "https://arxiv.org/abs/2511.14952", "authors": ["Mohamed Abdallah Salem", "Hamdy Ahmed Ashour", "Ahmed Elshenawy"], "title": "Artificial intelligence approaches for energy-efficient laser cutting machines", "comment": null, "summary": "This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.", "AI": {"tldr": "文章提出了一种基于深度学习的自适应控制方法，通过动态调整CO2激光切割过程中抽风泵的功率输出，实现了20%至50%的能量节省，推动了制造业的可持续发展。", "motivation": "研究旨在解决激光切割过程中的能源消耗和环境影响问题，针对CO2激光抽风泵目前存在的控制缺乏自适应性和闭环系统的问题，提出了新的解决方案。", "method": "提出了一种基于深度学习的方法，包括使用定制的卷积神经网络（CNN）进行无透镜散斑传感的材料分类，以及利用带有迁移学习的预训练VGG16 CNN模型的USB相机进行材料分类。此外，还采用了单独的深度学习模型来检测烟雾水平，从而动态地调整抽风泵的功率输出。", "result": "实验结果表明，通过该方法能够实现烟雾抽风泵能量消耗减少20%至50%，这对于推动制造业的可持续发展具有重要意义。", "conclusion": "本研究所提出的方法，通过采用基于深度学习的自适应控制系统，显著降低了激光切割过程中的能量消耗，减少环境影响，对实现制造业的可持续发展具有积极影响。"}}
{"id": "2511.15355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15355", "abs": "https://arxiv.org/abs/2511.15355", "authors": ["Alexis Correa-Guillén", "Carlos Gómez-Rodríguez", "David Vilares"], "title": "HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning", "comment": "Preprint. 12 pages", "summary": "We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.", "AI": {"tldr": "HEAD-QA v2增强了医疗推理数据集，证明了模型规模和推理能力对性能的影响。", "motivation": "响应高质量数据集需求的增长，这些数据集需要捕捉医疗推理的词汇和概念复杂性。", "method": "HEAD-QA v2扩展并更新了之前发布的西班牙语/英语医疗多选推理数据集，增加了超过12,000个问题，这些问题来自十年内的西班牙专业考试，并对几种开源的大规模语言模型进行了基准测试。", "result": "结果表明，性能主要由模型规模和内在推理能力驱动，而复杂的推理策略没有显著的优势。", "conclusion": "这些结果共同确立了HEAD-QA v2作为推动生物医学推理研究和模型改进的可靠资源。"}}
{"id": "2511.14970", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.14970", "abs": "https://arxiv.org/abs/2511.14970", "authors": ["Gbenga Omotara", "Ramy Farag", "Seyed Mohamad Ali Tousi", "G. N. DeSouza"], "title": "EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects", "comment": null, "summary": "Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.", "AI": {"tldr": "本文提出了EGSA方法，通过加入边界信息来减少不同任务间的相互干扰，提高了透明物体深度估计的准确性。", "motivation": "现有的多任务学习框架由于任务间的负面影响导致性能降低，为此本研究提出新的方法来提升透明物体理解的性能。", "method": "本文提出了边缘引导空间注意力机制(EGSA)，用于在语义和几何特征融合时降低不同任务之间的负面影响。", "result": "EGSA在Syn-TODD和ClearPose基准上提高了深度估计的准确性，特别是在透明区域的性能显著提高，同时保持了语义分割的性能。", "conclusion": "边缘引导融合是提高透明物体识别的一种稳健方法。"}}
{"id": "2511.15370", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15370", "abs": "https://arxiv.org/abs/2511.15370", "authors": ["Guoqiang Liang", "Jingqian Gong", "Mengxuan Li", "Gege Lin", "Shuo Zhang"], "title": "The Empowerment of Science of Science by Large Language Models: New Tools and Methods", "comment": "The manuscript is currently ongoing the underreview process of the journal of information science", "summary": "Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.", "AI": {"tldr": "本文综述了大语言模型（LLMs）背后的核心技术，并探讨了LLMs在科学计量领域的应用前景。", "motivation": "鉴于LLMs在多个领域的出色表现及其在全球技术竞赛中的重要性，本文旨在从用户视角综述支持LLMs的相关技术，并探讨其在未来科学评价、研究前沿检测和知识图谱构建等方面的潜力。", "method": "本论文从用户角度出发，综述了大语言模型（LLMs）背后的核心技术，包括prompt工程、知识增强检索生成、微调、预训练和工具学习。", "result": "本文综述了LLMs的核心技术，并指出了AI代理模型在科学评价中的潜力，以及LLMs在科研前沿检测和知识图谱构建方面的新研究方法。", "conclusion": "综上所述，LLMs已经在多个领域展现了强大的能力，并将成为通往AGI的关键。本文不仅综述了其核心技术，还为LLMs在未来科学计量领域中的应用提供了一些有前景的视角和方向。"}}
{"id": "2511.14981", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14981", "abs": "https://arxiv.org/abs/2511.14981", "authors": ["Nicholas Cooper", "Lijun Chen", "Sailesh Dwivedy", "Danna Gurari"], "title": "Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation", "comment": "NeurIPS Workshop on Symmetry and Geometry in Neural Representations (NeurReps), December 2025", "summary": "Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.", "AI": {"tldr": "A new framework for knowledge distillation is proposed which uses feature-based losses exclusively. It achieves state-of-the-art performance, showing significant improvements over traditional methods with up to 15% increase in top-1 accuracy across various models and datasets.", "motivation": "The motivation is to improve the effectiveness of knowledge distillation in transferring knowledge from a large teacher model to a smaller student model, aiming to enhance the performance of the student model without the need for computationally expensive logit-based losses.", "method": "The paper introduces a knowledge distillation (KD) framework that exclusively uses feature-based losses for training a student model's backbone, differing from the traditional approach that involves logit-based losses. This method also includes a knowledge quality metric for identifying effective teacher layers based on recent findings about the geometry of latent representations.", "result": "Experiments conducted on three image classification datasets with various student-teacher model pairs, which include both convolutional neural networks and vision transformers, showed that this KD method can significantly boost the top-1 accuracy of the student model by up to 15% compared to conventional methods.", "conclusion": "The proposed KD framework which solely employs feature-based losses for distillation demonstrates state-of-the-art performance, indicating its efficacy in enhancing the student model's performance in image classification tasks."}}
{"id": "2511.15383", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.15383", "abs": "https://arxiv.org/abs/2511.15383", "authors": ["Byungho Jo"], "title": "A Compliance-Preserving Retrieval System for Aircraft MRO Task Search", "comment": null, "summary": "Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.", "AI": {"tldr": "研究建立了一个能够提高MRO工作效率，同时保持合规性的技术检索系统，该系统被证实能大幅减少查找手册的时间，并保持高水平的检索准确性。", "motivation": "目前，航空维修技术人员在查找手册上花费了高达30%的工作时间，这在MRO操作中是一个公认的效率瓶颈。本研究旨在通过提供一个可以与认证的遗留查看器配合使用的检索系统来解决这一问题，同时确保合规性。", "method": "本研究开发了一个合规的检索系统，该系统利用LLM重排序和语义搜索技术适应航空MRO环境。系统通过ATA章节层级构建修订鲁棒嵌入，并使用视觉语言解析来结构化认证内容，使技术人员能够预览排序的任务，并在现有的查看器中访问验证程序。", "result": "在49,000个合成查询上的评估显示，检索准确率超过90%，而使用10名持证AMT进行的双语控制研究显示，前十个结果的成功率为90.9%，查找时间减少了95%，从每任务6-15分钟减少到18秒。", "conclusion": "这些结果提供了有力的证据，证明语义检索可以在严格的监管约束下操作，并在实际的多语言MRO流程中显著减少运营工作量。"}}
{"id": "2511.14993", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14993", "abs": "https://arxiv.org/abs/2511.14993", "authors": ["Vladimir Arkhipkin", "Vladimir Korviakov", "Nikolai Gerasimenko", "Denis Parkhomenko", "Viacheslav Vasilev", "Alexey Letunovskiy", "Maria Kovaleva", "Nikolai Vaulin", "Ivan Kirillov", "Lev Novitskiy", "Denis Koposov", "Nikita Kiselev", "Alexander Varlamov", "Dmitrii Mikhailov", "Vladimir Polovnikov", "Andrey Shutkin", "Ilya Vasiliev", "Julia Agafonova", "Anastasiia Kargapoltseva", "Anna Dmitrienko", "Anastasia Maltseva", "Anna Averchenkova", "Olga Kim", "Tatiana Nikulina", "Denis Dimitrov"], "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "comment": "Website: https://kandinskylab.ai/", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "AI": {"tldr": "Kandinsky 5.0是一个用于高分辨率图像和10秒视频合成的最先进基础模型家族，涵盖各种优化技术，性能优越。", "motivation": "介绍Kandinsky 5.0这个最先进基础模型家族，适用于高分辨率图像和10秒视频的生成。", "method": "Kandinsky 5.0 作为一个家族的模型，包含图像生成和视频生成，其架构针对高分辨率图像和10秒视频合成进行了优化。模型分成三个系列：Kandinsky 5.0 Image Lite（60亿参数的图像生成模型）、Kandinsky 5.0 Video Lite（快速且轻量级的20亿参数文本到视频和图像到视频模型）和Kandinsky 5.0 Video Pro（190亿参数的视频生成质量较高的模型）。", "result": "通过人类评估，展示了Kandinsky 5.0在各种任务中的高速生成和顶级性能。", "conclusion": "作为大规模且公开可用的生成框架，Kandinsky 5.0能够适应各种生成应用，期望该报告及其开源代码和训练检查点的发布将大大推进高质量生成模型的发展和获取。"}}
{"id": "2511.15392", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15392", "abs": "https://arxiv.org/abs/2511.15392", "authors": ["Sirui Chen", "Mengshi Zhao", "Lei Xu", "Yuying Zhao", "Beier Zhu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents", "comment": "Accepted to AAAI 2026", "summary": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.", "AI": {"tldr": "提出DEPO方法优化LLM代理效率，通过奖励简洁响应和减少行动步骤实现双重效率提升。", "motivation": "推动LLM代理效率的系统定义及针对性改进，解决现实场景中交互效率低下的问题。", "method": "DEPO方法，包括简洁响应奖励和减少行动步骤奖励的双重效率偏好优化。", "result": "实验表明，在WebShop和BabyAI上，DEPO最多减少60.9%的令牌使用量，减少26.9%的步骤，并提高性能29.3%。", "conclusion": "DEPO方法不仅能提高效率还能在跨领域的数学基准测试中保持性能，并在小数据量下也有效。"}}
{"id": "2511.14998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.14998", "abs": "https://arxiv.org/abs/2511.14998", "authors": ["Yueru He", "Xueqing Peng", "Yupeng Cao", "Yan Wang", "Lingfei Qian", "Haohang Li", "Yi Han", "Ruoyu Xiang", "Mingquan Lin", "Prayag Tiwari", "Jimin Huang", "Guojun Xiong", "Sophia Ananiadou"], "title": "FinCriticalED: A Visual Benchmark for Financial Fact-Level OCR Evaluation", "comment": "Yueru He, Xueqing Peng: These two authors contributed equally to this work", "summary": "We introduce FinCriticalED (Financial Critical Error Detection), a visual benchmark for evaluating OCR and vision language models on financial documents at the fact level. Financial documents contain visually dense and table heavy layouts where numerical and temporal information is tightly coupled with structure. In high stakes settings, small OCR mistakes such as sign inversion or shifted dates can lead to materially different interpretations, while traditional OCR metrics like ROUGE and edit distance capture only surface level text similarity. \\ficriticaled provides 500 image-HTML pairs with expert annotated financial facts covering over seven hundred numerical and temporal facts. It introduces three key contributions. First, it establishes the first fact level evaluation benchmark for financial document understanding, shifting evaluation from lexical overlap to domain critical factual correctness. Second, all annotations are created and verified by financial experts with strict quality control over signs, magnitudes, and temporal expressions. Third, we develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for visually complex financial documents. We benchmark OCR systems, open source vision language models, and proprietary models on FinCriticalED. Results show that although the strongest proprietary models achieve the highest factual accuracy, substantial errors remain in visually intricate numerical and temporal contexts. Through quantitative evaluation and expert case studies, FinCriticalED provides a rigorous foundation for advancing visual factual precision in financial and other precision critical domains.", "AI": {"tldr": "FinCriticalED是一个用于评价OCR和视觉语言模型在金融文档中的事实精度的基准，它由500个图像-HTML对组成，涵盖大量数字和时间事实，专家进行了注释和验证，显示出在复杂视觉语境下仍存在大量潜在错误。", "motivation": "传统的OCR指标如ROUGE和编辑距离仅捕捉表面文本的相似性，而忽视了文本的实质错误。因此，需要一个基于领域关键事实正确性的评估基准，而非仅基于文本覆盖度。", "method": "引入了FinCriticalED，这是一个用于在事实层面评估OCR和视觉语言模型在金融文档上的视觉基准。金融文档含有密集的视觉布局和大量的表格，其中数字和时间信息与结构紧密相关。在高风险环境中，即使是小的OCR错误，例如符号反转或日期偏移，也可能会导致完全不同的解释。", "result": "通过质量控制严格的金融专家的注释和验证，FinCriticalED提供了500个图像-HTML对，覆盖了超过七百个数字和时间事实。实验表明，尽管最强的专有模型实现了最高的事实准确性，但在视觉复杂的数字和时间语境下仍然存在大量错误。", "conclusion": "FinCriticalED为提高视觉事实精确性提供了一个严格的基准，特别是在金融和其他要求高度精确的领域。通过定量评估和专家案例研究，它为相关模型的进一步发展奠定了基础。"}}
{"id": "2511.15408", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.15408", "abs": "https://arxiv.org/abs/2511.15408", "authors": ["Shanlin Zhou", "Xinpeng Wang", "Jianxun Lian", "Zhenghao Liu", "Laks V. S. Lakshmanan", "Xiaoyuan Yi", "Yongtao Hao"], "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework", "comment": "13 pages,9 figures. This work has been submitted to the IEEE for possible publication", "summary": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.", "AI": {"tldr": "The paper proposes NAMeGEn, a multi-agent optimization framework, to improve Chinese baby naming by meeting diverse, personalized requirements and providing meaningful explanations, outperforming six baseline methods without training.", "motivation": "Addressing the challenges of multi-objective flexibility and interpretive complexity in Creative Natural Language Generation (CNLG), especially in the context of short-form text generation.", "method": "NAMeGEn, a novel multi-agent optimization framework, is proposed for the Chinese baby naming task, which iteratively alternates between objective extraction, name generation, and evaluation. A classical Chinese poetry corpus and a new benchmark named CBNames are introduced to support this research.", "result": "Experiments show that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming other methods without requiring any additional training.", "conclusion": "The research demonstrates that using a multi-agent optimization framework like NAMeGEn can enhance the effectiveness of short-form Creative Natural Language Generation tasks such as Chinese baby naming, leading to improved content creation that better serves personalized user needs."}}
{"id": "2511.15016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15016", "abs": "https://arxiv.org/abs/2511.15016", "authors": ["Zhenyu Cui", "Jiahuan Zhou", "Yuxin Peng"], "title": "CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification", "comment": null, "summary": "Lifelong person Re-IDentification (LReID) aims to match the same person employing continuously collected individual data from different scenarios. To achieve continuous all-day person matching across day and night, Visible-Infrared Lifelong person Re-IDentification (VI-LReID) focuses on sequential training on data from visible and infrared modalities and pursues average performance over all data. To this end, existing methods typically exploit cross-modal knowledge distillation to alleviate the catastrophic forgetting of old knowledge. However, these methods ignore the mutual interference of modality-specific knowledge acquisition and modality-common knowledge anti-forgetting, where conflicting knowledge leads to collaborative forgetting. To address the above problems, this paper proposes a Cross-modality Knowledge Disentanglement and Alignment method, called CKDA, which explicitly separates and preserves modality-specific knowledge and modality-common knowledge in a balanced way. Specifically, a Modality-Common Prompting (MCP) module and a Modality-Specific Prompting (MSP) module are proposed to explicitly disentangle and purify discriminative information that coexists and is specific to different modalities, avoiding the mutual interference between both knowledge. In addition, a Cross-modal Knowledge Alignment (CKA) module is designed to further align the disentangled new knowledge with the old one in two mutually independent inter- and intra-modality feature spaces based on dual-modality prototypes in a balanced manner. Extensive experiments on four benchmark datasets verify the effectiveness and superiority of our CKDA against state-of-the-art methods. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/CKDA-AAAI2026.", "AI": {"tldr": "本文提出了Cross-modality Knowledge Disentanglement and Alignment (CKDA)方法解决Vi-LReID中模态特定知识获取与模态共性知识抗遗忘之间的相互干扰问题，提高了可见光-红外终身行人重识别的性能。", "motivation": "现有的VI-LReID方法通常采用跨模态知识蒸馏技术以减轻对先前知识的灾难性遗忘，但忽略了模态特定知识获取与模态共性知识抗遗忘之间的相互干扰问题。为解决上述问题，本文提出了交叉模态知识解缠与对齐方法。", "method": "CKDA方法通过设计Modality-Common Prompting (MCP)模块与Modality-Specific Prompting (MSP)模块来明确区分和净化共性和特定模态中共同存在的鉴别性信息，避免两者知识互相干扰。同时，设计了Cross-modal Knowledge Alignment (CKA)模块来进一步在两个独立的互模态和同模态特征空间中以平衡方式对解耦的新知识与旧知识进行对齐。", "result": "在四个基准数据集上的大量实验验证了所提出的CKDA方法的有效性和优越性。", "conclusion": "研究表明，所提出的CKDA方法有效地解决了模态特定知识获取与模态共性知识抗遗忘之间的相互干扰问题，提高了可见光-红外终身行人重识别性能。"}}
{"id": "2511.15418", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15418", "abs": "https://arxiv.org/abs/2511.15418", "authors": ["Arjun Gangwar", "Kaousheik Jayakumar", "S. Umesh"], "title": "Building Robust and Scalable Multilingual ASR for Indian Languages", "comment": null, "summary": "This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).", "AI": {"tldr": "本文描述了由印度理工学院马德拉斯分校的SPRING实验室开发的系统，这些系统参与了ASRU MADASR 2.0挑战赛。通过使用多解码器架构与音位公共标签集作为中间表示，系统在预测8种语言的33种方言时表现出色，并在3种语言的WER/CER和语言/方言识别准确性上超越了基线系统。", "motivation": "开发能够适应并改进多种语言和方言识别的ASR系统的动机在于解决多语言环境下的语音识别挑战，尤其是在没有额外数据的限制下如何构建从零开始的多语种系统。", "method": "采用了多解码器架构结合音位共同标签集作为中间表示的新训练方法。该方法旨在提高系统的性能，并探讨了如何在转化回对应音标时保持获得的增益。", "result": "该方法提高了基线性能，系统在Tracks 1和2中表现出色，超过了3种语言的基线，测得的WER/CER较低，并且在所有参赛队伍中实现了最高的语言和方言识别准确性。", "conclusion": "通过提出的新训练方法，系统在多语言语音识别任务上得到了改进，尤其是在有限的数据环境下，展现了良好的语言和方言识别能力。"}}
{"id": "2511.15022", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15022", "abs": "https://arxiv.org/abs/2511.15022", "authors": ["Yicheng Zhan", "Xiangjun Gao", "Long Quan", "Kaan Akşit"], "title": "Complex-Valued 2D Gaussian Representation for Computer-Generated Holography", "comment": "8 pages, 11 figures", "summary": "We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.", "AI": {"tldr": "A new hologram representation using 2D Gaussian primitives with a differentiable rasterizer achieves up to 2.5x lower VRAM usage, 50% faster optimization, and higher-fidelity reconstructions compared to existing methods.", "motivation": "To replace per-pixel information storage and reduce the parameter search space in hologram representation, improving scalability and performance in computer-generated holography systems.", "method": "We propose a new hologram representation using structured complex-valued 2D Gaussian primitives which reduces the parameter search space by up to 10:1 and develop a differentiable rasterizer integrated with a GPU-optimized light propagation kernel in free space.", "result": "The method achieves up to 2.5x lower VRAM usage, 50% faster optimization and produces higher-fidelity reconstructions than existing methods. Additionally, a conversion procedure that adapts the representation to practical hologram formats while effectively suppressing noise artifacts is introduced.", "conclusion": "Our new hologram representation enables more scalable hologram estimation in next-generation computer-generated holography systems, with reduced VRAM usage and faster optimization while maintaining high fidelity."}}
{"id": "2511.15424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15424", "abs": "https://arxiv.org/abs/2511.15424", "authors": ["Yuanjie Zhu", "Liangwei Yang", "Ke Xu", "Weizhi Zhang", "Zihe Song", "Jindong Wang", "Philip S. Yu"], "title": "LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering", "comment": null, "summary": "Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.", "AI": {"tldr": "本文提出了LLM-MemCluster，一种用于基于大型语言模型的文本聚类的新框架，实现有效、可解释和端到端的聚类方法。", "motivation": "现有的文本聚类方法受限于大型语言模型缺乏状态记忆和聚类粒度控制的难题，常常依赖复杂的管道和外部模块，无法实现端到端处理。", "method": "LLM-MemCluster框架，利用动态内存和双提示策略来进行文本聚类，使得大型语言模型具备状态意识并能够确定聚类数量。", "result": "在多个基准数据集上，该框架显著且一致地优于强基线。", "conclusion": "LLM-MemCluster提供了一种有效的、可解释的、真正端到端的基于大型语言模型的文本聚类范式。"}}
{"id": "2511.15029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15029", "abs": "https://arxiv.org/abs/2511.15029", "authors": ["Zekun Wang", "Sashank Varma"], "title": "Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans", "comment": "11 pages, 7 figures", "summary": "Mathematical thinking is a fundamental aspect of human cognition. Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan. Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults. Building on this demonstrated cognitive alignment, the current study investigates whether CV models also show developmental alignment: whether their performance improvements across training to match the developmental progressions observed in children. In a detailed case study of the ResNet-50 model, we show that this is the case. For the case of geometry and topology, we find developmental alignment for some classes of concepts (Euclidean Geometry, Geometrical Figures, Metric Properties, Topology) but not others (Chiral Figures, Geometric Transformations, Symmetrical Figures). For the case of number, we find developmental alignment in the emergence of a human-like ``mental number line'' representation with experience. These findings show the promise of computer vision models for understanding the development of mathematical understanding in humans. They point the way to future research exploring additional model architectures and building larger benchmarks.", "AI": {"tldr": "研究发现，训练用于图像分类任务的计算机视觉模型，在几何、拓扑和数字概念的理解上，展示了与儿童认知发展进程相匹配的学习模式和性能提升。", "motivation": "先前的研究表明，即使是为了图像分类这一无关任务训练的计算机视觉模型，也会无意识地学习到与成人相似的几何和数值概念的潜在表示。本研究旨在检测这些模型是否在这一过程中展现了与儿童认知发展相匹配的性能提升。", "method": "通过训练计算机视觉模型来进行图像分类任务，研究其是否会在这一过程中无意识地学习到与成人相似的几何和数值概念的潜在表示。本研究以ResNet-50模型为例，调查了计算机视觉模型在训练过程中性能的提升是否与儿童认知发展的进程相匹配。", "result": "研究表明，这种匹配存在于某些几何和拓扑的概念类别（如欧几里得几何、几何图形、度量性质、拓扑）中，但并不存在于其他类别中（如手征图形、几何变换、对称图形）。对于数值概念，研究在经验积累过程中发现模型发展出了一种类似人类的“心理数字线”表示。", "conclusion": "结果表明，计算机视觉模型有潜力帮助我们理解人类数学理解的发展过程，并为未来更多研究方向提供了灵感，比如探索更多样的模型架构和建立更大的基准数据集。"}}
{"id": "2511.15512", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15512", "abs": "https://arxiv.org/abs/2511.15512", "authors": ["Yves Pauli", "Jan-Bernard Marsman", "Finn Rabe", "Victoria Edkins", "Roya Hüppi", "Silvia Ciampelli", "Akhil Ratan Misra", "Nils Lang", "Wolfram Hinzen", "Iris Sommer", "Philipp Homan"], "title": "Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis", "comment": "26 pages, 3 figures", "summary": "The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.", "AI": {"tldr": "This paper introduces LPDS for linguistic data structure and pelican nlp for reproducible language processing, aiming to standardise and enhance methodological transparency in linguistic research.", "motivation": "The paper aims to tackle the challenges in standardising the organisation, sharing, and processing methodologies for linguistic data in the era of advanced AI-based language technologies.", "method": "We propose the Language Processing Data Structure (LPDS) and introduce pelican nlp, a modular and extensible Python package for language processing.", "result": "LPDS and pelican nlp together provide a standardised, reproducible, and transparent end-to-end pipeline for processing linguistic data, addressing the lack of standardisation in the field.", "conclusion": "The proposed solutions, LPDS and pelican nlp, offer a promising direction towards achieving standardisation and reproducibility in linguistic data processing."}}
{"id": "2511.15046", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15046", "abs": "https://arxiv.org/abs/2511.15046", "authors": ["Panqi Yang", "Haodong Jing", "Nanning Zheng", "Yongqiang Ma"], "title": "UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space", "comment": "Accepted by AAAI 2026,9 pages, 4 figures", "summary": "In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.", "AI": {"tldr": "研究提出了UniHOI框架，联合建模HOI检测和生成任务，采用统一标记空间和半监学习方法，显著提升了检测和生成性能。", "motivation": "为了克服人类对象交互领域中检测和生成任务的分离，促进互补知识共享和改善泛化能力。", "method": "我们提出了UniHOI，通过统一的标记空间联合建模HOI检测和生成。为此，我们引入了对称交互感知注意力模块和统一的半监督学习范式，即使在注释有限的情况下，也能够实现图像和交互语义之间的有效双向映射。", "result": "实验表明，UniHOI在HOI检测和生成任务中达到了最先进的性能。特别是在长尾HOI检测中提升准确度4.9%，在开放词汇生成任务中提升交互指标42.0%。", "conclusion": "UniHOI通过构建联合模型改善了人类物体交互领域中的知识共享和增强泛化能力，展示了显著的性能提升。"}}
{"id": "2511.15552", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15552", "abs": "https://arxiv.org/abs/2511.15552", "authors": ["Artem Chervyakov", "Ulyana Isaeva", "Anton Emelyanov", "Artem Safin", "Maria Tikhonova", "Alexander Kharitonov", "Yulia Lyakh", "Petr Surovtsev", "Denis Shevelev Vildan Saburov", "Vasily Konovalov", "Elisei Rykov", "Ivan Sviridov", "Amina Miftakhova", "Ilseyar Alimova", "Alexander Panchenko", "Alexander Kapitanov", "Alena Fenogenova"], "title": "Multimodal Evaluation of Russian-language Architectures", "comment": null, "summary": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.", "AI": {"tldr": "本文介绍了一个针对俄语多模态架构的开放性评估框架Mera Multi，包含18个新构建的评估任务，适用于通用模型及模态特定架构。", "motivation": "鉴于目前缺乏针对俄语的多模态基准测试，此研究旨在填补这一空白，并探索这些模型的智能、限制与风险。", "method": "引入了一个名为Mera Multi的开放多模态评估框架，用于评估俄语环境下的多模态大语言模型，包含文本、图像、音频和视频四种模态，涉及18个新构建的评估任务。", "result": "贡献包括一套多模态能力的通用分类法、18个从头构建的数据集、闭源和开源模型的基线结果以及防止基准测试泄漏的方法。", "conclusion": "尽管目前专注于俄语，但提出的基准测试框架为在类型学多样化的语言中构建多模态基准测试提供了一个可复制的方法，特别是斯拉夫语言家族。"}}
{"id": "2511.15052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15052", "abs": "https://arxiv.org/abs/2511.15052", "authors": ["Yue Wen", "Kunjing Yang", "Minru Bai"], "title": "Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method", "comment": null, "summary": "The fusion of hyperspectral image (HSI) with multispectral image (MSI) provides an effective way to enhance the spatial resolution of HSI. However, due to different acquisition conditions, there may exist spectral variability and spatially localized changes between HSI and MSI, referred to as inter-image variability, which can significantly affect the fusion performance. Existing methods typically handle inter-image variability by applying direct transformations to the images themselves, which can exacerbate the ill-posedness of the fusion model. To address this challenge, we propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. First, we model the spectral variability as change in the spectral degradation operator. Second, to recover the lost spatial details caused by spatially localized changes, we decompose the target HSI into low rank and residual components, where the latter is used to capture the lost details. By exploiting the spectral correlation within the images, we perform dimensionality reduction on both components. Additionally, we introduce an implicit regularizer to utilize the spatial prior information from the images. The proposed DLRRF model is solved using the Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where the subproblem regarding implicit regularizer is addressed by an external denoiser. We further provide a comprehensive convergence analysis of the algorithm. Finally, extensive numerical experiments demonstrate that DLRRF achieves superior performance in fusing HSI and MSI with inter-image variability.", "AI": {"tldr": "本文提出了一种新的高光谱和多光谱图像融合方法——基于退化低秩和残差融合（DLRRF）模型，该模型能够有效缓解光谱变异性及恢复空间细节。实验表明，DLRRF模型在此类图像融合任务中表现优异。", "motivation": "现有的图像融合方法通常通过直接转换图像来处理图像间变异性，这会加剧融合模型的病态性。为了应对这一挑战，提出了可以从光谱退化和空间细节恢复来改善融合效果的方法。", "method": "提出一种基于退化低秩和残差融合（DLRRF）模型的方法。首先，将光谱变异性建模为光谱退化算子的变化；其次，为恢复由空间局部变化引起的丢失空间细节，将目标高光谱图像分解成低秩和残差组件，其中后者用于捕捉丢失的细节。通过利用图像内的光谱相关性，对这两个组件进行降维。此外，引入了一个隐式正则化器来利用图像的空间先验信息。DLRRF模型使用插件和播放（PnP）框架中的近端交替优化（PAO）算法求解。", "result": "通过广泛的数值实验表明，DLRRF在融合具有图像间变异性的高光谱和多光谱图像时，表现出色。", "conclusion": "研究结果验证了DLRRF模型在解决光谱变异性问题并恢复空间细节方面的有效性。"}}
{"id": "2511.15574", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15574", "abs": "https://arxiv.org/abs/2511.15574", "authors": ["Qihao Yang", "Xuelin Wang", "Jiale Chen", "Xuelian Dong", "Yuxin Hao", "Tianyong Hao"], "title": "HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning", "comment": "Accepted by AAAI-2026", "summary": "Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.", "AI": {"tldr": "本文提出了HSKBenchmark，这是第一个针对中文第二语言习得（SLA）的基准，旨在支持LLMs在中文SLA中的分阶段建模和写作评估，不仅能够有效地模拟中文语言习得过程，还为动态写作评估提供了可靠的参考。", "motivation": "语言习得对于揭示人类语言智能的内在性质和提高大型语言模型的可解释性至关重要。然而，控制人类学习者语言输入的实验是不道德且难以实施的，这对语言习得建模的验证性和可扩展性构成了挑战。特别是中文第二语言习得（SLA）。虽然LLMs提供了一种可控和可重复的替代方案，但仍然缺乏系统化的基准来支持阶段性建模和评估。", "method": "我们提出了HSKBenchmark，这是第一个针对中文第二语言习得(LLMs)的分阶段建模和写作评估基准。该基准涵盖HSK水平3到6，包括真实的教科书，有676万个标记、16,000个合成指令样本、30个测试主题和一个基于语言学的评估系统。为了模拟人类学习路径，我们引入了一个从初级到高级水平训练模型的课程调优框架。我们还建立了HSKAgent，它是在10,000个学习者作文上进行微调的模型。", "result": "实验结果表明，HSKBenchmark不仅能够有效地模拟中文第二语言习得，还可以作为动态写作评估LLMs的可靠基准。微调后的LLMs在写作性能上与高级人类学习者相当，并表现出类似人类的习得特征。", "conclusion": "HSKBenchmark、HSKAgent和检查点作为基础工具和资源，有潜力为未来在语言习得建模和LLMs可解释性研究铺平道路。代码和数据都是公开可用的。"}}
{"id": "2511.15054", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15054", "abs": "https://arxiv.org/abs/2511.15054", "authors": ["Srijan Ray", "Bikesh K. Nirala", "Jason T. Yustein", "Sundaresh Ram"], "title": "CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues", "comment": "4 pages, 3 figures, Submitted to IEEE SSIAI 2026", "summary": "Accurate nuclei segmentation in microscopy whole slide images (WSIs) remains challenging due to variability in staining, imaging conditions, and tissue morphology. We propose CellGenNet, a knowledge distillation framework for robust cross-tissue cell segmentation under limited supervision. CellGenNet adopts a student-teacher architecture, where a capacity teacher is trained on sparse annotations and generates soft pseudo-labels for unlabeled regions. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function combining binary cross-entropy and Tversky loss, enabling asymmetric penalties to mitigate class imbalance and better preserve minority nuclear structures. Consistency regularization and layerwise dropout further stabilize feature representations and promote reliable feature transfer. Experiments across diverse cancer tissue WSIs show that CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines, supporting scalable and reproducible histopathology analysis.", "AI": {"tldr": "提出CellGenNet，一种使用学生-教师架构的知识蒸馏框架，用于在有限监督条件下实现显微镜全切片成像中的鲁棒跨组织细胞分割。", "motivation": "由于染色、成像条件和组织形态的可变性，显微镜全切片成像中的准确核分割仍然具有挑战性。", "method": "CellGenNet采用学生-教师架构，教师模型在稀疏注释上训练，并为未标记区域生成软伪标签。学生模型使用结合了二元交叉熵和Tversky损失的混合损失函数的联合目标进行优化，一致性正则化和层级dropout进一步稳定特征表示。", "result": "实验结果表明，与监督和半监督基线相比，CellGenNet提高分割精度和泛化性能，支持可扩展和可重复的组织病理学分析。", "conclusion": "CellGenNet通过知识蒸馏框架，在有限的监督下实现了跨组织细胞分割的鲁棒性，适用于大规模和可重复的组织病理学分析。"}}
{"id": "2511.15709", "categories": ["cs.CL", "cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15709", "abs": "https://arxiv.org/abs/2511.15709", "authors": ["Violeta Kastreva", "Philip Whittington", "Dennis Komm", "Tiago Pimentel"], "title": "Tokenisation over Bounded Alphabets is Hard", "comment": null, "summary": "Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.", "AI": {"tldr": "本文通过研究证明，分词在任何有界字母表上，包括二进制和一元字母表上都是NP完全问题且不存在多项式时间近似方案，指出开发近似算法对解决此问题至关重要。", "motivation": "近期的研究表明分词是NP完全问题，但这些研究假设分词应用在输入具有无限大字母表的基础上，这在实践中并不现实。因此，本文尝试填补这一理论与实践间的空白，分析有界字母表上的分词问题。", "method": "本文通过分析有界n元字母表上的分词过程，特别是两种自然变体：自底向上分词和直接分词。第一种变体涉及选择一系列合并操作，而第二种变体则是选择一个词汇库，通过应用它可以最优化地压缩数据集。", "result": "研究证明，即使是对二进制字母表来说，这两种分词方法不仅是NP完全问题，而且不存在多项式时间近似方案（除非P=NP）。此外，直接分词在应用于一元字母表时也会变为NP完全问题。", "conclusion": "本文结果解释了为何在实践中使用的算法如BPE和UnigramLM只能是启发式的，并指出开发近似算法是未来分词研究的重要方向。"}}
{"id": "2511.15057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15057", "abs": "https://arxiv.org/abs/2511.15057", "authors": ["Yaxiong Chen", "Qicong Wang", "Chunlei Li", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling", "comment": "AAAI 2026", "summary": "Existing approaches for the problem of ultrasound image segmentation, whether supervised or semi-supervised, are typically specialized for specific anatomical structures or tasks, limiting their practical utility in clinical settings. In this paper, we pioneer the task of universal semi-supervised ultrasound image segmentation and propose ProPL, a framework that can handle multiple organs and segmentation tasks while leveraging both labeled and unlabeled data. At its core, ProPL employs a shared vision encoder coupled with prompt-guided dual decoders, enabling flexible task adaptation through a prompting-upon-decoding mechanism and reliable self-training via an uncertainty-driven pseudo-label calibration (UPLC) module. To facilitate research in this direction, we introduce a comprehensive ultrasound dataset spanning 5 organs and 8 segmentation tasks. Extensive experiments demonstrate that ProPL outperforms state-of-the-art methods across various metrics, establishing a new benchmark for universal ultrasound image segmentation.", "AI": {"tldr": "本文提出ProPL框架，解决通用半监督超声图像分割问题，适用于多个器官和分割任务，超越现有方法，创建了新的基准。", "motivation": "现有的超声图像分割方法无论是有监督还是半监督，大多只能针对特定的解剖结构或任务，限制了它们在临床环境中的实用价值。本文旨在探索通用的半监督超声图像分割任务。", "method": "ProPL采用共享视觉编码器和prompt引导的双解码器，通过在解码时进行prompt自适应以及基于不确定性的伪标签校准模块（UPLC）实现可靠自训练。", "result": "实验结果表明，ProPL在各种评估指标上均超越了最先进的方法。", "conclusion": "ProPL通过引入共享视觉编码器、prompt引导双解码器及UPLC模块，证明了其在通用超声图像分割任务上的优势。"}}
{"id": "2511.15059", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15059", "abs": "https://arxiv.org/abs/2511.15059", "authors": ["Keito Sasagawa", "Shuhei Kurita", "Daisuke Kawahara"], "title": "Evaluating Multimodal Large Language Models on Vertically Written Japanese Text", "comment": "17pages, 8 figures", "summary": "Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.", "AI": {"tldr": "研究评估了现有MLLMs对垂直书写日文的理解能力，并展示了通过特殊数据集训练可以改善这种识别能力。", "motivation": "鉴于现有研究中对垂直书写日文的关注有限，本研究旨在评估现有MLLMs在理解垂直书写日文文本方面的表现。", "method": "通过生成一个合成的日语OCR数据集来评估现有MLLMs对垂直书写日文的识别能力。该数据集包含水平和垂直书写方式的日文文本。还创建了一个来自现实世界文档图像的评估数据集，其中包含了垂直书写的日文文本。", "result": "实验证明，现有的MLLMs在处理垂直书写的日文文本方面的表现不如处理水平书写的日文文本。同时展示了在合成的日语OCR数据集上对MLLMs进行训练可以改进模型处理垂直书写的能力。", "conclusion": "通过研究可以发现，要提高MLLMs对垂直书写日文文本的理解能力，需要特别针对这种书写格式进行数据集的构建与模型训练。"}}
{"id": "2511.15065", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15065", "abs": "https://arxiv.org/abs/2511.15065", "authors": ["Cheng Yang", "Haiyuan Wan", "Yiran Peng", "Xin Cheng", "Zhaoyang Yu", "Jiayi Zhang", "Junchi Yu", "Xinlei Yu", "Xiawu Zheng", "Dongzhan Zhou", "Chenglin Wu"], "title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks", "comment": null, "summary": "Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.", "AI": {"tldr": "此研究引入了VR-Bench，一个专注于评估视频模型在解决迷宫任务时的空间推理能力的综合基准。实验表明，视频模型在空间感知和推理方面表现突出，并且通过多样的采样方式可以在推理过程中提高10-20%的可靠性。", "motivation": "随着视频模型在生成高保真视频方面取得了显著进展，研究者开始探索这些模型的推理能力，特别是在基于视频的空间推理方面。研究者提出了通过视频生成的方式进行推理这一方向。", "method": "研究者设计并使用了VR-Bench，这是一个包含7,920个程序生成视频的基准测试，所使用的迷宫任务需要多步骤和空间规划的能力。同时，研究者探索了SFT（Scaled Fine-Tuning）技术来激发视频模型的推理能力，并分析了不同采样方式下推理的可靠性和一致性。", "result": "视频模型在空间感知和推理任务中表现优异，超过了现有的视觉语言模型，并且能够很好地泛化到不同的场景、任务和复杂度。更重要的是，通过多样的采样方式，推理的可靠性可以提高10-20%。", "conclusion": "研究表明，通过视频生成的方式进行推理具有独特潜力，并且可以通过不同的采样技术提升推理能力的可靠性。这为视频模型在空间推理任务中的应用提供了新的见解。"}}
{"id": "2511.15159", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15159", "abs": "https://arxiv.org/abs/2511.15159", "authors": ["Firdavs Nasriddinov", "Rafal Kocielnik", "Anima Anandkumar", "Andrew J. Hung"], "title": "Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation", "comment": "Accepted as proceedings paper for ML4H 2025", "summary": "High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.", "AI": {"tldr": "研究引入了一种结构感知的流程，以生成基于临床相关表示的训练者风格反馈，显著提升了视频到IAT识别和反馈文本生成的准确性和保真度。", "motivation": "旨在通过自动化自然的训练者风格反馈，提高手术培训效果，提供一种及时、易获取且一致的指导方式，这需要能理解临床相关表示的模型。", "method": "通过挖掘真实世界反馈文本中的操作-目标-工具三元组，结合对手术过程和任务的上下文理解以及精细时间工具运动的利用，优化了一个视频到IAT模型，并使用该模型指导GPT-4o生成符合临床标准且类似于训练者的反馈。", "result": "经过上下文嵌入和时间跟踪，IAT识别任务的AUC数值有所提升。对于任务2：反馈文本生成评分（在1-5评分标准上），GPT-4o仅从视频得到的评分为2.17，而通过IAT配置可达到2.44（提升12.4%），使得得分在3以上的生成比例翻倍，从21%提升至42%。传统的文本相似性度量也得到改善。", "conclusion": "将生成过程锚定在明确的IAT结构上可以提高生成反馈的保真度，并提供可以进行临床验证的理由，从而支持在手术培训中的审计应用。"}}
{"id": "2511.15066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15066", "abs": "https://arxiv.org/abs/2511.15066", "authors": ["Yachuan Huang", "Xianrui Luo", "Qiwen Wang", "Liao Shen", "Jiaqi Li", "Huiqiang Sun", "Zihao Huang", "Wei Jiang", "Zhiguo Cao"], "title": "BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching", "comment": null, "summary": "Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.", "AI": {"tldr": "BokehFlow proposes a depth-free framework for controllable bokeh rendering, achieving high-quality and efficient bokeh effects from all-in-focus images using a cross-attention mechanism.", "motivation": "To overcome the challenges of rendering controllable bokeh effects without depth inputs and to improve controllability and efficiency over existing methods.", "method": "BokehFlow synthesizes bokeh effects by leveraging a cross-attention mechanism to allow control over focus regions and blur intensity through text prompts, avoiding the need for depth maps.", "result": "Experiments show that BokehFlow can generate visually impressive bokeh effects and provides a high level of precise control, surpassing current depth-based and generative approaches in quality and efficiency.", "conclusion": "BokehFlow marks an advancement in the field of controllable bokeh rendering, proving to be a more effective and flexible alternative to existing techniques."}}
{"id": "2511.15567", "categories": ["cs.CV", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.15567", "abs": "https://arxiv.org/abs/2511.15567", "authors": ["Kevin Qinghong Lin", "Siyuan Hu", "Linjie Li", "Zhengyuan Yang", "Lijuan Wang", "Philip Torr", "Mike Zheng Shou"], "title": "Computer-Use Agents as Judges for Generative User Interface", "comment": "Project: https://showlab.github.io/AUI Github: https://github.com/showlab/AUI", "summary": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.", "AI": {"tldr": "The paper introduces AUI-Gym, a benchmark for automatic GUI design using language models and a CUA-Coder collaboration framework, where CUA acts as a judge for more efficient and reliable design.", "motivation": "The paper aims to address the inefficiencies in GUI design for computer agents by integrating their feedback through a collaboration framework that uses both language models and computer-use agents.", "method": "The researchers developed AUI-Gym with 52 applications and 1560 tasks, a verifier for task reliability, and a CUA Dashboard for interpretative feedback, forming a Coder-CUA collaboration framework for automatic GUI design.", "result": "The study proposes a new approach that enhances automatic GUI design efficiency and reliability with CUA feedback, shifting from human-centric to agent-native design principles.", "conclusion": "The paper concludes by suggesting that involving CUA as judges in automatic GUI design leads to more efficient and reliable interfaces, marking a pivotal shift in digital environment design from passive user experiences to active agent participation."}}
{"id": "2511.15077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15077", "abs": "https://arxiv.org/abs/2511.15077", "authors": ["Shengjing Tian", "Yinan Han", "Xiantong Zhao", "Xuehu Liu", "Qi Lang"], "title": "MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation", "comment": "This work has been submitted to a journal for possible publication", "summary": "Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.", "AI": {"tldr": "MambaTrack3D提出了一种新的跟踪框架，专门用于处理高时间变化量环境下的3D对象跟踪问题。通过引入MIP和GFEM模块，有效地改善了计算复杂性和时间冗余，同时显著提高了跟踪性能，尤其在高时间变化量环境中表现优于其他方法。", "motivation": "动态户外环境中，具备高时间变化量（HTV）的环境对于LiDAR点云中的单个对象3D跟踪提出了巨大挑战。现有的基于记忆的跟踪器通常存在二次计算复杂性、时间冗余及对几何先验利用不足的问题。", "method": "提出了一种名为MambaTrack3D的新HTV定向跟踪框架，基于状态空间模型Mamba。设计了一个基于Mamba的帧间传播（MIP）模块，取代了传统的单帧特征提取，实现了近线性复杂度，同时显式建模了历史帧之间的空间关系。此外，引入了分组特征增强模块（GFEM），在通道级别上分离前景和背景语义，从而减轻了内存中的时间冗余。", "result": "在KITTI-HTV和nuScenes-HTV基准测试上，MambaTrack3D在大多数情况下优于其他HTV定向和正常场景的tracker，达到了6.5成功度和9.5精度的提高，而是在标准KITTI数据集上，MambaTrack3D同样可以与最先进的方法竞争。", "conclusion": "总体而言，MambaTrack3D实现了优越的精度-效率Trade-off，在HTV和常规跟踪场景中提供强大的性能。"}}
{"id": "2511.15613", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15613", "abs": "https://arxiv.org/abs/2511.15613", "authors": ["Jing Bi", "Filippos Bellos", "Junjia Guo", "Yayuan Li", "Chao Huang", "Yunlong", "Tang", "Luchuan Song", "Susan Liang", "Zhongfei", "Zhang", "Jason J. Corso", "Chenliang Xu"], "title": "When to Think and When to Look: Uncertainty-Guided Lookback", "comment": null, "summary": "Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.", "AI": {"tldr": "该研究通过大规模实验证明思考对LVLMs视觉推理的影响复杂，特定短回顾语句可提升视觉定位效果。提出的方法在多数据集上均有显著提升。", "motivation": "目前尚无对思考如何影响视觉推理的系统性分析，本研究旨在提供首个大规模受控比较，探讨思考对视觉语言模型的具体影响。", "method": "通过大规模受控实验分析了视觉语言模型在视觉推理中的思考过程，提出了不确定性引导的回顾策略。该策略结合了不确定性信号、自适应回顾提示和广度搜索。", "result": "该方法在MMMU性能上有所提升，并在标准思考表现较弱的类别中获得了最大的增益。该策略超越了几种强大的解码基线，设定了新的固定模型系列和令牌预算下的性能标准。并且该解码策略具有泛化能力，在五个额外的基准测试中一致提升。", "conclusion": "研究证明增加思考并不总是有益的，而是特定类型的思考，尤其是带图片回顾的短语能够提升模型的表现。不确定性引导的回顾策略是一个无需训练的改进方法，已在多个数据集上验证其有效性。"}}
{"id": "2511.15085", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15085", "abs": "https://arxiv.org/abs/2511.15085", "authors": ["Wen Yin", "Siyu Zhan", "Cencen Liu", "Xin Hu", "Guiduo Duan", "Xiurui Xie", "Yuan-Fang Li", "Tao He"], "title": "TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition", "comment": "11 pages, 5 figures", "summary": "Multimodal Emotion Recognition (MER) aims to accurately identify human emotional states by integrating heterogeneous modalities such as visual, auditory, and textual data. Existing approaches predominantly rely on unified emotion labels to supervise model training, often overlooking a critical challenge: inter-modal emotion conflicts, wherein different modalities within the same sample may express divergent emotional tendencies. In this work, we address this overlooked issue by proposing a novel framework, Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise nature of human emotion perception. TiCAL dynamically assesses the consistency of each training sample by leveraging pseudo unimodal emotion labels alongside a typicality estimation. To further enhance emotion representation, we embed features in a hyperbolic space, enabling the capture of fine-grained distinctions among emotional categories. By incorporating consistency estimates into the learning process, our method improves model performance, particularly on samples exhibiting high modality inconsistency. Extensive experiments on benchmark datasets, e.g, CMU-MOSEI and MER2023, validate the effectiveness of TiCAL in mitigating inter-modal emotional conflicts and enhancing overall recognition accuracy, e.g., with about 2.6% improvements over the state-of-the-art DMD.", "AI": {"tldr": "This paper presents TiCAL, a novel framework for Multimodal Emotion Recognition (MER) that improves model performance by assessing the consistency of training samples and embedding features in a hyperbolic space.", "motivation": "The motivation is to address the issue of inter-modal emotion conflicts in multimodal emotion recognition, where different modalities within the same sample may express different emotional tendencies.", "method": "The paper proposes TiCAL, a framework that uses pseudo unimodal emotion labels and typicality estimation to assess the consistency of training samples. It embeds features in a hyperbolic space to capture fine-grained distinctions in emotional categories.", "result": "Extensive experiments on benchmark datasets like CMU-MOSEI and MER2023 show that the TiCAL model improves recognition accuracy by about 2.6% compared to the state-of-the-art DMD model.", "conclusion": "Experiments show that TiCAL effectively mitigates inter-modal emotional conflicts and enhances overall recognition accuracy, demonstrating its superior performance over the existing DMD model with notable improvements."}}
{"id": "2511.15661", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15661", "abs": "https://arxiv.org/abs/2511.15661", "authors": ["Yicheng He", "Chengsong Huang", "Zongxia Li", "Jiaxin Huang", "Yonghui Yang"], "title": "VisPlay: Self-Evolving Vision-Language Models from Images", "comment": null, "summary": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/", "AI": {"tldr": "VisPlay is a self-evolving RL framework for VLMs that uses unsupervised learning to enhance reasoning skills without relying on human-annotated labels, achieving better performance on various benchmarks.", "motivation": "Existing RL approaches often depend on human-annotated labels or task-specific heuristics which are costly and hard to scale. VisPlay aims to overcome these limitations by leveraging unlabeled data.", "method": "We introduce VisPlay, a self-evolving RL framework that enables Vision-Language Models (VLMs) to autonomously improve their reasoning abilities using large amounts of unlabeled image data. It assigns the model into two roles: an Image-Conditioned Questioner and a Multimodal Reasoner, which are trained with Group Relative Policy Optimization (GRPO) to balance the complexity of generated questions with the quality of the silver answers.", "result": "When trained on Qwen2.5-VL and MiMo-VL, VisPlay shows consistent enhancements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks.", "conclusion": "VisPlay demonstrates a viable method for improving the reasoning, compositional generalization, and reducing hallucination in VLMs, indicating a scalable path towards self-evolving multimodal intelligence."}}
{"id": "2511.15092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15092", "abs": "https://arxiv.org/abs/2511.15092", "authors": ["Chengyu Xie", "Zhi Gong", "Junchi Ren", "Linkun Yu", "Si Shen", "Fei Shen", "Xiaoyu Du"], "title": "Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis", "comment": null, "summary": "Pose-guided human image generation is limited by incomplete textures from single reference views and the absence of explicit cross-view interaction. We present jointly conditioned diffusion model (JCDM), a jointly conditioned diffusion framework that exploits multi-view priors. The appearance prior module (APM) infers a holistic identity preserving prior from incomplete references, and the joint conditional injection (JCI) mechanism fuses multi-view cues and injects shared conditioning into the denoising backbone to align identity, color, and texture across poses. JCDM supports a variable number of reference views and integrates with standard diffusion backbones with minimal and targeted architectural modifications. Experiments demonstrate state of the art fidelity and cross-view consistency.", "AI": {"tldr": "提出了JCDM，一种利用多视角先验的联合条件扩散框架，显示出在基于姿态的人体图像生成方面的卓越性能。", "motivation": "现有的基于姿态引导的人体图像生成方法受到单视角参考图像不完整纹理的限制，并且缺乏明确的多视角交互。", "method": "提出JCDM，通过APM模块从不完整的参考图像中推断出一个整体的身份保持先验，JCI机制融合多视角线索并将其注入到去噪主干中，以对齐身份、颜色和纹理。", "result": "实验表明，JCDM在保真度和跨视角一致性方面达到了业界领先水平。", "conclusion": "JCDM展现出了卓越的保真度和跨视角一致性，证明了其在基于姿态引导的人体图像生成中的先进性。"}}
{"id": "2511.15690", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15690", "abs": "https://arxiv.org/abs/2511.15690", "authors": ["Yushi Huang", "Zining Wang", "Zhihang Yuan", "Yifu Ding", "Ruihao Gong", "Jinyang Guo", "Xianglong Liu", "Jun Zhang"], "title": "MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping", "comment": "Code will be released upon acceptance", "summary": "Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\\times$ and the decoding time by 1.26$\\times$.", "AI": {"tldr": "本文提出MoDES，这是一套针对混合专家(MoE)多模态大型语言模型(MLLMs)的无需训练的框架，旨在通过自适应剔除专家来实现高效准确的推理。实验结果表明，MoDES显著优于之前的方法，在提升推理速度的同时还能保持甚至提高模型性能。", "motivation": "鉴于现有的专家剔除方法在作用于MMMLs时会导致性能下降的问题，本文动机在于设计一个能在不降低性能的前提下提高推理效率的方法，特别是针对MMMLs的异构贡献和模态特异性行为。", "method": "本文介绍了一种无需训练的框架MoDES，它采用全局调制局部门控(GMLG)机制来整合全局层级重要性信息到局部路由概率估计中，同时采用双模态阈值处理(DMT)方法来制定专家剔除策略。此外，还提出了一种利用单调性性质缩小最优阈值搜索范围的前沿搜索算法。", "result": "在13个基准测试中，对3个模型系列的广泛实验表明，MoDES的表现显著超过了以前的方法。例如，当了剔除88%的专家后，对于Qwen3-VL-MoE-30B-A3B-Instruct模型，其性能提升了10.67%。同时，MoDES显著提升了推理速度，加速了2.16倍的预填充时间，1.26倍的解码时间。", "conclusion": "MoDES作为一种无需训练的框架，通过设计先进的全局调制局部门控和双模态阈值处理机制，能够有效区分专家的重要性并制定出适当的剔除策略，从而在提高推理效率的同时保持甚至超出之前方法的性能。实验结果证明了MoDES的有效性和竞争力。"}}
{"id": "2511.15098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15098", "abs": "https://arxiv.org/abs/2511.15098", "authors": ["Duo Li", "Zuhao Yang", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models", "comment": "14 pages, 2 figures", "summary": "Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.", "AI": {"tldr": "本研究探讨了视觉令牌冗余在dMLLM架构与任务上的变化，提出剪枝策略对dMLLM响应和效率的影响，为dMLLMs的效率优化提供了新方法。", "motivation": "现有的dMLLMs在推理过程中计算开销大，特别是在每次去噪步骤中的全序列注意力计算。本研究试图从多模态视角来解决这一问题，而非忽视了视觉信号特有的冗余问题。", "method": "研究具体分析了不同dMLLM架构和任务中视觉令牌冗余的发展，以及剪枝操作对dMLLM响应和效率的影响。", "result": "研究揭示了只有在处理长答案任务时，从头开始训练的dMLLMs才会出现视觉冗余。此外，验证了视觉令牌剪枝会导致非轻微的信息丢失，并且只有从头开始的dMLLMs可以在后期去噪步骤中逐渐恢复丢失的信息。", "conclusion": "这项研究为多模态大型语言模型（dMLLMs）的效率优化提供了新视角，大大增强了它们在各种多模态理解任务中的适用性。研究还表明特定层跳跃技术可加速AR到扩散dMLLMs，而逐步或后期步骤修剪方法对于从头开始训练的dMLLMs更有效。"}}
{"id": "2511.15703", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15703", "abs": "https://arxiv.org/abs/2511.15703", "authors": ["Beichen Zhang", "Yuhang Zang", "Xiaoyi Dong", "Yuhang Cao", "Haodong Duan", "Dahua Lin", "Jiaqi Wang"], "title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC", "comment": null, "summary": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.", "AI": {"tldr": "论文提出了视觉和语言相结合的方法来改进基础模型在抽象推理任务上的性能，特别是在ARC-AGI测试集上。通过视觉-语言协同推理和模态切换自我校正策略，实现了比纯文本方法更高的准确率。", "motivation": "核心动机在于解决基础模型在处理抽象推理时的不足，尤其是当模型面对需要从少量示例中推导出结构转换规则时的表现不佳，这是人类智能的关键表征。", "method": "论文提出了两种策略：（1）视觉-语言协同推理（VLSR），将ARC-AGI的任务分解为适合不同模态的子任务；（2）模态切换自我校正（MSSC），利用视觉来验证基于文本的推理，以实现内在错误校正。", "result": "实验表明，与纯文本基线相比，新的方法在ARC-AGI上的不同任务中达到了最多4.33%的性能提升。", "conclusion": "研究表明，统一视觉抽象与语言推理对于开发出能够实现通用性的人类智能的未来基础模型至关重要。"}}
{"id": "2511.15102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15102", "abs": "https://arxiv.org/abs/2511.15102", "authors": ["Junseo Koo", "Jinseo Jeong", "Gunhee Kim"], "title": "Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting", "comment": "AAAI 2026", "summary": "The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.", "AI": {"tldr": "This paper introduces Gaussian Blending to improve 3D Gaussian Splatting technique, reducing blurring and staircase artifacts for novel view synthesis at various unseen sampling rates, while allowing for easy integration into existing frameworks.", "motivation": "To address the shortcomings of conventional 3D Gaussian Splatting (3DGS) methods, which suffer from blurring and staircase artifacts when synthesizing novel views at unseen sampling rates, by proposing a new technique for handling the blending process that maintains high detail without additional memory or computational overhead.", "method": "Our proposal, Gaussian Blending, replaces the conventional alpha blending that computes scalar quantities of alpha and transmittance per pixel with spatially varying distributions. This technique updates transmittances by considering the spatially distributed alpha values across the pixel area, permitting background splats near the area to contribute to the final image, thereby alleviating blurring and staircase artifacts in 3DGS rendering.", "result": "Gaussian Blending has been shown to outperform existing novel view synthesis models, whether the sampling rate is seen or unseen, effectively reducing visual discrepancies like blurring and staircase artifacts, while maintaining real-time rendering speed and no additional memory cost.", "conclusion": "The novel Gaussian Blending technique proposed in this paper effectively resolves issues of blurring and staircase artifacts in 3DGS by improving the transmittance calculation, leading to better performance across varying sampling rates and integration into existing frameworks."}}
{"id": "2511.15117", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.15117", "abs": "https://arxiv.org/abs/2511.15117", "authors": ["Jun-Yi Liu", "Chung-Hao Chen", "Ya-Chi Tsao", "Ssu-Yao Wu", "Yu-Ting Tsao", "Lyn Chao-ling Chen"], "title": "An Event-triggered System for Social Persuasion and Danger Alert in Elder Home Monitoring", "comment": "Accepted in the 35th IPPR Conference on Computer Vision, Graphics, and Image Processing (CVGIP2022)", "summary": "In the study, the physical state and mental state of elders are both considered, and an event-triggered system has developed to detect events: watch dog, danger notice and photo link. By adopting GMM background modeling, the motion behavior of visitors and elders can be detected in the watch dog event and danger notice event respectively. Experiments set in home scenarios and 5 families participated in the experiments for detecting and recording three types of events from their life activities. In addition, the captured images were analyzed using SVM machine learning. For lack of technical experiences of elders, an intuitive operation as normal life activity was designed to create communication between elder and relatives via social media.", "AI": {"tldr": "研究开发了一种事件触发的监测系统，使用GMM背景建模和SVM机器学习在家中检测事件。旨在提升老年人安全和福祉，并简化老年人与家庭成员的沟通。", "motivation": "考虑到老年人的物理状态和心理状态，研究旨在开发一种事件触发的系统，以检测重要事件如看护、危险预警和通过社交媒体的照片共享，从而提高老年人的生活质量和安全性。", "method": "研究采用了GMM背景建模来检测看护和危险警示事件中的动作行为，并使用SVM机器学习分析捕获的图像。设计了普通人生活的交互方式，使得老年人能够通过社交媒体和亲人交流。", "result": "通过在家中部署实验并分析了5个家庭的活动事件，包括看护、危险警示和照片连接，采用GMM背景建模检测动作行为，并使用SVM机器学习分析捕获的图像。实验结果显示了系统的有效性和实用性。此外，为缺乏技术经验的老年人设计了一种直观的操作方式，使得他们能够通过社交媒体与亲人进行交流。", "conclusion": "该系统有效提升了老年人安全和福祉，简化了他们与家人沟通的方式。实验结果表明了该系统的可行性和便利性。"}}
{"id": "2511.15118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15118", "abs": "https://arxiv.org/abs/2511.15118", "authors": ["Jin Wang", "Bingfeng Zhang", "Jian Pang", "Weifeng Liu", "Baodi Liu", "Honglong Chen"], "title": "Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation", "comment": null, "summary": "Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.", "AI": {"tldr": "本文提出了一种无偏语义解码（USD）策略，用于结合SAM模型和对比语言-图像预训练（CLIP）模型，增强少样本分割任务中的目标区域识别能力，同时保持无偏和通用性。", "motivation": "现有的针对少样本分割的方法主要集中在从支持集中提取提示，这难以充分激发SAM模型的泛化能力，并可能导致适应未知类别的解码过程出现偏差。因此，提出了一种新的无偏语义解码（USD）策略，旨在解决上述问题，并优化SAM模型在少样本分割中的性能。", "method": "采用了一种无偏语义解码（USD）策略，该策略结合了SAM模型，并从支持集和查询集同时提取目标信息，以实现由对比语言-图像预训练（CLIP）模型语义引导的一致预测。具体来说，为了增强SAM的无偏语义辨别能力，设计了两种特征增强策略，利用CLIP模型的语义对齐能力来丰富原始SAM特征。这主要包括在图像级别提供的全局补充，以提供一个通用类别指示和支持图像，以及在像素级别提供的局部指导，以提供关于查询图像的有用目标位置。此外，提出了一种可学习的视觉-文本目标提示生成器，通过目标文本嵌入和剪辑视觉特征的交互来生成目标导向的提示嵌入。", "result": "尚未具体提及实验结果。", "conclusion": "从支持集和查询集同时提取目标信息，并结合CLIP模型的语义引导，增强SAM模型的无偏语义辨别能力，通过设计的特征增强策略和目标导向的提示生成器，实现了更好的少样本分割能力。"}}
{"id": "2511.15132", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15132", "abs": "https://arxiv.org/abs/2511.15132", "authors": ["Nishchala Thakur", "Swati Kochhar", "Deepti R. Bathula", "Sukrit Gupta"], "title": "WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images", "comment": null, "summary": "Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.", "AI": {"tldr": "A novel active learning framework, WaveFuse-AL, improves upon single-strategy methods by adaptively fusing multiple acquisition strategies, resulting in consistent performance improvements across three medical imaging benchmarks.", "motivation": "To address the inconsistent behavior of individual acquisition strategies in active learning across different stages of the learning cycle in medical imaging, which can lead to higher annotation costs.", "method": "Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a framework that adaptively fuses multiple established acquisition strategies (BALD, BADGE, Entropy, and CoreSet) with cyclical temporal priors and performance-driven adaptation.", "result": "WaveFuse-AL consistently outperformed both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements on ten out of twelve metric measurements.", "conclusion": "WaveFuse-AL effectively maximizes the utility of limited annotation budgets while maintaining superior performance across various medical imaging tasks."}}
{"id": "2511.15151", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15151", "abs": "https://arxiv.org/abs/2511.15151", "authors": ["Meihua Zhou", "Xinyu Tong", "Jiarui Zhao", "Min Cheng", "Li Yang", "Lei Tian", "Nan Wan"], "title": "DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging", "comment": null, "summary": "High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.", "AI": {"tldr": "研究提出DCL-SE框架，使用ARP和DGM改进神经影像数据的解析，提高诊断准确性、鲁棒性和可解释性。", "motivation": "高维神经影像分析中的临床诊断往往受到时空保真度和大规模通用模型适应性的限制。", "method": "提出了一种名为DCL-SE的端到端框架，该框架基于数据驱动的时空编码（DaSE）。使用近似秩池化 (ARP) 将三维脑体积数据高效编码为信息丰富的二维动态表示，然后采用由动态分组机制 (DGM) 引导的动态课程学习策略，逐步训练解码器，从全局解剖结构到病理细节进行特征提取的优化。", "result": "在六项公开数据集中评估，包括阿尔茨海默病和脑肿瘤分类、脑动脉分割和脑龄预测，DCL-SE在准确性、鲁棒性和可解释性方面始终优于现有方法。", "conclusion": "这些发现强调了在大规模预训练网络时代，紧凑、任务特定的架构的重要性。"}}
{"id": "2511.15153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15153", "abs": "https://arxiv.org/abs/2511.15153", "authors": ["Chun-Jung Lin", "Tat-Jun Chin", "Sourav Garg", "Feras Dayoub"], "title": "SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection", "comment": "accepted by WACV 2026", "summary": "Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.", "AI": {"tldr": "Introduces SceneEdited, a dataset for HD map 3D point cloud updating, covering urban changes with detailed annotations.", "motivation": "To address the gap between 2D change detection and 3D map updating by providing a dedicated dataset.", "method": "Developed a city-scale dataset with 23,000 synthesized object changes in 3D scenes and included a toolkit for research.", "result": "Created a comprehensive set of 800 scenes with annotations for change detection, supporting the maintenance of HD maps.", "conclusion": "Establishes SceneEdited as a standardized benchmark for advancing research on 3D HD map updating techniques."}}
{"id": "2511.15164", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15164", "abs": "https://arxiv.org/abs/2511.15164", "authors": ["Songze Li", "Mingyu Gao", "Tonghua Su", "Xu-Yao Zhang", "Zhongjie Wang"], "title": "Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance", "comment": null, "summary": "Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.", "AI": {"tldr": "The paper introduces a method to mitigate catastrophic forgetting in multimodal continual instruction tuning by approximating missing gradients from old tasks using geometric properties of the parameter space and integrating them with real gradients from a limited replay buffer, achieving state-of-the-art performance without model expansion.", "motivation": "The motivation behind this paper is to address the problem of catastrophic forgetting in multimodal continual instruction tuning, where the model's performance on previously learned tasks deteriorates as it learns new tasks.", "method": "The method involves approximating missing gradients from old tasks using the directional vector between current parameters and previously optimal parameters, integrating these approximated gradients with real gradients from a replay buffer, and using a Bernoulli sampling strategy to balance model stability and plasticity.", "result": "The results show that the proposed method achieves state-of-the-art performance in multimodal continual instruction tuning datasets, effectively mitigating catastrophic forgetting without expanding the model size.", "conclusion": "The conclusion is that the novel approach to approximate missing gradients, combined with selective real gradient integration, effectively mitigates catastrophic forgetting, allowing the model to maintain good performance on previously learned tasks while learning new ones, all within a compact architecture."}}
{"id": "2511.15167", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15167", "abs": "https://arxiv.org/abs/2511.15167", "authors": ["Jing Cao", "Kui Jiang", "Shenyi Li", "Xiaocheng Feng", "Yong Huang"], "title": "Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation", "comment": null, "summary": "Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.", "AI": {"tldr": "论文提出了一种新的自监督深度估计框架SEC-Depth，在恶劣天气条件下也能保持良好的性能。框架通过自适应地调整学习目标来隐式感受天气退化，从而提高鲁棒性。实验表明，该方法集成到不同类型模型中表现出显著增强的鲁棒性。", "motivation": "作者提出了这种方法是因为现有的自我监督深度估计方法在恶劣天气条件下表现出显著的性能下降。在这种情况下，降低的能见度严重地影响了深度预测。为了应对这一问题，作者引入了SEC-Depth框架。", "method": "提出了一种名为SEC-Depth的新自进化对比学习框架，用于自监督的鲁棒深度估计任务。该方法利用训练过程中生成的中间参数构建随时间演化的延迟模型，并设计了一种自进化对比方案以减轻在具有挑战性条件下的性能下降。具体来说，提出了一种深度估计任务中延迟模型的动态更新策略，以捕捉训练阶段的优化状态。为了有效利用延迟模型，引入了自进化对比损失（SECL），将历史延迟模型的输出作为负样本处理。该机制自适应地调整学习目标，同时隐式感受天气退化严重程度，减少了人工干预的需求。", "result": "实验结果显示，该方法无缝集成到各种基线模型中，并在零样本评估中显著增强了鲁棒性。", "conclusion": "通过利用自进化对比学习框架，可以提高自我监督深度估计在恶劣天气条件下的性能，减少手动干预的需求，并且能够集成到不同的基线模型中提升鲁棒性。"}}
{"id": "2511.15179", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15179", "abs": "https://arxiv.org/abs/2511.15179", "authors": ["Kyotaro Tokoro", "Hiromu Taketsugu", "Norimichi Ukita"], "title": "MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction", "comment": "Accepted to WACV2026", "summary": "This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \\textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \\textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM", "AI": {"tldr": "此论文提出了一种新的针对人类运动预测（HMP）的评估度量标准——MMCM，用于改进现有评估方法在多模式覆盖和有效性方面的问题。", "motivation": "论文是为了改善人类运动预测中的多重预测结果的评估方式，现有方法仅关注单一可能的未来运动和简单分布度量，但并未足够关注多模式分布的多样性和预测运动的合理性。论文希望通过MMCM方法来解决这些问题。", "method": "此论文提出了一种新的评估人类运动预测(HMP)方法的度量标准，称为MMCM，重点解决了现有度量标准的两个问题：(a) 覆盖率：预测出的多个运动应涵盖多种可能性；(b) 有效性：预测出的多种运动应是可观察到的未来运动。MMCM使用基于聚类的模式分析运动空间，并对该运动空间进行分割，评估预测运动是否分布在多个模式中，并将有效模式定义为从运动数据集中收集的所有可能的未来运动。", "result": "实验结果表明，MMCM能识别出合理的模式定义并与现有的度量标准相比，能够更准确地评价多模式的HMP。", "conclusion": "MMCM是一个针对HMP评估的有效工具，能够更好地评估多模式运动预测的覆盖性和有效性。"}}
{"id": "2511.15186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15186", "abs": "https://arxiv.org/abs/2511.15186", "authors": ["Geon Choi", "Hangyul Yoon", "Hyunju Shin", "Hyunki Park", "Sang Hoon Seo", "Eunho Yang", "Edward Choi"], "title": "Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset", "comment": null, "summary": "The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.", "AI": {"tldr": "为了解决病变分割模型在胸部X光片上应用的限制，我们引入了指令引导的病变分割（ILS）范式，并构建了MIMIC-ILS数据集。我们的视觉语言模型ROSALIA展示了在病变分割和文本解释方面的高精度。", "motivation": "我们研究的动机是解决当前病变分割模型在胸部X光片（CXRs）上应用受限的问题，这些问题包括目标标签数量少和依赖长篇详细的专家级文本输入。这些问题导致了实际应用中的障碍。为了改善这些情况，我们提出了一种新的方法。", "method": "我们引入了一种新的范式——指令引导的病变分割（ILS），该范式旨在基于简单的、用户友好的指令来分割多种类型的病变。我们利用全自动的多模态管道，从胸部X光图像及其对应的报告中生成标注，构建了MIMIC-ILS，这是首个针对胸部X光病变分割的大规模指令-答案数据集。MIMIC-ILS包含了110万条指令-答案对，来源于19.2万张图像和9.1万个独特的分割掩模，覆盖了七种主要病变类型。为了证明其效用，我们推出了ROSALIA，这是一种基于MIMIC-ILS微调的视觉语言模型，能够根据用户指令精确地分割病变并提供文本解释。", "result": "我们的模型ROSALIA在新提出的任务中实现了高分割和文本精度，这突显了我们管道的有效性和MIMIC-ILS作为基石资源的价值。", "conclusion": "我们确实证明了我们的ILS方法的有效性，并展示了MIMIC-ILS数据集在高精度分割和提供文本解释方面的能力。"}}
{"id": "2511.15188", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15188", "abs": "https://arxiv.org/abs/2511.15188", "authors": ["Wasif Jalal", "Md Nafiu Rahman", "M. Sohel Rahman"], "title": "BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI", "comment": null, "summary": "Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.", "AI": {"tldr": "本文提出了一种结合视觉变换器（ViT）全局上下文建模能力和残差卷积神经网络（CNN）局部细化能力的混合架构——BrainRotViT，用于脑龄预测，具有高精度和泛化性，可应用于老龄化和神经退行性疾病研究。", "motivation": "传统回归方法和基于CNN的方法在脑龄估计上面临手动特征工程、有限的感受野及异构数据过拟合等难题，纯transformer模型虽有效但仍需大数据集和计算资源。本文旨在通过新型架构改进这些问题。", "method": "第一阶段使用ViT编码器进行辅助年龄和性别分类任务训练，然后应用于所有冠状面切片生成嵌入向量矩阵。第二阶段利用残差CNN回归器进行脑龄连续值估计，引入受试者性别作为FC层输入。", "result": "新方法在包含超过130个采集站点的11个MRI数据集上的验证中实现了3.34年的平均绝对误差，表现优于基线和前沿模型，并且在四个独立队列上具有良好的泛化能力。", "conclusion": "研究结果展示了BrainRotViT方法的有效性、解释性和普遍适用性，为脑龄预测提供了一种新的桥梁，连接了基于CNN和transformer的方法，并为进一步的算老龄化神经退行性疾病研究铺平了道路。"}}
{"id": "2511.15197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15197", "abs": "https://arxiv.org/abs/2511.15197", "authors": ["Raghu Vamsi Chittersu", "Yuvraj Singh Rathore", "Pranav Adlinge", "Kunal Swami"], "title": "Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition", "comment": null, "summary": "Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical \"blenders\" that lack generative fidelity and \"generators\" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.", "AI": {"tldr": "本文提出了一种新的零样本生成框架Insert In Style，适用于将现实对象插入风格化场景，通过创新的训练协议和特殊架构解决了现有方法的局限性，综合性能超越了现有技术。", "motivation": "现有方法在将现实世界对象插入到风格化领域时存在局限性，主要集中在实用性和生成保真度上。因此，本文提出了一种零样本生成框架，旨在同时满足实用性和高保真度。", "method": "我们的方法提出了一个统一的框架，包含两个关键创新：1) 一个新颖的多阶段训练协议，将身份、风格和组合的表示分离；2) 一种专门的掩码注意力架构，在生成过程中严格执行这种分离，从而防止广泛使用的统一注意力模型中的概念干扰。", "result": "我们的模型在新的100K样本数据集上进行了训练，该数据集由新颖的数据管道生成，确保了高保真度的语义身份和风格一致性。实验结果显示了领先性能，特别是在身份和风格度量上显著优于现有方法，用户研究也强烈验证了这一结果。", "conclusion": "本工作介绍的Insert In Style是首个零样本生成框架，提供了高保真度和实用性的结合。通过新的训练协议和掩码注意力架构，模型在风格化组合的任务上展现了显著的性能提升。"}}
{"id": "2511.15201", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.15201", "abs": "https://arxiv.org/abs/2511.15201", "authors": ["Qing Wang", "Chong-Wah Ngo", "Ee-Peng Lim"], "title": "Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval", "comment": null, "summary": "This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions. The current representation learning tends to capture dominant visual-text alignment while overlooking subtle variations that determine retrieval relevance. In this paper, we model such bias in cross-modal representation learning using causal theory. The causal view of this problem suggests ingredients as one of the confounder sources and a simple backdoor adjustment can alleviate the bias. By causal intervention, we reformulate the conventional model for food-to-recipe retrieval with an additional term to remove the potential bias in similarity judgment. Based on this theory-informed formulation, we empirically prove the oracle performance of retrieval on the Recipe1M dataset to be MedR=1 across the testing data sizes of 1K, 10K, and even 50K. We also propose a plug-and-play neural module, which is essentially a multi-label ingredient classifier for debiasing. New state-of-the-art search performances are reported on the Recipe1M dataset.", "AI": {"tldr": "该论文通过引入因果理论来解决食谱和食物图像跨模态检索中的偏差问题，并提出了一个可插拔的神经模块，改善了检索性能。", "motivation": "现有方法将食谱视为描述菜品视觉外观的文本来源，导致跨模态检索中的偏差。", "method": "利用因果理论建模偏差，并通过因果干预重新定义了传统的食谱检索模型。", "result": "在Recipe1M数据集上，测试数据量分别为1K、10K和50K时，中位检索距离MedR均达到了1。", "conclusion": "该研究展示了因果干预方法在跨模态检索中消除偏差的有效性，并提出了一个可插拔的神经模块来改进检索性能。"}}
{"id": "2511.15204", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15204", "abs": "https://arxiv.org/abs/2511.15204", "authors": ["Kishor Datta Gupta", "Marufa Kamal", "Md. Mahfuzur Rahman", "Fahad Rahman", "Mohd Ariful Haque", "Sunzida Siddique"], "title": "Physics-Based Benchmarking Metrics for Multimodal Synthetic Images", "comment": null, "summary": "Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.", "AI": {"tldr": "This paper proposes PCMDE, a physics-constrained multimodal evaluation metric, to improve the evaluation of semantic and structural accuracy beyond the capabilities of existing metrics.", "motivation": "The motivation is to address the limitations of current state-of-the-art evaluation metrics, such as BLEU, CIDEr, VQA score, SigLIP-2, and CLIPScore, which do not adequately capture semantic or structural accuracy in certain scenarios.", "method": "The paper introduces Physics-Constrained Multimodal Data Evaluation (PCMDE) metric incorporating large language models and vision-language models to improve semantic and structural accuracy in domain-specific and context-dependent scenarios. The PCMDE metric has three stages: feature extraction of spatial and semantic information, confidence-weighted component fusion for validation, and physics-guided reasoning for enforcing structural and relational constraints.", "result": "N/A, the abstract does not specify concrete experimental results.", "conclusion": "N/A, the abstract does not provide a conclusion, only the methodology and motivation are described."}}
{"id": "2511.15242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15242", "abs": "https://arxiv.org/abs/2511.15242", "authors": ["Yuhao Shen", "Jiahe Qian", "Zhangtianyi Chen", "Yuanhao He", "Juexiao Zhou"], "title": "SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning", "comment": null, "summary": "We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.", "AI": {"tldr": "本论文介绍了一种专注于皮肤病的视觉语言模型SkinGPT-R1，它采用了DermCoT数据集进行训练，并在多个皮肤病诊断任务上展示了优于基础模型的性能。", "motivation": "开发SkinGPT-R1是为了提供一种更有效、更透明的皮肤病诊断工具，能够帮助医生进行明确和可验证的诊断推理。", "method": "文中介绍了SkinGPT-R1，这是一个专注于皮肤病的视觉语言模型，它能够明确、分步骤、可验证地进行诊断推理。为了支持皮肤特定的推理，作者创建了DermCoT数据集，它结合了10,000个经过筛选的训练案例和3,000个经过皮肤科医生评分的认证案例。此外，还定义了DermEval作为六个维度的评估器，DermBench作为相应的基准。", "result": "SkinGPT-R1在DermBench上的六维度上平均得分4.031（满分5分），在所有系统中排名第一，并在三种皮肤病分类基准上显示出稳定的准确率增益。实验还显示，基于DermCoT的推理监督和增加视觉蒸馏对于提升模型性能有显著贡献。", "conclusion": "实验结果表明，SkinGPT-R1在DermBench上的得分比Vision-R1提高了约41%，并且在三种皮肤病分类基准上也显示出稳定的准确率增益。同时还发现，在基础模型之上，基于DermCoT的推理监督可以带来显著的提升，而增加皮肤病感知的视觉蒸馏也能够提高叙述的质量和识别率。"}}
{"id": "2511.15258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15258", "abs": "https://arxiv.org/abs/2511.15258", "authors": ["Yitong Yang", "Yinglin Wang", "Changshuo Wang", "Yongjun Zhang", "Ziyang Chen", "Shuting He"], "title": "SplitFlux: Learning to Decouple Content and Style from a Single Image", "comment": null, "summary": "Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.", "AI": {"tldr": "本文提出了SplitFlux方法，有效解决了现有SDXL和Flux模型在内容和风格解耦方面存在的问题，实验表明其在多个场景中表现突出。", "motivation": "现有的SDXL方法难以生成高质量的结果，而Flux模型也未能有效分离内容和风格。因此，我们通过系统分析Flux模型，提出了SplitFlux来解决这些问题。", "method": "我们提出了一种名为SplitFlux的方法，通过LoRA微调 Flux模型中的单梦块来实现内容和风格的解耦。该方法有两个核心组件：排名约束适应和视觉门控LoRA。", "result": "实验结果表明，SplitFlux在不同的场景下始终优于最先进的方法，实现了卓越的内容保留和风格化质量。", "conclusion": "通过SplitFlux方法，我们成功地实现了内容和风格的解耦，不仅提高了图像生成的质量，而且具有更强的通用性和适应性。"}}
{"id": "2511.15271", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15271", "abs": "https://arxiv.org/abs/2511.15271", "authors": ["Loveneet Saini", "Hasan Tercan", "Tobias Meisen"], "title": "Graph Query Networks for Object Detection with Automotive Radar", "comment": "Accepted in WACV 2026 Main Conference", "summary": "Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.", "AI": {"tldr": "通过Graph Query Networks (GQN)模型提高雷达对象检测精度，该模型在数据集上展示了显著的性能提升和计算效率。", "motivation": "雷达波长较长，产生稀疏且不规则的反射，这给基于网格和序列的卷积及转换检测器带来了挑战。GQN旨在解决这一问题，以提高360度汽车感知中雷达对象检测的精度。", "method": "GQN使用了一种新的图查询概念，动态关注鸟瞰视图空间，构建对象特定的图，这两个图由两个新的模块处理：EdgeFocus用于关系推理，DeepContext Pooling用于上下文聚集。", "result": "该论文提出了用于雷达对象检测的Graph Query Networks (GQN)框架，通过将雷达检测到的对象建模成图，并使用图查询来动态关注鸟瞰视图空间，提高雷达对象检测的精度。实验结果表明，GQN在NuScenes数据集上将mAP相对提高了最多53%，同时显著减少了计算复杂度。", "conclusion": "GQN相较于最强的之前雷达方法，mAP相对提高了8.2%，同时将图构造的峰值开销减少了80%，尽管其FLOPs成本有所增加。"}}
{"id": "2511.15288", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15288", "abs": "https://arxiv.org/abs/2511.15288", "authors": ["Yanni Ma", "Hao Liu", "Yulan Guo", "Theo Gevers", "Martin R. Oswald"], "title": "Edge-Centric Relational Reasoning for 3D Scene Graph Prediction", "comment": null, "summary": "3D scene graph prediction aims to abstract complex 3D environments into structured graphs consisting of objects and their pairwise relationships. Existing approaches typically adopt object-centric graph neural networks, where relation edge features are iteratively updated by aggregating messages from connected object nodes. However, this design inherently restricts relation representations to pairwise object context, making it difficult to capture high-order relational dependencies that are essential for accurate relation prediction. To address this limitation, we propose a Link-guided Edge-centric relational reasoning framework with Object-aware fusion, namely LEO, which enables progressive reasoning from relation-level context to object-level understanding. Specifically, LEO first predicts potential links between object pairs to suppress irrelevant edges, and then transforms the original scene graph into a line graph where each relation is treated as a node. A line graph neural network is applied to perform edge-centric relational reasoning to capture inter-relation context. The enriched relation features are subsequently integrated into the original object-centric graph to enhance object-level reasoning and improve relation prediction. Our framework is model-agnostic and can be integrated with any existing object-centric method. Experiments on the 3DSSG dataset with two competitive baselines show consistent improvements, highlighting the effectiveness of our edge-to-object reasoning paradigm.", "AI": {"tldr": "提出LEO框架，通过从关系水平到物体水平的逐步推理来改善3D场景图预测。", "motivation": "解决现有3D场景图预测方法中关系表示受限于成对物体上下文，难以捕捉高阶关系依赖的问题。", "method": "LEO框架，先预测物体对之间的潜在链接以抑制无关边，再将原始场景图转换为线图进行边中心的关系推理，最后将关系特征整合回物体中心图中以提升关系预测精度。", "result": "LEO框架在3DSSG数据集上相比两个有竞争力的基线展现出一致的性能提升。", "conclusion": "LEO框架证明了其通过边缘到物体的推理范式改善3D场景图预测的有效性。"}}
{"id": "2511.15299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15299", "abs": "https://arxiv.org/abs/2511.15299", "authors": ["Jialong Sun", "Hongguang Zhu", "Weizhe Liu", "Yunda Sun", "Renshuai Tao", "Yunchao Wei"], "title": "Taming Generative Synthetic Data for X-ray Prohibited Item Detection", "comment": null, "summary": "Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.", "AI": {"tldr": "A new one-stage X-ray image synthesis pipeline, Xsyn, is proposed, which reduces extra labor costs and surpasses existing methods by generating high-quality images that enhance detection performance.", "motivation": "The motivation is to solve the problem of insufficient and labor-intensive data for training prohibited item detection models in X-ray security images.", "method": "The paper proposes Xsyn, a one-stage X-ray security image synthesis pipeline. It includes two strategies: Cross-Attention Refinement (CAR) and Background Occlusion Modeling (BOM).", "result": "Experiments show that the method enhances the prohibited item detection performance with a 1.2% mAP improvement over prior methods.", "conclusion": "Xsyn introduces an efficient and high-quality solution for X-ray security image synthesis, reducing labor costs and improving detection performance."}}
{"id": "2511.15308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15308", "abs": "https://arxiv.org/abs/2511.15308", "authors": ["Yan Xia", "Letian Shi", "Yilin Di", "Joao F. Henriques", "Daniel Cremers"], "title": "Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language", "comment": "This paper builds upon and extends our earlier conference paper Text2Loc presented at CVPR 2024", "summary": "We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.", "AI": {"tldr": "研究提出了Text2Loc++，一种新的神经网络，用于在粗到细定位管道中有效对齐语言与3D点云。该方法表现出比现有方法更好的性能，并能在复杂语言和多种城市环境中展现良好的泛化性能。", "motivation": "该研究旨在解决使用复杂和多样的自然语言描述进行3D点云子地图定位的问题，提出了Text2Loc++来改善语言与点云之间的跨模态对齐，以提高整体定位的精度和可靠性。", "method": "Text2Loc++是一个新型神经网络，设计用于在粗到细定位管道中有效对齐语言和点云之间的跨模态。为了支持基准测试，引入了一个新的城市规模数据集，涵盖了来自不同城市场景的颜色和非颜色点云，并将位置描述组织成三个语言复杂度级别。在全球地点识别阶段，Text2Loc++结合了预训练的语言模型和带有最大池化的分层转换器(HTM)来实现句子级语义理解，并使用基于注意力的点云编码器来实现空间理解。此外，提出了掩码实例训练(MIT)，用于过滤出不匹配的对象并改善跨模态鲁棒性。为了增强嵌入空间，引入了跨模态、子地图、文本和实例级别的损失，这种方法称为面向多模态层次对比学习(MHCL)。在细定位阶段，完全去除了显式的文本实例匹配，设计了一种基于原型地图克隆(PMC)和级联交叉注意力Transformer (CCAT)的轻量级但强大的框架。", "result": "基于KITTI360Pose数据集进行了广泛的实验，表明Text2Loc++相比现有方法提高了最多15%的性能。此外，所提出的模型在新数据集上的评估中表现出强大的泛化能力，能够有效处理复杂的语言表达和各种城市环境。", "conclusion": "该论文通过介绍Text2Loc++和相关的实验验证显示，在3D点云的细粒度定位上能够有效利用复杂的自然语言描述，并实现了在性能上的显著提升；提出的框架和方法展示了出色的跨模态理解能力和良好的泛化性能。"}}
{"id": "2511.15311", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15311", "abs": "https://arxiv.org/abs/2511.15311", "authors": ["Mehran Tamjidi", "Hamidreza Dastmalchi", "Mohammadreza Alimoradijazi", "Ali Cheraghian", "Aijun An", "Morteza Saberi"], "title": "Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models", "comment": "Accepted by AAAI 2026. 7 pages, 4 figures", "summary": "3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.", "AI": {"tldr": "提出了一种无需重新训练的在线测试时适应（TTA）策略——Uni-Adapter，显著提高了不同类型3D数据基准的表现，并取得SOTA结果。", "motivation": "解决3D视觉语言基础模型（VLFM）在实际场景中对噪声、不完整数据或不同分布数据下表现不佳的问题。", "method": "动态原型学习方法，使用3D缓存存储类别特定的聚类中心作为原型，并通过相似度评分进行缓存对数计算。同时，采用基于图的标签平滑模块来强制具有相似原型间的标签一致性，并通过熵加权聚合来统一来自原始3D VLFM和修正后的3D缓存的预测。", "result": "在不同的3D VLFMs基准测试中，Uni-Adapter提升了ModelNet-40C 10.55%，ScanObjectNN-C 8.26%，ShapeNet-C 4.49%的表现。", "conclusion": "通过使用动态原型学习和熵加权聚合，Uni-Adapter能够在不重新训练的情况下有效缓解分布转换，即使在各类3D挑战性数据上也能取得领先性能。"}}
{"id": "2511.15312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15312", "abs": "https://arxiv.org/abs/2511.15312", "authors": ["Mauro Larrat", "Claudomiro Sales"], "title": "A Multimodal Transformer Approach for UAV Detection and Aerial Object Recognition Using Radar, Audio, and Video Data", "comment": "23 pages, 7 figures", "summary": "Unmanned aerial vehicle (UAV) detection and aerial object recognition are critical for modern surveillance and security, prompting a need for robust systems that overcome limitations of single-modality approaches. This research addresses these challenges by designing and rigorously evaluating a novel multimodal Transformer model that integrates diverse data streams: radar, visual band video (RGB), infrared (IR) video, and audio. The architecture effectively fuses distinct features from each modality, leveraging the Transformer's self-attention mechanisms to learn comprehensive, complementary, and highly discriminative representations for classification. The model demonstrated exceptional performance on an independent test set, achieving macro-averaged metrics of 0.9812 accuracy, 0.9873 recall, 0.9787 precision, 0.9826 F1-score, and 0.9954 specificity. Notably, it exhibited particularly high precision and recall in distinguishing drones from other aerial objects. Furthermore, computational analysis confirmed its efficiency, with 1.09 GFLOPs, 1.22 million parameters, and an inference speed of 41.11 FPS, highlighting its suitability for real-time applications. This study presents a significant advancement in aerial object classification, validating the efficacy of multimodal data fusion via a Transformer architecture for achieving state-of-the-art performance, thereby offering a highly accurate and resilient solution for UAV detection and monitoring in complex airspace.", "AI": {"tldr": "研究设计并评估了一种用于无人机检测和空中物体识别的多模态Transformer模型，综合使用雷达、RGB视频、红外视频和音频数据，展示了在复杂背景下的实时性能和高精度检测能力。", "motivation": "现有的单模态方法在无人机检测和空中物体识别方面存在局限性，这些任务对于现代监控和安全至关重要。本研究旨在解决这些问题。", "method": "本研究设计了一种新颖的多模态Transformer模型，该模型结合了雷达、RGB视频、红外视频和音频等多种数据流，利用Transformer的自注意力机制学习全面、互补且高度区分的表示形式，用于分类。", "result": "该模型在独立测试集上实现了卓越的性能，宏平均度量（准确率、召回率、精度、F1分数和特异性）分别为0.9812、0.9873、0.9787、0.9826和0.9954，特别是在区分无人机和其他空中物体方面显示出高精度和高召回率。计算分析显示其计算效率高，推理速率为41.11 FPS，适用于实时应用。", "conclusion": "本研究在空中物体分类方面取得了重要进展，验证了通过Transformer架构融合多模态数据实现最先进的性能的有效性，为复杂空域中的无人机检测和监控提供了高度准确和可靠解决方案。"}}
{"id": "2511.15316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15316", "abs": "https://arxiv.org/abs/2511.15316", "authors": ["Zhihan Ren", "Lijun He", "Jiaxi Liang", "Xinzhu Fu", "Haixia Bi", "Fan Li"], "title": "What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs", "comment": null, "summary": "Split DNNs enable edge devices by offloading intensive computation to a cloud server, but this paradigm exposes privacy vulnerabilities, as the intermediate features can be exploited to reconstruct the private inputs via Feature Inversion Attack (FIA). Existing FIA methods often produce limited reconstruction quality, making it difficult to assess the true extent of privacy leakage. To reveal the privacy risk of the leaked features, we introduce FIA-Flow, a black-box FIA framework that achieves high-fidelity image reconstruction from intermediate features. To exploit the semantic information within intermediate features, we design a Latent Feature Space Alignment Module (LFSAM) to bridge the semantic gap between the intermediate feature space and the latent space. Furthermore, to rectify distributional mismatch, we develop Deterministic Inversion Flow Matching (DIFM), which projects off-manifold features onto the target manifold with one-step inference. This decoupled design simplifies learning and enables effective training with few image-feature pairs. To quantify privacy leakage from a human perspective, we also propose two metrics based on a large vision-language model. Experiments show that FIA-Flow achieves more faithful and semantically aligned feature inversion across various models (AlexNet, ResNet, Swin Transformer, DINO, and YOLO11) and layers, revealing a more severe privacy threat in Split DNNs than previously recognized.", "AI": {"tldr": "本文提出FIA-Flow框架以提高中间特征的图像重建质量，揭示分割DNN中中间特征泄露带来的严重隐私威胁。", "motivation": "现有的FIA方法通常产生有限的重建质量，难以准确评估隐私泄漏的真实程度。本文旨在揭示中间特征泄露导致的隐私威胁，提出更高的图像重建质量，以便更准确地评估隐私风险。", "method": "为揭示分割DNN中特征泄露的隐私风险，本文提出了一种名为FIA-Flow的黑盒FIA框架，该框架能从中间特征中实现高质量的图像重建。设计了一个中间特征空间与潜在空间之间语义鸿沟的Latent Feature Space Alignment Module (LFSAM)模块，同时开发了一种Deterministic Inversion Flow Matching (DIFM)，用于单一推理步骤中将离群特征投影到目标流形上，以解决分布不匹配问题。此外，为了从人类角度量化隐私泄漏，本文还提出了两种基于大型视觉语言模型的指标。", "result": "实验表明FIA-Flow在多种模型和层上实现了更忠实、语义一致的特征反转，揭示了比先前认识到的更为严重的隐私风险。", "conclusion": "实验结果表明，FIA-Flow能在多个模型和层中实现忠实且语义对齐的特征反转，揭示了分割DNN中存在的严重隐私威胁。"}}
{"id": "2511.15322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15322", "abs": "https://arxiv.org/abs/2511.15322", "authors": ["Zahra Farzadpour", "Masoumeh Azghani"], "title": "Adaptive thresholding pattern for fingerprint forgery detection", "comment": "25 pages, 10 figures, Journal paper", "summary": "Fingerprint liveness detection systems have been affected by spoofing, which is a severe threat for fingerprint-based biometric systems. Therefore, it is crucial to develop some techniques to distinguish the fake fingerprints from the real ones. The software based techniques can detect the fingerprint forgery automatically. Also, the scheme shall be resistant against various distortions such as noise contamination, pixel missing and block missing, so that the forgers cannot deceive the detector by adding some distortions to the faked fingerprint. In this paper, we propose a fingerprint forgery detection algorithm based on a suggested adaptive thresholding pattern. The anisotropic diffusion of the input image is passed through three levels of the wavelet transform. The coefficients of different layers are adaptively thresholded and concatenated to produce the feature vector which is classified using the SVM classifier. Another contribution of the paper is to investigate the effect of various distortions such as pixel missing, block missing, and noise contamination. Our suggested approach includes a novel method that exhibits improved resistance against a range of distortions caused by environmental phenomena or manipulations by malicious users. In quantitative comparisons, our proposed method outperforms its counterparts by approximately 8% and 5% in accuracy for missing pixel scenarios of 90% and block missing scenarios of size 70x70 , respectively. This highlights the novelty approach in addressing such challenges.", "AI": {"tldr": "A new adaptive thresholding pattern for detecting fingerprint forgery is presented, which shows improved accuracy and resistance against distortions compared to existing methods.", "motivation": "The motivation is to enhance fingerprint liveness detection systems by mitigating spoofing threats through a distortion-resistant software-based solution.", "method": "The paper proposes a fingerprint forgery detection algorithm based on adaptive thresholding. It involves three levels of wavelet transform on anisotropically diffused images. The feature vectors, formed from adaptively thresholded wavelet coefficients, are classified using SVM.", "result": "The proposed method outperforms other methods by 8% and 5% in accuracy for 90% pixel missing and 70x70 block missing scenarios, respectively.", "conclusion": "The paper concludes that their novel approach improves resistance against various distortions and enhances the detection of fingerprint forgery compared to existing techniques."}}
{"id": "2511.15343", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15343", "abs": "https://arxiv.org/abs/2511.15343", "authors": ["Spyridon Loukovitis", "Vasileios Karampinis", "Athanasios Voulodimos"], "title": "Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection", "comment": null, "summary": "Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.", "AI": {"tldr": "本文提出了一种轻量级的后处理框架，旨在区分未知对象和背景，并改进开放集检测性能。该框架使用多层感知器合并多个置信度估计值，实现实时三分类：已知目标、未知对象和背景。实验表明，该方法在AUROC和mAP方面优于当前基准。", "motivation": "传统的无人机导航系统中物体检测方法通常只处理封闭集检测，对于未知物体的检测能力有限。本研究的动机在于提高对已知和未知物体同时进行准确检测的能力，以增强无人机导航的安全性和可靠性。", "method": "本文提出了一种轻量级、模型无关的后处理框架，通过多层感知器整合多个置信度估计和检测特征进行决策，从而在开放集检测中实现实时三分类。", "result": "实验结果表明，该方法在二分类和三分类下的性能均有所提高，其中二分类的AUROC平均提高了2.7%，并且开放集mAP保持或有所改善。同时，这项技术也提高了封闭集mAP，相对增益可达18%。", "conclusion": "该框架相比现有的方法在多种数据集上AUROC的比较分析中都有优异的表现，其在保留现有性能的同时增强了三分类能力，为无人机的导航安全提供了更为可靠的检测方法。"}}
{"id": "2511.15369", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15369", "abs": "https://arxiv.org/abs/2511.15369", "authors": ["Gihwan Kim", "Jemin Lee", "Hyungshin Kim"], "title": "IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers", "comment": "accepted in WACV 2026 (10 pages)", "summary": "Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\\%p (avg. 1.78\\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.", "AI": {"tldr": "本文介绍了 IPTQ-ViT，一种用于无需重新训练即可实现完全整数推理视觉转换器的新PTQ框架，通过引入多项式GELU和位移Softmax的近似函数以及统一的量化优化指标，取得了比现有PTQ和部分浮点方法更高的精度和可观的性能提升。", "motivation": "先前的QAT方法依赖于昂贵的重新训练来恢复非线性层量化造成的精度损失，这限制了它们在资源受限环境中的应用。现有PTQ方法要么部分量化非线性功能，要么调整激活分布以保持精度，但未能实现完全整数推理。", "method": "引入了IPTQ-ViT，这是一种无需重新训练即可实现完全整数推理的视觉转换器的新型PTQ框架。提出了针对视觉数据优化的多项式GELU近似函数和基于位移的Softmax近似函数。还提出了一种结合量化敏感性、扰动和计算成本的统一指标来选择每个激活层的最佳近似函数。", "result": "IPTQ-ViT 在图像分类中实现了最高达6.44\\%p（平均1.78\\%p）的top-1精度提升，在目标检测中实现了1.0 mAP的提升。它在W8A8和W4A8下优于部分浮点PTQ方法，其精度和延迟与整数化的QAT方法相当。", "conclusion": "IPTQ-ViT展示了在不牺牲精度的前提下在资源受限环境下部署转换器模型的潜力。"}}
{"id": "2511.15379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15379", "abs": "https://arxiv.org/abs/2511.15379", "authors": ["Yunjiao Zhou", "Xinyan Chen", "Junlang Qian", "Lihua Xie", "Jianfei Yang"], "title": "Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training", "comment": null, "summary": "Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.", "AI": {"tldr": "本文提出了一种零样本、开放词汇框架ZOMG，可以将运动序列分解为语义上有意义的子动作，而无需任何注释或微调。实验结果展示了其在运动接地性能方面的优势，相较于先前的方法在HumanML3D基准测试中提高了+8.7% mAP。", "motivation": "理解复杂的人类活动需要能够将动作分解为细粒度、语义对齐的子动作，但现有的大多数方法依赖于密集的监督和预定义的动作类别，这在开放词汇和真实世界的设置中是不切实际的。", "method": "ZOMG框架整合了语言语义划分技术和软掩码优化技术，前者使用大型语言模型将指令分解为有序的子动作单元，后者通过学习实例特定的时间掩码来聚焦于对子动作至关重要的帧，同时保持段内连续性和段间分离性。整个过程不需要对预训练的编码器进行任何修改。", "result": "实验在三个动作-语言数据集上的结果显示，该方法在动作接地性能上达到了最先进的效果，并在检索任务中也显示出显著改进。", "conclusion": "ZOMG框架在没有任何注释的情况下实现了强大的动作理解和分割能力，开创了一个新的无需注释的动作理解范式。"}}
{"id": "2511.15390", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15390", "abs": "https://arxiv.org/abs/2511.15390", "authors": ["Haidong Kang", "Lihong Lin", "Enneng Yang", "Hongning Dai", "Hao Wang"], "title": "Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \\textit{huge labor costs} and \\textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \\textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \\textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.", "AI": {"tldr": "提出AutoPrune方法，利用LLMs实现自我剪枝优化，解决了手工设计剪枝算法的高成本及一致稀疏性下的性能退化问题。", "motivation": "动机在于解决现有模型剪枝技术依赖手动设计算法导致的高劳动力成本和需要专业知识的问题，并首次指出一致稀疏性下的严重性能退化问题。", "method": "AutoPrune方法首次利用大规模语言模型自身设计最优剪枝算法，无需专家知识。具体来说，通过Graph-driven Chain-of-Thought (GCoT) 优化提示，增强推理过程，并提出Skew-aware Dynamic Sparsity Allocation (SDSA) 策略来解决异常值问题，以缓解高剪枝比率下的性能下降。", "result": "实验结果表明，AutoPrune在主流的大规模语言模型基准测试中超越了最先进的竞争对手，表现优异。", "conclusion": "AutoPrune能够自动设计最优剪枝算法，自我优化，并能有效处理高剪枝比率下的异常值问题，证明了其在大规模语言模型压缩方面的有效性。"}}
{"id": "2511.15396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15396", "abs": "https://arxiv.org/abs/2511.15396", "authors": ["Simon Boeder", "Fabian Gigengack", "Simon Roesler", "Holger Caesar", "Benjamin Risse"], "title": "ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation", "comment": null, "summary": "Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.", "AI": {"tldr": "ShelfOcc 方法通过视频实现3D场景理解，无需LiDAR，克服了现有的自监督和弱监督占用估计方法的几何不一致性和深度渗入问题，并在Occ3D-nuScenes基准上优于所有先前的弱监督方法。", "motivation": "该研究旨在克服现有的基于2D投影或渲染监督的自监督和弱监督占用估计方法的固有限制，包括几何不一致性和深度渗入问题。提出了一种仅基于视觉的方法，以克服这些问题，而不依赖于LiDAR。", "method": "ShelfOcc, 一种仅基于视觉的方法，通过从视频中生成度量一致的语义体素标签来实现真正的3D监督，无需依赖LiDAR或手动3D注释。该方法引入了一个专门的框架来解决最近基于视觉的3D几何基础模型在动态驾驶场景中面临的稀疏或噪声几何问题，通过过滤并跨帧一致地积累静态几何信息，处理动态内容并将语义信息传播到稳定的体素表示中。", "result": "在Occ3D-nuScenes基准上，ShelfOcc显著优于所有先前的弱监督/自监督方法，相对改进高达34％。", "conclusion": "ShelfOcc 提供了一种新的数据驱动的方式进行无LiDAR的3D场景理解，显示了高质量监督对于实现健壮的占用学习的重要性，并作为架构创新的一个重要补充途径。"}}
{"id": "2511.15406", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15406", "abs": "https://arxiv.org/abs/2511.15406", "authors": ["Luca Mossina", "Corentin Friedrich"], "title": "Controlling False Positives in Image Segmentation via Conformal Prediction", "comment": null, "summary": "Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.", "AI": {"tldr": "研究开发了一种后处理框架，可为预训练分割模型生成具有统计保证的置信度掩模，以降低假阳性的风险，适用于医疗影像分割等临床场景。", "motivation": "在临床决策中，可靠的语义分割至关重要，然而深度学习模型很少能够提供有关错误的明确统计保证。此研究旨在解决这一问题，通过引入一种后处理框架来弥补这一不足。", "method": "研究提出了一种后处理框架，该框架能够为任何预训练的分割模型生成置信度掩模，并提供图像级别的错误控制，从而确保假阳性预测比例低于用户指定的容忍度，同时提供有限样本的统计保证。该方法经过形态学腐蚀或提高评分阈值实现掩模收缩，并通过带有标签的校准数据集选择收缩参数，从而实现无模型依赖且无需重新训练的分割置信度提升。", "result": "实验结果表明，该方法在息肉分割基准上能够实现目标水平的经验有效性，展示了其在临床应用中减少过分割风险的潜力。", "conclusion": "该框架能够在过分割可能导致临床后果的应用场景中实现实用的、风险意识的分割，提供一种无需重新训练且模型无关的方法。"}}
{"id": "2511.15411", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15411", "abs": "https://arxiv.org/abs/2511.15411", "authors": ["Wenlun Zhang", "Yunshan Zhong", "Zihao Ding", "Xinyu Li", "Kentaro Yoshioka"], "title": "D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models", "comment": null, "summary": "Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.", "AI": {"tldr": "本文提出了D4C框架，解决了直接将DFQ应用于CLIP所导致的性能下降问题，并通过实验展示了在多个数据集上的显著性能提升。", "motivation": "现有DFQ技术直接应用于CLIP会导致性能大幅下降，主要是因为生成样本的语义内容不足和结构多样性低。为此，我们提出了D4C框架。", "method": "提出了D4C（DFQ for CLIP），一种专为CLIP设计的无数据量化框架，通过三个关键组件解决语义丰富性和结构多样性问题：1) 提示引导的语义注入；2) 结构对比生成；3) 扰动感知增强。", "result": "D4C显著提升了CLIP模型的性能。例如，在W4A8设置下，CLIP ResNet-50和ViT-B/32分别在CIFAR-10上提高了Top-1准确率12.4%和18.9%，在CIFAR-100上提高了6.8%和19.7%，在ImageNet-1K的零样本分类上提高了1.4%和5.7%。", "conclusion": "D4C通过生成语义丰富且结构多样的图像样本，有效缩小了DFQ在CLIP上的性能差距，证明了在多种比特宽度和模型上的有效性。"}}
{"id": "2511.15429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15429", "abs": "https://arxiv.org/abs/2511.15429", "authors": ["Marc-Emmanuel Coupvent des Graviers", "Hejer Ammar", "Christophe Guettier", "Yann Dumortier", "Romaric Audigier"], "title": "WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes", "comment": "Accepted at CAID (Conference on Artificial Intelligence for Defence)", "summary": "We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.", "AI": {"tldr": "WarNav is a new dataset for training autonomous vehicles to navigate war zones, providing a gap between urban driving datasets and dangerous, unstructured environments, with a focus on ethical considerations and performance baselines.", "motivation": "The motivation is to address the gap between conventional urban driving datasets and the needs for autonomous systems in hazardous and damaged war-zones, thereby enabling the development and benchmarking of semantic segmentation models.", "method": "We introduce WarNav, a novel real-world dataset for semantic segmentation models targeting autonomous ground vehicle navigation in conflict-affected environments. The dataset is constructed from images in the DATTALION repository, and the authors discuss methodological challenges and ethical considerations.", "result": "Baseline results on WarNav using state-of-the-art semantic segmentation models trained on structured urban scenes are reported, and the impact of training data environments is analyzed.", "conclusion": "The conclusion targets enhancing the robustness and safety of autonomous vehicles in high-risk scenarios with limited annotated data, fostering impactful research in this area."}}
{"id": "2511.15433", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15433", "abs": "https://arxiv.org/abs/2511.15433", "authors": ["YiKang Shao", "Tao Shi"], "title": "Representation Space Constrained Learning with Modality Decoupling for Multimodal Object Detection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Multimodal object detection has attracted significant attention in both academia and industry for its enhanced robustness. Although numerous studies have focused on improving modality fusion strategies, most neglect fusion degradation, and none provide a theoretical analysis of its underlying causes. To fill this gap, this paper presents a systematic theoretical investigation of fusion degradation in multimodal detection and identifies two key optimization deficiencies: (1) the gradients of unimodal branch backbones are severely suppressed under multimodal architectures, resulting in under-optimization of the unimodal branches; (2) disparities in modality quality cause weaker modalities to experience stronger gradient suppression, which in turn results in imbalanced modality learning. To address these issues, this paper proposes a Representation Space Constrained Learning with Modality Decoupling (RSC-MD) method, which consists of two modules. The RSC module and the MD module are designed to respectively amplify the suppressed gradients and eliminate inter-modality coupling interference as well as modality imbalance, thereby enabling the comprehensive optimization of each modality-specific backbone. Extensive experiments conducted on the FLIR, LLVIP, M3FD, and MFAD datasets demonstrate that the proposed method effectively alleviates fusion degradation and achieves state-of-the-art performance across multiple benchmarks. The code and training procedures will be released at https://github.com/yikangshao/RSC-MD.", "AI": {"tldr": "This paper investigates the problem of fusion degradation in multimodal object detection, identifying issues with gradient suppression and modality quality disparity, and proposes RSC-MD to improve performance.", "motivation": "The motivation is to address the less explored issue of fusion degradation in multimodal object detection by identifying key optimization problems not solved by current fusion strategies.", "method": "The paper proposes a Representation Space Constrained Learning with Modality Decoupling (RSC-MD) method to address the optimization deficiencies in fusion degradation of multimodal object detection, consisting of an RSC module to amplify the suppressed gradients and an MD module to eliminate inter-modality coupling and imbalance.", "result": "Extensive experiments on various datasets (FLIR, LLVIP, M3FD, MFAD) show that the proposed RSC-MD method effectively alleviates fusion degradation and outperforms existing methods.", "conclusion": "The RSC-MD method is effective in improving the performance of multimodal object detection by addressing the issues of gradient suppression and modality imbalance. The developed approach thus significantly enhances multimodal detection capabilities."}}
{"id": "2511.15435", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.15435", "abs": "https://arxiv.org/abs/2511.15435", "authors": ["Linyin Luo", "Yujuan Ding", "Yunshan Ma", "Wenqi Fan", "Hanjiang Lai"], "title": "HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation", "comment": null, "summary": "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.", "AI": {"tldr": "本研究提出了一种新颖的针对多模态检索增强生成技术的分层视觉攻击方法，通过在图像输入添加细微扰动来误导系统，显著降低了系统的检索和生成性能。", "motivation": "尽管MRAG技术增强了大型多模态模型的能力，但也带来了新的安全问题。本研究旨在研究通过视觉攻击来破坏MRAG系统的可行性，探索新颖攻击方法，以发现并改进系统潜在的脆弱性。", "method": "本研究提出了一种新颖的分层视觉攻击方法，通过在用户图像输入中添加微小的不可察觉的扰动，来误导检索器召回无关知识，从而混淆生成器。此方法分为两个阶段：首先破坏跨模态对齐，然后破坏多模态语义对齐。", "result": "实验结果证明了此视觉攻击的有效性，在OK-VQA和InfoSeek两个数据集上，使用CLIP为基础的检索器和BLIP-2及LLaVA生成器时，系统性能显著下降。", "conclusion": "研究表明，通过运用分层视觉攻击策略，即使是最先进的多模态模型也存在一定的脆弱性，这提醒了设计时需要进一步注重安全性的考虑。"}}
{"id": "2511.15440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15440", "abs": "https://arxiv.org/abs/2511.15440", "authors": ["Johannes C. Bauer", "Paul Geng", "Stephan Trattnig", "Petr Dokládal", "Rüdiger Daub"], "title": "A Dataset and Baseline for Deep Learning-Based Visual Quality Inspection in Remanufacturing", "comment": null, "summary": "Remanufacturing describes a process where worn products are restored to like-new condition and it offers vast ecological and economic potentials. A key step is the quality inspection of disassembled components, which is mostly done manually due to the high variety of parts and defect patterns. Deep neural networks show great potential to automate such visual inspection tasks but struggle to generalize to new product variants, components, or defect patterns. To tackle this challenge, we propose a novel image dataset depicting typical gearbox components in good and defective condition from two automotive transmissions. Depending on the train-test split of the data, different distribution shifts are generated to benchmark the generalization ability of a classification model. We evaluate different models using the dataset and propose a contrastive regularization loss to enhance model robustness. The results obtained demonstrate the ability of the loss to improve generalisation to unseen types of components.", "AI": {"tldr": "研究提出了一种新的图像数据集来评估分类模型在面对不同变速箱零件及缺陷模式下的泛化能力，并提出了一种对比正则化损失来提高模型对未见过的零部件类型的鲁棒性。", "motivation": "目前，再制造过程中关键的质量检测步骤主要依赖人工，由于零件和缺陷模式的多样性，深度神经网络在自动化视觉检测方面难以实现泛化，因此研究旨在改进这一点。", "method": "研究构建了一个包含两个汽车变速箱典型齿轮组件图像的数据集，并使用对比正则化损失来增强分类模型的鲁棒性。", "result": "实验结果表明，所提出的对比正则化损失能够提高模型对未见过的零件类型的泛化性能。", "conclusion": "研究证明了对比正则化损失在提高模型在零件质量和缺陷检测任务中的泛化性能方面是有效的。"}}
{"id": "2511.15459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15459", "abs": "https://arxiv.org/abs/2511.15459", "authors": ["Ziyan Liu", "Qi Su", "Lulu Tang", "Zhaofei Yu", "Tiejun Huang"], "title": "Driving in Spikes: An Entropy-Guided Object Detector for Spike Cameras", "comment": null, "summary": "Object detection in autonomous driving suffers from motion blur and saturation under fast motion and extreme lighting. Spike cameras, offer microsecond latency and ultra high dynamic range for object detection by using per pixel asynchronous integrate and fire. However, their sparse, discrete output cannot be processed by standard image-based detectors, posing a critical challenge for end to end spike stream detection. We propose EASD, an end to end spike camera detector with a dual branch design: a Temporal Based Texture plus Feature Fusion branch for global cross slice semantics, and an Entropy Selective Attention branch for object centric details. To close the data gap, we introduce DSEC Spike, the first driving oriented simulated spike detection benchmark.", "AI": {"tldr": "针对脉冲相机数据提出了EASD，一种双分支设计方法，填补了数据空白，并引入了DSEC Spike基准。", "motivation": "传统的图像检测方法难以处理由于快速运动和极端光照引起的运动模糊和饱和问题，而脉冲相机虽然能够提供微秒级延迟和超高的动态范围，但其稀疏离散的输出无法被标准图像检测器处理。", "method": "一种基于双分支设计的方法EASD，用于处理脉冲相机输出：一个基于时间的纹理加特征融合分支用于全局跨切片语义，另一个是熵选择性注意分支用于对象中心细节。", "result": "提出了一种处理脉冲相机数据的解决方案，填补了数据空白，并推出了首个驾驶导向的模拟脉冲检测基准DSEC Spike。", "conclusion": "所提出的方法能够有效转换脉冲相机的稀疏离散输出，适用于自动驾驶中的对象检测。"}}
