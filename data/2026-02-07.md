<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations](https://arxiv.org/abs/2602.04982)
*Deepak Gupta,Davis Bartels,Dina Demner-Fuhsman*

Main category: cs.CL

> 研究提出了BioACE框架，用于评估生物医学领域中LLM生成的文本质量和引用的准确性，并通过实验确定了最佳评估方法。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型(LLM)在生成生物医学问题答案方面的广泛应用，评估生成答案及其引用的质量变得至关重要。生物医学领域的文本生成评估面临挑战，因为它需要专家评估来验证文本是否符合科学文献并使用复杂的医学术语。

**Method:** 提出了一个名为BioACE的自动化框架，用于评估生物医学问题答案及其引用的准确性。该框架考虑了多个方面，包括答案的完整性、正确性、精确性和召回率，以评估答案与事实的吻合度。同时，该研究引入了多种现有方法，如自然语言推理(NLI)和预训练语言模型，来评估用于支持生成答案的引用文献的质量。

**Result:** 通过详尽的实验和分析，为BioACE评估包提供了最佳的答案和引文评估方法。实验结果显示了这些自动化评估方法与人类评估的相关性。

**Conclusion:** BioACE是一个评估生物医药领域生成答案和引用质量的自动化框架，该研究为该框架提供了多种评估方法，并验证了这些方法的有效性。

**Abstract:** With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.

</details>


### [2] [CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System](https://arxiv.org/abs/2602.05004)
*Zexin Lin,Jiachen Yu,Haoyang Zhang,Yuzhao Li,Zhonghang Li,Yujiu Yang,Junjie Wang,Xiaoqiang Ji*

Main category: cs.CL

> 提出了CoWork-X框架，该框架通过技能代理和协同优化器实现跨回合闭合优化，解决了强协作任务中的延迟和执行困难问题。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在条件化互动环境中启用语言代理，但在需要亚秒级实时协调和持续多回合适应的强协作任务中，现有方法或产生延迟和时间抖动，或提供难以转化为可靠低成本执行的非结构化文本改进。

**Method:** 本文提出了CoWork-X框架，该框架将同伴协作视为跨越多个回合的闭合优化问题，其灵感来源于快速-慢速记忆分离的概念。该框架包含了通过基于分层任务网络（HTN）的技能检索执行任务的技能代理，以及在回合后进行带显式预算约束和偏移规整化的技能整合的协同优化器。

**Result:** 实验表明，在Overcooked-AI类实时协作的挑战性基准测试中，CoWork-X使性能稳步提升，同时减少了在线延迟和token使用。

**Conclusion:** CoWork-X能够在减少在线延迟和token使用的前提下实现稳健和累积的性能提升。

**Abstract:** Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.

</details>


### [3] [Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation](https://arxiv.org/abs/2602.05035)
*Sean Trott,Pamela D. Rivière*

Main category: cs.CL

> 研究表明，多语种语言模型在词汇消歧上的性能损耗可能由于其多种容量限制因素。

<details>
  <summary>Details</summary>

**Motivation:** 探究多语种语言模型在某些任务上表现不如单语种模型的原因。

**Method:** 通过对比同一模型家族中的单语种和多语种语言模型在含糊词汇消歧任务上的表现，分析多语种性能损耗的原因。

**Result:** 多语种语言模型在某些任务上的表现不如单语种模型，这可能与其容量限制有关。本文通过控制数据集对英语和西班牙语中含糊词汇的人类相关性判断，量化了‘多语种性能损耗’。研究发现，与同系列中的单语种模型相比，多语种语言模型在词汇消歧任务上的表现一致较差。研究还探讨了三种潜在的容量限制：表示能力限制（嵌入等向性降低）、注意力限制（对消歧线索的注意力降低）以及词汇相关限制（多词分隔增加）。结果显示，多语种语言模型确实存在这些限制，并且这些限制与较差的消歧性能有关。

**Conclusion:** 多语种语言模型在处理含糊词汇的消歧任务上表现出性能损耗，这与模型的容量限制有关。

**Abstract:** Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.

</details>


### [4] [Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories](https://arxiv.org/abs/2602.05085)
*Sidi Lu,Zhenwen Liang,Dongyang Ma,Yan Wang,Haitao Mi,Dong Yu*

Main category: cs.CL

> 本论文提出了一种名为Locas的新类型参数化记忆机制，可用于改进模型在持续学习任务中的性能，同时保持较小的上下文窗口和防止灾难性遗忘。

<details>
  <summary>Details</summary>

**Motivation:** 本论文旨在将测试时的训练与一种新的参数化记忆结合起来，这种记忆可以灵活地卸载到或合并到模型参数中，从而提升模型在持续学习任务中的能力。

**Method:** 本论文提出了一种名为Locas的局部支持参数化记忆机制，该机制可以灵活地从模型参数中卸载或合并。提出两种主要的Locas变体：一种是传统的两层MLP设计，另一种与顶尖的LLMs共享相同的GLU-FFN结构，均可以被轻松添加到现有的模型中以实现参数和计算的高效持续学习。论文着重讨论了正确的初始化方式，通过再利用模型参数、激活值或梯度来对低秩侧向FFN记忆模块执行有原则的原则初始化，这对于快速收敛、改进泛化和防止灾难性遗忘至关重要。

**Result:** 实验结果表明，即使在最低的情况下，Locas-GLU仅增加了0.02%的额外参数，就能够存储过去上下文的信息，同时保持较小的上下文窗口。此外，通过比较MMLU评估，发现Locas在记忆整本书后能将过去上下文永久化为参数知识，而不会大幅降低模型现有内部知识的表现。

**Conclusion:** 提出的记忆机制展示了强大的潜在能力，能够将过去的上下文转换为参数化知识，并能够最小化模型内部知识的灾难性遗忘。

**Abstract:** In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.

</details>


### [5] [Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models](https://arxiv.org/abs/2602.05106)
*Michael Browder,Kevin Duh,J. David Harris,Vince Lyzinski,Paul McNamee,Youngser Park,Carey E. Priebe,Peter Viechnicki*

Main category: cs.CL

> 论文提出了一种新方法DKPS，用于分析和保证使用transformer模型生成的合成数据的质量，特别是在标注数据不足的情况下。

<details>
  <summary>Details</summary>

**Motivation:** 由于标注训练数据的稀缺，以及通过黑盒方式难以预测合成数据的性质，使得语言技术和生成AI模型的性能面临挑战。通过DKPS提供一个解决办法，缓解这一问题。

**Method:** 提出了Data Kernel Perspective Space (DKPS), 用于提供数学分析的基础，以获得关于transformer模型输出质量的具体统计保证。

**Result:** 展示了DKPS的数学推导及其如何提供性能保证，并说明了DKPS的性能保证如何有助于阐明下游任务的表现，比如神经机器翻译模型或使用对比偏好优化训练的LLM。

**Conclusion:** 讨论了当前工作的局限性以及未来研究的方向。这项工作的成果是在理解和量化由LLM生成的合成数据质量方面迈出了第一步。

**Abstract:** Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.

</details>


### [6] [Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text](https://arxiv.org/abs/2602.05107)
*Ahmed Ruby,Christian Hardmeier,Sara Stymne*

Main category: cs.CL

> 研究提出一种多模态方法，结合文本和音频信息，改善隐式话语关系分类任务在多语言环境中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 隐式话语关系分类任务极具挑战性，需要从上下文中推断意义。文本中的上下文线索可以在不同模式中分布并跨语言变化，单独依靠文本并不总是能够捕捉到这些线索。

**Method:** 我们提出了一种多模态方法，该方法通过Qwen2-Audio整合文本和音频信息，实现了跨语言隐式话语关系分类的联合建模。

**Result:** 研究发现，基于文本的模型比基于音频的模型性能更好，但整合两种模态可以提高性能，并且跨语言迁移可以为低资源语言带来显著改进。

**Conclusion:** 该研究通过结合文本和音频信息，改进了跨语言隐式话语关系的分类，并表明跨语言迁移对低资源语言有所帮助。

**Abstract:** Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.

</details>


### [7] [GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek](https://arxiv.org/abs/2602.05150)
*Yang Zhang,Mersin Konomi,Christos Xypolopoulos,Konstantinos Divriotis,Konstantinos Skianis,Giannis Nikolentzos,Giorgos Stamou,Guokan Shang,Michalis Vazirgiannis*

Main category: cs.CL

> 本文提出了一个名为GreekMMLU的基准测试，用于评估希腊语大型语言模型。该测试基于真实多选题数据库，评估结果显示不同语言模型间的性能差异及影响因素。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的希腊语评估基准大多基于从英语机器翻译的内容，这些基准未能捕捉到希腊语的语义和文化特征，因此研究团队希望创建一个基于希腊本土内容的可靠的评估基准。

**Method:** 构建了一个名为GreekMMLU的多任务语言理解基准测试，包含21,805个多选题，涵盖了45个学科领域，这些问题源自或由希腊语编写，从学术、专业和政府考试中提取。测试分为公开部分（16,857个样本）和保密部分（4,948个样本）用于评估。

**Result:** 评估了80多个开源和闭源的大型语言模型，结果显示前沿模型与开放权重模型之间的性能存在显著差距，并且本土适应模型比通用多语言模型表现更好。

**Conclusion:** 通过系统地分析影响性能的因素，比如模型规模、适应性、和提示工程，得出了可以提高希腊语大型语言模型能力的见解。

**Abstract:** Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.

</details>


### [8] [Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems](https://arxiv.org/abs/2602.05176)
*Ziyuan Yang,Wenxuan Ding,Shangbin Feng,Yulia Tsvetkov*

Main category: cs.CL

> 研究发现恶意模型会对多语言模型系统造成严重影响，尤其是在推理和安全领域，性能分别下降7.12%和7.94%。提出了一些缓解策略，可以恢复约95.31%的初始性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究在分布式多语言模型合作系统中，部分模型被恶意操控的风险，并量化恶意模型的影响。

**Method:** 通过设计四种类型的恶意语言模型，并将它们植入四种流行的模型协作系统中，进而评估这些被恶意模型影响的系统在十个数据集上的表现。

**Result:** 结果表明，通过引入外部监督来阻止或屏蔽恶意模型，能够减轻其负面影响，并恢复大约95.31%的初始性能。

**Conclusion:** 尽管策略能在很大程度上缓解恶意模型的影响，但使多语言模型合作系统完全抵御恶意模型仍然是一个开放的研究课题。

**Abstract:** Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

> 本文提出SIDeR，一种语义驱动的面部隐私保护框架，它通过将人脸图像分解和重组生成视觉匿名的对抗样本，同时保持机器身份一致性，并能在授权下恢复图像。实验表明它在隐私保护和图像质量上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 由于人脸识别在在线银行、身份验证等网络服务中的深度整合，因此，在图像存储和传输过程中实现身份信息与视觉表现的有效解耦，成为隐私保护的关键挑战。

**Method:** SIDeR提出了一种基于语义解耦的无限制面部隐私保护框架。该框架将人脸图像分解为机器可识别的身份特征向量和可视感知语义外观组件。通过语义引导的潜在扩散模型重组，它生成视觉匿名的对抗人脸，同时保持机器级身份一致性。框架使用了动量驱动的无限制扰动优化和语义-视觉平衡因子，来合成多个视觉多样化的高自然度对抗样本。

**Result:** 在CelebA-HQ和FFHQ数据集上的广泛实验表明，SIDeR在黑盒攻击场景中达到了99%的成功率，并且在基于PSNR的恢复质量上比基准方法提高了41.28%。

**Conclusion:** SIDeR框架提供了一种新的方法来保护面部隐私，其利用语义引导重组技术在保持机器识别的一致性的同时实现视觉匿名性，并且能够通过正确密码恢复原始图像。

**Abstract:** With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

</details>


### [10] [UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking](https://arxiv.org/abs/2602.05037)
*Bishoy Galoaa,Xiangyu Bai,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

> UniTrack提供了一种无需修改架构即可集成的图理论损失函数，用于提升多目标跟踪模型的性能。研究表明，它在多家长测试模型上实现了显著的身份交换减少和IDF1提升，特别是在GTR模型上的SportsMOT基准表现尤为突出。

<details>
  <summary>Details</summary>

**Motivation:** 开发UniTrack的动机是为了提高多目标跟踪的性能。研究者们希望设计一种可以直接优化追踪特定目标的插件式损失函数，该函数能够提高跟踪准确性，同时能够兼容现有的MOT系统。

**Method:** UniTrack采用图理论损失函数，直接优化多目标跟踪（MOT）性能，其特点是整合检测准确性、身份保持和时空一致性到一个端到端的可训练损失函数中。不同于先前需要重新设计跟踪架构的基于图的方法，UniTrack可以无缝集成到现有的MOT系统中，没有架构上的改动需求。通过可微图表示学习，UniTrack使网络能够学习跨帧的运动连续性和身份关系的整体表示。

**Result:** UniTrack在多种追踪模型和多个具有挑战性的基准测试中进行了验证，结果表明在所有测试架构和数据集上都有显著提升，包括Trackformer, MOTR, FairMOT, ByteTrack, GTR和MOTE。广泛评估显示，身份交换减少高达53%，IDF1提升12%。在SportsMOT上，GTR的MOTA性能提升达到9.7%的峰值。

**Conclusion:** 研究表明，UniTrack可以作为一个强有力的工具，通过统一的可微学习目标，直接优化多目标跟踪任务中的关键目标，提升整体MOT性能，尤其在身份保持和运动准确性方面。

**Abstract:** We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.

</details>


### [11] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

> 该研究提出了一种无需修改架构或收集额外数据就能提高Vision-Language-Action模型视觉条件和任务表现的训练方法。

<details>
  <summary>Details</summary>

**Motivation:** 受到视觉依赖性成功的观察启发，该方法旨在明确地增强Vision-Language-Action模型中的视觉条件。

**Method:** 我们提出了一种训练框架，该框架通过偏好优化在轨迹跟踪代理任务上使动作预测与视觉输入对齐，并通过监督微调期间的潜空间蒸馏将增强的对齐转移到指令跟随任务上。

**Result:** 该方法提高了离散OpenVLA的视觉条件和任务表现，并且扩展到连续OpenVLA-OFT设置时也保持了一致的提升效果。

**Conclusion:** 通过可视化条件增强了Vision-Language-Action模型的视觉对齐能力，而无需对架构进行修改或收集额外的数据。

**Abstract:** Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

</details>


### [12] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

> 该文探讨了基于图像的饮食评估策略，尤其是准确估算食物份量的方法。

<details>
  <summary>Details</summary>

**Motivation:** 依赖图像进行饮食评估是准确且便捷地监测个人健康的重要策略，对于预防和管理慢性疾病及肥胖症具有重要意义。然而，从二维图像中估计食物的三维大小是一个重要挑战。

**Method:** 研究了多种策略以克服这一局限，包括使用辅助输入如深度图、多视角输入，以及模板匹配等模型方法。深度学习技术也被使用，可以单独使用单目图像或结合辅助输入进行准确的份量预测。

**Result:** 无具体实验结果分享，主要从策略上讨论了不同的图像处理和深度学习方法。

**Conclusion:** 文中分析了几种准确估算食物份量的不同策略，对基于图像的饮食评估领域提供了指导。

**Abstract:** Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

</details>


### [13] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

> This paper presents Visual Concept Ranking (VCR), a method to uncover shortcomings in large multimodal models (LMMs) used in medical tasks, such as classifying skin lesions, by investigating the models' responses to different prompts and demographic subgroups.

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of machine learning models in safety-critical domains like healthcare by identifying gaps in model performance and understanding the underlying visual features that influence predictions.

**Method:** The paper introduces Visual Concept Ranking (VCR), a method for identifying important visual concepts within large multimodal models. VCR is used to analyze model behavior when prompted with medical tasks involving different demographic groups.

**Result:** The study reveals unexpected gaps in performance across different demographic subgroups in medical tasks using LMMs. VCR successfully generates hypotheses about visual feature dependencies, which are confirmed through manual interventions.

**Conclusion:** VCR can be used to improve the understanding of large multimodal models' performance in healthcare tasks, particularly in identifying and mitigating biases across different demographic groups.

**Abstract:** Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

</details>


### [14] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

> CLEAR-HPV is a framework that enhances morphologic interpretability in attention-based multiple instance learning (MIL) for HPV-related cancers by discovering key tissue concepts.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to provide better morphologic interpretability to attention-based MIL for HPV-related whole-slide histopathology, which previously lacked detailed morphologic explanations.

**Method:** The method employed is called CLEAR-HPV, which restructures the MIL latent space using attention to automatically discover and map morphologic concepts such as keratinizing, basaloid, and stromal features.

**Result:** The result is the generation of compact concept-fraction vectors that reduce high-dimensional data to 10 interpretable concepts while maintaining predictive performance across various datasets.

**Conclusion:** The conclusion is that CLEAR-HPV provides a general, consistent, and backbone-agnostic approach to improve the interpretability of attention-based MIL models in cancer histopathology analysis.

**Abstract:** Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

</details>


### [15] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

> ARGaze方法通过在变压器解码器中加入过去的凝视目标估计窗口，提出了一个改进的第一人称凝视估计模型，实现了更好的在线性能，并证明了有界凝视历史的自回归建模对于稳健预测至关重要。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是解决在线自我中心凝视估计的问题，这是一种利用摄像机佩戴者过去和当前的视频帧来预测其视觉注意力的技术，对增强现实和辅助技术至关重要。这一任务的特点是没有明确的人脸或眼部信号，需要模型从间接线索中推断出视觉注意力。

**Method:** ARGaze方法通过将凝视估计重新定义为顺序预测问题来预测从第一人称视频中获取的摄像机佩戴者的视觉注意力。具体来说，它利用当前的视觉特征和一个固定长度的最近凝视目标估计窗口，使用变压器解码器来进行预测。这种方法通过设定因果性和资源有限的流式推理来实现这种预测。

**Result:** ARGaze方法在多个自我中心基准测试中实现了最先进（SOTA）的性能，并且在一系列消融实验中验证了使用有界凝视历史的自回归模型对于准确的凝视预测至关重要。

**Conclusion:** 通过对在线自我中心凝视估计任务的研究，ARGaze通过利用过去的凝视历史和当前的视觉信息的自回归模型，实现了在资源有限的情况下进行流式处理的能力，并取得了优异的预测结果。此外，该方法源代码和预训练模型都将公开发布，以促进更多研究领域的发展。

**Abstract:** Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

</details>


### [16] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

> 本文通过系统评估现有视觉追踪模型应用于传感手套的性能，发现显著性能下降的问题，并提出了AirGlove方法来解决这一问题，实现了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机在于解决现有的基于视觉的裸手跟踪模型在应用于传感手套时出现的性能下降问题，尝试通过AirGlove方法提高对不同手套设计的手势识别能力。

**Method:** 本文采用了一系列基于视觉的手部追踪模型，并进行了零样本和微调设置下的系统评估。提出AirGlove方法，利用现有手套数据泛化手套表示，以适应新的手套设计。

**Result:** 本文通过系统评估基于视觉的手部追踪模型在佩戴手套情况下的表现，揭示了当前基于裸手的手势识别模型在其应用于传感手套时存在的性能下降问题。为解决这一问题，作者提出了AirGlove方法，利用现有手套数据来泛化手套表示，从而有效地提高对新手套设计的手势识别性能。实验表明，该方法在多种传感手套上实现了显著的性能提升。

**Conclusion:** 现有基于视觉的手部追踪模型在应用于传感手套时出现了显著的性能下降，主要原因是手套外观与裸手间的巨大差异。通过本文提出的AirGlove方法，可以有效改善手套设计新旧之间的泛化能力，对于提升端到端的手部姿态追踪性能具有重要意义。

**Abstract:** Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

</details>


### [17] [SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition](https://arxiv.org/abs/2602.05162)
*Anay Majee,Rishabh Iyer*

Main category: cs.CV

> Proposes SHaSaM to enhance fairness in DNNs by mitigating data imbalance and minimizing the influence of sensitive attributes, achieving state-of-the-art results in fairness and accuracy.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of social and demographic biases in deep neural networks inherited from imbalanced annotated data, which leads to unfair predictions involving sensitive attributes.

**Method:** SHaSaM (Submodular Hard Sample Mining), a two-stage approach: SHaSaM-MINE mines hard positives and negatives, and SHaSaM-LEARN uses combinatorial loss functions to maximize decision boundaries while minimizing sensitive attribute influence.

**Result:** SHaSaM demonstrates a 2.7 points improvement in Equalized Odds and a 3.5% gain in Accuracy compared to existing methods on CelebA and UTKFace datasets.

**Conclusion:** The unified formulation of SHaSaM effectively mitigates unfairness without sacrificing performance, offering a significant advancement in fairness-enhanced machine learning.

**Abstract:** Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.

</details>


### [18] [LOBSTgER-enhance: an underwater image enhancement pipeline](https://arxiv.org/abs/2602.05163)
*Andreas Mentzelopoulos,Keith Ellenbogen*

Main category: cs.CV

> Developed an AI pipeline using diffusion-based generation to correct for underwater image degradations, resulting in better image quality for underwater photography.

<details>
  <summary>Details</summary>

**Motivation:** Underwater photography faces significant challenges like reduced contrast, spatial blur, and color distortions, which obscure the vibrancy of marine life. Previous methods require heavy post-processing to correct for these issues.

**Method:** We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation.

**Result:** The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

**Conclusion:** The developed pipeline shows significant potential for enhancing underwater photography without extensive post-processing.

**Abstract:** Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.
  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

</details>


### [19] [ShapePuri: Shape Guided and Appearance Generalized Adversarial Purification](https://arxiv.org/abs/2602.05175)
*Zhe Li,Bernhard Kainz*

Main category: cs.CV

> Shape Guided Purification (ShapePuri) uses geometric and appearance debiasing to improve the robustness of neural networks against adversarial attacks, achieving high robust accuracy under the AutoAttack protocol while maintaining efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To enhance the robustness of deep neural networks against imperceptible adversarial attacks by addressing the issue of high computational costs and information loss associated with diffusion-based purification methods.

**Method:** Shape Guided Purification (ShapePuri), which comprises a Shape Encoding Module (SEM) for geometric guidance using Signed Distance Functions (SDF) and a Global Appearance Debiasing (GAD) module to mitigate appearance bias by stochastic transformations.

**Result:** ShapePuri achieves 84.06% clean accuracy and 81.64% robust accuracy under the AutoAttack protocol, which is the first to pass the 80% threshold on this benchmark.

**Conclusion:** The proposed ShapePuri provides a scalable and efficient defense against adversarial attacks without additional computational costs or reliance on auxiliary modules.

**Abstract:** Deep neural networks demonstrate impressive performance in visual recognition, but they remain vulnerable to adversarial attacks that is imperceptible to the human. Although existing defense strategies such as adversarial training and purification have achieved progress, diffusion-based purification often involves high computational costs and information loss. To address these challenges, we introduce Shape Guided Purification (ShapePuri), a novel defense framework enhances robustness by aligning model representations with stable structural invariants. ShapePuri integrates two components: a Shape Encoding Module (SEM) that provides dense geometric guidance through Signed Distance Functions (SDF), and a Global Appearance Debiasing (GAD) module that mitigates appearance bias via stochastic transformations. In our experiments, ShapePuri achieves $84.06\%$ clean accuracy and $81.64\%$ robust accuracy under the AutoAttack protocol, representing the first defense framework to surpass the $80\%$ threshold on this benchmark. Our approach provides a scalable and efficient adversarial defense that preserves prediction stability during inference without requiring auxiliary modules or additional computational cost.

</details>
