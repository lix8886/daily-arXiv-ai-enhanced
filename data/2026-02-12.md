<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Reviewing the Reviewer: Elevating Peer Review Quality through LLM-Guided Feedback](https://arxiv.org/abs/2602.10118)
*Sukannya Purkayastha,Qile Wan,Anne Lauscher,Lizhen Qu,Iryna Gurevych*

Main category: cs.CL

> 本文提出了一种基于LLM的框架，通过将审稿意见分解为论辩段落、识别问题并生成有针对性的反馈，来改进审稿意见的质量，实验表明该方法在提高审稿质量方面有显著效果。

<details>
  <summary>Details</summary>

**Motivation:** 传统的审稿意见检测仅作为单标签任务，忽视了审稿意见可能存在的多种问题及改进的必要性。

**Method:** 提出一个结合LLM与传统分类器的神经符号模块，识别审稿段落中的问题，并通过遗传算法优化的模板生成具体反馈。

**Result:** 实验结果表明，该方法优于零样本LLM基线，并将审稿质量提高了92.4%。

**Conclusion:** 引进的框架能够有效检测并减少审稿意见中的懒思考问题，提升了审稿质量，并且提供了一种新的审稿意见处理方法。

**Abstract:** Peer review is central to scientific quality, yet reliance on simple heuristics -- lazy thinking -- has lowered standards. Prior work treats lazy thinking detection as a single-label task, but review segments may exhibit multiple issues, including broader clarity problems, or specificity issues. Turning detection into actionable improvements requires guideline-aware feedback, which is currently missing. We introduce an LLM-driven framework that decomposes reviews into argumentative segments, identifies issues via a neurosymbolic module combining LLM features with traditional classifiers, and generates targeted feedback using issue-specific templates refined by a genetic algorithm. Experiments show our method outperforms zero-shot LLM baselines and improves review quality by up to 92.4\%. We also release LazyReviewPlus, a dataset of 1,309 sentences labeled for lazy thinking and specificity.

</details>


### [2] [Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens](https://arxiv.org/abs/2602.10229)
*Weihao Liu,Dehai Min,Lu Cheng*

Main category: cs.CL

> 本文提出了Latent Thoughts Tuning (LT-Tuning)，一种重新定义潜在思考构建和部署方式的框架。LT-Tuning结合了上下文和预测语义指导，并采用逐步的三阶段课程学习管道，有效解决了特征崩溃问题，提升了推理准确性。

<details>
  <summary>Details</summary>

**Motivation:** 目前的潜在思考范式常常受到特征崩溃和不稳定性的困扰，这些问题主要源于当反复使用隐藏状态作为输入嵌入时出现的分布不匹配，或是依赖辅助模型时的对齐问题。

**Method:** 我们的方法结合了上下文和预测语义指导，提出了一个名为Latent Thoughts Tuning (LT-Tuning)的框架。这个框架包含了一个上下文-预测-融合机制，它同时使用了隐藏状态和词汇嵌入空间中的语义指导。此外，LT-Tuning还使用了一个逐步的三阶段课程学习管道，能够动态切换潜在思考和显式思考模式。

**Result:** 实验结果证明了LT-Tuning方法的有效性，它优于现有的潜在推理基线，有效解决特征崩溃问题，提升推理准确性。

**Conclusion:** 实验表明，我们的方法优于现有的潜在推理基线，能够有效缓解特征崩溃，并达到稳健的推理准确性。

**Abstract:** While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy.

</details>


### [3] [Learning to Evict from Key-Value Cache](https://arxiv.org/abs/2602.10238)
*Luca Moschella,Laura Manduchi,Ozan Sener*

Main category: cs.CL

> 研究提出了一种名为KV Policy（KVP）的框架，该框架通过强化学习来学习预测未来解码时令牌的有用性，从而以一种轻量级的方式对KV缓存进行管理，并且在不同模型家族的多项基准上表现优于基线方法。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）的不断增大使得高效推理变得具有挑战性，主要用于自回归键值（KV）缓存的记忆需求。现有置换或压缩方法虽能降低成本，但依赖于启发式方法，如最近使用度或过去的注意力分数，这些方法仅作为未来利用率的间接代理，并引入计算开销。

**Method:** 我们将KV缓存的置换问题重新定义为强化学习（RL）问题：学习根据预测的未来解码有用性对令牌进行排名。为此，我们引入了KV Policy（KVP）框架，这是一个轻量级的多头RL代理训练框架，仅使用键和值向量在预先计算的生成追踪上进行训练。每个代理学习一种由未来效用指导的专门置换策略。

**Result:** KVP在长上下文基准RULER和多轮对话基准OASST2-4k上的不同模型系列中显著优于基线方法。此外，在标准下游任务（例如，LongBench，BOOLQ，ARC）的零样本测试表明KVP能很好地泛化到超出训练数据分布和更长的上下文长度。

**Conclusion:** 这些结果证明了学习预测未来令牌效用是一种强大且可扩展的适配KV缓存管理范式。

**Abstract:** The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.

</details>


### [4] [On Emergent Social World Models -- Evidence for Functional Integration of Theory of Mind and Pragmatic Reasoning in Language Models](https://arxiv.org/abs/2602.10298)
*Polina Tsvilodub,Jan-Felix Klumpp,Amir Mohammadpour,Jennifer Hu,Michael Franke*

Main category: cs.CL

> 该研究探讨了语言模型是否利用了通用的“心智理论”(ToM)和语言特异性语用推理机制，并通过行为评估和因果机制实验提出，语言模型可能形成了互联的“社会世界模型”，而非孤立的技能。

<details>
  <summary>Details</summary>

**Motivation:** 动机是理解语言模型是否能形成所谓的“社会世界模型”——即，在不同任务中可以重新利用的心理状态表示，以验证功能整合假设。

**Method:** 通过行为评估和基于认知神经科学的功能定位方法的因果机制实验，分析了语言模型在七类心智理论能力上的表现，使用了比之前研究更大的定位数据集。

**Result:** 严格的假设驱动的统计测试提供了支持功能整合假设的暗示性证据，表明语言模型可能形成了互联的“社会世界模型”。

**Conclusion:** 这项工作为人工系统中社会认知的出现提供了新的心智理论定位数据、功能定位技术的方法论改进以及实证见解。

**Abstract:** This paper investigates whether LMs recruit shared computational mechanisms for general Theory of Mind (ToM) and language-specific pragmatic reasoning in order to contribute to the general question of whether LMs may be said to have emergent "social world models", i.e., representations of mental states that are repurposed across tasks (the functional integration hypothesis). Using behavioral evaluations and causal-mechanistic experiments via functional localization methods inspired by cognitive neuroscience, we analyze LMs' performance across seven subcategories of ToM abilities (Beaudoin et al., 2020) on a substantially larger localizer dataset than used in prior like-minded work. Results from stringent hypothesis-driven statistical testing offer suggestive evidence for the functional integration hypothesis, indicating that LMs may develop interconnected "social world models" rather than isolated competencies. This work contributes novel ToM localizer data, methodological refinements to functional localization techniques, and empirical insights into the emergence of social cognition in artificial systems.

</details>


### [5] [Are More Tokens Rational? Inference-Time Scaling in Language Models as Adaptive Resource Rationality](https://arxiv.org/abs/2602.10329)
*Zhimin Hu,Riya Roshan,Sashank Varma*

Main category: cs.CL

> 研究通过引入变量归因任务，观察了IT模型和LRM模型在不同任务复杂度下单变量归因的能力，结果显示模型可以自我调整推理策略，表明资源理性是推理时间扩展过程中的一个涌现属性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探索在没有显式计算成本相关的奖励下，资源理性是否可以通过推理时间的扩展自然地出现。

**Method:** 本研究引入了一个变量归因任务，通过变化候选变量和试验的数量来系统性地操纵任务复杂度。

**Result:** 研究结果显示，随着任务复杂度的增加，两种模型都会从暴力策略转向分析策略。但是在处理XOR和XNOR函数时，IT模型表现退化，而LRM模型依然保持稳健。

**Conclusion:** 这表明，即使在没有基于成本的奖励情况下，模型的推理行为也可以根据任务复杂度做出调整，这说明资源理性是推理时间扩展本身的涌现属性。

**Abstract:** Human reasoning is shaped by resource rationality -- optimizing performance under constraints. Recently, inference-time scaling has emerged as a powerful paradigm to improve the reasoning performance of Large Language Models by expanding test-time computation. Specifically, instruction-tuned (IT) models explicitly generate long reasoning steps during inference, whereas Large Reasoning Models (LRMs) are trained by reinforcement learning to discover reasoning paths that maximize accuracy. However, it remains unclear whether resource-rationality can emerge from such scaling without explicit reward related to computational costs. We introduce a Variable Attribution Task in which models infer which variables determine outcomes given candidate variables, input-output trials, and predefined logical functions. By varying the number of candidate variables and trials, we systematically manipulate task complexity. Both models exhibit a transition from brute-force to analytic strategies as complexity increases. IT models degrade on XOR and XNOR functions, whereas LRMs remain robust. These findings suggest that models can adjust their reasoning behavior in response to task complexity, even without explicit cost-based reward. It provides compelling evidence that resource rationality is an emergent property of inference-time scaling itself.

</details>


### [6] [The Subjectivity of Respect in Police Traffic Stops: Modeling Community Perspectives in Body-Worn Camera Footage](https://arxiv.org/abs/2602.10339)
*Preni Golazizian,Elnaz Rahmati,Jackson Trager,Zhivar Sourati,Nona Ghazizadeh,Georgios Chochlakis,Jose Alcocer,Kerby Bennett,Aarya Vijay Devnani,Parsa Hejabi,Harry G. Muttram,Akshay Kiran Padte,Mehrshad Saadatinia,Chenhao Wu,Alireza S. Zaibari,Michael Sierra-Arévalo,Nick Weller,Shrikanth Narayanan,Benjamin A. T. Graham,Morteza Dehghani*

Main category: cs.CL

> 本文使用洛杉矶警察局的随身摄像机录制的交通拦截视频，构建了首个大型交通拦截数据集，以评分和自由文本理由来标注尊重维度，研究跨不同社区的感知差异，提高了评分预测性能和理由的对齐。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在研究警察与市民互动中的尊重维度，这有助于塑造公众信任和合法性。考虑到尊重的解释具有主观性且受个人经历影响，必须将社区特定的视角纳入考量。通过分析交通拦截过程中的尊重评分，研究者们希望提供一种工具，帮助执法部门更好地理解不同社区的期望，从而促进公众信任和程序正义。

**Method:** 通过使用洛杉矶警察局安装的随身摄像设备录制的交通拦截视频，本文构建了一个大型交通拦截数据集，该数据集附有从多个角度进行尊重评分和自由文本理由的标注。研究者们精心招募了来自警察相关群体、司法系统影响群体和非相关洛杉矶居民三个群体的标注员，以此系统性地研究来自不同社区的感知差异。他们（i）基于程序正义理论、洛杉矶警察局培训材料以及大量的实地工作开发了一个专业领域内的评估标准；（ii）提出了一个基于标准的偏好数据构建框架，以实现视角一致性的对齐；（iii）提出了一种视角感知的建模框架，可以根据交通拦截的文字记录来预测个性化的尊重评分，并为警察和民用驾驶员生成标注员特定的解释。

**Result:** 通过这种方法，研究跨越所有三个标注员群体，提升了评分预测性能和理由的对齐度。

**Conclusion:** 研究提出的方法和框架为理解不同社区的期望提供了一种有价值的方式，有助于执法部门建立公众信任和程序正义。

**Abstract:** Traffic stops are among the most frequent police-civilian interactions, and body-worn cameras (BWCs) provide a unique record of how these encounters unfold. Respect is a central dimension of these interactions, shaping public trust and perceived legitimacy, yet its interpretation is inherently subjective and shaped by lived experience, rendering community-specific perspectives a critical consideration. Leveraging unprecedented access to Los Angeles Police Department BWC footage, we introduce the first large-scale traffic-stop dataset annotated with respect ratings and free-text rationales from multiple perspectives. By sampling annotators from police-affiliated, justice-system-impacted, and non-affiliated Los Angeles residents, we enable the systematic study of perceptual differences across diverse communities. To this end, we (i) develop a domain-specific evaluation rubric grounded in procedural justice theory, LAPD training materials, and extensive fieldwork; (ii) introduce a rubric-driven preference data construction framework for perspective-consistent alignment; and (iii) propose a perspective-aware modeling framework that predicts personalized respect ratings and generates annotator-specific rationales for both officers and civilian drivers from traffic-stop transcripts. Across all three annotator groups, our approach improves both rating prediction performance and rationale alignment. Our perspective-aware framework enables law enforcement to better understand diverse community expectations, providing a vital tool for building public trust and procedural legitimacy.

</details>


### [7] [Geometry-Aware Decoding with Wasserstein-Regularized Truncation and Mass Penalties for Large Language Models](https://arxiv.org/abs/2602.10346)
*Arash Gholami Davoodi,Navid Rezazadeh,Seyed Pouyan Mousavi Davoudi,Pouya Pezeshkpour*

Main category: cs.CL

> Top-W采用几何感知的截断规则，利用Wasserstein距离保持分布连贯性并平衡概率质量与熵，实验表明其优于现有方法，特别是在提高准确性和创造性方面。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型(LLMs)在开放生成中必须平衡多样性和创造性与逻辑连贯性。现有的基于截断的采样器虽然有效，但大多基于经验规则，主要依赖概率质量和熵，而忽视了token空间的语义几何。

**Method:** 提出了一种基于Wasserstein距离的几何感知裁剪规则Top-W，该规则在保持裁剪后的分布接近原始分布的同时，显式地平衡了保留的概率质量和保留集的熵。理论结果给出了固定势能子集更新的简单封闭形式结构：根据质量-熵权衡，最优裁剪要么坍缩为一个单一的token，要么采用可以通过线性扫描高效找到的一维前缀形式。

**Result:** 在四个基准(GSM8K, GPQA, AlpacaEval, and MT-Bench)上，在三种指令调优模型的广泛实验表明，Top-W在所有情况下都优于以前最先进（SoTA）的解码方式，最高可达到33.7%的改进。此外，还发现Top-W不仅改善了以准确性为重点的表现，而且在基于法官的开放式评估中也增强了创造性。

**Conclusion:** Top-W通过引入几何感知的截断规则解决了现有方法在保持分布连贯性和多样创意方面的问题，实验结果表明其在提高精度和创造力方面显著优于现有的解码方法。

**Abstract:** Large language models (LLMs) must balance diversity and creativity against logical coherence in open-ended generation. Existing truncation-based samplers are effective but largely heuristic, relying mainly on probability mass and entropy while ignoring semantic geometry of the token space. We present Top-W, a geometry-aware truncation rule that uses Wasserstein distance-defined over token-embedding geometry-to keep the cropped distribution close to the original, while explicitly balancing retained probability mass against the entropy of the kept set. Our theory yields a simple closed-form structure for the fixed-potential subset update: depending on the mass-entropy trade-off, the optimal crop either collapses to a single token or takes the form of a one-dimensional prefix that can be found efficiently with a linear scan. We implement Top-W using efficient geometry-based potentials (nearest-set or k-NN) and pair it with an alternating decoding routine that keeps the standard truncation-and-sampling interface unchanged. Extensive experiments on four benchmarks (GSM8K, GPQA, AlpacaEval, and MT-Bench) across three instruction-tuned models show that Top-W consistently outperforms prior state-of-the-art decoding approaches achieving up to 33.7% improvement. Moreover, we find that Top-W not only improves accuracy-focused performance, but also boosts creativity under judge-based open-ended evaluation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Multi-encoder ConvNeXt Network with Smooth Attentional Feature Fusion for Multispectral Semantic Segmentation](https://arxiv.org/abs/2602.10137)
*Leo Thomas Ramos,Angel D. Sappa*

Main category: cs.CV

> MeCSAFNet是一种专门为多光谱图像土地覆盖分割设计的网络架构。该模型通过双ConvNeXt编码器和紧密的解码器结构实现了高性能的分割结果，并且能够适应不同的光谱配置。实验结果表明，MeCSAFNet在多个基准数据集上均显著优于其他现有方法，包括U-Net、DeepLabV3+和SegFormer。此外，该模型还具有迅速部署的优点。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于提高多光谱图像中土地覆盖分割的性能，通过提出一种能够处理不同光谱配置的新型网络架构，以实现更准确的分割结果。

**Method:** 该论文提出了一种名为MeCSAFNet的多分支编解码器架构，专为多光谱图像的土地覆盖分割而设计。模型通过双ConvNeXt编码器分别处理可见光和非可见光通道，并通过各自的解码器重建空间信息。一个专门的融合解码器在多个尺度上结合中间特征，将精细的空间线索与高水平的光谱表示结合在一起。在特征融合中进一步使用CBAM注意力机制和ASAU激活函数来促进稳定且高效的优化过程。模型设计用于处理不同的光谱配置，包括4通道（4c）输入（结合RGB和NIR波段）和6通道（6c）输入（集成NDVI和NDWI指数）.

**Result:** 在Five-Billion-Pixels (FBP)和Potsdam数据集上的实验显示了显著的性能提升。例如，在FBP上，MeCSAFNet-base (6c)的mIoU值分别比U-Net (4c)、U-Net (6c)、SegFormer (4c)和SegFormer (6c)高出+19.21%、+14.72%、+19.62%和+14.74%。在Potsdam上，MeCSAFNet-large (4c)的mIoU值分别比DeepLabV3+ (4c)、DeepLabV3+ (6c)、SegFormer (4c)、SegFormer (6c)高出+6.48%、+5.85%、+9.11%和+4.80%。此外，紧凑型的MeCSAFNet变体也表现出良好的性能，同时具有较低的训练时间和推理成本。

**Conclusion:** 该工作表明，MeCSAFNet能够有效处理多光谱图像的土地覆盖分割任务，通过融合多种特征和引入注意力机制，模型能够显著提高分割精度。其不同的光谱配置适应性也使得其在不同应用场景中都有很好的表现。此外，紧凑型变体还能够在资源受限的环境中提供支持。

**Abstract:** This work proposes MeCSAFNet, a multi-branch encoder-decoder architecture for land cover segmentation in multispectral imagery. The model separately processes visible and non-visible channels through dual ConvNeXt encoders, followed by individual decoders that reconstruct spatial information. A dedicated fusion decoder integrates intermediate features at multiple scales, combining fine spatial cues with high-level spectral representations. The feature fusion is further enhanced with CBAM attention, and the ASAU activation function contributes to stable and efficient optimization. The model is designed to process different spectral configurations, including a 4-channel (4c) input combining RGB and NIR bands, as well as a 6-channel (6c) input incorporating NDVI and NDWI indices. Experiments on the Five-Billion-Pixels (FBP) and Potsdam datasets demonstrate significant performance gains. On FBP, MeCSAFNet-base (6c) surpasses U-Net (4c) by +19.21%, U-Net (6c) by +14.72%, SegFormer (4c) by +19.62%, and SegFormer (6c) by +14.74% in mIoU. On Potsdam, MeCSAFNet-large (4c) improves over DeepLabV3+ (4c) by +6.48%, DeepLabV3+ (6c) by +5.85%, SegFormer (4c) by +9.11%, and SegFormer (6c) by +4.80% in mIoU. The model also achieves consistent gains over several recent state-of-the-art approaches. Moreover, compact variants of MeCSAFNet deliver notable performance with lower training time and reduced inference cost, supporting their deployment in resource-constrained environments.

</details>


### [9] [Multimodal Information Fusion for Chart Understanding: A Survey of MLLMs -- Evolution, Limitations, and Cognitive Enhancement](https://arxiv.org/abs/2602.10138)
*Zhihang Yi,Jian Zhao,Jiancheng Lv,Tao Wang*

Main category: cs.CV

> 本文提供了一篇关于MLLMs在图表理解任务中的全面综述，涵盖了任务挑战、数据集分类、方法论发展历程，并指出了模型的局限性和未来的研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 多模态大语言模型在图表理解领域带来了革命性变化，但该领域的知识碎片化且缺少系统性的组织。本文通过构建该领域的核心组成部分提供了一个全面的路线图。

**Method:** 本文首先分析了图表理解任务中融合视觉和语言信息的基本挑战，随后对下游任务和数据集进行了分类，并提出了一个包括典型和非典型基准测试的新分类法。接着，文章详细阐述了方法论的发展，追溯从传统的深度学习技术到利用复杂融合策略的最先进的多模态大语言模型（MLLMs）的演变过程。

**Result:** 本文旨在帮助研究者和实践者获得关于MLLMs如何改变图表信息融合的理解结构，并推动开发更强大、更可靠的系统。

**Conclusion:** 通过批判性地分析现有模型的局限性，尤其是他们的感知和推理缺陷，本文为更先进对齐技术和认知增强的强化学习指出了有希望的未来方向。

**Abstract:** Chart understanding is a quintessential information fusion task, requiring the seamless integration of graphical and textual data to extract meaning. The advent of Multimodal Large Language Models (MLLMs) has revolutionized this domain, yet the landscape of MLLM-based chart analysis remains fragmented and lacks systematic organization. This survey provides a comprehensive roadmap of this nascent frontier by structuring the domain's core components. We begin by analyzing the fundamental challenges of fusing visual and linguistic information in charts. We then categorize downstream tasks and datasets, introducing a novel taxonomy of canonical and non-canonical benchmarks to highlight the field's expanding scope. Subsequently, we present a comprehensive evolution of methodologies, tracing the progression from classic deep learning techniques to state-of-the-art MLLM paradigms that leverage sophisticated fusion strategies. By critically examining the limitations of current models, particularly their perceptual and reasoning deficits, we identify promising future directions, including advanced alignment techniques and reinforcement learning for cognitive enhancement. This survey aims to equip researchers and practitioners with a structured understanding of how MLLMs are transforming chart information fusion and to catalyze progress toward more robust and reliable systems.

</details>


### [10] [MPA: Multimodal Prototype Augmentation for Few-Shot Learning](https://arxiv.org/abs/2602.10143)
*Liwen Wu,Wei Wang,Lei Zhao,Zhan Gao,Qika Lin,Shaowen Yao,Zuozhu Liu,Bin Pu*

Main category: cs.CV

> This paper proposes MPA, a novel framework for few-shot learning that integrates multimodal data and advanced data augmentation techniques, achieving state-of-the-art performance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of existing FSL methods by incorporating multimodal information and comprehensive augmentation techniques to improve recognition performance with a few labeled examples.

**Method:** Our method proposes a Multimodal Prototype Augmentation FSL framework called MPA, which includes LLM-based Multi-Variant Semantic Enhancement, Hierarchical Multi-View Augmentation, and an Adaptive Uncertain Class Absorber. LMSE uses large language models to enhance semantic cues, HMA leverages natural and multi-view augmentations to diversify features, and AUCA models uncertainty by introducing uncertain classes via interpolation and Gaussian sampling.

**Result:** Our experiments on four single-domain and six cross-domain FSL benchmarks show that MPA outperforms existing state-of-the-art methods, achieving a 12.29% and 24.56% improvement respectively in the single-domain and cross-domain setting under the 5-way 1-shot setting.

**Conclusion:** The conclusion is that the proposed MPA framework demonstrates superior performance in FSL tasks, showing significant improvements over existing methods in various settings.

**Abstract:** Recently, few-shot learning (FSL) has become a popular task that aims to recognize new classes from only a few labeled examples and has been widely applied in fields such as natural science, remote sensing, and medical images. However, most existing methods focus only on the visual modality and compute prototypes directly from raw support images, which lack comprehensive and rich multimodal information. To address these limitations, we propose a novel Multimodal Prototype Augmentation FSL framework called MPA, including LLM-based Multi-Variant Semantic Enhancement (LMSE), Hierarchical Multi-View Augmentation (HMA), and an Adaptive Uncertain Class Absorber (AUCA). LMSE leverages large language models to generate diverse paraphrased category descriptions, enriching the support set with additional semantic cues. HMA exploits both natural and multi-view augmentations to enhance feature diversity (e.g., changes in viewing distance, camera angles, and lighting conditions). AUCA models uncertainty by introducing uncertain classes via interpolation and Gaussian sampling, effectively absorbing uncertain samples. Extensive experiments on four single-domain and six cross-domain FSL benchmarks demonstrate that MPA achieves superior performance compared to existing state-of-the-art methods across most settings. Notably, MPA surpasses the second-best method by 12.29% and 24.56% in the single-domain and cross-domain setting, respectively, in the 5-way 1-shot setting.

</details>


### [11] [VERA: Identifying and Leveraging Visual Evidence Retrieval Heads in Long-Context Understanding](https://arxiv.org/abs/2602.10146)
*Rongcan Pei,Huan Li,Fang Guo,Qi Zhu*

Main category: cs.CV

> 本文提出VERA框架，通过分析视觉证据检索头部在视觉语言模型中的作用，提高了模型在长上下文理解上的性能，展示了在多个基准测试中的显著改进。

<details>
  <summary>Details</summary>

**Motivation:** 由于VLMs在处理长上下文和复杂推理任务上存在挑战，本研究旨在通过理解和优化其内部机制，提高其在这些领域的能力。

**Method:** 我们通过注意力分析来研究视觉语言模型（VLMs）在处理长上下文时的内部机制，特别是识别出视觉证据检索（VER）头部作为关键组件，这些头部在推理过程中负责定位视觉线索。我们提出了一种无训练的框架VERA，该框架可以检测模型的不确定性并触发对VER头部关注的视觉证据的显式表达。

**Result:** 实验表明，VERA能够显著提升开源VLMs在长上下文理解上的表现，在多个基准测试中，Qwen3-VL-8B-Instruct和GLM-4.1V-Thinking分别平均相对提高了21.3%和20.1%。

**Conclusion:** VERA作为一种无训练的框架，通过改进VLMs处理长上下文的性能表现，揭示了VER头部在长上下文推理中的重要性，并为解决复杂任务中的性能瓶颈提供了新方法。

**Abstract:** While Vision-Language Models (VLMs) have shown promise in textual understanding, they face significant challenges when handling long context and complex reasoning tasks. In this paper, we dissect the internal mechanisms governing long-context processing in VLMs to understand their performance bottlenecks. Through the lens of attention analysis, we identify specific Visual Evidence Retrieval (VER) Heads - a sparse, dynamic set of attention heads critical for locating visual cues during reasoning, distinct from static OCR heads. We demonstrate that these heads are causal to model performance; masking them leads to significant degradation. Leveraging this discovery, we propose VERA (Visual Evidence Retrieval Augmentation), a training-free framework that detects model uncertainty (i.e., entropy) to trigger the explicit verbalization of visual evidence attended by VER heads. Comprehensive experiments demonstrate that VERA significantly improves long-context understanding of open-source VLMs: it yields an average relative improvement of 21.3% on Qwen3-VL-8B-Instruct and 20.1% on GLM-4.1V-Thinking across five benchmarks.

</details>


### [12] [Beyond Closed-Pool Video Retrieval: A Benchmark and Agent Framework for Real-World Video Search and Moment Localization](https://arxiv.org/abs/2602.10159)
*Tao Yu,Yujia Yang,Haopeng Jin,Junhao Gong,Xinlong Chen,Yuxuan Zhou,Shanbin Zhang,Jiabing Yang,Xinming Wang,Hongzhu Yi,Ping Nie,Kai Zou,Zhang Zhang,Yan Huang,Liang Wang,Yeshani,Ruiwen Tao,Jin Ma,Haijin Liang,Jinwen Luo*

Main category: cs.CV

> 此论文介绍了RVMS-Bench，一个用于评估现实世界视频记忆搜索的系统，以及RACLO，一个模仿人类记忆、搜索、验证过程的框架，揭示了当前多语言大型语言模型在基于模糊记忆的视频检索和时刻定位上的不足。

<details>
  <summary>Details</summary>

**Motivation:** 传统的视频检索基准集中在精确描述与封闭视频库的匹配上，未能反映出实际搜索中的模糊多维记忆。因此，提出了RVMS-Bench和RACLO框架，旨在模拟更实际的搜索环境，以改进在现实世界无结构化场景中的视频检索和定位能力。

**Method:** RVMS-Bench包括1440个样本，覆盖20个多样化的类别和四个时长组，并使用分层描述框架模拟多维搜索线索。RACLO通过推测性推理模拟人类的回忆-搜索-验证认知过程，以应对基于模糊记忆的现实世界视频搜索挑战。

**Result:** 实验表明，当前的多语言大型语言模型在现实世界的视频检索和时刻定位上，基于模糊记忆的表现仍然不足。

**Conclusion:** 该工作将促进视频检索在现实世界无结构化场景中的鲁棒性的提高。

**Abstract:** Traditional video retrieval benchmarks focus on matching precise descriptions to closed video pools, failing to reflect real-world searches characterized by fuzzy, multi-dimensional memories on the open web. We present \textbf{RVMS-Bench}, a comprehensive system for evaluating real-world video memory search. It consists of \textbf{1,440 samples} spanning \textbf{20 diverse categories} and \textbf{four duration groups}, sourced from \textbf{real-world open-web videos}. RVMS-Bench utilizes a hierarchical description framework encompassing \textbf{Global Impression, Key Moment, Temporal Context, and Auditory Memory} to mimic realistic multi-dimensional search cues, with all samples strictly verified via a human-in-the-loop protocol. We further propose \textbf{RACLO}, an agentic framework that employs abductive reasoning to simulate the human ``Recall-Search-Verify'' cognitive process, effectively addressing the challenge of searching for videos via fuzzy memories in the real world. Experiments reveal that existing MLLMs still demonstrate insufficient capabilities in real-world Video Retrieval and Moment Localization based on fuzzy memories. We believe this work will facilitate the advancement of video retrieval robustness in real-world unstructured scenarios.

</details>


### [13] [AD$^2$: Analysis and Detection of Adversarial Threats in Visual Perception for End-to-End Autonomous Driving Systems](https://arxiv.org/abs/2602.10160)
*Ishan Sahu,Somnath Hazra,Somak Aditya,Soumyajit Dey*

Main category: cs.CV

> 本文评估了最先进的自动驾驶智能体在不同视觉攻击下的脆弱性，并提出了一种新的攻击检测模型AD$^2$。

<details>
  <summary>Details</summary>

**Motivation:** 自动驾驶系统的端到端进展显著，但其在对抗鲁棒性方面的研究仍不足。本文旨在填补这一研究空白，评估自动驾驶系统的安全性。

**Method:** 我们在CARLA环境中对最先进的自主驾驶智能体进行了闭环评估，针对视觉感知管道的三种攻击向量：(i) 由声波诱导的基于物理学的模糊攻击，(ii) 电磁干扰攻击，它会扭曲捕获的图像，以及(iii) 数字攻击，在图像中加入精心设计的有界干扰鬼魂物体。

**Result:** 实验显示了两种先进智能体，Transfuser和Interfuser，面对上述攻击存在严重的脆弱性，驾驶分数在最坏情况下下降了99%。

**Conclusion:** 为减轻这类威胁，我们提出了一种轻量级的自主驾驶系统攻击检测模型AD$^2$，基于注意力机制捕捉时空一致性，实验表明该检测器具有优越的检测能力和计算效率。

**Abstract:** End-to-end autonomous driving systems have achieved significant progress, yet their adversarial robustness remains largely underexplored. In this work, we conduct a closed-loop evaluation of state-of-the-art autonomous driving agents under black-box adversarial threat models in CARLA. Specifically, we consider three representative attack vectors on the visual perception pipeline: (i) a physics-based blur attack induced by acoustic waves, (ii) an electromagnetic interference attack that distorts captured images, and (iii) a digital attack that adds ghost objects as carefully crafted bounded perturbations on images. Our experiments on two advanced agents, Transfuser and Interfuser, reveal severe vulnerabilities to such attacks, with driving scores dropping by up to 99% in the worst case, raising valid safety concerns. To help mitigate such threats, we further propose a lightweight Attack Detection model for Autonomous Driving systems (AD$^2$) based on attention mechanisms that capture spatial-temporal consistency. Comprehensive experiments across multi-camera inputs on CARLA show that our detector achieves superior detection capability and computational efficiency compared to existing approaches.

</details>


### [14] [ArtisanGS: Interactive Tools for Gaussian Splat Selection with AI and Human in the Loop](https://arxiv.org/abs/2602.10173)
*Clement Fuji Tsang,Anita Hu,Or Perel,Carsten Kolve,Maria Shugrina*

Main category: cs.CV

> 论文介绍了一套交互式工具用于高斯点云的灵活选择和分割，包括AI驱动的选择蒙版传播技术和灵活的手动工具，实现了广泛的二值分割效果，并展示了在下游应用中的编辑潜力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管高斯点云表示在3D图形中应用广泛，但仍面临从野外捕捉中提取有用对象及可控编辑技术有限的挑战。为解决这一问题，研究引入了以高斯点云选择和分割为中心的交互式工具套件。

**Method:** 研究提出了一种快速的AI驱动技术，将用户指导的2D选择蒙版传播到3D高斯点云选择中，并结合了柔性手动选择和分割工具。

**Result:** 该研究提出了一套交互式工具，专注于高斯点云的灵活选择和分割。方法包括一种快速的AI驱动技术，用于将用户指导的2D选择蒙版传播到3D高斯点云的选择中。此外，还提供了灵活的手动选择和分割工具，以实现几乎任何的二值分割。研究通过开发一种基于用户指导局部编辑的方法，展示了这套工具集在高斯点云选择中的实用性和对下游应用的价值。

**Conclusion:** 研究得出结论，所提出的高斯点云选择和编辑工具可以被用于任何野外捕捉的高斯点云，无需额外优化。

**Abstract:** Representation in the family of 3D Gaussian Splats (3DGS) are growing into a viable alternative to traditional graphics for an expanding number of application, including recent techniques that facilitate physics simulation and animation. However, extracting usable objects from in-the-wild captures remains challenging and controllable editing techniques for this representation are limited. Unlike the bulk of emerging techniques, focused on automatic solutions or high-level editing, we introduce an interactive suite of tools centered around versatile Gaussian Splat selection and segmentation. We propose a fast AI-driven method to propagate user-guided 2D selection masks to 3DGS selections. This technique allows for user intervention in the case of errors and is further coupled with flexible manual selection and segmentation tools. These allow a user to achieve virtually any binary segmentation of an unstructured 3DGS scene. We evaluate our toolset against the state-of-the-art for Gaussian Splat selection and demonstrate their utility for downstream applications by developing a user-guided local editing approach, leveraging a custom Video Diffusion Model. With flexible selection tools, users have direct control over the areas that the AI can modify. Our selection and editing tools can be used for any in-the-wild capture without additional optimization.

</details>


### [15] [When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models](https://arxiv.org/abs/2602.10179)
*Jiacheng Hou,Yining Sun,Ruochong Jin,Haochen Han,Fangming Liu,Wai Kin Victor Chan,Alex Jinpeng Wang*

Main category: cs.CV

> 本文提出了视觉中心的越狱攻击(VJA)，展示了其对先进商用图像编辑模型的有效性，并提出了一种无训练的防御方法，提高了模型的整体安全性。

<details>
  <summary>Details</summary>

**Motivation:** 大型图像编辑模型已将范式从基于文本的指令编辑转向基于视觉提示的编辑，这种范式引入了一些尚未被充分探索的安全风险。本文旨在探索和解决这些安全风险。

**Method:** 通过引入IESBench，一个针对图像编辑模型的安全定向基准，系统性研究新兴威胁。提出了一种无训练防御方法，基于内省多模态推理，提高模型的安全性。

**Result:** 实验表明，VJA对Nano Banana Pro和GPT-Image-1.5的攻击成功率分别高达80.9%和70.1%，而提出的防御方法显著提高了模型的安全性。

**Conclusion:** 该研究揭示了新漏洞并提出解决方案，有利于促进现代图像编辑系统的安全性和可信赖性。

**Abstract:** Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.

</details>


### [16] [DEGMC: Denoising Diffusion Models Based on Riemannian Equivariant Group Morphological Convolutions](https://arxiv.org/abs/2602.10221)
*El Hadji S. Diop,Thierno Fall,Mohamed Daoudi*

Main category: cs.CV

> 本研究提出了一种通过引入黎曼流形上的群形态学卷积和特征线法到DDPM中来解决几何关键特征提取和网络等变性问题的方法，实验表明新方法在MNIST、RotoMNIST和CIFAR-10数据集上优于基线模型。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决当前DDPM在几何关键特征提取和网络等变性上的不足。由于DDPM的预测网络依赖于理论上只具有平移等变性的U-net架构，引入一种结合更加一般的欧几里得群（包括旋转、反射和排列）等变性的几何方法。

**Method:** 本研究通过引入在黎曼流形上的群形态学卷积来解决DDPM中的几何关键特征提取和网络等变性问题。这些卷积源于作为形态学多尺度扩张和腐蚀的一阶哈密顿-雅各比型偏微分方程的粘度解。通过在模型中添加对流项并使用特征线方法求解它，改进了对非线性、纤细几何结构的捕捉能力，并在学习过程中引入了对称性。

**Result:** 在MNIST、RotoMNIST和CIFAR-10数据集上的实验结果表明，与基线DDPM模型相比，改进方法有显著提升。

**Conclusion:** 新的模型通过群形态学卷积和特征线法的结合，有效提升了几何关键特征的提取能力和模型的等变性。实验结果表明这一方法具有更高的准确性和鲁棒性。

**Abstract:** In this work, we address two major issues in recent Denoising Diffusion Probabilistic Models (DDPM): {\bf 1)} geometric key feature extraction and {\bf 2)} network equivariance. Since the DDPM prediction network relies on the U-net architecture, which is theoretically only translation equivariant, we introduce a geometric approach combined with an equivariance property of the more general Euclidean group, which includes rotations, reflections, and permutations. We introduce the notion of group morphological convolutions in Riemannian manifolds, which are derived from the viscosity solutions of first-order Hamilton-Jacobi-type partial differential equations (PDEs) that act as morphological multiscale dilations and erosions. We add a convection term to the model and solve it using the method of characteristics. This helps us better capture nonlinearities, represent thin geometric structures, and incorporate symmetries into the learning process. Experimental results on the MNIST, RotoMNIST, and CIFAR-10 datasets show noticeable improvements compared to the baseline DDPM model.

</details>


### [17] [XSPLAIN: XAI-enabling Splat-based Prototype Learning for Attribute-aware INterpretability](https://arxiv.org/abs/2602.10239)
*Dominik Galus,Julia Farganus,Tymoteusz Zapala,Mikołaj Czachorowski,Piotr Borycki,Przemysław Spurek,Piotr Syga*

Main category: cs.CV

> XSPLAIN is a novel interpretability framework for 3D Gaussian Splatting that shows improved user understanding and preference according to a study.

<details>
  <summary>Details</summary>

**Motivation:** To improve the interpretability of 3DGS generation models and the classification of Splats, overcoming the ambiguity of saliency maps in existing explainability methods for other 3D representations.

**Method:** XSPLAIN, an ante-hoc, prototype-based interpretability framework for 3D Gaussian Splatting classification, uses a voxel-aggregated PointNet backbone and an invertible orthogonal transformation to enhance interpretability without compromising classification performance.

**Result:** XSPLAIN provides intuitive explanations grounded in representative training examples which significantly improved user preference in a user study without degrading classification performance.

**Conclusion:** XSPLAIN effectively enhances interpretability for 3DGS models, as evidenced by a user study demonstrating significant preference over baseline methods.

**Abstract:** 3D Gaussian Splatting (3DGS) has rapidly become a standard for high-fidelity 3D reconstruction, yet its adoption in multiple critical domains is hindered by the lack of interpretability of the generation models as well as classification of the Splats. While explainability methods exist for other 3D representations, like point clouds, they typically rely on ambiguous saliency maps that fail to capture the volumetric coherence of Gaussian primitives. We introduce XSPLAIN, the first ante-hoc, prototype-based interpretability framework designed specifically for 3DGS classification. Our approach leverages a voxel-aggregated PointNet backbone and a novel, invertible orthogonal transformation that disentangles feature channels for interpretability while strictly preserving the original decision boundaries. Explanations are grounded in representative training examples, enabling intuitive ``this looks like that'' reasoning without any degradation in classification performance. A rigorous user study (N=51) demonstrates a decisive preference for our approach: participants selected XSPLAIN explanations 48.4\% of the time as the best, significantly outperforming baselines $(p<0.001)$, showing that XSPLAIN provides transparency and user trust. The source code for this work is available at: https://github.com/Solvro/ml-splat-xai

</details>


### [18] [PMMA: The Polytechnique Montreal Mobility Aids Dataset](https://arxiv.org/abs/2602.10259)
*Qingwu Liu,Nicolas Saunier,Guillaume-Alexandre Bilodeau*

Main category: cs.CV

> The paper presents the PMMA dataset for detecting pedestrians with mobility aids, tests several object detection models, and concludes YOLOX, Deformable DETR, and Faster R-CNN perform best.

<details>
  <summary>Details</summary>

**Motivation:** To address the need for a dataset that includes pedestrians using mobility aids, such as wheelchairs, canes, and walkers, to improve object detection in outdoor scenarios involving these specific groups.

**Method:** This study builds a new object detection dataset called PMMA, which includes various types of pedestrians using different mobility aids. Seven object detection models and three tracking algorithms are implemented to establish a benchmark.

**Result:** Experimental results indicate that YOLOX, Deformable DETR, and Faster R-CNN achieved superior detection performance among the tested models. Differences between the tracking algorithms were minor.

**Conclusion:** The study successfully develops and validates the PMMA dataset, showing significant potential for advancing object detection technology, especially for pedestrians using mobility aids.

**Abstract:** This study introduces a new object detection dataset of pedestrians using mobility aids, named PMMA. The dataset was collected in an outdoor environment, where volunteers used wheelchairs, canes, and walkers, resulting in nine categories of pedestrians: pedestrians, cane users, two types of walker users, whether walking or resting, five types of wheelchair users, including wheelchair users, people pushing empty wheelchairs, and three types of users pushing occupied wheelchairs, including the entire pushing group, the pusher and the person seated on the wheelchair. To establish a benchmark, seven object detection models (Faster R-CNN, CenterNet, YOLOX, DETR, Deformable DETR, DINO, and RT-DETR) and three tracking algorithms (ByteTrack, BOT-SORT, and OC-SORT) were implemented under the MMDetection framework. Experimental results show that YOLOX, Deformable DETR, and Faster R-CNN achieve the best detection performance, while the differences among the three trackers are relatively small. The PMMA dataset is publicly available at https://doi.org/10.5683/SP3/XJPQUG, and the video processing and model training code is available at https://github.com/DatasetPMMA/PMMA.

</details>
