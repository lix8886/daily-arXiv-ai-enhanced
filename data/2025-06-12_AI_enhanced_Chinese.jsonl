{"id": "2506.09147", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09147", "abs": "https://arxiv.org/abs/2506.09147", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perli\u0107", "Ekaterina Borisova", "Markarit Vartampetian"], "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLM-as-a-qualitative-judge\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5176\u4e3b\u8981\u8f93\u51fa\u662f\u5bf9NLG\u7cfb\u7edf\u8f93\u51fa\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\u7c7b\u578b\u7684\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u8bc6\u522b\u7279\u5b9a\u5b9e\u4f8b\u95ee\u9898\u7684\u60c5\u51b5\u3002", "motivation": "\u5f53\u524d\uff0c\u5c06LLM\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u6587\u672c\u7684\u65b9\u6cd5\u4e3b\u8981\u662f\u5b9a\u91cf\u5de5\u5177\uff0c\u4e3b\u8981\u8f93\u51fa\u662f\u6570\u503c\u5206\u6570\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u76ee\u6807\u63d0\u4f9b\u6709\u5173\u5982\u4f55\u6539\u8fdb\u7ed9\u5b9a\u7684NLG\u7cfb\u7edf\u7684\u6709\u610f\u4e49\u89c1\u89e3\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a\u5f00\u653e\u5f0f\u9010\u5b9e\u4f8b\u95ee\u9898\u5206\u6790\u548c\u4f7f\u7528\u76f4\u89c2\u7684\u7d2f\u79ef\u7b97\u6cd5\u5bf9\u53d1\u73b0\u7684\u95ee\u9898\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLM-as-a-qualitative-judge\u57282/3\u7684\u60c5\u51b5\u4e0b\u6b63\u786e\u5730\u8bc6\u522b\u4e86\u5b9e\u4f8b\u7279\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u80fd\u591f\u4ea7\u751f\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6807\u6ce8\u5458\u7f16\u5236\u7684\u9519\u8bef\u7c7b\u578b\u62a5\u544a\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86LLM\u9664\u4e86\u4f5c\u4e3a\u5b9a\u91cf\u8bc4\u4f30\u5de5\u5177\u5916\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5b9a\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u95ee\u9898\u62a5\u544a\uff0c\u8fdb\u800c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u548c\u6539\u8fdbNLG\u7cfb\u7edf\u3002"}}
{"id": "2506.09175", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09175", "abs": "https://arxiv.org/abs/2506.09175", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u77ed\u8bed\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\uff0c\u76f8\u8f83\u4e8e\u77ed\u8bed\u5217\u8868\u504f\u7f6e\u65b9\u6cd5\uff0c\u63d0\u5347\u4e8621%\u7684\u76f8\u5bf9\u6027\u80fd\uff0c\u5e76\u4f7f\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77ed\u8bed\u53ec\u56de\u7387\u63d0\u9ad8\u4e8685%\u3002", "motivation": "\u77ed\u8bed\u5bf9\u4e8e\u7406\u89e3\u5bf9\u8bdd\u4e2d\u7684\u6838\u5fc3\u6982\u5ff5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7f55\u89c1\u51fa\u73b0\uff0c\u77ed\u8bed\u7684\u51c6\u786e\u7ffb\u8bd1\u5728\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u9887\u5177\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u4ece\u6e90\u8bed\u8a00\u5230\u76ee\u6807\u8bed\u8a00\u7684\u77ed\u8bed\u6620\u5c04\u5bf9\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u5230\u4e86\u4e24\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6a21\u578b\u4e2d\uff1a\u57fa\u4e8e\u8f6c\u5bfc\u5668\u7684\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u65b9\u6cd5\u6bd4\u77ed\u8bed\u5217\u8868\u504f\u7f6e\u65b9\u6cd5\u76f8\u5bf9\u63d0\u9ad8\u4e8621%\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u4f7f\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4f7f\u7528\u5916\u90e8\u77ed\u8bed\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e8685%\u7684\u76f8\u5bf9\u53ec\u56de\u7387\u63d0\u5347\u3002", "conclusion": "\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u8bed\u7ffb\u8bd1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09218", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09218", "abs": "https://arxiv.org/abs/2506.09218", "authors": ["Bruno Ferenc \u0160egedin"], "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "AI": {"tldr": "A study examining the capacity of CNNs to generate phonotactic generalizations from raw audio data finds that, with a narrow fully-connected layer, convolutional layers can dynamically generalize phonetic dependencies beyond the specific learned lexical configurations.", "motivation": "The motivation of this study is to understand if DNNs, particularly CNNs trained on audio waveforms, can make generalizations about phonotactics that go beyond the specific instances of words they've been trained on, and to examine how the architecture of the neural network influences this ability.", "method": "This study uses generative convolutional neural networks (CNNs) trained on raw audio waveforms of lexical items. It explores the impact of a narrow fully-connected layer (FC) bottleneck (shrunk from 1024 channels to 8) on the model's ability to generalize phonotactic rules derived from lexical learning.", "result": "The study finds that the convolutional layers can dynamically generalize phonetic dependencies beyond lexically-constrained configurations. A novel technique for probing the model's lexically-independent generalizations was proposed, which bypasses the FC layer and uses randomized feature maps fed into the convolutional block to generate audio outputs.", "conclusion": "The results suggest that generative convolutional neural networks are capable of learning and applying phonotactic generalizations beyond the specific lexical items they are trained on, especially when the fully-connected layer is constrained."}}
{"id": "2506.09251", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09251", "abs": "https://arxiv.org/abs/2506.09251", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4efb\u52a1\u5173\u8054\u5c55\u793a\u4e86Transformer\u6a21\u578b\u7684\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u53ef\u4ee5\u8de8\u4efb\u52a1\u8f6c\u79fb\uff0c\u5e76\u63d0\u4f9b\u4e86\u673a\u5236\u4e0a\u7684\u521d\u6b65\u8bc1\u636e\u3002", "motivation": "\u63a2\u8ba8Transformer\u8bed\u8a00\u6a21\u578b\u5904\u7406\u66f4\u957f\u8f93\u5165\u7684\u80fd\u529b\u662f\u5982\u4f55\u5f62\u6210\u7684\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u8fc7\u7684\u66f4\u957f\u8f93\u5165\u4e0a\u5982\u4f55\u6cdb\u5316\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u5173\u8054\u7684\u89c6\u89d2\u7814\u7a76\u4e86\u6a21\u578b\u5904\u7406\u66f4\u957f\u8f93\u5165\u7684\u80fd\u529b\uff0c\u5373\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u8868\u660e\uff0c\u8bad\u7ec3\u6a21\u578b\u65f6\u4f7f\u7528\u4e00\u4e2a\u8f83\u957f\u4e14\u76f8\u5173\u7684\u8f85\u52a9\u4efb\u52a1\uff0c\u53ef\u4ee5\u4f7f\u6a21\u578b\u5728\u5176\u4ed6\u76ee\u6807\u4efb\u52a1\u4e2d\u66f4\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u66f4\u957f\u8f93\u5165\u3002", "result": "\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u80fd\u591f\u5728\u4e0d\u540c\u7684\u7b97\u6cd5\u4efb\u52a1\u4e4b\u95f4\u8f6c\u79fb\uff0c\u5305\u62ec\u7b97\u672f\u8fd0\u7b97\u3001\u5b57\u7b26\u4e32\u8f6c\u6362\u548c\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\u3002\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u4e5f\u89c2\u5bdf\u5230\u4e86\u7c7b\u4f3c\u7684\u8f6c\u79fb\u6548\u679c\u3002", "conclusion": "\u6a21\u578b\u53ef\u4ee5\u7ee7\u627f\u6765\u81ea\u76f8\u4f3c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u957f\u5ea6\u6cdb\u5316\u7684\u8f6c\u79fb\u4e0e\u4efb\u52a1\u95f4\u6ce8\u610f\u529b\u5934\u7684\u91cd\u7528\u6709\u5173\uff0c\u8fd9\u52a0\u6df1\u4e86\u5bf9\u6a21\u578b\u5982\u4f55\u6cdb\u5316\u5230\u5206\u5e03\u5916\u8f93\u5165\u7684\u7406\u89e3\u3002"}}
{"id": "2506.09066", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09066", "abs": "https://arxiv.org/abs/2506.09066", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "comment": null, "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "AI": {"tldr": "ReStNet solves the problem of deploying pre-trained models in IoT with varying resources by stitching two models together and selectively fine-tuning them for efficiency and adaptability.", "motivation": "The motivation behind ReStNet is to address resource constraints in IoT applications, where deploying a single model across all platforms with heterogeneous computational and memory resources is not feasible and existing compression methods are inflexible.", "method": "ReStNet, a Reusable and Stitchable Network, dynamically constructs a hybrid network by stitching two pre-trained models together, using Centered Kernel Alignment (CKA) for determining stitching points, and fine-tuning only the stitching layer for efficiency.", "result": "ReStNet demonstrates flexible accuracy-efficiency trade-offs at runtime and significantly reduces training cost through its novel approach to stitching models.", "conclusion": "ReStNet enables rapid adaptation to changing resource budgets and flexible combination of different model families, making it suitable for efficient deployment in varying IoT environments."}}
{"id": "2506.09259", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2506.09259", "abs": "https://arxiv.org/abs/2506.09259", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u81ea\u6211\u951a\u5b9a\u6ce8\u610f\u529b\u6a21\u578b\uff08SAAM\uff09\u6765\u8bc6\u522b\u5e76\u5206\u7c7b\u6e38\u620f\u4e2d\u4eb2\u793e\u4f1a\u884c\u4e3a\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u300a\u4f7f\u547d\u53ec\u5524\uff1a\u73b0\u4ee3\u6218\u4e89II\u300b\u4e2d\uff0c\u63d0\u9ad8\u4e86\u4eb2\u793e\u4f1a\u6c9f\u901a\u884c\u4e3a\u7684\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u68c0\u6d4b\u6e38\u620f\u4e2d\u5c11\u91cf\u7684\u6709\u6bd2\u5185\u5bb9\u4ee5\u5b9e\u73b0\u7ba1\u7406\u76ee\u7684\uff0c\u800c\u65b0\u5174\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u68c0\u6d4b\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u91cd\u8981\u6027\u3002\u7136\u800c\uff0c\u5728\u6e38\u620f\u804a\u5929\u6587\u672c\u4e2d\u8bc6\u522b\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8d44\u6e90\u76f8\u5f53\u532e\u4e4f\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u79ef\u6781\u4e92\u52a8\u7684\u9f13\u52b1\u3002", "method": "\u91c7\u7528\u4e86\u65e0\u76d1\u7763\u7684\u53d1\u73b0\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6e38\u620f\u9886\u57df\u4e13\u5bb6\u7684\u5408\u4f5c\uff0c\u4ece\u6e38\u620f\u4e2d\u73a9\u5bb6\u7684\u804a\u5929\u8bb0\u5f55\u4e2d\u8bc6\u522b\u548c\u5206\u7c7b\u4eb2\u793e\u4f1a\u884c\u4e3a\u3002\u63d0\u51fa\u4e86\u81ea\u6211\u951a\u5b9a\u6ce8\u610f\u529b\u6a21\u578b\uff08Self-Anchored Attention Model\uff0cSAAM\uff09\uff0c\u8be5\u6a21\u578b\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u4f73\u6280\u672f\u63d0\u9ad8\u4e867.9%\u3002\u901a\u8fc7\u4f7f\u7528\u6574\u4e2a\u8bad\u7ec3\u96c6\u4f5c\u4e3a\u201c\u951a\u70b9\u201d\u6765\u5e2e\u52a9\u5728\u7f3a\u4e4f\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6210\u679c\u5305\u62ec\u63d0\u51fa\u4e86\u81ea\u6211\u951a\u5b9a\u6ce8\u610f\u529b\u6a21\u578b\uff08SAAM\uff09\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u4f73\u6280\u672f\u63d0\u9ad8\u4e867.9%\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u81ea\u52a8\u5206\u7c7b\u3002\u7814\u7a76\u5c55\u793a\u4e86\u5728\u4ec5\u6709\u5c11\u91cf\u5316\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u4ecd\u7136\u6709\u6548\u3002", "conclusion": "\u8fd9\u7bc7\u7814\u7a76\u662f\u9996\u6b21\u5c06NLP\u6280\u672f\u5e94\u7528\u4e8e\u53d1\u73b0\u548c\u5206\u7c7b\u73a9\u5bb6\u5728\u6e38\u620f\u4e2d\u7684\u4eb2\u793e\u4f1a\u6c9f\u901a\u884c\u4e3a\u7684\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u4ece\u4ec5\u5173\u6ce8\u51cf\u5c11\u6d88\u6781\u5185\u5bb9\u5230\u4e3b\u52a8\u9f13\u52b1\u79ef\u6781\u4e92\u52a8\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.09067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09067", "abs": "https://arxiv.org/abs/2506.09067", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "comment": null, "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u62b5\u5fa1\u89c6\u89c9\u548c\u6587\u672c\u653b\u51fb\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u540c\u65f6\u63d0\u9ad8Med-VLMs\u7684\u5b89\u5168\u6027\u800c\u4e0d\u4f1a\u663e\u8457\u964d\u4f4e\u5176\u6027\u80fd\u3002", "motivation": "Med-VLMs\u7684\u5b89\u5168\u6f0f\u6d1e\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\uff0c\u8ba9\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u62d2\u7edd\u6709\u5bb3\u67e5\u8be2\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u9632\u5fa1\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u65f6\u9632\u5fa1\u7b56\u7565\uff0c\u80fd\u591f\u62b5\u5fa1\u89c6\u89c9\u548c\u6587\u672c\u5f62\u5f0f\u7684\u653b\u51fb\uff0c\u901a\u8fc7\u589e\u52a0\u5408\u6210\u4e34\u5e8a\u6f14\u793a\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6df7\u5408\u5c55\u793a\u7b56\u7565\uff0c\u65e8\u5728\u9884\u7b97\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5e73\u8861\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u9632\u5fa1\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u6df7\u5408\u5c55\u793a\u7b56\u7565\u5728\u9762\u5bf9\u6709\u9650\u9884\u7b97\u65f6\u4e5f\u80fd\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u901a\u8fc7\u5408\u6210\u4e34\u5e8a\u6f14\u793a\u589e\u91cf\uff0c\u63d0\u51fa\u7684\u9632\u5fa1\u7b56\u7565\u53ef\u4ee5\u589e\u5f3aMed-VLMs\u7684\u5b89\u5168\u6027\u800c\u4e0d\u663e\u8457\u635f\u5bb3\u6027\u80fd\u3002\u589e\u52a0\u6f14\u793a\u9884\u7b97\u80fd\u8fdb\u4e00\u6b65\u89e3\u51b3\u8fc7\u5ea6\u9632\u5fa1\u95ee\u9898\u3002"}}
{"id": "2506.09277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09277", "abs": "https://arxiv.org/abs/2506.09277", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE.", "AI": {"tldr": "The paper develops a framework to measure the faithfulness of LLM self-explanations by comparing them to the model's internal processes, aiming to enhance the reliability of these explanations and foster future improvements.", "motivation": "The motivation arises from the fact that current LLM-generated self-NLEs can be logically coherent but not truly reflective of the model's actual decision-making process, leading to unfaithful explanations. This study aims to fill the gap in understanding and improving the faithfulness of these explanations.", "method": "This paper proposes a flexible framework to measure the faithfulness of LLM-generated self-Natural Language Explanations (self-NLE) by comparing self-NLE with the interpretations of the model's internal neural activities, thereby offering deep insights into the connection between self-NLE and the model's reasoning process.", "result": "The paper establishes a novel approach for quantifying self-NLE faithfulness which connects the explanations to the model's internal reasoning, advancing the understanding of self-NLE authenticity and laying groundwork for the generation of more reliable self-NLE in the future.", "conclusion": "The work concludes with a robust framework for assessing self-NLE faithfulness in LLMs by linking the generated explanations to the model's internal neural states, paving the way for more faithful self-NLEs in the future."}}
{"id": "2506.09068", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09068", "abs": "https://arxiv.org/abs/2506.09068", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86BG-HOP\uff0c\u4e00\u79cd\u65e8\u5728\u5efa\u6a213D\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\u7684\u751f\u6210\u5148\u9a8c\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6355\u6349\u624b\u548c\u7269\u8054\u5408\u5206\u5e03\u7684\u521d\u6b65\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u5c55\u73b0\u6709\u7684\u5355\u624b\u751f\u6210\u5148\u9a8c\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86BG-HOP\u6a21\u578b\u6765\u751f\u6210\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\uff0c\u5e76\u4e3a\u7ed9\u5b9a\u7269\u4f53\u5408\u6210\u6293\u53d6\u52a8\u4f5c\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\uff0c\u5e76\u4e3a\u7ed9\u5b9a\u7269\u4f53\u5408\u6210\u6293\u63e1\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86BG-HOP\u5728\u751f\u6210\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6a21\u578b\u3002"}}
{"id": "2506.09301", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09301", "abs": "https://arxiv.org/abs/2506.09301", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86$(RSA)^2$\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u8bf4\u8bdd\u4eba\u7684\u4fee\u8f9e\u7b56\u7565\u6765\u89e3\u91ca\u6bd4\u55bb\u8bed\u8a00\uff0c\u907f\u514d\u4e86\u5bf9\u52a8\u673a\u7684\u7279\u5b9a\u573a\u666f\u5316\u5efa\u6a21\uff0c\u5e76\u5728\u8bbd\u523a\u8bed\u8a00\u7406\u89e3\u4e0a\u8fbe\u5230\u4e86\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684RSA\u6846\u67b6\u65e0\u6cd5\u89e3\u91ca\u6bd4\u55bb\u8868\u8fbe\uff0c\u6216\u8005\u9700\u8981\u4ee5\u7279\u5b9a\u573a\u666f\u7684\u65b9\u5f0f\u6a21\u62df\u8bf4\u8bdd\u4eba\u4f7f\u7528\u6bd4\u55bb\u8bed\u8a00\u7684\u9690\u542b\u52a8\u673a\u3002\u4e3a\u4e86\u4eba\u7c7b\u517c\u5bb9\u5730\u89e3\u91ca\u975e\u5b57\u9762\u8bed\u8a00\uff0c\u672c\u6587\u63d0\u51fa\u4e86$(RSA)^2$\u6846\u67b6\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86$(RSA)^2$\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u8003\u8651\u8bf4\u8bdd\u4eba\u4f7f\u7528\u7684\u4fee\u8f9e\u7b56\u7565\u6765\u5efa\u6a21\u6bd4\u55bb\u8bed\u8a00\u7684\u4f7f\u7528\u3002", "result": "$(RSA)^2$\u6846\u67b6\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5728\u65b0\u5f15\u5165\u7684PragMega+\u6570\u636e\u96c6\u7684\u8bbd\u523a\u5206\u5272\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86$(RSA)^2$\u6846\u67b6\u5728\u7406\u89e3\u975e\u5b57\u9762\u8bed\u8a00\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u65e0\u9700\u6a21\u62df\u8bf4\u8bdd\u4eba\u4f7f\u7528\u975e\u5b57\u9762\u8bed\u8a00\u7684\u5177\u4f53\u52a8\u673a\uff0c\u5373\u53ef\u5b9e\u73b0\u4eba\u7c7b\u517c\u5bb9\u7684\u89e3\u91ca\u3002"}}
{"id": "2506.09071", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09071", "abs": "https://arxiv.org/abs/2506.09071", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "comment": null, "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8bed\u4e49\u5f15\u5bfc\u7684\u5efa\u7b51\u5916\u5899\u548c\u7a97\u6237\u5206\u5272\u6a21\u578bSAAF\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u6587\u672c\u63cf\u8ff0\u5230\u56fe\u50cf\u5206\u5272\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5e76\u5728\u591a\u4e2a\u5916\u5899\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u5728\u5efa\u7b51\u6570\u5b57\u5316\u53d1\u5c55\u80cc\u666f\u4e0b\uff0c\u5efa\u7b51\u7269\u4fe1\u606f\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\u7684\u6548\u7387\u53ef\u4ee5\u901a\u8fc7\u81ea\u52a8\u5206\u5272\u5899\u4f53\u548c\u7a97\u6237\u5f97\u5230\u6709\u6548\u63d0\u5347\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8bed\u4e49\u5f15\u5bfc\u7684\u5efa\u7b51\u5916\u5899\u548c\u7a97\u6237\u81ea\u52a8\u5206\u5272\u6a21\u578bSegment Any Architectural Facades (SAAF)\u3002\u9996\u5148\uff0cSAAF\u5177\u5907\u591a\u6a21\u6001\u8bed\u4e49\u534f\u540c\u7279\u5f81\u63d0\u53d6\u673a\u5236\uff0c\u53ef\u4ee5\u878d\u5408\u6587\u672c\u63cf\u8ff0\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u548c\u56fe\u50cf\u7279\u5f81\uff0c\u63d0\u9ad8\u5bf9\u5efa\u7b51\u5916\u5899\u6784\u4ef6\u7684\u8bed\u4e49\u7406\u89e3\u3002\u5176\u6b21\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u4ece\u6587\u672c\u63cf\u8ff0\u5230\u56fe\u50cf\u5206\u5272\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u81ea\u52a8\u5316\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5916\u5899\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cSAAF\u7684\u5206\u5272\u7ed3\u679c\u5728mIoU\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u8868\u660e\u8be5\u6a21\u578b\u5728\u9762\u5bf9\u591a\u6837\u5316\u6570\u636e\u96c6\u65f6\u80fd\u591f\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u5206\u5272\u80fd\u529b\u3002", "conclusion": "SAAF\u6a21\u578b\u5728\u63d0\u9ad8\u5899\u4f53\u548c\u7a97\u6237\u5206\u5272\u4efb\u52a1\u7684\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u8fdb\u5c55\uff0c\u53ef\u4ee5\u63d0\u4f9b\u5173\u4e8e\u5efa\u7b51\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u548c\u591a\u6a21\u6001\u5b66\u4e60\u5728\u5efa\u7b51\u9886\u57df\u7684\u65b0\u601d\u8def\u548c\u6280\u672f\u8def\u5f84\u53c2\u8003\u3002"}}
{"id": "2506.09315", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09315", "abs": "https://arxiv.org/abs/2506.09315", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Mistral-7B\u6a21\u578b\u589e\u5f3a\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u8bed\u8a00\u68c0\u6d4b\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6e05\u6670\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u662f\u4e00\u79cd\u5f71\u54cd\u8ba4\u77e5\u80fd\u529b\u7684\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff0c\u901a\u5e38\u4f1a\u5f71\u54cd\u8bed\u8a00\u80fd\u529b\u3002\u8fd9\u9879\u7814\u7a76\u7684\u76ee\u7684\u662f\u63d0\u9ad8\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e86Mistral-7B\u6307\u4ee4\u8ddf\u968f\u7248\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u6210\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\u3002", "result": "\u76f8\u6bd4\u4e8e\u5f53\u524d\u6700\u4f73\u6210\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\u548cADReSS 2020\u6311\u6218\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u597d\u7684\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e863.33%\u548c6.35%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff0c\u5e76\u4e14\u5177\u6709\u6e05\u6670\u4e14\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u6a21\u578b\u5df2\u7ecf\u5b66\u4f1a\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u60a3\u8005\u7279\u6709\u7684\u8bed\u8a00\u6a21\u5f0f\uff0c\u4e3a\u6a21\u578b\u89e3\u91ca\u548c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.09079", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09079", "abs": "https://arxiv.org/abs/2506.09079", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "comment": null, "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "AI": {"tldr": "This paper introduces VersaVid-R1, a new model for video reasoning, using innovative datasets and reinforcement learning.", "motivation": "The motivation behind this paper is to address the underdeveloped field of video-based reasoning due to data scarcity and ineffective training methodologies.", "method": "The paper develops a new model named VersaVid-R1 by leveraging two novel datasets, DarkEventInfer and MixVidQA, and training with reinforcement learning guided by diverse reward functions.", "result": "Experiments show that VersaVid-R1 surpasses existing models across various benchmarks, including video understanding, cognitive reasoning, and captioning tasks.", "conclusion": "The conclusion is that the developed model, VersaVid-R1, represents a significant progress in video understanding and reasoning under the Reason-Then-Respond paradigm."}}
{"id": "2506.09329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09329", "abs": "https://arxiv.org/abs/2506.09329", "authors": ["Yuxin Jiang"], "title": "Towards Efficient and Effective Alignment of Large Language Models", "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "AI": {"tldr": "The paper presents new methodologies for data collection, training, and evaluation to improve LLM alignment with human expectations, introducing frameworks like Lion, WebR, LTE, and BMC and the FollowBench for constraint adherence.", "motivation": "To efficiently and effectively align LLMs with human expectations.", "method": "Lion, an adversarial distillation framework for alignment data collection; Web Reconstruction (WebR), a fully automated framework for synthesizing instruction-tuning data; Learning to Edit (LTE), a framework for efficient knowledge integration; Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) for better alignment across QA and mathematical reasoning tasks.", "result": "State-of-the-art zero-shot reasoning with Lion; improved data diversity and scalability with WebR; improved real-time and batch knowledge updates with LTE; superior alignment across QA and mathematical reasoning tasks with BMC; key weaknesses in current models' constraint adherence identified with FollowBench.", "conclusion": "The paper proposes and evaluates several novel methodologies in data collection, training, and evaluation for LLM alignment, significantly advancing the state-of-the-art in zero-shot reasoning, knowledge integration, and constraint adherence."}}
{"id": "2506.09081", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09081", "abs": "https://arxiv.org/abs/2506.09081", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "AI": {"tldr": "FlagEvalMM is an open-source evaluation framework for multimodal models, designed to assess various vision-language tasks efficiently and accurately.", "motivation": "The motivation is to provide a comprehensive evaluation framework for multimodal models that can assess vision-language understanding and generation tasks with improved efficiency and accuracy.", "method": "We decouple model inference from evaluation through an independent evaluation service, enabling flexible resource allocation and seamless integration of new tasks and models. FlagEvalMM uses advanced inference acceleration tools and asynchronous data loading to enhance evaluation efficiency.", "result": "Extensive experiments demonstrate that FlagEvalMM provides accurate and efficient insights into the performance of multimodal models.", "conclusion": "FlagEvalMM is a valuable tool for advancing research in multimodal models, offering a framework for accurate and efficient model evaluation."}}
{"id": "2506.09331", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.09331", "abs": "https://arxiv.org/abs/2506.09331", "authors": ["Arjun Vaithilingam Sudhakar"], "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "AI": {"tldr": "\u7814\u7a76LLMs\u662f\u5426\u62e5\u6709\u7406\u8bba\u5fc3\u667a\uff0c\u901a\u8fc7\u5728\u5176\u57fa\u7840\u4e0a\u521b\u5efa\u80fd\u591f\u81ea\u7136\u8bed\u8a00\u4e92\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u4ee5\u5b9e\u73b0\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e0b\u7684\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u7c7b\u7684\u826f\u597d\u534f\u4f5c\u3002", "motivation": "\u63a2\u8ba8LLMs\u662f\u5426\u80fd\u6a21\u62df\u548c\u63a8\u7406\u4ed6\u4eba\u7684\u610f\u56fe\uff0c\u5373LLMs\u662f\u5426\u5177\u6709\u67d0\u79cd\u5f62\u5f0f\u7684\u7406\u8bba\u5fc3\u667a\u3002\u8fd9\u5728\u4eba\u4e0e\u4eba\u4ee5\u53ca\u4eba\u4e0e\u81ea\u4e3b\u7cfb\u7edf\u95f4\u7684\u6709\u6548\u534f\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u89c6\u89d2\u7814\u7a76LLMs\u7684\u7406\u8bba\u5fc3\u667a\uff0c\u8be5\u89c6\u89d2\u4e2d\u667a\u80fd\u4f53\u901a\u8fc7\u91cd\u590d\u4e92\u52a8\u5b66\u4e60\u534f\u4f5c\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u793e\u4f1a\u63a8\u7406\u3002", "result": "\u672a\u76f4\u63a5\u7ed9\u51fa\u7814\u7a76\u6210\u679c\uff0c\u4f46\u8be5\u9879\u7814\u7a76\u65e8\u5728\u63d0\u5347\u4eba\u5de5\u667a\u80fd\u667a\u80fd\u4f53\u9002\u5e94\u5e76\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u53ca\u4eba\u7c7b\u5408\u4f5c\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6765\u521b\u5efa\u4eba\u673a\u6df7\u5408\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u6709\u671d\u4e00\u65e5\u80fd\u591f\u5b9e\u73b0\u65e0\u7f1d\u534f\u4f5c\uff0c\u5bf9\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u4e92\u52a8\u672a\u6765\u5177\u6709\u5e7f\u6cdb\u7684\u610f\u4e49\u3002"}}
{"id": "2506.09082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09082", "abs": "https://arxiv.org/abs/2506.09082", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "comment": "First two authors contribute equally", "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "AI": {"tldr": "\u4e3a\u4e86\u7cfb\u7edf\u5730\u8bc4\u4f30\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFMs)\uff0c\u7814\u7a76\u5f15\u5165\u4e86AVA-Bench\uff0c\u4e00\u4e2a\u660e\u786e\u5206\u89e314\u4e2a\u539f\u5b50\u89c6\u89c9\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684VFMs\u8bc4\u4f30\uff0c\u5e76\u4e14\u51cf\u5c11\u4e86GPU\u65f6\u95f4\uff0c\u63d0\u5347\u4e86\u8bc4\u4f30\u6548\u7387\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFMs)\u8bc4\u4f30\u4e2d\u7684\u4e24\u4e2a\u76f2\u70b9\uff1a1)\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u53ef\u80fd\u4e0eVQA\u6d4b\u8bd5\u5206\u5e03\u4e0d\u5339\u914d\uff1b2)VQA\u57fa\u51c6\u53ef\u80fd\u9700\u8981\u591a\u79cd\u89c6\u89c9\u80fd\u529b\uff0c\u96be\u4ee5\u5b9a\u4f4d\u9519\u8bef\u6765\u6e90\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u5f15\u5165AVA-Bench\u6765\u4f18\u5316VFMs\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51faAVA-Bench\uff0c\u7b2c\u4e00\u4e2a\u660e\u786e\u5206\u89e314\u4e2a\u539f\u5b50\u89c6\u89c9\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5982\u5b9a\u4f4d\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u7a7a\u95f4\u7406\u89e3\uff0c\u8fd9\u4e9b\u80fd\u529b\u5171\u540c\u652f\u6301\u590d\u6742\u7684\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u3002\u901a\u8fc7\u5206\u79bb\u8fd9\u4e9bAVAs\u5e76\u5728\u6bcf\u4e2aAVAs\u4e2d\u5339\u914d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\uff0cAVA-Bench\u53ef\u4ee5\u7cbe\u786e\u6307\u51faVFMs\u7684\u8868\u73b0\u3002", "result": "\u5e94\u7528AVA-Bench\u8bc4\u4f30\u9886\u5148\u7684VFMs\u63ed\u793a\u4e86\u72ec\u7279\u7684\"\u80fd\u529b\u6307\u7eb9\"\uff0c\u5c06VFMs\u7684\u9009\u62e9\u4ece\u7ecf\u9a8c\u731c\u6d4b\u8f6c\u53d8\u4e3a\u539f\u5219\u6027\u5de5\u7a0b\u5de5\u4f5c\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c0.5B LLM\u4e0e7B LLM\u53ef\u4ee5\u5f97\u5230\u7c7b\u4f3c\u7684VFMs\u6392\u540d\uff0c\u540c\u65f6\u51cf\u5c11\u4e868\u500d\u7684GPU\u65f6\u95f4\uff0c\u4f7f\u8bc4\u4f30\u66f4\u9ad8\u6548\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u5168\u9762\u900f\u660e\u7684\u57fa\u51c6\uff0cAVA-Bench\u4e3a\u4e0b\u4e00\u4ee3\u8868\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFMs)\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.09340", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09340", "abs": "https://arxiv.org/abs/2506.09340", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "title": "RePO: Replay-Enhanced Policy Optimization", "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "This paper presents Replay-Enhanced Policy Optimization (RePO), a method that improves upon GRPO in terms of data efficiency and computational cost while optimizing large language models for reinforcement learning.", "motivation": "The motivation behind this paper is to find a more efficient method in terms of computational costs and data usage for optimizing LLMs using reinforcement learning, compared to the existing Group Relative Policy Optimization (GRPO) method.", "method": "Replay-Enhanced Policy Optimization (RePO) is introduced to improve data efficiency and reduce computational costs when optimizing large language models (LLMs) for reinforcement learning tasks. It retrieves off-policy samples from a replay buffer to enrich the dataset used for policy optimization.", "result": "Experimental results show that RePO achieves significant performance gains over GRPO, with absolute average performance improvements of 18.4 and 4.1 points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively. Additionally, for Qwen3-1.7B, the number of effective optimization steps rose by 48% while increasing computational cost by only 15%.", "conclusion": "The conclusion is that RePO increases the data efficiency and reduces the computational cost while optimizing large language models for reinforcement learning tasks. It provides significant performance gains with a relatively small increase in computational costs."}}
{"id": "2506.09083", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09083", "abs": "https://arxiv.org/abs/2506.09083", "authors": ["Jerry Lin", "Partick P. W. Chen"], "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "comment": "4 pages, 3 figures, 1 Table", "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BakuFlow\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u534a\u81ea\u52a8\u6807\u6ce8\u751f\u6210\u5de5\u5177\uff0c\u5b83\u5177\u6709\u591a\u79cd\u521b\u65b0\u529f\u80fd\uff0c\u65e8\u5728\u63d0\u9ad8\u6807\u6ce8\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u51c6\u786e\u6807\u6ce8\u6570\u636e\u4ecd\u7136\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u4efb\u52a1\u4e2d\uff0c\u624b\u52a8\u6807\u6ce8\u8d39\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86BakuFlow\uff0c\u4e00\u79cd\u534a\u81ea\u52a8\u6807\u6ce8\u751f\u6210\u5de5\u5177\uff0c\u5b83\u5177\u6709\u591a\u79cd\u529f\u80fd\uff1a\uff081\uff09\u4e00\u4e2a\u53ef\u8c03\u653e\u5927\u955c\uff0c\u7528\u4e8e\u50cf\u7d20\u7ea7\u7cbe\u786e\u7684\u624b\u52a8\u4fee\u6b63\uff1b\uff082\uff09\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6570\u636e\u589e\u5f3a\u6a21\u5757\uff0c\u53ef\u7528\u4e8e\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u96c6\uff1b\uff083\uff09\u6807\u7b7e\u4f20\u64ad\u529f\u80fd\uff0c\u53ef\u4ee5\u5c06\u6807\u6ce8\u5bf9\u8c61\u8fc5\u901f\u590d\u5236\u5230\u8fde\u7eed\u5e27\u4e2d\uff1b\uff084\uff09\u81ea\u52a8\u6807\u6ce8\u6a21\u5757\uff0c\u57fa\u4e8e\u6539\u8fdb\u7684YOLOE\u6846\u67b6\u3002", "result": "BakuFlow\u5728\u5b9e\u9645\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u5de5\u4e1a\u573a\u666f\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u63d0\u5347\u4e86\u6807\u6ce8\u6548\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u521b\u65b0\u4f7f\u5f97BakuFlow\u7279\u522b\u9002\u7528\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u548c\u8ffd\u8e2a\u4efb\u52a1\uff0c\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u5927\u5e45\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u4e86\u6548\u7387\u3002"}}
{"id": "2506.09342", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09342", "abs": "https://arxiv.org/abs/2506.09342", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Latent Multi-Head Attention for Small Language Models", "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86MLA+RoPE\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6548\u7387\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u663e\u8457\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u8282\u7701\u548c\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u6f5c\u591a\u5934\u6ce8\u610f\u529b\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u4ee5\u63ed\u793a\u6f5c\u5728\u7684\u6548\u7387\u548c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "method": "\u6211\u4eec\u7814\u7a76\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6f5c\u591a\u5934\u6ce8\u610f\u529b\uff08MLA\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u67b6\u6784\u53d8\u4f53\u7684\u6bd4\u8f83\uff1a\u6807\u51c6\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u3001MLA\u4ee5\u53ca\u5e26\u6709\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u7684MLA\uff08MLA+RoPE\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e26\u6709\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u7684MLA\uff08MLA+RoPE\uff09\u5728\u4f7f\u7528\u534a\u79e9\u6f5c\u5728\u7ef4\u5ea6\u65f6\uff08r=d/2\uff09\uff0c\u53ef\u4ee5\u5b9e\u73b045%\u7684KV\u7f13\u5b58\u5185\u5b58\u964d\u4f4e\uff0c\u540c\u65f6\u53ea\u589e\u52a00.3%\u7684\u9a8c\u8bc1\u635f\u5931\uff1b\u5e76\u4e14\uff0c\u5728\u5c0f\u6a21\u578b\u4e2d\uff0c\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u5bf9\u4e8eMLA\u7684\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "MLA+RoPE\u5728\u786e\u4fdd\u6a21\u578b\u8d28\u91cf\uff087.4/10\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u4e5f\u5177\u5907\u4e86\u66f4\u597d\u7684\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u901f\u5ea6\u3002\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u5728\u63a5\u53d7\u540e\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2506.09106", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09106", "abs": "https://arxiv.org/abs/2506.09106", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "title": "Bias Analysis in Unconditional Image Generative Models", "comment": null, "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u504f\u89c1\u68c0\u6d4b\u7684\u654f\u611f\u5ea6\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8fde\u7eed\u6027\u5c5e\u6027\u4e0a\u3002\u7814\u7a76\u8868\u660e\u9700\u8981\u6539\u5584\u8bc4\u4f30\u6846\u67b6\u5e76\u66f4\u6df1\u5165\u7406\u89e3\u5c5e\u6027\u7684\u793e\u4f1a\u590d\u6742\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u504f\u89c1\u95ee\u9898\u7684\u6587\u732e\u8d8a\u6765\u8d8a\u591a\uff0c\u4f46\u5173\u4e8e\u65e0\u6761\u4ef6\u751f\u6210\u4e2d\u504f\u89c1\u4ea7\u751f\u7684\u673a\u5236\u4ecd\u7136\u4e0d\u6e05\u6670\u3002\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e00\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u504f\u89c1\u53d8\u5316\u3002", "method": "\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5c5e\u6027\u7684\u504f\u89c1\u4e3a\u8be5\u5c5e\u6027\u5728\u89c2\u5bdf\u5206\u5e03\u4e2d\u7684\u5b58\u5728\u6982\u7387\u4e0e\u5176\u5728\u7406\u60f3\u53c2\u8003\u5206\u5e03\u4e2d\u7684\u671f\u671b\u6bd4\u4f8b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u7814\u7a76\u901a\u8fc7\u8bad\u7ec3\u82e5\u5e72\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u4e00\u4e2a\u5e38\u7528\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\u6765\u7814\u7a76\u8bad\u7ec3\u5206\u5e03\u548c\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684\u504f\u89c1\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u68c0\u6d4b\u5230\u7684\u5c5e\u6027\u8f6c\u79fb\u76f8\u5bf9\u8f83\u5c0f\u3002\u5c5e\u6027\u8f6c\u79fb\u5bf9\u7528\u4e8e\u6807\u8bb0\u751f\u6210\u56fe\u50cf\u7684\u5206\u7c7b\u5668\u975e\u5e38\u654f\u611f\uff0c\u5c24\u5176\u662f\u5728\u51b3\u7b56\u8fb9\u754c\u4f4d\u4e8e\u9ad8\u5bc6\u5ea6\u533a\u57df\u65f6\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u5728\u8fde\u7eed\u6027\u5c5e\u6027\u4e0a\u8fd9\u79cd\u5206\u7c7b\u5668\u654f\u611f\u5ea6\u5e38\u88ab\u89c2\u5bdf\u5230\uff0c\u800c\u975e\u663e\u793a\u51fa\u4e8c\u5143\u7279\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u5177\u6709\u4ee3\u8868\u6027\u7684\u6807\u6ce8\u5b9e\u8df5\uff0c\u5bf9\u8bc4\u4f30\u6846\u67b6\u7684\u4e0d\u8db3\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u5ba1\u67e5\uff0c\u5e76\u8ba4\u8bc6\u5230\u5728\u8bc4\u4ef7\u504f\u89c1\u65f6\u5c5e\u6027\u7684\u793e\u4f1a\u590d\u6742\u6027\u3002"}}
{"id": "2506.09349", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09349", "abs": "https://arxiv.org/abs/2506.09349", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "AI": {"tldr": "This paper introduces OmniDRCA, a parallel speech-text generation model based on joint autoregressive modeling, achieving new state-of-the-art performance in speech-text generation.", "motivation": "The motivation behind this paper is to address the limitations of existing methods for generating discrete speech tokens with LLMs, which fall short in either failing to incorporate speech tokens into the LLM's autoregressive process or generate speech and text tokens independently.", "method": "Our approach, named OmniDRCA, relies on parallel speech and text processing featuring dual-resolution speech representations and contrastive cross-modal alignment to enhance audio comprehension.", "result": "OmniDRCA has achieved state-of-the-art performance on Spoken Question Answering benchmarks among models based on parallel joint speech-text modeling, and competitive performance relative to interleaved models.", "conclusion": "This work presents an innovative method to parallelly generate speech and text with autoregressive modeling, advancing the state-of-the-art in speech-text foundation models. It also lays the groundwork for future work in full-duplex conversational scenarios."}}
{"id": "2506.09109", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09109", "abs": "https://arxiv.org/abs/2506.09109", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "AI": {"tldr": "\u5f15\u5165\u65b0\u578b\u8bc4\u4f30\u5ea6\u91cfCAIRe\uff0c\u8ba1\u7b97\u7ed9\u5b9a\u6807\u7b7e\u96c6\u4e0b\u7684\u56fe\u50cf\u6587\u5316\u76f8\u5173\u6027\uff0c\u514b\u670d\u4e86\u6587\u5316\u504f\u89c1\u7684\u6d4b\u91cf\u74f6\u9888\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5b83\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u4e00\u81f4\u6027\u80fd\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u3001\u4e8b\u5b9e\u4e0d\u51c6\u786e\u6216\u8f93\u51fa\u5192\u72af\u6027\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002\u7531\u4e8e\u7f3a\u4e4f\u53ef\u9760\u8bc4\u4f30\u6587\u5316\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e00\u95ee\u9898\u96be\u4ee5\u89e3\u51b3\u3002", "method": "CAIRe\u8bc4\u4f30\u6307\u6807\u7528\u4e8e\u8bc4\u4f30\u7ed9\u5b9a\u7528\u6237\u5b9a\u4e49\u6807\u7b7e\u96c6\u7684\u56fe\u50cf\u7684\u6587\u5316\u76f8\u5173\u6027\u7a0b\u5ea6\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u4e2d\u7684\u5b9e\u4f53\u548c\u6982\u5ff5\u4e0e\u77e5\u8bc6\u5e93\u8fdb\u884c\u5173\u8054\uff0c\u5e76\u4f7f\u7528\u4e8b\u5b9e\u4fe1\u606f\u4e3a\u6bcf\u4e2a\u6587\u5316\u6807\u7b7e\u63d0\u4f9b\u72ec\u7acb\u7684\u5206\u7ea7\u5224\u65ad\u3002", "result": "\u5728\u624b\u52a8\u521b\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0cCAIRe\u8d85\u8fc7\u6240\u6709\u57fa\u7ebf\u6a21\u578b28\u4e2a\u767e\u5206\u70b9\u7684F1\u5f97\u5206\u3002\u53e6\u5916\uff0c\u5bf9\u4e8e\u4e24\u79cd\u666e\u904d\u6982\u5ff5\u7684\u6570\u636e\u96c6\uff0cCAIRe\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8fbe\u52300.56\u548c0.66\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u5f70\u663e\u51fa\u5176\u5bf9\u5927\u89c4\u6a21\u56fe\u50cf\u6e90\u4eba\u7c7b\u5224\u65ad\u7684\u5f3a\u5927\u4e00\u81f4\u6027\u3002", "conclusion": "CAIRe\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u5ea6\u91cf\u6807\u51c6\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8861\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u4ea7\u751f\u7684\u6587\u5316\u76f8\u5173\u6027\uff0c\u5176\u7ed3\u679c\u4e0e\u4eba\u7c7b\u7684\u8bc4\u4ef7\u9ad8\u5ea6\u4e00\u81f4\u3002"}}
{"id": "2506.09351", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09351", "abs": "https://arxiv.org/abs/2506.09351", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters.", "AI": {"tldr": "\u63d0\u51faDIVE\u65b9\u6cd5\u5bf9MoE\u67b6\u6784\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u7684\u91cd\u7ec4\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u6821\u51c6\u6570\u636e\u96c6\u4e0a\u526a\u679d\u540e\uff0c\u6709\u6548\u5229\u7528\u4e13\u5bb6\u591a\u6837\u6027\u51cf\u5c11\u8bad\u7ec3\u5197\u4f59\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8bad\u7ec3\u6548\u7387\u9ad8\u4e14\u51c6\u786e\u7387\u635f\u5931\u5c0f\u3002", "motivation": "\u89c2\u5bdf\u5230\u5728\u4e0d\u540c\u7684\u6821\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u526a\u679d\u540e\uff0c\u4f1a\u51fa\u73b0\u663e\u8457\u7684\u591a\u6837\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709\u91cd\u7ec4\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u8fd9\u79cd\u591a\u6837\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DIVE\u65b9\u6cd5\uff0c\u5305\u62ec\u9886\u57df\u4eb2\u548c\u529b\u6316\u6398\u3001\u57fa\u4e8e\u526a\u679d\u7684\u4e13\u5bb6\u91cd\u7ec4\u4ee5\u53ca\u9ad8\u6548\u518d\u8bad\u7ec3\u3002\u7279\u522b\u5730\uff0c\u91cd\u7ec4\u5305\u62ec\u524d\u9988\u7f51\u7edc\u6a21\u5757\u7684\u526a\u679d\u548c\u91cd\u7ec4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDIVE\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\uff0c\u4e14\u51c6\u786e\u7387\u635f\u5931\u6700\u5c0f\uff0c\u5728\u6fc0\u6d3b\u53c2\u6570\u6570\u91cf\u76f8\u540c\u7684\u6761\u4ef6\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u526a\u679d\u548cMoE\u91cd\u7ec4\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u91cd\u7ec4\u65b9\u6cd5\u4e2d\u7684\u591a\u6837\u6027\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u8bad\u7ec3\u6548\u7387\uff0c\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\u3002"}}
{"id": "2506.09113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09113", "abs": "https://arxiv.org/abs/2506.09113", "authors": ["Yu Gao", "Haoyuan Guo", "Tuyen Hoang", "Weilin Huang", "Lu Jiang", "Fangyuan Kong", "Huixia Li", "Jiashi Li", "Liang Li", "Xiaojie Li", "Xunsong Li", "Yifu Li", "Shanchuan Lin", "Zhijie Lin", "Jiawei Liu", "Shu Liu", "Xiaonan Nie", "Zhiwu Qing", "Yuxi Ren", "Li Sun", "Zhi Tian", "Rui Wang", "Sen Wang", "Guoqiang Wei", "Guohong Wu", "Jie Wu", "Ruiqi Xia", "Fei Xiao", "Xuefeng Xiao", "Jiangqiao Yan", "Ceyuan Yang", "Jianchao Yang", "Runkai Yang", "Tao Yang", "Yihang Yang", "Zilyu Ye", "Xuejiao Zeng", "Yan Zeng", "Heng Zhang", "Yang Zhao", "Xiaozheng Zheng", "Peihao Zhu", "Jiaxin Zou", "Feilong Zuo"], "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "comment": "Seedance 1.0 Technical Report", "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.", "AI": {"tldr": "Seedance 1.0 \u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6e90\u6570\u636e\u6574\u7406\u3001\u67b6\u6784\u8bbe\u8ba1\u4ee5\u53ca\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u5feb\u901f\u548c\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u9075\u5faa\u6307\u4ee4\u3001\u8fd0\u52a8\u5408\u7406\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u6b64\u62a5\u544a\u65e8\u5728\u4ecb\u7ecdSeedance 1.0\u5728\u8fd9\u4e9b\u65b9\u9762\u7684\u6539\u8fdb\u3002", "method": "Seedance 1.0 \u901a\u8fc7\u591a\u6e90\u6570\u636e\u6574\u7406\u3001\u9ad8\u6548\u7684\u67b6\u6784\u8bbe\u8ba1\u3001\u4f18\u5316\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u53ca\u591a\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u5b9e\u73b0\u6a21\u578b\u52a0\u901f\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u901f\u5ea6\u3002", "result": "Seedance 1.0 \u80fd\u591f\u5728\u590d\u6742\u591a\u4e3b\u4f53\u7684\u60c5\u5883\u4e2d\u51c6\u786e\u9075\u5faa\u6307\u4ee4\uff0c\u751f\u6210\u5177\u6709\u65f6\u7a7a\u8fde\u8d2f\u6027\u548c\u4e3b\u4f53\u4e00\u81f4\u6027\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5feb10\u500d\u3002", "conclusion": "Seedance 1.0 \u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e0e\u5feb\u901f\u89c6\u9891\u751f\u6210\u7684\u7ed3\u5408\uff0c\u5177\u6709\u663e\u8457\u7684\u65f6\u7a7a\u6d41\u7545\u6027\u548c\u7ed3\u6784\u7a33\u5b9a\u6027\uff0c\u5728\u590d\u6742\u591a\u4e3b\u4f53\u60c5\u5883\u4e2d\u4fdd\u6301\u6307\u4ee4\u7cbe\u786e\u9075\u5faa\u548c\u591a\u573a\u666f\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002"}}
{"id": "2506.09359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09359", "abs": "https://arxiv.org/abs/2506.09359", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u89e3\u51b3Text-to-SQL\u7cfb\u7edf\u4e2d\u751f\u6210SQL\u8bed\u53e5\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\u8bc4\u4f30\u95ee\u9898\u7684\u65b9\u6cd5\u53ca\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5927\u5e45\u5ea6\u63d0\u5347\u4e86Text-to-SQL\uff08NL2SQL\uff09\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4f46\u5bf9\u751f\u6210SQL\u8bed\u53e5\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\u8bc4\u4f30\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u6a21\u7cca\u7684\u7528\u6237\u67e5\u8be2\u548c\u591a\u79cd\u6709\u6548\u7684SQL\u89e3\u91ca\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u8bc4\u4f30\u751f\u6210SQL\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\u548c\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u201c\u5f31\u201d\u8bed\u4e49\u7b49\u4ef7\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u672c\u7814\u7a76\u5206\u6790\u4e86SQL\u7b49\u4ef7\u6027\u548c\u975e\u7b49\u4ef7\u6027\u7684\u5e38\u89c1\u6a21\u5f0f\uff0c\u5e76\u8ba8\u8bba\u4e86\u57fa\u4e8eLLM\u8bc4\u4f30\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u5bf9SQL\u7b49\u4ef7\u6027\u548c\u975e\u7b49\u4ef7\u6027\u6a21\u5f0f\u7684\u5206\u6790\u548cLLM\u8bc4\u4f30\u65b9\u6cd5\u8ba8\u8bba\uff0c\u672c\u7814\u7a76\u4e3a\u6539\u8fdbSQL\u8bed\u4e49\u7b49\u4ef7\u6027\u8bc4\u4ef7\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.09229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09229", "abs": "https://arxiv.org/abs/2506.09229", "authors": ["Sungwon Hwang", "Hyojin Jang", "Kinam Kim", "Minho Park", "Jaegul choo"], "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models", "comment": "24 pages, 25 figures", "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u5e27\u8868\u5f81\u5bf9\u9f50\u6280\u672fCREPA\uff0c\u65e8\u5728\u6539\u8fdb\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5e27\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u5728\u89c6\u89c9\u8d28\u91cf\u53ca\u8de8\u5e27\u4e00\u81f4\u6027\u4e0a\u7684\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u6307\u51fa\uff0c\u5c3d\u7ba1\u5728\u7528\u6237\u7ea7\u522b\u7684\u89c6\u9891\u6a21\u578b\u5fae\u8c03\u4ee5\u751f\u6210\u53cd\u6620\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u5c5e\u6027\u7684\u89c6\u9891\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4f46\u8fd9\u4e00\u65b9\u5411\u4ecd\u9c9c\u6709\u63a2\u7d22\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u610f\u4e49\u3002", "method": "\u7814\u7a76\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u9002\u5e94\u4e8e\u89c6\u9891\u6a21\u578b\u7684REPA\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u5176\u9650\u5236\u5728\u4e8e\u65e0\u6cd5\u6709\u6548\u4fdd\u6301\u5e27\u95f4\u8bed\u4e49\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u6b64\uff0c\u5f15\u5165CREPA\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u5c06\u5f53\u524d\u5e27\u7684\u9690\u85cf\u72b6\u6001\u4e0e\u76f8\u90bb\u5e27\u7684\u5916\u90e8\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\uff0c\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7528\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u7684\u8de8\u5e27\u8868\u5f81\u5bf9\u9f50\uff08CREPA\uff09\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u7559\u5e27\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0cCREPA \u80fd\u591f\u5728\u5927\u91cf\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\uff09\u4e2d\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u5e27\u95f4\u8bed\u4e49\u8fde\u8d2f\u6027\u3002\u7814\u7a76\u5728\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u5c5e\u6027\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CREPA\u7684\u9002\u7528\u6027\u3002", "conclusion": "CREPA\u4f5c\u4e3a\u4e00\u9879\u65b0\u6280\u672f\uff0c\u5df2\u5728\u5927\u89c4\u6a21VDMs\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8de8\u5e27\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u63d0\u5347\u6548\u679c\uff0c\u5e76\u5728\u4e0d\u540c\u5c5e\u6027\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.09367", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09367", "abs": "https://arxiv.org/abs/2506.09367", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u547d\u540d\u4e3aCOGENT\u7684\u8bfe\u7a0b\u5bfc\u5411\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9002\u5408\u5e74\u7ea7\u9605\u8bfb\u6c34\u5e73\u7684\u6559\u80b2\u5185\u5bb9\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u751f\u6210\u5f0fAI\u5e94\u7528\u4e8e\u6559\u80b2\u65f6\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u5185\u5bb9\u751f\u6210\u65b9\u9762\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6f5c\u529b\u548c\u591a\u6837\u6027\uff0c\u4f46\u5728\u6559\u80b2\u9886\u57df\u4e2d\u5e94\u7528\u65f6\u5b58\u5728\u6311\u6218\uff0c\u5982\u6a21\u578b\u96be\u4ee5\u4e0e\u8bfe\u7a0b\u6807\u51c6\u4e00\u81f4\u5e76\u7ef4\u6301\u5408\u9002\u7684\u5e74\u7ea7\u9605\u8bfb\u6c34\u5e73\u3002\u7279\u522b\u662fSTEM\u6559\u80b2\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u79d1\u5b66\u6027\u7684\u540c\u65f6\u7528\u901a\u4fd7\u6613\u61c2\u7684\u8bed\u8a00\u89e3\u91ca\u590d\u6742\u62bd\u8c61\u7684\u6982\u5ff5\u7ed9\u5b66\u751f\u5e26\u6765\u4e86\u989d\u5916\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOGENT\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u65e8\u5728\u751f\u6210\u7b26\u5408\u5e74\u7ea7\u6c34\u5e73\u7684\u6559\u80b2\u5185\u5bb9\u3002\u6846\u67b6\u4e2d\u5305\u542b\u4e86\u4e09\u4e2a\u8bfe\u7a0b\u7ec4\u6210\u90e8\u5206\uff08\u79d1\u5b66\u6982\u5ff5\u3001\u6838\u5fc3\u601d\u60f3\u548c\u5b66\u4e60\u76ee\u6807\uff09\uff0c\u901a\u8fc7\u63a7\u5236\u957f\u5ea6\u3001\u8bcd\u6c47\u548c\u53e5\u5b50\u590d\u6742\u6027\u6765\u8c03\u63a7\u53ef\u8bfb\u6027\uff0c\u5e76\u91c7\u7528\"\u57fa\u4e8e\u597d\u5947\"\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5b66\u751f\u53c2\u4e0e\u5ea6\u548c\u5174\u8da3\u3002", "result": "\u901a\u8fc7\u5229\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0cCOGENT\u80fd\u591f\u6301\u7eed\u751f\u6210\u4e0e\u53c2\u8003\u6587\u732e\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u5e74\u7ea7\u9002\u5f53\u6bb5\u843d\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u751f\u6210\u9002\u5e94\u6027\u548c\u9ad8\u8d28\u91cf\u5b66\u4e60\u8d44\u6e90\u7684\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2506.09237", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09237", "abs": "https://arxiv.org/abs/2506.09237", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5bf9\u6297\u9c81\u68d2\u7684PatchGuard\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u91c7\u7528ViT\u67b6\u6784\u52a0\u5165\u524d\u666f\u611f\u77e5\u4f2a\u5f02\u5e38\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u8fb9\u7f18\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u539f\u56e0\u5728\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u53ea\u5305\u542b\u4e86\u6b63\u5e38\u7684\u65e0\u6807\u7b7e\u6837\u672c\u3002PatchGuard\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4f2a\u5f02\u5e38\u6837\u672c\u548c\u5c40\u90e8\u63a9\u6a21\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "PatchGuard, \u4e00\u79cd\u57fa\u4e8e\u524d\u666f\u611f\u77e5\u4f2a\u5f02\u5e38\u6837\u672c\u6765\u589e\u5f3aViT\u67b6\u6784\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u5de5\u4e1a\u548c\u533b\u7597\u6570\u636e\u96c6\u4e0a\uff0cPatchGuard\u76f8\u8f83\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u5728\u5bf9\u6297\u8bbe\u7f6e\u4e0b\u5206\u522b\u63d0\u9ad8\u4e8653.2%\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u548c68.5%\u7684\u5f02\u5e38\u5b9a\u4f4d\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u975e\u5bf9\u6297\u8bbe\u7f6e\u4e0b\u4e5f\u4fdd\u6301\u4e86\u826f\u597d\u7684\u51c6\u786e\u5ea6\u3002", "conclusion": "PatchGuard\u5c55\u793a\u4e86\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.09375", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09375", "abs": "https://arxiv.org/abs/2506.09375", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "AI": {"tldr": "CoLMbo, a Speaker Language Model, integrates a speaker encoder with prompt-based conditioning to generate detailed and structured speaker descriptions, including dialect, gender, and age, marking an advancement in speaker recognition systems.", "motivation": "The motivation behind this paper is to overcome the limitations of current speaker recognition systems that struggle to provide detailed speaker characteristics or context-rich descriptions.", "method": "The method involves integrating a speaker encoder with prompt-based conditioning to create detailed captions based on speaker embeddings.", "result": "CoLMbo provides customized descriptions including regional dialect variations and age-related traits, performing well in zero-shot scenarios across diverse datasets.", "conclusion": "This innovative approach enhances traditional speaker profiling and marks a significant advancement in the field of speaker recognition systems."}}
{"id": "2506.09278", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09278", "abs": "https://arxiv.org/abs/2506.09278", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "comment": "Project Page: https://uniflowmatch.github.io/", "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "AI": {"tldr": "UFM, a unified model for image correspondence and optical flow, outperforms specialized models in both accuracy and speed.", "motivation": "To improve performance and generalization for dense image correspondence tasks by unifying wide-baseline image matching and optical flow estimation through a single model.", "method": "Unified Flow & Matching model (UFM) uses a simple, generic transformer architecture for dense image correspondence across wide-baseline scenarios and optical flow estimation.", "result": "UFM is 28% more accurate than state-of-the-art flow methods and 62% less error and 6.7x faster than dense wide-baseline matchers.", "conclusion": "The development of UFM showcases the potential of unified training in achieving better and faster results for both optical flow and dense correspondence tasks, opening new avenues for real-time and multi-modal applications."}}
{"id": "2506.09381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09381", "abs": "https://arxiv.org/abs/2506.09381", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u65b0\u95fb\u6807\u9898\u548c\u94fe\u63a5\u7684\u8d28\u91cf\uff0c\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90fd\u53ef\u4ee5\u6709\u6548\u533a\u5206\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u7684\u8d28\u91cf\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u80fd\u5426\u81ea\u52a8\u533a\u5206\u4f4e\u8d28\u91cf\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u4e0e\u9ad8\u8d28\u91cf\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\uff0c\u4ee5\u5e94\u5bf9\u7ebf\u4e0a\u65b0\u95fb\u7684\u5e7f\u6cdb\u53d1\u5e03\u5e26\u6765\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e8612\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u5206\u679057,544,214\u6761\u5168\u7403\u65b0\u95fb\u7f51\u7ad9\u7684\u94fe\u63a5\u548c\u6807\u9898\u3002\u6570\u636e\u96c6\u4e2d\uff0c\u6bcf\u4e2a\u6587\u672c\u7684\u4e8c\u5143\u6807\u7b7e\u57fa\u4e8e\u4e13\u5bb6\u5bf9\u65b0\u95fb\u9886\u57df\u8d28\u91cf\u7684\u4e00\u81f4\u8bc4\u5206\u5f97\u51fa\u3002\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u4e86115\u4e2a\u63d0\u53d6\u51fa\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u88c5\u888b\u5206\u7c7b\u5668\uff0c\u8868\u73b0\u826f\u597d\uff08\u51c6\u786e\u738788.1%\uff0cF1\u5206\u657088.3%\uff09\u3002\u5fae\u8c03\u8fc7\u7684DistilBERT\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u738790.3%\uff0c\u4f46\u9700\u8981\u66f4\u957f\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7279\u5f81\u7684\u4f20\u7edf\u5206\u7c7b\u5668\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90fd\u80fd\u6709\u6548\u5730\u533a\u5206\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u7684\u8d28\u91cf\uff0c\u4f46\u9700\u8981\u5728\u9884\u6d4b\u6027\u80fd\u548c\u8bad\u7ec3\u65f6\u95f4\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002"}}
{"id": "2506.09299", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09299", "abs": "https://arxiv.org/abs/2506.09299", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "comment": "6 Pages, 3 figures", "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "AI": {"tldr": "This paper introduces an optimized YOLOv4-Tiny model for object detection in aerial emergency response imagery. The model is quantized to INT8, reducing its size and enhancing its inference speed without sacrificing accuracy, making it ideal for real-time use on low-power devices.", "motivation": "The motivation behind this work is the need for a lightweight and energy-efficient object detection model specifically tailored for aerial imagery in emergency response, as existing models are not optimized for these conditions and publicly available datasets are insufficient.", "method": "The paper employs the YOLOv4-Tiny model optimized through post-training quantization to INT8 precision for object detection in aerial imagery during emergencies. A custom-curated dataset with 10,820 annotated images specific to emergency scenarios is used for training, addressing the scarcity of publicly available drone-view emergency data.", "result": "The quantized YOLOv4-Tiny model achieves comparable detection performance to the YOLOv5-small model but with a significantly reduced model size (from 22.5 MB to 6.4 MB) and 44% faster inference speed.", "conclusion": "The quantized YOLOv4-Tiny model is highly suitable for real-time object detection on low-power edge devices in emergency response situations, providing energy efficiency and speed."}}
{"id": "2506.09391", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09391", "abs": "https://arxiv.org/abs/2506.09391", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "title": "Comparing human and LLM politeness strategies in free production", "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u793c\u8c8c\u8bed\u8a00\u65f6\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8d85\u8fc7700\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e2d\uff0c\u5b83\u4eec\u80fd\u591f\u590d\u5236\u5173\u952e\u7684\u793c\u8c8c\u7b56\u7565\u504f\u597d\uff0c\u5e76\u5728\u5f00\u653e\u6027\u4efb\u52a1\u4e2d\u83b7\u5f97\u4eba\u7c7b\u8bc4\u4ef7\u8005\u7684\u504f\u597d\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u503e\u5411\u4e8e\u5728\u6240\u6709\u60c5\u5883\u4e0b\u8fc7\u5ea6\u4f7f\u7528\u6d88\u6781\u793c\u8c8c\u7b56\u7565\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u4fe1\u606f\u4f20\u9012\u548c\u793e\u4f1a\u4e92\u52a8\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u5e76\u901a\u8fc7\u4e0e\u4eba\u7c7b\u7684\u5bf9\u6bd4\u8bc4\u4f30\u5176\u5728\u793c\u8c8c\u7528\u8bed\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u4eba\u7c7b\u548c\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u53d7\u63a7\u548c\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u751f\u6210\u7684\u793c\u8c8c\u8a00\u8bed\u7b56\u7565\uff0c\u7814\u7a76\u4e86LLMs\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u4f7f\u7528\u793c\u8c8c\u7b56\u7565\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8d85\u8fc7700\u4ebf\u53c2\u6570\u7684LLMs\u80fd\u591f\u6709\u6548\u590d\u5236\u8ba1\u7b97\u8bed\u7528\u5b66\u6587\u732e\u4e2d\u7684\u5173\u952e\u793c\u8c8c\u7b56\u7565\u504f\u597d\uff1b\u5728\u5f00\u653e\u6027\u573a\u666f\u4e0b\uff0c\u4eba\u7c7b\u8bc4\u4ef7\u8005\u5bf9\u6a21\u578b\u751f\u6210\u7684\u54cd\u5e94\u8868\u73b0\u51fa\u504f\u597d\uff1b\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u6240\u6709\u60c5\u5883\u4e0b\u90fd\u503e\u5411\u4e8e\u4f7f\u7528\u6d88\u6781\u793c\u8c8c\u7b56\u7565\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u3002", "conclusion": "\u5c3d\u7ba1\u73b0\u4ee3LLMs\u5728\u793c\u8c8c\u7b56\u7565\u7684\u5e94\u7528\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u4f7f\u7528\u6b63\u9762\u4e0e\u8d1f\u9762\u7b56\u7565\u7684\u4e0d\u5e73\u8861\u6027\uff0c\u5f15\u8d77\u4e86\u5173\u4e8eAI\u7cfb\u7edf\u8bed\u7528\u5b66\u5bf9\u9f50\u7684\u91cd\u8981\u7591\u95ee\u3002"}}
{"id": "2506.09300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09300", "abs": "https://arxiv.org/abs/2506.09300", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5", "comment": null, "summary": "This paper presents the deployment and performance evaluation of a quantized\nYOLOv4-Tiny model for real-time object detection in aerial emergency imagery on\na resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model\nwas quantized to INT8 precision using TensorFlow Lite post-training\nquantization techniques and evaluated for detection speed, power consumption,\nand thermal feasibility under embedded deployment conditions. The quantized\nmodel achieved an inference time of 28.2 ms per image with an average power\nconsumption of 13.85 W, demonstrating a significant reduction in power usage\ncompared to its FP32 counterpart. Detection accuracy remained robust across key\nemergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These\nresults highlight the potential of low-power embedded AI systems for real-time\ndeployment in safety-critical emergency response applications.", "AI": {"tldr": "\u91cf\u5316\u540e\u7684YOLOv4-Tiny\u6a21\u578b\u5728Raspberry Pi 5\u4e0a\u5b9e\u73b0\u4e86\u5feb\u901f\u9ad8\u6548\u7684\u7a7a\u88ad\u7d27\u6025\u56fe\u50cf\u5bf9\u8c61\u68c0\u6d4b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u548c\u90e8\u7f72\u5b9e\u65f6\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u5728\u7a7a\u88ad\u7d27\u6025\u56fe\u50cf\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528TensorFlow Lite\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6280\u672f\u5c06YOLOv4-Tiny\u6a21\u578b\u91cf\u5316\u5230INT8\u7cbe\u5ea6\uff0c\u5e76\u5728Raspberry Pi 5\u4e0a\u8bc4\u4f30\u4e86\u68c0\u6d4b\u901f\u5ea6\u3001\u529f\u8017\u548c\u70ed\u6027\u80fd\u3002", "result": "\u91cf\u5316\u6a21\u578b\u5728\u6bcf\u5f20\u56fe\u7247\u4e0a\u7684\u63a8\u7406\u65f6\u95f4\u4e3a28.2\u6beb\u79d2\uff0c\u5e76\u4e14\u5e73\u5747\u529f\u8017\u4e3a13.85\u74e6\uff0c\u76f8\u6bd4\u4e8e32\u4f4d\u6d6e\u70b9\u683c\u5f0f\u6a21\u578b\u663e\u793a\u51fa\u4e86\u663e\u8457\u7684\u529f\u8017\u964d\u4f4e\u3002\u68c0\u6d4b\u7cbe\u5ea6\u5728\u5173\u952e\u7684\u51e0\u7c7b\u7d27\u6025\u60c5\u51b5\u5982\u6551\u62a4\u8f66\u3001\u8b66\u8f66\u3001\u6d88\u9632\u8f66\u548c\u8f66\u7978\u4e2d\u7684\u8868\u73b0\u4ecd\u65e7\u7a33\u5065\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u4f4e\u529f\u8017\u5d4c\u5165\u5f0fAI\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u7684\u5e94\u6025\u54cd\u5e94\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u90e8\u7f72\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.09393", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09393", "abs": "https://arxiv.org/abs/2506.09393", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86KT$^2$\uff0c\u901a\u8fc7\u9690\u85cf\u9a6c\u5c14\u53ef\u592b\u6811\u6a21\u578b\u5efa\u6a21\u5b66\u751f\u5bf9\u77e5\u8bc6\u6982\u5ff5\u7684\u638c\u63e1\uff0c\u5e76\u5229\u7528EM\u7b97\u6cd5\u4f30\u8ba1\u5b66\u751f\u77e5\u8bc6\u638c\u63e1\u7a0b\u5ea6\u3002\u5b9e\u9a8c\u8868\u660eKT$^2$\u5728\u4f4e\u8d44\u6e90\u7684\u771f\u5b9e\u5728\u7ebf\u73af\u5883\u4e2d\uff0c\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8bb8\u591a\u77e5\u8bc6\u8ffd\u8e2a\u7684\u5b9e\u9645\u8bfe\u5802\u73af\u5883\u901a\u5e38\u6570\u636e\u8d44\u6e90\u6709\u9650\uff0c\u5e76\u4e14\u9700\u8981\u968f\u7740\u5b66\u751f\u7ec3\u4e60\u5386\u53f2\u7684\u589e\u957f\u8fdb\u884c\u5728\u7ebf\u66f4\u65b0\uff0c\u8fd9\u5bf9\u73b0\u6709\u7684\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u6062\u590d\u5f3a\u5927\u6027\u80fd\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u5178\u578b\u7684\u8bfe\u5802\u73af\u5883\u4e2d\u53ef\u7528\u7684\u5c42\u6b21\u77e5\u8bc6\u6982\u5ff5\u4fe1\u606f\uff0c\u5728\u6570\u636e\u7a00\u758f\u65f6\u63d0\u4f9b\u6709\u529b\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u77e5\u8bc6\u6811\u7684\u77e5\u8bc6\u8ffd\u8e2a(KT$^2$)\uff0c\u8fd9\u662f\u4e00\u4e2a\u6982\u7387\u6027\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u9690\u85cf\u9a6c\u5c14\u53ef\u592b\u6811\u6a21\u578b\u6765\u5efa\u6a21\u5b66\u751f\u5bf9\u77e5\u8bc6\u6982\u5ff5\u6811\u72b6\u5c42\u7ea7\u7ed3\u6784\u7684\u7406\u89e3\u3002KT$^2$\u5229\u7528EM\u7b97\u6cd5\u4f30\u8ba1\u5b66\u751f\u638c\u63e1\u77e5\u8bc6\u7684\u60c5\u51b5\uff0c\u5e76\u901a\u8fc7\u589e\u91cf\u66f4\u65b0\u673a\u5236\u5728\u63a5\u6536\u5230\u65b0\u7b54\u6848\u65f6\u652f\u6301\u4e2a\u6027\u5316\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKT$^2$\u5728\u4f4e\u8d44\u6e90\u7684\u771f\u5b9e\u5728\u7ebf\u73af\u5883\u4e0b\uff0c\u76f8\u6bd4\u4e8e\u5f3a\u5927\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u4fdd\u6301\u4e00\u81f4\u6027\u5730\u66f4\u4f18\u8868\u73b0\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660eKT$^2$\u5728\u4f4e\u8d44\u6e90\u7684\u771f\u5b9e\u5728\u7ebf\u73af\u5883\u4e2d\uff0c\u59cb\u7ec8\u4fdd\u6301\u4f18\u4e8e\u5f3a\u5927\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09327", "abs": "https://arxiv.org/abs/2506.09327", "authors": ["Tong Wang", "Guanzhou Chen", "Xiaodong Zhang", "Chenxi Liu", "Jiaqi Wang", "Xiaoliang Tan", "Wenchao Guo", "Qingyuan Yang", "Kaiqi Zhang"], "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning", "comment": null, "summary": "Remote sensing image interpretation plays a critical role in environmental\nmonitoring, urban planning, and disaster assessment. However, acquiring\nhigh-quality labeled data is often costly and time-consuming. To address this\nchallenge, we proposes a multi-modal self-supervised learning framework that\nleverages high-resolution RGB images, multi-spectral data, and digital surface\nmodels (DSM) for pre-training. By designing an information-aware adaptive\nmasking strategy, cross-modal masking mechanism, and multi-task self-supervised\nobjectives, the framework effectively captures both the correlations across\ndifferent modalities and the unique feature structures within each modality. We\nevaluated the proposed method on multiple downstream tasks, covering typical\nremote sensing applications such as scene classification, semantic\nsegmentation, change detection, object detection, and depth estimation.\nExperiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.\nThe results demonstrate that the proposed method outperforms existing\npretraining approaches in most tasks. Specifically, on the Potsdam and\nVaihingen semantic segmentation tasks, our method achieved mIoU scores of\n78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation\ntask, the RMSE error is reduced to 0.182, and for the binary change detection\ntask in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing\nthe second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and\nHR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u7684\u9884\u8bad\u7ec3\uff0c\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u5bf9\u4e8e\u73af\u5883\u76d1\u6d4b\u3001\u57ce\u5e02\u89c4\u5212\u548c\u707e\u5bb3\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u9ad8\u5206\u8fa8\u7387RGB\u56fe\u50cf\u3001\u591a\u5149\u8c31\u6570\u636e\u548c\u6570\u5b57\u8868\u9762\u6a21\u578b\uff08DSM\uff09\uff0c\u5f15\u5165\u4e86\u4fe1\u606f\u611f\u77e5\u81ea\u9002\u5e94\u63a9\u6a21\u7b56\u7565\u3001\u8de8\u6a21\u6001\u63a9\u6a21\u673a\u5236\u548c\u591a\u4efb\u52a1\u81ea\u76d1\u7763\u76ee\u6807\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5177\u4f53\u5728Potsdam\u548cVaihingen\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u4f7f\u752850%\u8bad\u7ec3\u96c6\u8fbe\u6210\u7684mIoU\u5206\u6570\u5206\u522b\u8fbe\u5230\u4e8678.30%\u548c76.50%\uff0c\u5728SECOND\u6570\u636e\u96c6\u4e8c\u5143\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2dmIoU\u5206\u6570\u8fbe\u523047.51%\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u9065\u611f\u5e94\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09408", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09408", "abs": "https://arxiv.org/abs/2506.09408", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.09343", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09343", "abs": "https://arxiv.org/abs/2506.09343", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "comment": "CVPR 2025 Highlight", "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u5bb6\u7535\u64cd\u4f5c\u57fa\u51c6CheckManual\u548c\u9996\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u64cd\u4f5c\u89c4\u5212\u6a21\u578bManualPlan\uff0c\u5f3a\u8c03\u673a\u5668\u4eba\u901a\u8fc7\u7406\u89e3\u624b\u518c\u64cd\u4f5c\u5bb6\u7535\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e0e\u624b\u518c\u76f8\u5173\u7684\u7814\u7a76\u5c40\u9650\u4e8e\u95ee\u7b54\u4efb\u52a1\uff0c\u800c\u73b0\u6709\u7684\u64cd\u4f5c\u7814\u7a76\u5ffd\u89c6\u4e86\u624b\u518c\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u65e0\u6cd5\u7406\u89e3\u591a\u9875\u624b\u518c\u3002\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u5bf9\u624b\u518c\u7684\u7406\u89e3\uff0c\u8fdb\u800c\u63d0\u5347\u5176\u64cd\u4f5c\u5bb6\u7535\u7684\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u518c\u7684\u5bb6\u7535\u64cd\u4f5c\u57fa\u51c6CheckManual\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5927\u578b\u6a21\u578b\u8f85\u52a9\u7684\u4eba\u7c7b\u4fee\u8ba2\u6570\u636e\u751f\u6210\u7ba1\u9053\u6765\u521b\u5efa\u57fa\u4e8eCAD\u5bb6\u7535\u6a21\u578b\u7684\u624b\u518c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u64cd\u4f5c\u89c4\u5212\u6a21\u578bManualPlan\uff0c\u8bbe\u7acb\u4e86\u4e00\u7ec4\u57fa\u51c6\u4ee5\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u4e8e\u624b\u518c\u7684\u64cd\u4f5c\u6311\u6218\u3001\u5ea6\u91cf\u6807\u51c6\u548c\u6a21\u62df\u73af\u5883\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u63d0\u51faManualPlan\u6a21\u578b\uff0c\u8bbe\u5b9a\u4e86CheckManual\u57fa\u51c6\u7684\u4e00\u7ec4\u521d\u6b65\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u673a\u5668\u4eba\u7406\u89e3\u548c\u5b66\u4e60\u5bb6\u7535\u624b\u518c\u6765\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u672a\u6765\u7684\u7814\u7a76\u5c06\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u7406\u89e3\u548c\u64cd\u4f5c\u5bb6\u7535\u7684\u80fd\u529b\u3002"}}
{"id": "2506.09414", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.09414", "abs": "https://arxiv.org/abs/2506.09414", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "AI": {"tldr": "\u63d0\u51faPGDA-KGQA\u4ee5\u89e3\u51b3\u591a\u8df3\u63a8\u7406\u6837\u672c\u7a00\u7f3a\u548c\u8bed\u4e49\u626d\u66f2\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u5355\u8df3\u95ee\u9898\u4e14\u5bb9\u6613\u8bed\u4e49\u626d\u66f2\uff0c\u540c\u65f6\u89e3\u51b3\u591a\u8df3\u63a8\u7406\u6837\u672c\u7a00\u7f3a\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u63d0\u793a\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u7ed3\u5408LLMs\u751f\u6210\u5927\u89c4\u6a21\u7684(\u95ee\u9898, \u903b\u8f91\u5f62\u5f0f)\u5bf9\uff0c\u91c7\u53d6\u4e09\u79cd\u7b56\u7565\u4e30\u5bcc\u8bad\u7ec3\u96c6\u3002", "result": "\u5728\u6807\u51c6KGQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u5728WebQSP\u548cComplexWebQuestions\u4e0a\u5206\u522b\u63d0\u9ad8\u4e862.8%-3.1%\u548c1.8%-2.4%\u7684F1, Hits@1, \u548cAccuracy\u3002", "conclusion": "PGDA-KGQA\u6846\u67b6\u901a\u8fc7\u6574\u5408LLM\uff0c\u591a\u6837\u5316\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347KGQA\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.09345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09345", "abs": "https://arxiv.org/abs/2506.09345", "authors": ["Songping Wang", "Xiantao Hu", "Yueming Lyu", "Caifeng Shan"], "title": "An Effective End-to-End Solution for Multimodal Action Recognition", "comment": null, "summary": "Recently, multimodal tasks have strongly advanced the field of action\nrecognition with their rich multimodal information. However, due to the\nscarcity of tri-modal data, research on tri-modal action recognition tasks\nfaces many challenges. To this end, we have proposed a comprehensive multimodal\naction recognition solution that effectively utilizes multimodal information.\nFirst, the existing data are transformed and expanded by optimizing data\nenhancement techniques to enlarge the training scale. At the same time, more\nRGB datasets are used to pre-train the backbone network, which is better\nadapted to the new task by means of transfer learning. Secondly, multimodal\nspatial features are extracted with the help of 2D CNNs and combined with the\nTemporal Shift Module (TSM) to achieve multimodal spatial-temporal feature\nextraction comparable to 3D CNNs and improve the computational efficiency. In\naddition, common prediction enhancement methods, such as Stochastic Weight\nAveraging (SWA), Ensemble and Test-Time augmentation (TTA), are used to\nintegrate the knowledge of models from different training periods of the same\narchitecture and different architectures, so as to predict the actions from\ndifferent perspectives and fully exploit the target information. Ultimately, we\nachieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the\ncompetition leaderboard, demonstrating the superiority of our solution.", "AI": {"tldr": "A multimodal action recognition solution is proposed, utilizing data enhancement, pre-training with transfer learning, and various prediction enhancement methods, achieving high accuracy on the competition leaderboard.", "motivation": "To address the challenges faced in tri-modal action recognition tasks due to the scarcity of tri-modal data, a comprehensive multimodal action recognition solution is proposed.", "method": "First, existing data are transformed and expanded via optimized data enhancement techniques. More RGB datasets are used for pre-training the backbone network through transfer learning. Secondly, multimodal spatial features are extracted using 2D CNNs and combined with TSM for multimodal spatial-temporal feature extraction and improved computational efficiency. Common prediction enhancement methods, such as SWA, Ensemble, and TTA, are employed to integrate the knowledge of models from different training periods of the same architecture and different architectures for comprehensive action prediction.", "result": "The approach achieved a Top-1 accuracy of 99% and a Top-5 accuracy of 100% on the competition leaderboard.", "conclusion": "The proposed solution showcases superiority in tri-modal action recognition tasks by effectively utilizing multimodal information and improving computational efficiency."}}
{"id": "2506.09424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09424", "abs": "https://arxiv.org/abs/2506.09424", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u4e0d\u540c\u9886\u57df\u7684\u81ea\u52a8\u6b3a\u9a97\u68c0\u6d4b\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5fae\u8c03\u540e\u7684LLMs\u5728\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9876\u7ea7\u6c34\u5e73\uff0c\u800cLMMs\u5728\u5229\u7528\u8de8\u6a21\u6001\u7ebf\u7d22\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u8f85\u52a9\u7279\u5f81\u548c\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u7684\u5f71\u54cd\u3002", "motivation": "\u9274\u4e8e\u6570\u5b57\u4e16\u754c\u4e2d\u6b3a\u9a97\u68c0\u6d4b\u7684\u91cd\u8981\u6027\u4e0e\u6311\u6218\u6027\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u578b\u5728\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u4fbf\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u8005\u901a\u8fc7\u5bf9\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\uff08\u73b0\u5b9e\u751f\u6d3b\u8bbf\u8c08\u3001\u4eba\u9645\u60c5\u666f\u4e2d\u7684\u6307\u793a\u6027\u6b3a\u9a97\u4ee5\u53ca\u6b3a\u8bc8\u6027\u8bc4\u8bba\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u91c7\u7528\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u6837\u672c\u9009\u62e9\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u5fae\u8c03\u540e\u7684LLMs\u5728\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0cLMMs\u5728\u8de8\u6a21\u6001\u7ebf\u7d22\u5229\u7528\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff1b\u8f85\u52a9\u7279\u5f81\u548c\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\u5bf9\u7ed3\u679c\u6709\u4e00\u5b9a\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u5904\u7406\u548c\u89e3\u91ca\u6b3a\u9a97\u7ebf\u7d22\u65b9\u9762\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2506.09350", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09350", "abs": "https://arxiv.org/abs/2506.09350", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "comment": null, "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u5bf9\u6297\u540e\u8bad\u7ec3\uff08AAPT\uff09\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u6362\u4e3a\u5b9e\u65f6\u3001\u4ea4\u4e92\u5f0f\u7684\u89c6\u9891\u751f\u6210\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u5bc6\u96c6\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u4f7f\u7528\u3002", "method": "Structure", "result": "{\\\"tldr\\\": \\\"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u5bf9\u6297\u540e\u8bad\u7ec3\uff08AAPT\uff09\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u6362\u4e3a\u5b9e\u65f6\u3001\u4ea4\u4e92\u5f0f\u7684\u89c6\u9891\u751f\u6210\u5668\u3002\\\", \\\"motivation\\\": \\\"\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u5bc6\u96c6\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u4f7f\u7528\u3002\\\", \\\"method\\\": \\\"\u8be5\u65b9\u6cd5\u4f7f\u7528\u5355\u4e2a\u795e\u7ecf\u51fd\u6570\u8bc4\u4f30\uff081NFE\uff09\u9010\u5e27\u751f\u6210\u6f5c\u5728\u5e27\uff0c\u5e76\u4f7f\u7528\u5bf9\u6297\u6027\u8bad\u7ec3\u4f5c\u4e3a\u81ea\u56de\u5f52\u751f\u6210\u7684\u6709\u6548\u8303\u5f0f\u3002\\\", \\\"result\\\": \\\"\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\uff0824fps\uff09\uff0c\u6d41\u5f0f\u89c6\u9891\u751f\u6210\uff0c\u5728\u5355\u4e2aH100\u4e0a\u8fbe\u5230736x416\u5206\u8fa8\u7387\uff0c\u6216\u57288xH100\u4e0a\u8fbe\u52301280x720\u5206\u8fa8\u7387\uff0c\u6700\u957f\u53ef\u8fbe\u4e00\u5206\u949f\u3002\\\", \\\"conclusion\\\": \\\"\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u957f\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u6297\u6027\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\uff0c\u5e76\u4e14\u4f7f\u5176\u5728\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u53ef\u884c\u3002\\\"}", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u957f\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u6297\u6027\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\uff0c\u5e76\u4e14\u4f7f\u5176\u5728\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u53ef\u884c\u3002"}}
{"id": "2506.09428", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09428", "abs": "https://arxiv.org/abs/2506.09428", "authors": ["Fei Ding", "Baiqiao Wang"], "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9SFT\u65b9\u6cd5\u5bfc\u81f4\u7684\u8bed\u8a00\u6a21\u578b\u666e\u904d\u80fd\u529b\u964d\u4f4e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u52a0\u7ecf\u6d4e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u6307\u4ee4\u5206\u5e03\u548c\u591a\u6a21\u578b\u7b5b\u9009\u6765\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u867d\u7136SFT\u589e\u5f3a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u9075\u4ece\u80fd\u529b\u548c\u7279\u5b9a\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u4f46\u4e5f\u5f80\u5f80\u964d\u4f4e\u4e86\u5b83\u4eec\u7684\u666e\u904d\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u539f\u59cb\u9884\u8bad\u7ec3\u6570\u636e\u7684\u4e0d\u53ef\u8bbf\u95ee\u6027\uff0c\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\u5728\u7b2c\u4e09\u65b9\u5b9e\u8df5\u8005\u5bf9\u5f00\u6e90\u6a21\u578b\u5b9e\u65bdSFT\u65f6\u4f1a\u6076\u5316\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76d1\u7763\u5fae\u8c03(SFT)\u65b9\u6cd5\uff0c\u65e8\u5728\u4ee5\u8f83\u4f4e\u6210\u672c\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\uff0c\u540c\u65f6\u4e0d\u4f9d\u8d56\u539f\u59cb\u5fae\u8c03\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u91cd\u6784\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u7684SFT\u6307\u4ee4\u5206\u5e03\uff0c\u7136\u540e\u901a\u8fc7\u591a\u6a21\u578b\u7b5b\u9009\u8fc7\u7a0b\u9009\u62e9\u6700\u4f18\u6570\u636e\uff0c\u5e76\u5c06\u5176\u4e0e\u65b0\u6570\u636e\u6df7\u5408\u8fdb\u884cSFT\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u901a\u7528\u9886\u57df\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u7279\u5b9a\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u901a\u7528\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u53ef\u4ee5\u6539\u8fdb\u7279\u5b9a\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u4ee5\u8f83\u4f4e\u7684\u6210\u672c\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\u3002"}}
{"id": "2506.09357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09357", "abs": "https://arxiv.org/abs/2506.09357", "authors": ["Junchao Zhou"], "title": "A new approach for image segmentation based on diffeomorphic registration and gradient fields", "comment": null, "summary": "Image segmentation is a fundamental task in computer vision aimed at\ndelineating object boundaries within images. Traditional approaches, such as\nedge detection and variational methods, have been widely explored, while recent\nadvances in deep learning have shown promising results but often require\nextensive training data. In this work, we propose a novel variational framework\nfor 2D image segmentation that integrates concepts from shape analysis and\ndiffeomorphic transformations. Our method models segmentation as the\ndeformation of a template curve via a diffeomorphic transformation of the image\ndomain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. The curve evolution is guided by a loss function that compares the\ndeformed curve to the image gradient field, formulated through the varifold\nrepresentation of geometric shapes. The approach is implemented in Python with\nGPU acceleration using the PyKeops library. This framework allows for accurate\nsegmentation with a flexible and theoretically grounded methodology that does\nnot rely on large datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u540c\u80da\u53d8\u6362\u76842D\u56fe\u50cf\u5206\u5272\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528LDDMM\u6846\u67b6\u4e0b\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6a21\u677f\u66f2\u7ebf\u53d8\u5f62\u4e3a\u56fe\u50cf\u57df\u4e2d\u7684\u66f2\u7ebf\u6765\u8fdb\u884c\u56fe\u50cf\u5206\u5272\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7406\u8bba\u57fa\u7840\u548c\u7075\u6d3b\u6027\u7684\u7ed3\u5408\u3002", "motivation": "\u4f20\u7edf\u7684\u8fb9\u7f18\u68c0\u6d4b\u548c\u53d8\u5206\u65b9\u6cd5\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u800c\u6700\u8fd1\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u663e\u793a\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u4e0d\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\u4e14\u7406\u8bba\u57fa\u7840\u575a\u5b9e\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u76842D\u56fe\u50cf\u5206\u5272\u53d8\u5206\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5f62\u72b6\u5206\u6790\u548c\u5fae\u5206\u540c\u80da\u53d8\u6362\u7684\u6982\u5ff5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7LDDMM\u6846\u67b6\u4e0b\u7684\u5fae\u5206\u540c\u80da\u53d8\u6362\u5c06\u6a21\u677f\u66f2\u7ebf\u53d8\u5f62\u4e3a\u56fe\u50cf\u57df\u4e2d\u7684\u66f2\u7ebf\uff0c\u4f7f\u7528PyKeops\u5e93\u5b9e\u73b0Python\u4e2d\u57fa\u4e8eGPU\u7684\u52a0\u901f\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u4f9d\u8d56\u4e8e\u7406\u8bba\u57fa\u7840\u7075\u6d3b\u51c6\u786e\u5730\u5206\u5272\u56fe\u50cf\uff0c\u800c\u65e0\u9700\u5927\u91cf\u6570\u636e\u96c6\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7406\u8bba\u57fa\u7840\u548c\u7075\u6d3b\u6027\u7684\u56fe\u50cf\u5206\u5272\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a2D\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u57fa\u7840\u6df1\u539a\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u8bba\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2506.09440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09440", "abs": "https://arxiv.org/abs/2506.09440", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u4fc4\u8bed\u5927\u6a21\u578bGigaChat\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86\u5176\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u8fc7\u7a0b\u53ca\u5b9e\u9a8c\uff0c\u5e76\u5f00\u653e\u4e86\u90e8\u5206\u6a21\u578b\u4f9b\u7814\u7a76\u548c\u5de5\u4e1a\u4f7f\u7528\u3002", "motivation": "\u7531\u4e8e\u5f00\u53d1\u9488\u5bf9\u4fc4\u8bed\u7684\u57fa\u7840\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u56e0\u6b64\u4e13\u95e8\u4f18\u5316\u7684\u6a21\u578b\u4e0d\u591a\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u7f3a\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86GigaChat\u7cfb\u5217\u4fc4\u8bedLLM\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u6a21\u578b\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u8fc7\u7a0b\u548c\u5b9e\u9a8c\u7684\u8be6\u7ec6\u62a5\u544a\uff0c\u4ee5\u53ca\u5982\u4f55\u6307\u5bfc\u8bbe\u8ba1\u9009\u62e9\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8bc4\u4f30\u4e86GigaChat\u6a21\u578b\u5728\u4fc4\u8bed\u548c\u82f1\u8bed\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5c06\u5176\u4e0e\u591a\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7API\u3001Telegram\u673a\u5668\u4eba\u548cWeb\u754c\u9762\u63d0\u4f9b\u9876\u7ea7\u6a21\u578b\u7684\u7cfb\u7edf\u6f14\u793a\uff0c\u5e76\u5f00\u653e\u4e86\u4e09\u4e2a\u516c\u5f00\u6a21\u578b\uff0c\u4ee5\u4fc3\u8fdb\u4fc4\u8bedNLP\u7814\u7a76\u548c\u5de5\u4e1a\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09363", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.09363", "abs": "https://arxiv.org/abs/2506.09363", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "comment": "Under review", "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "AI": {"tldr": "Introduces SAGE, a method combining semantic-augment erasing and global-local retention mechanism for safer diffusion models", "motivation": "address safety risks such as unsafe content generation and copyright infringement in diffusion models, and avoid the limitation of existing concept erasing methods", "method": "semantic-augment erasing and global-local collaborative retention mechanism", "result": "SAGE demonstrates comprehensive superiority in the safe generation of diffusion models compared with other methods", "conclusion": "SAGE effectively mitigates safety risks and retains performance in irrelevant concepts compared to previous methods"}}
{"id": "2506.09450", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09450", "abs": "https://arxiv.org/abs/2506.09450", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165\u4e86UniToMBench\u2014\u2014\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u548c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\u7684\u7efc\u5408\u6027\u57fa\u51c6\u5de5\u5177\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u6a21\u578b\u5728ToM\u4efb\u52a1\u4e0a\u7684\u6210\u5c31\u4e0e\u4e0d\u8db3\u3002", "motivation": "\u76ee\u524d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u9884\u6d4b\u4eba\u7c7b\u601d\u60f3\u72b6\u6001\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7UniToMBench \u63d0\u4f9b\u4e00\u4e2a\u7efc\u5408\u5de5\u5177\uff0c\u5e2e\u52a9\u6539\u5584\u8fd9\u4e00\u60c5\u51b5\u5e76\u8bc4\u4f30\u6a21\u578b\u7684ToM\uff08Theory of Mind\uff09\u80fd\u529b\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86UniToMBench\uff0c\u8be5\u57fa\u51c6\u901a\u8fc7\u6574\u5408SimToM\u548cTOMBENCH\u7684\u4f18\u70b9\uff0c\u5f15\u5165\u4e86\u591a\u4ea4\u4e92\u4efb\u52a1\u8bbe\u8ba1\u548c\u53d8\u5316\u7684\u6545\u4e8b\u573a\u666f\uff0c\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u63d0\u5347\u548c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u65b9\u9762\u7684\u80fd\u529b\u3002UniToMBench \u4f7f\u7528\u8d85\u8fc71,000\u4e2a\u624b\u5de5\u7f16\u5199\u7684\u60c5\u666f\u7ec4\u6210\u7684\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89d2\u7406\u89e3\u548c\u591a\u6837\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u66f4\u597d\u5730\u523a\u6fc0\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u50cfGPT-4o\u548cGPT-4o Mini\u8fd9\u6837\u7684\u6a21\u578b\u5728\u60c5\u7eea\u548c\u4fe1\u5ff5\u76f8\u5173\u7684\u60c5\u5f62\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u9ad8\u51c6\u786e\u7387\uff0c\u901a\u5e38\u8d85\u8fc780%\u3002\u7136\u800c\uff0c\u5728\u57fa\u4e8e\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\uff0c\u5176\u8868\u73b0\u5219\u663e\u793a\u51fa\u8f83\u5927\u7684\u5dee\u5f02\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u5982\u6b64\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8fd8\u662f\u5c55\u73b0\u4e86\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406ToM\u4efb\u52a1\u7684\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86UniToMBench\u4f5c\u4e3a\u4e00\u79cd\u5168\u9762\u8bc4\u4f30\u548c\u53d1\u5c55\u5de5\u5177\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.09369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09369", "abs": "https://arxiv.org/abs/2506.09369", "authors": ["Zeran Ke", "Bin Tan", "Xianwei Zheng", "Yujun Shen", "Tianfu Wu", "Nan Xue"], "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined", "comment": "accepted to CVPR 2025; 17 pages, appendices included", "summary": "This paper studies the problem of Line Segment Detection (LSD) for the\ncharacterization of line geometry in images, with the aim of learning a\ndomain-agnostic robust LSD model that works well for any natural images. With\nthe focus of scalable self-supervised learning of LSD, we revisit and\nstreamline the fundamental designs of (deep and non-deep) LSD approaches to\nhave a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the\ncuration of line geometry at scale from over 10M unlabeled real-world images.\nOur ScaleLSD works very well to detect much more number of line segments from\nany natural images even than the pioneered non-deep LSD approach, having a more\ncomplete and accurate geometric characterization of images using line segments.\nExperimentally, our proposed ScaleLSD is comprehensively testified under\nzero-shot protocols in detection performance, single-view 3D geometry\nestimation, two-view line segment matching, and multiview 3D line mapping, all\nwith excellent performance obtained. Based on the thorough evaluation, our\nScaleLSD is observed to be the first deep approach that outperforms the\npioneered non-deep LSD in all aspects we have tested, significantly expanding\nand reinforcing the versatility of the line geometry of images. Code and Models\nare available at https://github.com/ant-research/scalelsd", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86ScaleLSD\uff0c\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u9ad8\u6548\u7684\u7ebf\u6bb5\u68c0\u6d4b\u6a21\u578b\uff0c\u4e13\u4e3a\u5927\u89c4\u6a21\u548c\u591a\u6837\u5316\u7684\u81ea\u7136\u56fe\u50cf\u8bbe\u8ba1\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5b66\u4e60\u4e00\u4e2a\u9002\u7528\u4e8e\u4efb\u4f55\u81ea\u7136\u56fe\u50cf\u7684\u9c81\u68d2\u9886\u57df\u65e0\u5173\u7684\u7ebf\u6bb5\u68c0\u6d4b\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u56fe\u50cf\u7684\u7ebf\u6bb5\u51e0\u4f55\u8868\u5f81\u95ee\u9898\u3002", "method": "\u6b64\u7814\u7a76\u63d0\u51fa\u4e86ScaleLSD\uff0c\u4e00\u79cd\u4e13\u6ce8\u4e8e\u5728\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u73b0\u5b9e\u56fe\u50cf\u4e2d\u5b66\u4e60\u7ebf\u6bb5\u68c0\u6d4b\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002ScaleLSD\u901a\u8fc7\u590d\u4e60\u548c\u7b80\u5316\u6df1\u5ea6\u548c\u975e\u6df1\u5ea6LSD\u65b9\u6cd5\u7684\u57fa\u672c\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u7684\u7ebf\u6bb5\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cScaleLSD\u5728\u96f6\u6837\u672c\u56fe\u50cf\u68c0\u6d4b\u6027\u80fd\uff0c\u5355\u89c6\u89d23D\u51e0\u4f55\u4f30\u8ba1\uff0c\u53cc\u89c6\u89d2\u7ebf\u6bb5\u5339\u914d\u548c\u591a\u89c6\u89d23D\u7ebf\u6620\u5c04\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9996\u6b21\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u975e\u6df1\u5ea6\u7ebf\u6bb5\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u5f97\u51fa\u4e86ScaleLSD\u662f\u7b2c\u4e00\u4e2a\u5728\u6240\u6d4b\u8bd5\u7684\u6240\u6709\u65b9\u9762\u90fd\u4f18\u4e8e\u539f\u6709\u7684\u975e\u6df1\u5ea6LSD\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u6269\u5c55\u548c\u589e\u5f3a\u4e86\u56fe\u50cf\u7ebf\u51e0\u4f55\u7684\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2506.09457", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09457", "abs": "https://arxiv.org/abs/2506.09457", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u79f0\u4e3aPrefix-Oriented Equal-length Training (POET)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u622a\u65ad\u4f18\u9009\u548c\u975e\u4f18\u9009\u54cd\u5e94\u4ee5\u5339\u914d\u8f83\u77ed\u54cd\u5e94\u7684\u957f\u5ea6\uff0c\u4ee5\u89e3\u51b3Direct Alignment Algorithms (DAAs)\u4e2d\u5b58\u5728\u7684\u5956\u52b1\u751f\u6210\u5dee\u8ddd\u95ee\u9898\uff0c\u4ece\u800c\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u89e3\u51b3Direct Alignment Algorithms (DAAs)\u4e2d\u7684\u201c\u5956\u52b1\u751f\u6210\u5dee\u8ddd\u201d\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u6e90\u4e8e\u8bad\u7ec3\u671f\u95f4\u4f18\u5316\u76ee\u6807\u4e0e\u63a8\u7406\u671f\u95f4\u5b9e\u9645\u751f\u6210\u8868\u73b0\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "method": "\u5f15\u5165POET\u65b9\u6cd5\uff0c\u5373\u901a\u8fc7\u622a\u65ad\u4f18\u9009\u548c\u975e\u4f18\u9009\u54cd\u5e94\u4ee5\u5339\u914d\u8f83\u77ed\u54cd\u5e94\u7684\u957f\u5ea6\uff0c\u4ece\u800c\u89e3\u51b3\u5956\u52b1\u751f\u6210\u5dee\u8ddd\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6bd4DPO\u548cSimPO\u7b49DAAs\u7684\u4f20\u7edf\u5b9e\u73b0\u65b9\u5f0f\uff0cPOET\u65b9\u6cd5\u5728AlpacaEval 2\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u6700\u591a15.6\u5206\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5747\u6709\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7a81\u663e\u4e86\u5728DAAs\u4e2d\u89e3\u51b3\u5956\u52b1\u4f18\u5316\u4e0e\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u5bf9\u9f50\u95ee\u9898\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.09378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09378", "abs": "https://arxiv.org/abs/2506.09378", "authors": ["Qijian Tian", "Xin Tan", "Jingyu Gong", "Yuan Xie", "Lizhuang Ma"], "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images", "comment": null, "summary": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and\nsemantic field reconstruction. Combining 3D scenes with semantic fields\nfacilitates the perception and understanding of the surrounding environment.\nHowever, key challenges include embedding semantics into 3D representations,\nachieving generalizable real-time reconstruction, and ensuring practical\napplicability by using only images as input without camera parameters or ground\ntruth depth. To this end, we propose UniForward, a feed-forward model to\npredict 3D Gaussians with anisotropic semantic features from only uncalibrated\nand unposed sparse-view images. To enable the unified representation of the 3D\nscene and semantic field, we embed semantic features into 3D Gaussians and\npredict them through a dual-branch decoupled decoder. During training, we\npropose a loss-guided view sampler to sample views from easy to hard,\neliminating the need for ground truth depth or masks required by previous\nmethods and stabilizing the training process. The whole model can be trained\nend-to-end using a photometric loss and a distillation loss that leverages\nsemantic features from a pre-trained 2D semantic model. At the inference stage,\nour UniForward can reconstruct 3D scenes and the corresponding semantic fields\nin real time from only sparse-view images. The reconstructed 3D scenes achieve\nhigh-quality rendering, and the reconstructed 3D semantic field enables the\nrendering of view-consistent semantic features from arbitrary views, which can\nbe further decoded into dense segmentation masks in an open-vocabulary manner.\nExperiments on novel view synthesis and novel view segmentation demonstrate\nthat our method achieves state-of-the-art performances for unifying 3D scene\nand semantic field reconstruction.", "AI": {"tldr": "This paper introduces UniForward, a model that reconstructs 3D scenes and semantic fields from sparse-view images in real time, achieving high-quality results without requiring camera parameters or depth information, and demonstrating state-of-the-art performance in novel view synthesis and segmentation.", "motivation": "The motivation is to overcome the challenges of embedding semantics into 3D representations, achieving real-time reconstruction, and ensuring the practical applicability using only images as inputs without additional information like camera parameters or ground truth depth.", "method": "Our method, UniForward, is a feed-forward model designed to predict 3D Gaussians with anisotropic semantic features from uncalibrated, sparse-view images. It employs a dual-branch decoupled decoder to unify the 3D scene and semantic field representation, and uses a loss-guided view sampler for training stability.", "result": "Experiments have shown that UniForward can reconstruct 3D scenes and semantic fields in real-time from sparse-view images, achieving high-quality rendering and view-consistent semantic features with state-of-the-art performance.", "conclusion": "The conclusion is that the proposed UniForward model effectively unifies 3D scene and semantic field reconstruction by predicting anisotropic semantic features in 3D Gaussians, from sparse-view input images, and achieves state-of-the-art performance without needing camera parameters or ground truth depth."}}
{"id": "2506.09495", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09495", "abs": "https://arxiv.org/abs/2506.09495", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790181\u4e2a\u66fe\u6709\u751f\u547d\u5371\u9669\u81ea\u6740\u5c1d\u8bd5\u8005\u7684YouTube\u9891\u9053\u89c6\u9891\uff0c\u7ed3\u5408134\u4e2a\u5bf9\u7167\u7ec4\u9891\u9053\uff0c\u91c7\u7528\u8ba1\u7b97\u3001\u6df7\u5408\u548c\u4e13\u5bb6\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u8bc6\u522b\u51fa\u4e0e\u81ea\u6740\u884c\u4e3a\u76f8\u5173\u7684\u4e3b\u9898\uff0c\u53d1\u73b0\u5fc3\u7406\u5065\u5eb7\u6323\u624e\u548cYouTube\u4e92\u52a8\u662f\u4e24\u4e2a\u4e3b\u8981\u6307\u6807\uff0c\u4e14\u52a8\u673a\u7684\u5dee\u5f02\u8868\u660e\u81ea\u6740\u8005\u5e0c\u671b\u5e2e\u52a9\u4ed6\u4eba\u6216\u4e2a\u4eba\u5eb7\u590d\u3002", "motivation": "\u9274\u4e8e\u81ea\u6740\u5728\u5168\u7403\u591a\u4e2a\u897f\u65b9\u56fd\u5bb6\u662f\u4e3b\u8981\u6b7b\u56e0\uff0c\u8be5\u7814\u7a76\u57fa\u4e8e\u793e\u4ea4\u5a92\u4f53\u4e0a\u5185\u5bb9\uff0c\u7279\u522b\u662fYouTube\u89c6\u9891\uff0c\u63a2\u8ba8\u81ea\u6740\u884c\u4e3a\u5982\u4f55\u5448\u73b0\u53ca\u5176\u4e0e\u4e13\u5bb6\u77e5\u8bc6\u7684\u5dee\u5f02\uff0c\u4ee5\u671f\u4e3a\u81ea\u6740\u884c\u4e3a\u7684\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u8ba1\u7b97\u81ea\u4e0b\u800c\u4e0a\u3001\u6df7\u5408\u548c\u4e13\u5bb6\u9a71\u52a8\u81ea\u4e0a\u800c\u4e0b\u7684\u4e09\u4e2a\u65b9\u6cd5\u6765\u5206\u6790\u6570\u636e\uff0c\u5305\u62ec\u957f\u5468\u671f\u7684181\u4e2a\u81ea\u6740\u5c1d\u8bd5\u8005\u7684YouTube\u9891\u9053\uff0c\u5e94\u7528LLM\u4e3b\u9898\u5efa\u6a21\u6765\u8bc6\u522b\u884c\u4e3a\u6307\u793a\u5668\uff0c\u5e76\u7ed3\u5408\u4e34\u5e8a\u4e13\u5bb6\u7684\u5ba1\u6838\u548c\u5fc3\u7406\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\u8bc6\u522b\u4e86\u4e24\u4e2a\u4e0e\u81ea\u6740\u884c\u4e3a\u663e\u8457\u76f8\u5173\u7684\u4e3b\u9898\uff1a\u5fc3\u7406\u5065\u5eb7\u6323\u624e\u548cYouTube\u4e92\u52a8\uff1b\u6df7\u5408\u65b9\u6cd5\u7531\u4e13\u5bb6\u5ba1\u6838\uff0c\u4f46\u672a\u53d1\u73b0\u989d\u5916\u7684\u4e3b\u9898\u3002\u81ea\u4e0a\u800c\u4e0b\u7684\u5fc3\u7406\u8bc4\u4f30\u63ed\u793a\u4e86\u81ea\u6740\u8005\u4e0a\u4f20\u89c6\u9891\u524d\u540e\u5728\u52a8\u673a\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5206\u522b\u5e0c\u671b\u901a\u8fc7\u5206\u4eab\u7ecf\u5386\u5e2e\u52a9\u4ed6\u4eba\u6216\u4f5c\u4e3a\u4e2a\u4eba\u5eb7\u590d\u7684\u4e00\u90e8\u5206\u3002", "conclusion": "\u7814\u7a76\u6574\u5408\u4e86\u81ea\u4e0b\u800c\u4e0a\u548c\u81ea\u4e0a\u800c\u4e0b\u7684\u53d1\u73b0\uff0c\u6307\u51fa\u81ea\u6740\u884c\u4e3a\u4e0e\u6570\u5b57\u884c\u4e3a\u53ca\u4e34\u5e8a\u89c1\u89e3\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u5f3a\u8c03\u81ea\u4e0b\u800c\u4e0a\u65b9\u6cd5\u5728\u63ed\u793a\u5e73\u53f0\u7279\u5b9a\u6307\u6807\uff08\u5982YouTube\u4e92\u52a8\uff09\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.09147", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09147", "abs": "https://arxiv.org/abs/2506.09147", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perli\u0107", "Ekaterina Borisova", "Markarit Vartampetian"], "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u5e76\u89e3\u51b3NLG\u7cfb\u7edf\u4e2d\u7684\u95ee\u9898\u3002", "motivation": "\u8be5\u7814\u7a76\u53d7\u5230\u5f53\u524dLLM\u8bc4\u4f30\u4e3b\u8981\u4f5c\u4e3a\u5b9a\u91cf\u5de5\u5177\u7684\u8d8b\u52bf\u5f71\u54cd\uff0c\u5176\u76ee\u7684\u5728\u4e8e\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6709\u5173\u7cfb\u7edf\u6539\u8fdb\u7684\u6709\u610f\u4e49\u89c1\u89e3\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM-as-a-qualitative-judge\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u8f93\u51fa\u4e3aNLG\u7cfb\u7edf\u8f93\u51fa\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\u7c7b\u578b\u7684\u7ed3\u6784\u5316\u62a5\u544a\u3002\u8be5\u65b9\u6cd5\u7531\u4e24\u4e2a\u4e3b\u8981\u6b65\u9aa4\u7ec4\u6210\uff1a\u5f00\u73af\u5f0f\u5b9e\u4f8b\u95ee\u9898\u5206\u6790\u548c\u4f7f\u7528\u76f4\u89c2\u7684\u7d2f\u79ef\u7b97\u6cd5\u5bf9\u53d1\u73b0\u7684\u95ee\u9898\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM-as-a-qualitative-judge\u5728\u4e09\u5206\u4e4b\u4e8c\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u6b63\u786e\u8bc6\u522b\u5b9e\u4f8b\u7279\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6ce8\u91ca\u8005\u7684\u9519\u8bef\u7c7b\u578b\u62a5\u544a\u3002", "conclusion": "LLM-as-a-qualitative-judge\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5176\u5728\u8bc6\u522b\u548c\u62a5\u544aNLG\u7cfb\u7edf\u8f93\u51fa\u95ee\u9898\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u53cd\u9988\u3002"}}
{"id": "2506.09175", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09175", "abs": "https://arxiv.org/abs/2506.09175", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u65b9\u6cd5\u4ee5\u6539\u5584\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u77ed\u8bed\u7684\u51c6\u786e\u7ffb\u8bd1\uff0c\u6b64\u65b9\u6cd5\u5728\u4e24\u79cd\u6a21\u578b\u4e2d\u90fd\u5927\u5927\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u77ed\u8bed\u53ec\u56de\u7387\u3002", "motivation": "\u7531\u4e8e\u77ed\u8bed\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u51fa\u73b0\u9891\u7387\u8f83\u4f4e\uff0c\u8fd9\u4f7f\u5f97\u5728\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u6b63\u786e\u7ffb\u8bd1\u77ed\u8bed\u53d8\u5f97\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u65b9\u6cd5\u6539\u5584\u77ed\u8bed\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u7684\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u4ece\u6e90\u8bed\u8a00\u5230\u76ee\u6807\u8bed\u8a00\u7684\u77ed\u8bed\u6620\u5c04\u5bf9\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u4e24\u79cd\u5e7f\u6cdb\u91c7\u7528\u7684\u6a21\u578b\uff1a\u57fa\u4e8e\u8f6c\u5bfc\u5668\u7684\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u77ed\u8bed\u5217\u8868\u504f\u7f6e\u76f8\u6bd4\uff0c\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u65b9\u6cd5\u5728\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u4e2d\u63d0\u9ad8\u4e8621%\u7684\u76f8\u5bf9\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd9\u9879\u65b9\u6cd5\u4f7f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5229\u7528\u5916\u90e8\u77ed\u8bed\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e8685%\u7684\u76f8\u5bf9\u77ed\u8bed\u53ec\u56de\u7387\u63d0\u5347\u3002", "conclusion": "\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u65b9\u6cd5\u5728\u6539\u5584\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u77ed\u8bed\u7ffb\u8bd1\u8d28\u91cf\u4e0a\u663e\u793a\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5bf9\u4e8e\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u548c\u6a21\u578b\u6027\u80fd\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2506.09218", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09218", "abs": "https://arxiv.org/abs/2506.09218", "authors": ["Bruno Ferenc \u0160egedin"], "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u5728\u74f6\u9888\u5168\u8fde\u63a5\u5c42\u5927\u5e45\u7f29\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u80fd\u57fa\u4e8e\u968f\u673a\u7279\u5f81\u56fe\u76f4\u63a5\u751f\u6210\u97f3\u9891\u8f93\u51fa\uff0c\u5e76\u663e\u793a\u4e86\u5728\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\u52a8\u6001\u6cdb\u5316\u8bed\u97f3\u4f9d\u5b58\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u8fd9\u8bf4\u660e\u4e86\u6a21\u578b\u5177\u5907\u8bcd\u6c47\u72ec\u7acb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5229\u7528\u8bcd\u6c47\u5b66\u4e60\u884d\u751f\u7684\u97f3\u7cfb\u6cdb\u5316\u7684\u8868\u793a\u80fd\u529b\uff0c\u5e76\u63a2\u7a76\u5728\u589e\u52a0\u6a21\u578b\u538b\u7f29\u7387\uff08\u5373\u7f29\u5c0f\u5168\u8fde\u63a5\u5c42\uff09\u7684\u60c5\u51b5\u4e0b\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u751f\u6210\u5f0f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5bf9\u8bcd\u6c47\u9879\u7684\u539f\u59cb\u97f3\u9891\u6ce2\u5f62\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u63a2\u7a76\u5728\u74f6\u9888\u5168\u8fde\u63a5\u5c42\uff08FC\uff09\u7f29\u5c0f\u81f38\u4fe1\u9053\u540e\uff0c\u6a21\u578b\u5728\u8bcd\u6c47\u4e0d\u53d8\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u74f6\u9888\u5168\u8fde\u63a5\u5c42\u5927\u5e45\u51cf\u5c11\u65f6\uff0c\u53ef\u4ee5\u76f4\u63a5\u5411\u5377\u79ef\u6a21\u5757\u8f93\u5165\u968f\u673a\u7279\u5f81\u56fe\uff0c\u4ee5\u751f\u6210\u97f3\u9891\u8f93\u51fa\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0e\u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u751f\u6210\u7684\u8f93\u51fa\u540c\u6837\u53d7\u5230\u8bad\u7ec3\u4e2d\u8bed\u97f3\u5b66\u9650\u5236\u7684\u5f71\u54cd\u3002\u8fd9\u8bf4\u660e\u5377\u79ef\u5c42\u53ef\u4ee5\u5728\u5168\u8fde\u63a5\u5c42\u5b66\u4e60\u7684\u8bcd\u6c47\u9650\u5236\u914d\u7f6e\u4e4b\u5916\u52a8\u6001\u6cdb\u5316\u8bed\u97f3\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a2\u6d4b\u6a21\u578b\u662f\u5426\u5177\u5907\u8bcd\u6c47\u72ec\u7acb\u6cdb\u5316\u80fd\u529b\u7684\u6280\u672f\uff0c\u8be5\u6280\u672f\u53ea\u5728\u74f6\u9888\u5168\u8fde\u63a5\u5c42\u6781\u7a84\u7684\u60c5\u51b5\u4e0b\u6709\u6548\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u80fd\u591f\u5728\u8bcd\u6c47\u7ea6\u675f\u4e4b\u5916\u6cdb\u5316\u8bed\u97f3\u6a21\u5f0f\u7684\u80fd\u529b\u3002"}}
{"id": "2506.09251", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09251", "abs": "https://arxiv.org/abs/2506.09251", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4efb\u52a1\u5173\u8054\u5206\u6790\u4e86Transformer\u6a21\u578b\u4ece\u77ed\u8f93\u5165\u5230\u957f\u8f93\u5165\u6cdb\u5316\u7684\u8fc1\u79fb\u73b0\u8c61\uff0c\u8868\u660e\u6a21\u578b\u8bad\u7ec3\u65f6\u8054\u5408\u5b66\u4e60\u76f8\u5173\u7684\u957f\u8f93\u5165\u4efb\u52a1\u6709\u52a9\u4e8e\u63d0\u9ad8\u5176\u5728\u76ee\u6807\u4efb\u52a1\u4e2d\u5904\u7406\u957f\u8f93\u5165\u7684\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22Transformer\u6a21\u578b\u5728\u5904\u7406\u957f\u8f93\u5165\u65f6\u7684\u80fd\u529b\u662f\u5982\u4f55\u5f62\u6210\u7684\uff0c\u5c24\u5176\u662f\u5728\u77ed\u8f93\u5165\u8bad\u7ec3\u7684\u57fa\u7840\u4e0a\u5982\u4f55\u80fd\u591f\u63a8\u5e7f\u5230\u66f4\u957f\u7684\u8f93\u5165\u60c5\u5f62\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u6d89\u53ca\u901a\u8fc7\u4efb\u52a1\u5173\u8054\u6765\u63a2\u7d22\u8de8\u4efb\u52a1\u957f\u5ea6\u6cdb\u5316\u7684\u8f6c\u79fb\u73b0\u8c61\uff0c\u4f5c\u8005\u8bbe\u8ba1\u4e86\u591a\u4e2a\u7b97\u6cd5\u4efb\u52a1\uff08\u5982\u7b97\u672f\u8fd0\u7b97\u3001\u5b57\u7b26\u8f6c\u6362\u548c\u8ff7\u5bab\u5bfc\u822a\uff09\u6765\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u3002", "result": "\u672c\u6587\u901a\u8fc7\u4efb\u52a1\u5173\u8054\u7684\u89d2\u5ea6\u7814\u7a76\u4e86Transformer\u8bed\u8a00\u6a21\u578b\u4ece\u77ed\u8f93\u5165\u5230\u957f\u8f93\u5165\u7684\u5916\u63a8\u80fd\u529b\uff0c\u53d1\u73b0\u8fd9\u79cd\u80fd\u529b\u53ef\u4ee5\u5728\u76f8\u5173\u4efb\u52a1\u4e4b\u95f4\u8f6c\u79fb\u3002\u4f5c\u8005\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u7684\u7b97\u6cd5\u4efb\u52a1\u4e2d\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u4e0e\u67d0\u4e2a\u957f\u8f93\u5165\u76f8\u5173\u7684\u8f85\u52a9\u4efb\u52a1\u8bad\u7ec3\uff0c\u83b7\u5f97\u5bf9\u5176\u4ed6\u4efb\u52a1\u7684\u957f\u8f93\u5165\u8fdb\u884c\u6709\u6548\u5904\u7406\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u7c7b\u4f3c\u7684\u4efb\u52a1\u4e4b\u95f4\u8f6c\u79fb\u7684\u7279\u6027\uff0c\u63d0\u793a\u8fd9\u4e9b\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u65f6\u53ef\u80fd\u6784\u5efa\u4e86\u53ef\u590d\u7528\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u4e0b\u6e38\u4efb\u52a1\u7684\u63a8\u65ad\u3002\u7814\u7a76\u8fd8\u521d\u6b65\u63ed\u793a\u4e86\u8fd9\u79cd\u5916\u63a8\u80fd\u529b\u7684\u8f6c\u79fb\u4e0e\u540c\u4e00\u6ce8\u610f\u529b\u5934\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u91cd\u590d\u4f7f\u7528\u6709\u5173\u3002\u603b\u7684\u6765\u8bf4\uff0c\u672c\u6587\u52a0\u6df1\u4e86\u6211\u4eec\u5bf9\u4e8eTransformer\u6a21\u578b\u5982\u4f55\u5904\u7406\u5206\u5e03\u5916\u8f93\u5165\u7684\u673a\u5236\u7406\u89e3\uff0c\u5e76\u5f3a\u8c03\u4e86\u4efb\u52a1\u4e4b\u95f4\u5f52\u7eb3\u7ed3\u6784\u7684\u590d\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u662fTransformer\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u4efb\u52a1\u4e4b\u95f4\u7684\u5173\u8054\u5b66\u4e60\u6765\u7ee7\u627f\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u6846\u67b6\u6709\u52a9\u4e8e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5916\u63a8\u63a8\u7406\u3002\u6ce8\u610f\u529b\u5934\u7684\u91cd\u590d\u4f7f\u7528\u53ef\u80fd\u662f\u8fd9\u79cd\u8fc1\u79fb\u6548\u679c\u80cc\u540e\u7684\u673a\u5236\u3002"}}
{"id": "2506.09066", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09066", "abs": "https://arxiv.org/abs/2506.09066", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "comment": null, "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "AI": {"tldr": "ReStNet dynamically stitches two pre-trained models to adapt to varying resource constraints in IoT devices, achieving flexible accuracy-efficiency trade-offs and reduced training costs.", "motivation": "The motivation behind ReStNet is to address the challenges of deploying fixed pre-trained models in IoT devices with heterogeneous resources. Traditional compression methods fail to adapt dynamically to changing resource constraints.", "method": "ReStNet stitches two pre-trained models together, determining the stitching points using layer-wise similarity calculated via Centered Kernel Alignment. It retains early layers from a higher capacity model and appends deeper layers from a smaller model. Only the stitching layer is fine-tuned. It supports both homogeneous and heterogeneous stitching.", "result": "Experiments show that ReStNet achieves flexible accuracy-efficiency trade-offs and significantly reduces training costs.", "conclusion": "ReStNet offers a reusable and stitchable design that enables rapid adaptation to different resource constraints and supports flexible combination of various model families."}}
{"id": "2506.09259", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2506.09259", "abs": "https://arxiv.org/abs/2506.09259", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u4e0e\u6e38\u620f\u9886\u57df\u4e13\u5bb6\u534f\u4f5c\uff0c\u8bc6\u522b\u5e76\u5206\u7c7b\u6e38\u620f\u4e2d\u73a9\u5bb6\u7684\u4eb2\u793e\u4f1a\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u6211\u951a\u5b9a\u6ce8\u610f\u529b\u6a21\u578b(SAAM)\uff0c\u4f7f\u6a21\u578b\u6027\u80fd\u63d0\u9ad8\u4e867.9%\u3002", "motivation": "\u7814\u7a76\u7684\u91cd\u70b9\u5728\u4e8e\u8bc6\u522b\u548c\u5206\u7c7b\u6e38\u620f\u4e2d\u73a9\u5bb6\u7684\u4eb2\u793e\u4f1a\u884c\u4e3a\uff0c\u8fd9\u662f\u4e0e\u6709\u6bd2\u4e92\u52a8\u8bc6\u522b\u540c\u6837\u91cd\u8981\u7684\u5de5\u4f5c\uff0c\u5c24\u5176\u662f\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u4e86\u65e0\u76d1\u7763\u53d1\u73b0\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6e38\u620f\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\uff0c\u6765\u8bc6\u522b\u548c\u5f52\u7c7b\u6e38\u620f\u4e2d\u73a9\u5bb6\u7684\u4eb2\u793e\u4f1a\u884c\u4e3a\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u6211\u951a\u5b9a\u6ce8\u610f\u529b\u6a21\u578b\uff08SAAM\uff09\uff0c\u8be5\u6a21\u578b\u4e0e\u73b0\u6709\u7684\u6700\u4f73\u6280\u672f\u76f8\u6bd4\u63d0\u9ad8\u4e867.9\uff05\u7684\u6027\u80fd\u3002", "result": "\u5f00\u53d1\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u5206\u7c7b\u6e38\u620f\u4e2d\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u5e76\u5728Call of Duty(R): Modern Warfare(R)II\u6e38\u620f\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6b64\u7814\u7a76\u521b\u65b0\u6027\u5730\u5e94\u7528NLP\u6280\u672f\u6765\u53d1\u73b0\u548c\u5206\u7c7b\u6e38\u620f\u4e2d\u73a9\u5bb6\u7684\u4eb2\u793e\u4f1a\u884c\u4e3a\uff0c\u5c55\u73b0\u4e86\u4ece\u60e9\u7f5a\u6709\u6bd2\u884c\u4e3a\u5230\u9f13\u52b1\u79ef\u6781\u4e92\u52a8\u7684\u8f6c\u53d8\u6f5c\u529b\u3002"}}
{"id": "2506.09067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09067", "abs": "https://arxiv.org/abs/2506.09067", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "comment": null, "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9632\u5fa1\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u751f\u6210\u5f0f\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u5904\u7406\u6709\u5bb3\u67e5\u8be2\uff0c\u540c\u65f6\u907f\u514d\u964d\u7ea7\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u7b56\u7565\u53ef\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\uff0c\u5e76\u901a\u8fc7\u589e\u52a0\u6f14\u793a\u9884\u7b97\u6539\u5584\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u6765\u5e94\u5bf9\u9884\u7b97\u7ea6\u675f\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(Med-VLMs)\u7684\u4e3b\u8981\u529f\u80fd\u662f\u4ece\u5305\u62ec\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\u5728\u5185\u7684\u591a\u6a21\u6001\u8f93\u5165\u751f\u6210\u590d\u6742\u7684\u6587\u672c\u4fe1\u606f\uff0c\u4f46\u5176\u5b89\u5168\u6f0f\u6d1e\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u8fd9\u4e9b\u6a21\u578b\u9700\u8981\u80fd\u591f\u62d2\u7edd\u6709\u5bb3\u67e5\u8be2\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u9632\u5fa1\u5bfc\u81f4\u62d2\u7edd\u826f\u6027\u4e34\u5e8a\u67e5\u8be2\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u65f6\u9632\u5fa1\u7b56\u7565\uff0c\u7528\u4e8e\u7f13\u89e3\u6709\u5bb3\u67e5\u8be2\uff0c\u53ef\u4ee5\u9632\u5fa1\u89c6\u89c9\u548c\u6587\u672c\u8d8a\u72f1\u653b\u51fb\u3002\u8be5\u7b56\u7565\u57fa\u4e8e\u5408\u6210\u4e34\u5e8a\u6f14\u793a\uff0c\u901a\u8fc7\u5728\u591a\u6837\u5316\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u63d0\u9ad8\u6a21\u578b\u5b89\u5168\u6027\u7684\u540c\u65f6\u4e0d\u663e\u8457\u964d\u4f4e\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6f14\u793a\u7b56\u7565\uff0c\u4f5c\u4e3a\u5728\u5c11\u91cf\u6f14\u793a\u9884\u7b97\u7ea6\u675f\u4e0b\u5e73\u8861\u5b89\u5168\u6027\u548c\u6027\u80fd\u7684\u6298\u8877\u65b9\u6848\u3002", "result": "\u6211\u4eec\u7684\u9632\u5fa1\u7b56\u7565\u57fa\u4e8e\u5408\u6210\u4e34\u5e8a\u6f14\u793a\uff0c\u4f7f\u7528\u6765\u81ea\u4e5d\u79cd\u6a21\u5f0f\u7684\u591a\u6837\u5316\u533b\u7597\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u8be5\u7b56\u7565\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u7684\u5b89\u5168\u6027\u800c\u4e0d\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u589e\u52a0\u6f14\u793a\u9884\u7b97\u53ef\u4ee5\u7f13\u89e3\u8fc7\u5ea6\u9632\u5fa1\u95ee\u9898\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u63a8\u7406\u65f6\u9632\u5fa1\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u5408\u6210\u4e34\u5e8a\u6f14\u793a\u6765\u7f13\u89e3\u6709\u5bb3\u67e5\u8be2\uff0c\u53ef\u4ee5\u9632\u5fa1\u89c6\u89c9\u548c\u6587\u672c\u8d8a\u72f1\u653b\u51fb\u3002\u8fd9\u79cd\u7b56\u7565\u80fd\u591f\u63d0\u9ad8\u6a21\u578b\u7684\u5b89\u5168\u6027\u800c\u4e0d\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u5e76\u4e14\u6211\u4eec\u8fd8\u53d1\u73b0\u589e\u52a0\u6f14\u793a\u9884\u7b97\u53ef\u4ee5\u51cf\u8f7b\u8fc7\u5ea6\u9632\u5fa1\u7684\u95ee\u9898\u3002\u9488\u5bf9\u5c11\u91cf\u6f14\u793a\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u5e73\u8861\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6f14\u793a\u7b56\u7565\u4f5c\u4e3a\u6298\u8877\u65b9\u6848\u3002"}}
{"id": "2506.09277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09277", "abs": "https://arxiv.org/abs/2506.09277", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u6211\u81ea\u7136\u8bed\u8a00\u89e3\u91ca(self-NLE)\u7684\u53ef\u4fe1\u5ea6\u3002\u901a\u8fc7\u5bf9\u6bd4self-NLE\u548c\u6a21\u578b\u5185\u90e8\u9690\u85cf\u72b6\u6001\u6765\u6539\u8fdb\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4ef7\u3002", "motivation": "\u9274\u4e8e\u73b0\u6709\u7684\u81ea\u6211\u81ea\u7136\u8bed\u8a00\u89e3\u91ca(self-NLE)\u65b9\u6cd5\u901a\u5e38\u4e0d\u53cd\u6620\u6a21\u578b\u7684\u5b9e\u9645\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0d\u53ef\u4fe1\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5bf9\u6bd4\u6a21\u578b\u5185\u90e8\u72b6\u6001\u6765\u6539\u8fdbself-NLE\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4ef7\u65b9\u6cd5\u3002", "method": "\u6b64\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7075\u6d3b\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u81ea\u4ea7\u751f\u81ea\u7136\u8bed\u8a00\u89e3\u91ca(self-NLE)\u4e0e\u6a21\u578b\u5185\u90e8\u9690\u85cf\u72b6\u6001\u7684\u89e3\u91ca\u76f4\u63a5\u6bd4\u8f83\uff0c\u6765\u5b9a\u91cf\u6d4b\u91cfself-NLE\u7684\u53ef\u4fe1\u5ea6\u3002", "result": "\u8be5\u6846\u67b6\u7684\u4f7f\u7528\u4f7f\u5f97\u7814\u7a76\u8005\u80fd\u591f\u66f4\u52a0\u6df1\u5165\u5730\u4e86\u89e3self-NLE\u7684\u53ef\u4fe1\u5ea6\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u751f\u6210\u66f4\u53ef\u9760\u7684self-NLE\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u5efa\u7acbself-NLE\u4e0e\u6a21\u578b\u63a8\u7406\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u8fd9\u9879\u5de5\u4f5c\u63d0\u9ad8\u4e86\u5bf9self-NLE\u53ef\u4fe1\u5ea6\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u751f\u6210\u66f4\u53ef\u4fe1\u7684self-NLE\u63d0\u4f9b\u4e86\u6784\u5efa\u6a21\u5757\u3002"}}
{"id": "2506.09068", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09068", "abs": "https://arxiv.org/abs/2506.09068", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u7684BG-HOP\u662f\u4e00\u79cd\u751f\u6210\u6027\u5148\u9a8c\u6a21\u578b\uff0c\u7528\u4e8e\u5efa\u6a21\u4e09\u7ef4\u53cc\u624b\u4e0e\u7269\u4f53\u7684\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u53cc\u624b\u4ea4\u4e92\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53cc\u624b\u4ea4\u4e92\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\u5e76\u6355\u6349\u53cc\u624b\u4e0e\u7269\u4f53\u4ea4\u4e92\u7684\u8054\u5408\u5206\u5e03\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u73b0\u6709\u7684\u5355\u624b\u751f\u6210\u5148\u9a8c\u6a21\u578b\uff0cBG-HOP\u65e8\u5728\u5efa\u6a21\u4e09\u7ef4\u7684\u53cc\u624b\u4e0e\u7269\u4f53\u7684\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u8be5\u6a21\u578b\u751f\u6210\u53cc\u624b\u4ea4\u4e92\u548c\u4e3a\u7ed9\u5b9a\u7269\u4f53\u5408\u6210\u6293\u53d6\u59ff\u52bf\u7684\u80fd\u529b\u3002", "conclusion": "\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2506.09301", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09301", "abs": "https://arxiv.org/abs/2506.09301", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86$(RSA)^2$\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RSA\u7406\u8bba\u4e0d\u80fd\u6709\u6548\u5efa\u6a21\u4fee\u8f9e\u8868\u8fbe\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408LLMs\uff0c\u5b9e\u73b0\u5728\u8bbd\u523a\u8868\u8fbe\u7406\u89e3\u65b9\u9762\u7684\u5c16\u7aef\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684RSA\u7406\u8bba\u65e0\u6cd5\u89e3\u91ca\u4fee\u8f9e\u8868\u8fbe\uff0c\u6216\u8005\u9700\u8981\u5728\u7279\u5b9a\u73af\u5883\u4e2d\u5efa\u6a21\u8bf4\u8bdd\u4eba\u4f7f\u7528\u4fee\u8f9e\u8bed\u7684\u6f5c\u5728\u52a8\u673a\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u63d0\u51fa$(RSA)^2$\u6846\u67b6\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86$(RSA)^2$\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8003\u8651\u8bf4\u8bdd\u4eba\u4f7f\u7528\u7684\u4fee\u8f9e\u7b56\u7565\u6765\u5efa\u6a21\u4fee\u8f9e\u8bed\u7684\u4f7f\u7528\u3002", "result": "$(RSA)^2$\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u76f8\u517c\u5bb9\u7684\u975e\u5b57\u9762\u610f\u4e49\u7684\u7406\u89e3\uff0c\u5e76\u4e14\u5728\u65b0\u5f15\u5165\u7684PragMega+\u8a00\u8bed\u89e3\u91ca\u6570\u636e\u96c6\u7684\u8bbd\u523a\u90e8\u5206\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u8bf4\u8bdd\u4eba\u4fee\u8f9e\u7b56\u7565\u7684\u5efa\u6a21\uff0c$(RSA)^2$\u6846\u67b6\u5728\u7406\u89e3\u975e\u5b57\u9762\u610f\u4e49\u65b9\u9762\u66f4\u8fdb\u4e00\u6b65\uff0c\u65e0\u9700\u8003\u8651\u8bf4\u8bdd\u4eba\u4f7f\u7528\u975e\u5b57\u9762\u8bed\u8a00\u7684\u5177\u4f53\u52a8\u673a\u3002"}}
{"id": "2506.09071", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09071", "abs": "https://arxiv.org/abs/2506.09071", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "comment": null, "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAAF\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u63cf\u8ff0\u548c\u56fe\u50cf\u7279\u5f81\uff0c\u5b9e\u73b0\u5efa\u7b51\u7acb\u9762\u5899\u548c\u7a97\u6237\u7684\u9ad8\u7cbe\u5ea6\u81ea\u52a8\u5206\u5272\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u5efa\u7b51\u7684\u6570\u5b57\u5316\u53d1\u5c55\u4e2d\uff0c\u5efa\u7b51\u7acb\u9762\u5899\u548c\u7a97\u6237\u7684\u81ea\u52a8\u5206\u5272\u662f\u63d0\u5347\u5efa\u7b51\u4fe1\u606f\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\u6548\u7387\u7684\u5173\u952e\u6b65\u9aa4\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8bed\u4e49\u5f15\u5bfc\u7684\u5efa\u7b51\u7acb\u9762\u5899\u548c\u7a97\u6237\u7684\u81ea\u52a8\u5206\u5272\u6a21\u578b\uff0c\u79f0\u4e3aSAAF\uff08Segment Any Architectural Facades\uff09\u3002\u9996\u5148\uff0cSAAF\u5177\u6709\u591a\u6a21\u6001\u8bed\u4e49\u534f\u4f5c\u7279\u5f81\u63d0\u53d6\u673a\u5236\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u5c06\u6587\u672c\u63cf\u8ff0\u4e2d\u6240\u5305\u542b\u7684\u8bed\u4e49\u4fe1\u606f\u4e0e\u56fe\u50cf\u7279\u5f81\u878d\u5408\u8d77\u6765\uff0c\u589e\u5f3a\u5bf9\u5efa\u7b51\u7acb\u9762\u7ec4\u4ef6\u7684\u8bed\u4e49\u7406\u89e3\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u4ece\u6587\u672c\u63cf\u8ff0\u5230\u56fe\u50cf\u5206\u5272\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\u5bf9\u5206\u5272\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "result": "\u6211\u4eec\u5728\u591a\u4e2a\u5efa\u7b51\u7acb\u9762\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002SAAF\u7684\u5206\u5272\u7ed3\u679c\u5728mIoU\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u8868\u660eSAAF\u6a21\u578b\u5728\u9762\u5bf9\u591a\u79cd\u6570\u636e\u96c6\u65f6\u80fd\u4fdd\u6301\u8f83\u9ad8\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "\u672c\u6a21\u578b\u5728\u63d0\u9ad8\u5899\u548c\u7a97\u6237\u5206\u5272\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u8fdb\u5c55\u3002\u9884\u8ba1\u5c06\u4e3a\u5efa\u7b51\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u53d1\u5c55\u63d0\u4f9b\u53c2\u8003\uff0c\u4e5f\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u5728\u5efa\u7b51\u9886\u57df\u7684\u5e94\u7528\u63a2\u7d22\u65b0\u7684\u601d\u8def\u548c\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2506.09315", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09315", "abs": "https://arxiv.org/abs/2506.09315", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Mistral-7B\u8bed\u8a00\u6a21\u578b\u6539\u8fdb\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u66f4\u52a0\u900f\u660e\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u662f\u4e00\u79cd\u5f71\u54cd\u8ba4\u77e5\u529f\u80fd\uff0c\u5c24\u5176\u662f\u8bed\u8a00\u80fd\u529b\u7684\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8AD\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u5e76\u521b\u5efa\u66f4\u5177\u89e3\u91ca\u6027\u7684\u51b3\u7b56\u6a21\u578b\uff0c\u4ee5\u8d85\u8d8a\u5f53\u524d\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528Mistral-7B\u6307\u4ee4\u8ddf\u968f\u7248\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6269\u5c55\u4e86\u7528\u4e8e\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u914d\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u7814\u7a76\u63d0\u9ad8\u4e86\u6574\u4f53\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u6e05\u6670\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u76f8\u6bd4\u4e8e\u6700\u4f73\u5f53\u524d\u914d\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.33%\uff0c\u76f8\u6bd4ADReSS 2020\u6311\u6218\u4e2d\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e866.35%\u3002\u901a\u8fc7\u4ed4\u7ec6\u5206\u6790\u6a21\u578b\u751f\u6210\u7684\u56de\u5e94\u548c\u4eba\u7c7b\u56de\u5e94\u7684\u5bf9\u6bd4\uff0c\u7814\u7a76\u8fd8\u5c55\u793a\u4e86LLMs\u5b66\u4e60\u5230\u4e86AD\u60a3\u8005\u7684\u7279\u6b8a\u8bed\u8a00\u6a21\u5f0f\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff0c\u5e76\u5960\u5b9a\u4e86\u89e3\u91ca\u6027\u51b3\u7b56\u8fb9\u754c\u7684\u7406\u8bba\u57fa\u7840\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u6a21\u578b\u89e3\u91ca\u548c\u6570\u636e\u589e\u5f3a\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.09079", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09079", "abs": "https://arxiv.org/abs/2506.09079", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "comment": null, "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165DarkEventInfer\u548cMixVidQA\u4e24\u4e2a\u6570\u636e\u96c6\u4ee5\u53caVersaVid-R1\u6a21\u578b\uff0c\u7814\u7a76\u89e3\u51b3\u4e86\u89c6\u9891\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u4efb\u52a1\u5904\u7406\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cf\u7684\u4ee5\u63a8\u7406\u4e3a\u5bfc\u5411\u7684\u6570\u636e\u548c\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u8f83\u4e3a\u7a00\u7f3a\uff0c\u5bfc\u81f4\u89c6\u9891\u63a8\u7406\u9886\u57df\u53d1\u5c55\u6ede\u540e\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u7279\u5b9a\u7684\u6570\u636e\u96c6\u548c\u5f00\u53d1\u65b0\u6a21\u578b\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6DarkEventInfer\u548cMixVidQA\uff0c\u5206\u522b\u8981\u6c42\u6a21\u578b\u6839\u636e\u4e0a\u4e0b\u6587\u89c6\u9891\u7ebf\u7d22\u63a8\u65ad\u7f3a\u5931\u6bb5\u843d\u4ee5\u53ca\u5728\u4ea4\u7ec7\u7684\u89c6\u9891\u5e8f\u5217\u4e2d\u9694\u79bb\u5e76\u7406\u89e3\u7247\u6bb5\u3002\u6a21\u578b\u8bad\u7ec3\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u591a\u6837\u5316\u7684\u5956\u52b1\u51fd\u6570\u6765\u5f15\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cVersaVid-R1\u6a21\u578b\u5728\u591a\u9879\u89c6\u9891\u7406\u89e3\u3001\u8ba4\u77e5\u63a8\u7406\u548c\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u660e\u663e\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "VersaVid-R1\u662f\u9996\u6b3e\u91c7\u7528Reason-Then-Respond\u8303\u5f0f\u7684\u591a\u7528\u9014\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u9879\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u51fa\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2506.09329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09329", "abs": "https://arxiv.org/abs/2506.09329", "authors": ["Yuxin Jiang"], "title": "Towards Efficient and Effective Alignment of Large Language Models", "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "AI": {"tldr": "The paper introduces methods for refining and collecting training data (Lion, WebR), techniques for enhancing model training (LTE, BMC), and an evaluation benchmark (FollowBench) to better align large language models with human expectations.", "motivation": "The motivation is to improve the alignment of large language models (LLMs) with human expectations by overcoming limitations in data collection, enhancing training techniques, and developing a more accurate evaluation system.", "method": "Our approach comprises three main parts: Lion, an adversarial distillation framework to refine training data by generating challenging instructions; WebR, an automated framework to synthesize instruction data from web documents, enhancing data diversity and scalability; Learning to Edit (LTE), a knowledge updating framework using meta-learning; and BMC, an optimization technique that captures token-level correlations, improving alignment in reasoning tasks.", "result": "The results show state-of-the-art zero-shot reasoning with Lion, enhanced data diversity and scalability with WebR, efficient knowledge updates with LTE, and improved alignment in QA and mathematical reasoning tasks with BMC. FollowBench provides insights into models' constraint adherence weaknesses.", "conclusion": "The study introduces novel methodologies to enhance the alignment of large language models with human expectations through improved data collection, training techniques, and evaluation methods, paving the way for future advancements."}}
{"id": "2506.09081", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09081", "abs": "https://arxiv.org/abs/2506.09081", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "AI": {"tldr": "FlagEvalMM\u662f\u4e00\u4e2a\u5f00\u6e90\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u6a21\u578b\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u5b83\u63d0\u5347\u4e86\u8bc4\u4f30\u6548\u7387\u5e76\u63d0\u4f9b\u6a21\u578b\u6027\u80fd\u7684\u7cbe\u786e\u6d1e\u89c1\u3002", "motivation": "\u65e8\u5728\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u548c\u53d1\u5c55\u3002", "method": "\u6211\u4eec\u4ecb\u7ecd\u4e86FlagEvalMM\uff0c\u4e00\u4e2a\u5f00\u6e90\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u95ee\u7b54\u3001\u6587\u672c\u5230\u56fe\u50cf/\u89c6\u9891\u751f\u6210\u3001\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\uff09\u4e0a\u7684\u8868\u73b0\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u72ec\u7acb\u7684\u8bc4\u4f30\u670d\u52a1\u5c06\u6a21\u578b\u63a8\u7406\u4e0e\u8bc4\u4f30\u8fc7\u7a0b\u89e3\u8026\uff0c\u4ece\u800c\u5b9e\u73b0\u7075\u6d3b\u7684\u8d44\u6e90\u5206\u914d\u548c\u65b0\u4efb\u52a1\u3001\u6a21\u578b\u7684\u65e0\u7f1d\u96c6\u6210\u3002\u6b64\u5916\uff0cFlagEvalMM\u5229\u7528\u5148\u8fdb\u7684\u63a8\u7406\u52a0\u901f\u5de5\u5177\uff08\u5982vLLM\u3001SGLang\uff09\u548c\u5f02\u6b65\u6570\u636e\u52a0\u8f7d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u6548\u7387\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFlagEvalMM\u4e3a\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u51c6\u786e\u9ad8\u6548\u7684\u6d1e\u89c1\uff0c\u662f\u63a8\u8fdb\u591a\u6a21\u6001\u7814\u7a76\u7684\u6709\u529b\u5de5\u5177\u3002", "conclusion": "FlagEvalMM\u6846\u67b6\u516c\u5f00\u53ef\u8bbf\u95ee\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u9014\u5f84\u3002"}}
{"id": "2506.09331", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.09331", "abs": "https://arxiv.org/abs/2506.09331", "authors": ["Arjun Vaithilingam Sudhakar"], "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u662f\u5426\u5177\u6709\u7406\u8bba\u4e0a\u7684\u601d\u7ef4\u80fd\u529b\uff0c\u5373\u80fd\u5426\u7406\u89e3\u4ed6\u4eba\u7684\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u5408\u4f5c\u578b\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(MARL)\u8fdb\u884c\u63a2\u7d22\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u590d\u6742\u7684\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u662f\u5426\u7406\u89e3\u6587\u672c\u4ea4\u6d41\u80cc\u540e\u7684\u610f\u56fe\u8fd8\u6709\u5f85\u9a8c\u8bc1\u3002\u7406\u89e3\u4ed6\u4eba\u7684\u610f\u56fe\u5bf9\u4e8e\u6709\u6548\u7684\u534f\u4f5c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5408\u4f5c\u578b\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(MARL)\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u601d\u7ef4\u7406\u8bba\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u901a\u8fc7\u91cd\u590d\u4ea4\u4e92\u5b66\u4f1a\u534f\u4f5c\uff0c\u8fd9\u53cd\u6620\u4e86\u4eba\u7c7b\u793e\u4f1a\u63a8\u7406\u3002", "result": "\u7814\u7a76\u65e8\u5728\u63d0\u5347\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u9002\u5e94\u548c\u5408\u4f5c\u80fd\u529b\uff0c\u4e0e\u4eba\u5de5\u53ca\u4eba\u7c7b\u4f19\u4f34\u5171\u540c\u5408\u4f5c\u3002", "conclusion": "\u7ed3\u5408\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u7814\u7a76\u65e8\u5728\u53d1\u5c55\u6df7\u5408\u7684\u4eba\u5de5\u667a\u80fd-\u4eba\u7c7b\u7cfb\u7edf\uff0c\u5b9e\u73b0\u6d41\u7545\u7684\u5408\u4f5c\uff0c\u5bf9\u672a\u6765\u4eba\u673a\u4ea4\u4e92\u6709\u4e00\u5b9a\u7684\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2506.09082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09082", "abs": "https://arxiv.org/abs/2506.09082", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "comment": "First two authors contribute equally", "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86AVA-Bench\uff0c\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30vision foundation models (VFMs)\u768414\u79cd\u57fa\u7840\u89c6\u89c9\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VQA\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e24\u4e2a\u5173\u952e\u76f2\u70b9\uff0c\u5e76\u8868\u660e\u8f83\u5c0f\u7684LLM\uff080.5B\uff09\u4e5f\u80fd\u63d0\u4f9b\u7c7b\u4f3c\u7684VFM\u6392\u540d\u8bc4\u4f30\uff0c\u540c\u65f6\u5927\u5927\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u6d88\u8017\u3002", "motivation": "\u76ee\u7684\u662f\u89e3\u51b3\u73b0\u6709VFMs\u8bc4\u4f30\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u4e24\u4e2a\u4e3b\u8981\u76f2\u70b9\uff1a\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u53ef\u80fd\u4e0eVQA\u6d4b\u8bd5\u5206\u5e03\u4e0d\u4e00\u81f4\uff0c\u4ee5\u53caVQA\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u8981\u6c42\u591a\u79cd\u89c6\u89c9\u80fd\u529b\uff0c\u96be\u4ee5\u786e\u5b9a\u9519\u8bef\u662f\u7531\u6240\u6709\u5fc5\u9700\u80fd\u529b\u7684\u7f3a\u5931\u8fd8\u662f\u7531\u5355\u4e00\u5173\u952e\u80fd\u529b\u7f3a\u5931\u5bfc\u81f4\u3002", "method": "\u63d0\u51fa\u4e86AVA-Bench, \u8be5\u57fa\u51c6\u6d4b\u8bd5\u660e\u786e\u62c6\u5206\u4e8614\u79cd\u57fa\u7840\u89c6\u89c9\u80fd\u529b\uff0c\u5e76\u786e\u4fdd\u6bcf\u79cd\u80fd\u529b\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u4e0a\u7684\u5206\u5e03\u4e00\u81f4\u6027\uff0c\u4ee5\u4fbf\u51c6\u786e\u6307\u51faVFMs\u5728\u54ea\u65b9\u9762\u8868\u73b0\u51fa\u8272\u6216\u4e0d\u8db3\uff0c\u8f6c\u800c\u4f7fVFM\u9009\u62e9\u66f4\u52a0\u79d1\u5b66\u5408\u7406\u3002", "result": "\u5e94\u7528AVA-Bench\u5230\u9886\u5148\u7684VFMs\u4e0a\u63ed\u793a\u4e86\u72ec\u7279\u7684\"\u80fd\u529b\u6307\u7eb9\"\uff0c\u5e76\u4e14\u53d1\u73b0\u4e00\u4e2a\u8f83\u5c0f\u76840.5B LLM\u80fd\u4e0e\u4e00\u4e2a7B LLM\u63d0\u4f9b\u76f8\u4f3c\u7684VFM\u8bc4\u4ef7\u7ed3\u679c\uff0c\u4f46\u53ef\u8282\u77018\u500d\u7684GPU\u5c0f\u65f6\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bc4\u4f30\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u5168\u9762\u900f\u660e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0cAVA-Bench\u4e3a\u8fdb\u4e00\u6b65\u6539\u8fdbVFMs\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.09340", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09340", "abs": "https://arxiv.org/abs/2506.09340", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "title": "RePO: Replay-Enhanced Policy Optimization", "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "RePO enhances the efficiency and performance of reinforcement learning in optimizing large language models by utilizing a replay buffer for diverse sample retrieval, achieving significant performance gains compared to previous methods while managing computational costs.", "motivation": "The motivation behind introducing RePO is to address the limitations of GRPO, which requires multiple on-policy outputs per prompt and suffers from high computational costs and low data efficiency.", "method": "The paper introduces Replay-Enhanced Policy Optimization (RePO), which retrieves off-policy samples from a replay buffer to optimize the policy based on a more diverse set of samples for each prompt, aiming to enhance data efficiency and reduce computational costs compared to Group Relative Policy Optimization (GRPO).", "result": "Experiments conducted on five LLMs across seven mathematical reasoning benchmarks show that RePO achieves absolute average performance gains of 18.4 and 4.1 points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Furthermore, RePO increases the computational cost by 15% but raises the number of effective optimization steps by 48% for Qwen3-1.7B.", "conclusion": "The conclusion of the paper is that RePO is effective in improving the performance of LLMs in mathematical reasoning tasks while requiring only a small increase in computational cost, making it a promising approach for optimizing large language models."}}
{"id": "2506.09083", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09083", "abs": "https://arxiv.org/abs/2506.09083", "authors": ["Jerry Lin", "Partick P. W. Chen"], "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "comment": "4 pages, 3 figures, 1 Table", "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "AI": {"tldr": "BakuFlow\u662f\u4e00\u79cd\u534a\u81ea\u52a8\u6807\u6ce8\u5de5\u5177\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6807\u6ce8\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5927\u89c4\u6a21\u4efb\u52a1\u3002", "motivation": "\u51c6\u786e\u7684\u6570\u636e\u6807\u6ce8\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u4e00\u4e2a\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u4efb\u52a1\uff0c\u624b\u52a8\u6807\u6ce8\u65e2\u8017\u65f6\u53c8\u5bb9\u6613\u51fa\u9519\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u6807\u7b7e\u751f\u6210\u5de5\u5177BakuFlow\uff0c\u8be5\u5de5\u5177\u5177\u6709\u5b9e\u65f6\u53ef\u8c03\u653e\u5927\u955c\u3001\u4ea4\u4e92\u5f0f\u6570\u636e\u589e\u5f3a\u6a21\u5757\u3001\u6807\u7b7e\u4f20\u64ad\u529f\u80fd\u4ee5\u53ca\u57fa\u4e8e\u4fee\u6539\u540e\u7684YOLOE\u6846\u67b6\u7684\u81ea\u52a8\u6807\u7b7e\u6a21\u5757\u3002", "result": "\u8be5\u5de5\u5177\u901a\u8fc7\u521b\u65b0\u529f\u80fd\u5728\u6807\u6ce8\u89c6\u9891\u6570\u636e\u65f6\u5927\u5e45\u52a0\u5feb\u6807\u6ce8\u901f\u5ea6\uff0c\u5e76\u4f7f\u6807\u6ce8\u8fc7\u7a0b\u53d8\u5f97\u66f4\u52a0\u7075\u6d3b\u548c\u53ef\u6269\u5c55\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u548c\u8ddf\u8e2a\u3002", "conclusion": "BakuFlow\u5728\u5b9e\u9645\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u5de5\u4e1a\u573a\u666f\u4e2d\u5927\u5927\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u7279\u522b\u6709\u6548\u5730\u9002\u7528\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u548c\u8ddf\u8e2a\u4efb\u52a1\u3002"}}
{"id": "2506.09342", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09342", "abs": "https://arxiv.org/abs/2506.09342", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Latent Multi-Head Attention for Small Language Models", "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "AI": {"tldr": "\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u4f18\u5316\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5185\u5b58\u5360\u7528\u548c\u63a8\u7406\u901f\u5ea6\u7684\u65b9\u6cd5\uff0c\u5373\u5728\u6f5c\u5728\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u52a0\u5165\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u8282\u7701\u548c\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5f15\u5165\u6f5c\u5728\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u7ed3\u5408\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\uff0c\u6765\u4f18\u5316\u6a21\u578b\u7684\u5185\u5b58\u5360\u7528\uff0c\u5e76\u63d0\u9ad8\u63a8\u7406\u7684\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u8bc1\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u3002", "method": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u6f5c\u5728\u591a\u5934\u6ce8\u610f\u529b\uff08MLA\uff09\u7684\u9996\u4e2a\u5168\u9762\u7814\u7a76\uff0c\u5e76\u5bf9\u5176\u6548\u7387\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u8fdb\u884c\u4e86\u63ed\u793a\u3002\u5b9e\u9a8c\u901a\u8fc7\u5728100,000\u4e2a\u5408\u6210\u6545\u4e8b\u4e0a\u8bad\u7ec330M\u53c2\u6570\u7684GPT\u6a21\u578b\uff0c\u5bf9\u6bd4\u4e86\u4e09\u79cd\u67b6\u6784\u53d8\u4f53\uff1a\u6807\u51c6\u7684\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u3001MLA\u4ee5\u53ca\u5e94\u7528\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u7684MLA\uff08MLA+RoPE\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0MLA+RoPE\u4f7f\u7528\u534a\u79e9\u6f5c\u5728\u7ef4\u5ea6\uff08r = d/2\uff09\u53ef\u4ee5\u51cf\u5c1145%\u7684KV\u7f13\u5b58\u5185\u5b58\u5360\u7528\uff0c\u800c\u9a8c\u8bc1\u635f\u5931\u4ec5\u589e\u52a00.3%\uff0c\u51e0\u4e4e\u6ca1\u6709\u635f\u5931\u8d28\u91cf\uff0c\u662f\u5185\u5b58\u53d7\u9650\u90e8\u7f72\u4e2d\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u5728\u5c0f\u578b\u6a21\u578b\u4e2d\uff0cRoPE\u662fMLA\u6027\u80fd\u7684\u5173\u952e\u3002MLA\u5728\u5e94\u7528RoPE\u540e\u8d85\u8fc7\u4e86\u6807\u51c6\u6ce8\u610f\u529b\u673a\u52362%\u3002\u5728NVIDIA A100 GPU\u4e0a\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u91c7\u7528r=d/2\u7684MLA\u53ef\u4ee5\u5b9e\u73b0\u76f8\u5bf9\u4e8e\u5168\u79e9MLA\u76841.4\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u8282\u7701\u3002GPT-4\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u56f0\u60d1\u5ea6\u7ed3\u679c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8bed\u6cd5\u3001\u521b\u610f\u548c\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u8d28\u91cf\u5206\u6570\uff087.4/10\uff09\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0cMLA+RoPE\u901a\u8fc7\u51cf\u5c11KV\u7f13\u5b58\u5185\u5b58\u7684\u4f7f\u7528\u6765\u4f18\u5316\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8fbe\u5230\u5728\u4e0d\u727a\u7272\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u63a8\u7406\u7684\u76ee\u7684\u3002"}}
{"id": "2506.09106", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09106", "abs": "https://arxiv.org/abs/2506.09106", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "title": "Bias Analysis in Unconditional Image Generative Models", "comment": null, "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.09349", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09349", "abs": "https://arxiv.org/abs/2506.09349", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "AI": {"tldr": "OmniDRCA\u6a21\u578b\u5b9e\u73b0\u4e86\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u793a\u548c\u5bf9\u6bd4\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u5e76\u884c\u8bed\u97f3\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u8fbe\u5230\u8be5\u9886\u57df\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u79bb\u6563\u8bed\u97f3\u4ee4\u724c\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u65e8\u5728\u6539\u5584\u8bed\u97f3\u548c\u6587\u672c\u751f\u6210\u4e4b\u95f4\u7684\u4ea4\u4e92\u6027\u548c\u534f\u540c\u6027\u3002", "method": "OmniDRCA\u91c7\u7528\u5e76\u884c\u7684\u8bed\u97f3-\u6587\u672c\u57fa\u7840\u6a21\u578b\uff0c\u57fa\u4e8e\u8054\u5408\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u5177\u6709\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u793a\u548c\u5bf9\u6bd4\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5b9e\u73b0\u8bed\u97f3\u548c\u6587\u672c\u7684\u5e76\u884c\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u6765\u589e\u5f3a\u97f3\u9891\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOmniDRCA\u5728\u53e3\u8bed\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u5e76\u884c\u8054\u5408\u8bed\u97f3-\u6587\u672c\u5efa\u6a21\u57fa\u7840\u6a21\u578b\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u5e76\u4e0e\u4ea4\u9519\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8868\u793a\uff0cOmniDRCA\u6a21\u578b\u5728\u5e73\u884c\u8bed\u97f3\u548c\u6587\u672c\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63a2\u7d22\u4e86\u5c06\u5176\u6846\u67b6\u6269\u5c55\u81f3\u5168\u53cc\u5de5\u5bf9\u8bdd\u573a\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.09109", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09109", "abs": "https://arxiv.org/abs/2506.09109", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807CAIRe\uff0c\u7528\u4e8e\u8861\u91cf\u751f\u6210\u56fe\u50cf\u7684\u6587\u5316\u76f8\u5173\u6027\uff0c\u4e0e\u73b0\u6709\u7684\u57fa\u7ebf\u6bd4\u8f83\u5c55\u793a\u4e86\u663e\u8457\u7684\u4f18\u52bf\uff0c\u5e76\u4e0e\u4eba\u7c7b\u7684\u5224\u65ad\u4fdd\u6301\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5728\u6587\u672c\u5230\u56fe\u50cf\u7684\u8f6c\u6362\u6a21\u578b\u4e2d\uff0c\u8de8\u6587\u5316\u504f\u5dee\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5305\u62ec\u6027\u80fd\u4e0b\u964d\u3001\u4e8b\u5b9e\u4e0d\u51c6\u786e\u6216\u8f93\u51fa\u5185\u5bb9\u5192\u72af\u7b49\u95ee\u9898\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807CAIRe\uff0c\u8be5\u6307\u6807\u7528\u4e8e\u8bc4\u4f30\u56fe\u50cf\u7684\u6587\u5316\u76f8\u5173\u6027\uff0c\u7ed9\u5b9a\u7528\u6237\u5b9a\u4e49\u7684\u6807\u7b7e\u96c6\uff0c\u5c06\u5b9e\u4f53\u548c\u6982\u5ff5\u4e0e\u77e5\u8bc6\u5e93\u4e2d\u7684\u4fe1\u606f\u8fdb\u884c\u94fe\u63a5\uff0c\u5e76\u4f7f\u7528\u4e8b\u5b9e\u4fe1\u606f\u5bf9\u6bcf\u4e2a\u6587\u5316\u6807\u7b7e\u8fdb\u884c\u72ec\u7acb\u7684\u5206\u7ea7\u5224\u65ad\u3002", "result": "\u5728\u624b\u52a8\u6574\u7406\u7684\u6570\u636e\u96c6\u4e0a\uff0cCAIRe\u7684F1\u503c\u8d85\u8fc7\u6240\u6709\u57fa\u7ebf28%\uff1b\u5728\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7684\u6587\u5316\u76f8\u5173\u4f46\u7f55\u89c1\u9879\u76ee\u7684\u6570\u636e\u96c6\u4e0a\uff0cCAIRe\u4e0e\u4eba\u5de5\u8bc4\u7ea7\u5728\u6587\u5316\u76f8\u5173\u6027\u65b9\u9762\u8fbe\u5230\u4e860.56\u548c0.66\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u57fa\u4e8e5\u7ea7Likert\u91cf\u8868\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cCAIRe\u5728\u8bc4\u4f30\u56fe\u50cf\u6587\u5316\u76f8\u5173\u6027\u7684\u8fc7\u7a0b\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u529f\u80fd\uff0c\u5e76\u80fd\u4e0e\u4eba\u7c7b\u7684\u5224\u65ad\u4fdd\u6301\u9ad8\u5ea6\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u56fe\u50cf\u6765\u6e90\u3002"}}
{"id": "2506.09351", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09351", "abs": "https://arxiv.org/abs/2506.09351", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6837\u6027\u7684\u589e\u5f3a\u91cd\u5efa\u65b9\u6cd5DIVE\uff0c\u7528\u4e8e\u91cd\u6784\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u91cd\u6784\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u8f83\u5c0f\u7684\u7cbe\u5ea6\u6743\u8861\u3002", "motivation": "\u5c3d\u7ba1MoE\u67b6\u6784\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u65f6\u6548\u7387\u5f88\u9ad8\uff0c\u4f46\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u5927\u91cf\u4e13\u5bb6\u4f1a\u5e26\u6765\u663e\u8457\u7684\u5f00\u9500\uff0c\u800c\u5c06\u5bc6\u96c6\u7684LLMs\u8f6c\u6362\u4e3aMoE LLMs\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u9884\u7b97\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u91cd\u6784\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u4e86\u4e13\u5bb6\u4e4b\u95f4\u7684\u591a\u6837\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6837\u6027\u7684\u589e\u5f3a\u91cd\u5efa\u65b9\u6cd5DIVE\u3002", "method": "DIVE\u65b9\u6cd5\u6d89\u53ca\u9886\u57df\u4eb2\u548c\u529b\u6316\u6398\u3001\u57fa\u4e8e\u526a\u679d\u7684\u4e13\u5bb6\u91cd\u6784\u4ee5\u53ca\u9ad8\u6548\u7684\u518d\u8bad\u7ec3\u3002\u7279\u522b\u662f\uff0c\u91cd\u6784\u8fc7\u7a0b\u5305\u62ec\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u6a21\u5757\u7684\u526a\u679d\u548c\u91cd\u7ec4\u3002\u91cd\u6784\u540e\uff0c\u6211\u4eec\u5bf9\u8def\u7531\u5668\u3001\u4e13\u5bb6\u548c\u5f52\u4e00\u5316\u6a21\u5757\u8fdb\u884c\u9ad8\u6548\u7684\u518d\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86DIVE\u7684\u6709\u6548\u6027\uff0c\u5b83\u80fd\u591f\u4ee5\u8f83\u5c0f\u7684\u7cbe\u5ea6\u6743\u8861\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u526a\u679d\u548cMoE\u91cd\u6784\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0cDIVE\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u526a\u679d\u548cMoE\u91cd\u6784\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u6fc0\u6d3b\u53c2\u6570\u6570\u91cf\u76f8\u540c\u7684\u60c5\u51b5\u4e0b\uff0c\u5177\u6709\u8f83\u5c0f\u7684\u7cbe\u5ea6\u6743\u8861\uff0c\u5e76\u4f18\u4e8e\u5b83\u4eec\u3002"}}
{"id": "2506.09113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09113", "abs": "https://arxiv.org/abs/2506.09113", "authors": ["Yu Gao", "Haoyuan Guo", "Tuyen Hoang", "Weilin Huang", "Lu Jiang", "Fangyuan Kong", "Huixia Li", "Jiashi Li", "Liang Li", "Xiaojie Li", "Xunsong Li", "Yifu Li", "Shanchuan Lin", "Zhijie Lin", "Jiawei Liu", "Shu Liu", "Xiaonan Nie", "Zhiwu Qing", "Yuxi Ren", "Li Sun", "Zhi Tian", "Rui Wang", "Sen Wang", "Guoqiang Wei", "Guohong Wu", "Jie Wu", "Ruiqi Xia", "Fei Xiao", "Xuefeng Xiao", "Jiangqiao Yan", "Ceyuan Yang", "Jianchao Yang", "Runkai Yang", "Tao Yang", "Yihang Yang", "Zilyu Ye", "Xuejiao Zeng", "Yan Zeng", "Heng Zhang", "Yang Zhao", "Xiaozheng Zheng", "Peihao Zhu", "Jiaxin Zou", "Feilong Zuo"], "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "comment": "Seedance 1.0 Technical Report", "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Seedance 1.0\uff0c\u4e00\u4e2a\u9ad8\u6027\u80fd\u548c\u63a8\u7406\u9ad8\u6548\u7684\u57fa\u7840\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u6539\u8fdb\u4e86\u6570\u636e\u5904\u7406\u3001\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u3001\u540e\u8bad\u7ec3\u4f18\u5316\u548c\u6a21\u578b\u52a0\u901f\u6280\u672f\uff0c\u5728\u89c6\u9891\u751f\u6210\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5728\u540c\u6b65\u5e73\u8861\u6307\u4ee4\u9075\u5faa\u3001\u52a8\u4f5c\u5408\u7406\u6027\u4ee5\u53ca\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002Seedance 1.0\u6b63\u662f\u4e3a\u4e86\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u5404\u9879\u6027\u80fd\u800c\u5f00\u53d1\u3002", "method": "Seedance 1.0\u91c7\u7528\u4e86\u591a\u4e2a\u6838\u5fc3\u6280\u672f\u6539\u8fdb\uff1a(i) \u591a\u6e90\u6570\u636e\u6574\u7406\uff0c\u589e\u5f3a\u7cbe\u786e\u548c\u6709\u610f\u4e49\u7684\u89c6\u9891\u6807\u6ce8\uff0c\u4ee5\u5b9e\u73b0\u8de8\u8d8a\u591a\u79cd\u573a\u666f\u7684\u5168\u9762\u5b66\u4e60\uff1b(ii) \u4e00\u4e2a\u9ad8\u6548\u67b6\u6784\u8bbe\u8ba1\u7684\u8bad\u7ec3\u6a21\u5f0f\uff0c\u652f\u6301\u591a\u955c\u5934\u751f\u6210\uff0c\u5e76\u540c\u65f6\u5b66\u4e60\u6587\u672c-\u89c6\u9891\u548c\u56fe\u50cf-\u89c6\u9891\u4efb\u52a1\uff1b(iii) \u7ec6\u81f4\u4f18\u5316\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u7cbe\u7ec6\u76d1\u7763\u5fae\u8c03\u548c\u591a\u7ef4\u5956\u52b1\u673a\u5236\u7684\u89c6\u9891\u7279\u5f02\u6027RLHF\u8fdb\u884c\u7efc\u5408\u6027\u80fd\u6539\u8fdb\uff1b(iv) \u4f18\u79c0\u6a21\u578b\u52a0\u901f\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u77e5\u8bc6\u63d0\u70bc\u7b56\u7565\u548c\u7cfb\u7edf\u7ea7\u4f18\u5316\u5b9e\u73b0\u5927\u7ea610\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "result": "Seedance 1.0\u80fd\u591f\u4ee51080p\u5206\u8fa8\u7387\u4ec5\u752841.4\u79d2\u751f\u62105\u79d2\u7684\u89c6\u9891\uff0c\u5728NVIDIA-L20\u4e0a\u5b9e\u73b0\u4e86\u901f\u5ea6\u548c\u8d28\u91cf\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u76f8\u6bd4\u4e8e\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0cSeedance 1.0\u5728\u9ad8\u8d28\u91cf\u548c\u5feb\u901f\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5177\u5907\u51fa\u8272\u7684\u65f6\u7a7a\u6d41\u7545\u6027\u3001\u7ed3\u6784\u7a33\u5b9a\u6027\u3001\u5728\u590d\u6742\u591a\u4e3b\u4f53\u573a\u666f\u4e2d\u7684\u7cbe\u786e\u6307\u4ee4\u9075\u5faa\u6027\u4ee5\u53ca\u591a\u955c\u5934\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002"}}
{"id": "2506.09359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09359", "abs": "https://arxiv.org/abs/2506.09359", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation.", "AI": {"tldr": "The paper explores the use of LLMs to evaluate the semantic equivalence of SQL queries generated from natural language inputs in NL2SQL systems.", "motivation": "The motivation is to address the difficulty in assessing the semantic correctness of SQL outputs derived from natural language inputs using LLMs, a problem that has increased in complexity with the advent of advanced LLMs in NL2SQL systems.", "method": "This paper examines the use of Large Language Models (LLMs) to evaluate the semantic equivalence of SQL queries generated by Text-to-SQL systems, considering the nuances of ambiguous natural language inputs and multiple possible SQL interpretations.", "result": "Not explicitly mentioned in the abstract, but the analysis of patterns of SQL equivalence and inequivalence and the discussion of LLM-based evaluation challenges suggest insights into how LLMs can be leveraged for SQL semantic evaluation.", "conclusion": "The paper concludes by underscoring the potential of LLMs in assessing SQL equivalence and inequivalence, and by highlighting the challenges and patterns of SQL equivalence in the context of LLM-based evaluation."}}
{"id": "2506.09229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09229", "abs": "https://arxiv.org/abs/2506.09229", "authors": ["Sungwon Hwang", "Hyojin Jang", "Kinam Kim", "Minho Park", "Jaegul choo"], "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models", "comment": "24 pages, 25 figures", "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io", "AI": {"tldr": "This paper introduces Cross-frame Representation Alignment (CREPA), a new technique for improving semantic consistency in fine-tuned Video Diffusion Models (VDMs) across frames.", "motivation": "The motivation is to explore the fine-tuning of VDMs at the user level to generate videos that better reflect specific attributes of training data, which is an underexplored area with high practical relevance.", "method": "Content provided discusses the adaptation of Representation Alignment (REPA) for fine-tuning Video Diffusion Models (VDMs). It identifies limitations in maintaining semantic consistency and introduces Cross-frame Representation Alignment (CREPA) as a novel regularization technique addressing this issue.", "result": "Empirical evaluations show that CREPA improves visual fidelity and cross-frame semantic coherence when fine-tuned with parameter-efficient methods such as LoRA.", "conclusion": "The conclusion validates the effectiveness of CREPA, showing that it not only enhances the visual quality of video generation but also improves semantic coherence across frames when fine-tuned with LoRA, and its broad applicability is confirmed across diverse datasets."}}
{"id": "2506.09367", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09367", "abs": "https://arxiv.org/abs/2506.09367", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aCOGENT\u7684\u8bfe\u7a0b\u5bfc\u5411\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u4e0e\u5e74\u7ea7\u76f8\u7b26\u7684\u6559\u80b2\u5185\u5bb9\uff0c\u8be5\u6846\u67b6\u5728\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u4e0e\u4eba\u7c7b\u53c2\u8003\u76f8\u5ab2\u7f8e\u6216\u66f4\u4f18\u7684\u7ed3\u679c\u3002", "motivation": "\u7531\u4e8e\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u6559\u5b66\u5e94\u7528\u4e2d\u5b58\u5728\u65e0\u6cd5\u59cb\u7ec8\u4e0e\u8bfe\u7a0b\u6807\u51c6\u53ca\u5e74\u7ea7\u9605\u8bfb\u6c34\u5e73\u4fdd\u6301\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728STEM\u6559\u80b2\u4e2d\uff0c\u5982\u4f55\u7528\u65e5\u5e38\u8bed\u8a00\u89e3\u91ca\u590d\u6742\u62bd\u8c61\u7684\u6982\u5ff5\u548c\u73b0\u8c61\u6210\u4e3a\u4e00\u4e2a\u6311\u6218\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u8be5\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86COGENT\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4e09\u4e2a\u8bfe\u7a0b\u7ec4\u4ef6\uff08\u79d1\u5b66\u6982\u5ff5\u3001\u6838\u5fc3\u7406\u5ff5\u548c\u5b66\u4e60\u76ee\u6807\uff09\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u5185\u5bb9\u7684\u957f\u5ea6\u3001\u8bcd\u6c47\u548c\u53e5\u5b50\u590d\u6742\u5ea6\u6765\u8c03\u6574\u53ef\u8bfb\u6027\u3002\u540c\u65f6\u8fd8\u91c7\u7528\u4e86\u201c\u597d\u5947\u5fc3\u9a71\u52a8\u201d\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5b66\u751f\u7684\u5b66\u4e60\u5174\u8da3\u548c\u53c2\u4e0e\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCOGENT\u80fd\u591f\u7a33\u5b9a\u5730\u751f\u6210\u4e0e\u5e74\u7ea7\u76f8\u7b26\u7684\u6bb5\u843d\uff0c\u8fd9\u4e9b\u6bb5\u843d\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u4e86\u4e0e\u4eba\u7c7b\u53c2\u8003\u6750\u6599\u76f8\u5f53\u6216\u66f4\u597d\u7684\u8d28\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u786e\u7acb\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6269\u5c55\u5e76\u63d0\u9ad8\u5b66\u4e60\u8d44\u6e90\u7684\u8d28\u91cf\u3002"}}
{"id": "2506.09237", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09237", "abs": "https://arxiv.org/abs/2506.09237", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u9c81\u68d2\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\u2014\u2014PatchGuard\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8eVision Transformer\uff0c\u901a\u8fc7\u5f15\u5165\u524d\u666f\u611f\u77e5\u4f2a\u5f02\u5e38\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPatchGuard\u5728\u5de5\u4e1a\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5bf9\u6297\u6027\u6027\u80fd\u5927\u5927\u63d0\u9ad8\u3002", "motivation": "\u5f53\u524d\u7684AD\u548cAL\u65b9\u6cd5\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u901a\u5e38\u662f\u53ea\u5305\u542b\u6b63\u5e38\u6837\u672c\u4e14\u672a\u6807\u6ce8\u7684\u6570\u636e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u8fd9\u4e9b\u7cfb\u7edf\u7684\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86PatchGuard\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u57fa\u4e8eVision Transformer (ViT)\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u4f2a\u5f02\u5e38\u548c\u5b9a\u4f4d\u63a9\u6a21\uff0c\u4ee5\u5e94\u5bf9AD\u548cAL\u7cfb\u7edf\u7684\u6613\u53d7\u653b\u51fb\u6027\u3002PatchGuard\u5229\u7528\u524d\u666f\u611f\u77e5\u4f2a\u5f02\u5e38\u6765\u514b\u670d\u524d\u4eba\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u501f\u52a9\u65b0\u7684\u635f\u5931\u51fd\u6570\u6307\u5bfc\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPatchGuard\u5728\u5de5\u4e1a\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e0b\uff0cAD\u6027\u80fd\u63d0\u9ad8\u4e8653.2%\uff0cAL\u6027\u80fd\u63d0\u9ad8\u4e8668.5%\u3002\u5b83\u540c\u65f6\u4e5f\u5728\u5e38\u89c4\u73af\u5883\u4e0b\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "PatchGuard\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u524d\u666f\u611f\u77e5\u4f2a\u5f02\u5e38\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u663e\u7740\u63d0\u9ad8\u4e86AD\u548cAL\u6a21\u578b\u7684\u5bf9\u6297\u6027\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u5176\u5b83\u65b9\u9762\u7684\u7ade\u4e89\u529b\u3002"}}
{"id": "2506.09375", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09375", "abs": "https://arxiv.org/abs/2506.09375", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u7684CoLMbo\u8bf4\u8bdd\u4eba\u8bed\u8a00\u6a21\u578b(SLM)\u901a\u8fc7\u6574\u5408\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u6761\u4ef6\u8c03\u8282\uff0c\u80fd\u591f\u751f\u6210\u8be6\u7ec6\u4e14\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u8bf4\u8bdd\u4eba\u63cf\u8ff0\uff0c\u63d0\u5347\u4e86\u4f20\u7edf\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\u7684\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u96be\u4ee5\u751f\u6210\u8be6\u7ec6\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\u6216\u8868\u8fbe\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u63cf\u8ff0\u3002\u8fd9\u4e9b\u6a21\u578b\u4e3b\u8981\u63d0\u53d6\u8bf4\u8bdd\u4eba\u7684\u5d4c\u5165\u8fdb\u884c\u8eab\u4efd\u8bc6\u522b\uff0c\u4f46\u65e0\u6cd5\u4ee5\u7ed3\u6784\u5316\u65b9\u5f0f\u6355\u6349\u8bf8\u5982\u65b9\u8a00\u3001\u6027\u522b\u548c\u5e74\u9f84\u7b49\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u3002", "method": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aCoLMbo\u7684\u8bf4\u8bdd\u4eba\u8bed\u8a00\u6a21\u578b(SLM)\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5c06\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u4e0e\u57fa\u4e8e\u63d0\u793a\u7684\u8c03\u8282\u65b9\u6cd5\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u751f\u6210\u8be6\u7ec6\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\u548c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u4e30\u5bcc\u63cf\u8ff0\u7684\u95ee\u9898\u3002\u901a\u8fc7\u4f7f\u7528\u7528\u6237\u5b9a\u4e49\u7684\u63d0\u793a\u6765\u9002\u5e94\u65b0\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\uff0cCoLMbo\u80fd\u591f\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u63cf\u8ff0\uff0c\u5305\u62ec\u533a\u57df\u65b9\u8a00\u7684\u53d8\u4f53\u548c\u4e0e\u5e74\u9f84\u76f8\u5173\u7684\u7279\u5f81\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cCoLMbo\u5728\u4f20\u7edf\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\u63cf\u8ff0\u4e4b\u5916\u6709\u6240\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u96f6\u6837\u672c\u573a\u666f\u4e2d\u8de8\u591a\u79cd\u6570\u636e\u96c6\u8868\u73b0\u4f18\u5f02\uff0c\u6807\u5fd7\u7740\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u9886\u57df\u6709\u4e86\u91cd\u8981\u7684\u8fdb\u5c55\u3002", "conclusion": "CoLMbo\u6a21\u578b\u901a\u8fc7\u5176\u72ec\u7279\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u8c03\u8282\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u5730\u751f\u6210\u8be6\u7ec6\u7684\u8bf4\u8bdd\u4eba\u63cf\u8ff0\uff0c\u5305\u62ec\u8bf4\u8bdd\u4eba\u7684\u5730\u533a\u65b9\u8a00\u548c\u5e74\u9f84\u7279\u5f81\u7b49\u3002\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u8bf4\u8bdd\u4eba\u8bc6\u522b\u6280\u672f\u9886\u57df\u7684\u4e00\u5927\u8fdb\u6b65\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.09278", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09278", "abs": "https://arxiv.org/abs/2506.09278", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "comment": "Project Page: https://uniflowmatch.github.io/", "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "AI": {"tldr": "The paper introduces the Unified Flow & Matching (UFM) model, which uses a transformer architecture to provide a unified solution for dense correspondence tasks, outperforming specialized methods in both accuracy and speed.", "motivation": "To address the dense correspondence problem, which is crucial for various applications like visual odometry and 3D reconstruction, by providing a unified approach that works for both wide-baseline scenarios and optical flow estimation.", "method": "Unified Flow & Matching model (UFM) using a generic transformer architecture to directly regress (u,v) flow.", "result": "UFM outperforms state-of-the-art flow methods by 28% in accuracy, has 62% less error compared to dense wide-baseline matchers, and is 6.7x faster.", "conclusion": "The paper demonstrates that unified training of dense correspondence can surpass specialized methods, offering potential for fast, general-purpose correspondence tasks and opening new research avenues for multi-modal applications."}}
{"id": "2506.09381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09381", "abs": "https://arxiv.org/abs/2506.09381", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\u53ef\u4ee5\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6709\u6548\u5730\u533a\u5206\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u7684\u8d28\u91cf\u3002Bagging\u5206\u7c7b\u5668\u548cDistilBERT\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6027\u80fd\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u6709\u6240\u4e0d\u540c\u3002", "motivation": "\u968f\u7740\u5728\u7ebf\u65b0\u95fb\u7684\u589e\u591a\uff0c\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u7684\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u5927\u91cf\u6d8c\u73b0\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u662f\u5426\u53ef\u4ee5\u81ea\u52a8\u533a\u5206\u8d28\u91cf\u8f83\u4f4e\u7684\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u4e0e\u8d28\u91cf\u8f83\u9ad8\u7684\u6807\u9898/\u94fe\u63a5\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8bc4\u4f3012\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528\u5305\u542b2018-2024\u5e74\u95f4\u6765\u81ea\u5168\u7403\u65b0\u95fb\u7f51\u7ad9\u768457,544,214\u6761\u65b0\u95fb\u94fe\u63a5/\u6807\u9898\u7684\u5e73\u8861\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u63d0\u53d6\u4e86115\u4e2a\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f20\u7edf\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u7279\u522b\u662fBagging\u5206\u7c7b\u5668\u8868\u73b0\u7a81\u51fa\uff08\u51c6\u786e\u738788.1%\uff0cF1\u503c88.3%\uff0c\u57fa\u4e8e80/20\u7684\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\uff09\u3002\u5fae\u8c03\u540e\u7684DistilBERT\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387\uff0890.3%\uff0c\u57fa\u4e8e80/20\u7684\u8bad\u7ec3/\u6d4b\u8bd5\u5206\u5272\uff09\uff0c\u4f46\u9700\u8981\u66f4\u957f\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u4f20\u7edf\u7684\u5206\u7c7b\u5668\u4e0eNLP\u7279\u5f81\uff0c\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u90fd\u80fd\u591f\u6709\u6548\u5730\u5bf9\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u7684\u8d28\u91cf\u8fdb\u884c\u533a\u5206\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2506.09299", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09299", "abs": "https://arxiv.org/abs/2506.09299", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "comment": "6 Pages, 3 figures", "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "AI": {"tldr": "This paper develops a small, power-efficient object detection model for aerial imagery during emergencies using a custom dataset and YOLOv4-Tiny quantized to INT8.", "motivation": "The motivation behind this paper is to develop a lightweight and energy-efficient approach to object detection for aerial imagery in emergency response scenarios. The authors address the need for a custom dataset as publicly available datasets lack the necessary drone-view emergency imagery.", "method": "The paper utilizes the YOLOv4-Tiny model, which has been optimized via post-training quantization to INT8 precision. The model is trained using a custom dataset made up of 10,820 annotated images, tailored for emergency response aerial imagery.", "result": "The quantized YOLOv4-Tiny model demonstrates comparable detection performance to YOLOv5-small, with significant improvements in model size (reduced from 22.5MB to 6.4MB) and inference speed (improved by 44%).", "conclusion": "The study concludes that the quantized YOLOv4-Tiny model is well-suited for real-time emergency detection on low-power edge devices, due to its optimized model size and fast inference time."}}
{"id": "2506.09391", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09391", "abs": "https://arxiv.org/abs/2506.09391", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "title": "Comparing human and LLM politeness strategies in free production", "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u793c\u8c8c\u4ea4\u6d41\u7b56\u7565\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u79ef\u6781\u8bed\u5883\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u6d88\u6781\u793c\u8c8c\u7b56\u7565\u53ef\u80fd\u4f1a\u5bfc\u81f4\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u793c\u8c8c\u7528\u8bed\u65b9\u9762\u7684\u95ee\u9898\uff0c\u770b\u5b83\u4eec\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e00\u6837\u7075\u6d3b\u5730\u8c03\u6574\u8bed\u8a00\u7b56\u7565\u6765\u6ee1\u8db3\u4fe1\u606f\u548c\u793e\u4ea4\u7684\u76ee\u6807\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u4eba\u7c7b\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53d7\u9650\u548c\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u54cd\u5e94\uff0c\u7814\u7a76LLMs\u662f\u5426\u4e5f\u91c7\u7528\u4e86\u540c\u6837\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u7b56\u7565\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8f83\u5927\u7684\u6a21\u578b\uff08\u81f3\u5c11700\u4ebf\u53c2\u6570\uff09\u80fd\u591f\u6210\u529f\u590d\u5236\u8ba1\u7b97\u8bed\u7528\u5b66\u6587\u732e\u4e2d\u7684\u5173\u952e\u504f\u597d\uff0c\u5e76\u4e14\u4eba\u7c7b\u8bc4\u4f30\u8005\u610f\u5916\u5730\u66f4\u559c\u6b22\u5728\u5f00\u653e\u6027\u73af\u5883\u4e2d\u7531LLMs\u751f\u6210\u7684\u56de\u590d\u3002\u4f46\u662f\uff0c\u8fdb\u4e00\u6b65\u7684\u8bed\u8a00\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u5728\u79ef\u6781\u7684\u8bed\u5883\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u6d88\u6781\u793c\u8c8c\u7b56\u7565\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u3002", "conclusion": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f7f\u7528\u793c\u8c8c\u7b56\u7565\u65b9\u9762\u5c55\u73b0\u51fa\u4e86\u60ca\u4eba\u7684\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u7ec6\u5fae\u5dee\u5f02\u5bf9AI\u7cfb\u7edf\u7684\u8bed\u7528\u5bf9\u9f50\u63d0\u51fa\u4e86\u91cd\u8981\u95ee\u9898\u3002"}}
{"id": "2506.09300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09300", "abs": "https://arxiv.org/abs/2506.09300", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5", "comment": null, "summary": "This paper presents the deployment and performance evaluation of a quantized\nYOLOv4-Tiny model for real-time object detection in aerial emergency imagery on\na resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model\nwas quantized to INT8 precision using TensorFlow Lite post-training\nquantization techniques and evaluated for detection speed, power consumption,\nand thermal feasibility under embedded deployment conditions. The quantized\nmodel achieved an inference time of 28.2 ms per image with an average power\nconsumption of 13.85 W, demonstrating a significant reduction in power usage\ncompared to its FP32 counterpart. Detection accuracy remained robust across key\nemergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These\nresults highlight the potential of low-power embedded AI systems for real-time\ndeployment in safety-critical emergency response applications.", "AI": {"tldr": "The paper shows that a quantized YOLOv4-Tiny model can effectively perform real-time object detection with reduced power consumption on a Raspberry Pi 5, suitable for emergency response applications.", "motivation": "To enhance real-time object detection capabilities for safety-critical emergency response applications, by reducing power consumption and improving thermal feasibility on a resource-constrained edge device.", "method": "This paper quantizes the YOLOv4-Tiny model to INT8 precision using TensorFlow Lite for deployment on a Raspberry Pi 5, targeting real-time object detection in aerial emergency imagery.", "result": "The quantized model achieved an inference time of 28.2 ms per image, with an average power consumption of 13.85 W, showing significant power savings compared to FP32. Detection accuracy is robust for emergency classes.", "conclusion": "The study demonstrates the suitability of low-power embedded AI systems for real-time emergency response tasks on resource-constrained devices like the Raspberry Pi 5."}}
{"id": "2506.09393", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09393", "abs": "https://arxiv.org/abs/2506.09393", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86KT$^2$\uff0c\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u6811\u7684\u5c42\u6b21\u5316\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff0c\u7528\u4e8e\u77e5\u8bc6\u8ffd\u8e2a\uff0c\u5b83\u5728\u4f4e\u8d44\u6e90\u7684\u8bfe\u5802\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u8bb8\u591a\u5b9e\u9645\u7684\u8bfe\u5802\u73af\u5883\u5bf9\u4e8e\u77e5\u8bc6\u8ffd\u8e2a\u6765\u8bf4\u5f80\u5f80\u662f\u4f4e\u8d44\u6e90\u7684\u6570\u636e\u73af\u5883\uff0c\u5e76\u4e14\u9700\u8981\u5728\u7ebf\u66f4\u65b0\u4ee5\u9002\u5e94\u5b66\u751f\u4e0d\u65ad\u589e\u957f\u7684\u7ec3\u4e60\u5386\u53f2\uff0c\u8fd9\u4e3a\u73b0\u6709\u7684\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\u5e26\u6765\u4e86\u5f88\u5927\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5728\u8fd9\u79cd\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u6062\u590d\u826f\u597d\u7684\u6027\u80fd\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u5c42\u6b21\u5316\u7684\u77e5\u8bc6\u6982\u5ff5\uff08KC\uff09\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u5728\u8bb8\u591a\u8bfe\u5802\u73af\u5883\u4e2d\u901a\u5e38\u53ef\u7528\uff0c\u5e76\u4e14\u5728\u6570\u636e\u7a00\u758f\u65f6\u53ef\u4ee5\u63d0\u4f9b\u5f3a\u5148\u9a8c\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u6811\u7684\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\uff08KT$^2$\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6982\u7387\u6027\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6811\u6a21\u578b\u6765\u5bf9\u5b66\u751f\u7406\u89e3\u8fc7\u7684\u4e00\u7cfb\u5217\u5206\u7ea7\u77e5\u8bc6\u6982\u5ff5\u8fdb\u884c\u5efa\u6a21\u3002KT$^2$\u901a\u8fc7EM\u7b97\u6cd5\u4f30\u8ba1\u5b66\u751f\u638c\u63e1\u7a0b\u5ea6\uff0c\u5e76\u901a\u8fc7\u589e\u91cf\u66f4\u65b0\u673a\u5236\u652f\u6301\u4e2a\u6027\u5316\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKT$^2$\u5728\u5b9e\u9645\u7684\u5728\u7ebf\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u672a\u5728\u6458\u8981\u4e2d\u76f4\u63a5\u4f53\u73b0\uff0c\u4f46\u53ef\u4ee5\u63a8\u65ad\uff0cKT$^2$\u6846\u67b6\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2506.09327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09327", "abs": "https://arxiv.org/abs/2506.09327", "authors": ["Tong Wang", "Guanzhou Chen", "Xiaodong Zhang", "Chenxi Liu", "Jiaqi Wang", "Xiaoliang Tan", "Wenchao Guo", "Qingyuan Yang", "Kaiqi Zhang"], "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning", "comment": null, "summary": "Remote sensing image interpretation plays a critical role in environmental\nmonitoring, urban planning, and disaster assessment. However, acquiring\nhigh-quality labeled data is often costly and time-consuming. To address this\nchallenge, we proposes a multi-modal self-supervised learning framework that\nleverages high-resolution RGB images, multi-spectral data, and digital surface\nmodels (DSM) for pre-training. By designing an information-aware adaptive\nmasking strategy, cross-modal masking mechanism, and multi-task self-supervised\nobjectives, the framework effectively captures both the correlations across\ndifferent modalities and the unique feature structures within each modality. We\nevaluated the proposed method on multiple downstream tasks, covering typical\nremote sensing applications such as scene classification, semantic\nsegmentation, change detection, object detection, and depth estimation.\nExperiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.\nThe results demonstrate that the proposed method outperforms existing\npretraining approaches in most tasks. Specifically, on the Potsdam and\nVaihingen semantic segmentation tasks, our method achieved mIoU scores of\n78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation\ntask, the RMSE error is reduced to 0.182, and for the binary change detection\ntask in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing\nthe second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and\nHR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9ad8\u5206\u8fa8\u7387RGB\u56fe\u50cf\u3001\u591a\u5149\u8c31\u6570\u636e\u548cDSM\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u9065\u611f\u56fe\u50cf\u89e3\u8bfb\u6027\u80fd\uff0c\u7279\u522b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u5728\u73af\u5883\u76d1\u6d4b\u3001\u57ce\u5e02\u89c4\u5212\u548c\u707e\u5bb3\u8bc4\u4f30\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u83b7\u5f97\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\u5f80\u5f80\u662f\u4ee3\u4ef7\u9ad8\u6602\u4e14\u8017\u65f6\u7684\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e0a\u8ff0\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u5206\u8fa8\u7387RGB\u56fe\u50cf\u3001\u591a\u5149\u8c31\u6570\u636e\u548c\u6570\u5b57\u8868\u9762\u6a21\u578b\uff08DSM\uff09\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u901a\u8fc7\u8bbe\u8ba1\u4fe1\u606f\u611f\u77e5\u81ea\u9002\u5e94\u63a9\u853d\u7b56\u7565\u3001\u8de8\u6a21\u6001\u63a9\u853d\u673a\u5236\u548c\u591a\u4efb\u52a1\u81ea\u76d1\u7763\u76ee\u6807\uff0c\u8be5\u6846\u67b6\u6709\u6548\u6355\u6349\u4e86\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u5173\u8054\u4ee5\u53ca\u6bcf\u4e2a\u6a21\u6001\u5185\u7684\u72ec\u7279\u7279\u5f81\u7ed3\u6784\u3002", "result": "\u572815\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6db5\u76d6\u4e8626\u4e2a\u4efb\u52a1\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728Potsdam\u548cVaihingen\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e8678.30%\u548c76.50%\u7684mIoU\uff0c\u53ea\u4f7f\u752850%\u7684\u8bad\u7ec3\u96c6\u3002\u5bf9\u4e8eUS3D\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\uff0cRMSE\u8bef\u5dee\u51cf\u5c11\u52300.182\uff0c\u5bf9\u4e8eSECOND\u6570\u636e\u96c6\u4e2d\u7684\u4e8c\u5143\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e8647.51%\u7684mIoU\uff0c\u8d85\u8fc7\u4e86\u7b2c\u4e8c\u540dCS-MAE 3\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u5145\u5206\u5229\u7528\u9ad8\u5206\u8fa8\u7387RGB\u56fe\u50cf\u3001\u591a\u5149\u8c31\u6570\u636e\u548cDSM\u6570\u636e\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u7684\u6027\u80fd\u3002\u8fd9\u4e00\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u9700\u8981\u9ad8\u6210\u672c\u6570\u636e\u6807\u6ce8\u7684\u9065\u611f\u56fe\u50cf\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.09408", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09408", "abs": "https://arxiv.org/abs/2506.09408", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u63a8\u7406\u65f6\u7b97\u6cd5\uff0cToken Constraint Decoding (TCD)\uff0c\u5b83\u80fd\u591f\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6709\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5f53\u4e0eprompt engineering\u4fee\u590d\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u9879\u9009\u62e9\u95ee\u9898\u56de\u7b54\uff08MCQA\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u4ecd\u5bb9\u6613\u53d7\u5230\u8f93\u5165\u5fae\u5c0f\u6270\u52a8\u7684\u5f71\u54cd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToken Constraint Decoding (TCD) \u7684\u7b80\u5355\u4f46\u5728\u63a8\u7406\u65f6\u975e\u5e38\u6709\u6548\u7684\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u786e\u4fddtoken\u7ea7\u522b\u9884\u6d4b\u4e4b\u95f4\u7684\u5bf9\u9f50\u6765\u63d0\u9ad8\u5728\u6709\u566a\u58f0\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u5728CommonsenseQA\u3001MMLU\u548cMMLU-Pro\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6211\u4eec\u5c55\u793a\u4e86TCD\uff0c\u7279\u522b\u662f\u5f53\u4e0eprompt engineering\uff08PE\uff09\u4fee\u590d\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86\u5f31\u5316\u6a21\u578b\u5982Gemma3 1B\u5728\u8f93\u5165\u566a\u58f0\u60c5\u51b5\u4e0b\u6027\u80fd\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe39%\u3002", "conclusion": "\u6211\u4eec\u7684\u53d1\u73b0\u786e\u5b9a\u4e86TCD\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u6a21\u578b\u4e0d\u53ef\u77e5\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u6539\u8fdb\u73b0\u5b9e\u4e16\u754c\u4e0d\u5b8c\u7f8e\u60c5\u51b5\u4e0b\u7684\u63a8\u7406\u7a33\u5b9a\u6027\uff0c\u5e76\u4e3a\u66f4\u53ef\u9760\u7684LLMs\u5728\u5b89\u5168\u5173\u952e\u6216\u9762\u5411\u7528\u6237\u7684\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.09343", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09343", "abs": "https://arxiv.org/abs/2506.09343", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "comment": "CVPR 2025 Highlight", "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u518c\u7684\u5bb6\u7535\u64cd\u4f5c\u57fa\u51c6CheckManual\u548c\u4e00\u4e2a\u76f8\u5e94\u6a21\u578bManualPlan\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u5bb6\u7535\u64cd\u4f5c\u80fd\u529b\u3002\u4ee5\u524d\u7684\u7814\u7a76\u8981\u4e48\u5c40\u9650\u4e8e\u95ee\u7b54\u4efb\u52a1\uff0c\u8981\u4e48\u5ffd\u89c6\u4e86\u624b\u518c\u7684\u91cd\u8981\u6027\uff0c\u800c\u672c\u6587\u5219\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "motivation": "\u4e4b\u524d\u7684\u4e0e\u624b\u518c\u76f8\u5173\u7684\u5de5\u4f5c\u5c40\u9650\u4e8e\u95ee\u7b54\u4efb\u52a1\uff0c\u800c\u73b0\u6709\u7684\u64cd\u4f5c\u7814\u7a76\u5219\u5ffd\u89c6\u4e86\u624b\u518c\u7684\u91cd\u8981\u89d2\u8272\uff0c\u5e76\u4e14\u65e0\u6cd5\u7406\u89e3\u591a\u9875\u624b\u518c\u7684\u5185\u5bb9\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u521b\u5efa\u57fa\u4e8e\u624b\u518c\u7684\u64cd\u4f5c\u57fa\u51c6\u548c\u6a21\u578b\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u7406\u89e3\u5e76\u4f7f\u7528\u5bb6\u7535\u7684\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u5bb6\u7528\u7535\u5668\u64cd\u4f5c\u57fa\u51c6CheckManual\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5927\u578b\u6a21\u578b\u8f85\u52a9\u7684\u4eba\u5de5\u4fee\u8ba2\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u6839\u636eCAD\u5bb6\u7535\u6a21\u578b\u521b\u5efa\u624b\u518c\u3002\u901a\u8fc7\u8fd9\u4e9b\u624b\u518c\uff0c\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8e\u624b\u518c\u7684\u64cd\u4f5c\u6311\u6218\u3001\u8bc4\u4f30\u6307\u6807\u548c\u6a21\u62df\u73af\u5883\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u64cd\u4f5c\u89c4\u5212\u6a21\u578bManualPlan\uff0c\u4e3aCheckManual\u57fa\u51c6\u5efa\u7acb\u4e86\u4e00\u7ec4\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u867d\u7136\u6587\u4e2d\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u672a\u5b8c\u5168\u9610\u8ff0\uff0c\u4f46\u901a\u8fc7\u5efa\u7acbCheckManual\u57fa\u51c6\u548cManualPlan\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u7684\u673a\u5668\u4eba\u7406\u89e3\u548c\u64cd\u4f5c\u5bb6\u7528\u7535\u5668\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u9996\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u5bb6\u7535\u64cd\u4f5c\u57fa\u51c6CheckManual\u4ee5\u53ca\u9996\u4e2a\u624b\u518c\u4e3a\u57fa\u7840\u7684\u64cd\u4f5c\u89c4\u5212\u6a21\u578bManualPlan\uff0c\u8fd9\u4e3a\u6211\u4eec\u66f4\u597d\u5730\u7406\u89e3\u548c\u64cd\u4f5c\u5bb6\u7528\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2506.09414", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.09414", "abs": "https://arxiv.org/abs/2506.09414", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "AI": {"tldr": "PGDA-KGQA, a prompt-guided generative framework, enhances Knowledge Graph Question Answering by introducing diverse data augmentation strategies that include generating single-hop pseudo questions, semantic-preserving question rewriting, and creating multi-hop questions through answer-guided reverse path exploration.", "motivation": "The motivation behind PGDA-KGQA is to tackle the difficulty in obtaining a large amount of diverse annotated data and the scarcity of multi-hop reasoning samples, which are critical challenges for effective model training in KGQA tasks.", "method": "PGDA-KGQA adopts a unified prompt-design paradigm integrating various data augmentation strategies to enhance training data. It includes generating single-hop questions for better semantic alignment, conducting semantic-preserving question rewriting for robustness, and creating multi-hop questions through answer-guided reverse path exploration.", "result": "Experimental results show that PGDA-KGQA surpasses existing state-of-the-art methods on KGQA datasets like WebQSP and ComplexWebQuestions, leading to improved accuracies in F1, Hits@1, and Accuracy metrics.", "conclusion": "By employing a combination of data augmentation techniques centered around prompt-guided generative strategies, PGDA-KGQA demonstrates its effectiveness in enhancing training data diversity and improving logical form generation and answer retrieval performance for KGQA."}}
{"id": "2506.09345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09345", "abs": "https://arxiv.org/abs/2506.09345", "authors": ["Songping Wang", "Xiantao Hu", "Yueming Lyu", "Caifeng Shan"], "title": "An Effective End-to-End Solution for Multimodal Action Recognition", "comment": null, "summary": "Recently, multimodal tasks have strongly advanced the field of action\nrecognition with their rich multimodal information. However, due to the\nscarcity of tri-modal data, research on tri-modal action recognition tasks\nfaces many challenges. To this end, we have proposed a comprehensive multimodal\naction recognition solution that effectively utilizes multimodal information.\nFirst, the existing data are transformed and expanded by optimizing data\nenhancement techniques to enlarge the training scale. At the same time, more\nRGB datasets are used to pre-train the backbone network, which is better\nadapted to the new task by means of transfer learning. Secondly, multimodal\nspatial features are extracted with the help of 2D CNNs and combined with the\nTemporal Shift Module (TSM) to achieve multimodal spatial-temporal feature\nextraction comparable to 3D CNNs and improve the computational efficiency. In\naddition, common prediction enhancement methods, such as Stochastic Weight\nAveraging (SWA), Ensemble and Test-Time augmentation (TTA), are used to\nintegrate the knowledge of models from different training periods of the same\narchitecture and different architectures, so as to predict the actions from\ndifferent perspectives and fully exploit the target information. Ultimately, we\nachieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the\ncompetition leaderboard, demonstrating the superiority of our solution.", "AI": {"tldr": "A comprehensive solution for multimodal action recognition utilizes data enhancement, transfer learning, and multimodal feature extraction, achieving high accuracy on a competition leaderboard.", "motivation": "Due to the scarcity of tri-modal data, the paper aims to propose a comprehensive multimodal action recognition solution to effectively utilize multimodal information.", "method": "First, the data is enhanced to enlarge the training scale, and RGB datasets are used to pre-train the backbone network for transfer learning. Secondly, multimodal spatial-temporal features are extracted using 2D CNNs combined with the TSM. Additionally, SWA, Ensemble, and TTA methods are used to enhance predictions.", "result": "The solution achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the competition leaderboard.", "conclusion": "The proposed solution demonstrates superior performance in multimodal action recognition by effectively utilizing data enhancement techniques, transfer learning, and multimodal feature extraction, leading to high accuracy results."}}
{"id": "2506.09424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09424", "abs": "https://arxiv.org/abs/2506.09424", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5927\u8de8\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u81ea\u52a8\u68c0\u6d4b\u6b3a\u9a97\u7684\u80fd\u529b\u3002\u7814\u7a76\u8868\u660e\uff0c\u5fae\u8c03\u7684LLMs\u5728\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u800cLMMs\u5728\u5229\u7528\u8de8\u6a21\u6001\u7ebf\u7d22\u65b9\u9762\u4ecd\u5b58\u5728\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u4e16\u754c\u7684\u65e5\u76ca\u53d1\u5c55\uff0c\u68c0\u6d4b\u6b3a\u9a97\u6210\u4e3a\u4e86\u4e00\u4e2a\u5173\u952e\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u7814\u7a76\u7684\u76ee\u7684\u662f\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u5728\u68c0\u6d4b\u6b3a\u9a97\u65b9\u9762\u7684\u80fd\u529b\u548c\u9650\u5236\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5f00\u6e90\u548c\u5546\u4e1aLLMs\u5728\u771f\u5b9e\u751f\u6d3b\u5ead\u5ba1\u9762\u8bd5\u3001\u53d7\u6307\u5bfc\u7684\u793e\u4ea4\u6b3a\u9a97\u573a\u666f\u548c\u6b3a\u9a97\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u7684\u6548\u80fd\u3002\u7814\u7a76\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u4e0d\u540c\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u7684\u6b3a\u9a97\u68c0\u6d4b\u6548\u679c\uff0c\u5305\u62ec\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u65b9\u6cd5\u4ee5\u53ca\u968f\u673a\u6216\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u4e0a\u4e0b\u6587\u5b9e\u4f8b\u9009\u62e9\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5fae\u8c03\u7684LLMs\u5728\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cLMMs\u5728\u5229\u7528\u8de8\u6a21\u6001\u7ebf\u7d22\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLMs\u5728\u5904\u7406\u548c\u89e3\u91ca\u8de8\u6a21\u6001\u6b3a\u9a97\u7ebf\u7d22\u65b9\u9762\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u4e5f\u6307\u51fa\u5b83\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2506.09350", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09350", "abs": "https://arxiv.org/abs/2506.09350", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "comment": null, "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAAPT\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8f6c\u53d8\u4e3a\u5b9e\u65f6\u3001\u4ea4\u4e92\u5f0f\u7684\u89c6\u9891\u751f\u6210\u5668\uff0c\u5e76\u901a\u8fc7\u81ea\u56de\u5f52\u5bf9\u6297\u6027\u8bad\u7ec3\u6709\u6548\u51cf\u5c11\u4e86\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\uff0c\u5927\u5e45\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u5bc6\u96c6\uff0c\u963b\u788d\u4e86\u5176\u5728\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u5e94\u7528\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u5b9e\u65f6\u6027\u548c\u4ea4\u4e92\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u81ea\u56de\u5f52\u5bf9\u6297\u540e\u8bad\u7ec3\uff08AAPT\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u9884\u5148\u8bad\u7ec3\u597d\u7684\u6f5c\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u6362\u4e3a\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u5668\u3002\u8be5\u6a21\u578b\u80fd\u591f\u9010\u5e27\u81ea\u56de\u5f52\u5730\u751f\u6210\u6f5c\u5e27\uff0c\u5e76\u4f7f\u7528\u5355\u4e00\u795e\u7ecf\u51fd\u6570\u8bc4\u4f30\uff081NFE\uff09\uff0c\u5b9e\u65f6\u5411\u7528\u6237\u4f20\u8f93\u7ed3\u679c\u5e76\u63a5\u6536\u4ea4\u4e92\u53cd\u9988\u4ee5\u751f\u6210\u4e0b\u4e00\u5e27\u3002\u672c\u6587\u7684\u65b9\u6cd5\u63a2\u7d22\u4e86\u5bf9\u6297\u6027\u8bad\u7ec3\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u81ea\u56de\u5f52\u751f\u6210\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u672c\u6587\u768480\u4ebf\u53c2\u6570\u6a21\u578b\u80fd\u591f\u5728\u5355\u4e2aH100\u4e0a\u4ee5736x416\u5206\u8fa8\u7387\u5b9e\u65f6\u5b9e\u73b024fps\u89c6\u9891\u751f\u6210\uff0c\u6216\u57288xH100\u4e0a\u8fbe\u52301280x720\u5206\u8fa8\u7387\uff0c\u751f\u6210\u957f\u8fbe1\u5206\u949f\uff081440\u5e27\uff09\u7684\u89c6\u9891\u3002", "conclusion": "\u672c\u6587\u7684\u6a21\u578b\u901a\u8fc7\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u5355\u6b65\u751f\u6210\u67b6\u6784\u548c\u5229\u7528KV\u7f13\u5b58\uff0c\u4ee5\u53ca\u91c7\u7528\u5b66\u751f\u5f3a\u8feb\u6837\u5f0f\u7684\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u6210\u529f\u5730\u51cf\u5c11\u4e86\u957f\u671f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2506.09428", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09428", "abs": "https://arxiv.org/abs/2506.09428", "authors": ["Fei Ding", "Baiqiao Wang"], "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u66f4\u6210\u672c\u6548\u76ca\u9ad8\u7684SFT\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63d0\u9ad8\u7279\u5b9a\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u7531\u4e8eSFT\u867d\u7136\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u548c\u7279\u5b9a\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u4f46\u5f80\u5f80\u4e5f\u4f1a\u524a\u5f31\u5b83\u4eec\u7684\u901a\u7528\u80fd\u529b\uff0c\u540c\u65f6\u7531\u4e8e\u539f\u59cb\u9884\u8bad\u7ec3\u6570\u636e\u7684\u4e0d\u53ef\u83b7\u5f97\u6027\uff0c\u5f53\u7b2c\u4e09\u65b9\u5728\u5f00\u6e90\u6a21\u578b\u4e0a\u8fdb\u884cSFT\u65f6\u707e\u96be\u6027\u9057\u5fd8\u5f80\u5f80\u4f1a\u88ab\u52a0\u5267\uff0c\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u8fd9\u4e00\u65b9\u6cd5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u6210\u672c\u6548\u76ca\u9ad8\u7684SFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u5efa\u57fa\u7840\u6a21\u578b\u7684\u53ef\u80fdSFT\u6307\u4ee4\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u591a\u6a21\u578b\u7b5b\u9009\u8fc7\u7a0b\u9009\u62e9\u6700\u4f73\u6570\u636e\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6570\u636e\u4e0e\u65b0\u6570\u636e\u6df7\u5408\u8fdb\u884cSFT\uff0c\u4ee5\u6709\u6548\u964d\u4f4e\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u901a\u7528\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u7279\u5b9a\u4efb\u52a1\u7684\u8868\u73b0\u3002", "conclusion": "\u6b64\u65b9\u6cd5\u80fd\u591f\u5728\u65e0\u9700\u8bbf\u95ee\u539f\u59cbSFT\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\uff0c\u5e76\u5728\u63d0\u9ad8\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\u6c34\u51c6\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.09357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09357", "abs": "https://arxiv.org/abs/2506.09357", "authors": ["Junchao Zhou"], "title": "A new approach for image segmentation based on diffeomorphic registration and gradient fields", "comment": null, "summary": "Image segmentation is a fundamental task in computer vision aimed at\ndelineating object boundaries within images. Traditional approaches, such as\nedge detection and variational methods, have been widely explored, while recent\nadvances in deep learning have shown promising results but often require\nextensive training data. In this work, we propose a novel variational framework\nfor 2D image segmentation that integrates concepts from shape analysis and\ndiffeomorphic transformations. Our method models segmentation as the\ndeformation of a template curve via a diffeomorphic transformation of the image\ndomain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. The curve evolution is guided by a loss function that compares the\ndeformed curve to the image gradient field, formulated through the varifold\nrepresentation of geometric shapes. The approach is implemented in Python with\nGPU acceleration using the PyKeops library. This framework allows for accurate\nsegmentation with a flexible and theoretically grounded methodology that does\nnot rely on large datasets.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f62\u72b6\u5206\u6790\u548c\u5fae\u5206\u540c\u80da\u53d8\u6362\u7684\u4e8c\u7ef4\u56fe\u50cf\u5206\u5272\u65b0\u6846\u67b6\uff0c\u5176\u4f7f\u7528LDDMM\u6846\u67b6\u548cvarifold\u8868\u793a\u5bf9\u56fe\u50cf\u8fdb\u884c\u5206\u5272\uff0c\u4e0d\u9700\u8981\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u5206\u5272\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u5272\u65b9\u6cd5\u5982\u8fb9\u7f18\u68c0\u6d4b\u548c\u53d8\u5206\u65b9\u6cd5\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fd1\u671f\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u6548\u679c\uff0c\u4f46\u5f80\u5f80\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u6b64\u65b9\u6cd5\u4ee5\u89e3\u51b3\u6570\u636e\u9700\u6c42\u95ee\u9898\u5e76\u63d0\u4f9b\u4e00\u4e2a\u7406\u8bba\u4e0a\u5404\u5177\u57fa\u7840\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e8c\u7ef4\u56fe\u50cf\u5206\u5272\u53d8\u5206\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5f62\u72b6\u5206\u6790\u548c\u5fae\u5206\u540c\u80da\u53d8\u6362\u7684\u6982\u5ff5\u3002\u8be5\u65b9\u6cd5\u5c06\u5206\u5272\u5efa\u6a21\u4e3a\u901a\u8fc7\u5bf9\u56fe\u50cf\u57df\u8fdb\u884c\u5fae\u5206\u540c\u80da\u53d8\u6362\u800c\u4f7f\u6a21\u677f\u66f2\u7ebf\u53d8\u5f62\u7684\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5927\u53d8\u5f62\u5fae\u5206\u540c\u80da\u5ea6\u91cf\u6620\u5c04\uff08LDDMM\uff09\u6846\u67b6\u3002\u5229\u7528varifold\u8868\u793a\u51e0\u4f55\u5f62\u72b6\uff0c\u901a\u8fc7\u5c06\u53d8\u5f62\u66f2\u7ebf\u4e0e\u56fe\u50cf\u68af\u5ea6\u573a\u8fdb\u884c\u6bd4\u8f83\u6765\u5f15\u5bfc\u66f2\u7ebf\u6f14\u5316\u3002", "result": "\u901a\u8fc7Python\u5b9e\u73b0GPU\u52a0\u901f\u540e\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u5206\u5272\u65b9\u9762\u6709\u7740\u7075\u6d3b\u4e14\u7406\u8bba\u4e0a\u575a\u5b9e\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u51c6\u786e\u5206\u5272\u7684\u80fd\u529b\u3002"}}
{"id": "2506.09440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09440", "abs": "https://arxiv.org/abs/2506.09440", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GigaChat\u7cfb\u5217\u4fc4\u7f57\u65afLLM\u6a21\u578b\u53ca\u5176\u9884\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u8be6\u7ec6\u62a5\u544a\u4e86\u6a21\u578b\u67b6\u6784\u548c\u8bbe\u8ba1\u9009\u62e9\uff0c\u63d0\u4f9b\u4e86\u6a21\u578b\u5728\u4fc4\u8bed\u548c\u82f1\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7API\uff0cTelegram\u673a\u5668\u4eba\u548cWeb\u754c\u9762\u8bbf\u95ee\u7684\u9876\u7ea7\u6a21\u578b\u7cfb\u7edf\u6f14\u793a\u3002\u5f00\u6e90\u53d1\u5e03\u4e86\u4e09\u4e2aGigaChat\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u5f00\u53d1\u9488\u5bf9\u4fc4\u8bed\u7684\u57fa\u7840\u6a21\u578b\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u5f88\u5927\uff0c\u4fc4\u8bed\u7279\u5b9a\u7684\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u53d7\u5230\u9650\u5236\u3002\u672c\u6587\u7684\u52a8\u673a\u662f\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e00\u5957\u4e13\u95e8\u9488\u5bf9\u4fc4\u8bed\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86GigaChat\u7cfb\u5217\u4fc4\u7f57\u65afLLM\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4f9b\u4e0d\u540c\u9700\u6c42\u4f7f\u7528\uff0c\u5305\u62ec\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u8c03\u6574\u7248\u672c\u3002\u8be6\u7ec6\u62a5\u544a\u4e86\u6a21\u578b\u67b6\u6784\uff0c\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4ee5\u53ca\u5b9e\u9a8c\u8bbe\u8ba1\u9009\u62e9\u3002", "result": "\u901a\u8fc7\u5bf9\u4fc4\u7f57\u65af\u548c\u82f1\u8bed\u57fa\u51c6\u7684\u6027\u80fd\u8bc4\u4f30\uff0cGigaChat\u4e0e\u591a\u8bed\u8a00\u7c7b\u4f3c\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u8bc1\u660e\u5728API\uff0cTelegram\u673a\u5668\u4eba\u548cWeb\u754c\u9762\u7cfb\u7edf\u6f14\u793a\u4e2d\u7684\u9876\u7ea7\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63a8\u51fa\u4e86\u4e00\u7cfb\u5217\u9488\u5bf9\u4fc4\u7f57\u65af\u8bed\u8a00\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578bGigaChat\uff0c\u5f00\u653e\u4e86\u4e09\u4e2a\u6a21\u578b\u7684\u6e90\u4ee3\u7801\uff0c\u63a8\u52a8\u4e86NLP\u7814\u7a76\u673a\u4f1a\u548c\u57fa\u4e8e\u4fc4\u8bed\u7684\u5de5\u4e1a\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u3002"}}
{"id": "2506.09363", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.09363", "abs": "https://arxiv.org/abs/2506.09363", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "comment": "Under review", "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "AI": {"tldr": "SAGE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\u7684\u4e0d\u5b89\u5168\u5185\u5bb9\u548c\u7248\u6743\u4fb5\u72af\u95ee\u9898\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u4e0d\u53ef\u907f\u514d\u5730\u5305\u542b\u654f\u611f\u4fe1\u606f\u6240\u5e26\u6765\u7684\u4e00\u7cfb\u5217\u5b89\u5168\u98ce\u9669\uff0c\u6bd4\u5982\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\u548c\u4fb5\u72af\u7248\u6743\u7684\u98ce\u9669\u3002", "method": "SAGE\u65b9\u6cd5\u5305\u62ec\u8bed\u4e49\u589e\u5f3a\u64e6\u9664\u548c\u5168\u5c40-\u5c40\u90e8\u534f\u540c\u4fdd\u7559\u673a\u5236\u4e24\u79cd\u673a\u5236\uff0c\u524d\u8005\u80fd\u591f\u5c06\u6982\u5ff5\u8bcd\u64e6\u9664\u8f6c\u6362\u4e3a\u6982\u5ff5\u9886\u57df\u64e6\u9664\uff0c\u901a\u8fc7\u5faa\u73af\u81ea\u6211\u68c0\u67e5\u548c\u81ea\u6211\u64e6\u9664\u6765\u63a2\u7d22\u548c\u53d6\u6d88\u6982\u5ff5\u9886\u57df\u7684\u8fb9\u754c\u8868\u793a\uff0c\u540e\u8005\u5219\u65e8\u5728\u51cf\u8f7b\u64e6\u9664\u4e0d\u5b89\u5168\u6982\u5ff5\u65f6\u5bf9\u65e0\u5173\u6982\u5ff5\u7684\u4fdd\u7559\u9000\u5316\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAGE\u7684\u65b9\u6cd5\uff0c\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b83\u66f4\u6709\u6548\u5730\u5904\u7406\u4e86\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u9884\u8bad\u7ec3\u4e2d\u53ef\u80fd\u5305\u542b\u7684\u654f\u611f\u4fe1\u606f\u95ee\u9898\uff0c\u907f\u514d\u4e86\u201c\u8bcd\u6982\u5ff5\u6df1\u6e0a\u201d\u7684\u9677\u9631\uff0c\u5e76\u4e14\u5728\u5b89\u5168\u751f\u6210DMs\u65b9\u9762\u5c55\u793a\u51fa\u4e86\u5168\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "SAGE\u901a\u8fc7\u5f15\u5165\u8bed\u4e49\u589e\u5f3a\u64e6\u9664\u548c\u5168\u5c40-\u5c40\u90e8\u534f\u540c\u4fdd\u7559\u673a\u5236\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6269\u6563\u6a21\u578b\u5b89\u5168\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.09450", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09450", "abs": "https://arxiv.org/abs/2506.09450", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578bToM\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6UniToMBench\uff0c\u901a\u8fc7\u591a\u4ea4\u4e92\u4efb\u52a1\u548c\u6f14\u53d8\u6545\u4e8b\u573a\u666f\u6765\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u65b0\u7684\u8bc4\u6d4b\u57fa\u51c6\u6765\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5bf9\u6b64\u7c7b\u4efb\u52a1\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u4e86UniToMBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u6d4b\u8bd5\u96c6\u6210\u4e86SimToM\u548cTOMBENCH\u7684\u4f18\u70b9\uff0c\u8bbe\u8ba1\u4e86\u591a\u4ea4\u4e92\u4efb\u52a1\u548c\u6f14\u53d8\u6545\u4e8b\u573a\u666f\u6765\u7cfb\u7edf\u5730\u6539\u8fdb\u548c\u8bc4\u4f30LLMs\u7684ToM\u80fd\u529b\u3002\u901a\u8fc7\u8d85\u8fc71,000\u4e2a\u624b\u5199\u573a\u666f\u6784\u6210\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89d2\u9009\u53d6\u6280\u672f\u548c\u591a\u6837\u5316\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u66f4\u597d\u5730\u523a\u6fc0LLMs\u7684\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\u3002", "result": "GPT-4o\u548cGPT-4o Mini\u7b49\u6a21\u578b\u5728\u60c5\u7eea\u548c\u4fe1\u5ff5\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u901a\u5e38\u51c6\u786e\u7387\u9ad8\u4e8e80%\uff0c\u4f46\u5728\u77e5\u8bc6\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5219\u663e\u793a\u51fa\u8f83\u5927\u7684\u53d8\u5f02\u6027\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u67d0\u4e9b\u6a21\u578b\u5728\u6d89\u53ca\u60c5\u7eea\u548c\u4fe1\u5ff5\u76f8\u5173\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u77e5\u8bc6\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5219\u5b58\u5728\u8f83\u5927\u5dee\u5f02\uff0c\u8fd9\u7a81\u663e\u4e86UniToMBench\u4f5c\u4e3a\u672a\u6765\u5f00\u53d1\u5168\u9762\u5de5\u5177\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.09369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09369", "abs": "https://arxiv.org/abs/2506.09369", "authors": ["Zeran Ke", "Bin Tan", "Xianwei Zheng", "Yujun Shen", "Tianfu Wu", "Nan Xue"], "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined", "comment": "accepted to CVPR 2025; 17 pages, appendices included", "summary": "This paper studies the problem of Line Segment Detection (LSD) for the\ncharacterization of line geometry in images, with the aim of learning a\ndomain-agnostic robust LSD model that works well for any natural images. With\nthe focus of scalable self-supervised learning of LSD, we revisit and\nstreamline the fundamental designs of (deep and non-deep) LSD approaches to\nhave a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the\ncuration of line geometry at scale from over 10M unlabeled real-world images.\nOur ScaleLSD works very well to detect much more number of line segments from\nany natural images even than the pioneered non-deep LSD approach, having a more\ncomplete and accurate geometric characterization of images using line segments.\nExperimentally, our proposed ScaleLSD is comprehensively testified under\nzero-shot protocols in detection performance, single-view 3D geometry\nestimation, two-view line segment matching, and multiview 3D line mapping, all\nwith excellent performance obtained. Based on the thorough evaluation, our\nScaleLSD is observed to be the first deep approach that outperforms the\npioneered non-deep LSD in all aspects we have tested, significantly expanding\nand reinforcing the versatility of the line geometry of images. Code and Models\nare available at https://github.com/ant-research/scalelsd", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ScaleLSD\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u6d77\u91cf\u65e0\u6807\u8bb0\u81ea\u7136\u56fe\u50cf\u4e0a\u7684\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ebf\u6bb5\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u662f\u5728\u6240\u6709\u6d4b\u8bd5\u65b9\u9762\u90fd\u8d85\u8d8a\u4f20\u7edf\u975e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9996\u4e2a\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5728\u4efb\u4f55\u81ea\u7136\u56fe\u50cf\u4e0a\u5b66\u4e60\u9c81\u68d2\u7684\u3001\u9886\u57df\u65e0\u5173\u7684\u7ebf\u6bb5\u68c0\u6d4b\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u548c\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u51e0\u4f55\u7279\u5f81\u63d0\u70bc\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c6\u548c\u7b80\u5316\u6df1\u5ea6\u4e0e\u975e\u6df1\u5ea6\u65b9\u6cd5\u7684\u57fa\u672c\u8bbe\u8ba1\uff0c\u5f00\u53d1\u4e86ScaleLSD\uff0c\u9488\u5bf9\u6d77\u91cf\u65e0\u6807\u8bb0\u56fe\u50cf\u7684\u7ebf\u6bb5\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cScaleLSD\u5728\u96f6\u6837\u672c\u68c0\u6d4b\u6027\u80fd\u3001\u5355\u89c6\u56fe3D\u51e0\u4f55\u4f30\u8ba1\u3001\u53cc\u89c6\u89d2\u7ebf\u6bb5\u5339\u914d\u548c\u591a\u89c6\u89d23D\u7ebf\u6bb5\u6620\u5c04\u7b49\u591a\u4e2a\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ScaleLSD\u662f\u9996\u4e2a\u5728\u6240\u6709\u6d4b\u8bd5\u65b9\u9762\u5747\u8d85\u8d8a\u4f20\u7edf\u975e\u6df1\u5ea6\u5b66\u4e60\u7ebf\u6bb5\u68c0\u6d4b\u65b9\u6cd5\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u56fe\u50cf\u51e0\u4f55\u7ebf\u7279\u5f81\u7684\u591a\u6837\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.09457", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09457", "abs": "https://arxiv.org/abs/2506.09457", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "The paper identifies a 'reward-generation gap' in Direct Alignment Algorithms (DAAs) and introduces POET to improve their performance in aligning large language models with human preferences.", "motivation": "To address the reward-generation gap in DAAs by improving how the importance of prefix tokens during the LLM generation process is reflected in DAA's implicit reward functions.", "method": "Prefix-Oriented Equal-length Training (POET) which truncates both preferred and dispreferred responses to match the shorter one's length to optimize DAA objectives.", "result": "POET improves the performance of Direct Alignment Algorithms (DAAs) like DPO and SimPO, achieving up to 15.6 points improvement on AlpacaEval 2 and improvements across various downstream tasks.", "conclusion": "The study highlights the critical role of addressing the reward-generation gap in the effectiveness of DAAs for aligning large language models with human preferences."}}
{"id": "2506.09378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09378", "abs": "https://arxiv.org/abs/2506.09378", "authors": ["Qijian Tian", "Xin Tan", "Jingyu Gong", "Yuan Xie", "Lizhuang Ma"], "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images", "comment": null, "summary": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and\nsemantic field reconstruction. Combining 3D scenes with semantic fields\nfacilitates the perception and understanding of the surrounding environment.\nHowever, key challenges include embedding semantics into 3D representations,\nachieving generalizable real-time reconstruction, and ensuring practical\napplicability by using only images as input without camera parameters or ground\ntruth depth. To this end, we propose UniForward, a feed-forward model to\npredict 3D Gaussians with anisotropic semantic features from only uncalibrated\nand unposed sparse-view images. To enable the unified representation of the 3D\nscene and semantic field, we embed semantic features into 3D Gaussians and\npredict them through a dual-branch decoupled decoder. During training, we\npropose a loss-guided view sampler to sample views from easy to hard,\neliminating the need for ground truth depth or masks required by previous\nmethods and stabilizing the training process. The whole model can be trained\nend-to-end using a photometric loss and a distillation loss that leverages\nsemantic features from a pre-trained 2D semantic model. At the inference stage,\nour UniForward can reconstruct 3D scenes and the corresponding semantic fields\nin real time from only sparse-view images. The reconstructed 3D scenes achieve\nhigh-quality rendering, and the reconstructed 3D semantic field enables the\nrendering of view-consistent semantic features from arbitrary views, which can\nbe further decoded into dense segmentation masks in an open-vocabulary manner.\nExperiments on novel view synthesis and novel view segmentation demonstrate\nthat our method achieves state-of-the-art performances for unifying 3D scene\nand semantic field reconstruction.", "AI": {"tldr": "Researchers created a feed-forward model named UniForward that predicts 3D scenes and their semantic fields from uncalibrated, sparse-view images, achieving real-time synthesis and high-quality rendering without requiring camera parameters or ground truth depth, outperforming current methods.", "motivation": "The motivation is to unify 3D scene and semantic field reconstruction by proposing a technique that overcomes the challenges of incorporating semantics into 3D representations, achieving real-time reconstruction, and ensuring practicality by only using images as input.", "method": "The paper develops a model named UniForward, which is a feed-forward model designed to predict 3D Gaussians with anisotropic semantic features from sparse-view images without need for camera parameters or ground truth depth. It embeds semantic features into 3D Gaussians and uses a dual-branch decoupled decoder to predict them. For training, a loss-guided view sampler is introduced to stabilize the process without relying on ground truth depth or masks.", "result": "Experiments on novel view synthesis and novel view segmentation show that the method achieves state-of-the-art performances for unifying 3D scene and semantic field reconstruction. The model can reconstruct 3D scenes with high-quality rendering and generate view-consistent semantic features, which can be decoded into dense segmentation masks.", "conclusion": "This research proposes a novel approach to simultaneously reconstruct 3D scenes and corresponding semantic fields from sparse-view images through a feed-forward Gaussian Splatting model that requires no camera calibration or ground truth depth data. It demonstrates high-quality reconstruction and real-time synthesis capabilities."}}
{"id": "2506.09495", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09495", "abs": "https://arxiv.org/abs/2506.09495", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "\u7814\u7a76\u91c7\u7528\u591a\u79cd\u65b9\u6cd5\u5206\u6790\u4e86181\u4e2a\u6709\u81ea\u6740\u503e\u5411\u7684YouTube\u9891\u9053\u548c134\u4e2a\u5bf9\u7167\u7ec4\u9891\u9053\uff0c\u53d1\u73b0\u5fc3\u7406\u5065\u5eb7\u6323\u624e\u548cYouTube\u53c2\u4e0e\u5ea6\u662f\u4e0e\u81ea\u6740\u884c\u4e3a\u6709\u5173\u7684\u91cd\u8981\u6307\u6807\u3002", "motivation": "\u81ea\u6740\u4ecd\u7136\u662f\u897f\u65b9\u56fd\u5bb6\u7684\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u8fd9\u9879\u7814\u7a76\u5c1d\u8bd5\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u6570\u5b57\u8db3\u8ff9\u6765\u6d1e\u5bdf\u81ea\u6740\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u7684\u81ea\u4e0b\u800c\u4e0a\u3001\u6df7\u5408\u578b\u548c\u4e13\u5bb6\u9a71\u52a8\u7684\u81ea\u4e0a\u800c\u4e0b\u4e09\u79cd\u4e92\u8865\u65b9\u6cd5\uff0c\u57fa\u4e8e181\u4e2aYouTube\u9891\u9053\u7684\u7eb5\u5411\u6570\u636e\u96c6\uff08\u5305\u542b\u6709\u751f\u547d\u5a01\u80c1\u7684\u5c1d\u8bd5\u8005\uff09\u548c134\u4e2a\u63a7\u5236\u9891\u9053\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\u53d1\u73b0\u5fc3\u7406\u5065\u5eb7\u6323\u624e\u548cYouTube\u53c2\u4e0e\u5ea6\u4e0e\u81ea\u6740\u5c1d\u8bd5\u6709\u5173\uff1b\u6df7\u5408\u65b9\u6cd5\u7531\u4e34\u5e8a\u4e13\u5bb6\u5ba1\u6838\u751f\u6210\u7684\u8bdd\u9898\uff0c\u4f46\u6ca1\u6709\u53d1\u73b0\u66f4\u591a\u663e\u8457\u5173\u8054\u3002\u81ea\u4e0a\u800c\u4e0b\u7684\u65b9\u6cd5\u53d1\u73b0\uff0c\u5728\u4e0a\u4f20\u671f\u95f4\u5c1d\u8bd5\u81ea\u6740\u7684\u4eba\u548c\u5c1d\u8bd5\u81ea\u6740\u4e4b\u524d\u5c31\u4e0a\u4f20\u89c6\u9891\u7684\u4eba\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\u662f\u52a8\u673a\u4e0d\u540c\u3002", "conclusion": "\u7814\u7a76\u6574\u5408\u4e86\u591a\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ec6\u81f4\u7684\u81ea\u6740\u6027\u7406\u89e3\uff0c\u7ed3\u5408\u4e86\u6570\u5b57\u884c\u4e3a\u548c\u4e34\u5e8a\u89c1\u89e3\u3002"}}
{"id": "2506.09385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09385", "abs": "https://arxiv.org/abs/2506.09385", "authors": ["Jialong Zuo", "Yongtai Deng", "Mengdan Tan", "Rui Jin", "Dongyue Wu", "Nong Sang", "Liang Pan", "Changxin Gao"], "title": "ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model", "comment": null, "summary": "In real-word scenarios, person re-identification (ReID) expects to identify a\nperson-of-interest via the descriptive query, regardless of whether the query\nis a single modality or a combination of multiple modalities. However, existing\nmethods and datasets remain constrained to limited modalities, failing to meet\nthis requirement. Therefore, we investigate a new challenging problem called\nOmni Multi-modal Person Re-identification (OM-ReID), which aims to achieve\neffective retrieval with varying multi-modal queries. To address dataset\nscarcity, we construct ORBench, the first high-quality multi-modal dataset\ncomprising 1,000 unique identities across five modalities: RGB, infrared, color\npencil, sketch, and textual description. This dataset also has significant\nsuperiority in terms of diversity, such as the painting perspectives and\ntextual information. It could serve as an ideal platform for follow-up\ninvestigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal\nlearning framework for person ReID. It enables synergistic fusion and\ncross-modal alignment of arbitrary modality combinations in a single model,\nwith a unified encoding and multi-expert routing mechanism proposed. Extensive\nexperiments verify the advancement and practicality of our ORBench. A wide\nrange of possible models have been evaluated and compared on it, and our\nproposed ReID5o model gives the best performance. The dataset and code will be\nmade publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.", "AI": {"tldr": "The paper introduces ReID5o, a multi-modal learning framework for OM-ReID, and constructs ORBench, a benchmark dataset for evaluating multi-modal ReID methods.", "motivation": "The motivation is to address the limitation of existing ReID methods and datasets that do not handle multiple modalities effectively by introducing a challenging problem, Omni Multi-modal Person Re-identification (OM-ReID).", "method": "The paper proposes ReID5o, a new multi-modal learning framework for person Re-identification (ReID) that allows for synergistic fusion and cross-modal alignment of any combination of modalities using a unified encoding and multi-expert routing mechanism.", "result": "Extensive experiments on the new multi-modal dataset ORBench demonstrate the advancement and practicality of ReID5o, which outperforms other models on this dataset.", "conclusion": "The paper contributes a novel multi-modal learning framework called ReID5o and a comprehensive multi-modal dataset ORBench, which offer a valuable resource for research in multi-modal person ReID."}}
{"id": "2506.09501", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09501", "abs": "https://arxiv.org/abs/2506.09501", "authors": ["Jiayi Yuan", "Hao Li", "Xinheng Ding", "Wenya Xie", "Yu-Jhe Li", "Wentian Zhao", "Kun Wan", "Jing Shi", "Xia Hu", "Zirui Liu"], "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning", "comment": null, "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e86\u6570\u503c\u7cbe\u5ea6\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6027\u80fd\u53ef\u91cd\u590d\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u7ba1\u9053LayerCast\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u8f93\u51fa\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u5df2\u6210\u4e3a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5176\u6027\u80fd\u7684\u91cd\u73b0\u6027\u53d7\u5230\u7cfb\u7edf\u914d\u7f6e\u66f4\u6539\u7684\u5f71\u54cd\uff0c\u6bd4\u5982\u8bc4\u4f30\u6279\u6b21\u5927\u5c0f\u3001GPU\u6570\u91cf\u548cGPU\u7248\u672c\u7b49\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u5c24\u5176\u663e\u8457\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u56e0\u7d20\u53ef\u4ee5\u5f71\u54cd\u5230\u6700\u7ec8\u7684\u51c6\u786e\u6027\u3002\u6b64\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u6570\u503c\u7cbe\u5ea6\u5982\u4f55\u5f71\u54cdLLMs\u7684\u53ef\u91cd\u590d\u6027\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\uff0c\u7814\u7a76\u4e86\u5728\u4e0d\u540c\u786c\u4ef6\u3001\u8f6f\u4ef6\u548c\u7cbe\u5ea6\u8bbe\u7f6e\u4e0b\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u6570\u503c\u7cbe\u5ea6\u5bf9\u53ef\u91cd\u590d\u6027\u7684\u5f71\u54cd\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63a8\u7406\u7ba1\u9053LayerCast\uff0c\u5728\u8be5\u7ba1\u9053\u4e2d\u6743\u91cd\u4ee516\u4f4d\u7cbe\u5ea6\u5b58\u50a8\uff0c\u4f46\u6240\u6709\u8ba1\u7b97\u90fd\u5728FP32\u4e2d\u8fdb\u884c\uff0c\u4ece\u800c\u5728\u5185\u5b58\u6548\u7387\u548c\u6570\u503c\u7a33\u5b9a\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u786c\u4ef6\u3001\u8f6f\u4ef6\u548c\u7cbe\u5ea6\u8bbe\u7f6e\u53ef\u4ee5\u5f15\u53d1\u663e\u8457\u7684\u751f\u6210\u56de\u7b54\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u3002\u4f8b\u5982\uff0cDeepSeek-R1-Distill-Qwen-7B\u6a21\u578b\u5728bfloat16\u7cbe\u5ea6\u548c\u8d2a\u5a6a\u89e3\u7801\u4e0b\uff0c\u7531\u4e8eGPU\u6570\u91cf\u3001\u7c7b\u578b\u548c\u8bc4\u4f30\u6279\u6b21\u5927\u5c0f\u7684\u5dee\u5f02\u53ef\u4ee5\u5c55\u793a\u51fa\u9ad8\u8fbe9%\u7684\u51c6\u786e\u7387\u5dee\u5f02\u548c9000\u4e2atoken\u7684\u56de\u7b54\u957f\u5ea6\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u7cfb\u7edf\u8c03\u67e5\u6570\u503c\u7cbe\u5ea6\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u53ef\u91cd\u590d\u6027\u5f71\u54cd\u7684\u5de5\u4f5c\u3002\u63d0\u51fa\u4e86LayerCast\uff0c\u4e00\u4e2a\u6743\u8861\u5185\u5b58\u6548\u7387\u548c\u6570\u503c\u7a33\u5b9a\u6027\u7684\u8f7b\u91cf\u7ea7\u63a8\u7406\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u7684\u4ee3\u7801\u5728https://github.com/nanomaoli/llm_reproducibility\u4e0a\u5f00\u653e\u83b7\u53d6\u3002"}}
{"id": "2506.09399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09399", "abs": "https://arxiv.org/abs/2506.09399", "authors": ["Kaiyu Guo", "Zijian Wang", "Brian C. Lovell", "Mahsa Baktashmotlagh"], "title": "Improving Out-of-Distribution Detection via Dynamic Covariance Calibration", "comment": null, "summary": "Out-of-Distribution (OOD) detection is essential for the trustworthiness of\nAI systems. Methods using prior information (i.e., subspace-based methods) have\nshown effective performance by extracting information geometry to detect OOD\ndata with a more appropriate distance metric. However, these methods fail to\naddress the geometry distorted by ill-distributed samples, due to the\nlimitation of statically extracting information geometry from the training\ndistribution. In this paper, we argue that the influence of ill-distributed\nsamples can be corrected by dynamically adjusting the prior geometry in\nresponse to new data. Based on this insight, we propose a novel approach that\ndynamically updates the prior covariance matrix using real-time input features,\nrefining its information. Specifically, we reduce the covariance along the\ndirection of real-time input features and constrain adjustments to the residual\nspace, thus preserving essential data characteristics and avoiding effects on\nunintended directions in the principal space. We evaluate our method on two\npre-trained models for the CIFAR dataset and five pre-trained models for\nImageNet-1k, including the self-supervised DINO model. Extensive experiments\ndemonstrate that our approach significantly enhances OOD detection across\nvarious models. The code is released at https://github.com/workerbcd/ooddcc.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.09507", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09507", "abs": "https://arxiv.org/abs/2506.09507", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "comment": null, "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\\textbf{\\ourRoPE}\u65b9\u6cd5\u548c\\textbf{\\model}\u67b6\u6784\uff0c\u89e3\u51b3\u4e86Transformer\u4e0e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u96c6\u6210\u4e2d\u7684\u4f4d\u7f6e\u7f16\u7801\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6bd4\u6807\u51c6Transformer\u6a21\u578b\u66f4\u5feb\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u4ee5\u53ca\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u89e3\u51b3Transformer\u6a21\u578b\u4e0e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u96c6\u6210\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u4f4d\u7f6e\u7f16\u7801\u4e0d\u4e00\u81f4\u96be\u9898\uff0c\u8be5\u95ee\u9898\u5bfc\u81f4\u96c6\u6210\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u548c\u4e0d\u8fde\u7eed\u73b0\u8c61\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\uff08\\textbf{\\ourRoPE}\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86Transformer\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e4b\u95f4\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u57fa\u4e8e\\textbf{\\ourRoPE}\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\\textbf{\\model}\u7684\u6df7\u5408\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5c06Transformer\u548cSSM\u5c42\u5728\u7edf\u4e00\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u4e0b\u65e0\u7f1d\u7ed3\u5408\u3002", "result": "\\textbf{\\model}\u57284K\u5e8f\u5217\u957f\u5ea6\u4e0b\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u6bd4\u6807\u51c6Transformer\u6a21\u578b\u5206\u522b\u5feb42.3%\u548c29.5%\uff0c\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u4e0a\u6bd4\u6807\u51c6Transformer\u57fa\u7ebf\u9ad8\u51fa4%\u4ee5\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u4e14\u6bd4\u5e38\u89c4\u7684Transformer\u6216SSM\u5177\u6709\u66f4\u597d\u7684\u6269\u5c55\u6027\uff0c1.3B\u7248\u672c\u7684\u5e73\u5747\u51c6\u786e\u6027\u6bd4320M\u7248\u672c\u5347\u9ad8\u4e867.22%\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u7edf\u4e00\u7684\u4f4d\u7f6e\u7f16\u7801\u89e3\u51b3\u4e86\u6df7\u5408\u6a21\u578b\u4e2d\u7684\u4f4d\u7f6e\u7f16\u7801\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u4f7f\u5f97\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u5f97\u4ee5\u5b9e\u73b0\u3002"}}
{"id": "2506.09403", "categories": ["cs.CV", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2506.09403", "abs": "https://arxiv.org/abs/2506.09403", "authors": ["Xinya Liu", "Jianghao Wu", "Tao Lu", "Shaoting Zhang", "Guotai Wang"], "title": "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation", "comment": "18 pages, 4 figures. Accepted for publication in Neurocomputing", "summary": "Domain Adaptation (DA) is crucial for robust deployment of medical image\nsegmentation models when applied to new clinical centers with significant\ndomain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal\nwith privacy concerns and access constraints on source-domain data during\nadaptation to target-domain data. However, SFDA faces challenges such as\ninsufficient supervision in the target domain with unlabeled images. In this\nwork, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels\nmethod for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch\nIntensity Enhancement (T3IE) that not only improves quality of raw\npseudo-labels in the target domain, but also leads to SAM-compatible inputs\nwith three channels to better leverage SAM's zero-shot inference ability for\nrefining the pseudo-labels; 2) A reliable pseudo-label selection module that\nrejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs\n(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training\nprocedure in the unlabeled target domain where reliable pseudo-labels are used\nfor supervision and unreliable parts are regularized by entropy minimization.\nExperiments conducted on two multi-domain medical image segmentation datasets\nfor fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA\neffectively enhances pseudo-label quality in the unlabeled target domain, and\nimproves SFDA performance by leveraging the reliability-aware training; 2)\nSRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is\nclose to that of supervised training in the target domain. The code of this\nwork is available online: https://github.com/HiLab-git/SRPL-SFDA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684\u4e00\u79cdSRPL-SFDA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u523b\u4e09\u5206\u652f\u5f3a\u5ea6\u589e\u5f3a\u3001\u53ef\u9760\u4f2a\u6807\u7b7e\u9009\u62e9\u548c\u53ef\u9760\u6027\u611f\u77e5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5728\u65e0\u6e90\u81ea\u57df\u9002\u5e94\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u8f83\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5728\u533b\u7597\u56fe\u50cf\u5206\u5272\u6a21\u578b\u90e8\u7f72\u5230\u65b0\u4e34\u5e8a\u4e2d\u5fc3\u65f6\uff0c\u57df\u81ea\u9002\u5e94\uff08DA\uff09\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u663e\u8457\u57df\u8f6c\u6362\u65f6\u3002\u65e0\u6e90\u81ea\u57df\u81ea\u9002\u5e94\uff08SFDA\uff09\u80fd\u591f\u5904\u7406\u9690\u79c1\u4fdd\u62a4\u548c\u5bf9\u6e90\u57df\u6570\u636e\u8bbf\u95ee\u7684\u5236\u7ea6\u95ee\u9898\uff0c\u4f46\u9762\u4e34\u5728\u76ee\u6807\u57df\u4e2d\u76d1\u7763\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSRPL-SFDA\u7684\u65b9\u6cd5\uff0c\u9488\u5bf9\u65e0\u6e90\u9886\u57df\u9002\u5e94\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff1a1\uff09\u6d4b\u8bd5\u65f6\u523b\u4e09\u5206\u652f\u5f3a\u5ea6\u589e\u5f3a\uff08T3IE\uff09\uff0c\u4ee5\u63d0\u9ad8\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u5e76\u589e\u5f3aSAM\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\uff1b2\uff09\u4e00\u4e2a\u57fa\u4e8e\u8f93\u5165\u6270\u52a8\u4e0b\u591a\u4e2aSAM\u8f93\u51fa\u4e00\u81f4\u6027\u7684\u53ef\u9760\u4f2a\u6807\u7b7e\u9009\u62e9\u6a21\u5757\uff1b3\uff09\u4f7f\u7528\u53ef\u9760\u4f2a\u6807\u7b7e\u8fdb\u884c\u76d1\u7763\u5e76\u4ee5\u71b5\u6700\u5c0f\u5316\u65b9\u5f0f\u5bf9\u4e0d\u53ef\u9760\u90e8\u5206\u8fdb\u884c\u6b63\u5219\u5316\u7684\u53ef\u9760\u6027\u611f\u77e5\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u4e24\u4e2a\u591a\u57df\u533b\u7597\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u5206\u522b\u9a8c\u8bc1\u4e86\uff1a1\uff09SRPL-SFDA\u80fd\u591f\u6709\u6548\u589e\u5f3a\u672a\u6807\u8bb0\u76ee\u6807\u57df\u7684\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u53ef\u9760\u6027\u611f\u77e5\u8bad\u7ec3\u63d0\u9ad8SFDA\u6027\u80fd\uff1b2\uff09SRPL-SFDA\u4f18\u4e8e\u73b0\u6709\u7684SFDA\u65b9\u6cd5\uff0c\u5e76\u4e14\u5176\u6027\u80fd\u63a5\u8fd1\u4e8e\u76ee\u6807\u57df\u7684\u76d1\u7763\u8bad\u7ec3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5SRPL-SFDA\uff0c\u9488\u5bf9\u65e0\u6e90\u81ea\u57df\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u548c\u9009\u62e9\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u533b\u7597\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u76ee\u6807\u57df\u7684\u9002\u5e94\u6027\u80fd\u3002"}}
{"id": "2506.09513", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.09513", "abs": "https://arxiv.org/abs/2506.09513", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "comment": "24 pages, 6 figures, 7 tables", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "AI": {"tldr": "\u6784\u5efa\u4e86ReasonMed\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u7ed3\u5408\u8be6\u7ec6\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4e0e\u7b80\u6d01\u7b54\u6848\u6458\u8981\u7684\u5fae\u8c03\u7b56\u7565\u6700\u6709\u6548\uff0c\u8bad\u7ec3\u51fa\u7684ReasonMed-7B\u6a21\u578b\u5728\u5c0f\u4e8e10B\u53c2\u6570\u7684\u6a21\u578b\u4e2d\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u63a8\u7406\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u533b\u5b66\u95ee\u7b54\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u548c\u7ec6\u5316\u8fc7\u7a0b\u6784\u9020\u4e86ReasonMed\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u62ec\u4f7f\u7528Error Refiner\u6765\u6539\u8fdb\u548c\u7ea0\u6b63\u7531\u9a8c\u8bc1\u5668\u6807\u51fa\u7684\u6613\u9519\u6b65\u9aa4\u3002", "result": "\u7ed3\u5408\u8be6\u7ec6\u7684Chain-of-Thought\u63a8\u7406\u4e0e\u7b80\u6d01\u7684\u7b54\u6848\u6458\u8981\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd9\u79cd\u7b56\u7565\u80fd\u4ea7\u751f\u6700\u6709\u6548\u7684\u533b\u7597\u63a8\u7406\u6a21\u578b\u3002\u57fa\u4e8e\u6b64\u7b56\u7565\u8bad\u7ec3\u7684ReasonMed-7B\u6a21\u578b\u4e3a\u5c0f\u4e8e10B\u53c2\u6570\u7684\u6a21\u578b\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u6bd4\u524d\u4e00\u4e2a\u6700\u4f73\u6a21\u578b\u63d0\u5347\u4e864.17%\uff0c\u751a\u81f3\u5728PubMedQA\u4e0a\u7684\u8868\u73b0\u8d85\u8fc7LLaMA3.1-70B\u6a21\u578b4.60%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be6\u7ec6\u7684\u94fe\u5f0f\u601d\u7ef4\u65b9\u6cd5\u4e0e\u7b80\u6d01\u7684\u7b54\u6848\u603b\u7ed3\u76f8\u7ed3\u5408\u662f\u8bad\u7ec3\u533b\u7597\u63a8\u7406\u6a21\u578b\u7684\u6700\u4f73\u5b9e\u8df5\u3002"}}
{"id": "2506.09411", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09411", "abs": "https://arxiv.org/abs/2506.09411", "authors": ["Vaclav Knapp", "Matyas Bohacek"], "title": "Synthetic Human Action Video Data Generation with Pose Transfer", "comment": null, "summary": "In video understanding tasks, particularly those involving human motion,\nsynthetic data generation often suffers from uncanny features, diminishing its\neffectiveness for training. Tasks such as sign language translation, gesture\nrecognition, and human motion understanding in autonomous driving have thus\nbeen unable to exploit the full potential of synthetic data. This paper\nproposes a method for generating synthetic human action video data using pose\ntransfer (specifically, controllable 3D Gaussian avatar models). We evaluate\nthis method on the Toyota Smarthome and NTU RGB+D datasets and show that it\nimproves performance in action recognition tasks. Moreover, we demonstrate that\nthe method can effectively scale few-shot datasets, making up for groups\nunderrepresented in the real training data and adding diverse backgrounds. We\nopen-source the method along with RANDOM People, a dataset with videos and\navatars of novel human identities for pose transfer crowd-sourced from the\ninternet.", "AI": {"tldr": "A method to generate synthetic human action video data using pose transfer aims to improve action recognition tasks by diversifying datasets and compensating for underrepresented groups, open-sourced with a new dataset called RANDOM People.", "motivation": "The motivation behind this research is the uncanny features in synthetic data that diminish its effectiveness for training in video understanding tasks such as sign language translation, gesture recognition, and human motion understanding in autonomous driving.", "method": "The paper proposes a method for generating synthetic human action video data using pose transfer, specifically controllable 3D Gaussian avatar models.", "result": "The proposed method improved performance in action recognition tasks when evaluated on the Toyota Smarthome and NTU RGB+D datasets, effectively adding diversity to few-shot datasets and compensating for underrepresented groups in real training data.", "conclusion": "The method not only enhances synthetic data generation for human action recognition but also makes up for diversity gaps in the datasets. The researchers open-sourced the method and a related dataset named RANDOM People."}}
{"id": "2506.09542", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09542", "abs": "https://arxiv.org/abs/2506.09542", "authors": ["Dingjun Wu", "Yukun Yan", "Zhenghao Liu", "Zhiyuan Liu", "Maosong Sun"], "title": "KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding\nresponses in external knowledge. However, existing methods typically rely on a\nsingle source, either unstructured text or structured knowledge. Moreover, they\nlack cognitively inspired mechanisms for activating relevant knowledge. To\naddress these issues, we propose KG-Infused RAG, a framework that integrates\nKGs into RAG systems to implement spreading activation, a cognitive process\nthat enables concept association and inference. KG-Infused RAG retrieves KG\nfacts, expands the query accordingly, and enhances generation by combining\ncorpus passages with structured facts, enabling interpretable, multi-source\nretrieval grounded in semantic structure. We further improve KG-Infused RAG via\npreference learning on sampled key stages in the pipeline. Experiments on five\nQA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by\n3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG\nbrings further performance gains, demonstrating its effectiveness and\nversatility as a plug-and-play enhancement module for corpus-based RAG methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86KG-Infused RAG\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u5b9e\u65bd\u4f20\u64ad\u6fc0\u6d3b\uff0c\u6539\u8fdb\u4e86RAG\u7cfb\u7edf\u5728\u68c0\u7d22\u751f\u6210\u4e2d\u7684\u51c6\u786e\u6027\u548c\u591a\u6e90\u4fe1\u606f\u6574\u5408\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u7684\u77e5\u8bc6\u6e90\uff0c\u6216\u662f\u975e\u7ed3\u6784\u5316\u7684\u6587\u672c\u6216\u662f\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\uff0c\u5e76\u4e14\u7f3a\u4e4f\u8ba4\u77e5\u542f\u53d1\u673a\u5236\u6765\u6fc0\u6d3b\u76f8\u5173\u7684\u77e5\u8bc6\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86KG-Infused RAG\u6846\u67b6\u3002", "method": "KG-Infused RAG\u6846\u67b6\u5c06\u77e5\u8bc6\u56fe\u8c31\u6574\u5408\u5230RAG\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u73b0\u4f20\u64ad\u6fc0\u6d3b\u8fd9\u4e00\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u80fd\u591f\u4f7f\u6982\u5ff5\u5173\u8054\u548c\u63a8\u7406\u3002KG-Infused RAG\u68c0\u7d22KG\u4e8b\u5b9e\uff0c\u76f8\u5e94\u5730\u6269\u5c55\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u8bed\u6599\u5e93\u7247\u6bb5\u548c\u7ed3\u6784\u5316\u4e8b\u5b9e\u6765\u589e\u5f3a\u751f\u6210\uff0c\u4ece\u800c\u5b9e\u73b0\u57fa\u4e8e\u8bed\u4e49\u7ed3\u6784\u7684\u53ef\u89e3\u91ca\u3001\u591a\u6e90\u68c0\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKG-Infused RAG\u5728\u4e94\u4e2aQA\u57fa\u51c6\u7684\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u666e\u901a\u7684RAG\u7cfb\u7edf\uff0c\u63d0\u9ad8\u4e863.8%\u523013.8%\u7684\u6548\u679c\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cKG-Infused RAG\u5728\u4e94\u4e2a\u95ee\u7b54\u57fa\u51c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u666e\u901a\u7684RAG\uff08\u63d0\u9ad8\u4e863.8%\u523013.8%\uff09\u3002\u6b64\u5916\uff0c\u5f53KG-Infused RAG\u96c6\u6210\u5230Self-RAG\u4e2d\u65f6\uff0c\u4e5f\u5e26\u6765\u4e86\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5b83\u4f5c\u4e3a\u8bed\u6599\u5e93RAG\u65b9\u6cd5\u589e\u5f3a\u6a21\u5757\u7684\u6709\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2506.09416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09416", "abs": "https://arxiv.org/abs/2506.09416", "authors": ["Xinyu Peng", "Ziyang Zheng", "Yaoming Wang", "Han Li", "Nuowen Kan", "Wenrui Dai", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "title": "Noise Conditional Variational Score Distillation", "comment": null, "summary": "We propose Noise Conditional Variational Score Distillation (NCVSD), a novel\nmethod for distilling pretrained diffusion models into generative denoisers. We\nachieve this by revealing that the unconditional score function implicitly\ncharacterizes the score function of denoising posterior distributions. By\nintegrating this insight into the Variational Score Distillation (VSD)\nframework, we enable scalable learning of generative denoisers capable of\napproximating samples from the denoising posterior distribution across a wide\nrange of noise levels. The proposed generative denoisers exhibit desirable\nproperties that allow fast generation while preserve the benefit of iterative\nrefinement: (1) fast one-step generation through sampling from pure Gaussian\nnoise at high noise levels; (2) improved sample quality by scaling the\ntest-time compute with multi-step sampling; and (3) zero-shot probabilistic\ninference for flexible and controllable sampling. We evaluate NCVSD through\nextensive experiments, including class-conditional image generation and inverse\nproblem solving. By scaling the test-time compute, our method outperforms\nteacher diffusion models and is on par with consistency models of larger sizes.\nAdditionally, with significantly fewer NFEs than diffusion-based methods, we\nachieve record-breaking LPIPS on inverse problems.", "AI": {"tldr": "Noise Conditional Variational Score Distillation (NCVSD) \u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8f6c\u5316\u4e3a\u751f\u6210\u53bb\u566a\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u751f\u6210\u6837\u672c\u7684\u5feb\u901f\u6027\u548c\u8d28\u91cf\u6539\u5584\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u901a\u8fc7\u84b8\u998f\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u6765\u6784\u5efa\u80fd\u591f\u5feb\u901f\u751f\u6210\u6837\u672c\u540c\u65f6\u4fdd\u6301\u8fed\u4ee3\u4f18\u5316\u4f18\u52bf\u7684\u751f\u6210\u53bb\u566a\u5668\u3002", "method": "Noise Conditional Variational Score Distillation (NCVSD) \u65b9\u6cd5\u901a\u8fc7\u63ed\u793a\u65e0\u6761\u4ef6\u8bc4\u5206\u51fd\u6570\u9690\u5f0f\u5730\u8868\u5f81\u53bb\u566a\u540e\u9a8c\u5206\u5e03\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u5c06\u8fd9\u4e00\u89c1\u89e3\u6574\u5408\u5230\u53d8\u5206\u8bc4\u5206\u84b8\u998f\uff08VSD\uff09\u6846\u67b6\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u5728\u5e7f\u6cdb\u566a\u97f3\u6c34\u5e73\u8303\u56f4\u5185\uff0c\u53ef\u6269\u5c55\u5730\u5b66\u4e60\u80fd\u591f\u8fd1\u4f3c\u91c7\u6837\u53bb\u566a\u540e\u9a8c\u5206\u5e03\u7684\u751f\u6210\u53bb\u566a\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7\u8c03\u8282\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u91cf\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u6559\u5e08\u6269\u6563\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u53cd\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8fbe\u5230\u4e86\u521b\u7eaa\u5f55\u7684LPIPS\u503c\uff08\u7528\u663e\u8457\u5c11\u4e8e\u6269\u6563\u65b9\u6cd5\u7684NFEs\uff09\uff0c\u540c\u65f6\u5728\u7c7b\u522b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "NCVSD \u65b9\u6cd5\u901a\u8fc7\u63d0\u5347\u751f\u6210\u53bb\u566a\u5668\u7684\u5feb\u901f\u751f\u6210\u80fd\u529b\u548c\u9ad8\u8d28\u91cf\u6837\u672c\u751f\u6210\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u53cd\u95ee\u9898\u89e3\u51b3\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4f7f\u7528\u4e86\u6bd4\u6269\u6563\u65b9\u6cd5\u5c11\u5f97\u591a\u7684NFEs\u3002"}}
{"id": "2506.09556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09556", "abs": "https://arxiv.org/abs/2506.09556", "authors": ["Georgios Chatzichristodoulou", "Despoina Kosmopoulou", "Antonios Kritikos", "Anastasia Poulopoulou", "Efthymios Georgiou", "Athanasios Katsamanis", "Vassilis Katsouros", "Alexandros Potamianos"], "title": "MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions", "comment": "Accepted at Interspeech 2025", "summary": "SER is a challenging task due to the subjective nature of human emotions and\ntheir uneven representation under naturalistic conditions. We propose MEDUSA, a\nmultimodal framework with a four-stage training pipeline, which effectively\nhandles class imbalance and emotion ambiguity. The first two stages train an\nensemble of classifiers that utilize DeepSER, a novel extension of a deep\ncross-modal transformer fusion mechanism from pretrained self-supervised\nacoustic and linguistic representations. Manifold MixUp is employed for further\nregularization. The last two stages optimize a trainable meta-classifier that\ncombines the ensemble predictions. Our training approach incorporates human\nannotation scores as soft targets, coupled with balanced data sampling and\nmultitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion\nRecognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic\nConditions Challenge.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MEDUSA\uff0c\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u81ea\u7136\u6761\u4ef6\u4e0b\u7684\u60c5\u611f\u8868\u793a\uff0c\u8be5\u6846\u67b6\u5728Interspeech 2025\u6311\u6218\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "motivation": "\u60c5\u611f\u8bc6\u522b\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u4eba\u7c7b\u60c5\u611f\u5177\u6709\u4e3b\u89c2\u6027\uff0c\u800c\u4e14\u5728\u81ea\u7136\u6761\u4ef6\u4e0b\u60c5\u611f\u7684\u8868\u73b0\u662f\u4e0d\u5747\u8861\u7684\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86MEDUSA\uff0c\u4e00\u4e2a\u5305\u542b\u56db\u4e2a\u8bad\u7ec3\u9636\u6bb5\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u60c5\u611f\u6a21\u7cca\u95ee\u9898\u3002\u524d\u4e24\u4e2a\u9636\u6bb5\u8bad\u7ec3\u4e86\u4e00\u4e2a\u4f7f\u7528DeepSER\uff08\u6df1\u5ea6\u8de8\u6a21\u6001\u53d8\u538b\u5668\u878d\u5408\u673a\u5236\u7684\u6269\u5c55\uff09\u7684\u5206\u7c7b\u5668\u96c6\u5408\u3002DeepSER\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u8bed\u97f3\u548c\u8bed\u8a00\u8868\u793a\u3002\u540c\u65f6\u91c7\u7528\u6d41\u5f62MixUp\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u6b63\u5219\u5316\u5904\u7406\u3002\u6700\u540e\u4e24\u4e2a\u9636\u6bb5\u4f18\u5316\u4e86\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u5143\u5206\u7c7b\u5668\uff0c\u5b83\u7ed3\u5408\u4e86\u96c6\u5408\u9884\u6d4b\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728Interspeech 2025\u7684\u81ea\u7136\u6761\u4ef6\u4e0b\u57fa\u4e8e\u8bed\u97f3\u7684\u60c5\u611f\u8bc6\u522b\u6311\u6218\u4efb\u52a11\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5MEDUSA\u5229\u7528\u591a\u6a21\u6001\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u4e3b\u89c2\u6027\u548c\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2506.09417", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09417", "abs": "https://arxiv.org/abs/2506.09417", "authors": ["Yunxiao Shi", "Yinhao Zhu", "Shizhong Han", "Jisoo Jeong", "Amin Ansari", "Hong Cai", "Fatih Porikli"], "title": "ODG: Occupancy Prediction Using Dual Gaussians", "comment": null, "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, ODG, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG\nalso delivers competitive inference speed when compared to the latest efficient\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BEV\u548c\u7a00\u758f\u70b9\u4e91\u8868\u793a\u7684\u53cc\u5206\u652f3D\u5360\u7528\u9884\u6d4b\u65b9\u6cd5ODG\uff0c\u5b83\u901a\u8fc7\u5171\u4eab\u4e09\u7ef4\u4fe1\u606f\u548c\u878d\u5408\u4e24\u5206\u652f\u6765\u6539\u5584\u5bf9\u5c0f\u7269\u4f53\u548c\u5176\u4ed6\u590d\u6742\u7269\u4f53\u7684\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u89e3\u51b3\u73b0\u67093D\u5360\u7528\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4ee5\u53caBEV\u548c\u7a00\u758f\u70b9\u4e91\u5404\u81ea\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002BEV\u8868\u793a\u96be\u4ee5\u5904\u7406\u5c0f\u7269\u4f53\uff0c\u800c\u7a00\u758f\u70b9\u4e91\u8868\u8fbe\u5728\u6355\u6349\u5e73\u5766\u6216\u5927\u7269\u4f53\u65f6\u6548\u7387\u4f4e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u7ed3\u5408\u4e24\u79cd\u8868\u793a\u65b9\u6cd5\u4f18\u52bf\u7684\u65b9\u6848\u6765\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u573a\u666f\u7406\u89e3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aODG\u7684\u65b0\u98963D\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86BEV\u8868\u793a\u548c\u7a00\u758f\u70b9\u4e91\u8868\u793a\u3002\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u53cc\u5206\u652f\u7ed3\u6784\uff1a\u57fa\u4e8e\u67e5\u8be2\u7684\u7a00\u758f\u70b9\u4e91\u5206\u652f\u548cBEV\u5206\u652f\u3002\u7a00\u758f\u70b9\u4e91\u5206\u652f\u5b66\u4e60\u5230\u7684\u4e09\u7ef4\u4fe1\u606f\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u4e0eBEV\u5206\u652f\u5171\u4eab\uff0c\u4ee5\u6b64\u6765\u589e\u5f3a\u96be\u4ee5\u68c0\u6d4b\u5bf9\u8c61\u5728BEV\u5e73\u9762\u4e0a\u7684\u4fe1\u53f7\u3002\u6700\u540e\uff0c\u4e24\u4e2a\u5206\u652f\u7684\u8f93\u51fa\u878d\u5408\uff0c\u751f\u6210\u9884\u6d4b\u76843D\u5360\u7528\u3002", "result": "ODG\u65b9\u6cd5\u5728Occ3D-nuScenes\u548cOcc3D-Waymo\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u4e14\u5177\u6709\u9ad8\u6548\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684ODG\u65b9\u6cd5\u5728Occ3D-nuScenes\u548cOcc3D-Waymo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5e76\u4e14\u4e0e\u6700\u65b0\u7684\u9ad8\u6548\u65b9\u6cd5\u76f8\u6bd4\uff0cODG\u8fd8\u63d0\u4f9b\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2506.09558", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09558", "abs": "https://arxiv.org/abs/2506.09558", "authors": ["Eleni Gkovedarou", "Joke Daems", "Luna De Bruyne"], "title": "Gender Bias in English-to-Greek Machine Translation", "comment": "Accepted at GITT 2025 (MT Summit)", "summary": "As the demand for inclusive language increases, concern has grown over the\nsusceptibility of machine translation (MT) systems to reinforce gender\nstereotypes. This study investigates gender bias in two commercial MT systems,\nGoogle Translate and DeepL, focusing on the understudied English-to-Greek\nlanguage pair. We address three aspects of gender bias: i) male bias, ii)\noccupational stereotyping, and iii) errors in anti-stereotypical translations.\nAdditionally, we explore the potential of prompted GPT-4o as a bias mitigation\ntool that provides both gender-explicit and gender-neutral alternatives when\nnecessary. To achieve this, we introduce GendEL, a manually crafted bilingual\ndataset of 240 gender-ambiguous and unambiguous sentences that feature\nstereotypical occupational nouns and adjectives. We find persistent gender bias\nin translations by both MT systems; while they perform well in cases where\ngender is explicitly defined, with DeepL outperforming both Google Translate\nand GPT-4o in feminine gender-unambiguous sentences, they are far from\nproducing gender-inclusive or neutral translations when the gender is\nunspecified. GPT-4o shows promise, generating appropriate gendered and neutral\nalternatives for most ambiguous cases, though residual biases remain evident.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86Google Translate\u548cDeepL\u5728\u82f1\u8bed\u5230\u5e0c\u814a\u8bed\u7ffb\u8bd1\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u867d\u7136\u5728\u76f4\u63a5\u5b9a\u4e49\u6027\u522b\u7684\u53e5\u5b50\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6027\u522b\u6a21\u7cca\u7684\u60c5\u51b5\u4e0b\u79bb\u6027\u522b\u5305\u5bb9\u6027\u7ffb\u8bd1\u8fd8\u6709\u5dee\u8ddd\u3002\u7814\u7a76\u8fd8\u521d\u6b65\u9a8c\u8bc1\u4e86GPT-4o\u4f5c\u4e3a\u504f\u89c1\u7f13\u89e3\u5de5\u5177\u7684\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u5bf9\u5305\u5bb9\u6027\u8bed\u8a00\u9700\u6c42\u7684\u589e\u957f\uff0c\u4eba\u4eec\u5bf9\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u7cfb\u7edf\u53ef\u80fd\u5f3a\u5316\u6027\u522b\u523b\u677f\u5370\u8c61\u7684\u62c5\u5fe7\u65e5\u76ca\u589e\u52a0\u3002\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u8c03\u67e5\u5546\u4e1aMT\u7cfb\u7edf\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u5e76\u63a2\u7d22\u51cf\u5c11\u8fd9\u7c7b\u504f\u89c1\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u624b\u52a8\u7f16\u5199\u7684\u53cc\u8bed\u6570\u636e\u96c6GendEL\uff0c\u5305\u542b240\u4e2a\u6027\u522b\u6a21\u7cca\u548c\u660e\u786e\u7684\u53e5\u5b50\uff0c\u6765\u8bc4\u4f30Google Translate\u548cDeepL\u5728\u82f1\u8bed\u5230\u5e0c\u814a\u8bed\u7ffb\u8bd1\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u6d4b\u8bd5\u4e86GPT-4o\u4f5c\u4e3a\u51cf\u5c11\u7ffb\u8bd1\u504f\u89c1\u5de5\u5177\u7684\u53ef\u80fd\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u4e24\u4e2aMT\u7cfb\u7edf\u5728\u76f4\u63a5\u5b9a\u4e49\u6027\u522b\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u672a\u6307\u5b9a\u6027\u522b\u65f6\uff0c\u5b83\u4eec\u79bb\u4ea7\u751f\u6027\u522b\u5305\u5bb9\u6216\u4e2d\u7acb\u7ffb\u8bd1\u4ecd\u6709\u4e00\u5b9a\u5dee\u8ddd\u3002GPT-4o\u5c55\u73b0\u4e86\u4e00\u5b9a\u7684\u6f5c\u529b\uff0c\u4f46\u5bf9\u4e8e\u5927\u591a\u6570\u6a21\u7cca\u60c5\u5883\u8fd8\u662f\u5b58\u5728\u6b8b\u7559\u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u5f97\u51fa\u7ed3\u8bba\uff0c\u73b0\u6709\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5728\u5904\u7406\u6027\u522b\u504f\u89c1\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u800cGPT-4o\u5c55\u73b0\u51fa\u4e86\u7f13\u89e3\u8fd9\u4e9b\u504f\u89c1\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u4ecd\u7136\u6709\u5f85\u4e8e\u8fdb\u4e00\u6b65\u51cf\u5c11\u5185\u90e8\u6b8b\u7559\u7684\u6027\u522b\u504f\u89c1\u3002"}}
{"id": "2506.09427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09427", "abs": "https://arxiv.org/abs/2506.09427", "authors": ["Yukang Feng", "Jianwen Sun", "Chuanhao Li", "Zizhen Li", "Jiaxin Ai", "Fanrui Zhang", "Yifan Chang", "Sizhuo Zhou", "Shenglin Zhang", "Yu Dai", "Kaipeng Zhang"], "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation", "comment": null, "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.", "AI": {"tldr": "Introduce InterSyn and SynJudge for improving the generation of interleaved image-text outputs in LMMs, with experimental studies showing improved quality and system performance.", "motivation": "To enhance LMMs' capability in generating tightly interleaved image-text outputs by creating a new, richly-instructional dataset and an evaluation tool.", "method": "Large Multimodal Models (LMMs) are improved with the introduction of InterSyn, a new dataset created using the Self-Evaluation with Iterative Refinement (SEIR) method. SynJudge, an evaluation model, is also introduced to assess the quality of interleaved multimodal outputs.", "result": "SEIR leads to higher quality datasets compared to unrefined processes, and LMMs trained on InterSyn show performance gains across all evaluation metrics.", "conclusion": "The SEIR method and InterSyn dataset significantly improve multimodal dataset quality and performance in LMMs, confirming their utility for advancing multimodal systems."}}
{"id": "2506.09560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09560", "abs": "https://arxiv.org/abs/2506.09560", "authors": ["Stefan Krsteski", "Matea Tashkovska", "Borjan Sazdov", "Hristijan Gjoreski", "Branislav Gerazov"], "title": "Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language", "comment": "Camera-ready version accepted at SlavNLP-2025@ACL", "summary": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data.", "AI": {"tldr": "Developed resources and a state-of-the-art model for Macedonian to enhance LLM proficiency, outperforming existing models in key benchmarks and receiving positive qualitative feedback.", "motivation": "The motivation is to address the limitations of LLM capabilities for underrepresented languages such as Macedonian and to enhance applications in regions where such languages are spoken.", "method": "We created several resources to facilitate the adoption of Large Language Models (LLMs) for Macedonian language proficiency. This includes the creation of the largest Macedonian corpus to date, totaling 3.5B words and a conversational instruction dataset, as well as a Macedonian evaluation suite. A domestic-yak model, an 8B-parameter state-of-the-art model, was trained on these resources.", "result": "The developed model, domestic-yak, outperformed all existing 8B-parameter models across various benchmarks, and showed performance on par with much larger models. It also received higher qualitative ratings from native speakers for grammatical correctness and cultural appropriateness.", "conclusion": "The research advances LLM proficiency in Macedonian and sets a foundation for progress in similarly underrepresented languages. All resources were openly released to encourage further research and application advancements."}}
{"id": "2506.09429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09429", "abs": "https://arxiv.org/abs/2506.09429", "authors": ["Swadhin Das", "Divyansh Mundra", "Priyanshu Dayal", "Raksha Sharma"], "title": "A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning", "comment": null, "summary": "Transformer-based models have achieved strong performance in remote sensing\nimage captioning by capturing long-range dependencies and contextual\ninformation. However, their practical deployment is hindered by high\ncomputational costs, especially in multi-modal frameworks that employ separate\ntransformer-based encoders and decoders. In addition, existing remote sensing\nimage captioning models primarily focus on high-level semantic extraction while\noften overlooking fine-grained structural features such as edges, contours, and\nobject boundaries. To address these challenges, a lightweight transformer\narchitecture is proposed by reducing the dimensionality of the encoder layers\nand employing a distilled version of GPT-2 as the decoder. A knowledge\ndistillation strategy is used to transfer knowledge from a more complex teacher\nmodel to improve the performance of the lightweight network. Furthermore, an\nedge-aware enhancement strategy is incorporated to enhance image representation\nand object boundary understanding, enabling the model to capture fine-grained\nspatial details in remote sensing images. Experimental results demonstrate that\nthe proposed approach significantly improves caption quality compared to\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Transformer\u67b6\u6784\u548c\u8fb9\u7f18\u611f\u77e5\u589e\u5f3a\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u9065\u611f\u56fe\u50cf\u63cf\u8ff0\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u548c\u7ec6\u7c92\u5ea6\u7ed3\u6784\u7279\u5f81\u5ffd\u7565\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u867d\u7136\u5728\u9065\u611f\u56fe\u50cf\u63cf\u8ff0\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u591a\u6a21\u6001\u6846\u67b6\u4e2d\u5355\u72ec\u91c7\u7528Transformer\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u5176\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u963b\u788d\u3002\u540c\u65f6\uff0c\u73b0\u6709\u6a21\u578b\u5f80\u5f80\u4fa7\u91cd\u4e8e\u9ad8\u5c42\u6b21\u7684\u8bed\u4e49\u63d0\u53d6\u800c\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u7684\u7ed3\u6784\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u51cf\u5c11\u7f16\u7801\u5668\u5c42\u7684\u7ef4\u5ea6\u5e76\u4f7f\u7528GPT-2\u7684\u7cbe\u70bc\u7248\u672c\u4f5c\u4e3a\u89e3\u7801\u5668\u6765\u5e94\u5bf9\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u8fb9\u7f18\u611f\u77e5\u589e\u5f3a\u7b56\u7565\u4ee5\u589e\u5f3a\u56fe\u50cf\u8868\u793a\u548c\u5bf9\u8c61\u8fb9\u754c\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u63cf\u8ff0\u7684\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7Transformer\u67b6\u6784\u4e0e\u8fb9\u7f18\u611f\u77e5\u589e\u5f3a\u7b56\u7565\u5728\u63d0\u5347\u9065\u611f\u56fe\u50cf\u63cf\u8ff0\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\u4e0e\u53ef\u884c\u6027\u3002"}}
{"id": "2506.09566", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09566", "abs": "https://arxiv.org/abs/2506.09566", "authors": ["Bla\u017e \u0160krlj", "Boshko Koloski", "Senja Pollak", "Nada Lavra\u010d"], "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "comment": "To-appear as a book chapter", "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "AI": {"tldr": "\u672c\u7bc7\u8c03\u67e5\u8bba\u6587\u5206\u6790\u4e86\u77e5\u8bc6\u56fe\u8c31(KGs)\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u534f\u540c\u6548\u5e94\uff0c\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u53ef\u6269\u5c55\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u63a2\u8ba8\u5c06KGs\u4e2d\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u878d\u5165LLMs\u4e2d\u4ee5\u63d0\u5347\u4e8b\u5b9e\u57fa\u7840\u548c\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc6\u522b\u8fd9\u4e00\u9886\u57df\u7684\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u77e5\u8bc6\u56fe\u8c31(KGs)\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u4e24\u5927\u7c7b\uff1a\u589e\u5f3aLLMs\u7684KG\uff0c\u4ee5\u53ca\u589e\u5f3aKGs\u7684LLMs\u3002", "result": "\u8bba\u6587\u8bc6\u522b\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u5e76\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u6574\u5408\u7684\u76f8\u4e92\u4f18\u52bf\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u795e\u7ecf\u7b26\u53f7\u7ed3\u5408\u3001\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u66f4\u65b0\u3001\u6570\u636e\u53ef\u9760\u6027\u53ca\u4f26\u7406\u8003\u91cf\uff0c\u63a8\u52a8\u667a\u80fd\u7cfb\u7edf\u7ba1\u7406\u66f4\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u4efb\u52a1\u3002"}}
{"id": "2506.09445", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09445", "abs": "https://arxiv.org/abs/2506.09445", "authors": ["Ayush Gupta", "Anirban Roy", "Rama Chellappa", "Nathaniel D. Bastian", "Alvaro Velasquez", "Susmit Jha"], "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision", "comment": null, "summary": "We address the problem of video question answering (video QA) with temporal\ngrounding in a weakly supervised setup, without any temporal annotations. Given\na video and a question, we generate an open-ended answer grounded with the\nstart and end time. For this task, we propose TOGA: a vision-language model for\nTemporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune\nTOGA to jointly generate the answer and the temporal grounding. We operate in a\nweakly supervised setup where the temporal grounding annotations are not\navailable. We generate pseudo labels for temporal grounding and ensure the\nvalidity of these labels by imposing a consistency constraint between the\nquestion of a grounding response and the response generated by a question\nreferring to the same temporal segment. We notice that jointly generating the\nanswers with the grounding improves performance on question answering as well\nas grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For\ngrounded QA, we consider the NExT-GQA benchmark which is designed to evaluate\nweakly supervised grounded question answering. For open-ended QA, we consider\nthe MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art\nperformance for both tasks on these benchmarks.", "AI": {"tldr": "TOGA\u6a21\u578b\u5728\u5f31\u76d1\u7763\u73af\u5883\u4e0b\u89e3\u51b3\u4e86\u89c6\u9891\u4e2d\u7684\u65f6\u5e8f\u5b9a\u4f4d\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5bfc\u8c03\u4f18\uff0c\u6a21\u578b\u53ef\u4ee5\u540c\u65f6\u751f\u6210\u95ee\u9898\u7b54\u6848\u548c\u65f6\u95f4\u5b9a\u4f4d\u4fe1\u606f\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u672c\u7814\u7a76\u81f4\u529b\u4e8e\u89e3\u51b3\u5728\u5f31\u76d1\u7763\u73af\u5883\u4e0b\u5bf9\u4e8e\u89c6\u9891\u95ee\u7b54\u7684\u65f6\u5e8f\u5b9a\u4f4d\u95ee\u9898\uff0c\u5373\u5728\u6ca1\u6709\u65f6\u95f4\u6233\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u4ece\u89c6\u9891\u4e2d\u5b9a\u4f4d\u76f8\u5173\u7684\u56de\u7b54\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTOGA\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u80fd\u591f\u5728\u6ca1\u6709\u65f6\u95f4\u6233\u6ce8\u91ca\u7684\u5f31\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0c\u4ece\u89c6\u9891\u4e2d\u751f\u6210\u5f00\u653e\u6027\u95ee\u9898\u7684\u7b54\u6848\u5e76\u9644\u5e26\u65f6\u95f4\u5b9a\u4f4d\u3002\u901a\u8fc7\u5f15\u5bfc\u8c03\u4f18\uff08instruct-tune\uff09\u8ba9TOGA\u53ef\u4ee5\u540c\u65f6\u751f\u6210\u7b54\u6848\u548c\u65f6\u95f4\u5b9a\u4f4d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u751f\u6210\u4f2a\u6807\u7b7e\u4ee5\u8fdb\u884c\u65f6\u95f4\u5b9a\u4f4d\uff0c\u5e76\u901a\u8fc7\u65bd\u52a0\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u786e\u4fdd\u8fd9\u4e9b\u6807\u7b7e\u7684\u6709\u6548\u6027\uff0c\u8be5\u7ea6\u675f\u8981\u6c42\u4e00\u4e2a\u65f6\u95f4\u7247\u6bb5\u4e2d\u7684\u95ee\u7b54\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "result": "\u6211\u4eec\u5728NExT-GQA\uff08\u9488\u5bf9\u5f31\u76d1\u7763\u73af\u5883\u4e0b\u7684\u95ee\u9898\u56de\u7b54\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff09\u4e0a\u8bc4\u4f30\u4e86TOGA\u5728\u65f6\u95f4\u5b9a\u4f4d\u95ee\u9898\u56de\u7b54\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5728MSVD-QA\u548cActivityNet-QA\u4e0a\u8bc4\u4f30\u4e86\u5176\u5728\u5f00\u653e\u6027\u95ee\u9898\u56de\u7b54\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTOGA\u5728\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u540c\u65f6\u751f\u6210\u7b54\u6848\u548c\u65f6\u95f4\u5b9a\u4f4d\u80fd\u591f\u63d0\u5347\u89c6\u9891\u95ee\u7b54\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6211\u4eec\u7684\u65b9\u6cd5TOGA\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09591", "abs": "https://arxiv.org/abs/2506.09591", "authors": ["Stefan Arnold"], "title": "Memorization in Language Models through the Lens of Intrinsic Dimension", "comment": null, "summary": "Language Models (LMs) are prone to memorizing parts of their data during\ntraining and unintentionally emitting them at generation time, raising concerns\nabout privacy leakage and disclosure of intellectual property. While previous\nresearch has identified properties such as context length, parameter size, and\nduplication frequency, as key drivers of unintended memorization, little is\nknown about how the latent structure modulates this rate of memorization. We\ninvestigate the role of Intrinsic Dimension (ID), a geometric proxy for the\nstructural complexity of a sequence in latent space, in modulating\nmemorization. Our findings suggest that ID acts as a suppressive signal for\nmemorization: compared to low-ID sequences, high-ID sequences are less likely\nto be memorized, particularly in overparameterized models and under sparse\nexposure. These findings highlight the interaction between scale, exposure, and\ncomplexity in shaping memorization.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5185\u5728\u7ef4\u5ea6\uff08ID\uff09\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u8bb0\u5fc6\u5316\u6709\u6291\u5236\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9ad8ID\u5e8f\u5217\uff0c\u5728\u8fc7\u5ea6\u53c2\u6570\u5316\u6a21\u578b\u548c\u7a00\u758f\u66dd\u5149\u7684\u60c5\u51b5\u4e0b\uff0c\u8bb0\u5fc6\u5316\u7684\u53ef\u80fd\u6027\u66f4\u4f4e\u3002", "motivation": "\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u786e\u5b9a\u4e86\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u53c2\u6570\u5927\u5c0f\u548c\u91cd\u590d\u9891\u7387\u7b49\u56e0\u7d20\u5bf9\u975e\u9884\u671f\u8bb0\u5fc6\u5316\u7684\u5f71\u54cd\uff0c\u4f46\u5173\u4e8e\u6f5c\u5728\u7ed3\u6784\u662f\u5982\u4f55\u8c03\u8282\u8bb0\u5fc6\u5316\u7684\u901f\u7387\u77e5\u4e4b\u751a\u5c11\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e00\u70b9\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e86\u5185\u5728\u7ef4\u5ea6\uff08ID\uff09\u4f5c\u4e3a\u91cf\u5ea6\u5e8f\u5217\u5728\u9690\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u6027\u590d\u6742\u5ea6\u7684\u51e0\u4f55\u4ee3\u7406\uff0c\u6765\u7814\u7a76\u5176\u5982\u4f55\u8c03\u8282\u8bed\u8a00\u6a21\u578b\u7684\u8bb0\u5fc6\u5316\u8fc7\u7a0b\u3002", "result": "\u6211\u4eec\u53d1\u73b0\u5185\u5728\u7ef4\u5ea6\uff08ID\uff09\u662f\u6291\u5236\u8bb0\u5fc6\u5316\u7684\u4fe1\u53f7\uff1a\u4e0e\u4f4eID\u5e8f\u5217\u76f8\u6bd4\uff0c\u9ad8ID\u5e8f\u5217\u4e0d\u592a\u53ef\u80fd\u88ab\u8bb0\u4f4f\uff0c\u5c24\u5176\u662f\u5728\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u6a21\u578b\u548c\u7a00\u758f\u66dd\u5149\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u89c4\u6a21\u3001\u66dd\u5149\u548c\u590d\u6742\u6027\u5728\u5851\u9020\u8bb0\u5fc6\u5316\u8fc7\u7a0b\u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2506.09446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09446", "abs": "https://arxiv.org/abs/2506.09446", "authors": ["Yuhe Ding", "Jian Liang", "Bo Jiang", "Zi Wang", "Aihua Zheng", "Bin Luo"], "title": "Harmonizing and Merging Source Models for CLIP-based Domain Generalization", "comment": null, "summary": "CLIP-based domain generalization aims to improve model generalization to\nunseen domains by leveraging the powerful zero-shot classification capabilities\nof CLIP and multiple source datasets. Existing methods typically train a single\nmodel across multiple source domains to capture domain-shared information.\nHowever, this paradigm inherently suffers from two types of conflicts: 1)\nsample conflicts, arising from noisy samples and extreme domain shifts among\nsources; and 2) optimization conflicts, stemming from competition and\ntrade-offs during multi-source training. Both hinder the generalization and\nlead to suboptimal solutions. Recent studies have shown that model merging can\neffectively mitigate the competition of multi-objective optimization and\nimprove generalization performance. Inspired by these findings, we propose\nHarmonizing and Merging (HAM), a novel source model merging framework for\nCLIP-based domain generalization. During the training process of the source\nmodels, HAM enriches the source samples without conflicting samples, and\nharmonizes the update directions of all models. Then, a redundancy-aware\nhistorical model merging method is introduced to effectively integrate\nknowledge across all source models. HAM comprehensively consolidates source\ndomain information while enabling mutual enhancement among source models,\nultimately yielding a final model with optimal generalization capabilities.\nExtensive experiments on five widely used benchmark datasets demonstrate the\neffectiveness of our approach, achieving state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u591a\u6e90\u57df\u6cdb\u5316\u65b9\u6cd5\u2014\u2014Harmonizing and Merging (HAM)\uff0c\u53ef\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u672a\u89c1\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u591a\u6e90\u9886\u57df\u65f6\u5b58\u5728\u7684\u6837\u672c\u51b2\u7a81\u548c\u4f18\u5316\u51b2\u7a81\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faHarmonizing and Merging (HAM)\uff0c\u4e00\u79cd\u6e90\u6a21\u578b\u5408\u5e76\u6846\u67b6\uff0c\u7528\u4e8eCLIP\u9886\u57df\u7684\u591a\u6e90\u6cdb\u5316\u3002\u8be5\u6846\u67b6\u9996\u5148\u5728\u6e90\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u907f\u514d\u51b2\u7a81\u6837\u672c\uff0c\u8c03\u8282\u6a21\u578b\u66f4\u65b0\u65b9\u5411\uff0c\u63a5\u7740\u4f7f\u7528\u5197\u4f59\u611f\u77e5\u7684\u5386\u53f2\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u6574\u5408\u6240\u6709\u6e90\u6a21\u578b\u7684\u77e5\u8bc6\u3002", "result": "CLIP-based\u9886\u57df\u6cdb\u5316\u65e8\u5728\u5229\u7528CLIP\u7684\u5f3a\u5927\u96f6\u6837\u672c\u5206\u7c7b\u80fd\u529b\u548c\u591a\u4e2a\u6e90\u6570\u636e\u96c6\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u591a\u4e2a\u6e90\u9886\u57df\u4e0a\u8bad\u7ec3\u5355\u4e2a\u6a21\u578b\u4ee5\u6355\u83b7\u9886\u57df\u5171\u4eab\u4fe1\u606f\uff0c\u4f46\u8be5\u8303\u5f0f\u5185\u5728\u5730\u5b58\u5728\u4e24\u79cd\u51b2\u7a81\uff1a1) \u6837\u672c\u51b2\u7a81\uff0c\u6e90\u4e8e\u6837\u672c\u566a\u58f0\u548c\u6e90\u4e4b\u95f4\u7684\u6781\u7aef\u9886\u57df\u53d8\u5316\uff1b2) \u4f18\u5316\u51b2\u7a81\uff0c\u6765\u81ea\u591a\u6e90\u8bad\u7ec3\u65f6\u7684\u7ade\u4e89\u548c\u6743\u8861\u3002\u8fd9\u4e9b\u51b2\u7a81\u963b\u788d\u4e86\u6cdb\u5316\u5e76\u5bfc\u81f4\u6b21\u4f18\u89e3\u3002\u8fd1\u671f\u7814\u7a76\u8868\u660e\uff0c\u6a21\u578b\u5408\u5e76\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u7ade\u4e89\uff0c\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Harmonizing and Merging (HAM)\uff0c\u4e00\u79cd\u57fa\u4e8eCLIP\u9886\u57df\u7684\u65b0\u578b\u6e90\u6a21\u578b\u5408\u5e76\u6846\u67b6\u3002\u5728\u6e90\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cHAM\u4e30\u5bcc\u6837\u672c\uff0c\u907f\u514d\u51b2\u7a81\u6837\u672c\uff0c\u8c03\u8282\u6240\u6709\u6a21\u578b\u7684\u66f4\u65b0\u65b9\u5411\u3002\u63a5\u7740\u5f15\u5165\u5197\u4f59\u611f\u77e5\u7684\u5386\u53f2\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff0c\u6709\u6548\u6574\u5408\u6240\u6709\u6e90\u6a21\u578b\u7684\u77e5\u8bc6\u3002HAM\u5168\u9762\u6574\u5408\u6e90\u9886\u57df\u4fe1\u606f\uff0c\u540c\u65f6\u5b9e\u73b0\u6e90\u6a21\u578b\u95f4\u76f8\u4e92\u589e\u5f3a\uff0c\u6700\u7ec8\u751f\u6210\u5177\u6709\u6700\u4f18\u6cdb\u5316\u80fd\u529b\u7684\u6a21\u578b\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e94\u4e2a\u5e38\u7528\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\uff0cHAM\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728CLIP-based\u9886\u57df\u6cdb\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.09627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09627", "abs": "https://arxiv.org/abs/2506.09627", "authors": ["Nicolas Audinet de Pieuchon", "Adel Daoud", "Connor T. Jerzak", "Moa Johansson", "Richard Johansson"], "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates", "comment": null, "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
