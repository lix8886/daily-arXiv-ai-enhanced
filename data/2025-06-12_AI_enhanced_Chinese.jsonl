{"id": "2506.09147", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09147", "abs": "https://arxiv.org/abs/2506.09147", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perli\u0107", "Ekaterina Borisova", "Markarit Vartampetian"], "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLM-as-a-qualitative-judge\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5176\u4e3b\u8981\u8f93\u51fa\u662f\u5bf9NLG\u7cfb\u7edf\u8f93\u51fa\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\u7c7b\u578b\u7684\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u8bc6\u522b\u7279\u5b9a\u5b9e\u4f8b\u95ee\u9898\u7684\u60c5\u51b5\u3002", "motivation": "\u5f53\u524d\uff0c\u5c06LLM\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u6587\u672c\u7684\u65b9\u6cd5\u4e3b\u8981\u662f\u5b9a\u91cf\u5de5\u5177\uff0c\u4e3b\u8981\u8f93\u51fa\u662f\u6570\u503c\u5206\u6570\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u76ee\u6807\u63d0\u4f9b\u6709\u5173\u5982\u4f55\u6539\u8fdb\u7ed9\u5b9a\u7684NLG\u7cfb\u7edf\u7684\u6709\u610f\u4e49\u89c1\u89e3\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a\u5f00\u653e\u5f0f\u9010\u5b9e\u4f8b\u95ee\u9898\u5206\u6790\u548c\u4f7f\u7528\u76f4\u89c2\u7684\u7d2f\u79ef\u7b97\u6cd5\u5bf9\u53d1\u73b0\u7684\u95ee\u9898\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLM-as-a-qualitative-judge\u57282/3\u7684\u60c5\u51b5\u4e0b\u6b63\u786e\u5730\u8bc6\u522b\u4e86\u5b9e\u4f8b\u7279\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u80fd\u591f\u4ea7\u751f\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u6807\u6ce8\u5458\u7f16\u5236\u7684\u9519\u8bef\u7c7b\u578b\u62a5\u544a\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86LLM\u9664\u4e86\u4f5c\u4e3a\u5b9a\u91cf\u8bc4\u4f30\u5de5\u5177\u5916\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5b9a\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u95ee\u9898\u62a5\u544a\uff0c\u8fdb\u800c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u548c\u6539\u8fdbNLG\u7cfb\u7edf\u3002"}}
{"id": "2506.09175", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09175", "abs": "https://arxiv.org/abs/2506.09175", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u77ed\u8bed\u5728\u8bed\u97f3\u7ffb\u8bd1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\uff0c\u76f8\u8f83\u4e8e\u77ed\u8bed\u5217\u8868\u504f\u7f6e\u65b9\u6cd5\uff0c\u63d0\u5347\u4e8621%\u7684\u76f8\u5bf9\u6027\u80fd\uff0c\u5e76\u4f7f\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77ed\u8bed\u53ec\u56de\u7387\u63d0\u9ad8\u4e8685%\u3002", "motivation": "\u77ed\u8bed\u5bf9\u4e8e\u7406\u89e3\u5bf9\u8bdd\u4e2d\u7684\u6838\u5fc3\u6982\u5ff5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7f55\u89c1\u51fa\u73b0\uff0c\u77ed\u8bed\u7684\u51c6\u786e\u7ffb\u8bd1\u5728\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u9887\u5177\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u4ece\u6e90\u8bed\u8a00\u5230\u76ee\u6807\u8bed\u8a00\u7684\u77ed\u8bed\u6620\u5c04\u5bf9\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u5230\u4e86\u4e24\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6a21\u578b\u4e2d\uff1a\u57fa\u4e8e\u8f6c\u5bfc\u5668\u7684\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u65b9\u6cd5\u6bd4\u77ed\u8bed\u5217\u8868\u504f\u7f6e\u65b9\u6cd5\u76f8\u5bf9\u63d0\u9ad8\u4e8621%\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u4f7f\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4f7f\u7528\u5916\u90e8\u77ed\u8bed\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e8685%\u7684\u76f8\u5bf9\u53ec\u56de\u7387\u63d0\u5347\u3002", "conclusion": "\u77ed\u8bed\u5b57\u5178\u504f\u7f6e\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u8bed\u7ffb\u8bd1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09218", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09218", "abs": "https://arxiv.org/abs/2506.09218", "authors": ["Bruno Ferenc \u0160egedin"], "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "AI": {"tldr": "A study examining the capacity of CNNs to generate phonotactic generalizations from raw audio data finds that, with a narrow fully-connected layer, convolutional layers can dynamically generalize phonetic dependencies beyond the specific learned lexical configurations.", "motivation": "The motivation of this study is to understand if DNNs, particularly CNNs trained on audio waveforms, can make generalizations about phonotactics that go beyond the specific instances of words they've been trained on, and to examine how the architecture of the neural network influences this ability.", "method": "This study uses generative convolutional neural networks (CNNs) trained on raw audio waveforms of lexical items. It explores the impact of a narrow fully-connected layer (FC) bottleneck (shrunk from 1024 channels to 8) on the model's ability to generalize phonotactic rules derived from lexical learning.", "result": "The study finds that the convolutional layers can dynamically generalize phonetic dependencies beyond lexically-constrained configurations. A novel technique for probing the model's lexically-independent generalizations was proposed, which bypasses the FC layer and uses randomized feature maps fed into the convolutional block to generate audio outputs.", "conclusion": "The results suggest that generative convolutional neural networks are capable of learning and applying phonotactic generalizations beyond the specific lexical items they are trained on, especially when the fully-connected layer is constrained."}}
{"id": "2506.09251", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09251", "abs": "https://arxiv.org/abs/2506.09251", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4efb\u52a1\u5173\u8054\u5c55\u793a\u4e86Transformer\u6a21\u578b\u7684\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u53ef\u4ee5\u8de8\u4efb\u52a1\u8f6c\u79fb\uff0c\u5e76\u63d0\u4f9b\u4e86\u673a\u5236\u4e0a\u7684\u521d\u6b65\u8bc1\u636e\u3002", "motivation": "\u63a2\u8ba8Transformer\u8bed\u8a00\u6a21\u578b\u5904\u7406\u66f4\u957f\u8f93\u5165\u7684\u80fd\u529b\u662f\u5982\u4f55\u5f62\u6210\u7684\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u8fc7\u7684\u66f4\u957f\u8f93\u5165\u4e0a\u5982\u4f55\u6cdb\u5316\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u5173\u8054\u7684\u89c6\u89d2\u7814\u7a76\u4e86\u6a21\u578b\u5904\u7406\u66f4\u957f\u8f93\u5165\u7684\u80fd\u529b\uff0c\u5373\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u8868\u660e\uff0c\u8bad\u7ec3\u6a21\u578b\u65f6\u4f7f\u7528\u4e00\u4e2a\u8f83\u957f\u4e14\u76f8\u5173\u7684\u8f85\u52a9\u4efb\u52a1\uff0c\u53ef\u4ee5\u4f7f\u6a21\u578b\u5728\u5176\u4ed6\u76ee\u6807\u4efb\u52a1\u4e2d\u66f4\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u66f4\u957f\u8f93\u5165\u3002", "result": "\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u80fd\u591f\u5728\u4e0d\u540c\u7684\u7b97\u6cd5\u4efb\u52a1\u4e4b\u95f4\u8f6c\u79fb\uff0c\u5305\u62ec\u7b97\u672f\u8fd0\u7b97\u3001\u5b57\u7b26\u4e32\u8f6c\u6362\u548c\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\u3002\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u4e5f\u89c2\u5bdf\u5230\u4e86\u7c7b\u4f3c\u7684\u8f6c\u79fb\u6548\u679c\u3002", "conclusion": "\u6a21\u578b\u53ef\u4ee5\u7ee7\u627f\u6765\u81ea\u76f8\u4f3c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u957f\u5ea6\u6cdb\u5316\u7684\u8f6c\u79fb\u4e0e\u4efb\u52a1\u95f4\u6ce8\u610f\u529b\u5934\u7684\u91cd\u7528\u6709\u5173\uff0c\u8fd9\u52a0\u6df1\u4e86\u5bf9\u6a21\u578b\u5982\u4f55\u6cdb\u5316\u5230\u5206\u5e03\u5916\u8f93\u5165\u7684\u7406\u89e3\u3002"}}
{"id": "2506.09066", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09066", "abs": "https://arxiv.org/abs/2506.09066", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "comment": null, "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "AI": {"tldr": "ReStNet solves the problem of deploying pre-trained models in IoT with varying resources by stitching two models together and selectively fine-tuning them for efficiency and adaptability.", "motivation": "The motivation behind ReStNet is to address resource constraints in IoT applications, where deploying a single model across all platforms with heterogeneous computational and memory resources is not feasible and existing compression methods are inflexible.", "method": "ReStNet, a Reusable and Stitchable Network, dynamically constructs a hybrid network by stitching two pre-trained models together, using Centered Kernel Alignment (CKA) for determining stitching points, and fine-tuning only the stitching layer for efficiency.", "result": "ReStNet demonstrates flexible accuracy-efficiency trade-offs at runtime and significantly reduces training cost through its novel approach to stitching models.", "conclusion": "ReStNet enables rapid adaptation to changing resource budgets and flexible combination of different model families, making it suitable for efficient deployment in varying IoT environments."}}
{"id": "2506.09259", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2506.09259", "abs": "https://arxiv.org/abs/2506.09259", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u81ea\u6211\u951a\u5b9a\u6ce8\u610f\u529b\u6a21\u578b\uff08SAAM\uff09\u6765\u8bc6\u522b\u5e76\u5206\u7c7b\u6e38\u620f\u4e2d\u4eb2\u793e\u4f1a\u884c\u4e3a\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u300a\u4f7f\u547d\u53ec\u5524\uff1a\u73b0\u4ee3\u6218\u4e89II\u300b\u4e2d\uff0c\u63d0\u9ad8\u4e86\u4eb2\u793e\u4f1a\u6c9f\u901a\u884c\u4e3a\u7684\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u68c0\u6d4b\u6e38\u620f\u4e2d\u5c11\u91cf\u7684\u6709\u6bd2\u5185\u5bb9\u4ee5\u5b9e\u73b0\u7ba1\u7406\u76ee\u7684\uff0c\u800c\u65b0\u5174\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u68c0\u6d4b\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u91cd\u8981\u6027\u3002\u7136\u800c\uff0c\u5728\u6e38\u620f\u804a\u5929\u6587\u672c\u4e2d\u8bc6\u522b\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8d44\u6e90\u76f8\u5f53\u532e\u4e4f\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u79ef\u6781\u4e92\u52a8\u7684\u9f13\u52b1\u3002", "method": "\u91c7\u7528\u4e86\u65e0\u76d1\u7763\u7684\u53d1\u73b0\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u6e38\u620f\u9886\u57df\u4e13\u5bb6\u7684\u5408\u4f5c\uff0c\u4ece\u6e38\u620f\u4e2d\u73a9\u5bb6\u7684\u804a\u5929\u8bb0\u5f55\u4e2d\u8bc6\u522b\u548c\u5206\u7c7b\u4eb2\u793e\u4f1a\u884c\u4e3a\u3002\u63d0\u51fa\u4e86\u81ea\u6211\u951a\u5b9a\u6ce8\u610f\u529b\u6a21\u578b\uff08Self-Anchored Attention Model\uff0cSAAM\uff09\uff0c\u8be5\u6a21\u578b\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u4f73\u6280\u672f\u63d0\u9ad8\u4e867.9%\u3002\u901a\u8fc7\u4f7f\u7528\u6574\u4e2a\u8bad\u7ec3\u96c6\u4f5c\u4e3a\u201c\u951a\u70b9\u201d\u6765\u5e2e\u52a9\u5728\u7f3a\u4e4f\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6210\u679c\u5305\u62ec\u63d0\u51fa\u4e86\u81ea\u6211\u951a\u5b9a\u6ce8\u610f\u529b\u6a21\u578b\uff08SAAM\uff09\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u4f73\u6280\u672f\u63d0\u9ad8\u4e867.9%\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u81ea\u52a8\u5206\u7c7b\u3002\u7814\u7a76\u5c55\u793a\u4e86\u5728\u4ec5\u6709\u5c11\u91cf\u5316\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u4ecd\u7136\u6709\u6548\u3002", "conclusion": "\u8fd9\u7bc7\u7814\u7a76\u662f\u9996\u6b21\u5c06NLP\u6280\u672f\u5e94\u7528\u4e8e\u53d1\u73b0\u548c\u5206\u7c7b\u73a9\u5bb6\u5728\u6e38\u620f\u4e2d\u7684\u4eb2\u793e\u4f1a\u6c9f\u901a\u884c\u4e3a\u7684\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u4ece\u4ec5\u5173\u6ce8\u51cf\u5c11\u6d88\u6781\u5185\u5bb9\u5230\u4e3b\u52a8\u9f13\u52b1\u79ef\u6781\u4e92\u52a8\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.09067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09067", "abs": "https://arxiv.org/abs/2506.09067", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "comment": null, "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u62b5\u5fa1\u89c6\u89c9\u548c\u6587\u672c\u653b\u51fb\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u540c\u65f6\u63d0\u9ad8Med-VLMs\u7684\u5b89\u5168\u6027\u800c\u4e0d\u4f1a\u663e\u8457\u964d\u4f4e\u5176\u6027\u80fd\u3002", "motivation": "Med-VLMs\u7684\u5b89\u5168\u6f0f\u6d1e\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\uff0c\u8ba9\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u62d2\u7edd\u6709\u5bb3\u67e5\u8be2\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u9632\u5fa1\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u7406\u65f6\u9632\u5fa1\u7b56\u7565\uff0c\u80fd\u591f\u62b5\u5fa1\u89c6\u89c9\u548c\u6587\u672c\u5f62\u5f0f\u7684\u653b\u51fb\uff0c\u901a\u8fc7\u589e\u52a0\u5408\u6210\u4e34\u5e8a\u6f14\u793a\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6df7\u5408\u5c55\u793a\u7b56\u7565\uff0c\u65e8\u5728\u9884\u7b97\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5e73\u8861\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u9632\u5fa1\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u6df7\u5408\u5c55\u793a\u7b56\u7565\u5728\u9762\u5bf9\u6709\u9650\u9884\u7b97\u65f6\u4e5f\u80fd\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\uff0c\u901a\u8fc7\u5408\u6210\u4e34\u5e8a\u6f14\u793a\u589e\u91cf\uff0c\u63d0\u51fa\u7684\u9632\u5fa1\u7b56\u7565\u53ef\u4ee5\u589e\u5f3aMed-VLMs\u7684\u5b89\u5168\u6027\u800c\u4e0d\u663e\u8457\u635f\u5bb3\u6027\u80fd\u3002\u589e\u52a0\u6f14\u793a\u9884\u7b97\u80fd\u8fdb\u4e00\u6b65\u89e3\u51b3\u8fc7\u5ea6\u9632\u5fa1\u95ee\u9898\u3002"}}
{"id": "2506.09277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09277", "abs": "https://arxiv.org/abs/2506.09277", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE.", "AI": {"tldr": "The paper develops a framework to measure the faithfulness of LLM self-explanations by comparing them to the model's internal processes, aiming to enhance the reliability of these explanations and foster future improvements.", "motivation": "The motivation arises from the fact that current LLM-generated self-NLEs can be logically coherent but not truly reflective of the model's actual decision-making process, leading to unfaithful explanations. This study aims to fill the gap in understanding and improving the faithfulness of these explanations.", "method": "This paper proposes a flexible framework to measure the faithfulness of LLM-generated self-Natural Language Explanations (self-NLE) by comparing self-NLE with the interpretations of the model's internal neural activities, thereby offering deep insights into the connection between self-NLE and the model's reasoning process.", "result": "The paper establishes a novel approach for quantifying self-NLE faithfulness which connects the explanations to the model's internal reasoning, advancing the understanding of self-NLE authenticity and laying groundwork for the generation of more reliable self-NLE in the future.", "conclusion": "The work concludes with a robust framework for assessing self-NLE faithfulness in LLMs by linking the generated explanations to the model's internal neural states, paving the way for more faithful self-NLEs in the future."}}
{"id": "2506.09068", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09068", "abs": "https://arxiv.org/abs/2506.09068", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86BG-HOP\uff0c\u4e00\u79cd\u65e8\u5728\u5efa\u6a213D\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\u7684\u751f\u6210\u5148\u9a8c\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6355\u6349\u624b\u548c\u7269\u8054\u5408\u5206\u5e03\u7684\u521d\u6b65\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u5c55\u73b0\u6709\u7684\u5355\u624b\u751f\u6210\u5148\u9a8c\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86BG-HOP\u6a21\u578b\u6765\u751f\u6210\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\uff0c\u5e76\u4e3a\u7ed9\u5b9a\u7269\u4f53\u5408\u6210\u6293\u53d6\u52a8\u4f5c\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\uff0c\u5e76\u4e3a\u7ed9\u5b9a\u7269\u4f53\u5408\u6210\u6293\u63e1\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86BG-HOP\u5728\u751f\u6210\u53cc\u624b\u52a8-\u7269\u4e92\u52a8\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6a21\u578b\u3002"}}
{"id": "2506.09301", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09301", "abs": "https://arxiv.org/abs/2506.09301", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86$(RSA)^2$\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u8bf4\u8bdd\u4eba\u7684\u4fee\u8f9e\u7b56\u7565\u6765\u89e3\u91ca\u6bd4\u55bb\u8bed\u8a00\uff0c\u907f\u514d\u4e86\u5bf9\u52a8\u673a\u7684\u7279\u5b9a\u573a\u666f\u5316\u5efa\u6a21\uff0c\u5e76\u5728\u8bbd\u523a\u8bed\u8a00\u7406\u89e3\u4e0a\u8fbe\u5230\u4e86\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684RSA\u6846\u67b6\u65e0\u6cd5\u89e3\u91ca\u6bd4\u55bb\u8868\u8fbe\uff0c\u6216\u8005\u9700\u8981\u4ee5\u7279\u5b9a\u573a\u666f\u7684\u65b9\u5f0f\u6a21\u62df\u8bf4\u8bdd\u4eba\u4f7f\u7528\u6bd4\u55bb\u8bed\u8a00\u7684\u9690\u542b\u52a8\u673a\u3002\u4e3a\u4e86\u4eba\u7c7b\u517c\u5bb9\u5730\u89e3\u91ca\u975e\u5b57\u9762\u8bed\u8a00\uff0c\u672c\u6587\u63d0\u51fa\u4e86$(RSA)^2$\u6846\u67b6\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86$(RSA)^2$\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u8003\u8651\u8bf4\u8bdd\u4eba\u4f7f\u7528\u7684\u4fee\u8f9e\u7b56\u7565\u6765\u5efa\u6a21\u6bd4\u55bb\u8bed\u8a00\u7684\u4f7f\u7528\u3002", "result": "$(RSA)^2$\u6846\u67b6\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5728\u65b0\u5f15\u5165\u7684PragMega+\u6570\u636e\u96c6\u7684\u8bbd\u523a\u5206\u5272\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86$(RSA)^2$\u6846\u67b6\u5728\u7406\u89e3\u975e\u5b57\u9762\u8bed\u8a00\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u65e0\u9700\u6a21\u62df\u8bf4\u8bdd\u4eba\u4f7f\u7528\u975e\u5b57\u9762\u8bed\u8a00\u7684\u5177\u4f53\u52a8\u673a\uff0c\u5373\u53ef\u5b9e\u73b0\u4eba\u7c7b\u517c\u5bb9\u7684\u89e3\u91ca\u3002"}}
{"id": "2506.09071", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09071", "abs": "https://arxiv.org/abs/2506.09071", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "comment": null, "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8bed\u4e49\u5f15\u5bfc\u7684\u5efa\u7b51\u5916\u5899\u548c\u7a97\u6237\u5206\u5272\u6a21\u578bSAAF\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u6587\u672c\u63cf\u8ff0\u5230\u56fe\u50cf\u5206\u5272\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5e76\u5728\u591a\u4e2a\u5916\u5899\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u5728\u5efa\u7b51\u6570\u5b57\u5316\u53d1\u5c55\u80cc\u666f\u4e0b\uff0c\u5efa\u7b51\u7269\u4fe1\u606f\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\u7684\u6548\u7387\u53ef\u4ee5\u901a\u8fc7\u81ea\u52a8\u5206\u5272\u5899\u4f53\u548c\u7a97\u6237\u5f97\u5230\u6709\u6548\u63d0\u5347\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8bed\u4e49\u5f15\u5bfc\u7684\u5efa\u7b51\u5916\u5899\u548c\u7a97\u6237\u81ea\u52a8\u5206\u5272\u6a21\u578bSegment Any Architectural Facades (SAAF)\u3002\u9996\u5148\uff0cSAAF\u5177\u5907\u591a\u6a21\u6001\u8bed\u4e49\u534f\u540c\u7279\u5f81\u63d0\u53d6\u673a\u5236\uff0c\u53ef\u4ee5\u878d\u5408\u6587\u672c\u63cf\u8ff0\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u548c\u56fe\u50cf\u7279\u5f81\uff0c\u63d0\u9ad8\u5bf9\u5efa\u7b51\u5916\u5899\u6784\u4ef6\u7684\u8bed\u4e49\u7406\u89e3\u3002\u5176\u6b21\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u4ece\u6587\u672c\u63cf\u8ff0\u5230\u56fe\u50cf\u5206\u5272\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u81ea\u52a8\u5316\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u5916\u5899\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cSAAF\u7684\u5206\u5272\u7ed3\u679c\u5728mIoU\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u8868\u660e\u8be5\u6a21\u578b\u5728\u9762\u5bf9\u591a\u6837\u5316\u6570\u636e\u96c6\u65f6\u80fd\u591f\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u5206\u5272\u80fd\u529b\u3002", "conclusion": "SAAF\u6a21\u578b\u5728\u63d0\u9ad8\u5899\u4f53\u548c\u7a97\u6237\u5206\u5272\u4efb\u52a1\u7684\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u8fdb\u5c55\uff0c\u53ef\u4ee5\u63d0\u4f9b\u5173\u4e8e\u5efa\u7b51\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u548c\u591a\u6a21\u6001\u5b66\u4e60\u5728\u5efa\u7b51\u9886\u57df\u7684\u65b0\u601d\u8def\u548c\u6280\u672f\u8def\u5f84\u53c2\u8003\u3002"}}
{"id": "2506.09315", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09315", "abs": "https://arxiv.org/abs/2506.09315", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Mistral-7B\u6a21\u578b\u589e\u5f3a\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u8bed\u8a00\u68c0\u6d4b\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6e05\u6670\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u662f\u4e00\u79cd\u5f71\u54cd\u8ba4\u77e5\u80fd\u529b\u7684\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff0c\u901a\u5e38\u4f1a\u5f71\u54cd\u8bed\u8a00\u80fd\u529b\u3002\u8fd9\u9879\u7814\u7a76\u7684\u76ee\u7684\u662f\u63d0\u9ad8\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e86Mistral-7B\u6307\u4ee4\u8ddf\u968f\u7248\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u6210\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\u3002", "result": "\u76f8\u6bd4\u4e8e\u5f53\u524d\u6700\u4f73\u6210\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\u548cADReSS 2020\u6311\u6218\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u597d\u7684\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e863.33%\u548c6.35%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff0c\u5e76\u4e14\u5177\u6709\u6e05\u6670\u4e14\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u6a21\u578b\u5df2\u7ecf\u5b66\u4f1a\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u60a3\u8005\u7279\u6709\u7684\u8bed\u8a00\u6a21\u5f0f\uff0c\u4e3a\u6a21\u578b\u89e3\u91ca\u548c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.09079", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09079", "abs": "https://arxiv.org/abs/2506.09079", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "comment": null, "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "AI": {"tldr": "This paper introduces VersaVid-R1, a new model for video reasoning, using innovative datasets and reinforcement learning.", "motivation": "The motivation behind this paper is to address the underdeveloped field of video-based reasoning due to data scarcity and ineffective training methodologies.", "method": "The paper develops a new model named VersaVid-R1 by leveraging two novel datasets, DarkEventInfer and MixVidQA, and training with reinforcement learning guided by diverse reward functions.", "result": "Experiments show that VersaVid-R1 surpasses existing models across various benchmarks, including video understanding, cognitive reasoning, and captioning tasks.", "conclusion": "The conclusion is that the developed model, VersaVid-R1, represents a significant progress in video understanding and reasoning under the Reason-Then-Respond paradigm."}}
{"id": "2506.09329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09329", "abs": "https://arxiv.org/abs/2506.09329", "authors": ["Yuxin Jiang"], "title": "Towards Efficient and Effective Alignment of Large Language Models", "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "AI": {"tldr": "The paper presents new methodologies for data collection, training, and evaluation to improve LLM alignment with human expectations, introducing frameworks like Lion, WebR, LTE, and BMC and the FollowBench for constraint adherence.", "motivation": "To efficiently and effectively align LLMs with human expectations.", "method": "Lion, an adversarial distillation framework for alignment data collection; Web Reconstruction (WebR), a fully automated framework for synthesizing instruction-tuning data; Learning to Edit (LTE), a framework for efficient knowledge integration; Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) for better alignment across QA and mathematical reasoning tasks.", "result": "State-of-the-art zero-shot reasoning with Lion; improved data diversity and scalability with WebR; improved real-time and batch knowledge updates with LTE; superior alignment across QA and mathematical reasoning tasks with BMC; key weaknesses in current models' constraint adherence identified with FollowBench.", "conclusion": "The paper proposes and evaluates several novel methodologies in data collection, training, and evaluation for LLM alignment, significantly advancing the state-of-the-art in zero-shot reasoning, knowledge integration, and constraint adherence."}}
{"id": "2506.09081", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09081", "abs": "https://arxiv.org/abs/2506.09081", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "AI": {"tldr": "FlagEvalMM is an open-source evaluation framework for multimodal models, designed to assess various vision-language tasks efficiently and accurately.", "motivation": "The motivation is to provide a comprehensive evaluation framework for multimodal models that can assess vision-language understanding and generation tasks with improved efficiency and accuracy.", "method": "We decouple model inference from evaluation through an independent evaluation service, enabling flexible resource allocation and seamless integration of new tasks and models. FlagEvalMM uses advanced inference acceleration tools and asynchronous data loading to enhance evaluation efficiency.", "result": "Extensive experiments demonstrate that FlagEvalMM provides accurate and efficient insights into the performance of multimodal models.", "conclusion": "FlagEvalMM is a valuable tool for advancing research in multimodal models, offering a framework for accurate and efficient model evaluation."}}
{"id": "2506.09331", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.09331", "abs": "https://arxiv.org/abs/2506.09331", "authors": ["Arjun Vaithilingam Sudhakar"], "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "AI": {"tldr": "\u7814\u7a76LLMs\u662f\u5426\u62e5\u6709\u7406\u8bba\u5fc3\u667a\uff0c\u901a\u8fc7\u5728\u5176\u57fa\u7840\u4e0a\u521b\u5efa\u80fd\u591f\u81ea\u7136\u8bed\u8a00\u4e92\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u4ee5\u5b9e\u73b0\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e0b\u7684\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u7c7b\u7684\u826f\u597d\u534f\u4f5c\u3002", "motivation": "\u63a2\u8ba8LLMs\u662f\u5426\u80fd\u6a21\u62df\u548c\u63a8\u7406\u4ed6\u4eba\u7684\u610f\u56fe\uff0c\u5373LLMs\u662f\u5426\u5177\u6709\u67d0\u79cd\u5f62\u5f0f\u7684\u7406\u8bba\u5fc3\u667a\u3002\u8fd9\u5728\u4eba\u4e0e\u4eba\u4ee5\u53ca\u4eba\u4e0e\u81ea\u4e3b\u7cfb\u7edf\u95f4\u7684\u6709\u6548\u534f\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u89c6\u89d2\u7814\u7a76LLMs\u7684\u7406\u8bba\u5fc3\u667a\uff0c\u8be5\u89c6\u89d2\u4e2d\u667a\u80fd\u4f53\u901a\u8fc7\u91cd\u590d\u4e92\u52a8\u5b66\u4e60\u534f\u4f5c\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u793e\u4f1a\u63a8\u7406\u3002", "result": "\u672a\u76f4\u63a5\u7ed9\u51fa\u7814\u7a76\u6210\u679c\uff0c\u4f46\u8be5\u9879\u7814\u7a76\u65e8\u5728\u63d0\u5347\u4eba\u5de5\u667a\u80fd\u667a\u80fd\u4f53\u9002\u5e94\u5e76\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u53ca\u4eba\u7c7b\u5408\u4f5c\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6765\u521b\u5efa\u4eba\u673a\u6df7\u5408\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u6709\u671d\u4e00\u65e5\u80fd\u591f\u5b9e\u73b0\u65e0\u7f1d\u534f\u4f5c\uff0c\u5bf9\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u7684\u4e92\u52a8\u672a\u6765\u5177\u6709\u5e7f\u6cdb\u7684\u610f\u4e49\u3002"}}
{"id": "2506.09082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09082", "abs": "https://arxiv.org/abs/2506.09082", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "comment": "First two authors contribute equally", "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "AI": {"tldr": "\u4e3a\u4e86\u7cfb\u7edf\u5730\u8bc4\u4f30\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFMs)\uff0c\u7814\u7a76\u5f15\u5165\u4e86AVA-Bench\uff0c\u4e00\u4e2a\u660e\u786e\u5206\u89e314\u4e2a\u539f\u5b50\u89c6\u89c9\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684VFMs\u8bc4\u4f30\uff0c\u5e76\u4e14\u51cf\u5c11\u4e86GPU\u65f6\u95f4\uff0c\u63d0\u5347\u4e86\u8bc4\u4f30\u6548\u7387\u3002", "motivation": "\u9488\u5bf9\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFMs)\u8bc4\u4f30\u4e2d\u7684\u4e24\u4e2a\u76f2\u70b9\uff1a1)\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u53ef\u80fd\u4e0eVQA\u6d4b\u8bd5\u5206\u5e03\u4e0d\u5339\u914d\uff1b2)VQA\u57fa\u51c6\u53ef\u80fd\u9700\u8981\u591a\u79cd\u89c6\u89c9\u80fd\u529b\uff0c\u96be\u4ee5\u5b9a\u4f4d\u9519\u8bef\u6765\u6e90\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\uff0c\u5f15\u5165AVA-Bench\u6765\u4f18\u5316VFMs\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51faAVA-Bench\uff0c\u7b2c\u4e00\u4e2a\u660e\u786e\u5206\u89e314\u4e2a\u539f\u5b50\u89c6\u89c9\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5982\u5b9a\u4f4d\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u7a7a\u95f4\u7406\u89e3\uff0c\u8fd9\u4e9b\u80fd\u529b\u5171\u540c\u652f\u6301\u590d\u6742\u7684\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u3002\u901a\u8fc7\u5206\u79bb\u8fd9\u4e9bAVAs\u5e76\u5728\u6bcf\u4e2aAVAs\u4e2d\u5339\u914d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\uff0cAVA-Bench\u53ef\u4ee5\u7cbe\u786e\u6307\u51faVFMs\u7684\u8868\u73b0\u3002", "result": "\u5e94\u7528AVA-Bench\u8bc4\u4f30\u9886\u5148\u7684VFMs\u63ed\u793a\u4e86\u72ec\u7279\u7684\"\u80fd\u529b\u6307\u7eb9\"\uff0c\u5c06VFMs\u7684\u9009\u62e9\u4ece\u7ecf\u9a8c\u731c\u6d4b\u8f6c\u53d8\u4e3a\u539f\u5219\u6027\u5de5\u7a0b\u5de5\u4f5c\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c0.5B LLM\u4e0e7B LLM\u53ef\u4ee5\u5f97\u5230\u7c7b\u4f3c\u7684VFMs\u6392\u540d\uff0c\u540c\u65f6\u51cf\u5c11\u4e868\u500d\u7684GPU\u65f6\u95f4\uff0c\u4f7f\u8bc4\u4f30\u66f4\u9ad8\u6548\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u5168\u9762\u900f\u660e\u7684\u57fa\u51c6\uff0cAVA-Bench\u4e3a\u4e0b\u4e00\u4ee3\u8868\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFMs)\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.09340", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09340", "abs": "https://arxiv.org/abs/2506.09340", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "title": "RePO: Replay-Enhanced Policy Optimization", "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "This paper presents Replay-Enhanced Policy Optimization (RePO), a method that improves upon GRPO in terms of data efficiency and computational cost while optimizing large language models for reinforcement learning.", "motivation": "The motivation behind this paper is to find a more efficient method in terms of computational costs and data usage for optimizing LLMs using reinforcement learning, compared to the existing Group Relative Policy Optimization (GRPO) method.", "method": "Replay-Enhanced Policy Optimization (RePO) is introduced to improve data efficiency and reduce computational costs when optimizing large language models (LLMs) for reinforcement learning tasks. It retrieves off-policy samples from a replay buffer to enrich the dataset used for policy optimization.", "result": "Experimental results show that RePO achieves significant performance gains over GRPO, with absolute average performance improvements of 18.4 and 4.1 points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively. Additionally, for Qwen3-1.7B, the number of effective optimization steps rose by 48% while increasing computational cost by only 15%.", "conclusion": "The conclusion is that RePO increases the data efficiency and reduces the computational cost while optimizing large language models for reinforcement learning tasks. It provides significant performance gains with a relatively small increase in computational costs."}}
{"id": "2506.09083", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09083", "abs": "https://arxiv.org/abs/2506.09083", "authors": ["Jerry Lin", "Partick P. W. Chen"], "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "comment": "4 pages, 3 figures, 1 Table", "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BakuFlow\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u534a\u81ea\u52a8\u6807\u6ce8\u751f\u6210\u5de5\u5177\uff0c\u5b83\u5177\u6709\u591a\u79cd\u521b\u65b0\u529f\u80fd\uff0c\u65e8\u5728\u63d0\u9ad8\u6807\u6ce8\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u51c6\u786e\u6807\u6ce8\u6570\u636e\u4ecd\u7136\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u4efb\u52a1\u4e2d\uff0c\u624b\u52a8\u6807\u6ce8\u8d39\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86BakuFlow\uff0c\u4e00\u79cd\u534a\u81ea\u52a8\u6807\u6ce8\u751f\u6210\u5de5\u5177\uff0c\u5b83\u5177\u6709\u591a\u79cd\u529f\u80fd\uff1a\uff081\uff09\u4e00\u4e2a\u53ef\u8c03\u653e\u5927\u955c\uff0c\u7528\u4e8e\u50cf\u7d20\u7ea7\u7cbe\u786e\u7684\u624b\u52a8\u4fee\u6b63\uff1b\uff082\uff09\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6570\u636e\u589e\u5f3a\u6a21\u5757\uff0c\u53ef\u7528\u4e8e\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u96c6\uff1b\uff083\uff09\u6807\u7b7e\u4f20\u64ad\u529f\u80fd\uff0c\u53ef\u4ee5\u5c06\u6807\u6ce8\u5bf9\u8c61\u8fc5\u901f\u590d\u5236\u5230\u8fde\u7eed\u5e27\u4e2d\uff1b\uff084\uff09\u81ea\u52a8\u6807\u6ce8\u6a21\u5757\uff0c\u57fa\u4e8e\u6539\u8fdb\u7684YOLOE\u6846\u67b6\u3002", "result": "BakuFlow\u5728\u5b9e\u9645\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u5de5\u4e1a\u573a\u666f\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u63d0\u5347\u4e86\u6807\u6ce8\u6548\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u521b\u65b0\u4f7f\u5f97BakuFlow\u7279\u522b\u9002\u7528\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u548c\u8ffd\u8e2a\u4efb\u52a1\uff0c\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u5927\u5e45\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u4e86\u6548\u7387\u3002"}}
{"id": "2506.09342", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09342", "abs": "https://arxiv.org/abs/2506.09342", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Latent Multi-Head Attention for Small Language Models", "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86MLA+RoPE\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6548\u7387\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u663e\u8457\u6743\u8861\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u8282\u7701\u548c\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u6f5c\u591a\u5934\u6ce8\u610f\u529b\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u4ee5\u63ed\u793a\u6f5c\u5728\u7684\u6548\u7387\u548c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "method": "\u6211\u4eec\u7814\u7a76\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6f5c\u591a\u5934\u6ce8\u610f\u529b\uff08MLA\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u4e0d\u540c\u67b6\u6784\u53d8\u4f53\u7684\u6bd4\u8f83\uff1a\u6807\u51c6\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u3001MLA\u4ee5\u53ca\u5e26\u6709\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u7684MLA\uff08MLA+RoPE\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e26\u6709\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u7684MLA\uff08MLA+RoPE\uff09\u5728\u4f7f\u7528\u534a\u79e9\u6f5c\u5728\u7ef4\u5ea6\u65f6\uff08r=d/2\uff09\uff0c\u53ef\u4ee5\u5b9e\u73b045%\u7684KV\u7f13\u5b58\u5185\u5b58\u964d\u4f4e\uff0c\u540c\u65f6\u53ea\u589e\u52a00.3%\u7684\u9a8c\u8bc1\u635f\u5931\uff1b\u5e76\u4e14\uff0c\u5728\u5c0f\u6a21\u578b\u4e2d\uff0c\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u5bf9\u4e8eMLA\u7684\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "MLA+RoPE\u5728\u786e\u4fdd\u6a21\u578b\u8d28\u91cf\uff087.4/10\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u4e5f\u5177\u5907\u4e86\u66f4\u597d\u7684\u5185\u5b58\u4f7f\u7528\u548c\u63a8\u7406\u901f\u5ea6\u3002\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u5728\u63a5\u53d7\u540e\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2506.09106", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09106", "abs": "https://arxiv.org/abs/2506.09106", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "title": "Bias Analysis in Unconditional Image Generative Models", "comment": null, "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u504f\u89c1\u68c0\u6d4b\u7684\u654f\u611f\u5ea6\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8fde\u7eed\u6027\u5c5e\u6027\u4e0a\u3002\u7814\u7a76\u8868\u660e\u9700\u8981\u6539\u5584\u8bc4\u4f30\u6846\u67b6\u5e76\u66f4\u6df1\u5165\u7406\u89e3\u5c5e\u6027\u7684\u793e\u4f1a\u590d\u6742\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u504f\u89c1\u95ee\u9898\u7684\u6587\u732e\u8d8a\u6765\u8d8a\u591a\uff0c\u4f46\u5173\u4e8e\u65e0\u6761\u4ef6\u751f\u6210\u4e2d\u504f\u89c1\u4ea7\u751f\u7684\u673a\u5236\u4ecd\u7136\u4e0d\u6e05\u6670\u3002\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u8fd9\u4e00\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u504f\u89c1\u53d8\u5316\u3002", "method": "\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5c5e\u6027\u7684\u504f\u89c1\u4e3a\u8be5\u5c5e\u6027\u5728\u89c2\u5bdf\u5206\u5e03\u4e2d\u7684\u5b58\u5728\u6982\u7387\u4e0e\u5176\u5728\u7406\u60f3\u53c2\u8003\u5206\u5e03\u4e2d\u7684\u671f\u671b\u6bd4\u4f8b\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u7814\u7a76\u901a\u8fc7\u8bad\u7ec3\u82e5\u5e72\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u4e00\u4e2a\u5e38\u7528\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\u6765\u7814\u7a76\u8bad\u7ec3\u5206\u5e03\u548c\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684\u504f\u89c1\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u68c0\u6d4b\u5230\u7684\u5c5e\u6027\u8f6c\u79fb\u76f8\u5bf9\u8f83\u5c0f\u3002\u5c5e\u6027\u8f6c\u79fb\u5bf9\u7528\u4e8e\u6807\u8bb0\u751f\u6210\u56fe\u50cf\u7684\u5206\u7c7b\u5668\u975e\u5e38\u654f\u611f\uff0c\u5c24\u5176\u662f\u5728\u51b3\u7b56\u8fb9\u754c\u4f4d\u4e8e\u9ad8\u5bc6\u5ea6\u533a\u57df\u65f6\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u5728\u8fde\u7eed\u6027\u5c5e\u6027\u4e0a\u8fd9\u79cd\u5206\u7c7b\u5668\u654f\u611f\u5ea6\u5e38\u88ab\u89c2\u5bdf\u5230\uff0c\u800c\u975e\u663e\u793a\u51fa\u4e8c\u5143\u7279\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u5177\u6709\u4ee3\u8868\u6027\u7684\u6807\u6ce8\u5b9e\u8df5\uff0c\u5bf9\u8bc4\u4f30\u6846\u67b6\u7684\u4e0d\u8db3\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u5ba1\u67e5\uff0c\u5e76\u8ba4\u8bc6\u5230\u5728\u8bc4\u4ef7\u504f\u89c1\u65f6\u5c5e\u6027\u7684\u793e\u4f1a\u590d\u6742\u6027\u3002"}}
{"id": "2506.09349", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09349", "abs": "https://arxiv.org/abs/2506.09349", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "AI": {"tldr": "This paper introduces OmniDRCA, a parallel speech-text generation model based on joint autoregressive modeling, achieving new state-of-the-art performance in speech-text generation.", "motivation": "The motivation behind this paper is to address the limitations of existing methods for generating discrete speech tokens with LLMs, which fall short in either failing to incorporate speech tokens into the LLM's autoregressive process or generate speech and text tokens independently.", "method": "Our approach, named OmniDRCA, relies on parallel speech and text processing featuring dual-resolution speech representations and contrastive cross-modal alignment to enhance audio comprehension.", "result": "OmniDRCA has achieved state-of-the-art performance on Spoken Question Answering benchmarks among models based on parallel joint speech-text modeling, and competitive performance relative to interleaved models.", "conclusion": "This work presents an innovative method to parallelly generate speech and text with autoregressive modeling, advancing the state-of-the-art in speech-text foundation models. It also lays the groundwork for future work in full-duplex conversational scenarios."}}
{"id": "2506.09109", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09109", "abs": "https://arxiv.org/abs/2506.09109", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "AI": {"tldr": "\u5f15\u5165\u65b0\u578b\u8bc4\u4f30\u5ea6\u91cfCAIRe\uff0c\u8ba1\u7b97\u7ed9\u5b9a\u6807\u7b7e\u96c6\u4e0b\u7684\u56fe\u50cf\u6587\u5316\u76f8\u5173\u6027\uff0c\u514b\u670d\u4e86\u6587\u5316\u504f\u89c1\u7684\u6d4b\u91cf\u74f6\u9888\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5b83\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u4e00\u81f4\u6027\u80fd\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u3001\u4e8b\u5b9e\u4e0d\u51c6\u786e\u6216\u8f93\u51fa\u5192\u72af\u6027\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002\u7531\u4e8e\u7f3a\u4e4f\u53ef\u9760\u8bc4\u4f30\u6587\u5316\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e00\u95ee\u9898\u96be\u4ee5\u89e3\u51b3\u3002", "method": "CAIRe\u8bc4\u4f30\u6307\u6807\u7528\u4e8e\u8bc4\u4f30\u7ed9\u5b9a\u7528\u6237\u5b9a\u4e49\u6807\u7b7e\u96c6\u7684\u56fe\u50cf\u7684\u6587\u5316\u76f8\u5173\u6027\u7a0b\u5ea6\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u4e2d\u7684\u5b9e\u4f53\u548c\u6982\u5ff5\u4e0e\u77e5\u8bc6\u5e93\u8fdb\u884c\u5173\u8054\uff0c\u5e76\u4f7f\u7528\u4e8b\u5b9e\u4fe1\u606f\u4e3a\u6bcf\u4e2a\u6587\u5316\u6807\u7b7e\u63d0\u4f9b\u72ec\u7acb\u7684\u5206\u7ea7\u5224\u65ad\u3002", "result": "\u5728\u624b\u52a8\u521b\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0cCAIRe\u8d85\u8fc7\u6240\u6709\u57fa\u7ebf\u6a21\u578b28\u4e2a\u767e\u5206\u70b9\u7684F1\u5f97\u5206\u3002\u53e6\u5916\uff0c\u5bf9\u4e8e\u4e24\u79cd\u666e\u904d\u6982\u5ff5\u7684\u6570\u636e\u96c6\uff0cCAIRe\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8fbe\u52300.56\u548c0.66\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u5f70\u663e\u51fa\u5176\u5bf9\u5927\u89c4\u6a21\u56fe\u50cf\u6e90\u4eba\u7c7b\u5224\u65ad\u7684\u5f3a\u5927\u4e00\u81f4\u6027\u3002", "conclusion": "CAIRe\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u5ea6\u91cf\u6807\u51c6\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8861\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u4ea7\u751f\u7684\u6587\u5316\u76f8\u5173\u6027\uff0c\u5176\u7ed3\u679c\u4e0e\u4eba\u7c7b\u7684\u8bc4\u4ef7\u9ad8\u5ea6\u4e00\u81f4\u3002"}}
{"id": "2506.09351", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09351", "abs": "https://arxiv.org/abs/2506.09351", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters.", "AI": {"tldr": "\u63d0\u51faDIVE\u65b9\u6cd5\u5bf9MoE\u67b6\u6784\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u7684\u91cd\u7ec4\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u6821\u51c6\u6570\u636e\u96c6\u4e0a\u526a\u679d\u540e\uff0c\u6709\u6548\u5229\u7528\u4e13\u5bb6\u591a\u6837\u6027\u51cf\u5c11\u8bad\u7ec3\u5197\u4f59\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8bad\u7ec3\u6548\u7387\u9ad8\u4e14\u51c6\u786e\u7387\u635f\u5931\u5c0f\u3002", "motivation": "\u89c2\u5bdf\u5230\u5728\u4e0d\u540c\u7684\u6821\u51c6\u6570\u636e\u96c6\u4e0a\u5bf9\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u526a\u679d\u540e\uff0c\u4f1a\u51fa\u73b0\u663e\u8457\u7684\u591a\u6837\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709\u91cd\u7ec4\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u8fd9\u79cd\u591a\u6837\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DIVE\u65b9\u6cd5\uff0c\u5305\u62ec\u9886\u57df\u4eb2\u548c\u529b\u6316\u6398\u3001\u57fa\u4e8e\u526a\u679d\u7684\u4e13\u5bb6\u91cd\u7ec4\u4ee5\u53ca\u9ad8\u6548\u518d\u8bad\u7ec3\u3002\u7279\u522b\u5730\uff0c\u91cd\u7ec4\u5305\u62ec\u524d\u9988\u7f51\u7edc\u6a21\u5757\u7684\u526a\u679d\u548c\u91cd\u7ec4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDIVE\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\uff0c\u4e14\u51c6\u786e\u7387\u635f\u5931\u6700\u5c0f\uff0c\u5728\u6fc0\u6d3b\u53c2\u6570\u6570\u91cf\u76f8\u540c\u7684\u6761\u4ef6\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u526a\u679d\u548cMoE\u91cd\u7ec4\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u91cd\u7ec4\u65b9\u6cd5\u4e2d\u7684\u591a\u6837\u6027\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u8bad\u7ec3\u6548\u7387\uff0c\u51cf\u5c11\u7cbe\u5ea6\u635f\u5931\u3002"}}
{"id": "2506.09113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09113", "abs": "https://arxiv.org/abs/2506.09113", "authors": ["Yu Gao", "Haoyuan Guo", "Tuyen Hoang", "Weilin Huang", "Lu Jiang", "Fangyuan Kong", "Huixia Li", "Jiashi Li", "Liang Li", "Xiaojie Li", "Xunsong Li", "Yifu Li", "Shanchuan Lin", "Zhijie Lin", "Jiawei Liu", "Shu Liu", "Xiaonan Nie", "Zhiwu Qing", "Yuxi Ren", "Li Sun", "Zhi Tian", "Rui Wang", "Sen Wang", "Guoqiang Wei", "Guohong Wu", "Jie Wu", "Ruiqi Xia", "Fei Xiao", "Xuefeng Xiao", "Jiangqiao Yan", "Ceyuan Yang", "Jianchao Yang", "Runkai Yang", "Tao Yang", "Yihang Yang", "Zilyu Ye", "Xuejiao Zeng", "Yan Zeng", "Heng Zhang", "Yang Zhao", "Xiaozheng Zheng", "Peihao Zhu", "Jiaxin Zou", "Feilong Zuo"], "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "comment": "Seedance 1.0 Technical Report", "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.", "AI": {"tldr": "Seedance 1.0 \u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6e90\u6570\u636e\u6574\u7406\u3001\u67b6\u6784\u8bbe\u8ba1\u4ee5\u53ca\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u5feb\u901f\u548c\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u9075\u5faa\u6307\u4ee4\u3001\u8fd0\u52a8\u5408\u7406\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u6b64\u62a5\u544a\u65e8\u5728\u4ecb\u7ecdSeedance 1.0\u5728\u8fd9\u4e9b\u65b9\u9762\u7684\u6539\u8fdb\u3002", "method": "Seedance 1.0 \u901a\u8fc7\u591a\u6e90\u6570\u636e\u6574\u7406\u3001\u9ad8\u6548\u7684\u67b6\u6784\u8bbe\u8ba1\u3001\u4f18\u5316\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u53ca\u591a\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u5b9e\u73b0\u6a21\u578b\u52a0\u901f\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u901f\u5ea6\u3002", "result": "Seedance 1.0 \u80fd\u591f\u5728\u590d\u6742\u591a\u4e3b\u4f53\u7684\u60c5\u5883\u4e2d\u51c6\u786e\u9075\u5faa\u6307\u4ee4\uff0c\u751f\u6210\u5177\u6709\u65f6\u7a7a\u8fde\u8d2f\u6027\u548c\u4e3b\u4f53\u4e00\u81f4\u6027\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5feb10\u500d\u3002", "conclusion": "Seedance 1.0 \u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e0e\u5feb\u901f\u89c6\u9891\u751f\u6210\u7684\u7ed3\u5408\uff0c\u5177\u6709\u663e\u8457\u7684\u65f6\u7a7a\u6d41\u7545\u6027\u548c\u7ed3\u6784\u7a33\u5b9a\u6027\uff0c\u5728\u590d\u6742\u591a\u4e3b\u4f53\u60c5\u5883\u4e2d\u4fdd\u6301\u6307\u4ee4\u7cbe\u786e\u9075\u5faa\u548c\u591a\u573a\u666f\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002"}}
{"id": "2506.09359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09359", "abs": "https://arxiv.org/abs/2506.09359", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u89e3\u51b3Text-to-SQL\u7cfb\u7edf\u4e2d\u751f\u6210SQL\u8bed\u53e5\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\u8bc4\u4f30\u95ee\u9898\u7684\u65b9\u6cd5\u53ca\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5927\u5e45\u5ea6\u63d0\u5347\u4e86Text-to-SQL\uff08NL2SQL\uff09\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4f46\u5bf9\u751f\u6210SQL\u8bed\u53e5\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\u8bc4\u4f30\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u6a21\u7cca\u7684\u7528\u6237\u67e5\u8be2\u548c\u591a\u79cd\u6709\u6548\u7684SQL\u89e3\u91ca\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u8bc4\u4f30\u751f\u6210SQL\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\u548c\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u201c\u5f31\u201d\u8bed\u4e49\u7b49\u4ef7\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u672c\u7814\u7a76\u5206\u6790\u4e86SQL\u7b49\u4ef7\u6027\u548c\u975e\u7b49\u4ef7\u6027\u7684\u5e38\u89c1\u6a21\u5f0f\uff0c\u5e76\u8ba8\u8bba\u4e86\u57fa\u4e8eLLM\u8bc4\u4f30\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u5bf9SQL\u7b49\u4ef7\u6027\u548c\u975e\u7b49\u4ef7\u6027\u6a21\u5f0f\u7684\u5206\u6790\u548cLLM\u8bc4\u4f30\u65b9\u6cd5\u8ba8\u8bba\uff0c\u672c\u7814\u7a76\u4e3a\u6539\u8fdbSQL\u8bed\u4e49\u7b49\u4ef7\u6027\u8bc4\u4ef7\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.09229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09229", "abs": "https://arxiv.org/abs/2506.09229", "authors": ["Sungwon Hwang", "Hyojin Jang", "Kinam Kim", "Minho Park", "Jaegul choo"], "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models", "comment": "24 pages, 25 figures", "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u5e27\u8868\u5f81\u5bf9\u9f50\u6280\u672fCREPA\uff0c\u65e8\u5728\u6539\u8fdb\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5e27\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u5728\u89c6\u89c9\u8d28\u91cf\u53ca\u8de8\u5e27\u4e00\u81f4\u6027\u4e0a\u7684\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u6307\u51fa\uff0c\u5c3d\u7ba1\u5728\u7528\u6237\u7ea7\u522b\u7684\u89c6\u9891\u6a21\u578b\u5fae\u8c03\u4ee5\u751f\u6210\u53cd\u6620\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u5c5e\u6027\u7684\u89c6\u9891\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4f46\u8fd9\u4e00\u65b9\u5411\u4ecd\u9c9c\u6709\u63a2\u7d22\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u610f\u4e49\u3002", "method": "\u7814\u7a76\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u9002\u5e94\u4e8e\u89c6\u9891\u6a21\u578b\u7684REPA\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u5176\u9650\u5236\u5728\u4e8e\u65e0\u6cd5\u6709\u6548\u4fdd\u6301\u5e27\u95f4\u8bed\u4e49\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u6b64\uff0c\u5f15\u5165CREPA\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u5c06\u5f53\u524d\u5e27\u7684\u9690\u85cf\u72b6\u6001\u4e0e\u76f8\u90bb\u5e27\u7684\u5916\u90e8\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\uff0c\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7528\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u7684\u8de8\u5e27\u8868\u5f81\u5bf9\u9f50\uff08CREPA\uff09\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u7559\u5e27\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0cCREPA \u80fd\u591f\u5728\u5927\u91cf\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\uff09\u4e2d\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u5e27\u95f4\u8bed\u4e49\u8fde\u8d2f\u6027\u3002\u7814\u7a76\u5728\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u5c5e\u6027\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CREPA\u7684\u9002\u7528\u6027\u3002", "conclusion": "CREPA\u4f5c\u4e3a\u4e00\u9879\u65b0\u6280\u672f\uff0c\u5df2\u5728\u5927\u89c4\u6a21VDMs\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8de8\u5e27\u8bed\u4e49\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u63d0\u5347\u6548\u679c\uff0c\u5e76\u5728\u4e0d\u540c\u5c5e\u6027\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.09367", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09367", "abs": "https://arxiv.org/abs/2506.09367", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u547d\u540d\u4e3aCOGENT\u7684\u8bfe\u7a0b\u5bfc\u5411\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9002\u5408\u5e74\u7ea7\u9605\u8bfb\u6c34\u5e73\u7684\u6559\u80b2\u5185\u5bb9\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u751f\u6210\u5f0fAI\u5e94\u7528\u4e8e\u6559\u80b2\u65f6\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u5185\u5bb9\u751f\u6210\u65b9\u9762\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6f5c\u529b\u548c\u591a\u6837\u6027\uff0c\u4f46\u5728\u6559\u80b2\u9886\u57df\u4e2d\u5e94\u7528\u65f6\u5b58\u5728\u6311\u6218\uff0c\u5982\u6a21\u578b\u96be\u4ee5\u4e0e\u8bfe\u7a0b\u6807\u51c6\u4e00\u81f4\u5e76\u7ef4\u6301\u5408\u9002\u7684\u5e74\u7ea7\u9605\u8bfb\u6c34\u5e73\u3002\u7279\u522b\u662fSTEM\u6559\u80b2\uff0c\u5982\u4f55\u5728\u4fdd\u6301\u79d1\u5b66\u6027\u7684\u540c\u65f6\u7528\u901a\u4fd7\u6613\u61c2\u7684\u8bed\u8a00\u89e3\u91ca\u590d\u6742\u62bd\u8c61\u7684\u6982\u5ff5\u7ed9\u5b66\u751f\u5e26\u6765\u4e86\u989d\u5916\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOGENT\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u65e8\u5728\u751f\u6210\u7b26\u5408\u5e74\u7ea7\u6c34\u5e73\u7684\u6559\u80b2\u5185\u5bb9\u3002\u6846\u67b6\u4e2d\u5305\u542b\u4e86\u4e09\u4e2a\u8bfe\u7a0b\u7ec4\u6210\u90e8\u5206\uff08\u79d1\u5b66\u6982\u5ff5\u3001\u6838\u5fc3\u601d\u60f3\u548c\u5b66\u4e60\u76ee\u6807\uff09\uff0c\u901a\u8fc7\u63a7\u5236\u957f\u5ea6\u3001\u8bcd\u6c47\u548c\u53e5\u5b50\u590d\u6742\u6027\u6765\u8c03\u63a7\u53ef\u8bfb\u6027\uff0c\u5e76\u91c7\u7528\"\u57fa\u4e8e\u597d\u5947\"\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5b66\u751f\u53c2\u4e0e\u5ea6\u548c\u5174\u8da3\u3002", "result": "\u901a\u8fc7\u5229\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0cCOGENT\u80fd\u591f\u6301\u7eed\u751f\u6210\u4e0e\u53c2\u8003\u6587\u732e\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u5e74\u7ea7\u9002\u5f53\u6bb5\u843d\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u751f\u6210\u9002\u5e94\u6027\u548c\u9ad8\u8d28\u91cf\u5b66\u4e60\u8d44\u6e90\u7684\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2506.09237", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09237", "abs": "https://arxiv.org/abs/2506.09237", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5bf9\u6297\u9c81\u68d2\u7684PatchGuard\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u91c7\u7528ViT\u67b6\u6784\u52a0\u5165\u524d\u666f\u611f\u77e5\u4f2a\u5f02\u5e38\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u8fb9\u7f18\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u539f\u56e0\u5728\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u53ea\u5305\u542b\u4e86\u6b63\u5e38\u7684\u65e0\u6807\u7b7e\u6837\u672c\u3002PatchGuard\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4f2a\u5f02\u5e38\u6837\u672c\u548c\u5c40\u90e8\u63a9\u6a21\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "PatchGuard, \u4e00\u79cd\u57fa\u4e8e\u524d\u666f\u611f\u77e5\u4f2a\u5f02\u5e38\u6837\u672c\u6765\u589e\u5f3aViT\u67b6\u6784\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u4f18\u5316\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u5de5\u4e1a\u548c\u533b\u7597\u6570\u636e\u96c6\u4e0a\uff0cPatchGuard\u76f8\u8f83\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u5728\u5bf9\u6297\u8bbe\u7f6e\u4e0b\u5206\u522b\u63d0\u9ad8\u4e8653.2%\u7684\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u548c68.5%\u7684\u5f02\u5e38\u5b9a\u4f4d\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u975e\u5bf9\u6297\u8bbe\u7f6e\u4e0b\u4e5f\u4fdd\u6301\u4e86\u826f\u597d\u7684\u51c6\u786e\u5ea6\u3002", "conclusion": "PatchGuard\u5c55\u793a\u4e86\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.09375", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09375", "abs": "https://arxiv.org/abs/2506.09375", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "AI": {"tldr": "CoLMbo, a Speaker Language Model, integrates a speaker encoder with prompt-based conditioning to generate detailed and structured speaker descriptions, including dialect, gender, and age, marking an advancement in speaker recognition systems.", "motivation": "The motivation behind this paper is to overcome the limitations of current speaker recognition systems that struggle to provide detailed speaker characteristics or context-rich descriptions.", "method": "The method involves integrating a speaker encoder with prompt-based conditioning to create detailed captions based on speaker embeddings.", "result": "CoLMbo provides customized descriptions including regional dialect variations and age-related traits, performing well in zero-shot scenarios across diverse datasets.", "conclusion": "This innovative approach enhances traditional speaker profiling and marks a significant advancement in the field of speaker recognition systems."}}
{"id": "2506.09278", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09278", "abs": "https://arxiv.org/abs/2506.09278", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "comment": "Project Page: https://uniflowmatch.github.io/", "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "AI": {"tldr": "UFM, a unified model for image correspondence and optical flow, outperforms specialized models in both accuracy and speed.", "motivation": "To improve performance and generalization for dense image correspondence tasks by unifying wide-baseline image matching and optical flow estimation through a single model.", "method": "Unified Flow & Matching model (UFM) uses a simple, generic transformer architecture for dense image correspondence across wide-baseline scenarios and optical flow estimation.", "result": "UFM is 28% more accurate than state-of-the-art flow methods and 62% less error and 6.7x faster than dense wide-baseline matchers.", "conclusion": "The development of UFM showcases the potential of unified training in achieving better and faster results for both optical flow and dense correspondence tasks, opening new avenues for real-time and multi-modal applications."}}
{"id": "2506.09381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09381", "abs": "https://arxiv.org/abs/2506.09381", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u65b0\u95fb\u6807\u9898\u548c\u94fe\u63a5\u7684\u8d28\u91cf\uff0c\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90fd\u53ef\u4ee5\u6709\u6548\u533a\u5206\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u7684\u8d28\u91cf\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u80fd\u5426\u81ea\u52a8\u533a\u5206\u4f4e\u8d28\u91cf\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u4e0e\u9ad8\u8d28\u91cf\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\uff0c\u4ee5\u5e94\u5bf9\u7ebf\u4e0a\u65b0\u95fb\u7684\u5e7f\u6cdb\u53d1\u5e03\u5e26\u6765\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e8612\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u5206\u679057,544,214\u6761\u5168\u7403\u65b0\u95fb\u7f51\u7ad9\u7684\u94fe\u63a5\u548c\u6807\u9898\u3002\u6570\u636e\u96c6\u4e2d\uff0c\u6bcf\u4e2a\u6587\u672c\u7684\u4e8c\u5143\u6807\u7b7e\u57fa\u4e8e\u4e13\u5bb6\u5bf9\u65b0\u95fb\u9886\u57df\u8d28\u91cf\u7684\u4e00\u81f4\u8bc4\u5206\u5f97\u51fa\u3002\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u4e86115\u4e2a\u63d0\u53d6\u51fa\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u88c5\u888b\u5206\u7c7b\u5668\uff0c\u8868\u73b0\u826f\u597d\uff08\u51c6\u786e\u738788.1%\uff0cF1\u5206\u657088.3%\uff09\u3002\u5fae\u8c03\u8fc7\u7684DistilBERT\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u738790.3%\uff0c\u4f46\u9700\u8981\u66f4\u957f\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7279\u5f81\u7684\u4f20\u7edf\u5206\u7c7b\u5668\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90fd\u80fd\u6709\u6548\u5730\u533a\u5206\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u7684\u8d28\u91cf\uff0c\u4f46\u9700\u8981\u5728\u9884\u6d4b\u6027\u80fd\u548c\u8bad\u7ec3\u65f6\u95f4\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002"}}
{"id": "2506.09299", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09299", "abs": "https://arxiv.org/abs/2506.09299", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "comment": "6 Pages, 3 figures", "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "AI": {"tldr": "This paper introduces an optimized YOLOv4-Tiny model for object detection in aerial emergency response imagery. The model is quantized to INT8, reducing its size and enhancing its inference speed without sacrificing accuracy, making it ideal for real-time use on low-power devices.", "motivation": "The motivation behind this work is the need for a lightweight and energy-efficient object detection model specifically tailored for aerial imagery in emergency response, as existing models are not optimized for these conditions and publicly available datasets are insufficient.", "method": "The paper employs the YOLOv4-Tiny model optimized through post-training quantization to INT8 precision for object detection in aerial imagery during emergencies. A custom-curated dataset with 10,820 annotated images specific to emergency scenarios is used for training, addressing the scarcity of publicly available drone-view emergency data.", "result": "The quantized YOLOv4-Tiny model achieves comparable detection performance to the YOLOv5-small model but with a significantly reduced model size (from 22.5 MB to 6.4 MB) and 44% faster inference speed.", "conclusion": "The quantized YOLOv4-Tiny model is highly suitable for real-time object detection on low-power edge devices in emergency response situations, providing energy efficiency and speed."}}
{"id": "2506.09391", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09391", "abs": "https://arxiv.org/abs/2506.09391", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "title": "Comparing human and LLM politeness strategies in free production", "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u793c\u8c8c\u8bed\u8a00\u65f6\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8d85\u8fc7700\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e2d\uff0c\u5b83\u4eec\u80fd\u591f\u590d\u5236\u5173\u952e\u7684\u793c\u8c8c\u7b56\u7565\u504f\u597d\uff0c\u5e76\u5728\u5f00\u653e\u6027\u4efb\u52a1\u4e2d\u83b7\u5f97\u4eba\u7c7b\u8bc4\u4ef7\u8005\u7684\u504f\u597d\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u503e\u5411\u4e8e\u5728\u6240\u6709\u60c5\u5883\u4e0b\u8fc7\u5ea6\u4f7f\u7528\u6d88\u6781\u793c\u8c8c\u7b56\u7565\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u4fe1\u606f\u4f20\u9012\u548c\u793e\u4f1a\u4e92\u52a8\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u5e76\u901a\u8fc7\u4e0e\u4eba\u7c7b\u7684\u5bf9\u6bd4\u8bc4\u4f30\u5176\u5728\u793c\u8c8c\u7528\u8bed\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u4eba\u7c7b\u548c\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u53d7\u63a7\u548c\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u751f\u6210\u7684\u793c\u8c8c\u8a00\u8bed\u7b56\u7565\uff0c\u7814\u7a76\u4e86LLMs\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u4f7f\u7528\u793c\u8c8c\u7b56\u7565\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8d85\u8fc7700\u4ebf\u53c2\u6570\u7684LLMs\u80fd\u591f\u6709\u6548\u590d\u5236\u8ba1\u7b97\u8bed\u7528\u5b66\u6587\u732e\u4e2d\u7684\u5173\u952e\u793c\u8c8c\u7b56\u7565\u504f\u597d\uff1b\u5728\u5f00\u653e\u6027\u573a\u666f\u4e0b\uff0c\u4eba\u7c7b\u8bc4\u4ef7\u8005\u5bf9\u6a21\u578b\u751f\u6210\u7684\u54cd\u5e94\u8868\u73b0\u51fa\u504f\u597d\uff1b\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u6240\u6709\u60c5\u5883\u4e0b\u90fd\u503e\u5411\u4e8e\u4f7f\u7528\u6d88\u6781\u793c\u8c8c\u7b56\u7565\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u3002", "conclusion": "\u5c3d\u7ba1\u73b0\u4ee3LLMs\u5728\u793c\u8c8c\u7b56\u7565\u7684\u5e94\u7528\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u4f7f\u7528\u6b63\u9762\u4e0e\u8d1f\u9762\u7b56\u7565\u7684\u4e0d\u5e73\u8861\u6027\uff0c\u5f15\u8d77\u4e86\u5173\u4e8eAI\u7cfb\u7edf\u8bed\u7528\u5b66\u5bf9\u9f50\u7684\u91cd\u8981\u7591\u95ee\u3002"}}
{"id": "2506.09300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09300", "abs": "https://arxiv.org/abs/2506.09300", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5", "comment": null, "summary": "This paper presents the deployment and performance evaluation of a quantized\nYOLOv4-Tiny model for real-time object detection in aerial emergency imagery on\na resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model\nwas quantized to INT8 precision using TensorFlow Lite post-training\nquantization techniques and evaluated for detection speed, power consumption,\nand thermal feasibility under embedded deployment conditions. The quantized\nmodel achieved an inference time of 28.2 ms per image with an average power\nconsumption of 13.85 W, demonstrating a significant reduction in power usage\ncompared to its FP32 counterpart. Detection accuracy remained robust across key\nemergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These\nresults highlight the potential of low-power embedded AI systems for real-time\ndeployment in safety-critical emergency response applications.", "AI": {"tldr": "\u91cf\u5316\u540e\u7684YOLOv4-Tiny\u6a21\u578b\u5728Raspberry Pi 5\u4e0a\u5b9e\u73b0\u4e86\u5feb\u901f\u9ad8\u6548\u7684\u7a7a\u88ad\u7d27\u6025\u56fe\u50cf\u5bf9\u8c61\u68c0\u6d4b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u548c\u90e8\u7f72\u5b9e\u65f6\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u5728\u7a7a\u88ad\u7d27\u6025\u56fe\u50cf\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528TensorFlow Lite\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6280\u672f\u5c06YOLOv4-Tiny\u6a21\u578b\u91cf\u5316\u5230INT8\u7cbe\u5ea6\uff0c\u5e76\u5728Raspberry Pi 5\u4e0a\u8bc4\u4f30\u4e86\u68c0\u6d4b\u901f\u5ea6\u3001\u529f\u8017\u548c\u70ed\u6027\u80fd\u3002", "result": "\u91cf\u5316\u6a21\u578b\u5728\u6bcf\u5f20\u56fe\u7247\u4e0a\u7684\u63a8\u7406\u65f6\u95f4\u4e3a28.2\u6beb\u79d2\uff0c\u5e76\u4e14\u5e73\u5747\u529f\u8017\u4e3a13.85\u74e6\uff0c\u76f8\u6bd4\u4e8e32\u4f4d\u6d6e\u70b9\u683c\u5f0f\u6a21\u578b\u663e\u793a\u51fa\u4e86\u663e\u8457\u7684\u529f\u8017\u964d\u4f4e\u3002\u68c0\u6d4b\u7cbe\u5ea6\u5728\u5173\u952e\u7684\u51e0\u7c7b\u7d27\u6025\u60c5\u51b5\u5982\u6551\u62a4\u8f66\u3001\u8b66\u8f66\u3001\u6d88\u9632\u8f66\u548c\u8f66\u7978\u4e2d\u7684\u8868\u73b0\u4ecd\u65e7\u7a33\u5065\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u4f4e\u529f\u8017\u5d4c\u5165\u5f0fAI\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u7684\u5e94\u6025\u54cd\u5e94\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u90e8\u7f72\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.09393", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09393", "abs": "https://arxiv.org/abs/2506.09393", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86KT$^2$\uff0c\u901a\u8fc7\u9690\u85cf\u9a6c\u5c14\u53ef\u592b\u6811\u6a21\u578b\u5efa\u6a21\u5b66\u751f\u5bf9\u77e5\u8bc6\u6982\u5ff5\u7684\u638c\u63e1\uff0c\u5e76\u5229\u7528EM\u7b97\u6cd5\u4f30\u8ba1\u5b66\u751f\u77e5\u8bc6\u638c\u63e1\u7a0b\u5ea6\u3002\u5b9e\u9a8c\u8868\u660eKT$^2$\u5728\u4f4e\u8d44\u6e90\u7684\u771f\u5b9e\u5728\u7ebf\u73af\u5883\u4e2d\uff0c\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8bb8\u591a\u77e5\u8bc6\u8ffd\u8e2a\u7684\u5b9e\u9645\u8bfe\u5802\u73af\u5883\u901a\u5e38\u6570\u636e\u8d44\u6e90\u6709\u9650\uff0c\u5e76\u4e14\u9700\u8981\u968f\u7740\u5b66\u751f\u7ec3\u4e60\u5386\u53f2\u7684\u589e\u957f\u8fdb\u884c\u5728\u7ebf\u66f4\u65b0\uff0c\u8fd9\u5bf9\u73b0\u6709\u7684\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u4e3a\u4e86\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u6062\u590d\u5f3a\u5927\u6027\u80fd\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86\u5178\u578b\u7684\u8bfe\u5802\u73af\u5883\u4e2d\u53ef\u7528\u7684\u5c42\u6b21\u77e5\u8bc6\u6982\u5ff5\u4fe1\u606f\uff0c\u5728\u6570\u636e\u7a00\u758f\u65f6\u63d0\u4f9b\u6709\u529b\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u77e5\u8bc6\u6811\u7684\u77e5\u8bc6\u8ffd\u8e2a(KT$^2$)\uff0c\u8fd9\u662f\u4e00\u4e2a\u6982\u7387\u6027\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u9690\u85cf\u9a6c\u5c14\u53ef\u592b\u6811\u6a21\u578b\u6765\u5efa\u6a21\u5b66\u751f\u5bf9\u77e5\u8bc6\u6982\u5ff5\u6811\u72b6\u5c42\u7ea7\u7ed3\u6784\u7684\u7406\u89e3\u3002KT$^2$\u5229\u7528EM\u7b97\u6cd5\u4f30\u8ba1\u5b66\u751f\u638c\u63e1\u77e5\u8bc6\u7684\u60c5\u51b5\uff0c\u5e76\u901a\u8fc7\u589e\u91cf\u66f4\u65b0\u673a\u5236\u5728\u63a5\u6536\u5230\u65b0\u7b54\u6848\u65f6\u652f\u6301\u4e2a\u6027\u5316\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKT$^2$\u5728\u4f4e\u8d44\u6e90\u7684\u771f\u5b9e\u5728\u7ebf\u73af\u5883\u4e0b\uff0c\u76f8\u6bd4\u4e8e\u5f3a\u5927\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u4fdd\u6301\u4e00\u81f4\u6027\u5730\u66f4\u4f18\u8868\u73b0\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660eKT$^2$\u5728\u4f4e\u8d44\u6e90\u7684\u771f\u5b9e\u5728\u7ebf\u73af\u5883\u4e2d\uff0c\u59cb\u7ec8\u4fdd\u6301\u4f18\u4e8e\u5f3a\u5927\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09327", "abs": "https://arxiv.org/abs/2506.09327", "authors": ["Tong Wang", "Guanzhou Chen", "Xiaodong Zhang", "Chenxi Liu", "Jiaqi Wang", "Xiaoliang Tan", "Wenchao Guo", "Qingyuan Yang", "Kaiqi Zhang"], "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning", "comment": null, "summary": "Remote sensing image interpretation plays a critical role in environmental\nmonitoring, urban planning, and disaster assessment. However, acquiring\nhigh-quality labeled data is often costly and time-consuming. To address this\nchallenge, we proposes a multi-modal self-supervised learning framework that\nleverages high-resolution RGB images, multi-spectral data, and digital surface\nmodels (DSM) for pre-training. By designing an information-aware adaptive\nmasking strategy, cross-modal masking mechanism, and multi-task self-supervised\nobjectives, the framework effectively captures both the correlations across\ndifferent modalities and the unique feature structures within each modality. We\nevaluated the proposed method on multiple downstream tasks, covering typical\nremote sensing applications such as scene classification, semantic\nsegmentation, change detection, object detection, and depth estimation.\nExperiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.\nThe results demonstrate that the proposed method outperforms existing\npretraining approaches in most tasks. Specifically, on the Potsdam and\nVaihingen semantic segmentation tasks, our method achieved mIoU scores of\n78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation\ntask, the RMSE error is reduced to 0.182, and for the binary change detection\ntask in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing\nthe second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and\nHR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u7684\u9884\u8bad\u7ec3\uff0c\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u5bf9\u4e8e\u73af\u5883\u76d1\u6d4b\u3001\u57ce\u5e02\u89c4\u5212\u548c\u707e\u5bb3\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u9ad8\u5206\u8fa8\u7387RGB\u56fe\u50cf\u3001\u591a\u5149\u8c31\u6570\u636e\u548c\u6570\u5b57\u8868\u9762\u6a21\u578b\uff08DSM\uff09\uff0c\u5f15\u5165\u4e86\u4fe1\u606f\u611f\u77e5\u81ea\u9002\u5e94\u63a9\u6a21\u7b56\u7565\u3001\u8de8\u6a21\u6001\u63a9\u6a21\u673a\u5236\u548c\u591a\u4efb\u52a1\u81ea\u76d1\u7763\u76ee\u6807\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5177\u4f53\u5728Potsdam\u548cVaihingen\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u4f7f\u752850%\u8bad\u7ec3\u96c6\u8fbe\u6210\u7684mIoU\u5206\u6570\u5206\u522b\u8fbe\u5230\u4e8678.30%\u548c76.50%\uff0c\u5728SECOND\u6570\u636e\u96c6\u4e8c\u5143\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2dmIoU\u5206\u6570\u8fbe\u523047.51%\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u9065\u611f\u5e94\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09408", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09408", "abs": "https://arxiv.org/abs/2506.09408", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.09343", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09343", "abs": "https://arxiv.org/abs/2506.09343", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "comment": "CVPR 2025 Highlight", "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u5bb6\u7535\u64cd\u4f5c\u57fa\u51c6CheckManual\u548c\u9996\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u64cd\u4f5c\u89c4\u5212\u6a21\u578bManualPlan\uff0c\u5f3a\u8c03\u673a\u5668\u4eba\u901a\u8fc7\u7406\u89e3\u624b\u518c\u64cd\u4f5c\u5bb6\u7535\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e0e\u624b\u518c\u76f8\u5173\u7684\u7814\u7a76\u5c40\u9650\u4e8e\u95ee\u7b54\u4efb\u52a1\uff0c\u800c\u73b0\u6709\u7684\u64cd\u4f5c\u7814\u7a76\u5ffd\u89c6\u4e86\u624b\u518c\u7684\u91cd\u8981\u4f5c\u7528\uff0c\u65e0\u6cd5\u7406\u89e3\u591a\u9875\u624b\u518c\u3002\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u673a\u5668\u4eba\u5bf9\u624b\u518c\u7684\u7406\u89e3\uff0c\u8fdb\u800c\u63d0\u5347\u5176\u64cd\u4f5c\u5bb6\u7535\u7684\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u518c\u7684\u5bb6\u7535\u64cd\u4f5c\u57fa\u51c6CheckManual\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5927\u578b\u6a21\u578b\u8f85\u52a9\u7684\u4eba\u7c7b\u4fee\u8ba2\u6570\u636e\u751f\u6210\u7ba1\u9053\u6765\u521b\u5efa\u57fa\u4e8eCAD\u5bb6\u7535\u6a21\u578b\u7684\u624b\u518c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u64cd\u4f5c\u89c4\u5212\u6a21\u578bManualPlan\uff0c\u8bbe\u7acb\u4e86\u4e00\u7ec4\u57fa\u51c6\u4ee5\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u4e8e\u624b\u518c\u7684\u64cd\u4f5c\u6311\u6218\u3001\u5ea6\u91cf\u6807\u51c6\u548c\u6a21\u62df\u73af\u5883\uff0c\u4e3a\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u63d0\u51faManualPlan\u6a21\u578b\uff0c\u8bbe\u5b9a\u4e86CheckManual\u57fa\u51c6\u7684\u4e00\u7ec4\u521d\u6b65\u57fa\u51c6\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u901a\u8fc7\u673a\u5668\u4eba\u7406\u89e3\u548c\u5b66\u4e60\u5bb6\u7535\u624b\u518c\u6765\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u672a\u6765\u7684\u7814\u7a76\u5c06\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u7406\u89e3\u548c\u64cd\u4f5c\u5bb6\u7535\u7684\u80fd\u529b\u3002"}}
{"id": "2506.09414", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.09414", "abs": "https://arxiv.org/abs/2506.09414", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "AI": {"tldr": "\u63d0\u51faPGDA-KGQA\u4ee5\u89e3\u51b3\u591a\u8df3\u63a8\u7406\u6837\u672c\u7a00\u7f3a\u548c\u8bed\u4e49\u626d\u66f2\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u5355\u8df3\u95ee\u9898\u4e14\u5bb9\u6613\u8bed\u4e49\u626d\u66f2\uff0c\u540c\u65f6\u89e3\u51b3\u591a\u8df3\u63a8\u7406\u6837\u672c\u7a00\u7f3a\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u63d0\u793a\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u7ed3\u5408LLMs\u751f\u6210\u5927\u89c4\u6a21\u7684(\u95ee\u9898, \u903b\u8f91\u5f62\u5f0f)\u5bf9\uff0c\u91c7\u53d6\u4e09\u79cd\u7b56\u7565\u4e30\u5bcc\u8bad\u7ec3\u96c6\u3002", "result": "\u5728\u6807\u51c6KGQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u5728WebQSP\u548cComplexWebQuestions\u4e0a\u5206\u522b\u63d0\u9ad8\u4e862.8%-3.1%\u548c1.8%-2.4%\u7684F1, Hits@1, \u548cAccuracy\u3002", "conclusion": "PGDA-KGQA\u6846\u67b6\u901a\u8fc7\u6574\u5408LLM\uff0c\u591a\u6837\u5316\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347KGQA\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.09345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09345", "abs": "https://arxiv.org/abs/2506.09345", "authors": ["Songping Wang", "Xiantao Hu", "Yueming Lyu", "Caifeng Shan"], "title": "An Effective End-to-End Solution for Multimodal Action Recognition", "comment": null, "summary": "Recently, multimodal tasks have strongly advanced the field of action\nrecognition with their rich multimodal information. However, due to the\nscarcity of tri-modal data, research on tri-modal action recognition tasks\nfaces many challenges. To this end, we have proposed a comprehensive multimodal\naction recognition solution that effectively utilizes multimodal information.\nFirst, the existing data are transformed and expanded by optimizing data\nenhancement techniques to enlarge the training scale. At the same time, more\nRGB datasets are used to pre-train the backbone network, which is better\nadapted to the new task by means of transfer learning. Secondly, multimodal\nspatial features are extracted with the help of 2D CNNs and combined with the\nTemporal Shift Module (TSM) to achieve multimodal spatial-temporal feature\nextraction comparable to 3D CNNs and improve the computational efficiency. In\naddition, common prediction enhancement methods, such as Stochastic Weight\nAveraging (SWA), Ensemble and Test-Time augmentation (TTA), are used to\nintegrate the knowledge of models from different training periods of the same\narchitecture and different architectures, so as to predict the actions from\ndifferent perspectives and fully exploit the target information. Ultimately, we\nachieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the\ncompetition leaderboard, demonstrating the superiority of our solution.", "AI": {"tldr": "A multimodal action recognition solution is proposed, utilizing data enhancement, pre-training with transfer learning, and various prediction enhancement methods, achieving high accuracy on the competition leaderboard.", "motivation": "To address the challenges faced in tri-modal action recognition tasks due to the scarcity of tri-modal data, a comprehensive multimodal action recognition solution is proposed.", "method": "First, existing data are transformed and expanded via optimized data enhancement techniques. More RGB datasets are used for pre-training the backbone network through transfer learning. Secondly, multimodal spatial features are extracted using 2D CNNs and combined with TSM for multimodal spatial-temporal feature extraction and improved computational efficiency. Common prediction enhancement methods, such as SWA, Ensemble, and TTA, are employed to integrate the knowledge of models from different training periods of the same architecture and different architectures for comprehensive action prediction.", "result": "The approach achieved a Top-1 accuracy of 99% and a Top-5 accuracy of 100% on the competition leaderboard.", "conclusion": "The proposed solution showcases superiority in tri-modal action recognition tasks by effectively utilizing multimodal information and improving computational efficiency."}}
{"id": "2506.09424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09424", "abs": "https://arxiv.org/abs/2506.09424", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u4e0d\u540c\u9886\u57df\u7684\u81ea\u52a8\u6b3a\u9a97\u68c0\u6d4b\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5fae\u8c03\u540e\u7684LLMs\u5728\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9876\u7ea7\u6c34\u5e73\uff0c\u800cLMMs\u5728\u5229\u7528\u8de8\u6a21\u6001\u7ebf\u7d22\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u8f85\u52a9\u7279\u5f81\u548c\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u7684\u5f71\u54cd\u3002", "motivation": "\u9274\u4e8e\u6570\u5b57\u4e16\u754c\u4e2d\u6b3a\u9a97\u68c0\u6d4b\u7684\u91cd\u8981\u6027\u4e0e\u6311\u6218\u6027\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u578b\u5728\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u4fbf\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u8005\u901a\u8fc7\u5bf9\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\uff08\u73b0\u5b9e\u751f\u6d3b\u8bbf\u8c08\u3001\u4eba\u9645\u60c5\u666f\u4e2d\u7684\u6307\u793a\u6027\u6b3a\u9a97\u4ee5\u53ca\u6b3a\u8bc8\u6027\u8bc4\u8bba\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u91c7\u7528\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u6837\u672c\u9009\u62e9\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u5fae\u8c03\u540e\u7684LLMs\u5728\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0cLMMs\u5728\u8de8\u6a21\u6001\u7ebf\u7d22\u5229\u7528\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff1b\u8f85\u52a9\u7279\u5f81\u548c\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\u5bf9\u7ed3\u679c\u6709\u4e00\u5b9a\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u5904\u7406\u548c\u89e3\u91ca\u6b3a\u9a97\u7ebf\u7d22\u65b9\u9762\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2506.09350", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09350", "abs": "https://arxiv.org/abs/2506.09350", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "comment": null, "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u5bf9\u6297\u540e\u8bad\u7ec3\uff08AAPT\uff09\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u6362\u4e3a\u5b9e\u65f6\u3001\u4ea4\u4e92\u5f0f\u7684\u89c6\u9891\u751f\u6210\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u5bc6\u96c6\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u4f7f\u7528\u3002", "method": "Structure", "result": "{\\\"tldr\\\": \\\"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u5bf9\u6297\u540e\u8bad\u7ec3\uff08AAPT\uff09\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u6362\u4e3a\u5b9e\u65f6\u3001\u4ea4\u4e92\u5f0f\u7684\u89c6\u9891\u751f\u6210\u5668\u3002\\\", \\\"motivation\\\": \\\"\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u5bc6\u96c6\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u4f7f\u7528\u3002\\\", \\\"method\\\": \\\"\u8be5\u65b9\u6cd5\u4f7f\u7528\u5355\u4e2a\u795e\u7ecf\u51fd\u6570\u8bc4\u4f30\uff081NFE\uff09\u9010\u5e27\u751f\u6210\u6f5c\u5728\u5e27\uff0c\u5e76\u4f7f\u7528\u5bf9\u6297\u6027\u8bad\u7ec3\u4f5c\u4e3a\u81ea\u56de\u5f52\u751f\u6210\u7684\u6709\u6548\u8303\u5f0f\u3002\\\", \\\"result\\\": \\\"\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\uff0824fps\uff09\uff0c\u6d41\u5f0f\u89c6\u9891\u751f\u6210\uff0c\u5728\u5355\u4e2aH100\u4e0a\u8fbe\u5230736x416\u5206\u8fa8\u7387\uff0c\u6216\u57288xH100\u4e0a\u8fbe\u52301280x720\u5206\u8fa8\u7387\uff0c\u6700\u957f\u53ef\u8fbe\u4e00\u5206\u949f\u3002\\\", \\\"conclusion\\\": \\\"\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u957f\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u6297\u6027\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\uff0c\u5e76\u4e14\u4f7f\u5176\u5728\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u53ef\u884c\u3002\\\"}", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u957f\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u6297\u6027\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\uff0c\u5e76\u4e14\u4f7f\u5176\u5728\u5b9e\u65f6\u548c\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u53ef\u884c\u3002"}}
{"id": "2506.09428", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09428", "abs": "https://arxiv.org/abs/2506.09428", "authors": ["Fei Ding", "Baiqiao Wang"], "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9SFT\u65b9\u6cd5\u5bfc\u81f4\u7684\u8bed\u8a00\u6a21\u578b\u666e\u904d\u80fd\u529b\u964d\u4f4e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u52a0\u7ecf\u6d4e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u6307\u4ee4\u5206\u5e03\u548c\u591a\u6a21\u578b\u7b5b\u9009\u6765\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u867d\u7136SFT\u589e\u5f3a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u9075\u4ece\u80fd\u529b\u548c\u7279\u5b9a\u4efb\u52a1\u7684\u9002\u5e94\u6027\uff0c\u4f46\u4e5f\u5f80\u5f80\u964d\u4f4e\u4e86\u5b83\u4eec\u7684\u666e\u904d\u80fd\u529b\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u539f\u59cb\u9884\u8bad\u7ec3\u6570\u636e\u7684\u4e0d\u53ef\u8bbf\u95ee\u6027\uff0c\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\u5728\u7b2c\u4e09\u65b9\u5b9e\u8df5\u8005\u5bf9\u5f00\u6e90\u6a21\u578b\u5b9e\u65bdSFT\u65f6\u4f1a\u6076\u5316\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76d1\u7763\u5fae\u8c03(SFT)\u65b9\u6cd5\uff0c\u65e8\u5728\u4ee5\u8f83\u4f4e\u6210\u672c\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\uff0c\u540c\u65f6\u4e0d\u4f9d\u8d56\u539f\u59cb\u5fae\u8c03\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u91cd\u6784\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u7684SFT\u6307\u4ee4\u5206\u5e03\uff0c\u7136\u540e\u901a\u8fc7\u591a\u6a21\u578b\u7b5b\u9009\u8fc7\u7a0b\u9009\u62e9\u6700\u4f18\u6570\u636e\uff0c\u5e76\u5c06\u5176\u4e0e\u65b0\u6570\u636e\u6df7\u5408\u8fdb\u884cSFT\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u901a\u7528\u9886\u57df\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u7279\u5b9a\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u901a\u7528\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u53ef\u4ee5\u6539\u8fdb\u7279\u5b9a\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u4ee5\u8f83\u4f4e\u7684\u6210\u672c\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\u3002"}}
{"id": "2506.09357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09357", "abs": "https://arxiv.org/abs/2506.09357", "authors": ["Junchao Zhou"], "title": "A new approach for image segmentation based on diffeomorphic registration and gradient fields", "comment": null, "summary": "Image segmentation is a fundamental task in computer vision aimed at\ndelineating object boundaries within images. Traditional approaches, such as\nedge detection and variational methods, have been widely explored, while recent\nadvances in deep learning have shown promising results but often require\nextensive training data. In this work, we propose a novel variational framework\nfor 2D image segmentation that integrates concepts from shape analysis and\ndiffeomorphic transformations. Our method models segmentation as the\ndeformation of a template curve via a diffeomorphic transformation of the image\ndomain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. The curve evolution is guided by a loss function that compares the\ndeformed curve to the image gradient field, formulated through the varifold\nrepresentation of geometric shapes. The approach is implemented in Python with\nGPU acceleration using the PyKeops library. This framework allows for accurate\nsegmentation with a flexible and theoretically grounded methodology that does\nnot rely on large datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u540c\u80da\u53d8\u6362\u76842D\u56fe\u50cf\u5206\u5272\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528LDDMM\u6846\u67b6\u4e0b\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6a21\u677f\u66f2\u7ebf\u53d8\u5f62\u4e3a\u56fe\u50cf\u57df\u4e2d\u7684\u66f2\u7ebf\u6765\u8fdb\u884c\u56fe\u50cf\u5206\u5272\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7406\u8bba\u57fa\u7840\u548c\u7075\u6d3b\u6027\u7684\u7ed3\u5408\u3002", "motivation": "\u4f20\u7edf\u7684\u8fb9\u7f18\u68c0\u6d4b\u548c\u53d8\u5206\u65b9\u6cd5\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u800c\u6700\u8fd1\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u663e\u793a\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u4e0d\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\u4e14\u7406\u8bba\u57fa\u7840\u575a\u5b9e\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u76842D\u56fe\u50cf\u5206\u5272\u53d8\u5206\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5f62\u72b6\u5206\u6790\u548c\u5fae\u5206\u540c\u80da\u53d8\u6362\u7684\u6982\u5ff5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7LDDMM\u6846\u67b6\u4e0b\u7684\u5fae\u5206\u540c\u80da\u53d8\u6362\u5c06\u6a21\u677f\u66f2\u7ebf\u53d8\u5f62\u4e3a\u56fe\u50cf\u57df\u4e2d\u7684\u66f2\u7ebf\uff0c\u4f7f\u7528PyKeops\u5e93\u5b9e\u73b0Python\u4e2d\u57fa\u4e8eGPU\u7684\u52a0\u901f\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u4f9d\u8d56\u4e8e\u7406\u8bba\u57fa\u7840\u7075\u6d3b\u51c6\u786e\u5730\u5206\u5272\u56fe\u50cf\uff0c\u800c\u65e0\u9700\u5927\u91cf\u6570\u636e\u96c6\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u57fa\u4e8e\u7406\u8bba\u57fa\u7840\u548c\u7075\u6d3b\u6027\u7684\u56fe\u50cf\u5206\u5272\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a2D\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u57fa\u7840\u6df1\u539a\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u8bba\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2506.09440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09440", "abs": "https://arxiv.org/abs/2506.09440", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u6b3e\u4fc4\u8bed\u5927\u6a21\u578bGigaChat\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86\u5176\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u8fc7\u7a0b\u53ca\u5b9e\u9a8c\uff0c\u5e76\u5f00\u653e\u4e86\u90e8\u5206\u6a21\u578b\u4f9b\u7814\u7a76\u548c\u5de5\u4e1a\u4f7f\u7528\u3002", "motivation": "\u7531\u4e8e\u5f00\u53d1\u9488\u5bf9\u4fc4\u8bed\u7684\u57fa\u7840\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u56e0\u6b64\u4e13\u95e8\u4f18\u5316\u7684\u6a21\u578b\u4e0d\u591a\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u7f3a\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86GigaChat\u7cfb\u5217\u4fc4\u8bedLLM\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u6a21\u578b\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u8fc7\u7a0b\u548c\u5b9e\u9a8c\u7684\u8be6\u7ec6\u62a5\u544a\uff0c\u4ee5\u53ca\u5982\u4f55\u6307\u5bfc\u8bbe\u8ba1\u9009\u62e9\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8bc4\u4f30\u4e86GigaChat\u6a21\u578b\u5728\u4fc4\u8bed\u548c\u82f1\u8bed\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5c06\u5176\u4e0e\u591a\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7API\u3001Telegram\u673a\u5668\u4eba\u548cWeb\u754c\u9762\u63d0\u4f9b\u9876\u7ea7\u6a21\u578b\u7684\u7cfb\u7edf\u6f14\u793a\uff0c\u5e76\u5f00\u653e\u4e86\u4e09\u4e2a\u516c\u5f00\u6a21\u578b\uff0c\u4ee5\u4fc3\u8fdb\u4fc4\u8bedNLP\u7814\u7a76\u548c\u5de5\u4e1a\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09363", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.09363", "abs": "https://arxiv.org/abs/2506.09363", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "comment": "Under review", "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "AI": {"tldr": "Introduces SAGE, a method combining semantic-augment erasing and global-local retention mechanism for safer diffusion models", "motivation": "address safety risks such as unsafe content generation and copyright infringement in diffusion models, and avoid the limitation of existing concept erasing methods", "method": "semantic-augment erasing and global-local collaborative retention mechanism", "result": "SAGE demonstrates comprehensive superiority in the safe generation of diffusion models compared with other methods", "conclusion": "SAGE effectively mitigates safety risks and retains performance in irrelevant concepts compared to previous methods"}}
{"id": "2506.09450", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09450", "abs": "https://arxiv.org/abs/2506.09450", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165\u4e86UniToMBench\u2014\u2014\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u548c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\u7684\u7efc\u5408\u6027\u57fa\u51c6\u5de5\u5177\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u6a21\u578b\u5728ToM\u4efb\u52a1\u4e0a\u7684\u6210\u5c31\u4e0e\u4e0d\u8db3\u3002", "motivation": "\u76ee\u524d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u9884\u6d4b\u4eba\u7c7b\u601d\u60f3\u72b6\u6001\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7UniToMBench \u63d0\u4f9b\u4e00\u4e2a\u7efc\u5408\u5de5\u5177\uff0c\u5e2e\u52a9\u6539\u5584\u8fd9\u4e00\u60c5\u51b5\u5e76\u8bc4\u4f30\u6a21\u578b\u7684ToM\uff08Theory of Mind\uff09\u80fd\u529b\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86UniToMBench\uff0c\u8be5\u57fa\u51c6\u901a\u8fc7\u6574\u5408SimToM\u548cTOMBENCH\u7684\u4f18\u70b9\uff0c\u5f15\u5165\u4e86\u591a\u4ea4\u4e92\u4efb\u52a1\u8bbe\u8ba1\u548c\u53d8\u5316\u7684\u6545\u4e8b\u573a\u666f\uff0c\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u63d0\u5347\u548c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u65b9\u9762\u7684\u80fd\u529b\u3002UniToMBench \u4f7f\u7528\u8d85\u8fc71,000\u4e2a\u624b\u5de5\u7f16\u5199\u7684\u60c5\u666f\u7ec4\u6210\u7684\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89d2\u7406\u89e3\u548c\u591a\u6837\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u66f4\u597d\u5730\u523a\u6fc0\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u50cfGPT-4o\u548cGPT-4o Mini\u8fd9\u6837\u7684\u6a21\u578b\u5728\u60c5\u7eea\u548c\u4fe1\u5ff5\u76f8\u5173\u7684\u60c5\u5f62\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u9ad8\u51c6\u786e\u7387\uff0c\u901a\u5e38\u8d85\u8fc780%\u3002\u7136\u800c\uff0c\u5728\u57fa\u4e8e\u77e5\u8bc6\u7684\u4efb\u52a1\u4e2d\uff0c\u5176\u8868\u73b0\u5219\u663e\u793a\u51fa\u8f83\u5927\u7684\u5dee\u5f02\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u5982\u6b64\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8fd8\u662f\u5c55\u73b0\u4e86\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406ToM\u4efb\u52a1\u7684\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86UniToMBench\u4f5c\u4e3a\u4e00\u79cd\u5168\u9762\u8bc4\u4f30\u548c\u53d1\u5c55\u5de5\u5177\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.09369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09369", "abs": "https://arxiv.org/abs/2506.09369", "authors": ["Zeran Ke", "Bin Tan", "Xianwei Zheng", "Yujun Shen", "Tianfu Wu", "Nan Xue"], "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined", "comment": "accepted to CVPR 2025; 17 pages, appendices included", "summary": "This paper studies the problem of Line Segment Detection (LSD) for the\ncharacterization of line geometry in images, with the aim of learning a\ndomain-agnostic robust LSD model that works well for any natural images. With\nthe focus of scalable self-supervised learning of LSD, we revisit and\nstreamline the fundamental designs of (deep and non-deep) LSD approaches to\nhave a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the\ncuration of line geometry at scale from over 10M unlabeled real-world images.\nOur ScaleLSD works very well to detect much more number of line segments from\nany natural images even than the pioneered non-deep LSD approach, having a more\ncomplete and accurate geometric characterization of images using line segments.\nExperimentally, our proposed ScaleLSD is comprehensively testified under\nzero-shot protocols in detection performance, single-view 3D geometry\nestimation, two-view line segment matching, and multiview 3D line mapping, all\nwith excellent performance obtained. Based on the thorough evaluation, our\nScaleLSD is observed to be the first deep approach that outperforms the\npioneered non-deep LSD in all aspects we have tested, significantly expanding\nand reinforcing the versatility of the line geometry of images. Code and Models\nare available at https://github.com/ant-research/scalelsd", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86ScaleLSD\uff0c\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u9ad8\u6548\u7684\u7ebf\u6bb5\u68c0\u6d4b\u6a21\u578b\uff0c\u4e13\u4e3a\u5927\u89c4\u6a21\u548c\u591a\u6837\u5316\u7684\u81ea\u7136\u56fe\u50cf\u8bbe\u8ba1\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5b66\u4e60\u4e00\u4e2a\u9002\u7528\u4e8e\u4efb\u4f55\u81ea\u7136\u56fe\u50cf\u7684\u9c81\u68d2\u9886\u57df\u65e0\u5173\u7684\u7ebf\u6bb5\u68c0\u6d4b\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u56fe\u50cf\u7684\u7ebf\u6bb5\u51e0\u4f55\u8868\u5f81\u95ee\u9898\u3002", "method": "\u6b64\u7814\u7a76\u63d0\u51fa\u4e86ScaleLSD\uff0c\u4e00\u79cd\u4e13\u6ce8\u4e8e\u5728\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u73b0\u5b9e\u56fe\u50cf\u4e2d\u5b66\u4e60\u7ebf\u6bb5\u68c0\u6d4b\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002ScaleLSD\u901a\u8fc7\u590d\u4e60\u548c\u7b80\u5316\u6df1\u5ea6\u548c\u975e\u6df1\u5ea6LSD\u65b9\u6cd5\u7684\u57fa\u672c\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u7684\u7ebf\u6bb5\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cScaleLSD\u5728\u96f6\u6837\u672c\u56fe\u50cf\u68c0\u6d4b\u6027\u80fd\uff0c\u5355\u89c6\u89d23D\u51e0\u4f55\u4f30\u8ba1\uff0c\u53cc\u89c6\u89d2\u7ebf\u6bb5\u5339\u914d\u548c\u591a\u89c6\u89d23D\u7ebf\u6620\u5c04\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9996\u6b21\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u975e\u6df1\u5ea6\u7ebf\u6bb5\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u5f97\u51fa\u4e86ScaleLSD\u662f\u7b2c\u4e00\u4e2a\u5728\u6240\u6d4b\u8bd5\u7684\u6240\u6709\u65b9\u9762\u90fd\u4f18\u4e8e\u539f\u6709\u7684\u975e\u6df1\u5ea6LSD\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u6269\u5c55\u548c\u589e\u5f3a\u4e86\u56fe\u50cf\u7ebf\u51e0\u4f55\u7684\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2506.09457", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09457", "abs": "https://arxiv.org/abs/2506.09457", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u79f0\u4e3aPrefix-Oriented Equal-length Training (POET)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u622a\u65ad\u4f18\u9009\u548c\u975e\u4f18\u9009\u54cd\u5e94\u4ee5\u5339\u914d\u8f83\u77ed\u54cd\u5e94\u7684\u957f\u5ea6\uff0c\u4ee5\u89e3\u51b3Direct Alignment Algorithms (DAAs)\u4e2d\u5b58\u5728\u7684\u5956\u52b1\u751f\u6210\u5dee\u8ddd\u95ee\u9898\uff0c\u4ece\u800c\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u89e3\u51b3Direct Alignment Algorithms (DAAs)\u4e2d\u7684\u201c\u5956\u52b1\u751f\u6210\u5dee\u8ddd\u201d\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u6e90\u4e8e\u8bad\u7ec3\u671f\u95f4\u4f18\u5316\u76ee\u6807\u4e0e\u63a8\u7406\u671f\u95f4\u5b9e\u9645\u751f\u6210\u8868\u73b0\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "method": "\u5f15\u5165POET\u65b9\u6cd5\uff0c\u5373\u901a\u8fc7\u622a\u65ad\u4f18\u9009\u548c\u975e\u4f18\u9009\u54cd\u5e94\u4ee5\u5339\u914d\u8f83\u77ed\u54cd\u5e94\u7684\u957f\u5ea6\uff0c\u4ece\u800c\u89e3\u51b3\u5956\u52b1\u751f\u6210\u5dee\u8ddd\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6bd4DPO\u548cSimPO\u7b49DAAs\u7684\u4f20\u7edf\u5b9e\u73b0\u65b9\u5f0f\uff0cPOET\u65b9\u6cd5\u5728AlpacaEval 2\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u6700\u591a15.6\u5206\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5747\u6709\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7a81\u663e\u4e86\u5728DAAs\u4e2d\u89e3\u51b3\u5956\u52b1\u4f18\u5316\u4e0e\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u5bf9\u9f50\u95ee\u9898\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.09378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09378", "abs": "https://arxiv.org/abs/2506.09378", "authors": ["Qijian Tian", "Xin Tan", "Jingyu Gong", "Yuan Xie", "Lizhuang Ma"], "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images", "comment": null, "summary": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and\nsemantic field reconstruction. Combining 3D scenes with semantic fields\nfacilitates the perception and understanding of the surrounding environment.\nHowever, key challenges include embedding semantics into 3D representations,\nachieving generalizable real-time reconstruction, and ensuring practical\napplicability by using only images as input without camera parameters or ground\ntruth depth. To this end, we propose UniForward, a feed-forward model to\npredict 3D Gaussians with anisotropic semantic features from only uncalibrated\nand unposed sparse-view images. To enable the unified representation of the 3D\nscene and semantic field, we embed semantic features into 3D Gaussians and\npredict them through a dual-branch decoupled decoder. During training, we\npropose a loss-guided view sampler to sample views from easy to hard,\neliminating the need for ground truth depth or masks required by previous\nmethods and stabilizing the training process. The whole model can be trained\nend-to-end using a photometric loss and a distillation loss that leverages\nsemantic features from a pre-trained 2D semantic model. At the inference stage,\nour UniForward can reconstruct 3D scenes and the corresponding semantic fields\nin real time from only sparse-view images. The reconstructed 3D scenes achieve\nhigh-quality rendering, and the reconstructed 3D semantic field enables the\nrendering of view-consistent semantic features from arbitrary views, which can\nbe further decoded into dense segmentation masks in an open-vocabulary manner.\nExperiments on novel view synthesis and novel view segmentation demonstrate\nthat our method achieves state-of-the-art performances for unifying 3D scene\nand semantic field reconstruction.", "AI": {"tldr": "This paper introduces UniForward, a model that reconstructs 3D scenes and semantic fields from sparse-view images in real time, achieving high-quality results without requiring camera parameters or depth information, and demonstrating state-of-the-art performance in novel view synthesis and segmentation.", "motivation": "The motivation is to overcome the challenges of embedding semantics into 3D representations, achieving real-time reconstruction, and ensuring the practical applicability using only images as inputs without additional information like camera parameters or ground truth depth.", "method": "Our method, UniForward, is a feed-forward model designed to predict 3D Gaussians with anisotropic semantic features from uncalibrated, sparse-view images. It employs a dual-branch decoupled decoder to unify the 3D scene and semantic field representation, and uses a loss-guided view sampler for training stability.", "result": "Experiments have shown that UniForward can reconstruct 3D scenes and semantic fields in real-time from sparse-view images, achieving high-quality rendering and view-consistent semantic features with state-of-the-art performance.", "conclusion": "The conclusion is that the proposed UniForward model effectively unifies 3D scene and semantic field reconstruction by predicting anisotropic semantic features in 3D Gaussians, from sparse-view input images, and achieves state-of-the-art performance without needing camera parameters or ground truth depth."}}
{"id": "2506.09495", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09495", "abs": "https://arxiv.org/abs/2506.09495", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790181\u4e2a\u66fe\u6709\u751f\u547d\u5371\u9669\u81ea\u6740\u5c1d\u8bd5\u8005\u7684YouTube\u9891\u9053\u89c6\u9891\uff0c\u7ed3\u5408134\u4e2a\u5bf9\u7167\u7ec4\u9891\u9053\uff0c\u91c7\u7528\u8ba1\u7b97\u3001\u6df7\u5408\u548c\u4e13\u5bb6\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u8bc6\u522b\u51fa\u4e0e\u81ea\u6740\u884c\u4e3a\u76f8\u5173\u7684\u4e3b\u9898\uff0c\u53d1\u73b0\u5fc3\u7406\u5065\u5eb7\u6323\u624e\u548cYouTube\u4e92\u52a8\u662f\u4e24\u4e2a\u4e3b\u8981\u6307\u6807\uff0c\u4e14\u52a8\u673a\u7684\u5dee\u5f02\u8868\u660e\u81ea\u6740\u8005\u5e0c\u671b\u5e2e\u52a9\u4ed6\u4eba\u6216\u4e2a\u4eba\u5eb7\u590d\u3002", "motivation": "\u9274\u4e8e\u81ea\u6740\u5728\u5168\u7403\u591a\u4e2a\u897f\u65b9\u56fd\u5bb6\u662f\u4e3b\u8981\u6b7b\u56e0\uff0c\u8be5\u7814\u7a76\u57fa\u4e8e\u793e\u4ea4\u5a92\u4f53\u4e0a\u5185\u5bb9\uff0c\u7279\u522b\u662fYouTube\u89c6\u9891\uff0c\u63a2\u8ba8\u81ea\u6740\u884c\u4e3a\u5982\u4f55\u5448\u73b0\u53ca\u5176\u4e0e\u4e13\u5bb6\u77e5\u8bc6\u7684\u5dee\u5f02\uff0c\u4ee5\u671f\u4e3a\u81ea\u6740\u884c\u4e3a\u7684\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u8ba1\u7b97\u81ea\u4e0b\u800c\u4e0a\u3001\u6df7\u5408\u548c\u4e13\u5bb6\u9a71\u52a8\u81ea\u4e0a\u800c\u4e0b\u7684\u4e09\u4e2a\u65b9\u6cd5\u6765\u5206\u6790\u6570\u636e\uff0c\u5305\u62ec\u957f\u5468\u671f\u7684181\u4e2a\u81ea\u6740\u5c1d\u8bd5\u8005\u7684YouTube\u9891\u9053\uff0c\u5e94\u7528LLM\u4e3b\u9898\u5efa\u6a21\u6765\u8bc6\u522b\u884c\u4e3a\u6307\u793a\u5668\uff0c\u5e76\u7ed3\u5408\u4e34\u5e8a\u4e13\u5bb6\u7684\u5ba1\u6838\u548c\u5fc3\u7406\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\u8bc6\u522b\u4e86\u4e24\u4e2a\u4e0e\u81ea\u6740\u884c\u4e3a\u663e\u8457\u76f8\u5173\u7684\u4e3b\u9898\uff1a\u5fc3\u7406\u5065\u5eb7\u6323\u624e\u548cYouTube\u4e92\u52a8\uff1b\u6df7\u5408\u65b9\u6cd5\u7531\u4e13\u5bb6\u5ba1\u6838\uff0c\u4f46\u672a\u53d1\u73b0\u989d\u5916\u7684\u4e3b\u9898\u3002\u81ea\u4e0a\u800c\u4e0b\u7684\u5fc3\u7406\u8bc4\u4f30\u63ed\u793a\u4e86\u81ea\u6740\u8005\u4e0a\u4f20\u89c6\u9891\u524d\u540e\u5728\u52a8\u673a\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5206\u522b\u5e0c\u671b\u901a\u8fc7\u5206\u4eab\u7ecf\u5386\u5e2e\u52a9\u4ed6\u4eba\u6216\u4f5c\u4e3a\u4e2a\u4eba\u5eb7\u590d\u7684\u4e00\u90e8\u5206\u3002", "conclusion": "\u7814\u7a76\u6574\u5408\u4e86\u81ea\u4e0b\u800c\u4e0a\u548c\u81ea\u4e0a\u800c\u4e0b\u7684\u53d1\u73b0\uff0c\u6307\u51fa\u81ea\u6740\u884c\u4e3a\u4e0e\u6570\u5b57\u884c\u4e3a\u53ca\u4e34\u5e8a\u89c1\u89e3\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u5f3a\u8c03\u81ea\u4e0b\u800c\u4e0a\u65b9\u6cd5\u5728\u63ed\u793a\u5e73\u53f0\u7279\u5b9a\u6307\u6807\uff08\u5982YouTube\u4e92\u52a8\uff09\u7684\u4ef7\u503c\u3002"}}
