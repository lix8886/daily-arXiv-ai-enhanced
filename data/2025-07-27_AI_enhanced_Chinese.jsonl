{"id": "2507.17801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17801", "abs": "https://arxiv.org/abs/2507.17801", "authors": ["Yi Xin", "Juncheng Yan", "Qi Qin", "Zhen Li", "Dongyang Liu", "Shicheng Li", "Victor Shea-Jay Huang", "Yupeng Zhou", "Renrui Zhang", "Le Zhuo", "Tiancheng Han", "Xiaoqing Sun", "Siqi Luo", "Mengmeng Wang", "Bin Fu", "Yuewen Cao", "Hongsheng Li", "Guangtao Zhai", "Xiaohong Liu", "Yu Qiao", "Peng Gao"], "title": "Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling", "comment": "Tech Report, 23 pages, 11 figures, 7 tables", "summary": "We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model\nthat revisits and revitalizes the autoregressive paradigm for high-quality\nimage generation and beyond. Unlike existing approaches that rely on pretrained\ncomponents or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from\nscratch, enabling unrestricted architectural design and licensing freedom. It\nachieves generation quality on par with state-of-the-art diffusion models such\nas DALL-E 3 and SANA, while preserving the inherent flexibility and\ncompositionality of autoregressive modeling. Our unified tokenization scheme\nallows the model to seamlessly handle a wide spectrum of tasks-including\nsubject-driven generation, image editing, controllable synthesis, and dense\nprediction-within a single generative framework. To further boost usability, we\nincorporate efficient decoding strategies like inference-time scaling and\nspeculative Jacobi sampling to improve quality and speed, respectively.\nExtensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)\ndemonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses\ndiffusion-based models. Moreover, we confirm its multi-task capabilities on the\nGraph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally\nwell. These results position Lumina-mGPT 2.0 as a strong, flexible foundation\nmodel for unified multimodal generation. We have released our training details,\ncode, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.", "AI": {"tldr": "Lumina-mGPT 2.0 是一种独立的，解码器仅有的自回归模型，专用于高质量图像生成及其他任务，其性能可媲美顶级扩散模型，并具有自回归建模的灵活性和组合性。通过统一标记方案和高效解码策略，该模型在多项基准测试中表现出色，展示了其作为统一多模态生成基础模型的潜力。", "motivation": "解决现有方法依赖预训练组件或混合架构的问题，提供一个完全从零开始训练的自回归模型，以实现无限制的架构设计和许可证自由。", "method": "使用统一标记方案来处理各种任务（如主题驱动生成、图像编辑、可控合成和密集预测）并在单一生成框架内无缝操作。引入了如推理时缩放和推测Jacobi采样等高效解码策略以提高质量和速度。", "result": "在标准文本到图像基准测试（如GenEval、DPG）中，Lumina-mGPT 2.0 不仅匹配甚至在某些情况下超越了扩散模型。在Graph200K基准测试中，原生Lumina-mGPT 2.0 表现非常出色。", "conclusion": "Lumina-mGPT 2.0 被定位为统一多模态生成的强大灵活的基础模型，其训练细节、代码和模型已在GitHub上公开。"}}
{"id": "2507.17844", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17844", "abs": "https://arxiv.org/abs/2507.17844", "authors": ["Sai Varun Kodathala", "Yashwanth Reddy Vutukoori", "Rakesh Vunnam"], "title": "SV3.3B: A Sports Video Understanding Model for Action Recognition", "comment": "8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025", "summary": "This paper addresses the challenge of automated sports video analysis, which\nhas traditionally been limited by computationally intensive models requiring\nserver-side processing and lacking fine-grained understanding of athletic\nmovements. Current approaches struggle to capture the nuanced biomechanical\ntransitions essential for meaningful sports analysis, often missing critical\nphases like preparation, execution, and follow-through that occur within\nseconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B\nparameter video understanding model that combines novel temporal motion\ndifference sampling with self-supervised learning for efficient on-device\ndeployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction\nmechanism that intelligently identifies the 16 most representative frames from\nsports sequences, followed by a V-DWT-JEPA2 encoder pretrained through\nmask-denoising objectives and an LLM decoder fine-tuned for sports action\ndescription generation. Evaluated on a subset of the NSVA basketball dataset,\nSV3.3B achieves superior performance across both traditional text generation\nmetrics and sports-specific evaluation criteria, outperforming larger\nclosed-source models including GPT-4o variants while maintaining significantly\nlower computational requirements. Our model demonstrates exceptional capability\nin generating technically detailed and analytically rich sports descriptions,\nachieving 29.2% improvement over GPT-4o in ground truth validation metrics,\nwith substantial improvements in information density, action complexity, and\nmeasurement precision metrics essential for comprehensive athletic analysis.\nModel Available at https://huggingface.co/sportsvision/SV3.3B.", "AI": {"tldr": "本文提出SV3.3B，一种轻量级的视频理解模型，适用于设备端部署的体育视频分析，实现了对运动动作的细致分析，超越了更大规模的专有模型的同时，计算需求更低。", "motivation": "解决传统体育视频分析模型计算密集、依赖服务器且缺乏对运动员动作精细理解的问题。", "method": "结合新颖的时序运动差异采样与自监督学习，通过DWT-VGG16-LDA方法提取关键帧，使用V-DWT-JEPA2编码器和语义生成解码器对体育动作描述进行预训练和微调。", "result": "在NSVA篮球数据集的子集上评估，SV3.3B在文本生成度量和体育特定位准方面均表现出色，尤其是在信息密度、动作复杂性和测量精度方面显著提升。", "conclusion": "SV3.3B模型在生成技术详细和分析复杂度高的体育描述方面具有卓越能力，其表现优于GPT-4o模型，并在关键指标上提升了29.2%。"}}
{"id": "2507.17853", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17853", "abs": "https://arxiv.org/abs/2507.17853", "authors": ["Lifeng Chen", "Jiner Wang", "Zihao Pan", "Beier Zhu", "Xiaofeng Yang", "Chi Zhang"], "title": "Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models", "comment": null, "summary": "Recent advances in text-to-image (T2I) generation have led to impressive\nvisual results. However, these models still face significant challenges when\nhandling complex prompt, particularly those involving multiple subjects with\ndistinct attributes. Inspired by the human drawing process, which first\noutlines the composition and then incrementally adds details, we propose\nDetail++, a training-free framework that introduces a novel Progressive Detail\nInjection (PDI) strategy to address this limitation. Specifically, we decompose\na complex prompt into a sequence of simplified sub-prompts, guiding the\ngeneration process in stages. This staged generation leverages the inherent\nlayout-controlling capacity of self-attention to first ensure global\ncomposition, followed by precise refinement. To achieve accurate binding\nbetween attributes and corresponding subjects, we exploit cross-attention\nmechanisms and further introduce a Centroid Alignment Loss at test time to\nreduce binding noise and enhance attribute consistency. Extensive experiments\non T2I-CompBench and a newly constructed style composition benchmark\ndemonstrate that Detail++ significantly outperforms existing methods,\nparticularly in scenarios involving multiple objects and complex stylistic\nconditions.", "AI": {"tldr": "为了提高文本到图像生成模型在复杂提示场景中的性能，研究人员提出了Detail++，通过将复杂提示分解为一系列简化子提示，并采用质心对齐损失来减少绑定噪声，从而生成更复杂的图像。实验表明，该方法在多个指标上优于现有方法。", "motivation": "尽管文本到图像生成模型取得了显著进步，但在处理涉及多个具有不同属性对象的复杂提示时仍面临挑战。为了提升生成图像的准确性和复杂性，提出了Detail++框架。", "method": "Detail++框架通过引入Progressive Detail Injection (PDI)策略来解决复杂提示生成图像的挑战。该策略将复杂的提示分解为一系列简化子提示，指导生成过程分阶段进行。首先利用自注意力机制控制图像布局，确保全局组成，然后进行精确细化。为了减少绑定噪声并增强属性一致性，使用交叉注意力机制并在测试时引入质心对齐损失。", "result": "在T2I-CompBench和一个新构建的风格组合基准测试中的广泛实验表明，Detail++框架在涉及多个对象和复杂风格条件的情景下显著优于现有方法。", "conclusion": "通过Progressive Detail Injection策略和质心对齐损失，Detail++框架能够生成包含多种对象和复杂属性的高质量图像，尤其适用于文本到图像生成任务中的复杂提示场景。"}}
{"id": "2507.17842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17842", "abs": "https://arxiv.org/abs/2507.17842", "authors": ["Yimeng Zhang", "Tian Wang", "Jiri Gesi", "Ziyi Wang", "Yuxuan Lu", "Jiacheng Lin", "Sinong Zhan", "Vianne Gao", "Ruochen Jiao", "Junze Liu", "Kun Qian", "Yuxin Tang", "Ran Xue", "Houyu Zhang", "Qingjun Cui", "Yufan Guo", "Dakuo Wang"], "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in\ngenerating 'believable human-like' behavior in web environments. Prior work has\nexplored augmenting training data with LLM-synthesized rationales and applying\nsupervised fine-tuning (SFT) to enhance reasoning ability, which in turn can\nimprove downstream action prediction. However, the performance of such\napproaches remains inherently bounded by the reasoning capabilities of the\nmodel used to generate the rationales. In this paper, we introduce Shop-R1, a\nnovel reinforcement learning (RL) framework aimed at enhancing the reasoning\nability of LLMs for simulation of real human behavior in online shopping\nenvironments Specifically, Shop-R1 decomposes the human behavior simulation\ntask into two stages: rationale generation and action prediction, each guided\nby distinct reward signals. For rationale generation, we leverage internal\nmodel signals (e.g., logit distributions) to guide the reasoning process in a\nself-supervised manner. For action prediction, we propose a hierarchical reward\nstructure with difficulty-aware scaling to prevent reward hacking and enable\nfine-grained reward assignment. This design evaluates both high-level action\ntypes and the correctness of fine-grained sub-action details (attributes and\nvalues), rewarding outputs proportionally to their difficulty. Experimental\nresults show that our method achieves a relative improvement of over 65%\ncompared to the baseline.", "AI": {"tldr": "This paper introduces Shop-R1, a reinforcement learning framework for improving the reasoning and action prediction of LLMs to simulate human behavior more accurately in online shopping environments.", "motivation": "To overcome the performance limitations of previous methods in enhancing LLMs' reasoning abilities, which are bounded by the reasoning capabilities of the model used for rationale generation.", "method": "Shop-R1, a novel reinforcement learning (RL) framework that decomposes the human behavior simulation task into reasoning and action prediction stages, each guided by distinct reward signals for better simulation of human behavior in online shopping environments.", "result": "Experimental results show a relative improvement of over 65% compared to the baseline method.", "conclusion": "The Shop-R1 framework demonstrates significant improvement in simulating human-like behavior in online shopping environments compared to previous methods, achieving a substantial relative improvement over the baseline."}}
{"id": "2507.17859", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17859", "abs": "https://arxiv.org/abs/2507.17859", "authors": ["Muayad Abujabal", "Lyes Saad Saoud", "Irfan Hussain"], "title": "FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains", "comment": null, "summary": "Accurate fish detection in underwater imagery is essential for ecological\nmonitoring, aquaculture automation, and robotic perception. However, practical\ndeployment remains limited by fragmented datasets, heterogeneous imaging\nconditions, and inconsistent evaluation protocols. To address these gaps, we\npresent \\textit{FishDet-M}, the largest unified benchmark for fish detection,\ncomprising 13 publicly available datasets spanning diverse aquatic environments\nincluding marine, brackish, occluded, and aquarium scenes. All data are\nharmonized using COCO-style annotations with both bounding boxes and\nsegmentation masks, enabling consistent and scalable cross-domain evaluation.\nWe systematically benchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.\nEvaluations are conducted using standard metrics including mAP, mAP@50, and\nmAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and\ninference profiling in terms of latency and parameter count. The results\nhighlight the varying detection performance across models trained on FishDet-M,\nas well as the trade-off between accuracy and efficiency across models of\ndifferent architectures. To support adaptive deployment, we introduce a\nCLIP-based model selection framework that leverages vision-language alignment\nto dynamically identify the most semantically appropriate detector for each\ninput image. This zero-shot selection strategy achieves high performance\nwithout requiring ensemble computation, offering a scalable solution for\nreal-time applications. FishDet-M establishes a standardized and reproducible\nplatform for evaluating object detection in complex aquatic scenes. All\ndatasets, pretrained models, and evaluation tools are publicly available to\nfacilitate future research in underwater computer vision and intelligent marine\nsystems.", "AI": {"tldr": " 介绍FishDet-M基准和一个零样本模型选择策略以突破鱼类检测在水下场景中的瓶颈。", "motivation": " 寻找一种方法来解决水下成像条件下鱼类检测面临的挑战，包括数据集分散、成像条件异构以及评估协议不一致。", "method": " danmark", "result": "{{\"name\": \"Structure\", \"arguments\": {\"tldr\": \"本文介绍了FishDet-M，这是一个最大的统一鱼类检测基准，包含多种类型的水下场景，用于克服数据集分散、异构成像条件和评估协议不一致的问题。文章系统地评估了28种目标检测模型，提出了基于CLIP的模型选择框架，以实现零样本下对于水下场景的适应性部署。\", \"motivation\": \"准确的鱼类检测在水下图像领域对于生态监测、养殖自动化和机器人感知至关重要。但实践部署受到数据集分散、异构成像条件和评估协议不一致的限制。\", \"method\": \"本文提出FishDet-M基准，整合了多种水下环境的公开数据集并统一标注格式。系统评估了28种当代目标检测模型，涵盖YOLOv8至YOLOv12系列以及不同的检测模型架构。还引入了基于CLIP的模型选择框架。\", \"result\": \"实验结果揭示了在FishDet-M上训练的不同模型的检测表现差异，展示了准确性和效率之间的权衡。基于CLIP的零样本模型选择策略在实际应用中到达了高表现，并且无需集成计算。\", \"conclusion\": \"FishDet-M为复杂的水下场景检测建立了一个标准化和可复现的评估平台，所有数据集、预训练模型和评估工具均公开，推动水下计算机视觉和智能海洋系统的未来研究。\"}}}", "conclusion": " FishDet-M为提升鱼类检测性能提供了一个综合解决方案，推动相关技术的发展。"}}
{"id": "2507.17849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17849", "abs": "https://arxiv.org/abs/2507.17849", "authors": ["Zhangyue Yin", "Qiushi Sun", "Zhiyuan Zeng", "Qinyuan Cheng", "Xipeng Qiu", "Xuanjing Huang"], "title": "Dynamic and Generalizable Process Reward Modeling", "comment": "Accepted by ACL 2025 Main", "summary": "Process Reward Models (PRMs) are crucial for guiding Large Language Models\n(LLMs) in complex scenarios by providing dense reward signals. However,\nexisting PRMs primarily rely on heuristic approaches, which struggle with\ncross-domain generalization. While LLM-as-judge has been proposed to provide\ngeneralized rewards, current research has focused mainly on feedback results,\noverlooking the meaningful guidance embedded within the text. Additionally,\nstatic and coarse-grained evaluation criteria struggle to adapt to complex\nprocess supervision. To tackle these challenges, we propose Dynamic and\nGeneralizable Process Reward Modeling (DG-PRM), which features a reward tree to\ncapture and store fine-grained, multi-dimensional reward criteria. DG-PRM\ndynamically selects reward signals for step-wise reward scoring. To handle\nmultifaceted reward signals, we pioneeringly adopt Pareto dominance estimation\nto identify discriminative positive and negative pairs. Experimental results\nshow that DG-PRM achieves stunning performance on prevailing benchmarks,\nsignificantly boosting model performance across tasks with dense rewards.\nFurther analysis reveals that DG-PRM adapts well to out-of-distribution\nscenarios, demonstrating exceptional generalizability.", "AI": {"tldr": "提出了动态和泛化的过程奖励模型（DG-PRM），采用了奖励树结构和帕累托优势估计，实现了在多任务中的优异性能和泛化能力。", "motivation": "现有的过程奖励模型（PRMs）主要依赖于启发式方法，难以跨领域泛化。尽管利用LLM作为评判者被提出以提供泛化的奖励，但当前研究主要关注反馈结果，而忽略了文本中的有意义指导。同时，静态和粗粒度的评估标准难以适应复杂的流程监督。", "method": "Dynamic and Generalizable Process Reward Modeling (DG-PRM)采用奖励树来捕获和存储细粒度的多维奖励标准，并动态选择奖励信号进行逐步骤的奖励评分。为了处理多方面的奖励信号，首次采用帕累托优势估计来识别有区分度的正负样本对。", "result": "实验结果表明，DG-PRM在主流基准上取得了卓越性能，显著提升了具有密集奖励任务的模型性能。进一步分析表明，DG-PRM能够很好地适应分布外场景，表现出色的泛化能力。", "conclusion": "DG-PRM通过引入奖励树和帕累托优势估计，有效地解决了现有PRMs跨域泛化难、静态评估标准不适应复杂情境的问题，证明了其在多个任务上以及面对分布外场景时的泛化能力和卓越性能。"}}
{"id": "2507.17860", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17860", "abs": "https://arxiv.org/abs/2507.17860", "authors": ["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"], "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "comment": null, "summary": "Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.", "AI": {"tldr": "研究使用生成模型评估公开黑色素瘤分类器的公平性，强调使用合成数据评估的潜力以及面临的挑战。", "motivation": "评估和改善此类系统公平性的重要性，尤其是在边缘设备上的深度学习技术应用于皮肤癌（如黑色素瘤）的常规筛查的背景下。", "method": "利用最先进的生成式AI模型（LightningDiT）来评估公开可用的黑色素瘤分类器的公平性。", "result": "研究结果表明，使用高度真实的合成数据评估公平性是一个有前景的方向，但当评估使用的黑色素瘤检测模型训练数据与生成合成图像的数据集不同时，验证公平性变得困难。", "conclusion": "研究提出了使用合成数据评估和增强医学影像生成式AI系统的公平性的一个有价值的新型途径。"}}
{"id": "2507.17896", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.17896", "abs": "https://arxiv.org/abs/2507.17896", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL", "comment": null, "summary": "Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity.", "AI": {"tldr": "VeriMinder 是一个用于检测和缓解自然语言接口数据库系统中分析偏差的交互式系统，通过三个关键创新实现了这一目标：为特定分析场景创建语义映射框架、采用难以变化原理的分析框架以及通过多候选方案、批评反馈和自我反思生成高质量的任务特定提示词。实验结果显示，该系统显著提升了分析的质量、全面性和准确性。", "motivation": "许多没有统计分析背景的用户使用自然语言接口数据库系统进行数据分析时，往往会忽视认知偏差所带来的挑战。因此，解决这些潜在的偏差问题被认为是至关重要的。", "method": "通过引入三个关键创新来解决认知偏差问题，分别是：创建一个语义映射框架以识别特定分析环境中的偏差、操作化的难以变化分析框架以系统地指导数据分析、利用优化的LLM（大规模语言模型）系统生成高质量的任务特定提示词。", "result": "实验测评中，82.5%的参与者报告说该系统对分析质量产生了积极的影响，在比较评估中，VeriMinder的表现明显优于其他方法，至少在20%的数量级上更好地满足了分析质量和全面性的要求。", "conclusion": "VeriMinder系统有效辅助用户在数据分析过程中避免因提出“错误问题”导致的漏洞，并作为开源软件提供给研究社区和从业者使用。"}}
{"id": "2507.17892", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17892", "abs": "https://arxiv.org/abs/2507.17892", "authors": ["Hanzhou Liu", "Binghan Li", "Chengkai Liu", "Mi Lu"], "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration", "comment": null, "summary": "Transformers, with their self-attention mechanisms for modeling long-range\ndependencies, have become a dominant paradigm in image restoration tasks.\nHowever, the high computational cost of self-attention limits scalability to\nhigh-resolution images, making efficiency-quality trade-offs a key research\nfocus. To address this, Restormer employs channel-wise self-attention, which\ncomputes attention across channels instead of spatial dimensions. While\neffective, this approach may overlook localized artifacts that are crucial for\nhigh-quality image restoration. To bridge this gap, we explore Dilated\nNeighborhood Attention (DiNA) as a promising alternative, inspired by its\nsuccess in high-level vision tasks. DiNA balances global context and local\nprecision by integrating sliding-window attention with mixed dilation factors,\neffectively expanding the receptive field without excessive overhead. However,\nour preliminary experiments indicate that directly applying this global-local\ndesign to the classic deblurring task hinders accurate visual restoration,\nprimarily due to the constrained global context understanding within local\nattention. To address this, we introduce a channel-aware module that\ncomplements local attention, effectively integrating global context without\nsacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based\narchitecture specifically designed for image restoration, achieves competitive\nresults across multiple benchmarks, offering a high-quality solution for\ndiverse low-level computer vision problems.", "AI": {"tldr": "本文提出了DiNAT-IR架构，它结合了通道感知模块来提升Dilated Neighborhood Attention（DiNA）在图像复原任务中的性能，该方法在多个基准测试中实现了有竞争力的结果。", "motivation": "鉴于自我关注机制在处理图像复原任务时的计算成本高昂，特别是在高分辨率图像上，作者提出改善自我关注机制的效率，同时保持高质量的图像恢复，特别是解决局部细节遗漏的问题。", "method": "Restormer采用通道自我关注机制来处理图像复原任务。然而，由于直接使用全局-局部设计在经典的去模糊任务上会导致准确复原的困难，本文提出了一种基于通道感知模块来补充局部注意的新方法，以有效整合全局上下文信息而不牺牲像素级精度。", "result": "DiNAT-IR架构在多个基准测试中展示了其在处理低级计算机视觉问题中的高效率和高质量，特别在图像复原任务中取得了显著效果。", "conclusion": "研究表明，通过引入新的通道感知模块，可以有效提升基于Transformer的图像复原模型在全局上下文和局部细节处理上的平衡，实现了图像复原任务中的高质量恢复。"}}
{"id": "2507.17918", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17918", "abs": "https://arxiv.org/abs/2507.17918", "authors": ["Nhan Phan", "Anusha Porwal", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tamás Grósz", "Mikko Kurimo"], "title": "One Whisper to Grade Them All", "comment": "Accepted to SLaTE 2025 workshop", "summary": "We present an efficient end-to-end approach for holistic Automatic Speaking\nAssessment (ASA) of multi-part second-language tests, developed for the 2025\nSpeak & Improve Challenge. Our system's main novelty is the ability to process\nall four spoken responses with a single Whisper-small encoder, combine all\ninformation via a lightweight aggregator, and predict the final score. This\narchitecture removes the need for transcription and per-part models, cuts\ninference time, and makes ASA practical for large-scale Computer-Assisted\nLanguage Learning systems.\n  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming\nthe text-based baseline (0.44) while using at most 168M parameters (about 70%\nof Whisper-small). Furthermore, we propose a data sampling strategy, allowing\nthe model to train on only 44.8% of the speakers in the corpus and still reach\n0.383 RMSE, demonstrating improved performance on imbalanced classes and strong\ndata efficiency.", "AI": {"tldr": "该研究提出了一种高效的整体自动口语评估方法，适用于多部分的二语测试，并在2025 Speak & Improve挑战赛中取得了优异效果。系统通过单一的Whisper-small编码器处理所有口语回应，并使用轻量级聚合器组合信息并预测最终分数，大幅减少了推理时间，提高了ASL学习系统的实用性。", "motivation": "解决现有自动口语评估系统中存在的复杂模型结构和长时间推理问题，提高ASL系统在大规模数据中的应用效率。", "method": "使用Whisper-small编码器处理所有口语回应，通过轻量级聚合器将信息组合，并预测最终分数，同时提出了一种新的数据抽样策略。", "result": "系统达到了0.384的RMSE，优于文本基线系统（0.44），同时使用不超过168M参数。在仅使用语料库中44.8%的说话者数据训练时，系统仍能达到0.383的RMSE，提高了在不平衡分类中的性能和数据效率。", "conclusion": "提出的系统方法展示了高的自动口语评估性能和数据效率，在ASL系统中具有良好的应用前景。"}}
{"id": "2507.17957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17957", "abs": "https://arxiv.org/abs/2507.17957", "authors": ["Md. Al-Masrur Khan", "Durgakant Pushp", "Lantao Liu"], "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation", "comment": null, "summary": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is\ntrained on labeled source domain data (e.g., synthetic images) and adapted to\nan unlabeled target domain (e.g., real-world images) without access to target\nannotations. Existing UDA-SS methods often struggle to balance fine-grained\nlocal details with global contextual information, leading to segmentation\nerrors in complex regions. To address this, we introduce the Adaptive Feature\nRefinement (AFR) module, which enhances segmentation accuracy by refining\nhighresolution features using semantic priors from low-resolution logits. AFR\nalso integrates high-frequency components, which capture fine-grained\nstructures and provide crucial boundary information, improving object\ndelineation. Additionally, AFR adaptively balances local and global information\nthrough uncertaintydriven attention, reducing misclassifications. Its\nlightweight design allows seamless integration into HRDA-based UDA methods,\nleading to state-of-the-art segmentation performance. Our approach improves\nexisting UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on\nSynthia-->Cityscapes. The implementation of our framework is available at:\nhttps://github.com/Masrur02/AFRDA", "AI": {"tldr": "论文提出了一种Adaptive Feature Refinement（AFR）模块，旨在解决无监督领域适应语义分割问题中细节和全局信息的平衡问题，通过改进高分辨率特征的精确度，提升分割性能。", "motivation": "针对无监督领域适应语义分割中现有方法难以平衡细节与全局信息的问题，导致复杂区域分割错误。", "method": "AFR模块通过使用来自低分辨率日志的语义先验来增强高分辨率特征的分割准确性，同时集成高频分量以捕捉细微结构并提供关键边界信息，通过不确定性驱动的注意力机制平衡局部和全局信息。", "result": "AFR框架在GTA V到Cityscapes的分割准确度上提高了1.05%，在Synthia到Cityscapes上提高了1.04%。", "conclusion": "AFR模块通过轻量级设计，能够无缝集成到HRDA基础的UDA方法中，实现分割性能的最先进水平。"}}
{"id": "2507.17944", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17944", "abs": "https://arxiv.org/abs/2507.17944", "authors": ["Hulayyil Alshammari", "Praveen Rao"], "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text", "comment": null, "summary": "Large language models (LLMs) have rapidly transformed the creation of written\nmaterials. LLMs have led to questions about writing integrity, thereby driving\nthe creation of artificial intelligence (AI) detection technologies.\nAdversarial attacks, such as standard and humanized paraphrasing, inhibit\ndetectors' ability to detect machine-generated text. Previous studies have\nmainly focused on ChatGPT and other well-known LLMs and have shown varying\naccuracy across detectors. However, there is a clear gap in the literature\nabout DeepSeek, a recently published LLM. Therefore, in this work, we\ninvestigate whether six generally accessible AI detection tools -- AI Text\nClassifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can\nconsistently recognize text generated by DeepSeek. The detectors were exposed\nto the aforementioned adversarial attacks. We also considered DeepSeek as a\ndetector by performing few-shot prompting and chain-of-thought reasoning (CoT)\nfor classifying AI and human-written text. We collected 49 human-authored\nquestion-answer pairs from before the LLM era and generated matching responses\nusing DeepSeek-v3, producing 49 AI-generated samples. Then, we applied\nadversarial techniques such as paraphrasing and humanizing to add 196 more\nsamples. These were used to challenge detector robustness and assess accuracy\nimpact. While QuillBot and Copyleaks showed near-perfect performance on\noriginal and paraphrased DeepSeek text, others -- particularly AI Text\nClassifier and GPT-2 -- showed inconsistent results. The most effective attack\nwas humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and\n52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best\nfive-shot result misclassifying only one of 49 samples (AI recall 96%, human\nrecall 100%).", "AI": {"tldr": "此研究评估了六种AI检测工具（AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, GPTZero）对DeepSeek生成文本的检测效果，并发现QuillBot和Copyleaks在识别原版文本和改述文本时表现优异，而其他工具在检测对抗性改述和人性化处理时表现不佳。此外，使用DeepSeek自身进行文本分类显示出高准确度。", "motivation": "该研究旨在填补文献中关于DeepSeek这一新发布的大型语言模型的检测技术的空白，同时评估现有AI检测工具在面对DeepSeek生成文本时的性能，特别是对抗技术的影响。", "method": "本研究调查了六个常见的AI检测工具（AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, GPTZero）对DeepSeek生成文本的检测能力，并探索了DeepSeek自身通过few-shot prompting和chain-of-thought (CoT)推理分类AI生成文本和人类编写文本的效能。研究数据包括49个人类编写的问答对及其对应的DeepSeek-v3生成的回答，以及通过对抗技术（如改述和人性化处理）额外增加的196个样本。", "result": "研究结果表明，QuillBot和Copyleaks在识别DeepSeek原始生成文本和改述文本时表现几乎完美，而AI Text Classifier和GPT-2则表现不一致。对抗技术中，人性化处理最有效地降低了检测准确率，分别为Copyleaks 71%，QuillBot 58%，GPTZero 52%。采用few-shot和CoT prompting方法识别AI生成文本的效果非常好，其中五次示例的最优秀结果仅将49个样本中的一个误分类。", "conclusion": "研究显示，现有的AI检测工具在检测DeepSeek生成文本时效果参差不齐，尤其是对抗性改述和人性化处理技术大幅降低了检测准确性。而DeepSeek通过few-shot和CoT推理方法能有效且准确地识别AI和人类编写的文本。"}}
{"id": "2507.17959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17959", "abs": "https://arxiv.org/abs/2507.17959", "authors": ["Ali Abedi", "Sadaf Safa", "Tracey J. F. Colella", "Shehroz S. Khan"], "title": "OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments", "comment": "14 pages, 3 figures, 7 tables", "summary": "Engagement in virtual learning is essential for participant satisfaction,\nperformance, and adherence, particularly in online education and virtual\nrehabilitation, where interactive communication plays a key role. Yet,\naccurately measuring engagement in virtual group settings remains a challenge.\nThere is increasing interest in using artificial intelligence (AI) for\nlarge-scale, real-world, automated engagement recognition. While engagement has\nbeen widely studied in younger academic populations, research and datasets\nfocused on older adults in virtual and telehealth learning settings remain\nlimited. Existing methods often neglect contextual relevance and the\nlongitudinal nature of engagement across sessions. This paper introduces OPEN\n(Older adult Patient ENgagement), a novel dataset supporting AI-driven\nengagement recognition. It was collected from eleven older adults participating\nin weekly virtual group learning sessions over six weeks as part of cardiac\nrehabilitation, producing over 35 hours of data, making it the largest dataset\nof its kind. To protect privacy, raw video is withheld; instead, the released\ndata include facial, hand, and body joint landmarks, along with affective and\nbehavioral features extracted from video. Annotations include binary engagement\nstates, affective and behavioral labels, and context-type indicators, such as\nwhether the instructor addressed the group or an individual. The dataset offers\nversions with 5-, 10-, 30-second, and variable-length samples. To demonstrate\nutility, multiple machine learning and deep learning models were trained,\nachieving engagement recognition accuracy of up to 81 percent. OPEN provides a\nscalable foundation for personalized engagement modeling in aging populations\nand contributes to broader engagement recognition research.", "AI": {"tldr": "论文介绍了OPEN数据集，这是针对老年人在虚拟学习中参与度识别的AI驱动数据集，包含超过35小时的数据。使用机器学习和深度学习模型进行的测试显示出高达81%的参与识别准确率。", "motivation": "由于准确测量虚拟群体中的参与度仍然是一个挑战，特别是对于老年人在虚拟和远程医疗学习环境中的参与度研究和数据集相对较少。现有方法往往忽视了参与度在不同会话中的情境相关性和纵向特性。", "method": "数据集来自11名老年人每周参加为期6周的虚拟小组学习会议，产生的数据包括面部、手部和身体关节标志点以及从视频中提取的情感和行为特征。提供了5秒、10秒、30秒和可变长度样本的版本。", "result": "该论文引入了OPEN（老年人参与）数据集，旨在支持AI驱动的参与识别。该数据集来自11名老年人每周参加为期6周的虚拟小组学习会议，产生超过35小时的数据，是该领域最大的数据集。数据包括面部、手部和身体关节标志点以及从视频中提取的情感和行为特征。注释包括二元参与状态、情感和行为标签以及情境类型指标。论文还展示了使用机器学习和深度学习模型进行参与识别的准确性，最高可达81%。该数据集为老年人口的个性化参与建模提供了可扩展的基础。", "conclusion": "该数据集为AI驱动的老年人参与度识别提供了基础，展示了其在个性化参与建模方面的应用潜力，并将对更广泛的研究领域作出贡献。"}}
