{"id": "2510.25797", "categories": ["cs.CV", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25797", "abs": "https://arxiv.org/abs/2510.25797", "authors": ["Sai Likhith Karri", "Ansh Saxena"], "title": "Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks", "comment": null, "summary": "This study examines the effectiveness of spatio-temporal modeling and the\nintegration of spatial attention mechanisms in deep learning models for\nunderwater object detection. Specifically, in the first phase, the performance\nof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with\nthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is\ndeveloped, through the addition of a Convolutional Block Attention Module\n(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and\nT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the\nresearch highlights how temporal modeling improves detection accuracy in\ndynamic marine environments, particularly under conditions of sudden movements,\npartial occlusions, and gradual motion. The testing results showed that YOLOv5\nachieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM\noutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,\nhighlighting their superior accuracy and generalization in detecting complex\nobjects. The findings demonstrate that T-YOLOv5 significantly enhances\ndetection reliability compared to the standard model, while T-YOLOv5 with CBAM\nfurther improves performance in challenging scenarios, although there is a loss\nof accuracy when it comes to simpler scenarios.", "AI": {"tldr": "研究了在水下目标检测中，时空建模和深度学习模型中空间注意力机制的整合效果。结果显示，与标准YOLOv5相比，T-YOLOv5和T-YOLOv5加CBAM取得了更高的精度，特别是在复杂场景下的性能显著提升。", "motivation": "改进水下目标检测的准确性，特别是在动态海洋环境中，目标可能突然移动或被部分遮挡。", "method": "首先评估了T-YOLOv5相比标准YOLOv5的性能，然后开发了一个加入了Convolutional Block Attention Module (CBAM) 的增强版T-YOLOv5。", "result": "实验结果表明，标准YOLOv5的mAP@50-95为0.563，而T-YOLOv5和T-YOLOv5加CBAM的mAP@50-95分别达到了0.813和0.811。", "conclusion": "T-YOLOv5显著提高了检测可靠性，而T-YOLOv5加CBAM进一步提升了在复杂场景中的性能，但在简单场景中的精度略有下降。"}}
{"id": "2510.25897", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25897", "abs": "https://arxiv.org/abs/2510.25897", "authors": ["Nicolas Dufour", "Lucas Degeorge", "Arijit Ghosh", "Vicky Kalogeiton", "David Picard"], "title": "MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency", "comment": "Project page: https://nicolas-dufour.github.io/miro", "summary": "Current text-to-image generative models are trained on large uncurated\ndatasets to enable diverse generation capabilities. However, this does not\nalign well with user preferences. Recently, reward models have been\nspecifically designed to perform post-hoc selection of generated images and\nalign them to a reward, typically user preference. This discarding of\ninformative data together with the optimizing for a single reward tend to harm\ndiversity, semantic fidelity and efficiency. Instead of this post-processing,\nwe propose to condition the model on multiple reward models during training to\nlet the model learn user preferences directly. We show that this not only\ndramatically improves the visual quality of the generated images but it also\nsignificantly speeds up the training. Our proposed method, called MIRO,\nachieves state-of-the-art performances on the GenEval compositional benchmark\nand user-preference scores (PickAScore, ImageReward, HPSv2).", "AI": {"tldr": "本文提出了一种名为MIRO的方法，在训练过程中利用多个奖励模型直接学习用户偏好，提升了生成图像的视觉质量，同时加快了训练速度。", "motivation": "现有文本转图像生成模型的训练依赖于大量未经整理的数据集，以实现多样化的生成能力。然而，这并不能很好地满足用户的偏好。当前通过奖励模型来进行生成图像的后期选择，并优化用户偏好，这种方法往往会导致信息损失，损害多样性和效率。", "method": "通过在训练过程中利用多个奖励模型对模型进行条件设置，使模型能够直接学习用户偏好，从而避免在训练后选择生成的图像时导致的信息损失及单一奖励优化带来的多样性、语义保真度和效率的损害。这种方法称为MIRO，它在GenEval组合基准和用户偏好评分（PickAScore，ImageReward，HPSv2）上达到了最先进的性能。", "result": "该方法显著提升了生成图像的视觉质量，并且大大加速了模型的训练过程。", "conclusion": "通过MIRO模型，可以在训练过程中直接学习用户的偏好，解决了传统方法在训练后选择生成图像时存在的问题，从而不仅提高了图像的视觉质量，也优化了训练效率。"}}
{"id": "2510.25901", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.25901", "abs": "https://arxiv.org/abs/2510.25901", "authors": ["Denniz Goren", "Holger Caesar"], "title": "BikeScenes: Online LiDAR Semantic Segmentation for Bicycles", "comment": null, "summary": "The vulnerability of cyclists, exacerbated by the rising popularity of faster\ne-bikes, motivates adapting automotive perception technologies for bicycle\nsafety. We use our multi-sensor 'SenseBike' research platform to develop and\nevaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the\nautomotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg\nDataset, comprising 3021 consecutive LiDAR scans around the university campus\nof the TU Delft, semantically annotated for 29 dynamic and static classes. By\nevaluating model performance, we demonstrate that fine-tuning on our BikeScenes\ndataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly\noutperforming the 13.8% obtained with SemanticKITTI pre-training alone. This\nresult underscores the necessity and effectiveness of domain-specific training.\nWe highlight key challenges specific to bicycle-mounted, hardware-constrained\nperception systems and contribute the BikeScenes dataset as a resource for\nadvancing research in cyclist-centric LiDAR segmentation.", "AI": {"tldr": "该论文旨在提高骑车人的安全，通过SenseBike平台开发用了3D激光雷达切割技术，并创建了一个标注有自行车周围环境的数据集，证明了特定领域训练的有效性。", "motivation": "由于电动自行车的流行使骑车人的脆弱性加剧，因此有必要将汽车感知技术应用于自行车安全。", "method": "我们使用多传感器“SenseBike”研究平台，开发并评估了一种针对自行车的3D激光雷达分割方法。为了缩小汽车领域和自行车领域的差距，我们引入了新的BikeScenes-lidarseg数据集，包含了荷兰代尔夫特理工大学校园周围连续的3021次激光雷达扫描，对29类动态和静态物体进行了语义标注。", "result": "通过评估模型性能，我们发现与仅使用SemanticKITTI预训练相比，在我们的BikeScenes数据集上进行微调可以达到63.6%的平均交并比（mIoU），而SemanticKITTI预训练仅能达到13.8%。这证明了特定领域的训练不仅必要而且有效。", "conclusion": "我们强调了自行车上硬件受限感知系统的关键挑战，并贡献了BikeScenes数据集，这将有助于推进针对骑车人的LiDAR分割研究。"}}
{"id": "2510.25921", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.25921", "abs": "https://arxiv.org/abs/2510.25921", "authors": ["Nikola L. Kolev", "Tommaso Rodani", "Neil J. Curson", "Taylor J. Z. Stock", "Alberto Cazzaniga"], "title": "Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy", "comment": null, "summary": "Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and\natom manipulation, but its utility is often limited by tip degradation and slow\nserial data acquisition. Fabrication adds another layer of complexity since the\ntip is often subjected to large voltages, which may alter the shape of its\napex, requiring it to be conditioned. Here, we propose a machine learning (ML)\napproach for image repair and super-resolution to alleviate both challenges.\nUsing a dataset of only 36 pristine experimental images of Si(001):H, we\ndemonstrate that a physics-informed synthetic data generation pipeline can be\nused to train several state-of-the-art flow-matching and diffusion models.\nQuantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy\n(CMMD) score and structural similarity demonstrates that our models are able to\neffectively restore images and offer a two- to fourfold reduction in image\nacquisition time by accurately reconstructing images from sparsely sampled\ndata. Our framework has the potential to significantly increase STM\nexperimental throughput by offering a route to reducing the frequency of\ntip-conditioning procedures and to enhancing frame rates in existing high-speed\nSTM systems.", "AI": {"tldr": "本文通过机器学习方法解决了STM中的针尖退化和数据采集慢的问题，使用36张原始实验图像的Si(001):H数据集训练并验证了几种先进的流匹配和扩散模型，结果显示，图像修复和重建精度提高，同时图像采集时间减少。", "motivation": "扫描隧道显微镜（STM）可以进行原子级的成像和原子操纵，但其针尖退化和慢速串行数据采集限制了其使用。此外，在STM实验中，针尖通常会受到高压的影响，导致其顶端形状改变，需要针尖预处理。为解决这些问题，本文旨在通过机器学习技术改进STM性能。", "method": "本文提出了一种基于机器学习（ML）的方法，旨在通过图像修复和超分辨率来解决STM中针尖退化和数据采集慢的问题。使用只有36张原始实验图像的Si(001):H数据集，展示了可以使用物理信息的合成数据生成流水线来训练几种最先进的流匹配和扩散模型。", "result": "使用CMMD得分和结构相似性等指标的定量评估表明，所提出的模型能够有效修复图像，而且通过稀疏采样数据的精确重建，图像采集时间可减少到原来的四分之一到二分之一。", "conclusion": "本研究提出的框架有潜力通过降低针尖预处理频率和提高现有高速STM系统的帧率，显著提高STM实验的吞吐量。"}}
