<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations](https://arxiv.org/abs/2602.04982)
*Deepak Gupta,Davis Bartels,Dina Demner-Fuhsman*

Main category: cs.CL

> 本文提出了BioACE框架用于评估大规模语言模型在生物医学问答中生成的答案和引用的质量。

<details>
  <summary>Details</summary>

**Motivation:** 随着大规模语言模型在生成生物医学答案中的广泛应用，需要评估生成答案和引用的质量，以确保与科学文献的一致性和准确性。

**Method:** 提出BioACE框架，评估生物医学领域生成的答案及其引用的质量。框架考虑了多个方面，包括答案的完整性、准确性、精确度和召回率。使用自动化方法评估这些方面，并通过广泛实验分析它们与人工评估的相关性。还考虑了现有的自然语言推理（NLI）和其他预训练语言模型方法来评估提供给生成答案作为参考的证据质量。

**Result:** 通过详细实验和分析，确定了生物医学领域答案和引用评估的最佳方法，并将其作为BioACE评估包的一部分。

**Conclusion:** BioACE框架提供了自动化方法来评估大规模语言模型在生成生物医学答案时的质量，基于实验结果，提高了评估的准确性和与人类评估的相关性。

**Abstract:** With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.

</details>


### [2] [CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System](https://arxiv.org/abs/2602.05004)
*Zexin Lin,Jiachen Yu,Haoyang Zhang,Yuzhao Li,Zhonghang Li,Yujiu Yang,Junjie Wang,Xiaoqiang Ji*

Main category: cs.CL

> 本文提出了CoWork-X框架，使语言条件下的代理在高度合作任务中实现亚秒级实时协调和持续适应，相较于现有方法有了明显的性能和效率提升。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型使语言条件下的代理能够在交互式环境中得以实现，但高度合作的任务往往同时施加两个约束：亚秒级实时协调和在严格的在线代币预算下持续多个回合的适应。现有的方法要么依赖于在每个回合内部进行频繁推理，从而导致延迟和时间抖动，要么通过难以整合为可靠的低开销执行的非结构化文字来提供事后改进。

**Method:** CoWork-X, 一个主动共进化框架，将同伴协作视为跨多个回合的闭环优化问题，灵感来源于快-慢记忆分离理论。其中包括一个SKill-Agent，它通过基于HTN（分层任务网络）的结构化、可解释的和组合式技能库来执行技能检索，以及一个用于在回合后执行带有显式预算约束和漂移正则化的补丁式技能整合的共优化器。

**Result:** 在类似于Overcooked-AI的实时协作挑战性基准测试中，CoWork-X实现了稳定、累积的性能提升，同时稳步降低了在线延迟和代币使用。

**Conclusion:** CoWork-X框架通过分解技能检索和而后进行高效的技能整合过程解决了现有方法的局限性，进而提高了协作代理的稳定性和效率。

**Abstract:** Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.

</details>


### [3] [Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation](https://arxiv.org/abs/2602.05035)
*Sean Trott,Pamela D. Rivière*

Main category: cs.CL

> 研究揭示了多语言模型在词汇消歧任务上表现不如单语模型的原因，指出多语言模型可能受到表示能力、注意力机制和词汇相关的限制。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于多语言语言模型在某些任务上表现不如单语模型，本文动机在于量化并理解这种表现差距背后的原因，特别是集中在词汇消歧这一需要精确语义表示和上下文机制的任务上。

**Method:** 通过对多语言和单语模型进行对比测试，使用人为产生相关性判断的数据集对英语和西班牙语中多义词进行分析，以评估多语言模型在词汇消歧方面的表现及背后可能的能力限制因素。

**Result:** 研究发现多语言模型在词汇消歧任务上的表现确实不如单语模型，并且这些表现差异可以用多语言模型的表示能力受限、注意力机制不足和词汇处理问题来解释。

**Conclusion:** 多语言模型在词汇消歧任务上的表现劣于单语模型，这种现象与表示能力、注意力机制以及词汇处理相关的限制因素相关联。

**Abstract:** Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.

</details>


### [4] [Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories](https://arxiv.org/abs/2602.05085)
*Sidi Lu,Zhenwen Liang,Dongyang Ma,Yan Wang,Haitao Mi,Dong Yu*

Main category: cs.CL

> 提出Locas，一种灵活支持持续学习的局部支持型参数化记忆，可在模型参数间灵活地载入或卸载，验证了其在最小化灾难性遗忘的同时持续学习的能力。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决在持续学习过程中模型面临的数据流变性问题，特别是在保持已有知识的同时学习新知识的问题。

**Method:** 通过引入Locas，一种局部支持型参数化记忆结构，模仿现代Transformer的FFN块设计，实现灵活的模型参数化。区分了两种主要的Locas变体形式，并探讨了记忆模块的恰当初始化方法。

**Result:** 实验结果证明了Locas有能力储存过去上下文信息，减少模型的灾难性遗忘现象。在增加很少的额外参数情况下，Locas-GLU表现出了显著的能力。

**Conclusion:** Locas展示了将过去的深度记忆永久化为模型知识的有效方法，同时最小化了模型已有知识的遗忘，证明了其在持续学习中的潜力。

**Abstract:** In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.

</details>


### [5] [Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models](https://arxiv.org/abs/2602.05106)
*Michael Browder,Kevin Duh,J. David Harris,Vince Lyzinski,Paul McNamee,Youngser Park,Carey E. Priebe,Peter Viechnicki*

Main category: cs.CL

> 本文提出了一种新的框架——DKPS，用于解决LLM生成合成数据时存在的不确定性，并提供了数学上的性能保证。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏标注的训练数据成为构建语言技术和生成AI模型的瓶颈，而变换器模型，尤其是LLM，被广泛用于生成合成数据来缓解数据短缺问题。然而，由于模型的黑箱性质，合成数据的特性难以预测。

**Method:** 提出数据核心视角空间（DKPS），用于提供数学分析的基础，从而给出变换器模型输出质量的具体统计保证。

**Result:** 展示了DKPS的数学推导以及如何提供性能保证；阐述了DKPS如何阐明下游任务的性能，例如神经机器翻译模型或使用对比偏好优化（CPO）训练的LLM。

**Conclusion:** 讨论了当前工作的局限性以及未来的研究方向。

**Abstract:** Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.

</details>


### [6] [Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text](https://arxiv.org/abs/2602.05107)
*Ahmed Ruby,Christian Hardmeier,Sara Stymne*

Main category: cs.CL

> 本文提出了一个多模态方法，结合文本和音频信息进行隐式语篇关系分类，并发现跨语言转移可以改善低资源语言的性能。

<details>
  <summary>Details</summary>

**Motivation:** 隐式语篇关系分类是一项具有挑战性的任务，因为它需要从上下文中推断意思。仅靠文本可能无法捕获所有的上下文线索。

**Method:** 我们提出了一种多模态方法，结合文本和听觉信息通过Qwen2-Audio，实现了跨语言隐式语篇关系分类的联合建模。

**Result:** 研究发现，基于文本的模型优于基于音频的模型，但将两种模态结合起来可以提高性能，而跨语言传输可以为低资源语言提供显著改进。

**Conclusion:** 在多语言和多模态数据集上，结合文本和音频信息进行隐式语篇关系分类能够提升分类性能，尤其是在低资源语言中。

**Abstract:** Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.

</details>


### [7] [GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek](https://arxiv.org/abs/2602.05150)
*Yang Zhang,Mersin Konomi,Christos Xypolopoulos,Konstantinos Divriotis,Konstantinos Skianis,Giannis Nikolentzos,Giorgos Stamou,Guokan Shang,Michalis Vazirgiannis*

Main category: cs.CL

> 本文介绍了一个新的希腊语基准测试GreekMMLU，用以评估大规模语言模型在希腊语多任务理解方面的能力，并揭示了不同模型在此基准测试上的性能差异。

<details>
  <summary>Details</summary>

**Motivation:** 现有针对希腊语的评估数据集通常是从英语机器翻译而来，未能准确反映希腊语的语言和文化特征。因此，需要一个基于本土内容的可靠评估基准。

**Method:** 我们引入了GreekMMLU，这是一个基于希腊本土来源的大规模多任务理解基准，包含21,805个多选题，涵盖了45个学科领域。这些问题均源自或由希腊学术、专业和政府考试编制。

**Result:** 通过对80多个开源和闭源语言模型进行评估，发现了前沿模型与开源模型、以及专门针对希腊语训练的模型与通用多语言模型之间的显著性能差距。

**Conclusion:** 系统分析了影响模型表现的因素，包括模型规模、适应性和提示策略等，并提供了提升希腊语大规模语言模型能力的见解。

**Abstract:** Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.

</details>


### [8] [Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems](https://arxiv.org/abs/2602.05176)
*Ziyuan Yang,Wenxuan Ding,Shangbin Feng,Yulia Tsvetkov*

Main category: cs.CL

> 研究表明，恶意语言模型会对多语言模型系统的性能产生严重影响，尤其是推理和安全方面。提出了缓解策略，主要是通过外部监管者来监督模型协作以减轻恶意模型的影响。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于在多语言模型系统中包含恶意或被篡改的模型会导致严重的安全隐患，该研究旨在量化恶意模型的影响并提出缓解策略。

**Method:** 通过设计四类恶意语言模型，并将它们嵌入到四种流行的模型协作系统中，作者们评估了这些被恶意模型破坏的系统的性能，这四个系统在十个数据集上进行了测试。

**Result:** 研究发现，恶意模型对多语言模型系统的性能有很大影响，尤其是在推理和安全领域，平均性能降低了7.12%和7.94%。

**Conclusion:** 尽管所提的缓解策略能够在很大程度上恢复系统的初始性能（平均恢复95.31%），但如何使模型协作系统完全抵御恶意模型的攻击仍是一个开放的课题。

**Abstract:** Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.

</details>


### [9] [The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems](https://arxiv.org/abs/2602.05182)
*Shangbin Feng,Kishan Panaganti,Yulia Tsvetkov,Wenhao Yu*

Main category: cs.CL

> The paper presents a framework for efficient model collaboration through distillation into a single model and iterative model improvement within a single-multi evolution loop.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to combine the advantages of multi-model systems while minimizing the costs associated with loading and maintaining multiple models. The goal is to improve model performance and efficiency through collaboration and distillation.

**Method:** The method involves distilling collaborative patterns from multiple LMs into a single model to enhance efficiency. The single-multi evolution loop is also introduced where models collaborate, distill from this collaboration, and then re-collaborate in an iterative process.

**Result:** Results show that the distilled single model captures collaborative strengths, leading to an 8.0% average improvement, and that the iterative model-improvement loop leads to a 14.9% average improvement in the collaborative system's performance.

**Conclusion:** The conclusion is that the proposed method enhances individual model performance by 8.0% on average and overall system efficiency by 14.9%, while also proving adaptable and effective across a range of tasks and AI evolutionary methods.

**Abstract:** Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.

</details>


### [10] [Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky](https://arxiv.org/abs/2602.05189)
*Hsuan-Yu Chou,Wajiha Naveed,Shuyan Zhou,Xiaowei Yang*

Main category: cs.CL

> 研究分析了七个最先进的语言模型（LLMs）在社交媒体有害内容检测方面的性能，并发现开源LLMs与专有的LLMs在敏感性和特异性上存在显著的相似性，这表明开源LLMs可以用于支持隐私的保护式审查，尤其是在个性化的审核环境中。

<details>
  <summary>Details</summary>

**Motivation:** 受到关于推理LLMs最新进展的启发，研究动机在于评估开源和专有LLMs在社交媒体有害内容检测上的表现，尤其是比较它们的零样本能力。

**Method:** 研究测试了四个专有模型和三个开源模型在Bluesky平台上真实帖子上的表现，使用了Bluesky Moderation Service的审核决定和两位作者的注释进行评估。

**Result:** 结果显示，开源LLMs的敏感度（81%至97%）和特异性（91%至100%）与专有模型（敏感度72%至98%，特异性93%至99%）非常相似，并且在不同类型的有害内容检测中表现出不同的性能模式。

**Conclusion:** 研究结论表明，开源LLMs能够在保护用户隐私的同时执行平台规模或个性化的审查任务，并提供了新的方向来设计平衡社区价值观与用户个性化偏好的审查系统。

**Abstract:** As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.
  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.

</details>


### [11] [Aligning Large Language Model Behavior with Human Citation Preferences](https://arxiv.org/abs/2602.05205)
*Kenichiro Ando,Tatsuya Harada*

Main category: cs.CL

> 研究聚焦于LLM在何种文本上倾向于引用，分析其与人类引用行为的关系，发现模型的行为在某些情况下与人类偏好不符，提供了将模型行为调整以匹配人类偏好的方法。

<details>
  <summary>Details</summary>

**Motivation:** 研究集中在LLM当前的引用行为及其与人类偏好之间的对齐情况，填补LLM如何识别引用相关性和此过程应如何控制的研究空白。

**Method:** 通过构建一个数据集来描述人类引用偏好与LLM行为之间的关系。网络衍生文本被分为八种引用动机类型，对所有类型组合的引用偏好进行了详尽评估，以捕捉精细对比。

**Result:** 人类对医学文本的引用需求最高，性能更强的模型显示了类似的趋势。模型更倾向于在需要引用的文本上添加引用，尤其是来源如维基百科上明确标记的，但这种过度引用降低了准确性。对含有数字或个人名字的句子，模型的引用选择少于人类需求。

**Conclusion:** 通过直接偏好优化实验表明，可以校准模型行为更好地匹配人类的引用偏好，为LLM引用偏好更详细的调查提供基础。

**Abstract:** Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\%$ relative to humans) and sentences containing personal names (by $-20.1\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.

</details>


### [12] [Quantifying the Knowledge Proximity Between Academic and Industry Research: An Entity and Semantic Perspective](https://arxiv.org/abs/2602.05211)
*Hongye Zhao,Yi Zhao,Chengzhi Zhang*

Main category: cs.CL

> 本研究通过量化学界与业界共同演化的轨迹，并通过精细实体和语义空间来分析知识接近度，证明了随着技术变革，两者之间的知识接近度提升。

<details>
  <summary>Details</summary>

**Motivation:** 当前关于学界与业界知识接近度的研究主要依赖于宏观指标，缺乏对文献中的知识单元进行分析，这导致了对学界与业界精细知识接近度的理解不足，可能影响合作框架和资源配置效率。

**Method:** 该研究通过精细实体测量和语义空间量化来弥补现有研究中关于学界与业界知识接近度的不足。在实体测量部分，研究提取了精细的知识实体，使用余弦相似度测量序列重叠，并通过复杂网络分析研究拓扑特征。在语义层面，该研究运用无监督对比学习量化跨机构文本相似性以衡量语义空间中的收敛情况。最后，通过引用分布模式研究双向知识流动与相似性的相关性。

**Result:** 分析结果显示，学界与业界的知识接近度随着技术变革而提高，这提供了双向适应共同演化的文本证据。此外，在技术范式转变期间，学界的知识主导地位减弱。

**Conclusion:** 研究通过提取精细的知识实体和使用复杂网络分析来测量序列重叠和拓扑特征。此外，通过无监督对比学习量化跨机构文本相似性来衡量语义空间中的收敛情况，并使用引用分布模式来研究双向知识流动与相似性的相关性，证明了在技术变革背景下学界与业界的双向适应和共同演化，并揭示了在技术范式转变时期学界知识主导地位的减弱。

**Abstract:** The academia and industry are characterized by a reciprocal shaping and dynamic feedback mechanism. Despite distinct institutional logics, they have adapted closely in collaborative publishing and talent mobility, demonstrating tension between institutional divergence and intensive collaboration. Existing studies on their knowledge proximity mainly rely on macro indicators such as the number of collaborative papers or patents, lacking an analysis of knowledge units in the literature. This has led to an insufficient grasp of fine-grained knowledge proximity between industry and academia, potentially undermining collaboration frameworks and resource allocation efficiency. To remedy the limitation, this study quantifies the trajectory of academia-industry co-evolution through fine-grained entities and semantic space. In the entity measurement part, we extract fine-grained knowledge entities via pre-trained models, measure sequence overlaps using cosine similarity, and analyze topological features through complex network analysis. At the semantic level, we employ unsupervised contrastive learning to quantify convergence in semantic spaces by measuring cross-institutional textual similarities. Finally, we use citation distribution patterns to examine correlations between bidirectional knowledge flows and similarity. Analysis reveals that knowledge proximity between academia and industry rises, particularly following technological change. This provides textual evidence of bidirectional adaptation in co-evolution. Additionally, academia's knowledge dominance weakens during technological paradigm shifts. The dataset and code for this paper can be accessed at https://github.com/tinierZhao/Academic-Industrial-associations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

> 本文提出了SIDeR框架，这是一种基于语义解耦的无限制面部隐私保护框架。它分解面部图像以生成视觉匿名的对抗性人脸，同时保持机器级别的身份一致性。增加动量驱动的优化和语义-视觉平衡因子提高了样本的多样性与自然度。

<details>
  <summary>Details</summary>

**Motivation:** 随着人脸识别技术在在线银行、身份验证等网络服务中的深度应用，如何在图像存储和传输过程中有效分离身份信息与视觉表现已成为隐私保护的关键挑战。

**Method:** SIDeR框架通过将面部图像分解为机器可识别的身份特征向量和可感知的语义外观组件来实现面部隐私保护。进一步利用语义引导的潜扩散模型重组过程生成视觉匿名的对抗性人脸，同时保持机器级别的身份一致性。框架采用了动量驱动的无约束扰动优化以及语义-视觉平衡因子，以合成多样的高自然度对抗样本。

**Result:** 实验表明，SIDeR在黑盒场景中的攻击成功率达到了99%，并在基于PSNR的恢复质量上比基线方法高出41.28%。

**Conclusion:** SIDeR框架能够有效地保护面部隐私，并在提供授权访问时能够恢复原图。实验验证了其在黑盒场景中的高效攻击成功率和高质量的恢复效果。

**Abstract:** With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

</details>


### [14] [UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking](https://arxiv.org/abs/2602.05037)
*Bishoy Galoaa,Xiangyu Bai,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

> UniTrack是一种新的插件式图论损失函数，通过优化跟踪特定的目标来显著提升多目标跟踪性能，展示了在多个基准测试上的一致性能改进。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于图的多目标跟踪方法重新设计了跟踪架构，而UniTrack旨在提供一个通用的训练目标，通过集成检测精确度、身份保持和时空一致性来提升多目标跟踪的表现，不改变现有架构。

**Method:** UniTrack是一个基于图论的插件式损失函数，通过统一的可微分学习直接优化跟踪特定的目标，从而显著提高多目标跟踪（MOT）的表现。它将检测准确性、身份保持和时空一致性集成到一个单一的端到端可训练的损失函数中，允许无缝集成到现有的MOT系统中，无需修改架构。

**Result:** 在多个挑战性的基准测试中，UniTrack展示了在所有测试架构和数据集上的持续改进，包括Trackformer、MOTR、FairMOT、ByteTrack、GTR和MOTE，减少了高达53%的身份切换，IDF1性能提高了12%。特别是在SportsMOT数据集上，GTR达到了MOTA性能提高9.7%的峰值。

**Conclusion:** UniTrack证明了其作为多目标跟踪系统中一个有效的插件式损失函数的潜力和优势，提升了不同跟踪模型在各类数据集上的性能。

**Abstract:** We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.

</details>


### [15] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

> 本文提出一种训练框架，通过增强视觉条件，解决了Vision-Language-Action (VLA) 模型在动作空间中的视觉动作不对齐问题，不需要修改架构或收集额外数据，提高了任务性能。

<details>
  <summary>Details</summary>

**Motivation:** 通过实证研究，我们发现成功的轨迹展示比失败的轨迹展示具有更强的视觉依赖性。因此，我们提出了一种训练框架，该框架显式地增强了Vision-Language-Action (VLA) 模型中的视觉条件。

**Method:** 我们的方法首先通过在轨迹跟踪代理任务上的偏好优化来对齐动作预测与视觉输入，然后在监督微调过程中通过潜在空间蒸馏将增强的对齐转移到指令跟随任务。

**Result:** 未具体列出测试结果，但提到该方法在没有引入架构修改或额外数据收集的情况下提高了视觉条件和任务性能。

**Conclusion:** 此方法在离散OpenVLA上提高了视觉条件和任务性能，并且进一步扩展到连续的OpenVLA-OFT设置时表现一致的增益。

**Abstract:** Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

</details>


### [16] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

> The paper explores methods for accurately estimating food portion sizes using images, focusing on overcoming the challenge of estimating 3D food sizes from 2D images.

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and convenience of dietary assessment methods that use images, which are important for health monitoring particularly in relation to chronic diseases and obesity.

**Method:** The paper reviews strategies including the use of depth maps, multi-view inputs, template matching, and deep learning methods with monocular or auxiliary inputs to estimate 3D food portion sizes.

**Result:** The paper details the various methodologies used to enhance the accuracy of portion size estimation from images.

**Conclusion:** The review provides insights into current and effective approaches for overcoming the limitations of 2D to 3D estimation in dietary assessment, highlighting the role of advanced image processing techniques.

**Abstract:** Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

</details>


### [17] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

> 研究提出一种新方法VCR来分析多模态模型在医疗任务中的行为，并揭示了它们在不同人口统计群体中的性能差异。

<details>
  <summary>Details</summary>

**Motivation:** 为了确保在医疗等关键安全领域中机器学习模型的可靠性，需要审计方法来揭示模型的不足。研究的重点是通过临床皮肤病图像分类恶性皮肤病变，并进行包括胸部放射图像和自然图像的补充实验。

**Method:** 使用名为视觉概念排名（VCR）的方法来识别大型多模态模型（LMMs）中的重要视觉概念，并通过提供示例来调查这些模型的医疗任务表现。VCR生成关于不同视觉特征依赖的假设，并通过手动干预验证这些假设。

**Result:** 研究展示了LMMs在不同的人口统计子群体中显示出预期之外的性能差距。

**Conclusion:** 通过VCR，能够识别和验证LMMs在进行医疗任务时依赖的视觉特征，并揭示了模型中的性能差异。

**Abstract:** Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

</details>


### [18] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

> CLEAR-HPV框架通过重构MIL的潜在空间实现了HPV相关癌症中的可解释形态学概念发现，提高了基于注意力的MIL模型的解释性。

<details>
  <summary>Details</summary>

**Motivation:** 由于基于注意力的多实例学习方法虽然在HPV相关全幻灯片组织病理学中实现了强劲的幻灯片级预测，但在形态学解释方面较为有限，因此提出了这种新的框架旨在提高其形态学可解释性。

**Method:** 提出了一种新的框架CLEAR-HPV，利用注意力机制重构多实例学习(MIL)的潜在空间，实现形态学概念的自动发现，无需在训练过程中使用概念标签。该方法在注意权重潜在空间中工作，可以自动生成空间概念图，并用简洁的概念分数向量表示每张幻灯片。

**Result:** CLEAR-HPV框架保留了原始MIL嵌入的预测信息，同时将高维特征空间（例如1536维）减少到仅10个可解释的概念，提高了模型的可解释性。并且该方法在TCGA-HNSCC、TCGA-CESC和CPTAC-HNSCC数据集上具有一致的泛化能力。

**Conclusion:** CLEAR-HPV作为一个通用的、与基础模型无关的框架，为基于注意力的全幻灯片组织病理学MIL模型提供了紧凑的概念层次解释，同时保持了预测性能。

**Abstract:** Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

</details>


### [19] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

> 本文提出ARGaze方法用于在线自我中心凝视估计任务，通过变压器解码器实现基于当前视觉特征和过去凝视估计的连续预测，表现出优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统第三方凝视估计缺乏使用直接头部或眼睛信号的能力，而ARGaze利用视觉条件下的自回归解码技术，有效利用过去的视觉注意力信息来预测当前的凝视目标。这种方法在目标导向的活动中显示出强大的时间连续性，可以提高预测的准确性。

**Method:** ARGaze方法通过将凝视估计重新定义为顺序预测任务来解决在线自我中心凝视估计问题。在每个时间步，采用变压器解码器基于当前视觉特征和最近凝视目标估计的固定长度窗口（凝视上下文窗口）来预测当前凝视。

**Result:** ARGaze在多个自我中心凝视估算基准测试中取得了最先进的性能，特别是在基于在线评估时。实验显示，具有固定长度的凝视历史上下文的自回归模型对于稳健的预测至关重要。

**Conclusion:** ARGaze方法通过引入变压器解码器和固定的凝视上下文窗口，成功解决了自我中心视觉数据中的凝视估计问题。该方法能够在线处理视觉数据，并已在多个数据集中展示了优异效果。源代码和预训练模型将被公开发布。

**Abstract:** Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

</details>


### [20] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

> 研究提出AirGlove，利用现有手套数据，提升视觉手部追踪模型在不同设计手套上的表现。实验表明AirGlove能有效泛化手部姿势模型到新的手套设计中，并显著提高性能。

<details>
  <summary>Details</summary>

**Motivation:** 传感器手套在遥感操作和机器人政策学习中的使用日益增加，但基于传感器的手部追踪存在精度受限的问题，视觉方法虽在裸手追踪中表现出色，但在手套手部追踪中的表现仍需探索。

**Method:** 系统评估了视觉手部追踪模型在零样本和微调设置下的手套手部追踪性能，发现现有裸手模型在传感器手套上性能下降，因此提出了AirGlove。

**Result:** 实验显示AirGlove能有效泛化手部姿势模型到新的手套设计，与比较方案相比性能显著提升。

**Conclusion:** AirGlove能够通过利用现有手套的数据来应对手套外观与裸手之间的巨大差距，从而提高手套手部追踪的精确性。

**Abstract:** Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

</details>
