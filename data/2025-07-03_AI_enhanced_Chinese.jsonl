{"id": "2507.01099", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "AI": {"tldr": "该论文提出了一种4D视频生成模型，通过多视角的3D一致性监督来提高预测视频的时间连贯性和几何一致性，从而增强机器人在复杂环境中的规划和交互能力。", "motivation": "为了提升机器人在复杂环境中的计划和交互能力，需要一种有效的方法来理解和预测物理世界的动态变化。当前视频生成模型在生成时空连贯和几何一致的多视角视频方面存在挑战。", "method": "提出一个4D视频生成模型，通过交叉视角点图对齐监督训练，学习共享的3D场景表示，基于给定的RGB-D观察预测未来视频序列，无需使用相机姿势作为输入。", "result": "该方法在多个模拟和真实世界机器人数据集上产生更加稳定和空间对齐的预测，改进了现有基线方法。同时，预测的4D视频可用于恢复机器人末端执行器轨迹，使用现成的6DoF姿态跟踪器支持稳健的机器人操作和对新的摄像头视角的泛化。", "conclusion": "所提出的4D视频生成模型能够有效提升机器人对未来视频序列的预测能力，促进其在复杂环境中的功能应用。"}}
{"id": "2507.01123", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01123", "abs": "https://arxiv.org/abs/2507.01123", "authors": ["Rahul A. Burange", "Harsh K. Shinde", "Omkar Mutyalwar"], "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions", "comment": "20 pages, 24 figures", "summary": "Landslides pose severe threats to infrastructure, economies, and human lives,\nnecessitating accurate detection and predictive mapping across diverse\ngeographic regions. With advancements in deep learning and remote sensing,\nautomated landslide detection has become increasingly effective. This study\npresents a comprehensive approach integrating multi-source satellite imagery\nand deep learning models to enhance landslide identification and prediction. We\nleverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and\nDigital Elevation Model (DEM) layers to capture critical environmental features\ninfluencing landslide occurrences. Various geospatial analysis techniques are\nemployed to assess the impact of terra in characteristics, vegetation cover,\nand rainfall on detection accuracy. Additionally, we evaluate the performance\nof multiple stateof-the-art deep learning segmentation models, including U-Net,\nDeepLabV3+, and Res-Net, to determine their effectiveness in landslide\ndetection. The proposed framework contributes to the development of reliable\nearly warning systems, improved disaster risk management, and sustainable\nland-use planning. Our findings provide valuable insights into the potential of\ndeep learning and multi-source remote sensing in creating robust, scalable, and\ntransferable landslide prediction models.", "AI": {"tldr": "The study presents a comprehensive approach utilizing multi-source satellite imagery and deep learning models to enhance landslide identification and prediction.", "motivation": "To improve the accuracy and reliability of landslide detection and predictive mapping across diverse geographic regions, enhancing infrastructure protection, economic stability, and human safety.", "method": "The team uses Sentinel-2 multispectral data and ALOS PALSAR-derived slope and DEM layers to capture critical environmental features. They also evaluate the performance of deep learning segmentation models such as U-Net, DeepLabV3+, and Res-Net.", "result": "The framework improves the accuracy of landslide detection through the use of multi-source data and advanced neural networks.", "conclusion": "The approach contributes to more reliable early warning systems and aids in disaster risk management and sustainable land-use planning by demonstrating the effectiveness of integrating deep learning models with remote sensing data."}}
{"id": "2507.01163", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM", "I.4.7"], "pdf": "https://arxiv.org/pdf/2507.01163", "abs": "https://arxiv.org/abs/2507.01163", "authors": ["Alán F. Muñoz", "Tim Treis", "Alexandr A. Kalinin", "Shatavisha Dasgupta", "Fabian Theis", "Anne E. Carpenter", "Shantanu Singh"], "title": "cp_measure: API-first feature extraction for image-based profiling workflows", "comment": "10 pages, 4 figures, 4 supplementary figures. CODEML Workshop paper\n  accepted (non-archival), as a part of ICML2025 events", "summary": "Biological image analysis has traditionally focused on measuring specific\nvisual properties of interest for cells or other entities. A complementary\nparadigm gaining increasing traction is image-based profiling - quantifying\nmany distinct visual features to form comprehensive profiles which may reveal\nhidden patterns in cellular states, drug responses, and disease mechanisms.\nWhile current tools like CellProfiler can generate these feature sets, they\npose significant barriers to automated and reproducible analyses, hindering\nmachine learning workflows. Here we introduce cp_measure, a Python library that\nextracts CellProfiler's core measurement capabilities into a modular, API-first\ntool designed for programmatic feature extraction. We demonstrate that\ncp_measure features retain high fidelity with CellProfiler features while\nenabling seamless integration with the scientific Python ecosystem. Through\napplications to 3D astrocyte imaging and spatial transcriptomics, we showcase\nhow cp_measure enables reproducible, automated image-based profiling pipelines\nthat scale effectively for machine learning applications in computational\nbiology.", "AI": {"tldr": "本文介绍了cp_measure，一个用于程序化特征提取的Python库，它提取了CellProfiler的核心测量能力，并展示了其在3D星形胶质细胞成像和空间转录组学中的应用，以实现可重现的自动化基于图像的分析流程。", "motivation": "当前的工具有如 CellProfiler 能生成这些特征集，但它们对自动化和可重复分析构成了重大障碍，阻碍了机器学习工作流程。文章旨在介绍 cp_measure 这一新工具，可以无缝集成到科学 Python 生态系统中，使其更适合机器学习应用。", "method": "我们介绍了 cp_measure，这是一个 Python 库，它从 CellProfiler 中提取核心测量能力，设计为模块化和 API 优先工具，以便程序化特征提取。通过将其应用于 3D 星形胶质细胞成像和空间转录组学，我们展示了 cp_measure 如何在计算生物学中的机器学习应用中实现可重现、自动化的基于图像的分析流程。", "result": "cp_measure 的特征与 CellProfiler 特征保持高度一致性，同时能够无缝集成到科学 Python 生态系统中。通过其在计算生物学的机器学习应用中的表现，说明了其在基于图像的分析流程中的有效性。", "conclusion": "通过应用到 3D 星形胶质细胞成像和空间转录组学，研究展示了 cp_measure 在计算生物学中可以实现可重现、自动化的基于图像的分析流程，有效地服务于机器学习应用。"}}
{"id": "2507.01182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01182", "abs": "https://arxiv.org/abs/2507.01182", "authors": ["Zhuo Su", "Li Liu", "Matthias Müller", "Jiehua Zhang", "Diana Wofk", "Ming-Ming Cheng", "Matti Pietikäinen"], "title": "Rapid Salient Object Detection with Difference Convolutional Neural Networks", "comment": "16 pages, accepted in TPAMI", "summary": "This paper addresses the challenge of deploying salient object detection\n(SOD) on resource-constrained devices with real-time performance. While recent\nadvances in deep neural networks have improved SOD, existing top-leading models\nare computationally expensive. We propose an efficient network design that\ncombines traditional wisdom on SOD and the representation power of modern CNNs.\nLike biologically-inspired classical SOD methods relying on computing contrast\ncues to determine saliency of image regions, our model leverages Pixel\nDifference Convolutions (PDCs) to encode the feature contrasts. Differently,\nPDCs are incorporated in a CNN architecture so that the valuable contrast cues\nare extracted from rich feature maps. For efficiency, we introduce a difference\nconvolution reparameterization (DCR) strategy that embeds PDCs into standard\nconvolutions, eliminating computation and parameters at inference.\nAdditionally, we introduce SpatioTemporal Difference Convolution (STDC) for\nvideo SOD, enhancing the standard 3D convolution with spatiotemporal contrast\ncapture. Our models, SDNet for image SOD and STDNet for video SOD, achieve\nsignificant improvements in efficiency-accuracy trade-offs. On a Jetson Orin\ndevice, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on\nstreamed images and videos, surpassing the second-best lightweight models in\nour experiments by more than $2\\times$ and $3\\times$ in speed with superior\naccuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.", "AI": {"tldr": "The paper presents SDNet and STDNet models, which are efficient and accurate salient object detection models for real-time deployment on resource-constrained devices.", "motivation": "To enable efficient real-time salient object detection on resource-constrained devices, addressing the high computational costs of current top-performing models.", "method": "Our model utilizes Pixel Difference Convolutions (PDCs), which are integrated into a CNN architecture for effective saliency detection. Additionally, a difference convolution reparameterization (DCR) strategy and SpatioTemporal Difference Convolution (STDC) for video processing are introduced to improve efficiency and accuracy.", "result": "The SDNet and STDNet models operate at 46 FPS and 150 FPS on streamed images and videos with fewer than 1M parameters, significantly outperforming the second-best models by more than $2\\times$ and $3\\times$ in speed, respectively, while maintaining superior accuracy.", "conclusion": "The models, SDNet and STDNet, demonstrate improved efficiency-accuracy trade-offs, capable of operating in real-time on devices like the Jetson Orin, while achieving superior accuracy compared to lightweight alternatives."}}
