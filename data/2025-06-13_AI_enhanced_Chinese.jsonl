{"id": "2506.10019", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10019", "abs": "https://arxiv.org/abs/2506.10019", "authors": ["Tian Lan", "Yang-Hao Zhou", "Zi-Ao Ma", "Fanshu Sun", "Rui-Qing Sun", "Junyu Luo", "Rong-Cheng Tu", "Heyan Huang", "Chen Xu", "Zhijing Wu", "Xian-Ling Mao"], "title": "A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations", "comment": null, "summary": "Recent advances in deep learning have significantly enhanced generative AI\ncapabilities across text, images, and audio. However, automatically evaluating\nthe quality of these generated outputs presents ongoing challenges. Although\nnumerous automatic evaluation methods exist, current research lacks a\nsystematic framework that comprehensively organizes these methods across text,\nvisual, and audio modalities. To address this issue, we present a comprehensive\nreview and a unified taxonomy of automatic evaluation methods for generated\ncontent across all three modalities; We identify five fundamental paradigms\nthat characterize existing evaluation approaches across these domains. Our\nanalysis begins by examining evaluation methods for text generation, where\ntechniques are most mature. We then extend this framework to image and audio\ngeneration, demonstrating its broad applicability. Finally, we discuss\npromising directions for future research in cross-modal evaluation\nmethodologies.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u5185\u5bb9\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6846\u67b6\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u4e00\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\u6765\u5168\u9762\u7ec4\u7ec7\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u672c\u6587\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u5185\u5bb9\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u7684\u7efc\u5408\u56de\u987e\u548c\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc6\u522b\u51fa\u4e86\u4e94\u4e2a\u57fa\u672c\u8303\u5f0f\u6765\u63cf\u8ff0\u8fd9\u4e9b\u9886\u57df\u7684\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u7684\u5185\u5bb9\u8bc4\u4f30\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u8de8\u6a21\u6001\u8bc4\u4f30\u65b9\u6cd5\u5728\u672a\u6765\u7684\u7814\u7a76\u4e2d\u5177\u6709\u6f5c\u5728\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2506.10055", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10055", "abs": "https://arxiv.org/abs/2506.10055", "authors": ["Dingfeng Shi", "Jingyi Cao", "Qianben Chen", "Weichen Sun", "Weizhen Li", "Hongxuan Lu", "Fangchen Dong", "Tianrui Qin", "King Zhu", "Minghao Yang", "Jian Yang", "Ge Zhang", "Jiaheng Liu", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "TaskCraft: Automated Generation of Agentic Tasks", "comment": null, "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.", "AI": {"tldr": "TaskCraft\u662f\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5177\u6709\u4e0d\u540c\u96be\u5ea6\u7684\u80fd\u52a8\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u5176\u4efb\u52a1\u7ed3\u6784\u590d\u6742\u4e14\u53ef\u9a8c\u8bc1\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u6570\u636e\u96c6\u3002", "motivation": "\u80fd\u52a8\u4efb\u52a1\u9700\u8981\u591a\u6b65\u9aa4\u7684\u95ee\u9898\u89e3\u51b3\u3001\u81ea\u4e3b\u6027\u3001\u5de5\u5177\u4f7f\u7528\u53ca\u9002\u5e94\u6027\u63a8\u7406\uff0c\u5728NLP\u548cAI\u7684\u8fdb\u5c55\u4e2d\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u6307\u4ee4\u6570\u636e\u7f3a\u4e4f\u5de5\u5177\u4ea4\u4e92\uff0c\u5f53\u524d\u7684\u80fd\u52a8\u57fa\u51c6\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u4eba\u7c7b\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTaskCraft\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u4e0d\u540c\u96be\u5ea6\u3001\u591a\u5de5\u5177\u5e76\u53ef\u9a8c\u8bc1\u7684\u80fd\u52a8\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5e26\u6709\u6267\u884c\u8f68\u8ff9\u3002TaskCraft\u901a\u8fc7\u57fa\u4e8e\u6df1\u5ea6\u548c\u5bbd\u5ea6\u7684\u6269\u5c55\u6765\u6269\u5c55\u539f\u5b50\u4efb\u52a1\uff0c\u521b\u5efa\u7ed3\u6784\u4e0a\u548c\u5c42\u6b21\u4e0a\u590d\u6742\u7684\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5728\u751f\u6210\u5de5\u4f5c\u6d41\u4e2d\u6539\u5584\u4e86\u63d0\u793a\u4f18\u5316\uff0c\u5e76\u589e\u5f3a\u4e86\u80fd\u52a8\u57fa\u7840\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\u3002", "conclusion": "\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b\u7ea636,000\u4e2a\u96be\u5ea6\u5404\u5f02\u7684\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff0c\u652f\u6301\u672a\u6765\u7684\u80fd\u52a8\u6a21\u578b\u8c03\u6574\u548c\u8bc4\u4f30\u7814\u7a76\u3002"}}
{"id": "2506.10077", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.10077", "abs": "https://arxiv.org/abs/2506.10077", "authors": ["Christopher J. Agostino", "Quan Le Thien", "Molly Apsel", "Denizhan Pak", "Elina Lesyk", "Ashabari Majumdar"], "title": "A quantum semantic framework for natural language processing", "comment": "12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025", "summary": "Semantic degeneracy represents a fundamental property of natural language\nthat extends beyond simple polysemy to encompass the combinatorial explosion of\npotential interpretations that emerges as semantic expressions increase in\ncomplexity. Large Language Models (LLMs) and other modern NLP systems face\ninherent limitations precisely because they operate within natural language\nitself, making them subject to the same interpretive constraints imposed by\nsemantic degeneracy. In this work, we argue using Kolmogorov complexity that as\nan expression's complexity grows, the likelihood of any interpreting agent\n(human or LLM-powered AI) recovering the single intended meaning vanishes. This\ncomputational intractability suggests the classical view that linguistic forms\npossess meaning in and of themselves is flawed. We alternatively posit that\nmeaning is instead actualized through an observer-dependent interpretive act.\nTo test this, we conducted a semantic Bell inequality test using diverse LLM\nagents as ``computational cognitive systems'' to interpret ambiguous word pairs\nunder varied contextual settings. Across several independent experiments, we\nfound average CHSH expectation values ranging from 1.2 to 2.8, with several\nruns yielding values (e.g., 2.3-2.4) that significantly violate the classical\nboundary ($|S|\\leq2$). This demonstrates that linguistic interpretation under\nambiguity can exhibit non-classical contextuality, consistent with results from\nhuman cognition experiments. These results inherently imply that classical\nfrequentist-based analytical approaches for natural language are necessarily\nlossy. Instead, we propose that Bayesian-style repeated sampling approaches can\nprovide more practically useful and appropriate characterizations of linguistic\nmeaning in context.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bed\u4e49\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u6d4b\u8bd5\u5c55\u793a\u4e86\u8bed\u8a00\u89e3\u91ca\u5728\u6a21\u7cca\u6027\u7684\u4e0a\u4e0b\u6587\u8868\u73b0\u4e2d\u5177\u6709\u975e\u7ecf\u5178\u7684\u4e0a\u4e0b\u6587\u6027\uff0c\u5e76\u63d0\u8bae\u4f7f\u7528\u8d1d\u53f6\u65af\u98ce\u683c\u7684\u91cd\u590d\u62bd\u6837\u65b9\u6cd5\u800c\u975e\u4f20\u7edf\u7684\u57fa\u4e8e\u9891\u7387\u7684\u65b9\u6cd5\u6765\u5206\u6790\u81ea\u7136\u8bed\u8a00\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u81ea\u7136\u8bed\u8a00\u4e2d\u8bed\u4e49\u9000\u5316\u5e26\u6765\u7684\u6839\u672c\u95ee\u9898\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5176\u4ed6\u73b0\u4ee3NLP\u7cfb\u7edf\u7531\u4e8e\u8fd0\u884c\u4e8e\u81ea\u7136\u8bed\u8a00\u672c\u8eab\u800c\u9762\u5bf9\u7684\u76f8\u5173\u9650\u5236\u3002", "method": "\u901a\u8fc7Kolmogorov\u590d\u6742\u6027\u7406\u8bba\u8bba\u8bc1\uff0c\u968f\u7740\u8868\u8fbe\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u4efb\u4f55\u89e3\u8bfb\u4ee3\u7406\uff08\u65e0\u8bba\u662f\u4eba\u7c7b\u8fd8\u662fLLM\u9a71\u52a8\u7684AI\uff09\u6062\u590d\u5355\u4e00\u610f\u6307\u610f\u4e49\u7684\u53ef\u80fd\u6027\u4f1a\u6d88\u5931\u3002\u4e3a\u4e86\u9a8c\u8bc1\u8fd9\u4e00\u89c2\u70b9\uff0c\u4f5c\u8005\u8fdb\u884c\u4e86\u4e00\u573a\u8bed\u4e49\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u7684\u6d4b\u8bd5\uff0c\u5229\u7528\u591a\u79cdLLM\u4ee3\u7406\u4f5c\u4e3a\u201c\u8ba1\u7b97\u8ba4\u77e5\u7cfb\u7edf\u201d\uff0c\u5728\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u73af\u5883\u4e0b\u6765\u89e3\u91ca\u6a21\u7cca\u7684\u8bcd\u5bf9\u3002", "result": "\u5728\u51e0\u4e2a\u72ec\u7acb\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747CHSH\u671f\u671b\u503c\u8303\u56f4\u4ece1.2\u52302.8\uff0c\u6709\u51e0\u7ec4\u5b9e\u9a8c\u7ed3\u679c\uff08\u4f8b\u5982\uff0c2.3-2.4\uff09\u663e\u8457\u8fdd\u53cd\u4e86\u7ecf\u5178\u8fb9\u754c\uff08$|S|\\leq2$\uff09\u3002\u8fd9\u8868\u660e\uff0c\u5728\u6a21\u7cca\u6027\u4e0b\u7684\u8bed\u8a00\u89e3\u91ca\u53ef\u4ee5\u8868\u73b0\u51fa\u975e\u7ecf\u5178\u7684\u4e0a\u4e0b\u6587\u6027\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b9e\u9a8c\u7684\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6697\u793a\u7ecf\u5178\u7684\u57fa\u4e8e\u9891\u7387\u5206\u6790\u7684\u81ea\u7136\u8bed\u8a00\u65b9\u6cd5\u662f\u5fc5\u7136\u7684\u4e0d\u5b8c\u6574\u3002\u76f8\u53cd\uff0c\u4f5c\u8005\u63d0\u8bae\u8d1d\u53f6\u65af\u98ce\u683c\u7684\u91cd\u590d\u62bd\u6837\u65b9\u6cd5\u53ef\u4ee5\u63d0\u4f9b\u66f4\u5b9e\u7528\u4e14\u66f4\u5408\u9002\u7684\u65b9\u5f0f\u6765\u523b\u753b\u8bed\u8a00\u610f\u4e49\u7684\u4e0a\u4e0b\u6587\u6027\u3002"}}
{"id": "2506.10086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10086", "abs": "https://arxiv.org/abs/2506.10086", "authors": ["Christodoulos Constantinides", "Shuxin Lin", "Nianjun Zhou", "Dhaval Patel"], "title": "Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information", "comment": null, "summary": "This paper presents a novel multi-agent system called Chat-of-Thought,\ndesigned to facilitate the generation of Failure Modes and Effects Analysis\n(FMEA) documents for industrial assets. Chat-of-Thought employs multiple\ncollaborative Large Language Model (LLM)-based agents with specific roles,\nleveraging advanced AI techniques and dynamic task routing to optimize the\ngeneration and validation of FMEA tables. A key innovation in this system is\nthe introduction of a Chat of Thought, where dynamic, multi-persona-driven\ndiscussions enable iterative refinement of content. This research explores the\napplication domain of industrial equipment monitoring, highlights key\nchallenges, and demonstrates the potential of Chat-of-Thought in addressing\nthese challenges through interactive, template-driven workflows and\ncontext-aware agent collaboration.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edfChat-of-Thought\uff0c\u4ee5\u5229\u7528\u5148\u8fdbAI\u6280\u672f\u4f18\u5316\u5de5\u4e1a\u8d44\u4ea7\u7684FMEA\u6587\u6863\u751f\u6210\u6d41\u7a0b\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u7cfb\u7edf\u5728\u5de5\u4e1a\u76d1\u63a7\u9886\u57df\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u5de5\u4e1a\u8bbe\u5907\u76d1\u63a7\u9886\u57df\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5728\u4e8e\u89e3\u51b3\u751f\u6210\u53ca\u9a8c\u8bc1FMEA\u6587\u6863\u65f6\u7684\u5173\u952e\u6311\u6218\uff0c\u5c55\u793aChat-of-Thought\u7cfb\u7edf\u5728\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6a21\u677f\u5de5\u4f5c\u6d41\u7a0b\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u79f0\u4e3aChat-of-Thought\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u591a\u4e2a\u5177\u6709\u7279\u5b9a\u89d2\u8272\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u5148\u8fdb\u7684AI\u6280\u672f\u548c\u52a8\u6001\u4efb\u52a1\u8c03\u5ea6\uff0c\u4f18\u5316\u751f\u6210\u548c\u9a8c\u8bc1FMEA\u8868\u683c\u7684\u8fc7\u7a0b\u3002\u4e00\u4e2a\u5173\u952e\u7684\u521b\u65b0\u70b9\u662f\u5f15\u5165\u4e86\u601d\u60f3\u5bf9\u8bdd\uff0c\u901a\u8fc7\u52a8\u6001\u3001\u591a\u89d2\u8272\u9a71\u52a8\u7684\u8ba8\u8bba\uff0c\u5b9e\u73b0\u4e86FMEA\u5185\u5bb9\u7684\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u7ed3\u679c\u672a\u660e\u786e\u8bf4\u660e\uff0c\u4f46\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u751f\u6210\u548c\u4f18\u5316FMEA\u8868\u683c\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u5728\u5de5\u4e1a\u8bbe\u5907\u76d1\u63a7\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002", "conclusion": "Chat-of-Thought\u7cfb\u7edf\u7684\u63d0\u51fa\uff0c\u4e3a\u89e3\u51b3\u5de5\u4e1a\u8d44\u4ea7\u7684FMEA\u6587\u6863\u751f\u6210\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\uff0c\u63d0\u9ad8\u4e86FMEA\u8868\u683c\u751f\u6210\u548c\u9a8c\u8bc1\u7684\u6548\u7387\u4e0e\u8d28\u91cf\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5de5\u4e1a\u76d1\u63a7\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.10005", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.10005", "abs": "https://arxiv.org/abs/2506.10005", "authors": ["Sridhar S", "Nithin A", "Shakeel Rifath", "Vasantha Raj"], "title": "Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models", "comment": "10 pages, seven figures about Multimodal Cinematic Video Synthesis\n  Using Text-to-Image and Audio Generation Models", "summary": "Advances in generative artificial intelligence have altered multimedia\ncreation, allowing for automatic cinematic video synthesis from text inputs.\nThis work describes a method for creating 60-second cinematic movies\nincorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for\nnarrative structuring, and a hybrid audio pipeline using gTTS and\nYouTube-sourced music. It uses a five-scene framework, which is augmented by\nlinear frame interpolation, cinematic post-processing (e.g., sharpening), and\naudio-video synchronization to provide professional-quality results. It was\ncreated in a GPU-accelerated Google Colab environment using Python 3.11. It has\na dual-mode Gradio interface (Simple and Advanced), which supports resolutions\nof up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA\nmemory management and error handling ensure reliability. The experiments\ndemonstrate outstanding visual quality, narrative coherence, and efficiency,\nfurthering text-to-video synthesis for creative, educational, and industrial\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528Stable Diffusion\u3001GPT-2\u548c\u97f3\u9891\u6280\u672f\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u611f60\u79d2\u7535\u5f71\u7684\u65b9\u6cd5\uff0c\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u89c6\u89c9\u548c\u53d9\u8ff0\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\uff0c\u81ea\u52a8\u4ece\u6587\u672c\u8f93\u5165\u5408\u6210\u7535\u5f71\u89c6\u9891\u6210\u4e3a\u53ef\u80fd\uff0c\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u6539\u8fdb\u591a\u5a92\u4f53\u521b\u4f5c\u6280\u672f\u3002", "method": "\u8be5\u7814\u7a76\u7ed3\u5408\u4e86Stable Diffusion\u7528\u4e8e\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\u3001GPT-2\u7528\u4e8e\u53d9\u4e8b\u6784\u5efa\u4ee5\u53cagTTS\u548cYouTube\u97f3\u9891\u6e90\u7684\u6df7\u5408\u97f3\u9891\u7ba1\u9053\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u4e94\u573a\u666f\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u5e27\u63d2\u503c\u3001\u7535\u5f71\u7ea7\u540e\u5904\u7406\u548c\u97f3\u89c6\u9891\u540c\u6b65\u6280\u672f\u5236\u6210\u4e86\u4e13\u4e1a\u7ea760\u79d2\u7535\u5f71\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e86\u51fa\u8272\u7684\u89c6\u89c9\u8d28\u91cf\u3001\u53d9\u8ff0\u8fde\u8d2f\u6027\u548c\u6548\u7387\uff0c\u8fdb\u4e00\u6b65\u63a8\u8fdb\u4e86\u6587\u672c\u5230\u89c6\u9891\u5408\u6210\u6280\u672f\u5728\u521b\u610f\u3001\u6559\u80b2\u548c\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728GPU\u52a0\u901f\u7684Google Colab\u73af\u5883\u4e2d\u4f7f\u7528Python 3.11\u548cCUDA\u5185\u5b58\u7ba1\u7406\uff0c\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u768460\u79d2\u7535\u5f71\u89c6\u9891\uff0c\u652f\u6301\u9ad8\u8fbe1024x768\u5206\u8fa8\u7387\u548c15-30 FPS\u5e27\u7387\u3002"}}
{"id": "2506.10095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10095", "abs": "https://arxiv.org/abs/2506.10095", "authors": ["Xiao Li", "Joel Kreuzwieser", "Alan Peters"], "title": "When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs", "comment": "This paper was developed for presentation at ICML 2025 Tokshop\n  Workshop, but is now submitted as a standalone contribution", "summary": "We investigate how large language models respond to prompts that differ only\nin their token-level realization but preserve the same semantic intent, a\nphenomenon we call prompt variance. We propose Prompt-Based Semantic Shift\n(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under\nsemantically equivalent prompt rewordings. Applied to ten constrained tasks,\nPBSS reveals consistent, model-specific response shifts, suggesting statistical\nregularities linked to tokenization and decoding. These results highlight an\noverlooked dimension of model evaluation stability under rephrasing and suggest\nthat tokenization strategies and decoding dynamics may contribute to\npost-training quality of service instability.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7PBSS\u6846\u67b6\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u76f8\u540c\u7684\u63d0\u793a\u53d8\u5316\u4e0b\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u54cd\u5e94\u7684\u7edf\u8ba1\u89c4\u5f8b\u6027\uff0c\u5f3a\u8c03\u4e86\u91cd\u8ff0\u5bf9\u6a21\u578b\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u6211\u4eec\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5982\u4f55\u54cd\u5e94\u4ec5\u6709\u5355\u8bcd\u7ea7\u522b\u5b9e\u73b0\u5dee\u5f02\u4f46\u4fdd\u6301\u76f8\u540c\u8bed\u4e49\u610f\u56fe\u7684\u63d0\u793a\uff0c\u8fd9\u4e00\u73b0\u8c61\u88ab\u79f0\u4e3a\u63d0\u793a\u53d8\u5316\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bca\u65ad\u6846\u67b6\u2014\u2014\u57fa\u4e8e\u63d0\u793a\u7684\u8bed\u4e49\u6f02\u79fb\uff08PBSS\uff09\uff0c\u7528\u4e8e\u6d4b\u91cf\u5728\u8bed\u4e49\u7b49\u6548\u7684\u63d0\u793a\u91cd\u8ff0\u4e0b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u6f02\u79fb\u3002", "result": "\u5e94\u7528PBSS\u5230\u5341\u4e2a\u53d7\u9650\u4efb\u52a1\u4e2d\uff0c\u63ed\u793a\u4e86\u4e00\u81f4\u7684\u6a21\u578b\u7279\u5b9a\u54cd\u5e94\u53d8\u5316\uff0c\u8fd9\u8868\u660e\u4e0e\u5206\u8bcd\u548c\u89e3\u7801\u76f8\u5173\u7684\u7edf\u8ba1\u89c4\u5f8b\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u6a21\u578b\u8bc4\u4f30\u7a33\u5b9a\u6027\u7ef4\u5ea6\uff0c\u5373\u91cd\u8ff0\u5bf9\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u8868\u660e\u5206\u8bcd\u7b56\u7565\u548c\u89e3\u7801\u52a8\u6001\u53ef\u80fd\u5bf9\u6a21\u578b\u5728\u8bad\u7ec3\u540e\u670d\u52a1\u8d28\u91cf\u7684\u7a33\u5b9a\u6027\u6709\u8d21\u732e\u3002"}}
{"id": "2506.10082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10082", "abs": "https://arxiv.org/abs/2506.10082", "authors": ["Chenjian Gao", "Lihe Ding", "Xin Cai", "Zhanpeng Huang", "Zibin Wang", "Tianfan Xue"], "title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning", "comment": "12 pages", "summary": "Video editing using diffusion models has achieved remarkable results in\ngenerating high-quality edits for videos. However, current methods often rely\non large-scale pretraining, limiting flexibility for specific edits.\nFirst-frame-guided editing provides control over the first frame, but lacks\nflexibility over subsequent frames. To address this, we propose a mask-based\nLoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video\n(I2V) models for flexible video editing. Our approach preserves background\nregions while enabling controllable edits propagation. This solution offers\nefficient and adaptable video editing without altering the model architecture.\nTo better steer this process, we incorporate additional references, such as\nalternate viewpoints or representative scene states, which serve as visual\nanchors for how content should unfold. We address the control challenge using a\nmask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model\nto the editing context. The model must learn from two distinct sources: the\ninput video provides spatial structure and motion cues, while reference images\noffer appearance guidance. A spatial mask enables region-specific learning by\ndynamically modulating what the model attends to, ensuring that each area draws\nfrom the appropriate source. Experimental results show our method achieves\nsuperior video editing performance compared to state-of-the-art methods.", "AI": {"tldr": "This paper introduces a mask-based LoRA tuning method that improves the flexibility and adaptability of video editing by modifying pre-trained image-to-video models.", "motivation": "The motivation behind this method is to overcome the limitations of existing video editing techniques, which often require large-scale pretraining and lack the flexibility needed for specific edits. The proposed method aims to provide a more adaptable and efficient editing solution.", "method": "Our approach involves a mask-based LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video (I2V) models to allow for flexible video editing. This method preserves the background while enabling controlled edits to propagate through the video. It uses spatial masks to guide the learning process, drawing attention to different sources - the input video for spatial structure and motion, and reference images for appearance.", "result": "The experimental results demonstrate that the proposed method achieves superior performance in video editing compared to state-of-the-art techniques.", "conclusion": "The conclusion is that the mask-based LoRA tuning method effectively addresses the need for more flexible and adaptable video editing without altering the underlying model architecture, providing a robust solution for video editing tasks."}}
{"id": "2506.10116", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10116", "abs": "https://arxiv.org/abs/2506.10116", "authors": ["Caijun Jia", "Nan Xu", "Jingxuan Wei", "Qingli Wang", "Lei Wang", "Bihui Yu", "Junnan Zhu"], "title": "ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering", "comment": null, "summary": "Recently, large language models have shown remarkable reasoning capabilities\nthrough long-chain reasoning before responding. However, how to extend this\ncapability to visual reasoning tasks remains an open challenge. Existing\nmultimodal reasoning approaches transfer such visual reasoning task into\ntextual reasoning task via several image-to-text conversions, which often lose\ncritical structural and semantic information embedded in visualizations,\nespecially for tasks like chart question answering that require a large amount\nof visual details. To bridge this gap, we propose ChartReasoner, a code-driven\nnovel two-stage framework designed to enable precise, interpretable reasoning\nover charts. We first train a high-fidelity model to convert diverse chart\nimages into structured ECharts codes, preserving both layout and data semantics\nas lossless as possible. Then, we design a general chart reasoning data\nsynthesis pipeline, which leverages this pretrained transport model to\nautomatically and scalably generate chart reasoning trajectories and utilizes a\ncode validator to filter out low-quality samples. Finally, we train the final\nmultimodal model using a combination of supervised fine-tuning and\nreinforcement learning on our synthesized chart reasoning dataset and\nexperimental results on four public benchmarks clearly demonstrate the\neffectiveness of our proposed ChartReasoner. It can preserve the original\ndetails of the charts as much as possible and perform comparably with\nstate-of-the-art open-source models while using fewer parameters, approaching\nthe performance of proprietary systems like GPT-4o in out-of-domain settings.", "AI": {"tldr": "ChartReasoner is a two-stage framework for visual reasoning over charts, which outperforms existing models while maintaining simplicity and reducing parameter usage.", "motivation": "To address the challenge of extending large language models' reasoning capabilities to visual reasoning tasks, specifically for chart question answering, where critical visual details are often lost in existing multimodal reasoning approaches.", "method": "We propose ChartReasoner, a two-stage framework that first converts chart images into structured ECharts codes, and then uses a chart reasoning data synthesis pipeline for automatic and scalable generation of reasoning datasets, combined with supervised fine-tuning and reinforcement learning for training the final model.", "result": "Experimental results on four public benchmarks demonstrate the effectiveness of ChartReasoner, showcasing its ability to maintain original chart details and perform comparably with state-of-the-art models while using fewer parameters.", "conclusion": "ChartReasoner preserves the specific details of charts and can closely match the performance of proprietary models with fewer parameters, especially in out-of-domain settings."}}
{"id": "2506.10084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10084", "abs": "https://arxiv.org/abs/2506.10084", "authors": ["Bin Guo", "John H. L. Hansen"], "title": "DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding", "comment": "NeurIPS 2025", "summary": "Conventional vision backbones, despite their success, often construct\nfeatures through a largely uniform cascade of operations, offering limited\nexplicit pathways for adaptive, iterative refinement. This raises a compelling\nquestion: can principles from classical search algorithms instill a more\nalgorithmic, structured, and logical processing flow within these networks,\nleading to representations built through more interpretable, perhaps\nreasoning-like decision processes? We introduce DeepTraverse, a novel vision\narchitecture directly inspired by algorithmic search strategies, enabling it to\nlearn features through a process of systematic elucidation and adaptive\nrefinement distinct from conventional approaches. DeepTraverse operationalizes\nthis via two key synergistic components: recursive exploration modules that\nmethodically deepen feature analysis along promising representational paths\nwith parameter sharing for efficiency, and adaptive calibration modules that\ndynamically adjust feature salience based on evolving global context. The\nresulting algorithmic interplay allows DeepTraverse to intelligently construct\nand refine feature patterns. Comprehensive evaluations across a diverse suite\nof image classification benchmarks show that DeepTraverse achieves highly\ncompetitive classification accuracy and robust feature discrimination, often\noutperforming conventional models with similar or larger parameter counts. Our\nwork demonstrates that integrating such algorithmic priors provides a\nprincipled and effective strategy for building more efficient, performant, and\nstructured vision backbones.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u53d7\u7ecf\u5178\u641c\u7d22\u7b97\u6cd5\u542f\u53d1\u7684DeepTraverse\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7\u9012\u5f52\u548c\u81ea\u9002\u5e94\u7684\u65b9\u5f0f\u667a\u80fd\u5316\u5730\u6784\u5efa\u7279\u5f81\uff0c\u901a\u8fc7\u591a\u7ec4\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u5c55\u793a\u4e86\u5176\u9ad8\u5ea6\u7ade\u4e89\u529b\u7684\u8868\u73b0\u3002", "motivation": "\u8bba\u6587\u52a8\u673a\u5728\u4e8e\u6539\u8fdb\u4f20\u7edf\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u8fd9\u4e9b\u7f51\u7edc\u901a\u8fc7\u4e00\u7cfb\u5217\u76f8\u4f3c\u7684\u64cd\u4f5c\u6784\u5efa\u7279\u5f81\uff0c\u7f3a\u4e4f\u81ea\u9002\u5e94\u8fed\u4ee3\u6539\u8fdb\u7684\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u7ecf\u5178\u641c\u7d22\u7b97\u6cd5\u7684\u539f\u5219\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u8fdb\u884c\u66f4\u52a0\u7ed3\u6784\u5316\u548c\u903b\u8f91\u5316\u7684\u7279\u5f81\u5904\u7406\u3002", "method": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86DeepTraverse\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u67b6\u6784\uff0c\u53d7\u5230\u7ecf\u5178\u641c\u7d22\u7b97\u6cd5\u7684\u542f\u53d1\u3002DeepTraverse\u901a\u8fc7\u9012\u5f52\u63a2\u7d22\u6a21\u5757\u548c\u81ea\u9002\u5e94\u6821\u51c6\u6a21\u5757\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u7cfb\u7edf\u5730\u6df1\u5316\u7279\u5f81\u5206\u6790\u5e76\u6839\u636e\u5168\u5c40\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u7279\u5f81\u7684\u91cd\u8981\u6027\uff0c\u5b9e\u73b0\u4e86\u7279\u5f81\u6a21\u5f0f\u7684\u667a\u80fd\u6784\u5efa\u548c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5177\u6709\u76f8\u4f3c\u53c2\u6570\u6570\u91cf\u7684\u4f20\u7edf\u6a21\u578b\u76f8\u6bd4\uff0cDeepTraverse\u5728\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u80fd\u591f\u5b9e\u73b0\u9ad8\u5ea6\u51c6\u786e\u7684\u5206\u7c7b\u548c\u5f3a\u5927\u7684\u7279\u5f81\u533a\u5206\u80fd\u529b\uff0c\u5e76\u4e14\u7ecf\u5e38\u4f18\u4e8e\u8fd9\u4e9b\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8bc1\u660e\uff0c\u901a\u8fc7\u5c06\u7b97\u6cd5\u5148\u9a8c\u77e5\u8bc6\u6574\u5408\u8fdb\u5165\u89c6\u89c9\u9aa8\u5e72\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5efa\u7acb\u66f4\u6709\u6548\u3001\u6027\u80fd\u66f4\u597d\u4e14\u7ed3\u6784\u66f4\u6e05\u6670\u89c6\u89c9\u9aa8\u5e72\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.10139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10139", "abs": "https://arxiv.org/abs/2506.10139", "authors": ["Jiaxin Wen", "Zachary Ankner", "Arushi Somani", "Peter Hase", "Samuel Marks", "Jacob Goldman-Wetzler", "Linda Petrini", "Henry Sleight", "Collin Burns", "He He", "Shi Feng", "Ethan Perez", "Jan Leike"], "title": "Unsupervised Elicitation of Language Models", "comment": null, "summary": "To steer pretrained language models for downstream tasks, today's\npost-training paradigm relies on humans to specify desired behaviors. However,\nfor models with superhuman capabilities, it is difficult or impossible to get\nhigh-quality human supervision. To address this challenge, we introduce a new\nunsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune\npretrained language models on their own generated labels, \\emph{without\nexternal supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward\nmodeling tasks, our method matches the performance of training on golden\nsupervision and outperforms training on crowdsourced human supervision. On\ntasks where LMs' capabilities are strongly superhuman, our method can elicit\nthose capabilities significantly better than training on human labels. Finally,\nwe show that our method can improve the training of frontier LMs: we use our\nmethod to train an unsupervised reward model and use reinforcement learning to\ntrain a Claude 3.5 Haiku-based assistant. Both the reward model and the\nassistant outperform their human-supervised counterparts.", "AI": {"tldr": "\u63d0\u51fa\u4e86Internal Coherence Maximization (ICM)\u7b97\u6cd5\uff0c\u65e0\u76d1\u7763\u5730\u4f18\u5316\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u6027\u80fd\u8d76\u4e0a\u6216\u8d85\u8fc7\u4eba\u7c7b\u76d1\u7763\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u8d85\u7ea7\u80fd\u529b\u6a21\u578b\u96be\u4ee5\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b\u76d1\u7763\u7684\u95ee\u9898\u3002", "method": "Internal Coherence Maximization (ICM)\u7b97\u6cd5\u7528\u4e8e\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002", "result": "\u5728GSM8k-verification, TruthfulQA, \u548c Alpaca\u5956\u52b1\u6a21\u578b\u4efb\u52a1\u4e2d\u5339\u914d\u5230\u9ec4\u91d1\u76d1\u7763\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u8d85\u7ea7\u80fd\u529b\u6a21\u578b\u7684\u80fd\u529b\u8d85\u8fc7\u4eba\u7c7b\u65f6\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u53d1\u6325\u8fd9\u4e9b\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u524d\u6cbf\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u65e0\u8bba\u662f\u5956\u52b1\u6a21\u578b\u8fd8\u662f\u52a9\u7406\u6a21\u578b\u90fd\u4f18\u4e8e\u4eba\u7c7b\u76d1\u7763\u8bad\u7ec3\u7684\u6a21\u578b\u3002"}}
{"id": "2506.10085", "categories": ["cs.CV", "cs.AI", "I.2.6; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.10085", "abs": "https://arxiv.org/abs/2506.10085", "authors": ["Christos Ziakas", "Alessandra Russo"], "title": "Test-Time Adaptation for Generalizable Task Progress Estimation", "comment": "pages, 2 figures, accepted to the 2nd Workshop on Test-Time\n  Adaptation: Putting Updates to the Test (PUT) at 42nd International\n  Conference on Machine Learning (ICML), Vancouver, Canada, 2025", "summary": "We propose a test-time adaptation method that enables a progress estimation\nmodel to adapt online to the visual and temporal context of test trajectories\nby optimizing a learned self-supervised objective. To this end, we introduce a\ngradient-based meta-learning strategy to train the model on expert visual\ntrajectories and their natural language task descriptions, such that test-time\nadaptation improves progress estimation relying on semantic content over\ntemporal order. Our test-time adaptation method generalizes from a single\ntraining environment to diverse out-of-distribution tasks, environments, and\nembodiments, outperforming the state-of-the-art in-context learning approach\nusing autoregressive vision-language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u4f7f\u7528\u5143\u5b66\u4e60\u6765\u63d0\u9ad8\u6a21\u578b\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u9002\u5e94\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8fdb\u5ea6\u4f30\u8ba1\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u89c6\u89c9\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4ee5\u6539\u8fdb\u8fdb\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u8fdb\u5ea6\u4f30\u8ba1\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u4f18\u5316\u5b66\u4e60\u7684\u81ea\u76d1\u7763\u76ee\u6807\u6765\u5728\u7ebf\u9002\u5e94\u6d4b\u8bd5\u8f68\u8ff9\u7684\u89c6\u89c9\u548c\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u4f7f\u7528\u4e13\u5bb6\u89c6\u89c9\u8f68\u8ff9\u53ca\u5176\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4ece\u800c\u4f7f\u6d4b\u8bd5\u65f6\u9002\u5e94\u80fd\u591f\u4f9d\u9760\u8bed\u4e49\u5185\u5bb9\u800c\u4e0d\u662f\u65f6\u95f4\u987a\u5e8f\u6765\u63d0\u9ad8\u8fdb\u5ea6\u4f30\u8ba1\u3002", "result": "\u6211\u4eec\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u4e00\u8bad\u7ec3\u73af\u5883\u6cdb\u5316\u5230\u591a\u6837\u5316\u7684\u51fa\u5206\u5e03\u4efb\u52a1\u3001\u73af\u5883\u548c\u5b9e\u73b0\uff0c\u4f18\u4e8e\u4f7f\u7528\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u7b56\u7565\u80fd\u591f\u5728\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u76ee\u524d\u6700\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2506.10150", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.10150", "abs": "https://arxiv.org/abs/2506.10150", "authors": ["Aakriti Kumar", "Nalin Poungpeth", "Diyi Yang", "Erina Farrell", "Bruce Lambert", "Matthew Groh"], "title": "When Large Language Models are Reliable for Judging Empathic Communication", "comment": null, "summary": "Large language models (LLMs) excel at generating empathic responses in\ntext-based conversations. But, how reliably do they judge the nuances of\nempathic communication? We investigate this question by comparing how experts,\ncrowdworkers, and LLMs annotate empathic communication across four evaluative\nframeworks drawn from psychology, natural language processing, and\ncommunications applied to 200 real-world conversations where one speaker shares\na personal problem and the other offers support. Drawing on 3,150 expert\nannotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess\ninter-rater reliability between these three annotator groups. We find that\nexpert agreement is high but varies across the frameworks' sub-components\ndepending on their clarity, complexity, and subjectivity. We show that expert\nagreement offers a more informative benchmark for contextualizing LLM\nperformance than standard classification metrics. Across all four frameworks,\nLLMs consistently approach this expert level benchmark and exceed the\nreliability of crowdworkers. These results demonstrate how LLMs, when validated\non specific tasks with appropriate benchmarks, can support transparency and\noversight in emotionally sensitive applications including their use as\nconversational companions.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u4e13\u5bb6\u3001\u7fa4\u4f17\u5de5\u4f5c\u8005\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56db\u4e2a\u8bc4\u4f30\u6846\u67b6\u4e0b\u7684\u6807\u6ce8\u7ed3\u679c\uff0c\u53d1\u73b0LLMs\u5728\u5224\u65ad\u540c\u7406\u5fc3\u4ea4\u6d41\u7684\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u51e0\u4e4e\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\uff0c\u5e76\u8d85\u8fc7\u4e86\u7fa4\u4f17\u5de5\u4f5c\u8005\u3002\u8be5\u7814\u7a76\u652f\u6301\u5728\u60c5\u611f\u654f\u611f\u5e94\u7528\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u76d1\u7763\u4f7f\u7528LLMs\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u63a2\u7d22LLMs\u5728\u4ea7\u751f\u540c\u7406\u5fc3\u56de\u5e94\u65b9\u9762\u662f\u5426\u53ef\u9760\uff0c\u7279\u522b\u662f\u5728\u7406\u89e3\u540c\u7406\u5fc3\u4ea4\u6d41\u7684\u7ec6\u5fae\u5dee\u5f02\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u4e13\u5bb6\u3001\u7fa4\u4f17\u5de5\u4f5c\u8005\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56db\u5957\u8bc4\u4f30\u6846\u67b6\u4e0b\u5bf9\u540c\u7406\u5fc3\u4ea4\u6d41\u7684\u6807\u6ce8\u7ed3\u679c\u6765\u63a2\u7a76LLMs\u5728\u5224\u65ad\u540c\u7406\u5fc3\u4ea4\u6d41\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u7684\u53ef\u9760\u6027\u3002\u7814\u7a76\u91c7\u7528\u4e86200\u4e2a\u771f\u5b9e\u4e16\u754c\u4e2d\u4e00\u4e2a\u8bf4\u8bdd\u4eba\u5206\u4eab\u4e2a\u4eba\u95ee\u9898\uff0c\u53e6\u4e00\u4e2a\u8bf4\u8bdd\u4eba\u63d0\u4f9b\u652f\u6301\u7684\u5bf9\u8bdd\u6837\u672c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e13\u5bb6\u7684\u534f\u8bae\u6c34\u5e73\u5f88\u9ad8\uff0c\u4f46\u5728\u4e0d\u540c\u6846\u67b6\u7684\u5b50\u7ec4\u4ef6\u4e2d\u6709\u6240\u4e0d\u540c\uff0c\u8fd9\u53d6\u51b3\u4e8e\u8fd9\u4e9b\u7ec4\u4ef6\u7684\u6e05\u6670\u5ea6\u3001\u590d\u6742\u6027\u548c\u4e3b\u89c2\u6027\u3002LLMs\u5728\u6240\u6709\u56db\u4e2a\u6846\u67b6\u4e0b\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\u7684\u57fa\u51c6\uff0c\u5176\u53ef\u9760\u6027\u8d85\u8fc7\u4e86\u7fa4\u4f17\u5de5\u4f5c\u8005\uff0c\u5e76\u80fd\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4ee5\u9002\u5f53\u7684\u57fa\u51c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u663e\u793a\uff0c\u5f53\u7528\u7279\u5b9a\u4efb\u52a1\u7684\u9002\u5f53\u57fa\u51c6\u9a8c\u8bc1\u65f6\uff0cLLMs\u53ef\u4ee5\u652f\u6301\u5728\u60c5\u611f\u654f\u611f\u5e94\u7528\u4e2d\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u76d1\u7763\uff0c\u5305\u62ec\u4f5c\u4e3a\u5bf9\u8bdd\u4f34\u4fa3\u7684\u7528\u9014\u3002"}}
{"id": "2506.10100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10100", "abs": "https://arxiv.org/abs/2506.10100", "authors": ["Yantai Yang", "Yuhao Wang", "Zichen Wen", "Luo Zhongwei", "Chang Zou", "Zhipeng Zhang", "Chuan Wen", "Linfeng Zhang"], "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.", "AI": {"tldr": "\u672c\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aEfficientVLA\u7684\u7cfb\u7edf\u6027\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u7b56\u7565\u6d88\u9664\u5197\u4f59\u4ee5\u63d0\u9ad8VLA\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u5b58\u7684\u52a0\u901f\u65b9\u6cd5\u5f80\u5f80\u53ea\u9488\u5bf9\u5355\u4e00\u7684\u4f4e\u6548\u7387\u95ee\u9898\uff0c\u4e0d\u80fd\u5168\u9762\u89e3\u51b3\u6574\u4e2aVLA\u7ba1\u9053\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u672c\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u7cfb\u7edf\u800c\u4e0d\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEfficientVLA\u7684\u52a0\u901f\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5730\u6d88\u9664\u5197\u4f59\u6765\u52a0\u901fVision-Language-Action (VLA) \u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u5177\u4f53\u5305\u62ec\u4e09\u4e2a\u7b56\u7565\uff1a\uff081\uff09\u53bb\u9664\u8bed\u8a00\u6a21\u5757\u4e2d\u529f\u80fd\u4e0d\u91cd\u8981\u7684\u5c42\uff1b\uff082\uff09\u4f18\u5316\u89c6\u89c9\u5904\u7406\u8def\u5f84\uff0c\u9009\u62e9\u4e00\u7ec4\u7cbe\u7b80\u4e14\u591a\u6837\u5316\u7684\u89c6\u89c9\u4ee4\u724c\uff1b\uff083\uff09\u5728\u8fed\u4ee3\u6269\u6563\u7684\u52a8\u4f5c\u5934\u4e2d\u7f13\u5b58\u548c\u91cd\u7528\u5173\u952e\u4e2d\u95f4\u7279\u5f81\uff0c\u4ee5\u51cf\u8f7b\u65f6\u95f4\u4e0a\u7684\u8ba1\u7b97\u5197\u4f59\u3002", "result": "\u5e94\u7528EfficientVLA\u65b9\u6cd5\u5230\u6807\u51c6\u7684VLA\u6a21\u578bCogACT\u4e0a\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u4e861.93\u500d\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e8671.1%\uff0c\u540c\u65f6\u5728SIMPLER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6210\u529f\u7387\u4ec5\u4e0b\u964d\u4e860.6%\u3002", "conclusion": "EfficientVLA\u8bc1\u660e\u4e86\u5176\u5728\u52a0\u901fVLA\u6a21\u578b\u63a8\u7406\u7684\u540c\u65f6\uff0c\u80fd\u591f\u4fdd\u6301\u8f83\u9ad8\u7684\u6027\u80fd\uff0c\u8fd9\u5bf9\u4e8e\u63a8\u52a8\u57fa\u4e8e\u6269\u6563\u67b6\u6784\u7684VLA\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.10154", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10154", "abs": "https://arxiv.org/abs/2506.10154", "authors": ["Bidyarthi Paul", "SM Musfiqur Rahman", "Dipta Biswas", "Md. Ziaul Hasan", "Md. Zahid Hossain"], "title": "Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME", "comment": null, "summary": "Research on understanding emotions in written language continues to expand,\nespecially for understudied languages with distinctive regional expressions and\ncultural features, such as Bangla. This study examines emotion analysis using\n22,698 social media comments from the EmoNoBa dataset. For language analysis,\nwe employ machine learning models: Linear SVM, KNN, and Random Forest with\nn-gram data from a TF-IDF vectorizer. We additionally investigated how PCA\naffects the reduction of dimensionality. Moreover, we utilized a BiLSTM model\nand AdaBoost to improve decision trees. To make our machine learning models\neasier to understand, we used LIME to explain the predictions of the AdaBoost\nclassifier, which uses decision trees. With the goal of advancing sentiment\nanalysis in languages with limited resources, our work examines various\ntechniques to find efficient techniques for emotion identification in Bangla.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5404\u79cd\u6280\u672f\u5728\u5b5f\u52a0\u62c9\u8bed\u60c5\u611f\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u60c5\u611f\u8bc6\u522b\u7684\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u7ee7\u7eed\u63a2\u7d22\u7406\u89e3\u4e66\u9762\u8bed\u8a00\u4e2d\u7684\u60c5\u611f\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u72ec\u7279\u533a\u57df\u8868\u8fbe\u548c\u6587\u5316\u7279\u5f81\u7684\u8f83\u5c11\u7814\u7a76\u7684\u8bed\u8a00\u4e2d\uff0c\u5982\u5b5f\u52a0\u62c9\u8bed\u3002", "method": "\u7814\u7a76\u4e2d\u4f7f\u7528\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec\u7ebf\u6027SVM\uff0cKNN\uff0c\u968f\u673a\u68ee\u6797\uff0c\u4ee5\u53caTF-IDF\u5411\u91cf\u5316\u5668\u7684n-gram\u6570\u636e\uff0c\u8fd8\u7814\u7a76\u4e86PCA\u5bf9\u964d\u7ef4\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u4e86BiLSTM\u6a21\u578b\u548cAdaBoost\u6765\u6539\u8fdb\u51b3\u7b56\u6811\uff0c\u5e76\u7528LIME\u6765\u89e3\u91caAdaBoost\u5206\u7c7b\u5668\u7684\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u5bf9EmoNoBa\u6570\u636e\u96c6\u4e2d\u768422,698\u4e2a\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u7814\u7a76\u8868\u660e\uff0c\u6240\u4f7f\u7528\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u9ad8\u5b5f\u52a0\u62c9\u8bed\u60c5\u611f\u5206\u6790\u7684\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u79cd\u6280\u672f\u4ee5\u627e\u5230\u5728\u5b5f\u52a0\u62c9\u8bed\u4e2d\u8fdb\u884c\u60c5\u7eea\u8bc6\u522b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u3002"}}
{"id": "2506.10117", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2506.10117", "abs": "https://arxiv.org/abs/2506.10117", "authors": ["Klim Kireev", "Ana-Maria Cre\u0163u", "Raphael Meier", "Sarah Adel Bargal", "Elissa Redmiles", "Carmela Troncoso"], "title": "A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild", "comment": "14 pages, 6 figures", "summary": "Platforms and the law regulate digital content depicting minors (defined as\nindividuals under 18 years of age) differently from other types of content.\nGiven the sheer amount of content that needs to be assessed, machine\nlearning-based automation tools are commonly used to detect content depicting\nminors. To our knowledge, no dataset or benchmark currently exists for\ndetecting these identification methods in a multi-modal environment. To fill\nthis gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an\nimage-caption dataset aimed at benchmarking tools that detect depictions of\nminors. Our dataset is richer than previous child image datasets, containing\nimages of children in a variety of contexts, including fictional depictions and\npartially visible bodies. ICCWD contains 10,000 image-caption pairs manually\nlabeled to indicate the presence or absence of a child in the image. To\ndemonstrate the possible utility of our dataset, we use it to benchmark three\ndifferent detectors, including a commercial age estimation system applied to\nimages. Our results suggest that child detection is a challenging task, with\nthe best method achieving a 75.3% true positive rate. We hope the release of\nour dataset will aid in the design of better minor detection methods in a wide\nrange of scenarios.", "AI": {"tldr": "\u7814\u7a76\u8005\u521b\u5efa\u4e86ICWCD\u6570\u636e\u96c6\uff0c\u5305\u542b10,000\u5bf9\u56fe\u50cf\u548c\u6807\u9898\uff0c\u7528\u4e8e\u68c0\u6d4b\u672a\u6210\u5e74\u4eba\uff0c\u7ed3\u679c\u663e\u793a\u68c0\u6d4b\u672a\u6210\u5e74\u4eba\u662f\u9879\u6311\u6218\u3002", "motivation": "Content\u4e2d\u7684\u52a8\u673a\u5728\u4e8e\u5e73\u53f0\u548c\u6cd5\u5f8b\u5bf9\u4e8e\u6d89\u53ca\u672a\u6210\u5e74\u4eba\u7684\u6570\u5b57\u5185\u5bb9\u6709\u7279\u522b\u7684\u76d1\u7ba1\u63aa\u65bd\uff0c\u4f46\u662f\u7531\u4e8e\u6240\u9700\u8bc4\u4f30\u7684\u5185\u5bb9\u91cf\u5de8\u5927\uff0c\u56e0\u6b64\u9700\u8981\u673a\u5668\u5b66\u4e60\u81ea\u52a8\u5316\u7684\u5de5\u5177\u8fdb\u884c\u68c0\u6d4b\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u9488\u5bf9\u6b64\u7c7b\u68c0\u6d4b\u7684\u591a\u6a21\u6001\u73af\u5883\u4e0b\u7684\u6570\u636e\u96c6\u3002", "method": "Content\u4e2d\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u5efa\u7acb\u4e86\u4e00\u4e2a\u540d\u4e3aImage-Caption Children in the Wild Dataset (ICCWD)\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u5305\u542b\u4e8610,000\u4e2a\u56fe\u50cf-\u6807\u9898\u5bf9\uff0c\u65e8\u5728\u7528\u6765\u68c0\u6d4b\u56fe\u50cf\u4e2d\u662f\u5426\u5305\u542b\u672a\u6210\u5e74\u4eba\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u4f7f\u7528ICCWD\u6570\u636e\u96c6\u6d4b\u8bd5\u4e86\u4e09\u4e2a\u4e0d\u540c\u7684\u68c0\u6d4b\u5668\uff0c\u5305\u62ec\u4e00\u4e2a\u5546\u4e1a\u5e74\u9f84\u8bc4\u4f30\u7cfb\u7edf\uff0c\u7ed3\u679c\u663e\u793a\u6700\u597d\u7684\u65b9\u6cd5\u8fbe\u5230\u4e8675.3%\u7684\u771f\u6b63\u4f8b\u7387\uff0c\u8fd9\u8bf4\u660e\u672a\u6210\u5e74\u4eba\u68c0\u6d4b\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u8be5\u6570\u636e\u96c6ICWCD\u7684\u53d1\u5e03\u80fd\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u597d\u7684\u672a\u6210\u5e74\u4eba\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u60c5\u51b5\u3002"}}
{"id": "2506.10155", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10155", "abs": "https://arxiv.org/abs/2506.10155", "authors": ["Elizabeth Demers", "Victor Xiaoqi Wang", "Kean Wu"], "title": "Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities", "comment": "50 pages, 6 figures, 5 tables", "summary": "Human capital (HC) is increasingly important to corporate value creation.\nUnlike other assets, however, HC is not currently subject to well-defined\nmeasurement or disclosure rules. We use a machine learning algorithm (word2vec)\ntrained on a confirmed set of HC disclosures to develop a comprehensive list of\nHC-related keywords classified into five subcategories (DEI; health and safety;\nlabor relations and culture; compensation and benefits; and demographics and\nother) that capture the multidimensional nature of HC management. We share our\nlexicon, corporate HC disclosures, and the Python code used to develop the\nlexicon, and we provide detailed examples of using our data and code, including\nfor fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the\ncode to capture another construct of interest) with their samples of corporate\ncommunications to address pertinent HC questions. We close with a discussion of\nfuture research opportunities related to HC management and disclosure.", "AI": {"tldr": "\u901a\u8fc7word2vec\u7b97\u6cd5\u5904\u7406\u4eba\u529b\u8d44\u672c\u62ab\u9732\u4fe1\u606f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u7c7b\u5b50\u7c7b\u522b\u7684\u7efc\u5408\u4eba\u529b\u8d44\u672c\u5173\u952e\u8bcd\u5217\u8868\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ee3\u7801\u53ca\u793a\u4f8b\u4ee5\u4f9b\u7814\u7a76\u8005\u4f7f\u7528\u6216\u4fee\u6539\u4ee5\u66f4\u597d\u5730\u6316\u6398\u4eba\u529b\u8d44\u672c\u4fe1\u606f\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u660e\u786e\u7684\u8861\u91cf\u548c\u62ab\u9732\u89c4\u5219\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6784\u5efa\u4eba\u529b\u8d44\u672c\u5173\u952e\u8bcd\u5217\u8868\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u8861\u91cf\u4eba\u529b\u8d44\u672c\u5bf9\u4f01\u4e1a\u7684\u4ef7\u503c\u521b\u9020\u7684\u91cd\u8981\u6027\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5word2vec\u5206\u6790\u5df2\u786e\u8ba4\u7684\u4eba\u529b\u8d44\u672c\u62ab\u9732\u4fe1\u606f\uff0c\u5efa\u7acb\u4e86\u5305\u542b\u4e94\u7c7b\u5b50\u7c7b\u522b\u7684\u4eba\u529b\u8d44\u672c\u76f8\u5173\u5173\u952e\u8bcd\u5217\u8868\uff0c\u5e76\u63d0\u4f9bPython\u4ee3\u7801\u53ca\u8be6\u7ec6\u4f7f\u7528\u793a\u4f8b\u3002", "result": "\u7814\u7a76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08word2vec\uff09\u5904\u7406\u5df2\u786e\u8ba4\u7684\u4eba\u529b\u8d44\u672c\u62ab\u9732\u4fe1\u606f\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u7c7b\u5b50\u7c7b\u522b\u7684\u5168\u9762\u4eba\u529b\u8d44\u672c\u76f8\u5173\u5173\u952e\u8bcd\u5217\u8868\uff0c\u5305\u62ecDEI\uff1b\u5065\u5eb7\u4e0e\u5b89\u5168\uff1b\u52b3\u8d44\u5173\u7cfb\u548c\u6587\u5316\uff1b\u85aa\u916c\u548c\u798f\u5229\uff1b\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u5176\u4ed6\u3002\u7814\u7a76\u8005\u53ef\u4ee5\u5229\u7528\u6211\u4eec\u7684\u4eba\u529b\u8d44\u672c\u8bcd\u6c47\u8868\uff08\u6216\u4fee\u6539\u4ee3\u7801\u4ee5\u6355\u6349\u5176\u4ed6\u611f\u5174\u8da3\u7684\u6784\u5efa\u6a21\u5757\uff09\u6765\u5206\u6790\u516c\u53f8\u901a\u8baf\u4e2d\u6d89\u53ca\u7684\u4eba\u529b\u8d44\u672c\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86Python\u4ee3\u7801\u548c\u793a\u4f8b\u6765\u8f85\u52a9\u4f7f\u7528\u6216\u8fdb\u4e00\u6b65\u7ec6\u5316BERT\u6a21\u578b\u3002\u6700\u540e\u8ba8\u8bba\u4e86\u4e0e\u4eba\u529b\u8d44\u672c\u7ba1\u7406\u548c\u62ab\u9732\u76f8\u5173\u672a\u6765\u7814\u7a76\u7684\u673a\u4f1a\u3002", "conclusion": "\u7814\u7a76\u5206\u4eab\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5f00\u53d1\u7684\u4eba\u529b\u8d44\u672c\u8bcd\u6c47\u8868\u53ca\u5176\u53ef\u80fd\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u516c\u53f8\u901a\u8baf\u4e2d\u5206\u6790\u4eba\u529b\u8d44\u672c\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u6570\u636e\uff0c\u548c\u4ee3\u7801\u3002\u4e0d\u65ad\u8fdb\u6b65\u7684\u6280\u672f\u80fd\u591f\u4f7f\u672a\u6765\u7684\u4eba\u529b\u8d44\u672c\u7ba1\u7406\u548c\u62ab\u9732\u7814\u7a76\u53d7\u76ca\u3002"}}
{"id": "2506.10119", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10119", "abs": "https://arxiv.org/abs/2506.10119", "authors": ["Natanael Lucena", "F\u00e1bio S. da Silva", "Ricardo Rios"], "title": "Detec\u00e7\u00e3o da Psor\u00edase Utilizando Vis\u00e3o Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers", "comment": "12 pages, in Portuguese language, 2 figures, 2 tables, and 4\n  formulas. To be published in the Proceedings of the LII Brazilian Integrated\n  Software and Hardware Seminar 2025 (SEMISH 2025)", "summary": "This paper presents a comparison of the performance of Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying\nimages containing lesions of psoriasis and diseases similar to it. Models\npre-trained on ImageNet were adapted to a specific data set. Both achieved high\npredictive metrics, but the ViTs stood out for their superior performance with\nsmaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the\nbest results, with an f1-score of 96.4%, and is recommended as the most\nefficient architecture for automated psoriasis detection. This article\nreinforces the potential of ViTs for medical image classification tasks.", "AI": {"tldr": "ViTs, notably the DaViT-B model, outperform CNNs in psoriasis detection with an f1-score of 96.4%, making them a promising tool for medical image classification.", "motivation": "To investigate whether ViTs can outperform CNNs in the specific domain of psoriasis and similar diseases image classification with comparable or smaller model size.", "method": "This paper compares the performance of CNNs and ViTs in classifying images of psoriasis lesions and similar diseases. The models were pre-trained on ImageNet and then fine-tuned on a specific dataset.", "result": "ViTs demonstrated better performance with smaller models. The DaViT-B model achieved the highest f1-score of 96.4%.", "conclusion": "ViTs, particularly DaViT-B, are efficient for automated psoriasis detection, highlighting the potential of ViTs in medical image classification tasks."}}
{"id": "2506.10161", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10161", "abs": "https://arxiv.org/abs/2506.10161", "authors": ["Yi Wang", "Max Kreminski"], "title": "Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective", "comment": "In 2025 IEEE Conference on Games (CoG)", "summary": "Story generation has been a prominent application of Large Language Models\n(LLMs). However, understanding LLMs' ability to produce high-quality stories\nremains limited due to challenges in automatic evaluation methods and the high\ncost and subjectivity of manual evaluation. Computational narratology offers\nvaluable insights into what constitutes a good story, which has been applied in\nthe symbolic narrative planning approach to story generation. This work aims to\ndeepen the understanding of LLMs' story generation capabilities by using them\nto solve narrative planning problems. We present a benchmark for evaluating\nLLMs on narrative planning based on literature examples, focusing on causal\nsoundness, character intentionality, and dramatic conflict. Our experiments\nshow that GPT-4 tier LLMs can generate causally sound stories at small scales,\nbut planning with character intentionality and dramatic conflict remains\nchallenging, requiring LLMs trained with reinforcement learning for complex\nreasoning. The results offer insights on the scale of stories that LLMs can\ngenerate while maintaining quality from different aspects. Our findings also\nhighlight interesting problem solving behaviors and shed lights on challenges\nand considerations for applying LLM narrative planning in game environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u4f7f\u7528\u8ba1\u7b97\u53d9\u4e8b\u5b66\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u53d9\u4e8b\u89c4\u5212\u4e2d\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGPT-4\u7ea7\u522b\u7684LLMs\u80fd\u591f\u751f\u6210\u76f8\u5bf9\u7b80\u5355\u7684\u56e0\u679c\u8fde\u8d2f\u7684\u6545\u4e8b\uff0c\u4f46\u5bf9\u4e8e\u5e26\u6709\u89d2\u8272\u52a8\u673a\u548c\u620f\u5267\u51b2\u7a81\u7684\u590d\u6742\u53d9\u4e8b\uff0c\u4ecd\u9700\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u6539\u8fdb\u3002\u7ed3\u679c\u5bf9LLMs\u751f\u6210\u6545\u4e8b\u65f6\u7684\u8d28\u91cf\u4fdd\u6301\u63d0\u51fa\u4e86\u89c1\u89e3\u3002\u7814\u7a76\u8fd8\u6307\u51fa\u8fd9\u4e9b\u6a21\u578b\u5728\u89e3\u51b3\u53d9\u4e8b\u95ee\u9898\u65f6\u7684\u6709\u8da3\u884c\u4e3a\uff0c\u5e76\u63d0\u4f9b\u4e86\u6e38\u620f\u73af\u5883\u4e2d\u5e94\u7528LLMs\u53d9\u4e8b\u89c4\u5212\u7684\u6311\u6218\u548c\u8003\u8651\u56e0\u7d20\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u5728\u4e8e\u6df1\u5165\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6545\u4e8b\u65f6\u7684\u80fd\u529b\u3002\u5c3d\u7ba1LLMs\u5df2\u7ecf\u6210\u4e3a\u6545\u4e8b\u751f\u6210\u7684\u70ed\u95e8\u5e94\u7528\uff0c\u4f46\u5bf9\u5176\u751f\u6210\u9ad8\u8d28\u91cf\u6545\u4e8b\u7684\u80fd\u529b\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002\u8fd9\u662f\u56e0\u4e3a\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6311\u6218\uff0c\u624b\u52a8\u8bc4\u4f30\u7684\u6210\u672c\u548c\u4e3b\u89c2\u6027\u90fd\u5f88\u9ad8\u3002\u901a\u8fc7\u5e94\u7528\u8ba1\u7b97\u53d9\u4e8b\u5b66\uff0c\u8bba\u6587\u5e0c\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6df1\u5165\u5206\u6790LLMs\u5728\u53d9\u4e8b\u89c4\u5212\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bba\u6587\u91c7\u7528\u8ba1\u7b97\u53d9\u4e8b\u5b66\u63d0\u4f9b\u7684\u57fa\u51c6\u65b9\u6cd5\u6765\u8bc4\u4f30LLMs\u5728\u53d9\u4e8b\u89c4\u5212\u65b9\u9762\u7684\u80fd\u529b\u3002\u8fd9\u4e00\u57fa\u51c6\u662f\u57fa\u4e8e\u6587\u5b66\u4e2d\u7684\u5177\u4f53\u4f8b\u5b50\u6765\u8bbe\u8ba1\u7684\uff0c\u8bc4\u4f30\u7684\u7ef4\u5ea6\u5305\u62ec\u56e0\u679c\u4e00\u81f4\u6027\u3001\u89d2\u8272\u52a8\u673a\u548c\u620f\u5267\u51b2\u7a81\u3002\u4f7f\u7528\u8fd9\u4e2a\u65b9\u6cd5\u5bf9LLMs\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5c24\u5176\u662f\u5e94\u7528GPT-4\u7ea7\u522b\u7684\u8bed\u8a00\u6a21\u578b\u6765\u751f\u6210\u548c\u8bc4\u4f30\u6545\u4e8b\u3002\u4e3a\u4e86\u5904\u7406\u66f4\u590d\u6742\u7684\u53d9\u4e8b\u95ee\u9898\uff0c\u8fd8\u8003\u8651\u4e86\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3LLMs\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eGPT-4\u7684LLMs\u80fd\u591f\u751f\u6210\u5c0f\u89c4\u6a21\u7684\u56e0\u679c\u8fde\u8d2f\u7684\u6545\u4e8b\uff0c\u4f46\u5bf9\u4e8e\u6d89\u53ca\u89d2\u8272\u52a8\u673a\u548c\u620f\u5267\u51b2\u7a81\u7684\u590d\u6742\u53d9\u4e8b\u89c4\u5212\uff0c\u4ecd\u65e7\u662f\u76f8\u5f53\u5177\u6709\u6311\u6218\u6027\u7684\u3002\u8fd9\u8868\u660e\uff0c\u53ea\u6709\u5728\u5e94\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\u7684LLMs\u624d\u80fd\u591f\u5e94\u5bf9\u590d\u6742\u7684\u53d9\u4e8b\u63a8\u7406\u4efb\u52a1\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u6709\u5173LLMs\u80fd\u591f\u7ef4\u6301\u6545\u4e8b\u8d28\u91cf\u7684\u89c4\u6a21\u7684\u89c1\u89e3\u3002", "conclusion": "\u603b\u4f53\u7ed3\u8bba\u662f\uff0c\u867d\u7136GPT-4\u7ea7\u522b\u7684LLMs\u5728\u751f\u6210\u7b80\u5355\u53d9\u4e8b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u7684\u53d9\u4e8b\u89c4\u5212\u65b9\u9762\u4ecd\u65e7\u9762\u4e34\u7740\u6311\u6218\uff0c\u7279\u522b\u662f\u8981\u5728\u6709\u9650\u7684\u89c4\u6a21\u5185\u7ef4\u6301\u56e0\u679c\u4e00\u81f4\u6027\u3001\u89d2\u8272\u52a8\u673a\u548c\u620f\u5267\u51b2\u7a81\u3002\u8fd9\u4e3a\u5982\u4f55\u5728\u6e38\u620f\u73af\u5883\u4e2d\u6709\u6548\u4f7f\u7528LLMs\u8fdb\u884c\u53d9\u4e8b\u89c4\u5212\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.10128", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10128", "abs": "https://arxiv.org/abs/2506.10128", "authors": ["Xiyao Wang", "Zhengyuan Yang", "Chao Feng", "Yongyuan Liang", "Yuhang Zhou", "Xiaoyu Liu", "Ziyi Zang", "Ming Li", "Chung-Ching Lin", "Kevin Lin", "Linjie Li", "Furong Huang", "Lijuan Wang"], "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs", "comment": null, "summary": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning\nlarge language models (LLMs) using tasks that are challenging yet easily\nverifiable, such as math reasoning or code generation. However, extending this\nsuccess to visual perception in vision-language models (VLMs) has been impeded\nby the scarcity of vision-centric tasks that are simultaneously challenging and\nunambiguously verifiable. To this end, we introduce ViCrit (Visual Caption\nHallucination Critic), an RL proxy task that trains VLMs to localize a subtle,\nsynthetic visual hallucination injected into paragraphs of human-written image\ncaptions. Starting from a 200-word captions, we inject a single, subtle visual\ndescription error-altering a few words on objects, attributes, counts, or\nspatial relations-and task the model to pinpoint the corrupted span given the\nimage and the modified caption. This formulation preserves the full perceptual\ndifficulty while providing a binary, exact-match reward that is easy to compute\nand unambiguous. Models trained with the ViCrit Task exhibit substantial gains\nacross a variety of VL benchmarks. Crucially, the improvements transfer beyond\nnatural-image training data to abstract image reasoning and visual math,\nshowing promises of learning to perceive rather than barely memorizing seen\nobjects. To facilitate evaluation, we further introduce ViCrit-Bench, a\ncategory-balanced diagnostic benchmark that systematically probes perception\nerrors across diverse image domains and error types. Together, our results\ndemonstrate that fine-grained hallucination criticism is an effective and\ngeneralizable objective for enhancing visual perception in VLMs.", "AI": {"tldr": "\u901a\u8fc7\u65b0\u5f15\u5165\u7684ViCrit\u4efb\u52a1\uff0c\u7814\u7a76\u4eba\u5458\u6210\u529f\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u5e7b\u89c9\u7ec6\u8282\u7684\u5b9a\u4f4d\uff0c\u5e76\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u65e2\u5177\u6709\u6311\u6218\u6027\u53c8\u6613\u4e8e\u9a8c\u8bc1\u7684\u89c6\u89c9\u76f8\u5173\u4efb\u52a1\uff0c\u963b\u788d\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5f15\u5165ViCrit\u4efb\u52a1\uff0c\u65e8\u5728\u63d0\u5347\u6a21\u578b\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f15\u5165ViCrit\uff08\u89c6\u89c9\u63cf\u8ff0\u5e7b\u60f3\u8bc4\u5224\u8005\uff09\u4efb\u52a1\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5bf9\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u4efb\u52a1\u8981\u6c42\u6a21\u578b\u5728\u4eba\u7c7b\u64b0\u5199\u7684\u56fe\u50cf\u63cf\u8ff0\u4e2d\u5b9a\u4f4d\u6ce8\u5165\u7684\u7ec6\u5fae\u3001\u5408\u6210\u7684\u89c6\u89c9\u5e7b\u89c9\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4ece200\u5b57\u7684\u63cf\u8ff0\u5f00\u59cb\uff0c\u6ce8\u5165\u4e00\u4e2a\u7ec6\u5fae\u7684\u89c6\u89c9\u63cf\u8ff0\u9519\u8bef\uff0c\u901a\u8fc7\u6539\u53d8\u4e00\u4e9b\u8bcd\u8bed\u6765\u63cf\u8ff0\u5bf9\u8c61\u3001\u5c5e\u6027\u3001\u6570\u91cf\u6216\u7a7a\u95f4\u5173\u7cfb\uff0c\u4efb\u52a1\u662f\u6a21\u578b\u5728\u7ed9\u5b9a\u56fe\u50cf\u548c\u4fee\u6539\u540e\u7684\u63cf\u8ff0\u4e0b\u627e\u51fa\u53d7\u635f\u7684\u6bb5\u843d\u3002", "result": "\u7ecf\u8fc7ViCrit\u4efb\u52a1\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u79cd\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u63d0\u5347\uff0c\u5e76\u80fd\u5c06\u5176\u63d0\u5347\u5e94\u7528\u5230\u62bd\u8c61\u56fe\u50cf\u63a8\u7406\u548c\u89c6\u89c9\u6570\u5b66\uff0c\u8868\u660e\u6a21\u578b\u5b66\u4f1a\u4e86\u611f\u77e5\u800c\u4e0d\u662f\u7b80\u5355\u5730\u8bb0\u5fc6\u5df2\u89c1\u8fc7\u7684\u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86ViCrit-Bench\u4f5c\u4e3a\u4e00\u4e2a\u7c7b\u522b\u5e73\u8861\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u4e0d\u540c\u56fe\u50cf\u9886\u57df\u548c\u9519\u8bef\u7c7b\u578b\u7684\u611f\u77e5\u9519\u8bef\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u7684\u5e7b\u89c9\u6279\u8bc4\u662f\u4e00\u79cd\u6709\u6548\u7684\u76ee\u6807\uff0c\u80fd\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2506.10202", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10202", "abs": "https://arxiv.org/abs/2506.10202", "authors": ["Shubhashis Roy Dipta", "Francis Ferraro"], "title": "Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval", "comment": null, "summary": "Recent approaches have shown impressive proficiency in extracting and\nleveraging parametric knowledge from Large-Language Models (LLMs) and\nVision-Language Models (VLMs). In this work, we consider how we can improve the\nidentification and retrieval of videos related to complex real-world events by\nautomatically extracting latent parametric knowledge about those events. We\npresent Q2E: a Query-to-Event decomposition method for zero-shot multilingual\ntext-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our\napproach demonstrates that we can enhance the understanding of otherwise overly\nsimplified human queries by decomposing the query using the knowledge embedded\nin LLMs and VLMs. We additionally show how to apply our approach to both visual\nand speech-based inputs. To combine this varied multimodal knowledge, we adopt\nentropy-based fusion scoring for zero-shot fusion. Through evaluations on two\ndiverse datasets and multiple retrieval metrics, we demonstrate that Q2E\noutperforms several state-of-the-art baselines. Our evaluation also shows that\nintegrating audio information can significantly improve text-to-video\nretrieval. We have released code and data for future research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u79f0\u4e3aQ2E\u7684Query-to-Event\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u591a\u8bed\u8a00\u7684\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6765\u63d0\u5347\u5bf9\u590d\u6742\u4e8b\u4ef6\u89c6\u9891\u7684\u7406\u89e3\u548c\u68c0\u7d22\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u590d\u6742\u73b0\u5b9e\u4e8b\u4ef6\u76f8\u5173\u89c6\u9891\u7684\u8bc6\u522b\u548c\u68c0\u7d22\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u52a8\u63d0\u53d6\u6709\u5173\u8fd9\u4e9b\u4e8b\u4ef6\u7684\u6f5c\u5728\u53c2\u6570\u77e5\u8bc6\u6765\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5173\u6ce8\u4e86\u96f6\u6837\u672c\u3001\u591a\u8bed\u8a00\u7684\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u7684\u8de8\u6570\u636e\u96c6\u3001\u8de8\u57df\u3001\u8de8\u6a21\u578b\u7684\u81ea\u9002\u5e94\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQ2E\u7684Query-to-Event\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u591a\u8bed\u8a00\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u5d4c\u5165\u7684\u77e5\u8bc6\uff0c\u6765\u589e\u5f3a\u4eba\u7c7b\u67e5\u8be2\u7684\u7406\u89e3\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u88ab\u5e94\u7528\u4e8e\u89c6\u89c9\u548c\u8bed\u97f3\u8f93\u5165\u7684\u5904\u7406\u3002\u4e3a\u4e86\u6574\u5408\u4e0d\u540c\u6a21\u6001\u7684\u77e5\u8bc6\uff0c\u91c7\u7528\u4e86\u57fa\u4e8e\u71b5\u7684\u878d\u5408\u8bc4\u5206\u65b9\u6cd5\u6765\u8fdb\u884c\u96f6\u6837\u672c\u878d\u5408\u3002", "result": "\u901a\u8fc7\u5bf9\u4e24\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u91c7\u7528\u591a\u79cd\u68c0\u7d22\u5ea6\u91cf\u6807\u51c6\uff0c\u7ed3\u679c\u663e\u793aQ2E\u65b9\u6cd5\u8d85\u8d8a\u4e86\u51e0\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u7ed3\u5408\u97f3\u9891\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6587\u672c\u5230\u89c6\u9891\u7684\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cQ2E\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u63d0\u5347\u4eba\u7c7b\u67e5\u8be2\u7684\u7406\u89e3\u4e14\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u901a\u8fc7\u5c06\u97f3\u9891\u4fe1\u606f\u6574\u5408\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6570\u636e\u4f9b\u672a\u6765\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2506.10145", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10145", "abs": "https://arxiv.org/abs/2506.10145", "authors": ["Rajeev Yasarla", "Shizhong Han", "Hsin-Pai Cheng", "Litian Liu", "Shweta Mahajan", "Apratim Bhattacharyya", "Yunxiao Shi", "Risheek Garrepalli", "Hong Cai", "Fatih Porikli"], "title": "RoCA: Robust Cross-Domain End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,\noffering significant potential. However, few studies have looked into the\npractical challenge of deployment across domains (e.g., cities). Although\nseveral works have incorporated Large Language Models (LLMs) to leverage their\nopen-world knowledge, LLMs do not guarantee cross-domain driving performance\nand may incur prohibitive retraining costs during domain adaptation. In this\npaper, we propose RoCA, a novel framework for robust cross-domain E2E\nautonomous driving. RoCA formulates the joint probabilistic distribution over\nthe tokens that encode ego and surrounding vehicle information in the E2E\npipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of\nbasis tokens with corresponding trajectories, which span diverse driving\nscenarios. Then, given any driving scene, it is able to probabilistically infer\nthe future trajectory. By using RoCA together with a base E2E model in\nsource-domain training, we improve the generalizability of the base model,\nwithout requiring extra inference computation. In addition, RoCA enables robust\nadaptation on new target domains, significantly outperforming direct\nfinetuning. We extensively evaluate RoCA on various cross-domain scenarios and\nshow that it achieves strong domain generalization and adaptation performance.", "AI": {"tldr": "\u63d0\u51faRoCA\u6846\u67b6\u4ee5\u63d0\u5347\u8de8\u57df\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u6027\u80fd\uff0c\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u53ca\u9002\u5e94\u6027\uff0c\u63d0\u5347\u5728\u4e0d\u540c\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u8de8\u57df\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u8de8\u57df\u81ea\u52a8\u9a7e\u9a76\u8868\u73b0\u4e0d\u4f73\u4e14\u9700\u8981\u9ad8\u6602\u7684\u91cd\u8bad\u7ec3\u6210\u672c\u95ee\u9898\u3002", "method": "RoCA\u6846\u67b6\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u4e00\u7ec4\u57fa\u8bcd\u6c47\u548c\u5bf9\u5e94\u7684\u8f68\u8ff9\uff0c\u8fd9\u7ec4\u57fa\u8bcd\u6c47\u80fd\u591f\u6db5\u76d6\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u3002\u5728\u7ed9\u5b9a\u4efb\u4f55\u9a7e\u9a76\u573a\u666f\u4e0b\uff0cRoCA\u80fd\u6982\u7387\u6027\u5730\u63a8\u6d4b\u672a\u6765\u8f68\u8ff9\u3002", "result": "\u7ed3\u5408RoCA\u4e0e\u57fa\u7ebf\u7684E2E\u6a21\u578b\u5728\u6e90\u57df\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u6539\u8fdb\u4e86\u57fa\u7ebf\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u5728\u65b0\u7684\u76ee\u6807\u57df\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u9002\u5e94\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u76f4\u63a5\u5fae\u8c03\u7684\u65b9\u6cd5\u3002", "conclusion": "RoCA\u80fd\u591f\u5b9e\u73b0\u5f3a\u7684\u9886\u57df\u6cdb\u5316\u548c\u9002\u5e94\u6027\u8868\u73b0\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8de8\u57df\u9a7e\u9a76\u573a\u666f\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u8de8\u57df\u6027\u80fd\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.10209", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10209", "abs": "https://arxiv.org/abs/2506.10209", "authors": ["Prakamya Mishra", "Jiang Liu", "Jialian Wu", "Xiaodong Yu", "Zicheng Liu", "Emad Barsoum"], "title": "TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games", "comment": null, "summary": "Large reasoning models (LRMs) have demonstrated impressive reasoning\ncapabilities across a broad range of tasks including Olympiad-level\nmathematical problems, indicating evidence of their complex reasoning\nabilities. While many reasoning benchmarks focus on the STEM domain, the\nability of LRMs to reason correctly in broader task domains remains\nunderexplored. In this work, we introduce \\textbf{TTT-Bench}, a new benchmark\nthat is designed to evaluate basic strategic, spatial, and logical reasoning\nabilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games\nthat humans can effortlessly solve from a young age. We propose a simple yet\nscalable programmatic approach for generating verifiable two-player game\nproblems for TTT-Bench. Although these games are trivial for humans, they\nrequire reasoning about the intentions of the opponent, as well as the game\nboard's spatial configurations, to ensure a win. We evaluate a diverse set of\nstate-of-the-art LRMs, and \\textbf{discover that the models that excel at hard\nmath problems frequently fail at these simple reasoning games}. Further testing\nreveals that our evaluated reasoning models score on average $\\downarrow$ 41\\%\n\\& $\\downarrow$ 5\\% lower on TTT-Bench compared to MATH 500 \\& AIME 2024\nrespectively, with larger models achieving higher performance using shorter\nreasoning traces, where most of the models struggle on long-term strategic\nreasoning situations on simple and new TTT-Bench tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5 TTT-Bench\uff0c\u5b83\u901a\u8fc7\u56db\u4e2a\u4e24\u4eba\u73a9\u7684\u7c7b\u4f3c\u4e8e\u4e95\u5b57\u6e38\u620f\u6765\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u57fa\u672c\u7b56\u7565\u3001\u7a7a\u95f4\u548c\u903b\u8f91\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002\u867d\u7136\u8fd9\u4e9b\u6e38\u620f\u5bf9\u4eba\u7c7b\u6765\u8bf4\u975e\u5e38\u7b80\u5355\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u8fd9\u4e9b\u6e38\u620f\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u5373\u4f7f\u5b83\u4eec\u5728\u590d\u6742\u7684\u6570\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728STEM\u9886\u57df\u4e4b\u5916\u7684\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u4f20\u7edf\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u96c6\u4e2d\u5728STEM\u9886\u57df\uff0c\u800c\u8fd9\u7c7b\u6a21\u578b\u5728\u66f4\u5e7f\u6cdb\u4efb\u52a1\u9886\u57df\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u5c1a\u5f85\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e00\u5957\u56db\u79cd\u4e24\u4eba\u73a9\u7684\u7c7b\u4f3c\u4e95\u5b57\u6e38\u620f\u7684\u535a\u5f08\u6765\u6d4b\u8bd5\u8fd9\u4e9b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u4e9b\u6e38\u620f\u8981\u6c42\u6a21\u578b\u7406\u89e3\u5bf9\u624b\u610f\u56fe\u4ee5\u53ca\u68cb\u76d8\u7684\u7a7a\u95f4\u914d\u7f6e\uff0c\u4ee5\u786e\u4fdd\u80dc\u5229\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u90a3\u4e9b\u5728\u56f0\u96be\u6570\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u7684\u6a21\u578b\uff0c\u5728\u8fd9\u4e9b\u7b80\u5355\u7684\u63a8\u7406\u6e38\u620f\u4e2d\u5374\u7ecf\u5e38\u5931\u8d25\u3002\u76f8\u8f83\u4e8eMATH 500\u548cAIME 2024\u6d4b\u8bd5\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728TTT-Bench\u4e0a\u7684\u5f97\u5206\u5e73\u5747\u5206\u522b\u4f4e\u4e8641%\u548c5%\u3002\u8f83\u5927\u7684\u6a21\u578b\u867d\u7136\u4f7f\u7528\u8f83\u77ed\u7684\u63a8\u7406\u8f68\u8ff9\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u6210\u7ee9\uff0c\u4f46\u5728TTT-Bench\u4e0a\u7684\u957f\u671f\u7b56\u7565\u6027\u63a8\u7406\u573a\u666f\u4e2d\u4ecd\u7136\u6323\u624e\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u867d\u7136\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u67d0\u4e9bSTEM\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u50cfTTT-Bench\u8fd9\u6837\u7b80\u5355\u7684\u975eSTEM\u63a8\u7406\u4efb\u52a1\u4e2d\u5374\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.10173", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10173", "abs": "https://arxiv.org/abs/2506.10173", "authors": ["Mohammad Jalali", "Haoyu Lei", "Amin Gohari", "Farzan Farnia"], "title": "SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score", "comment": null, "summary": "Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86SPARKE\u65b9\u6cd5\uff0c\u5229\u7528\u6761\u4ef6\u71b5\u8fdb\u884c\u591a\u6837\u6027\u548c\u63d0\u793a\u611f\u77e5\u7684\u52a8\u6001\u591a\u6837\u6027\u6d4b\u91cf\uff0c\u901a\u8fc7\u6761\u4ef6\u6f5c\u5728RKE\u5f97\u5206\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u5728\u5927\u89c4\u6a21\u751f\u6210\u8bbe\u7f6e\u4e2d\u6709\u6548\u63a7\u5236\u591a\u6837\u6027\u7684\u76ee\u6807\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u4f46\u5728\u4fdd\u8bc1\u63d0\u793a\u5f15\u5bfc\u751f\u6210\u6a21\u578b\u6240\u751f\u6210\u6837\u672c\u7684\u5145\u5206\u591a\u6837\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u9700\u8981\u5728\u8bed\u4e49\u76f8\u4f3c\u7684\u63d0\u793a\u4e0b\u63d0\u793a\u611f\u77e5\u5730\u8bc4\u4f30\u751f\u6210\u6570\u636e\u7684\u591a\u6837\u6027\u3002", "method": "SPARKE\u65b9\u6cd5\u5229\u7528\u6761\u4ef6\u71b5\u8fdb\u884c\u591a\u6837\u6027\u5f15\u5bfc\uff0c\u5b83\u6839\u636e\u76f8\u4f3c\u63d0\u793a\u52a8\u6001\u8c03\u6574\u591a\u6837\u6027\u6d4b\u91cf\uff0c\u5e76\u5141\u8bb8\u63d0\u793a\u611f\u77e5\u7684\u591a\u6837\u6027\u63a7\u5236\u3002\u4e3a\u4e86\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\uff0c\u63d0\u51fa\u4e86\u6761\u4ef6\u6f5c\u5728RKE\u5f97\u5206\u5f15\u5bfc\uff0c\u4f7f\u5f97\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u666e\u904d\u71b5\u5ea6\u91cf\u7684$O(n^3)$\u51cf\u5c11\u5230$O(n)$\uff0c\u652f\u6301\u6f5c\u5728\u6570\u5343\u6b21\u5206\u522b\u9488\u5bf9\u4e0d\u540c\u63d0\u793a\u8fdb\u884c\u591a\u6837\u6027\u548c\u5f15\u5bfc\u91c7\u6837\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684SPARKE\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u6570\u636e\u7684\u63d0\u793a\u611f\u77e5\u591a\u6837\u6027\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6210\u672c\u4e0e\u591a\u6837\u6027\u95f4\u7684\u5e73\u8861\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cSPARKE\u65b9\u6cd5\u5728\u63d0\u5347\u751f\u6210\u6570\u636e\u7684\u63d0\u793a\u611f\u77e5\u591a\u6837\u6027\u7684\u540c\u65f6\uff0c\u5e76\u672a\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.10231", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.10231", "abs": "https://arxiv.org/abs/2506.10231", "authors": ["Anneliese Brei", "Katharine Henry", "Abhisheik Sharma", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "Classifying Unreliable Narrators with Large Language Models", "comment": "ACL 2025", "summary": "Often when we interact with a first-person account of events, we consider\nwhether or not the narrator, the primary speaker of the text, is reliable. In\nthis paper, we propose using computational methods to identify unreliable\nnarrators, i.e. those who unintentionally misrepresent information. Borrowing\nliterary theory from narratology to define different types of unreliable\nnarrators based on a variety of textual phenomena, we present TUNa, a\nhuman-annotated dataset of narratives from multiple domains, including blog\nposts, subreddit posts, hotel reviews, and works of literature. We define\nclassification tasks for intra-narrational, inter-narrational, and\ninter-textual unreliabilities and analyze the performance of popular\nopen-weight and proprietary LLMs for each. We propose learning from literature\nto perform unreliable narrator classification on real-world text data. To this\nend, we experiment with few-shot, fine-tuning, and curriculum learning\nsettings. Our results show that this task is very challenging, and there is\npotential for using LLMs to identify unreliable narrators. We release our\nexpert-annotated dataset and code and invite future research in this area.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u8ba1\u7b97\u65b9\u6cd5\u6765\u8bc6\u522b\u6587\u672c\u4e2d\u65e0\u610f\u8bef\u5bfc\u4fe1\u606f\u7684\u4e0d\u53ef\u9760\u53d9\u8ff0\u8005\uff0c\u6784\u5efa\u4e86TUNa\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u4f7f\u7528LLMs\u8fdb\u884c\u5206\u7c7b\u7684\u6f5c\u529b\u3002", "motivation": "\u6587\u7ae0\u52a8\u673a\u5728\u4e8e\u5229\u7528\u8ba1\u7b97\u65b9\u6cd5\u81ea\u52a8\u8bc6\u522b\u6587\u672c\u4e2d\u7684\u4e0d\u53ef\u9760\u53d9\u8ff0\u8005\uff0c\u4e3a\u6587\u672c\u5206\u6790\u63d0\u4f9b\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u8ba1\u7b97\u65b9\u6cd5\u6765\u8bc6\u522b\u4e0d\u53ef\u9760\u7684\u53d9\u8ff0\u8005\uff0c\u5373\u90a3\u4e9b\u65e0\u610f\u4e2d\u8bef\u5bfc\u4fe1\u606f\u7684\u53d9\u8ff0\u8005\uff0c\u5e76\u6807\u6ce8\u4e86\u4e00\u4e2a\u540d\u4e3aTUNa\u7684\u591a\u9886\u57df\u53d9\u8ff0\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u5305\u62ec\u535a\u5ba2\u6587\u7ae0\u3001\u5b50\u7248\u5757\u6587\u7ae0\u3001\u9152\u5e97\u8bc4\u8bba\u548c\u6587\u5b66\u4f5c\u54c1\u7b49\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u9879\u4efb\u52a1\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc6\u522b\u4e0d\u53ef\u9760\u53d9\u8ff0\u8005\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\uff0c\u501f\u52a9\u6587\u5b66\u7406\u8bba\u8fdb\u884c\u4e0d\u53ef\u9760\u53d9\u8ff0\u8005\u7684\u5206\u7c7b\u5b9e\u9a8c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e13\u5bb6\u6807\u6ce8\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u9080\u8bf7\u672a\u6765\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.10174", "categories": ["cs.CV", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2506.10174", "abs": "https://arxiv.org/abs/2506.10174", "authors": ["Yael Frischholz", "Devis Tuia", "Michael Lehning"], "title": "Retrieval of Surface Solar Radiation through Implicit Albedo Recovery from Temporal Context", "comment": "14 pages, 7 figures", "summary": "Accurate retrieval of surface solar radiation (SSR) from satellite imagery\ncritically depends on estimating the background reflectance that a spaceborne\nsensor would observe under clear-sky conditions. Deviations from this baseline\ncan then be used to detect cloud presence and guide radiative transfer models\nin inferring atmospheric attenuation. Operational retrieval algorithms\ntypically approximate background reflectance using monthly statistics, assuming\nsurface properties vary slowly relative to atmospheric conditions. However,\nthis approach fails in mountainous regions where intermittent snow cover and\nchanging snow surfaces are frequent. We propose an attention-based emulator for\nSSR retrieval that implicitly learns to infer clear-sky surface reflectance\nfrom raw satellite image sequences. Built on the Temporo-Spatial Vision\nTransformer, our approach eliminates the need for hand-crafted features such as\nexplicit albedo maps or cloud masks. The emulator is trained on instantaneous\nSSR estimates from the HelioMont algorithm over Switzerland, a region\ncharacterized by complex terrain and dynamic snow cover. Inputs include\nmulti-spectral SEVIRI imagery from the Meteosat Second Generation platform,\naugmented with static topographic features and solar geometry. The target\nvariable is HelioMont's SSR, computed as the sum of its direct and diffuse\nhorizontal irradiance components, given at a spatial resolution of 1.7 km. We\nshow that, when provided a sufficiently long temporal context, the model\nmatches the performances of albedo-informed models, highlighting the model's\nability to internally learn and exploit latent surface reflectance dynamics.\nOur geospatial analysis shows this effect is most powerful in mountainous\nregions and improves generalization in both simple and complex topographic\nsettings. Code and datasets are publicly available at\nhttps://github.com/frischwood/HeMu-dev.git", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u62df\u5668\u6765\u6539\u8fdb\u5c71\u533a\u7b49\u5730\u8868\u53cd\u5c04\u7387\u53d8\u5316\u590d\u6742\u7684\u5730\u533a\u7684\u8868\u9762\u592a\u9633\u8f90\u5c04(SUR)\u7684\u4f30\u7b97\u3002", "motivation": "\u73b0\u6709\u7684\u536b\u661f\u53cd\u5c04\u7387\u4f30\u7b97\u65b9\u6cd5\u5728\u5c71\u533a\u8868\u73b0\u4e0d\u4f73,\u56e0\u4e3a\u8fd9\u4e9b\u5730\u533a\u96ea\u8986\u76d6\u53d8\u5316\u9891\u7e41,\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e9b\u533a\u57df\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u62df\u5668,\u7528\u4e8e\u4ece\u539f\u59cb\u536b\u661f\u56fe\u50cf\u5e8f\u5217\u4e2d\u9690\u5f0f\u5730\u63a8\u65ad\u6674\u5929\u5929\u9762\u53cd\u5c04\u7387\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u65f6\u7a7a\u89c6\u89c9\u53d8\u538b\u5668,\u6d88\u9664\u4e86\u624b\u5de5\u5236\u4f5c\u7279\u5f81\u5982\u663e\u5f0f\u7684\u53cd\u7167\u7387\u56fe\u6216\u4e91\u63a9\u6a21\u7684\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e,\u5f53\u63d0\u4f9b\u8db3\u591f\u957f\u7684\u65f6\u7a7a\u4e0a\u4e0b\u6587\u65f6,\u8be5\u6a21\u578b\u7684\u6027\u80fd\u53ef\u4ee5\u5339\u654c\u4f7f\u7528\u53cd\u7167\u7387\u4fe1\u606f\u7684\u6a21\u578b,\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5c24\u5176\u6709\u6548\u3002", "conclusion": "\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u548c\u5229\u7528\u6f5c\u5728\u7684\u5730\u8868\u53cd\u5c04\u7387\u52a8\u6001,\u63d0\u9ad8\u4e86\u5728\u5c71\u533a\u5730\u5f62\u4e2d\u592a\u9633\u8f90\u5c04\u4f30\u7b97\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.10245", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10245", "abs": "https://arxiv.org/abs/2506.10245", "authors": ["Iago Alves Brito", "Julia Soares Dollis", "Fernanda Bufon F\u00e4rber", "Diogo Fernandes Costa Silva", "Arlindo Rodrigues Galv\u00e3o Filho"], "title": "ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese", "comment": "8 pages, 5 tables, 1 figure", "summary": "We present ToxSyn-PT, the first large-scale Portuguese corpus that enables\nfine-grained hate-speech classification across nine legally protected minority\ngroups. The dataset contains 53,274 synthetic sentences equally distributed\nbetween minorities groups and toxicity labels. ToxSyn-PT is created through a\nnovel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot\nexpansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and\n(4) enrichment, plus additional neutral texts to curb overfitting to\ngroup-specific cues. The resulting corpus is class-balanced, stylistically\ndiverse, and free from the social-media domain that dominate existing\nPortuguese datasets. Despite domain differences with traditional benchmarks,\nexperiments on both binary and multi-label classification on the corpus yields\nstrong results across five public Portuguese hate-speech datasets,\ndemonstrating robust generalization even across domain boundaries. The dataset\nis publicly released to advance research on synthetic data and hate-speech\ndetection in low-resource settings.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ToxSyn-PT\uff0c\u4e00\u4e2a\u9488\u5bf9\u8461\u8404\u7259\u8bed\u4ec7\u6068\u8a00\u8bba\u7684\u5408\u6210\u8bed\u6599\u5e93\uff0c\u8be5\u8bed\u6599\u5e93\u5305\u542b\u4e86\u9488\u5bf9\u5c11\u6570\u7fa4\u4f53\u768453,274\u4e2a\u5408\u6210\u53e5\u5b50\u3002\u901a\u8fc7\u56db\u9636\u6bb5\u7684\u5408\u6210\u8fc7\u7a0b\uff0c\u8bed\u6599\u5e93\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u8461\u8404\u7259\u8bed\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u7684\u4ec7\u6068\u8a00\u8bba\u5206\u7c7b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e5d\u7c7b\u53d7\u5230\u6cd5\u5f8b\u4fdd\u62a4\u7684\u5c11\u6570\u7fa4\u4f53\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u56db\u9636\u6bb5\u7684\u7ba1\u9053\u6765\u521b\u5efaToxSyn-PT\u8bed\u6599\u5e93\uff0c\u5305\u62ec\u624b\u52a8\u7b56\u5c55\u7684\u79cd\u5b50\u96c6\u3001\u5c11\u91cf\u6837\u672c\u6269\u5c55\u3001\u57fa\u4e8e\u540c\u4e49\u53e5\u7684\u6269\u5145\u4ee5\u53ca\u52a0\u5165\u989d\u5916\u7684\u4e2d\u6027\u6587\u672c\u4ee5\u9632\u6b62\u6a21\u578b\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u7fa4\u4f53\u7684\u6807\u5fd7\u6027\u8bed\u8a00\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8bed\u6599\u5e93\u5728\u4e8c\u5143\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u826f\u597d\u8868\u73b0\uff0c\u5e76\u5728\u4e94\u79cd\u516c\u5f00\u7684\u8461\u8404\u7259\u8bed\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u9886\u57df\u8fb9\u754c\u4e5f\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u8bed\u6599\u5e93\u4e30\u5bcc\u4e86\u5408\u6210\u6570\u636e\u7684\u7814\u7a76\uff0c\u5e76\u6709\u52a9\u4e8e\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u63d0\u9ad8\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2506.10178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10178", "abs": "https://arxiv.org/abs/2506.10178", "authors": ["Bill Psomas", "Dionysis Christopoulos", "Eirini Baltzi", "Ioannis Kakogeorgiou", "Tilemachos Aravanis", "Nikos Komodakis", "Konstantinos Karantzalos", "Yannis Avrithis", "Giorgos Tolias"], "title": "Attention, Please! Revisiting Attentive Probing for Masked Image Modeling", "comment": null, "summary": "As fine-tuning (FT) becomes increasingly impractical at scale, probing is\nemerging as the preferred evaluation protocol for self-supervised learning\n(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the\npotential of models trained with Masked Image Modeling (MIM), due to the\ndistributed nature of patch tokens. This motivates the need for attentive\nprobing, an alternative that uses attention to selectively aggregate\npatch-level features. Despite its growing adoption, attentive probing remains\nunder-explored, with existing methods suffering from excessive parameterization\nand poor computational efficiency.\n  In this work, we revisit attentive probing through the lens of the\naccuracy-efficiency trade-off. We conduct a systematic study of existing\nmethods, analyzing their mechanisms and benchmarking their performance. We\nintroduce efficient probing (EP), a multi-query cross-attention mechanism that\neliminates redundant projections, reduces the number of trainable parameters,\nand achieves up to a 10$\\times$ speed-up over conventional multi-head\nattention. Despite its simplicity, EP outperforms LP and prior attentive\nprobing approaches across seven benchmarks, generalizes well beyond MIM to\ndiverse pre-training paradigms, produces interpretable attention maps, and\nachieves strong gains in low-shot and layer-wise settings. Code available at\nhttps://github.com/billpsomas/efficient-probing.", "AI": {"tldr": "This paper introduces efficient probing (EP) as an improved probing method compared to linear probing and existing attentive probing methods.", "motivation": "existing linear and attentive probing methods have limitations in reflecting the potential of models trained with Masked Image Modeling, suffering from excessive parameterization and poor computational efficiency", "method": "efficient probing (EP), a multi-query cross-attention mechanism that eliminates redundant projections, reduces the number of trainable parameters", "result": "EP achieves up to a 10x speed-up over conventional multi-head attention and outperforms LP and prior attentive probing approaches across seven benchmarks, generalizes well beyond MIM", "conclusion": "efficient probing not only improves computational efficiency but also achieves strong performance improvements in various settings, demonstrating its potential as a better probing method"}}
{"id": "2506.10268", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10268", "abs": "https://arxiv.org/abs/2506.10268", "authors": ["Andrea Yaoyun Cui", "Pengfei Yu"], "title": "Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models", "comment": null, "summary": "Language models are essentially probability distributions over token\nsequences. Auto-regressive models generate sentences by iteratively computing\nand sampling from the distribution of the next token. This iterative sampling\nintroduces stochasticity, leading to the assumption that language models make\nprobabilistic decisions, similar to sampling from unknown distributions.\nBuilding on this assumption, prior research has used simulated Gibbs sampling,\ninspired by experiments designed to elicit human priors, to infer the priors of\nlanguage models. In this paper, we revisit a critical question: Do language\nmodels possess Bayesian brains? Our findings show that under certain\nconditions, language models can exhibit near-deterministic decision-making,\nsuch as producing maximum likelihood estimations, even with a non-zero sampling\ntemperature. This challenges the sampling assumption and undermines previous\nmethods for eliciting human-like priors. Furthermore, we demonstrate that\nwithout proper scrutiny, a system with deterministic behavior undergoing\nsimulated Gibbs sampling can converge to a \"false prior.\" To address this, we\npropose a straightforward approach to distinguish between stochastic and\ndeterministic decision patterns in Gibbs sampling, helping to prevent the\ninference of misleading language model priors. We experiment on a variety of\nlarge language models to identify their decision patterns under various\ncircumstances. Our results provide key insights in understanding decision\nmaking of large language models.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u5c55\u73b0\u51fa\u8fd1\u4f3c\u786e\u5b9a\u6027\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u8fd9\u6311\u6218\u4e86\u5b83\u4eec\u9075\u5faa\u6982\u7387\u51b3\u7b56\u7684\u4f20\u7edf\u5047\u8bbe\uff0c\u5e76\u5f71\u54cd\u4e86\u4e4b\u524d\u57fa\u4e8e\u6a21\u62dfGibbs\u91c7\u6837\u63a8\u5bfc\u6a21\u578b\u504f\u597d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u56de\u987e\u4e86\u201c\u8bed\u8a00\u6a21\u578b\u662f\u5426\u62e5\u6709\u8d1d\u53f6\u65af\u5927\u8111\u201d\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\uff0c\u56e0\u4e3a\u8fd9\u4e00\u95ee\u9898\u5bf9\u5efa\u7acb\u51c6\u786e\u7684\u6a21\u578b\u5185\u90e8\u51b3\u7b56\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6a21\u62dfGibbs\u91c7\u6837\u6765\u5206\u6790\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u8d1d\u53f6\u65af\u51b3\u7b56\u673a\u5236\uff0c\u5373\u5b83\u4eec\u5728\u751f\u6210\u5e8f\u5217\u65f6\u662f\u5426\u5982\u91c7\u6837\u672a\u77e5\u5206\u5e03\u4e00\u6837\u505a\u51fa\u6982\u7387\u6027\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8868\u73b0\u51fa\u51e0\u4e4e\u786e\u5b9a\u6027\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u5373\u4fbf\u91c7\u6837\u6e29\u5ea6\u4e0d\u4e3a\u96f6\uff0c\u8fd9\u6311\u6218\u4e86\u4e4b\u524d\u7684\u91c7\u6837\u5047\u8bbe\uff0c\u63ed\u793a\u4e86\u4e4b\u524d\u7684\u6a21\u62df\u4eba\u7c7b\u504f\u597d\u7684\u65b9\u6cd5\u53ef\u80fd\u5931\u6548\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u6765\u533a\u5206Gibbs\u91c7\u6837\u4e2d\u7684\u968f\u673a\u4e0e\u786e\u5b9a\u6027\u51b3\u7b56\u6a21\u5f0f\uff0c\u6709\u52a9\u4e8e\u9632\u6b62\u8bef\u5224\u8bed\u8a00\u6a21\u578b\u7684\u504f\u597d\uff0c\u5e76\u4e3a\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u65b9\u5f0f\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2506.10182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10182", "abs": "https://arxiv.org/abs/2506.10182", "authors": ["Fiona Ryan", "Josef Sivic", "Fabian Caba Heilbron", "Judy Hoffman", "James M. Rehg", "Bryan Russell"], "title": "Improving Personalized Search with Regularized Low-Rank Parameter Updates", "comment": "CVPR 2025 Highlight. Code: http://github.com/adobe-research/polar-vl", "summary": "Personalized vision-language retrieval seeks to recognize new concepts (e.g.\n\"my dog Fido\") from only a few examples. This task is challenging because it\nrequires not only learning a new concept from a few images, but also\nintegrating the personal and general knowledge together to recognize the\nconcept in different contexts. In this paper, we show how to effectively adapt\nthe internal representation of a vision-language dual encoder model for\npersonalized vision-language retrieval. We find that regularized low-rank\nadaption of a small set of parameters in the language encoder's final layer\nserves as a highly effective alternative to textual inversion for recognizing\nthe personal concept while preserving general knowledge. Additionally, we\nexplore strategies for combining parameters of multiple learned personal\nconcepts, finding that parameter addition is effective. To evaluate how well\ngeneral knowledge is preserved in a finetuned representation, we introduce a\nmetric that measures image retrieval accuracy based on captions generated by a\nvision language model (VLM). Our approach achieves state-of-the-art accuracy on\ntwo benchmarks for personalized image retrieval with natural language queries -\nDeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal\nretrievals.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u4e2a\u6027\u5316\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u4f4e\u79e9\u9002\u5e94\u5c11\u91cf\u8bed\u8a00\u7f16\u7801\u5668\u53c2\u6570\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u8bc6\u522b\u4e2a\u4eba\u6982\u5ff5\u5e76\u4fdd\u6301\u901a\u7528\u77e5\u8bc6\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4e2a\u6027\u5316\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\u7684\u6311\u6218\uff0c\u8be5\u4efb\u52a1\u4e0d\u4ec5\u8981\u6c42\u4ece\u5c11\u91cf\u6837\u4f8b\u4e2d\u5b66\u4e60\u65b0\u7684\u6982\u5ff5\uff0c\u8fd8\u9700\u8981\u6574\u5408\u4e2a\u4eba\u548c\u901a\u7528\u77e5\u8bc6\u4ee5\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u8bc6\u522b\u8fd9\u4e9b\u6982\u5ff5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u4f4e\u79e9\u9002\u5e94\u6765\u8c03\u6574\u89c6\u89c9-\u8bed\u8a00\u53cc\u7f16\u7801\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\u3002\u5177\u4f53\u800c\u8a00\uff0c\u53ea\u9700\u8981\u8c03\u8282\u8bed\u8a00\u7f16\u7801\u5668\u6700\u7ec8\u5c42\u7684\u4e00\u5c0f\u90e8\u5206\u53c2\u6570\u5373\u53ef\u8bc6\u522b\u4e2a\u4eba\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u5982\u4f55\u7ed3\u5408\u591a\u4e2a\u5b66\u4e60\u5230\u7684\u4e2a\u4eba\u6982\u5ff5\u7684\u53c2\u6570\uff0c\u53d1\u73b0\u53c2\u6570\u76f8\u52a0\u662f\u4e00\u4e2a\u6709\u6548\u7684\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5373\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5b57\u5e55\u7684\u56fe\u50cf\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5DeepFashion2\u548cConCon-Chi\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e864%-22%\u7684\u4e2a\u6027\u5316\u68c0\u7d22\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u8bc1\u5b9e\u4e86\u901a\u8fc7\u6b63\u5219\u5316\u4f4e\u79e9\u9002\u5e94\u5c11\u91cf\u53c2\u6570\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u9002\u5e94\u89c6\u89c9-\u8bed\u8a00\u53cc\u7f16\u7801\u6a21\u578b\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u89c6\u89c9-\u8bed\u8a00\u68c0\u7d22\uff0c\u5e76\u4e14\u80fd\u591f\u8f83\u4f18\u5730\u4fdd\u6301\u901a\u7528\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u53c2\u6570\u76f8\u52a0\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u7ed3\u5408\u591a\u4e2a\u4e2a\u4eba\u6982\u5ff5\u7684\u53c2\u6570\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u68c0\u7d22\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2506.10288", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10288", "abs": "https://arxiv.org/abs/2506.10288", "authors": ["Zige Wang", "Qi Zhu", "Fei Mi", "Minghui Xu", "Ruochun Jin", "Wenjing Yang"], "title": "ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs", "comment": null, "summary": "Gradient-based data influence approximation has been leveraged to select\nuseful data samples in the supervised fine-tuning of large language models.\nHowever, the computation of gradients throughout the fine-tuning process\nrequires too many resources to be feasible in practice. In this paper, we\npropose an efficient gradient-based data selection framework with clustering\nand a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition\nthat data samples with similar gradient features will have similar influences,\nwe first perform clustering on the training data pool. Then, we frame the\ninter-cluster data selection as a constrained computing budget allocation\nproblem and consider it a multi-armed bandit problem. A modified UCB algorithm\nis leveraged to solve this problem. Specifically, during the iterative sampling\nprocess, historical data influence information is recorded to directly estimate\nthe distributions of each cluster, and a cold start is adopted to balance\nexploration and exploitation. Experimental results on various benchmarks show\nthat our proposed framework, ClusterUCB, can achieve comparable results to the\noriginal gradient-based data selection methods while greatly reducing computing\nconsumption.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u805a\u7c7b\u548c\u6539\u8fdb\u7684\u4e0a\u7f6e\u4fe1\u754c\u7b97\u6cd5\u7684\u9ad8\u6548\u57fa\u4e8e\u68af\u5ea6\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6ClusterUCB\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0c\u7ed3\u679c\u4e0e\u539f\u59cb\u68af\u5ea6\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u539f\u6709\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u592a\u9ad8\uff0c\u4e0d\u53ef\u884c\u3002", "method": "\u9996\u5148\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u805a\u7c7b\uff0c\u7136\u540e\u91c7\u7528\u6539\u8fdb\u7684\u4e0a\u7f6e\u4fe1\u754c\u7b97\u6cd5\u89e3\u51b3\u6570\u636e\u9009\u62e9\u95ee\u9898\uff0c\u5229\u7528\u5386\u53f2\u6570\u636e\u5f71\u54cd\u4fe1\u606f\u6765\u4f30\u8ba1\u6bcf\u4e2a\u7c07\u7684\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cClusterUCB\u6846\u67b6\u5728\u964d\u4f4e\u8ba1\u7b97\u6d88\u8017\u7684\u540c\u65f6\uff0c\u53ef\u4ee5\u83b7\u5f97\u4e0e\u539f\u59cb\u68af\u5ea6\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cClusterUCB\u6846\u67b6\u80fd\u591f\u5728\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u6570\u636e\u9009\u62e9\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.10226", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10226", "abs": "https://arxiv.org/abs/2506.10226", "authors": ["Parsa Rahimi", "Sebastien Marcel"], "title": "ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators", "comment": null, "summary": "In this paper, we propose ScoreMix, a novel yet simple data augmentation\nstrategy leveraging the score compositional properties of diffusion models to\nenhance discriminator performance, particularly under scenarios with limited\nlabeled data. By convexly mixing the scores from different class-conditioned\ntrajectories during diffusion sampling, we generate challenging synthetic\nsamples that significantly improve discriminative capabilities in all studied\nbenchmarks. We systematically investigate class-selection strategies for mixing\nand discover that greater performance gains arise when combining classes\ndistant in the discriminator's embedding space, rather than close in the\ngenerator's condition space. Moreover, we empirically show that, under standard\nmetrics, the correlation between the generator's learned condition space and\nthe discriminator's embedding space is minimal. Our approach achieves notable\nperformance improvements without extensive parameter searches, demonstrating\npractical advantages for training discriminative models while effectively\nmitigating problems regarding collections of large datasets. Paper website:\nhttps://parsa-ra.github.io/scoremix", "AI": {"tldr": "\u63d0\u51faScoreMix\uff0c\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u5f97\u5206\u7ec4\u5408\u5c5e\u6027\u63d0\u5347\u5206\u7c7b\u5668\u7684\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u7684\u60c5\u51b5\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u901a\u8fc7\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u5408\u6210\u6837\u672c\u6765\u589e\u5f3a\u5224\u522b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u7684\u95ee\u9898\u3002", "method": "ScoreMix\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u5206\u6570\u7ec4\u5408\u5c5e\u6027\u6765\u589e\u5f3a\u5206\u7c7b\u5668\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u901a\u8fc7\u5728\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u51f8\u6df7\u5408\u6765\u81ea\u4e0d\u540c\u7c7b\u522b\u7684\u8f68\u8ff9\u5f97\u5206\uff0c\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u5408\u6210\u6837\u672c\u4ee5\u63d0\u5347\u6240\u6709\u7814\u7a76\u57fa\u51c6\u4e2d\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9009\u62e9\u5728\u5206\u7c7b\u5668\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8ddd\u79bb\u8f83\u8fdc\uff08\u800c\u975e\u5728\u751f\u6210\u5668\u6761\u4ef6\u7a7a\u95f4\u4e2d\u9760\u8fd1\uff09\u7684\u7c7b\u522b\u8fdb\u884c\u6df7\u5408\u53ef\u4ee5\u5e26\u6765\u66f4\u5927\u7684\u6027\u80fd\u589e\u76ca\u3002\u6b64\u5916\uff0c\u5728\u6807\u51c6\u6307\u6807\u4e0b\uff0c\u751f\u6210\u5668\u5b66\u4e60\u5230\u7684\u6761\u4ef6\u7a7a\u95f4\u4e0e\u5206\u7c7b\u5668\u7684\u5d4c\u5165\u7a7a\u95f4\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u8fdb\u884c\u5927\u91cf\u53c2\u6570\u641c\u7d22\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u8bad\u7ec3\u5224\u522b\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u9645\u4f18\u52bf\uff0c\u5e76\u6709\u6548\u5730\u7f13\u89e3\u4e86\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u7684\u95ee\u9898\u3002"}}
{"id": "2506.10292", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10292", "abs": "https://arxiv.org/abs/2506.10292", "authors": ["Ali Almutairi", "Abdullah Alsuhaibani", "Shoaib Jameel", "Usman Naseem", "Gelareh Mohammadi", "Imran Razzak"], "title": "Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages", "comment": null, "summary": "Training deep learning networks with minimal supervision has gained\nsignificant research attention due to its potential to reduce reliance on\nextensive labelled data. While self-training methods have proven effective in\nsemi-supervised learning, they remain vulnerable to errors from noisy pseudo\nlabels. Moreover, most recent approaches to the few-label classification\nproblem are either designed for resource-rich languages such as English or\ninvolve complex cascading models that are prone to overfitting. To address the\npersistent challenge of few-label text classification in truly low-resource\nlinguistic contexts, where existing methods often struggle with noisy\npseudo-labels and domain adaptation, we propose Flick. Unlike prior methods\nthat rely on generic multi-cluster pseudo-labelling or complex cascading\narchitectures, Flick leverages the fundamental insight that distilling\nhigh-confidence pseudo-labels from a broader set of initial clusters can\ndramatically improve pseudo-label quality, particularly for linguistically\ndiverse, low-resource settings. Flick introduces a novel pseudo-label\nrefinement component, a departure from traditional pseudo-labelling strategies\nby identifying and leveraging top-performing pseudo-label clusters. This\ncomponent specifically learns to distil highly reliable pseudo-labels from an\ninitial broad set by focusing on single-cluster cohesion and leveraging an\nadaptive top-k selection mechanism. This targeted refinement process is crucial\nfor mitigating the propagation of errors inherent in low-resource data,\nallowing for robust fine-tuning of pre-trained language models with only a\nhandful of true labels. We demonstrate Flick's efficacy across 14 diverse\ndatasets, encompassing challenging low-resource languages such as Arabic, Urdu,\nand Setswana, alongside English, showcasing its superior performance and\nadaptability.", "AI": {"tldr": "Flick \u662f\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u5c11\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u4e2d\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\u5e76\u589e\u5f3a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u5c11\u6837\u672c\u6587\u672c\u5206\u7c7b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u771f\u6b63\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u566a\u97f3\u4f2a\u6807\u7b7e\u548c\u9886\u57df\u9002\u5e94\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e9b\u65b9\u9762\u901a\u5e38\u8868\u73b0\u4e0d\u4f73\u3002", "method": "Flick \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f2a\u6807\u7b7e\u7cbe\u70bc\u7ec4\u4ef6\uff0c\u901a\u8fc7\u4e13\u6ce8\u4e8e\u5355\u4e2a\u805a\u7c7b\u7684\u5185\u805a\u6027\u548c\u81ea\u9002\u5e94\u7684 top-k \u9009\u62e9\u673a\u5236\uff0c\u4ece\u521d\u59cb\u5e7f\u6cdb\u96c6\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u4e2d\u63d0\u53d6\u9ad8\u5ea6\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\uff0c\u4ee5\u6539\u8fdb\u4f20\u7edf\u4f2a\u6807\u7b7e\u7b56\u7565\u3002", "result": "Flick \u5728\u6db5\u76d6\u963f\u62c9\u4f2f\u8bed\u3001\u4e4c\u5c14\u90fd\u8bed\u3001\u585e\u8328\u74e6\u7eb3\u8bed\u7b49\u591a\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u4ee5\u53ca\u82f1\u8bed\u5728\u5185\u768414\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u5353\u8d8a\u7684\u6027\u80fd\u548c\u9002\u7528\u6027\u3002", "conclusion": "Flick \u901a\u8fc7\u7cbe\u70bc\u4f2a\u6807\u7b7e\uff0c\u63d0\u5347\u4e86\u5c11\u6837\u672c\u6587\u672c\u5206\u7c7b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5b83\u5728\u591a\u79cd\u8bed\u8a00\u4e0a\u7684\u9002\u5e94\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.10228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10228", "abs": "https://arxiv.org/abs/2506.10228", "authors": ["Hamid Kamangir", "Mona Hajiesmaeeli", "Mason Earles"], "title": "California Crop Yield Benchmark: Combining Satellite Image, Climate, Evapotranspiration, and Soil Data Layers for County-Level Yield Forecasting of Over 70 Crops", "comment": null, "summary": "California is a global leader in agricultural production, contributing 12.5%\nof the United States total output and ranking as the fifth-largest food and\ncotton supplier in the world. Despite the availability of extensive historical\nyield data from the USDA National Agricultural Statistics Service, accurate and\ntimely crop yield forecasting remains a challenge due to the complex interplay\nof environmental, climatic, and soil-related factors. In this study, we\nintroduce a comprehensive crop yield benchmark dataset covering over 70 crops\nacross all California counties from 2008 to 2022. The benchmark integrates\ndiverse data sources, including Landsat satellite imagery, daily climate\nrecords, monthly evapotranspiration, and high-resolution soil properties. To\neffectively learn from these heterogeneous inputs, we develop a multi-modal\ndeep learning model tailored for county-level, crop-specific yield forecasting.\nThe model employs stratified feature extraction and a timeseries encoder to\ncapture spatial and temporal dynamics during the growing season. Static inputs\nsuch as soil characteristics and crop identity inform long-term variability.\nOur approach achieves an overall R2 score of 0.76 across all crops of unseen\ntest dataset, highlighting strong predictive performance across California\ndiverse agricultural regions. This benchmark and modeling framework offer a\nvaluable foundation for advancing agricultural forecasting, climate adaptation,\nand precision farming. The full dataset and codebase are publicly available at\nour GitHub repository.", "AI": {"tldr": "\u672c\u6587\u4e3a\u52a0\u5dde\u5404\u53bf\u768470\u591a\u79cd\u4f5c\u7269\u7684\u4ea7\u91cf\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "\u5c3d\u7ba1\u6709\u4e30\u5bcc\u7684USDA\u519c\u4e1a\u7edf\u8ba1\u670d\u52a1\u7684\u5386\u53f2\u4ea7\u91cf\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u73af\u5883\u3001\u6c14\u5019\u548c\u571f\u58e4\u76f8\u5173\u56e0\u7d20\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u7cbe\u786e\u548c\u53ca\u65f6\u7684\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u65b0\u7684\u9884\u6d4b\u6a21\u578b\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u53bf\u7ea7\u522b\u3001\u4f5c\u7269\u7279\u5b9a\u4ea7\u91cf\u9884\u6d4b\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u8be5\u6a21\u578b\u91c7\u7528\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u6765\u6355\u6349\u751f\u957f\u5b63\u8282\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u52a8\u6001\u53d8\u5316\u3002\u9759\u6001\u8f93\u5165\u5982\u571f\u58e4\u7279\u6027\u548c\u4f5c\u7269\u79cd\u7c7b\u63d0\u4f9b\u957f\u671f\u53ef\u53d8\u6027\u4fe1\u606f\u3002", "result": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4f5c\u7269\u4e0a\u7684\u672a\u89c1\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e860.76\u7684\u6574\u4f53R2\u5f97\u5206\uff0c\u5c55\u793a\u4e86\u5728\u52a0\u5dde\u591a\u6837\u5316\u7684\u519c\u4e1a\u533a\u57df\u4e2d\u8f83\u5f3a\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u6240\u6784\u5efa\u7684\u6570\u636e\u96c6\u548c\u5efa\u6a21\u6846\u67b6\u4e3a\u63a8\u8fdb\u519c\u4e1a\u9884\u6d4b\u3001\u6c14\u5019\u9002\u5e94\u548c\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u57fa\u7840\u3002"}}
{"id": "2506.10297", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.10297", "abs": "https://arxiv.org/abs/2506.10297", "authors": ["Chuck Arvin"], "title": "\"Check My Work?\": Measuring Sycophancy in a Simulated Educational Context", "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "summary": "This study examines how user-provided suggestions affect Large Language\nModels (LLMs) in a simulated educational context, where sycophancy poses\nsignificant risks. Testing five different LLMs from the OpenAI GPT-4o and\nGPT-4.1 model classes across five experimental conditions, we show that\nresponse quality varies dramatically based on query framing. In cases where the\nstudent mentions an incorrect answer, the LLM correctness can degrade by as\nmuch as 15 percentage points, while mentioning the correct answer boosts\naccuracy by the same margin. Our results also show that this bias is stronger\nin smaller models, with an effect of up to 30% for the GPT-4.1-nano model,\nversus 8% for the GPT-4o model. Our analysis of how often LLMs \"flip\" their\nanswer, and an investigation into token level probabilities, confirm that the\nmodels are generally changing their answers to answer choices mentioned by\nstudents in line with the sycophancy hypothesis. This sycophantic behavior has\nimportant implications for educational equity, as LLMs may accelerate learning\nfor knowledgeable students while the same tools may reinforce misunderstanding\nfor less knowledgeable students. Our results highlight the need to better\nunderstand the mechanism, and ways to mitigate, such bias in the educational\ncontext.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u5728\u6a21\u62df\u6559\u80b2\u73af\u5883\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u7528\u6237\u5efa\u8bae\u7684\u54cd\u5e94\u5b58\u5728\u663e\u8457\u504f\u89c1\uff0c\u8fd9\u79cd\u504f\u89c1\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u4e2d\u5f71\u54cd\u66f4\u5927\uff0c\u5bf9\u6559\u80b2\u516c\u5e73\u6027\u9020\u6210\u6f5c\u5728\u5a01\u80c1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u8ba8LLMs\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u65f6\uff0c\u5982\u4f55\u907f\u514d\u7531\u4e8e\u987a\u4ece\u5b66\u751f\u610f\u89c1\u800c\u5f15\u8d77\u7684\u504f\u89c1\u98ce\u9669\uff0c\u8fd9\u79cd\u73b0\u8c61\u53ef\u80fd\u4f1a\u52a0\u5267\u6559\u80b2\u4e0d\u516c\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u6a21\u62df\u6559\u80b2\u73af\u5883\u4e2d\u6d4b\u8bd5\u6765\u81eaOpenAI\u7684GPT-4o\u548cGPT-4.1\u6a21\u578b\u7cfb\u5217\u7684\u4e94\u79cd\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5206\u6790\u7528\u6237\u63d0\u4f9b\u7684\u5efa\u8bae\u5982\u4f55\u5f71\u54cd\u8fd9\u4e9b\u6a21\u578b\u3002\u7814\u7a76\u8003\u8651\u4e86\u4e94\u79cd\u4e0d\u540c\u7684\u5b9e\u9a8c\u6761\u4ef6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u67e5\u8be2\u7684\u63aa\u8f9e\uff0cLLM\u7684\u54cd\u5e94\u8d28\u91cf\u5dee\u5f02\u663e\u8457\u3002\u9519\u8bef\u7b54\u6848\u7684\u63d0\u793a\u4f1a\u4f7f\u5176\u6b63\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe15%\uff0c\u800c\u6b63\u786e\u7b54\u6848\u7684\u63d0\u793a\u5219\u4f1a\u63d0\u5347\u540c\u7b49\u7684\u6b63\u786e\u7387\u3002\u8fd9\u79cd\u504f\u89c1\u5728\u8f83\u5c0f\u6a21\u578b\u4e2d\u66f4\u4e3a\u660e\u663e\uff0c\u4f8b\u5982GPT-4.1-nano\u6a21\u578b\u7684\u5f71\u54cd\u53ef\u8fbe30%\uff0c\u800cGPT-4o\u4ec5\u4e3a8%\u3002LLMs\u5f80\u5f80\u4f1a\u6539\u53d8\u7b54\u6848\u4ee5\u8fce\u5408\u5b66\u751f\u63d0\u5230\u7684\u7b54\u6848\u9009\u62e9\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5728\u6559\u80b2\u573a\u666f\u4e2d\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u5e76\u7f13\u89e3\u6b64\u7c7b\u504f\u89c1\uff0c\u5e76\u5f3a\u8c03LLMs\u53ef\u80fd\u5bf9\u77e5\u8bc6\u4e30\u5bcc\u7684\u5b66\u751f\u6709\u6240\u52a9\u76ca\uff0c\u800c\u5bf9\u77e5\u8bc6\u4e0d\u8db3\u7684\u5b66\u751f\u5219\u53ef\u80fd\u8fdb\u4e00\u6b65\u56fa\u5316\u8bef\u89e3\u3002"}}
{"id": "2506.10242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10242", "abs": "https://arxiv.org/abs/2506.10242", "authors": ["Rajeev Yasarla", "Shizhong Han", "Hong Cai", "Fatih Porikli"], "title": "DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos", "comment": "CVPR 2025 Workshop on Autonomous Driving", "summary": "Camera-based 3D object detection in Bird's Eye View (BEV) is one of the most\nimportant perception tasks in autonomous driving. Earlier methods rely on dense\nBEV features, which are costly to construct. More recent works explore sparse\nquery-based detection. However, they still require a large number of queries\nand can become expensive to run when more video frames are used. In this paper,\nwe propose DySS, a novel method that employs state-space learning and dynamic\nqueries. More specifically, DySS leverages a state-space model (SSM) to\nsequentially process the sampled features over time steps. In order to\nencourage the model to better capture the underlying motion and correspondence\ninformation, we introduce auxiliary tasks of future prediction and masked\nreconstruction to better train the SSM. The state of the SSM then provides an\ninformative yet efficient summarization of the scene. Based on the state-space\nlearned features, we dynamically update the queries via merge, remove, and\nsplit operations, which help maintain a useful, lean set of detection queries\nthroughout the network. Our proposed DySS achieves both superior detection\nperformance and efficient inference. Specifically, on the nuScenes test split,\nDySS achieves 65.31 NDS and 57.4 mAP, outperforming the latest state of the\nart. On the val split, DySS achieves 56.2 NDS and 46.2 mAP, as well as a\nreal-time inference speed of 33 FPS.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u7684DySS\u65b9\u6cd5\u91c7\u7528\u72b6\u6001\u7a7a\u95f4\u5b66\u4e60\u548c\u52a8\u6001\u67e5\u8be2\u6280\u672f\u8fdb\u884c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u76843D\u7269\u4f53\u68c0\u6d4b\uff0c\u65e2\u63d0\u9ad8\u4e86\u6027\u80fd\u53c8\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684BEV\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u7684BEV\u7279\u5f81\uff0c\u6784\u5efa\u6210\u672c\u8f83\u9ad8\u3002\u800c\u8fd1\u671f\u7684\u65b9\u6cd5\u867d\u7136\u63a2\u7d22\u7a00\u758f\u67e5\u8be2\u8fdb\u884c\u4e86\u68c0\u6d4b\uff0c\u4f46\u4ecd\u7136\u9700\u8981\u5927\u91cf\u7684\u67e5\u8be2\uff0c\u8fd9\u5728\u5904\u7406\u66f4\u591a\u7684\u89c6\u9891\u5e27\u65f6\u4f1a\u53d8\u5f97\u6602\u8d35\u3002", "method": "DySS\u91c7\u7528\u72b6\u6001\u7a7a\u95f4\u5b66\u4e60\u548c\u52a8\u6001\u67e5\u8be2\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u6765\u5904\u7406\u4e0d\u540c\u65f6\u95f4\u6b65\u7684\u91c7\u6837\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u4e86\u672a\u6765\u9884\u6d4b\u548c\u906e\u7f69\u91cd\u5efa\u7684\u8f85\u52a9\u4efb\u52a1\u6765\u8bad\u7ec3SSM\uff0c\u4f7f\u5f97SSM\u7684\u72b6\u6001\u80fd\u66f4\u6709\u6548\u548c\u9ad8\u6548\u5730\u603b\u7ed3\u573a\u666f\u4fe1\u606f\u3002\u52a8\u6001\u67e5\u8be2\u901a\u8fc7\u5408\u4e95\u3001\u79fb\u9664\u548c\u5206\u5272\u64cd\u4f5c\u6765\u66f4\u65b0\uff0c\u4ee5\u4fdd\u6301\u6574\u4e2a\u7f51\u7edc\u4e2d\u6709\u7528\u4e14\u7cbe\u7b80\u7684\u68c0\u6d4b\u67e5\u8be2\u96c6\u5408\u3002", "result": "\u5728nuScenes\u6d4b\u8bd5\u5206\u5272\u4e0a\uff0cDySS\u5b9e\u73b0\u4e8665.31\u7684NDS\u548c57.4\u7684mAP\uff0c\u8d85\u8fc7\u4e86\u6700\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002\u5728\u9a8c\u8bc1\u5206\u5272\u4e0a\uff0cDySS\u5b9e\u73b0\u4e8656.2\u7684NDS\u548c46.2\u7684mAP\uff0c\u5e76\u4e14\u53ef\u4ee5\u8fbe\u523033FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "DySS\u5728\u4fdd\u6301\u4f18\u8d8a\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u9002\u5408\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5feb\u901f\u51c6\u786e3D\u7269\u4f53\u68c0\u6d4b\u3002"}}
{"id": "2506.10299", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.10299", "abs": "https://arxiv.org/abs/2506.10299", "authors": ["Hayato Futami", "Emiru Tsunoo", "Yosuke Kashiwagi", "Yuki Ito", "Hassan Shahmohammadi", "Siddhant Arora", "Shinji Watanabe"], "title": "Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs", "comment": "Accepted to Interspeech2025", "summary": "Speech-to-speech translation (S2ST) has been advanced with large language\nmodels (LLMs), which are fine-tuned on discrete speech units. In such\napproaches, modality adaptation from text to speech has been an issue. LLMs are\ntrained on text-only data, which presents challenges to adapt them to speech\nmodality with limited speech-to-speech data. To address the training\ndifficulty, we propose scheduled interleaved speech--text training in this\nstudy. We use interleaved speech--text units instead of speech units during\ntraining, where aligned text tokens are interleaved at the word level. We\ngradually decrease the ratio of text as training progresses, to facilitate\nprogressive modality adaptation from text to speech. We conduct experimental\nevaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show\nthat the proposed method consistently improves the translation performances,\nespecially for languages with limited training data.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.10286", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.10286", "abs": "https://arxiv.org/abs/2506.10286", "authors": ["Eunkyu Park", "Minyeong Kim", "Gunhee Kim"], "title": "HalLoc: Token-level Localization of Hallucinations for Vision Language Models", "comment": "CVPR 2025", "summary": "Hallucinations pose a significant challenge to the reliability of large\nvision-language models, making their detection essential for ensuring accuracy\nin critical applications. Current detection methods often rely on\ncomputationally intensive models, leading to high latency and resource demands.\nTheir definitive outcomes also fail to account for real-world scenarios where\nthe line between hallucinated and truthful information is unclear. To address\nthese issues, we propose HalLoc, a dataset designed for efficient,\nprobabilistic hallucination detection. It features 150K token-level annotated\nsamples, including hallucination types, across Visual Question Answering (VQA),\ninstruction-following, and image captioning tasks. This dataset facilitates the\ndevelopment of models that detect hallucinations with graded confidence,\nenabling more informed user interactions. Additionally, we introduce a baseline\nmodel trained on HalLoc, offering low-overhead, concurrent hallucination\ndetection during generation. The model can be seamlessly integrated into\nexisting VLMs, improving reliability while preserving efficiency. The prospect\nof a robust plug-and-play hallucination detection module opens new avenues for\nenhancing the trustworthiness of vision-language models in real-world\napplications. The HalLoc dataset and code are publicly available at:\nhttps://github.com/dbsltm/cvpr25_halloc.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.10343", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10343", "abs": "https://arxiv.org/abs/2506.10343", "authors": ["Dongwon Jung", "Wenxuan Zhou", "Muhao Chen"], "title": "Code Execution as Grounded Supervision for LLM Reasoning", "comment": null, "summary": "Training large language models (LLMs) with chain-of-thought (CoT) supervision\nhas proven effective for enhancing their reasoning abilities. However,\nobtaining reliable and accurate reasoning supervision remains a significant\nchallenge. We propose a scalable method for generating a high-quality CoT\nsupervision dataset by leveraging the determinism of program execution. Unlike\nexisting reasoning dataset generation methods that rely on costly human\nannotations or error-prone LLM-generated CoT, our approach extracts verifiable,\nstep-by-step reasoning traces from code execution and transforms them into a\nnatural language CoT reasoning. Experiments on reasoning benchmarks across\nvarious domains show that our method effectively equips LLMs with transferable\nreasoning abilities across diverse tasks. Furthermore, the ablation studies\nvalidate that our method produces highly accurate reasoning data and reduces\noverall token length during inference by reducing meaningless repetition and\noverthinking.", "AI": {"tldr": "This paper introduces a scalable approach to generate high-quality chain-of-thought (CoT) data for enhancing large language models' (LLMs) reasoning capabilities by using accurate, step-by-step reasoning traces extracted from program execution, reducing the need for costly human annotations or unreliable LLM-generated CoT.", "motivation": "The motivation behind this method is to address the challenge of acquiring reliable and accurate CoT supervision data. Current methods, which primarily depend on human annotations or LLM-generated CoT, are either costly or prone to errors. This scalable method seeks to offer a more reliable and efficient solution.", "method": "We propose a scalable method that generates high-quality chain-of-thought (CoT) supervision data for large language models (LLMs) by using the determinism of program execution. This approach extracts verifiable reasoning traces from code and converts them into natural language CoT, aiming to enhance the LLMs' reasoning abilities.", "result": "Experiments on a variety of reasoning benchmarks across different domains demonstrated that this method significantly improves LLMs' transferable reasoning abilities. Additionally, ablation studies confirmed the high accuracy of the reasoning data generated, and the method also helps in reducing meaningless repetition and overthinking.", "conclusion": "The proposed method presents an innovative approach to generating reliable CoT supervision data for LLMs, leveraging the precise nature of program execution. It not only improves LLMs' ability to reason effectively across various tasks but also results in data that reduces token length and unnecessary repetition, enhancing efficiency."}}
{"id": "2506.10302", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.10302", "abs": "https://arxiv.org/abs/2506.10302", "authors": ["Hamzeh Asgharnezhad", "Pegah Tabarisaadi", "Abbas Khosravi", "Roohallah Alizadehsani", "U. Rajendra Acharya"], "title": "Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation", "comment": null, "summary": "Accurate and reliable skin cancer diagnosis is critical for early treatment\nand improved patient outcomes. Deep learning (DL) models have shown promise in\nautomating skin cancer classification, but their performance can be limited by\ndata scarcity and a lack of uncertainty awareness. In this study, we present a\ncomprehensive evaluation of DL-based skin lesion classification using transfer\nlearning and uncertainty quantification (UQ) on the HAM10000 dataset. In the\nfirst phase, we benchmarked several pre-trained feature extractors-including\nContrastive Language-Image Pretraining (CLIP) variants, Residual Network-50\n(ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual\nGeometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range\nof traditional classifiers such as Support Vector Machine (SVM), eXtreme\nGradient Boosting (XGBoost), and logistic regression. Our results show that\nCLIP-based vision transformers, particularly LAION CLIP ViT-H/14 with SVM,\ndeliver the highest classification performance. In the second phase, we\nincorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte\nCarlo Dropout (EMCD) to assess not only prediction accuracy but also the\nreliability of model outputs. We evaluated these models using uncertainty-aware\nmetrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen),\nuncertainty specificity(USpe), and uncertainty precision(UPre). The results\ndemonstrate that ensemble methods offer a good trade-off between accuracy and\nuncertainty handling, while EMCD is more sensitive to uncertain predictions.\nThis study highlights the importance of integrating UQ into DL-based medical\ndiagnosis to enhance both performance and trustworthiness in real-world\nclinical applications.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8f6c\u79fb\u5b66\u4e60\u53ca\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u8bc4\u4f30\u4e86\u51e0\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728HAM10000\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793aCLIP\u642d\u914dSVM\u8868\u73b0\u6700\u4f73\uff0c\u800c\u96c6\u6210\u8499\u7279\u5361\u6d1b\u4e22\u5f03\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63d0\u5347\u76ae\u80a4\u764c\u8bca\u65ad\u7684\u7cbe\u51c6\u6027\u548c\u53ef\u9760\u6027\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u81ea\u52a8\u5316\u76ae\u80a4\u764c\u5206\u7c7b\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u53ca\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u95ee\u9898\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u8bc4\u4f30\u4e86\u51e0\u79cd\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u7ed3\u5408\u4f20\u7edf\u5206\u7c7b\u5668\u7684\u6027\u80fd\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u4e22\u5f03\u6cd5\u3001\u96c6\u6210\u6cd5\u53ca\u96c6\u6210\u8499\u7279\u5361\u6d1b\u4e22\u5f03\u6cd5\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "CLIP\u642d\u914dSVM\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\uff1b\u96c6\u6210\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u4e0a\u627e\u5230\u826f\u597d\u5e73\u8861\uff0c\u96c6\u6210\u8499\u7279\u5361\u6d1b\u4e22\u5f03\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u4e0a\u66f4\u654f\u611f\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u7684\u533b\u5b66\u8bca\u65ad\u4e2d\u53ef\u4ee5\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u5e94\u7528\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.10380", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.10380", "abs": "https://arxiv.org/abs/2506.10380", "authors": ["Xiaohan Yu", "Pu Jian", "Chong Chen"], "title": "TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning", "comment": "Under review. Codes are available at\n  https://github.com/yxh-y/TableRAG/tree/main", "summary": "Retrieval-Augmented Generation (RAG) has demonstrated considerable\neffectiveness in open-domain question answering. However, when applied to\nheterogeneous documents, comprising both textual and tabular components,\nexisting RAG approaches exhibit critical limitations. The prevailing practice\nof flattening tables and chunking strategies disrupts the intrinsic tabular\nstructure, leads to information loss, and undermines the reasoning capabilities\nof LLMs in multi-hop, global queries. To address these challenges, we propose\nTableRAG, an hybrid framework that unifies textual understanding and complex\nmanipulations over tabular data. TableRAG iteratively operates in four steps:\ncontext-sensitive query decomposition, text retrieval, SQL programming and\nexecution, and compositional intermediate answer generation. We also develop\nHeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous\nreasoning capabilities. Experimental results demonstrate that TableRAG\nconsistently outperforms existing baselines on both public datasets and our\nHeteQA, establishing a new state-of-the-art for heterogeneous document question\nanswering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6TableRAG\uff0c\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406\u5305\u542b\u6587\u672c\u548c\u8868\u683c\u7ec4\u4ef6\u7684\u5f02\u6784\u6587\u6863\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709RAG\u65b9\u6cd5\u5728\u5904\u7406\u6b64\u7c7b\u6587\u6863\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u65b0\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eRAG\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u6587\u6863\u65f6\u8868\u73b0\u51fa\u4e86\u5173\u952e\u7684\u9650\u5236\uff0c\u5373\u5c55\u5e73\u8868\u683c\u548c\u5206\u6bb5\u7b56\u7565\u4f1a\u7834\u574f\u8868\u683c\u7684\u5185\u5728\u7ed3\u6784\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u5e76\u635f\u5bb3\u591a\u8df3\u3001\u5168\u5c40\u67e5\u8be2\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u8be5\u6846\u67b6\u5206\u4e3a\u56db\u4e2a\u6b65\u9aa4\u64cd\u4f5c\uff1a\u67e5\u8be2\u7684\u4e0a\u4e0b\u6587\u654f\u611f\u5206\u89e3\u3001\u6587\u672c\u68c0\u7d22\u3001SQL\u7f16\u7a0b\u548c\u6267\u884c\uff0c\u4ee5\u53ca\u7ec4\u5408\u4e2d\u95f4\u7b54\u6848\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTableRAG\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u65b0\u5f00\u53d1\u7684HeteQA\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u786e\u7acb\u4e86\u5f02\u6784\u6587\u6863\u95ee\u7b54\u7684\u65b0\u6807\u51c6\u3002", "conclusion": "\u901a\u8fc7TableRAG\u7684\u5f00\u53d1\u548c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u4f5c\u8005\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u5305\u62ec\u6587\u672c\u548c\u8868\u683c\u7ec4\u4ef6\u7684\u5f02\u6784\u6587\u6863\u65f6\u7684\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u8df3\u5f02\u6784\u63a8\u7406\u65b9\u9762\u3002"}}
{"id": "2506.10328", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.10328", "abs": "https://arxiv.org/abs/2506.10328", "authors": ["Sadia Kamal", "Tim Oates", "Joy Wan"], "title": "Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework", "comment": "Accepted at IEEE/CVF Computer Society Conference on Computer Vision\n  and Pattern Recognition Workshops (CVPRW)", "summary": "Skin carcinoma is the most prevalent form of cancer globally, accounting for\nover $8 billion in annual healthcare expenditures. In clinical settings,\nphysicians document patient visits using detailed SOAP (Subjective, Objective,\nAssessment, and Plan) notes. However, manually generating these notes is\nlabor-intensive and contributes to clinician burnout. In this work, we propose\na weakly supervised multimodal framework to generate clinically structured SOAP\nnotes from limited inputs, including lesion images and sparse clinical text.\nOur approach reduces reliance on manual annotations, enabling scalable,\nclinically grounded documentation while alleviating clinician burden and\nreducing the need for large annotated data. Our method achieves performance\ncomparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical\nrelevance metrics. To evaluate clinical quality, we introduce two novel metrics\nMedConceptEval and Clinical Coherence Score (CCS) which assess semantic\nalignment with expert medical concepts and input features, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f31\u76d1\u7763\u5b66\u4e60\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u4ee5\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u7684SOAP\u7b14\u8bb0\uff0c\u4f7f\u7528\u75c5\u53d8\u56fe\u50cf\u548c\u90e8\u5206\u4e34\u5e8a\u6587\u672c\u4f5c\u4e3a\u8f93\u5165\u3002\u8be5\u65b9\u6cd5\u5728\u5173\u952e\u4e34\u5e8a\u6307\u6807\u4e0a\u4e0e\u5148\u8fdb\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u9274\u4e8e\u76ae\u80a4\u764c\u662f\u5168\u7403\u6700\u5e38\u89c1\u7684\u764c\u75c7\u5f62\u5f0f\u4e4b\u4e00\uff0c\u4e34\u5e8a\u533b\u751f\u5728\u8bb0\u5f55\u60a3\u8005\u8bbf\u95ee\u65f6\u624b\u5de5\u7f16\u5199SOAP\u7b14\u8bb0\u65e2\u8017\u65f6\u53c8\u5bb9\u6613\u5bfc\u81f4\u533b\u751f\u5026\u6020\u3002\u672c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u964d\u4f4e\u8fd9\u79cd\u8d1f\u62c5\uff0c\u540c\u65f6\u63d0\u9ad8\u6587\u6863\u5316\u8fc7\u7a0b\u7684\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f31\u76d1\u7763\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u6709\u9650\u7684\u8f93\u5165\uff08\u5982\u75c5\u53d8\u56fe\u50cf\u548c\u7a00\u758f\u7684\u4e34\u5e8a\u6587\u672c\uff09\u6765\u751f\u6210\u7ed3\u6784\u5316\u7684SOAP\u7b14\u8bb0\u3002\u8fd9\u79cd\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u57fa\u4e8e\u4e34\u5e8a\u7684\u6587\u6863\u8bb0\u5f55\uff0c\u5e76\u51cf\u8f7b\u4e86\u4e34\u5e8a\u533b\u751f\u7684\u8d1f\u62c5\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u7684\u9700\u6c42\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5173\u952e\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u4e0eGPT-4o\u3001Claude\u548cDeepSeek Janus Pro\u76f8\u5f53\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u8bc4\u4f30\u4e34\u5e8a\u8d28\u91cf\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u6307\u6807\uff1aMedConceptEval\u548c\u4e34\u5e8a\u8fde\u8d2f\u6027\u5f97\u5206(CCS)\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0e\u4e13\u5bb6\u533b\u7597\u6982\u5ff5\u548c\u8f93\u5165\u7279\u5f81\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u6709\u52a9\u4e8e\u51cf\u8f7b\u4e34\u5e8a\u533b\u751f\u8d1f\u62c5\u5e76\u7f29\u51cf\u533b\u7597\u8bb0\u5f55\u6210\u672c\u7684\u6709\u6548\u5de5\u5177\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u5176\u5728\u751f\u6210\u7ed3\u6784\u5316\u7684SOAP\u7b14\u8bb0\u65b9\u9762\u5177\u6709\u53ef\u89c2\u7684\u6027\u80fd\u3002"}}
