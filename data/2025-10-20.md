<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments](https://arxiv.org/abs/2510.14992)
*Leela Krishna,Mengyang Zhao,Saicharithreddy Pasula,Harshit Rajgarhia,Abhishek Mukherji*

Main category: cs.CV

> GAZE pipeline for automating video annotation for world-model training, increasing efficiency and dataset quality.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is the need for automated solutions for large-scale, precise annotation of multimodal datasets, essential for robust world models. This is due to the historically expensive and slow nature of manual annotation.

**Method:** The paper introduces the GAZE pipeline for converting raw, long-form video into structured, task-ready supervision for world-model training. It consists of three main stages: normalization and sharding of 360-degree videos, dense pre-annotation using a suite of AI models, and consolidation into a structured output for human validation.

**Result:** The GAZE workflow yields significant efficiency gains, saving about 19 minutes per review hour, and reduces human review volume by over 80%. The method enhances label density and consistency, incorporating privacy protections and metadata.

**Conclusion:** The paper concludes that the GAZE pipeline provides a scalable blueprint for generating high-fidelity, privacy-aware datasets for training world models, without sacrificing throughput or governance.

**Abstract:** Training robust world models requires large-scale, precisely labeled
multimodal datasets, a process historically bottlenecked by slow and expensive
manual annotation. We present a production-tested GAZE pipeline that automates
the conversion of raw, long-form video into rich, task-ready supervision for
world-model training. Our system (i) normalizes proprietary 360-degree formats
into standard views and shards them for parallel processing; (ii) applies a
suite of AI models (scene understanding, object tracking, audio transcription,
PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii)
consolidates signals into a structured output specification for rapid human
validation.
  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per
review hour) and reduces human review volume by >80% through conservative
auto-skipping of low-salience segments. By increasing label density and
consistency while integrating privacy safeguards and chain-of-custody metadata,
our method generates high-fidelity, privacy-aware datasets directly consumable
for learning cross-modal dynamics and action-conditioned prediction. We detail
our orchestration, model choices, and data dictionary to provide a scalable
blueprint for generating high-quality world model training data without
sacrificing throughput or governance.

</details>
