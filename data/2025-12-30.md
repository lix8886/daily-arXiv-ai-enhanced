<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA](https://arxiv.org/abs/2512.22208)
*Pu Zhao,Xuan Shen,Zhenglun Kong,Yixin Shen,Sung-En Chang,Arash Akbari,Timothy Rupprecht,Lei Lu,Enfu Nan,Changdi Yang,Yumei He,Weiyan Shi,Xingchen Xu,Yu Huang,Wei Jiang,Wei Wang,Yue Chen,Yong He,Yanzhi Wang*

Main category: cs.CL

> 论文介绍了Moxin 7B及其变体，这些模型在多项任务中表现出色，并且完全开源，促进了健康开源生态系统的建立。

<details>
  <summary>Details</summary>

**Motivation:** 为了推动更加包容和协作的研究环境，维持健康的开源生态系统，并赋予Moxin在不同任务下的多种能力。

**Method:** 该论文介绍了Moxin 7B，一个完全开源的大型语言模型，它遵循模型开放框架，除了共享模型权重外，还实现了训练、数据集和实现细节的完全透明化。此外，研究团队基于Moxin开发了三个变体，包括面向视觉语言任务的Moxin-VLM，面向视觉语言行动任务的Moxin-VLA，以及针对中文能力的Moxin-Chinese。

**Result:** 实验结果显示，该论文的模型在各种评估中都取得了优异的成绩。

**Conclusion:** 论文通过完全开放的数据、代码和模型细节，为研究社区提供了宝贵的资源，同时展示了Moxin及其变体在各种任务中的性能优势。

**Abstract:** Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.

</details>


### [2] [Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces](https://arxiv.org/abs/2512.22227)
*Sophie Zhao*

Main category: cs.CL

> 研究发现，变压器模型中的句子嵌入展现了与人类解释的认知属性相一致的层级结构。

<details>
  <summary>Details</summary>

**Motivation:** 探讨句子嵌入是否编码了与人类可解释的认知或心理属性相一致的分级层次结构。

**Method:** 构建了一个包含480个自然语言句子的数据集，并使用线性和浅层非线性探针来评估多个变压器模型中固定句子嵌入对这些注释的可恢复性。

**Result:** 结果表明，连续评分和等级标签在多个模型中都能可靠地被解码，浅层非线性探针比线性探针表现更好，提示符性能超过随机标签的零假设。

**Conclusion:** 这些结果表明变压器嵌入空间显示出与人为定义的认知属性相一致的分层几何组织。

**Abstract:** Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores and discrete tier labels spanning seven ordered cognitive categories. Using fixed sentence embeddings from multiple transformer models, we evaluate the recoverability of these annotations via linear and shallow nonlinear probes. Across models, both continuous scores and tier labels are reliably decodable, with shallow nonlinear probes providing consistent performance gains over linear probes. Lexical TF-IDF baselines perform substantially worse, indicating that the observed structure is not attributable to surface word statistics alone. Nonparametric permutation tests further confirm that probe performance exceeds chance under label-randomization nulls. Qualitative analyses using UMAP visualizations and confusion matrices reveal smooth low-to-high gradients and predominantly adjacent-tier confusions in embedding space. Taken together, these results provide evidence that transformer embedding spaces exhibit a hierarchical geometric organization aligned with human-defined cognitive attributes, while remaining agnostic to claims of internal awareness or phenomenology.

</details>


### [3] [SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents](https://arxiv.org/abs/2512.22322)
*Shaofei Cai,Yulei Qin,Haojia Lin,Zihan Xu,Gang Li,Yuchen Shi,Zongyi Li,Yong Mao,Siqi Cai,Xiaoyu Tan,Yitao Liang,Ke Li,Xing Sun*

Main category: cs.CL

> SmartSnap shifts agentic reinforcement learning task verification from post-hoc passive to proactive self-verification, leveraging minimal, decisive snapshots, leading to scalable and efficient agent training with performance boosts.

<details>
  <summary>Details</summary>

**Motivation:** To address the scalability issue of agentic reinforcement learning in complex GUI tasks, specifically the costly and unreliable task verification process.

**Method:** SmartSnap, a proactive, in-situ self-verification paradigm for reinforcement learning agents, introducing the Self-Verifying Agent that aims to complete tasks while providing relevant evidence.

**Result:** Experiments on mobile tasks show performance gains of up to 26.08% for 8B and 16.66% for 30B models, indicating the paradigm's effectiveness in training scalable and efficient LLM-driven agents.

**Conclusion:** The SmartSnap paradigm facilitates the development of efficient, self-verifying agents, enhancing the scalability of agentic reinforcement learning in complex environments.

**Abstract:** Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

> 研究了视频扩散模型中运动编码的时间步长特性，简化了运动定制过程，实现强有力且无需额外模块的运动转移。

<details>
  <summary>Details</summary>

**Motivation:** 研究视频扩散模型在不同时间步长上如何编码运动，特别是在早期时间步长主要塑造运动和布局而后期时间步长细化外观这一经验性假设。

**Method:** 通过在指定的时间步长范围内注入新的条件来代理视频扩散时间步长中的运动编码，从而衡量外观编辑和运动保持之间的权衡，以此大规模定量研究来表征这一代理。

**Result:** 在不同架构中一致地识别出早期的以运动为主导的阶段和后期的以外观为主导的阶段，确定了时间步长空间中的一个运动-外观边界。研究简化了目前的一次性运动定制范式，通过训练和推理限定在运动为主导阶段来实现强有力的运动转移。

**Conclusion:** 将一个广泛使用的经验法则转化为时空解耦原则，并提供了一个时间步约束的方案，该方案可以轻松地集成到现有的运动转移和编辑方法中。

**Abstract:** Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

</details>


### [5] [Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment](https://arxiv.org/abs/2512.22177)
*Dawnena Key*

Main category: cs.CV

> 本文提出了一种能够处理网络摄像头视频流、识别单词级别的ASL手势，且F1分数范围在0.71到0.99之间的实时ASL手势识别系统。

<details>
  <summary>Details</summary>

**Motivation:** 解决全球超过7000万聋哑人士的交流障碍，提供一种基于网络摄像头视频流的实时ASL手势识别系统。

**Method:** 该系统利用结合3D卷积神经网络（3D CNN）和长短期记忆（LSTM）网络的混合深度学习架构，捕捉视频帧中的时空特征，并通过LSTM层建模手势的序列依赖性。

**Result:** 该论文提出了一种利用结合3D卷积神经网络（3D CNN）和长短期记忆（LSTM）网络的混合深度学习架构的实时美式手语（ASL）识别系统。该系统能够处理网络摄像头视频流，识别单词级别的ASL手势，解决全球7000多万聋哑人士的交流障碍。该架构利用3D卷积捕捉视频帧中的时空特征，并通过LSTM层建模手势的序列依赖性。在WLASL数据集（2000个常见单词）、ASL-LEX词汇数据库（约2700个手势）和一组100个专家标注的ASL手势上训练，该系统在不同手势类别上的F1分数范围在0.71到0.99之间。该模型在AWS基础设施上部署，并且能够在OAK-D相机上实现边缘部署以进行实时推理。文中讨论了架构设计、训练方法、评估指标和实际应用的部署考虑。

**Conclusion:** 开发了一种基于混合深度学习架构的ASL识别系统，该系统在不同手势类别上实现了高F1分数，并在实际应用中具有潜在的实用价值。

**Abstract:** This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.

</details>


### [6] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

> The paper explores the use of an AI-enhanced Locally Linear Embedding model to improve medical billing and transcription, showcasing enhanced accuracy and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** To leverage AI and LLE techniques for the processing of high-dimensional medical data to reduce human error and streamline medical operations.

**Method:** Integration of AI with LLE to develop a new model aimed at optimizing medical billing and transcription.

**Result:** Experiments demonstrated a significant improvement in the accuracy and efficiency of processing medical data.

**Conclusion:** The study highlights the potential of AI-enhanced LLE in healthcare, particularly for data-rich areas such as billing and transcription, advocating for further research in this direction.

**Abstract:** The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

</details>


### [7] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

> VISTA是一个模块化框架，通过明确的信息瓶颈将感知与推理分离，以避免视觉语言模型在回答视觉问题时依赖于非因果的偶然关联。实验表明，VISTA显著提高了在真实世界模型中的鲁棒性，并且其推理过程更依赖于视觉证据，而非偶然属性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是解决视觉语言模型在处理视觉问题时过度依赖偶然关联，而非实际视觉证据的问题，特别是在进行微调之后这个问题更为明显。

**Method:** 方法是采用了一个冻结的感知器视觉语言模型，限制其对简短且客观的感知查询进行回应，并通过文本推理器分解问题，计划查询并聚合视觉事实。利用强化学习进行训练。

**Result:** 测试结果显示，与基线模型相比，VISTA在SpuriVerse数据集上显著提高了鲁棒性（分别提高了16.29%和6.77%），并且在MMVP和SeedBench数据集上也有不错的表现。

**Conclusion:** 结论是VISTA框架能够有效减少视觉语言模型对非因果偶然关联的依赖，提高模型在处理视觉任务时的鲁棒性，且在推理过程中更依赖于视觉事实，而非偶然属性。

**Abstract:** End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

</details>


### [8] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

> The paper presents SAMM2D, a dual-encoder framework which achieves superior aneurysm detection performance on an imbalanced medical dataset without data augmentation, contradicting the common belief that more augmentation always improves performance.

<details>
  <summary>Details</summary>

**Motivation:** To improve aneurysm detection performance which is critical to preventing life-threatening hemorrhages but is challenging due to subtle aneurysm morphology, class imbalance, and limited annotated data.

**Method:** Developed SAMM2D, a dual-encoder framework, which leverages a strong pretrained backbone without the need for data augmentation techniques to detect aneurysms more effectively.

**Result:** Achieved an AUC of 0.686, a 32% improvement over the clinical baseline. The model also demonstrated 95% sensitivity, outperforming radiologists on average, and included visualizations showing the model focuses on relevant vascular areas.

**Conclusion:** In low-data medical imaging, strong pretraining may be more beneficial than data augmentation. SAMM2D's unaugmented baseline model outperforms augmented variants, challenging the assumption that more augmentation leads to better performance.

**Abstract:** Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

</details>


### [9] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

> 提出了HookMIL，一种上下文感知和计算高效的多实例学习框架，通过可学习的hook token进行聚合，实现了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多实例学习方法往往丢失了关键的上下文信息，而基于变压器的变体虽然表达性更强，但计算复杂度高且存在冗余计算。HookMIL旨在解决这些限制，提供一个上下文感知且计算高效的解决方案。

**Method:** 我们提出了一种名为HookMIL的多实例学习框架，它利用可学习的hook token来进行结构化的上下文聚合，这些token可以从关键补丁的视觉特征、来自视觉-语言病理模型的文本嵌入以及来自空间转录组学-视觉模型的空间相关特征中初始化。这样可以加速收敛并提高表现力。

**Result:** 在四个公共病理数据集上进行的广泛实验表明，HookMIL实现了最先进的性能，且具有改进的计算效率和可解释性。

**Conclusion:** 实验验证了HookMIL在病理图像分析中的优越性能，编码已公开。

**Abstract:** Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

</details>


### [10] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

> 本文提出Tiny-YOLOSAM，通过使用YOLOv12生成前景对象提示，并针对未覆盖区域采样稀疏点提示，大幅提升了TinySAM的分割质量和速度。

<details>
  <summary>Details</summary>

**Motivation:** 尽管TinySAM在分割的质量上表现出色，但其"一切分割"模式仍然需要大量提示并且速度较慢，尤其是在低延时需求的场景下。

**Method:** 该论文提出了一种名为Tiny-YOLOSAM的混合管线，利用YOLOv12检测算法生成前景对象的框提示，并在检测结果未覆盖的区域使用稀疏点提示来补充，从而提高TinySAM的全场景分割效率。

**Result:** 在COCO val2017数据集上，该混合系统显著提升了类别无关的覆盖范围（AR从16.4%提高到77.1%，mIoU从19.2%提高到67.8%），同时将端到端运行时间从49.20秒/图像减少至10.39秒/图像（4.7倍提速）。

**Conclusion:** 检测器引导的提示加上目标稀疏采样是实用全场景分割中有效的替代全密集提示的方法。

**Abstract:** The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

</details>
