{"id": "2509.03609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03609", "abs": "https://arxiv.org/abs/2509.03609", "authors": ["Shengkai Sun", "Zefan Zhang", "Jianfeng Dong", "Zhiyong Cheng", "Xiaojun Chang", "Meng Wang"], "title": "Towards Efficient General Feature Prediction in Masked Skeleton Modeling", "comment": "Accepted by ICCV 2025", "summary": "Recent advances in the masked autoencoder (MAE) paradigm have significantly\npropelled self-supervised skeleton-based action recognition. However, most\nexisting approaches limit reconstruction targets to raw joint coordinates or\ntheir simple variants, resulting in computational redundancy and limited\nsemantic representation. To address this, we propose a novel General Feature\nPrediction framework (GFP) for efficient mask skeleton modeling. Our key\ninnovation is replacing conventional low-level reconstruction with high-level\nfeature prediction that spans from local motion patterns to global semantic\nrepresentations. Specifically, we introduce a collaborative learning framework\nwhere a lightweight target generation network dynamically produces diversified\nsupervision signals across spatial-temporal hierarchies, avoiding reliance on\npre-computed offline features. The framework incorporates constrained\noptimization to ensure feature diversity while preventing model collapse.\nExperiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits\nof our approach: Computational efficiency (with 6.2$\\times$ faster training\nthan standard masked skeleton modeling methods) and superior representation\nquality, achieving state-of-the-art performance in various downstream tasks.", "AI": {"tldr": "论文提出了GFP框架，通过高级特征预测方法改进了基于掩码自编码器的骨架动作识别模型，提高了计算效率和特征表示的质量，实现了在多个数据集上的最优性能。", "motivation": "研究旨在解决现有的基于掩码自编码器的方法存在计算冗余和语义表示有限的问题，通过提出新的框架来改进基于骨架的动作识别的自监督学习方法。", "method": "提出了一种名为GFP（通用特征预测框架）的新方法，该方法通过使用轻量级的目标生成网络来进行空间-时间分层的多样化监督信号生成，从而动态地取代传统的低级重建目标，生成从局部运动模式到全局语义表示的高级特征预测。这种方法通过约束优化来确保特征多样性并防止模型崩溃。", "result": "实验表明，在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD数据集上，该方法不仅训练速度提高了6.2倍，而且在多种下游任务中都达到了最先进的性能。", "conclusion": "研究表明，使用高级特征预测替代低级重建目标能够显著提高计算效率和特征表示的质量，为基于骨架的动作识别任务提供了有效且高效的解决方案。"}}
{"id": "2509.03614", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03614", "abs": "https://arxiv.org/abs/2509.03614", "authors": ["Seungho Choe", "Xiaoli Qin", "Abubakr Shafique", "Amanda Dy", "Susan Done", "Dimitrios Androutsos", "April Khademi"], "title": "Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge", "comment": "4 pages, 1 figures, final submission for MIDOG 2025 challenge", "summary": "Counting mitotic figures is time-intensive for pathologists and leads to\ninter-observer variability. Artificial intelligence (AI) promises a solution by\nautomatically detecting mitotic figures while maintaining decision consistency.\nHowever, AI tools are susceptible to domain shift, where a significant drop in\nperformance can occur due to differences in the training and testing sets,\nincluding morphological diversity between organs, species, and variations in\nstaining protocols. Furthermore, the number of mitoses is much less than the\ncount of normal nuclei, which introduces severely imbalanced data for the\ndetection task. In this work, we formulate mitosis detection as a pixel-level\nsegmentation and propose a teacher-student model that simultaneously addresses\nmitosis detection (Track 1) and atypical mitosis classification (Track 2). Our\nmethod is based on a UNet segmentation backbone that integrates domain\ngeneralization modules, namely contrastive representation learning and\ndomain-adversarial training. A teacher-student strategy is employed to generate\npixel-level pseudo-masks not only for annotated mitoses and hard negatives but\nalso for normal nuclei, thereby enhancing feature discrimination and improving\nrobustness against domain shift. For the classification task, we introduce a\nmulti-scale CNN classifier that leverages feature maps from the segmentation\nmodel within a multi-task learning paradigm. On the preliminary test set, the\nalgorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of\n0.8414 in Track 2, demonstrating the effectiveness of integrating\nsegmentation-based detection and classification into a unified framework for\nrobust mitosis analysis.", "AI": {"tldr": "本文提出一种结合分割和分类的师生模型以应对有丝分裂检测中的领域偏移问题，实验证明其在初步测试集上的表现良好。", "motivation": "病理学家进行有丝分裂计数是耗时的，并且导致观察者之间的一致性问题。人工智能（AI）提出通过自动化检测有丝分裂，并保持决策一致性来解决这一问题。但是，AI工具容易受到领域偏移的影响，在训练集和测试集之间的性能会显著下降，这种差异包括器官、物种之间的形态多样性，以及染色方案的变化。此外，有丝分裂的数量远少于正常细胞核的数量，这引入了高度不平衡的数据检测任务。", "method": "本研究将有丝分裂检测定义为像素级别的分割问题，并提出了一种师生模型，该模型同时解决了有丝分裂检测（Track 1）和非典型有丝分裂分类（Track 2）的问题。方法基于UNet分割骨干网络，并整合了领域泛化模块，包括对比表征学习和领域对抗训练。通过师生策略生成像素级别的伪掩模，不仅用于标注的有丝分裂和难以区分的负样本，还有普通的细胞核，从而增强特征的区分性并改善领域偏移的鲁棒性。对于分类任务，采用了一个多尺度CNN分类器，利用分割模型中的特征图进行多任务学习。", "result": "在初步测试集上，算法在Track 1中的F1得分为0.7660，在Track 2中的平衡准确率为0.8414，展示了结合基于分割的检测和分类到统一框架中进行稳健有丝分裂分析的有效性。", "conclusion": "提出的方法通过整合领域泛化的模块和一个师生策略实现了一种稳健的有丝分裂分析系统，该系统在初步测试集上展示了良好的性能，为有丝分裂检测和分类任务提供了一个可靠的方法。"}}
{"id": "2509.03616", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03616", "abs": "https://arxiv.org/abs/2509.03616", "authors": ["Rajeev Ranjan Dwivedi", "Ankur Kumar", "Vinod K Kurmi"], "title": "Multi Attribute Bias Mitigation via Representation Learning", "comment": "ECAI 2025 (28th European Conference on Artificial Intelligence)", "summary": "Real world images frequently exhibit multiple overlapping biases, including\ntextures, watermarks, gendered makeup, scene object pairings, etc. These biases\ncollectively impair the performance of modern vision models, undermining both\ntheir robustness and fairness. Addressing these biases individually proves\ninadequate, as mitigating one bias often permits or intensifies others. We\ntackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a\nlean two stage framework that needs group labels only while training and\nminimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)\ndeliberately identifies the influence of known shortcuts by training encoders\nfor each attribute and integrating them with the main backbone, compelling the\nclassifier to explicitly recognize these biases. Then Gradient Suppression Fine\nTuning prunes those very bias directions from the backbone's gradients, leaving\na single compact network that ignores all the shortcuts it just learned to\nrecognize. Moreover we find that existing bias metrics break under subgroup\nimbalance and train test distribution shifts, so we introduce Scaled Bias\nAmplification (SBA): a test time measure that disentangles model induced bias\namplification from distributional differences. We validate GMBM on FB CMNIST,\nCelebA, and COCO, where we boost worst group accuracy, halve multi attribute\nbias amplification, and set a new low in SBA even as bias complexity and\ndistribution shifts intensify, making GMBM the first practical, end to end\nmultibias solution for visual recognition. Project page:\nhttp://visdomlab.github.io/GMBM/", "AI": {"tldr": "GMBM 是一个双阶段框架，旨在解决视觉识别中的多重偏差问题，通过训练阶段识别偏差并在测试阶段消除它，从而提升模型在复杂偏差情境下的表现。", "motivation": "现代视觉模型在面对包括纹理、水印、化妆和场景对象搭配等多重重叠偏差时，其健壮性和公平性都会受到影响。单独缓解这些偏差是不够的，因为消除一个偏差可能允许或加剧其他偏差。", "method": "GMBM 框架包括两个阶段：1) ABIL 阶段通过训练属性编码器并将其与主骨架网络集成来识别已知捷径的影响；2) 通过梯度抑制微调去掉与偏差相关联的梯度方向，得到一个简洁的忽略捷径的网络。", "result": "该方法在 FB CMNIST、CelebA 和 COCO 数据集上验证效果，提升了最差结果组的准确率，减少了多属性偏差放大，并在 SBA 评分中取得了新的低分。", "conclusion": "GMBM 是第一个实际的端到端多重偏差解决方案，能够在偏差复杂性和分布变化增加的情况下提高视觉识别模型的性能。"}}
{"id": "2509.03631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03631", "abs": "https://arxiv.org/abs/2509.03631", "authors": ["Anders Kjelsrud", "Lasse Løvstakken", "Erik Smistad", "Håvard Dalen", "Gilles Van De Vyver"], "title": "Lightweight image segmentation for echocardiography", "comment": "4 pages, 6 figures, The 2025 IEEE International Ultrasonics Symposium", "summary": "Accurate segmentation of the left ventricle in echocardiography can enable\nfully automatic extraction of clinical measurements such as volumes and\nejection fraction. While models configured by nnU-Net perform well, they are\nlarge and slow, thus limiting real-time use. We identified the most effective\ncomponents of nnU-Net for cardiac segmentation through an ablation study,\nincrementally evaluating data augmentation schemes, architectural\nmodifications, loss functions, and post-processing techniques. Our analysis\nrevealed that simple affine augmentations and deep supervision drive\nperformance, while complex augmentations and large model capacity offer\ndiminishing returns. Based on these insights, we developed a lightweight U-Net\n(2M vs 33M parameters) that achieves statistically equivalent performance to\nnnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89\nfor LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster\n(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.\nCross-dataset evaluation on an internal dataset (N=311) confirms comparable\ngeneralization.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03525", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03525", "abs": "https://arxiv.org/abs/2509.03525", "authors": ["Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sepehr Karimi", "Sina Rashidi", "Ali Zolnour", "Maryam Dadkhah", "Yasaman Haghbin", "Hossein AzadMaleki", "Maryam Zolnoori"], "title": "Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies", "comment": null, "summary": "Over half of US adults with Alzheimer disease and related dementias remain\nundiagnosed, and speech-based screening offers a scalable detection approach.\nWe compared large language model adaptation strategies for dementia detection\nusing the DementiaBank speech corpus, evaluating nine text-only models and\nthree multimodal audio-text models on recordings from DementiaBank speech\ncorpus. Adaptations included in-context learning with different demonstration\nselection policies, reasoning-augmented prompting, parameter-efficient\nfine-tuning, and multimodal integration. Results showed that class-centroid\ndemonstrations achieved the highest in-context learning performance, reasoning\nimproved smaller models, and token-level fine-tuning generally produced the\nbest scores. Adding a classification head substantially improved\nunderperforming models. Among multimodal models, fine-tuned audio-text systems\nperformed well but did not surpass the top text-only models. These findings\nhighlight that model adaptation strategies, including demonstration selection,\nreasoning design, and tuning method, critically influence speech-based dementia\ndetection, and that properly adapted open-weight models can match or exceed\ncommercial systems.", "AI": {"tldr": "研究针对DementiaBank语音语料库评估了九种单文本模型和三种多模态音频文本模型，以改进痴呆症检测。发现类中心示范和推理设计对于模型的优化至关重要。", "motivation": "鉴于超过一半的美国老年人患有未被诊断的阿尔茨海默病和相关痴呆症，基于语言的筛查为可扩展的检测方法提供了潜在解决方案。", "method": "我们研究了使用DementiaBank语音语料库进行痴呆症检测时，大型语言模型的多种适应策略。这些策略包括上下文学习、不同示范选择策略、推理增强提示、参数高效微调和多模态集成。", "result": "研究结果显示，类中心示范在上下文学习中表现最佳，推理改善了较小模型的表现，而标记级微调通常产生最佳分数。增加分类头显著提高了表现不佳的模型。在多模态模型中，微调的音频-文本系统表现出色但未超过顶级文本模型。", "conclusion": "这些发现强调了模型适应策略在基于语言的痴呆症检测中的关键作用，这些策略包括示范选择和推理设计。合适适应后的开放式模型可以与商用系统媲美或超过其性能。"}}
{"id": "2509.03633", "categories": ["cs.CV", "cs.AI", "I.4.6; I.5.2; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.03633", "abs": "https://arxiv.org/abs/2509.03633", "authors": ["Josafat-Mattias Burmeister", "Andreas Tockner", "Stefan Reder", "Markus Engel", "Rico Richter", "Jan-Peter Mund", "Jürgen Döllner"], "title": "treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds", "comment": null, "summary": "Close-range laser scanning provides detailed 3D captures of forest stands but\nrequires efficient software for processing 3D point cloud data and extracting\nindividual trees. Although recent studies have introduced deep learning methods\nfor tree instance segmentation, these approaches require large annotated\ndatasets and substantial computational resources. As a resource-efficient\nalternative, we present a revised version of the treeX algorithm, an\nunsupervised method that combines clustering-based stem detection with region\ngrowing for crown delineation. While the original treeX algorithm was developed\nfor personal laser scanning (PLS) data, we provide two parameter presets, one\nfor ground-based laser scanning (stationary terrestrial - TLS and PLS), and one\nfor UAV-borne laser scanning (ULS). We evaluated the method on six public\ndatasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham\nWoods) and compared it to six open-source methods (original treeX, treeiso,\nRayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original\ntreeX algorithm, our revision reduces runtime and improves accuracy, with\ninstance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.\nFor ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original\nalgorithm fails to segment any correct instances. For TLS and PLS data, our\nalgorithm achieves accuracy similar to recent open-source methods, including\ndeep learning. Given its algorithmic design, we see two main applications for\nour method: (1) as a resource-efficient alternative to deep learning approaches\nin scenarios where the data characteristics align with the method design\n(sufficient stem visibility and point density), and (2) for the semi-automatic\ngeneration of labels for deep learning models. To enable broader adoption, we\nprovide an open-source Python implementation in the pointtree package.", "AI": {"tldr": "本文提出了一种改进的无监督树X算法，用更少的资源进行高效的3D点云数据处理和独立树提取，尤其适合地面和无人机激光扫描数据。", "motivation": "目前，虽然有研究引入了深度学习方法进行树木实例分割，但这些方法需要大规模标注数据集和大量的计算资源。而改进的treeX算法能够在减少资源消耗的同时，提高处理3D点云数据和提取独立树的效率。", "method": "一种改进的无监督算法treeX的修订版，该算法结合了基于聚类的树干检测与区域生长进行树冠划分。该方法提供了两组参数设置，一组适用于地面基于激光扫描数据（静态地面激光扫描TLS及个人激光扫描PLS），另一组适用于无人机搭载的激光扫描数据（ULS）.", "result": "经过对比测试，该改进算法在地面数据上的实例检测F1分数比原算法提高了0.11至0.49，对于无人机激光扫描数据，改进版获得了0.58的F1分数，而原算法无法正确分割实例。", "conclusion": "改进后的treeX算法在准确性和时间效率方面超过了原算法，适用于资源受限情况下的应用，或者作为深度学习模型的半自动标签生成工具。此外，提供了一个开源Python实现。"}}
{"id": "2509.03526", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03526", "abs": "https://arxiv.org/abs/2509.03526", "authors": ["Yansong Liu", "Jiateng Li", "Yuan Liu"], "title": "Enhancing Speech Large Language Models through Reinforced Behavior Alignment", "comment": null, "summary": "The recent advancements of Large Language Models (LLMs) have spurred\nconsiderable research interest in extending their linguistic capabilities\nbeyond text to other modalities, which leads to emergence of speech-based LLMs\n(SpeechLMs) with capability of processing user request in either speech or\ntextual formats. However, owing to inter-modal discrepancies, these SpeechLMs\nstill exhibit a significant performance gap compared to their text-based LLM\ncounterparts in instruction-following, particularly when confronted with the\ndynamic and variable nature of user speech. To address this challenge, this\npaper introduces a framework termed Reinforced Behavior Alignment (RBA),\ndesigned to bolster the language generation proficiency of SpeechLMs. Instead\nof relying on supervised fine-tuning from human annotations, RBA employs a\nself-synthesis methodology to generate extensive, high-fidelity alignment data\nby a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of\na teacher using a reinforcement learning-based approach. Experimental results\ndemonstrate that this method effectively enhances the instruction-following\ncapabilities of SpeechLMs that outperform conventional distillation baselines.\nCrucially, we demonstrate that RBA can be seamlessly extended to tasks such\nincluding spoken question answering and speech-to-text translation, attaining\nstate-of-the-art performance on open benchmarks with only self-generated data.", "AI": {"tldr": "针对基于语音的大型语言模型在指令遵循方面存在的性能差距，本文提出了一个名为强化行为对齐（RBA）的框架。RBA通过自我合成方法生成对齐数据并使用强化学习对齐模型行为，实验表明该方法提升了SpeechLMs的指令遵循性能，并扩展至多种任务中，达到了最新的性能水平。", "motivation": "尽管大型语言模型（LLMs）的最新进展引发了将它们的语言能力扩展到其他模态（如语音）的研究兴趣，但是由于模态间的差异，这些基于语音的LLMs（SpeechLMs）在指令遵循方面仍存在显著的性能差距，尤其是面对用户语音的动态变化时。", "method": "通过引入一个名为强化行为对齐（RBA）的框架来解决这一问题，该框架旨在提高SpeechLMs的语言生成能力。RBA不依赖于基于人类标注的监督微调，而是采用自我合成的方法，通过一个强大的教师LLM生成大量高保真的对齐数据，并使用基于强化学习的方法来对齐SpeechLMs的行为。", "result": "实验结果表明，该方法有效地提高了SpeechLMs的指令遵循能力，并超越了传统的蒸馏基线。此外，我们证明了RBA可以无缝扩展到包括口语问答和语音到文本翻译等任务，并在公开基准上取得了最先进的性能。", "conclusion": "通过自我生成数据，所提出的方法在开放基准上达到了最先进的性能，证明了RBA框架的有效性和适应性。"}}
{"id": "2509.03635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03635", "abs": "https://arxiv.org/abs/2509.03635", "authors": ["Hongpei Zheng", "Lintao Xiang", "Qijun Yang", "Qian Lin", "Hujun Yin"], "title": "Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding", "comment": "16 pages, 6 figures", "summary": "The rapid development of Large Multimodal Models (LMMs) has led to remarkable\nprogress in 2D visual understanding; however, extending these capabilities to\n3D scene understanding remains a significant challenge. Existing approaches\npredominantly rely on text-only supervision, which fails to provide the\ngeometric constraints required for learning robust 3D spatial representations.\nIn this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction\nTuning framework that addresses this limitation by incorporating geometry-aware\nsupervision directly into the training process. Our key insight is that\neffective 3D understanding necessitates reconstructing underlying geometric\nstructures rather than merely describing them. Unlike existing methods that\ninject 3D information solely at the input level, Reg3D adopts a\ndual-supervision paradigm that leverages 3D geometric information both as input\nand as explicit learning targets. Specifically, we design complementary\nobject-level and frame-level reconstruction tasks within a dual-encoder\narchitecture, enforcing geometric consistency to encourage the development of\nspatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,\nScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance\nimprovements, establishing a new training paradigm for spatially aware\nmultimodal models.", "AI": {"tldr": "本文介绍了Reg3D，一种用于3D空间理解的新重建几何指令调优框架，该框架解决了现有方法依赖于文本监督而无法提供几何约束的问题，通过在训练过程中直接引入几何感知监督来改进3D理解能力，并通过多个基准测试证明了其有效性。", "motivation": "大型多模态模型（LMMs）在2D视觉理解方面取得了显著的进步，但将这些能力扩展到3D场景理解仍然是一项重大挑战。当前的方法主要依赖于文本监督，这无法提供学习鲁棒3D空间表示所需的几何约束。我们的动机是解决这一限制，以改进3D空间理解。", "method": "我们的方法名为Reg3D，这是一种重建几何指令调优框架，它通过在训练过程中直接引入几何感知监督，解决了现有方法主要依赖于仅使用文本监督的问题，这种方法无法提供学习鲁棒的3D空间表示所需的几何约束。与现有方法仅在输入级别注入3D信息不同，Reg3D采用双监督范式，既利用3D几何信息作为输入，也作为明确的学习目标。具体来说，我们设计了互补的对象级别和帧级别重建任务，在双编码器架构内鼓励几何一致性，以促进空间推理能力的发展。", "result": "在ScanQA、Scan2Cap、ScanRefer和SQA3D上的广泛实验表明，与现有方法相比，Reg3D实现了显著的性能提升，确立了一种新的具有空间意识的多模态模型训练范式。", "conclusion": "本文提出的Reg3D框架通过引入几何感知监督，证明了它可以有效改进3D场景理解的能力。实验证明了该框架的有效性和优越性，为具有空间意识的多模态模型训练树立了一个新的标准。"}}
{"id": "2509.03527", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03527", "abs": "https://arxiv.org/abs/2509.03527", "authors": ["Bohdan M. Pavlyshenko"], "title": "Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model", "comment": null, "summary": "In the paper, we consider multilevel multitask analysis of cryptocurrency\nnews using a fine-tuned Mistral 7B large language model with\nretrieval-augmented generation (RAG).\n  On the first level of analytics, the fine-tuned model generates graph and\ntext summaries with sentiment scores as well as JSON representations of\nsummaries. Higher levels perform hierarchical stacking that consolidates sets\nof graph-based and text-based summaries as well as summaries of summaries into\ncomprehensive reports. The combination of graph and text summaries provides\ncomplementary views of cryptocurrency news. The model is fine-tuned with 4-bit\nquantization using the PEFT/LoRA approach. The representation of cryptocurrency\nnews as knowledge graph can essentially eliminate problems with large language\nmodel hallucinations.\n  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM\nmodels for multilevel cryptocurrency news analysis can conduct informative\nqualitative and quantitative analytics, providing important insights.", "AI": {"tldr": "A fine-tuned Mistral 7B large language model with retrieval-augmented generation is used for multilevel analysis of cryptocurrency news, generating graph and text summaries which are combined into comprehensive reports. This approach reduces model hallucinations and provides high-quality, insightful analysis of the news.", "motivation": "The motivation behind this paper is to provide informative qualitative and quantitative analytics of cryptocurrency news using hierarchical analysis techniques. This approach aims to complement existing methods and provide a more comprehensive view of the news, while also aiming to reduce hallucinations typical of large language models through the use of knowledge graphs.", "method": "The paper uses a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG) for multilevel multitask analysis of cryptocurrency news. On the first level, the model generates graph and text summaries with sentiment scores and JSON representations. Higher levels consolidate these summaries into comprehensive reports. The model is fine-tuned with 4-bit quantization using the PEFT/LoRA approach.", "result": "The results demonstrate that the fine-tuned Mistral 7B LLM models can provide insightful analysis of cryptocurrency news through multilevel summarization and consolidation techniques. The approach effectively combines graph and text-based insights to eliminate model hallucinations.", "conclusion": "The conclusion is that the method proposed in the paper can effectively perform multilevel analysis of cryptocurrency news, providing both qualitative and quantitative insights. The model, fine-tuned with 4-bit quantization and using knowledge graph representations, reduces hallucinations and enhances the accuracy of the news analysis."}}
{"id": "2509.03704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03704", "abs": "https://arxiv.org/abs/2509.03704", "authors": ["Seth Z. Zhao", "Huizhi Zhang", "Zhaowei Li", "Juntong Peng", "Anthony Chui", "Zewei Zhou", "Zonglin Meng", "Hao Xiang", "Zhiyu Huang", "Fujia Wang", "Ran Tian", "Chenfeng Xu", "Bolei Zhou", "Jiaqi Ma"], "title": "QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception", "comment": null, "summary": "Cooperative perception through Vehicle-to-Everything (V2X) communication\noffers significant potential for enhancing vehicle perception by mitigating\nocclusions and expanding the field of view. However, past research has\npredominantly focused on improving accuracy metrics without addressing the\ncrucial system-level considerations of efficiency, latency, and real-world\ndeployability. Noticeably, most existing systems rely on full-precision models,\nwhich incur high computational and transmission costs, making them impractical\nfor real-time operation in resource-constrained environments. In this paper, we\nintroduce \\textbf{QuantV2X}, the first fully quantized multi-agent system\ndesigned specifically for efficient and scalable deployment of multi-modal,\nmulti-agent V2X cooperative perception. QuantV2X introduces a unified\nend-to-end quantization strategy across both neural network models and\ntransmitted message representations that simultaneously reduces computational\nload and transmission bandwidth. Remarkably, despite operating under low-bit\nconstraints, QuantV2X achieves accuracy comparable to full-precision systems.\nMore importantly, when evaluated under deployment-oriented metrics, QuantV2X\nreduces system-level latency by 3.2$\\times$ and achieves a +9.5 improvement in\nmAP30 over full-precision baselines. Furthermore, QuantV2X scales more\neffectively, enabling larger and more capable models to fit within strict\nmemory budgets. These results highlight the viability of a fully quantized\nmulti-agent intermediate fusion system for real-world deployment. The system\nwill be publicly released to promote research in this field:\nhttps://github.com/ucla-mobility/QuantV2X.", "AI": {"tldr": "提出了QuantV2X系统，一个用于V2X协同感知的全量化多代理系统，该系统在保持高精度的同时降低了计算负荷和传输带宽，提高了系统性能和扩展性，适合于实际部署。", "motivation": "过去的V2X研究主要关注于提高准确性指标，但忽视了基于系统级别的效率、延迟和实际部署关键考虑。大多数现有系统依赖于高精度模型，导致计算和传输成本高昂，使得其在资源有限的环境中实时运行变得不可行。", "method": "介绍了一种名为QuantV2X的全量化多代理系统，该系统设计用于V2X协同感知的高效和可扩展部署。QuantV2X引入了一种统一的端到端量化策略，同时应用于神经网络模型和传输的消息表示，以减少计算负载和传输带宽。", "result": "尽管在低比特约束条件下运行，QuantV2X仍能实现与全精度系统相当的精度。在部署导向的指标评估下，QuantV2X相比全精度基线系统大幅度减少了系统级延迟，并且在mAP30指标上提升了9.5。此外，QuantV2X能够更有效地扩展，使更大的、更强大的模型可以在严格的内存预算内运行。", "conclusion": "结果表明了全量化多代理中级融合系统在现实世界部署中的可行性。"}}
{"id": "2509.03528", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03528", "abs": "https://arxiv.org/abs/2509.03528", "authors": ["Matilde Contestabile", "Chiara Ferrara", "Alberto Giovannetti", "Giovanni Parrillo", "Andrea Vandin"], "title": "The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process", "comment": null, "summary": "Process Mining (PM), initially developed for industrial and business\ncontexts, has recently been applied to social systems, including legal ones.\nHowever, PM's efficacy in the legal domain is limited by the accessibility and\nquality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in\nItalian Chambers), a comprehensive event log of the Italian lawmaking process\nfrom 1987 to 2022. Created from unstructured data from the Normattiva portal\nand structured using large language models (LLMs), ProLiFIC aligns with recent\nefforts in integrating PM with LLMs. We exemplify preliminary analyses and\npropose ProLiFIC as a benchmark for legal PM, fostering new developments.", "AI": {"tldr": "我们提出ProLiFIC，这是一个根据Normattiva门户网站的非结构化数据构建的、从1987年到2022年意大利立法过程的全面事件日志，通过使用大型语言模型，旨在作为法律过程挖掘的基准。", "motivation": "鉴于过程挖掘（PM）在法律领域的效用受到数据集的可访问性和质量的限制，我们引入一个全面的意大利立法过程事件日志，以证明PM在法律领域的潜在价值。", "method": "介绍ProLiFIC（意大利立法流程的程序法制定流程），这是一个全面的意大利立法过程事件日志，时间跨度为1987年至2022年。该日志由Normattiva门户网站的非结构化数据创建，并使用大型语言模型（LLMs）进行结构调整，体现了将过程挖掘（PM）与LLMs结合的最新努力。", "result": "作为初步分析的例子，提出ProLiFIC作为法律过程挖掘的基准，以推动新的进展。", "conclusion": "通过初步分析实例，ProLiFIC被提出作为法律过程挖掘的基准，旨在推动该领域的新发展。"}}
{"id": "2509.03729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03729", "abs": "https://arxiv.org/abs/2509.03729", "authors": ["Bandita Bharadwaj", "Ankur Mishra", "Saurav Bharadwaj"], "title": "Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns", "comment": null, "summary": "This study evaluates the efficacy of three deep learning architectures:\nResNet50, MobileNetV2, and EfficientNetB0 for automated plant species\nclassification based on leaf venation patterns, a critical morphological\nfeature with high taxonomic relevance. Using the Swedish Leaf Dataset\ncomprising images from 15 distinct species (75 images per species, totalling\n1,125 images), the models were demonstrated using standard performance metrics\nduring training and testing phases. ResNet50 achieved a training accuracy of\n94.11% but exhibited overfitting, reflected by a reduced testing accuracy of\n88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better\ngeneralization capabilities, attaining a testing accuracy of 93.34% and an F1\nscore of 93.23%, indicating its suitability for lightweight, real-time\napplications. EfficientNetB0 outperformed both models, achieving a testing\naccuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,\nhighlighting its robustness in venation-based classification. The findings\nunderscore the potential of deep learning, particularly EfficientNetB0, in\ndeveloping scalable and accurate tools for automated plant taxonomy using\nvenation traits.", "AI": {"tldr": "本文使用Swedish Leaf Dataset评估了三种深度学习模型在基于叶脉图案分类植物物种的应用，最终发现EfficientNetB0表现最优，凸显了其在植物分类中的潜力。", "motivation": "植物物种分类基于叶脉图案是一个具有高度分类意义的重要形态特征，因此本文研究了三种深度学习模型在此领域的潜力。", "method": "本文评估了三种深度学习架构在基于叶脉图案的植物物种自动分类中的有效性：ResNet50、MobileNetV2和EfficientNetB0。", "result": "实验结果表明，EfficientNetB0的表现最好，测试准确率为94.67%，精度、召回率和F1分数均超过94.6%。", "conclusion": "研究结果突显了深度学习，尤其是EfficientNetB0，在利用叶脉特征开发可扩展且准确的植物分类工具方面的潜力。"}}
{"id": "2509.03529", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03529", "abs": "https://arxiv.org/abs/2509.03529", "authors": ["Alejandro Álvarez Castro", "Joaquín Ordieres-Meré"], "title": "Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages", "comment": "Presented at NLMLT2025 (https://airccse.org/csit/V15N16.html), 15\n  pages, 5 figures", "summary": "Earnings calls represent a uniquely rich and semi-structured source of\nfinancial communication, blending scripted managerial commentary with\nunscripted analyst dialogue. Although recent advances in financial sentiment\nanalysis have integrated multi-modal signals, such as textual content and vocal\ntone, most systems rely on flat document-level or sentence-level models,\nfailing to capture the layered discourse structure of these interactions. This\npaper introduces a novel multi-modal framework designed to generate\nsemantically rich and structurally aware embeddings of earnings calls, by\nencoding them as hierarchical discourse trees. Each node, comprising either a\nmonologue or a question-answer pair, is enriched with emotional signals derived\nfrom text, audio, and video, as well as structured metadata including coherence\nscores, topic labels, and answer coverage assessments. A two-stage transformer\narchitecture is proposed: the first encodes multi-modal content and discourse\nmetadata at the node level using contrastive learning, while the second\nsynthesizes a global embedding for the entire conference. Experimental results\nreveal that the resulting embeddings form stable, semantically meaningful\nrepresentations that reflect affective tone, structural logic, and thematic\nalignment. Beyond financial reporting, the proposed system generalizes to other\nhigh-stakes unscripted communicative domains such as tele-medicine, education,\nand political discourse, offering a robust and explainable approach to\nmulti-modal discourse representation. This approach offers practical utility\nfor downstream tasks such as financial forecasting and discourse evaluation,\nwhile also providing a generalizable method applicable to other domains\ninvolving high-stakes communication.", "AI": {"tldr": "A novel multi-modal framework, using a two-stage transformer architecture, is introduced to create structurally aware and emotionally enriched embeddings of earnings call interactions, which generally improves the understanding of the discourse.", "motivation": "While previous works on financial sentiment analysis incorporate multi-modal signals, they generally use flat models that fail to capture the hierarchical discourse structure of earnings calls.", "method": "This paper proposes a two-stage transformer architecture to generate multi-modal discourse embeddings for earnings calls. The first stage encodes multi-modal content and structured metadata at the node level using contrastive learning, while the second synthesizes a global embedding for the entire conference.", "result": "Experiments show that the generated embeddings are stable and semantically meaningful, capturing affective tone, structural logic, and thematic alignment.", "conclusion": "The multi-modal discourse representation approach not only provides useful embeddings for financial reporting, but also has the potential to be applied to other domains, such as telemedicine, education, and political discourse."}}
{"id": "2509.03737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03737", "abs": "https://arxiv.org/abs/2509.03737", "authors": ["Casper van Engelenburg", "Jan van Gemert", "Seyran Khademi"], "title": "LayoutGKN: Graph Similarity Learning of Floor Plans", "comment": "BMVC (2025)", "summary": "Floor plans depict building layouts and are often represented as graphs to\ncapture the underlying spatial relationships. Comparison of these graphs is\ncritical for applications like search, clustering, and data visualization. The\nmost successful methods to compare graphs \\ie, graph matching networks, rely on\ncostly intermediate cross-graph node-level interactions, therefore being slow\nin inference time. We introduce \\textbf{LayoutGKN}, a more efficient approach\nthat postpones the cross-graph node-level interactions to the end of the joint\nembedding architecture. We do so by using a differentiable graph kernel as a\ndistance function on the final learned node-level embeddings. We show that\nLayoutGKN computes similarity comparably or better than graph matching networks\nwhile significantly increasing the speed.\n\\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are\nopen.", "AI": {"tldr": "本文提出了 LayoutGKN 方法，通过优化图匹配过程，实现了在保持或提高准确性的同时，大幅提升计算速度。", "motivation": "传统图匹配网络在比对图时需要昂贵的跨图节点级别交互，导致推理时间长。该方法旨在提高效率的同时保持或提高相似性计算的能力。", "method": "LayoutGKN，通过将跨图节点级别的交互推迟到联合嵌入架构的最后，使用可微分图核作为最终学习到的节点级别嵌入的距离函数，来提高效率和速度。", "result": "LayoutGKN 在计算相似性方面可以达到与图匹配网络相当或更好的结果，同时显著提高了速度。", "conclusion": "提出了 LayoutGKN 方法，它在保持或提高相似性计算质量的同时，显著提高了计算速度。"}}
{"id": "2509.03530", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03530", "abs": "https://arxiv.org/abs/2509.03530", "authors": ["Paul Blum", "Enrico Liscio", "Ruixuan Zhang", "Caroline Figueroa", "Pradeep K. Murukannaiah"], "title": "Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts", "comment": null, "summary": "Suicide is a leading cause of death among adolescents (12-18), yet predicting\nit remains a significant challenge. Many cases go undetected due to a lack of\ncontact with mental health services. Social media, however, offers a unique\nopportunity, as young people often share their thoughts and struggles online in\nreal time. In this work, we propose a novel task and method to approach it:\npredicting suicidal ideation and behavior (SIB) from forum posts before an\nadolescent explicitly expresses suicidal ideation on an online forum. This\npredictive framing, where no self-disclosure is used as input at any stage,\nremains largely unexplored in the suicide prediction literature. To this end,\nwe introduce Early-SIB, a transformer-based model that sequentially processes\nthe posts a user writes and engages with to predict whether they will write a\nSIB post. Our model achieves a balanced accuracy of 0.73 for predicting future\nSIB on a Dutch youth forum, demonstrating that such tools can offer a\nmeaningful addition to traditional methods.", "AI": {"tldr": "研究表明，利用在线论坛数据预测青少年的自杀行为是可能的，并提出了一种基于变压器的预测模型Early-SIB，该模型在测试中表现良好。", "motivation": "自杀是青少年（12-18岁）的主要死因之一，但由于缺乏接触心理健康服务的机会，许多情况下未被发现。鉴于年轻人经常在网上实时分享他们的想法和困扰，本研究希望通过在线论坛帖子预测青少年的自杀意念和行为。", "method": "本研究提出了一种基于变压器的Early-SIB模型，该模型可以依次处理用户撰写的帖子及其互动情况，以预测他们是否将发表涉及自杀意念和行为的帖子。", "result": "Early-SIB模型在荷兰青少年论坛上的SIB预测中达到了0.73的平衡准确性，证明了此模型预测青少年未来SIB行为的有效性。", "conclusion": "研究显示，基于变压器的Early-SIB模型可以通过分析青少年在在线论坛上的帖子来预测未来的自杀意念和行为，这表明此类工具可以为传统方法提供有价值的补充。"}}
{"id": "2509.03740", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03740", "abs": "https://arxiv.org/abs/2509.03740", "authors": ["Taha Koleilat", "Hassan Rivaz", "Yiming Xiao"], "title": "Singular Value Few-shot Adaptation of Vision-Language Models", "comment": "10 pages, 2 figures, 8 tables", "summary": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and\nfew-shot learning capabilities across diverse applications. However, adapting\nthese models to new fine-grained domains remains difficult due to reliance on\nprompt engineering and the high cost of full model fine-tuning. Existing\nadaptation approaches rely on augmented components, such as prompt tokens and\nadapter modules, which could limit adaptation quality, destabilize the model,\nand compromise the rich knowledge learned during pretraining. In this work, we\npresent \\textbf{CLIP-SVD}, a novel \\textit{multi-modal} and\n\\textit{parameter-efficient} adaptation technique that leverages Singular Value\nDecomposition (SVD) to modify the internal parameter space of CLIP without\ninjecting additional modules. Specifically, we fine-tune only the singular\nvalues of the CLIP parameter matrices to rescale the basis vectors for domain\nadaptation while retaining the pretrained model. This design enables enhanced\nadaptation performance using only \\textbf{0.04\\%} of the model's total\nparameters and better preservation of its generalization ability. CLIP-SVD\nachieves state-of-the-art classification results on 11 natural and 10\nbiomedical datasets, outperforming previous methods in both accuracy and\ngeneralization under few-shot settings. Additionally, we leverage a natural\nlanguage-based approach to analyze the effectiveness and dynamics of the CLIP\nadaptation to allow interpretability of CLIP-SVD. The code is publicly\navailable at https://github.com/HealthX-Lab/CLIP-SVD.", "AI": {"tldr": "提出了基于SVD的CLIP-SVD，实现CLIP视觉-语言模型的高效领域适应，节省参数，改善泛化能力，在多个数据集中取得优秀效果。", "motivation": "解决零样本和小样本学习中，适应新细粒度领域的困难问题，尝试改进现有依赖于提示工程和完整模型精细调节的较低适应质量，不稳定性和损害预训练习得知识的现状。", "method": "CLIP-SVD，一种多模态和参数高效适应技术，利用奇异值分解（SVD）修改CLIP的内部参数空间，仅微调CLIP参数矩阵的奇异值来调整基础向量，以实现领域适应同时保留预训练模型。仅需该模型总参数的0.04％，有效提升了适应性能，并保持其泛化能力。通过自然语言方法分析CLIP-SVD的适应效果和动态以便解释其性能提升原因。", "result": "在11个自然数据集和10个生物医学数据集中实现了分类的最先进结果，不仅在准确性上，还在小样本设置下的泛化能力上超越了以前的方法。", "conclusion": "CLIP-SVD技术通过极少的额外参数调整实现高效且稳定地适应新领域的目标，同时保持了模型原有的丰富知识。此外，此方法还提供了可解释性分析，提升了理解和使用的便捷性。代码公开可以访问。"}}
{"id": "2509.03531", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03531", "abs": "https://arxiv.org/abs/2509.03531", "authors": ["Oscar Obeso", "Andy Arditi", "Javier Ferrando", "Joshua Freeman", "Cameron Holmes", "Neel Nanda"], "title": "Real-Time Detection of Hallucinated Entities in Long-Form Generation", "comment": null, "summary": "Large language models are now routinely used in high-stakes applications\nwhere hallucinations can cause serious harm, such as medical consultations or\nlegal advice. Existing hallucination detection methods, however, are\nimpractical for real-world use, as they are either limited to short factual\nqueries or require costly external verification. We present a cheap, scalable\nmethod for real-time identification of hallucinated tokens in long-form\ngenerations, and scale it effectively to 70B parameter models. Our approach\ntargets \\emph{entity-level hallucinations} -- e.g., fabricated names, dates,\ncitations -- rather than claim-level, thereby naturally mapping to token-level\nlabels and enabling streaming detection. We develop an annotation methodology\nthat leverages web search to annotate model responses with grounded labels\nindicating which tokens correspond to fabricated entities. This dataset enables\nus to train effective hallucination classifiers with simple and efficient\nmethods such as linear probes. Evaluating across four model families, our\nclassifiers consistently outperform baselines on long-form responses, including\nmore expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for\nLlama-3.3-70B), and are also an improvement in short-form question-answering\nsettings. Moreover, despite being trained only with entity-level labels, our\nprobes effectively detect incorrect answers in mathematical reasoning tasks,\nindicating generalization beyond entities. While our annotation methodology is\nexpensive, we find that annotated responses from one model can be used to train\neffective classifiers on other models; accordingly, we publicly release our\ndatasets to facilitate reuse. Overall, our work suggests a promising new\napproach for scalable, real-world hallucination detection.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03754", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03754", "abs": "https://arxiv.org/abs/2509.03754", "authors": ["Zongsen Qiu"], "title": "STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification", "comment": null, "summary": "Responding to rising global food security needs, precision agriculture and\ndeep learning-based plant disease diagnosis have become crucial. Yet, deploying\nhigh-precision models on edge devices is challenging. Most lightweight networks\nuse attention mechanisms designed for generic object recognition, which poorly\ncapture subtle pathological features like irregular lesion shapes and complex\ntextures. To overcome this, we propose a twofold solution: first, using a\ntraining-free neural architecture search method (DeepMAD) to create an\nefficient network backbone for edge devices; second, introducing the\nShape-Texture Attention Module (STAM). STAM splits attention into two branches\n-- one using deformable convolutions (DCNv4) for shape awareness and the other\nusing a Gabor filter bank for texture awareness. On the public CCMT plant\ndisease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)\nreached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm\nSTAM significantly improves performance over baseline and standard attention\nmodels. Integrating domain knowledge via decoupled attention thus presents a\npromising path for edge-deployed precision agriculture AI. The source code is\navailable at https://github.com/RzMY/STA-Net.", "AI": {"tldr": "研究提出了一种包含Shape-Texture Attention Module (STAM)的新模型，解决了在边缘设备上部署高精度病害诊断模型的难题。STA-Net模型在CCMT数据集上表现出优异的性能。", "motivation": "提高基于深度学习的植物病害诊断模型在边缘设备上的部署精度，因该模型能更好地捕捉到病害细微特征，如不规则病斑形状和复杂纹理。", "method": "使用培训免费的神经架构搜索方法(DeepMAD)创建有效的边缘设备网络骨架并引入形状-纹理注意力模块(STAM)来解决传统注意力机制的问题，STAM分为两个分支，一个分支使用可变形卷积来识别形状，另一个分支使用Gabor滤波器组来识别纹理。", "result": "在CCMT植物病害公开数据集上，其模型STA-Net（拥有401K参数和51.1MFLOPS）达到了89.00%的准确率和88.96%的F1分数，并且断层研究证实了STAM显著提升了基线和标准注意力模型的性能。", "conclusion": "通过将领域知识集成到分离注意力来进行边缘部署的精准农业AI展示了很有前景的方向。"}}
{"id": "2509.03533", "categories": ["cs.CL", "cs.LG", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2509.03533", "abs": "https://arxiv.org/abs/2509.03533", "authors": ["Igor Halperin"], "title": "Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck", "comment": "26 pages, 4 figures", "summary": "Large Language Models (LLMs) are prone to critical failure modes, including\n\\textit{intrinsic faithfulness hallucinations} (also known as confabulations),\nwhere a response deviates semantically from the provided context. Frameworks\ndesigned to detect this, such as Semantic Divergence Metrics (SDM), rely on\nidentifying latent topics shared between prompts and responses, typically by\napplying geometric clustering to their sentence embeddings. This creates a\ndisconnect, as the topics are optimized for spatial proximity, not for the\ndownstream information-theoretic analysis. In this paper, we bridge this gap by\ndeveloping a principled topic identification method grounded in the\nDeterministic Information Bottleneck (DIB) for geometric clustering. Our key\ncontribution is to transform the DIB method into a practical algorithm for\nhigh-dimensional data by substituting its intractable KL divergence term with a\ncomputationally efficient upper bound. The resulting method, which we dub UDIB,\ncan be interpreted as an entropy-regularized and robustified version of K-means\nthat inherently favors a parsimonious number of informative clusters. By\napplying UDIB to the joint clustering of LLM prompt and response embeddings, we\ngenerate a shared topic representation that is not merely spatially coherent\nbut is fundamentally structured to be maximally informative about the\nprompt-response relationship. This provides a superior foundation for the SDM\nframework and offers a novel, more sensitive tool for detecting confabulations.", "AI": {"tldr": "本文通过提出UDIB算法改进了大型语言模型中的语义发散检测框架，该框架能在处理提示和响应时生成更具信息量和简化的话题表示，从而增强检测编造内容的能力。", "motivation": "传统的语义发散度量(SDM)框架依赖于识别提示和响应之间的潜在主题，通常通过应用于句子嵌入的几何聚类来实现。这种方法侧重于空间接近性而非信息论分析的下游需求。", "method": "本研究提出了一种基于确定性信息瓶颈(DIB)的几何聚类方法，通过将DIB方法转化为高维数据的实用算法，用计算效率高的上界替换其不可计算的KL散度项，从而解决了现有SDM框架中话题优化的空间邻近性和信息论分析需求之间的不匹配问题。", "result": "研究中产生的一种新方法UDIB，可以看作是一种熵正则化和鲁棒化的K-means，能够自动生成数量精简且最能提供关于提示-响应关系信息的话题表示。这为现有的SDM框架提供了更好的基础，并提供了一种检测编造更敏感的新型工具。", "conclusion": "通过将DIB方法转化为针对高维数据的实用版UDIB，并应用于大型语言模型(LLM)提示和响应嵌入的联合聚类，创造出一种不仅空间上一致而且本质上能够最大化提示-响应关系信息的话题表示，从而改善了SDM框架，使之成为检测编造回答的一种更敏感的工具。"}}
{"id": "2509.03786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03786", "abs": "https://arxiv.org/abs/2509.03786", "authors": ["Xinxin Wang", "Han Sun", "Ningzhong Liu", "Huiyu Zhou", "Yinan Yao"], "title": "SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection", "comment": "14pages, accepted by PRCV2025", "summary": "Underwater Camouflaged Object Detection (UCOD) aims to identify objects that\nblend seamlessly into underwater environments. This task is critically\nimportant to marine ecology. However, it remains largely underexplored and\naccurate identification is severely hindered by optical distortions, water\nturbidity, and the complex traits of marine organisms. To address these\nchallenges, we introduce the UCOD task and present DeepCamo, a benchmark\ndataset designed for this domain. We also propose Semantic Localization and\nEnhancement Network (SLENet), a novel framework for UCOD. We first benchmark\nstate-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet\nis built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)\nmodule and a Localization Guidance Branch (LGB) to enhance multi-scale feature\nrepresentation while generating a location map enriched with global semantic\ninformation. This map guides the Multi-Scale Supervised Decoder (MSSD) to\nproduce more accurate predictions. Experiments on our DeepCamo dataset and\nthree benchmark COD datasets confirm SLENet's superior performance over SOTA\nmethods, and underscore its high generality for the broader COD task.", "AI": {"tldr": "本文介绍了水下伪装对象检测（UCOD）任务，提出了一个基准数据集DeepCamo，并开发了一个名为SLENet的新框架，该框架能生成更精确的预测结果，并优于现有方法。", "motivation": "水下伪装对象检测（UCOD）是为了在水下环境中识别与周围环境融为一体的对象，这对于海洋生态学至关重要。但由于光的扭曲、水体浑浊及海洋生物的复杂性，这一任务面临巨大挑战。", "method": "我们提出了SLENet框架，该框架结合了Gamma-Asymmetric Enhancement (GAE) 模块和定位引导分支（LGB），用来增强多尺度特征表示并生成富含全局语义信息的位置图。位置图引导多尺度监督解码器（MSSD）生成更准确的预测结果。", "result": "实验结果表明，SLENet在DeepCamo数据集以及三个基准COD数据集上的表现优于现有方法，证明了其在更广泛的COD任务中的高度通用性。", "conclusion": "通过提出SLENet框架，研究解决了水下环境中的对象检测难题，并证明该方法的性能优于现有的COD模型，在广泛的水下伪装对象检测任务中具有高级别通用性。"}}
{"id": "2509.03535", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03535", "abs": "https://arxiv.org/abs/2509.03535", "authors": ["Ahmed Mubarak", "Amna Ahmed", "Amira Nasser", "Aya Mohamed", "Fares El-Sadek", "Mohammed Ahmed", "Ahmed Salah", "Youssef Sobhy"], "title": "QuesGenie: Intelligent Multimodal Question Generation", "comment": "7 pages, 8 figures, 12 tables. Supervised by Dr. Ahmed Salah and TA\n  Youssef Sobhy", "summary": "In today's information-rich era, learners have access to abundant educational\nresources, but the lack of practice materials tailored to these resources\npresents a significant challenge. This project addresses that gap by developing\na multi-modal question generation system that can automatically generate\ndiverse question types from various content formats. The system features four\nmajor components: multi-modal input handling, question generation,\nreinforcement learning from human feedback (RLHF), and an end-to-end\ninteractive interface. This project lays the foundation for automated,\nscalable, and intelligent question generation, carefully balancing resource\nefficiency, robust functionality and a smooth user experience.", "AI": {"tldr": "该项目开发了一个多模态问题生成系统，能从多种内容格式自动生成各种类型的问题，系统包含四个主要组件：多模态输入处理、问题生成、基于人类反馈的强化学习和一个端到端的交互界面，旨在实现自动化、可扩展且智能的问题生成，同时平衡资源效率、强大功能和用户友好性。", "motivation": "为了解决学习资源丰富但配套练习材料不足的问题，该项目开发了该多模态问题生成系统。", "method": "系统设计包含了多模态输入处理、问题生成模块、基于人类反馈的强化学习机制和一个整体的交互界面。", "result": "项目的成果是一个能够自动地、智能化地生成多种格式内容的问题生成系统，提高了资源的使用效率并提供了良好的用户体验。", "conclusion": "这个项目为自动、可扩展且智能的问题生成奠定了基础，能够有效解决当前学习资源与配套练习材料脱节的情况。"}}
{"id": "2509.03794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03794", "abs": "https://arxiv.org/abs/2509.03794", "authors": ["Juhun Lee", "Simon S. Woo"], "title": "Fitting Image Diffusion Models on Video Datasets", "comment": "ICCV25 Workshop", "summary": "Image diffusion models are trained on independently sampled static images.\nWhile this is the bedrock task protocol in generative modeling, capturing the\ntemporal world through the lens of static snapshots is information-deficient by\ndesign. This limitation leads to slower convergence, limited distributional\ncoverage, and reduced generalization. In this work, we propose a simple and\neffective training strategy that leverages the temporal inductive bias present\nin continuous video frames to improve diffusion training. Notably, the proposed\nmethod requires no architectural modification and can be seamlessly integrated\ninto standard diffusion training pipelines. We evaluate our method on the\nHandCo dataset, where hand-object interactions exhibit dense temporal coherence\nand subtle variations in finger articulation often result in semantically\ndistinct motions. Empirically, our method accelerates convergence by over\n2$\\text{x}$ faster and achieves lower FID on both training and validation\ndistributions. It also improves generative diversity by encouraging the model\nto capture meaningful temporal variations. We further provide an optimization\nanalysis showing that our regularization reduces the gradient variance, which\ncontributes to faster convergence.", "AI": {"tldr": "本文提出了一种通过利用视频中时间线索改进图像扩散模型训练的方法。该方法加速收敛速度并提高生成的多样性和质量。", "motivation": "现有的图像扩散模型训练仅依赖于独立采样的静态图像，这在捕捉时间变化时存在信息不足的问题，导致训练收敛慢、分布覆盖有限和泛化能力弱。本研究旨在解决这些问题。", "method": "提出了一种简单有效的训练策略，该策略利用连续视频帧中的时间归纳偏置来改进扩散模型训练。这种方法不需要对现有架构进行修改，可以无缝集成到现有的扩散模型训练流程中。", "result": "通过利用连续视频帧中的时间归纳偏置，本研究提出了一种简单有效的训练策略来改进扩散模型训练。该方法无需对架构进行修改，并且可以无缝集成到标准扩散训练流水线中。实验结果表明，该方法在HandCo数据集上能够加速2倍以上收敛速度，降低FID分数，并提高生成多样性。此外，优化分析表明，该正则化方法通过减少梯度方差有助于更快的收敛。", "conclusion": "实验证明，利用连续视频帧改进扩散模型训练的方法加速了2倍以上的收敛速度，降低了FID分数，并提高了生成多样性和捕捉时间变化的能力。优化分析显示该方法通过降低梯度方差贡献了更快的收敛速度。"}}
{"id": "2509.03537", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03537", "abs": "https://arxiv.org/abs/2509.03537", "authors": ["Cheng-Kai Yeh", "Hsing-Wang Lee", "Chung-Hung Kuo", "Hen-Hsen Huang"], "title": "AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models", "comment": "7 pages, accepted by CIKM 2025 as a short paper", "summary": "Abstraction--the ability to recognize and distill essential computational\npatterns from complex problem statements--is a foundational skill in computer\nscience, critical both for human problem-solvers and coding-oriented large\nlanguage models (LLMs). Despite recent advances in training LLMs for code\ngeneration using reinforcement learning (RL), most existing approaches focus\nprimarily on superficial pattern recognition, overlooking explicit training for\nabstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement\nLearning for Abstract Reasoning), a novel framework explicitly designed to\nenhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to\ntransform kernel problems into narrative-rich, challenging descriptions without\nchanging their fundamental logic. Simultaneously, a student coding model is\ntrained to solve these complex narrative problems by extracting their\nunderlying computational kernels. Experimental results demonstrate that AR$^2$\nsubstantially improves the student model's accuracy on previously unseen,\nchallenging programming tasks, underscoring abstraction as a key skill for\nenhancing LLM generalization.", "AI": {"tldr": "AR$^2$ enhances the abstraction abilities of LLMs through adversarial reinforcement learning, improving their accuracy on challenging programming tasks.", "motivation": "The motivation for this study is to improve abstraction, a crucial skill in computer science, which is not adequately addressed by the existing approaches focused on superficial pattern recognition in LLMs.", "method": "AR$^2$ (Adversarial Reinforcement Learning for Abstract Reasoning) is introduced to enhance the abstraction abilities of LLMs.", "result": "Experiments show that AR$^2$ significantly enhances the student model's performance on challenging programming tasks.", "conclusion": "AR$^2$ demonstrates the importance of focusing on abstraction to improve the generalization ability of LLMs."}}
{"id": "2509.03800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03800", "abs": "https://arxiv.org/abs/2509.03800", "authors": ["Yuheng Li", "Yenho Chen", "Yuxiang Lai", "Jike Zhong", "Vanessa Wildman", "Xiaofeng Yang"], "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting", "comment": null, "summary": "Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03540", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03540", "abs": "https://arxiv.org/abs/2509.03540", "authors": ["Shanglin Wu", "Lihui Liu", "Jinho D. Choi", "Kai Shu"], "title": "Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction", "comment": null, "summary": "Large Language Models (LLMs) often struggle with producing factually\nconsistent answers due to limitations in their parametric memory.\nRetrieval-Augmented Generation (RAG) methods address this issue by\nincorporating external knowledge from trusted sources at inference time.\nHowever, such methods typically treat knowledge as unstructured text, which\nlimits their ability to support compositional reasoning and identify factual\ninconsistencies. To overcome these limitations, we propose a novel framework\nthat dynamically constructs and expands knowledge graphs (KGs) during\ninference, integrating both internal knowledge extracted from LLMs and external\ninformation retrieved from external sources. Our method begins by extracting a\nseed KG from the question via prompting, followed by iterative expansion using\nthe LLM's latent knowledge. The graph is then selectively refined through\nexternal retrieval, enhancing factual coverage and correcting inaccuracies. We\nevaluate our approach on three diverse factual QA benchmarks, demonstrating\nconsistent improvements in factual accuracy, answer precision, and\ninterpretability over baseline prompting and static KG-augmented methods. Our\nfindings suggest that inference-time KG construction is a promising direction\nfor enhancing LLM factuality in a structured, interpretable, and scalable\nmanner.", "AI": {"tldr": "A new framework is proposed to dynamically build and expand knowledge graphs during inference, integrating internal and external knowledge for large language models (LLMs) to enhance factuality in responses.", "motivation": "The motivation behind this research is to improve the factuality of large language model (LLM) responses by addressing their difficulty in generating factually consistent answers due to limitations in parametric memory, and the compositional reasoning and factual inconsistency issues posed by traditional retrieval-augmented generation methods.", "method": "Our method starts by extracting a seed knowledge graph from the question using prompts, then iteratively expands the graph using the LLM's internal knowledge. It selectively refines the graph through external retrieval to enhance factual coverage and correct inaccuracies.", "result": "The approach was evaluated on three factual question-answering benchmarks, and it showed consistent improvements in factual accuracy, answer precision, and interpretability compared to baseline prompting and static KG-augmented methods.", "conclusion": "The dynamic construction and expansion of knowledge graphs during inference improve the factuality, precision, and interpretability of large language model answers, offering a structured, interpretable, and scalable enhancement to LLMs."}}
{"id": "2509.03803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03803", "abs": "https://arxiv.org/abs/2509.03803", "authors": ["Mengyu Gao", "Qiulei Dong"], "title": "Causality-guided Prompt Learning for Vision-language Models via Visual Granulation", "comment": "ICCV 2025 Accepted", "summary": "Prompt learning has recently attracted much attention for adapting\npre-trained vision-language models (e.g., CLIP) to downstream recognition\ntasks. However, most of the existing CLIP-based prompt learning methods only\nshow a limited ability for handling fine-grained datasets. To address this\nissue, we propose a causality-guided text prompt learning method via visual\ngranulation for CLIP, called CaPL, where the explored visual granulation\ntechnique could construct sets of visual granules for the text prompt to\ncapture subtle discrepancies among different fine-grained classes through\ncasual inference. The CaPL method contains the following two modules: (1) An\nattribute disentanglement module is proposed to decompose visual features into\nnon-individualized attributes (shared by some classes) and individualized\nattributes (specific to single classes) using a Brownian Bridge Diffusion\nModel; (2) A granule learning module is proposed to construct visual granules\nby integrating the aforementioned attributes for recognition under two causal\ninference strategies. Thanks to the learned visual granules, more\ndiscriminative text prompt is expected to be learned. Extensive experimental\nresults on 15 datasets demonstrate that our CaPL method significantly\noutperforms the state-of-the-art prompt learning methods, especially on\nfine-grained datasets.", "AI": {"tldr": "本文提出了一个通过视觉细化引导的因果提示学习方法CaPL，该方法在细粒度数据集上显著优于现有的提示学习方法。", "motivation": "现有的大多数基于CLIP的提示学习方法在处理细粒度数据集时表现有限，因此提出了一个通过视觉细化引导的因果提示学习方法。", "method": "CaPL方法包含两个模块：1. 使用布朗桥扩散模型将视觉特征分解为非个性化属性（被某些类别共享）和个人化属性（特定于单个类别）的属性解缠模块；2. 根据两种因果推理策略，通过集成上述属性来构建视觉颗粒的颗粒学习模块。", "result": "在15个数据集上的广泛实验结果显示，我们的CaPL方法显著优于最先进提示学习方法，尤其是在细粒度数据集上。", "conclusion": "通过学习到的视觉颗粒，CaPL方法能学习到更具有辨别性的文本提示，从而在细粒度识别任务上取得更好的效果。"}}
{"id": "2509.03565", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.03565", "abs": "https://arxiv.org/abs/2509.03565", "authors": ["Qi Chen", "Jingxuan Wei", "Zhuoya Yao", "Haiguang Wang", "Gaowei Wu", "Bihui Yu", "Siyuan Li", "Cheng Tan"], "title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference", "comment": "Accepted to ACM MM 2025", "summary": "Understanding how scientific ideas evolve requires more than summarizing\nindividual papers-it demands structured, cross-document reasoning over\nthematically related research. In this work, we formalize multi-document\nscientific inference, a new task that extracts and aligns motivation,\nmethodology, and experimental results across related papers to reconstruct\nresearch development chains. This task introduces key challenges, including\ntemporally aligning loosely structured methods and standardizing heterogeneous\nexperimental tables. We present ResearchPulse, an agent-based framework that\nintegrates instruction planning, scientific content extraction, and structured\nvisualization. It consists of three coordinated agents: a Plan Agent for task\ndecomposition, a Mmap-Agent that constructs motivation-method mind maps, and a\nLchart-Agent that synthesizes experimental line charts. To support this task,\nwe introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper\nclusters. Experiments show that our system, despite using 7B-scale agents,\nconsistently outperforms strong baselines like GPT-4o in semantic alignment,\nstructural consistency, and visual fidelity. The dataset are available in\nhttps://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.", "AI": {"tldr": "本文提出了多文档科学推理任务，旨在从相关论文中提取并整合动机、方法与实验结果，以重建研究发展链。为此设计了ResearchPulse框架，包括三个代理：计划代理、动机-方法心智图代理和实验线图合成代理。通过ResearchPulse-Bench基准测试，研究展示了其系统在语义对齐、结构一致性和视觉保真性上优于GPT-4等方法。", "motivation": "理解科学思想如何演变不仅仅是总结单篇论文，还需要进行结构化、跨越文档的研究推理。", "method": "提出了ResearchPulse框架，包含三个代理来处理任务分解、动机-方法心智图构建以及实验线图合成。", "result": "实验表明，尽管使用的是7B规模的代理，系统在语义对齐、结构一致性和视觉保真性方面超越了强大的基线方法。", "conclusion": "本文引入了一个新的科学研究任务和用于完成该任务的框架，展示了其在相关任务中的优越性能，并提供了用于评估的基准数据集。"}}
{"id": "2509.03808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03808", "abs": "https://arxiv.org/abs/2509.03808", "authors": ["Huanan Li", "Rui Fan", "Juntao Guan", "Weidong Hao", "Lai Rui", "Tong Wu", "Yikai Wang", "Lin Gu"], "title": "EGTM: Event-guided Efficient Turbulence Mitigation", "comment": null, "summary": "Turbulence mitigation (TM) aims to remove the stochastic distortions and\nblurs introduced by atmospheric turbulence into frame cameras. Existing\nstate-of-the-art deep-learning TM methods extract turbulence cues from multiple\ndegraded frames to find the so-called \"lucky'', not distorted patch, for \"lucky\nfusion''. However, it requires high-capacity network to learn from\ncoarse-grained turbulence dynamics between synchronous frames with limited\nframe-rate, thus fall short in computational and storage efficiency. Event\ncameras, with microsecond-level temporal resolution, have the potential to\nfundamentally address this bottleneck with efficient sparse and asynchronous\nimaging mechanism. In light of this, we (i) present the fundamental\n\\textbf{``event-lucky insight''} to reveal the correlation between turbulence\ndistortions and inverse spatiotemporal distribution of event streams. Then,\nbuild upon this insight, we (ii) propose a novel EGTM framework that extracts\npixel-level reliable turbulence-free guidance from the explicit but noisy\nturbulent events for temporal lucky fusion. Moreover, we (iii) build the first\nturbulence data acquisition system to contribute the first real-world\nevent-driven TM dataset. Extensive experimental results demonstrate that our\napproach significantly surpass the existing SOTA TM method by 710 times, 214\ntimes and 224 times in model size, inference latency and model complexity\nrespectively, while achieving the state-of-the-art in restoration quality\n(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating\nthe great efficiency merit of introducing event modality into TM task. Demo\ncode and data have been uploaded in supplementary material and will be released\nonce accepted.", "AI": {"tldr": "The paper presents a novel EGTM framework for turbulence mitigation leveraging event cameras, achieving significant improvements in efficiency and restoration quality over current state-of-the-art methods.", "motivation": "The motivation behind the research is to improve the computational and storage efficiency of turbulence mitigation (TM) techniques, which currently rely on high-capacity networks to handle coarse-grained turbulence dynamics.", "method": "The paper introduces the 'event-lucky insight' and proposes the EGTM framework to extract reliable turbulence-free guidance using event cameras, addressing the limitations of existing TM methods that require high-capacity networks for 'lucky fusion'.", "result": "The proposed approach outperforms existing state-of-the-art methods by 710 times in model size, 214 times in inference latency, and 224 times in model complexity, while achieving state-of-the-art restoration quality (+0.94 PSNR and +0.08 SSIM).", "conclusion": "The research introduces the first real-world event-driven TM dataset, built through a custom turbulence data acquisition system. The results demonstrate significant efficiency and quality improvements in TM when using event cameras."}}
{"id": "2509.03610", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03610", "abs": "https://arxiv.org/abs/2509.03610", "authors": ["Josh Wisoff", "Yao Tang", "Zhengyu Fang", "Jordan Guzman", "YuTang Wang", "Alex Yu"], "title": "NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management", "comment": null, "summary": "Note-taking is a critical practice for capturing, organizing, and reflecting\non information in both academic and professional settings. The recent success\nof large language models has accelerated the development of AI-assisted tools,\nyet existing solutions often struggle with efficiency. We present NoteBar, an\nAI-assisted note-taking tool that leverages persona information and efficient\nlanguage models to automatically organize notes into multiple categories and\nbetter support user workflows. To support research and evaluation in this\nspace, we further introduce a novel persona-conditioned dataset of 3,173 notes\nand 8,494 annotated concepts across 16 MBTI personas, offering both diversity\nand semantic richness for downstream tasks. Finally, we demonstrate that\nNoteBar can be deployed in a practical and cost-effective manner, enabling\ninteractive use without reliance on heavy infrastructure. Together, NoteBar and\nits accompanying dataset provide a scalable and extensible foundation for\nadvancing AI-assisted personal knowledge management.", "AI": {"tldr": "NoteBar是一款结合Persona信息和高效大型语言模型进行自动笔记分类的AI辅助工具，配有支持研究与评估的数据集，部署成本低且效率高。", "motivation": "鉴于大型语言模型成功的加速推动了AI辅助工具的发展，但现有解决方案在效率方面常常表现不佳，提出了NoteBar以期改善这一情况。", "method": "NoteBar采用大型语言模型，并结合Persona信息来自动对笔记进行分类和组织，旨在提高工作效率。同时，引入了一个新的带有16种MBTI人设注解的数据集，该数据集含有3,173条笔记和8,494个标注概念，以此支持研究与评估。", "result": "NoteBar能以实用且经济的方式部署，支持无需依赖大量基础设施的互动使用，显示出其作为AI辅助个人知识管理的基础功能。", "conclusion": "NoteBar及其配套数据集为推进AI辅助个人知识管理工作提供了一个可扩展、可扩展的基础。"}}
{"id": "2509.03872", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03872", "abs": "https://arxiv.org/abs/2509.03872", "authors": ["Nan Yang", "Yang Wang", "Zhanwen Liu", "Yuchao Dai", "Yang Liu", "Xiangmo Zhao"], "title": "Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection", "comment": null, "summary": "Existing RGB-Event detection methods process the low-information regions of\nboth modalities (background in images and non-event regions in event data)\nuniformly during feature extraction and fusion, resulting in high computational\ncosts and suboptimal performance. To mitigate the computational redundancy\nduring feature extraction, researchers have respectively proposed token\nsparsification methods for the image and event modalities. However, these\nmethods employ a fixed number or threshold for token selection, hindering the\nretention of informative tokens for samples with varying complexity. To achieve\na better balance between accuracy and efficiency, we propose FocusMamba, which\nperforms adaptive collaborative sparsification of multimodal features and\nefficiently integrates complementary information. Specifically, an Event-Guided\nMultimodal Sparsification (EGMS) strategy is designed to identify and\nadaptively discard low-information regions within each modality by leveraging\nscene content changes perceived by the event camera. Based on the\nsparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed\nto effectively capture and integrate complementary features from both\nmodalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate\nthat the proposed method achieves superior performance in both accuracy and\nefficiency compared to existing methods. The code will be available at\nhttps://github.com/Zizzzzzzz/FocusMamba.", "AI": {"tldr": "FocusMamba通过自适应稀疏化处理和交叉模态融合，提升了RGB-Event检测的准确性和效率。", "motivation": "当前的RGB事件检测方法在特征提取和融合过程中对低信息区域进行统一处理，这导致了较高的计算成本和次优的性能表现。为了解决计算冗余问题，研究者提出了针对图像和事件模态的token稀疏化方法，但是这些方法使用固定的阈值或数量选择token，无法保留不同复杂度样本的有用token信息。为此，他们在准确性与效率之间寻求更好的平衡。", "method": "研究者提出了FocusMamba方法，该方法通过自适应协作稀疏化多模态特征并在稀疏化的结果基础上融合图像和事件两种模态的互补信息，从而有效地捕获信息。具体地说，他们设计了一种基于事件相机感知的场景内容变化来识别并自适应地丢弃低信息区域的策略（EGMS），并通过交叉模态融合模块（CMFF）来提升检测性能。", "result": "实验结果证明，所提出的方法在DSEC-Det和PKU-DAVIS-SOD数据集上，在准确性和效率方面均优于现有方法。代码将在https://github.com/Zizzzzzzz/FocusMamba上提供。", "conclusion": "在DSEC-Det和PKU-DAVIS-SOD数据集上的实验表明，提出的方法在准确性与效率方面优于现有方法。"}}
{"id": "2509.03615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03615", "abs": "https://arxiv.org/abs/2509.03615", "authors": ["Aryan Gupta", "Anupam Purwar"], "title": "E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition", "comment": "Sprinklr OCR provides a fast and compute light way of performing OCR", "summary": "Optical Character Recognition (OCR) in multilingual, noisy, and diverse\nreal-world images remains a significant challenge for optical character\nrecognition systems. With the rise of Large Vision-Language Models (LVLMs),\nthere is growing interest in their ability to generalize and reason beyond\nfixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR\nsystem built specifically optimized for edge deployment in resource-constrained\nenvironments. We present a large-scale comparative evaluation of five\nstate-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two\ntraditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly\nhand annotated dataset of multilingual (54 languages) images. Our benchmark\ncovers a broad range of metrics including accuracy, semantic consistency,\nlanguage coverage, computational efficiency (latency, memory, GPU usage), and\ndeployment cost. To better reflect real-world applicability, we also conducted\nedge case deployment analysis, evaluating model performance on CPU only\nenvironments. Among the results, Qwen achieved the highest precision (0.54),\nwhile Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and\noutperformed others in efficiency, processing images 35 faster (0.17 seconds\nper image on average) and at less than 0.01 of the cost (0.006 USD per 1,000\nimages) compared to LVLM. Our findings demonstrate that the most optimal OCR\nsystems for edge deployment are the traditional ones even in the era of LLMs\ndue to their low compute requirements, low latency, and very high\naffordability.", "AI": {"tldr": "研究发现，尽管大型视觉语言模型在某些指标上表现突出，但在资源受限的边缘部署场景中，传统OCR系统因其低计算需求、低延迟和高性价比，仍然是最优选择。", "motivation": "研究的动机在于评估大视觉语言模型在多语言、嘈杂和多样的现实世界图像中的OCR能力，并对比传统的OCR系统，尤其是在资源受限的边缘部署环境下的性能。", "method": "本文介绍了一种名为Sprinklr-Edge-OCR的新型OCR系统，该系统专门针对资源受限环境进行了优化。研究团队对五种先进的大视觉语言模型（LVLM）和两种传统的OCR系统进行了大规模的对比评估。评估基于一个专有的双手动注释数据集，该数据集包含了54种语言的图像。", "result": "Qwen在精度上表现最佳（精度为0.54），而Sprinklr-Edge-OCR在整体F1得分上表现最好（0.46），并且在效率方面优于其他模型，图像处理速度加快35%，每处理1000张图像的成本仅为0.006美元，远低于LVLM。", "conclusion": "研究结论表明，即使在大型语言模型时代，传统的OCR系统仍是边缘部署最优化的选择，因其低计算资源需求、低延迟和高性价比。"}}
{"id": "2509.03873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03873", "abs": "https://arxiv.org/abs/2509.03873", "authors": ["Jiajun Song", "Xiaoou Liu"], "title": "SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition", "comment": "34th International Conference on Artificial Neural Networks - ICANN\n  2025", "summary": "Food recognition has gained significant attention, but the rapid emergence of\nnew dishes requires methods for recognizing unseen food categories, motivating\nZero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot\nFood Recognition (CZSFR), where cuisines and ingredients naturally align with\nattributes and objects in Compositional Zero-Shot learning (CZSL). However,\nCZSFR faces three challenges: (1) Redundant background information distracts\nmodels from learning meaningful food features, (2) Role confusion between\nstaple and side dishes leads to misclassification, and (3) Semantic bias in a\nsingle attribute can lead to confusion of understanding. Therefore, we propose\nSalientFusion, a context-aware CZSFR method with two components: SalientFormer,\nwhich removes background redundancy and uses depth features to resolve role\nconfusion; DebiasAT, which reduces the semantic bias by aligning prompts with\nvisual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we\nshow that SalientFusion achieves state-of-the-art results on these benchmarks\nand the most popular general datasets for the general CZSL. The code is\navaliable at https://github.com/Jiajun-RUC/SalientFusion.", "AI": {"tldr": "文章提出了一种新的组合零样本食物识别方法——SalientFusion，它解决了食物识别中背景冗余、角色混淆和语义偏差的问题，表现优于现有方法。", "motivation": "现有方法难以识别未见过的食物类别，因此提出了零样本食物识别（ZSFL）以及组合零样本食物识别（CZSFR）任务，旨在识别包含新组合食物类别。", "method": "研究提出了SalientFusion方法，包括两个组件：SalientFormer用于移除冗余背景并使用深度特征解决主食和配菜角色混淆问题；DebiasAT通过调整提示和视觉特征的对齐来减少语义偏差。", "result": "在作者提出的两个基准数据集CZSFood-90和CZSFood-164以及通用组合零样本学习最受欢迎的数据集上，通过使用SalientFusion，达到了最先进的结果。", "conclusion": "SalientFusion是一种语境感知的组合零样本食物识别方法，能有效解决食物识别中的背景冗余、角色混淆和语义偏差等挑战。"}}
{"id": "2509.03647", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03647", "abs": "https://arxiv.org/abs/2509.03647", "authors": ["Dani Roytburg", "Matthew Bozoukov", "Matthew Nguyen", "Jou Barzdukas", "Simon Fu", "Narmeen Oozeer"], "title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators", "comment": null, "summary": "Large language models (LLMs) increasingly serve as automated evaluators, yet\nthey suffer from \"self-preference bias\": a tendency to favor their own outputs\nover those of other models. This bias undermines fairness and reliability in\nevaluation pipelines, particularly for tasks like preference tuning and model\nrouting. We investigate whether lightweight steering vectors can mitigate this\nproblem at inference time without retraining. We introduce a curated dataset\nthat distinguishes self-preference bias into justified examples of\nself-preference and unjustified examples of self-preference, and we construct\nsteering vectors using two methods: Contrastive Activation Addition (CAA) and\nan optimization-based approach. Our results show that steering vectors can\nreduce unjustified self-preference bias by up to 97\\%, substantially\noutperforming prompting and direct preference optimization baselines. Yet\nsteering vectors are unstable on legitimate self-preference and unbiased\nagreement, implying self-preference spans multiple or nonlinear directions.\nThis underscores both their promise and limits as safeguards for LLM-as-judges\nand motivates more robust interventions.", "AI": {"tldr": "论文研究了如何通过使用引导向量减少大型语言模型在评估任务中的自我偏好偏差，结果显示其在减少不合理的自我偏好偏差方面非常有效，但对合理自我偏好和无偏见一致性的影响不稳定。", "motivation": "研究目的是探讨在推理阶段使用轻量级引导向量是否能够减少大型语言模型的自我偏好偏差，而无需重新训练模型。", "method": "我们引入了一个精选的数据集，将自我偏好偏差区分为合理的自我偏好示例和不合理的自我偏好示例，并使用两种方法构建了引导向量：对比激活增强（CAA）和基于优化的方法。", "result": "实验结果显示，引导向量可以将不合理的自我偏好偏差减少高达97%，远超过提示方法和直接偏好优化基线。然而，引导向量在合理的自我偏好和无偏见的一致性方面不稳定，这表明自我偏好跨越了多个或非线性方向。", "conclusion": "研究表明，引导向量作为保护措施在减少大型语言模型作为评估器时的自我偏好偏差方面显示出潜力和局限性，这推动了更加强大的干预措施的研究。"}}
{"id": "2509.03883", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.03883", "abs": "https://arxiv.org/abs/2509.03883", "authors": ["Haiwei Xue", "Xiangyang Luo", "Zhanghao Hu", "Xin Zhang", "Xunzhi Xiang", "Yuqin Dai", "Jianzhuang Liu", "Zhensong Zhang", "Minglei Li", "Jian Yang", "Fei Ma", "Zhiyong Wu", "Changpeng Yang", "Zonghong Dai", "Fei Richard Yu"], "title": "Human Motion Video Generation: A Survey", "comment": "Accepted by TPAMI. Github Repo:\n  https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation IEEE Access:\n  https://ieeexplore.ieee.org/document/11106267", "summary": "Human motion video generation has garnered significant research interest due\nto its broad applications, enabling innovations such as photorealistic singing\nheads or dynamic avatars that seamlessly dance to music. However, existing\nsurveys in this field focus on individual methods, lacking a comprehensive\noverview of the entire generative process. This paper addresses this gap by\nproviding an in-depth survey of human motion video generation, encompassing\nover ten sub-tasks, and detailing the five key phases of the generation\nprocess: input, motion planning, motion video generation, refinement, and\noutput. Notably, this is the first survey that discusses the potential of large\nlanguage models in enhancing human motion video generation. Our survey reviews\nthe latest developments and technological trends in human motion video\ngeneration across three primary modalities: vision, text, and audio. By\ncovering over two hundred papers, we offer a thorough overview of the field and\nhighlight milestone works that have driven significant technological\nbreakthroughs. Our goal for this survey is to unveil the prospects of human\nmotion video generation and serve as a valuable resource for advancing the\ncomprehensive applications of digital humans. A complete list of the models\nexamined in this survey is available in Our Repository\nhttps://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03662", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03662", "abs": "https://arxiv.org/abs/2509.03662", "authors": ["Ali Noori", "Somya Mohanty", "Prashanti Manda"], "title": "Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV", "comment": null, "summary": "Clinical notes contain rich clinical narratives but their unstructured format\nposes challenges for large-scale analysis. Standardized terminologies such as\nSNOMED CT improve interoperability, yet understanding how concepts relate\nthrough co-occurrence and semantic similarity remains underexplored. In this\nstudy, we leverage the MIMIC-IV database to investigate the relationship\nbetween SNOMED CT concept co-occurrence patterns and embedding-based semantic\nsimilarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained\nembeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently\nco-occurring concepts are also semantically close, whether embeddings can\nsuggest missing concepts, and how these relationships evolve temporally and\nacross specialties. Our analyses reveal that while co-occurrence and semantic\nsimilarity are weakly correlated, embeddings capture clinically meaningful\nassociations not always reflected in documentation frequency. Embedding-based\nsuggestions frequently matched concepts later documented, supporting their\nutility for augmenting clinical annotations. Clustering of concept embeddings\nyielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular\nconditions) that map to patient phenotypes and care patterns. Finally,\nco-occurrence patterns linked to outcomes such as mortality and readmission\ndemonstrate the practical utility of this approach. Collectively, our findings\nhighlight the complementary value of co-occurrence statistics and semantic\nembeddings in improving documentation completeness, uncovering latent clinical\nrelationships, and informing decision support and phenotyping applications.", "AI": {"tldr": "研究通过分析临床笔记中的SNOMED CT概念，利用NPMI和预训练嵌入，探讨了概念共现和语义相似性之间的关系，并展示了这种方法在改善临床记录完整性和支持决策方面的效用。", "motivation": "临床笔记包含丰富的临床叙述，但其非结构化格式给大规模分析带来了挑战。标准化术语，如SNOMED CT，提高了互操作性，但理解概念之间的共现关系和语义相似性仍然研究不够。本研究旨在解决这一问题。", "method": "本研究通过利用MIMIC-IV数据库，旨在研究SNOMED CT概念的共现模式和基于嵌入的语义相似性之间的关系。采用归一化点互信息(NPMI)和预训练嵌入（例如ClinicalBERT，BioBERT），研究了频率共现的概念是否在语义上接近，嵌入能否提供缺失的概念，以及这些关系随着时间的推移和在不同的专业领域中的变化。", "result": "研究结果表明，尽管共现和语义相似性之间存在弱相关性，但嵌入捕捉到的临床相关性不一定反映在文档频率中。基于嵌入的建议经常匹配后来记录的概念，支持其用于增强临床注释的效用。概念嵌入的聚类产生了连贯的临床主题（如症状、实验室结果、诊断和心血管状况），这些与患者的临床表型和护理模式相匹配。最后，共现模式与如死亡率和再入院等结果相关，证明了这种方法的实际效用。", "conclusion": "我们发现，共现统计和语义嵌入在提高文件完整性、揭示潜在临床关系以及为决策支持和表型应用提供信息方面具有互补价值。"}}
{"id": "2509.03887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03887", "abs": "https://arxiv.org/abs/2509.03887", "authors": ["Bu Jin", "Songen Gu", "Xiaotao Hu", "Yupeng Zheng", "Xiaoyang Guo", "Qian Zhang", "Xiaoxiao Long", "Wei Yin"], "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction", "comment": null, "summary": "In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.", "AI": {"tldr": "研究提出了OccTENS，一种能够进行高效、高保真长期占用生成的生成占用世界模型。通过重新定义为TENS任务和引入TensFormer，解决了现有模型的时间衰减及缺乏可控性的问题。", "motivation": "由于基于自回归（AR）的方法在长期生成时效率低下、随时间衰减并且缺乏可控性，因此提出了OccTENS，以在保持计算效率的同时实现可控和高质量的长期占用生成。", "method": "OccTENS通过将占用世界模型重新定义为时空下一尺度预测（TENS）任务来解决现有问题，该任务将时间序列建模问题分解为空间尺度生成和时间场景预测。使用TensFormer，可以有效地管理和建模占用序列的时间因果关系和空间关系。为了增强姿态可控性，提出了一个整体的姿态聚集策略，该策略为占用和自我运动提供统一的序列建模。", "result": "实验结果表明，与现有最先进方法相比，OccTENS不仅具有更高的占用质量，而且推理速度更快。", "conclusion": "OccTENS通过将问题分解为更易于处理的子任务以及使用TensFormer来管理时间和空间关系，成功解决了长期占用生成中的效率低下、衰减和缺乏可控性的问题，实验结果表明它具有更高的质量以及更快的推理速度。"}}
{"id": "2509.03725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03725", "abs": "https://arxiv.org/abs/2509.03725", "authors": ["Parush Gera", "Tempestt Neal"], "title": "MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection", "comment": null, "summary": "We present the novel approach for stance detection across domains and\ntargets, Metric Learning-Based Few-Shot Learning for Cross-Target and\nCross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with\ntriplet loss to capture semantic similarities and differences between stance\ntargets, enhancing domain adaptation. By constructing a discriminative\nembedding space, MLSD allows a cross-target or cross-domain stance detection\nmodel to acquire useful examples from new target domains. We evaluate MLSD in\nmultiple cross-target and cross-domain scenarios across two datasets, showing\nstatistically significant improvement in stance detection performance across\nsix widely used stance detection models.", "AI": {"tldr": "本文提出并验证了一种跨领域和目标立场检测新方法MLSD，该方法有助于增强模型在多样环境下的表现。", "motivation": "为了实现更有效的跨领域和跨目标的立场检测，提升模型在新领域中迁移学习的能力，作者提出此方法以增强模型在不同环境下的表现。", "method": "MLSD利用度量学习和三元损失技术，构建了判别性嵌入空间，这种方法有助于在新领域中获取有用的特征实例，提升模型适应跨领域和跨目标情境的能力。", "result": "本文提出了一种新的跨领域和目标立场检测方法Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection (MLSD)，利用三元损失的度量学习来捕捉立场目标之间的语义相似性和差异，增强了领域的适应性。通过建立判别嵌入空间，MLSD允许跨目标或跨领域的立场检测模型从新的目标领域中获取有用的例子。我们在两个数据集中的多种跨目标和跨领域场景中对MLSD进行了评估，结果显示，MMLSD在六种常用的立场检测模型上立场检测性能有统计学显著的提高。", "conclusion": "实验结果表明，相比于六种常用的立场检测模型，MLSD在多项跨目标和跨领域的场景中展示了显著的性能提升。这表明使用三元损失的度量学习方法能够有效提升立场检测系统的性能。"}}
{"id": "2509.03893", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03893", "abs": "https://arxiv.org/abs/2509.03893", "authors": ["Stefan Stojanov", "Linan Zhao", "Yunzhi Zhang", "Daniel L. K. Yamins", "Jiajun Wu"], "title": "Weakly-Supervised Learning of Dense Functional Correspondences", "comment": "Accepted at ICCV 2025. Project website:\n  https://dense-functional-correspondence.github.io/", "summary": "Establishing dense correspondences across image pairs is essential for tasks\nsuch as shape reconstruction and robot manipulation. In the challenging setting\nof matching across different categories, the function of an object, i.e., the\neffect that an object can cause on other objects, can guide how correspondences\nshould be established. This is because object parts that enable specific\nfunctions often share similarities in shape and appearance. We derive the\ndefinition of dense functional correspondence based on this observation and\npropose a weakly-supervised learning paradigm to tackle the prediction task.\nThe main insight behind our approach is that we can leverage vision-language\nmodels to pseudo-label multi-view images to obtain functional parts. We then\nintegrate this with dense contrastive learning from pixel correspondences to\ndistill both functional and spatial knowledge into a new model that can\nestablish dense functional correspondence. Further, we curate synthetic and\nreal evaluation datasets as task benchmarks. Our results demonstrate the\nadvantages of our approach over baseline solutions consisting of off-the-shelf\nself-supervised image representations and grounded vision language models.", "AI": {"tldr": "本文提出了一种弱监督学习方法，使用视觉语言模型来伪标签化多视角图像的功能部件，结合密集对比学习来建立密集的功能对应关系。", "motivation": "在不同类别间匹配图像时，通过对象的功能来指导对应关系的建立，因为用于特定功能的对象部件在形状和外观上往往具有相似性。", "method": "基于视觉语言模型的弱监督学习方法，通过伪标签多视角图像来识别功能部件，并结合密集对比学习来提炼功能和空间知识，以建立密集的功能对应关系。", "result": "实验结果表明，该方法优于基于现成的自监督图像表示和基于视觉语言模型的基线解决方案。", "conclusion": "本文提出的方法在合成和真实数据集上均表现出色，证明其能有效建立不同类别物体间的密集对应关系。"}}
{"id": "2509.03791", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03791", "abs": "https://arxiv.org/abs/2509.03791", "authors": ["Saki Imai", "Mert İnan", "Anthony Sicilia", "Malihe Alikhani"], "title": "SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation", "comment": null, "summary": "Evaluating sign language generation is often done through back-translation,\nwhere generated signs are first recognized back to text and then compared to a\nreference using text-based metrics. However, this two-step evaluation pipeline\nintroduces ambiguity: it not only fails to capture the multimodal nature of\nsign language-such as facial expressions, spatial grammar, and prosody-but also\nmakes it hard to pinpoint whether evaluation errors come from sign generation\nmodel or the translation system used to assess it. In this work, we propose\nSiLVERScore, a novel semantically-aware embedding-based evaluation metric that\nassesses sign language generation in a joint embedding space. Our contributions\ninclude: (1) identifying limitations of existing metrics, (2) introducing\nSiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness\nto semantic and prosodic variations, and (4) exploring generalization\nchallenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore\nachieves near-perfect discrimination between correct and random pairs (ROC AUC\n= 0.99, overlap < 7%), substantially outperforming traditional metrics.", "AI": {"tldr": "本论文提出了一种新的手语生成评估方法SiLVERScore，针对传统评估方法中存在的多模态信息无法捕捉及评估误差来源不明确的问题，利用一个联合嵌入空间进行语义感知的评估。", "motivation": "现有的手语生成评估方法通过将生成的手语重新识别为文本并与参考文本进行比较，这种方法引入了二步评估流程的问题，难以准确反映手语的多模态性质，并且无法明确评估误差的来源是生成模型还是评价系统。", "method": "我们提出了一种名为SiLVERScore的新方法，它是一种基于嵌入的评价指标，能够在一个联合嵌入空间中评估手语生成的有效性。这一方法能够考虑语义信息，更加准确地评价手语生成的质量。", "result": "在PHOENIX-14T和CSL-Daily数据集上，SiLVERScore在正确与随机手语对之间的区分度上取得了接近完美的表现（ROC AUC = 0.99，重叠度小于7%），大大优于传统评估方法。", "conclusion": "通过对手语生成的质量进行语义感知的评价，SiLVERScore解决了传统评估方式的局限性，并且在多种数据集上展示了其可靠性和应用性能。"}}
{"id": "2509.03895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03895", "abs": "https://arxiv.org/abs/2509.03895", "authors": ["Phuoc-Nguyen Bui", "Khanh-Binh Nguyen", "Hyunseung Choo"], "title": "Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model", "comment": "ICCV 2025 - LIMIT Workshop", "summary": "Contrastive vision-language models excel in zero-shot image recognition but\nface challenges in few-shot scenarios due to computationally intensive offline\nfine-tuning using prompt learning, which risks overfitting. To overcome these\nlimitations, we propose Attn-Adapter, a novel online few-shot learning\nframework that enhances CLIP's adaptability via a dual attention mechanism. Our\ndesign incorporates dataset-specific information through two components: the\nMemory Attn-Adapter, which refines category embeddings using support examples,\nand the Local-Global Attn-Adapter, which enriches image embeddings by\nintegrating local and global features. This architecture enables dynamic\nadaptation from a few labeled samples without retraining the base model.\nAttn-Adapter outperforms state-of-the-art methods in cross-category and\ncross-dataset generalization, maintaining efficient inference and scaling\nacross CLIP backbones.", "AI": {"tldr": "提出Attn-Adapter框架，通过双注意力机制解决视觉-语言模型在少样本场景中的问题，提升泛化性能。", "motivation": "解决对比视觉-语言模型在少样本场景中的计算密集型离线微调风险和过拟合问题。", "method": "提出了Attn-Adapter框架，该框架通过双注意力机制来增强CLIP的适应性，包含两个组件：Memory Attn-Adapter和Local-Global Attn-Adapter。", "result": "Attn-Adapter在跨类别和跨数据集泛化方面超越了最先进的方法，同时保持高效推断并在CLIP骨干网络中扩展。", "conclusion": "通过提出的Attn-Adapter框架，成功克服了少样本场景中的这些问题，实现了更好的跨类别和跨数据集泛化性能。"}}
{"id": "2509.03805", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03805", "abs": "https://arxiv.org/abs/2509.03805", "authors": ["Saki Imai", "Mert İnan", "Anthony Sicilia", "Malihe Alikhani"], "title": "Measuring How (Not Just Whether) VLMs Build Common Ground", "comment": null, "summary": "Large vision language models (VLMs) increasingly claim reasoning skills, yet\ncurrent benchmarks evaluate them in single-turn or question answering settings.\nHowever, grounding is an interactive process in which people gradually develop\nshared understanding through ongoing communication. We introduce a four-metric\nsuite (grounding efficiency, content alignment, lexical adaptation, and\nhuman-likeness) to systematically evaluate VLM performance in interactive\ngrounding contexts. We deploy the suite on 150 self-play sessions of\ninteractive referential games between three proprietary VLMs and compare them\nwith human dyads. All three models diverge from human patterns on at least\nthree metrics, while GPT4o-mini is the closest overall. We find that (i) task\nsuccess scores do not indicate successful grounding and (ii) high\nimage-utterance alignment does not necessarily predict task success. Our metric\nsuite and findings offer a framework for future research on VLM grounding.", "AI": {"tldr": "针对视觉语言模型在交互式情境下的grounding能力评估，提出了一个新的评价体系，结果显示某些模型与人类的实际表现存在较大差距。这套新体系为今后的研究指明了方向。", "motivation": "当前的基准评估在单轮或问答设置中进行，然而，grounding是一个交互过程，在这个过程中，人们通过持续沟通逐步建立共同理解。", "method": "介绍了一个四指标套件（grounding效率、内容对齐、词汇适应性和类人性）来系统评估视觉语言模型（VLMs）在交互式grounding场景中的表现。通过部署这个套件，对150个自玩互动参照游戏会话中的三个专有VLMs进行评估，并将它们与人类二元组进行了比较。", "result": "所有三个模型在至少三个指标上偏离了人类模式，而GPT4o-mini总体上最为接近。发现（i）任务成功得分并不能表明成功的grounding；（ii）高图像-话语对齐并不一定预示任务成功。", "conclusion": "这套指标体系和发现为未来关于VLM ground的研究提供了一个框架。"}}
{"id": "2509.03897", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03897", "abs": "https://arxiv.org/abs/2509.03897", "authors": ["Xiaofu Chen", "Israfel Salazar", "Yova Kementchedjhieva"], "title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation", "comment": null, "summary": "As interest grows in generating long, detailed image captions, standard\nevaluation metrics become increasingly unreliable. N-gram-based metrics though\nefficient, fail to capture semantic correctness. Representational Similarity\n(RS) metrics, designed to address this, initially saw limited use due to high\ncomputational costs, while today, despite advances in hardware, they remain\nunpopular due to low correlation to human judgments. Meanwhile, metrics based\non large language models (LLMs) show strong correlation with human judgments,\nbut remain too expensive for iterative use during model development.\n  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS\nmetric tailored to long image captioning. SPECS modifies CLIP with a new\nobjective that emphasizes specificity: rewarding correct details and penalizing\nincorrect ones. We show that SPECS matches the performance of open-source\nLLM-based metrics in correlation to human judgments, while being far more\nefficient. This makes it a practical alternative for iterative checkpoint\nevaluation during image captioning model development.Our code can be found at\nhttps://github.com/mbzuai-nlp/SPECS.", "AI": {"tldr": "本论文介绍了一种新的评估方法SPECS，用于长图像描述任务，其优点在于效率高且能有效地评估描述的准确性。", "motivation": "随着生成长且详细的图像描述的需求增加，标准的评估指标变得越来越不可靠。虽然基于大型语言模型的指标与人类判断有很强的相关性，但其成本很高，不利于模型开发过程中的迭代使用。因此，本研究旨在寻找一个与人类判断相关性强且计算成本低的解决方案。", "method": "本研究提出了SPECS（具体性增强的CLIPScore），这是一种无参考的相似性度量方法，专门用于长图像描述任务。SPECS通过新目标修改了CLIP模型，强调了对正确细节的奖励和对不正确细节的惩罚。", "result": "实验结果表明，SPECS在与基于大型语言模型的开源评估指标的人类判断相关性上达到了相似的性能，但计算效率更高。", "conclusion": "SPECS是用于图像描述模型开发过程中迭代检查的有效替代方案，因为它既保持了与人类判断的相关性，又提高了计算效率。"}}
{"id": "2509.03809", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03809", "abs": "https://arxiv.org/abs/2509.03809", "authors": ["Jiaxin Guo", "Daimeng Wei", "Yuanchang Luo", "Xiaoyu Chen", "Zhanglin Wu", "Huan Yang", "Hengchao Shang", "Zongyao Li", "Zhiqiang Rao", "Jinlong Yang", "Hao Yang"], "title": "Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation", "comment": "under preview", "summary": "Large language models (LLMs) have ushered in a new era for document-level\nmachine translation (\\textit{doc}-mt), yet their whole-document outputs\nchallenge existing evaluation methods that assume sentence-by-sentence\nalignment. We introduce \\textit{\\textbf{Align-then-Slide}}, a complete\nevaluation framework for ultra-long doc-mt. In the Align stage, we\nautomatically infer sentence-level source-target correspondences and rebuild\nthe target to match the source sentence number, resolving omissions and\nmany-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we\ncalculate averaged metric scores under 1-, 2-, 3- and 4-chunk for\nmulti-granularity assessment. Experiments on the WMT benchmark show a Pearson\ncorrelation of 0.929 between our method with expert MQM rankings. On a newly\ncurated real-world test set, our method again aligns closely with human\njudgments. Furthermore, preference data produced by Align-then-Slide enables\neffective CPO training and its direct use as a reward model for GRPO, both\nyielding translations preferred over a vanilla SFT baseline. The results\nvalidate our framework as an accurate, robust, and actionable evaluation tool\nfor doc-mt systems.", "AI": {"tldr": "提出了一种新框架Align-then-Slide用于超长文档级机器翻译评估，该框架包括自动句子级对齐和多粒度滑动评估两个阶段，能有效解决大型语言模型在doc-mt中的评估挑战，显示出良好的与人工评价一致性和翻译质量提升。", "motivation": "大型语言模型促进了文档级机器翻译的发展，但挑战了假设句与句对齐的现有评估方法。这个问题促使研究者开发一种新的、更适合超长文本翻译评估的方法。", "method": "介绍了一种名为Align-then-Slide的评估框架，用于超长文档级机器翻译的评估。该框架包含两个阶段：对齐阶段，自动推断源语言和目标语言句子级别的对应关系，并调整目标语言句子数量，解决漏译和多对一或多对多的问题；n-Chunk滑动评估阶段，计算1-, 2-, 3-, 4-chunk的平均评估分数，以实现多粒度评估。", "result": "实验结果显示，该方法在WMT基准数据集上和专家MQM排名之间的皮尔逊相关系数为0.929。在新构建的真实世界测试集上，该方法也与人类判断高度一致。此外，Align-then-Slide生成的偏好数据可用于CPO训练以及作为GRPO的奖励模型，从而生成更受欢迎的翻译结果。", "conclusion": "该研究打造了一种准确、稳健且有助于采取行动的doc-mt系统评估工具。通过该框架，可以有效解决超长文本翻译中的对齐和评估问题。"}}
{"id": "2509.03903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03903", "abs": "https://arxiv.org/abs/2509.03903", "authors": ["Yuanfeng Ji", "Dan Lin", "Xiyue Wang", "Lu Zhang", "Wenhui Zhou", "Chongjian Ge", "Ruihang Chu", "Xiaoli Yang", "Junhan Zhao", "Junsong Chen", "Xiangde Luo", "Sen Yang", "Jin Fang", "Ping Luo", "Ruijiang Li"], "title": "A Generative Foundation Model for Chest Radiography", "comment": null, "summary": "The scarcity of well-annotated diverse medical images is a major hurdle for\ndeveloping reliable AI models in healthcare. Substantial technical advances\nhave been made in generative foundation models for natural images. Here we\ndevelop `ChexGen', a generative vision-language foundation model that\nintroduces a unified framework for text-, mask-, and bounding box-guided\nsynthesis of chest radiographs. Built upon the latent diffusion transformer\narchitecture, ChexGen was pretrained on the largest curated chest X-ray dataset\nto date, consisting of 960,000 radiograph-report pairs. ChexGen achieves\naccurate synthesis of radiographs through expert evaluations and quantitative\nmetrics. We demonstrate the utility of ChexGen for training data augmentation\nand supervised pretraining, which led to performance improvements across\ndisease classification, detection, and segmentation tasks using a small\nfraction of training data. Further, our model enables the creation of diverse\npatient cohorts that enhance model fairness by detecting and mitigating\ndemographic biases. Our study supports the transformative role of generative\nfoundation models in building more accurate, data-efficient, and equitable\nmedical AI systems.", "AI": {"tldr": "本文提出 ChexGen，这是一种用于生成胸部X光片的生成式视觉-语言基础模型。通过结合文本、掩码和边界框信息，ChexGen 能合成高质量的胸部X光片，并通过少量数据实现多个医疗任务的性能改进。", "motivation": "医疗图像的多样性和高质量标注稀缺问题是开发可靠医疗AI模型的关键障碍。本文解决了这个问题，通过创建一个能够生成高质量胸部X光图像的模型，以克服图像数据缺乏的问题。", "method": "开发了名为 ChexGen 的生成式视觉语言基础模型，该模型引入了用于文本引导、掩码引导和边界框引导的胸部X光片合成的统一框架。ChexGen 基于潜在扩散变压器架构，并在迄今最大的整理胸部X光数据集上进行了预训练，该数据集包含960,000张X光片-报告配对。", "result": "ChexGen 通过专家评估和定量指标实现了精确的图像合成，并通过少量训练数据实现了疾病分类、检测和分割任务的性能提升。此外，它还能够创建多样化的患者队列，通过检测和减轻人口统计学偏差来增强模型的公平性。", "conclusion": "研究表明，生成式基础模型在构建更准确、数据高效且公平的医学AI系统方面具有变革作用。"}}
{"id": "2509.03829", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03829", "abs": "https://arxiv.org/abs/2509.03829", "authors": ["Huhong Xian", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "title": "NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation", "comment": null, "summary": "Different from traditional sentence-level audio deepfake detection (ADD),\npartial audio deepfake detection (PADD) requires frame-level positioning of the\nlocation of fake speech. While some progress has been made in this area,\nleveraging semantic information from audio, especially named entities, remains\nan underexplored aspect. To this end, we propose NE-PADD, a novel method for\nPartial Audio Deepfake Detection (PADD) that leverages named entity knowledge\nthrough two parallel branches: Speech Name Entity Recognition (SpeechNER) and\nPADD. The approach incorporates two attention aggregation mechanisms: Attention\nFusion (AF) for combining attention weights and Attention Transfer (AT) for\nguiding PADD with named entity semantics using an auxiliary loss. Built on the\nPartialSpoof-NER dataset, experiments show our method outperforms existing\nbaselines, proving the effectiveness of integrating named entity knowledge in\nPADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.", "AI": {"tldr": "NE-PADD proposes a new approach for partial audio deepfake detection that uses two attention mechanisms to incorporate named entity knowledge, achieving superior performance on the PartialSpoof-NER dataset.", "motivation": "To address the underexplored area of leveraging semantic information, particularly named entities, for frame-level positioning in partial audio deepfake detection.", "method": "Our method, NE-PADD, comprises two parallel branches: SpeechNER and PADD, incorporating two attention mechanisms: AF and AT to leverage named entity knowledge for frame-level fake speech detection.", "result": "Experiments on PartialSpoof-NER dataset show that NE-PADD outperforms existing baselines, demonstrating the effectiveness of integrating named entity knowledge.", "conclusion": "The integration of named entity knowledge through the proposed NE-PADD method proves effective in enhancing the performance of partial audio deepfake detection."}}
