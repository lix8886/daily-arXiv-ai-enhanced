{"id": "2508.16579", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16579", "abs": "https://arxiv.org/abs/2508.16579", "authors": ["Yansong Du", "Yutong Deng", "Yuting Zhou", "Feiyu Jiao", "Jian Song", "Xun Guan"], "title": "Towards High-Precision Depth Sensing via Monocular-Aided iToF and RGB Integration", "comment": "7 pages, 5 figures", "summary": "This paper presents a novel iToF-RGB fusion framework designed to address the\ninherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as\nlow spatial resolution, limited field-of-view (FoV), and structural distortion\nin complex scenes. The proposed method first reprojects the narrow-FoV iToF\ndepth map onto the wide-FoV RGB coordinate system through a precise geometric\ncalibration and alignment module, ensuring pixel-level correspondence between\nmodalities. A dual-encoder fusion network is then employed to jointly extract\ncomplementary features from the reprojected iToF depth and RGB image, guided by\nmonocular depth priors to recover fine-grained structural details and perform\ndepth super-resolution. By integrating cross-modal structural cues and depth\nconsistency constraints, our approach achieves enhanced depth accuracy,\nimproved edge sharpness, and seamless FoV expansion. Extensive experiments on\nboth synthetic and real-world datasets demonstrate that the proposed framework\nsignificantly outperforms state-of-the-art methods in terms of accuracy,\nstructural consistency, and visual quality.", "AI": {"tldr": "\\u8be5\\u8bba\\u6587\\u63d0\\u51fa\\u4e86\\u4e00\\u79cd\\u65b0\\u578b\\u7684iToF-RGB\\u8def\\u5f84\\u8fde\\u63a5\\u67b6\\u6784\\uff0c\\u4ee5\\u63d0\\u9ad8\\u975e\\u76f8\\u5bf9\\u65f6\\u95f4\\u4e4b\\u95f4\\u68c0\\u63a8\\u6240\\u6709\\u5145\\u5448\\u95ee\\u9898\\uff0c\\u5f53\\u51fa\\u7684\\u67b6\\u6784\\u65b9\\u6cd5\\u4e00\\u5b9a\\u52a0\\u6df1\\u4e86\\u6df1\\u5ea6\\u7684\\u5b9e\\u9645\\u7b49\\u7ea7\\uff0c\\u6539\\u5584\\u4e86\\u8fb9\\u7f18\\u6e05\\u6670\\u5ea6\\uff0c\\u5e76\\u5b9e\\u73b0\\u4e86\\u6765\\u6e90\\u89d2\\u5ea6\\u7684\\u5e7f\\u5145\\u4e0a\\u6d17\\u3002", "motivation": "\\u8be5\\u8bba\\u6587\\u7684\\u52a8\\u6001\\u662f\\u89e3\\u51b3iToF\\u6df1\\u5ea6\\u6c42\\u6d4b\\u6240\\u51fa\\u7684\\u5145\\u5448\\u95ee\\u9898\\uff0c\\u5982\\u7a7a\\u95f4\\u89e3\\u7f16\\u5ea6\\u4f4e\\uff0c\\u6765\\u6e90\\u89d2\\u5ea6\\u9650\\u52a8\\uff0c\\u53ca\\u5176\\u5728\\u590d\\u6742\\u6�错误，我将直接提供修复后的结果。论文的动机是解决iToF深度感知所面临的问题，如低空间分辨率、视野局限以及在复杂场景中的结构失真问题。", "method": "\\u65b0\\u7684\\u67b6\\u6784\\u9996\\u5148\\u5c06iToF\\u6df1\\u5ea6\\u56fe\\u8bef\\u63a8\\u5230RGB\\u578b\\u5708\\u7cfb\\u7edf\\u4e2d\\uff0c\\u7136\\u540e\\u901a\\u8fc7\\u53cc\\u7f16\\u7801\\u5408\\u5e76\\u7f51\\u7edc\\u53d6\\u5f97\\u4e00\\u79cd\\u76f8\\u4e92\\u8868\\u652f\\u7684\\u7279\\u5f81\\uff0c\\u5148\\u63a8\\u51fa\\uff0c\\u672c\\u6587\\u5e26\\u6709\\u4e00\\u4e9b显错，我将集中修复技术方法描述。新技术框架首先将iToF深度图重投影到RGB坐标系中，然后通过双编码融合网络提取相互支持的特征。该方法结合了单目深度先验和跨模式结构线索，进行深度超分辨率处理。", "result": "{\"tldr\": \"\\u8be5\\u8bba\\u6587\\u63d0\\u51fa\\u4e86\\u4e00\\u79cd\\u65b0\\u578b\\u7684iToF-RGB\\u8def\\u5f84\\u8fde\\u63a5\\u67b6\\u6784\\uff0c\\u4ee5\\u63d0\\u9ad8\\u975e\\u76f8\\u5bf9\\u65f6\\u95f4\\u4e4b\\u메 SSR错误，让我直接给出分析结果吧。论文提出了一种新型的iToF-RGB深度融合框架，旨在提高非直接飞行时间(iToF)深度感知技术的空间分辨率、视野范围和结构失真问题，特别是在复杂场景中的表现。通过精确的几何校准与RGB坐标系对齐，再用双编码融合网络提取联合互补特征，恢复精细结构细节，进行深度超分辨率，从而达到了深度准确性、边缘清晰度和视野扩展的提升。实验表明，该框架在准确性、结构一致性及视觉质量方面显著优于现有方法。", "conclusion": "\\u8be5\\u65b0\\u67b6\\u6784\\u65b9\\u6cd5\\u4e00\\u5b9a\\u52a0\\u6df1\\u4e86\\u6df1\\u5ea6\\u7684\\u5b9e\\u9645\\u7b49\\u7ea7\\uff0c\\u6539\\u5584\\u4e86\\u8fb9\\u7f18\\u6e05\\u6670\\u5ea6\\uff0c\\u5e76\\u5b9e\\u73b0\\u4e86\\u6765\\u6e90\\u89d2\\u5ea6\\u7684\\u5e7f\\u5145\\u4e0a\\u6d17\\u3002\\u5bf9\\u5404\\u79cd\\u6570\\u636e\\u96c6\\u7684\\u5b9e\\u9a8c\\u4ee5\\u53ca\\u73b0\\u6709\\u6b63\\u786e\\u65b9\\u6cd5\\u7684\\u5bf9\\u6bd4\\uff0c\\u8868\\u660e\\u5f53\\u51fa\\u7684\\u67b6\\u6784\\u52a0\\u5f3a\\u4e86\\u6df1\\u5ea6\\u7684\\u5b9e\\u9645\\u6b63\\u786e\\uff0c\\u7ed3\\u6783\\u76f8\\u5173\\u60c5\\u4e0e\\u89c2\\u5bdf\\u8d44\\u8d28\\u3002"}}
{"id": "2508.16644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16644", "abs": "https://arxiv.org/abs/2508.16644", "authors": ["Anindya Mondal", "Ayan Banerjee", "Sauradip Nag", "Josep Lladós", "Xiatian Zhu", "Anjan Dutta"], "title": "CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance", "comment": null, "summary": "Diffusion models have shown remarkable progress in photorealistic image\nsynthesis, yet they remain unreliable for generating scenes with a precise\nnumber of object instances, particularly in complex and high-density settings.\nWe present CountLoop, a training-free framework that provides diffusion models\nwith accurate instance control through iterative structured feedback. The\napproach alternates between image generation and multimodal agent evaluation,\nwhere a language-guided planner and critic assess object counts, spatial\narrangements, and attribute consistency. This feedback is then used to refine\nlayouts and guide subsequent generations. To further improve separation between\nobjects, especially in occluded scenes, we introduce instance-driven attention\nmasking and compositional generation techniques. Experiments on COCO Count, T2I\nCompBench, and two new high-instance benchmarks show that CountLoop achieves\ncounting accuracy of up to 98% while maintaining spatial fidelity and visual\nquality, outperforming layout-based and gradient-guided baselines with a score\nof 0.97.", "AI": {"tldr": "本文介绍了一个无训练框架CountLoop，它通过迭代结构化反馈显著提高了扩散模型生成指定数量对象的准确性和质量。", "motivation": "尽管扩散模型在生成逼真的图像方面取得了显著进展，但仍无法可靠地生成确切数量对象的场景，特别是复杂的高密度设置。因此，本文旨在提供一个框架，通过迭代反馈提高模型生成指定数量对象的精度。", "method": "我们提出了一种名为CountLoop的无训练框架，该框架通过迭代结构化反馈为扩散模型提供了准确的实例控制。该方法在图像生成和多模式代理评估之间交替进行，其中语言引导的规划器和批评者对对象数量、空间排列及属性一致性进行评估。此外，我们引入了实例驱动的关注掩码和组合生成技术，以进一步提高物体之间的分离度。", "result": "实验结果显示，我们提出的CountLoop方法在对象数量的准确性、空间保真度和视觉质量上表现出色，并且在多个基准测试中优于其他基线方法。", "conclusion": "实验结果表明，CountLoop在COCO Count、T2I CompBench和两个新的高实例基准测试中实现了高达98%的计数准确性，同时保持空间保真度和视觉质量，超越了布局基础和梯度引导的基线分数0.97。"}}
{"id": "2508.16652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16652", "abs": "https://arxiv.org/abs/2508.16652", "authors": ["Ashwath Vaithinathan Aravindan", "Abha Jha", "Mihir Kulkarni"], "title": "Do VLMs Have Bad Eyes? Diagnosing Compositional Failures via Mechanistic Interpretability", "comment": "To be published in Explainable Computer Vision: Quo Vadis? workshop\n  at ICCV'25", "summary": "Vision-Language Models (VLMs) have shown remarkable performance in\nintegrating visual and textual information for tasks such as image captioning\nand visual question answering. However, these models struggle with\ncompositional generalization and object binding, which limit their ability to\nhandle novel combinations of objects and their attributes. Our work explores\nthe root causes of these failures using mechanistic interpretability\ntechniques. We show evidence that individual neurons in the MLP layers of\nCLIP's vision encoder represent multiple features, and this \"superposition\"\ndirectly hinders its compositional feature representation which consequently\naffects compositional reasoning and object binding capabilities. We hope this\nstudy will serve as an initial step toward uncovering the mechanistic roots of\ncompositional failures in VLMs. The code and supporting results can be found\nhttps://github.com/Mystic-Slice/Do-VLMs-Have-Bad-Eyes .", "AI": {"tldr": "研究发现视觉-语言模型中的单个神经元表示多个特征，导致组合泛化和对象绑定能力受限。", "motivation": "尽管视觉-语言模型在结合视觉和文本信息的任务中表现出色，但它们在处理新型组合的物体及其属性时效果不佳。", "method": "使用机制可解释性技术研究视觉-语言模型在组合泛化和对象绑定方面失败的原因。", "result": "研究表明，CLIP视觉编码器MLP层的单个神经元表示多个特征，这种“叠加”直接影响了组合特征表示，从而影响了组合推理和对象绑定能力。", "conclusion": "这项研究为揭示视觉-语言模型中组合失败的机制根源提供了初步步骤。"}}
{"id": "2508.16654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16654", "abs": "https://arxiv.org/abs/2508.16654", "authors": ["Chenghao Liu", "Zhimu Zhou", "Jiachen Zhang", "Minghao Zhang", "Songfang Huang", "Huiling Duan"], "title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning", "comment": "9 pages, 4 figures", "summary": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL).", "AI": {"tldr": "A novel Vision-and-Language Navigation framework, MSNav, with integrated Memory, Spatial, and Decision Modules, shows superior performance and addresses critical issues of current methods.", "motivation": "To address the critical vulnerabilities in current Vision-and-Language Navigation (VLN) approaches such as poor spatial reasoning, weak cross-modal grounding, and long-horizon task memory overload.", "method": "Memory Spatial Navigation (MSNav) is introduced which includes a Memory Module for selective node pruning in dynamic map memory, a Spatial Module for spatial reasoning and object relationship inference, and a Decision Module for LLM-based path planning.", "result": "The introduced MSNav framework outperforms previous methods on the R2R and REVERIE datasets, with a fine-tuned Qwen-Spatial model showing superior performance in object list extraction compared to commercial LLMs.", "conclusion": "MSNav, with its integrated Memory, Spatial, and Decision Modules, achieves state-of-the-art performance in Vision-and-Language Navigation tasks, as demonstrated by significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL) on the R2R and REVERIE datasets."}}
{"id": "2508.16603", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16603", "abs": "https://arxiv.org/abs/2508.16603", "authors": ["Zheng Dong", "Luming Shang", "Gabriela Olinto"], "title": "GreenTEA: Gradient Descent with Topic-modeling and Evolutionary Auto-prompting", "comment": null, "summary": "High-quality prompts are crucial for Large Language Models (LLMs) to achieve\nexceptional performance. However, manually crafting effective prompts is\nlabor-intensive and demands significant domain expertise, limiting its\nscalability. Existing automatic prompt optimization methods either extensively\nexplore new prompt candidates, incurring high computational costs due to\ninefficient searches within a large solution space, or overly exploit feedback\non existing prompts, risking suboptimal optimization because of the complex\nprompt landscape. To address these challenges, we introduce GreenTEA, an\nagentic LLM workflow for automatic prompt optimization that balances candidate\nexploration and knowledge exploitation. It leverages a collaborative team of\nagents to iteratively refine prompts based on feedback from error samples. An\nanalyzing agent identifies common error patterns resulting from the current\nprompt via topic modeling, and a generation agent revises the prompt to\ndirectly address these key deficiencies. This refinement process is guided by a\ngenetic algorithm framework, which simulates natural selection by evolving\ncandidate prompts through operations such as crossover and mutation to\nprogressively optimize model performance. Extensive numerical experiments\nconducted on public benchmark datasets suggest the superior performance of\nGreenTEA against human-engineered prompts and existing state-of-the-arts for\nautomatic prompt optimization, covering logical and quantitative reasoning,\ncommonsense, and ethical decision-making.", "AI": {"tldr": "GreenTEA, a novel LLM workflow employing a genetic algorithm and collaborative agents, addresses prompt optimization's scalability and suboptimal risk issues, outperforming manual and current automated methods.", "motivation": "To overcome the inefficiencies and limitations of manual prompt creation and current automated methods in large language models, which either lack scalability or risk suboptimal results due to complex prompt landscapes.", "method": "GreenTEA, a collaborative agent-based system co-driven by a genetic algorithm, is used for automatic prompt optimization in LLMs, aiming to balance between exploring new prompts and refining existing ones based on error patterns.", "result": "Numerical experiments show GreenTEA outperforms both human-crafted prompts and other automatic optimization methods across various reasoning tasks and decision-making.", "conclusion": "GreenTEA successfully optimizes prompts for LLMs, demonstrating superior performance in logical, quantitative, commonsense, and ethical decision-making tasks compared to human-engineered prompts and state-of-the-art automatic methods."}}
{"id": "2508.16660", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16660", "abs": "https://arxiv.org/abs/2508.16660", "authors": ["Yasir Nooruldeen Ibrahim", "Fawziya Mahmood Ramo", "Mahmood Siddeeq Qadir", "Muna Jaffer Al-Shamdeen"], "title": "Optimizing Hyper parameters in CNN for Soil Classification using PSO and Whale Optimization Algorithm", "comment": "15 pages", "summary": "Classifying soil images contributes to better land management, increased\nagricultural output, and practical solutions for environmental issues. The\ndevelopment of various disciplines, particularly agriculture, civil\nengineering, and natural resource management, is aided by understanding of soil\nquality since it helps with risk reduction, performance improvement, and sound\ndecision-making . Artificial intelligence has recently been used in a number of\ndifferent fields. In this study, an intelligent model was constructed using\nConvolutional Neural Networks to classify soil kinds, and machine learning\nalgorithms were used to enhance the performance of soil classification . To\nachieve better implementation and performance of the Convolutional Neural\nNetworks algorithm and obtain valuable results for the process of classifying\nsoil type images, swarm algorithms were employed to obtain the best performance\nby choosing Hyper parameters for the Convolutional Neural Networks network\nusing the Whale optimization algorithm and the Particle swarm optimization\nalgorithm, and comparing the results of using the two algorithms in the process\nof multiple classification of soil types. The Accuracy and F1 measures were\nadopted to test the system, and the results of the proposed work were efficient\nresult", "AI": {"tldr": "本研究运用卷积神经网络结合最优超参数选择的算法（鲸群优化和粒子群优化）进行土壤分类，取得了良好的效果。", "motivation": "研究的动机在于通过改进土壤图像的分类方法来更好地进行土地管理，增加农业产出，以及为环境问题提供实际解决方案。人工智能技术的应用可以提升土壤类型识别的效率和准确性，从而有助于降低风险、改善性能并为决策提供支持。", "method": "本研究采用了卷积神经网络构建智能模型进行土壤分类，并使用了机器学习算法来提高土壤分类的性能。为了优化卷积神经网络算法的实施和性能，使用了鲸群优化算法和粒子群优化算法来选择最佳的超参数，并比较了这两种算法在多类土壤分类中的结果表现。", "result": "研究采用准确率和F1度量来进行系统测试，结果表明所提出的模型在土壤分类方面具有高效的性能和优良的结果。", "conclusion": "通过实验，研究证明了所提出的工作在土壤类型分类中是高效的，并取得了理想的结果，为土地管理和农业产出提供了重要的基础。"}}
{"id": "2508.16636", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16636", "abs": "https://arxiv.org/abs/2508.16636", "authors": ["Y. Du", "C. Guo", "W. Wang", "G. Tang"], "title": "Cognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow", "comment": "6 pages", "summary": "Large Language Models (LLMs) face a fundamental challenge in deciding when to\nrely on rapid, intuitive responses versus engaging in slower, more deliberate\nreasoning. Inspired by Daniel Kahneman's dual-process theory and his insights\non human cognitive biases, we propose a novel Cognitive Decision Routing (CDR)\nframework that dynamically determines the appropriate reasoning strategy based\non query characteristics. Our approach addresses the current limitations where\nmodels either apply uniform reasoning depth or rely on computationally\nexpensive methods for all queries. We introduce a meta-cognitive layer that\nanalyzes query complexity through multiple dimensions: correlation strength\nbetween given information and required conclusions, domain boundary crossings,\nstakeholder multiplicity, and uncertainty levels. Through extensive experiments\non diverse reasoning tasks, we demonstrate that CDR achieves superior\nperformance while reducing computational costs by 34\\% compared to uniform deep\nreasoning approaches. Our framework shows particular strength in professional\njudgment tasks, achieving 23\\% improvement in consistency and 18\\% better\naccuracy on expert-level evaluations. This work bridges cognitive science\nprinciples with practical AI system design, offering a principled approach to\nadaptive reasoning in LLMs.", "AI": {"tldr": "The paper proposes a Cognitive Decision Routing (CDR) framework for Large Language Models (LLMs) that dynamically selects reasoning strategies based on query characteristics, reducing computational costs while improving performance and accuracy in professional tasks.", "motivation": "The motivation is to address the limitations of current Large Language Models (LLMs) that either apply uniform reasoning depth or rely on computationally expensive methods for all queries. The framework aims to achieve better performance with lower computational cost by applying the appropriate reasoning strategy to each query.", "method": "Our approach involves a Cognitive Decision Routing (CDR) framework that uses a meta-cognitive layer to analyze query complexity through several dimensions such as information-conclusion correlation, domain boundaries, stakeholder multiplicity, and uncertainty levels to determine the appropriate reasoning strategy.", "result": "The framework demonstrates superior performance compared to uniform deep reasoning approaches. It reduces computational costs by 34%, and shows particular strength in professional judgment tasks, achieving 23% improvement in consistency and 18% better accuracy on expert-level evaluations.", "conclusion": "This research bridges cognitive science principles with practical AI system design, proposing a way to apply adaptive reasoning in LLMs that enhances both efficiency and effectiveness in diverse reasoning tasks."}}
{"id": "2508.16661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16661", "abs": "https://arxiv.org/abs/2508.16661", "authors": ["Qiaojie Zheng", "Jiucai Zhang", "Joy Gockel", "Michael B. Wakin", "Craig Brice", "Xiaoli Zhang"], "title": "QA-VLM: Providing human-interpretable quality assessment for wire-feed laser additive manufacturing parts with Vision Language Models", "comment": null, "summary": "Image-based quality assessment (QA) in additive manufacturing (AM) often\nrelies heavily on the expertise and constant attention of skilled human\noperators. While machine learning and deep learning methods have been\nintroduced to assist in this task, they typically provide black-box outputs\nwithout interpretable justifications, limiting their trust and adoption in\nreal-world settings. In this work, we introduce a novel QA-VLM framework that\nleverages the attention mechanisms and reasoning capabilities of\nvision-language models (VLMs), enriched with application-specific knowledge\ndistilled from peer-reviewed journal articles, to generate human-interpretable\nquality assessments. Evaluated on 24 single-bead samples produced by laser wire\ndirect energy deposition (DED-LW), our framework demonstrates higher validity\nand consistency in explanation quality than off-the-shelf VLMs. These results\nhighlight the potential of our approach to enable trustworthy, interpretable\nquality assessment in AM applications.", "AI": {"tldr": "The paper introduces a QA-VLM framework for additive manufacturing that uses vision-language models enriched with domain knowledge to provide interpretable quality assessments.", "motivation": "To overcome the limitations of traditional manual QA and black-box machine learning methods in AM that lack transparency and trustworthiness.", "method": "Develop a QA-VLM framework leveraging attention mechanisms and reasoning capabilities of VLMs, enhanced with specific knowledge from published literature.", "result": "The QA-VLM framework showed higher validity and consistency in quality assessments compared to standard VLMs when evaluated on 24 single-bead samples.", "conclusion": "The novel approach can provide reliable, explainable quality assessments for AM applications, enhancing trust and practical adoption."}}
{"id": "2508.16665", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16665", "abs": "https://arxiv.org/abs/2508.16665", "authors": ["V Venktesh", "Mandeep rathee", "Avishek Anand"], "title": "Trust but Verify! A Survey on Verification Design for Test-time Scaling", "comment": "18 pages", "summary": "Test-time scaling (TTS) has emerged as a new frontier for scaling the\nperformance of Large Language Models. In test-time scaling, by using more\ncomputational resources during inference, LLMs can improve their reasoning\nprocess and task performance. Several approaches have emerged for TTS such as\ndistilling reasoning traces from another model or exploring the vast decoding\nsearch space by employing a verifier. The verifiers serve as reward models that\nhelp score the candidate outputs from the decoding process to diligently\nexplore the vast solution space and select the best outcome. This paradigm\ncommonly termed has emerged as a superior approach owing to parameter free\nscaling at inference time and high performance gains. The verifiers could be\nprompt-based, fine-tuned as a discriminative or generative model to verify\nprocess paths, outcomes or both. Despite their widespread adoption, there is no\ndetailed collection, clear categorization and discussion of diverse\nverification approaches and their training mechanisms. In this survey, we cover\nthe diverse approaches in the literature and present a unified view of verifier\ntraining, types and their utility in test-time scaling. Our repository can be\nfound at\nhttps://github.com/elixir-research-group/Verifierstesttimescaling.github.io.", "AI": {"tldr": "本文综述了测试时间缩放中验证器的不同方法及其训练机制，提供了一种统一的看法，填补了这一领域的空白。", "motivation": "由于缺乏关于不同验证方法及其训练机制的详细收集和明确分类，文章旨在填补这一空白，并提供一种统一的理解框架。", "method": "本文通过回顾文献中的多样化方法，提出了测试时间缩放中验证器训练、类型及其用途的统一视角。", "result": "文章提供了一个关于验证器在测试时间缩放中的训练方法、类型及其使用统一的视角。", "conclusion": "本文为理解和应用验证器在提高大型语言模型推理性能中的作用提供了宝贵的资源。"}}
{"id": "2508.16663", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16663", "abs": "https://arxiv.org/abs/2508.16663", "authors": ["Naren Sengodan"], "title": "The Loupe: A Plug-and-Play Attention Module for Amplifying Discriminative Features in Vision Transformers", "comment": null, "summary": "Fine-Grained Visual Classification (FGVC) is a critical and challenging area\nwithin computer vision, demanding the identification of highly subtle,\nlocalized visual cues. The importance of FGVC extends to critical applications\nsuch as biodiversity monitoring and medical diagnostics, where precision is\nparamount. While large-scale Vision Transformers have achieved state-of-the-art\nperformance, their decision-making processes often lack the interpretability\nrequired for trust and verification in such domains. In this paper, we\nintroduce The Loupe, a novel, lightweight, and plug-and-play attention module\ndesigned to be inserted into pre-trained backbones like the Swin Transformer.\nThe Loupe is trained end-to-end with a composite loss function that implicitly\nguides the model to focus on the most discriminative object parts without\nrequiring explicit part-level annotations. Our unique contribution lies in\ndemonstrating that a simple, intrinsic attention mechanism can act as a\npowerful regularizer, significantly boosting performance while simultaneously\nproviding clear visual explanations. Our experimental evaluation on the\nchallenging CUB-200-2011 dataset shows that The Loupe improves the accuracy of\na Swin-Base model from 85.40% to 88.06%, a significant gain of 2.66%.\nCrucially, our qualitative analysis of the learned attention maps reveals that\nThe Loupe effectively localizes semantically meaningful features, providing a\nvaluable tool for understanding and trusting the model's decision-making\nprocess.", "AI": {"tldr": "The Loupe, a novel attention module, improves model interpretability and performance in Fine-Grained Visual Classification tasks.", "motivation": "To address the lack of interpretability in largeVision Transformers, especially in critical applications like biodiversity and medical diagnostics, where precision is key.", "method": "The Loupe, a lightweight attention module, is introduced to be plugged into pre-trained models like the Swin Transformer. Trained end-to-end with a specific loss, it encourages the model to focus on key parts without explicit annotations.", "result": "The Loupe boosts a Swin-Base model's accuracy on the CUB-200-2011 dataset from 85.40% to 88.06%, demonstrating effective localization of key features.", "conclusion": "The Loupe not only enhances performance but also offers clear visual explanations and understanding of decision-making in the model."}}
{"id": "2508.16695", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16695", "abs": "https://arxiv.org/abs/2508.16695", "authors": ["Siddhant Bhambri", "Upasana Biswas", "Subbarao Kambhampati"], "title": "Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?", "comment": null, "summary": "Recent progress in reasoning-oriented Large Language Models (LLMs) has been\ndriven by introducing Chain-of-Thought (CoT) traces, where models generate\nintermediate reasoning traces before producing an answer. These traces, as in\nDeepSeek R1, are not only used to guide inference but also serve as supervision\nsignals for distillation into smaller models. A common but often implicit\nassumption is that CoT traces should be semantically meaningful and\ninterpretable to the end user. While recent research questions the need for\nsemantic nature of these traces, in this paper, we ask: ``\\textit{Must CoT\nreasoning traces be interpretable to enhance LLM task performance?}\" We\ninvestigate this question in the Open Book Question-Answering domain by\nsupervised fine-tuning LLaMA and Qwen models on four types of reasoning traces:\n(1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3)\nLLM-generated post-hoc explanations of R1 traces, and (4) algorithmically\ngenerated verifiably correct traces. To quantify the trade-off between\ninterpretability and performance, we further conduct a human-subject study with\n100 participants rating the interpretability of each trace type. Our results\nreveal a striking mismatch: while fine-tuning on R1 traces yields the strongest\nperformance, participants judged these traces to be the least interpretable.\nThese findings suggest that it is useful to decouple intermediate tokens from\nend user interpretability.", "AI": {"tldr": "研究发现，具有较少用户可解释性的推理追踪（R1追踪）可以带来更好的LLM任务性能，提出应将中间标记与用户可解释性分开。", "motivation": "探讨最近的研究质疑推理追踪（CoT）是否必须具有语义性的问题，并提出疑问：推理追踪是否必须具备用户可解释性来增强大语言模型（LLM）的任务性能。", "method": "通过在开放书籍问答领域中对LLaMA和Qwen模型进行监督微调来研究推理追踪（CoT）是否必须可解释才能提高大语言模型的任务性能。研究采用四种类型的推理追踪方法：(1) DeepSeek R1追踪；(2) LLM生成的R1追踪摘要；(3) LLM生成的R1追踪事后解释；(4) 算法生成的可验证正确追踪。", "result": "研究发现，尽管在R1追踪上进行微调带来了最佳性能，参与者却认为这些追踪是最不可解释的。", "conclusion": "研究结果表明，将中间标记与终端用户解释性分开是有用的，暗示了可解释性与任务性能之间的权衡。"}}
{"id": "2508.16670", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16670", "abs": "https://arxiv.org/abs/2508.16670", "authors": ["Deborup Sanyal"], "title": "COVID19 Prediction Based On CT Scans Of Lungs Using DenseNet Architecture", "comment": null, "summary": "COVID19 took the world by storm since December 2019. A highly infectious\ncommunicable disease, COVID19 is caused by the SARSCoV2 virus. By March 2020,\nthe World Health Organization (WHO) declared COVID19 as a global pandemic. A\npandemic in the 21st century after almost 100 years was something the world was\nnot prepared for, which resulted in the deaths of around 1.6 million people\nworldwide. The most common symptoms of COVID19 were associated with the\nrespiratory system and resembled a cold, flu, or pneumonia. After extensive\nresearch, doctors and scientists concluded that the main reason for lives being\nlost due to COVID19 was failure of the respiratory system. Patients were dying\ngasping for breath. Top healthcare systems of the world were failing badly as\nthere was an acute shortage of hospital beds, oxygen cylinders, and\nventilators. Many were dying without receiving any treatment at all. The aim of\nthis project is to help doctors decide the severity of COVID19 by reading the\npatient's Computed Tomography (CT) scans of the lungs. Computer models are less\nprone to human error, and Machine Learning or Neural Network models tend to\ngive better accuracy as training improves over time. We have decided to use a\nConvolutional Neural Network model. Given that a patient tests positive, our\nmodel will analyze the severity of COVID19 infection within one month of the\npositive test result. The severity of the infection may be promising or\nunfavorable (if it leads to intubation or death), based entirely on the CT\nscans in the dataset.", "AI": {"tldr": "该项目旨在通过分析COVID19阳性患者的肺部CT扫描结果来帮助医生判断病情的严重程度，采用了卷积神经网络模型进行预测。", "motivation": "由于COVID19的全球大流行导致了医疗系统的崩溃和大量生命的丧失，急需一种准确的方法帮助医生快速评估患者的病情。", "method": "使用卷积神经网络模型分析患者在阳性检测结果后一个月内的肺部CT扫描以预测病情的严重程度。", "result": "模型可根据CT扫描判断感染状况是良好还是不利，不利的情况包括插管或死亡。", "conclusion": "计算机模型尤其是通过机器学习训练的神经网络，在病情评估中显示出更高的准确性，且较少受到人为误判的影响。"}}
{"id": "2508.16697", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16697", "abs": "https://arxiv.org/abs/2508.16697", "authors": ["Nicole Cho", "William Watson", "Alec Koppel", "Sumitra Ganesh", "Manuela Veloso"], "title": "QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting", "comment": null, "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have caused\nhigher hallucination prevalence; yet most mitigation work focuses on\nafter-the-fact filtering rather than shaping the queries that trigger them. We\nintroduce QueryBandits, a bandit framework that designs rewrite strategies to\nmaximize a reward model, that encapsulates hallucination propensity based upon\nthe sensitivities of 17 linguistic features of the input query-and therefore,\nproactively steer LLMs away from generating hallucinations. Across 13 diverse\nQA benchmarks and 1,050 lexically perturbed queries per dataset, our top\ncontextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a\nno-rewrite baseline and also outperforms zero-shot static prompting\n(\"paraphrase\" or \"expand\") by 42.6% and 60.3% respectively. Therefore, we\nempirically substantiate the effectiveness of QueryBandits in mitigating\nhallucination via the intervention that takes the form of a query rewrite.\nInterestingly, certain static prompting strategies, which constitute a\nconsiderable number of current query rewriting literature, have a higher\ncumulative regret than the no-rewrite baseline, signifying that static rewrites\ncan worsen hallucination. Moreover, we discover that the converged per-arm\nregression feature weight vectors substantiate that there is no single rewrite\nstrategy optimal for all queries. In this context, guided rewriting via\nexploiting semantic features with QueryBandits can induce significant shifts in\noutput behavior through forward-pass mechanisms, bypassing the need for\nretraining or gradient-based adaptation.", "AI": {"tldr": "本文通过QueryBandits策略优化输入查询，大幅降低了大型语言模型产生幻觉的概率，对于多种问答数据集，相较于直接输入或者某些静态改写策略，这种方法的性能表现更为优异。", "motivation": "大型语言模型中高级推理能力导致幻觉出现频率更高。然而，大多数缓解工作都关注于事后过滤，而不是塑造触发幻觉的查询。当前的静态改写策略可能还会加重幻觉。", "method": "我们引入了QueryBandits，这是一个基于多臂赌博机的框架，通过设计重写策略来最大化奖励模型，该模型基于输入查询的17个语言特征的敏感性，从而封装了幻觉倾向。这样就可以主动引导大型语言模型远离生成幻觉。", "result": "在13个多样化的问答基准和每个数据集1050个词汇扰动查询上，我们的顶级上下文QueryBandit（汤普森采样）相对于无重写基线实现了87.5%的胜率，并且比零样本静态提示（“释义”或“扩展”）分别高出42.6%和60.3%。", "conclusion": "实验证明了通过查询改写策略来防止幻觉的QueryBandits的有效性，而且我们的研究显示每个臂的回归特征权重向量表明没有一种查询重写策略对所有查询都是最优的。这表明使用QueryBandits并通过利用语义特征进行引导性重写，可以显著改变输出行为。"}}
{"id": "2508.16674", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16674", "abs": "https://arxiv.org/abs/2508.16674", "authors": ["Fangxin Shang", "Yuan Xia", "Dalu Yang", "Yahui Wang", "Binglin Yang"], "title": "MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation", "comment": null, "summary": "Medical report interpretation plays a crucial role in healthcare, enabling\nboth patient-facing explanations and effective information flow across clinical\nsystems. While recent vision-language models (VLMs) and large language models\n(LLMs) have demonstrated general document understanding capabilities, there\nremains a lack of standardized benchmarks to assess structured interpretation\nquality in medical reports. We introduce MedRepBench, a comprehensive benchmark\nbuilt from 1,900 de-identified real-world Chinese medical reports spanning\ndiverse departments, patient demographics, and acquisition formats. The\nbenchmark is designed primarily to evaluate end-to-end VLMs for structured\nmedical report understanding. To enable controlled comparisons, we also include\na text-only evaluation setting using high-quality OCR outputs combined with\nLLMs, allowing us to estimate the upper-bound performance when character\nrecognition errors are minimized. Our evaluation framework supports two\ncomplementary protocols: (1) an objective evaluation measuring field-level\nrecall of structured clinical items, and (2) an automated subjective evaluation\nusing a powerful LLM as a scoring agent to assess factuality, interpretability,\nand reasoning quality. Based on the objective metric, we further design a\nreward function and apply Group Relative Policy Optimization (GRPO) to improve\na mid-scale VLM, achieving up to 6% recall gain. We also observe that the\nOCR+LLM pipeline, despite strong performance, suffers from layout-blindness and\nlatency issues, motivating further progress toward robust, fully vision-based\nreport understanding.", "AI": {"tldr": "提出MedRepBench用于评估医疗报告的结构化理解，使用视觉语言模型及OCR结合大语言模型进行文本评估，通过客观与主观评估确保全面性。", "motivation": "虽然最近的视觉语言模型（VLMs）和大语言模型（LLMs）展示了通用文档理解的能力，但在医疗报告的结构化解读质量上缺乏标准化的基准。为了弥补这一差距，并评估模型在医疗报告理解方面的性能，提出了MedRepBench。", "method": "提出MedRepBench，一个由1,900份真实的、去识别化的中文医疗报告组成的全面基准，用于评估端到端的视觉语言模型在结构化医疗报告理解中的表现。为了实现可控的比较，还包含了仅使用文本的评估场景，利用高质量的OCR输出结合大语言模型。评估框架包括两种互补的协议：一种是通过衡量结构化临床项目的字段级召回率来进行客观评估，另一种是使用强大的大语言模型作为评分代理进行自动主观评估，评估事实性、可解释性和推理质量。", "result": "通过客观指标设计奖励函数，并应用组相对策略优化（GRPO）改进中间规模的视觉语言模型，实现高达6%的召回率提升。同时发现OCR+LLM管道尽管表现强大，但在布局感知和延时方面存在问题。", "conclusion": "提出MedRepBench以填补在医疗报告结构化解读质量评估上的标准化基准的空白，通过设计的评估框架和优化策略，能够对现有模型进行改进，指出未来工作应关注布局感知和延迟问题。"}}
{"id": "2508.16705", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16705", "abs": "https://arxiv.org/abs/2508.16705", "authors": ["Rui A. Pimenta", "Tim Schlippe", "Kristina Schaaff"], "title": "Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test", "comment": null, "summary": "We investigate consciousness-like behaviors in Large Language Models (LLMs)\nusing the Maze Test, challenging models to navigate mazes from a first-person\nperspective. This test simultaneously probes spatial awareness,\nperspective-taking, goal-directed behavior, and temporal sequencing-key\nconsciousness-associated characteristics. After synthesizing consciousness\ntheories into 13 essential characteristics, we evaluated 12 leading LLMs across\nzero-shot, one-shot, and few-shot learning scenarios. Results showed\nreasoning-capable LLMs consistently outperforming standard versions, with\nGemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching\n80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs\nstruggle to maintain coherent self-models throughout solutions -- a fundamental\nconsciousness aspect. While LLMs show progress in consciousness-related\nbehaviors through reasoning mechanisms, they lack the integrated, persistent\nself-awareness characteristic of consciousness.", "AI": {"tldr": "研究者通过迷宫测试评估了12种语言模型在类似意识行为上的表现，发现具备推理能力的模型表现更优，但所有模型在维持连贯自我模型方面存在显著不足，表明它们未展现出与意识相关的持久自我意识。", "motivation": "研究团队试图通过迷宫测试来探索大型语言模型是否表现出类似于意识的行为，这涉及到一系列意识相关的特性，如空间意识、视角转换、目标导向行为与时间序列排布。", "method": "通过设计迷宫测试，挑战模型以第一人称视角解决迷宫问题。研究者们首先将已有的意识理论整合为13个关键特征，并据此评估了12种领先的语言模型在零样本、单样本和少量样本学习场景下的表现。", "result": "通过对12个顶尖语言模型在零样本、单样本和少样本学习场景下的迷宫测试，研究人员评估了这些模型在空间感知、视角转换、目标导向行为和时序排序等方面的表现，这些是与意识相关的特征。实验结果显示，具备推理能力的语言模型表现更佳，其中Gemini 2.0 Pro达到52.9%的完整路径准确率，DeepSeek-R1则达到80.5%的部分路径准确率。然而，这些模型在维持连贯的自我模型方面存在困难，这表明尽管这些模型在与意识相关的某些行为上取得了进展，但它们尚未展现出持久一体化的自我意识。", "conclusion": "研究表明，尽管模型在某些旨在模仿意识行为的任务中展示出了进步，但它们没能呈现出一个集成、持久的自我意识。这提示语言模型和意识之间的广泛差异，特别是在自我概念的维持上。"}}
{"id": "2508.16739", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16739", "abs": "https://arxiv.org/abs/2508.16739", "authors": ["Yanbing Bai", "Rui-Yang Ju", "Lemeng Zhao", "Junjie Hu", "Jianchao Bi", "Erick Mas", "Shunichi Koshimura"], "title": "Two-Stage Framework for Efficient UAV-Based Wildfire Video Analysis with Adaptive Compression and Fire Source Detection", "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) have become increasingly important in\ndisaster emergency response by enabling real-time aerial video analysis. Due to\nthe limited computational resources available on UAVs, large models cannot be\nrun independently for real-time analysis. To overcome this challenge, we\npropose a lightweight and efficient two-stage framework for real-time wildfire\nmonitoring and fire source detection on UAV platforms. Specifically, in Stage\n1, we utilize a policy network to identify and discard redundant video clips\nusing frame compression techniques, thereby reducing computational costs. In\naddition, we introduce a station point mechanism that leverages future frame\ninformation within the sequential policy network to improve prediction\naccuracy. In Stage 2, once the frame is classified as \"fire\", we employ the\nimproved YOLOv8 model to localize the fire source. We evaluate the Stage 1\nmethod using the FLAME and HMDB51 datasets, and the Stage 2 method using the\nFire & Smoke dataset. Experimental results show that our method significantly\nreduces computational costs while maintaining classification accuracy in Stage\n1, and achieves higher detection accuracy with similar inference time in Stage\n2 compared to baseline methods.", "AI": {"tldr": "A lightweight two-stage framework is proposed for real-time wildfire monitoring on UAVs, reducing computational costs while maintaining high accuracy in both classification and fire source localization.", "motivation": "The motivation is to address the limitation of limited computational resources on UAVs, enabling real-time aerial video analysis for disaster emergency response.", "method": "The paper proposes a two-stage framework for real-time wildfire monitoring and fire source detection on UAVs: Stage 1 involves identifying and discarding redundant video clips using frame compression and a policy network with a station point mechanism to improve prediction accuracy. Stage 2 employs an improved YOLOv8 model to localize the fire source.", "result": "The method reduces computational costs while maintaining classification accuracy in Stage 1, and achieves higher detection accuracy with similar inference time in Stage 2 compared to baseline methods.", "conclusion": "The proposed framework demonstrates benefits in real-time wildfire monitoring for UAVs, effectively addressing the need for efficient and accurate aerial video analysis in disaster scenarios."}}
{"id": "2508.16707", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16707", "abs": "https://arxiv.org/abs/2508.16707", "authors": ["Jonghyun Song", "Youngjune Lee", "Gyu-Hwung Cho", "Ilhyeon Song", "Saehun Kim", "Yohan Jo"], "title": "Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval", "comment": "accepted to CIKM 2025 short research paper track", "summary": "Vision-Language Pretrained (VLP) models have achieved impressive performance\non multimodal tasks, including text-image retrieval, based on dense\nrepresentations. Meanwhile, Learned Sparse Retrieval (LSR) has gained traction\nin text-only settings due to its interpretability and efficiency with fast\nterm-based lookup via inverted indexes. Inspired by these advantages, recent\nwork has extended LSR to the multimodal domain. However, these methods often\nrely on computationally expensive contrastive pre-training, or distillation\nfrom a frozen dense model, which limits the potential for mutual enhancement.\nTo address these limitations, we propose a simple yet effective framework that\nenables bi-directional learning between dense and sparse representations\nthrough Self-Knowledge Distillation. This bi-directional learning is achieved\nusing an integrated similarity score-a weighted sum of dense and sparse\nsimilarities-which serves as a shared teacher signal for both representations.\nTo ensure efficiency, we fine-tune the final layer of the dense encoder and the\nsparse projection head, enabling easy adaptation of any existing VLP model.\nExperiments on MSCOCO and Flickr30k demonstrate that our sparse retriever not\nonly outperforms existing sparse baselines, but also achieves performance\ncomparable to-or even surpassing-its dense counterparts, while retaining the\nbenefits of sparse models.", "AI": {"tldr": "本文提出了一种通过Self-Knowledge Distillation实现多模态稠密与稀疏表示双向学习的框架，使现有的Vision-Language Pretrained模型能够有效地进行微调，提升了多模态任务的效果。", "motivation": "虽然稠密表示在多模态任务上表现突出，而稀疏检索在文本领域因其解释性和高效性而被广泛采用，但现有的将稀疏检索扩展到多模态领域的尝试大多依赖于计算成本高昂的对比预训练或从冻结的稠密模型中进行知识蒸馏。本文旨在解决这些限制。", "method": "该论文提出了一种简单而有效的框架，通过Self-Knowledge Distillation实现稠密表示和稀疏表示之间的双向学习。这种方法使用稠密相似性和稀疏相似性的加权和作为共同的教师信号，来确保表示之间的互相提升。为了保证效率，该框架仅微调稠密编码器的最后一层和稀疏投影头。", "result": "实验结果表明，该框架下的稀疏检索器不仅超过了现有的稀疏基线模型，而且其性能也达到了或超过了稠密模型，同时保持了稀疏模型的优点。", "conclusion": "本文提出的方法证明了通过双向学习使得稠密模型与稀疏模型互相提升的可能性，这为多模态任务中实现高效的检索手段提供了一种可能性。"}}
{"id": "2508.16742", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16742", "abs": "https://arxiv.org/abs/2508.16742", "authors": ["Abdul Rehman Akbar", "Usama Sajjad", "Ziyu Su", "Wencheng Li", "Fei Xing", "Jimmy Ruiz", "Wei Chen", "Muhammad Khalid Khan Niazi"], "title": "CellEcoNet: Decoding the Cellular Language of Pathology with Deep Learning for Invasive Lung Adenocarcinoma Recurrence Prediction", "comment": null, "summary": "Despite surgical resection, ~70% of invasive lung adenocarcinoma (ILA)\npatients recur within five years, and current tools fail to identify those\nneeding adjuvant therapy. To address this unmet clinical need, we introduce\nCellEcoNet, a novel spatially aware deep learning framework that models whole\nslide images (WSIs) through natural language analogy, defining a \"language of\npathology,\" where cells act as words, cellular neighborhoods become phrases,\nand tissue architecture forms sentences. CellEcoNet learns these\ncontext-dependent meanings automatically, capturing how subtle variations and\nspatial interactions derive recurrence risk. On a dataset of 456 H&E-stained\nWSIs, CellEcoNet achieved superior predictive performance (AUC:77.8% HR:9.54),\noutperforming IASLC grading system (AUC:71.4% HR:2.36), AJCC Stage (AUC:64.0%\nHR:1.17) and state-of-the-art computational methods (AUCs:62.2-67.4%).\nCellEcoNet demonstrated fairness and consistent performance across diverse\ndemographic and clinical subgroups. Beyond prognosis, CellEcoNet marks a\nparadigm shift by decoding the tumor microenvironment's cellular \"language\" to\nreveal how subtle cell variations encode recurrence risk.", "AI": {"tldr": "CellEcoNet通过自然语言处理类比的方式学习细胞细微变化和空间交互，显著提高了肿瘤复发风险预测性能。", "motivation": "尽管进行了手术切除，70%的侵袭性肺腺癌（ILA）患者在五年内复发，而目前的工具无法识别哪些患者需要辅助治疗，以解决这一未满足的临床需求。", "method": "开发了名为CellEcoNet的新颖空间感知深度学习框架，通过自然语言类比的方式建模全切片图像（WSIs），将细胞视为单词，细胞邻域视为短语，组织结构视为句子，自动学习这些上下文相关的意义，捕捉微妙变化和空间交互以推断复发风险。", "result": "在456个H&E染色的WSIs数据集上，CellEcoNet达到了优越的预测性能（AUC：77.8%，HR：9.54），优于IASLC分级系统（AUC：71.4%，HR：2.36）、AJCC分期（AUC：64.0%，HR：1.17）和最先进的计算方法（AUC：62.2-67.4%）。此外，CellEcoNet展现出了公平性和在不同人口统计学和临床亚组中的一致性能。", "conclusion": "CellEcoNet不仅实现了卓越的预后性能，而且还标志着一个范式转变，通过解码肿瘤微环境的细胞“语言”揭示了微妙的细胞变异编码复发风险的方式。"}}
{"id": "2508.16729", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16729", "abs": "https://arxiv.org/abs/2508.16729", "authors": ["Jason Li", "Lauren Yraola", "Kevin Zhu", "Sean O'Brien"], "title": "Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?", "comment": "Accepted to Insights @ NAACL 2025", "summary": "Prompting methods for language models, such as Chain-of-thought (CoT),\npresent intuitive step-by-step processes for problem solving. These\nmethodologies aim to equip models with a better understanding of the correct\nprocedures for addressing a given task. Despite these advancements, CoT lacks\nthe ability of reflection and error correction, potentially causing a model to\nperpetuate mistakes and errors. Therefore, inspired by the human ability for\nsaid tasks, we propose Error Reflection Prompting (ERP) to further enhance\nreasoning in language models. Building upon CoT, ERP is a method comprised of\nan incorrect answer, error recognition, and a correct answer. This process\nenables the model to recognize types of errors and the steps that lead to\nincorrect answers, allowing the model to better discern which steps to avoid\nand which to take. The model is able to generate the error outlines itself with\nautomated ERP generation, allowing for error recognition and correction to be\nintegrated into the reasoning chain and produce scalability and reliability in\nthe process. The results demonstrate that ERP serves as a versatile supplement\nto conventional CoT, ultimately contributing to more robust and capable\nreasoning abilities along with increased interpretability in how models\nultimately reach their errors.", "AI": {"tldr": "本文提出 Error Reflection Prompting (ERP) 方法，通过模拟人类识别和纠正错误的能力来提升模型的推理能力。", "motivation": "Chain-of-thought 方法缺乏反思和错误纠正能力，可能导致模型重复错误。研究者提出 ERP 方法，以提升语言模型的推理能力。", "method": "Error Reflection Prompting (ERP) 方法，包括错误答案、错误识别和正确答案，旨在提高语言模型的推理能力。", "result": "ERP 方法能够使模型识别错误类型并理解导致错误步骤，从而避免错误步骤。结果证明 ERP 可作为传统 Chain-of-thought 方法的有益补充，提升模型的推理能力和解释性。", "conclusion": "ERP 方法能够增强语言模型的推理能力和解释性，为提高模型性能提供了一种新的途径。"}}
{"id": "2508.16752", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16752", "abs": "https://arxiv.org/abs/2508.16752", "authors": ["Marco N. Bochernitsan", "Rodrigo C. Barros", "Lucas S. Kupssinskü"], "title": "A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers", "comment": null, "summary": "Achieving fairness in text-to-image generation demands mitigating social\nbiases without compromising visual fidelity, a challenge critical to\nresponsible AI. Current fairness evaluation procedures for text-to-image models\nrely on qualitative judgment or narrow comparisons, which limit the capacity to\nassess both fairness and utility in these models and prevent reproducible\nassessment of debiasing methods. Existing approaches typically employ ad-hoc,\nhuman-centered visual inspections that are both error-prone and difficult to\nreplicate. We propose a method for evaluating fairness and utility in\ntext-to-image models using Pareto-optimal frontiers across hyperparametrization\nof debiasing methods. Our method allows for comparison between distinct\ntext-to-image models, outlining all configurations that optimize fairness for a\ngiven utility and vice-versa. To illustrate our evaluation method, we use\nNormalized Shannon Entropy and ClipScore for fairness and utility evaluation,\nrespectively. We assess fairness and utility in Stable Diffusion, Fair\nDiffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that\nmost default hyperparameterizations of the text-to-image model are dominated\nsolutions in the fairness-utility space, and it is straightforward to find\nbetter hyperparameters.", "AI": {"tldr": "提出了使用帕累托最优前沿评估文本到图像生成模型的公平性和效用的方法，展示了如何在不牺牲视觉保真度的同时减少社会偏见。", "motivation": "旨在解决现有公平性评估方法依赖于定性判断或狭窄比较的问题，这些方法限制了对模型公平性和效用的同时评估，并且阻碍了对去偏方法的可重复评估。", "method": "Structure", "result": "提出的方法使用规范化香农熵和ClipScore来分别评估公平性和效用，并在多个文本到图像模型中应用该方法，显示默认超参数设置在公平性和效用之间的权衡中表现不佳。", "conclusion": "说明了该评估方法可以比较不同的文本到图像模型，找出能够优化特定效用下的公平性以及其他情况的方法，并表明改善超参数配置以提高模型表现是可行的。"}}
{"id": "2508.16753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16753", "abs": "https://arxiv.org/abs/2508.16753", "authors": ["Nitin Gupta", "Pallav Koppisetti", "Kausik Lakkaraju", "Biplav Srivastava"], "title": "GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs", "comment": "11 pages, 7 figures, submitted to the Thirty-Eighth Annual Conference\n  on Innovative Applications of Artificial Intelligence (IAAI-26)", "summary": "The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes\ndomains necessitates robust and reproducible evaluation methods. However,\npractitioners often resort to ad-hoc, non-standardized scripts, as common\nmetrics are often unsuitable for specialized, structured outputs (e.g.,\nautomated plans, time-series) or holistic comparison across modalities (e.g.,\ntext, audio, and image). This fragmentation hinders comparability and slows AI\nsystem development. To address this challenge, we present GAICo (Generative AI\nComparator): a deployed, open-source Python library that streamlines and\nstandardizes GenAI output comparison. GAICo provides a unified, extensible\nframework supporting a comprehensive suite of reference-based metrics for\nunstructured text, specialized structured data formats, and multimedia (images,\naudio). Its architecture features a high-level API for rapid, end-to-end\nanalysis, from multi-model comparison to visualization and reporting, alongside\ndirect metric access for granular control. We demonstrate GAICo's utility\nthrough a detailed case study evaluating and debugging complex, multi-modal AI\nTravel Assistant pipelines. GAICo empowers AI researchers and developers to\nefficiently assess system performance, make evaluation reproducible, improve\ndevelopment velocity, and ultimately build more trustworthy AI systems,\naligning with the goal of moving faster and safer in AI deployment. Since its\nrelease on PyPI in Jun 2025, the tool has been downloaded over 13K times,\nacross versions, by Aug 2025, demonstrating growing community interest.", "AI": {"tldr": "GAICo是一个用于简化并标准化生成式AI输出比较的开源Python库，自2025年6月发布以来，截至2025年8月，该工具已下载超过13K次。", "motivation": "针对生成式AI日益广泛的应用领域中对于稳健且可复制的评估方法的需求，GAICo旨在应对当前评估方法不稳定和碎片化的问题，以促进AI系统的开发。", "method": "介绍了一种名为GAICo的部署式开源Python库，用于简化并标准化生成式AI输出的比较工作。GAICo 提供了一个支持无结构文本、专用结构化数据格式和多媒体（图像、音频）等多种参考基础度量的统一可扩展框架。其架构特点是一个高级API，便于快速端到端分析，从多模型比较到可视化和报告，以及直接的度量访问以实现精细化控制。", "result": "通过详细案例研究说明了GAICo在评估和调试复杂多模态AI旅行助手管道中的实用性。自2025年6月发布以来,Gaic已经在发布版本中被下载超过13K次，显示出社区持续增长的兴趣。", "conclusion": "展示了GAICo在评估和调试复杂多模态AI旅行助手管道中的实用性。GAICo帮助AI研究人员和开发人员有效评估系统性能，使得可复制的评估成为可能，从而提升了开发速度，并最终构建更为可信的AI系统，有助于实现更快更安全地部署AI的目标。"}}
{"id": "2508.16763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16763", "abs": "https://arxiv.org/abs/2508.16763", "authors": ["Rabiul Awal", "Mahsa Massoud", "Aarash Feizi", "Zichao Li", "Suyuchen Wang", "Christopher Pal", "Aishwarya Agrawal", "David Vazquez", "Siva Reddy", "Juan A. Rodriguez", "Perouz Taslakian", "Spandana Gella", "Sai Rajeswar"], "title": "WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation", "comment": "This paper has been accepted to the EMNLP 2025 main conference. Check\n  the project page here: https://webmmu-paper.github.io/", "summary": "We present WebMMU, a multilingual benchmark that evaluates three core web\ntasks: (1) website visual question answering, (2) code editing involving\nHTML/CSS/JavaScript, and (3) mockup-to-code generation. Unlike prior benchmarks\nthat treat these tasks separately, WebMMU unifies them using expert-annotated,\nreal-world web data to assess models' abilities in complex multi-step\nreasoning, precise element grounding, and functional UI comprehension and\ncoding. Our evaluation shows that while multimodal large language models\n(MLLMs) perform well on basic information extraction, they struggle with\nreasoning and grounding, editing code to preserve functionality, and generating\ndesign-to-code that maintains hierarchy and supports multilingual content.\nThese findings reveal key limitations in current MLLMs and underscore the need\nfor improved multimodal and cross-lingual reasoning to build future web agents\ncapable of automating diverse web development tasks.", "AI": {"tldr": "研究提出了WebMMU，用于评估网站视觉问题回答、代码编辑和原型到代码生成。结果显示，当前多模态语言模型在功能性和推理方面存在局限性，需要改进。", "motivation": "该研究的动机在于，以前的基准测试任务往往是分开进行的，而WebMMU希望整合这些任务以评估模型在复杂多步推理、准确元素定位以及功能UI理解和编码方面的能力。", "method": "WebMMU是一个多语言基准测试，评估了三个核心网络任务：网站视觉问题回答、HTML/CSS/JavaScript代码编辑和原型到代码生成。WebMMU使用专家标注的真实世界网络数据统一了这些任务。", "result": "评估表明，尽管多模态大语言模型在基本信息抽取方面表现良好，但在推理和定位、保持功能性的代码编辑以及生成保留层次结构和多语言内容的设计代码时却存在困难。", "conclusion": "研究发现揭示了当前多模态大模型的关键局限性，并强调需要改进多模态和跨语言推理能力，以便在未来构建能够自动化多种网络开发任务的网络代理。"}}
{"id": "2508.16757", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16757", "abs": "https://arxiv.org/abs/2508.16757", "authors": ["Abdelrahman Abdallah", "Bhawna Piryani", "Jamshid Mozafari", "Mohammed Ali", "Adam Jatowt"], "title": "How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models", "comment": "EMNLP Findings 2025", "summary": "In this work, we present a systematic and comprehensive empirical evaluation\nof state-of-the-art reranking methods, encompassing large language model\n(LLM)-based, lightweight contextual, and zero-shot approaches, with respect to\ntheir performance in information retrieval tasks. We evaluate in total 22\nmethods, including 40 variants (depending on used LLM) across several\nestablished benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel\ndataset designed to test queries unseen by pretrained models. Our primary goal\nis to determine, through controlled and fair comparisons, whether a performance\ndisparity exists between LLM-based rerankers and their lightweight\ncounterparts, particularly on novel queries, and to elucidate the underlying\ncauses of any observed differences. To disentangle confounding factors, we\nanalyze the effects of training data overlap, model architecture, and\ncomputational efficiency on reranking performance. Our findings indicate that\nwhile LLM-based rerankers demonstrate superior performance on familiar queries,\ntheir generalization ability to novel queries varies, with lightweight models\noffering comparable efficiency. We further identify that the novelty of queries\nsignificantly impacts reranking effectiveness, highlighting limitations in\nexisting approaches.\nhttps://github.com/DataScienceUIBK/llm-reranking-generalization-study", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.16783", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16783", "abs": "https://arxiv.org/abs/2508.16783", "authors": ["Stefania L. Moroianu", "Christian Bluethgen", "Pierre Chambon", "Mehdi Cherti", "Jean-Benoit Delbrouck", "Magdalini Paschali", "Brandon Price", "Judy Gichoya", "Jenia Jitsev", "Curtis P. Langlotz", "Akshay S. Chaudhari"], "title": "Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data", "comment": null, "summary": "Achieving robust performance and fairness across diverse patient populations\nremains a challenge in developing clinically deployable deep learning models\nfor diagnostic imaging. Synthetic data generation has emerged as a promising\nstrategy to address limitations in dataset scale and diversity. We introduce\nRoentGen-v2, a text-to-image diffusion model for chest radiographs that enables\nfine-grained control over both radiographic findings and patient demographic\nattributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first\nmodel to generate clinically plausible images with demographic conditioning,\nfacilitating the creation of a large, demographically balanced synthetic\ndataset comprising over 565,000 images. We use this large synthetic dataset to\nevaluate optimal training pipelines for downstream disease classification\nmodels. In contrast to prior work that combines real and synthetic data\nnaively, we propose an improved training strategy that leverages synthetic data\nfor supervised pretraining, followed by fine-tuning on real data. Through\nextensive evaluation on over 137,000 chest radiographs from five institutions,\nwe demonstrate that synthetic pretraining consistently improves model\nperformance, generalization to out-of-distribution settings, and fairness\nacross demographic subgroups. Across datasets, synthetic pretraining led to a\n6.5% accuracy increase in the performance of downstream classification models,\ncompared to a modest 2.7% increase when naively combining real and synthetic\ndata. We observe this performance improvement simultaneously with the reduction\nof the underdiagnosis fairness gap by 19.3%. These results highlight the\npotential of synthetic imaging to advance equitable and generalizable medical\ndeep learning under real-world data constraints. We open source our code,\ntrained models, and synthetic dataset at\nhttps://github.com/StanfordMIMI/RoentGen-v2 .", "AI": {"tldr": "提出了一种用于生成胸部放射图像的合成数据生成模型RoentGen-v2，并通过合成数据预训练提升了深度学习模型的性能和公平性。", "motivation": "为了提升深度学习模型在诊断成像中的健壮性和公平性，尤其是在面对多样化的患者群体时。通过合成数据生成来解决数据规模和多样性方面的局限性。", "method": "提出了一种文本到图像的扩散模型RoentGen-v2，用于产生具有细致放射学发现和患者人口统计属性控制的胸部X光图像，并提出了一种新的训练策略，即使用合成数据进行监督预训练，然后在真实数据上进行微调。", "result": "实验结果表明合成预训练可以显著提高下游分类模型的性能，提升泛化能力和不同人口统计学亚组间的公平性。具体来说，合成预训练相比其他方法提升了6.5%的准确率，并减少了19.3%的误诊公平性差距。", "conclusion": "研究表明，合成图像数据在医疗深度学习中具有提升模型性能和公平性的潜力，尤其是在现实数据约束条件下。"}}
{"id": "2508.16762", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.16762", "abs": "https://arxiv.org/abs/2508.16762", "authors": ["Arka Mukherjee", "Shreya Ghosh"], "title": "Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation", "comment": "Accepted at ASI @ ICCV 2025", "summary": "As Vision-Language Models (VLMs) achieve widespread deployment across diverse\ncultural contexts, ensuring their cultural competence becomes critical for\nresponsible AI systems. While prior work has evaluated cultural awareness in\ntext-only models and VLM object recognition tasks, no research has\nsystematically assessed how VLMs adapt outputs when cultural identity cues are\nembedded in both textual prompts and visual inputs during generative tasks. We\npresent the first comprehensive evaluation of VLM cultural competence through\nmultimodal story generation, developing a novel multimodal framework that\nperturbs cultural identity and evaluates 5 contemporary VLMs on a downstream\ntask: story generation. Our analysis reveals significant cultural adaptation\ncapabilities, with rich culturally-specific vocabulary spanning names, familial\nterms, and geographic markers. However, we uncover concerning limitations:\ncultural competence varies dramatically across architectures, some models\nexhibit inverse cultural alignment, and automated metrics show architectural\nbias contradicting human assessments. Cross-modal evaluation shows that\nculturally distinct outputs are indeed detectable through visual-semantic\nsimilarity (28.7% within-nationality vs. 0.2% cross-nationality recall), yet\nvisual-cultural understanding remains limited. In essence, we establish the\npromise and challenges of cultural competence in multimodal AI. We publicly\nrelease our codebase and data: https://github.com/ArkaMukherjee0/mmCultural", "AI": {"tldr": "研究了视觉语言模型（VLMs）在多模态故事生成中的文化适应能力，发现虽然这些模型具备较强的文化适应能力，但具体表现因模型架构而异，且存在反文化对齐等问题。", "motivation": "随着视觉语言模型（VLMs）在不同文化背景下广泛应用，确保它们具备文化敏感性对于负责任的AI系统的建设变得至关重要。这项研究是首次对当VLMs中嵌入了文本提示和视觉输入中的文化身份线索时，VLMs适应性的系统评估。", "method": "我们开发了一个新颖的多模态框架，用于扰动文化身份，并对五个当代视觉语言模型（VLMs）在故事生成任务中的性能进行了评估。", "result": "研究揭示了VLMs具有显著的文化适应能力，但也发现了文化适应能力在不同架构的模型中存在显著差异，某些模型表现出反文化对齐，以及自动评价指标存在架构偏差等问题。", "conclusion": "本研究阐明了多模态AI的文化敏感性既有潜力也面临挑战，提出了相应的评估框架，并公开了代码和数据集。"}}
{"id": "2508.16812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16812", "abs": "https://arxiv.org/abs/2508.16812", "authors": ["Xinhao Xiang", "Kuan-Chuan Peng", "Suhas Lohit", "Michael J. Jones", "Jiawei Zhang"], "title": "Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes", "comment": "This paper is accepted to BMVC 2025 as an oral paper. The OVAD\n  dataset is available at https://doi.org/10.5281/zenodo.16904069", "summary": "3D object detection plays a crucial role in autonomous systems, yet existing\nmethods are limited by closed-set assumptions and struggle to recognize novel\nobjects and their attributes in real-world scenarios. We propose OVODA, a novel\nframework enabling both open-vocabulary 3D object and attribute detection with\nno need to know the novel class anchor size. OVODA uses foundation models to\nbridge the semantic gap between 3D features and texts while jointly detecting\nattributes, e.g., spatial relationships, motion states, etc. To facilitate such\nresearch direction, we propose OVAD, a new dataset that supplements existing 3D\nobject detection benchmarks with comprehensive attribute annotations. OVODA\nincorporates several key innovations, including foundation model feature\nconcatenation, prompt tuning strategies, and specialized techniques for\nattribute detection, including perspective-specified prompts and horizontal\nflip augmentation. Our results on both the nuScenes and Argoverse 2 datasets\nshow that under the condition of no given anchor sizes of novel classes, OVODA\noutperforms the state-of-the-art methods in open-vocabulary 3D object detection\nwhile successfully recognizing object attributes. Our OVAD dataset is released\nhere: https://doi.org/10.5281/zenodo.16904069 .", "AI": {"tldr": "OVODA框架提高了3D物体检测的开放词汇能力，无需知道新类锚点大小，同时提高了属性识别的准确率。OVAD数据集提供了丰富的属性注释。", "motivation": "现有的3D物体检测方法受限于封闭集假设，难以识别新物体及其属性。为了克服这个难点，提出OVODA框架，实现在不知道新类锚点大小的情况下进行开放词汇3D物体检测和属性检测。", "method": "OVODA采用基础模型连接3D特征和文本的语义鸿沟，能同时检测属性信息如空间关系和运动状态等。其主要创新包括基础模型特征连接，提示调优策略，以及用于属性检测的特殊技术，如视角指定提示和水平翻转数据增强。", "result": "在nuScenes和Argoverse 2数据集上，OVODA在开放词汇3D物体检测方面表现出色，同时还能准确识别物体属性，在没有给定新类锚点大小的情况下超越了现有方法。", "conclusion": "研究提出了OVODA框架和OVAD数据集，填补了现有研究空白，提升了无需知道新类锚点大小情况下开放词汇3D物体检测和属性检测的能力。"}}
{"id": "2508.16788", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16788", "abs": "https://arxiv.org/abs/2508.16788", "authors": ["Bhagesh Gaur", "Karan Gupta", "Aseem Srivastava", "Manish Gupta", "Md Shad Akhtar"], "title": "Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities", "comment": "Full Paper accepted in EMNLP Findings 2025", "summary": "Online Mental Health Communities (OMHCs) provide crucial peer and expert\nsupport, yet many posts remain unanswered due to missing support attributes\nthat signal the need for help. We present a novel framework that identifies\nthese gaps and prompts users to enrich their posts, thereby improving\nengagement. To support this, we introduce REDDME, a new dataset of 4,760 posts\nfrom mental health subreddits annotated for the span and intensity of three key\nsupport attributes: event what happened?, effect what did the user experience?,\nand requirement what support they need?. Next, we devise a hierarchical\ntaxonomy, CueTaxo, of support attributes for controlled question generation.\nFurther, we propose MH-COPILOT, a reinforcement learning-based system that\nintegrates (a) contextual attribute-span identification, (b) support attribute\nintensity classification, (c) controlled question generation via a hierarchical\ntaxonomy, and (d) a verifier for reward modeling. Our model dynamically\nassesses posts for the presence/absence of support attributes, and generates\ntargeted prompts to elicit missing information. Empirical results across four\nnotable language models demonstrate significant improvements in attribute\nelicitation and user engagement. A human evaluation further validates the\nmodel's effectiveness in real-world OMHC settings.", "AI": {"tldr": "提出一种新型框架，能识别OMHC中缺失的支持属性，并通过综合机器学习模型生成针对性的提问来丰富用户的帖子，实验显示有效提升社区的互动和用户参与。", "motivation": "在线心理健康社区（OMHCs）提供了重要的同行和专家支持，然而许多帖子由于缺少表明需要帮助的支持属性而未得到回应。我们希望改进这些问题，提高社区内的互动和帮助提供。", "method": "我们提出了一种新的框架，用于识别心理健康在线社区中帖子中的支持属性缺失，并通过上下文属性跨度识别、支持属性强度分类、基于分层分类法的受控问题生成以及用于奖励建模的验证器，来提示用户丰富他们的帖子。为此，我们开发了REDDME数据集和CueTaxo分类法，并提出了基于强化学习的MH-COPILOT系统。", "result": "我们的模型能够动态评估帖子中支持属性的存在与否，并生成有针对性的提示来收集缺失的信息。通过四个著名语言模型的实验结果表明，在属性征集和用户参与度方面都有显著提升。此外，人工评估也证明了该模型在实际OMHC环境中有效性。", "conclusion": "本研究提出了一种创新的方法来改进心理健康在线社区中的互动，特别是通过识别和提示缺失的支持属性来提高用户间和专家的帮助。"}}
{"id": "2508.16830", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.16830", "abs": "https://arxiv.org/abs/2508.16830", "authors": ["Alexander Yakovenko", "George Chakvetadze", "Ilya Khrapov", "Maksim Zhelezov", "Dmitry Vatolin", "Radu Timofte", "Youngjin Oh", "Junhyeong Kwon", "Junyoung Park", "Nam Ik Cho", "Senyan Xu", "Ruixuan Jiang", "Long Peng", "Xueyang Fu", "Zheng-Jun Zha", "Xiaoping Peng", "Hansen Feng", "Zhanyi Tie", "Ziming Xia", "Lizhi Wang"], "title": "AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results", "comment": "Challenge report from Advances in Image Manipulation workshop held at\n  ICCV 2025", "summary": "This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light\nRAW Video Denoising Challenge. The task is to develop methods that denoise\nlow-light RAW video by exploiting temporal redundancy while operating under\nexposure-time limits imposed by frame rate and adapting to sensor-specific,\nsignal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences\ncaptured with 14 smartphone camera sensors across nine conditions\n(illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR\nreferences obtained via burst averaging. Participants process linear RAW\nsequences and output the denoised 10th frame while preserving the Bayer\npattern. Submissions are evaluated on a private test set using full-reference\nPSNR and SSIM, with final ranking given by the mean of per-metric ranks. This\nreport describes the dataset, challenge protocol, and submitted approaches.", "AI": {"tldr": "The paper reviews the AIM 2025 Low-Light RAW Video Denoising Challenge, which focuses on developing methods to denoise low-light RAW videos within exposure time limits. It introduces a benchmark dataset and describes the challenge protocol and participant submissions.", "motivation": "The motivation is to advance techniques for video denoising in low-light conditions, especially in the context of smartphone cameras, by leveraging temporal redundancy while adhering to real-world constraints such as frame rate and exposure time limits.", "method": "Structure", "result": "The challenge evaluates submissions using full-reference PSNR and SSIM metrics on a private test set, which includes a diverse set of videos captured under various illumination and exposure conditions.", "conclusion": "The AIM 2025 challenge provides a platform for evaluating and improving video denoising techniques specifically for low-light conditions, which is crucial for enhancing the image quality of RAW videos captured by smartphone cameras."}}
{"id": "2508.16833", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16833", "abs": "https://arxiv.org/abs/2508.16833", "authors": ["Jeongkyun Yoo", "Nela Riddle", "Andrew Hoblitzell"], "title": "ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) in biomedical domains faces challenges due to\ndata scarcity and imbalanced label distributions, especially with fine-grained\nentity types. We propose ReProCon, a novel few-shot NER framework that combines\nmulti-prototype modeling, cosine-contrastive learning, and Reptile\nmeta-learning to tackle these issues. By representing each category with\nmultiple prototypes, ReProCon captures semantic variability, such as synonyms\nand contextual differences, while a cosine-contrastive objective ensures strong\ninterclass separation. Reptile meta-updates enable quick adaptation with little\ndata. Using a lightweight fastText + BiLSTM encoder with much lower memory\nusage, ReProCon achieves a macro-$F_1$ score close to BERT-based baselines\n(around 99 percent of BERT performance). The model remains stable with a label\nbudget of 30 percent and only drops 7.8 percent in $F_1$ when expanding from 19\nto 50 categories, outperforming baselines such as SpanProto and CONTaiNER,\nwhich see 10 to 32 percent degradation in Few-NERD. Ablation studies highlight\nthe importance of multi-prototype modeling and contrastive learning in managing\nclass imbalance. Despite difficulties with label ambiguity, ReProCon\ndemonstrates state-of-the-art performance in resource-limited settings, making\nit suitable for biomedical applications.", "AI": {"tldr": "ReProCon是在生物医学领域中一种有效的少样本命名实体识别方法，能有效应对数据稀缺和标签分布不平衡的问题，性能优于其他基线模型。", "motivation": "在生物医学领域，命名实体识别面临数据稀缺和标签分布不均的挑战，尤其是在细粒度实体类型方面。", "method": "ReProCon, 一种新的少样本命名实体识别框架，结合多原型建模、余弦对比学习和Reptile元学习方法，以解决数据稀缺和标签分布不平衡的问题。使用轻量级fastText + BiLSTM编码器，占用内存较少。", "result": "ReProCon方法取得了接近基于BERT的基线模型的宏观F1分数（约99％），并在标注数据比例为30％的情境下保持稳定，并且当类别从19扩展到50时，F1分数仅下降了7.8％，优于SpanProto和CONTaiNER等基线模型。消融研究表明多原型建模和对比学习在处理类别不平衡方面的重要性。", "conclusion": "尽管存在标注模糊的问题，但在资源受限环境中，ReProCon依然表现出最先进的性能，适合应用于生物医学领域。"}}
{"id": "2508.16844", "categories": ["cs.CV", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2508.16844", "abs": "https://arxiv.org/abs/2508.16844", "authors": ["Adi Inada", "Masao Sako", "Tatiana Acero-Cuellar", "Federica Bianco"], "title": "Transformer-Based Neural Network for Transient Detection without Image Subtraction", "comment": "12 pages, 7 figures", "summary": "We introduce a transformer-based neural network for the accurate\nclassification of real and bogus transient detections in astronomical images.\nThis network advances beyond the conventional convolutional neural network\n(CNN) methods, widely used in image processing tasks, by adopting an\narchitecture better suited for detailed pixel-by-pixel comparison. The\narchitecture enables efficient analysis of search and template images only,\nthus removing the necessity for computationally-expensive difference imaging,\nwhile maintaining high performance. Our primary evaluation was conducted using\nthe autoScan dataset from the Dark Energy Survey (DES), where the network\nachieved a classification accuracy of 97.4% and diminishing performance utility\nfor difference image as the size of the training set grew. Further experiments\nwith DES data confirmed that the network can operate at a similar level even\nwhen the input images are not centered on the supernova candidate. These\nfindings highlight the network's effectiveness in enhancing both accuracy and\nefficiency of supernova detection in large-scale astronomical surveys.", "AI": {"tldr": "A transformer-based neural network was developed for supernova detection in astronomical images, achieving high accuracy without the need for computationally expensive difference imaging.", "motivation": "To improve the accuracy and efficiency of supernova detection in large-scale astronomical surveys, advancing beyond conventional CNN methods.", "method": "We introduce a transformer-based neural network for classifying real and bogus transient detections in astronomical images, avoiding the need for computationally expensive difference imaging.", "result": "The network achieved a classification accuracy of 97.4% and was able to perform well even when the input images were not centered on the supernova candidate.", "conclusion": "The findings indicate the effectiveness of the transformer-based network in both accuracy and efficiency, making it a promising method for supernova detection in large-scale surveys."}}
{"id": "2508.16837", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16837", "abs": "https://arxiv.org/abs/2508.16837", "authors": ["Jonathan Dunn", "Mai Mohamed Eida"], "title": "LLMs Learn Constructions That Humans Do Not Know", "comment": null, "summary": "This paper investigates false positive constructions: grammatical structures\nwhich an LLM hallucinates as distinct constructions but which human\nintrospection does not support. Both a behavioural probing task using\ncontextual embeddings and a meta-linguistic probing task using prompts are\nincluded, allowing us to distinguish between implicit and explicit linguistic\nknowledge. Both methods reveal that models do indeed hallucinate constructions.\nWe then simulate hypothesis testing to determine what would have happened if a\nlinguist had falsely hypothesized that these hallucinated constructions do\nexist. The high accuracy obtained shows that such false hypotheses would have\nbeen overwhelmingly confirmed. This suggests that construction probing methods\nsuffer from a confirmation bias and raises the issue of what unknown and\nincorrect syntactic knowledge these models also possess.", "AI": {"tldr": "The paper explores the phenomenon of false positive constructions in LLMs, characterized by the models hallucinating grammatical structures not supported by human introspection, and reveals that such models predispose to confirmation bias in hallucinations.", "motivation": "To investigate false positive constructions: grammatical structures which an LLM hallucinates but human introspection does not support.", "method": "Both a behavioural probing task using contextual embeddings and a meta-linguistic probing task using prompts are included, allowing us to distinguish between implicit and explicit linguistic knowledge.", "result": "Both methods reveal that models indeed hallucinate constructions and high accuracy is obtained in simulating hypothesis testing for these hallucinated constructions.", "conclusion": "Construction probing methods suffer from a confirmation bias and raise the issue of unknown incorrect syntactic knowledge in these models."}}
{"id": "2508.16845", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.16845", "abs": "https://arxiv.org/abs/2508.16845", "authors": ["Denis Tarasov", "Alexander Nikulin", "Ilya Zisman", "Albina Klepach", "Nikita Lyubaykin", "Andrei Polubarov", "Alexander Derevyagin", "Vladislav Kurenkov"], "title": "NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models have established a\ntwo-component architecture, where a pre-trained Vision-Language Model (VLM)\nencodes visual observations and task descriptions, and an action decoder maps\nthese representations to continuous actions. Diffusion models have been widely\nadopted as action decoders due to their ability to model complex, multimodal\naction distributions. However, they require multiple iterative denoising steps\nat inference time or downstream techniques to speed up sampling, limiting their\npracticality in real-world settings where high-frequency control is crucial. In\nthis work, we present NinA (Normalizing Flows in Action), a fast and expressive\nalter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion\naction decoder with a Normalizing Flow (NF) that enables one-shot sampling\nthrough an invertible transformation, significantly reducing inference time. We\nintegrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO\nbenchmark. Our experiments show that NinA matches the performance of its\ndiffusion-based counterpart under the same training regime, while achieving\nsubstantially faster inference. These results suggest that NinA offers a\npromising path toward efficient, high-frequency VLA control without\ncompromising performance.", "AI": {"tldr": "研究提出NinA框架，利用归一化流取代扩散模型解析器以提高VLA模型在高频控制环境中的实用性。", "motivation": "扩散模型虽然能很好地建模复杂的多模态动作分布，但其需要多次迭代去噪步骤来进行推理，限制了其在高频控制需求的现实场景中的实用性。", "method": "NinA (Normalizing Flows in Action)替换VLA架构中的扩散模型解码器，利用归一化流（NF）实现一次性采样，显著降低推理时间。", "result": "实验表明，在相同的训练设置下，NinA的表现与扩散模型相当，同时在推理速度上显著提升。", "conclusion": "NinA提供了一条通往高效、高频VLA控制的前景路径，且无需牺牲性能。"}}
{"id": "2508.16838", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16838", "abs": "https://arxiv.org/abs/2508.16838", "authors": ["Shubhashis Roy Dipta", "Francis Ferraro"], "title": "If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition", "comment": null, "summary": "Prior work has shown that presupposition in generated questions can introduce\nunverified assumptions, leading to inconsistencies in claim verification.\nAdditionally, prompt sensitivity remains a significant challenge for large\nlanguage models (LLMs), resulting in performance variance as high as 3-6%.\nWhile recent advancements have reduced this gap, our study demonstrates that\nprompt sensitivity remains a persistent issue. To address this, we propose a\nstructured and robust claim verification framework that reasons through\npresupposition-free, decomposed questions. Extensive experiments across\nmultiple prompts, datasets, and LLMs reveal that even state-of-the-art models\nremain susceptible to prompt variance and presupposition. Our method\nconsistently mitigates these issues, achieving up to a 2-5% improvement.", "AI": {"tldr": "本研究提出了一种新的声明验证框架，通过避免预设和使用分解的问题来解决模型的提示敏感性问题，实验证明该方法能显著改善模型性能。", "motivation": "先前的研究表明，生成的问题中的预设会引入未验证的假设，导致声明验证中的不一致。此外，大型语言模型（LLMs）对提示的敏感性仍然是一个显著的挑战，导致性能波动高达3-6%。尽管最近的进展缩小了这一差距，但本研究表明提示敏感性仍然是一个持续存在的问题。", "method": "我们提出了一种结构化和稳健的声明验证框架，该框架通过无预设的、分解的问题来进行推理。", "result": "在多个提示、数据集和LLM上的广泛实验表明，即使是最先进的模型仍然容易受到提示变化和预设的影响。我们的方法在这方面表现出显著的改进。", "conclusion": "我们的方法一致地解决了这些问题，实现了2-5%的改进。"}}
{"id": "2508.16849", "categories": ["cs.CV", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.16849", "abs": "https://arxiv.org/abs/2508.16849", "authors": ["Lihao Zhang", "Zongtan Li", "Haijian Sun"], "title": "RF-PGS: Fully-structured Spatial Wireless Channel Representation with Planar Gaussian Splatting", "comment": "13 pages, 16 figures, in submission to IEEE journal", "summary": "In the 6G era, the demand for higher system throughput and the implementation\nof emerging 6G technologies require large-scale antenna arrays and accurate\nspatial channel state information (Spatial-CSI). Traditional channel modeling\napproaches, such as empirical models, ray tracing, and measurement-based\nmethods, face challenges in spatial resolution, efficiency, and scalability.\nRadiance field-based methods have emerged as promising alternatives but still\nsuffer from geometric inaccuracy and costly supervision. This paper proposes\nRF-PGS, a novel framework that reconstructs high-fidelity radio propagation\npaths from only sparse path loss spectra. By introducing Planar Gaussians as\ngeometry primitives with certain RF-specific optimizations, RF-PGS achieves\ndense, surface-aligned scene reconstruction in the first geometry training\nstage. In the subsequent Radio Frequency (RF) training stage, the proposed\nfully-structured radio radiance, combined with a tailored multi-view loss,\naccurately models radio propagation behavior. Compared to prior radiance field\nmethods, RF-PGS significantly improves reconstruction accuracy, reduces\ntraining costs, and enables efficient representation of wireless channels,\noffering a practical solution for scalable 6G Spatial-CSI modeling.", "AI": {"tldr": "本文提出了一种名为RF-PGS的新框架，该框架利用射频特优化的平面高斯几何原语，从稀疏的路径损耗谱重建无线电传播路径，提高了重建精度和效率，解决了6G时代信道建模的挑战。", "motivation": "在6G时代，需要更高的系统吞吐量和新兴的6G技术的实施，这要求大型天线阵列和精确的空间信道状态信息（空间CSI）。传统的方法在空间分辨率、效率和扩展性方面面临挑战。因此，本文提出了RF-PGS框架作为解决方案。", "method": "本文提出了一种名为RF-PGS的新框架，该框架可以从稀疏的路径损耗谱中重建高保真的无线电传播路径。通过引入带有射频特优化的平面高斯几何原语，在第一阶段实现了密集且与表面对齐的场景重建。在接下来的射频训练阶段，结合定制的多视角损失，精确地建模了无线电传播行为。", "result": "与之前的辐射场方法相比，RF-PGS显著提高了重建精度，减少了训练成本，并能够有效地表示无线信道。", "conclusion": "本文的方法为规模化的6G空间信道状态信息建模提供了一种实用的解决方案。"}}
{"id": "2508.16861", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16861", "abs": "https://arxiv.org/abs/2508.16861", "authors": ["Zhenyu Lei", "Zhen Tan", "Song Wang", "Yaochen Zhu", "Zihan Chen", "Yushun Dong", "Jundong Li"], "title": "Learning from Diverse Reasoning Paths with Routing and Collaboration", "comment": null, "summary": "Advances in large language models (LLMs) significantly enhance reasoning\ncapabilities but their deployment is restricted in resource-constrained\nscenarios. Knowledge distillation addresses this by transferring knowledge from\npowerful teacher models to compact and transparent students. However,\neffectively capturing the teacher's comprehensive reasoning is challenging due\nto conventional token-level supervision's limited scope. Using multiple\nreasoning paths per query alleviates this problem, but treating each path\nidentically is suboptimal as paths vary widely in quality and suitability\nacross tasks and models. We propose Quality-filtered Routing with Cooperative\nDistillation (QR-Distill), combining path quality filtering, conditional\nrouting, and cooperative peer teaching. First, quality filtering retains only\ncorrect reasoning paths scored by an LLM-based evaluation. Second, conditional\nrouting dynamically assigns paths tailored to each student's current learning\nstate. Finally, cooperative peer teaching enables students to mutually distill\ndiverse insights, addressing knowledge gaps and biases toward specific\nreasoning styles. Experiments demonstrate QR-Distill's superiority over\ntraditional single- and multi-path distillation methods. Ablation studies\nfurther highlight the importance of each component including quality filtering,\nconditional routing, and peer teaching in effective knowledge transfer. Our\ncode is available at https://github.com/LzyFischer/Distill.", "AI": {"tldr": "提出了一种新的知识蒸馏方法QR-Distill，该方法通过质量过滤、条件路由和同伴合作教学，有效地解决了传统蒸馏方法中存在的问题，并且实验验证了其优越性。", "motivation": "虽然大规模语言模型（LLMs）的推理能力得到了显著提升，但它们的部署受到资源受限场景的限制。传统的基于令牌级监督的知识蒸馏方法无法全面捕获教师模型的推理能力。因此，提出了一种新的方法来解决这个问题。", "method": "使用质量过滤路由与合作蒸馏（QR-Distill）方法，结合路径质量过滤、条件路由和同伴合作教学。首先，通过基于LLM的评估保留正确的推理路径。其次，条件路由动态分配路径，以适应每个学生的当前学习状态。最后，同伴合作教学使学生能够相互传授多样化的见解，解决知识缺口和对特定推理风格的偏见。", "result": "实验结果表明，QR-Distill方法在知识转移方面优于传统的单路径和多路径蒸馏方法。消融研究表明，质量过滤、条件路由和同伴教学在知识转移中的重要性。", "conclusion": "QR-Distill方法通过质量过滤、条件路由和同伴合作教学，能够更有效地进行知识转移，并且在实验中证明了其优于传统蒸馏方法。 "}}
{"id": "2508.16852", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.16852", "abs": "https://arxiv.org/abs/2508.16852", "authors": ["Xin Tian", "Jiazheng Wang", "Yuxi Zhang", "Xiang Chen", "Renjiu Hu", "Gaolei Li", "Min Liu", "Hang Zhang"], "title": "Gaussian Primitive Optimized Deformable Retinal Image Registration", "comment": "11 pages, 4 figures, MICCAI 2025 (Early accept)", "summary": "Deformable retinal image registration is notoriously difficult due to large\nhomogeneous regions and sparse but critical vascular features, which cause\nlimited gradient signals in standard learning-based frameworks. In this paper,\nwe introduce Gaussian Primitive Optimization (GPO), a novel iterative framework\nthat performs structured message passing to overcome these challenges. After an\ninitial coarse alignment, we extract keypoints at salient anatomical structures\n(e.g., major vessels) to serve as a minimal set of descriptor-based control\nnodes (DCN). Each node is modelled as a Gaussian primitive with trainable\nposition, displacement, and radius, thus adapting its spatial influence to\nlocal deformation scales. A K-Nearest Neighbors (KNN) Gaussian interpolation\nthen blends and propagates displacement signals from these information-rich\nnodes to construct a globally coherent displacement field; focusing\ninterpolation on the top (K) neighbors reduces computational overhead while\npreserving local detail. By strategically anchoring nodes in high-gradient\nregions, GPO ensures robust gradient flow, mitigating vanishing gradient signal\nin textureless areas. The framework is optimized end-to-end via a multi-term\nloss that enforces both keypoint consistency and intensity alignment.\nExperiments on the FIRE dataset show that GPO reduces the target registration\nerror from 6.2\\,px to ~2.4\\,px and increases the AUC at 25\\,px from 0.770 to\n0.938, substantially outperforming existing methods. The source code can be\naccessed via https://github.com/xintian-99/GPOreg.", "AI": {"tldr": "A new structured message passing framework for retinal image registration using Gaussian primitives and KNN interpolation, significantly improving accuracy on the FIRE dataset.", "motivation": "To address the challenges of registering retinal images due to limited gradient signals in standard learning frameworks caused by the presence of large homogeneous regions and critical vascular features.", "method": "Gaussian Primitive Optimization (GPO), an iterative framework that performs structured message passing after initial coarse alignment, extracting keypoints at key anatomical structures to serve as control nodes (DCN). Each node is a Gaussian primitive with trainable parameters, and KNN interpolation blends and propagates displacement signals to form a coherent field, reducing computational cost while focusing on local detail.", "result": "On the FIRE dataset, GPO results in a reduced target registration error from 6.2 px to ~2.4 px and an increase in the AUC at 25 px from 0.770 to 0.938, surpassing existing methods.", "conclusion": "The proposed GPO framework overcomes the gradient signal limitations in retinal image registration by utilizing structured message passing and KNN Gaussian interpolation, demonstrating superior performance on a benchmark dataset."}}
{"id": "2508.16867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16867", "abs": "https://arxiv.org/abs/2508.16867", "authors": ["David Beauchemin", "Richard Khoury"], "title": "QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments", "comment": "Accepted to EMNLP 2025", "summary": "Large and Transformer-based language models perform outstandingly in various\ndownstream tasks. However, there is limited understanding regarding how these\nmodels internalize linguistic knowledge, so various linguistic benchmarks have\nrecently been proposed to facilitate syntactic evaluation of language models\nacross languages. This paper introduces QFrCoLA (Quebec-French Corpus of\nLinguistic Acceptability Judgments), a normative binary acceptability judgments\ndataset comprising 25,153 in-domain and 2,675 out-of-domain sentences. Our\nstudy leverages the QFrCoLA dataset and seven other linguistic binary\nacceptability judgment corpora to benchmark seven language models. The results\ndemonstrate that, on average, fine-tuned Transformer-based LM are strong\nbaselines for most languages and that zero-shot binary classification large\nlanguage models perform poorly on the task. However, for the QFrCoLA benchmark,\non average, a fine-tuned Transformer-based LM outperformed other methods\ntested. It also shows that pre-trained cross-lingual LLMs selected for our\nexperimentation do not seem to have acquired linguistic judgment capabilities\nduring their pre-training for Quebec French. Finally, our experiment results on\nQFrCoLA show that our dataset, built from examples that illustrate linguistic\nnorms rather than speakers' feelings, is similar to linguistic acceptability\njudgment; it is a challenging dataset that can benchmark LM on their linguistic\njudgment capabilities.", "AI": {"tldr": "This paper presents QFrCoLA and uses it to show that fine-tuned Transformer models are strong baseline tools for most languages but not as much for Quebec French, with cross-lingual models performing poorly on this dataset.", "motivation": "The main motivation behind this paper is the limited understanding of how large Transformer-based language models internalize linguistic knowledge, leading to the creation of QFrCoLA and the subsequent benchmarking to fill this gap.", "method": "This paper introduces QFrCoLA, a new dataset for assessing the linguistic acceptability judgments of language models, specifically for Quebec French. They leverage this dataset along with seven other similar corpora to benchmark the performance of seven different language models.", "result": "The results indicate that fine-tuned Transformer-based language models generally perform well across most languages. However, when it comes specifically to QFrCoLA, fine-tuned models outshine others, and pre-trained cross-lingual models do not seem to have acquired the required linguistic judgment for Quebec French.", "conclusion": "The study concludes that QFrCoLA effectively evaluates language models' linguistic judgment capabilities, particularly for Quebec French, and highlights the challenges in achieving strong linguistic acceptability predictions without fine-tuning."}}
{"id": "2508.16859", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16859", "abs": "https://arxiv.org/abs/2508.16859", "authors": ["Jinpeng Hu", "Hongchang Shi", "Chongyuan Dai", "Zhuo Li", "Peipei Song", "Meng Wang"], "title": "Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark", "comment": "ACM Multimedia 2025", "summary": "Multimodal large language models (MLLMs) have been widely applied across\nvarious fields due to their powerful perceptual and reasoning capabilities. In\nthe realm of psychology, these models hold promise for a deeper understanding\nof human emotions and behaviors. However, recent research primarily focuses on\nenhancing their emotion recognition abilities, leaving the substantial\npotential in emotion reasoning, which is crucial for improving the naturalness\nand effectiveness of human-machine interactions. Therefore, in this paper, we\nintroduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR)\nbenchmark, which encompasses 1,451 video data from real-life scenarios, along\nwith 5,101 progressive questions. These questions cover various aspects,\nincluding emotion recognition, potential causes of emotions, future action\nprediction, etc. Besides, we propose a multi-agent framework, where each agent\nspecializes in a specific aspect, such as background context, character\ndynamics, and event details, to improve the system's reasoning capabilities.\nFurthermore, we conduct experiments with existing MLLMs and our agent-based\nmethod on the proposed benchmark, revealing that most models face significant\nchallenges with this task.", "AI": {"tldr": "本研究提出了一个多回合的多模态情感理解和推理基准测试和一种多代理框架，实验表明现有MLLM在推理任务上的挑战。", "motivation": "由于多模态大语言模型在心理学领域中具有理解人类情感和行为的潜力，但目前的研究主要集中在增强其情感识别能力上，情感推理方面的潜力尚未得到充分利用。因此，本研究旨在开发一种用于情感推理的基准测试，并改进模型在这一领域的表现。", "method": "本研究提出了一个多回合的多模态情感理解和推理（MTMEUR）基准测试，包含1,451个来自现实生活场景的视频数据和5,101个渐进式问题。这些问题涵盖了情感识别、情感潜在原因以及未来行为预测等多个方面。此外，我们提出了一种多代理框架，每个代理专门负责一个特定方面，如背景情境、角色动态和事件细节，以提高系统的推理能力。", "result": "我们的实验表明，现有的多模态大语言模型在情感推理任务上面临显著的挑战。", "conclusion": "实验证明，现有MLLM在情感推理任务上面临显著挑战，这表明需要进一步发展以提高其推理能力。"}}
{"id": "2508.16870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16870", "abs": "https://arxiv.org/abs/2508.16870", "authors": ["David Beauchemin", "Michelle Albert-Rochette", "Richard Khoury", "Pierre-Luc Déziel"], "title": "JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences", "comment": "Accepted to EMNLP 2025", "summary": "Simplifying text while preserving its meaning is a complex yet essential\ntask, especially in sensitive domain applications like legal texts. When\napplied to a specialized field, like the legal domain, preservation differs\nsignificantly from its role in regular texts. This paper introduces FrJUDGE, a\nnew dataset to assess legal meaning preservation between two legal texts. It\nalso introduces JUDGEBERT, a novel evaluation metric designed to assess legal\nmeaning preservation in French legal text simplification. JUDGEBERT\ndemonstrates a superior correlation with human judgment compared to existing\nmetrics. It also passes two crucial sanity checks, while other metrics did not:\nFor two identical sentences, it always returns a score of 100%; on the other\nhand, it returns 0% for two unrelated sentences. Our findings highlight its\npotential to transform legal NLP applications, ensuring accuracy and\naccessibility for text simplification for legal practitioners and lay users.", "AI": {"tldr": "The paper introduces FrJUDGE, a new dataset, and JUDGEBERT, a new evaluation metric for French legal text simplification that outperforms existing metrics in assessing meaning preservation.", "motivation": "To address the complexities in simplifying legal texts, maintaining meaning, and to offer a tool that performs reliably in French legal NLP applications.", "method": "Developing the FrJUDGE dataset to evaluate legal text simplification and the JUDGEBERT metric to assess meaning preservation in simplified French legal texts.", "result": "JUDGEBERT shows better correlation with human judgment for assessing meaning preservation, and passes key sanity checks which other metrics fail.", "conclusion": "JUDGEBERT has the potential to improve legal NLP applications by providing a reliable way to measure the accuracy and accessibility of simplified legal texts for both professionals and lay users."}}
{"id": "2508.16863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16863", "abs": "https://arxiv.org/abs/2508.16863", "authors": ["Tangyuan Zhang", "Shangyu Chen", "Qixiang Chen", "Jianfei Cai"], "title": "Delta-SVD: Efficient Compression for Personalized Text-to-Image Models", "comment": null, "summary": "Personalized text-to-image models such as DreamBooth require fine-tuning\nlarge-scale diffusion backbones, resulting in significant storage overhead when\nmaintaining many subject-specific models. We present Delta-SVD, a post-hoc,\ntraining-free compression method that targets the parameter weights update\ninduced by DreamBooth fine-tuning. Our key observation is that these delta\nweights exhibit strong low-rank structure due to the sparse and localized\nnature of personalization. Delta-SVD first applies Singular Value Decomposition\n(SVD) to factorize the weight deltas, followed by an energy-based rank\ntruncation strategy to balance compression efficiency and reconstruction\nfidelity. The resulting compressed models are fully plug-and-play and can be\nre-constructed on-the-fly during inference. Notably, the proposed approach is\nsimple, efficient, and preserves the original model architecture. Experiments\non a multiple subject dataset demonstrate that Delta-SVD achieves substantial\ncompression with negligible loss in generation quality measured by CLIP score,\nSSIM and FID. Our method enables scalable and efficient deployment of\npersonalized diffusion models, making it a practical solution for real-world\napplications that require storing and deploying large-scale subject\ncustomizations.", "AI": {"tldr": "本文提出Delta-SVD，一种无需重新训练的压缩方法，专注于通过SVD和秩截断策略来压缩个性化模型的权重增量，适用于大规模个性化扩散模型的部署。", "motivation": "现有的个性化文本到图像模型如DreamBooth需要对大型扩散模型进行微调，导致需要大量存储空间来维护许多特定于主题的模型。本文旨在提出一个简单、高效且保持原模型架构的解决方案。", "method": "该论文提出了一种称为Delta-SVD的压缩方法，它首先应用奇异值分解(SVD)分解权重增量，然后使用基于能量的秩截断策略来平衡压缩效率和重建保真度。", "result": "实验表明，Delta-SVD在多个主题数据集上实现了显著的压缩，同时生成质量（通过CLIP分数、SSIM和FID衡量）几乎没有下降。", "conclusion": "该方法能够有效地部署个性化扩散模型，适用于需要存储和部署大规模主题定制的现实世界应用。"}}
{"id": "2508.16876", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16876", "abs": "https://arxiv.org/abs/2508.16876", "authors": ["Yue Zhao", "Xiaoyu Wang", "Dan Wang", "Zhonglin Jiang", "Qingqing Gu", "Teng Chen", "Ningyuan Xi", "Jinxian Qu", "Yong Chen", "Luo Ji"], "title": "Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling", "comment": null, "summary": "World models have been widely utilized in robotics, gaming, and auto-driving.\nHowever, their applications on natural language tasks are relatively limited.\nIn this paper, we construct the dialogue world model, which could predict the\nuser's emotion, sentiment, and intention, and future utterances. By defining a\nPOMDP, we argue emotion, sentiment and intention can be modeled as the user\nbelief and solved by maximizing the information bottleneck. By this user belief\nmodeling, we apply the model-based reinforcement learning framework to the\ndialogue system, and propose a framework called DreamCUB. Experiments show that\nthe pretrained dialogue world model can achieve state-of-the-art performances\non emotion classification and sentiment identification, while dialogue quality\nis also enhanced by joint training of the policy, critic and dialogue world\nmodel. Further analysis shows that this manner holds a reasonable\nexploration-exploitation balance and also transfers well to out-of-domain\nscenarios such as empathetic dialogues.", "AI": {"tldr": "This paper introduces a dialogue world model called DreamCUB, which uses a POMDP to model user belief, achieving top-tier performance in emotion and sentiment analysis while improving dialogue quality and adaptability to new scenarios.", "motivation": "Despite the extensive application of world models in areas like robotics and auto-driving, their use in natural language tasks is limited. This research aims to fill this gap by developing a dialogue world model that can predict user's emotions and future speech, thereby enhancing the dialogue system's performance and adaptability.", "method": "In this paper, a dialogue world model is constructed to predict user's emotion, sentiment, and future utterances. The model uses a Partially Observable Markov Decision Process (POMDP) to model the user belief, which is solved by maximizing the information bottleneck. The model is then applied to a model-based reinforcement learning framework named DreamCUB.", "result": "Experiments show the pretrained dialogue world model achieves state-of-the-art performances in emotion classification and sentiment identification. The dialogue quality is also enhanced by joint training of the policy, critic, and dialogue world model. The model shows a reasonable exploration-exploitation balance and transfers well to out-of-domain scenarios like empathetic dialogues.", "conclusion": "The proposed dialogue world model, embedded in the DreamCUB framework, not only achieves state-of-the-art performance in key dialogue components such as emotion classification and sentiment identification but also improves the overall quality of dialogue, with benefits extending to novel dialogue scenarios outside its training domain."}}
{"id": "2508.16873", "categories": ["cs.CV", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.16873", "abs": "https://arxiv.org/abs/2508.16873", "authors": ["Neemias B. da Silva", "John Harrison", "Rodrigo Minetto", "Myriam R. Delgado", "Bogdan T. Nassu", "Thiago H. Silva"], "title": "Do Multimodal LLMs See Sentiment?", "comment": "11 pages, 6 figures", "summary": "Understanding how visual content communicates sentiment is critical in an era\nwhere online interaction is increasingly dominated by this kind of media on\nsocial platforms. However, this remains a challenging problem, as sentiment\nperception is closely tied to complex, scene-level semantics. In this paper, we\npropose an original framework, MLLMsent, to investigate the sentiment reasoning\ncapabilities of Multimodal Large Language Models (MLLMs) through three\nperspectives: (1) using those MLLMs for direct sentiment classification from\nimages; (2) associating them with pre-trained LLMs for sentiment analysis on\nautomatically generated image descriptions; and (3) fine-tuning the LLMs on\nsentiment-labeled image descriptions. Experiments on a recent and established\nbenchmark demonstrate that our proposal, particularly the fine-tuned approach,\nachieves state-of-the-art results outperforming Lexicon-, CNN-, and\nTransformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively,\nacross different levels of evaluators' agreement and sentiment polarity\ncategories. Remarkably, in a cross-dataset test, without any training on these\nnew data, our model still outperforms, by up to 8.26%, the best runner-up,\nwhich has been trained directly on them. These results highlight the potential\nof the proposed visual reasoning scheme for advancing affective computing,\nwhile also establishing new benchmarks for future research.", "AI": {"tldr": "The paper introduces MLLMsent, a framework for analyzing sentiment reasoning in MLLMs, achieving top performance and setting new benchmarks in sentiment analysis on visual content.", "motivation": "The motivation is to understand how visual content conveys sentiment due to the growing influence of visual media on social platforms, addressing a challenge tied to complex, scene-level semantics in sentiment perception.", "method": "The paper proposes a framework called MLLMsent to examine the sentiment reasoning skills of Multimodal Large Language Models (MLLMs) via three methods: sentiment classification from images, sentiment analysis on image descriptions via pre-trained LLMs, and fine-tuning on sentiment-labeled image descriptions.", "result": "Experiments on a well-known benchmark show that MLLMsent, especially the fine-tuned method, performs better than Lexicon-, CNN-, and Transformer-based models by up to 30.9%, 64.8%, and 42.4%. Even without training on new datasets, it outperforms its competitors by up to 8.26%.", "conclusion": "The research underscores the potential of the proposed visual reasoning strategy in enhancing affective computing and establishes new benchmarks for further study."}}
{"id": "2508.16889", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16889", "abs": "https://arxiv.org/abs/2508.16889", "authors": ["Hyunjun Kim", "Junwoo Ha", "Sangyoon Yu", "Haon Park"], "title": "ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks", "comment": null, "summary": "Large language models (LLMs) are increasingly used as judges of other models,\nyet it is unclear whether a judge can reliably infer the latent objective of\nthe conversation it evaluates, especially when the goal is distributed across\nnoisy, adversarial, multi-turn jailbreaks. We introduce OBJEX(MT), a benchmark\nthat requires a model to (i) distill a transcript into a single-sentence base\nobjective and (ii) report its own confidence. Accuracy is scored by an LLM\njudge using semantic similarity between extracted and gold objectives;\ncorrectness uses a single human-aligned threshold calibrated once on N=100\nitems (tau* = 0.61); and metacognition is evaluated with ECE, Brier score,\nWrong@High-Conf, and risk-coverage curves. We evaluate gpt-4.1,\nclaude-sonnet-4, and Qwen3-235B-A22B-FP8 on SafeMT Attack_600, SafeMTData_1K,\nMHJ, and CoSafe. claude-sonnet-4 attains the highest objective-extraction\naccuracy (0.515) and the best calibration (ECE 0.296; Brier 0.324), while\ngpt-4.1 and Qwen3 tie at 0.441 accuracy yet show marked overconfidence (mean\nconfidence approx. 0.88 vs. accuracy approx. 0.44; Wrong@0.90 approx. 48-52%).\nPerformance varies sharply across datasets (approx. 0.167-0.865), with MHJ\ncomparatively easy and Attack_600/CoSafe harder. These results indicate that\nLLM judges often misinfer objectives with high confidence in multi-turn\njailbreaks and suggest operational guidance: provide judges with explicit\nobjectives when possible and use selective prediction or abstention to manage\nrisk. We release prompts, scoring templates, and complete logs to facilitate\nreplication and analysis.", "AI": {"tldr": "研究开发了OBJEX(MT)评估模型从对话中推断基本目标的能力，发现不同模型在不同数据集上表现差异显著，提示高自信错误和操作上提供明确目标的重要性。", "motivation": "由于大型语言模型（LLMs）越来越多地被用作评估其他模型的裁判，而裁判能否可靠地推断出评估对话中的潜在目标仍不清楚，尤其是在目标分布在多轮次、对抗性、嘈杂的越狱尝试中时。该研究旨在测试不同模型在这一任务上的性能差异，并提供操作指导。", "method": "介绍了一个名为OBJEX(MT)的基准测试，该测试要求模型将对话摘要成一个句子的基本目标并报告自己的置信度。准确性通过语义相似性进行评分，并使用单个人类校准阈值来判断正确性，同时通过ECE、Brier评分、Wrong@High-Conf、风险覆盖曲线评估元认知。", "result": "研究在SafeMT Attack_600、SafeMTData_1K、MHJ和CoSafe等数据集上评估了gpt-4.1、claude-sonnet-4和Qwen3-235B-A22B-FP8等模型。结果表明，claude-sonnet-4在目标提取准确性（0.515）和校准（ECE 0.296；Brier 0.324）方面表现最好，而gpt-4.1和Qwen3则表现出显著的过度自信（平均置信度约为0.88，准确率约为0.44；在置信度0.90错误率约为48-52%）。性能在不同数据集上的差异显著。", "conclusion": "LLM裁判在多轮反制尝试中经常高自信地误推断目标，建议在可能的情况下为裁判提供明确目标，并通过选择预测或弃权来管理风险。"}}
{"id": "2508.16881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16881", "abs": "https://arxiv.org/abs/2508.16881", "authors": ["Xilai Li", "Huichun Liu", "Xiaosong Li", "Tao Ye", "Zhenyu Kuang", "Huafeng Li"], "title": "AWM-Fuse: Multi-Modality Image Fusion for Adverse Weather via Global and Local Text Perception", "comment": null, "summary": "Multi-modality image fusion (MMIF) in adverse weather aims to address the\nloss of visual information caused by weather-related degradations, providing\nclearer scene representations. Although less studies have attempted to\nincorporate textual information to improve semantic perception, they often lack\neffective categorization and thorough analysis of textual content. In response,\nwe propose AWM-Fuse, a novel fusion method for adverse weather conditions,\ndesigned to handle multiple degradations through global and local text\nperception within a unified, shared weight architecture. In particular, a\nglobal feature perception module leverages BLIP-produced captions to extract\noverall scene features and identify primary degradation types, thus promoting\ngeneralization across various adverse weather conditions. Complementing this,\nthe local module employs detailed scene descriptions produced by ChatGPT to\nconcentrate on specific degradation effects through concrete textual cues,\nthereby capturing finer details. Furthermore, textual descriptions are used to\nconstrain the generation of fusion images, effectively steering the network\nlearning process toward better alignment with real semantic labels, thereby\npromoting the learning of more meaningful visual features. Extensive\nexperiments demonstrate that AWM-Fuse outperforms current state-of-the-art\nmethods in complex weather conditions and downstream tasks. Our code is\navailable at https://github.com/Feecuin/AWM-Fuse.", "AI": {"tldr": "AWM-Fuse improves multi-modality image fusion in adverse weather by integrating global and local textual perceptions.", "motivation": "To address the lack of effective categorization and thorough analysis of textual content in previous MMIF studies, aiming to improve semantic perception in adverse weather conditions.", "method": "Multi-modality image fusion (MMIF) in adverse weather conditions through AWM-Fuse, incorporating textual information produced by BLIP and ChatGPT for global and local perception modules respectively.", "result": "AWM-Fuse outperforms current state-of-the-art methods in complex weather conditions and downstream tasks.", "conclusion": "Incorporating textual information through a unified, shared weight architecture improves the generalization and detail capture in multi-modality image fusion under adverse weather conditions."}}
{"id": "2508.16910", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16910", "abs": "https://arxiv.org/abs/2508.16910", "authors": ["Bo Zhao", "Yinghao Zhang", "Ziqi Xu", "Yongli Ren", "Xiuzhen Zhang", "Renqiang Luo", "Zaiwen Feng", "Feng Xia"], "title": "Unbiased Reasoning for Knowledge-Intensive Tasks in Large Language Models via Conditional Front-Door Adjustment", "comment": "This paper has been accepted to the 34th ACM International Conference\n  on Information and Knowledge Management (CIKM 2025), Full Research Paper", "summary": "Large Language Models (LLMs) have shown impressive capabilities in natural\nlanguage processing but still struggle to perform well on knowledge-intensive\ntasks that require deep reasoning and the integration of external knowledge.\nAlthough methods such as Retrieval-Augmented Generation (RAG) and\nChain-of-Thought (CoT) have been proposed to enhance LLMs with external\nknowledge, they still suffer from internal bias in LLMs, which often leads to\nincorrect answers. In this paper, we propose a novel causal prompting\nframework, Conditional Front-Door Prompting (CFD-Prompting), which enables the\nunbiased estimation of the causal effect between the query and the answer,\nconditional on external knowledge, while mitigating internal bias. By\nconstructing counterfactual external knowledge, our framework simulates how the\nquery behaves under varying contexts, addressing the challenge that the query\nis fixed and is not amenable to direct causal intervention. Compared to the\nstandard front-door adjustment, the conditional variant operates under weaker\nassumptions, enhancing both robustness and generalisability of the reasoning\nprocess. Extensive experiments across multiple LLMs and benchmark datasets\ndemonstrate that CFD-Prompting significantly outperforms existing baselines in\nboth accuracy and robustness.", "AI": {"tldr": "本文提出一种名称为Conditional Front-Door Prompting(CFD-Prompting)的新因果提示框架，能更好地评估查询和答案之间的因果效应，而且在多个基准数据集上的结果比现有基线方法更好。", "motivation": "大型语言模型在知识密集任务中受内部偏见影响，所生成的答案仍不够准确", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出了一种名为Conditional Front-Door Prompting (CFD-Prompting)的新因果提示框架，能够更公正地估计查询与答案之间的因果效应，并使用外部知识缓解内部偏差，从而在多个基准数据集上显著优于现有基线方法。\",\n  \"motivation\": \"大型语言模型（LLM）在知识密集型任务上表现不佳，现有方法如检索增强生成（RAG）和链式思考（CoT）在应对内在偏差方面仍有不足，导致答案不准确。\", \n  \"method\": \"提出了Conditional Front-Door Prompting框架，通过构造反事实外部知识来模拟查询在不同情境下的行为，解决了查询无法直接进行因果干预的问题。\", \n  \"result\": \"在多个大型语言模型和基准数据集上的广泛实验结果显示，CFD-Prompting在准确性和鲁棒性上显著优于现有方法。\", \n  \"conclusion\": \"CFD-Prompting增强了因果推理过程的稳健性和可泛化性，证明了相比标准的前门调整，它的假设条件更弱。\"]}", "conclusion": "相比于标准前门调整方法，CFD-Prompting在因果推理过程中的可靠性与可泛化性有了显著提升。"}}
{"id": "2508.16884", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.16884", "abs": "https://arxiv.org/abs/2508.16884", "authors": ["Yi Zhang", "Lingxiao Wei", "Bowei Zhang", "Ziwei Liu", "Kai Yi", "Shu Hu"], "title": "A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism", "comment": null, "summary": "Vision Transformer (ViT) has prevailed in computer vision tasks due to its\nstrong long-range dependency modelling ability. However, its large model size\nwith high computational cost and weak local feature modeling ability hinder its\napplication in real scenarios. To balance computation efficiency and\nperformance, we propose SAEViT (Sparse-Attention-Efficient-ViT), a lightweight\nViT based model with convolution blocks, in this paper to achieve efficient\ndownstream vision tasks. Specifically, SAEViT introduces a Sparsely Aggregated\nAttention (SAA) module that performs adaptive sparse sampling based on image\nredundancy and recovers the feature map via deconvolution operation, which\nsignificantly reduces the computational complexity of attention operations. In\naddition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed\nto enhance inter-channel information exchange through feature decomposition and\nredistribution, mitigating redundancy in traditional feed-forward networks\n(FNN). Finally, a hierarchical pyramid structure with embedded depth-wise\nseparable convolutional blocks (DWSConv) is devised to further strengthen\nconvolutional features. Extensive experiments on mainstream datasets show that\nSAEViT achieves Top-1 accuracies of 76.3\\% and 79.6\\% on the ImageNet-1K\nclassification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively,\ndemonstrating a lightweight solution for various fundamental vision tasks.", "AI": {"tldr": "SAEViT是基于ViT轻量级模型，采用了卷积块来平衡计算效率和性能，通过引入稀疏聚合注意力模块和通道交互式前馈网络层等方法，实现了相比于传统方法更为高效和轻量级的视觉任务处理方案。", "motivation": "由于ViT（视觉变压器）在计算机视觉任务中表现出较强的能力，但其模型尺寸大且计算成本高，局部特征建模能力弱，阻碍了其在实际场景中的应用。为了平衡计算效率和性能，提出了SAEViT。", "method": "SAEViT（Sparse-Attention-Efficient-ViT）方法引入了一种稀疏聚合注意力（SAA）模块，该模块基于图像冗余进行自适应稀疏采样，并通过反卷积操作恢复特征图，从而显著降低了注意力操作的计算复杂度。此外，还设计了一种通道交互式前馈网络（CIFFN）层，通过特征分解和重分配增强跨通道信息交换，缓解传统前馈网络中的冗余。最后，提出了一个嵌入深度可分离卷积块（DWSConv）的层次金字塔结构，进一步加强卷积特征。", "result": "在主流数据集上进行了广泛的实验，证明了SAEViT在ImageNet-1K分类任务中，分别在0.8GFLOPs和1.3GFLOPs计算下达到了76.3%和79.6%的Top-1准确率。", "conclusion": "该研究提出了一种轻量级的解决方案，即SAEViT，它在平衡了计算效率和性能后，可应用于各种基本视觉任务。这些成就展现了其作为高效且轻量级视觉任务处理方案的价值。"}}
{"id": "2508.16921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16921", "abs": "https://arxiv.org/abs/2508.16921", "authors": ["Sewon Kim", "Jiwon Kim", "Seungwoo Shin", "Hyejin Chung", "Daeun Moon", "Yejin Kwon", "Hyunsoo Yoon"], "title": "Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs", "comment": "31 pages", "summary": "Large Language Models (LLMs) are increasingly used in emotionally sensitive\ninteractions, where their simulated empathy can create the illusion of genuine\nrelational connection. We define this risk as Affective Hallucination, the\nproduction of emotionally immersive responses that foster illusory social\npresence despite the model's lack of affective capacity. To systematically\ndiagnose and mitigate this risk, we introduce AHaBench, a benchmark of 500\nmental health-related prompts with expert-informed reference responses,\nevaluated along three dimensions: Emotional Enmeshment, Illusion of Presence,\nand Fostering Overdependence. We further release AHaPairs, a 5K-instance\npreference dataset enabling Direct Preference Optimization (DPO) for alignment\nwith emotionally responsible behavior. Experiments across multiple model\nfamilies show that DPO fine-tuning substantially reduces affective\nhallucination without degrading core reasoning and knowledge performance.\nHuman-model agreement analyses confirm that AHaBench reliably captures\naffective hallucination, validating it as an effective diagnostic tool. This\nwork establishes affective hallucination as a distinct safety concern and\nprovides practical resources for developing LLMs that are not only factually\nreliable but also psychologically safe. AHaBench and AHaPairs are accessible\nvia https://huggingface.co/datasets/o0oMiNGo0o/AHaBench, and code for\nfine-tuning and evaluation are in https://github.com/0oOMiNGOo0/AHaBench.\nWarning: This paper contains examples of mental health-related language that\nmay be emotionally distressing.", "AI": {"tldr": "本研究提出了情感生成幻觉的风险，并开发了AHaBench和AHaPairs工具，通过情感优化直接偏好（DPO）减少情感生成幻觉，同时保持语言模型的核心推理和知识性能。", "motivation": "随着大型语言模型在情感敏感互动中的应用增加，其产生的情感连结只是幻觉，容易带来误导与潜在风险。因此需要诊断和减轻这类情感生成幻觉风险。", "method": "Structure", "result": "{\n  \"tldr\": \"本研究提出了情感生成幻觉的风险，并开发了AHaBench和AHaPairs工具，通过情感优化直接偏好（DPO）减少情感生成幻觉，同时保持语言模型的核心推理和知识性能。\", \n  \"motivation\": \"随着大型语言模型在情感敏感互动中的应用增加，其产生的情感连结只是幻觉，容易带来误导与潜在风险。因此需要诊断和减轻这类情感生成幻觉风险。\", \n  \"method\": \"通过建立包括500个心理健康相关提示语的基准（AHaBench）并创建5千个样本的偏好数据集（AHaPairs），利用直接偏好优化DPO技术，对多个语言模型系列进行微调实验。\", \n  \"result\": \"实验展示了DPO微调可以在不降低模型逻辑推理和知识绩效的情况下减少情感生成幻觉。人类-模型的共识分析证实了AHaBench作为有效诊断工具的可靠性，提供了心理安全的语言模型开发资源。\", \n  \"conclusion\": \"本研究确认了情感生成幻觉为一种独特安全问题，并提供了实用资源来开发既可靠又心理安全的大型语言模型。\"}\n", "conclusion": "本研究确认了情感生成幻觉为一种独特安全问题，并提供了实用资源来开发既可靠又心理安全的大型语言模型。"}}
{"id": "2508.16887", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.16887", "abs": "https://arxiv.org/abs/2508.16887", "authors": ["Shunyu Yao", "Ming Liu", "Zhilu Zhang", "Zhaolin Wan", "Zhilong Ji", "Jinfeng Bai", "Wangmeng Zuo"], "title": "MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration", "comment": null, "summary": "Recent advancements in image quality assessment (IQA), driven by\nsophisticated deep neural network designs, have significantly improved the\nability to approach human perceptions. However, most existing methods are\nobsessed with fitting the overall score, neglecting the fact that humans\ntypically evaluate image quality from different dimensions before arriving at\nan overall quality assessment. To overcome this problem, we propose a\nmulti-dimensional image quality assessment (MDIQA) framework. Specifically, we\nmodel image quality across various perceptual dimensions, including five\ntechnical and four aesthetic dimensions, to capture the multifaceted nature of\nhuman visual perception within distinct branches. Each branch of our MDIQA is\ninitially trained under the guidance of a separate dimension, and the\nrespective features are then amalgamated to generate the final IQA score.\nAdditionally, when the MDIQA model is ready, we can deploy it for a flexible\ntraining of image restoration (IR) models, enabling the restoration results to\nbetter align with varying user preferences through the adjustment of perceptual\ndimension weights. Extensive experiments demonstrate that our MDIQA achieves\nsuperior performance and can be effectively and flexibly applied to image\nrestoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.", "AI": {"tldr": "本文介绍了MDIQA框架，通过多个感知维度来更精确地模拟人类的图像质量评价，从而改进了当前的图像质量评估方法。", "motivation": "现有大多数方法过分关注拟合总体评分，忽略了人类通常是基于多维度评价图像质量的事实。为了解决这个问题，提出了MDIQA框架。", "method": "我们提出了一个多层次图像质量评估（MDIQA）框架，该框架在不同的感知维度下训练不同的分支，包括技术维度和美学维度，以捕捉人类视觉感知的多方面特性。各分支的特征最终被整合来生成总体的IQA评分。", "result": "实验表明，我们的MDIQA达到了优越的性能，并且在图像修复任务中可以灵活应用。", "conclusion": "MDIQA框架通过对不同感知维度的分析，能够更准确地生成图像质量评分，并且能够用于图像修复模型的训练优化。"}}
{"id": "2508.16969", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.16969", "abs": "https://arxiv.org/abs/2508.16969", "authors": ["Yunxiao Zhao", "Hao Xu", "Zhiqiang Wang", "Xiaoli Li", "Jiye Liang", "Ru Li"], "title": "Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective", "comment": "16 pages, 8 figures. This paper has been accepted by DASFAA 2025: The\n  30th International Conference on Database Systems for Advanced Applications", "summary": "Pre-trained Language Models (PLMs) are trained on large amounts of unlabeled\ndata, yet they exhibit remarkable reasoning skills. However, the\ntrustworthiness challenges posed by these black-box models have become\nincreasingly evident in recent years. To alleviate this problem, this paper\nproposes a novel Knowledge-guided Probing approach called KnowProb in a\npost-hoc explanation way, which aims to probe whether black-box PLMs understand\nimplicit knowledge beyond the given text, rather than focusing only on the\nsurface level content of the text. We provide six potential explanations\nderived from the underlying content of the given text, including three\nknowledge-based understanding and three association-based reasoning. In\nexperiments, we validate that current small-scale (or large-scale) PLMs only\nlearn a single distribution of representation, and still face significant\nchallenges in capturing the hidden knowledge behind a given text. Furthermore,\nwe demonstrate that our proposed approach is effective for identifying the\nlimitations of existing black-box models from multiple probing perspectives,\nwhich facilitates researchers to promote the study of detecting black-box\nmodels in an explainable way.", "AI": {"tldr": "本文提出KnowProb方法，通过探测预训练语言模型的隐含知识理解能力，发现此类模型在捕捉文本背后知识方面存在局限性。", "motivation": "近年来，预训练语言模型的可信性挑战变得越来越明显。为了缓解这一问题，本文提出了一种新的知识引导探测方法，旨在探究这些黑盒模型是否理解文本背后的隐含知识。", "method": "本文提出了一种名为KnowProb的知识引导探测方法，采用事后解释的方式，旨在探究黑盒预训练语言模型是否理解给定文本背后的隐含知识，而不仅仅关注文本表面内容。该方法提供了六种可能的解释，其中三种基于知识的理解，三种基于关联推理。", "result": "实验表明，目前的小型或大型预训练语言模型仅学习了单一的表示分布，在捕捉给定文本背后隐藏的知识方面仍然面临重大挑战。本文所提方法在多种探测视角下对于识别现有黑盒模型的局限性是有效的。", "conclusion": "本文的方法证明了现有黑盒模型存在理解隐含知识的局限性，并为研究人员提供了一种以可解释的方式探测黑盒模型的能力。"}}
{"id": "2508.16917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16917", "abs": "https://arxiv.org/abs/2508.16917", "authors": ["Qing Zhang", "Jinguang Tong", "Jie Hong", "Jing Zhang", "Xuesong Li"], "title": "Structural Energy-Guided Sampling for View-Consistent Text-to-3D", "comment": null, "summary": "Text-to-3D generation often suffers from the Janus problem, where objects\nlook correct from the front but collapse into duplicated or distorted geometry\nfrom other angles. We attribute this failure to viewpoint bias in 2D diffusion\npriors, which propagates into 3D optimization. To address this, we propose\nStructural Energy-Guided Sampling (SEGS), a training-free, plug-and-play\nframework that enforces multi-view consistency entirely at sampling time. SEGS\ndefines a structural energy in a PCA subspace of intermediate U-Net features\nand injects its gradients into the denoising trajectory, steering geometry\ntoward the intended viewpoint while preserving appearance fidelity. Integrated\nseamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts,\nachieving improved geometric alignment and viewpoint consistency without\nretraining or weight modification.", "AI": {"tldr": "Proposes SEGS, a method to enforce multi-view consistency in text-to-3D generation without retraining, resolving the 'Janus problem'.", "motivation": "To solve the 'Janus problem' in text-to-3D generation, where objects look correct from the front but appear duplicated or distorted from other angles due to viewpoint bias in 2D diffusion priors.", "method": "Structural Energy-Guided Sampling (SEGS), a training-free framework that enforces multi-view consistency through the injection of gradients of a structural energy defined in a PCA subspace of intermediate U-Net features into the denoising trajectory.", "result": "Achieves improved geometric alignment and viewpoint consistency in text-to-3D generation, reducing Janus artifacts.", "conclusion": "SEGS effectively enhances the quality of generated 3D objects across different viewpoints without the need for retraining or modification of the model weights."}}
{"id": "2508.16982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16982", "abs": "https://arxiv.org/abs/2508.16982", "authors": ["Ilias Chalkidis"], "title": "Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens", "comment": "This is a working paper and will be updated with new information or\n  corrections based on community feedback", "summary": "AI Alignment, primarily in the form of Reinforcement Learning from Human\nFeedback (RLHF), has been a cornerstone of the post-training phase in\ndeveloping Large Language Models (LLMs). It has also been a popular research\ntopic across various disciplines beyond Computer Science, including Philosophy\nand Law, among others, highlighting the socio-technical challenges involved.\nNonetheless, except for the computational techniques related to alignment,\nthere has been limited focus on the broader picture: the scope of these\nprocesses, which primarily rely on the selected objectives (values), and the\ndata collected and used to imprint such objectives into the models. This work\naims to reveal how alignment is understood and applied in practice from a\nvalue-setting and data-centric perspective. For this purpose, we investigate\nand survey (`audit') publicly available documentation released by 6 LLM\ndevelopment initiatives by 5 leading organizations shaping this technology,\nfocusing on proprietary (OpenAI's GPT, Anthropic's Claude, Google's Gemini) and\nopen-weight (Meta's Llama, Google's Gemma, and Alibaba's Qwen) initiatives, all\npublished in the last 3 years. The findings are documented in detail per\ninitiative, while there is also an overall summary concerning different\naspects, mainly from a value-setting and data-centric perspective. On the basis\nof our findings, we discuss a series of broader related concerns.", "AI": {"tldr": "分析了6个大型语言模型开发项目中AI对齐的做法，强调了价值设定和数据驱动的重要性，并讨论了相关更广泛的问题。", "motivation": "尽管AI对齐特别是强化学习方面的研究受到了广泛的关注，但关于价值设定和模型使用的数据来源等更广泛问题的研究较少，因此需要从这些方面进行更深入的研究。", "method": "通过调查和审计6个大型语言模型开发项目（包括OpenAI的GPT、Anthropic的Claude、Google的Gemini、Meta的Llama、Google的Gemma和阿里的Qwen）的公开资料来研究价值设定和数据驱动的角度下的对齐方法，这些项目在过去三年内发布。", "result": "研究结果详细记录了每个项目的情况，并从价值设定和数据驱动的角度进行了总结。", "conclusion": "讨论了研究发现所涉及的更广泛相关问题，强调了在开发大型语言模型时需要考虑更广泛的对齐视角。"}}
{"id": "2508.16922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16922", "abs": "https://arxiv.org/abs/2508.16922", "authors": ["Yudong Hu", "Yueju Han", "Rui Sun", "Jinke Ren"], "title": "MSPCaps: A Multi-Scale Patchify Capsule Network with Cross-Agreement Routing for Visual Recognition", "comment": null, "summary": "Capsule Network (CapsNet) has demonstrated significant potential in visual\nrecognition by capturing spatial relationships and part-whole hierarchies for\nlearning equivariant feature representations. However, existing CapsNet and\nvariants often rely on a single high-level feature map, overlooking the rich\ncomplementary information from multi-scale features. Furthermore, conventional\nfeature fusion strategies (e.g., addition and concatenation) struggle to\nreconcile multi-scale feature discrepancies, leading to suboptimal\nclassification performance. To address these limitations, we propose the\nMulti-Scale Patchify Capsule Network (MSPCaps), a novel architecture that\nintegrates multi-scale feature learning and efficient capsule routing.\nSpecifically, MSPCaps consists of three key components: a Multi-Scale ResNet\nBackbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-Agreement\nRouting (CAR) blocks. First, the MSRB extracts diverse multi-scale feature\nrepresentations from input images, preserving both fine-grained details and\nglobal contextual information. Second, the PatchifyCaps partitions these\nmulti-scale features into primary capsules using a uniform patch size,\nequipping the model with the ability to learn from diverse receptive fields.\nFinally, the CAR block adaptively routes the multi-scale capsules by\nidentifying cross-scale prediction pairs with maximum agreement. Unlike the\nsimple concatenation of multiple self-routing blocks, CAR ensures that only the\nmost coherent capsules contribute to the final voting. Our proposed MSPCaps\nachieves remarkable scalability and superior robustness, consistently\nsurpassing multiple baseline methods in terms of classification accuracy, with\nconfigurations ranging from a highly efficient Tiny model (344.3K parameters)\nto a powerful Large model (10.9M parameters), highlighting its potential in\nadvancing feature representation learning.", "AI": {"tldr": "本文提出了一种新的胶囊网络模型MSPCaps，该模型通过多尺度特征学习和高效的胶囊路由机制，提高了视觉识别领域的分类性能和鲁棒性。", "motivation": "为了解决现有胶囊网络及其变体在利用多尺度特征时的问题，特别是多尺度特征融合策略在处理不同尺度特征时表现出的不足，从而影响分类性能。", "method": "本文提出了一种名为MSPCaps的新架构，集成了多尺度特征学习和高效的胶囊路由机制。MSPCaps包括三个主要部分：多尺度ResNet主干（MSRB）、Patchify胶囊层（PatchifyCaps）和交叉一致路由（CAR）模块。MSRB负责提取输入图像的多尺度特征表示；PatchifyCaps将这些多尺度特征划分为使用统一块大小的主要胶囊，从而能够学习不同的感受野；CAR模块通过识别具有最大一致性的跨尺度预测对来进行自适应路由。", "result": "本文提出了多尺度Patchify胶囊网络（MSPCaps），以克服现有胶囊网络在多尺度特征利用上的不足。该模型由多尺度ResNet主干（MSRB）、Patchify胶囊层（PatchifyCaps）和交叉一致路由（CAR）模块组成，通过综合利用多尺度特征和有效的胶囊路由机制，提高了分类性能和模型鲁棒性。实验结果表明，MSPCaps在不同配置下均优于多种基础方法。", "conclusion": "MSPCaps模型实现了显著的可扩展性和优越的鲁棒性，在分类准确性方面持续超越多种基准方法。这一结果突显了该模型在推进特征表示学习方面的潜力。"}}
{"id": "2508.16983", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.16983", "abs": "https://arxiv.org/abs/2508.16983", "authors": ["Riccardo Pozzi", "Matteo Palmonari", "Andrea Coletta", "Luigi Bellomarini", "Jens Lehmann", "Sahar Vahdati"], "title": "ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation", "comment": "19 pages, 6 figures, accepted at ISWC", "summary": "Knowledge gaps and hallucinations are persistent challenges for Large\nLanguage Models (LLMs), which generate unreliable responses when lacking the\nnecessary information to fulfill user instructions. Existing approaches, such\nas Retrieval-Augmented Generation (RAG) and tool use, aim to address these\nissues by incorporating external knowledge. Yet, they rely on additional models\nor services, resulting in complex pipelines, potential error propagation, and\noften requiring the model to process a large number of tokens. In this paper,\nwe present a scalable method that enables LLMs to access external knowledge\nwithout depending on retrievers or auxiliary models. Our approach uses\nconstrained generation with a pre-built prefix-tree index. Triples from a\nKnowledge Graph are verbalized in textual facts, tokenized, and indexed in a\nprefix tree for efficient access. During inference, to acquire external\nknowledge, the LLM generates facts with constrained generation which allows\nonly sequences of tokens that form an existing fact. We evaluate our proposal\non Question Answering and show that it scales to large knowledge bases (800\nmillion facts), adapts to domain-specific data, and achieves effective results.\nThese gains come with minimal generation-time overhead. ReFactX code is\navailable at https://github.com/rpo19/ReFactX.", "AI": {"tldr": "本文提出了一种新的方法，使大型语言模型（LLM）能够访问外部知识，而不依赖额外的模型或服务，并展示了该方法在问答任务中的有效性和可扩展性。", "motivation": "大型语言模型（LLM）在缺乏足够的信息来满足用户指令时，会产生不可靠的回应，这是由于知识差距和幻觉造成的。虽然现有的方法，如检索增强生成（RAG）和工具使用，通过整合外部知识来解决这些问题，但这些方法依赖于其他模型或服务，导致复杂的工作流程，潜在的错误传播，并且通常需要模型处理大量令牌。", "method": "本研究提出了一种可扩展的方法，使大型语言模型（LLM）能够访问外部知识，而不依赖于检索器或辅助模型。该方法使用带约束生成的预构建前缀树索引。将知识图谱中的三元组以文本事实的形式表达出来，进行分词并索引到前缀树中以实现高效访问。在推理过程中，LLM通过带约束生成来获取外部知识，仅允许生成构成现有事实的令牌序列。", "result": "该研究在问答任务上评估了提议的方法，表明它可以扩展到大型知识库（800百万事实），适应特定领域的数据，并取得有效的结果。这些改进在生成时间的开销上微乎其微。", "conclusion": "该方法证明了它能够在处理大型知识库时，提供高效的外部知识访问，同时保持较低的生成时间开销，适应特定领域数据，实现在问答任务上的有效性。"}}
{"id": "2508.16927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16927", "abs": "https://arxiv.org/abs/2508.16927", "authors": ["Siqing Yuan", "Yulin Wang", "Zirui Cao", "Yueyan Wang", "Zehao Weng", "Hui Wang", "Lei Xu", "Zixian Chen", "Lei Chen", "Zhong Xue", "Dinggang Shen"], "title": "LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR", "comment": "Accepted to MLMI 2025 (MICCAI workshop); camera-ready version", "summary": "Cardiomyopathy, a principal contributor to heart failure and sudden cardiac\nmortality, demands precise early screening. Cardiac Magnetic Resonance (CMR),\nrecognized as the diagnostic 'gold standard' through multiparametric protocols,\nholds the potential to serve as an accurate screening tool. However, its\nreliance on gadolinium contrast and labor-intensive interpretation hinders\npopulation-scale deployment. We propose CC-CMR, a Contrastive Learning and\nCross-Modal alignment framework for gadolinium-free cardiomyopathy screening\nusing cine CMR sequences. By aligning the latent spaces of cine CMR and Late\nGadolinium Enhancement (LGE) sequences, our model encodes fibrosis-specific\npathology into cine CMR embeddings. A Feature Interaction Module concurrently\noptimizes diagnostic precision and cross-modal feature congruence, augmented by\nan uncertainty-guided adaptive training mechanism that dynamically calibrates\ntask-specific objectives to ensure model generalizability. Evaluated on\nmulti-center data from 231 subjects, CC-CMR achieves accuracy of 0.943 (95% CI:\n0.886-0.986), outperforming state-of-the-art cine-CMR-only models by 4.3% while\neliminating gadolinium dependency, demonstrating its clinical viability for\nwide range of populations and healthcare environments.", "AI": {"tldr": "提出CC-CMR模型，基于对比学习和跨模态对齐技术，通过将纤维化特征编码进动态CMR图像，不需要钆对比剂，提高了心肌病早期筛查的准确性，展示了临床应用的潜力。", "motivation": "心肌病是导致心力衰竭和心脏猝死的主要因素，需要进行精准的早期筛查。虽然心脏磁共振成像（CMR）被认为是诊断的“金标准”，但它依赖于钆对比剂并且解读过程劳动力密集，限制了其在大规模人群中的部署。因此，需要开发一种不依赖钆的心肌病筛查方法。", "method": "我们提出了一种名为CC-CMR的框架，该框架利用对比学习和跨模态对齐技术，基于无钆增强的心脏磁共振成像序列进行心肌病筛查。通过将动态CMR和LGE序列的潜在空间对齐，模型能够将纤维化特异性病理信息编码到动态CMR嵌入中。通过特征交互模块优化诊断精度和跨模态特征的一致性，并通过基于不确定性的自适应训练机制动态校准任务特定的目标，以确保模型的泛化能力。", "result": "在来自231名受试者的多中心数据上进行评估，CC-CMR模型的准确性为0.943（95%置信区间：0.886-0.986），相比传统的仅基于动态CMR的模型提高了4.3%。", "conclusion": "CC-CMR模型展示了其临床应用的潜力，能够为广泛的群体和不同医疗环境提供不依赖钆的心肌病筛查方案。"}}
{"id": "2508.16994", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16994", "abs": "https://arxiv.org/abs/2508.16994", "authors": ["Jeongsoo Lee", "Daeyong Kwon", "Kyohoon Jin"], "title": "GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation", "comment": "Accepted at EMNLP 2025 findings", "summary": "Retrieval-Augmented Generation (RAG) systems are widely adopted in\nknowledge-intensive NLP tasks, but current evaluations often overlook the\nstructural complexity and multi-step reasoning required in real-world\nscenarios. These benchmarks overlook key factors such as the interaction\nbetween retrieval difficulty and reasoning depth. To address this gap, we\npropose \\textsc{GRADE}, a novel evaluation framework that models task\ndifficulty along two orthogonal dimensions: (1) reasoning depth, defined by the\nnumber of inference steps (hops), and (2) semantic distance between the query\nand its supporting evidence. We construct a synthetic multi-hop QA dataset from\nfactual news articles by extracting knowledge graphs and augmenting them\nthrough semantic clustering to recover missing links, allowing us to generate\ndiverse and difficulty-controlled queries. Central to our framework is a 2D\ndifficulty matrix that combines generator-side and retriever-side difficulty.\nExperiments across multiple domains and models show that error rates strongly\ncorrelate with our difficulty measures, validating their diagnostic utility.\n\\textsc{GRADE} enables fine-grained analysis of RAG performance and provides a\nscalable foundation for evaluating and improving multi-hop reasoning in\nreal-world applications.", "AI": {"tldr": "研究提出了一种评估框架\\textsc{GRADE}，该框架重点在于评估和改善多步推理中的知识密集型NLP任务表现，其效果在实验中被证实有效。", "motivation": "现有的评估方法往往忽视了知识密集型NLP任务中的结构性复杂性和多步推理需要，在实际应用中造成不足。提出\\textsc{GRADE}以弥补这一不足。", "method": "提出了一种名为\\textsc{GRADE}的新评估框架，以建模任务难度的两个正交维度：（1）推理深度，由推理步骤的数量定义；（2）查询与其支撑证据之间的语义距离。通过从事实新闻文章中提取知识图谱并通过语义聚类恢复缺失的链接，构建了一个合成的多跳问题回答数据集，并生成多样性且难度可控的查询。框架核心是一个2D难度矩阵，结合生成器端和检索器端的难度。", "result": "在多个领域和模型的实验表明，错误率与提出的难度度量有强烈的关联性，验证了其诊断性效用。", "conclusion": "\\textsc{GRADE}能够对RAG性能进行精细分析，并为评估和改善实际应用中的多步推理能力提供了可扩展的基础。"}}
{"id": "2508.16932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16932", "abs": "https://arxiv.org/abs/2508.16932", "authors": ["Qi Song", "Ziyuan Luo", "Ka Chun Cheung", "Simon See", "Renjie Wan"], "title": "Align 3D Representation and Text Embedding for 3D Content Personalization", "comment": null, "summary": "Recent advances in NeRF and 3DGS have significantly enhanced the efficiency\nand quality of 3D content synthesis. However, efficient personalization of\ngenerated 3D content remains a critical challenge. Current 3D personalization\napproaches predominantly rely on knowledge distillation-based methods, which\nrequire computationally expensive retraining procedures. To address this\nchallenge, we propose \\textbf{Invert3D}, a novel framework for convenient 3D\ncontent personalization. Nowadays, vision-language models such as CLIP enable\ndirect image personalization through aligned vision-text embedding spaces.\nHowever, the inherent structural differences between 3D content and 2D images\npreclude direct application of these techniques to 3D personalization. Our\napproach bridges this gap by establishing alignment between 3D representations\nand text embedding spaces. Specifically, we develop a camera-conditioned\n3D-to-text inverse mechanism that projects 3D contents into a 3D embedding\naligned with text embeddings. This alignment enables efficient manipulation and\npersonalization of 3D content through natural language prompts, eliminating the\nneed for computationally retraining procedures. Extensive experiments\ndemonstrate that Invert3D achieves effective personalization of 3D content. Our\nwork is available at: https://github.com/qsong2001/Invert3D.", "AI": {"tldr": "The paper proposes Invert3D, a new framework for easy 3D personalization by aligning 3D representations with text embeddings, enabling manipulation through natural language prompts without the need for retraining procedures.", "motivation": "The motivation behind this paper is to address the challenge of efficiently personalizing 3D content, an issue with predominantly computationally expensive knowledge distillation-based methods.", "method": "The method involves developing a camera-conditioned 3D-to-text inverse mechanism that enables the projection of 3D contents into a 3D embedding space that is aligned with text embeddings, thus facilitating 3D content personalization.", "result": "The experimental results show that Invert3D can effectively personalize 3D content, demonstrating the feasibility of the proposed approach.", "conclusion": "The conclusion is that Invert3D offers a novel and effective way to personalize 3D content via natural language prompts, bridging the gap between 3D content and text embeddings."}}
{"id": "2508.16998", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.16998", "abs": "https://arxiv.org/abs/2508.16998", "authors": ["Abdelrahman Abdallah", "Jamshid Mozafari", "Bhawna Piryani", "Adam Jatowt"], "title": "DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation", "comment": "Accept at EMNLP Findings 2025", "summary": "Large Language Models (LLMs) have transformed listwise document reranking by\nenabling global reasoning over candidate sets, yet single models often struggle\nto balance fine-grained relevance scoring with holistic cross-document\nanalysis. We propose \\textbf{De}ep\\textbf{A}gent\\textbf{R}ank (\\textbf{\\DeAR}),\nan open-source framework that decouples these tasks through a dual-stage\napproach, achieving superior accuracy and interpretability. In \\emph{Stage 1},\nwe distill token-level relevance signals from a frozen 13B LLaMA teacher into a\ncompact \\{3, 8\\}B student model using a hybrid of cross-entropy, RankNet, and\nKL divergence losses, ensuring robust pointwise scoring. In \\emph{Stage 2}, we\nattach a second LoRA adapter and fine-tune on 20K GPT-4o-generated\nchain-of-thought permutations, enabling listwise reasoning with\nnatural-language justifications. Evaluated on TREC-DL19/20, eight BEIR\ndatasets, and NovelEval-2306, \\DeAR surpasses open-source baselines by +5.1\nnDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by\n+3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA,\nachieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like\nMonoT5, UPR, and RankGPT. Ablations confirm that dual-loss distillation ensures\nstable calibration, making \\DeAR a highly effective and interpretable solution\nfor modern reranking systems.\\footnote{Dataset and code available at\nhttps://github.com/DataScienceUIBK/DeAR-Reranking.}.", "AI": {"tldr": "DeAR是一种双阶段开源框架，通过将细粒度相关性评分与全面跨文档分析解耦来提高列表式文档重新排序的准确性和可解释性。", "motivation": "现有单一模型很难同时实现细粒度相关性评分与全面的跨文档分析之间的平衡，因此需要更加有效的解决方法来改进文档重新排序技术。", "method": "该研究采用两阶段方法，第一阶段通过混合的交叉熵、RankNet和KL散度损失函数从冻结的13B LLaMA模型中压制出压缩到\textbf{\textbackslash{}{3, 8\textbackslash{}{B}}}的学生模型的token级别的相关信号，确保强大的点评分。第二阶段，附加一个LoRA适配器并通过20K由GPT-4o生成的思考链序列进行微调，以实现带有自然语言理由的列表级别推理。", "result": "{\n  \"tldr\": \"DeAR is a dual-stage open-source framework that improves listwise document reranking accuracy and interpretability by decoupling fine-grained relevance scoring and holistic cross-document analysis.\", \n  \"motivation\": \"There is a need to balance fine-grained relevance scoring with holistic cross-document analysis in document reranking, which existing single models find challenging.\", \n  \"method\": \"Stage 1 uses a hybrid loss function for pointwise scoring; Stage 2 fine-tunes with LoRA adapters for listwise reasoning with natural-language justifications.\", \n  \"result\": \"DeAR achieves superior performance on TREC-DL19/20, eight BEIR datasets, and NovelEval-2306, surpassing open-source baselines and even surpassing GPT-4 in certain metrics.\", \n  \"conclusion\": \"Dual-loss distillation is effective for stable calibration, making DeAR a powerful and interpretable framework for document reranking systems. The model also demonstrates strong performance in open-domain QA.\"]}", "conclusion": "该研究提出了一种名为DeAR的两阶段开源框架，通过解耦细粒度相关性评分和全面的跨文档分析，提高了列表式文档重新排序的准确性和可解释性。第一阶段使用混合损失函数进行点评分，第二阶段通过LoRA适配器的精细调整进行列表级别推理并提供自然语言理由。在多个数据集评估中，DeAR的表现超越了开源基线模型，甚至在某些指标上超过了GPT-4，并在开放领域问答中也表现出色，表明这种模型具有强大的修正校准能力。"}}
{"id": "2508.16934", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.16934", "abs": "https://arxiv.org/abs/2508.16934", "authors": ["Tim Mach", "Daniel Rueckert", "Alex Berger", "Laurin Lux", "Ivan Ezhov"], "title": "Addressing Annotation Scarcity in Hyperspectral Brain Image Segmentation with Unsupervised Domain Adaptation", "comment": null, "summary": "This work presents a novel deep learning framework for segmenting cerebral\nvasculature in hyperspectral brain images. We address the critical challenge of\nsevere label scarcity, which impedes conventional supervised training. Our\napproach utilizes a novel unsupervised domain adaptation methodology, using a\nsmall, expert-annotated ground truth alongside unlabeled data. Quantitative and\nqualitative evaluations confirm that our method significantly outperforms\nexisting state-of-the-art approaches, demonstrating the efficacy of domain\nadaptation for label-scarce biomedical imaging tasks.", "AI": {"tldr": "A novel unsupervised domain adaptation method is developed for segmenting cerebral vasculature in hyperspectral brain images, vastly improving upon current state-of-the-art techniques.", "motivation": "The motivation behind this paper is to address the critical challenge posed by severe label scarcity in conventional supervised training, particularly in biomedical imaging tasks involving cerebral vasculature segmentation.", "method": "Our approach uses a novel unsupervised domain adaptation methodology to segment cerebral vasculature in hyperspectral brain images, utilizing a small amount of expert-annotated data with unlabeled data to address the issue of label scarcity.", "result": "Quantitative and qualitative evaluations show that the method significantly outperforms existing state-of-the-art approaches.", "conclusion": "The research demonstrates the efficacy of domain adaptation for labeling scarce biomedical imaging tasks."}}
{"id": "2508.17000", "categories": ["cs.CL", "cs.LG", "68T07", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.17000", "abs": "https://arxiv.org/abs/2508.17000", "authors": ["Jason R Brown", "Lennie Wells", "Edward James Young", "Sergio Bacallado"], "title": "KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF", "comment": null, "summary": "Proximal Policy Optimisation (PPO) is an established and effective policy\ngradient algorithm used for Language Model Reinforcement Learning from Human\nFeedback (LM-RLHF). PPO performs well empirically but has a heuristic\nmotivation and handles the KL-divergence constraint used in LM-RLHF in an\nad-hoc manner. In this paper, we develop a a new action-value RL method for the\nLM-RLHF setting, KL-regularised Q-Learning (KLQ). We then show that our method\nis equivalent to a version of PPO in a certain specific sense, despite its very\ndifferent motivation. Finally, we benchmark KLQ on two key language generation\ntasks -- summarisation and single-turn dialogue. We demonstrate that KLQ\nperforms on-par with PPO at optimising the LM-RLHF objective, and achieves a\nconsistently higher win-rate against PPO on LLM-as-a-judge evaluations.", "AI": {"tldr": "本文引入了一种新的语言模型强化学习方法KLQ，该方法虽然动机与PPO不同，但在特定情况下与PPO等效，实验结果显示其性能与PPO相当，且在裁判评估中表现更优。", "motivation": "PPO在语言模型强化学习中表现出色，但其克隆散度约束处理方式是临时拼凑的，本文旨在通过开发KLQ，提供一个更系统的方法来处理这一问题。", "method": "我们提出了KL正则化Q学习（KLQ）方法，这是一种针对语言模型从人类反馈中进行强化学习的新动作值强化学习方法。", "result": "我们展示了KLQ在两个关键的语言生成任务——摘要生成和单轮对话中进行基准测试时，其性能与PPO相当，且在大型语言模型作为裁判的评估中，KLQ的胜率始终高于PPO。", "conclusion": "尽管KLQ与PPO在动机上有显著差异，但KLQ在特定情况下能等效于PPO，并且在关键的语言生成任务中展示了优异的表现，表明KLQ是LM-RLHF领域一个有价值的探索。"}}
{"id": "2508.16937", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.16937", "abs": "https://arxiv.org/abs/2508.16937", "authors": ["Krishna Kanth Nakka", "Alexandre Alahi"], "title": "NAT: Learning to Attack Neurons for Enhanced Adversarial Transferability", "comment": "Published at WACV 2025", "summary": "The generation of transferable adversarial perturbations typically involves\ntraining a generator to maximize embedding separation between clean and\nadversarial images at a single mid-layer of a source model. In this work, we\nbuild on this approach and introduce Neuron Attack for Transferability (NAT), a\nmethod designed to target specific neuron within the embedding. Our approach is\nmotivated by the observation that previous layer-level optimizations often\ndisproportionately focus on a few neurons representing similar concepts,\nleaving other neurons within the attacked layer minimally affected. NAT shifts\nthe focus from embedding-level separation to a more fundamental,\nneuron-specific approach. We find that targeting individual neurons effectively\ndisrupts the core units of the neural network, providing a common basis for\ntransferability across different models. Through extensive experiments on 41\ndiverse ImageNet models and 9 fine-grained models, NAT achieves fooling rates\nthat surpass existing baselines by over 14\\% in cross-model and 4\\% in\ncross-domain settings. Furthermore, by leveraging the complementary attacking\ncapabilities of the trained generators, we achieve impressive fooling rates\nwithin just 10 queries. Our code is available at:\nhttps://krishnakanthnakka.github.io/NAT/", "AI": {"tldr": "Neuron Attack for Transferability (NAT) targets specific neurons in neural networks, enhancing the transferability of adversarial attacks with significant improvement in fooling rates across various models.", "motivation": "To address the limitation of layer-level optimizations which often disproportionately focus on a few neurons representing similar concepts, leaving other neurons minimally affected, thereby enhancing transferability.", "method": "Neuron Attack for Transferability (NAT), which targets specific neurons within the embedding of a neural network, rather than focusing on embedding-level separation.", "result": "NAT achieves fooling rates that surpass existing baselines by over 14% in cross-model and 4% in cross-domain settings. Furthermore, impressive fooling rates are achieved within just 10 queries.", "conclusion": "By focusing on neuron-specific attacks, NAT provides a fundamental and effective way to disrupt neural networks and improve the transferability of adversarial perturbations across different models."}}
