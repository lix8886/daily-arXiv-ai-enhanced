<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MedPI: Evaluating AI Systems in Medical Patient-facing Interactions](https://arxiv.org/abs/2601.04195)
*Diego Fajardo V.,Oleksii Proniakin,Victoria-Elisabeth Gruber,Razvan Marinescu*

Main category: cs.CL

> 本文介绍了MedPI，这是一个用于评估大型语言模型在医患对话中表现的高维度基准测试。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在填补现有问答基准测试无法充分评估医患对话质量的空白，通过详细和细致的标准来评估大型语言模型在医疗对话中的表现。

**Method:** MedPI包括五个层次：病人档案、AI病人、任务矩阵、评估框架和AI判官。通过这些层次，MedPI能够全面评估大型语言模型在医患对话中的性能。

**Result:** 评估结果显示，所有被测试的大型语言模型在多个维度上的表现较低，特别是在鉴别诊断方面。

**Conclusion:** 该研究可以指导未来大型语言模型在诊断和治疗建议中的应用。

**Abstract:** We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized "vanilla clinician" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.

</details>


### [2] [RAGVUE: A Diagnostic View for Explainable and Automated Evaluation of Retrieval-Augmented Generation](https://arxiv.org/abs/2601.04196)
*Keerthana Murugaraj,Salima Lamsiyah,Martin Theobald*

Main category: cs.CL

> 提出了RAGVUE框架，一个用于自动化、无参考的RAG系统评估的诊断和可解释框架。框架提供了多个指标以及不同的使用方式，包括Python API、CLI和本地GUI。在对比实验中显示优越性。

<details>
  <summary>Details</summary>

**Motivation:** 当前用于评估RAG系统的指标往往将异构行为简化为单一得分，并且很少提供关于检索、推理或内容关联中的错误来源的见解。

**Method:** 介绍了RAGVUE框架，该框架用于RAG系统的诊断和可解释性评估。该评估过程透明，并且可以支持手动选择指标或完全自动化的代理评估。框架提供了Python API、命令行接口以及本地Streamlit界面。


**Result:** 在对比实验中，RAGVUE揭示了现有工具如RAGAS常常忽视的细粒度失败情况。


**Conclusion:** 展示了RAGVUE的整个工作流程，并展示了它如何能够结合到研究管线和实用的RAG开发中去。该框架的源代码和使用说明在GitHub上免费提供。


**Abstract:** Evaluating Retrieval-Augmented Generation (RAG) systems remains a challenging task: existing metrics often collapse heterogeneous behaviors into single scores and provide little insight into whether errors arise from retrieval,reasoning, or grounding. In this paper, we introduce RAGVUE, a diagnostic and explainable framework for automated, reference-free evaluation of RAG pipelines. RAGVUE decomposes RAG behavior into retrieval quality, answer relevance and completeness, strict claim-level faithfulness, and judge calibration. Each metric includes a structured explanation, making the evaluation process transparent. Our framework supports both manual metric selection and fully automated agentic evaluation. It also provides a Python API, CLI, and a local Streamlit interface for interactive usage. In comparative experiments, RAGVUE surfaces fine-grained failures that existing tools such as RAGAS often overlook. We showcase the full RAGVUE workflow and illustrate how it can be integrated into research pipelines and practical RAG development. The source code and detailed instructions on usage are publicly available on GitHub

</details>


### [3] [Automatic Construction of Chinese Verb Collostruction Database](https://arxiv.org/abs/2601.04197)
*Xuri Tang,Daohuan Liu*

Main category: cs.CL

> 本文提出了一种完全无监督的方法来构建中文动词搭配数据库，以补充LLMs，提供明确且可解释的应用场景规则。

<details>
  <summary>Details</summary>

**Motivation:** 旨在通过提供明确和可解释的规则来补充LLMs，在解释和可解释性至关重要的应用场景中发挥作用。

**Method:** 正式定义动词搭配为一种投影式、有根、有序且有向的无环图，并采用一系列聚类算法从大规模语料库中提取的句子列表生成给定动词的搭配。

**Result:** 统计分析表明，生成的搭配具备功能独立和等级典型性的设计特征，并且使用基于搭配的最大匹配进行动词语法错误纠正的算法比LLMs表现更好。

**Conclusion:** 展示了所提方法在提供解释性和准确性方面的优势。

**Abstract:** This paper proposes a fully unsupervised approach to the construction of verb collostruction database for Chinese language, aimed at complementing LLMs by providing explicit and interpretable rules for application scenarios where explanation and interpretability are indispensable. The paper formally defines a verb collostruction as a projective, rooted, ordered, and directed acyclic graph and employs a series of clustering algorithms to generate collostructions for a given verb from a list of sentences retrieved from large-scale corpus. Statistical analysis demonstrates that the generated collostructions possess the design features of functional independence and graded typicality. Evaluation with verb grammatical error correction shows that the error correction algorithm based on maximum matching with collostructions achieves better performance than LLMs.

</details>


### [4] [Attribute-Aware Controlled Product Generation with LLMs for E-commerce](https://arxiv.org/abs/2601.04200)
*Virginia Negri,Víctor Martínez Gómez,Sergio A. Balanya,Subburam Rajaram*

Main category: cs.CL

> 通过大型语言模型生成合成电子商务产品数据，效果显著，可以作为真实数据的有效补充，特别是在资源有限的情况下。

<details>
  <summary>Details</summary>

**Motivation:** 电子商务服务中产品信息提取至关重要，但获得高质量的标注数据仍然具有挑战性。本研究旨在解决这一问题，提供一种合成电子商务产品数据的方法。

**Method:** 使用大型语言模型(LLMs)生成合成的电子商务产品数据，提出了一个带三种策略的控制修改框架：属性保持的修改、受控负例生成以及系统性属性去除。通过使用具有属性感知提示的最先进的LLM，我们保持了店铺约束条件的同时也确保了产品的连贯性。

**Result:** 通过人类对2000个合成产品进行评价，表现出色，99.6%被评定为自然的，96.5%包含有效的属性值，且超过90%使用一致的属性。在公开的MAVE数据集上，合成数据达到了60.5%的准确率，与真实训练数据(60.8%)相当，并显著超过了零样本基准(13.4%)。结合合成和真实数据的混合配置进一步提高了性能，达到了68.8%的准确率。

**Conclusion:** 我们的框架为增强电子商务数据集提供了一个实用的解决方案，特别是在资源匮乏的情况下具有很高的价值。

**Abstract:** Product information extraction is crucial for e-commerce services, but obtaining high-quality labeled datasets remains challenging. We present a systematic approach for generating synthetic e-commerce product data using Large Language Models (LLMs), introducing a controlled modification framework with three strategies: attribute-preserving modification, controlled negative example generation, and systematic attribute removal. Using a state-of-the-art LLM with attribute-aware prompts, we enforce store constraints while maintaining product coherence. Human evaluation of 2000 synthetic products demonstrates high effectiveness, with 99.6% rated as natural, 96.5% containing valid attribute values, and over 90% showing consistent attribute usage. On the public MAVE dataset, our synthetic data achieves 60.5% accuracy, performing on par with real training data (60.8%) and significantly improving upon the 13.4% zero-shot baseline. Hybrid configurations combining synthetic and real data further improve performance, reaching 68.8% accuracy. Our framework provides a practical solution for augmenting e-commerce datasets, particularly valuable for low-resource scenarios.

</details>


### [5] [Collective Narrative Grounding: Community-Coordinated Data Contributions to Improve Local AI Systems](https://arxiv.org/abs/2601.04201)
*Zihan Gao,Mohsin Y. K. Yousufi,Jacob Thebault-Spieker*

Main category: cs.CL

> 文章提出了集体叙事锚定协议，旨在将社区故事整合进AI系统以减少知识盲点和认识不公现象，并展示了该方法在解决本地问题中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 为了应对AI问答系统在处理社区特定查询时的失败，这种失败导致了“知识盲点”，边缘化了当地的声音，并强化了认识不公。

**Method:** 提出了一种名为集体叙事锚定的参与性协议，该协议将社区故事转化为结构化的叙事单元，并在社区治理下将其整合到AI系统中。通过与24名社区成员进行的三项参与性地图工作坊，设计了保留叙事丰富性的同时，能够抽取实体、时间和地点，验证并控制来源的方法和模式。

**Result:** 通过对一个县级基准测试的14,782个本地信息问答对的审计，发现76.7%的错误是由于事实差距、文化误解、地理混淆和时间错位造成的。在从工作坊中得出的参与性问答集中，最先进的语言模型在没有添加上下文的情况下正确回答问题的比例不到21%，进一步说明了本地锚定的需求。

**Conclusion:** 文章通过分类法、协议和参与性评估，为构建更好地回答当地问题的社区锚定AI提供了严格的依据，同时讨论了包括代表性与权力、治理与控制以及隐私与同意在内的关键设计张力。

**Abstract:** Large language model (LLM) question-answering systems often fail on community-specific queries, creating "knowledge blind spots" that marginalize local voices and reinforce epistemic injustice. We present Collective Narrative Grounding, a participatory protocol that transforms community stories into structured narrative units and integrates them into AI systems under community governance. Learning from three participatory mapping workshops with N=24 community members, we designed elicitation methods and a schema that retain narrative richness while enabling entity, time, and place extraction, validation, and provenance control. To scope the problem, we audit a county-level benchmark of 14,782 local information QA pairs, where factual gaps, cultural misunderstandings, geographic confusions, and temporal misalignments account for 76.7% of errors. On a participatory QA set derived from our workshops, a state-of-the-art LLM answered fewer than 21% of questions correctly without added context, underscoring the need for local grounding. The missing facts often appear in the collected narratives, suggesting a direct path to closing the dominant error modes for narrative items. Beyond the protocol and pilot, we articulate key design tensions, such as representation and power, governance and control, and privacy and consent, providing concrete requirements for retrieval-first, provenance-visible, locally governed QA systems. Together, our taxonomy, protocol, and participatory evaluation offer a rigorous foundation for building community-grounded AI that better answers local questions.

</details>


### [6] [TeleTables: A Benchmark for Large Language Models in Telecom Table Interpretation](https://arxiv.org/abs/2601.04202)
*Anas Ezzakri,Nicola Piovesan,Mohamed Sana,Antonio De Domenico,Fadhel Ayed,Haozhe Zhang*

Main category: cs.CL

> 我们提出了TeleTables，这一基准测试用于评估LLMs理解和解释技术规范中表格的能力。结果显示，较大的模型在表格解释上的推理能力更强，指出需要进行领域专用的微调来改进LLMs在这方面的表现。

<details>
  <summary>Details</summary>

**Motivation:** 最近的研究表明，LLMs在电信标准，特别是3GPP规范上的表现不佳。我们认为主要原因是这些标准中包含了大量表格来呈现重要信息，但LLMs对表格的理解能力尚待考察。

**Method:** 我们引入了TeleTables，这是一个基准测试，用于评估LLMs对技术规范中表格的隐性和显性理解能力。该基准通过一个新颖的多阶段数据生成流程构建，从3GPP标准中提取表格，并使用多模态和以推理为导向的LLMs生成并验证问题。

**Result:** 评估结果表明，较小的模型（参数少于100亿）在回忆3GPP知识和解释表格方面都存在困难，表明它们的预训练中对电信标准的接触有限，且不足以具备处理复杂技术材料的归纳偏见。较大的模型在这方面则表现出更强的推理能力。

**Conclusion:** 总的来说，TeleTables指出了需要领域专用的微调以可靠地解释和推理电信标准的需求。

**Abstract:** Language Models (LLMs) are increasingly explored in the telecom industry to support engineering tasks, accelerate troubleshooting, and assist in interpreting complex technical documents. However, recent studies show that LLMs perform poorly on telecom standards, particularly 3GPP specifications. We argue that a key reason is that these standards densely include tables to present essential information, yet the LLM knowledge and interpretation ability of such tables remains largely unexamined. To address this gap, we introduce TeleTables, a benchmark designed to evaluate both the implicit knowledge LLMs have about tables in technical specifications and their explicit ability to interpret them. TeleTables is built through a novel multi-stage data generation pipeline that extracts tables from 3GPP standards and uses multimodal and reasoning-oriented LLMs to generate and validate questions. The resulting dataset, which is publicly available, comprises 500 human-verified question-answer pairs, each associated with the corresponding table in multiple formats. Our evaluation shows that, smaller models (under 10B parameters) struggle both to recall 3GPP knowledge and to interpret tables, indicating the limited exposure to telecom standards in their pretraining and the insufficient inductive biases for navigating complex technical material. Larger models, on the other hand, show stronger reasoning on table interpretation. Overall, TeleTables highlights the need for domain-specialized fine-tuning to reliably interpret and reason over telecom standards.

</details>


### [7] [FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback](https://arxiv.org/abs/2601.04203)
*Xueqing Wu,Zihan Xue,Da Yin,Shuyan Zhou,Kai-Wei Chang,Nanyun Peng,Yeming Wen*

Main category: cs.CL

> FronTalk是一个前端代码生成基准测试，研究多模态反馈的对话式代码生成。它包含100个从真实网站收集的多回合对话，提出新的评估框架，并引入AceCoder方法解决了模型的记忆问题。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在研究多模态反馈下的前端代码生成的独特互动机制，特别是视觉反馈（如草图、模型、注释截图）在多回合代码生成中的作用，填补现有文献的相关空白。

**Method:** FronTalk采用多回合对话收集来自不同领域的100个真实网站的前端开发任务，并提出一个基于代理的评估框架，通过网络代理模拟用户行为，以衡量功能正确性和用户体验。此外，引入AceCoder方法以解决模型的记忆问题。

**Result:** 研究表明，模型面临着显著的记忆问题（忘记先前实现的功能）和解释视觉反馈的持续挑战，特别是在开源视觉-语言模型中。实验评估的20个模型中，AceCoder方法显著地解决了记忆问题，将任务失败率降至零，并提升了9.3%的性能。

**Conclusion:** FronTalk为未来研究前端开发任务提供了坚实的基础，特别是对于多回合多模态代码生成交互动态的研究。

**Abstract:** We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes](https://arxiv.org/abs/2601.04300)
*Chenye Meng,Zejian Li,Zhongni Liu,Yize Li,Changle Xie,Kaixin Jia,Ling Yang,Huanghuang Deng,Shiying Ding,Shengyuan Zhang,Jiayi Li,Lingyun Sun*

Main category: cs.CV

> 提出CPO方法优化了扩散模型与复杂人类专业知识的对齐，尤其是在绘画生成领域实现了显著的效果提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有的扩散模型对齐方法依赖于简化的信号，这限制了其与复杂的人类专业知识的对齐。

**Method:** 构建了一个分层的、精细的评价标准，并提出了一个两阶段对齐框架。首先，通过监督微调将领域知识注入辅助扩散模型。其次，提出了复杂偏好优化（CPO）方法，它扩展了DPO来对齐具有非二元、分层标准的目标扩散模型。

**Result:** 实验表明，CPO显著提高了生成质量和与专业知识的对齐。

**Conclusion:** 该方法开启了针对精细标准对齐的新方向。

**Abstract:** Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment.

</details>


### [9] [Embedding Textual Information in Images Using Quinary Pixel Combinations](https://arxiv.org/abs/2601.04302)
*A V Uday Kiran Kandala*

Main category: cs.CV

> 本文提出了一种利用RGB空间中的五进制像素强度组合将文本数据嵌入图像的新技术，该方法减少了图像失真并提高了嵌入效率。

<details>
  <summary>Details</summary>

**Motivation:** 现有的信息隐藏方法通常依赖于像素强度翻转或多步骤过程，这会导致图像噪音或较高的计算成本。本研究旨在提供一种计算效率更高且图像失真更小的方法。

**Method:** 利用RGB空间中的五进制像素强度组合，这些组合映射到文本符号，以代表大小写字母、数字、空白及常用特殊字符。

**Result:** 对比实验显示，该方法嵌入文本后图像几乎没有失真，且在计算效率上优于其他方法。

**Conclusion:** 该研究证实了利用五进制替代二进制（如LSB和MSB方法）进行文本嵌入的有效性，显示了更高的嵌入效率和更低的计算成本。

**Abstract:** This paper presents a novel technique for embedding textual data into images using quinary combinations of pixel intensities in RGB space. Existing methods predominantly rely on least and most significant bit (LSB & MSB) manipulation, Pixel Value Differencing (PVD), spatial perturbations in RGB channels, transform domain based methods, Quantization methods, Edge and Region based methods and more recently through deep learning methods and generative AI techniques for hiding textual information in spatial domain of images. Most of them are dependent on pixel intensity flipping over multiple pixels, such as LSB and combination of LSB based methodologies, and on transform coefficients, often resulting in the form of noise. Encoding and Decoding are deterministic in most of the existing approaches and are computationally heavy in case of higher models such as deep learning and gen AI approaches. The proposed method works on quinary pixel intensity combinations in RGB space, where five controlled different pixel intensity variations in each of the R, G, and B channels formulate up to one hundred and twenty five distinct pixel intensity combinations. These combinations are mapped to textual symbols, enabling the representation of uppercase and lowercase alphabetic characters, numeric digits, whitespace, and commonly used special characters. Different metrics such as MSE, MAE, SNR, PSNR, SSIM, Histogram Comparison and Heatmap analysis, were evaluated for both original and encoded images resulting in no significant distortion in the images. Furthermore, the method achieves improved embedding efficiency by encoding a complete textual symbol within a single RGB pixel, in contrast to LSB and MSB based approaches that typically require multiple pixels or multi-step processes, as well as transform and learning based methods that incur higher computational overhead.

</details>


### [10] [Unified Text-Image Generation with Weakness-Targeted Post-Training](https://arxiv.org/abs/2601.04339)
*Jiahui Chen,Philippe Hansen-Estruch,Xiaochuang Han,Yushi Hu,Emily Dinan,Amita Kamath,Michal Drozdzal,Reyhane Askari-Hemmat,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CV

> 研究通过后训练方法实现了完全统一的文本和图像生成，使用奖励加权和策略性设计的训练数据，在四个多模态图像生成基准上显示了改进效果。

<details>
  <summary>Details</summary>

**Motivation:** 许多现有的系统依赖于明确的模态切换，限制了跨模态耦合，阻碍了自动多模态生成。本研究旨在解决这一问题，改进文本到图像生成的表现。

**Method:** 本研究采用后训练方法，以实现完全统一的文本-图像生成，使模型能够在单一推理过程中自主地从文本推理转换到视觉合成。

**Result:** 使用离线奖励加权后训练以及完全自动生成的合成数据，本研究的方法提升了四个不同的文本到图像生成基准测试中的多模态图像生成性能。

**Conclusion:** 本研究证明了奖励加权对于两种模态的重要性以及策略性设计后训练数据可以提升多模态图像生成的效果。

**Abstract:** Unified multimodal generation architectures that jointly produce text and images have recently emerged as a promising direction for text-to-image (T2I) synthesis. However, many existing systems rely on explicit modality switching, generating reasoning text before switching manually to image generation. This separate, sequential inference process limits cross-modal coupling and prohibits automatic multimodal generation. This work explores post-training to achieve fully unified text-image generation, where models autonomously transition from textual reasoning to visual synthesis within a single inference process. We examine the impact of joint text-image generation on T2I performance and the relative importance of each modality during post-training. We additionally explore different post-training data strategies, showing that a targeted dataset addressing specific limitations achieves superior results compared to broad image-caption corpora or benchmark-aligned data. Using offline, reward-weighted post-training with fully self-generated synthetic data, our approach enables improvements in multimodal image generation across four diverse T2I benchmarks, demonstrating the effectiveness of reward-weighting both modalities and strategically designed post-training data.

</details>
