{"id": "2512.04175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04175", "abs": "https://arxiv.org/abs/2512.04175", "authors": ["Alejandro Cobo", "Roberto Valle", "José Miguel Buenaposada", "Luis Baumela"], "title": "Beyond Flicker: Detecting Kinematic Inconsistencies for Generalizable Deepfake Video Detection", "comment": null, "summary": "Generalizing deepfake detection to unseen manipulations remains a key challenge. A recent approach to tackle this issue is to train a network with pristine face images that have been manipulated with hand-crafted artifacts to extract more generalizable clues. While effective for static images, extending this to the video domain is an open issue. Existing methods model temporal artifacts as frame-to-frame instabilities, overlooking a key vulnerability: the violation of natural motion dependencies between different facial regions. In this paper, we propose a synthetic video generation method that creates training data with subtle kinematic inconsistencies. We train an autoencoder to decompose facial landmark configurations into motion bases. By manipulating these bases, we selectively break the natural correlations in facial movements and introduce these artifacts into pristine videos via face morphing. A network trained on our data learns to spot these sophisticated biomechanical flaws, achieving state-of-the-art generalization results on several popular benchmarks.", "AI": {"tldr": "提出一种通过合成视频生成具有微小运动不一致性的训练数据的方法，以训练网络识别深度伪造视频中的高级生物机械缺陷，从而提升对未见过的篡改手段的泛化能力。", "motivation": "目前深度伪造检测技术在处理静态图像时较为有效，但在视频领域存在挑战。现有方法通常关注帧间不稳定性的处理，但忽略了面部不同区域自然运动依赖性的破坏。", "method": "通过训练自动编码器将面部关键点配置分解为运动基础，并通过操控这些基础来打破面部运动的自然相关性，进而通过面部变形将这些瑕疵植入原始视频中。", "result": "该方法生成的数据训练的网络能够识别复杂的生物机械缺陷，相比现有方法在几个流行的基准数据集上呈现出更优的泛化性能。", "conclusion": "研究提出的方法通过引入自然运动依赖性破坏的特征库来提升网络对深度伪造视频中高级生物机械缺陷的识别能力，从而改进了泛化效果。"}}
{"id": "2512.04187", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04187", "abs": "https://arxiv.org/abs/2512.04187", "authors": ["Jinzhen Hu", "Kevin Faust", "Parsa Babaei Zadeh", "Adrienn Bourkas", "Shane Eaton", "Andrew Young", "Anzar Alvi", "Dimitrios George Oreopoulos", "Ameesha Paliwal", "Assem Saleh Alrumeh", "Evelyn Rose Kamski-Hennekam", "Phedias Diamandis"], "title": "OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology", "comment": null, "summary": "The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.", "AI": {"tldr": "本研究开发了OnSight Pathology软件，使用连续的自定义屏幕截图提供实时AI推断，实现在不同平台上低成本、本地化的部署，并展示了其在组织病理学任务上的高效性和多样化的应用潜力。", "motivation": "目前，组织的显微镜检查仍然是疾病分类的重要手段，但它依赖主观解释和专业知识的可获取性，这可能影响准确性及临床护理。尽管人工智能(AI)的突破为自动化组织学分析带来了希望，但不断增长的专有数字病理学解决方案导致实际部署的障碍。本研究旨在通过引入平台无关的计算机视觉软件OnSight Pathology来解决这些挑战。", "method": "OnSight Pathology是一个平台无关的计算机视觉软件，通过连续的自定义屏幕截图为用户提供实时AI推断。该软件可以作为一个单一的、独立的可执行文件运行，无需复杂的软件集成，可以在消费级个人电脑上本地操作，从而实现研究和临床工作流程中的成本效益和安全部署。", "result": "利用超过2,500张公开可用的全幻灯片图像，展示了OnSight Pathology在不同幻灯片查看器以及实际临床数字病理工作中的实用性。显示了该软件在常见脑肿瘤类型分类、有丝分裂检测和免疫组织化学染色量化的常规组织病理任务上的稳健性。软件具有一种内置的多模式聊天助理，可以提供图像的可验证描述，以便于质量控制。此外，还展示了其与活显微镜摄像头的兼容性，包括个人智能手机，这为潜在的模拟部署、外科手术中以及远程病理学应用提供了可能性。", "conclusion": "OnSight Pathology能够提供实时AI推断，覆盖广泛的病理管线，去除了AI工具在组织学中采纳的关键障碍。"}}
{"id": "2512.04213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04213", "abs": "https://arxiv.org/abs/2512.04213", "authors": ["Bishoy Galoaa", "Xiangyu Bai", "Shayda Moezzi", "Utsav Nandi", "Sai Siddhartha Vivek Dhir Rangoju", "Somaieh Amraee", "Sarah Ostadabbas"], "title": "Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers", "comment": null, "summary": "This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-", "AI": {"tldr": "LAPA是一种用于多相机点跟踪的基于Transformer的端到端架构，它解决了传统方法中的误差传播和时间不一致性问题，在复杂运动和遮挡场景中表现出色。", "motivation": "现有的多相机点跟踪方法通常将检测、关联和跟踪分离开来，这种方式在复杂场景中容易造成误差传播和时间不一致性。LAPA旨在处理这些场景下的不足，通过一个统一的架构来改善性能。", "method": "LAPA采用了一种全新的基于Transformer的端到端架构来实现多相机点追踪，与传统的分离检测、关联合并跟踪的方法不同，LAPA利用注意力机制在同一时间跨视角进行推理，从而实现通过带有几何先验的跨视角注意力机制来建立软对应关系。而且，LAPA通过注意力加权聚合构建3D点的表示，而不是依赖于传统的三角测量，这可以自然地处理不确定性和部分观测。时间一致性通过建模长程依赖关系的Transformer解码器保持，在长时间被遮挡的情况下也能保持身份的连续性。", "result": "实验结果表明，LAPA的方法在具有复杂运动和遮挡场景的数据集上表现出色，在TAPVid-3D-MC上取得了37.5%的APD，在PointOdyssey-MC上取得了90.3%的APD。", "conclusion": "LAPA作为一个统一且创新的点跟踪框架，展示了其在处理复杂场景时的显著优越性，它通过整合外观匹配和几何约束超越了当前的方法。"}}
{"id": "2512.04219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04219", "abs": "https://arxiv.org/abs/2512.04219", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur\\\\"], "title": "Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning", "comment": "16 pages, 7 figures, 3 tables. Under Review", "summary": "Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.", "AI": {"tldr": "PARSE是一个统一的框架，可以从未经标记的视频流中学习多尺度事件结构，并以类似人类的方式对视频进行层次化的时间分割。", "motivation": "该研究动机在于模仿人类如何自然地感知时间嵌套事件，并开发可以实时、预测性地将视频分割为分层事件的计算机视觉模型。", "method": "PARSE框架通过层次化的递归预测器组织感知，每个预测器以自己的时间粒度运行，低层处理短期动态，高层通过基于注意力的反馈整合长期上下文。事件边界作为预测误差的瞬时峰值自然出现。", "result": "在三个基准测试（Breakfast Actions，50 Salads和Assembly 101）中，PARSE展示了实时方法的最佳表现，并在时间同步和结构一致性上达到了离线方法的水平。", "conclusion": "研究结果表明，在不确定性环境下进行预测学习提供了一条可扩展的路径，通向更具人类特征的时间抽象和可组合事件的理解。"}}
{"id": "2512.04220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04220", "abs": "https://arxiv.org/abs/2512.04220", "authors": ["Wenlong Deng", "Yushu Li", "Boying Gong", "Yi Ren", "Christos Thrampoulidis", "Xiaoxiao Li"], "title": "On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral", "comment": null, "summary": "Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.", "AI": {"tldr": ".LEADING; 本文研究了Tool-integrated (TI) Reinforcement Learning（工具集成强化学习）中的问题，发现了一种称为Lazy Likelihood Displacement (LLD) 的机制，会导致训练崩溃，并提出了LLDS正则化方法来解决这个问题。", "motivation": ".LEADING; 为了应对Group Relative Policy Optimization (GRPO) 在Tool-integrated Reinforcement Learning (TIRL) 中容易出现的训练崩溃问题，进而推动TIRL的稳定性和可扩展性。", "method": ".LEADING; 提出了名为LLDS的轻量级似然性保留正则化方法，仅在轨迹似然性下降时激活，并且仅对负责的标记进行正则化。这能够在不显著干扰优化过程的情况下缓解LLD问题。", "result": ".LEADING; 在七个开放式和多跳QA基准测试中，该方法稳定了训练过程，防止了梯度爆炸，并带来了显著的性能提升，例如Qwen2.5-3B模型提升了+37.8%，Qwen2.5-7B模型提升了+32.0%。", "conclusion": ".LEADING; 本文识别并确认了LLD是GRPO方法在TIRL中遇到的一个核心问题，并通过LLDS正则化方法为解决此问题并实现工具集成的大语言模型的稳定和可扩展训练提供了实际路径。"}}
{"id": "2512.04221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04221", "abs": "https://arxiv.org/abs/2512.04221", "authors": ["Xiangyu Bai", "He Liang", "Bishoy Galoaa", "Utsav Nandi", "Shayda Moezzi", "Yuhang He", "Sarah Ostadabbas"], "title": "MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis", "comment": null, "summary": "While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.", "AI": {"tldr": "本文介绍了一种称为MoReGen的框架，用于生成物理上精确且运动连贯的视频，解诀了文本到视频生成中物理一致性的核心挑战。", "motivation": "本文研究了受牛顿运动控制的文本到视频生成和评估，强调了物理精度和运动连贯性，旨在解决生成视频的物理一致性问题。", "method": "本文介绍了一种名为MoReGen的框架，该框架结合了多代理大型语言模型、物理仿真器和渲染器，根据文本提示在代码域生成可重复的物理准确视频。", "result": "实验结果表明，现有最先进的文本到视频模型在保持物理有效性方面表现不佳，而MoReGen框架为视频合成提供了物理一致性的方向。", "conclusion": "MoReGen框架通过结合多个组件生成了物理上精确的视频，提出了物体轨迹对应作为直接评价物理有效性的指标，实验验证了现有模型的不足并展示了其潜在优势。"}}
{"id": "2512.04257", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04257", "abs": "https://arxiv.org/abs/2512.04257", "authors": ["Mansour Essgaer", "Khamis Massud", "Rabia Al Mamlook", "Najah Ghmaid"], "title": "Computational Linguistics Meets Libyan Dialect: A Study on Dialect Identification", "comment": "13 pages, 8 figures", "summary": "This study investigates logistic regression, linear support vector machine, multinomial Naive Bayes, and Bernoulli Naive Bayes for classifying Libyan dialect utterances gathered from Twitter. The dataset used is the QADI corpus, which consists of 540,000 sentences across 18 Arabic dialects. Preprocessing challenges include handling inconsistent orthographic variations and non-standard spellings typical of the Libyan dialect. The chi-square analysis revealed that certain features, such as email mentions and emotion indicators, were not significantly associated with dialect classification and were thus excluded from further analysis. Two main experiments were conducted: (1) evaluating the significance of meta-features extracted from the corpus using the chi-square test and (2) assessing classifier performance using different word and character n-gram representations. The classification experiments showed that Multinomial Naive Bayes (MNB) achieved the highest accuracy of 85.89% and an F1-score of 0.85741 when using a (1,2) word n-gram and (1,5) character n-gram representation. In contrast, Logistic Regression and Linear SVM exhibited slightly lower performance, with maximum accuracies of 84.41% and 84.73%, respectively. Additional evaluation metrics, including log loss, Cohen kappa, and Matthew correlation coefficient, further supported the effectiveness of MNB in this task. The results indicate that carefully selected n-gram representations and classification models play a crucial role in improving the accuracy of Libyan dialect identification. This study provides empirical benchmarks and insights for future research in Arabic dialect NLP applications.", "AI": {"tldr": "研究使用了逻辑回归、线性支持向量机、多项式朴素贝叶斯和伯努利朴素贝叶斯对来自Twitter的利比亚方言进行分类，使用QADI语料库进行实验。多项式朴素贝叶斯在使用特定n-gram表示时表现最优，准确率为85.89%，F1得分为0.85741，优于逻辑回归和线性SVM。", "motivation": "研究激励在于通过评估不同的分类模型和n-gram表示来提高利比亚方言识别的准确性。", "method": "使用QADI语料库，进行了两个主要实验：(1) 使用chi-square测试评估元特征的重要性；(2) 使用不同的词和字符n-gram表示评估分类器的表现。", "result": "多项式朴素贝叶斯分类器在使用(1,2)词n-gram和(1,5)字符n-gram表示时效果最佳，达到了85.89%的准确率和0.85741的F1得分。与其他分类模型如逻辑回归和线性SVM相比，效果略好。", "conclusion": "结果表明，精心选择的n-gram表示和分类模型能够显著提高利比亚方言的识别准确性。该研究为未来阿拉伯方言NLP应用提供了经验基准和见解。"}}
{"id": "2512.04222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04222", "abs": "https://arxiv.org/abs/2512.04222", "authors": ["Alara Dirik", "Tuanfeng Wang", "Duygu Ceylan", "Stefanos Zafeiriou", "Anna Frühstück"], "title": "ReasonX: MLLM-Guided Intrinsic Image Decomposition", "comment": null, "summary": "Intrinsic image decomposition aims to separate images into physical components such as albedo, depth, normals, and illumination. While recent diffusion- and transformer-based models benefit from paired supervision from synthetic datasets, their generalization to diverse, real-world scenarios remains challenging. We propose ReasonX, a novel framework that leverages a multimodal large language model (MLLM) as a perceptual judge providing relative intrinsic comparisons, and uses these comparisons as GRPO rewards for fine-tuning intrinsic decomposition models on unlabeled, in-the-wild images. Unlike RL methods for generative models, our framework aligns conditional intrinsic predictors by rewarding agreement between the judge's relational assessments and analytically derived relations from the model's outputs. ReasonX is model-agnostic and can be applied to different intrinsic predictors. Across multiple base architectures and modalities, ReasonX yields significant improvements, including 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D, highlighting the promise of MLLM-guided comparative supervision to bridge low- and high-level vision reasoning.", "AI": {"tldr": "ReasonX框架利用多模态大语言模型对未标记的真实图像进行内在分解模型的微调，显著提高了模型在阿尔贝多和深度估计上的性能。", "motivation": "虽然最近基于扩散和变压器的模型从合成数据集的配对监督中受益，但它们在泛化到多样化的现实世界场景方面的挑战仍然存在。", "method": "ReasonX框架利用多模态大语言模型（MLLM）作为感知裁判，提供相对的内在比较，并将这些比较作为GRPO奖励，用于微调未标记的真实场景图像的内在分解模型。该框架通过奖励模型输出与裁判关系评估的一致性来对条件内在预测器进行对齐。", "result": "与多种基础架构和模式相比，ReasonX显著改进了性能，IWI阿尔贝多的WHDR减少了9-25%，在ETH3D上深度精度提高了46%。", "conclusion": "研究展示了MLLM指导下的比较监督方法，对于连接低级和高级视觉推理具有巨大潜力。"}}
{"id": "2512.04292", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04292", "abs": "https://arxiv.org/abs/2512.04292", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "title": "SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats", "comment": "Accepted in The IEEE International Workshop on Large Language Models in Finance, Dec 8-11, Macau, China, 2025, Preprint Copy", "summary": "Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lacking consistent schemas. We present SQuARE, a hybrid retrieval framework with sheet-level, complexity-aware routing. It computes a continuous score based on header depth and merge density, then routes queries either through structure-preserving chunk retrieval or SQL over an automatically constructed relational representation. A lightweight agent supervises retrieval, refinement, or combination of results across both paths when confidence is low. This design maintains header hierarchies, time labels, and units, ensuring that returned values are faithful to the original cells and straightforward to verify. Evaluated on multi-header corporate balance sheets, a heavily merged World Bank workbook, and diverse public datasets, SQuARE consistently surpasses single-strategy baselines and ChatGPT-4o on both retrieval precision and end-to-end answer accuracy while keeping latency predictable. By decoupling retrieval from model choice, the system is compatible with emerging tabular foundation models and offers a practical bridge toward a more robust table understanding.", "AI": {"tldr": "介绍了一种新的混合型表格检索框架SQuARE，其能够在复杂表格结构中进行有效查询和答案检索，评估结果显示其在多个评价指标上优于现有方法。", "motivation": "现有的方法在处理包含多行表头、合并单元格和单位注释的真实电子表格上的问题回答不够准确，而刚性的SQL视图在不一致的表格结构上也表现不佳。", "method": "该论文提出了SQuARE，一个基于表格复杂性感知路由的混合检索框架。它通过计算表头深度和合并密度的连续得分来决定将查询通过结构保持的片段检索或通过SQL进行自动构建的关联表示进行路由。当信心低时，一个轻量级的代理会监督检索、细化或结合两个路径的结果。", "result": "SQuARE在多表头的公司资产负债表、高度合并的世界银行工作表和多种公共数据集上进行了评估，结果表明它在检索精度和最终答案准确性上均优于单一策略基线和ChatGPT-4o，并且保持了可预测的延迟。", "conclusion": "通过分离检索与模型选择，该系统与新兴的表格基础模型兼容，并为实现更加鲁棒的表格理解提供了一个实用的桥梁。"}}
{"id": "2512.04238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04238", "abs": "https://arxiv.org/abs/2512.04238", "authors": ["Leon Mayer", "Piotr Kalinowski", "Caroline Ebersbach", "Marcel Knopp", "Tim Rädsch", "Evangelia Christodoulou", "Annika Reinke", "Fiona R. Kolbinger", "Lena Maier-Hein"], "title": "6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models", "comment": null, "summary": "Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about \"typical\" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.", "AI": {"tldr": "该研究通过创建AdversarialAnatomyBench基准测试工具，揭示了现有的视觉-语言模型在面对罕见人体解剖变异时表现显著下降的问题。这一发现指出了当前模型在处理罕见临床情况时的局限性。", "motivation": "现有的基准测试主要关注于常见解剖情况的表现，无法捕捉到罕见变异所带来的挑战。为了填补这一空白，作者提出了一个新的基准测试。", "method": "该研究提出了AdversarialAnatomyBench基准，用于评估机器视觉-语言模型在处理罕见解剖变异时的表现。这项基准测试包括不同成像方式和解剖区域中自然出现的罕见解剖变异。", "result": "在测试了22个最先进的视觉-语言模型后，该研究发现模型平均准确率从常见解剖情况的74%下降到了罕见解剖情况的29%。即使表现最好的模型GPT-5、Gemini 2.5 Pro 和 Llama 4 Maverick，其表现下降了41%-51%。", "conclusion": "该研究揭示了一个关键性的、此前未被量化的问题，即现有视觉-语言模型面对罕见解剖情况的泛化能力较差。AdversarialAnatomyBench为系统性评估和减轻多模态医疗AI系统的解剖偏差提供了基础。"}}
{"id": "2512.04324", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04324", "abs": "https://arxiv.org/abs/2512.04324", "authors": ["Fangyu Lei", "Jinxiang Meng", "Yiming Huang", "Junjie Zhao", "Yitong Zhang", "Jianwen Luo", "Xin Zou", "Ruiyi Yang", "Wenbo Shi", "Yan Gao", "Shizhu He", "Zuo Wang", "Qian Liu", "Yang Wang", "Ke Wang", "Jun Zhao", "Kang Liu"], "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "comment": null, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "AI": {"tldr": "提出DAComp基准测试，该测试旨在评估代理在处理数据工程和开放性数据分析方面的能力。发现即使最先进的系统在执行这些任务时也有较大的局限性。", "motivation": "鉴于现有的工具在处理企业和数据分析任务方面存在不足，因此开发了一个新的基准测试DAComp以驱动更好地理解和改进自主数据代理的能力。", "method": "通过引入DAComp基准测试，该测试包括210个任务，涵盖了企业数据智能流程中的数据工程和数据分析。数据工程任务包括对工业模式进行多阶段SQL管道的设计和构建，而数据分析任务则包括解决开放式商业问题，需要战略规划、迭代式的探索分析和结果解释。", "result": "实验结果显示，即使是最先进的代理在DAComp上也表现不佳：在数据工程任务上的成功率低于20%，在数据分析任务上的平均得分也低于40%，这表明在开放性推理和综合能力上存在着重大缺陷。", "conclusion": "DAComp为开发更强大的自主数据代理提供了严格的测试环境，这有助于解决现有技术在复杂企业数据智能工作流中的瓶颈问题。"}}
{"id": "2512.04248", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04248", "abs": "https://arxiv.org/abs/2512.04248", "authors": ["Shaoheng Fang", "Chaohui Yu", "Fan Wang", "Qixing Huang"], "title": "MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models", "comment": null, "summary": "We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.", "AI": {"tldr": "MVRoom是一个高质量的可控3D室内场景多视角生成系统，通过使用两阶段设计和迭代框架，实现了高保真和可控制的生成结果，优于现有方法。", "motivation": "为了实现高质量的3D室内场景的合成，并在场景生成过程中确保多视角的一致性，从而提升总体的场景生成质量。", "method": "MVRoom是一个用于3D室内场景的可控新型视图合成（NVS）管道，利用多视角扩散并基于粗略3D布局。它采用两阶段设计，利用3D布局在整个过程中来保证多视角一致性。第一阶段采用新颖的表示方法，有效地链接了3D布局与一致的基于图像的条件信号，用于多视角生成。第二阶段执行基于图像条件的多视角生成，并引入了布局感知的极线注意力机制，以增强扩散过程中的多视角一致性。此外，还引入了一个迭代框架，通过递归执行多视角生成，支持不同对象数量和场景复杂度的3D场景生成，进而支持文本到场景的生成。", "result": "实验证明，该方法在NVS中实现了高保真和可控制的3D场景生成，并在定量和定性上均优于最先进基线方法。消融研究表明生成管道中的关键组件的有效性。", "conclusion": "研究表明MVRoom能够在多视角合成过程中有效保证3D场景的高保真度和可控性，并在不同场景复杂度和多视角条件下表现出色。"}}
{"id": "2512.04350", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04350", "abs": "https://arxiv.org/abs/2512.04350", "authors": ["Yiming Xu", "Yuan Yuan", "Vijay Viswanathan", "Graham Neubig"], "title": "ClusterFusion: Hybrid Clustering with Embedding Guidance and LLM Adaptation", "comment": null, "summary": "Text clustering is a fundamental task in natural language processing, yet traditional clustering algorithms with pre-trained embeddings often struggle in domain-specific contexts without costly fine-tuning. Large language models (LLMs) provide strong contextual reasoning, yet prior work mainly uses them as auxiliary modules to refine embeddings or adjust cluster boundaries. We propose ClusterFusion, a hybrid framework that instead treats the LLM as the clustering core, guided by lightweight embedding methods. The framework proceeds in three stages: embedding-guided subset partition, LLM-driven topic summarization, and LLM-based topic assignment. This design enables direct incorporation of domain knowledge and user preferences, fully leveraging the contextual adaptability of LLMs. Experiments on three public benchmarks and two new domain-specific datasets demonstrate that ClusterFusion not only achieves state-of-the-art performance on standard tasks but also delivers substantial gains in specialized domains. To support future work, we release our newly constructed dataset and results on all benchmarks.", "AI": {"tldr": "本文提出ClusterFusion，一种混合框架，该框架采用大型语言模型（LLMs）作为聚类的核心，结合轻量级嵌入方法进行领域特定的文本聚类，取得了优秀的性能。", "motivation": "传统的聚类算法在领域特定任务中表现不佳，而大型语言模型主要用作嵌入或调整聚类边界的辅助模块。", "method": "ClusterFusion框架通过三个阶段进行文本聚类：基于嵌入的子集分割、基于LLM的主题总结以及基于LLM的主题分配。", "result": "实验表明，ClusterFusion在三个公开基准和两个新的领域特定数据集上达到了最先进的性能。", "conclusion": "ClusterFusion不仅能有效处理标准任务，还在专门领域中表现出显著优势。该文章发布了一个新构建的数据集和所有基准上的结果以支持未来研究。"}}
{"id": "2512.04267", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04267", "abs": "https://arxiv.org/abs/2512.04267", "authors": ["Zitian Zhang", "Iliyan Georgiev", "Michael Fischer", "Yannick Hold-Geoffroy", "Jean-François Lalonde", "Valentin Deschaintre"], "title": "UniLight: A Unified Representation for Lighting", "comment": "Project page: https://lvsn.github.io/UniLight", "summary": "Lighting has a strong influence on visual appearance, yet understanding and representing lighting in images remains notoriously difficult. Various lighting representations exist, such as environment maps, irradiance, spherical harmonics, or text, but they are incompatible, which limits cross-modal transfer. We thus propose UniLight, a joint latent space as lighting representation, that unifies multiple modalities within a shared embedding. Modality-specific encoders for text, images, irradiance, and environment maps are trained contrastively to align their representations, with an auxiliary spherical-harmonics prediction task reinforcing directional understanding. Our multi-modal data pipeline enables large-scale training and evaluation across three tasks: lighting-based retrieval, environment-map generation, and lighting control in diffusion-based image synthesis. Experiments show that our representation captures consistent and transferable lighting features, enabling flexible manipulation across modalities.", "AI": {"tldr": "本文提出了一个名为UniLight的联合潜在空间，用于统一不同模式的光照表示，支持大规模训练和不同任务中的灵活光照控制。", "motivation": "尽管照明对视觉外观有很强的影响，但理解和表示图像中的照明仍然非常困难。现有的不同光照表示方法彼此不兼容，限制了跨模式转移的可能性。", "method": "提出了UniLight，这是一种联合潜在空间作为光照表示，统一了文本、图像、辐照度和环境图等多种模式的共享嵌入。通过对比训练模态特定的编码器来对齐它们的表示，并通过辅助的球谐预测任务来强化方向理解。", "result": "该方法允许跨三个任务进行大规模训练和评估：基于照明的检索、环境图生成和基于扩散的图像合成的光照控制。", "conclusion": "实验结果显示，这种表示方法捕捉了一致且可转移的光照特征，实现了跨模式的灵活操纵。"}}
{"id": "2512.04374", "categories": ["cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2512.04374", "abs": "https://arxiv.org/abs/2512.04374", "authors": ["Muyu Pan", "Matthew Walter", "Dheeraj Kodakandla", "Mahfuza Farooque"], "title": "LangSAT: A Novel Framework Combining NLP and Reinforcement Learning for SAT Solving", "comment": null, "summary": "Our work presents a novel reinforcement learning (RL) based framework to optimize heuristic selection within the conflict-driven clause learning (CDCL) process, improving the efficiency of Boolean satisfiability (SAT) solving. The proposed system, LangSAT, bridges the gap between natural language inputs and propositional logic by converting English descriptions into Conjunctive Normal Form (CNF) expressions and solving them using an RL-enhanced CDCL SAT solver. Unlike existing SAT-solving platforms that require CNF as input, LangSAT enables users to input standard English descriptions, making SAT-solving more accessible. The framework comprises two key components: Lang2Logic, which translates English sentences into CNF expressions, and SmartSAT, an RL-based SAT solver. SmartSAT encodes clause-variable relationships as structured graph representations and extracts global features specific to the SAT problem. This implementation provides the RL agent with deeper contextual information, enabling SAT problems to be solved more efficiently. Lang2Logic was evaluated on diverse natural language inputs, processing descriptions up to 450 words. The generated CNFs were solved by SmartSAT, which demonstrated comparable performance to traditional CDCL heuristics with respect to solving time. The combined LangSAT framework offers a more accessible and scalable solution for SAT-solving tasks across reasoning, formal verification, and debugging.", "AI": {"tldr": "本工作提出了一种基于强化学习的框架LangSAT，用于优化SAT求解过程，它自动将自然语言描述转化为CNF表达式，并使用强化学习增强的CDCL SAT求解器来解决问题，从而使得SAT求解更易于被各类用户使用。", "motivation": "现有的SAT求解平台要求将问题表示为CNF形式作为输入，这使得许多人难以理解和使用。于是，研究者旨在开发一种更易用和可扩展的SAT求解方法。", "method": "提出了一个基于强化学习（RL）的框架来优化冲突驱动子句学习（CDCL）过程中启发式的选取，从而提高布尔可满足性（SAT）求解的效率。该系统名为LangSAT，它通过将英语描述转换为合取范式（CNF）表达式，并使用强化学习增强的CDCL SAT求解器解决这些问题，从而弥合了自然语言输入与命题逻辑之间的差距。框架包括两个主要组件：一个是Lang2Logic，用于将英语句子转换为CNF表达式；另一个是SmartSAT，一个基于强化学习的SAT求解器。SmartSAT将子句-变量关系编码为结构化图表示，并提取特定于SAT问题的全局特征，这为RL代理提供了更深的上下文信息，使得SAT问题可以更有效地解决。", "result": "Lang2Logic在多样化的自然语言输入上进行了评估，处理了多达450个词的描述，生成的CNF由SmartSAT解决，与传统CDCL启发式算法相比，在求解时间上表现出了类似的性能。", "conclusion": "LangSAT框架为跨推理、形式验证和调试任务提供了更易用和可扩展的SAT求解解决方案。"}}
{"id": "2512.04282", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04282", "abs": "https://arxiv.org/abs/2512.04282", "authors": ["Tasmiah Haque", "Srinjoy Das"], "title": "Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer", "comment": null, "summary": "Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.", "AI": {"tldr": "The paper presents GRU-SNF, an inference-time refinement technique for video motion transfer applications that combines GRU-NF with stochastic sampling methods, leading to more diverse and accurate future predictions.", "motivation": "To enhance the diversity and realism of future predictions in real-time video motion transfer applications, which is crucial for immersive gaming and vision-based anomaly detection.", "method": "GRU-SNF, which combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with Markov Chain Monte Carlo (MCMC) steps for stochastic sampling to improve the diversity and accuracy of future predictions in real-time video motion transfer applications.", "result": "The proposed GRU-SNF outperforms the GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons, as demonstrated in a keypoint-based video motion transfer pipeline.", "conclusion": "Integrating stochastic dynamics with flow-based sequence models for generative time series forecasting can improve the diversity and accuracy of predictions, showcasing its potential for applications that require realistic synthesis and robust decision-making."}}
{"id": "2512.04386", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04386", "abs": "https://arxiv.org/abs/2512.04386", "authors": ["Zhou Yang", "Shunyan Luo", "Jiazhen Zhu", "Fang Jin"], "title": "MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation", "comment": null, "summary": "Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes. Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP. Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework. MASE offers local explanations for text-based predictive models without necessitating in-depth knowledge of a model's internal architecture. By leveraging Normalized Linear Gaussian Perturbations (NLGP) on the embedding layer instead of raw word inputs, MASE efficiently estimates input saliency. Our results indicate MASE's superiority over other model-agnostic interpretation methods, especially in terms of Delta Accuracy, positioning it as a promising tool for elucidating the operations of text-based models in NLP.", "AI": {"tldr": "本文提出MASE框架解决NLP中DNN模型的解释性问题，通过NLGP方法提高解释效率和准确性。", "motivation": "传统方法如重要性图或特征可视化可能不适用于NLP中的离散词数据。MASE框架旨在解决深度神经网络模型解释性的问题，特别是其复杂的决策过程。", "method": "MASE框架通过在嵌入层使用NLGP（正规化线性高斯扰动）而不是直接在原始词输入上操作，提供文本预测模型的本地解释。", "result": "实验结果显示，MASE框架在Delta Accuracy方面优于其他模型不可知解释方法，表现出色。", "conclusion": "MASE作为一种模型不可知的解释框架，在NLP中揭示基于文本模型的操作方面显示出巨大潜力。"}}
{"id": "2512.04283", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04283", "abs": "https://arxiv.org/abs/2512.04283", "authors": ["Fan Jia", "Yuhao Huang", "Shih-Hsin Wang", "Cristina Garcia-Cardona", "Andrea L. Bertozzi", "Bao Wang"], "title": "Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint", "comment": null, "summary": "Flow matching-based generative models have been integrated into the plug-and-play image restoration framework, and the resulting plug-and-play flow matching (PnP-Flow) model has achieved some remarkable empirical success for image restoration. However, the theoretical understanding of PnP-Flow lags its empirical success. In this paper, we derive a continuous limit for PnP-Flow, resulting in a stochastic differential equation (SDE) surrogate model of PnP-Flow. The SDE model provides two particular insights to improve PnP-Flow for image restoration: (1) It enables us to quantify the error for image restoration, informing us to improve step scheduling and regularize the Lipschitz constant of the neural network-parameterized vector field for error reduction. (2) It informs us to accelerate off-the-shelf PnP-Flow models via extrapolation, resulting in a rescaled version of the proposed SDE model. We validate the efficacy of the SDE-informed improved PnP-Flow using several benchmark tasks, including image denoising, deblurring, super-resolution, and inpainting. Numerical results show that our method significantly outperforms the baseline PnP-Flow and other state-of-the-art approaches, achieving superior performance across evaluation metrics.", "AI": {"tldr": "本文提出了PnP-Flow的随机微分方程(SDE)模型，通过该模型能够更好地理解和改进PnP-Flow在图像修复中的性能，并通过实验验证了该方法的有效性。", "motivation": "尽管基于流匹配的生成模型已经在图像修复框架中的取得了实证上的成功，但是其理论理解仍然不足，本文旨在通过数学建模来填补这一理论空白。", "method": "作者提出了一个PnP-Flow的连续极限，即一个SDE模型，该模型不仅提供了一个新的理论视角，也为提升PnP-Flow性能提供了方法。", "result": "实验结果显示，这样的改进方法显著优于基于PnP-Flow模型的基线方法和其他最新的方法，其性能在评估指标上均领先。", "conclusion": "本文提出的SDE模型及其改进方法在图像去噪、去模糊、超分辨率及中间修复等多个任务中均显示出了显著的效果，证明了SDE理论在PnP-Flow中的应用价值。"}}
{"id": "2512.04396", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04396", "abs": "https://arxiv.org/abs/2512.04396", "authors": ["Subrata Karmaker"], "title": "Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering", "comment": "11 pages, 2 figures, includes full Python code. Classical machine learning baseline for sarcasm detection on the SARC 2.0 dataset", "summary": "Sarcasm is common in online discussions, yet difficult for machines to identify because the intended meaning often contradicts the literal wording. In this work, I study sarcasm detection using only classical machine learning methods and explicit feature engineering, without relying on neural networks or context from parent comments. Using a 100,000-comment subsample of the Self-Annotated Reddit Corpus (SARC 2.0), I combine word-level and character-level TF-IDF features with simple stylistic indicators. Four models are evaluated: logistic regression, a linear SVM, multinomial Naive Bayes, and a random forest. Naive Bayes and logistic regression perform the strongest, achieving F1-scores around 0.57 for sarcastic comments. Although the lack of conversational context limits performance, the results offer a clear and reproducible baseline for sarcasm detection using lightweight and interpretable methods.", "AI": {"tldr": "本文研究了仅使用经典机器学习方法和显式特征工程识别网络讨论中的讽刺言论，未使用神经网络或上下文信息，达到了约0.57的F1分数。", "motivation": "讽刺在网络交流中很常见，但难以通过机器识别，因为其实际含义常常与字面意义相反。作者研究仅使用经典机器学习方法和显式特征工程的方式，以克服这种困难。", "method": "从Self-Annotated Reddit Corpus (SARC 2.0)中选取了10万条评论子集，结合使用基于单词和字符的TF-IDF特征与简单的风格指标，评估了逻辑回归、线性SVM、多项式朴素贝叶斯和随机森林四种模型。", "result": "朴素贝叶斯和逻辑回归的表现最好，达到了约0.57的讽刺评论F1分数。", "conclusion": "尽管缺乏对话上下文限制了性能，但本研究提供了一个清晰且可复制的轻量级、可解释方法的讽刺检测基准。"}}
{"id": "2512.04284", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04284", "abs": "https://arxiv.org/abs/2512.04284", "authors": ["Sruthi Srinivasan", "Elham Shakibapour", "Rajy Rawther", "Mehdi Saeedi"], "title": "Learning Single-Image Super-Resolution in the JPEG Compressed Domain", "comment": "7 pages, 4 figures, 2 tables, SEEDS Workshop, ICIP 2025", "summary": "Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.", "AI": {"tldr": "提出直接在JPEG特征上训练模型的方法以提高数据加载效率，并应用于单图像超分辨率任务，显著加速数据加载和训练速度同时保持视觉质量。", "motivation": "深层学习模型日趋复杂，输入数据规模也随之增加，尽管在深度学习硬件方面取得了显著进展，数据加载仍然是一个主要瓶颈，限制了训练和推理速度。", "method": "训练模型直接使用JPEG编码的特征，从而减少完全解码JPEG带来的计算开销，加速数据加载效率。研究针对单图像超分辨率(SISR)任务，提出了一个轻量级的超分辨率管道，该管道在JPEG离散余弦变换(DCT)系数的频率域操作。", "result": "该方法实现了数据加载2.6倍的加速，训练速度提高了2.5倍，并且保持了与标准SISR方法相当的视觉质量。", "conclusion": "在单图像超分辨率中直接应用JPEG编码特征的有效性已经得到了验证，该方法提高了数据加载效率和整体训练速度。"}}
{"id": "2512.04457", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04457", "abs": "https://arxiv.org/abs/2512.04457", "authors": ["Guoshenghui Zhao", "Huawei Lin", "Weijie Zhao"], "title": "RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language Model Unlearning", "comment": "Code available at: https://github.com/eyerf/RapidUn", "summary": "Removing specific data influence from large language models (LLMs) remains challenging, as retraining is costly and existing approximate unlearning methods are often unstable. The challenge is exacerbated when the forget set is small or imbalanced. We introduce RapidUn, an influence-driven and parameter-efficient unlearning framework. It first estimates per-sample influence through a fast estimation module, then maps these scores into adaptive update weights that guide selective parameter updates -- forgetting harmful behavior while retaining general knowledge. On Mistral-7B and Llama-3-8B across Dolly-15k and Alpaca-57k, RapidUn achieves up to 100 times higher efficiency than full retraining and consistently outperforms Fisher, GA, and LoReUn on both in-distribution and out-of-distribution forgetting. These results establish influence-guided parameter reweighting as a scalable and interpretable paradigm for LLM unlearning.", "AI": {"tldr": "本文提出RapidUn框架，通过影响估算和适应性更新权重来有效地去除大规模语言模型中的特定数据影响，提高效率。", "motivation": "本文旨在解决从大规模语言模型中去除特定数据影响的难题，尤其在数据集较小时不稳定的问题。", "method": "本文提出了一种名为RapidUn的高效无学习框架，它首先通过快速估计模块估算每个样本的影响，然后将这些分数映射到适应性更新权重中，指导参数的选择性更新，从而实现对有害行为的遗忘同时保留通用知识。", "result": "实验结果显示，RapidUn在Mistral-7B和Llama-3-8B模型上的Dolly-15k和Alpaca-57k数据集上，达到全重新训练效率的100倍，并在多种遗忘指标上优于其他方法。", "conclusion": "结果证明了基于影响的参数再加权方法作为一种可扩展且可解释的LLM无学习范式具有潜力。"}}
{"id": "2512.04303", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04303", "abs": "https://arxiv.org/abs/2512.04303", "authors": ["Gasser Elazab", "Maximilian Jansen", "Michael Unterreiner", "Olaf Hellwich"], "title": "Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications", "comment": "Accepted in 3DV 2026", "summary": "Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.", "AI": {"tldr": "Gamma-from-Mono (GfM) is a lightweight, self-supervised method for monocular geometry estimation that provides superior accuracy for fine-scale road geometry details, achieving state-of-the-art near-field performance on benchmark datasets.", "motivation": "The motivation is to improve the accuracy of monocular depth estimation for fine-scale road geometry details, which are often oversmoothed and lost by conventional methods, critical for vehicle control and safety.", "method": "The paper presents Gamma-from-Mono (GfM), a method for monocular geometry estimation that decouples global and local structure to enhance the perception of road surface details. It predicts a dominant road surface plane and local deviations by a dimensionless gamma measure, using only the camera’s height above ground to recover metric depth.", "result": "GfM achieves state-of-the-art near-field accuracy in depth and gamma estimation while maintaining competitive performance for global depth on the KITTI and Road Surface Reconstruction Dataset (RSRD).", "conclusion": "GfM is a lightweight, self-supervised method that effectively enhances the perception of road surface details for monocular geometry estimation, demonstrated through superior near-field accuracy with minimal parameters and robust cross-setup performance."}}
{"id": "2512.04492", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04492", "abs": "https://arxiv.org/abs/2512.04492", "authors": ["Yuanshuo Zhang", "Aohua Li", "Bo Chen", "Jingbo Sun", "Xiaobing Zhao"], "title": "MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection", "comment": null, "summary": "LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve compound entities or events that must be explicitly linked to stance labels, and rhetorical devices such as irony often obscure the author's actual intent. To address these challenges, we propose MSME, a Multi-Stage, Multi-Expert framework for zero-shot stance detection. MSME consists of three stages: (1) Knowledge Preparation, where relevant background knowledge is retrieved and stance labels are clarified; (2) Expert Reasoning, involving three specialized modules-Knowledge Expert distills salient facts and reasons from a knowledge perspective, Label Expert refines stance labels and reasons accordingly, and Pragmatic Expert detects rhetorical cues such as irony to infer intent from a pragmatic angle; (3) Decision Aggregation, where a Meta-Judge integrates all expert analyses to produce the final stance prediction. Experiments on three public datasets show that MSME achieves state-of-the-art performance across the board.", "AI": {"tldr": "提出MSME框架，提升零样本立场检测在复杂场景下的性能。", "motivation": "大型语言模型在零样本立场检测方面取得了显著成果，但在复杂现实场景中仍面临挑战，需要动态背景知识、明确目标定义及识别修辞手法如反讽。", "method": "MSME，一个用于零样本立场检测的多阶段、多专家框架，包括知识准备、专家推理和决策聚合三个阶段。", "result": "在三个公共数据集上的实验表明，MSME在各个方面均达到了最先进的性能。", "conclusion": "MSME通过多阶段、多专家设计，解决了零样本立场检测在复杂场景下的问题，展现了优越的性能。"}}
{"id": "2512.04305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04305", "abs": "https://arxiv.org/abs/2512.04305", "authors": ["Mainak Singha", "Masih Aminbeidokhti", "Paolo Casari", "Elisa Ricci", "Subhankar Roy"], "title": "How (Mis)calibrated is Your Federated CLIP and What To Do About It?", "comment": null, "summary": "While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\\text{FL}^2\\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\\text{FL}^2\\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.", "AI": {"tldr": "本文研究了联邦学习环境中CLIP微调的校准效果，并提出了$\\text{FL}^2\\text{oRA}$方法，改善了模型在该场景中的校准效果。", "motivation": "尽管一些先前的工作研究了CLIP在离线环境中的校准，但在联邦学习环境下微调CLIP的校准效果尚待研究。", "method": "我们研究了在联邦学习设置中对CLIP进行微调如何影响其校准，并评估了现有的在训练过程中校准技术，但发现这些技术改进有限。我们提出了一个基于LoRA的方法（$\\text{FL}^2\\text{oRA}$），它在联邦学习中自然改善了校准。", "result": "实验结果表明，$\\text{FL}^2\\text{oRA}$ 方法在多个基准测试中能产生校准良好的模型，减少了对显式校准程序的需求。", "conclusion": "校准的关键挑战不仅仅在于我们如何聚合或校准，还在于选择微调的哪个组件。提出的方法（$\\text{FL}^2\\text{oRA}$）能在联邦学习场景中改善模型的校准。"}}
{"id": "2512.04518", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04518", "abs": "https://arxiv.org/abs/2512.04518", "authors": ["Tianmai M. Zhang", "Zhaoyi Sun", "Sihang Zeng", "Chenxi Li", "Neil F. Abernethy", "Barbara D. Lam", "Fei Xia", "Meliha Yetisgen"], "title": "UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction", "comment": "To be published in Proceedings of the 7th Clinical Natural Language Processing Workshop", "summary": "The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.", "AI": {"tldr": "本论文针对癌症患者电子健康记录中化疗时间线的构建问题，评估了多种策略，结果显示微调的Qwen3-14B模型取得了非常高的评分，这些发现有望为相关任务提供有益的参考。", "motivation": "该论文的动机在于改进从电子健康记录中自动构建患者化疗时间线的准确性和效率，以辅助临床决策和支持癌症治疗的个性化管理。", "method": "本论文主要探讨了从癌症患者的电子健康记录中构建化疗时间线的方法。具体来说，讨论了次任务2：从原始临床笔记中生成患者化疗时间线的方法。评估了涉及链式思考、监督微调、直接偏好优化和基于词典查找的策略来改进时间线提取。所有方法都遵循两步工作流程：先使用LLM从单个临床笔记中提取化疗事件，再用算法对事件进行规范化和聚合，生成患者级别的时间线。各个具体方法在如何使用和训练相关LLM上有所不同。", "result": "多种方法在测试集排行榜上的表现都非常有竞争力，其中微调后的Qwen3-14B取得了最佳官方分数0.678。", "conclusion": "我们的研究成果和分析可以为未来在这一任务上的尝试以及设计类似任务提供有用的见解。"}}
{"id": "2512.04309", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04309", "abs": "https://arxiv.org/abs/2512.04309", "authors": ["Rui Fonseca", "Bruno Martins", "Gil Rocha"], "title": "Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction", "comment": "Submitted to CVPR 2026", "summary": "Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.", "AI": {"tldr": "本文提出了TOMCap，一种改进的纯文本训练方法，能够在无需对齐的图像-字幕对情况下进行图像字幕生成，方法主要通过减少模态差距和结合检索的字幕示例及潜在向量表示来指导生成过程。", "motivation": "减少对人工标注图像-文本配对数据的依赖，探索无需训练数据标注的图像字幕生成。", "method": "TOMCap方法通过对CLIP表示进行处理以减少模态差距，并结合检索到的字幕示例和潜在向量表示来引导生成过程，从而实现在没有对齐的图像-字幕对的情况下进行字幕生成。", "result": "通过广泛的实验，TOMCap方法在无训练和其他纯文本方法中表现最佳。", "conclusion": "TOMCap在无训练和纯文本方法中表现最佳，并分析了不同配置的检索增强和模态差距减少组件的影响。"}}
{"id": "2512.04545", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04545", "abs": "https://arxiv.org/abs/2512.04545", "authors": ["Pengfei Cao", "Zeao Ji", "Daojian Zeng", "Jun Zhao", "Kang Liu"], "title": "EvoEdit: Lifelong Free-Text Knowledge Editing through Latent Perturbation Augmentation and Knowledge-driven Parameter Fusion", "comment": null, "summary": "Adjusting the outdated knowledge of large language models (LLMs) after deployment remains a major challenge. This difficulty has spurred the development of knowledge editing, which seeks to accurately and efficiently modify a model's internal (parametric) knowledge without retraining it from scratch. However, existing methods suffer from two limitations. First, they depend on structured triplets that are misaligned with the free-text nature of LLM pretraining and fail to capture the nuanced relationships among facts. Second, they typically support one-time knowledge updates, with relatively limited research on the problem of sequential or lifelong editing. To address these gaps, we propose a new task, Lifelong Free-text Knowledge Editing (LF-Edit), which enables models to incorporate updates expressed in natural language and supports continual editing over time. Despite its promise, LF-Edit faces the dual challenge of integrating new knowledge while mitigating the forgetting of prior information. To foster research on this new task, we construct a large-scale benchmark, Multi-Rank Lifelong Free-text Editing Benchmark (MRLF-Bench), containing 16,835 free-text edit requests. We further design a cognitively inspired multi-rank evaluation framework encompassing four levels: memorization, understanding, constrained comprehension, and reasoning. To tackle the challenges inherent in LF-Edit, we introduce a novel approach named EvoEdit that enhances knowledge injection through Latent Perturbation Augmentation and preserves prior information via Knowledge-driven Parameter Fusion. Experimental results demonstrate that EvoEdit substantially outperforms existing knowledge editing methods on the proposed LF-Edit task.", "AI": {"tldr": "提出了LF-Edit任务和解决方案EvoEdit，解决了现有知识编辑方法无法有效更新语言模型内部知识的问题。", "motivation": "解决大型语言模型在部署后校正过时知识的难题，尤其是应对现有知识编辑方法依赖结构化三元组且只能支持一次性知识点更新的问题。", "method": "为了解决现有知识编辑方法的不足，提出了一种新的任务——终身自由文本知识编辑（LF-Edit），该任务能够使模型采纳自然语言表达的更新，并支持持续编辑。为了应对LF-Edit面临的挑战，即整合新知识同时降低之前信息的遗忘，提出了EvoEdit方法，通过潜在扰动增强知识注入，并采用知识驱动参数融合来保存先前信息。", "result": "实验结果表明，EvoEdit在所提出的LF-Edit任务上显著优于现有知识编辑方法。", "conclusion": "EvoEdit方法能够在不遗忘已有知识的情况下有效地注入新知识，为大型语言模型的终身编辑提供了有效方案。"}}
{"id": "2512.04311", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.04311", "abs": "https://arxiv.org/abs/2512.04311", "authors": ["Juan Manuel Cantarero Angulo", "Matthew Smith"], "title": "Real-time Cricket Sorting By Sex", "comment": "13 pages, 14 figures", "summary": "The global demand for sustainable protein sources is driving increasing interest in edible insects, with Acheta domesticus (house cricket) identified as one of the most suitable species for industrial production. Current farming practices typically rear crickets in mixed-sex populations without automated sex sorting, despite potential benefits such as selective breeding, optimized reproduction ratios, and nutritional differentiation. This work presents a low-cost, real-time system for automated sex-based sorting of Acheta domesticus, combining computer vision and physical actuation. The device integrates a Raspberry Pi 5 with the official Raspberry AI Camera and a custom YOLOv8 nano object detection model, together with a servo-actuated sorting arm. The model reached a mean Average Precision at IoU 0.5 (mAP@0.5) of 0.977 during testing, and real-world experiments with groups of crickets achieved an overall sorting accuracy of 86.8%. These results demonstrate the feasibility of deploying lightweight deep learning models on resource-constrained devices for insect farming applications, offering a practical solution to improve efficiency and sustainability in cricket production.", "AI": {"tldr": "一项研究开发了一种低成本系统，基于计算机视觉和物理驱动技术，能够实时自动分类家蟋蟀的性别，实现效率与可持续性养殖。", "motivation": "随着全球对于可持续蛋白质来源的需求增加，食用昆虫，特别是适用于工业化生产的家蟋蟀，引起了广泛关注。然而，现有的养殖方式并没有实现自动化性别分类，这限制了选择性育种及优化繁殖比例等潜在效益。因此，开发一种低成本的自动性别分类系统成为必要。", "method": "研究采用了Raspberry Pi 5硬件和官方的人工智能Raspberry相机，结合了一个自定义的YOLOv8纳米对象检测模型和一个伺服驱动的分类臂来实现家蟋蟀的性别分类。该系统能够实现实时性别识别，并对家蟋蟀群体实现了86.8%的性别分类准确度。", "result": "研究展示了一种低成本、实时的系统，用于自动区分并分类家蟋蟀的性别。该系统结合了计算机视觉和物理驱动的技术，使用了Raspberry Pi 5硬件和YOLOv8 nano目标识别模型。实验结果表明，该系统在性别分类上的准确率达到86.8%，展示了在昆虫养殖中运用轻量级深度学习模型的可行性。这为提高蟋蟀养殖的效率和可持续性提供了一种实用的方法。", "conclusion": "该研究证明，在资源受限的设备上部署轻量级深度学习模型的可行性，并为提高家蟋蟀养殖效率和可持续性提供了一种实用的解决方案。"}}
{"id": "2512.04550", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04550", "abs": "https://arxiv.org/abs/2512.04550", "authors": ["Yangning Li", "Shaoshen Chen", "Yinghui Li", "Yankai Chen", "Hai-Tao Zheng", "Hui Wang", "Wenhao Jiang", "Philip S. Yu"], "title": "AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees", "comment": "NeurIPS 2025", "summary": "The quadratic complexity of self-attention constrains Large Language Models (LLMs) in processing long contexts, a capability essential for many advanced applications. Context compression aims to alleviate this computational bottleneck while retaining critical semantic information. However, existing approaches often fall short: explicit methods may compromise local detail, whereas implicit methods can suffer from positional biases, information degradation, or an inability to capture long-range semantic dependencies. We propose AdmTree, a novel framework for adaptive, hierarchical context compression with a central focus on preserving high semantic fidelity while maintaining efficiency. AdmTree dynamically segments input based on information density, utilizing gist tokens to summarize variable-length segments as the leaves of a semantic binary tree. This structure, together with a lightweight aggregation mechanism and a frozen backbone LLM (thereby minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By preserving fine-grained details alongside global semantic coherence, mitigating positional bias, and dynamically adapting to content, AdmTree robustly retains the semantic information of long contexts.", "AI": {"tldr": "研究提出AdmTree來解決自注意力機制在處理長上下文時存在的計算瓶頸問題，該解決辦法通過分割和摘要的方式保留語義信息的同時也確保了效率。", "motivation": "自注意力的二次複雜度限制了大語言模型(LLM)處理長上下文的能力，而該能力對於許多高端應用而言至关重要。現有的上下文壓縮方法往往有不足之處：顯式方法可能損壞局部細節，而隱式方法可能遭受位置偏見、信息退化或無法捕捉長期語義依賴。", "method": "AdmTree是一種新的適應性分層上下文壓縮框架，旨在保留高語義保真度並保持效率。它基於信息密度動態分割輸入，利用摘要標記來總結變長的段落，作為語義二叉樹的葉子節點。這種結構，加上輕量級聚合機制和凍結的骨干LLM（從而將新的可訓練參數最小化），使能高效地階層抽象化上下文。", "result": "AdmTree通過保存細粒度的細節和全局語義一致性、減輕位置偏見、並動態適應內容，穩固地保持了長時間上下文的語義信息。", "conclusion": "AdmTree框架能夠有 hiệu率地對上下文進行階層抽象，同時保持語義信息的完整性和一致性，解決了長上下文處理中的計算效率問題。"}}
{"id": "2512.04313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04313", "abs": "https://arxiv.org/abs/2512.04313", "authors": ["Haolin Xiong", "Tianwen Fu", "Pratusha Bhuvana Prasad", "Yunxuan Cai", "Haiwei Chen", "Wenbin Teng", "Hanyuan Xiao", "Yajie Zhao"], "title": "Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding", "comment": "16 pages, 11 figures", "summary": "Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.", "AI": {"tldr": "Mind-to-Face是一个将EEG信号直接转化为高保真面部表情的框架，能够捕捉微妙的情感动态，并通过修改的3D高斯插图管道进行逼真的渲染，实现了神经驱动的化身体的经验。", "motivation": "现有的表情化身系统主要依赖视觉线索，当脸部被遮挡或情感保持内部时，其效果会受到影响。我们提出了Mind-to-Face，这是第一个将非侵入性的脑电图（EEG）信号直接解码成高保真面部表情的框架。", "method": "我们构建了一个双模态记录设置，该设置在情感刺激过程中同步获取EEG和多视角面部视频，以实现神经到视觉学习的精确监督。我们的模型使用CNN-Transformer编码器将EEG信号映射成密集的3D位置图，能够采样65000多个顶点，捕捉细粒度的几何形状和微妙的情感动态，并通过修改的3D高斯插图管道进行渲染，以获得逼真、视角一致的结果。", "result": "通过广泛评估，我们展示了仅使用EEG信号就可以可靠地预测动态的、特定于个人的面部表情，包括微妙的情感反应，表明神经信号包含的情感和几何信息比之前所认为的更加丰富。", "conclusion": "Mind-to-Face确立了神经驱动化身的新范式，可以实现个性化、情感感知的远程存在和沉浸式环境中的认知互动。"}}
{"id": "2512.04555", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04555", "abs": "https://arxiv.org/abs/2512.04555", "authors": ["Pritam Kadasi", "Abhishek Upperwal", "Mayank SIngh"], "title": "ADAPT: Learning Task Mixtures for Budget-Constrained Instruction Tuning", "comment": "Under Review", "summary": "We propose ADAPT, a meta-learning algorithm that \\emph{learns} task sampling proportions under an explicit token budget for multi-task instruction tuning. Instead of fixing task weights by hand, \\adapt{} maintains a continuous distribution over tasks and updates it via meta-gradients of a smooth worst-case validation objective, inducing an adaptive curriculum that allocates more tokens to useful tasks while avoiding collapse. We instantiate ADAPT on three $\\sim$1B-parameter open-weight LLMs (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B), training on 20 Natural Instructions task types under budgets of $1\\%$, $5\\%$, and $10\\%$ of the available supervised tokens, and compare against strong supervised fine-tuning baselines with uniform and size-proportional mixing. We conduct evaluations on 11 out-of-domain benchmarks spanning reasoning, reading comprehension, code generation, and instruction following, we find that ADAPT matches or slightly improves average downstream performance relative to the best static mixture, while using fewer effective training tokens and reallocating budget toward harder, benchmark-aligned tasks.", "AI": {"tldr": "ADAPT算法通过元学习优化多任务下的task采样比例，展示了在资源受限下提升模型性能的能力。", "motivation": "在多任务学习中，手动选择任务权重可能会导致性能不佳和资源分配不当的问题。通过自动学习任务权重，我们旨在提高每个任务的性能效率。", "method": "我们提出了ADAPT，这是一种元学习算法，它在多任务指令调优下显式地学习任务采样比例。与手动固定任务权重不同，ADAPT保持了任务的连续分布并通过平滑最坏情况验证目标的元梯度更新这个分布，从而自适应地调整课程分配，将更多的token分配给有用的tasks以避免崩溃。", "result": "我们在三个大约1B参数的开放权重LLMs（Gemma-3-1B，LLaMA-3.2-1B，Qwen-0.6B）上实现了ADAPT，并在占可用监督token的1%，5%和10%的预算下，用20种自然指令任务类型进行了训练。在监督微调基线中，我们比较了均匀混合和基于大小的比例混合方法。我们在11个跨推理、阅读理解、代码生成、指令跟随的离域评估基准上进行测试，发现ADAPT在平均下游性能上与最好的静态混合方法匹配，甚至稍有提升，同时使用更少的有效训练token，并重新分配预算向更难的、与基准对齐的任务。", "conclusion": "ADAPT算法能够有效地优化跨多种不同类型任务的模型性能，通过自适应调整训练中的任务权重来提升模型性能并减少训练资源的使用。"}}
{"id": "2512.04314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04314", "abs": "https://arxiv.org/abs/2512.04314", "authors": ["Jiashu Liao", "Pietro Liò", "Marc de Kamps", "Duygu Sarikaya"], "title": "DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision", "comment": null, "summary": "Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.", "AI": {"tldr": "提出了一种名为DisentangleFormer的新架构，通过空间-通道解耦进行特征学习，有效提升了在高光谱图像上的性能，同时保持较低的计算成本。", "motivation": "为了解决标准自注意力机制无法独立建模结构和语义依赖的问题，特别是在高光谱成像领域。这种问题在卫星高光谱遥感和红外病理成像中尤为明显，因为通道捕捉了不同的生物物理或生物化学线索。", "method": "提出了DisentangleFormer架构，通过空间-通道分离实现鲁棒的多通道视觉表示。该架构主要有三个核心组件：(1) 并行解缠：独立处理空间标记和通道标记流，实现空间和光谱维度的解相关特征学习，(2) 压缩标记增强器：一个自适应校准模块，动态融合空间和通道流，(3) 多尺度前馈网络：补充全局注意力，捕捉细粒度的结构和语义依赖。", "result": "在高光谱基准测试中，DisentangleFormer达到了最先进的性能，优于印度松树、帕维亚大学、休斯顿以及大规模的BigEarthNet遥感数据集和红外病理数据集上的现有模型。此外，它在ImageNet上保持了具有竞争力的精度，同时减少了17.8%的FLOPs计算成本。", "conclusion": "DisentangleFormer通过对空间和通道维度的独立建模，减少了两者之间的冗余，达到了解耦的特征学习。这使得模型在高光谱图像处理和标准图像分类任务中性能卓越，并且计算成本更低。"}}
{"id": "2512.04578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04578", "abs": "https://arxiv.org/abs/2512.04578", "authors": ["Wenjin Liu", "Haoran Luo", "Xin Feng", "Xiang Ji", "Lijuan Zhou", "Rui Mao", "Jiapu Wang", "Shirui Pan", "Erik Cambria"], "title": "LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence", "comment": null, "summary": "Legal general intelligence (GI) refers to artificial intelligence (AI) that encompasses legal understanding, reasoning, and decision-making, simulating the expertise of legal experts across domains. However, existing benchmarks are result-oriented and fail to systematically evaluate the legal intelligence of large language models (LLMs), hindering the development of legal GI. To address this, we propose LexGenius, an expert-level Chinese legal benchmark for evaluating legal GI in LLMs. It follows a Dimension-Task-Ability framework, covering seven dimensions, eleven tasks, and twenty abilities. We use the recent legal cases and exam questions to create multiple-choice questions with a combination of manual and LLM reviews to reduce data leakage risks, ensuring accuracy and reliability through multiple rounds of checks. We evaluate 12 state-of-the-art LLMs using LexGenius and conduct an in-depth analysis. We find significant disparities across legal intelligence abilities for LLMs, with even the best LLMs lagging behind human legal professionals. We believe LexGenius can assess the legal intelligence abilities of LLMs and enhance legal GI development. Our project is available at https://github.com/QwenQKing/LexGenius.", "AI": {"tldr": "研究提出LexGenius，一个专家级别的中文法律基准，用于评估大型语言模型的法律通用智能，并通过多轮审查确保数据质量，评估了12个最先进的语言模型，发现这些模型在法律智能方面比人类法律专业人士落后。", "motivation": "现有的基准往往是结果导向的，无法系统地评估大型语言模型的法律智能，从而阻碍了法律通用智能的发展。为了应对这一挑战，我们提出了LexGenius，旨在评估和提高LLMs的法律智能。", "method": "提出LexGenius，这是一个专家级别的中文法律基准，用于评估大型语言模型的法律通用智能。它遵循维度-任务-能力框架，涵盖七个维度，十一项任务和二十种能力。使用最近的法律案例和考试问题创建多项选择题，并结合手动和大型语言模型审查以减少数据泄露风险，通过多轮审查确保准确性和可靠性。", "result": "评估了12个最先进的大型语言模型，并进行了深入分析，发现LLMs在法律智能能力上存在显著差异，即使是表现最佳的LLMs也落后于人类法律专业人士。", "conclusion": "LexGenius可以评估大型语言模型的法律智能能力，并将促进法律通用智能的发展。项目在https://github.com/QwenQKing/LexGenius上是公开的。"}}
{"id": "2512.04315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04315", "abs": "https://arxiv.org/abs/2512.04315", "authors": ["Yonghan Lee", "Tsung-Wei Huang", "Shiv Gehlot", "Jaehoon Choi", "Guan-Ming Su", "Dinesh Manocha"], "title": "SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting", "comment": null, "summary": "Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.", "AI": {"tldr": "提出了SyncTrack4D方法，通过4D高斯投射技术处理多视角不同步视频，实现了时间同步和高精度三维重建。", "motivation": "解决动态三维场景建模挑战，尤其是处理非同步视频集合中的三维几何和运动重建问题。", "method": "使用融合Gromov-Wasserstein最优传输方法计算4D特征轨迹对应，全局时间对齐及子帧同步技术进行建模。", "result": "在Panoptic Studio和SyncNeRF Blender上评估，展示了子帧时间同步精度，平均时间误差小于0.26帧，4D重建达到26.3 PSNR得分。", "conclusion": "该工作首次提出了一种无需预先设定场景对象或模型的非同步视频集合通用4D高斯投射方法。"}}
{"id": "2512.04683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04683", "abs": "https://arxiv.org/abs/2512.04683", "authors": ["Carolin Mueller-Spitzer", "Samira Ochs", "Jan Oliver Ruediger", "Sascha Wolfer"], "title": "Geschlechtsübergreifende Maskulina im Sprachgebrauch Eine korpusbasierte Untersuchung zu lexemspezifischen Unterschieden", "comment": "32 pages, 8 figures", "summary": "This study examines the distribution and linguistic characteristics of generic masculines (GM) in contemporary German press texts. The use of masculine personal nouns to refer to mixed-gender groups or unspecified individuals has been widely debated in academia and the public, with con-flicting perspectives on its gender-neutrality. While psycholinguistic studies suggest that GM is more readily associated with male referents, corpus-based analyses of its actual use remain scarce. We investigate GM in a large corpus of press texts, focusing on lexeme-specific differences across dif-ferent types of personal nouns. We conducted manual annotations of the whole inflectional para-digm of 21 personal nouns, resulting in 6,195 annotated tokens. Our findings reveal considerable differences between lexical items, especially between passive role nouns and prestige-related per-sonal nouns. On a grammatical level, we find that GM occurs predominantly in the plural and in indefinite noun phrases. Furthermore, our data shows that GM is not primarily used to denote entire classes of people, as has been previously claimed. By providing an empirical insight into the use of GM in authentic written language, we contribute to a more nuanced understanding of its forms and manifestations. These findings provide a solid basis for aligning linguistic stimuli in psy-cholinguistic studies more closely with real-world language use.", "AI": {"tldr": "本研究通过分析当代德语新闻文本，探讨了通用阳性名词（GM）的分布和语言特性，揭示了GM在不同词汇和语法层面的使用情况，为GM的性别中立性提供了实证支持。", "motivation": "尽管在学术界和公共领域广泛讨论了使用阳性个人名词（GM）来指代混合性别群体或未指明个体的性别中立性，但其实际使用的语料库分析仍然稀缺。该研究旨在填补这一空白，并提供对GM形式和表现更细致的理解。", "method": "通过对当代德语新闻文本的大语料库进行研究，重点分析不同类型的个人名词在语汇上的差异。对21个个人名词的所有屈折变化形式进行了人工标注，共标注了6,195个词条。", "result": "研究发现，通用阳性名词（GM）在数量上存在词汇之间的显著差异，尤其是在被动角色名词和尊严相关的个人名词之间。从语法层面来看，GM主要出现在复数形式和不定名词短语中。此外，数据表明GM并不是主要用来指代整个阶级的人，这打破了之前的观点。", "conclusion": "该研究通过对真实书面语言使用GM的实证洞察，为心理学语言学研究中的语言刺激与现实世界语言使用的一致性提供了牢固的基础。"}}
{"id": "2512.04323", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04323", "abs": "https://arxiv.org/abs/2512.04323", "authors": ["Biao Chen", "Zhenhua Lei", "Yahui Zhang", "Tongzhi Niu"], "title": "Bayes-DIC Net: Estimating Digital Image Correlation Uncertainty with Bayesian Neural Networks", "comment": "17 pages, 8 figures", "summary": "This paper introduces a novel method for generating high-quality Digital Image Correlation (DIC) dataset based on non-uniform B-spline surfaces. By randomly generating control point coordinates, we construct displacement fields that encompass a variety of realistic displacement scenarios, which are subsequently used to generate speckle pattern datasets. This approach enables the generation of a large-scale dataset that capture real-world displacement field situations, thereby enhancing the training and generalization capabilities of deep learning-based DIC algorithms. Additionally, we propose a novel network architecture, termed Bayes-DIC Net, which extracts information at multiple levels during the down-sampling phase and facilitates the aggregation of information across various levels through a single skip connection during the up-sampling phase. Bayes-DIC Net incorporates a series of lightweight convolutional blocks designed to expand the receptive field and capture rich contextual information while minimizing computational costs. Furthermore, by integrating appropriate dropout modules into Bayes-DIC Net and activating them during the network inference stage, Bayes-DIC Net is transformed into a Bayesian neural network. This transformation allows the network to provide not only predictive results but also confidence levels in these predictions when processing real unlabeled datasets. This feature significantly enhances the practicality and reliability of our network in real-world displacement field prediction tasks. Through these innovations, this paper offers new perspectives and methods for dataset generation and algorithm performance enhancement in the field of DIC.", "AI": {"tldr": "本文提出了一种用于生成数字图像相关（DIC）数据集的新方法和Bayes-DIC Net，增强了DIC算法的训练、泛化以及预测能力，并能为预测结果提供置信度水平。", "motivation": "论文旨在通过生成大范围数据集，模拟现实世界的位移场情况，进而优化基于深度学习的DIC算法的训练和泛化能力。同时，提出Bayes-DIC Net通过增强网络的实用性和可靠性，为现实世界的位移场预测任务提供置信度评估的能力。", "method": "此论文提出了一种基于非均匀B样条表面生成高质量数字图像相关（DIC）数据集的新方法。通过随机生成控制点坐标来构建包含多种现实位移情况的位移场，并进一步用于生成斑点图案数据集。此外，该论文还提出了一种名为Bayes-DIC Net的新网络架构，该架构在下采样阶段抽取信息，并通过单一跳连接在上采样阶段整合不同层次的信息。该网络架构包括一系列设计来扩大感受野并捕捉丰富上下文信息的轻量级卷积块，同时减少计算成本。通过在网络推理阶段激活适当的dropout模块，Bayes-DIC Net成为一个Bayesian神经网络，能够提供预测结果及其在处理实际无标签数据集时的置信度水平。", "result": "通过新的数据生成方法及Bayes-DIC Net的设计改进，提高了DIC在新数据集上的预测能力，特别是在提供预测结果时还给出了置信度水平，增强了网络在现实世界应用中的可靠性和实用性。", "conclusion": "这项工作为DIC领域的数据集生成和算法性能改进提供了新的视角和方法，特别通过Bayes-DIC Net的创新构架，强化了预测结果的可靠性和实用价值。"}}
{"id": "2512.04738", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.04738", "abs": "https://arxiv.org/abs/2512.04738", "authors": ["Zhuoyue Wan", "Wentao Hu", "Chen Jason Zhang", "Yuanfeng Song", "Shuaimin Li", "Ruiqiang Xiao", "Xiao-Yong Wei", "Raymond Chi-Wing Wong"], "title": "OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models", "comment": "42nd IEEE International Conference on Data Engineering (ICDE)", "summary": "Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.", "AI": {"tldr": "开发了一种名为OsmT的开源模型，通过标签检索增强（TRA）机制提高OverpassQL生成的准确性和结构有效性，并通过定义逆任务提高用户可访问性。", "motivation": "解决将自然语言与结构化查询语言相连接这一领域长期存在的挑战，同时解决现有解决方案依赖于大规模闭源模型的问题，这些问题包括高推理成本、透明度低及缺乏轻量级部署的适应性。", "method": "介绍了一种名为OsmT的开源标签感知语言模型，该模型旨在将自然语言与Overpass查询语言（OverpassQL）进行连接。OverpassQL是一种用于访问大规模OpenStreetMap（OSM）数据的结构化查询语言。为提高生成查询的准确性和结构有效性，引入了标签检索增强（TRA）机制，能够在生成过程中将上下文相关的标签知识融入其中。此外，定义了一个逆任务，即将OverpassQL转换成自然语言解释，以支持查询解释和提高用户可访问性。", "result": "在公开基准测试中对OsmT进行评估，与强基线模型相比，在查询生成和解释方面均展现出持续的改进。", "conclusion": "尽管参数量大大减少，OsmT模型依然达到了与现有方法竞争的准确性，证明了开源预训练语言模型在连接自然语言和结构化查询语言方面的有效性和在富含模式的地理空间环境中的适用性。"}}
{"id": "2512.04329", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.04329", "abs": "https://arxiv.org/abs/2512.04329", "authors": ["Waleed Khalid", "Dmitry Ignatov", "Radu Timofte"], "title": "A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks", "comment": null, "summary": "Reusing existing neural-network components is central to research efficiency, yet discovering, extracting, and validating such modules across thousands of open-source repositories remains difficult. We introduce NN-RAG, a retrieval-augmented generation system that converts large, heterogeneous PyTorch codebases into a searchable and executable library of validated neural modules. Unlike conventional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, import-preserving reconstruction, and validator-gated promotion -- ensuring that every retrieved block is scope-closed, compilable, and runnable. Applied to 19 major repositories, the pipeline extracted 1,289 candidate blocks, validated 941 (73.0%), and demonstrated that over 80% are structurally unique. Through multi-level de-duplication (exact, lexical, structural), we find that NN-RAG contributes the overwhelming majority of unique architectures to the LEMUR dataset, supplying approximately 72% of all novel network structures. Beyond quantity, NN-RAG uniquely enables cross-repository migration of architectural patterns, automatically identifying reusable modules in one project and regenerating them, dependency-complete, in another context. To our knowledge, no other open-source system provides this capability at scale. The framework's neutral specifications further allow optional integration with language models for synthesis or dataset registration without redistributing third-party code. Overall, NN-RAG transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery, offering a first open-source solution that both quantifies and expands the diversity of executable neural architectures across repositories.", "AI": {"tldr": ">NN-RAG 是一种高度创新的分布式神经网络模块集成系统，通过其独特的方法显著增强了神经网络组件的发现、抽取和验证，是首个能够在多个开源仓库间实现可执行神经网络架构多样性的开源解决方案。", "motivation": ">NN-RAG 的设计动机在于解决在数千个开源仓库中发现、提取和验证现有神经网络组件的难度，从而提高研究效率。", "method": ">NN-RAG, 一个增强检索的生成系统，可将大型异构的PyTorch代码库转换为可搜索和可执行的验证神经模块库。不同于传统的代码搜索或克隆检测工具，NN-RAG执行作用域感知依赖解析、保留导入的重构和验证器门控提升，确保每个检索到的模块都是编译和可运行的。", "result": ">NN-RAG 从19个主要仓库中抽取了1,289个候选模块，验证了941个（73.0%），并展示了其中超过80%是结构独特的。NN-RAG 为 LEMUR 数据集供应了约72%的所有新网络结构。它还支持跨仓库迁移结构模式，自动识别一个项目中的可重用模块并在另一个上下文中重新生成，同时包含所有依赖项。", "conclusion": ">NN-RAG 将分裂的视觉代码转化为可重现、被追踪来源的创新算法发现的媒介，是首个能够在仓库之间扩展执行神经体系多样性并量化的开源解决方案。"}}
{"id": "2512.04746", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04746", "abs": "https://arxiv.org/abs/2512.04746", "authors": ["Wenhua Cheng", "Weiwei Zhang", "Heng Guo", "Haihao Shen"], "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs", "comment": null, "summary": "Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.", "AI": {"tldr": "介绍SignRoundV2框架，用以解决极端低比特量化时的性能下降问题，显著提高量化精度。", "motivation": "解决极端低比特量化（如2位和4位）部署大型语言模型时性能显著下降的问题。", "method": "SignRoundV2，一种后训练量化框架，包括快速敏感性度量和量化尺度的轻量级预调搜索。快速敏感性度量结合梯度信息和量化引起的偏差来指导逐层位分配。", "result": "实验表明，SignRoundV2方法能够保持大型语言模型的竞争力准确性，4-5位时与全精度模型相比仅有约1%的差异，在2位时也表现出强劲的性能。", "conclusion": "SignRoundV2成功地缩小了量化后模型与全精度模型之间的性能差距，使其能够在诸如大型语言模型等应用中实现生产级性能。"}}
{"id": "2512.04331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04331", "abs": "https://arxiv.org/abs/2512.04331", "authors": ["Zhongyi Cai", "Bryce Gernon", "Wentao Bao", "Yifan Li", "Matthew Wright", "Yu Kong"], "title": "Open Set Face Forgery Detection via Dual-Level Evidence Collection", "comment": null, "summary": "The proliferation of face forgeries has increasingly undermined confidence in the authenticity of online content. Given the rapid development of face forgery generation algorithms, new fake categories are likely to keep appearing, posing a major challenge to existing face forgery detection methods. Despite recent advances in face forgery detection, existing methods are typically limited to binary Real-vs-Fake classification or the identification of known fake categories, and are incapable of detecting the emergence of novel types of forgeries. In this work, we study the Open Set Face Forgery Detection (OSFFD) problem, which demands that the detection model recognize novel fake categories. We reformulate the OSFFD problem and address it through uncertainty estimation, enhancing its applicability to real-world scenarios. Specifically, we propose the Dual-Level Evidential face forgery Detection (DLED) approach, which collects and fuses category-specific evidence on the spatial and frequency levels to estimate prediction uncertainty. Extensive evaluations conducted across diverse experimental settings demonstrate that the proposed DLED method achieves state-of-the-art performance, outperforming various baseline models by an average of 20% in detecting forgeries from novel fake categories. Moreover, on the traditional Real-versus-Fake face forgery detection task, our DLED method concurrently exhibits competitive performance.", "AI": {"tldr": "This paper presents a method to detect novel types of face forgeries by estimating prediction uncertainty through a dual-level evidential approach, achieving better performance than existing methods.", "motivation": "The rapid development of face forgery generation algorithms results in new categories of fake faces, which challenges the current face forgery detection methods that are unable to detect these novel forgeries.", "method": "We propose the Dual-Level Evidential face forgery Detection (DLED) approach to detect novel types of face forgeries. This approach uses uncertainty estimation, collecting and fusing category-specific evidence on the spatial and frequency levels.", "result": "The proposed DLED method achieves state-of-the-art performance in detecting forgeries from novel categories, outperforming baseline models by an average of 20%. It also shows competitive performance on traditional Real-versus-Fake face forgery detection.", "conclusion": "This study addresses the Open Set Face Forgery Detection problem through a novel approach named DLED, which improves the capability of detecting novel types of face forgeries, thus enhancing the authenticity verification process in online content."}}
{"id": "2512.04748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04748", "abs": "https://arxiv.org/abs/2512.04748", "authors": ["Xinyue Kang", "Diwei Shi", "Li Chen"], "title": "Model Whisper: Steering Vectors Unlock Large Language Models' Potential in Test-time", "comment": "accepted to aaai2026", "summary": "It is a critical challenge to efficiently unlock the powerful reasoning potential of Large Language Models (LLMs) for specific tasks or new distributions. Existing test-time adaptation methods often require tuning model parameters, which is not only computationally expensive but also risks degrading the model's pre-existing abilities.To address this, we introduce a lightweight component, Test-Time Steering Vectors (TTSV), which is prepended to the input while keeping the LLM's parameters entirely frozen. By optimizing the TTSV on test data to minimize the model's output entropy, we steer the model towards an internal state of higher confidence, activating its inherent abilities most relevant to the current task. TTSV is both lightweight and highly efficient to optimize, making it a true plug-and-play enhancement. Extensive experiments validate our approach's effectiveness on both base models and reasoning-enhanced models. For instance, on the MATH500 task, TTSV achieves a 45.88% relative performance gain on the Qwen2.5-Math-7B model and a 16.22% relative gain on the Qwen3-4B model. Furthermore, our approach exhibits robust generalization, with its steering vectors proving highly transferable across diverse tasks.", "AI": {"tldr": "{\n  \"tldr\": \"本文介绍了一种轻量级组件测试时刻导向向量（TTSV），该组件在不调整大规模语言模型（LLMs）参数的情况下，通过优化输入前缀向量，提高了模型在特定任务上的推理能力与效率。实验表明该方法有效性和模型适应性。\", \n  \"motivation\": \"由于现有测试时刻模型调优方法往往需要调整模型参数，存在计算成本高和模型已有能力下降的风险，本文旨在提出一种轻量级且效果显著的测试时刻优化方法。\", \n  \"method\": \"引入了测试时刻导向向量（TTSV），通过最小化模型输出熵来优化该向量，从而引导模型进入对应任务更高置信度的状态，使其内在能力的最佳适应得以实现。\", \n  \"result\": \"实验结果显示，基于TTSV的方法在基线模型和推理增强模型上均表现出色，例如在MATH500任务上对Qwen2.5-Math-7B模型取得45.88%的相对性能提升。\", \n  \"conclusion\": \"TTSV作为一种轻量级、高效的优化方法，不需要调整模型参数，即可显著提高模型在特定任务中的表现，并展现出良好的任务跨适应性和潜在的广泛应用前景。\"}\n", "motivation": "{\n  \"tldr\": \"本文介绍了一种轻量级组件测试时刻导向向量（TTSV），该组件在不调整大规模语言模型（LLMs）参数的情况下，通过优化输入前缀向量，提高了模型在特定任务上的推理能力与效率。实验表明该方法有效性和模型适应性。\", \n  \"motivation\": \"由于现有测试时刻模型调优方法往往需要调整模型参数，存在计算成本高和模型已有能力下降的风险，本文旨在提出一种轻量级且效果显著的测试时刻优化方法。\", \n  \"method\": \"引入了测试时刻导向向量（TTSV），通过最小化模型输出熵来优化该向量，从而引导模型进入对应任务更高置信度的状态，使其内在能力的最佳适应得以实现。\", \n  \"result\": \"实验结果显示，基于TTSV的方法在基线模型和推理增强模型上均表现出色，例如在MATH500任务上对Qwen2.5-Math-7B模型取得45.88%的相对性能提升。\", \n  \"conclusion\": \"TTSV作为一种轻量级、高效的优化方法，不需要调整模型参数，即可显著提高模型在特定任务中的表现，并展现出良好的任务跨适应性和潜在的广泛应用前景。\"}\n", "method": "{\n  \"tldr\": \"本文介绍了一种轻量级组件测试时刻导向向量（TTSV），该组件在不调整大规模语言模型（LLMs）参数的情况下，通过优化输入前缀向量，提高了模型在特定任务上的推理能力与效率。实验表明该方法有效性和模型适应性。\", \n  \"motivation\": \"由于现有测试时刻模型调优方法往往需要调整模型参数，存在计算成本高和模型已有能力下降的风险，本文旨在提出一种轻量级且效果显著的测试时刻优化方法。\", \n  \"method\": \"引入了测试时刻导向向量（TTSV），通过最小化模型输出熵来优化该向量，从而引导模型进入对应任务更高置信度的状态，使其内在能力的最佳适应得以实现。\", \n  \"result\": \"实验结果显示，基于TTSV的方法在基线模型和推理增强模型上均表现出色，例如在MATH500任务上对Qwen2.5-Math-7B模型取得45.88%的相对性能提升。\", \n  \"conclusion\": \"TTSV作为一种轻量级、高效的优化方法，不需要调整模型参数，即可显著提高模型在特定任务中的表现，并展现出良好的任务跨适应性和潜在的广泛应用前景。\"}\n", "result": "{\n  \"tldr\": \"本文介绍了一种轻量级组件测试时刻导向向量（TTSV），该组件在不调整大规模语言模型（LLMs）参数的情况下，通过优化输入前缀向量，提高了模型在特定任务上的推理能力与效率。实验表明该方法有效性和模型适应性。\", \n  \"motivation\": \"由于现有测试时刻模型调优方法往往需要调整模型参数，存在计算成本高和模型已有能力下降的风险，本文旨在提出一种轻量级且效果显著的测试时刻优化方法。\", \n  \"method\": \"引入了测试时刻导向向量（TTSV），通过最小化模型输出熵来优化该向量，从而引导模型进入对应任务更高置信度的状态，使其内在能力的最佳适应得以实现。\", \n  \"result\": \"实验结果显示，基于TTSV的方法在基线模型和推理增强模型上均表现出色，例如在MATH500任务上对Qwen2.5-Math-7B模型取得45.88%的相对性能提升。\", \n  \"conclusion\": \"TTSV作为一种轻量级、高效的优化方法，不需要调整模型参数，即可显著提高模型在特定任务中的表现，并展现出良好的任务跨适应性和潜在的广泛应用前景。\"}\n}", "conclusion": "{\n  \"tldr\": \"本文介绍了一种轻量级组件测试时刻导向向量（TTSV），该组件在不调整大规模语言模型（LLMs）参数的情况下，通过优化输入前缀向量，提高了模型在特定任务上的推理能力与效率。实验表明该方法有效性和模型适应性。\", \n  \"motivation\": \"由于现有测试时刻模型调优方法往往需要调整模型参数，存在计算成本高和模型已有能力下降的风险，本文旨在提出一种轻量级且效果显著的测试时刻优化方法。\", \n  \"method\": \"引入了测试时刻导向向量（TTSV），通过最小化模型输出熵来优化该向量，从而引导模型进入对应任务更高置信度的状态，使其内在能力的最佳适应得以实现。\", \n  \"result\": \"实验结果显示，基于TTSV的方法在基线模型和推理增强模型上均表现出色，例如在MATH500任务上对Qwen2.5-Math-7B模型取得45.88%的相对性能提升。\", \n  \"conclusion\": \"TTSV作为一种轻量级、高效的优化方法，不需要调整模型参数，即可显著提高模型在特定任务中的表现，并展现出良好的任务跨适应性和潜在的广泛应用前景。\"}\n"}}
{"id": "2512.04356", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.04356", "abs": "https://arxiv.org/abs/2512.04356", "authors": ["Kai-Po Chang", "Wei-Yuan Cheng", "Chi-Pin Huang", "Fu-En Yang", "Yu-Chiang Frank Wang"], "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment", "comment": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026. Project page: https://kpc0810.github.io/santa/", "summary": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.", "AI": {"tldr": "本文提出了SANTA框架，专注于解决多模态大模型为视频生成描述时存在的幻觉问题，通过自我增强对比对齐的方法，在动态视频对象和动作上取得了更好的表现。", "motivation": "尽管多模态LLM在为视频生成描述性字幕方面表现出色，但在生成描述时存在事实不准确的问题，导致严重的幻觉问题。虽然以前的工作探索了减轻静态图像的幻觉，但对于动态视频中视觉对象和时间动作的幻觉问题，这是一个具有挑战性的未解决问题。", "method": "提出了一种名为SANTA的自我增强对比对齐框架，用以增强对动态视频中物体和动作的忠实度，该框架通过识别多模态大模型中的潜在幻觉，并将原始字幕转换为对比负面，以及开发了一种轨迹短语对比对齐技术，将区域物体和关系引导的动作与相应的视觉和时间短语相匹配。", "result": "广泛实验表明，SANTA在缓解物体和动作幻觉方面优于现有方法，在幻觉检测基准上表现出色。", "conclusion": "SANTA框架通过消除无关的相关性和强调视觉事实的技术，有效解决了视觉对象和时间动作的幻觉问题，该方法在缓解动态视频中的幻觉问题方面取得了显著的效果。"}}
{"id": "2512.04753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04753", "abs": "https://arxiv.org/abs/2512.04753", "authors": ["Ruilin Li", "Yibin Wang", "Wenhong Zhu", "Chenglin Li", "Jinghao Zhang", "Chenliang Li", "Junchi Yan", "Jiaqi Wang"], "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing", "comment": null, "summary": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.", "AI": {"tldr": "研究者通过实证分析及Edit-then-Consolidate框架解决了知识编辑中的过拟合与缺乏巩固问题，改进了模型的可靠性、泛化能力和保留原有能力。", "motivation": "现有的知识编辑方法在受控实验中表现良好，但在实际的终身学习场景中效果不佳。存在着模型过拟合新事实以及新信息与模型推断行为不吻合的问题。", "method": "文章提出了Edit-then-Consolidate框架，采用TPSFT技术来限制过拟合，并应用GRPO确保编辑的信息与模型的行为相匹配。", "result": "本次研究通过实证分析，揭示了当前传统的知识编辑方法存在的两个主要问题：过拟合新知识和缺乏知识巩固阶段，这导致编辑后的模型在实际应用中有局限性。为此，研究提出了Edit-then-Consolidate框架，采用目标导向的近似监督微调来减少过拟合，并通过群体相对策略优化对编辑的知识和推理策略进行对齐。实验表明，此框架显著提高了知识编辑的可靠性和泛化能力。", "conclusion": "此框架不仅提高了编辑后的模型的可靠性与泛化能力，还更好地保持了模型的局部性和预先训练的能力。"}}
{"id": "2512.04358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04358", "abs": "https://arxiv.org/abs/2512.04358", "authors": ["Ao Xu", "Rujin Zhao", "Xiong Xu", "Boceng Huang", "Yujia Jia", "Hongfeng Long", "Fuxuan Chen", "Zilong Cao", "Fangyuan Chen"], "title": "MAFNet:Multi-frequency Adaptive Fusion Network for Real-time Stereo Matching", "comment": null, "summary": "Existing stereo matching networks typically rely on either cost-volume construction based on 3D convolutions or deformation methods based on iterative optimization. The former incurs significant computational overhead during cost aggregation, whereas the latter often lacks the ability to model non-local contextual information. These methods exhibit poor compatibility on resource-constrained mobile devices, limiting their deployment in real-time applications. To address this, we propose a Multi-frequency Adaptive Fusion Network (MAFNet), which can produce high-quality disparity maps using only efficient 2D convolutions. Specifically, we design an adaptive frequency-domain filtering attention module that decomposes the full cost volume into high-frequency and low-frequency volumes, performing frequency-aware feature aggregation separately. Subsequently, we introduce a Linformer-based low-rank attention mechanism to adaptively fuse high- and low-frequency information, yielding more robust disparity estimation. Extensive experiments demonstrate that the proposed MAFNet significantly outperforms existing real-time methods on public datasets such as Scene Flow and KITTI 2015, showing a favorable balance between accuracy and real-time performance.", "AI": {"tldr": "我们提出了MAFNet，该网络使用高效的2D卷积，结合自适应频率域滤波注意模块和低秩注意机制，从而生成高质量且稳健的视差图，在公共数据集上表现出色。", "motivation": "现有的立体匹配网络通常依赖于基于3D卷积的成本体积构建或基于迭代优化的变形方法。前者在成本聚合期间计算开销大，而后者往往缺乏建模非局部语境信息的能力。这些方法在资源受限的移动设备上的兼容性较差，限制了它们在实时应用中的部署。为了应对这些问题，我们提出了MAFNet。", "method": "我们提出了一个Multi-frequency Adaptive Fusion Network (MAFNet)，使用高效的2D卷积来生成高质量的视差图。该网络设计了一个自适应频率域滤波注意模块，将完整的成本体分解为高频和低频体，并分别执行频率感知特征聚合。此外，引入了一种基于Linformer的低秩注意机制，用于自适应融合高频和低频信息，从而生成更稳健的视差估计。", "result": "大量的实验表明，我们提出的MAFNet在公共数据集如Scene Flow和KITTI 2015上显著优于现有的实时方法，在准确性和实时性能之间显示出良好的平衡。", "conclusion": "该研究提出的方法不仅解决了计算成本高的问题，而且通过引入自适应频率域滤波和低秩注意机制，在移动设备上实现了更佳的实时性能和高精度的视差图生成。"}}
{"id": "2512.04759", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04759", "abs": "https://arxiv.org/abs/2512.04759", "authors": ["Malvina Nissim", "Danilo Croce", "Viviana Patti", "Pierpaolo Basile", "Giuseppe Attanasio", "Elio Musacchio", "Matteo Rinaldi", "Federico Borazio", "Maria Francis", "Jacopo Gili", "Daniel Scalena", "Begoña Altuna", "Ekhi Azurmendi", "Valerio Basile", "Luisa Bentivogli", "Arianna Bisazza", "Marianna Bolognesi", "Dominique Brunato", "Tommaso Caselli", "Silvia Casola", "Maria Cassese", "Mauro Cettolo", "Claudia Collacciani", "Leonardo De Cosmo", "Maria Pia Di Buono", "Andrea Esuli", "Julen Etxaniz", "Chiara Ferrando", "Alessia Fidelangeli", "Simona Frenda", "Achille Fusco", "Marco Gaido", "Andrea Galassi", "Federico Galli", "Luca Giordano", "Mattia Goffetti", "Itziar Gonzalez-Dios", "Lorenzo Gregori", "Giulia Grundler", "Sandro Iannaccone", "Chunyang Jiang", "Moreno La Quatra", "Francesca Lagioia", "Soda Marem Lo", "Marco Madeddu", "Bernardo Magnini", "Raffaele Manna", "Fabio Mercorio", "Paola Merlo", "Arianna Muti", "Vivi Nastase", "Matteo Negri", "Dario Onorati", "Elena Palmieri", "Sara Papi", "Lucia Passaro", "Giulia Pensa", "Andrea Piergentili", "Daniele Potertì", "Giovanni Puccetti", "Federico Ranaldi", "Leonardo Ranaldi", "Andrea Amelio Ravelli", "Martina Rosola", "Elena Sofia Ruzzetti", "Giuseppe Samo", "Andrea Santilli", "Piera Santin", "Gabriele Sarti", "Giovanni Sartor", "Beatrice Savoldi", "Antonio Serino", "Andrea Seveso", "Lucia Siciliani", "Paolo Torroni", "Rossella Varvara", "Andrea Zaninello", "Asya Zanollo", "Fabio Massimo Zanzotto", "Kamyar Zeinalipour", "Andrea Zugarini"], "title": "Challenging the Abilities of Large Language Models in Italian: a Community Initiative", "comment": null, "summary": "The rapid progress of Large Language Models (LLMs) has transformed natural language processing and broadened its impact across research and society. Yet, systematic evaluation of these models, especially for languages beyond English, remains limited. \"Challenging the Abilities of LAnguage Models in ITAlian\" (CALAMITA) is a large-scale collaborative benchmarking initiative for Italian, coordinated under the Italian Association for Computational Linguistics. Unlike existing efforts that focus on leaderboards, CALAMITA foregrounds methodology: it federates more than 80 contributors from academia, industry, and the public sector to design, document, and evaluate a diverse collection of tasks, covering linguistic competence, commonsense reasoning, factual consistency, fairness, summarization, translation, and code generation. Through this process, we not only assembled a benchmark of over 20 tasks and almost 100 subtasks, but also established a centralized evaluation pipeline that supports heterogeneous datasets and metrics. We report results for four open-weight LLMs, highlighting systematic strengths and weaknesses across abilities, as well as challenges in task-specific evaluation. Beyond quantitative results, CALAMITA exposes methodological lessons: the necessity of fine-grained, task-representative metrics, the importance of harmonized pipelines, and the benefits and limitations of broad community engagement. CALAMITA is conceived as a rolling benchmark, enabling continuous integration of new tasks and models. This makes it both a resource -- the most comprehensive and diverse benchmark for Italian to date -- and a framework for sustainable, community-driven evaluation. We argue that this combination offers a blueprint for other languages and communities seeking inclusive and rigorous LLM evaluation practices.", "AI": {"tldr": "CALAMITA是一个针对意大利语的大规模协作基准项目，归纳了一系列评估任务，呈现了四种语言模型的系统性表现，强调了方法论的重要性，旨在成为一个持续更新的评估框架。", "motivation": "系统评估大型语言模型，特别是对于英语之外的语言的评估仍然有限，CALAMITA旨在填补这一空白，通过强调方法论来提升对意大利语的全面评估。", "method": "通过召集超过80位来自学术界、工业界和公共部门的贡献者，设计、记录和评估一个涵盖语言能力、常识推理、事实一致性、公平性、总结、翻译和代码生成等多个方面的多元化任务集合。", "result": "构建了一个包括超过20个任务和接近100个子任务的基准，并且支持异构数据集和度量标准的中心评估流程，报告了四种开源权重语言模型的结果。", "conclusion": "CALAMITA不仅是一个资源库，也是可持续的、社区驱动的评估框架，为其他语言和社区提供一个蓝图，促进包容和严谨的大型语言模型评估实践。"}}
{"id": "2512.04390", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04390", "abs": "https://arxiv.org/abs/2512.04390", "authors": ["Geunhyuk Youk", "Jihyong Oh", "Munchurl Kim"], "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring", "comment": "20 pages, 15 figures. Project Page: https://kaist-viclab.github.io/fmanetpp_site/", "summary": "Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.", "AI": {"tldr": "FMA-Net++框架实现了视频超分辨率和去模糊的联合处理，显著提升了在含有复杂退化的视频上的修复效果。", "motivation": "现实世界视频修复面临由运动和动态曝光变化导致的复杂退化，这一挑战在以往的工作中被忽视。", "method": "FMA-Net++采用分层细化双向传播块构建的序列级架构，其中每个块中包含曝光时间感知调制层和流动引导动态滤波模块，用于推断运动和曝光相关的退化核。FMA-Net++将退化学习与修复过程分开，以提高准确性和效率。", "result": "在新基准REDS-ME和REDS-RE以及GoPro数据集上，FMA-Net++展示了优秀的准确性和时间一致性，并且在修复质量和推理速度上超越了现有的方法，同时也很好地泛化到具有挑战性的现实世界视频场景。", "conclusion": "FMA-Net++通过引入曝光和运动感知的先验知识，显著提高了视频修复的质量和速度，适用于复杂退化场景下的视频处理。"}}
{"id": "2512.04765", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04765", "abs": "https://arxiv.org/abs/2512.04765", "authors": ["Pooja Singh", "Sandeep Kumar"], "title": "AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages", "comment": null, "summary": "Large language models and multilingual machine translation (MT) systems increasingly drive access to information, yet many languages of the tribal communities remain effectively invisible in these technologies. This invisibility exacerbates existing structural inequities in education, governance, and digital participation. We present AdiBhashaa, a community-driven initiative that constructs the first open parallel corpora and baseline MT systems for four major Indian tribal languages-Bhili, Mundari, Gondi, and Santali. This work combines participatory data creation with native speakers, human-in-the-loop validation, and systematic evaluation of both encoder-decoder MT models and large language models. In addition to reporting technical findings, we articulate how AdiBhashaa illustrates a possible model for more equitable AI research: it centers local expertise, builds capacity among early-career researchers from marginalized communities, and foregrounds human validation in the development of language technologies.", "AI": {"tldr": "AdiBhashaa项目旨在通过构建四种主要印度部落语言的开放平行语料库和基线机器翻译系统来促进更公平的人工智能研究。", "motivation": "很多部落社区的语言在大型语言模型和多语言机器翻译系统中依然不显眼，这加剧了教育、治理和数字参与方面的结构性不公平。项目旨在通过构建四种主要印度部落语言的平行语料库和基线MT系统来解决这一问题。", "method": "结合当地专家的知识，与说这四种语言的本地人一起创建数据，并采用人工干预验证的方法来构建模型。", "result": "构建了四种主要印度部落语言（Bhili、Mundari、Gondi和Santali）的基线机器翻译系统，并结合了参与式数据创建、人工干预验证以及对编码器-解码器模型和大型语言模型的系统性评估。", "conclusion": "AdiBhashaa项目展示了如何通过聚焦本地专业知识、在来自边缘化社区的早期职业研究者中建立能力，并将人工验证放在语言技术发展的前沿，实现更公平的人工智能研究。"}}
{"id": "2512.04395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04395", "abs": "https://arxiv.org/abs/2512.04395", "authors": ["Hieu Dinh Trung Pham", "Huy Minh Nhat Nguyen", "Cuong Tuan Nguyen"], "title": "Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models", "comment": null, "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.", "AI": {"tldr": "The paper discusses a novel framework named Fourier-Attentive Representation Learning (FARL), which improves Vision-Language Models' adaptability by disentangling visual representations using Fourier analysis, resulting in significant performance enhancements.", "motivation": "The motivation for this method stems from the observation that pre-trained Vision-Language Models (VLMs) inherently learn holistic representations, where the structural and stylistic elements of images are intertwined. The authors aim to improve generalization by explicitly separating these elements, thereby enhancing the model's adaptability.", "method": "The core of the method is a dual cross-attention mechanism named Fourier-Attentive Representation Learning (FARL). It involves learnable representation tokens querying an image's structural features from the phase spectrum and stylistic features from the amplitude spectrum using Fourier analysis. These tokens are then deeply integrated into the encoders of the Vision-Language Models (VLMs) to adapt the model better.", "result": "Experiments on 15 datasets showed that the proposed method effectively achieved its goal of improving the Vision-Language Model's adaptability through the disentanglement of visual representations.", "conclusion": "The research concludes that the disentanglement of visual representations in Vision-Language Models through Fourier analysis can significantly improve their performance, particularly in terms of adaptability and generalization."}}
{"id": "2512.04799", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04799", "abs": "https://arxiv.org/abs/2512.04799", "authors": ["Gianluca Barmina", "Nathalie Carmen Hau Norman", "Peter Schneider-Kamp", "Lukas Galke"], "title": "DaLA: Danish Linguistic Acceptability Evaluation Guided by Real World Errors", "comment": null, "summary": "We present an enhanced benchmark for evaluating linguistic acceptability in Danish. We first analyze the most common errors found in written Danish. Based on this analysis, we introduce a set of fourteen corruption functions that generate incorrect sentences by systematically introducing errors into existing correct Danish sentences. To ensure the accuracy of these corruptions, we assess their validity using both manual and automatic methods. The results are then used as a benchmark for evaluating Large Language Models on a linguistic acceptability judgement task. Our findings demonstrate that this extension is both broader and more comprehensive than the current state of the art. By incorporating a greater variety of corruption types, our benchmark provides a more rigorous assessment of linguistic acceptability, increasing task difficulty, as evidenced by the lower performance of LLMs on our benchmark compared to existing ones. Our results also suggest that our benchmark has a higher discriminatory power which allows to better distinguish well-performing models from low-performing ones.", "AI": {"tldr": "本文提出了一种增强的基准测试，用于评估丹麦语的语义接受度。通过使用手动和自动评估方法，我们的基准测试比现有技术更全面和严格。这导致语言模型在新的基准测试上的表现下降，表明了新的基准的较高判别能力。", "motivation": "提出改进的基准测试来评价丹麦语的语义接受度。", "method": "我们首先分析了书面丹麦语中常见的错误。基于这一分析，我们引入了一套由十四种误码函数组成的集合，这些函数通过对现有正确的丹麦语句子系统地引入错误来生成不正确的句子。为了确保这些误码的准确性，我们使用了手动和自动方法对其进行评估。", "result": "这一扩展不仅比当前最先进的技术更广泛，而且更全面。通过纳入更多种类的误码，我们的基准测试对语义接受度提供了更严格的评估，增加了任务的难度。结果表明，语言模型在我们的基准测试上的表现低于现有的测试。", "conclusion": "我们的基准测试具有更高的判别能力，能更好地区分表现良好模型与表现较差的模型。"}}
{"id": "2512.04397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04397", "abs": "https://arxiv.org/abs/2512.04397", "authors": ["Zeeshan Ahmad", "Shudi Bao", "Meng Chen"], "title": "Performance Evaluation of Transfer Learning Based Medical Image Classification Techniques for Disease Detection", "comment": null, "summary": "Medical image classification plays an increasingly vital role in identifying various diseases by classifying medical images, such as X-rays, MRIs and CT scans, into different categories based on their features. In recent years, deep learning techniques have attracted significant attention in medical image classification. However, it is usually infeasible to train an entire large deep learning model from scratch. To address this issue, one of the solutions is the transfer learning (TL) technique, where a pre-trained model is reused for a new task. In this paper, we present a comprehensive analysis of TL techniques for medical image classification using deep convolutional neural networks. We evaluate six pre-trained models (AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3) on a custom chest X-ray dataset for disease detection. The experimental results demonstrate that InceptionV3 consistently outperforms other models across all the standard metrics. The ResNet family shows progressively better performance with increasing depth, whereas VGG16 and AlexNet perform reasonably well but with lower accuracy. In addition, we also conduct uncertainty analysis and runtime comparison to assess the robustness and computational efficiency of these models. Our findings reveal that TL is beneficial in most cases, especially with limited data, but the extent of improvement depends on several factors such as model architecture, dataset size, and domain similarity between source and target tasks. Moreover, we demonstrate that with a well-trained feature extractor, only a lightweight feedforward model is enough to provide efficient prediction. As such, this study contributes to the understanding of TL in medical image classification, and provides insights for selecting appropriate models based on specific requirements.", "AI": {"tldr": "本文评估了六种预训练模型在胸部X光图像疾病检测中的表现，发现InceptionV3表现出色，并对模型的不确定性和运行效率进行了分析。研究强调迁移学习在医学图像分类中的重要性，并提出了选择合适模型的见解。", "motivation": "在医学图像分类领域，从头开始训练一个复杂的深度学习模型通常是不可行的。迁移学习作为一种解决方案，通过重用已有的预训练模型，对新的任务进行重新利用。本文旨在深入分析使用深度卷积神经网络在医学图像分类中迁移学习技术的相关性。", "method": "本研究利用深度卷积神经网络（CNN）分析了迁移学习（TL）技术在医学图像分类中的应用。作者评估了六种预训练模型（AlexNet、VGG16、ResNet18、ResNet34、ResNet50 和 InceptionV3），目的是检测胸部X光图像中的疾病。", "result": "实验结果表明，InceptionV3在所有标准指标上的表现都最好，ResNet家族随着网络深度的增加性能也逐步提升，而VGG16和AlexNet的表现良好但准确率较低。研究还进行了不确定性分析和运行时间比较，以评估这些模型的鲁棒性和计算效率。", "conclusion": "研究发现在大多数情况下，尤其是数据有限时，迁移学习是有效的，但改进的幅度取决于模型结构、数据集大小和源任务与目标任务之间的领域相似度等多个因素。此外，研究表明，通过合适的特征提取器，结合轻量级的前馈模型，就能提供高效的预测。"}}
{"id": "2512.04838", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04838", "abs": "https://arxiv.org/abs/2512.04838", "authors": ["L. D. M. S. Sai Teja", "N. Siva Gopala Krishna", "Ufaq Khan", "Muhammad Haris Khan", "Partha Pakray", "Atul Mishra"], "title": "DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable Attribution", "comment": "18 pages, 10 Figures", "summary": "In the age of advanced large language models (LLMs), the boundaries between human and AI-generated text are becoming increasingly blurred. We address the challenge of segmenting mixed-authorship text, that is identifying transition points in text where authorship shifts from human to AI or vice-versa, a problem with critical implications for authenticity, trust, and human oversight. We introduce a novel framework, called Info-Mask for mixed authorship detection that integrates stylometric cues, perplexity-driven signals, and structured boundary modeling to accurately segment collaborative human-AI content. To evaluate the robustness of our system against adversarial perturbations, we construct and release an adversarial benchmark dataset Mixed-text Adversarial setting for Segmentation (MAS), designed to probe the limits of existing detectors. Beyond segmentation accuracy, we introduce Human-Interpretable Attribution (HIA overlays that highlight how stylometric features inform boundary predictions, and we conduct a small-scale human study assessing their usefulness. Across multiple architectures, Info-Mask significantly improves span-level robustness under adversarial conditions, establishing new baselines while revealing remaining challenges. Our findings highlight both the promise and limitations of adversarially robust, interpretable mixed-authorship detection, with implications for trust and oversight in human-AI co-authorship.", "AI": {"tldr": "本文提出了一个检测人类和AI生成文本混合作者身份的新框架Info-Mask，该框架通过多种技术的集成来提高对抗条件下的稳健性，并评估了这种系统在特定基准数据集上的效果。", "motivation": "该论文的动机是解决在文本中分辨人类和AI生成内容边界的问题，特别是在存在对抗性扰动的情况下，以维护内容的真实性和人类的监督。", "method": "作者提出了一种名为Info-Mask的新框架，通过结合风格特征中的线索、困惑度驱动的信号以及结构化的边界建模来准确地分段混合人类和AI生成的内容。", "result": "通过一个设计用来测试现有检测器极限的对抗基准数据集MAS，验证了该系统的鲁棒性。此外，该系统还引入了人类可解释的归属（HIA）覆盖层来展示风格特征是如何影响边界预测的，并进行了一项小型的人类研究来评估其有效性。", "conclusion": "Info-Mask在多种架构下显著提高了对抗条件下的跨度级鲁棒性，建立了新的基准，同时揭示了仍然存在的挑战。该研究强调了对抗鲁棒性且可解释的混合作者身份检测的前景与局限性。"}}
{"id": "2512.04413", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04413", "abs": "https://arxiv.org/abs/2512.04413", "authors": ["Xiangyi Gao", "Danpei Zhao", "Bo Yuan", "Wentao Li"], "title": "Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection", "comment": "12 pages, 8 figures, 11 tables", "summary": "Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.", "AI": {"tldr": "提出 DS2D2 方法，结合光谱分解的显式和隐式知识蒸馏，有效解决了遥感图像知识蒸馏中的问题，增强了目标检测性能。实验显示在 DIOR 数据集上对 RetinaNet 和 Faster R-CNN 提高了 AP50。代码将在 https://github.com/PolarAid/DS2D2 上公开。", "motivation": "现有知识蒸馏方法在遥感图像中遇到混合特征问题，并忽视了细微特征变化带来的差异，导致知识混淆。因此，提出 DS2D2 方法来解决这些问题。", "method": "Knowledge distillation 方法用于轻量化遥感目标检测，提出了一个架构无关的知识蒸馏方法 DS2D2，结合了基于光谱分解的显式和隐式蒸馏。首先使用一阶小波变换进行光谱分解以保留关键的空间特征，设计了密度无关尺度权重（DISW）来应对稠密和小物体检测问题。其次，通过全频和高频放大器提取隐含在学生-教师特征差异中的知识，这些知识显著影响检测头的预测结果。", "result": "在 DIOR 和 DOTA 数据集上的大量实验验证了所提方法的有效性。在 DIOR 数据集上，DS2D2 对 RetinaNet 提高了 4.2% 的 AP50，对 Faster R-CNN 提高了 3.8% 的 AP50，超过了现有的知识蒸馏方法。", "conclusion": "DS2D2 方法是用于通用遥感目标检测任务的架构无关蒸馏方法，通过显式和隐式知识蒸馏提高了检测性能。"}}
{"id": "2512.04844", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04844", "abs": "https://arxiv.org/abs/2512.04844", "authors": ["Atsuki Yamaguchi", "Terufumi Morishita", "Aline Villavicencio", "Nikolaos Aletras"], "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates", "comment": null, "summary": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.", "AI": {"tldr": "研究提出SSU策略以解决大型语言模型在低资源条件下的灾难性遗忘问题，并通过实验证明了其有效性。", "motivation": "解决语言模型在低资源条件下，仅通过未标记的目标语言数据进行适应时，避免灾难性遗忘以及保持源语言能力的挑战。", "method": "提出了一种名为Source-Shielded Updates (SSU) 的选择性参数更新策略，该策略在适应之前通过列冻结策略保护关键参数来主动保存源知识。", "result": "在五种不同类型的语言和两个规模的模型上进行的实验表明，SSU显著降低源语言任务性能下降的幅度，并且在目标语言任务上表现与全面微调相当甚至更优。", "conclusion": "SSU策略成功地缓解了灾难性遗忘，仅使源语言任务的性能下降3.4%（7B模型）和2.8%（13B模型），并且在目标语言上的表现与全面微调相媲美，甚至在大多数情况下优于全面微调（对于13B模型）。"}}
{"id": "2512.04421", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.04421", "abs": "https://arxiv.org/abs/2512.04421", "authors": ["Changhe Liu", "Ehsan Javanmardi", "Naren Bao", "Alex Orsholits", "Manabu Tsukada"], "title": "UTrice: Unifying Primitives in Differentiable Ray Tracing and Rasterization via Triangles for Particle-Based 3D Scenes", "comment": "13 pages, 10 figures, submitted to CVPR2026", "summary": "Ray tracing 3D Gaussian particles enables realistic effects such as depth of field, refractions, and flexible camera modeling for novel-view synthesis. However, existing methods trace Gaussians through proxy geometry, which requires constructing complex intermediate meshes and performing costly intersection tests. This limitation arises because Gaussian-based particles are not well suited as unified primitives for both ray tracing and rasterization. In this work, we propose a differentiable triangle-based ray tracing pipeline that directly treats triangles as rendering primitives without relying on any proxy geometry. Our results show that the proposed method achieves significantly higher rendering quality than existing ray tracing approaches while maintaining real-time rendering performance. Moreover, our pipeline can directly render triangles optimized by the rasterization-based method Triangle Splatting, thus unifying the primitives used in novel-view synthesis.", "AI": {"tldr": "本文提出了一种新的三角形基光线追踪管线，提高了渲染效果，且保持实时性能，统一了用于新型视图合成的渲染原语。", "motivation": "现有的方法将高斯粒子通过代理几何体进行光线追踪，这需要构建复杂的中间网格并执行昂贵的相交测试。这种方法不适用于将高斯粒子作为统一的光线追踪和光栅化原语。", "method": "提出了一种可微分的三角形基光线追踪管线，该管线直接将三角形作为渲染原语，无需依赖任何代理几何体。", "result": "实验结果显示，提出的方法在渲染质量上显著优于现有的光线追踪方法，同时保持实时渲染性能。", "conclusion": "本方法能够直接渲染通过光栅化方法Triangle Splatting优化的三角形，统一路线了新型视图合成中使用的原始形状。"}}
{"id": "2512.04868", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04868", "abs": "https://arxiv.org/abs/2512.04868", "authors": ["Hao Wang", "Jialun Zhong", "Changcheng Wang", "Zhujun Nie", "Zheng Li", "Shunyu Yao", "Yanzeng Li", "Xinchi Li"], "title": "SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs", "comment": null, "summary": "Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.", "AI": {"tldr": "文章介绍SEAL方法，一种基于自我进化代理学习的两阶段语义解析框架，提高了问答系统的结构准确性和计算效率。", "motivation": "为了解决现有的知识图谱问答系统在处理复杂查询和保持结构准确性方面的局限性，尤其是其高昂的计算成本。", "method": "SEAL采用两阶段的语义解析框架，首先通过大型语言模型提取输入查询的核心S表达式，然后通过代理校准模块修正语法不一致并精确对齐实体和关系。第二阶段使用基于模板的完成，通过问题类型预测和占位符实例化来构建可执行的S表达式。框架还集成了自我进化机制，其通过整合局部和全球记忆与反思模块，使框架能够从对话历史和执行反馈中连续适应，而不需显式重新训练。", "result": "在SPICE基准测试中，SEAL达到了最先进的性能，尤其是在多跳推理、比较和聚合任务中的表现极为突出。", "conclusion": "SEAL框架通过在结构准确性和计算效率上的显著增强，证明了其在健壮和可扩展的对话推理能力中的优势。"}}
{"id": "2512.04425", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04425", "abs": "https://arxiv.org/abs/2512.04425", "authors": ["Manar Alnaasan", "Md Selim Sarowar", "Sungho Kim"], "title": "Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models", "comment": null, "summary": "Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM", "AI": {"tldr": "本文提出了一种结合RGB-D数据的可解释多模态框架，用于识别帕金森病患者的步态模式。该框架利用双YOLOv11编码器和特定机制增强时空表示，并结合LLM提供可解释性，实验结果显示其在多方面超越单输入基线方法的性能。", "motivation": "准确且可解释的步态分析在帕金森病(PD)的早期检测中起着至关重要的作用，但现有的大多数方法仍受限于单模态输入、低鲁棒性和缺乏临床透明度。本文旨在提出一种新的帕金森步态分析方法，以解决这些问题。", "method": "本文提出了一种可解释的多模态框架，该框架结合RGB和深度图(D)数据，以在实际条件下识别帕金森步态模式。系统采用双YOLOv11编码器进行模态特定特征提取，然后通过多尺度局部全局提取（MLGE）模块和跨空间颈部融合机制增强时空表示。为了确保可解释性，将冻结的大语言模型(LLM)整合进来，将融合的视觉嵌入和结构化元数据翻译成具有临床意义的文本解释。", "result": "实验评估显示，所提出的RGB-D融合框架相比单输入基线，在识别准确性、对环境变化的鲁棒性和清晰的视效语言推理方面均取得了更高的成绩。", "conclusion": "通过结合多模态特征学习与基于语言的可解释性，本研究弥补了视觉识别与临床理解之间的差距，为可靠的帕金森病步态分析提供了一种新型视效语言范式。"}}
{"id": "2512.04957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04957", "abs": "https://arxiv.org/abs/2512.04957", "authors": ["Weiye Shi", "Zhaowei Zhang", "Shaoheng Yan", "Yaodong Yang"], "title": "LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics", "comment": null, "summary": "Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.", "AI": {"tldr": "研究构建了一个多语言体裁分类数据集，用于评估大型语言模型（LLMs）的学习能力和所捕捉的语言特征对分类效果的影响。结果显示不同语言特征在任务中的贡献不均，表明了在训练模型时需要综合考虑复杂语言信号。", "motivation": "研究动机在于探讨大型语言模型（LLMs）是否能够从原始文本中捕捉深层次的语用特性和音韵模式，并将这些特征应用于重要的自然语言任务。", "method": "本研究构建了一个多语言体裁分类数据集，数据来源于Project Gutenberg，包含六种语言的数千个句子，用于二元分类任务（诗歌与小说；戏剧与诗歌；戏剧与小说）。此外，数据集还包括三种显式的语言特征（句法树结构、隐喻数量和音韵度量），用于评估这些特征对分类性能的影响。", "result": "实验表明，尽管LLMs分类器能够从原始文本或显式提供的特征中学习潜在的语言结构，但不同特征在不同任务中的贡献率不均。这强调了在模型训练过程中整合更复杂语言信号的重要性。", "conclusion": "结论支持大型语言模型能够学习并应用语言特征来改善分类性能，但是不同的特征在不同的任务中影响不一，凸显了在模型训练中整合多样语言信号的重要性。"}}
{"id": "2512.04426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04426", "abs": "https://arxiv.org/abs/2512.04426", "authors": ["Sidan Zhu", "Hongteng Xu", "Dixin Luo"], "title": "Self-Paced and Self-Corrective Masked Prediction for Movie Trailer Generation", "comment": null, "summary": "As a challenging video editing task, movie trailer generation involves selecting and reorganizing movie shots to create engaging trailers. Currently, most existing automatic trailer generation methods employ a \"selection-then-ranking\" paradigm (i.e., first selecting key shots and then ranking them), which suffers from inevitable error propagation and limits the quality of the generated trailers. Beyond this paradigm, we propose a new self-paced and self-corrective masked prediction method called SSMP, which achieves state-of-the-art results in automatic trailer generation via bi-directional contextual modeling and progressive self-correction. In particular, SSMP trains a Transformer encoder that takes the movie shot sequences as prompts and generates corresponding trailer shot sequences accordingly. The model is trained via masked prediction, reconstructing each trailer shot sequence from its randomly masked counterpart. The mask ratio is self-paced, allowing the task difficulty to adapt to the model and thereby improving model performance. When generating a movie trailer, the model fills the shot positions with high confidence at each step and re-masks the remaining positions for the next prediction, forming a progressive self-correction mechanism that is analogous to how human editors work. Both quantitative results and user studies demonstrate the superiority of SSMP in comparison to existing automatic movie trailer generation methods. Demo is available at: https://github.com/Dixin-Lab/SSMP.", "AI": {"tldr": "A novel SSMP method is introduced for automatic movie trailer generation, using a Transformer model with progressive self-correction to enhance trailer quality beyond existing 'selection-then-ranking' methods.", "motivation": "The motivation is to overcome the limitations of the traditional 'selection-then-ranking' approach in trailer generation, which can lead to error propagation and lower quality of the trailers.", "method": "The paper proposes a new self-paced and self-corrective masked prediction method (SSMP) for automatic trailer generation, which uses a Transformer encoder to model bi-directional context and progressively self-correct predictions during the generation process.", "result": "Both quantitative results and user studies show that SSMP outperforms existing methods for automatic movie trailer generation, demonstrating its superiority in trailer quality and engagement.", "conclusion": "The study concludes that SSMP, with its progressive self-correction mechanism and bi-directional contextual modeling, achieves state-of-the-art results in generating high-quality movie trailers."}}
{"id": "2512.04987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.04987", "abs": "https://arxiv.org/abs/2512.04987", "authors": ["Nex-AGI Team", ":", "Yuxuan Cai", "Lu Chen", "Qiaoling Chen", "Yuyang Ding", "Liwen Fan", "Wenjie Fu", "Yufei Gao", "Honglin Guo", "Pinxue Guo", "Zhenhua Han", "Zhengfu He", "Hanglei Hu", "Kai Hu", "Shengjia Hua", "Tianyu Huai", "Baodai Huang", "Li Ji", "Zhen Jiang", "Zhikai Lei", "Bufan Li", "Jiahang Lin", "Lizhi Lin", "Jinxiu Liu", "Shichun Liu", "Ziming Liu", "Yuchen Ni", "Pengfang Qian", "Yujiong Shen", "Qingyun Shi", "Wentao Shu", "Peng Sun", "Yiran Suo", "Tian Tang", "Boyu Tian", "Guoteng Wang", "Junzhe Wang", "Peixin Wang", "Zhiheng Xi", "Hang Yan", "Jie Yang", "Zhixiong Yang", "Tianchu Yao", "Guangze Ye", "Qianxi Yu", "Shuo Zhang", "Xinyue Zhang", "Yiqi Zhang", "Jiarong Zhao", "Miao Zheng", "Rui Zheng", "Enyu Zhou", "Jiazheng Zhou", "Maosen Zhou", "Yuhao Zhou", "Tao Gui", "Yining Zheng", "Xinchi Chen", "Jie Zhou", "Siyuan Feng", "Qin Chen", "Liang He", "Qi Zhang", "Xuanjing Huang", "Xipeng Qiu"], "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "comment": null, "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "AI": {"tldr": "论文提出了一种创新的方法，通过三个维度（复杂度、多样性和保真度）来扩展互动环境的多样性和复杂性，从而解决大规模语言模型从被动响应者转变到自主代理时面临的基础设施问题。这种方法显著提高了训练出的语言模型在复杂代理任务中的表现。", "motivation": "论文动机在于解决大语言模型从被动响应者转变为自主代理需要从静态模仿到激励驱动决策的基本学习范式的转变，同时解决由于缺乏能够构造高质量互动信号的可扩展基础设施而受到阻碍的问题。", "method": "此论文提出了一种全面的方法来系统地扩展互动环境的多样性和复杂性，以解决由于缺乏能够构建高质量交互信号的可扩展基础设施而导致的大规模语言模型转变为自动代理的过程受到阻碍的问题。该方法主要通过处理三个正交维度来实现扩展：复杂度、多样性和保真度。复杂度方面通过NexAU灵活的代理框架支持构建复杂的代理层级；多样性通过NexA4A自动从自然语言生成多样性的代理层级来覆盖无限的领域；保真度通过NexGAP集成动态的真实世界环境来合成基于现实的轨迹，从而弥合模拟与现实之间的差距。", "result": "实验结果显示，基于由所提出的基础设施建立的多样且复杂的互动环境训练出的Nex-N1在SWE-bench和tau2等基准测试中，一致超越了最先进的开源模型，并在复杂的代理任务上表现出与前沿的专有模型相当的性能。", "conclusion": "由于在各种基准测试上的成功，该论文建议在未来的语言模型研究中采用此提出的框架和方法，为大语言模型向自主代理的转型提供必要的复杂且多样化环境支持以实现更高效的政策学习。同时也开放了Nex生态系统和模型权重以促进进一步的研究。"}}
{"id": "2512.04441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04441", "abs": "https://arxiv.org/abs/2512.04441", "authors": ["Bin Suna", "Yaoguang Caob", "Yan Wanga", "Rui Wanga", "Jiachen Shanga", "Xiejie Fenga", "Jiayi Lu", "Jia Shi", "Shichun Yang", "Xiaoyu Yane", "Ziying Song"], "title": "MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of \"context simulation - candidate generation - multi-objective trade-off\". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned \"what-if\" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.", "AI": {"tldr": "MindDrive是一种结合高质量轨迹生成和全面决策推理的自动驾驶框架，通过未来的轨迹生成和多目标评估来提高决策合理性，优于现有的自动驾驶研究方法。研究表明它在多个驾驶指标上达到了最先进的性能，提高安全性、合规性和泛化能力。", "motivation": "现有的自动驾驶研究主要分为两类：轨迹生成导向的研究，侧重于产生高质量的轨迹但决策机制相对简单；轨迹选择导向的研究，侧重于多维度评估选择最佳轨迹但生成能力不足。为了弥补这些不足，本研究提出了一个融合高质量轨迹生成与全面决策推理的和谐框架MindDrive。", "method": "内容介绍了MindDrive框架，该框架结合了高质量轨迹生成和全面决策推理。它建立了“上下文模拟-候选生成-多目标权衡”的结构化推理范式。特别地，基于World Action Model (WaM) 的Future-aware Trajectory Generator (FaTG) 执行自我条件“假设”模拟，以预测潜在的未来场景并生成有前瞻性的轨迹候选。在此基础上，VLM-oriented Evaluator (VLoE) 利用大型视觉语言模型的推理能力来进行多目标评估，促进合理且符合人类直觉的决策。", "result": "在NAVSIM-v1和NAVSIM-v2基准测试中的广泛实验表明，MindDrive在多维度驾驶指标上达到了最先进的性能，显著提高了安全性、合规性以及泛化能力。", "conclusion": "该研究为可解释的、认知引导的自动驾驶提供了一个有希望的方向，证明了MindDrive作为一种结合高质量轨迹生成和全面决策推理的框架，在自动驾驶研究中的潜力。"}}
{"id": "2512.05012", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.05012", "abs": "https://arxiv.org/abs/2512.05012", "authors": ["Francielle Vargas", "Daniel Pedronette"], "title": "Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking", "comment": "This work was presented as a poster at the Applied Social Media Lab during the 2025 Synthesizer & Open Showcase at the Berkman Klein Center for Internet & Society at Harvard University", "summary": "This extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.", "AI": {"tldr": "The paper presents a method called Self-Explaining Contrastive Evidence Re-Ranking that uses contrastive learning to improve the factual accuracy and transparency of information retrieval, particularly beneficial in safety-critical sectors.", "motivation": "The motivation behind this paper is to enhance the reliability of retrieval systems, especially in domains with high safety requirements, such as healthcare, by mitigating the risk of misinterpretation and hallucinations in the data retrieval process.", "method": "Self-Explaining Contrastive Evidence Re-Ranking (CER) combines contrastive learning, embedding fine-tuning with a focus on factual evidence, and automatic selection of hard negatives based on a subjectivity criterion to improve retrieval accuracy and reliability.", "result": "Experimental results demonstrate that the proposed method improves the retrieval accuracy and provides better alignment with evidence-based reasoning, while reducing the potential for hallucinations within retrieval-augmented generation systems.", "conclusion": "The conclusion drawn from this study is that by focusing on factual evidence and employing a discriminative contrastive learning approach along with transparent attributions, it is possible to significantly enhance the reliability of information retrieval, ensuring that the systems are more robust and trustworthy."}}
{"id": "2512.04451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04451", "abs": "https://arxiv.org/abs/2512.04451", "authors": ["Yifei Wang", "Zhenkai Li", "Tianwen Qian", "Huanran Zheng", "Zheng Wang", "Yuqian Fu", "Xiaoling Wang"], "title": "StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios", "comment": null, "summary": "As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.", "AI": {"tldr": "StreamEQA是首个流媒体视频问答具身场景的基准测试，评估模型处理视觉输入和理解动态场景的能力，发现现有模型在流媒体理解方面仍面临挑战。", "motivation": "随着具身智能向实际部署的推进，持续感知和推理流媒体视觉输入的能力变得至关重要。该基准测试旨在促进这一方向的研究进展。", "method": "介绍了一个名为StreamEQA的首个基准测试，用于评估在具身场景下的流媒体视频问答能力。StreamEQA从具身和流媒体两个维度评估现有的多模态大型语言模型（MLLM），涉及感知、交互和规划三个逐步提升难度的任务等级，以及涉及往后、实时和往前推理的时间维度。", "result": "通过评估13个高级视频LLMs，发现在传统基准测试中的表现不一定能转化为在具身场景中的流媒体视频理解能力。", "conclusion": "StreamEQA有望促进对流媒体视频理解在具身应用方面的研究。"}}
{"id": "2512.05033", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05033", "abs": "https://arxiv.org/abs/2512.05033", "authors": ["Monishwaran Maheswaran", "Rishabh Tiwari", "Yuezhou Hu", "Kerem Dilmen", "Coleman Hooper", "Haocheng Xi", "Nicholas Lee", "Mehrdad Farajtabar", "Michael W. Mahoney", "Kurt Keutzer", "Amir Gholami"], "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation", "comment": "22 pages", "summary": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\\sim2\\times$ at matched accuracy.", "AI": {"tldr": "This paper introduces Arbitrage, a new speculative decoding framework that improves the efficiency-accuracy trade-offs for large language models in reasoning tasks.", "motivation": "The main motivation is to enhance the performance-cost ratio of large language models, particularly in tasks requiring long chains of reasoning. Traditional token-level speculative decoding fails due to wasted compute from unnecessary rejections, and existing step-level methods still have inefficiencies.", "method": "Arbitrage is introduced as a new step-level speculative generation framework. It employs a lightweight router, trained to predict the conditions under which the target model will produce a superior step, thereby reducing unnecessary rejections and improving efficiency compared to standard step-level speculative decoding.", "result": "Arbitrage achieves a significant reduction in inference latency of up to approximately 2 times while maintaining comparable accuracy against traditional step-level speculative decoding methods, across various mathematical reasoning benchmarks.", "conclusion": "Arbitrage demonstrates superior performance in terms of speed and accuracy when compared to existing step-level speculative decoding techniques, which can be attributed to its dynamic generation strategy and adaptive acceptance threshold mechanism."}}
{"id": "2512.04456", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04456", "abs": "https://arxiv.org/abs/2512.04456", "authors": ["Changjin Kim", "HyeokJun Lee", "YoungJoon Yoo"], "title": "GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis", "comment": "AAAI2026", "summary": "Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.", "AI": {"tldr": "GuidNoise通过单个图像对和特定损失函数，生成高质量的噪声图像，减少元数据依赖，提高实际场景中的去噪效率。", "motivation": "近期的图像去噪方法依赖于生成模型进行真实噪声合成，但这些模型通常需要摄像头的元数据和大量目标特定的噪声-清晰图像对，并且在不同设置之间泛化能力有限。该论文旨在减少这些需求。", "method": "提出GuidNoise方法，利用单个噪声/清晰图像对作为指导，引入指导感知仿射特征修改（GAFM）和噪声感知细化损失来训练模型，优化后向过程以生成更真实的噪声分布。", "result": "该论文提出了一种名为GuidNoise的方法，用于基于单个噪声/清晰图像对合成高质量的噪声图像。通过引入指导感知仿射特征修改（GAFM）和噪声感知细化损失，GuidNoise能够在不依赖额外元数据的情况下生成多种噪声环境下的真实噪声分布。这种方法不仅减少了对大量噪声-清晰图像对的需求，还使得噪声图像对的生成更加高效，从而提高了在实际场景特别是模型较轻且训练数据有限情况下的去噪性能。", "conclusion": "GuidNoise能够不依赖额外元数据，在多种噪声环境下生成高质量的噪声图像，且能够在推理阶段高效生成噪声-清晰图像对，这提高了自增广数据的去噪性能。"}}
{"id": "2512.05100", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05100", "abs": "https://arxiv.org/abs/2512.05100", "authors": ["Haiyue Song", "Johannes Eschbach-Dymanus", "Hour Kaing", "Sumire Honda", "Hideki Tanaka", "Bianka Buschbeck", "Masao Utiyama"], "title": "Structured Document Translation via Format Reinforcement Learning", "comment": "IJCNLP-AACL 2025 Main (Oral)", "summary": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.", "AI": {"tldr": "提出了FormatRL方法来解决现有模型在处理结构化文本翻译如XML或HTML文档结构方面的局限性。", "motivation": "现有对结构化文本翻译的研究通常局限于句子层面，难以有效处理复杂的XML或HTML文档级别的结构。为了应对这个问题，提出了一种新的方法。", "method": "提出了一种称为格式强化学习（FormatRL）的方法，它在监督微调模型之上应用了组相对策略优化，以直接优化新设计的结构感知奖励：TreeSim（衡量预测和参考XML树之间的结构相似度）和Node-chrF（衡量XML节点级别的翻译质量）。", "result": "在SAP软件文档基准上的实验证明了模型在六项指标上的改进，并进一步分析表明了不同奖励函数如何分别对结构和翻译质量的提高作出贡献。", "conclusion": "实验表明了模型在结构和翻译质量上的改进，证明了新提出的方法的有效性。"}}
{"id": "2512.04459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.04459", "abs": "https://arxiv.org/abs/2512.04459", "authors": ["Yingzi Ma", "Yulong Cao", "Wenhao Ding", "Shuibai Zhang", "Yan Wang", "Boris Ivanovic", "Ming Jiang", "Marco Pavone", "Chaowei Xiao"], "title": "dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning", "comment": null, "summary": "The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.", "AI": {"tldr": "提出了一种基于离散扩散模型的视觉语言模型 (dVLM-AD)，该模型通过统一感知、结构化推理和低层次规划来提高端到端驾驶系统的性能和一致性，优于基于自回归模型的方法。", "motivation": "现有的基于自回归的视觉语言模型在高层次推理和低层次规划之间的一致性和可控性上受限，因此研究尝试引入基于离散扩散模型的新方法，以改进在多样环境中的泛化能力。", "method": "引入了dVLM-AD，这是一种基于扩散模型的视觉语言模型，旨在统一感知、结构化推理和低层次规划，用于端到端驾驶。", "result": "在nuScenes和WOD-E2E数据集上的评估表明，dVLM-AD生成了一致的推理动作配对，并且与现有的VLM/VLA系统相比，dVLM-AD在行为轨迹一致性方面提高了9%，在WOD-E2E长尾场景中的RFS指标提升了6%。", "conclusion": "dVLM-AD在保持较为简单的模型架构的同时，相较于其他现有的视觉语言模型和视觉语言代理显著提升了驾驶行为和轨迹的一致性以及在长尾场景中的规划性能，表明其为端到端自动驾驶提供了一种可控且可靠的改进路径。"}}
