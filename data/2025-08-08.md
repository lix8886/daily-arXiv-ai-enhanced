<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)
*Thomas Thebaud,Yen-Ju Lu,Matthew Wiesner,Peter Viechnicki,Najim Dehak*

Main category: cs.CL

> 本文提出了一种在对话转录后处理中添加说话人特征标签的方法，使用冻结的Whisper/WavLM音频基础模型和LLAMA语言模型，展示了在保持模型速度和模块化的同时，具有竞争力的性能。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是在对话转录管道中，大型语言模型（LLM）常常用于改善语法、标点和可读性。本文探讨了一种补充的后处理步骤：通过添加元数据标签来丰富转录对话，标签包括说话人的特征，如年龄、性别和情绪。

**Method:** 文章的方法是利用冻结的音频基础模型（如Whisper或WavLM）与冻结的LLAMA语言模型相结合，无需对模型进行特定任务的微调，以推断说话人的特征，如年龄、性别和情绪。通过轻量级、高效的连接器来桥接音频和语言表示，从而在保持模块化和速度的同时达到竞争性的说话人剖析任务性能。此外，文章还展示了一个冻结的LLAMA模型可以直接比较x-vectors，在某些场景中达到8.8%的等错误率。

**Result:** 实验结果表明，该方法在说话人特征剖析任务中具有竞争力，同时保持了模型的模块化和速度优势。

**Conclusion:** 研究表明，冻结的音频基础模型与冻结的LLAMA语言模型相结合，可有效推断说话人的特征，无需进行任务特定的微调，同时保持了模型的速度和模块化优势。

**Abstract:** In dialogue transcription pipelines, Large Language Models (LLMs) are
frequently employed in post-processing to improve grammar, punctuation, and
readability. We explore a complementary post-processing step: enriching
transcribed dialogues by adding metadata tags for speaker characteristics such
as age, gender, and emotion. Some of the tags are global to the entire
dialogue, while some are time-variant. Our approach couples frozen audio
foundation models, such as Whisper or WavLM, with a frozen LLAMA language model
to infer these speaker attributes, without requiring task-specific fine-tuning
of either model. Using lightweight, efficient connectors to bridge audio and
language representations, we achieve competitive performance on speaker
profiling tasks while preserving modularity and speed. Additionally, we
demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving
an Equal Error Rate of 8.8% in some scenarios.

</details>


### [2] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)
*Negar Foroutan,Clara Meister,Debjit Paul,Joel Niklaus,Sina Ahmadi,Antoine Bosselut,Rico Sennrich*

Main category: cs.CL

> 提出Parity-aware BPE改进BPE算法，以实现跨语言分词更加公平，无显著负面影响于全局压缩率或语言模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 标准的分词器训练算法依赖于基于频率的目标，这偏向于在训练数据中占主导地位的语言，并且导致低资源语言的分词过长、不符合形态学或者充斥着<UNK>占位符。这一现象最终加剧了不同语言背景用户之间的计算和经济不平等。

**Method:** 引入了语言公平字节配对编码（Parity-aware Byte Pair Encoding，Parity-aware BPE），这是一种广泛使用的BPE算法的变体。在每一个合并步骤中，Parity-aware BPE最大化当前压缩效果最差的语言的压缩增益，以实现跨语言之间的均衡，即使是在全局压缩率上的增益相对较小。

**Result:** 实验证明，使用Parity-aware BPE在不同语言之间实现了更公平的词汇计数，对全局压缩率几乎没有影响，且对下游任务中的语言模型性能没有显著影响。

**Conclusion:** 该方法能够在不显著牺牲全局压缩效率和语言模型性能的情况下，改进低资源语言的分词质量，提高跨语言分词的公平性。

**Abstract:** Tokenization is the first -- and often least scrutinized -- step of most NLP
pipelines. Standard algorithms for learning tokenizers rely on frequency-based
objectives, which favor languages dominant in the training data and
consequently leave lower-resource languages with tokenizations that are
disproportionately longer, morphologically implausible, or even riddled with
<UNK> placeholders. This phenomenon ultimately amplifies computational and
financial inequalities between users from different language backgrounds. To
remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of
the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes
the compression gain of the currently worst-compressed language, trading a
small amount of global compression for cross-lingual parity. We find
empirically that Parity-aware BPE leads to more equitable token counts across
languages, with negligible impact on global compression rate and no substantial
effect on language-model performance in downstream tasks.

</details>


### [3] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)
*David Sasu,Natalie Schluter*

Main category: cs.CL

> 通过引入联合ASR和重音检测模型，结合使用半监督语音表示的ASR系统的性能得以提高。重音检测组件显著提升了该任务的F1得分，同时在有限资源微调下，ASR性能也降低了LibriSpeech上的WER。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在通过集成重音检测模块来提升使用半监督语音表示的ASR系统的性能。

**Method:** 提出了一种联合ASR和重音检测模型，该模型可以改进重音检测任务，并在联合训练中提高ASR性能。

**Result:** 重音检测任务的F1得分显著提高，ASR系统的WER在LibriSpeech数据集上减少了28.3%。

**Conclusion:** 研究结果显示，扩展预训练语音模型以保持或重新学习重要的韵律线索，如重音，对提升ASR性能至关重要。

**Abstract:** We show the performance of Automatic Speech Recognition (ASR) systems that
use semi-supervised speech representations can be boosted by a complimentary
pitch accent detection module, by introducing a joint ASR and pitch accent
detection model. The pitch accent detection component of our model achieves a
significant improvement on the state-of-the-art for the task, closing the gap
in F1-score by 41%. Additionally, the ASR performance in joint training
decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With
these results, we show the importance of extending pretrained speech models to
retain or re-learn important prosodic cues such as pitch accent.

</details>


### [4] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)
*Tommaso Tosato,Saskia Helbling,Yorguin-Jose Mantilla-Ramos,Mahmood Hegazy,Alberto Tosato,David John Lemay,Irina Rish,Guillaume Dumas*

Main category: cs.CL

> 研究提出PERSIST框架，用于评估大规模语言模型的行为稳定性，发现即使是非常大的模型也存在显著响应变异性，且无法通过现有策略提升一致性。这表明，目前的大规模语言模型缺乏行为上的一致性基础，对需要高度可控性的应用来说，传统的对齐策略可能是不足的。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在深入探索大规模语言模型的行为特性和人格稳定模式，特别是在实际部署安全性方面存在的问题。当前模型生成的文本在一致性上仍然存在很大的不确定性，这限制了其应用于实际中的安全和可信赖水平。通过系统的测试和评估，希望能够找到提升模型一致性和稳定性的途径。

**Method:** 使用PERSIST框架，全面测试了25个开源模型（参数从1B到671B），通过超过500,000个响应进行评估。采用了传统的人格测试工具（BFI-44, SD3）和为LLM适配的新型人格测试工具，系统地变化了问题顺序、重述、人格设定以及推理模式。

**Result:** 研究结果挑战了部署模型的基本假设：（1）即使4000亿以上参数量的模型也表现出显著的响应变异性（SD>0.4）；（2）仅通过改变问题的顺序就能使人格测量变化高达20%；（3）预期可以稳定行为的干预措施（如链式思维推理、详细的人格设定指导、包含对话历史等）可能反而增加了一致性；（4）为LLM适配的人格测试工具显示出与以人类为中心的版本相同的不稳定性，证实了这种不稳定性源于架构而非转化过程中存在的局限性。

**Conclusion:** 这些结果表明，当前的大规模语言模型缺乏行为上的一致性基础。对于需要可预测行为的安全关键性应用，基于个性对齐策略可能从根本上不足。

**Abstract:** Large language models require consistent behavioral patterns for safe
deployment, yet their personality-like traits remain poorly understood. We
present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive
evaluation framework testing 25+ open-source models (1B-671B parameters) across
500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted
personality instruments, we systematically vary question order, paraphrasing,
personas, and reasoning modes. Our findings challenge fundamental deployment
assumptions: (1) Even 400B+ models exhibit substantial response variability (SD
> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up
to 20%; (3) Interventions expected to stabilize behavior, such as
chain-of-thought reasoning, detailed personas instruction, inclusion of
conversation history, can paradoxically increase variability; (4) LLM-adapted
instruments show equal instability to human-centric versions, confirming
architectural rather than translational limitations. This persistent
instability across scales and mitigation strategies suggests current LLMs lack
the foundations for genuine behavioral consistency. For safety-critical
applications requiring predictable behavior, these findings indicate that
personality-based alignment strategies may be fundamentally inadequate.

</details>


### [5] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)
*Jun Liu,Zhenglun Kong,Changdi Yang,Fan Yang,Tianqi Li,Peiyan Dong,Joannah Nanjekye,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.CL

> 提出了RCR-Router框架，用于动态选择语义相关记忆子集，实现多智能体LLM系统在保持答案质量的同时减少标记使用。

<details>
  <summary>Details</summary>

**Motivation:** 现有的协调策略导致了标记的过度消耗、冗余的记忆暴露和有限的适应性，因此需要一种新的记忆路由框架。

**Method:** RCR-Router基于每个智能体的角色和任务阶段，动态选择语义相关的记忆子集，并遵循严格的标记预算。

**Result:** 实验结果表明，RCR-Router可以减少标记的使用（最高减少30%），同时保持或提高答案质量。

**Conclusion:** 实验说明了结构化记忆路由和输出感知评估对于推进可扩展多智能体LLM系统的重要性。

**Abstract:** Multi-agent large language model (LLM) systems have shown strong potential in
complex reasoning and collaborative decision-making tasks. However, most
existing coordination schemes rely on static or full-context routing
strategies, which lead to excessive token consumption, redundant memory
exposure, and limited adaptability across interaction rounds. We introduce
RCR-Router, a modular and role-aware context routing framework designed to
enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,
this is the first routing approach that dynamically selects semantically
relevant memory subsets for each agent based on its role and task stage, while
adhering to a strict token budget. A lightweight scoring policy guides memory
selection, and agent outputs are iteratively integrated into a shared memory
store to facilitate progressive context refinement. To better evaluate model
behavior, we further propose an Answer Quality Score metric that captures
LLM-generated explanations beyond standard QA accuracy. Experiments on three
multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate
that RCR-Router reduces token usage (up to 30%) while improving or maintaining
answer quality. These results highlight the importance of structured memory
routing and output-aware evaluation in advancing scalable multi-agent LLM
systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration](https://arxiv.org/abs/2508.04797)
*Mohab Kishawy,Ali Abdellatif Hussein,Jun Chen*

Main category: cs.CV

> 提出RetinexDual，一个基于Retinex理论的UHD图像复原框架，结合两个互补子网络，解决传统方法的缺陷，并在四个任务上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 传统的图像超分辨率复原方法存在明显缺陷，如极端降采样导致信息不可逆丢失，纯频域方法由于丢失降级局部性而无效。针对这些限制，提出了新的方法。

**Method:** 提出RetinexDual框架，包含两个互补子网络：SAMBA和FIA。SAMBA用于纠正反射分量，采用粗略到精细的机制，在减少伪影和恢复细节方面表现出色。FIA在频率域操作，纠正颜色和光照失真。

**Result:** 在去雨、去模糊、去雾、低光图像增强四个任务上，RetinexDual在定性和定量上均优于近期方法。消融研究证明了不同设计在ResinexDual中的重要性及各个组件的有效性。

**Conclusion:** 实验结果表明，RetinexDual在UHD图像复原上具有显著效果，各组成部分对于提升性能有重要作用。

**Abstract:** Advancements in image sensing have elevated the importance of
Ultra-High-Definition Image Restoration (UHD IR). Traditional methods, such as
extreme downsampling or transformation from the spatial to the frequency
domain, encounter significant drawbacks: downsampling induces irreversible
information loss in UHD images, while our frequency analysis reveals that pure
frequency-domain approaches are ineffective for spatially confined image
artifacts, primarily due to the loss of degradation locality. To overcome these
limitations, we present RetinexDual, a novel Retinex theory-based framework
designed for generalized UHD IR tasks. RetinexDual leverages two complementary
sub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination
Adaptor (FIA). SAMBA, responsible for correcting the reflectance component,
utilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,
which effectively reduces artifacts and restores intricate details. On the
other hand, FIA ensures precise correction of color and illumination
distortions by operating in the frequency domain and leveraging the global
context provided by it. Evaluating RetinexDual on four UHD IR tasks, namely
deraining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows
that it outperforms recent methods qualitatively and quantitatively. Ablation
studies demonstrate the importance of employing distinct designs for each
branch in RetinexDual, as well as the effectiveness of its various components.

</details>


### [7] [ACM Multimedia Grand Challenge on ENT Endoscopy Analysis](https://arxiv.org/abs/2508.04801)
*Trong-Thuan Nguyen,Viet-Tham Huynh,Thao Thi Phuong Dao,Ha Nguyen Thi,Tien To Vu Thuy,Uyen Hanh Tran,Tam V. Nguyen,Thanh Dinh Le,Minh-Triet Tran*

Main category: cs.CV

> ENTRep挑战结合了细粒度解剖分类与图像检索，支持双语环境下的临床评估，促进了ENT内窥镜影像分析的发展。

<details>
  <summary>Details</summary>

**Motivation:** 针对耳鼻喉科内窥镜分析中自动化影像分析的不足，提出ENTRep挑战，提高图像分类与检索能力，满足临床需求。

**Method:** 整合细粒度解剖分类，图像到图像，文本到图像检索；双语言临床监督；定义三个基准任务；标准化提交和评估协议。

**Result:** 该摘要介绍了ENTRep，ACM多媒体2025年ENT内窥镜分析的高难度挑战，旨在结合细粒度的解剖分类与图像之间的检索和文本到图像的检索，并使用双语（越南语和英语）临床监督。数据集包含专家标注的解剖区域及正常或异常状态的图像，并附有双语描述。定义了三个基准任务，标准化提交协议，并使用服务器端评分在公共和私人测试集中评估性能。同时，还报告了顶尖团队的结果，并进行了见解分析。

**Conclusion:** 该摘要展示了ENTRep挑战的重要性及其对于ENT医学影像分析的贡献，并强调了多语言支持和详细的临床应用。

**Abstract:** Automated analysis of endoscopic imagery is a critical yet underdeveloped
component of ENT (ear, nose, and throat) care, hindered by variability in
devices and operators, subtle and localized findings, and fine-grained
distinctions such as laterality and vocal-fold state. In addition to
classification, clinicians require reliable retrieval of similar cases, both
visually and through concise textual descriptions. These capabilities are
rarely supported by existing public benchmarks. To this end, we introduce
ENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,
which integrates fine-grained anatomical classification with image-to-image and
text-to-image retrieval under bilingual (Vietnamese and English) clinical
supervision. Specifically, the dataset comprises expert-annotated images,
labeled for anatomical region and normal or abnormal status, and accompanied by
dual-language narrative descriptions. In addition, we define three benchmark
tasks, standardize the submission protocol, and evaluate performance on public
and private test splits using server-side scoring. Moreover, we report results
from the top-performing teams and provide an insight discussion.

</details>


### [8] [CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework](https://arxiv.org/abs/2508.04816)
*Sriram Mandalika,Lalitha V*

Main category: cs.CV

> CoMAD unifies and distills the knowledge from multiple self-supervised Vision Transformers into a compact model, improving performance and practicality.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the challenges of large, impractical models and the isolation of knowledge in self-supervised learning, leading to a more practical and efficient model.

**Method:** Consensus-oriented Masked Distillation (CoMAD), a lightweight framework that unifies knowledge from multiple self-supervised Vision Transformers into a smaller student network. It uses asymmetric masking, linear adapter, layer normalization, joint consensus gating, and dual-level KL divergence for training.

**Result:** CoMAD's ViT-Tiny achieves 75.4% Top-1 on ImageNet-1K, setting new records in dense-prediction transfers on ADE20K and MS-COCO.

**Conclusion:** CoMAD demonstrates its effectiveness in creating a compact model that outperforms previous state-of-the-arts, offering a practical solution to self-supervised learning for resource-constrained environments.

**Abstract:** Numerous self-supervised learning paradigms, such as contrastive learning and
masked image modeling, learn powerful representations from unlabeled data but
are typically pretrained in isolation, overlooking complementary insights and
yielding large models that are impractical for resource-constrained deployment.
To overcome these challenges, we introduce Consensus-oriented Masked
Distillation (CoMAD), a lightweight, parameter-free framework that unifies
knowledge from multiple current state-of-the-art self-supervised Vision
Transformers into a compact student network. CoMAD distills from three
pretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct
semantic and contextual priors. Rather than naively averaging teacher outputs,
we apply asymmetric masking: the student sees only 25 percent of patches while
each teacher receives a progressively lighter, unique mask, forcing the student
to interpolate missing features under richer contexts. Teacher embeddings are
aligned to the student's space via a linear adapter and layer normalization,
then fused through our joint consensus gating, which weights each token by
combining cosine affinity with inter-teacher agreement. The student is trained
with dual-level KL divergence on visible tokens and reconstructed feature maps,
capturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny
achieves 75.4 percent Top-1, an increment of 0.4 percent over the previous
state-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU
on ADE20K, and 44.5 percent box average precision and 40.5 percent mask average
precision on MS-COCO, establishing a new state-of-the-art in compact SSL
distillation.

</details>


### [9] [Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models](https://arxiv.org/abs/2508.04818)
*Mehrdad Moradi,Marco Grasso,Bianca Maria Colosimo,Kamran Paynabar*

Main category: cs.CV

> 我们提出了RADAR，该方法在无重构的情况下，直接从扩散模型生成异常图，以此克服了现有方法的局限性，并在多个数据集上取得了比现有SOTA方法更好的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 近期，扩散模型作为异常检测和分割的一种强大替代方案，其由于在重构过程中存在的计算开销大、对于复杂或细微模式的重构可能产生不同正常模式的图像以及选择适当的中间噪声水平有难度等问题成为了研究热点。为了解决这些挑战，我们提出了一种新的模型。

**Method:** 我们提出了RADAR（无重构注意力扩散模型实时异常检测），它直接从扩散模型生成异常图，而非重构输入图像，这不仅提高了检测准确性，还提升了计算效率。

**Result:** 在MVTec-AD数据集上，我们的方法比次优模型F1提升了7%，在3D打印材料数据集上F1提升达13%。

**Conclusion:** RADAR方法在保持高检测准确性的同时，提升了计算效率，适用于实时应用，在多个关键指标上超越了现有的基于扩散模型和统计机器学习的方法。

**Abstract:** Generative models have demonstrated significant success in anomaly detection
and segmentation over the past decade. Recently, diffusion models have emerged
as a powerful alternative, outperforming previous approaches such as GANs and
VAEs. In typical diffusion-based anomaly detection, a model is trained on
normal data, and during inference, anomalous images are perturbed to a
predefined intermediate step in the forward diffusion process. The
corresponding normal image is then reconstructed through iterative reverse
sampling.
  However, reconstruction-based approaches present three major challenges: (1)
the reconstruction process is computationally expensive due to multiple
sampling steps, making real-time applications impractical; (2) for complex or
subtle patterns, the reconstructed image may correspond to a different normal
pattern rather than the original input; and (3) Choosing an appropriate
intermediate noise level is challenging because it is application-dependent and
often assumes prior knowledge of anomalies, an assumption that does not hold in
unsupervised settings.
  We introduce Reconstruction-free Anomaly Detection with Attention-based
diffusion models in Real-time (RADAR), which overcomes the limitations of
reconstruction-based anomaly detection. Unlike current SOTA methods that
reconstruct the input image, RADAR directly produces anomaly maps from the
diffusion model, improving both detection accuracy and computational
efficiency. We evaluate RADAR on real-world 3D-printed material and the
MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and
statistical machine learning models across all key metrics, including accuracy,
precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on
MVTec-AD and 13% on the 3D-printed material dataset compared to the next best
model.
  Code available at: https://github.com/mehrdadmoradi124/RADAR

</details>


### [10] [A deep learning approach to track eye movements based on events](https://arxiv.org/abs/2508.04827)
*Chirag Seth,Divya Naiken,Keyan Lin*

Main category: cs.CV

> 研究提出了一种基于CNN_LSTM模型的新型眼动追踪方法，能够以较低成本实现高达81%的准确性，并探索了未来增强模型解释性的方向。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于人眼快速移动的特点（速度可达300°/s），传统的精准眼动追踪通常需要昂贵且高速的相机。为解决这一问题并降低追踪成本，要求开发一种可解释且成本效益高的算法来预测人的注意力，以提升设备的舒适度及用户体验。

**Method:** 利用卷积神经网络与长短期记忆网络（CNN_LSTM）结合的方法来预测人的注意力，实现对眼部中心位置（x, y）的追踪。

**Result:** 该方法实现了大约81%的准确性。

**Conclusion:** 研究表明，CNN_LSTM模型在眼动追踪上最为有效。同时提出未来的研究将继续关注层间相关性传播（LRP）技术，以进一步提高模型的可解释性和预测性能。

**Abstract:** This research project addresses the challenge of accurately tracking eye
movements during specific events by leveraging previous research. Given the
rapid movements of human eyes, which can reach speeds of 300{\deg}/s, precise
eye tracking typically requires expensive and high-speed cameras. Our primary
objective is to locate the eye center position (x, y) using inputs from an
event camera. Eye movement analysis has extensive applications in consumer
electronics, especially in VR and AR product development. Therefore, our
ultimate goal is to develop an interpretable and cost-effective algorithm using
deep learning methods to predict human attention, thereby improving device
comfort and enhancing overall user experience. To achieve this goal, we
explored various approaches, with the CNN\_LSTM model proving most effective,
achieving approximately 81\% accuracy. Additionally, we propose future work
focusing on Layer-wise Relevance Propagation (LRP) to further enhance the
model's interpretability and predictive performance.

</details>


### [11] [LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction](https://arxiv.org/abs/2508.04847)
*Md Zahidul Hasan,A. Ben Hamza,Nizar Bouguila*

Main category: cs.CV

> LuKAN模型使用KAN（Kolmogorov-Arnold Networks）结合Lucas多项式激活函数，通过离散小波变换编码时间信息、空间投影层捕捉关节间依赖性、Temporal Dependency Learner核心模块进行高效函数近似，并通过逆离散小波变换重建3D人体运动序列预测，具有高性能和计算效率。

<details>
  <summary>Details</summary>

**Motivation:** 现有的3D人体运动预测方法在预测准确性和计算效率之间难以取得平衡。

**Method:** 模型首先使用离散小波变换编码时间信息；然后空间投影层捕捉关节间的结构依赖性；核心的Temporal Dependency Learner利用KAN层和Lucas多项式进行高效的函数近似；最后通过逆小波变换重建运动序列。

**Result:** 在三个基准数据集上的大量实验显示，该模型在与强基线模型比较时展现出竞争力，并且结合了紧凑的架构和Lucas多项式的线性循环过程以保证计算效率。

**Conclusion:** LuKAN模型通过引入KAN和Lucas多项式实现高效准确的3D人体运动预测，并且具备高效的计算性能。

**Abstract:** The goal of 3D human motion prediction is to forecast future 3D poses of the
human body based on historical motion data. Existing methods often face
limitations in achieving a balance between prediction accuracy and
computational efficiency. In this paper, we present LuKAN, an effective model
based on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations.
Our model first applies the discrete wavelet transform to encode temporal
information in the input motion sequence. Then, a spatial projection layer is
used to capture inter-joint dependencies, ensuring structural consistency of
the human body. At the core of LuKAN is the Temporal Dependency Learner, which
employs a KAN layer parameterized by Lucas polynomials for efficient function
approximation. These polynomials provide computational efficiency and an
enhanced capability to handle oscillatory behaviors. Finally, the inverse
discrete wavelet transform reconstructs motion sequences in the time domain,
generating temporally coherent predictions. Extensive experiments on three
benchmark datasets demonstrate the competitive performance of our model
compared to strong baselines, as evidenced by both quantitative and qualitative
evaluations. Moreover, its compact architecture coupled with the linear
recurrence of Lucas polynomials, ensures computational efficiency.

</details>


### [12] [VER-Bench: Evaluating MLLMs on Reasoning with Fine-Grained Visual Evidence](https://arxiv.org/abs/2508.04852)
*Chenhui Qiang,Zhaoyang Wei,Xumeng Han Zipeng Wang,Siyao Li,Xiangyuan Lan,Jianbin Jiao,Zhenjun Han*

Main category: cs.CV

> The paper presents VER-Bench, a new evaluation framework for MLLMs focusing on deep reasoning with subtle visual clues, revealing current models' limitations in this area.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the current limitations of benchmarks in evaluating MLLMs' deep reasoning capabilities from subtle, local visual details critical for profound visual understanding.

**Method:** To evaluate MLLMs' visual capabilities, the paper introduces VER-Bench, a novel framework focusing on identifying fine-grained visual clues and integrating them with world knowledge for reasoning.

**Result:** VER-Bench highlights that existing models struggle with extracting subtle visual evidence and constructing evidence-based reasoning, indicating a significant room for improvement in fine-grained visual reasoning.

**Conclusion:** The conclusion is that to achieve genuine visual understanding and human-like analysis, it's necessary to enhance models' capabilities in extracting and integrating subtle, fine-grained visual evidence.

**Abstract:** With the rapid development of MLLMs, evaluating their visual capabilities has
become increasingly crucial. Current benchmarks primarily fall into two main
types: basic perception benchmarks, which focus on local details but lack deep
reasoning (e.g., "what is in the image?"), and mainstream reasoning benchmarks,
which concentrate on prominent image elements but may fail to assess subtle
clues requiring intricate analysis. However, profound visual understanding and
complex reasoning depend more on interpreting subtle, inconspicuous local
details than on perceiving salient, macro-level objects. These details, though
occupying minimal image area, often contain richer, more critical information
for robust analysis. To bridge this gap, we introduce the VER-Bench, a novel
framework to evaluate MLLMs' ability to: 1) identify fine-grained visual clues,
often occupying on average just 0.25% of the image area; 2) integrate these
clues with world knowledge for complex reasoning. Comprising 374 carefully
designed questions across Geospatial, Temporal, Situational, Intent, System
State, and Symbolic reasoning, each question in VER-Bench is accompanied by
structured evidence: visual clues and question-related reasoning derived from
them. VER-Bench reveals current models' limitations in extracting subtle visual
evidence and constructing evidence-based arguments, highlighting the need to
enhance models's capabilities in fine-grained visual evidence extraction,
integration, and reasoning for genuine visual understanding and human-like
analysis. Dataset and additional materials are available
https://github.com/verbta/ACMMM-25-Materials.

</details>


### [13] [Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications](https://arxiv.org/abs/2508.04868)
*Noreen Anwar,Guillaume-Alexandre Bilodeau,Wassim Bouachir*

Main category: cs.CV

> DAMM框架通过引入查询适应性和结构化交叉注意力，解决了基于Transformer的对象检测器在遮挡、细粒度定位和计算效率方面的问题，并在多个基准测试中表现优异，达到先进水平。

<details>
  <summary>Details</summary>

**Motivation:** 基于Transformer的对象检测器常常在处理遮挡、细粒度定位和计算效率低的问题上表现不佳，这些问题由固定的查询和密集的注意力机制引发。

**Method:** DAMM（Dual-stream Attention with Multi-Modal queries）框架引入了查询适应性和结构化交叉注意力，用于提高准确性和效率，包括三种类型的查询：基于外观的查询、位置查询和随机学习到的查询，以及一个双重流交叉注意力模块，分别完善语义和空间特征，以提高混乱场景中的定位精度。

**Result:** DAMM在四个具有挑战性的基准上进行了评估，达到了最先进的平均精度（AP）和召回率。

**Conclusion:** 该研究结果表明，多模态查询适应和双重流注意力的有效性。

**Abstract:** Transformer-based object detectors often struggle with occlusions,
fine-grained localization, and computational inefficiency caused by fixed
queries and dense attention. We propose DAMM, Dual-stream Attention with
Multi-Modal queries, a novel framework introducing both query adaptation and
structured cross-attention for improved accuracy and efficiency. DAMM
capitalizes on three types of queries: appearance-based queries from
vision-language models, positional queries using polygonal embeddings, and
random learned queries for general scene coverage. Furthermore, a dual-stream
cross-attention module separately refines semantic and spatial features,
boosting localization precision in cluttered scenes. We evaluated DAMM on four
challenging benchmarks, and it achieved state-of-the-art performance in average
precision (AP) and recall, demonstrating the effectiveness of multi-modal query
adaptation and dual-stream attention. Source code is at:
\href{https://github.com/DET-LIP/DAMM}{GitHub}.

</details>


### [14] [Revealing Temporal Label Noise in Multimodal Hateful Video Classification](https://arxiv.org/abs/2508.04900)
*Shuonan Yang,Tailin Chen,Rahul Singh,Jiangbei Yue,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

> 该研究通过细粒度的方法分析了多模态仇恨视频中的时间标注噪声问题，发现粗略的视频级标注会导致标签噪声，影响模型的决策边界和分类信心，强调了时间感知模型和基准的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决由粗略的视频级标注产生的标签噪声问题，特别是这些标注忽略了仇恨内容的时间粒度，导致视频中包含大量的非仇恨片段。

**Method:** 方法是截取来自HateMM和MultiHateClip英语数据集的仇恨视频中被注释时间戳明确表示的仇恨段落，并分析这些截取段落中仇恨和非仇恨内容的分布和特征。

**Result:** 结果表明，在这些截取的仇恨段落中，语义重叠和由粗略的视频级标注引入的混淆显著存在，时间标注噪声极大地改变了模型的决策边界，削弱了分类信心。

**Conclusion:** 结论指出，了解多模态仇恨视频的时间动力学是必要的，并强调了开发能够考虑时间和连续性的情境感知模型的必要性。

**Abstract:** The rapid proliferation of online multimedia content has intensified the
spread of hate speech, presenting critical societal and regulatory challenges.
While recent work has advanced multimodal hateful video detection, most
approaches rely on coarse, video-level annotations that overlook the temporal
granularity of hateful content. This introduces substantial label noise, as
videos annotated as hateful often contain long non-hateful segments. In this
paper, we investigate the impact of such label ambiguity through a fine-grained
approach. Specifically, we trim hateful videos from the HateMM and
MultiHateClip English datasets using annotated timestamps to isolate explicitly
hateful segments. We then conduct an exploratory analysis of these trimmed
segments to examine the distribution and characteristics of both hateful and
non-hateful content. This analysis highlights the degree of semantic overlap
and the confusion introduced by coarse, video-level annotations. Finally,
controlled experiments demonstrated that time-stamp noise fundamentally alters
model decision boundaries and weakens classification confidence, highlighting
the inherent context dependency and temporal continuity of hate speech
expression. Our findings provide new insights into the temporal dynamics of
multimodal hateful videos and highlight the need for temporally aware models
and benchmarks for improved robustness and interpretability. Code and data are
available at
https://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.

</details>
