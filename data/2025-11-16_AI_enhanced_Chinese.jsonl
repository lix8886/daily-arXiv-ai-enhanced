{"id": "2511.09690", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09690", "abs": "https://arxiv.org/abs/2511.09690", "authors": ["Omnilingual ASR team", "Gil Keren", "Artyom Kozhevnikov", "Yen Meng", "Christophe Ropers", "Matthew Setzler", "Skyler Wang", "Ife Adebara", "Michael Auli", "Can Balioglu", "Kevin Chan", "Chierh Cheng", "Joe Chuang", "Caley Droof", "Mark Duppenthaler", "Paul-Ambroise Duquenne", "Alexander Erben", "Cynthia Gao", "Gabriel Mejia Gonzalez", "Kehan Lyu", "Sagar Miglani", "Vineel Pratap", "Kaushik Ram Sadagopan", "Safiyyah Saleem", "Arina Turkatenko", "Albert Ventayol-Boada", "Zheng-Xin Yong", "Yu-An Chung", "Jean Maillard", "Rashel Moritz", "Alexandre Mourachko", "Mary Williamson", "Shireen Yates"], "title": "Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages", "comment": null, "summary": "Automatic speech recognition (ASR) has advanced in high-resource languages, but most of the world's 7,000+ languages remain unsupported, leaving thousands of long-tail languages behind. Expanding ASR coverage has been costly and limited by architectures that restrict language support, making extension inaccessible to most--all while entangled with ethical concerns when pursued without community collaboration. To transcend these limitations, we introduce Omnilingual ASR, the first large-scale ASR system designed for extensibility. Omnilingual ASR enables communities to introduce unserved languages with only a handful of data samples. It scales self-supervised pre-training to 7B parameters to learn robust speech representations and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. This capability is grounded in a massive and diverse training corpus; by combining breadth of coverage with linguistic variety, the model learns representations robust enough to adapt to unseen languages. Incorporating public resources with community-sourced recordings gathered through compensated local partnerships, Omnilingual ASR expands coverage to over 1,600 languages, the largest such effort to date--including over 500 never before served by ASR. Automatic evaluations show substantial gains over prior systems, especially in low-resource conditions, and strong generalization. We release Omnilingual ASR as a family of models, from 300M variants for low-power devices to 7B for maximum accuracy. We reflect on the ethical considerations shaping this design and conclude by discussing its societal impact. In particular, we highlight how open-sourcing models and tools can lower barriers for researchers and communities, inviting new forms of participation. Open-source artifacts are available at https://github.com/facebookresearch/omnilingual-asr.", "AI": {"tldr": "该论文提出了Omnilingual ASR，这是一种能用少量数据样本引入未被服务的7000多种语言中的大部分的新ASR系统，它借助70亿参数的自监督预训练和零样本泛化编码器-解码器架构，覆盖超过1600种语言，包括以前未被ASR服务的500多种语言。", "motivation": "动机在于解决高资源语言之外的大部分世界语言（超过7000种）仍未被ASR支持的问题，并通过合作的方式减少扩展ASR所涉及的伦理问题。", "method": "该研究采用了一种扩展性强的ASR系统设计，系统包含70亿参数的自监督预训练以学习鲁棒的语音表征，同时使用了一种受LLM启发的编码器-解码器架构，以实现零样本的泛化。", "result": "该论文介绍了Omnilingual ASR，这是第一个大规模可扩展的ASR系统，能够通过少量数据样本引入未被服务的语言。该系统通过自监督的预训练和编码器-解码器架构学习强大的语音表征，覆盖超过1600种语言，包括以前未被ASR服务的500多种语言。自动评估表明，与先前的系统相比，尤其是在低资源条件下，该系统的性能有了显著提高。", "conclusion": "研究展示了Omnilingual ASR系统的强大泛化能力，并开放了包括3亿参数和70亿参数的模型库，希望能够降低研究者和社区的进入壁垒，推动更多形式的合作和参与。"}}
{"id": "2511.09700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09700", "abs": "https://arxiv.org/abs/2511.09700", "authors": ["Warren Li", "Yiqian Wang", "Zihan Wang", "Jingbo Shang"], "title": "Order Matters: Rethinking Prompt Construction in In-Context Learning", "comment": null, "summary": "In-context learning (ICL) enables large language models to perform new tasks by conditioning on a sequence of examples. Most prior work reasonably and intuitively assumes that which examples are chosen has a far greater effect on performance than how those examples are ordered, leading to a focus on example selection. We revisit this assumption and conduct a systematic comparison between the effect of selection and ordering. Through controlled experiments on both classification and generation tasks, using multiple open-source model families (0.5B to 27B parameters) and GPT-5, we find that the variance in performance due to different example orderings is comparable to that from using entirely different example sets. Furthermore, we show that strong orderings can be identified using only a development set, achieving performance close to an oracle that selects the best ordering based on test labels. Our findings highlight the equal and intertwined importance of example selection and ordering in prompt design, calling for a reexamination of the assumptions held in ICL.", "AI": {"tldr": "研究显示，在大语言模型的上下文学习中，示例的排序和选择对性能的影响同样重要，这需要改变原有的注意力结构。", "motivation": "重新审视并验证示例选择优于排序的传统观念，通过系统性对比以评估两者对模型性能的影响。", "method": "通过在分类和生成任务上的受控实验，使用多个开源模型系列（0.5B到27B参数）和GPT-5，比对示例选择与示例排序对性能的影响。", "result": "发现不同示例排序对性能的方差与完全不同的示例集的选择所引起的方差相当。进一步证明使用开发集即可找到较强的排序，其性能接近根据测试标签选择最佳排序的oracle。", "conclusion": "强调在指令设计中，示例选择和排序具有同等重要性和紧密联系，需要重新审视ICL中的假定。"}}
{"id": "2511.09709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09709", "abs": "https://arxiv.org/abs/2511.09709", "authors": ["Marisa Hudspeth", "Patrick J. Burns", "Brendan O'Connor"], "title": "Contextual morphologically-guided tokenization for Latin encoder models", "comment": null, "summary": "Tokenization is a critical component of language model pretraining, yet standard tokenization methods often prioritize information-theoretical goals like high compression and low fertility rather than linguistic goals like morphological alignment. In fact, they have been shown to be suboptimal for morphologically rich languages, where tokenization quality directly impacts downstream performance. In this work, we investigate morphologically-aware tokenization for Latin, a morphologically rich language that is medium-resource in terms of pretraining data, but high-resource in terms of curated lexical resources -- a distinction that is often overlooked but critical in discussions of low-resource language modeling. We find that morphologically-guided tokenization improves overall performance on four downstream tasks. Performance gains are most pronounced for out of domain texts, highlighting our models' improved generalization ability. Our findings demonstrate the utility of linguistic resources to improve language modeling for morphologically complex languages. For low-resource languages that lack large-scale pretraining data, the development and incorporation of linguistic resources can serve as a feasible alternative to improve LM performance.", "AI": {"tldr": "本研究针对拉丁语探讨了形态学导向的分词方法，该方法在多个下游任务中提升了模型性能，尤其在非域内文本上的性能提升明显。这表明在低资源语言建模中，语言学资源的重要性。", "motivation": "标准的分词方法往往更注重信息理论目标，如高度压缩和低生育能力，而不是语言学目标，如形态学对齐，特别是对于形态丰富的语言而言，标准分词方法可能会导致下游任务性能不佳。", "method": "本研究针对拉丁语这种形态丰富的语言，探讨了形态学导向的分词方法。拉丁语在预训练数据方面资源中等，但在精心策划的词汇资源方面资源丰富。研究特别强调低资源语言建模讨论中常被忽略的这种资源分布特点。", "result": "研究结果显示，采用形态学导向的分词方法在四个下游任务中提升了整体性能，尤其是在非域内文本上性能提升最为显著，显示出模型泛化能力的增强。", "conclusion": "研究结果表明，形态学资源可以用来提升形态复杂语言的语言建模性能。对于缺乏大规模预训练数据的低资源语言，开发和整合语言学资源是提升其语言模型性能的一个可行的替代方案。"}}
{"id": "2511.09738", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09738", "abs": "https://arxiv.org/abs/2511.09738", "authors": ["C. LeMay", "A. Lane", "J. Seales", "M. Winstead", "S. Baty"], "title": "Assessing the Applicability of Natural Language Processing to Traditional Social Science Methodology: A Case Study in Identifying Strategic Signaling Patterns in Presidential Directives", "comment": "24 pages", "summary": "Our research investigates how Natural Language Processing (NLP) can be used to extract main topics from a larger corpus of written data, as applied to the case of identifying signaling themes in Presidential Directives (PDs) from the Reagan through Clinton administrations. Analysts and NLP both identified relevant documents, demonstrating the potential utility of NLPs in research involving large written corpuses. However, we also identified discrepancies between NLP and human-labeled results that indicate a need for more research to assess the validity of NLP in this use case. The research was conducted in 2023, and the rapidly evolving landscape of AIML means existing tools have improved and new tools have been developed; this research displays the inherent capabilities of a potentially dated AI tool in emerging social science applications.", "AI": {"tldr": "研究探讨了使用自然语言处理技术从书面数据中提取主要主题，并应用于里根到克林顿时期的总统指令中信号主题的识别，发现NLP和人工标注结果间存在差异，提示需要进一步研究NLP的有效性。研究是在2023年进行的，当前人工智能工具不断演进，现有工具可能已稍显过时。", "motivation": "旨在探索自然语言处理在从大量书面资料中提取关键主题的研究应用，特别是在识别历史文档中的特定主题时的有效性和局限性。", "method": "研究比较了自然语言处理技术和人工分析方法，评估NLP在处理大范围书面数据时的准确性和一致性。", "result": "NLP技术证明在处理大规模书面数据的研究中有潜在的实用性，但与人工标注相比存在显著差异或错误，提示NLP在特定应用场景中的不确定性。", "conclusion": "尽管NLP技术展示了提取关键主题的潜力，但在评估其在特定领域内的有效性和准确性方面仍需进行更多研究。研究也说明了快速发展的AI技术可能使已有工具达到其界限。"}}
{"id": "2511.09599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.09599", "abs": "https://arxiv.org/abs/2511.09599", "authors": ["Ming Yang", "Dongrun Li", "Xin Wang", "Feng Li", "Lisheng Fan", "Chunxiao Wang", "Xiaoming Wu", "Peng Cheng"], "title": "FedeCouple: Fine-Grained Balancing of Global-Generalization and Local-Adaptability in Federated Learning", "comment": null, "summary": "In privacy-preserving mobile network transmission scenarios with heterogeneous client data, personalized federated learning methods that decouple feature extractors and classifiers have demonstrated notable advantages in enhancing learning capability. However, many existing approaches primarily focus on feature space consistency and classification personalization during local training, often neglecting the local adaptability of the extractor and the global generalization of the classifier. This oversight results in insufficient coordination and weak coupling between the components, ultimately degrading the overall model performance. To address this challenge, we propose FedeCouple, a federated learning method that balances global generalization and local adaptability at a fine-grained level. Our approach jointly learns global and local feature representations while employing dynamic knowledge distillation to enhance the generalization of personalized classifiers. We further introduce anchors to refine the feature space; their strict locality and non-transmission inherently preserve privacy and reduce communication overhead. Furthermore, we provide a theoretical analysis proving that FedeCouple converges for nonconvex objectives, with iterates approaching a stationary point as the number of communication rounds increases. Extensive experiments conducted on five image-classification datasets demonstrate that FedeCouple consistently outperforms nine baseline methods in effectiveness, stability, scalability, and security. Notably, in experiments evaluating effectiveness, FedeCouple surpasses the best baseline by a significant margin of 4.3%.", "AI": {"tldr": "The paper presents FedeCouple, a federated learning method designed to improve overall model performance through better coordination between local adaptability and global generalization. The method incorporates dynamic knowledge distillation and anchors for feature space refinement, leading to significant improvements over baseline methods in various metrics.", "motivation": "The motivation for this paper stems from the limitations of existing personalized federated learning methods, which often neglect the local adaptability of feature extractors and the global generalization of classifiers. This imbalance can lead to weak coupling between the components and degrade the overall model's performance.", "method": "The paper introduces FedeCouple, a federated learning method that balances global generalization and local adaptability by jointly learning global and local feature representations. It employs dynamic knowledge distillation to improve classifier generalization and uses anchors to refine feature space, thereby enhancing privacy and reducing communication overhead.", "result": "Experimental validation on five image-classification datasets shows FedeCouple outperforms nine baselines in effectiveness, stability, scalability, and security, achieving a 4.3% performance gain over the best baseline.", "conclusion": "FedeCouple effectively balances global generalization and local adaptability in personalized federated learning, demonstrating superior performance across multiple evaluation criteria compared to existing methods. Theoretical analysis supports its convergence for nonconvex objectives."}}
