<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 94]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
*Rania Al-Sabbagh*

Main category: cs.CL

> Introduces PEACH, a manually aligned English-Arabic healthcare text corpus, useful for language studies and machine translation, publicly available.

<details>
  <summary>Details</summary>

**Motivation:** To provide a gold-standard corpus for researchers in contrastive linguistics, translation studies, and natural language processing, enabling various evaluations and adaptations, such as bilingual lexicon creation and machine translation.

**Method:** Introduces PEACH, a manually aligned parallel English-Arabic healthcare text corpus consisting of patient information and educational materials.

**Result:** The corpus contains 51,671 parallel sentences, totaling approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths vary between 9.52 and 11.83 words on average.

**Conclusion:** PEACH is a valuable resource for deriving bilingual lexicons, adapting language models for healthcare-specific machine translation, assessing patient information readability, and as an educational tool in translation studies, and is publicly accessible.

**Abstract:** This paper introduces PEACH, a sentence-aligned parallel English-Arabic
corpus of healthcare texts encompassing patient information leaflets and
educational materials. The corpus contains 51,671 parallel sentences, totaling
approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths
vary between 9.52 and 11.83 words on average. As a manually aligned corpus,
PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,
translation studies, and natural language processing. It can be used to derive
bilingual lexicons, adapt large language models for domain-specific machine
translation, evaluate user perceptions of machine translation in healthcare,
assess patient information leaflets and educational materials' readability and
lay-friendliness, and as an educational resource in translation studies. PEACH
is publicly accessible.

</details>


### [2] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
*Chi Zhang,Changjia Zhu,Junjie Xiong,Xiaoran Xu,Lingyao Li,Yao Liu,Zhuo Lu*

Main category: cs.CL

> 文章回顾了大型语言模型（LLMs）的毒性问题，提出了一种LLM相关损害与防御的统一分类法，分析了缓解策略，并指出了未来的研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大型语言模型（LLM）在解决实际问题方面的强大功能及其潜在的语言危害，文章旨在探索这一两面性的社会技术挑战，并提出了未来的研究方向，以引导开发稳健且符合伦理的语言技术。

**Method:** 文章通过系统地回顾近期研究，涵盖了无意产生的毒性、对抗性越狱攻击和内容监管技术。作者提出了一个LLM相关的损害和防御的统一分类法，并分析了新兴的多模态和辅助越狱策略，以及评估了包括带有人类反馈的强化学习（RLHF）、提示工程和安全对齐在内的缓解措施。

**Result:** 文章提出了一种统一的分类法，分析了新兴的策略和评估了缓解措施，强调了当前评估方法的局限性。这为理解和解决LLM安全问题提供了框架。

**Conclusion:** 文章突出了LLM安全的不断变化的格局，指出现有评估方法的局限，并概述了未来的研究方向，以指导开发稳健和伦理上一致的语言技术。

**Abstract:** Large Language Models (LLMs) have revolutionized content creation across
digital platforms, offering unprecedented capabilities in natural language
generation and understanding. These models enable beneficial applications such
as content generation, question and answering (Q&A), programming, and code
reasoning. Meanwhile, they also pose serious risks by inadvertently or
intentionally producing toxic, offensive, or biased content. This dual role of
LLMs, both as powerful tools for solving real-world problems and as potential
sources of harmful language, presents a pressing sociotechnical challenge. In
this survey, we systematically review recent studies spanning unintentional
toxicity, adversarial jailbreaking attacks, and content moderation techniques.
We propose a unified taxonomy of LLM-related harms and defenses, analyze
emerging multimodal and LLM-assisted jailbreak strategies, and assess
mitigation efforts, including reinforcement learning with human feedback
(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the
evolving landscape of LLM safety, identifies limitations in current evaluation
methodologies, and outlines future research directions to guide the development
of robust and ethically aligned language technologies.

</details>


### [3] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
*Xiangyan Chen,Yufeng Li,Yujian Gan,Arkaitz Zubiaga,Matthew Purver*

Main category: cs.CL

> A NEW BENCHMARK FOR FINE-GRAINED DIALOGUE FACT VERIFICATION: This paper presents FineDialFact, a benchmark for verifying atomic facts within dialogue responses, aiming to improve the detection of hallucinations in large language models with more sophisticated methods.

<details>
  <summary>Details</summary>

**Motivation:** ADDRESSING HALLUCINATIONS IN DIALOGUE SYSTEMS: The motivation is to tackle the challenge of hallucinations - factually incorrect or fabricated information - produced by large language models, particularly in dialogue systems, where current detection methods are simplistic.

**Method:** STANCE DETECTION AND FINE-GRAINED DIALOGUE FACT VERIFICATION: The paper introduces a new benchmark named FineDialFact for fine-grained dialogue fact verification, which involves verifying atomic facts extracted from dialogue responses. It constructs a dataset based on existing dialogue datasets.

**Result:** METHODS INCORPORATING CHAIN-OF-THOUGHT REASONING IMPROVE PERFORMANCE: Experimental results on HybriDialogue, an open-domain dialogue dataset, show that methods incorporating Chain-of-Thought (CoT) reasoning can enhance performance in dialogue fact verification, but the highest F1-score achieved is only 0.75, indicating the difficulty of the task.

**Conclusion:** FINE-DIALOGUE VERIFICATION REMAINS A CHALLENGE: Although Chain-of-Thought methods improve the performance of dialogue fact verification, the best F1-score achieved is only 0.75, showing that the task is still a significant challenge for future research. The dataset and code will be made publicly available on GitHub.

**Abstract:** Large Language Models (LLMs) are known to produce hallucinations - factually
incorrect or fabricated information - which poses significant challenges for
many Natural Language Processing (NLP) applications, such as dialogue systems.
As a result, detecting hallucinations has become a critical area of research.
Current approaches to hallucination detection in dialogue systems primarily
focus on verifying the factual consistency of generated responses. However,
these responses often contain a mix of accurate, inaccurate or unverifiable
facts, making one factual label overly simplistic and coarse-grained. In this
paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact
verification, which involves verifying atomic facts extracted from dialogue
responses. To support this, we construct a dataset based on publicly available
dialogue datasets and evaluate it using various baseline methods. Experimental
results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning
can enhance performance in dialogue fact verification. Despite this, the best
F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is
only 0.75, indicating that the benchmark remains a challenging task for future
research. Our dataset and code will be public on GitHub.

</details>


### [4] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
*Abishek Thamma,Micha Heilbron*

Main category: cs.CL

> 研究发现记忆的短暂性有助于Transformer语言模型学习语言，但在预测人类阅读时间行为上的表现不如预期。

<details>
  <summary>Details</summary>

**Motivation:** 探索记忆的短暂性是否像传统认知科学模型所支持的那样能促进语言学习，尽管像Transformer这样的现代模型能够有效学习语言，即便没有记忆限制或其他架构上的时间偏差。

**Method:** 通过在具有和不具有短暂记忆的Transformer语言模型上进行严格控制的实验来研究记忆的短暂性对语言学习的假设益处。

**Result:** 实验结果表明记忆的短暂性在语言学习中有持续的优点，提高了整体语言建模性能和目标句法评估性能，但意外地减弱了对人类阅读时间的预测能力。

**Conclusion:** 这些结果支持神经网络语言学习中记忆限制的益处，但对于预测行为并没有帮助。

**Abstract:** Human memory is fleeting. As words are processed, the exact wordforms that
make up incoming sentences are rapidly lost. Cognitive scientists have long
believed that this limitation of memory may, paradoxically, help in learning
language - an idea supported by classic connectionist modelling work. The rise
of Transformers appears to challenge this idea, as these models can learn
language effectively, despite lacking memory limitations or other architectural
recency biases. Here, we investigate the hypothesized benefit of fleeting
memory for language learning in tightly controlled experiments on transformer
language models. Training transformers with and without fleeting memory on a
developmentally realistic training set, we find that fleeting memory
consistently improves language learning (as quantified by both overall language
modelling performance and targeted syntactic evaluation) but, unexpectedly,
impairs surprisal-based prediction of human reading times. Interestingly,
follow up analyses revealed that this discrepancy - better language modeling,
yet worse reading time prediction - could not be accounted for by prior
explanations of why better language models sometimes fit human reading time
worse. Together, these results support a benefit of memory limitations on
neural network language learning - but not on predicting behavior.

</details>


### [5] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
*Tong Li,Rasiq Hussain,Mehak Gupta,Joshua R. Oltmanns*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** A growing number of studies show near-perfect LLM language-based prediction
of depression assessment scores (up to R2 of .70). However, many develop these
models directly from language responses to depression assessments. These
"Mirror models" suffer from "criterion contamination", which arises when a
predicted score depends in part on the predictors themselves. This causes
artificial effect size inflation which reduces model generalizability. The
present study compares the performance of Mirror models versus "Non-Mirror
models", which are developed from language that does not mirror the assessment
they are developed to predict. N = 110 research participants completed two
different interviews: structured diagnostic and life history interviews. GPT-4,
GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic
interview depression scores from the two transcripts separately. Mirror models
(using structured diagnostic data) showed very large effect sizes (e.g., R2 =
.80). As expected, NonMirror models (using life history data) demonstrated
smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror
and Non-Mirror model-predicted structured interview depression scores were
correlated with self-reported depression symptoms, Mirror and NonMirror
performed the same (e.g., r = ~.54), indicating that Mirror models contain bias
perhaps due to criterion contamination. Topic modeling identified clusters
across Mirror and Non-Mirror models, as well as between true-positive and
false-positive predictions. In this head-to-head comparison study, Mirror
language AI models of depression showed artificially inflated effect sizes and
less generalizability. As language AI models for depression continue to evolve,
incorporating Non-Mirror models may identify interpretable, and generalizable
semantic features that have unique utility in real-world psychological
assessment.

</details>


### [6] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
*Miles Gilberti,Shane Storks,Huteng Dai*

Main category: cs.CL

> 本文对新兴通信中属性-值重构游戏施加小词汇量限制，模拟自然语言的形态特性，发现音系限制会引导句法组合，并且产生的语言有语法属性融合的趋势。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在重新解释Emergent communication（EmCom）中的一个常见设置，通过施加小词汇量限制以及模拟自然语言中的词形变化机制，深入探讨在特定约束下Agent之间通信方式的新形态，并将之与自然语言中的交际机制进行比较。

**Method:** 我们通过重新解释常见的EmCom设置，即属性-值重构游戏，来施加一个小词汇量限制，从而模拟双重构词，并形成与自然衍生形态学类比的新设置。这一新设置有助于与自然语言交流方案进行有意义的比较。我们开发了新的度量，并探索了由实际的衍生形态学属性所激发的游戏变体，包括句法组合性和融合性。

**Result:** 实验发现，模拟的音系限制鼓励了句法组合形态，而产生的语言也呈现了自然语言将语法属性融合的趋势。

**Conclusion:** 该研究展示了在特定约束条件下，通过模拟自然语言的特性和使用新度量方法可以带来对新兴通信模式的新见解，特别是其如何反映自然语言中语法属性的融合趋势。

**Abstract:** Emergent communication (EmCom) with deep neural network-based agents promises
to yield insights into the nature of human language, but remains focused
primarily on a few subfield-specific goals and metrics that prioritize
communication schemes which represent attributes with unique characters
one-to-one and compose them syntactically. We thus reinterpret a common EmCom
setting, the attribute-value reconstruction game, by imposing a
small-vocabulary constraint to simulate double articulation, and formulating a
novel setting analogous to naturalistic inflectional morphology (enabling
meaningful comparison to natural language communication schemes). We develop
new metrics and explore variations of this game motivated by real properties of
inflectional morphology: concatenativity and fusionality. Through our
experiments, we discover that simulated phonological constraints encourage
concatenative morphology, and emergent languages replicate the tendency of
natural languages to fuse grammatical attributes.

</details>


### [7] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

> 本文通过认知维度调查大型语言模型(LLMs)对情绪的推理能力，并引入了一个名为CoRE的大规模基准测试，以评估LLMs在情绪推理中隐含使用的认知结构。

<details>
  <summary>Details</summary>

**Motivation:** 大多数关于情绪预测或生成的研究采用监督方法进行情绪相关的任务，该论文意图超越表面情绪任务，通过认知维度来研究LMMs对情绪的推理能力。

**Method:** 基于认知评价理论，通过评估实验分析LLMs在处理情绪相关刺激时产生的认知推理是否一致和可信。引入了一个名为CoRE的大规模基准测试。

**Result:** 研究揭示了不同LLMs在情绪推理中表现出多样化的推理模式。

**Conclusion:** 通过大量的评价实验和分析，论文探讨了模型是否更倾向于依赖特定的认知评价维度进行情绪推理以及这些认知维度对于描述不同情绪的重要性，并尝试解释不同情绪类别在LMMs中的内部表示。

**Abstract:** Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [8] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
*Zhanghao Hu,Qinglin Zhu,Siya Qi,Yulan He,Hanqi Yan,Lin Gui*

Main category: cs.CL

> 研究介绍了SPS和xCompress，以改进大型语言模型使用检索增强生成的效果，并通过大量实验展示了它们在多个问答基准和开放源代码大语言模型中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 先前的工作通常通过评估检索器和阅读器的综合效果来评估RAG，这使得很难孤立地评估检索的真正贡献，特别是考虑到作为阅读器的大型语言模型对提示的敏感性。

**Method:** 我们介绍了Spectrum Projection Score (SPS)，这是一种轻量级且无需监督的度量方法，允许读取器通过比较摘要生成的标记形成的区域与读者子空间的主要方向来评估检索摘要的语义一致性。基于SPS，我们提出了xCompress，这是一个可以在推理时动态采样、排序和压缩检索摘要候选的框架。

**Result:** 广泛实验表明，在五个问答基准和四种开源大语言模型上的应用显示，SPS不仅在一系列任务中提升了性能，而且还提供了一个有关检索和生成之间交互的结构化视角。

**Conclusion:** SPS和xCompress框架能够提升基于检索增强生成的大型语言模型在各种任务中的表现，并提供了一个系统化的方法来理解检索和生成之间的相互作用。

**Abstract:** Large Language Models (LLMs) have shown improved generation performance
through retrieval-augmented generation (RAG) following the retriever-reader
paradigm, which supplements model inputs with externally retrieved knowledge.
However, prior work often evaluates RAG holistically, assessing the retriever
and reader jointly, making it difficult to isolate the true contribution of
retrieval, particularly given the prompt sensitivity of LLMs used as readers.
We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free
metric that allows the reader to gauge the semantic alignment of a retrieved
summary with its hidden representation by comparing the area formed by
generated tokens from the summary, and the principal directions of subspace in
the reader and to measure the relevance. Building on SPS we present xCompress,
an inference time controller framework that dynamically samples, ranks, and
compresses retrieval summary candidates. Extensive experiments on five QA
benchmarks with four open source LLMs show that SPS not only enhances
performance across a range of tasks but also provides a principled perspective
on the interaction between retrieval and generation.

</details>


### [9] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

> 本文提出了一种三阶段流水线，设计优化了亲社会内容分类，实现高精度分类和成本节省的一种AI解决方案。方法包括最佳标注策略确定、人工-人工智能相互调整以及合成大规模标签进行系统训练。

<details>
  <summary>Details</summary>

**Motivation:** 检测亲社会内容--旨在肯定、支持或改善他人行为的交流--是针对信任和安全系统的新兴且日益重要的挑战。这种方法相比于有毒内容检测，缺乏明确的定义和标注数据，因此需要新的注释和部署方法。

**Method:** 本文提出了一种实用的、三阶段的流水线设计，以实现大规模的高精度亲社会内容分类，同时最小化人工标注和推理成本：1. 使用少量的人工标注实例作为种子集，以确定最佳的基于LLM的标注策略；2. 采用人工-人工智能调整循环，在高分歧的案例中进行人工审核，以逐步细化任务定义；3. 合成10000个高质量标签，使用GPT-4训练了一个两阶段推理系统，其中轻量级分类器处理高置信度预测，只有35%的模棱两可的案例提交给GPT-4处理。

**Result:** 该流水线减少推理成本约70%，并实现了高精度约0.90，证明了这种方法在减少成本和提高亲社会内容检测效率方面的有效性。

**Conclusion:** 本文展示了针对亲社会内容检测这一新兴任务，有针对性的人工智能互动、精心的任务制作设计和部署友好架构如何达成可扩展的解决方案。

**Abstract:** Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [10] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
*Chunyun Zhang,Hongyan Zhao,Chaoran Cui,Qilong Song,Zhiqing Lu,Shuai Gong,Kailin Liu*

Main category: cs.CL

> 提出一种新型方法ATOP，通过优化主题感知提示来提高跨主题自动作文评分的性能。实验表明，该方法在ASAP++数据集上表现优异，超越了现有最先进方法。

<details>
  <summary>Details</summary>

**Motivation:** 面对跨主题自动作文评分中的主题差异挑战，大多数现有方法通过源主题和目标主题的分布对齐来提取共享特征，却忽视了特定主题特征，这限制了它们评估诸如主题一致性等关键特征的能力。为了解决这一限制，提出了ATOP方法。

**Method:** ATOP方法通过优化可学习的主题感知提示，该提示包括共享和特定主题的组成部分，以从预训练语言模型中提取相关知识，从而解决现有方法忽视特定主题特征的问题。此外，为了提升共享提示学习的鲁棒性并减少因主题对齐引起的特征尺度敏感性，该方法引入对抗训练，并采用基于邻居的分类器生成伪标签来指导目标主题的特定提示监督学习。

**Result:** ATOP方法在公共可用的ASAP++数据集上的广泛实验表明，该方法在整体和多特征文章评分中大幅超越现有最先进方法。

**Conclusion:** 实验结果证明了ATOP在跨主题自动作文评分中的有效性，展示了其比现有方法更优越的性能，特别是在整体和多特征作文评分方面的表现。

**Abstract:** Cross-topic automated essay scoring (AES) aims to develop a transferable
model capable of effectively evaluating essays on a target topic. A significant
challenge in this domain arises from the inherent discrepancies between topics.
While existing methods predominantly focus on extracting topic-shared features
through distribution alignment of source and target topics, they often neglect
topic-specific features, limiting their ability to assess critical traits such
as topic adherence. To address this limitation, we propose an Adversarial
TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns
topic-shared and topic-specific features to improve cross-topic AES. ATOP
achieves this by optimizing a learnable topic-aware prompt--comprising both
shared and specific components--to elicit relevant knowledge from pre-trained
language models (PLMs). To enhance the robustness of topic-shared prompt
learning and mitigate feature scale sensitivity introduced by topic alignment,
we incorporate adversarial training within a unified regression and
classification framework. In addition, we employ a neighbor-based classifier to
model the local structure of essay representations and generate pseudo-labels
for target-topic essays. These pseudo-labels are then used to guide the
supervised learning of topic-specific prompts tailored to the target topic.
Extensive experiments on the publicly available ASAP++ dataset demonstrate that
ATOP significantly outperforms existing state-of-the-art methods in both
holistic and multi-trait essay scoring. The implementation of our method is
publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [11] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

> 研究发现，结构化后处理注意力稀疏性不仅提高了计算效率，还在SST-2情绪分析任务中提升了模型的准确性，挑战了普遍认为稀疏化会降低模型准确性的观点。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在探索一种新的方法，以解决自注意力机制带来的二次复杂度计算问题，并且挑战稀疏性必须以牺牲模型准确性为代价的普遍假设。

**Method:** 本研究通过在DistilBERT模型中引入结构化后处理稀疏性，尤其是在SST-2情绪分析任务的微调过程中，验证了稀疏性对模型准确性的提升作用。

**Result:** 实验结果显示，随着80%注意力稀疏性的加入，模型在验证数据集上的准确率达到了91.59%，相比于密集基线模型，绝对准确率提高了0.97%。

**Conclusion:** 研究认为稀疏性可以作为一种强大的隐式正则化器，通过限制模型使用更受约束且稳健的一组特征进行预测，从而防止过拟合，同时将注意力稀疏性重新定位为一种不仅有助于计算效率提升，而且能改善Transformer模型泛化性能和表现的方法。

**Abstract:** The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [12] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

> Introduces Temporal Self-Rewarding Language Models to address limitations in self-rewarding architectures, demonstrating significant performance improvements and better generalization capabilities.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation in existing Self-Rewarding paradigms where the synchronization of improvement undermines effective preference learning.

**Method:** Temporal Self-Rewarding Language Models that strategically coordinate past, present, and future model generations to sustain learning signals through two phases: Anchored Rejection and Future-Guided Chosen.

**Result:** Significant improvements demonstrated in win rates and out-of-distribution generalization across various tasks compared to Self-Rewarding baselines.

**Conclusion:** The proposed method enhances LLM generative capabilities, showcasing superior performance and generalization without requiring task-specific training data.

**Abstract:** Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [13] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

> 文章提出了一种方法（PEEK），利用预先训练的嵌入模型作为代理，以更高效的方式估计大型语言模型（LLMs）的知识，可以准确率高达90%，并指出句子嵌入模型更适用于此任务。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有的探查LLMs知识的方法计算成本高且耗时，本文旨在提出一种更高效的方法来估计LLMs的知识。

**Method:** 提出了一种名为PEEK的方法，通过预先训练的嵌入模型来估计大型语言模型（LLMs）的知识。该方法包括识别LLMs已知的事实以及适应嵌入模型以预测LLMs的输出。

**Result:** 在3个维基百科衍生数据集上、4个LLMs和7个嵌入模型的综合评估证明了嵌入模型可以预测LLMs知识，准确率高达90%，并发现句子嵌入模型比图嵌入模型更适合预测LLMs的知识。

**Conclusion:** 实验表明，嵌入模型可以以高达90%的准确率预测LLMs的知识。此外，结果还表明，句子嵌入模型比图嵌入模型更适合预测LLMs的知识。因此，这些嵌入模型可以用作大规模识别LLMs知识差距的工具，并提供关于LLMs内在归纳偏好的更深入洞见。

**Abstract:** Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [14] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

> This paper addresses the challenge of accurate story evaluation by proposing EvolvR, a self-evolving adaptive framework, which demonstrates superior performance and enhances story generation quality.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing methods in story evaluation using LLMs, which either suffer from poor adaptability or lack rigorous reasoning capabilities.

**Method:** Self-Evolving Pairwise Reasoning (EvolvR) framework, which involves self-synthesizing score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy, followed by a self-filtering process using multi-agents, and then deploying the refined evaluator as a reward model.

**Result:** The proposed framework achieves state-of-the-art performance on evaluation benchmarks including StoryER, HANNA, and OpenMEVA, and significantly enhances the quality of generated stories when used as a reward model.

**Conclusion:** The self-evolving approach proposed in this paper, EvolvR, proves to be effective in story evaluation and significantly improves the quality of generated stories when used as a reward model.

**Abstract:** Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [15] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
*Morris Alper,Moran Yanuka,Raja Giryes,Gašper Beguš*

Main category: cs.CL

> This paper presents ConlangCrafter, a system using large language models to create constructed languages in a modular and consistent manner, highlighting the potential of AI in computational creativity.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to explore the use of LLMs for computational creativity, specifically in the creation of constructed languages (conlangs) without requiring human linguistic expertise.

**Method:** The paper introduces ConlangCrafter, a pipeline for creating constructed languages using modern large language models (LLMs). This multi-hop process breaks down language design into stages such as phonology, morphology, syntax, and lexicon generation. The method incorporates randomness for diversity and a feedback loop to ensure consistency.

**Result:** ConlangCrafter was evaluated based on metrics related to coherence and typological diversity, showing that it can produce conlangs that are both coherent and varied.

**Conclusion:** The paper concludes that modern LLMs can be effectively used as tools for the automated creation of new languages, demonstrating the potential of computational creativity in language design.

**Abstract:** Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international communication. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern LLMs as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages LLMs' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.

</details>


### [16] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

> The paper introduces two methods for improving extractive QA on the Quran using instruction-tuned large language models, achieving a pAP10 score of 0.637.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to tackle the challenges of complex language, unique terminology, and deep meaning in the Quranic text for extractive QA.

**Method:** The paper employs a specialized Arabic prompt framework for span extraction, and a robust post-processing system for subword alignment, overlap suppression, and semantic filtering.

**Result:** The evaluation shows that models with Arabic instructions perform better than fine-tuned models, with the highest pAP10 score of 0.637.

**Conclusion:** The conclusion is that prompt-based instruction tuning can effectively enhance performance in extractive QA tasks for low-resource, semantically rich texts like the Quran.

**Abstract:** This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [17] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
*Shengyuan Chen,Chuang Zhou,Zheng Yuan,Qinggang Zhang,Zeyang Cui,Hao Chen,Yilin Xiao,Jiannong Cao,Xiao Huang*

Main category: cs.CL

> 提出了一种名为LogicRAG的框架，通过动态构建推理结构，改进了现有基于图的检索增强生成方法的效率和效果。

<details>
  <summary>Details</summary>

**Motivation:** 针对现有基于图的检索增强生成（GraphRAG）方法将语料库转换成图的过程成本高昂、引入了沉重的Token成本和更新延迟的问题，提出了新的LogicRAG框架。通过动态建模逻辑依赖关系，而不是依赖于预先构建的图，该框架旨在提高知识检索的有效性和效率。

**Method:** 提出了一种名为LogicRAG的框架，该框架在推理时动态提取推理结构，指导自适应检索，无需预先构建图。LogicRAG首先将输入查询分解成一组子问题，并构建有向无环图（DAG）来建模它们之间的逻辑依赖关系。为了支持连贯的多步推理，LogicRAG接着使用拓扑排序线性化图，使得子问题可以在逻辑一致的顺序中得到解决。此外，LogicRAG通过图修剪减少冗余检索，并通过上下文修剪过滤无关上下文，显著减少总体Token成本。

**Result:** 通过广泛实验表明，LogicRAG在性能和效率方面超过了现有的先进基线。

**Conclusion:** LogicRAG框架通过动态结构提取和自适应检索提供了优于现有GraphRAG方法的性能和效率。

**Abstract:** Large language models (LLMs) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support LLM
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph pruning to reduce redundant retrieval and uses context pruning to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.

</details>


### [18] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
*Sayantan Adak,Pratyush Chatterjee,Somnath Banerjee,Rima Hazra,Somak Aditya,Animesh Mukherjee*

Main category: cs.CL

> 研究提出AURA框架，专注于改善大型语言模型在安全性和逻辑连贯性方面的表现，通过多层设计实现了更高级别的安全保障。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有的大模型（LLMs）在管理安全风险方面面临挑战，尤其是由于未被发现的逻辑影响而导致输出无意中促成有害行为的风险。传统安全解决方案（如标量结果奖励模型、参数调整或启发式解码策略）在检测和干预微妙但关键推理步骤时缺乏细节性和前瞻性。

**Method:** 介绍了一种名为AURA的多层框架，该框架围绕过程奖励模型（PRMs）构建，能够对逻辑连贯性和安全性进行细致的逐级评估。该框架将内省式自我批判、精细的PRM评估以及适应性的安全意识解码无缝结合，旨在动态地、前瞻性地引导模型走向更安全的推理路径。

**Result:** 实验证明，该方法在改善模型输出的逻辑完整性和对行为敏感度的安全性方面显著超越现有方法。

**Conclusion:** 这项研究代表了向更安全、更负责任和更有上下文意识的人工智能迈进的关键一步，为对齐敏感应用设定了新标准。

**Abstract:** Present day LLMs face the challenge of managing affordance-based safety
risks-situations where outputs inadvertently facilitate harmful actions due to
overlooked logical implications. Traditional safety solutions, such as scalar
outcome-based reward models, parameter tuning, or heuristic decoding
strategies, lack the granularity and proactive nature needed to reliably detect
and intervene during subtle yet crucial reasoning steps. Addressing this
fundamental gap, we introduce AURA, an innovative, multi-layered framework
centered around Process Reward Models (PRMs), providing comprehensive, step
level evaluations across logical coherence and safety-awareness. Our framework
seamlessly combines introspective self-critique, fine-grained PRM assessments,
and adaptive safety-aware decoding to dynamically and proactively guide models
toward safer reasoning trajectories. Empirical evidence clearly demonstrates
that this approach significantly surpasses existing methods, significantly
improving the logical integrity and affordance-sensitive safety of model
outputs. This research represents a pivotal step toward safer, more
responsible, and contextually aware AI, setting a new benchmark for
alignment-sensitive applications.

</details>


### [19] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

> SRD 是一种通过学生模型反馈来优化训练数据的方法，从而提高知识蒸馏的效果和效率。实验表明，SRD 能在多种蒸馏方法和模型架构上提高性能，并显著减少训练时间。

<details>
  <summary>Details</summary>

**Motivation:** 现有的白盒知识蒸馏方法主要集中在平衡真实数据和学生生成数据，但忽略了训练数据质量和学生模型兼容性的关键因素。SRD旨在解决这些问题。

**Method:** Structure

**Result:** {
  "tldr": "SRD 提出了一种通过学生模型反馈来优化训练数据的方法，从而提高知识蒸馏的效果和效率。实验表明，SRD 能在多种蒸馏方法和模型架构上提高性能，并显著减少训练时间。",
  "motivation": "现有的白盒知识蒸馏方法主要集中在平衡真实数据和学生生成数据，但忽略了训练数据质量和学生模型兼容性的关键因素。SRD旨在解决这些问题。",
  "method": "SRD 采用反射机制，评估并选择优质训练数据，并通过课程调度策略逐步引入这些数据进行蒸馏。",
  "result": "实验结果表明，SRD 能提高蒸馏模型的性能并减少训练时间可达39%，可以在不同蒸馏方法和模型上作为插件使用。",
  "conclusion": "SRD 是一种高效的数据优化框架，能够提高语言模型蒸馏的质量和效率，无需修改原始知识蒸馏算法。研究结果表明数据质量与兼容性对于有效蒸馏至关重要。"}
}


**Conclusion:** SRD 是一种高效的数据优化框架，能够提高语言模型蒸馏的质量和效率，无需修改原始知识蒸馏算法。研究结果表明数据质量与兼容性对于有效蒸馏至关重要。

**Abstract:** Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [20] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
*Gunhee Cho,Yun-Gyung Cheong*

Main category: cs.CL

> Big5-Scaler框架通过自然语言提示实现了大型语言模型的性格控制，无需额外训练，展现了在性格表达、对话生成和人类性格模仿方面的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在提供一种能够有效地让大型语言模型具备可控制性格特质的方法，以提高对话系统的多样性和自然性。

**Method:** 通过将数值性格特质嵌入到自然语言提示中，该研究提出了无需额外训练即可实现对大型语言模型的精细性格控制的框架。

**Result:** 该框架通过将数值性格特质嵌入到自然语言提示中，实现了在不进行额外训练的情况下对大型语言模型进行精细的性格控制。评估表明，它能够在不同模型中诱导出一致且可区分的性格特质，且表现因提示类型和尺度而异。研究表明，简洁的提示和较低的性格强度提供了构建有性格意识的对话代理的高效方法。

**Conclusion:** Big5-Scaler框架为构建性格化的对话代理提供了一种高效且有效的方法，特别是在使用简洁的提示和较低的性格强度时。

**Abstract:** We present Big5-Scaler, a prompt-based framework for conditioning large
language models (LLMs) with controllable Big Five personality traits. By
embedding numeric trait values into natural language prompts, our method
enables fine-grained personality control without additional training. We
evaluate Big5-Scaler across trait expression, dialogue generation, and human
trait imitation tasks. Results show that it induces consistent and
distinguishable personality traits across models, with performance varying by
prompt type and scale. Our analysis highlights the effectiveness of concise
prompts and lower trait intensities, providing a efficient approach for
building personality-aware dialogue agents.

</details>


### [21] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
*Renhan Zhang,Lian Lian,Zhen Qi,Guiran Liu*

Main category: cs.CL

> This paper introduces an interpretable bias detection method for identifying hidden social biases in large language model outputs. It uses nested semantic representation and contextual contrast to analyze model sensitivity towards social attributes, demonstrating strong detection performance validated with the StereoSet dataset.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the issue of implicit stereotypes that may arise during the generation process of large language models by proposing an interpretable bias detection method.

**Method:** The method combines nested semantic representation with a contextual contrast mechanism to extract latent bias features from the vector space structure of model outputs. Using attention weight perturbation, it analyzes the model's sensitivity to specific social attribute terms, thereby revealing the semantic pathways through which bias is formed.

**Result:** Experimental results show that the proposed method achieves strong detection performance across various dimensions. It can accurately identify bias differences between semantically similar texts while maintaining high semantic alignment and output stability.

**Conclusion:** The method provides a transparent and reliable technical foundation for bias detection, suitable for real-world applications where high trustworthiness of generated content is required.

**Abstract:** This paper addresses the issue of implicit stereotypes that may arise during
the generation process of large language models. It proposes an interpretable
bias detection method aimed at identifying hidden social biases in model
outputs, especially those semantic tendencies that are not easily captured
through explicit linguistic features. The method combines nested semantic
representation with a contextual contrast mechanism. It extracts latent bias
features from the vector space structure of model outputs. Using attention
weight perturbation, it analyzes the model's sensitivity to specific social
attribute terms, thereby revealing the semantic pathways through which bias is
formed. To validate the effectiveness of the method, this study uses the
StereoSet dataset, which covers multiple stereotype dimensions including
gender, profession, religion, and race. The evaluation focuses on several key
metrics, such as bias detection accuracy, semantic consistency, and contextual
sensitivity. Experimental results show that the proposed method achieves strong
detection performance across various dimensions. It can accurately identify
bias differences between semantically similar texts while maintaining high
semantic alignment and output stability. The method also demonstrates high
interpretability in its structural design. It helps uncover the internal bias
association mechanisms within language models. This provides a more transparent
and reliable technical foundation for bias detection. The approach is suitable
for real-world applications where high trustworthiness of generated content is
required.

</details>


### [22] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

> TADrop, a tensor-wise adaptive sparsification method, is proposed to improve multi-task learning by addressing the limitations of uniform sparsity ratios in model merging.

<details>
  <summary>Details</summary>

**Motivation:** To overcome suboptimal parameter pruning caused by a "one-size-fits-all" sparsity strategy, which often results in retaining less useful parameters and pruning crucial ones.

**Method:** TADrop assigns customized sparsity levels to each parameter tensor based on the tensors' distributional properties, allowing for more effective parameter pruning.

**Result:** Experiments on various tasks and models show consistent and significant performance improvement when TADrop is applied, for example, an average gain of 2.0% across 8 ViT-B/32 tasks.

**Conclusion:** TADrop boosts model performance by tailoring sparsification to the structural properties of the model, setting a new baseline for effective model merging in multi-task learning.

**Abstract:** Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [23] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

> UR2 framework unifies RAG and RLVR methods through advanced training techniques and hybrid knowledge access, achieving superior performance across diverse tasks.

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of isolated development and limited scope of Retrieval-Augmented Generation (RAG) and Reinforcement Learning from Verifiable Rewards (RLVR) methods, aiming to enhance generalization and broaden applicability.

**Method:** Unified RAG and Reasoning (UR2) framework, incorporating difficulty-aware curriculum training and a hybrid knowledge access strategy that integrates domain-specific offline corpora with LLM-generated summaries.

**Result:** UR2 demonstrates significant performance improvements over existing RAG and RL methods across a variety of tasks and achieves comparable results to GPT-4o-mini and GPT-4.1-mini.

**Conclusion:** UR2 effectively integrates retrieval and reasoning under a unified framework, showcasing superior adaptability and performance across multiple domains and tasks.

**Abstract:** Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [24] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*Vít Gvoždiak*

Main category: cs.CL

> 文章重新定义了语用学，认为其应该作为一种动态接口，通过这种接口语言可以作为嵌入社会中的行动工具。文章强调了传统语用学理论在面对大语言模型时的局限性，并提出了一些新的理论框架和概念来适应这一变化。

<details>
  <summary>Details</summary>

**Motivation:** 随着大语言模型在交流领域的出现，传统的语义三元分类法需要进一步调整和方法上的重新考虑。文章认为，传统的基于Grice的语用学在解释类似于LLM的预测系统时，存在人类特有的假设，这些假设并不适合。

**Method:** 文章提出了一个新的框架——人机沟通（HMC）框架，并探讨了人类中心的语用理论和以机器为中心的大语言模型（LLMs）之间的紧张关系。此外，文章分析了三种形式的替代主义，并介绍了'语境挫折'的概念，以更加全面地解释涉及生成AI的交流理论。

**Result:** 文章揭示了传统的语用理论在面对大语言模型时的不适用性，并且引入了其他更兼容的方法来解释人与机器之间的交流，同时指出了未来研究的方向。

**Conclusion:** 文章的结论是，传统的语用学理论可能需要调整或扩展，以便更好地解释包含生成AI的交流。提出了一个新的概念“语境挫折”来描述语境输入量增加与语境理解下降之间的悖论。

**Abstract:** The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [25] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
*Hugo Abonizio,Thales Almeida,Roberto Lotufo,Rodrigo Nogueira*

Main category: cs.CL

> The paper explores methods to inject small, unstructured information into large language models (LLMs), focusing on the challenge of learning new facts and retaining existing knowledge. Experiments show that diverse textual variations aid in learning new facts without significant loss of older knowledge.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to explore effective methods to update large language models with only a few thousand or million tokens, which remains a challenge, and to examine the balance between learning new content and retaining existing capabilities.

**Method:** We investigate the task of injecting a small amount of unstructured information into large language models and its relation to the catastrophic forgetting phenomenon. We experimented with various augmentation algorithms to generate synthetic data to improve knowledge acquisition.

**Result:** Experiments show that continuing pre-training on limited data yields modest improvements, while diverse textual variations, particularly those that induce greater variability, significantly improve the learning of new facts. RAG-based approaches were found to be sensitive and cause more degradation on control datasets compared to parametric methods.

**Conclusion:** The research concludes that large language models can generate effective synthetic training data themselves, offering a path toward self-improving model updates. The findings contribute to more efficient knowledge injection in LLMs with limited data.

**Abstract:** Large language models (LLMs) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an LLM with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into LLMs and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in LLMs with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [26] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
*Ali Sarabadani,Maryam Abdollahi Shamami,Hamidreza Sadeghsalehi,Borhan Asadi,Saba Hesaraki*

Main category: cs.CL

> 本文介绍了一种集成动态知识图与大型语言模型的框架（DKG-LLM），通过高级算法自动处理医疗数据，以提高诊断与个性化治疗推荐的准确性，评估结果显示该框架在临床实践中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 此框架旨在通过先进的自适应语义融合算法（ASFA）从异构医疗数据中提取语义信息，动态产生包含多种类型节点和关系的知识图谱，从而提高医疗诊断和治疗的准确性。

**Method:** 本文提出了DKG-LLM框架，该框架通过集成动态知识图（DKG）与Grok 3大型语言模型，以期实现医疗诊断和个性化治疗建议的新方法。

**Result:** 现实世界的数据集（如MIMIC-III和PubMed）评估显示，DKG-LLM在诊断准确率上达到了84.19%，治疗建议准确率为89.63%，语义覆盖率为93.48%。

**Conclusion:** DKG-LLM是一个可靠且具有变革性的工具，能够处理嘈杂的数据和复杂的多症状疾病，并支持基于医生反馈的学习。

**Abstract:** Large Language Models (LLMs) have grown exponentially since the release of
ChatGPT. These models have gained attention due to their robust performance on
various tasks, including language processing tasks. These models achieve
understanding and comprehension of tasks by training billions of parameters.
The development of these models is a transformative force in enhancing natural
language understanding and has taken a significant step towards artificial
general intelligence (AGI). In this study, we aim to present the DKG-LLM
framework. The DKG-LLM framework introduces a groundbreaking approach to
medical diagnosis and personalized treatment recommendations by integrating a
dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the
Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data
(including clinical reports and PubMed articles) and patient records
dynamically generate a knowledge graph consisting of 15,964 nodes in 13
distinct types (e.g., diseases, symptoms, treatments, patient profiles) and
127,392 edges in 26 relationship types (e.g., causal, therapeutic,
association). ASFA utilizes advanced probabilistic models, Bayesian inference,
and graph optimization to extract semantic information, dynamically updating
the graph with approximately 150 new nodes and edges in each data category
while maintaining scalability with up to 987,654 edges. Real-world datasets,
including MIMIC-III and PubMed, were utilized to evaluate the proposed
architecture. The evaluation results show that DKG-LLM achieves a diagnostic
accuracy of 84.19%. The model also has a treatment recommendation accuracy of
89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and
transformative tool that handles noisy data and complex multi-symptom diseases,
along with feedback-based learning from physician input.

</details>


### [27] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
*Lai Jiang,Yuekang Li,Xiaohan Zhang,Youtao Ding,Li Pan*

Main category: cs.CL

> 提出了一个名为SceneJailEval的场景适应型越狱评估框架，克服了现有方法的“一刀切”问题，并在多个数据集上取得了优于现有方法的结果。

<details>
  <summary>Details</summary>

**Motivation:** 精确的越狱评估对于LLM红队行动和越狱研究至关重要。当前方法仅提供二元分类结果，且多维框架在应用统一评价标准时存在场景特异性错配问题，影响评价精度。

**Method:** 介绍了一种突破性的场景自适应多维框架SceneJailEval，克服了现有方法‘一刀切’的限制，并具有强大的扩展性，能够灵活适应定制或新兴场景。

**Result:** SceneJailEval在全场景数据集上达到了最新的F1分数0.917（比之前的最佳方法高出6%）和JBB上的0.995（比之前的最佳方法高出3%），超越了现有评估方法在异构场景中的准确性限制。

**Conclusion:** SceneJailEval通过提供一个场景适应型多维框架，在提高了评估精度的同时，成功弥补了一个高质量、全面基准的长期空白。

**Abstract:** Precise jailbreak evaluation is vital for LLM red teaming and jailbreak
research. Current approaches employ binary classification ( e.g., string
matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no"
labels without quantifying harm intensity. Existing multi-dimensional
frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)
apply uniform evaluation criteria across scenarios, resulting in
scenario-specific mismatches--for instance, "Relative Truthfulness" is
irrelevant to "hate speech"--which compromise evaluation precision. To tackle
these limitations, we introduce SceneJailEval, with key contributions: (1) A
groundbreaking scenario-adaptive multi-dimensional framework for jailbreak
evaluation, overcoming the critical "one-size-fits-all" constraint of existing
multi-dimensional methods, and featuring strong extensibility to flexibly adapt
to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset
with diverse jailbreak variants and regional cases, filling the long-standing
gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)
SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on
our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over
prior SOTA), surpassing accuracy limits of existing evaluation methods in
heterogeneous scenarios and confirming its advantage.

</details>


### [28] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

> 本文提出了一个情感智能(EI)的四层分类框架，并开发了一个新的基准测试EICAP-Bench，用于评估大型语言模型的情感智能。基于UltraChat数据集的微调结果显示只有评价层有显著提升，突出了现有方法的局限性和改进数据与模型策略的需求。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在通过引入一个新框架和测试基准来填补大型语言模型情感智能维度上的研究空白。

**Method:** 本文提出了一个以心理学为基础的四层情感智能(EI)分类框架，适用于大型语言模型，涵盖情绪跟踪、原因推断、评价和生成情感上合适的响应。基于这个框架，开发了一个新的多回合多项选择题基准测试EICAP-Bench，用于评估不同语言和文化背景下开源大型语言模型的情感智能能力。同时还使用UltraChat数据集对Qwen2.5模型进行了微调。

**Result:** 在EICAP-Bench基准测试中，发现Qwen2.5-Instruct模型的表现最好。统计分析表明，情感智能的五个层级中，只有评价层在基于UltraChat的数据集上进行微调后有显著的提升。

**Conclusion:** 研究结果揭示了现有预训练和指令调优范式在赋予大型语言模型更深层次的情感推理方面的局限性，并强调了需要有针对性的数据和建模策略来实现全面的情感智能对齐。

**Abstract:** Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [29] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

> 本文提出了一种使用检索增强生成（RAG）方法的分类系统，该系统在不重新训练模型的情况下能够根据不同政策进行调整，并展示了其灵活性和准确性的优势。

<details>
  <summary>Details</summary>

**Motivation:** 该方法旨在让分类系统能够快速适应不断变化的政策，而不需要昂贵的重新训练过程，特别是在仇恨言论检测中更加灵活地应用政策。

**Method:** 本文提出了一种使用检索增强生成（RAG）技术的分类方法，通过检索获取上下文知识，在推理阶段对内容进行评估，而非依赖预训练参数确定正确类别。

**Result:** 实验证明，该系统在不重新训练的情况下能够精准地调整特定身份群体的保护措施，并且整体性能未受影响。

**Conclusion:** 这些发现表明，RAG可以将分类过程变成一个更加灵活、透明和适应性强的方法，不仅适用于内容审核，还适用于更广泛的分类问题。

**Abstract:** Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [30] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

> InfoCausalQA是一套基于信息图表的因果推理评估基准，用于测试视觉-语言模型的因果推理能力，显示出这些模型在此方面与人类之间存在显著差距。

<details>
  <summary>Details</summary>

**Motivation:** 尽管视觉-语言模型（VLMs）在感知和推理方面取得了显著进展，但因其在因果推理能力上的不足，特别是在多模式情境下，此次研究旨在填补这一空白。

**Method:** 介绍了一个名为InfoCausalQA的新基准，用于评估基于信息图表的因果推理能力。此基准包含两个任务：任务1基于推断的数值趋势进行定量因果推理；任务2涉及五种类型的因果关系：原因、结果、干预、反事实和时间关系。

**Result:** 实验结果显示，当前的VLMs在计算推理和语义因果推理方面的能力都存在局限，其性能远低于人类的表现。

**Conclusion:** 通过InfoCausalQA，研究指出提升多模态AI系统因果推理能力的重要性。

**Abstract:** Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [31] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

> 本研究针对德国老年人士的语音意图识别问题，结合了Whisper ASR模型和三个大型语言模型生成的合成文本数据集，并证明了使用合成数据能够提高分类性能和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服现有意图识别（IR）方法中针对短命令的限制，并且主要针对英语的限制，本研究专注于德国老年人的语音意图识别问题。

**Method:** 本研究提出了一种新型方法，结合了针对老年德语演讲者适应的Whisper ASR模型，以及基于Transformer的语言模型，这些模型是通过三个著名的大型语言模型（LLM）：LeoLM、Llama3和ChatGPT生成的合成文本数据集训练的。

**Result:** 研究结果表明，使用由LLM生成的合成数据显著增强了分类性能和对不同说话风格及未见过词汇的鲁棒性。特别是，发现130亿参数的领域特定模型LeoLM在德国意图识别的数据质量上超过了1750亿参数的ChatGPT。

**Conclusion:** 该方法证明了生成式AI可以有效地填补低资源领域的数据缺口。研究提供了详细的数据生成和训练过程文档，以确保透明和可重复性。

**Abstract:** Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [32] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
*Ruichong Zhang*

Main category: cs.CL

> Introduces MDIR to overcome limitations of existing LLM plagiarism detection methods by providing accurate weight correspondence, $p$-value estimation, and focusing on weight similarity alone.

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for detecting LLM plagiarism have limitations such as failing to accurately reconstruct weight correspondences and lack of statistical significance measures.

**Method:** MDIR, a novel method that leverages matrix analysis and Large Deviation Theory to detect plagiarism in LLMs.

**Result:** MDIR reliably detects plagiarism after extensive transformations and can be performed on a single PC within an hour.

**Conclusion:** MDIR is efficient, accessible, and overcomes the limitations of current plagiarism detection methods in LLMs.

**Abstract:** In recent years, concerns about intellectual property (IP) in large language
models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct
weight copying, upcycling, pruning, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting LLM plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.

</details>


### [33] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

> 通过动态调整图表示形式，该论文提出了一种提高大模型在零样本图问答任务中准确性和简洁性的框架。

<details>
  <summary>Details</summary>

**Motivation:** 论文指出现有方法普遍使用单一的图表示形式，忽略了模型或任务的具体偏好，从而导致回答错误或冗长的问题。

**Method:** 提出了多种零样本图问答的图表示形式$F_{ZS}$，引入了一种新的评估指标Graph Response Efficiency (GRE)，开发了DynamicTRF框架，通过训练一个图表示路由器以自适应选择最佳图表示形式。

**Result:** 在7个领域的算法图问答任务中和2个跨领域的下游任务中，DynamicTRF显著提升了大模型零样本图问答的准确性。

**Conclusion:** 通过自适应适合不同问题的图表示形式，DynamicTRF在提高零样本图问答的准确性和简洁性方面表现出显著的优势。

**Abstract:** Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [34] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
*Aisha Saeid,Anu Sabu,Girish A. Koushik,Ferrante Neri,Diptesh Kanojia*

Main category: cs.CL

> 研究探讨了将攻击性言论检测作为辅助任务，整合到统一训练框架中，以提高大型语言模型（LLMs）在社交媒体上的网络欺凌检测性能。实验表明，在几种测试策略中，嵌入了攻击性预测信息的增强型提示流水线方法表现最佳。这表明带有攻击性信息的上下文能显著提升网络欺凌的检测效果。

<details>
  <summary>Details</summary>

**Motivation:** 社交媒体上网络欺凌的检测一直是个重要且挑战性的问题，因为网络欺凌表现形式微妙且各异。研究试图通过加入攻击性言论检测作为辅助任务来提高大型语言模型在该检测任务上的表现和泛化能力。

**Method:** 研究通过零样本、少量样本、独立LoRA微调和多任务学习（MTL）等策略，在包含五个攻击性言论数据集和一个网络欺凌数据集上测试了指令调优后的LLMs性能。最终提出了嵌入了攻击性预测信息的增强型提示流水线方法。

**Result:** 

**Conclusion:** 初步结果显示，带攻击性信息的增强型提示流水线方法比标准LoRA微调方法表现更好。这表明辅助任务，如攻击性言论检测，对于改善LLMs在社交媒体上的安全关键应用的泛化能力具有潜在价值。

**Abstract:** Detecting cyberbullying on social media remains a critical challenge due to
its subtle and varied expressions. This study investigates whether integrating
aggression detection as an auxiliary task within a unified training framework
can enhance the generalisation and performance of large language models (LLMs)
in cyberbullying detection. Experiments are conducted on five aggression
datasets and one cyberbullying dataset using instruction-tuned LLMs. We
evaluated multiple strategies: zero-shot, few-shot, independent LoRA
fine-tuning, and multi-task learning (MTL). Given the inconsistent results of
MTL, we propose an enriched prompt pipeline approach in which aggression
predictions are embedded into cyberbullying detection prompts to provide
contextual augmentation. Preliminary results show that the enriched prompt
pipeline consistently outperforms standard LoRA fine-tuning, indicating that
aggression-informed context significantly boosts cyberbullying detection. This
study highlights the potential of auxiliary tasks, such as aggression
detection, to improve the generalisation of LLMs for safety-critical
applications on social networks.

</details>


### [35] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
*Anubhav Jangra,Bahareh Sarrafzadeh,Adrian de Wynter,Silviu Cucerzan,Sujay Kumar Jauhar*

Main category: cs.CL

> 研究提出了新的评估方法，采用多样化评估指标组合，以更好地评估低资源条件下的风格个性化文本生成任务。

<details>
  <summary>Details</summary>

**Motivation:** 此研究质疑广泛采用的评价指标如BLEU和ROUGE在低资源作者风格个性化文本生成任务中的有效性，探索其他评价范式如风格嵌入和LLM评分者。

**Method:** 通过构建风格识别基准，作者评估了多种评价指标及其组合，该基准涵盖了八个写作任务，并在三个设置下进行评估：领域识别、作者归属和LLM个性化与非个性化对比。

**Result:** 研究发现，采用多样化的评估指标组合可以更有效地评估风格个性化的文本生成任务。

**Conclusion:** 结论是，为了有效评估风格个性化的文本生成，应采用多样化的评估指标组合。

**Abstract:** While prior research has built tools and benchmarks towards style
personalized text generation, there has been limited exploration of evaluation
in low-resource author style personalized text generation space. Through this
work, we question the effectiveness of the widely adopted evaluation metrics
like BLEU and ROUGE, and explore other evaluation paradigms such as style
embeddings and LLM-as-judge to holistically evaluate the style personalized
text generation task. We evaluate these metrics and their ensembles using our
style discrimination benchmark, that spans eight writing tasks, and evaluates
across three settings, domain discrimination, authorship attribution, and LLM
personalized vs non-personalized discrimination. We provide conclusive evidence
to adopt ensemble of diverse evaluation metrics to effectively evaluate style
personalized text generation.

</details>


### [36] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
*Lanlan Qiu,Xiao Pu,Yeqi Feng,Tianxing He*

Main category: cs.CL

> null

<details>
  <summary>Details</summary>

**Motivation:** null

**Method:** null

**Result:** {"tldr": "\u8be5\u7406\u8bba\u672c\u9020\u5efa\u4e86\u4e00\u4e2a\u547d\u540d\u4e3aChatAnime\u7684\u6570\u636e\u9762\uff0c\u8be5\u6570\u636e\u9762\u67092400\u4e2a\u4eba\u5de5\u5199\u56de\u7b54\u548c24000\u4e2a\u6709\u6548\u7684\u5927\u578b\u8bed\u6587\u6a21\u578b\u56de\u7b54\uff0c\u7528\u4e8e\u4e25\u88c1\u8f93\u51fa\u6709\u6548\u7684\u547d\u540d\u89d2\u8272\u548c\u611b\u60c5\u652f\u6491\u7684\u5bf9\u活应用的动漫角色和情感支持。", "motivation": "\u7406\u8bba\u7684\u52a0\u5165\u529b\u662f\u5728LLMs中补充上情感支持和角色扮演的能力，目前在动漫角色扮演的情感支持虚拟交互上有研究空缺。", "method": "\u4ece20\u4e2a\u5148\u5148\u7ea7\u7684\u7f8e\u79c0\u89d2\u8272\u4e2d\u9009\u62e9\u51fa\u76f8\u5f53\u8840\u7528\u7684\u5143\u7d20\u53ee\u9762\uff0c\u5e76\u5728\u5143\u7d20\u89d2\u8272\u548c\u5b66\u8003\u8005\u4e4b\u95f4\u8fdb\u884c\u5bf9\u8baf\u540e\uff0c \u6536\u96c6\u4e86两轮的对话数据。", "result": "\u5b9e\u9a8c\u6570\u636e\u4e2d\uff0c\u4e0a\u4e00\u7ea7\u6a21\u578b\u7684\u5927\u578b\u8bed\u6587\u6a21\u578b\uff08LLM\uff09\u5728\u89d2\u8272\u5e38\u4e0e\u611b\u60c5\u652f\u6491\u4e0a\u8d85\u8d8a\u4eba\u7c7b\u5a21\u4e60\uff0c\u4f46\u4eba\u7c7b\u5148\u6765\u5728\u5bf9\u8baf\u591a\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u4f18\u52bf。", "conclusion": "\u8be5\u5de5\u4f5c\u4ece\u8003\u7a76\u89d2\u8272\u5143\u7d20\u4e2d\u9009\u62e9\uff0c\u53ea\u7b97\u662f\u4e25\u88c1\u5e0c\u5145\u7684LNMs\u5e02\u573a\uff0c\u5e76\u8868\u660e\u4e86\u6b64\u65b9\u6cd5\u5728ESRP环方面的表现。"}

**Conclusion:** null

**Abstract:** Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing conversations and providing emotional support as separate research
directions. However, there remains a significant research gap in combining
these capabilities to enable emotionally supportive interactions with virtual
characters. To address this research gap, we focus on anime characters as a
case study because of their well-defined personalities and large fan bases.
This choice enables us to effectively evaluate how well LLMs can provide
emotional support while maintaining specific character traits. We introduce
ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We
first thoughtfully select 20 top-tier characters from popular anime communities
and design 60 emotion-centric real-world scenario questions. Then, we execute a
nationwide selection process to identify 40 Chinese anime enthusiasts with
profound knowledge of specific characters and extensive experience in
role-playing. Next, we systematically collect two rounds of dialogue data from
10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP
performance of LLMs, we design a user experience-oriented evaluation system
featuring 9 fine-grained metrics across three dimensions: basic dialogue,
role-playing and emotional support, along with an overall metric for response
diversity. In total, the dataset comprises 2,400 human-written and 24,000
LLM-generated answers, supported by over 132,000 human annotations.
Experimental results show that top-performing LLMs surpass human fans in
role-playing and emotional support, while humans still lead in response
diversity. We hope this work can provide valuable resources and insights for
future research on optimizing LLMs in ESRP. Our datasets are available at
https://github.com/LanlanQiu/ChatAnime.

</details>


### [37] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
*Haoran Shi,Hongwei Yao,Shuo Shao,Shaopeng Jiao,Ziqi Peng,Zhan Qin,Cong Wang*

Main category: cs.CL

> 本文提出了SecMCP以解决MCP实施中引入的安全和隐私风险，通过潜在多面体空间中的模型检测异常活动，有效识别和量化可控的对话偏移。

<details>
  <summary>Details</summary>

**Motivation:** 增强型模型上下文协议（MCP）通过集成外部工具增强了大语言模型（LLMs），这引入了重要的安全和隐私风险。为了解决工具中毒或间接提示注入等问题，提出了SecMCP框架。

**Method:** 通过建模大语言模型（LLMs）的激活向量到潜在多面体空间，SecMCP能够识别在对话动态中由敌对的外部知识引起的异常偏移，从而实现对对话劫持、误导和数据泄露的主动检测。

**Result:** 在三个最先进的LLMs（Llama3, Vicuna, Mistral）上对SecMCP进行了评估，结果表明其检测效果稳健，AUROC得分超过0.915，同时保持了系统可用性。

**Conclusion:** 该研究贡献包括对MCP安全威胁的系统分类、一种新的基于潜在多面体的方法来量化对话偏离，以及SecMCP的有效性实证验证。

**Abstract:** The Model Context Protocol (MCP) enhances large language models (LLMs) by
integrating external tools, enabling dynamic aggregation of real-time data to
improve task execution. However, its non-isolated execution context introduces
critical security and privacy risks. In particular, adversarially crafted
content can induce tool poisoning or indirect prompt injection, leading to
conversation hijacking, misinformation propagation, or data exfiltration.
Existing defenses, such as rule-based filters or LLM-driven detection, remain
inadequate due to their reliance on static signatures, computational
inefficiency, and inability to quantify conversational hijacking. To address
these limitations, we propose SecMCP, a secure framework that detects and
quantifies conversation drift, deviations in latent space trajectories induced
by adversarial external knowledge. By modeling LLM activation vectors within a
latent polytope space, SecMCP identifies anomalous shifts in conversational
dynamics, enabling proactive detection of hijacking, misleading, and data
exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,
Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),
demonstrating robust detection with AUROC scores exceeding 0.915 while
maintaining system usability. Our contributions include a systematic
categorization of MCP security threats, a novel latent polytope-based
methodology for quantifying conversation drift, and empirical validation of
SecMCP's efficacy.

</details>


### [38] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

> 本文提出了Memp方法，用于改进代理的记忆存储方式，通过提炼步骤指令和高层脚本，提高了代理在不同任务中的成功率和效率，且该记忆模型可以迁移到其他模型上，带来性能的提升。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）代理在多种任务中表现出色，但由于其固有的程序记忆通常是手动编程或是静态参数的一部分，因此存在脆弱的问题。本文旨在研究如何赋予代理一种可学习、可更新的终身程序记忆。

**Method:** 我们提出了一种名为Memp的方法，它可以将过去的代理轨迹提炼成详细的分步指令和高层次的脚本抽象形式，并探讨了不同构建、检索和更新过程记忆策略的影响。

**Result:** 实验证明，在TravelPlanner和ALFWorld上的代理随着过程记忆库的不断优化获得了更高的成功率和更优的效率。此外，由更强模型生成的过程记忆迁移到较弱模型后，也能带来性能的显著提升。

**Conclusion:** 实验结果表明，随着记忆库的不断优化，代理的成功率和效率也在不断提高。此外，一个更强大的模型生成的过程记忆迁移到一个较弱的模型上，仍能显著提高性能。

**Abstract:** Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [39] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

> 研究使用轻量级LLaMA模型对跨越13种语言的移民相关推文进行分类，表明即使是少量的语言特化微调也足以在未见过的语言中进行主题检测，但对于立场识别，多语言微调更为有效。

<details>
  <summary>Details</summary>

**Motivation:** 探讨轻量级语言模型是否能在几种语言的微调后，转移到仅在预训练期间见过的其他语言，在移民相关的极化文化特定的讨论中进行分类。测试少量语言特定微调是否能实现跨语言主题检测，并修正预训练偏差。

**Method:** 使用轻量级LLaMA 3.2-3B模型对单语言、双语言或多语言数据集进行微调，以分类来自X/Twitter的移民相关推文，覆盖13种语言。评估少量语言特定微调对跨语言主题检测的影响，以及覆盖更多语言是否能修正预训练偏差。

**Result:** 仅在一种或两种语言上微调的模型能够可靠地分类未见过语言的移民相关内容，但多语言微调对于区分移民立场更为有效。即使是极小的量的语言暴露（仅预训练令牌总量的$9.62	imes10^{-11}$），也能显著改善以代表不足的语言。

**Conclusion:** 研究结果挑战了必须大量多语言训练才能实现跨语言掌握的假设，表明有限的语言覆盖足以实现主题层面的泛化，且采用轻量级干预可以纠正结构性偏差。通过发布轻量量化、LoRA微调模型，提出一种开源且成本低廉的替代方案，加速大规模、包容性研究。

**Abstract:** Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [40] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

> 研究发现近年来，AI生成的内容在新闻领域中的使用显著增加，尤其影响地方和大学新闻，提升了词汇丰富度和可读性但降低了正式性，改变了地方媒体的写作风格。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于生成式人工智能（特别是LLMs）的快速崛起提出了对新闻诚信和作者身份的担忧，本研究旨在探讨其在新闻行业应用中的影响。

**Method:** 本研究通过对超过40,000篇来自主流、地方和大学新闻媒体的文章分析，使用了三种先进的AI文本检测工具（如Binoculars、Fast-Detect GPT和GPTZero），来检测文本是否由AI生成。

**Result:** 研究发现近年来GenAI的使用显著增加，特别是在地方和大学新闻中。句子级别的分析显示，LLMs经常被用于新闻的开头部分，而结论部分则通常是手工编写。此外，语言分析表明，GenAI提高了文本的词汇丰富度和可读性，但降低了文本的正式性，导致了更统一的写作风格，特别是在地方媒体中。

**Conclusion:** 研究表明，GenAI在改善新闻语言的丰富度和可读性方面起到了积极作用，但也带来了风格上的变化，特别是降低了文本的正式程度。这提醒我们，在新闻业中使用生成式AI技术时需谨慎，以保障新闻的诚信和多样性。

**Abstract:** The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


### [41] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
*Lingkun Long,Rubing Yang,Yushi Huang,Desheng Hui,Ao Zhou,Jianlei Yang*

Main category: cs.CL

> SlimInfer introduces a dynamic token-pruning mechanism in the inference process of Large Language Models, significantly improving speed and efficiency without affecting performance.

<details>
  <summary>Details</summary>

**Motivation:** To address the high computational demands for long-context inference in LLMs, where current methods still process the full set of hidden states, leading to inefficiency.

**Method:** SlimInfer, a framework that prunes less critical prompt tokens during the forward pass in the inference of Large Language Models to enhance efficiency.

**Result:** Achieved up to 2.53x speedup in time-to-first-token (TTFT) and 1.88x reduction in end-to-end latency for LLaMA3.1-8B-Instruct on a single RTX 4090, without performance loss.

**Conclusion:** SlimInfer effectively accelerates LLM inference by dynamic fine-grained pruning of hidden state tokens, reducing memory usage and I/O costs without compromising performance.

**Abstract:** Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.

</details>


### [42] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
*GLM-4. 5 Team,:,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu,Yuanhao Wen,Yushi Bai,Zhengxiao Du,Zihan Wang,Zilin Zhu,Bohan Zhang,Bosi Wen,Bowen Wu,Bowen Xu,Can Huang,Casey Zhao,Changpeng Cai,Chao Yu,Chen Li,Chendi Ge,Chenghua Huang,Chenhui Zhang,Chenxi Xu,Chenzheng Zhu,Chuang Li,Congfeng Yin,Daoyan Lin,Dayong Yang,Dazhi Jiang,Ding Ai,Erle Zhu,Fei Wang,Gengzheng Pan,Guo Wang,Hailong Sun,Haitao Li,Haiyang Li,Haiyi Hu,Hanyu Zhang,Hao Peng,Hao Tai,Haoke Zhang,Haoran Wang,Haoyu Yang,He Liu,He Zhao,Hongwei Liu,Hongxi Yan,Huan Liu,Huilong Chen,Ji Li,Jiajing Zhao,Jiamin Ren,Jian Jiao,Jiani Zhao,Jianyang Yan,Jiaqi Wang,Jiayi Gui,Jiayue Zhao,Jie Liu,Jijie Li,Jing Li,Jing Lu,Jingsen Wang,Jingwei Yuan,Jingxuan Li,Jingzhao Du,Jinhua Du,Jinxin Liu,Junkai Zhi,Junli Gao,Ke Wang,Lekang Yang,Liang Xu,Lin Fan,Lindong Wu,Lintao Ding,Lu Wang,Man Zhang,Minghao Li,Minghuan Xu,Mingming Zhao,Mingshu Zhai,Pengfan Du,Qian Dong,Shangde Lei,Shangqing Tu,Shangtong Yang,Shaoyou Lu,Shijie Li,Shuang Li,Shuang-Li,Shuxun Yang,Sibo Yi,Tianshu Yu,Wei Tian,Weihan Wang,Wenbo Yu,Weng Lam Tam,Wenjie Liang,Wentao Liu,Xiao Wang,Xiaohan Jia,Xiaotao Gu,Xiaoying Ling,Xin Wang,Xing Fan,Xingru Pan,Xinyuan Zhang,Xinze Zhang,Xiuqing Fu,Xunkai Zhang,Yabo Xu,Yandong Wu,Yida Lu,Yidong Wang,Yilin Zhou,Yiming Pan,Ying Zhang,Yingli Wang,Yingru Li,Yinpei Su,Yipeng Geng,Yitong Zhu,Yongkun Yang,Yuhang Li,Yuhao Wu,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yuxuan Zhang,Zezhen Liu,Zhen Yang,Zhengda Zhou,Zhongpei Qiao,Zhuoer Feng,Zhuorui Liu,Zichen Zhang,Zihan Wang,Zijun Yao,Zikang Wang,Ziqiang Liu,Ziwei Chai,Zixuan Li,Zuodong Zhao,Wenguang Chen,Jidong Zhai,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.CL

> GLM-4.5是一种具有355B总参数和32B激活参数的开放源代码混合专家（MoE）大型语言模型，在较少的参数下实现了代理、推理和编码任务的高水平性能。

<details>
  <summary>Details</summary>

**Motivation:** 开发一种具有开放源代码的混合专家（MoE）大语言模型，旨在支持思考和直接响应模式的混合推理方法，从而在代理能力和推理任务中取得高水平的表现。

**Method:** 通过多阶段训练23T标记，并通过专家模型迭代和强化学习进行综合后期训练，GLM-4.5实现了在代理、推理和编码（ARC）任务中的强大性能。

**Result:** GLM-4.5在TAU-Bench上获得70.1%的成绩，在AIME 24上获得91.0%的成绩，在SWE-bench Verified上获得64.2%的成绩。在所有评估模型中，GLM-4.5总体排名第三，在代理基准测试中排名第二。

**Conclusion:** 研究发布了具有355B参数的GLM-4.5和一个紧凑版本GLM-4.5-Air（106B参数），以推进在代理和推理AI系统领域的研究。

**Abstract:** We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.

</details>


### [43] [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

> 本文提出了HapticLLaMA模型，通过振动信号生成触觉描述，研究了两种触觉分词器，并通过监督微调和人类反馈增强学习进行了模型训练。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在填补现有研究中对触摸感官信号的研究空白，提出了触觉字幕生成的任务，即从触觉信号中生成自然语言描述，这类研究可应用于虚拟现实、无障碍、康复等领域。

**Method:** 本文提出了HapticLLaMA，一种多模态感官语言模型，用于将振动信号转化为描述。研究了两种触觉分词器，一种是基于频率的分词器，另一种是基于EnCodec的分词器，用于将触觉信号转化为离散的序列单元。HapticLLaMA的训练分两阶段进行：（1）用LoRA进行监督微调，（2）通过从人类反馈中进行增强学习的再训练。

**Result:** HapticLLaMA模型在触觉字幕生成方面表现良好，显示出强大的振动信号解释能力，取得59.98的METEOR评分和32.06的BLEU-4评分。人工评级结果显示61%以上的生成字幕评分高于3.5（满分7分），人类反馈增强学习显著提高了与人类触觉感知的匹配度，评分分布提高了10%。

**Conclusion:** 这项研究突出了大型语言模型处理和适应感官数据的潜力。通过实验验证，该模型能够有效地生成触觉信号描述，展现出良好的性能。

**Abstract:** Haptic captioning is the task of generating natural language descriptions
from haptic signals, such as vibrations, for use in virtual reality,
accessibility, and rehabilitation applications. While previous multimodal
research has focused primarily on vision and audio, haptic signals for the
sense of touch remain underexplored. To address this gap, we formalize the
haptic captioning task and propose HapticLLaMA, a multimodal sensory language
model that interprets vibration signals into descriptions in a given sensory,
emotional, or associative category. We investigate two types of haptic
tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that
convert haptic signals into sequences of discrete units, enabling their
integration with the LLaMA model. HapticLLaMA is trained in two stages: (1)
supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,
and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We
assess HapticLLaMA's captioning performance using both automated n-gram metrics
and human evaluation. HapticLLaMA demonstrates strong capability in
interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a
BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated
captions received human ratings above 3.5 on a 7-point scale, with RLHF
yielding a 10% improvement in the overall rating distribution, indicating
stronger alignment with human haptic perception. These findings highlight the
potential of large language models to process and adapt to sensory data.

</details>


### [44] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

> 研究通过后训练过程改进了大语言模型在多轮交流中形成惯例的能力，并通过两个新基准测试证明了这一提升。

<details>
  <summary>Details</summary>

**Motivation:** 人类在多轮沟通中通过适应语言和形成临时惯例来提高交流效率，但研究表明，大语言模型（LLMs）并不能自然地表现出这种行为。

**Method:** 我们开发了一种后训练过程，通过针对识别出的惯例形成演示进行精细调整，来培养这一能力。

**Result:** 我们通过两种新的评估方法证明了后训练语言模型在惯例形成方面的能力有显著提升。

**Conclusion:** 研究表明，在通过特定的精细调整后，大语言模型在形成惯例方面的能力得到了显著提高。

**Abstract:** Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

> 文章提出ResPA攻击方法，以提高对抗样本的转移能力。通过引入输入梯度的指数加权平均值和梯度残差，增强了对全局扰动方向变化的捕捉，实验展示了更高的转移效果。

<details>
  <summary>Details</summary>

**Motivation:** 对抗样本在对抗性攻击下的转移能力受到忽视，特别是对抗样本在平坦损失景观中的转移能力更能减轻对替代模型的过拟合，但现有方法忽略了扰动方向对转移能力的影响，因此需提出新的攻击方法。

**Method:** 文章提出了一种名为Residual Perturbation Attack (ResPA) 的攻击方法，利用残差梯度作为扰动方向，指引对抗样本朝损失函数的平坦区移动。该方法使用输入梯度的指数加权平均值作为参考梯度，结合当前梯度与参考梯度之间的残差，捕捉全局扰动方向的变化。

**Result:** <tool_call>

**Conclusion:** 实验结果表明，ResPA在对抗样本的转移能力方面优于现有典型的对抗性转移攻击方法，结合当前输入变换方法可进一步提升其转移能力。

**Abstract:** Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [46] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

> 本文提出了一种新的少样本OOD检测框架GOOD，在开放世界中提高了泛化能力。通过引入辅助通用知识模型和知识动态嵌入机制，优化了广度-特异性平衡，从而改进检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的少样本OOD检测方法通常泛化能力不足，尤其是在开放世界中，这限制了它们在不同场景下的表现。为了克服这一挑战，提出了这项研究。

**Method:** 我们提出了一种名为Generalized Few-shot OOD Detection (GOOD) 的框架，通过引入一个辅助的通用知识模型（GKM）来增强OOD检测模型的泛化能力，而不是直接从少量数据中学习。我们还提出了一种知识动态嵌入（KDE）机制，通过基于GKM的广义信念（G-Belief）动态调整输出分布，从而优化广度-特异性平衡（GS-balance）。

**Result:** 实验结果表明，该方法在现实世界的OOD基准上取得了优越的性能。

**Conclusion:** GOOD框架通过引入辅助通用知识模型和使用知识动态嵌入机制，能够在少样本情况下提高OOD检测的泛化性能并激发新的研究方向。

**Abstract:** Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [47] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

> 本研究提出了UnGuide方法，以解决利用LoRA进行机器无学习时出现的与目标无关内容质量下降的问题。该方法通过动态调节指导尺度，实现了选择性无学习。

<details>
  <summary>Details</summary>

**Motivation:** 大型文本到图像扩散模型的最新进展提高了对这些模型可能被误用的关注，特别是在生成有害或误导性内容方面。这凸显了有效机器无学习的迫切需求，即从预训练模型中移除特定知识或概念而不影响整体性能。

**Method:** UnGuide方法利用UnGuidance，这是一种动态推理机制，通过无分类器指导（CFG）来精确控制无学习过程。UnGuide可以根据去噪过程的前几个步骤的稳定性来调节指导尺度，实现通过LoRA适配器的选择性无学习。

**Result:** 实验结果表明，UnGuide能够实现受控的概念删除，并保持扩散模型的表达能力，其在物体擦除和显式内容删除任务中优于现有的基于LoRA的方法。

**Conclusion:** UnGuide方法在保持扩散模型表达能力的同时，能够实现对特定概念的有效删除，并优于现有的LoRA方法。

**Abstract:** Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [48] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

> 本文提出了一种基于部分卷积的风格迁移网络，可以准确地对图像中的指定区域应用艺术风格转换，提高了风格化的效果。

<details>
  <summary>Details</summary>

**Motivation:** 大多数算法将艺术风格迁移应用于整个图像，但个别用户可能只需对图像中的特定区域应用风格转换。标准的做法是在风格化之后简单地屏蔽图像。这种方法往往不能很好地捕获感兴趣区域的风格特征。

**Method:** 提出了一种基于部分卷积的风格迁移网络，该网络可以准确地将风格特征仅应用于图像中的感兴趣区域。此外，还介绍了网络内部的融合技术，以解决区域选择中的不完美问题。

**Result:** 实验表明，这种技术在视觉和定量上都改进了样式转换，使用了SA-1B数据集中的示例来展示。

**Conclusion:** 研究表明，该方法在进行特定区域的艺术风格迁移时有效，提高了视觉和定量的满意度。代码可在https://github.com/davidmhart/StyleTransferMasked公开获取。

**Abstract:** Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [49] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

> MAISI-v2 是首个加速的3D医学图像合成框架，它结合了校正流来实现快速和高质量的生成，并通过引入区域特定对比损失来进一步增强条件保真度。实验表明，与之前的模型相比，它在潜扩散模型方面实现了33倍的加速，并且达到了当前最佳的图像质量。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法中存在的局限性，包括生成能力的局限性、推理速度慢以及与输入条件的对齐性较差这些问题。

**Method:** MAISI-v2 采用校正流加速生成过程，并使用区域特定对比损失来增强对感兴趣区域的敏感性。

**Result:** 实验显示，MAISI-v2 在潜扩散模型中实现了33倍加速，达到了当前最佳的图像质量，并且能够用于数据增强的下游分割实验中。

**Conclusion:** MAISI-v2 提供了快速高质量的3D医学图像生成，代码、训练细节、模型权重和GUI演示都已公开，促进了社区内的可重复性和进一步发展。

**Abstract:** Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [50] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

> 本文提出了一种用于有限数据条件下在多样化的脑成像任务中部署预训练MRI变压器的实用框架。该框架通过MAE预训练策略获得了良好的泛化性能，展示了在资源有限的环境中应用的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基于变压器的机器学习在医学成像领域显示出了巨大的潜力，但标注数据的稀缺限制了其在实际应用中的适用性。本研究旨在提出一个实用框架，用于在多样化的脑成像任务中有限数据条件下的预训练MRI变压器的部署。

**Method:** 通过在包含超过3100万个切片的大型多队列脑部MRI数据集上使用掩码自动编码器（MAE）预训练策略，我们获得了在任务和数据集之间具有良好泛化能力的高度迁移性潜在表示。对于像分类这样的高级任务，冻结的MAE编码器与轻量级线性头相结合，在基于最小监督的MRI序列识别中达到了最先进的准确性。对于像分割这样的低级任务，我们提出了一种融合多尺度CNN特征与预训练MAE嵌入的混合架构MAE-FUnet。该模型在数据有限的情况下，相对于其他强大的基线模型，在颅骨剥离和多类解剖分割中均表现出了更高的性能。

**Result:** 通过广泛的定量和定性评估，我们的框架展现了效率、稳定性和可扩展性，表明其适用于资源有限的临床环境和更广泛的神经成像应用。

**Conclusion:** 本研究证明了在有限的数据条件下，我们的方法能够有效且稳定地应用于各种神经成像任务中，并展示出了广泛的适用性，尤其是在资源有限的临床环境中。

**Abstract:** Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [51] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

> 本研究提出了一种无需重建和优化的3D高斯splat样式转移方法，通过生成splat表示隐含表面的图结构，使用前馈表面样化方法并插值回场景中的个别splat，实现了快速样化，无需额外训练或优化，可在普通硬件上达到2分钟内完成。

<details>
  <summary>Details</summary>

**Motivation:** 之前的不少工作都探索了3D高斯splat的样式转移任务，但这些方法都需要重建或调整splat以整合样式信息或优化特征提取网络。作者希望通过避免此过程来提供更高效的方法。

**Method:** 研究提出了一种无需重建和优化的3D高斯splat样式转移方法。首先生成splat表示隐含表面的图结构，然后使用前馈表面样化方法，并将样式化结果插值回至场景中的个别splat。

**Result:** 该方法允许使用任何样式图像和3D高斯splat而不需额外训练或优化。研究展示了这种样式转移方法的质量，并与其他方法进行了比较。

**Conclusion:** 实验表明，该方法在普通硬件上可以实现高质量和平面化，速度可快达2分钟内完成。结果证实了研究提出的无重建与优化样化方法的效率与实用性。

**Abstract:** The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [52] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

> The paper presents MZEN, a method that enhances NeRF for capturing fine-detailed structures in industrial inspection by handling multi-zoom image sets, improving reconstruction quality by up to 28% in PSNR and reducing perceptual difference by 222%.

<details>
  <summary>Details</summary>

**Motivation:** Standard NeRF methods struggle to capture fine-detailed structures important in industrial settings due to the incompatibility with multi-zoom images, which breaks multi-view consistency. This motivates the development of MZEN to handle such scenarios.

**Method:** The paper introduces Multi-Zoom Enhanced NeRF (MZEN), which handles multi-zoom image sets by adding a learnable zoom scalar to the pin-hole camera model and employing a pose strategy where wide-field images establish a global metric frame, and zoom-in images are aligned through a crop-and-match procedure.

**Result:** MZEN outperforms pose-free baselines and high-resolution variants across eight forward-facing scenes, improving PSNR by up to 28%, SSIM by 10%, and reducing LPIPS by up to 222%.

**Conclusion:** MZEN effectively extends the capabilities of NeRF to handle the strict requirements of industrial inspections, preserving global accuracy while improving the capture of micron-level details.

**Abstract:** Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [53] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

> 本文提出了TSMS-SAM2框架以改善手术视频中的可提示VOST，并通过实验验证了其在EndoVis2017和EndoVis2018数据集上的优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 本工作的动机在于解决SAM2在手术视频分析中的应用难题，特别是处理快速物体运动和内存冗余的问题。

**Method:** TSMS-SAM2框架通过引入多时间尺度视频采样增强策略以提高对运动变化的鲁棒性，并采用记忆分割和修剪机制来组织和过滤过去的帧特征以提高效率和准确性。

**Result:** 在EndoVis2017和EndoVis2018数据集上，TSMS-SAM2分别实现了95.24和86.73的最高平均dice分，优于之前的SAM基于方法和任务特定的方法。

**Conclusion:** TSMS-SAM2框架展示了在复杂手术场景中实现高效、准确分割的潜力。

**Abstract:** Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [54] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

> 介绍了TCA方法，能够在减少计算量的同时提升视频分割的准确性，适用于自然和特定领域的视频。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决视频分割中的计算成本高问题，现有方法在视频扩展上遇到瓶颈，无法利用时间冗余进一步优化视频分割效果。

**Method:** 引入了Temporal Cluster Assignment (TCA)，一种轻量级且有效的策略，无需微调即可通过利用帧间的时序相干性来改进基于聚类的方法，从而在减少计算量的同时保留细粒度细节。

**Result:** 在YouTube-VIS 2019，YouTube-VIS 2021，OVIS和私人外科视频数据集上的评估表明，TCA可以显著提高现有基于聚类方法的准确性和速度之间的平衡。

**Conclusion:** TCA展示了在不同视频数据集上的强大泛化能力，证明其在视频分割任务中具有较高效率和准确性。

**Abstract:** Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [55] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

> 研究通过视觉-语言框架整合低级视觉信息和高层次上下文，基于少量样本和零样本学习，利用自然语言预测驾驶员的视觉注意力变化，优于现有技术。

<details>
  <summary>Details</summary>

**Motivation:** 大多数先前的研究仅关注于使用静态RGB图像在某一时刻评估注意力分配，而本研究旨在通过自然语言描述驾驶员视觉注意力的变化，提供了一个自动驾驶和人机交互领域的新方向，以实现可解释的人工智能。

**Method:** 本研究提出了一种基于视觉语言的框架，该框架使用少量样本和零样本学习在单一RGB图像上预测驾驶员视觉注意力的变化。通过人类参与的反馈，从BDD-A数据集中整理和优化了高质量的标注，并对LLaVA进行了微调，使其视觉感知与关注场景理解相一致。该方法整合了低级线索和高层次的上下文（如路径语义、风险预期），允许用语言描述注视行为。

**Result:** 在多种训练算法（少量样本和一次样本）评估中，我们的微调模型优于普通视觉语言模型，在注意力转移检测和可解释性方面表现出色。

**Conclusion:** 研究提出的方法为后续的行为预测、人机协作和多代理协调等任务奠定了基础，开创新的研究方向。

**Abstract:** Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [56] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [57] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

> 本文提出了一个高效的测试时适应方法（ETTA），通过递归更新所有测试样本并自适应地结合模块得分，来改进预训练的视觉语言模型在新环境下的决策边界，提高其准确性和适应性。实验结果表明，ETTA在计算复杂度和准确性上优于同类方法。

<details>
  <summary>Details</summary>

**Motivation:** 预训练的视觉语言模型（VLMs）在零样本情况下表现出色，但面对分布变化时表现不佳。为解决这一问题，本文旨在改进基于缓存的测试时适应方法，以提高适应性和准确性。

**Method:** 本文提出了一种高效的测试时适应方法（ETTA），通过递归更新模块整合所有测试样本，逐步细化决策边界，并采用自适应集成模块减少对提示的依赖，提高图像文本匹配的准确率。此外，ETTA还根据置信度水平自适应地结合两个模块的得分，充分利用它们的互补优势。

**Result:** 实验结果表明，ETTA在计算复杂度和准确性方面超越了现有的测试时适应模型，确立了新的效果和效率标准。

**Conclusion:** 本研究表明，通过引入递归更新模块和自适应集成模块，可以在保持低计算和内存开销的情况下提高预训练视觉语言模型在新环境中的性能。

**Abstract:** Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [58] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

> HOLODECK 2.0通过先进的视觉语言模型和3D生成模型灵活生成高保真3D场景，支持基于文本描述的交互式编辑，适用于游戏等领域。

<details>
  <summary>Details</summary>

**Motivation:** HOLODECK 2.0旨在解决现有3D场景设计过于依赖手工操作的问题，提供一种能从文本直接生成多样化和风格丰富的3D场景的先进框架。

**Method:** HOLODECK 2.0利用视觉语言模型（VLMs）解析所需物体，并使用先进的3D生成模型创建高质量资产。然后迭代地应用来自VLMs的空间约束以获得语义连贯和物理上可信的布局。

**Result:** 人类评估和CLIP评分显示，HOLODECK 2.0能有效生成与详细文本描述一致的高质量场景，并在室内和开放领域场景中均优于基线模型。

**Conclusion:** HOLODECK 2.0不仅能够在多个风格（如现实、卡通、动漫和赛博朋克）中生成高保真的3D场景，还具备灵活适应人类反馈的编辑能力，在程序化游戏建模中的实际应用也预示着能极大地提升效率。

**Abstract:** 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [59] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

> 本文提出了一个名为\textit{RopStitch}的无监督深度图像拼接框架，通过双分支架构结合内容和结构信息，解决了鲁棒性和自然性问题，展现出与现有方法相比的优势。

<details>
  <summary>Details</summary>

**Motivation:** 研究的初衷是开发一种无监督的深度图像拼接框架，既能增强鲁棒性，又能保持自然外观。

**Method:** 本文提出了一个名为\textit{RopStitch}的无监督深度图像拼接框架，该框架具有鲁棒性和自然性。研究通过一个双分支架构将内容感知的通用先验融入图像拼接模型中，分别捕捉粗略和精细的特征并整合，以实现高度通用化的性能。具体而言，这个双分支模型由一个预训练分支组成，用于捕获语义不变表示，以及一个可学习分支，用来提取精细的辨别特征，并在关联层面通过一个可控因素合并这两个特征。进一步地，考虑到内容对齐和结构保持通常相互矛盾，提出了一种虚拟最优平面的概念来缓解这一矛盾。为此，研究将这个问题建模为一个估计仿射变换分解系数的过程，并设计了一个迭代系数预测器和最小语义扭曲约束条件，以识别最优平面。最终，这种方案以双向拍摄的方式被整合进入\textit{RopStitch}框架中。

**Result:** 实验表明，提出的\textit{RopStitch}方法在多个数据集上显著优于现有方法，特别是在场景鲁棒性和内容自然性上。

**Conclusion:** \textit{RopStitch}框架通过创新的方法实现了在图像拼接上更鲁棒和更自然的效果，相较于现有方法具有显著优势。

**Abstract:** We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [60] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

> This paper explores using neural field models for advanced imaging tasks on smartphones, outperforming existing methods without complex requirements.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the research is to leverage the advanced imaging technologies and sensors in modern smartphones to enhance computational imaging capabilities using minimal resources and without relying on traditional machine learning techniques.

**Method:** The paper uses carefully designed neural field models to represent complex geometry and lighting effects. These models are used for applications like depth estimation, layer separation, and image stitching using data directly from mobile photography.

**Result:** The methods proposed in the paper outperform state-of-the-art approaches without requiring complex preprocessing, labeled data, or machine learning priors.

**Conclusion:** Well-constructed, self-regularized neural field models can effectively solve challenging inverse problems in mobile imaging by fitting directly to raw smartphone measurements, significantly improving the capabilities of mobile photography.

**Abstract:** Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [61] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

> 论文评估了两种3D分割方法在建筑工地中的效果，并提出需要适合室外场景的分割方法来提高施工进度的自动监测技术。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于传统的数据采集方法在复杂、杂乱和不断变化的建筑工地环境中表现不佳，该论文旨在探索基于计算机视觉的方法来提高监测施工进度的效率和适用性。

**Method:** 该论文探讨了两种先进的3D分割方法——Segment Anything Model (SAM) 和 Mask3D 在复杂且不断变化的建筑工地环境中的适用性和性能。

**Result:** 该研究通过比较分析，展示了SAM和Mask3D在建筑工地中的相对有效性，并指出了在室外场景中目前分割方法的不足。

**Conclusion:** 该论文强调了需要针对建筑工地数据定制分割工作流，以便提取有价值的信息，推动该领域向更自动化的方向发展。

**Abstract:** Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [62] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

> SINGAD is a self-supervised normal estimation framework that addresses the limitations of current diffusion-based methods by incorporating physics-driven light interaction and differentiable rendering, leading to improved performance and multi-view consistency without annotated data.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitations of recent diffusion-based methods for normal estimation, such as the dependency on dense annotations and the issue of multi-view normal direction conflicts caused by their reliance on statistical priors.

**Method:** The paper proposes SINGAD, a self-supervised framework for normal estimation from a single image. It uses a physics-driven 3D geometric representation and differentiable rendering for reprojection. The framework includes a light-interaction-driven 3D Gaussian splatting model and a cross-domain feature fusion module within a conditional diffusion model to generate normals. The method minimizes geometric errors without relying on dense normal annotations.

**Result:** Quantitative evaluations on the Google Scanned Objects dataset show that SINGAD outperforms state-of-the-art approaches across multiple metrics.

**Conclusion:** The framework successfully integrates physics-based modeling and differentiable rendering to directly optimize normals based on geometric errors, eliminating the need for dense normal annotations and improving consistency across multi-views.

**Abstract:** The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [63] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

> Bifrost-1框架通过使用CLIP的patch-level图像嵌入作为潜在变量，将预训练的多模态语言模型和扩散模型结合，实现了高保真且计算成本低的图像生成。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是为了整合高保真视觉合成能力到大型语言模型中，同时不牺牲它们的强推理能力。现有的直接训练LLMs或是连接LLMs和扩散模型的方法通常需要高昂的训练成本，因为基础的LLMs在预训练期间并未接触过图像表示。

**Method:** 通过使用CLIP的patch-level图像嵌入作为潜在变量，Bifrost-1框架将预训练的多模态语言模型（MLLMs）和扩散模型相结合。此外，为了保留MLLMs的多模态推理能力，在预测patch-level图像嵌入时，会为MLLM添加一个初始化为原始MLLM参数的视觉生成分支。

**Result:** 实验表明，Bifrost-1在视觉保真度和多模态理解方面达到了与先前方法相当或更好的表现，并且在训练过程中计算量显著降低。

**Conclusion:** 通过无缝集成预训练的MLLMs和扩散模型，并使用CLIP的patch-level潜在变量，Bifrost-1框架实现了高保真可控的图像生成，并且具有显著的训练效率优势。

**Abstract:** There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [64] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

> 本文引入PASG框架解决机器人操作中高层次任务语义和低层次几何特征之间的割裂问题，通过自动提取几何特征并将其与视觉语言模型相结合，取得了与手动注释相当的表现。

<details>
  <summary>Details</summary>

**Motivation:** 当前机器人操作面临的一个持续挑战是高层次任务语义与低层次几何特征之间的割裂。视觉-语言模型虽然展示了生成感知意识的视觉表示的潜力，但其在标准空间中的语义接地不足和对手动注释的依赖限制了它们捕捉动态语义-适用性关系的能力。

**Method:** 本文提出了Primitive-Aware Semantic Grounding (PASG)，这是一个闭环框架，包括：1) 通过几何特征聚合进行自动原语提取，能够跨类别检测关键点和轴；2) 基于视觉语言模型的语义锚定，动态地将几何原语与功能性适用性以及任务相关描述结合；3) 一个空间-语义推理基准和一个微调过的视觉语言模型Qwen2.5VL-PA。

**Result:** 实验中，PASG在各种场景下的实际机器人操作任务中表现有效，其性能可与手动注释相媲美。

**Conclusion:** PASG实现了对物体更细粒度的语义-适用性理解，建立了将几何原语与机器人操作中的任务语义相联系的统一范式。

**Abstract:** The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [65] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

> 本文提出了AnimateScene框架，通过准确的定位模块、无需训练的样式对齐方法和可以插入摄像机轨迹的后重建方法解决了3D场景重建与4D人体动画的无缝融合问题。

<details>
  <summary>Details</summary>

**Motivation:** 虽然3D场景重建和4D人体动画近年来取得了快速发展，然而将两者无缝集成以产生视觉上吸引人的结果仍然具有挑战性。主要困难包括将人体放置在正确的场景位置和规模以避免不现实的穿模，以及背景和动画在光照和风格上的差异导致的不真实组合问题。

**Method:** AnimateScene提出了一种综合框架来解决人体动画与3D场景重建无缝集成的挑战。该方法包括三个主要部分：1) 设计了一个准确的定位模块，自动确定人体在场景中的合理3D位置并防止动画过程中出现穿模。2) 提出了一种无需训练的样式对齐方法，使得4D人体表示与背景的光照和风格相匹配，从而实现视觉上的连贯整合。3) 设计了一个联合后重建方法，用于4D人体和3D场景，允许插入摄像机轨迹，从而使得最终渲染的视频具有视觉上吸引人的摄像机运动。

**Result:** 实验表明，AnimateScene生成的动态场景视频在几何细节和各种摄像机及动作组合下的时空连贯性方面都表现出色。

**Conclusion:** AnimateScene提供了一种解决这些集成挑战的全面解决方案，使得生成的动态场景视频具有高度的细节和连贯性。

**Abstract:** 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [66] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

> 通过对抗扰动训练能量模型，实现测试时深度预测模型的自适应调整，提高在新环境下的预测准确性。

<details>
  <summary>Details</summary>

**Motivation:** 解决预训练深度完成模型在新环境数据下因协变量变化导致预测不准的问题。

**Method:** 提出Energy-based Test-time Adaptation (ETA)方法，利用对抗扰动探索数据空间，训练能量模型对预测区域评分。

**Result:** 在三个室内和三个室外数据集上，平均比之前的最先进技术提高了6.94%和10.23%。

**Conclusion:** 对抗扰动和能量模型的有效结合，能在测试时调整模型，提高其适应不同环境的能力。

**Abstract:** We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [67] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

> This paper presents an efficient video computer vision system that improves upon existing methods by directly using Bayer-format data, employing a fast motion estimation algorithm, and using a context-aware refinement network to achieve significant acceleration with minor performance loss.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenge of reducing temporal redundancy and the front end computation overhead in video computer vision systems that has not been sufficiently addressed by existing works.

**Method:** The paper proposes an efficient video computer vision system by removing the image signal processor and using Bayer-format data input. It introduces a fast block matching-based motion estimation algorithm with a MV refinement module and a context-aware block refinement network to correct errors. The system also employs a frame selection strategy to balance accuracy and efficiency.

**Result:** The experimental results show that the proposed method achieves significant acceleration while maintaining a high level of performance in multiple video computer vision tasks.

**Conclusion:** The conclusion of the paper is that the proposed system effectively reduces temporal redundancy and front end computation overhead, achieving faster processing with only slight performance loss as demonstrated by experiments on multiple video computer vision tasks.

**Abstract:** The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [68] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

> 本研究针对MER2025竞赛中的MER-SEMI挑战，通过多模态情感识别框架改进了情感识别性能，框架包括了多层次特征提取、多模态融合和标签优化策略。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在解决情绪识别中的数据稀缺问题，增强人机交互体验，特别是在MER2025竞赛的MER-SEMI挑战中。

**Method:** 我们提出了一个新颖的多模态情感识别框架，利用大规模预训练模型从视觉、音频和文本模态中提取有用特征。视觉模态上，设计了双分支视觉编码器捕获全局帧级特征和面部局部表示；文本模态上，引入了一种基于大语言模型的上下文丰富方法来增强文本中的情感线索。我们提出了一种集成了自注意力机制和残差连接的融合策略来整合这些多模态特征。此外，我们还采用多源标注策略来优化训练集中的噪声标签。

**Result:** 我们的方法在MER2025-SEMI数据集上显著提高了官方基线的性能，加权F值达到87.49%，相比基线的78.63%有了明显提升。

**Conclusion:** 实验结果表明，我们提出的情感识别框架在MER2025-SEMI数据集上取得的成绩显著优于官方基线，证明了框架的有效性。

**Abstract:** Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [69] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

> 本文引入MakeupQuad数据集和EvoMakeup框架，解决面部妆容编辑的身份保持和化妆真实性的难题，实现高质量的妆容编辑。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决现有方法中由于缺乏结构化的配对数据而导致的低质量结果、妆容细节粗糙以及难以同时保持身份和妆容真实性的问题。

**Method:** 我们的方法引入了MakeupQuad，这是一个大规模、高质量的数据集，包含无妆脸、参考妆容、编辑结果和文本妆容描述。我们还提出了EvoMakeup，这是一个统一的训练框架，通过多阶段蒸馏减少图像退化，实现数据和模型质量的迭代改进。

**Result:** 尽管仅在合成数据上进行训练，EvoMakeup在现实世界基准测试中表现出色，支持高保真度、可控的多任务化妆编辑，包括全脸和部分参考编辑以及基于文本的化妆编辑。

**Conclusion:** 实验结果表明，我们提出的方法在保持身份的同时，实现了妆容的真实性和控制性，优于先前的方法。

**Abstract:** Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [70] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

> 该研究指出现有MLLMs在处理K-12教育环境中真实场景的数学问题图象任务时面临挑战，为解决这一问题，创建了MathReal数据集来评估模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在填补现有基准测试中对多模态输入质量欠缺、未考虑K-12教育用户实际使用场景的空白。通过创建MathReal数据集，研究人员试图评估和分析大型语言模型在教育现实应用中的数学推理能力。

**Method:** 研究人员构建了一个名为MathReal的数据集，包括2,000个数学问题，这些问题的图片是通过手持设备在实际场景中拍摄的。他们将图像质量下降、透视变化和无关内容干扰分为14个子类别，并且根据难度和问题类型设置实验来研究模型的性能。

**Result:** 研究指出现有的多模态大语言模型（MLLMs）在处理由手持设备拍摄的真实场景数学问题图片时表现出色，但这些模型在实际K-12教育环境中效果不佳。为此，研究人员创建了MathReal数据集，以评估这些模型在真实场景中的数学推理能力。实验显示，现有模型在处理这些问题时存在局限性，研究同时提供了关于其性能分析和未来改进方向的见解。

**Conclusion:** 基于大量的实验，研究人员发现现有的MLLMs在真实的教育环境中的问题解决能力受到显著挑战，提出了一套详细的性能分析，总结了模型错误模式，以便为未来改进计划提供方向。

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [71] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [72] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

> 本文提出了一种基于扩散模型生成高质量子可见粒子图像的方法，解决训练数据集中的类别不平衡问题，提高了多分类深度学习模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服子可见粒子分析中数据稀缺和类别不平衡的问题，特别是在无意中出现的且数量较少的粒子类型（如硅油和气泡）的分类问题。

**Method:** 我们开发了一种基于扩散模型的状态-of-the-art方法来生成高保真图像，以解决数据不平衡问题并增强训练数据集，从而有效地训练多类深度神经网络分类器。

**Result:** 通过在含有500,000张蛋白质粒子图像的验证数据集上进行的大规模实验，验证了使用扩散模型生成的图像作为训练数据时，分类性能得到了明显提升。

**Conclusion:** 我们提出的方法通过生成真实粒子图像，在视觉质量和结构上都有很好的匹配，能够在多类分类问题中有效利用生成的数据来改善分类器的表现，并且我们已经公开了使用的扩散模型和训练的多类深度神经网络分类器。

**Abstract:** Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [73] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

> Spectrum是一种统一网络，用于解析级别的像素解析（身体部位和衣物）和实例级分组，通过微调一个图像到纹理扩散模型来改进对身体部位和衣物的解析，可以生成语义分割图，准确分割身体和衣物。

<details>
  <summary>Details</summary>

**Motivation:** 现有的人体解析方法通常使用固定且标签粗略的掩模类别，难以解析细粒度的衣物类型。而开放词汇分割方法虽能零样本迁移，但未能区分多样化的衣物或详细的身体部分。因此，提出Spectrum方法来解决这些问题。

**Method:** Spectrum方法通过对一个图像到纹理（I2Tx）扩散模型进行微调来改进身体部位和衣物的解析，该模型是在3D人体纹理图上对文本到图像（T2I）模型进行微调得到的。通过从输入图像中提取人体部位内部特征，并通过提示引导进行语义有效的掩模生成，以对多样化的衣物分类进行对齐。

**Result:** 在广泛的跨数据集实验中，Spectrum方法对身体部位、衣物部分、未见过的衣物类别以及全身掩模的评估显示，它在基于提示的分割任务中始终优于基线方法。

**Conclusion:** Spectrum方法展示了在多个人体解析任务中，通过调整和利用3D纹理生成模型的内部表示，可以提高对细粒度人体分割的性能，优于现有方法。

**Abstract:** Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [74] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

> A novel, fast, and effective text-guided image editing method named InstantEdit is proposed, leveraging RectifiedFlow framework, specialized inversion and regeneration strategies, and additional techniques for better detail preservation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind InstantEdit is to create a method that can quickly edit images based on textual instructions while maintaining the quality and critical content of the original image. This aims to improve upon current few-step editing methods.

**Method:** We propose a method called InstantEdit, which utilizes the RectifiedFlow framework for fast text-guided image editing. The method introduces a specialized inversion strategy named PerRFI and a regeneration method, Inversion Latent Injection, to ensure consistent and detailed regeneration. Additionally, it integrates Disentangled Prompt Guidance and a Canny-conditioned ControlNet to balance editability and preserve details.

**Result:** Evaluation on the PIE image editing dataset showed that InstantEdit not only achieves faster editing but also delivers superior qualitative and quantitative results compared to the state-of-the-art few-step editing methods.

**Conclusion:** The proposed InstantEdit method effectively combines rapid text-guided editing with high-quality results, outperforming existing few-step image editing techniques.

**Abstract:** We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [75] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

> 本文提出了一种基于“量多质优”原则的专家混合情绪识别系统，整合了多种输入模态，通过基于共识的伪标签策略和两阶段训练范式，提高了情绪识别的准确率，在MER2025挑战中取得第二名的成绩。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决MER2025的半监督学习问题，提出一个更加全面的框架，以创建一种更加鲁棒的专家混合情绪识别系统。

**Method:** 我们提出了一种基于“量多质优”原则的专家混合系统框架，整合了多种输入模态，包括来自大型视觉语言模型(VLMs)的知识和时间动作单元(AU)信息。我们还引入了一种基于共识的伪标签策略来有效利用未标记的数据，并采用了两阶段训练范式。最后，使用多专家投票集成结合基于规则的重新排序过程以校正预测偏差并更好地与人类偏好对齐。

**Result:** 在MER2025-SEMI挑战数据集上，我们的方法在测试集上实现了0.8772的F1得分，排名第二。

**Conclusion:** 实验结果证明了我们提出的方法的有效性，其代码已在GitHub上公开。

**Abstract:** In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [76] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

> Fourier-VLM 通过离散余弦变换在频域压缩视觉特征，实现了模型的高效性和高泛化能力，同时减少了推理 FLOPs 并提高了生成效率。

<details>
  <summary>Details</summary>

**Motivation:** Fourier-VLM 的动机在于解决传统视觉-语言模型中的视觉标记过多使上下文长度增加的问题，从而导致计算开销大和延迟高的问题。过去的方法通过选择重要的视觉特征或使用可学习查询来减少标记数量，但这会导致性能下降或引入额外的成本。而 Fourier-VLM 通过频域内的变换方法，有效解决了这些问题。

**Method:** Fourier-VLM 提出了一种简单而高效的方法，通过使用二维离散余弦变换（DCT）在频域中压缩视觉表示。这种方法利用了视觉编码器输出的视觉特征在低频分量中的集中能量。通过低通滤波，该方法不会增加额外的参数，并且计算效率很高，时间复杂度为 $\mathcal{O}(n\log n)$。

**Result:** 实验表明，Fourier-VLM 在各种基于图像的基准测试中展示了较强的泛化能力，并与 LLava 和 Qwen-VL 架构兼容。尤其相较于 LLaVA-v1.5，它减少了 83.8% 的推理 FLOPs 并将生成速度提高了 31.2%。

**Conclusion:** Fourier-VLM 方法在不降低性能的前提下，显著减少了视觉-语言模型中的计算复杂度和推理延迟，提高了生成效率。

**Abstract:** Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [77] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

> 为了改进文本引导的图像编辑方法，我们提出了NEP方法以及一个预先训练的T2I模型，实现了零样本的图像编辑，并在编辑基准上达到了新的最先进水平。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法通常会生成整个目标图像，而不是选择性地重新生成仅有的编辑区域。这不仅导致了不必要的计算成本，还使得非编辑区域的重建偏向影响了所需编辑的质量。

**Method:** 我们提出将图像编辑问题形式化为基于自回归图像生成的下一次编辑标记预测（NEP），这种方法只重新生成需要编辑的区域，避免了非编辑区域的无意修改。为此，我们提出了预训练一个可以任意顺序生成的自回归文本到图像（T2I）模型。一旦训练完成，它就能进行零样本图像编辑，并可以轻易地适应用于NEP的图像编辑任务。

**Result:** 我们的模型在广泛使用的图像编辑基准上实现了新的最先进水平。此外，我们的模型自然支持测试时放缩（TTS），通过零样本的方式迭代地改进生成效果。

**Conclusion:** 通过NEP方法和预先训练的T2I模型，我们解决了现有方法在图像编辑中的计算成本高和编辑质量差的问题。

**Abstract:** Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [78] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

> 提出VQAThinker，一种基于推理的视频质量评估框架，通过大规模多模态模型和强化学习解决现有VQA模型在面对分布外视频时的泛化问题和可解释性问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有VQA模型在面对分布外视频时的泛化能力和可解释性较差的问题。

**Method:** 采用组相对策略优化（GRPO）算法，引入了三个特定的VQA奖励：钟形回归奖励，成对排名奖励，时间一致性奖励。

**Result:** 实验表明，VQAThinker在领域内和领域外的VQA基准测试中实现了最先进的性能，显示了对视频质量评分的强大泛化能力。除此之外，该模型在视频质量理解和归因方面优于现有的可解释VQA模型和LMMs。

**Conclusion:** 强化学习为构建仅依靠评分水平监督的泛化性和可解释性的VQA模型提供了一种有效的途径。

**Abstract:** Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [79] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

> 提出LV-Net，一种新的框架，用于生成个性化的3D侧脑室网格，表现出较高的重建精度并能在各种数据集中提供可靠的形状描述符。该技术还识别出色的与阿尔茨海默病有关的侧脑室亚区域。

<details>
  <summary>Details</summary>

**Motivation:** 侧脑室形状分析在作为神经系统疾病生物标志物方面展现出潜力，但由于个体间形状变异性和MRI分辨率限制导致的分割困难，仍存在挑战。

**Method:** 引入了LV-Net，这是一种新的框架，用于从脑部MRI中生成个性化的3D侧脑室网格。通过改变一个解剖结构感知的联合侧脑室-海马体模板网格，该方法减少了边界分割伪影并提高了重建鲁棒性。通过根据模板网格顶点的解剖相邻性对其进行分类来增强点对应关系，这提高了跨受试者的侧脑室形状统计的准确性。

**Result:** LV-Net 在存在分割不完美的情况下实现了优越的重建精度，并在各种数据集中提供了更可靠的形状描述符。

**Conclusion:** LV-Net 被应用于阿尔茨海默病的分析，识别出与正常认知对照组相比较显示出显著关联的侧脑室亚区域。这些结果表明LV-Net在侧脑室分析中具有实用价值。

**Abstract:** Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [80] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

> 论文讨论了地球观测数据对AGI的重要性，指出现有的评估基准测试的不足，并提出了一套全面的任务集合以改进评估标准。

<details>
  <summary>Details</summary>

**Motivation:** 此论文的动机在于强调地球观测数据在增强AGI理解自然世界能力中的重要性。考虑到卫星光谱图像作为一种独特而未充分利用的模态，作者提出了一套全面的基准测试需求，以解决当前基准测试的局限性。

**Method:** 在论文中，作者首先阐述了为什么地球观测数据对于智能模型是有用的。接着，他们回顾了现有的基准测试，并指出了这些基准在评估基础模型在地球观测域中的泛化能力方面的局限性。为了促进这一领域的发展，作者提出了一套全面的任务集合，用于评估模型理解和处理地球观测数据的能力。

**Result:** 研究结果提出了改进地球观测模型评估的全面任务集合，强调了现有方法的局限性。

**Conclusion:** 结论认为，开发一个能够全面评估地球观测数据理解和处理能力的基准测试对于推进AGI在理解自然世界的整体性能具有重要意义。

**Abstract:** Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [81] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

> 研究人员开发了TSANet，一种轻量级设计的两阶段网络，对于处理基于事件相机（如HybridEVS）的数据在去马赛克过程中表现优越，能够提升移动摄影应用上的表现，同时显著降低了计算复杂度。

<details>
  <summary>Details</summary>

**Motivation:** 解决结合Quad Bayer彩色滤光片阵列（CFA）传感器和缺乏颜色信息的事件像素所带来的时间混叠和伪影问题，特别是在资源受限的移动设备上。

**Method:** TSANet, 一种通过状态空间增强交叉注意力的两阶段网络，该方法能够独立处理事件像素的插补和去马赛克，使得复杂任务能够被划分为可以管理的子任务。此外，引入了一种轻量级的交叉Swin状态块，该块利用位置先验信息进行去马赛克，并通过状态空间模型增强全局依赖性，具有线性复杂度。

**Result:** TSANet 在HybridEVS的模拟和真实数据上展示了出色的去马赛克性能，与当前的最先进方法DemosaicFormer相比，在七个不同的数据集上，TSANet在PSNR和SSIM上平均有更好的结果，同时将参数和计算成本分别减少了1.86倍和3.29倍。

**Conclusion:** 该方法为移动设备上高效的图像去马赛克打开了新的可能性。代码在补充材料中提供。

**Abstract:** Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [82] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

> 提出了一种名为SCJoint的新方法，可以实现显著物体检测和伪装物体检测任务的联合学习，并通过基于显著性的采样策略提高训练效率和质量。

<details>
  <summary>Details</summary>

**Motivation:** 显著物体检测和伪装物体检测是两种相关但不同的计算机视觉任务。尽管两者都是无关类别的分割任务，但前者旨在识别图像中最显著的物体，后者则专注于检测那些融入背景中的完美伪装物体。这两个任务表现出强烈的相反属性。以往的工作普遍认为同时学习这两个任务会混淆网络，降低其在两个任务上的表现。然而，本文提出了相反的观点：使用正确的学习方法，网络可以同时具备识别显著对象和伪装对象的能力，使得两个任务可以从联合学习中受益。

**Method:** 提出了一种名为SCJoint的联合学习方案来同时处理显著物体检测和伪装物体检测任务。该方法假设两任务的解码过程具有不同的分布特性，通过在网络中插入少量任务特定的可训练参数来学习两任务的解码过程各自的均值和方差，从而以极小的代价解耦两个任务的相互矛盾的属性。此外，还提出了一种基于显著性的采样策略(SBSS)来对显著物体检测任务的训练集进行采样，平衡两个任务训练集的规模，SBSS还提高了训练集的质量并缩短了训练时间。基于SCJoint和SBSS，训练了一个名为JoNet的强大通用网络，该网络具有同时捕获“显著”和“伪装”的能力。

**Result:** 广泛的实验结果展示了所提出方法的竞争力和有效性。

**Conclusion:** 实验表明，本文提出的方法表现出了强大的性能和有效性。代码已公开。

**Abstract:** Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [83] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

> 介绍了一个名为BioMotion Arena的新框架，通过视觉动画来评估大型语言模型和多模态大型语言模型，并发现超过90%的评估模型在生成人形点光源动作方面存在严重不足，证明BioMotion Arena是一个有效且灵活的评估框架。

<details>
  <summary>Details</summary>

**Motivation:** 目前的评估基准要么基于静态数据集上的ground-truth得分评估，要么通过模糊的文本聊天机器人式的用户偏好收集，无法为用户提供直接、直观且可感知的性能差异反馈。本论文旨在解决这一问题。

**Method:** 引入了BioMotion Arena，一个通过视觉动画评估大型语言模型(LLMs)和多模态大型语言模型(MLLMs)的新框架。此方法借鉴了生物运动的视觉感知特点，使用点光源成像来放大不同模型的性能差异。具体来说，采用成对比较的方法对53个主流LLMs和MLLMs的90种生物运动变体进行了评估，收集了超过45000个投票。

**Result:** 数据分析表明，众包的人类投票与专家评分者的结果有很好的一致性，证明了BioMotion Arena在提供有区分度的反馈方面的优越性。结果显示超过90%的评估模型无法生成基本的人形点光源群，更不用说流畅且具有生物合理性的运动。

**Conclusion:** BioMotion Arena可以作为性能可视化的有挑战性的基准，并作为一个不受ground-truth限制的灵活评估框架。

**Abstract:** Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [84] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

> 本文提出了一种基于临床MRI生成超分辨率3D目标形态的方法，用于提升Trochlear Dysplasia的治疗效果，显著改善了凹槽角度和深度。

<details>
  <summary>Details</summary>

**Motivation:** 目前针对Trochlear Dysplasia (TD)的治疗方法主要依赖低分辨率的临床MRI扫描和外科直觉，手术计划基于外科医生的经验，微创技术的应用有限，并导致结果不一致。本研究旨在提高这些手术的精度和一致性。

**Method:** 本研究提出了一种基于临床MR扫描生成超分辨率、个性化3D伪健康目标形态的流程。首先，使用隐式神经表示(INR)计算等向性超分辨率MR体积。然后，使用多标签定制训练的网络分割股骨、胫骨、髌骨和腓骨。最后，训练一个Wavelet扩散模型(WDM)生成膝关节凹槽区域的伪健康目标形态。

**Result:** 该研究在25例TD患者中进行了评估，结果表明其生成的目标形态显著改善了凹槽角度（SA）和膝关节凹槽深度（TGD）。

**Conclusion:** 本研究提出的方法为TD患者提供了一种无需CT即可生成亚毫米级别3D形态的解决方案，这在术前和术中都具有重要的应用价值，并有助于减少辐射暴露。

**Abstract:** To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [85] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

> 为了克服指令驱动的视频编辑技术所面临的训练数据不足的问题，我们介绍了一个名为DreamVE的统一模型，能够实现基于指令的图像和视频编辑。

<details>
  <summary>Details</summary>

**Motivation:** 指令驱动的编辑技术由于其简单和高效的交互编辑格式有着巨大的潜力，但在视频编辑领域，由于训练数据的限制，其实际应用受到了阻碍。我们的目标是提出一个解决这一问题的方案。

**Method:** 我们提出了一个两级训练策略：首先进行图像编辑，然后进行视频编辑。这种策略使模型能够利用图像数据规模更大、训练更高效的优点，从而提供有用先验，强化视频编辑的训练。此外，我们提出了全面的训练数据合成流水线，包括拼贴数据合成和生成模型数据合成。

**Result:** 采用了拼贴数据合成技术对模型进行预训练后，DreamVE在一个关键编辑类型中表现良好，概括能力得到增强。同时，利用生成模型数据进一步微调DreamVE，以克服拼贴数据在处理属性修改方面的不足。

**Conclusion:** DreamVE作为一个综合的图像和视频编辑模型，通过两级训练策略充分利用图像数据和视频数据的特性，结合多种数据合成流水线，取得了很好的编辑效果，并且提高了泛化和迁移能力。

**Abstract:** Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [86] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

> 本文提出了SwiftVideo框架，旨在解决现有视频生成模型计算开销大的问题，并提出一系列改进方法，使得在减少推理步骤的情况下仍能维持高质量的视频生成效果。

<details>
  <summary>Details</summary>

**Motivation:** 扩散模型或流模型在视频合成方面取得了显著进展，但需要多次迭代采样步骤，导致计算开销大。现有的蒸馏方法仅基于轨迹保持或分布匹配策略来加速视频生成模型，这些方法在少量采样步骤下经常出现性能下降或增加伪影的问题。

**Method:** SwiftVideo 是一个结合了轨迹保持和分布匹配优势的统一且稳定的蒸馏框架。它引入了连续时间一致性蒸馏以确保精确的ODE轨迹保留，并提出了双视角对齐，其中包括合成数据和真实数据间的分布对齐以及不同推理步骤间的轨迹对齐。

**Result:** 在OpenVid-1M基准上的定量评估表明，该方法在少步视频生成方面显著优于现有方法，同时大幅减少了推理步骤。

**Conclusion:** SwiftVideo框架通过结合连续时间一致性蒸馏和双视角对齐策略，能够在减少视频生成的推理步骤的同时，不仅维持了高质量的视频生成，还显著优于现有方法。

**Abstract:** Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [87] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

> 本文提出了一个五步数据合成流水线来生成有效图表数据集(ECD)，该数据集包括10k+图表和300k+ QA对，可用于训练多模态语言模型，显著提升了模型对科学图表的理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态大型语言模型(MLLMs)在理解和分析科学图表方面表现不佳，成功率只有30%-50%。过时的合成图表与真实的科学图表不相似，这限制了模型在真实图表上的训练及表现。

**Method:** 我们设计了一个五步数据合成流水线，包括单个图表生成时的数据和函数创建分离，在多子图图形生成时依据先前子图生成后续子图，视觉多样化生成图形，过滤低质量数据，最后使用GPT-4生成问题-答案(QA)对。

**Result:** 我们引入了有效图表数据集(ECD)，包含10k+图表和超过300k的QA对，涵盖了25个主题和250种图表类型，具有高视觉复杂度。实验显示，ECD可以提升各种MLLMs在真实和合成测试集上的性能。

**Conclusion:** 通过模块化图表生成和多样化视觉细节，可以提高多模态大型语言模型在科学图表理解上的表现。

**Abstract:** Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [88] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

> AdaptInfer proposes a dynamic text-guided mechanism for adaptive vision token pruning in visual-language models, reducing inference cost significantly without sacrificing accuracy.

<details>
  <summary>Details</summary>

**Motivation:** To reduce the inference cost of Vision-Language Models (VLMs) by improving upon static pruning methods through dynamic, evidence-driven token management.

**Method:** First, introduce a fine-grained, dynamic text-guided pruning mechanism for vision tokens in VLMs, which utilizes layer-wise text-to-text attention maps to inform pruning decisions. Second, conduct offline analysis of cross-modal attention shifts to establish a more efficient pruning schedule.

**Result:** The proposed method reduces CUDA latency by 61.3% while maintaining 92.9% accuracy on the LLaVA-1.5-7B model, outperforming existing state-of-the-art techniques under the same token budget.

**Conclusion:** AdaptInfer is an effective, lightweight, and generalizable solution for reducing the computational cost associated with the inference process in visual-language models, delivering a significant performance improvement in terms of latency reduction while preserving high accuracy.

**Abstract:** Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [89] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

> 新型Q-CLIP框架基于视觉语言模型，解决了传统视频质量评估方法在成本和效果上的局限性，经实验验证该方法在多个数据集上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 面临视频质量评估研究中现有的基于大规模预训练的挑战，如计算资源消耗巨大且仅依赖于语义知识不足，该研究旨在开发一种更高效、更具成本效益的VQA方法。

**Method:** 提出Q-CLIP，利用视觉语言模型并通过共享跨模态适配器（SCMA）来最小化计算资源需求。此外，使用质量级别提示来优化模型对于视频质量变化的感知能力。

**Result:** 该论文提出了一种名为Q-CLIP的新框架，该框架基于视觉语言模型（VLMs），用于视频质量评估（VQA）。与当前通过预训练大规模分类数据集然后在VQA数据集上微调的方法相比，Q-CLIP通过共享跨模态适配器（SCMA）增强视觉和文本表示，并使用少量可训练参数，显著减少了计算成本。此外，引入了五个可学习的质量级别提示来帮助模型感知细微的质量变化。实验表明，Q-CLIP在多个VQA数据集中表现出色。

**Conclusion:** 研究表明，在视频质量评估中使用基于视觉语言模型的方法（如Q-CLIP）不仅能够提高评估效果，还能显著减少计算资源需求，新的帧采样策略也进一步提升了模型性能的泛化能力。

**Abstract:** Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [90] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

> 本文提出了一种利用半监督情感先验，在演员-反应者扩散模型基础上生成情感驱动反应的新方法，克服了现有运动生成框架的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 现有运动生成框架未考虑情感的影响，导致生成的运动不自然且在交互任务中的应用受限。本文旨在通过引入情感因素来改善这一点。

**Method:** 本文介绍了一种新的任务：根据不同的情感线索生成多样化的反应动作。为应对从有限的运动数据学习情感表达并将其融入运动生成框架的挑战，本文提出了一种在行为者-反应者扩散模型中引入半监督情感先验的方法，以促进情感驱动的反应合成。通过观察短序列内的运动片段共享相似的情感，本文开发了一种半监督学习框架来训练情感先验。基于这一先验，进一步训练模型综合考虑空间交互和情感反应来生成反应动作。

**Result:** 实验结果表明，该模型在反应生成方面优于现有方法。

**Conclusion:** 通过半监督学习框架和情感驱动的反应合成方法，本文的方法能够生成更自然、真实的反应动作，并且在多种情感条件下表现出色。

**Abstract:** Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [91] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

> 我们提出了一个新的基于扩散模型的生成框架UGD-IML，统一了IML和CIML任务，在多个数据集上表现出色，显著超越现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现代图像编辑工具对视觉内容的完整性构成威胁，而现有图像篡改检测和定位方法依赖于大量高质量标注，但当前数据集规模和多样性不足，限制了模型在实际场景中的表现。为克服这些不足，我们提出了一种新的图像篡改定位方法。

**Method:** 我们提出了一种基于扩散模型的生成框架，名为UGD-IML，首次将IML和CIML任务统一在单一框架中。通过学习基本数据分布，该生成模型降低了对大规模标注数据的依赖，并在有限数据条件下表现良好。此外，利用类别嵌入机制和参数共享设计，该模型能在IML和CIML之间无缝切换，无需额外组件或训练开销。端到端的设计使该模型在数据标注过程中避免复杂步骤。

**Result:** 针对多个数据集进行的大量实验表明，UGD-IML显著超越当前最佳方法，在F1指标上分别超过了9.66和4.36，同时在不确定性估计、可视化和模型鲁棒性方面表现优异。

**Conclusion:** 实验结果显示，UGD-IML在IML和CIML任务上的F1指标分别超过了现有最佳方法9.66和4.36。此外，该方法在不确定性估计、可视化和鲁棒性方面的表现也优于现有方法。

**Abstract:** In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [92] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

> 本文提出了一个鲁棒的2D-3D跨模态自适应校正和对齐框架（MCA），通过多模态联合标签校正（MJC）机制实现可靠的标签修正，并通过多层级自适应对齐（MAA）策略提升跨模态特征语义和区分度。实验表明MCA在有噪声的3D数据上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 随着2D和3D数据的日益丰富，跨模态检索方面有了显著的进步，但不完美标注的存在提出了挑战，特别是在噪声标签条件下的2D-3D跨模态检索。现有的方法通常通过在每个模态内独立分割样本解决噪声问题，容易对受损标签过拟合。

**Method:** 我们提出了一种鲁棒的2D-3D跨模态自适应校正和对齐框架（MCA），包括多模态联合标签校正（MJC）机制和多层级自适应对齐（MAA）策略。MJC机制通过多模态历史自预测来联合建模模态预测一致性，实现可靠的标签修正。MAA策略有效提升不同层级上的跨模态特征语义和区分度。

**Result:** 大量实验表明，我们提出的方法MCA在常规和实际噪声3D基准测试中均达到了最先进的性能，凸显了它的通用性和有效性。

**Conclusion:** MCA框架在这项工作中证明了其作为一个有效且通用的解决方案，适用于2D-3D跨模态检索中的噪声标签问题。

**Abstract:** With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [93] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

> 研究提出了一种用于手写数学表达式识别的自我监督学习框架，该框架无需昂贵的标注数据，且在多个实验中显示出优越性。

<details>
  <summary>Details</summary>

**Motivation:** 由于手写数学表达式识别任务具有二维结构、符号尺寸变化和复杂的符号间空间关系等挑战，该研究旨在提出一种自我监督学习框架，以减少昂贵的标注数据需求。

**Method:** 我们的方法包括三个步骤：(1) 使用全局和局部对比损失进行图像编码器的自监督预训练，(2) 使用逐渐空间掩码策略训练自监督注意力网络，以学习有意义的聚焦区域，(3) 使用带Transformer解码器的监督微调生成LATEX序列。

**Result:** 在CROHME基准测试中，我们的方法优于现有的自我监督和全监督基线，验证了渐进式注意机制在提高手写数学表达式识别性能中的有效性。

**Conclusion:** 本研究通过自监督预训练和注意机制成功提高了手写数学表达式的识别效果，并证明渐进式注意机制的有效性。

**Abstract:** Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [94] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

> 该研究通过FMCE-Net++框架，利用特征图收敛分数(FMCS)辅助评估机制，增强了模型的训练效果，提高了测试数据集上的准确率，没有对模型结构或数据量进行变更。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在解决深度神经网络(DNNs)由于其不透明的内部表示而面临的可解释性挑战，特别指出现有的FMCE方法缺乏实验验证和闭环集成，从而提出改进方法。

**Method:** 该论文提出了一种新的训练框架FMCE-Net++，它将预训练好的FMCE-Net作为辅助头集成，生成的特征图收敛分数(FMCS)预测与任务标签一起监督主干优化，通过表示辅助损失(RAL)和可调的“表示抽象因子”动态平衡主要的分类损失和特征收敛优化。

**Result:** 在MNIST、CIFAR-10、FashionMNIST和CIFAR-100数据集上的实验表明，FMCE-Net++能够持续提高模型性能，而无需对架构进行修改或增加额外的数据。特别是在ResNet-50和CIFAR-10上的准确率提高了1.16个百分点，在ShuffleNet v2和CIFAR-100上的准确率提高了1.08个百分点。

**Conclusion:** 实验结果证明了FMCE-Net++能够有效地提升当前最先进的性能上限。

**Abstract:** Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [95] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

> 本文介绍了 GMF-Drive，提出了一种新的模型框架，通过增强的几何和高效的空间感知状态空间模型，克服了现有基于扩散模型中 Transformer 融合的局限性，实现了自动驾驶的新一代性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基于扩散的模型正在重塑端到端自动驾驶的前沿，但它们的性能越来越受到基于 Transformer 融合的限制。Transformer 架构面临根本性的限制，如二次计算复杂度限制了高分辨率特征的使用，以及缺乏空间先验，阻碍了 Bird's Eye View (BEV) 表示的有效建模。

**Method:** GMF-Drive 提出了一种端到端的框架，解决了依赖于 Transformer 融合的局限性。第一，用几何增强的柱体格式替换了信息量较低的基于直方图的 LiDAR 表示，保留了关键的 3D 几何细节。第二，设计了一种新颖的层次化门控 Mamba 融合架构，用高效的空间感知状态空间模型替换了昂贵的 Transformer。

**Result:** 在具有挑战性的 NAVSIM 数据集上进行的广泛实验表明，GMF-Drive 达到了新的最先进的性能，显著超越了 DiffusionDrive。详细的消融研究验证了每个组件的有效性。

**Conclusion:** 针对自动驾驶任务，特异性 SSM 在性能和效率上都比通用 Transformer 更占优势。

**Abstract:** Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [96] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

> 提出了SynSeg方法，通过多类别对比学习和特征协同结构，增强了弱监督下语义分割任务的表现，在多个数据集上超过了现有最优方法。

<details>
  <summary>Details</summary>

**Motivation:** 针对开放词汇场景中语义分割的挑战，特别是现有弱监督方法效果不佳的问题，提出了SynSeg，试图解决语义不对齐和表现力差的难题。

**Method:** SynSeg采用多类别对比学习(MCCL)作为训练信号，并使用特征协同结构(FSS)框架来重新构建对比学习中的判别特征，改善了视觉编码器带来的前景偏置问题。

**Result:** 

**Conclusion:** SynSeg在弱监督条件下显著提升了语义定位和区分能力，并在多个基准测试中超越了现有最佳方法的性能。

**Abstract:** Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [97] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

> 研究应用几种表示学习算法于卫星图像的天气事件分类上，实验显示，CAE取得了最佳的整体表现，而PT在热带气旋识别中表现突出。进一步的研究指出从高分辨率数据学习表示更好，且潜空间的大小会影响误报率。建议未来开发具备物理信息的CAE。

<details>
  <summary>Details</summary>

**Motivation:** 目的是探索用表示学习算法来提高卫星图像上天气事件分类的准确性，并分析不同算法的性能差异以及影响算法表现的因素。

**Method:** 应用了表示学习算法到卫星图像上，并评估了学习到的潜空间在各种天气事件分类上的表现。研究的算法包括经典的线性变换方法PCA，先进的深度学习方法卷积自编码器CAE，以及在大规模图像数据集上预训练的残差网络PT。

**Result:** 实验结果表明，CAE学习到的潜空间在所有分类任务中的威胁评分最高。PCA分类有着高击中率但也有高误报率。PT在识别热带气旋方面表现出色，但在其他任务上的表现较差。进一步的实验表明，从高分辨率数据集中学习到的表示在深度学习算法，即CAE和PT的分类任务上表现出色。我们还发现，相对较小的潜空间尺寸对分类任务的击中率影响不大，但是潜空间维度小于128时会导致误报率显著升高。

**Conclusion:** 虽然CAE能够有效且高效地学习潜空间，但学习到的表示与物理属性之间缺乏直接联系。因此，开发具有物理信息的CAE版本是未来工作的有希望的方向。

**Abstract:** This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [98] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

> 本文提出SC-Captioner框架，用于增强图像描述模型的自我修正能力，通过精心设计的奖励函数和改进的评估指标提高描述质量，并展示其实验效果优于直接偏好优化策略。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在提出一种新的方法来改善图像描述模型的自我修正能力，通过设计精妙的奖励函数和改进的评估指标来提升图像描述的质量。

**Method:** 本文提出了一种名为SC-Captioner的强化学习框架，该框架实现了图像描述模型的自我修正能力。框架的关键技术在于设计奖励函数来激励准确的描述修正。具体来说，预测的和参考的描述通过场景图解析算法被分解成对象、属性和关系三部分。通过计算初始描述和自我修正描述之间的集合差异，识别出新增和删除的元素，并将这些元素与参考集相匹配，从而计算出正确的修正奖励和错误增删的惩罚。

**Result:** 实验表明，SC-Captioner框架提高了图像描述质量，尤其在应用到大型视觉语言模型时表现出色，显著优于直接偏好优化训练策略。

**Conclusion:** 研究表明，本研究提出的SC-Captioner框架在改善图像描述质量和增加自我修正能力方面具有显著效果。

**Abstract:** We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [99] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

> VeSCA, a method that uses the encoder of the Segment Anything Model (SAM) to generate transferable adversarial examples by characterizing shared vulnerable regions through a parametric simplicial complex, demonstrates improved performance by 12.7% over existing methods.

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the transferable vulnerabilities of SAM that could cause the failure of downstream applications, and to address the limited transferability of previous adversarial attacks on SAM.

**Method:** Proposes VeSCA, which generates transferable adversarial examples by characterizing shared vulnerable regions using the encoder of SAM through a parametric simplicial complex.

**Result:** Experiments show VeSCA achieves a performance improvement of 12.7% over state-of-the-art methods across different downstream models and datasets.

**Conclusion:** Highlights the significant risks to downstream models due to SAM's inherent vulnerabilities and underscores the need for developing more robust models.

**Abstract:** While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [100] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

> 提出了一种新颖的基于3D眼球结构的注视方向重定向框架，该方法生成高度逼真的图像并显式地旋转和移动眼球结构，优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的注视方向重定向方法通常基于神经辐射场，该方法使用隐式神经表示通过体积渲染实现。这些方法中3D表示的旋转和平移没有被明确建模。

**Method:** 引入了专门的3D眼球结构，使用3D Gaussian Splatting（3DGS）来表示眼球，并通过显式旋转和移动3D眼球结构来生成逼真的图像以忠实再现期望的注视方向。还提出了一种自适应变形模块，能够复制眼睛周围的细微肌肉运动。

**Result:** 通过对ETH-XGaze数据集进行实验，证明了该框架能够生成多样化的新注视图像，并且在图像质量和注视估计准确性方面优于现有的方法。

**Conclusion:** 实验证明，该框架能够在生成不同注视方向的图像时，保持高质量和准确的注视估计，优于现有的state-of-the-art方法。

**Abstract:** We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [101] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

> 该论文提出了一种基于扩散模型的方法，结合稀疏IMU和单目相机信息进行实时人体动作捕捉，实验表明该系统设计有效，并在姿态估计方面优于先前的工作。

<details>
  <summary>Details</summary>

**Motivation:** 该论文的动机在于利用稀疏IMU和单目相机相结合的新型且有潜力的设置来进行实时人体动作捕捉。通过结合两种信号的特点，提高在视觉信息偶尔退化时的鲁棒性及利用IMU测量在遮挡情况下的稳定性。

**Method:** 该论文提出了一种基于扩散模型的解决方案，用于学习人体动作先验并将两种信号模态无缝融合到一个统一的框架中。视觉信息被作为一个整体考虑并转化为条件嵌入，而惯性测量则逐帧与有噪声的身体姿态拼接，以构建扩散模型的序列输入。

**Result:** 实验已经验证了系统设计的有效性，并且与先前的工作相比，在姿态估计方面达到了最先进的性能。

**Conclusion:** 本研究提出了一种结合视觉和惯性测量信号的人体动作捕捉框架，实现了在实时场景下的高效和高准确度的动作捕捉，展现了该方法在动作捕捉领域的潜力。

**Abstract:** Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [102] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

> 提出SDEval框架，动态调整多模态大语言模型的安全性基准，实验证明其有效。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态大语言模型的安全性关注日益增加，但是现有的数据集随着模型的进步可能变得过时，且易受数据污染。为了应对这些问题，提出了SDEval框架。

**Method:** SDEval采用三种动态策略：文本动态、图像动态以及图文动态策略，来生成新的安全基准样本。首先，研究了文本和图像动态对模型安全性的独立影响，然后探讨了文本动态注入图像与图像动态注入文本对安全性的进一步影响。

**Result:** 实验证明SDEval显著影响安全性评估，缓解数据污染，并揭示了MLLMs的安全限制。

**Conclusion:** SDEval具有通用性，能应用于多个现有的安全性和能力基准，显示出在安全评估中的重要作用。

**Abstract:** In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [103] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

> 本文提出了Prompt-DINO框架，解决多模态视觉模型中的几个关键问题，并在开放式场景检测中展示出优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决多模态视觉模型中后期特征融合的局限性、混合提示开放式分割中的次优查询选择和由标题导出的词汇约束，从而提高开放式场景中的多模态检测和数据生成能力。

**Method:** 提出了Prompt-DINO框架，该框架包括三种关键技术：1.早期融合机制，2.结构对齐的query选择，3.基于提示生成的0.5B数据合成引擎。

**Result:** 实验结果表明，Prompt-DINO在开放式检测基准测试中达到了最先进的性能，并且显著扩大了语义覆盖范围，超越了固定词汇约束。

**Conclusion:** 这项工作为开放式场景中的可扩展多模态检测和数据生成建立了新的范式。

**Abstract:** Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [104] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

> 本文提出了一种名为DSConv的新策略，通过动态分割卷积核并结合注意力机制，以提高多光谱图像和全色图像融合的效果，从而获得高分辨率图像。实验表明，该方法在遥感图像融合任务中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 传统的泛锐化方法主要依赖标准卷积，很少采用自适应卷积。考虑到遥感图像像素间的相关性，作者提出了一种新的策略，希望能在低层次视觉任务中，特别是在多光谱图像和全色图像融合方面，取得更好的效果。

**Method:** 本文提出的方法被称为DSConv，它结合了动态分割卷积核和注意力机制，可以在感受野内不同位置更有效地提取特征。这是通过选择感兴趣的位置，并将原始卷积核拆分成多个更小的核来实现的。

**Result:** 实验表明，提出的方法在泛锐化任务中表现优异，超过了当前的其他方法的性能。

**Conclusion:** 本文提出了一种创新的卷积策略DSConv，该策略通过动态分割卷积核和结合注意力机制，有效改善了网络的泛化、优化和特征表示能力。

**Abstract:** Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [105] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

> VISTAR是一个用户为中心的多维文本到图像（T2I）评估基准，采用两级混合范式评估特定和抽象属性，包含2845个验证提示和超过15000个人类比较，显示了高的人类一致性，并揭示了没有单一的最好的模型。

<details>
  <summary>Details</summary>

**Motivation:** VISTAR旨在解决现有T2I指标的局限性，通过结合确定性评估和基于约束的视觉语言模型来评估抽象语义，从而提供一个全方位的评估框架。

**Method:** VISTAR采用了一种两级混合范式来评估文本到图像(T2I)的性能。第一阶段使用确定性的可脚本化的指标来评估可量化属性（如文本渲染、照明等）。第二阶段引入了一种新的层次加权P/N问答（HWPQ）方案来评估抽象语义（如风格融合、文化保真度）。

**Result:** VISTAR达到了大于75%的人类一致性，特别是HWPQ方案对抽象语义的准确性达到了85.9%，超过了VQA基线，没有模型能在所有角色加权分数中胜出。

**Conclusion:** VISTAR通过引入新的评估方法，提供了一个用户为中心的多维T2I评估框架，旨在促进可重复的T2I性能评估，所有资源都是公开的。

**Abstract:** We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [106] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

> 本文提出了一种名为MPF-KANSC的框架，用以改进现有的深度学习方法，以实现更准确的阿尔茨海默病诊断。实验结果表明，该模型在AD诊断方面具有优越性能，同时提供新的亚皮层结构变化不对称性的证据，强调了模型的解释性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大多数深度学习方法只关注sMRI的单一平面，无法准确捕捉脑部病理区域之间的复杂非线性关系，限制了识别萎缩特征的能力。因此，该研究旨在解决这一局限性，实现更准确的阿尔茨海默病(AD)诊断。

**Method:** 提出了一种创新框架MPF-KANSC，该框架结合了多平面融合(MPF)和Kolmogorov-Arnold网络引导的空间-通道注意力机制(KANSC)，以更有效地学习和表示sMRI萎缩特征。MPF可以使模型并行抽取多个解剖平面中的特征，而KANSC注意力机制则进一步采用了一种更为灵活和准确的非线性函数近似技术，有助于疾病相关异常的精确识别和定位。

**Result:** 在ADNI数据集上的实验表明，提出的方法MPF-KANSC在AD诊断方面具有优越的性能。另外，研究发现提供了AD进展期间右侧化不对称性的新证据。

**Conclusion:** 综上所述，MPF-KANSC框架通过结合MPF和KANSC注意力机制提高了AD诊断的准确性，并为理解AD进展期间亚皮层结构的右侧化不对称性提供了新的见解。

**Abstract:** Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [107] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

> 研究发现，通过PostDiff框架，在资源有限的平台部署扩散模型时，降低每步骤推断成本比减少去噪步骤数量更有效。

<details>
  <summary>Details</summary>

**Motivation:** 探究在不进行微调的情况下，减少去噪步骤数量还是降低每步骤推断成本更有效的方法，以优化扩散模型的部署。

**Method:** PostDiff框架通过减少预训练扩散模型中的冗余来加速模型，包括在输入层面提出混合分辨率去噪方案，以及在模块层面采用混合模块缓存策略。

**Result:** 实验表明PostDiff显著提高了现有最佳扩散模型的保真度和效率之间的平衡。同时，在保持良好生成保真度的情况下，降低每步骤推断成本通常比减少去噪步骤数量更为有效。

**Conclusion:** PostDiff框架能够在保持或提高生成保真度的同时，大幅提高扩散模型的效率。

**Abstract:** Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [108] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

> The paper presents UW-3DGS, an innovative 3D scene reconstruction method for underwater environments, which effectively handles challenges such as light absorption and scattering, achieving superior results on datasets.

<details>
  <summary>Details</summary>

**Motivation:** to address the challenges of light absorption, scattering, and turbidity in traditional underwater 3D scene reconstruction methods like Neural Radiance Fields (NeRF), which degrade geometry and color fidelity.

**Method:** introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for robust underwater reconstruction, including a plug-and-play learnable underwater image formation module using voxel-based regression for spatially varying attenuation and backscatter, and a Physics-Aware Uncertainty Pruning (PAUP) branch for artifact-free geometry.

**Result:** experiments on SeaThru-NeRF and UWBundle datasets show superior performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with a ~65% reduction in floating artifacts.

**Conclusion:** the UW-3DGS framework achieves robust underwater 3D scene reconstruction with improved geometry and color fidelity, effectively addressing the limitations of previous methods in hazy underwater environments.

**Abstract:** Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [109] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

> The research introduces a multidirectional architectural framework for automating polyp detection in colonoscopy images, using synthetic data generation and a combination of Faster R-CNN for localization and the Segment Anything Model (SAM) for segmentation refinement, achieving high accuracy and recall rates in various models evaluated.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the early detection of colorectal cancer through better polyp detection during colonoscopies, overcoming challenges of limited healthcare data and annotation complexities.

**Method:** The method utilizes a comprehensive system that generates synthetic data with Stable Diffusion, along with detection and segmentation algorithms. It combines Faster R-CNN for object localization and SAM for refining segmentation masks.

**Result:** The study demonstrates that Faster R-CNN has a recall of 93.08% and precision of 88.97%, while SAM is used to generate image masks. Among several evaluated segmentation models (U-Net, PSPNet, FPN, LinkNet, and MANet), FPN showed the best performance.

**Conclusion:** The research concludes that the proposed multidirectional framework, combining Faster R-CNN and SAM, effectively enhances polyp detection accuracy and complemented by the evaluation of different segmentation models, shows promising results for improving colorectal cancer early detection through colonoscopies.

**Abstract:** Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [110] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

> 本文提出了一种创新的基于地面特征的机器人定位框架，采用图结构和图卷积网络，克服了传统方法的局限性，实现了更准确和有效的定位。

<details>
  <summary>Details</summary>

**Motivation:** 传统的定位方法如激光雷达或二维码系统在复杂环境中存在扩展性和适应性限制，作者提出了一种基于地面特征的创新定位框架以解决这些问题。

**Method:** 该框架利用图结构表示地板特征，并使用图卷积网络（GCN）进行建模。这种方法通过用图来表示地板上的特征，进行更准确和有效的定位，而无需复杂的滤波过程。

**Result:** 提出的框架能更准确地（误差0.64厘米）定位机器人，并解决“被绑架机器人问题”中的定位挑战。

**Conclusion:** 此方法为多样环境中的机器人导航开辟了新的可能性。

**Abstract:** Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [111] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

> 研究提出MA-CBP框架，通过多智能体异步协作实现实时犯罪行为预测，解决了传统方法的问题，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 由于城市化进程的加快，传统基于特征识别的异常行为检测方法难以从历史信息中捕获高级行为语义，而基于大型语言模型（LLMs）的方法又常常无法满足实时要求。因此，该文旨在解决这些挑战。

**Method:** 该论文提出了一种基于多智能体异步协作的犯罪行为预测框架（MA-CBP），将实时视频流转换为基于帧的语义描述，构建因果一致的历史摘要，并融合相邻图像帧以进行长时间和短时间上下文的联合推理。

**Result:** 实验结果表明，该方法在多个数据集上表现出色，并为城市公共安全场景的风险预警提供了一个有前景的解决方案。

**Conclusion:** 该研究通过提出MA-CBP框架，有效地结合了实时语义描述与因果历史记录，显著提升了犯罪行为预测的准确性和实时性。

**Abstract:** With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [112] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

> 提出了DBIF-AUNet模型，用于改善胸腔积液CT图像的语义分割。该模型通过DDFD和BIAF模块实现多尺度特征互补和动态特征融合，嵌套深度监督机制进一步优化了分割效果。与U-Net++和Swin-UNet相比，DBIF-AUNet在IoU和Dice分数方面有显著提高。

<details>
  <summary>Details</summary>

**Motivation:** 胸腔积液语义分割可以显著提高临床诊断和治疗的准确性和及时性，通过精确识别疾病严重程度和病变区域。目前，胸腔积液CT图像的语义分割面临着多种挑战。这些挑战包括积液和周围组织之间的灰度相似、边缘模糊以及形态变异。现有方法通常难以处理图像变异和复杂边缘，主要是由于直接特征级联造成了语义差距。为了解决这些问题，提出了本方法。

**Method:** 提出了双分支交互融合注意力模型(DBIF-AUNet)。该模型构建了一个密集嵌套跳跃连接网络，并创新性地优化了双域特征解耦模块（DDFD）。DDFD模块正交解耦了双域模块的功能，以实现多尺度特征互补并增强不同层次的特征。同时设计了分支交互注意力融合模块(BIAF)，与DDFD模块协同工作。该模块动态权重融合全局、局部和频率带特征，从而提高分割健壮性。此外，我们实现了一种嵌套深度监督机制，采用层次自适应混合损失来有效解决类别不平衡问题。

**Result:** 在西南医院的1,622张胸腔积液CT图像上进行验证，DBIF-AUNet实现了IoU和Dice分数分别为80.1%和89.0%。这些结果优于最先进的医学图像分割模型U-Net++和Swin-UNet，分别高出5.7%/2.7%和2.2%/1.5%，展示了在复杂胸腔积液CT图像分割中准确性方面的显著优化。

**Conclusion:** 该研究提出了DBIF-AUNet模型，能够更好地解决胸腔积液CT图像语义分割中的挑战，提高了准确性和鲁棒性。该模型在实际数据集上的测试表明，其性能优于现有的其他先进模型，并有效地处理了类别不平衡的问题。

**Abstract:** Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [113] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

> 我们引入了LiLoRA以解决MLLMs在CVIT过程中由于架构扩展而导致的参数开销大和可扩展性差的问题，结果表明LiLoRA在保持性能的同时提高了参数效率。

<details>
  <summary>Details</summary>

**Motivation:** Continual Visual Instruction Tuning (CVIT)使多模态大型语言模型（MLLMs）能够随着时间的推移逐步学习新任务。然而，这个过程会受到灾难性遗忘的挑战，即模型在适应新任务时，之前任务的性能会退化。现有的方法通常会对每个任务扩展整个层，导致显著的参数开销，并影响可扩展性。

**Method:** 通过引入LiLoRA（LoRA in LoRA），我们提出了一种专门针对MLLM中CVIT的高度有效的架构扩展方法。LiLoRA在任务之间共享LoRA矩阵A以减少冗余，对矩阵B应用额外的低秩分解以最小化任务特定参数，并引入了余弦正则化稳定损失以保持共享表示随时间的一致性。

**Result:** 实验结果表明，LiLoRA在一系列针对不同任务的CVIT基准实验中，在保持性能的同时，相对于现有方法显著提高了参数效率。

**Conclusion:** 在一系列针对不同任务的CVIT基准实验表明，LiLoRA在顺序任务学习中始终实现了优越的性能，同时与现有方法相比，显著提高了参数效率。

**Abstract:** Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [114] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

> 提出AnomalyMoE框架，通过三层语义划分和对应专家网络处理，实现了跨领域异常检测的通用性，实验表明其在多种类型的数据集上表现最优。

<details>
  <summary>Details</summary>

**Motivation:** 传统的异常检测方法往往高度专业化，限制了通用性。这些特定模型在它们的指定背景之外部署时通常表现不佳，因此需要一种能够广泛适用于不同类型的异常检测的通用方法。

**Method:** 本文提出了AnomalyMoE，一种基于专家混合模型的通用异常检测框架。它将复杂的异常检测问题分解为三个语义层次：局部结构异常、组件级别语义异常和全局逻辑异常，并针对性地配置三个专家网络分别检测这三个层次的异常。此外，引入了专家信息排斥（EIR）模块与专家选择均衡（ESB）模块来提高专家多样性与均衡性。

**Result:** 实验结果表明，AnomalyMoE在8个包含工业成像、三维点云、医学成像、视频监控和逻辑异常检测等领域的测试数据集上取得了新的最先进性能，并显著超越了各自领域中的专门方法。

**Conclusion:** AnomalyMoE通过专门化于重建特征和识别指定语义级别的偏差，并使用分层设计使单一模型能并行理解和检测广泛类型的异常。该框架展示了在多个领域超越现有专门化方法的能力。

**Abstract:** Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [115] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

> The paper presents the PA-HOI Motion Capture dataset, a significant contribution to HOI research by emphasizing the role of object physical attributes on human motion characteristics, further validated with motion generation methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the limitation of existing Human-Object Interaction (HOI) datasets, which focus on affordance details but neglect the influence of object physical properties on long-term human motion.

**Method:** The paper introduces the PA-HOI Motion Capture dataset, which includes 562 motion sequences of human-object interactions with a focus on the impact of objects' physical attributes (size, shape, weight) on human motion dynamics, such as posture, velocity, and motion characteristics.

**Result:** The dataset created stands out by significantly extending the scope of existing ones, providing a deeper understanding into how different object physical attributes affect human posture, speed, motion scale, and interacting strategies.

**Conclusion:** The paper concludes by demonstrating the applicability of the PA-HOI dataset with existing motion generation methods, confirming its potential to transfer realistic physical awareness.

**Abstract:** The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [116] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

> 该研究提出了一种基于X光片用于预测Sharpe/van der Heijde评分的两阶段管道，比手动评分更高效，且性能与专业放射科医生相当。

<details>
  <summary>Details</summary>

**Motivation:** 由于SvdH评分系统的复杂性，其在临床常规实践中普及受限，因此该研究旨在提高评分效率，为临床提供更高效的评分方法。

**Method:** 该研究提出了一种用于预测Sharpe/van der Heijde(SvdH)评分的双阶段管道，通过使用双手X光片进行可解释的图像级SvdH评分预测。方法包括抽样可能包含异常的图像块或裁剪包含疾病相关关节的补丁，然后使用基于注意力的多实例学习将它们整合成图像级特征来预测SvdH评分。

**Result:** 该管道实现了与经验丰富的放射科医生相当的准确性，产生的最佳个体评分预测模型取得了0.943的皮尔逊相关系数(PCC)和15.73的均方根误差(RMSE)。集成学习进一步提高了预测准确性，达到了0.945的PCC和15.57的RMSE。

**Conclusion:** 研究的管道能有效定位与RA进展相关的解剖结构，并做出决策，显示出与经验丰富的放射科医生相当的预测准确性，达到了最先进的性能。

**Abstract:** The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [117] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

> 本文介绍了TEFormer模型，该模型通过引入纹理感知和边缘引导机制，有效解决了城市遥感图像语义分割中存在的纹理差异和空间结构复杂性问题，实验结果表明该模型在多个数据集上具有良好的分割性能。

<details>
  <summary>Details</summary>

**Motivation:** 城市遥感图像中的地理对象常表现出微小的纹理差异和相似的空间结构，这可能导致语义歧义和误分类。此外，不规则的对象形状、模糊的边界以及语义对象的空间分布重叠等挑战导致边缘形态复杂多样，进一步加大了精确分割的难度。本文正是为了解决这些问题而设计的TEFormer模型。

**Method:** 本文提出了一种名为TEFormer的纹理感知边缘引导Transformer模型，该模型整合了纹理感知模块和边缘引导机制，旨在解决城市遥感图像中地物细小纹理差异和类似空间结构导致的语义歧义及误分类问题。模型的编码器部分设计了纹理感知模块来捕捉视觉上相似类别的细微纹理差异，增强语义区分；解码器部分构建了边缘引导三分支解码器来保持局部边缘和细节；最后，引入了边缘引导特征融合模块来融合上下文信息和细节信息以实现精细化语义分割。

**Result:** 实验表明，TEFormer在Potsdam、Vaihingen和LoveDA数据集上的mIoU分别达到了88.57%、81.46%和53.55%，显示了其在城市遥感图像语义分割中的有效性。

**Conclusion:** 实验结果证明，通过纹理感知和边缘引导机制，TEFormer模型能够有效提高城市遥感图像的语义分割性能，并在多个数据集上取得了显著效果。

**Abstract:** Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [118] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

> The paper introduces Depth-Jitter, a novel depth-based augmentation technique that improves the robustness of models in handling depth variations by generating synthetic depth perturbations. Evaluations show it enhances model stability and generalization in depth-sensitive applications, introducing depth-aware augmentation to enhance real-world performance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitation of conventional augmentation techniques in considering depth information, which are crucial for applications like underwater imaging, robotics, and autonomous navigation.

**Method:** Depth-Jitter applies adaptive depth offsetting to simulate natural depth variations. The offsetting is guided by depth variance thresholds and aims to preserve the structure while creating depth perturbations.

**Result:** The authors evaluated Depth-Jitter on FathomNet and UTDAC2020 datasets and demonstrated that it can enhance model stability and generalization compared to traditional augmentation methods like ColorJitter.

**Conclusion:** Depth-Jitter consistently improves model stability in depth-sensitive scenarios, even if not always outperforming traditional methods in absolute performance. The research promotes depth-aware augmentation as a promising direction for enhancing models in depth-focused applications.

**Abstract:** Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [119] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

> This paper introduces the first all-in-one image deblurring method that can efficiently restore images affected by various blur types using a mixture-of-experts (MoE) decoding module.

<details>
  <summary>Details</summary>

**Motivation:** Current methods for image deblurring lack generalization due to their specialization for particular blur types, leading to impractical solutions in many real scenarios. This paper aims to provide an all-in-one solution to address this issue.

**Method:** We propose a mixture-of-experts (MoE) decoding module that dynamically routes image features based on the recognized blur degradation, facilitating precise and efficient restoration for various blur types.

**Result:** The method achieves performance comparable to dedicated task-specific models while exhibiting remarkable robustness and generalization capabilities for unseen blur degradation scenarios.

**Conclusion:** Our unified approach demonstrates the ability to perform comparably to specialized models while showing robustness and generalization across different types of blur degradations.

**Abstract:** Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [120] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

> 该研究通过仅微调预训练CLIP模型的少量参数，提出了一个高效的深度伪造检测方法LNCLIP-DF，实现了跨数据集的高性能，表明了高效的模型训练策略和多样化的训练数据对提高模型泛化能力的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管许多方法通过引入显著的架构复杂性来适应基础模型，但本研究旨在证明，通过对预训练模型进行参数高效适应，也可以实现强大的泛化能力，以应对深度伪造检测器在未见过的操作技术中表现不佳的问题。

**Method:** LNCLIP-DF方法通过仅微调预训练CLIP视觉编码器中的Layer Normalization参数（占总参数的0.03%），并使用L2正则化和潜在空间增强技术强制执行超球面特征流形，从而增强泛化能力。

**Result:** 该方法在涵盖从2019年到2025年的13个基准数据集上进行了广泛的评估，取得了最先进的性能，平均跨数据集AUROC优于更加复杂、近期的方法。

**Conclusion:** 本研究提出了一个计算效率高且可复现的方法，证明了通过对预训练的CLIP模型进行有针对性的、最小的改动就能达到最先进的泛化性。此外，研究还指出，训练过程中使用来自相同源视频的真实假数据对减少捷径学习和提高泛化性至关重要，而学术数据集上的检测难度并没有随着时间的推移而显著增加。

**Abstract:** The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [121] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barış Büyüktaş,Jonas Klotz,Begüm Demir*

Main category: cs.CV

> 本论文提出了一种减少联邦学习通信开销的新策略FedX，通过剪枝技术减少模型更新的大小而不会损害性能，实验表明其有效改善了模型泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 联邦学习在实现分散数据归档（即客户端）中深度神经网络的协作训练方面具有潜力，但通信开销是将联邦学习应用于遥感图像分类任务的主要挑战，因为每个客户端本地存储数据并且仅与中央服务器共享模型更新。

**Method:** Fedx使用解释指导剪枝策略来减少联邦学习中客户端与中央服务器之间频繁交换大型模型更新造成的通信开销，通过最小化发送模型的大小而不影响性能来实现。Fedx在中央服务器处利用基于反向传播的解释方法来估计模型组件的任务特定重要性，并剪枝最不相关的组件。

**Result:** 实验结果表明，与未剪枝模型和最先进的剪枝方法相比，Fedx在显著减少共享模型参数数量的同时，增强了全局模型的泛化能力。

**Conclusion:** 论文结论表明，基于解释引导剪枝的FedX策略在减少通信开销同时提高了模型性能，是一个有效的解决方案。

**Abstract:** Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [122] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

> 本文提出了一种用于股骨MRI结构分割的2.5D U-Net架构XAG-Net，该架构使用了像素级CSA和AG机制，在提高准确性的同时保持了计算效率。

<details>
  <summary>Details</summary>

**Motivation:** 准确的股骨结构分割对于骨科诊断和手术规划至关重要，但由于现有的2D和3D深度学习分割方法的限制，它仍然是一个挑战。

**Method:** XAG-Net, 一种基于2.5D U-Net的新型架构，融合了像素级跨层注意力机制（CSA）和跳越注意力门控机制（AG），以增强跨层上下文建模和层内特征细化。

**Result:** 广泛的评估表明，XAG-Net在股骨分割准确性方面超过了基线的2D、2.5D和3D U-Net模型，同时保持了计算效率。

**Conclusion:** 剪枝研究表明CSA和AG模块的关键作用，确立了XAG-Net作为一种高效和准确的股骨MRI分割框架的前景。

**Abstract:** Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [123] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

> 本文介绍了一种名为 SIFThinker 的框架，它通过深度增强的边界框和自然语言的交替来提升多模态大语言模型在复杂视觉任务中的表现，特别是在空间理解和细粒度视觉感知上。

<details>
  <summary>Details</summary>

**Motivation:** 当前的多模态大语言模型在处理复杂的视觉任务时依然面临挑战，例如空间理解和细粒度感知。现有的方法未能充分利用空间线索进行注意力修正从而迭代地专注于与提示相关的区域。

**Method:** SIFThinker, 一种具有空间感知能力的 '用图像思考' 框架，该框架通过在增强深度的边界框和自然语言之间交替，提升了对提示相关区域的注意力修正和图像区域聚焦能力。

**Result:** 实验结果表明，SIFThinker 在空间理解和细粒度视觉感知上优于最先进的方法，并且保持了强大的通用能力，证明了该方法的有效性。

**Conclusion:** 提出的方法在具体任务上展示了显著的性能提升，表明空间感知和图像区域聚焦在多模态模型中的重要性。

**Abstract:** Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [124] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

> 提出了一种高效利用少量未标注目标域视频进行视频时间定位的新方法URPA，克服了GRPO方法需要大量标注数据、计算和存储开销大的问题，实现了跨域知识迁移和实时部署。

<details>
  <summary>Details</summary>

**Motivation:** 改进视频时间定位技术，以应对现有方法如GRPO标注数据依赖且计算存储成本高的问题，提高跨域适应性和实时性能。

**Method:** 利用不确定性量化的策略回放适应方法（URPA）来无需标注目标域标签进行视频时间定位的训练，通过生成多组候选预测并计算平均值形成伪标签，同时信心评分用于奖励训练，使模型能专注于可靠的监督。

**Result:** 实验覆盖三个数据集及六个跨域环境，结果证明URPA能够利用少量未标注的目标域视频取得良好的跨域泛化效果。

**Conclusion:** 提出的方法有效减少了模型对标注数据的依赖，同时在计算存储成本方面更低，实现了快速且适应性强的视频时间定位能力。

**Abstract:** Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [125] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

> 本论文提出GS-MoE框架，通过多个专家模型和时间高斯插补损失函数，解决了现有模型在视频异常检测任务中面临的问题，实现了类别特定异常的精准检测，并在多个数据集上取得了优异性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前模型在处理多种异常事件类型时，由于采用单一模型处理所有类别，忽略类别特定特征，以及弱监督信号缺乏精确时间信息的限制，它们无法很好地捕捉与正常事件混合的细微异常模式。

**Method:** GS-MoE框架使用一套专家模型，每个模型专门针对特定类型的异常事件。这些专家模型通过时间高斯插补损失函数的引导，增强了弱监督学习中对时间一致性和异常模式的捕捉能力。高斯插补方法专注于最有可能包含异常事件的时间段，使得模型可以更精准、全面地表示异常事件。预测结果通过专家混合机制综合，以建模跨多样异常模式的复杂关系。

**Result:** 该方法在UCF-Crime数据集上达到了91.58%的AUC，并在XD-Violence和MSAD数据集上表现出优越的结果。

**Conclusion:** 通过利用类别特定的专长和时间指导，GS-MoE为弱监督下的视频异常检测任务设定了新的基准。

**Abstract:** Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [126] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

> 提出了一种扩散模型来生成用于心脏MRI合成图像，改善了域移问题，提高了在未见域的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 磁共振成像（MRI）包括心脏MRI，由于成像设备和采集协议的差异而容易出现领域漂移。这限制了训练的人工智能模型在现实场景中的部署，并且性能在未见过的领域中下降。

**Method:** 我们提出了一种扩散模型（DM），该模型在源域上训练，以生成类似于给定参考的心脏磁共振成像（MRI）合成图像。这些合成图像在空间和结构上保持保真度，确保与源域的相似性，并与分割掩码兼容。

**Result:** 我们在多中心心脏MRI分割中评估了我们生成方法的效用，使用2D nnU-Net、3D nnU-Net和vanilla U-Net分割网络。我们的方法在目标域上的分割性能（基于Welch's t-test, p < 0.01）显著优于仅训练真实数据的分割模型。我们探索了领域泛化和领域适应两种策略，其中领域不变的分割模型在合成源域数据上训练，或将目标域数据转换为源域数据。

**Conclusion:** 本研究提出的方法在没有额外转移学习或在线训练的情况下改善了心脏MRI图像分析中的领域迁移挑战，尤其是在数据稀缺的情境中非常有用。

**Abstract:** Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [127] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

> 改进了ViPro模型，实现了从观测数据中无监督地推断状态，并扩展了数据集以接近真实场景。

<details>
  <summary>Details</summary>

**Motivation:** 先前工作的模型依赖于给定的真值初始符号状态，这导致模型仅学习了一个捷径，即没有真正将观察到的环境与预测的符号状态相联系，因此无法在状态有噪音的情况下估计状态。

**Method:** 通过对ViPro模型进行改进，新方法使得模型可以仅根据观测数据正确推断状态，而无需初始提供完整的真值状态。同时，扩展了Orbits数据集，增加了一个3D变体来缩小与现实场景之间的差距。

**Result:** 通过改进，新的模型可以正确地从观测数据中推断状态，无需提供完整的真值状态，并且在扩展的数据集上验证了这一点。

**Conclusion:** 改进的方法证明了在没有提供全真值初始状态的情况下，可以通过观测直接正确推断状态的可能性，这为预测未来视频帧打开了新的可能性。

**Abstract:** Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [128] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Bergés*

Main category: cs.CV

> 通过街景图像分析城市的社交互动，发现天际线视图指数与所有三种社会性类型相关，绿化视图指数预测持久社会性，场所依恋与短暂社会性呈正相关。

<details>
  <summary>Details</summary>

**Motivation:** 作者认为街景图像包含可被提取和解释的潜在社会信息，旨在证明街景图像可以成为研究城市社会互动的可扩展且保护隐私的工具。现有定量研究多数关注行人数量，而不是社会互动的质量。

**Method:** 本文使用多元大语言模型分析了来自15个城市的2,998张街景图像中的社会互动，基于Mehta的社会性分类进行指导。然后通过多元线性回归模型检验了推测的社会性度量与来自世界价值观调查的城市层面的场所依恋得分以及从单个街景图像中提取的环境预测指标之间的关系。

**Result:** 研究结果表明天际线视图指数与所有三种社会性类型相关，绿化视图指数预测持久社会性，场所依恋与短暂社会性呈正相关。

**Conclusion:** 研究表明街景图像能够用于推断特定类型社会互动与建设环境变量之间的关系，这为城市社会性研究提供了一种可扩展且保护隐私的方法，有助于跨文化交流理论测试和以证据为基础的城市设计。

**Abstract:** Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [129] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

> 本文提出了VA-GPT模型，利用视觉语言模型和大型语言模型的表示和泛化能力，通过SETS和TETG模块解决了异常事件视频理解中的时空信息捕捉问题，提高了模型在异常事件检测上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决现有视频理解多模态大型语言模型在处理异常事件时因异常事件的时空稀疏性和冗余信息导致结果次优的问题。

**Method:** 本文提出了VA-GPT，一种用于总结和定位各种视频中异常事件的新型多模态大型语言模型。该方法通过两个关键模块，空间有效令牌选择（SETS）和时间有效令牌生成（TETG），实现了视觉编码器和大型语言模型之间的有效令牌对齐，从而能够更有效地捕捉和分析与异常事件相关的时空信息。

**Result:** 本文的方法在多个基准测试上优于现有的最先进的方法。

**Conclusion:** 本文通过提出VA-GPT模型，有效解决了视频理解中异常事件的时空信息捕捉和处理问题，并通过特定的指令跟随数据集和跨领域评估基准进一步验证了这一方法的有效性。

**Abstract:** Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [130] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,Günay Doğan*

Main category: cs.CV

> 这篇论文描述了Goldstein, Bresson, Osher提出的两阶段图像分割算法的实现，该算法通过分割Bregman方法高效地划分图像为前景和背景区域。

<details>
  <summary>Details</summary>

**Motivation:** 论文动机在于实现一种能够将图像分割为前景和背景的算法，特别是当像素值可以用两个不同的平均值来描述，且区域边界是平滑的情况下。

**Method:** 该论文采用Goldstein, Bresson, Osher改进的两阶段图像分割算法，该算法基于Chan和Vese提出的能量函数，并通过分割的Bregman方法实现高效的最小化。

**Result:** 作者详细实现了这一方法，并通过一系列图像在不同算法参数范围内的表现来验证其性能。

**Conclusion:** 这项研究展示了改进的两阶段图像分割算法的有效性和实用性，特别是在区域边界平滑假设的情况下。

**Abstract:** In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [131] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

> 本研究提出了一种新的方法，通过利用身份额外登记图像来预测一比一结果是否为馆内或馆外，从而改进传统的馆外检测方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的馆外检测方法主要集中在设定相似度分数的阈值上。这种方法忽略了利用额外的登记图像来进行更精确预测的可能。为了减少误识别、错误逮捕和浪费的调查时间，本研究旨在提供一种客观的馆外检测方法。

**Method:** 本研究提出了一种利用身份额外登记图像的方法来预测一比多面部识别中的一比一结果是否是馆内/馆外检测。通过提取与一比一身份对应的额外登记图像的排名，生成训练数据。然后训练一个分类器，利用特征向量来预测一比一结果是馆内还是馆外。

**Result:** 通过两个不同的数据集和四种不同的匹配器验证了该方法的有效性，适用于质量不高或有降质因素的探头图像，并能在不同人群中保持相似的预测精度。

**Conclusion:** 实验结果显示，该方法对于质量不同的探头图像均有效，且在不同人群中馆内/馆外分类精度相似。同时证明，只有使用先进边际损失函数训练的面部识别匹配器，本方法的效果才会有所提升。

**Abstract:** A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [132] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

> 本文提出了TaAM-CPT方法，利用文本数据构建通用的多模态模型，可扩展到任意模态，不依赖大规模模态特定标注数据。

<details>
  <summary>Details</summary>

**Motivation:** 现有的模态学习方法高度依赖大量的模态特定标注数据，或为单一模态定制。TaAM-CPT旨在提供一个可扩展的方法，能够只使用文本数据构建面向无限模态的通用模型，解决现有方法的局限性。

**Method:** TaAM-CPT采用一致性的prompt调优方法，利用单一文本数据构建面向无限模态的通用表示模型。该方法包括模态prompt池、文本构建和模态一致的文本编码器从预训练模型中获取信息。通过增加prompt池和模态一致的文本编码器来扩展新的模态。其设计的模态内和模态间学习目标有助于在不同模态之间保持语义一致性。

**Result:** TaAM-CPT在不使用任何模态特定标注数据的情况下，在视频分类、图像分类和音频分类等多个模态的数据集上取得了领先结果。

**Conclusion:** TaAM-CPT证明了可以通过仅使用文本数据构建泛模态的先进模型，展示了在多个模态任务上的推广能力及领先性能。

**Abstract:** The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [133] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

> 本文介绍了一种新型框架FVGen，它通过生成对抗网络（GAN）和软KL散度最小化的视频扩散模型提炼方法，实现快速新视图合成，显著提高了在稀疏视图情况下下游重建任务的时间效率。

<details>
  <summary>Details</summary>

**Motivation:** 尽管3D重建方面取得了进展，但在使用稀疏视图时仍然存在挑战，导致未见区域出现瑕疵。此前的工作通过视频扩散模型（VDM）生成稠密观察来填充这些缝隙，但这些方法在采样速度上存在局限性。

**Method:** 我们提出了一种名为FVGen的新框架，使用生成对抗网络（GAN）和软化的逆KL散度最小化方法，将多步去噪教师模型提炼为少步去噪学生模型，从而实现使用视频扩散模型进行快速新视图合成。

**Result:** 实验证明，与之前的工作相比，我们的框架能够在生成相同数量新视图的同时，将采样时间减少90%以上，并且保持或提升了视觉质量。

**Conclusion:** FVGen框架在处理稀疏输入视图的3D重建任务中，显著提升了时间效率，解决了预训练VDM在较少视图下需要多次运行的问题。

**Abstract:** Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [134] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

> 本文提出了一种新的超分算法，该算法在提升图像质量的同时，也改善了分类准确性，特别是在合成孔径雷达图像上。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机在于探索将分类目标直接整合到超分辨率过程中是否可以进一步提高分类准确性。由于传统的超分辨率方法主要侧重于在像素级别上提升图像质量，而忽略了这种提升与下游分类任务准确性之间的关系，因此这项研究就显得尤为重要。

**Method:** 本文提出了一种新的算法策略，通过优化损失函数来同时优化图像质量和分类性能，这种损失函数在提升合成孔径雷达图像分辨率的同时考虑了分类准确性。

**Result:** 通过对论文摘要的分析，本文研究了超分辨率技术和分类之间的关系，并提出了一种新的方法，该方法通过优化损失函数来同时提升图像质量和分类准确性。特别地，该算法应用于合成孔径雷达图像，成功增强了低分辨率图像的分辨率，并提升了分类任务的性能。实验结果验证了该方法的有效性，即在提高图像质量的同时，也改善了分类准确性。

**Conclusion:** 结论表明，通过综合优化图像质量和分类性能，本文提出的方法能够提升超分辨率图像的忠实度并增加分类任务的准确性。实验结果验证了该方法的有效性。

**Abstract:** High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [135] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

> 本文介绍两种新的过采样算法，旨在解决SAR图像中船隻分类的类别不平衡问题，实验表明新方法相比原始方法和基线方法具有显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 由于SAR图像中船隻分类是长尾分布数据集中的一个挑战，特别是对于少数类别的船只，传统方法不易解决这种类别不平衡问题。因此，本文旨在探索特征空间中的过采样方法，以改善少数类别的船只分类效果。

**Method:** 本文提出了两种受Major-to-minor方法启发的新算法M2m$_f$和M2m$_u$，用于处理SAR图像中的船只分类问题，特别是在类别分布不均的情况下。

**Result:** 实验结果显示，与原始的M2m方法和基线方法相比，新方法在FuSARShip数据集上的平均F1分数提高8.82%，在OpenSARShip数据集上提高4.44%。

**Conclusion:** 实验结果表明，特征空间中的过采样方法能够有效地提升SAR船只分类任务中少数类别的识别率。

**Abstract:** SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [136] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

> The paper presents a novel GAN-based semi-supervised learning framework designed for scenarios with very limited labeled data, which outperforms state-of-the-art methods in handling low data regimes, especially in the 5-shot setting across 11 MedMNIST datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this work is the challenge presented by the scarcity of labeled training data in medical imaging, which limits the effectiveness of deep learning approaches.

**Method:** This paper introduces a GAN-based semi-supervised learning framework consisting of three neural networks: a generator for class-conditioned image translation, a discriminator for assessing image authenticity and classification, and a dedicated classifier. The framework employs a three-phase training process, combining supervised training on limited labeled data with unsupervised learning using abundant unlabeled images through image-to-image translation. An ensemble-based pseudo-labeling method that uses confidence-weighted predictions and temporal consistency is utilized for improved label estimation of unlabeled data.

**Result:** The framework demonstrated statistically significant improvements compared to six state-of-the-art GAN-based semi-supervised methods, particularly in the 5-shot setting. It maintains its superior performance for a range of labeled data amounts (5, 10, 20, and 50 shots per class).

**Conclusion:** The research concludes that the proposed approach is a practical solution for medical imaging applications with high annotation costs, enabling robust classification even with minimal labeled data.

**Abstract:** Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [137] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

> 本文改进了SimSwap模型，加入了注意力机制和优化的训练策略，显著提升了脸部识别和整体视觉效果。

<details>
  <summary>Details</summary>

**Motivation:** 人脸交换技术在学术研究和商业应用中受到了广泛关注。本文旨在改进现有的SimSwap模型，以提高身份保持性、属性一致性和整体视觉效果。

**Method:** 本文通过引入自注意力和交叉注意力机制、动态损失加权和余弦退火学习率调度等改进措施，增强了SimSwap这一高效的高质量人脸交换框架。

**Result:** 实验结果表明，在400,000次训练迭代后，增强模型在身份相似性和FID评分方面取得了优于基线模型的结果，并且在视觉表现上也更为出色。

**Conclusion:** 文章最后指出了未来的研究方向，例如整合StyleGAN3、增强唇部同步效果、加入3D面部建模，并实现视频应用中的时间一致性。

**Abstract:** Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [138] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

> Structure

<details>
  <summary>Details</summary>

**Motivation:** Structure

**Method:** Structure

**Result:** {
  "tldr": "本文提出了一种名为CLIPin的统一非对比插件，旨在改善对比语言-图像预训练模型的跨模态语义对齐问题。通过设计共享的预投影器，CLIPin可以灵活地结合对比学习和非对比学习，并在多种下游任务中表现出色。",
  "motivation": "大规模自然图像-文本数据集通常存在语义对齐不准确的问题，而医学数据集虽然语义相关性高但内容多样性低，这些特性限制了CLIP模型学习出稳健且通用的表征能力。",
  "method": "提出了一个名为CLIPin的非对比插件，可以整合到CLIP风格的架构中，增强多模态语义对齐。同时设计了两个共享的预投影器，分别用于图像和文本模态，以促进对比学习与非对比学习的整合。",
  "result": "在多种下游任务上的实验结果展示了CLIPin作为一种插件组件的有效性和普遍适用性，兼容多种对比框架。",
  "conclusion": "CLIPin适用于各种对比性结构，优化了语义对齐，增强了学习的稳健性和泛化能力。表明了非对比学习方法在提升对比学习基础上的重要价值。代码开源，可供进一步研究。")

**Conclusion:** Structure

**Abstract:** Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>
