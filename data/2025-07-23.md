<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [cs.CV](#cs.CV) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs](https://arxiv.org/abs/2507.15863)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.CL

> DEREK模块是一个安全和可扩展的Retrieval-Augmented Generation系统，专门用于企业文档问答，从多方面参数提高了系统性能，保证了答案的准确性和可信度。

<details>
  <summary>Details</summary>

**Motivation:** 开发DEREK模块是为了满足企业对安全、可审计且忠实于上下文的检索系统的需求，特别适用于法律和金融等高风险领域。

**Method:** 企业文档问答的Deep Extraction & Reasoning Engine for Knowledge（DEREK）模块，专门用于处理异构内容（如PDF、Office文件和网络内容），通过1000-token重叠块进行索引，并使用混合HNSW+BM25存储。用户查询经过GPT-4o优化，并通过向量+BM25搜索检索，Cohere进行重新排序，LLM使用CO-STAR提示工程回答问题。LangGraph验证器通过确保每个声明都基于引用，直到满足条件为止，来保证答案的可信度。

**Result:** 在四个LegalBench子集上测试表明，1000-token块帧提高了Recall@50约1个百分点，而混合搜索+重新排序提升了Precision@10约7个百分点。验证器提高了TRACe利用率到0.5以上，并将未经支持的陈述限制在3%以下。

**Conclusion:** DEREK模块能够在最小的操作开销下，提供准确、可追溯和生产就绪的文档问答系统，满足企业对安全、可审计和忠实于上下文的检索系统的需求，在高风险领域提供了可靠的基线。

**Abstract:** We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge)
Module, a secure and scalable Retrieval-Augmented Generation pipeline designed
specifically for enterprise document question answering. Designed and
implemented by eSapiens, the system ingests heterogeneous content (PDF, Office,
web), splits it into 1,000-token overlapping chunks, and indexes them in a
hybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via
combined vector+BM25 search, reranked with Cohere, and answered by an LLM using
CO-STAR prompt engineering. A LangGraph verifier enforces citation overlap,
regenerating answers until every claim is grounded. On four LegalBench subsets,
1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank
boosts Precision@10 by approximately 7 pp; the verifier raises TRACe
Utilization above 0.50 and limits unsupported statements to less than 3%. All
components run in containers, enforce end-to-end TLS 1.3 and AES-256. These
results demonstrate that the DEREK module delivers accurate, traceable, and
production-ready document QA with minimal operational overhead. The module is
designed to meet enterprise demands for secure, auditable, and context-faithful
retrieval, providing a reliable baseline for high-stakes domains such as legal
and finance.

</details>


### [2] [Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity](https://arxiv.org/abs/2507.15864)
*Guowen Yuan,Tien-Hsuan Wu,Lianghao Xia,Ben Kao*

Main category: cs.CL

> 研究基于演示学习的低资源场景下的命名实体识别问题，提出结合双重相似性选择示例及对抗性示例训练模型来改善性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的演示示例选择方法主要依赖于语义相似性，这限制了模型在低资源场景中的性能，并且模型参考示例的能力不足。

**Method:** 我们提出了一个结合语义相似性和特征相似性的双重相似性方法来选择示例，并通过对抗性示例训练NER模型，以解决模型参考示例能力不足的问题。

**Result:** 我们在低资源NER任务中进行了全面的实验，结果表明我们的方法优于多种现有方法。

**Conclusion:** 通过使用提出的演示示例选择方法和对抗性示例训练技术，在低资源的命名实体识别任务中取得了优于其他方法的性能。

**Abstract:** We study the problem of named entity recognition (NER) based on demonstration
learning in low-resource scenarios. We identify two issues in demonstration
construction and model training. Firstly, existing methods for selecting
demonstration examples primarily rely on semantic similarity; We show that
feature similarity can provide significant performance improvement. Secondly,
we show that the NER tagger's ability to reference demonstration examples is
generally inadequate. We propose a demonstration and training approach that
effectively addresses these issues. For the first issue, we propose to select
examples by dual similarity, which comprises both semantic similarity and
feature similarity. For the second issue, we propose to train an NER model with
adversarial demonstration such that the model is forced to refer to the
demonstrations when performing the tagging task. We conduct comprehensive
experiments in low-resource NER tasks, and the results demonstrate that our
method outperforms a range of methods.

</details>


### [3] [Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models](https://arxiv.org/abs/2507.15868)
*Altynbek Ismailov,Salia Asanova*

Main category: cs.CL

> 研究对前沿大型语言模型进行测试，发现其在处理提示扰动时存在过度鲁棒性和对关键变化的迟钝，主张评估和训练中应奖励差异敏感性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探讨大型语言模型在面对输入提示扰动时的鲁棒性和敏感性之间的界限。

**Method:** 该研究通过编制50个LeetCode问题并设计三种最小提示扰动来测试大型语言模型的鲁棒性。这些扰动分别是逐步欠规范（每步删除10%的单词）、词汇翻转（如将“max”翻转为“min”）以及术语膨胀（用晦涩的技术同义词替换一个常见的名词）。

**Result:** 在总共11,853次生成结果中发现，模型在面对线索减少时显示出过度鲁棒性，但对于关键变化过于迟钝。

**Conclusion:** 研究发现，尽管模型在90%的提示缺失时仍有85%的情况下保持正确，但对于单一的量化词翻转这种任务反转，只有54%的模型能作出反应，显示出过度鲁棒性及对关键变化的忽视。研究倡导在模型评估和训练中实施奖励差异敏感性的协议。

**Abstract:** Large language models (LLMs) now write code in settings where misreading a
single word can break safety or cost money, yet we still expect them to
overlook stray typos. To probe where useful robustness ends and harmful
insensitivity begins, we compile 50 LeetCode problems and craft three minimal
prompt perturbations that should vary in importance: (i) progressive
underspecification deleting 10 % of words per step; (ii) lexical flip swapping
a pivotal quantifier ("max" to "min"); and (iii) jargon inflation replacing a
common noun with an obscure technical synonym. Six frontier models, including
three "reasoning-tuned" versions, solve each mutated prompt, and their Python
outputs are checked against the original test suites to reveal whether they
reused the baseline solution or adapted. Among 11 853 generations we observe a
sharp double asymmetry. Models remain correct in 85 % of cases even after 90 %
of the prompt is missing, showing over-robustness to underspecification, yet
only 54 % react to a single quantifier flip that reverses the task, with
reasoning-tuned variants even less sensitive than their bases. Jargon edits lie
in between, passing through 56 %. Current LLMs thus blur the line between
harmless noise and meaning - changing edits, often treating both as ignorable.
Masking salient anchors such as function names can force re - evaluation. We
advocate evaluation and training protocols that reward differential
sensitivity: stay steady under benign noise but adapt - or refuse - when
semantics truly change.

</details>


### [4] [Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation](https://arxiv.org/abs/2507.16002)
*Sumit Singh,Rohit Mishra,Uma Shanker Tiwary*

Main category: cs.CL

> 研究通过使用印地语预训练模型、生成模型和检索增强技术提高印地语NER的性能，发现检索增强特别受生成模型GPT3.5-turbo欢迎，而对大型语言模型的效果提升有限。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于改进印地语命名实体识别（NER）技术。

**Method:** 研究调查了一种用于印地语命名实体识别（NER）的技术，利用了印地语特定的预训练编解码器（MuRIL和XLM-R），生成模型（如Llama-2-7B-chat-hf, Llama-2-70B-chat-hf, Llama-3-70B-Instruct, GPT3.5-turbo）以及从维基百科提取的相关外部上下文数据进行数据增强。

**Result:** 研究发现，使用检索增强（RA）的语言模型在大多数情况下优于不使用RA的基线方法。具体来说，MuRIL和XLM-R在没有RA时的宏观F1得分分别是0.69和0.495，在有RA的情况下分别增加到0.70和0.71。调整后的Llama2-7B显著优于未调整的Llama2-7B版本。而对于未进行微调的生成模型，在有额外数据的情况下，性能也有所提高。GPT3.5-turbo很好地采用了RA，但Llama2-70B和llama3-70B并未对RA做出同等程度的适应。

**Conclusion:** 研究结果表明，RA显着提升了性能，特别是对于低语境数据。这项研究表明，如何有效地利用数据增强技术和预训练模型来提高NER性能，尤其是在资源有限的语言中。

**Abstract:** One major challenge in natural language processing is named entity
recognition (NER), which identifies and categorises named entities in textual
input. In order to improve NER, this study investigates a Hindi NER technique
that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and
Generative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf
(Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments
the data with retrieved data from external relevant contexts, notably from
Wikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA.
However, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER
generation. Our investigation shows that the mentioned language models (LMs)
with Retrieval Augmentation (RA) outperform baseline methods that don't
incorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69
and 0.495, respectively, without RA and increase to 0.70 and 0.71,
respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B
by a significant margin. On the other hand the generative models which are not
fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA
well; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval
context. The findings show that RA significantly improves performance,
especially for low-context data. This study adds significant knowledge about
how best to use data augmentation methods and pretrained models to enhance NER
performance, particularly in languages with limited resources.

</details>


### [5] [Learning without training: The implicit dynamics of in-context learning](https://arxiv.org/abs/2507.16003)
*Benoit Dherin,Michael Munn,Hanna Mazzawi,Michael Wunder,Javier Gonzalvo*

Main category: cs.CL

> Transformer blocks modify MLP weights according to context, possibly explaining LLMs' contextual learning without additional training.

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanisms behind LLMs' ability to learn in context without additional training.

**Method:** Theoretical analysis and experimentation on how a transformer block with a self-attention layer and an MLP layer implicitly transforms the context into a low-rank weight-update for the MLP layer.

**Result:** Demonstrates through theory and experiments that a transformer block can implicitly adjust MLP weights based on context under certain assumptions.

**Conclusion:** Transformer blocks can implicitly modify MLP weights according to context, potentially explaining LLMs' ability to learn in context without additional training.

**Abstract:** One of the most striking features of Large Language Models (LLM) is their
ability to learn in context. Namely at inference time an LLM is able to learn
new patterns without any additional weight update when these patterns are
presented in the form of examples in the prompt, even if these patterns were
not seen during training. The mechanisms through which this can happen are
still largely unknown. In this work, we show that the stacking of a
self-attention layer with an MLP, allows the transformer block to implicitly
modify the weights of the MLP layer according to the context. We argue through
theory and experimentation that this simple mechanism may be the reason why
LLMs can learn in context and not only during training. Specifically, we show
under mild simplifying assumptions how a transformer block implicitly
transforms a context into a low-rank weight-update of the MLP layer.

</details>


### [6] [Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback](https://arxiv.org/abs/2507.16007)
*Hannah Rashkin,Elizabeth Clark,Fantine Huot,Mirella Lapata*

Main category: cs.CL

> 本文探讨了LLMs在写作反馈任务中的性能，发现模型能够提供有效且准确的反馈，但在识别主要问题和选择合适反馈类型方面存在问题。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在探讨大型语言模型（LLMs）是否能为创意作家提供有意义的写作反馈，定义了新的任务、数据集和评估框架，以研究模型在写作反馈任务中的挑战和局限性。

**Method:** 通过创建一个包含1,300篇故意引入写作问题的故事的数据集，采用自动和人工评估指标来评估常用LLMs在该任务中的表现。

**Result:** 研究显示，当前模型在许多方面表现出强大且直接有效的行为，能够提供具体且大多准确的写作反馈。然而，模型经常无法识别故事中的主要写作问题，并且在决定提供批评性反馈还是正面反馈时常常作出错误的判断。

**Conclusion:** 虽然LLMs能够提供准确并具体的写作反馈，但在识别故事中的主要问题和做出正确的反馈类型判断上还存在一些局限性。

**Abstract:** Can LLMs provide support to creative writers by giving meaningful writing
feedback? In this paper, we explore the challenges and limitations of
model-generated writing feedback by defining a new task, dataset, and
evaluation frameworks. To study model performance in a controlled manner, we
present a novel test set of 1,300 stories that we corrupted to intentionally
introduce writing issues. We study the performance of commonly used LLMs in
this task with both automatic and human evaluation metrics. Our analysis shows
that current models have strong out-of-the-box behavior in many respects --
providing specific and mostly accurate writing feedback. However, models often
fail to identify the biggest writing issue in the story and to correctly decide
when to offer critical vs. positive feedback.

</details>


### [7] [mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages](https://arxiv.org/abs/2507.16011)
*Hellina Hailu Nigatu,Min Li,Maartje ter Hoeve,Saloni Potdar,Sarah Chasins*

Main category: cs.CL

> 本文将多语言知识图谱构建任务定义为问答任务，并使用mRAKL系统通过头实体和关系预测尾实体。实验表明，mRAKL在提格雷尼亚语和阿姆哈拉语上的准确性分别提高了4.92和8.79个百分点。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机是改进多语言环境中知识图谱的自动构建或预测缺失实体和链接的方法。研究集中在低资源语言(提格雷尼亚语和阿姆哈拉语)上，同时探索使用高资源语言(阿拉伯语和英语)进行跨语言传输。

**Method:** 将多语言知识图谱构建(mKGC)任务重新定义为问答(QA)任务，并引入基于检索增强生成(RAG)的系统mRAKL来执行mKGC。系统通过将头实体和连接关系作为问题来让模型预测尾实体作为答案。

**Result:** 通过BM25检索器，基于RAG的方法与无上下文方法相比提高了性能。理想化检索系统下的消融研究显示，mRAKL系统在Tigrinya和Amharic上的准确性分别提高了4.92和8.79个百分点。

**Conclusion:** 将mKGC问题转化为问答问题，并通过引入mRAKL系统实现，实验结果显示该方法有效提升了在低资源语言中的多语言知识图谱构建性能。

**Abstract:** Knowledge Graphs represent real-world entities and the relationships between
them. Multilingual Knowledge Graph Construction (mKGC) refers to the task of
automatically constructing or predicting missing entities and links for
knowledge graphs in a multilingual setting. In this work, we reformulate the
mKGC task as a Question Answering (QA) task and introduce mRAKL: a
Retrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve
this by using the head entity and linking relation in a question, and having
our model predict the tail entity as an answer. Our experiments focus primarily
on two low-resourced languages: Tigrinya and Amharic. We experiment with using
higher-resourced languages Arabic and English for cross-lingual transfer. With
a BM25 retriever, we find that the RAG-based approach improves performance over
a no-context setting. Further, our ablation studies show that with an idealized
retrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points
for Tigrinya and Amharic, respectively.

</details>


### [8] [AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering](https://arxiv.org/abs/2507.16054)
*Simon Baeuerle,Max Radyschevski,Ulrike Pado*

Main category: cs.CL

> 研究展示了如何使用生成式AI技术来改进会议记录和知识管理流程，虽然技术上可行，但仍需关注伦理和组织方面的问题。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型组织中的信息共享主要集中在会议中，并且会议的记录存在文档不一致的问题，这里利用genAI如大型语言模型（LLMs）的出色语言处理能力来改进知识管理。

**Method:** 本研究提出了一种基于生成式人工智能（genAI）的端到端会议文档工作流自动化方案。会议记录并使用genAI生成会议纪要，并通过聊天机器人界面进行搜索。

**Result:** 在工程部门进行了基于genAI的软件工具测试，并收集了有关技术与伦理方面的详尽调查数据。用户反馈表明genAI可显著降低会议时间，同时技术上的问题已大部分解决，但伦理使用方面对组织层面的规范提出了较高要求。

**Conclusion:** 基于genAI的会议管理工具在技术上是可行的，但在真正的应用中需要额外考虑伦理和组织因素。

**Abstract:** In large organisations, knowledge is mainly shared in meetings, which takes
up significant amounts of work time. Additionally, frequent in-person meetings
produce inconsistent documentation -- official minutes, personal notes,
presentations may or may not exist. Shared information therefore becomes hard
to retrieve outside of the meeting, necessitating lengthy updates and
high-frequency meeting schedules.
  Generative Artificial Intelligence (genAI) models like Large Language Models
(LLMs) exhibit an impressive performance on spoken and written language
processing. This motivates a practical usage of genAI for knowledge management
in engineering departments: using genAI for transcribing meetings and
integrating heterogeneous additional information sources into an easily usable
format for ad-hoc searches.
  We implement an end-to-end pipeline to automate the entire meeting
documentation workflow in a proof-of-concept state: meetings are recorded and
minutes are created by genAI. These are further made easily searchable through
a chatbot interface. The core of our work is to test this genAI-based software
tooling in a real-world engineering department and collect extensive survey
data on both ethical and technical aspects. Direct feedback from this
real-world setup points out both opportunities and risks: a) users agree that
the effort for meetings could be significantly reduced with the help of genAI
models, b) technical aspects are largely solved already, c) organizational
aspects are crucial for a successful ethical usage of such a system.

</details>


### [9] [Deep Researcher with Test-Time Diffusion](https://arxiv.org/abs/2507.16075)
*Rujun Han,Yanfei Chen,Zoey CuiZhu,Lesly Miculicich,Guan Sun,Yuanjun Bi,Weiming Wen,Hui Wan,Chunfeng Wen,Solène Maître,George Lee,Vishy Tirumalashetty,Emily Xue,Zizhao Zhang,Salem Haykal,Burak Gokturk,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

> The paper proposes TTD-DR, a novel framework for generating high-quality research reports by focusing on iterative refinement through a diffusion process and enhancing the quality of context with an evolutionary algorithm.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind TTD-DR is to address the performance plateau issue of deep research agents when generating complex, long-form research reports using generic test-time scaling algorithms by copying the iterative nature of human research.

**Method:** This novel framework, TTD-DR, conceptualizes research report generation as a diffusion process that begins with a preliminary draft and gets iteratively refined through a denoising process, informed by a retrieval mechanism that incorporates external information at each step. The self-evolutionary algorithm enhances each component of the agentic workflow, ensuring the generation of high-quality context.

**Result:** TTD-DR achieves state-of-the-art results on a wide array of benchmarks that require intensive search and multi-hop reasoning, significantly outperforming existing deep research agents.

**Conclusion:** The draft-centric design of TTD-DR facilitates more timely and coherent report writing processes, reducing information loss during the iterative search process and demonstrating excellence in benchmarks demanding intensive search and multi-hop reasoning.

**Abstract:** Deep research agents, powered by Large Language Models (LLMs), are rapidly
advancing; yet, their performance often plateaus when generating complex,
long-form research reports using generic test-time scaling algorithms. Drawing
inspiration from the iterative nature of human research, which involves cycles
of searching, reasoning, and revision, we propose the Test-Time Diffusion Deep
Researcher (TTD-DR). This novel framework conceptualizes research report
generation as a diffusion process. TTD-DR initiates this process with a
preliminary draft, an updatable skeleton that serves as an evolving foundation
to guide the research direction. The draft is then iteratively refined through
a "denoising" process, which is dynamically informed by a retrieval mechanism
that incorporates external information at each step. The core process is
further enhanced by a self-evolutionary algorithm applied to each component of
the agentic workflow, ensuring the generation of high-quality context for the
diffusion process. This draft-centric design makes the report writing process
more timely and coherent while reducing information loss during the iterative
search process. We demonstrate that our TTD-DR achieves state-of-the-art
results on a wide array of benchmarks that require intensive search and
multi-hop reasoning, significantly outperforming existing deep research agents.

</details>


### [10] [The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models](https://arxiv.org/abs/2507.16076)
*Marlene Lutz,Indira Sen,Georg Ahnert,Elisa Rogers,Markus Strohmaier*

Main category: cs.CL

> 本研究旨在探讨不同的人设提示策略如何影响大型语言模型在模拟不同社会人口群体时的表现，发现适当的策略可以提升模型表现，甚至小模型可能更优。研究结果对于改善大型语言模型在模拟中的表现提供了指导。

<details>
  <summary>Details</summary>

**Motivation:** 研究背景在于，人设提示在大型语言模型中被广泛用来模拟视图，但是格式的不同会影响结果，从而影响了这种模拟的真实度。通过研究，希望探索如何更加准确地实施这种模拟。

**Method:** 研究通过使用五种开放源代码的语言模型(LLMs)，系统地检查了不同的人设提示策略，特别是在角色采用格式和人口统计提示策略方面，这些因素如何影响LLMs在15个人口统计交叉群体中开放和封闭任务的模拟效果。

**Result:** 研究发现，对于边缘群体（特别是非二元性、西班牙裔和中东身份）来说，LLMs在模拟方面存在困难。然而，选择合适的人口统计提示与角色采用策略可以显著改善这些问题。具体来说，以面试风格进行提示和基于姓名的提示可以有效地减少刻板印象并提高一致性。此外，意外地发现，较小的模型如OLMo-2-7B在模拟效果上优于较大的模型如Llama-3.3-70B。

**Conclusion:** 这项研究提供了关于如何在基于LLM的模拟研究中设计社会人口学特征人设提示的实用指南。

**Abstract:** Persona prompting is increasingly used in large language models (LLMs) to
simulate views of various sociodemographic groups. However, how a persona
prompt is formulated can significantly affect outcomes, raising concerns about
the fidelity of such simulations. Using five open-source LLMs, we
systematically examine how different persona prompt strategies, specifically
role adoption formats and demographic priming strategies, influence LLM
simulations across 15 intersectional demographic groups in both open- and
closed-ended tasks. Our findings show that LLMs struggle to simulate
marginalized groups, particularly nonbinary, Hispanic, and Middle Eastern
identities, but that the choice of demographic priming and role adoption
strategy significantly impacts their portrayal. Specifically, we find that
prompting in an interview-style format and name-based priming can help reduce
stereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B
outperform larger ones such as Llama-3.3-70B. Our findings offer actionable
guidance for designing sociodemographic persona prompts in LLM-based simulation
studies.

</details>


### [11] [Efficient Compositional Multi-tasking for On-device Large Language Models](https://arxiv.org/abs/2507.16083)
*Ondrej Bohdal,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.CL

> 本文探讨了基于文本的组合多任务问题，并提出了一种名为可学习校准的有效方法，特别适用于计算资源有限的设备端应用。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机是在设备端环境下研究基于文本的组合多任务问题，特别是在每个测试示例涉及多个任务的同时执行。

**Method:** 本文提出了一种称为可学习校准（Learnable Calibration）的有效方法，特别适用于设备端应用，强调了在计算资源有限的情况下，解决方案不仅需要高效还要高性能。

**Result:** 本研究的成果包括建立了一个由四个实际相关的组合任务组成的基准，推动了大语言模型在多任务情景下的应用。

**Conclusion:** 本文的研究奠定了在多任务场景中推进大语言模型能力的基础，扩展其在复杂、资源受限使用场景中的适用性。

**Abstract:** Adapter parameters provide a mechanism to modify the behavior of machine
learning models and have gained significant popularity in the context of large
language models (LLMs) and generative AI. These parameters can be merged to
support multiple tasks via a process known as task merging. However, prior work
on merging in LLMs, particularly in natural language processing, has been
limited to scenarios where each test example addresses only a single task. In
this paper, we focus on on-device settings and study the problem of text-based
compositional multi-tasking, where each test example involves the simultaneous
execution of multiple tasks. For instance, generating a translated summary of a
long text requires solving both translation and summarization tasks
concurrently. To facilitate research in this setting, we propose a benchmark
comprising four practically relevant compositional tasks. We also present an
efficient method (Learnable Calibration) tailored for on-device applications,
where computational resources are limited, emphasizing the need for solutions
that are both resource-efficient and high-performing. Our contributions lay the
groundwork for advancing the capabilities of LLMs in real-world multi-tasking
scenarios, expanding their applicability to complex, resource-constrained use
cases.

</details>


### [12] [BIDWESH: A Bangla Regional Based Hate Speech Detection Dataset](https://arxiv.org/abs/2507.16183)
*Azizul Hakim Fayaz,MD. Shorif Uddin,Rayhan Uddin Bhuiyan,Zakia Sultana,Md. Samiul Islam,Bidyarthi Paul,Tashreef Muhammad,Shahriar Manzoor*

Main category: cs.CL

> 该研究提出了BIDWESH，这是首个用于多方言孟加拉语仇恨言论检测的语料库，它填补了标准孟加拉语之外，特别是在区域方言中仇恨言论检测能力不足的空白。

<details>
  <summary>Details</summary>

**Motivation:** 当前的标准孟加拉语仇恨言论检测数据集和系统无法处理方言中的非正式和文化丰富的表达，导致检测能力有限和有偏见的管理，许多有害内容未被识别。本研究正是为了弥补这一空白。

**Method:** 为了填补标准孟加拉语之外，特别是区域方言如Barishal, Noakhali和Chittagong的仇恨言论检测的空白，本研究构建了BIDWESH。BIDWESH是第一个多方言的孟加拉语仇恨言论数据集，通过将BD-SHS语料库中的9,183个实例翻译和标注到这三个主要区域方言中。每个条目都经过人工核查和标记，涵盖了仇恨存在的确认、类型（诽谤、性别、宗教、暴力号召）和目标群体（个人、男性、女性、群体），确保了语言和语境的准确无误。

**Result:** BIDWESH为推动孟加拉语仇恨言论检测提供了语言丰富、平衡和包容的资源。

**Conclusion:** BIDWESH为开发敏感于方言的自然语言处理工具奠定了基础，为资源较少语言环境中的公平和情境感知内容管理做出了重要贡献。

**Abstract:** Hate speech on digital platforms has become a growing concern globally,
especially in linguistically diverse countries like Bangladesh, where regional
dialects play a major role in everyday communication. Despite progress in hate
speech detection for standard Bangla, Existing datasets and systems fail to
address the informal and culturally rich expressions found in dialects such as
Barishal, Noakhali, and Chittagong. This oversight results in limited detection
capability and biased moderation, leaving large sections of harmful content
unaccounted for. To address this gap, this study introduces BIDWESH, the first
multi-dialectal Bangla hate speech dataset, constructed by translating and
annotating 9,183 instances from the BD-SHS corpus into three major regional
dialects. Each entry was manually verified and labeled for hate presence, type
(slander, gender, religion, call to violence), and target group (individual,
male, female, group), ensuring linguistic and contextual accuracy. The
resulting dataset provides a linguistically rich, balanced, and inclusive
resource for advancing hate speech detection in Bangla. BIDWESH lays the
groundwork for the development of dialect-sensitive NLP tools and contributes
significantly to equitable and context-aware content moderation in low-resource
language settings.

</details>


### [13] [Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task](https://arxiv.org/abs/2507.16196)
*Jared Moore,Ned Cooper,Rasmus Overmark,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

> 该研究通过MindGames实验发现，尽管在给定他人偏好的情况下大语言模型比人类在规划方面表现更好，但在需要理解他人心智状态的任务上，人类显著优于这些模型。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于识别和评估语言模型在模拟社交推理能力方面的局限性，特别是它们在理解和预测他人行为方面的能力。

**Method:** 该研究提出了一个名为MindGames的新任务，用于评估代理在理解和影响他人的信念和欲望方面的能力，这被称为'规划理论心智'（PToM）任务。

**Result:** 研究发现人类在PToM任务上显著优于o1-preview（一种大语言模型），成功率高出约11%，p值为0.006。

**Conclusion:** 研究的结论是，当前的大型语言模型在模仿人类的社会推理能力上存在显著差距，尤其是在需要理解和干预他人心理状态的情境中。

**Abstract:** Recent evidence suggests Large Language Models (LLMs) display Theory of Mind
(ToM) abilities. Most ToM experiments place participants in a spectatorial
role, wherein they predict and interpret other agents' behavior. However, human
ToM also contributes to dynamically planning action and strategically
intervening on others' mental states. We present MindGames: a novel `planning
theory of mind' (PToM) task which requires agents to infer an interlocutor's
beliefs and desires to persuade them to alter their behavior. Unlike previous
evaluations, we explicitly evaluate use cases of ToM. We find that humans
significantly outperform o1-preview (an LLM) at our PToM task (11% higher;
$p=0.006$). We hypothesize this is because humans have an implicit causal model
of other agents (e.g., they know, as our task requires, to ask about people's
preferences). In contrast, o1-preview outperforms humans in a baseline
condition which requires a similar amount of planning but minimal mental state
inferences (e.g., o1-preview is better than humans at planning when already
given someone's preferences). These results suggest a significant gap between
human-like social reasoning and LLM abilities.

</details>


### [14] [WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability](https://arxiv.org/abs/2507.16199)
*Zipeng Ling,Yuehao Tang,Shuliang Liu,Junqi Yang,Shenghong Fu,Yao Wan,Kejia Huang,Zhichao Hou,Xuming Hu*

Main category: cs.CL

> 本文提出了一种新的评估框架，用于区分LLM产生'未知'回复的真实原因，并通过框架测试和引导策略来探索模型的实际推理能力和提升空间。

<details>
  <summary>Details</summary>

**Motivation:** 该论文的动机在于，当前的评估主要集中在LLM的'未知'回复是否诚实上，而不是探索这些回复为何出现。作者们认为这两个方面并不相同，一种是输入问题本质上未确定，另一种是问题可以解决但模型未能解决。

**Method:** 我们引入了一个新的框架，旨在量化'未知'回复中模型能力不足所占的比例，并测试了通过引导刺激是否能够将这些回复转化为正确的'已知'或本质上不确定的答案。通过区分不确定性的来源，我们的方法能够更清晰地展示出LLM的推理极限及其改进潜力。

**Result:** 通过理论分析和推理任务上不同LLMs的表现，研究人员发现可以通过他们设定的基准框架和不同方法测试模型是否能达到预测的准确性。这为研究LLM的实际推理能力提供了新的视角和解决方案思路。

**Conclusion:** 研究结论强调了此框架的意义在于探索了LLMs真正的推理能力，并提供了理解和解决'模糊感知'现象的新视角。通过这种方法，研究人员得以更清晰地了解模型的限制和提升潜力。

**Abstract:** Large Language Models (LLMs) frequently output the label \emph{Unknown}, yet
current evaluations focus almost exclusively on whether such answers are
\emph{honest} rather than why they arise. This blurs two distinct cases: (i) an
input that is genuinely indeterminate and (ii) a solvable problem that the
model fails to resolve. We call this phenomenon \emph{Vague Perception}. And
thus we introduce a framework that quantifies the proportion of \emph{Unknown}
responses attributable to model incapacity and tests whether guided stimulation
can convert them into either correct (\emph{Known}) or intrinsically
indeterminate outcomes. By separating these sources of uncertainty, our method
provides a clearer picture of LLM reasoning limits and their potential for
improvement. As we get a theoretical accuracy of reasoning task on different
LLMs, we apply different methods to test whether the model can reach the
accuracy given a baseline framework. Our work is meaningful in exploring the
true reasoning ability of LLMs and providing a new perspective on solving the
\emph{Vague Perception} phenomenon.

</details>


### [15] [Towards Compute-Optimal Many-Shot In-Context Learning](https://arxiv.org/abs/2507.16217)
*Shahriar Golchin,Yanfei Chen,Rujun Han,Manan Gandhi,Tianli Yu,Swaroop Mishra,Mihai Surdeanu,Rishabh Agarwal,Chen-Yu Lee,Tomas Pfister*

Main category: cs.CL

> The paper enhances ICL performance using targeted demonstration selection strategies that are cost-effective.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the paper is to address the high inference costs associated with many-shot ICL and improve performance by refining the selection of training demonstrations without overburdening computational resources.

**Method:** The paper proposes two demonstration selection strategies for many-shot in-context learning (ICL) to improve performance and reduce computational overhead. The first strategy combines similar demonstrations with a larger set of random, cached demonstrations. The second strategy further improves this by using demonstrations selected based on centroids derived from test sample representations using k-means clustering.

**Result:** Experiments with the Gemini Pro and Flash models across various datasets demonstrate consistent outperformance of random selection and achieve or surpass the best-performing selection methods while enabling caching and significantly reducing inference costs (up to ten times).

**Conclusion:** The strategies for demonstration selection proposed in the paper provide a means to balance performance gains with computational efficiency, effectively reducing inference costs while maintaining or improving model performance in many-shot ICL scenarios.

**Abstract:** Long-context large language models (LLMs) are able to process inputs
containing up to several million tokens. In the scope of in-context learning
(ICL), this translates into using hundreds/thousands of demonstrations in the
input prompt, enabling many-shot ICL. In practice, a fixed set of
demonstrations is often selected at random in many-shot settings due to (1)
high inference costs, (2) the benefits of caching and reusing computations, and
(3) the similar performance offered by this strategy compared to others when
scaled. In this work, we propose two straightforward strategies for
demonstration selection in many-shot ICL that improve performance with minimal
computational overhead. Our first method combines a small number of
demonstrations, selected based on their similarity to each test sample, with a
disproportionately larger set of random demonstrations that are cached. The
second strategy improves the first by replacing random demonstrations with
those selected using centroids derived from test sample representations via
k-means clustering. Our experiments with Gemini Pro and Flash across several
datasets indicate that our strategies consistently outperform random selection
and surpass or match the most performant selection approach while supporting
caching and reducing inference cost by up to an order of magnitude. We also
show that adjusting the proportion of demonstrations selected based on
different criteria can balance performance and inference cost in many-shot ICL.

</details>


### [16] [FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents](https://arxiv.org/abs/2507.16248)
*Run Sun,Zuo Bai,Wentao Zhang,Yuxiang Zhang,Li Zhao,Shan Sun,Zhengwen Qiu*

Main category: cs.CL

> 本文提出了一个针对金融研究AI代理的评估系统FinResearchBench，它基于逻辑树，提供全面可靠的自动评估，并包含70个典型的金融研究问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估框架和基准无法系统和自动地评估研究型AI代理的能力，特别是针对金融研究这种具有独特复杂性和细微差别的领域。为了填补这个空白，本研究提出了FinResearchBench。

**Method:** 本研究提出了FinResearchBench，这是一种基于逻辑树的Agent-as-a-Judge系统，专门针对金融研究中的AI代理进行评估。该系统能够全面自动地评估代理在金融研究中7个关键类型任务的表现。

**Result:** 该系统提供了对金融研究代理能力的全面、可靠和自动评估。

**Conclusion:** 本研究贡献了两个方面的创新：(1) 首次提出并创新地使用Agent-as-a-Judge系统，通过提取研究结果的逻辑树来实现全面、可靠和稳健的评估；(2) 针对金融研究领域的特点，覆盖了70个典型的金融研究问题，跨越7个最常见的任务类型。

**Abstract:** Recently, AI agents are rapidly evolving in intelligence and widely used in
professional research applications, such as STEM, software development,
finance, etc. Among these AI agents, deep research agent is a key category as
it can perform long-horizon tasks and solve problems of greater complexity.
However, there are few evaluation frameworks and benchmarks that systematically
and automatically investigate the capabilities of these research agents.
Furthermore, financial research problems have distinct complexity and subtlety.
To fill in the gap, we propose FinResearchBench, which is a logic tree based
Agent-as-a-Judge and targets specifically for the financial research agents. It
provides a comprehensive and automatic assessment of the research agents across
7 key types of tasks in the financial research domain. The contributions of
this work are two-folded: (1) the first and innovative Agent-as-a-Judge system
that extracts the logic tree of the research outcome and uses it as the
intermediate information to present a comprehensive, reliable and robust
evaluation; (2) finance oriented that it covers 70 typical financial research
questions, spreading across 7 frequently encountered types of tasks in the
domain.

</details>


### [17] [Efficient RL for optimizing conversation level outcomes with an LLM-based tutor](https://arxiv.org/abs/2507.16252)
*Hyunji Nam,Omer Gottesman,Amy Zhang,Dean Foster,Emma Brunskill,Lyle Ungar*

Main category: cs.CL

> 本研究提出了一种改进LLM为基础的辅导系统的方法，通过低维度潜在状态表示和长期策略优化，以改善多轮对话场景下的表现。实验表明该方法优于基于提示的方法，并具有较低的计算需求。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于人类反馈强化学习框架构建的语言模型通常根据即时的人类偏好来优化响应，这种方法在多轮对话场景（如在线数学辅导）中效果不佳

**Method:** 通过使用学生对话历史的低维度潜在状态表示，优化长期策略，以基于潜在状态确定高层次动作，从而改进LLM基础的辅导系统

**Result:** 实验结果表明，这些改进在LLM模拟的辅导任务中相比提示法（prompting）能够带来更好的长期表现

**Conclusion:** 本文提出的方法可以更好地使辅导系统的行为与引导学生自学解决问题这一长期目标保持一致，并且模型较轻量，计算资源需求较少

**Abstract:** Large language models (LLMs) built on existing reinforcement learning with
human feedback (RLHF) frameworks typically optimize responses based on
immediate turn-level human preferences. However, this approach falls short in
multi-turn dialogue settings, such as online math tutoring. We propose a method
to enhance LLM-based tutors by representing the dialogue history with a
lower-dimensional latent state representation of a student and optimizing a
long-term policy to determine high-level actions based on the latent state. The
goal is to better align the tutor's behavior with the long-term objective of
guiding the student towards solving a target math problem on their own. Our
model is lightweight, requiring less computational resources than prior work of
training the tutor policy end-to-end to directly output the tutor's next
utterance. Our experiment results demonstrate that these modifications lead to
improved long-term outcomes compared to prompting in LLM-simulated tutoring
tasks.

</details>


### [18] [iShumei-Chinchunmei at SemEval-2025 Task 4: A balanced forgetting and retention multi-task framework using effective unlearning loss](https://arxiv.org/abs/2507.16263)
*Yujian Sun,Tian Li*

Main category: cs.CL

> 研究了如何更有效地从大型语言模型（LLM）中删除敏感内容同时保留其标准能力，并提出了有效的遗忘损失方法，在比赛中排名第五。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型（LLM）的广泛应用，如何让LLM忘记在预训练过程中记忆的非合规数据成为一个亟待解决的问题。该研究旨在提高在有限计算资源下有效地从LLM中擦除敏感信息的技术。

**Method:** 提出了一种更可控的遗忘损失函数——有效遗忘损失，并探索了将其与各种技术相结合以实现更高效和可控遗忘的方法。

**Result:** 系统在比赛中排名第五。研究展示了有效遗忘损失方法在删除敏感信息方面的潜力。

**Conclusion:** 研究引入了有效遗忘损失方法来控制大型语言模型的遗忘过程，并证明了其在高效和可控遗忘方面的有效性。

**Abstract:** As the Large Language Model (LLM) gains widespread adoption, increasing
attention has been given to the challenge of making LLM forget non-compliant
data memorized during its pre-training. Machine Unlearning focuses on
efficiently erasing sensitive information from LLM under limited computational
resources. To advance research in this area, SemEval 2025 Task 4: "Unlearning
Sensitive Content from Large Language Models" introduces three unlearning
datasets and establishes a benchmark by evaluating both forgetting
effectiveness and the preservation of standard capabilities. In this work, we
propose a more controllable forgetting loss, Effective Unlearning Loss, and
explore its integration with various techniques to achieve more efficient and
controlled unlearning. Our system ultimately ranked 5th on the competition
leaderboard.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [19] [Salience Adjustment for Context-Based Emotion Recognition](https://arxiv.org/abs/2507.15878)
*Bin Han,Jonathan Gratch*

Main category: cs.CV

> 论文提出了一种调整显著性的框架，利用贝叶斯提示整合和视觉-语言模型在动态社交环境下进行情绪识别，测试结果表明这种方法可以提升情绪识别性能。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机在于理解和解决情绪识别中面部表情和情境线索相互作用的复杂性，特别是在动态社交背景下情绪识别的挑战。

**Method:** 本论文提出了一个基于贝叶斯提示整合（BCI）和视觉-语言模型（VLMs）的调整显著性框架，用于在动态社交环境中识别情绪。该框架能够根据面部线索的表达性动态地加权面部和情境信息。

**Result:** 通过使用人类标注和自动情绪识别系统在囚徒困境场景中的测试，发现引入显著性调整可以提高情绪识别的效果。

**Conclusion:** 研究结果表明，通过整合显著性调整，可以提升情绪识别性能，为未来的社交环境的扩展研究和多模态应用提供了良好的方向。

**Abstract:** Emotion recognition in dynamic social contexts requires an understanding of
the complex interaction between facial expressions and situational cues. This
paper presents a salience-adjusted framework for context-aware emotion
recognition with Bayesian Cue Integration (BCI) and Visual-Language Models
(VLMs) to dynamically weight facial and contextual information based on the
expressivity of facial cues. We evaluate this approach using human annotations
and automatic emotion recognition systems in prisoner's dilemma scenarios,
which are designed to evoke emotional reactions. Our findings demonstrate that
incorporating salience adjustment enhances emotion recognition performance,
offering promising directions for future research to extend this framework to
broader social contexts and multimodal applications.

</details>


### [20] [Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark](https://arxiv.org/abs/2507.15882)
*Goeric Huybrechts,Srikanth Ronanki,Sai Muralidhar Jayanthi,Jack Fitzgerald,Srinivasan Veeravanallur*

Main category: cs.CV

> 我们提出了Document Haystack，这是一个评估视觉语言模型处理长文档能力的新基准测试。它包括5到200页的文档，支撑了一种客观、自动的评估框架，并探讨了该领域的研究前景。

<details>
  <summary>Details</summary>

**Motivation:** 当前，多模态大型语言模型在处理长文档方面仍然研究较少，缺乏合适的基准测试。为了解决这个问题，本研究提出了Document Haystack基准测试。

**Method:** 我们介绍了一个名为Document Haystack的基准测试，用于评估视觉语言模型处理长且视觉复杂的文档的能力。该基准测试包含从5页到200页不等的文档，并在文档的不同深度处插入纯文本或文本+图像的“针”，以挑战VLMs的检索能力。

**Result:** 介绍了Document Haystack数据集的构建和特性，并展示了前沿视觉语言模型的初步结果，同时探讨了这一领域潜在的研究方向。

**Conclusion:** Document Haystack作为一个新的基准测试，展示了其在评估视觉语言模型处理长文档能力方面的价值，并为未来的研究提供了指导。

**Abstract:** The proliferation of multimodal Large Language Models has significantly
advanced the ability to analyze and understand complex data inputs from
different modalities. However, the processing of long documents remains
under-explored, largely due to a lack of suitable benchmarks. To address this,
we introduce Document Haystack, a comprehensive benchmark designed to evaluate
the performance of Vision Language Models (VLMs) on long, visually complex
documents. Document Haystack features documents ranging from 5 to 200 pages and
strategically inserts pure text or multimodal text+image "needles" at various
depths within the documents to challenge VLMs' retrieval capabilities.
Comprising 400 document variants and a total of 8,250 questions, it is
supported by an objective, automated evaluation framework. We detail the
construction and characteristics of the Document Haystack dataset, present
results from prominent VLMs and discuss potential research avenues in this
area.

</details>


### [21] [PAT++: a cautionary tale about generative visual augmentation for Object Re-identification](https://arxiv.org/abs/2507.15888)
*Leonardo Santiago Benitez Pereira,Arathy Jeevan*

Main category: cs.CV

> 研究评估了身份保留图像生成在对象重新识别中的有效性，提出PAT++管道，实验表明性能下降，这揭示了生成模型的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然生成性数据增强已经在多个视觉任务中展现了其优势，但其在对象重新识别（其中保持细粒度的视觉细节至关重要）方面的影响仍然鲜为人知。

**Method:** 该研究提出了一种新的管道PAT++, 它结合了Diffusion Self-Distillation与已建立的Part-Aware Transformer方法，用于对象重新识别任务。

**Result:** 通过使用Urban Elements ReID Challenge数据集进行广泛实验，结果表明使用生成图像进行模型训练和查询扩展导致了性能下降，这归因于域转换和未能保留标识定义特征。

**Conclusion:** 研究结果挑战了关于生成模型在细粒度识别任务中的可转移性的假设，并暴露了当前视觉增强方法在身份保持应用中的关键限制。

**Abstract:** Generative data augmentation has demonstrated gains in several vision tasks,
but its impact on object re-identification - where preserving fine-grained
visual details is essential - remains largely unexplored. In this work, we
assess the effectiveness of identity-preserving image generation for object
re-identification. Our novel pipeline, named PAT++, incorporates Diffusion
Self-Distillation into the well-established Part-Aware Transformer. Using the
Urban Elements ReID Challenge dataset, we conduct extensive experiments with
generated images used for both model training and query expansion. Our results
show consistent performance degradation, driven by domain shifts and failure to
retain identity-defining features. These findings challenge assumptions about
the transferability of generative models to fine-grained recognition tasks and
expose key limitations in current approaches to visual augmentation for
identity-preserving applications.

</details>


### [22] [Local Dense Logit Relations for Enhanced Knowledge Distillation](https://arxiv.org/abs/2507.15911)
*Liuchi Xu,Kang Liu,Jinshuai Liu,Lu Wang,Lisheng Xu,Jun Cheng*

Main category: cs.CV

> A new logit distillation method named LDRLD is proposed, enhancing the student model's performance by capturing detailed logit information and critical inter-class relationships dynamically.

<details>
  <summary>Details</summary>

**Motivation:** To delve deeper into fine-grained logit knowledge relationships and improve student model learning.

**Method:** Local Dense Relational Logit Distillation (LDRLD) captures inter-class relationships through recursively decoupling and recombining logit information. An Adaptive Decay Weight (ADW) strategy is introduced to dynamically adjust weights for inter-class pairs using Inverse Rank Weighting (IRW) and Exponential Rank Decay (ERD).

**Result:** Extensive experiments on CIFAR-100, ImageNet-1K, and Tiny-ImageNet show that the method outperforms state-of-the-art logit-based distillation approaches.

**Conclusion:** The proposed LDRLD method improves student performance by transferring detailed logit knowledge and focusing on critical relationships, outperforming existing methods on various datasets.

**Abstract:** State-of-the-art logit distillation methods exhibit versatility, simplicity,
and efficiency. Despite the advances, existing studies have yet to delve
thoroughly into fine-grained relationships within logit knowledge. In this
paper, we propose Local Dense Relational Logit Distillation (LDRLD), a novel
method that captures inter-class relationships through recursively decoupling
and recombining logit information, thereby providing more detailed and clearer
insights for student learning. To further optimize the performance, we
introduce an Adaptive Decay Weight (ADW) strategy, which can dynamically adjust
the weights for critical category pairs using Inverse Rank Weighting (IRW) and
Exponential Rank Decay (ERD). Specifically, IRW assigns weights inversely
proportional to the rank differences between pairs, while ERD adaptively
controls weight decay based on total ranking scores of category pairs.
Furthermore, after the recursive decoupling, we distill the remaining
non-target knowledge to ensure knowledge completeness and enhance performance.
Ultimately, our method improves the student's performance by transferring
fine-grained knowledge and emphasizing the most critical relationships.
Extensive experiments on datasets such as CIFAR-100, ImageNet-1K, and
Tiny-ImageNet demonstrate that our method compares favorably with
state-of-the-art logit-based distillation approaches. The code will be made
publicly available.

</details>


### [23] [An empirical study for the early detection of Mpox from skin lesion images using pretrained CNN models leveraging XAI technique](https://arxiv.org/abs/2507.15915)
*Mohammad Asifur Rahim,Muhammad Nazmul Arefin,Md. Mizanur Rahman,Md Ali Hossain,Ahmed Moustafa*

Main category: cs.CV

> 通过使用预训练的CNN模型和迁移学习，研究探讨了猴痘早期检测的有效性，并通过Grad-CAM展示了模型的关键特征提取能力。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在评估预训练的CNN模型在猴痘早期检测中的有效性，并通过Grad-CAM技术增强模型的可解释性。

**Method:** 此研究使用了迁移学习技术微调了几个预训练的CNN模型（VGG16, VGG19, InceptionV3, MobileNetV2），用于猴痘的早期检测。通过冻结模型的初始层并添加自定义层来避免过拟合，并使用了MSLD和MSLD v2.0两个数据集进行训练和验证。此外，采用Grad-CAM技术来提高模型的可解释性。

**Result:** 研究结果显示，InceptionV3在二分类数据集中表现最佳，准确率达到95%；MobileNetV2在多分类数据集中表现最优，准确率达到了93%。Grad-CAM成功地突出显示了关键的图像区域。然而，某些模型显示出过拟合的倾向，表现为训练损失与验证损失之间存在差异。

**Conclusion:** 研究强调了预训练的CNN模型在猴痘检测中的潜力，以及XAI技术的价值。未来的工作应该改进数据集的限制，纳入多模态数据，并探索其他可解释性技术以提高诊断的可靠性和模型的透明度。

**Abstract:** Context: Mpox is a zoonotic disease caused by the Mpox virus, which shares
similarities with other skin conditions, making accurate early diagnosis
challenging. Artificial intelligence (AI), especially Deep Learning (DL), has a
strong tool for medical image analysis; however, pre-trained models like CNNs
and XAI techniques for mpox detection is underexplored. Objective: This study
aims to evaluate the effectiveness of pre-trained CNN models (VGG16, VGG19,
InceptionV3, MobileNetV2) for the early detection of monkeypox using binary and
multi-class datasets. It also seeks to enhance model interpretability using
Grad-CAM an XAI technique. Method: Two datasets, MSLD and MSLD v2.0, were used
for training and validation. Transfer learning techniques were applied to
fine-tune pre-trained CNN models by freezing initial layers and adding custom
layers for adapting the final features for mpox detection task and avoid
overfitting. Models performance were evaluated using metrics such as accuracy,
precision, recall, F1-score and ROC. Grad-CAM was utilized for visualizing
critical features. Results: InceptionV3 demonstrated the best performance on
the binary dataset with an accuracy of 95%, while MobileNetV2 outperformed on
the multi-class dataset with an accuracy of 93%. Grad-CAM successfully
highlighted key image regions. Despite high accuracy, some models showed
overfitting tendencies, as videnced by discrepancies between training and
validation losses. Conclusion: This study underscores the potential of
pre-trained CNN models in monkeypox detection and the value of XAI techniques.
Future work should address dataset limitations, incorporate multimodal data,
and explore additional interpretability techniques to improve diagnostic
reliability and model transparency

</details>


### [24] [A Lightweight Face Quality Assessment Framework to Improve Face Verification Performance in Real-Time Screening Applications](https://arxiv.org/abs/2507.15961)
*Ahmed Aman Ibrahim,Hamad Mansour Alawar,Abdulnasser Abbas Zehi,Ahmed Mohammad Alkendi,Bilal Shafi Ashfaq Ahmed Mirza,Shan Ullah,Ismail Lujain Jaleel,Hassan Ugail*

Main category: cs.CV

> 本文提出了一种轻量且有效的自动面部图像质量评估框架，旨在预筛选低质量面部图像。利用标准化面部特征点和随机森林回归分类器，实现了96.67%的质量评估精度，大幅降低了误拒率并改善了ArcFace面验证模型的余弦相似度得分。在包含600多名受试者的实际数据集上验证了方法的有效性，有效缓解了低质量图像的影响，同时保持计算效率。该框架特别解决了实时筛选中的面部分辨率变化和姿态偏差问题。

<details>
  <summary>Details</summary>

**Motivation:** 面部图像质量对面部验证系统的准确性和可靠性至关重要，尤其是在实时监控和身份验证等应用中。低质量的面部图像，如运动模糊、光照条件差、部分遮挡和极端姿态变化，会导致错误接受率和拒绝率上升，严重影响面部识别模型的表现。为了改善这种情况，作者提出了一种新的自动面部质量评估框架。

**Method:** 作者的方法利用标准化的面部特征点和随机森林回归分类器，先对输入的面部图像进行质量评估，在低质量的面像被传递到验证流水线之前进行预筛选。

**Result:** 实验表明，该模型实现了96.67%的质量评估精度，并且在与ArcFace模型结合使用时带来了虚假拒绝率大幅下降等性能改进。在包含实际环境中采集的600多名参与者的数据集上的实验验证了这一方法的有效性。

**Conclusion:** 该框架在保持计算效率的同时，有效处理了低质量面部图像，大大减少了误拒率，并提高了余弦相似度得分。特别是，解决了实际监控场景中的面部分辨率变化和姿态偏差问题。这种方法优于现有面部图像质量评估技术。

**Abstract:** Face image quality plays a critical role in determining the accuracy and
reliability of face verification systems, particularly in real-time screening
applications such as surveillance, identity verification, and access control.
Low-quality face images, often caused by factors such as motion blur, poor
lighting conditions, occlusions, and extreme pose variations, significantly
degrade the performance of face recognition models, leading to higher false
rejection and false acceptance rates. In this work, we propose a lightweight
yet effective framework for automatic face quality assessment, which aims to
pre-filter low-quality face images before they are passed to the verification
pipeline. Our approach utilises normalised facial landmarks in conjunction with
a Random Forest Regression classifier to assess image quality, achieving an
accuracy of 96.67\%. By integrating this quality assessment module into the
face verification process, we observe a substantial improvement in performance,
including a comfortable 99.7\% reduction in the false rejection rate and
enhanced cosine similarity scores when paired with the ArcFace face
verification model. To validate our approach, we have conducted experiments on
a real-world dataset collected comprising over 600 subjects captured from CCTV
footage in unconstrained environments within Dubai Police. Our results
demonstrate that the proposed framework effectively mitigates the impact of
poor-quality face images, outperforming existing face quality assessment
techniques while maintaining computational efficiency. Moreover, the framework
specifically addresses two critical challenges in real-time screening:
variations in face resolution and pose deviations, both of which are prevalent
in practical surveillance scenarios.

</details>


### [25] [FW-VTON: Flattening-and-Warping for Person-to-Person Virtual Try-on](https://arxiv.org/abs/2507.16010)
*Zheng Wang,Xianbing Sun,Shengyi Wu,Jiahui Zhan,Jianlou Si,Chi Zhang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

> 本文提出了一种新的虚拟试穿方法Flattening-and-Warping Virtual Try-On (FW-VTON)，适用于人物到人物的试穿任务，该方法在实验评估中表现出色，取得了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的虚拟试穿方法主要专注于衣物到人物的试穿任务，这需要使用平面化的衣物表示。相比之下，本文引入了一种新的任务，即人物到人物的试穿任务。这项任务仅涉及两个输入图像：一个描绘目标人物，另一个展示不同人物身上的衣物。

**Method:** Flattening-and-Warping Virtual Try-On (FW-VTON)方法，包括三个阶段：1) 从源图像中提取平面化的衣物图像；2) 对衣物进行变形以匹配目标姿势；3) 将变形后的衣物无缝整合到目标人物上。

**Result:** 实验评估表明FW-VTON达到了最先进的性能，其在定性和定量评估方面均取得了优越的结果，并且在衣物提取子任务中也表现出色。

**Conclusion:** FW-VTON方法通过三个阶段处理人物到人物的试穿任务，并且实验表明其性能优于现有方法。

**Abstract:** Traditional virtual try-on methods primarily focus on the garment-to-person
try-on task, which requires flat garment representations. In contrast, this
paper introduces a novel approach to the person-to-person try-on task. Unlike
the garment-to-person try-on task, the person-to-person task only involves two
input images: one depicting the target person and the other showing the garment
worn by a different individual. The goal is to generate a realistic combination
of the target person with the desired garment. To this end, we propose
Flattening-and-Warping Virtual Try-On (\textbf{FW-VTON}), a method that
operates in three stages: (1) extracting the flattened garment image from the
source image; (2) warping the garment to align with the target pose; and (3)
integrating the warped garment seamlessly onto the target person. To overcome
the challenges posed by the lack of high-quality datasets for this task, we
introduce a new dataset specifically designed for person-to-person try-on
scenarios. Experimental evaluations demonstrate that FW-VTON achieves
state-of-the-art performance, with superior results in both qualitative and
quantitative assessments, and also excels in garment extraction subtasks.

</details>


### [26] [Is Tracking really more challenging in First Person Egocentric Vision?](https://arxiv.org/abs/2507.16015)
*Matteo Dunnhofer,Zaira Manigrasso,Christian Micheloni*

Main category: cs.CV

> 研究引入了新的基准测试，以便更精确地分离第一人称视觉视角的挑战与更广泛的人类-物体活动理解领域中的挑战，以推动该任务更针对性的发展。

<details>
  <summary>Details</summary>

**Motivation:** 近期的研究表明，第一人称视角的视觉理解在物体跟踪和分割任务中提出了独特的挑战。然而，这些挑战是否仅由第一人称视角造成，或与人类-物体活动的本质相关尚不清楚。

**Method:** 引入了一个新的基准测试研究，旨在区分第一人称视角与人类-物体活动领域带来的挑战。

**Result:** 该评估策略能够更精确地分离出与第一人称视角相关的问题，以及与人类-物体活动理解领域相关的问题。

**Conclusion:** 通过区分这两方面的困难，本研究为识别第一人称视觉跟踪和分割任务中的真实困难提供了更深入的见解，从而促进更有针对性的进步。

**Abstract:** Visual object tracking and segmentation are becoming fundamental tasks for
understanding human activities in egocentric vision. Recent research has
benchmarked state-of-the-art methods and concluded that first person egocentric
vision presents challenges compared to previously studied domains. However,
these claims are based on evaluations conducted across significantly different
scenarios. Many of the challenging characteristics attributed to egocentric
vision are also present in third person videos of human-object activities. This
raises a critical question: how much of the observed performance drop stems
from the unique first person viewpoint inherent to egocentric vision versus the
domain of human-object activities? To address this question, we introduce a new
benchmark study designed to disentangle such factors. Our evaluation strategy
enables a more precise separation of challenges related to the first person
perspective from those linked to the broader domain of human-object activity
understanding. By doing so, we provide deeper insights into the true sources of
difficulty in egocentric tracking and segmentation, facilitating more targeted
advancements on this task.

</details>


### [27] [Artifacts and Attention Sinks: Structured Approximations for Efficient Vision Transformers](https://arxiv.org/abs/2507.16018)
*Andrew Lu,Wentinn Liao,Liuhui Wang,Huzheng Yang,Jianbo Shi*

Main category: cs.CV

> 研究视觉变压器中的标记现象，提出了一种无需训练的注意力机制方法 Fast Nystr"om 注意力，并提出屏蔽策略来减少干扰，提高效果。

<details>
  <summary>Details</summary>

**Motivation:** 理解视觉变压器的操作机制并提高其性能，减少计算开销。

**Method:** 通过分析视觉变压器中的 '巨大标记' 和 '工件标记' 来提出新的注意力机制方法 Fast Nystr"om 注意力（FNA），该方法不需训练即可近似自注意力机制，同时提出了一种屏蔽策略来减少这些标记带来的干扰。

**Result:** 实验表明，Fast Nystr"om 注意力方法和屏蔽策略可以在保持良好性能的同时大幅度减少计算资源的消耗。

**Conclusion:** 提出的 Fast Nystr"om 注意力方法和屏蔽策略，可以有效地减少计算开销，同时在检索、分类、分割和视觉问题回答（VQA）等多个任务上保持竞争力。

**Abstract:** Vision transformers have emerged as a powerful tool across a wide range of
applications, yet their inner workings remain only partially understood. In
this work, we examine the phenomenon of massive tokens - tokens with
exceptionally high activation norms that act as attention sinks - and artifact
tokens that emerge as a byproduct during inference. Our analysis reveals that
these tokens mutually suppress one another through the attention mechanism,
playing a critical role in regulating information flow within the network.
Leveraging these insights, we introduce Fast Nystr\"om Attention (FNA), a
training-free method that approximates self-attention in linear time and space
by exploiting the structured patterns formed by massive and artifact tokens.
Additionally, we propose a masking strategy to mitigate noise from these
tokens, yielding modest performance gains at virtually no cost. We evaluate our
approach on popular pretrained vision backbones and demonstrate competitive
performance on retrieval, classification, segmentation, and visual question
answering (VQA), all while reducing computational overhead.

</details>


### [28] [Discovering and using Spelke segments](https://arxiv.org/abs/2507.16038)
*Rahul Venkatesh,Klemen Kotar,Lilian Naing Chen,Seungwoo Kim,Luca Thomas Wheeler,Jared Watrous,Ashley Xu,Gia Ancone,Wanhee Lee,Honglin Chen,Daniel Bear,Stefan Stojanov,Daniel Yamins*

Main category: cs.CV

> 本文介绍了一个新的数据集SpelkeBench和一个新模型SpelkeNet，用来识别图像中的Spelke物体，并展示了这种方法在物理对象操作任务中的实用性。

<details>
  <summary>Details</summary>

**Motivation:** 研究发现人类倾向于根据Spelke物体（物理上协同移动的物体组）感知世界，而不是基于类别的特定惯例。这可能更适合于操作和规划等任务。

**Method:** 构建了SpelkeNet，这是一种视觉世界模型，旨在预测未来的运动分布，从而估计两种关键概念：运动能力图和预期位移图，用于发现Spelke物体。

**Result:** SpelkeNet在SpelkeBench上优于监督基线模型如SegmentAnything（SAM），并且在使用各种现成的对象操作模型时，在3DEditBench基准上表现出色。

**Conclusion:** Spelke物体概念不仅在理论上有重要意义，而且对于实际任务，特别是物理对象操作应用中的下游任务，具有实际用途。

**Abstract:** Segments in computer vision are often defined by semantic considerations and
are highly dependent on category-specific conventions. In contrast,
developmental psychology suggests that humans perceive the world in terms of
Spelke objects--groupings of physical things that reliably move together when
acted on by physical forces. Spelke objects thus operate on category-agnostic
causal motion relationships which potentially better support tasks like
manipulation and planning. In this paper, we first benchmark the Spelke object
concept, introducing the SpelkeBench dataset that contains a wide variety of
well-defined Spelke segments in natural images. Next, to extract Spelke
segments from images algorithmically, we build SpelkeNet, a class of visual
world models trained to predict distributions over future motions. SpelkeNet
supports estimation of two key concepts for Spelke object discovery: (1) the
motion affordance map, identifying regions likely to move under a poke, and (2)
the expected-displacement map, capturing how the rest of the scene will move.
These concepts are used for "statistical counterfactual probing", where diverse
"virtual pokes" are applied on regions of high motion-affordance, and the
resultant expected displacement maps are used define Spelke segments as
statistical aggregates of correlated motion statistics. We find that SpelkeNet
outperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench.
Finally, we show that the Spelke concept is practically useful for downstream
applications, yielding superior performance on the 3DEditBench benchmark for
physical object manipulation when used in a variety of off-the-shelf object
manipulation models.

</details>


### [29] [Disrupting Semantic and Abstract Features for Better Adversarial Transferability](https://arxiv.org/abs/2507.16052)
*Yuyang Luo,Xiaosen Wang,Zhijin Ge,Yingzhe He*

Main category: cs.CV

> 本文提出了一种新的对抗样本生成方法SAFER，通过在语义信息和抽象特征（如纹理和边缘）之间取得平衡，提高对抗样本的迁移性。

<details>
  <summary>Details</summary>

**Motivation:** 现有特征级别的对抗攻击主要集中在语义信息上，但CNN更倾向于关注高频组成部分。因此，本文希望通过同时扰乱语义和抽象特征来提升对抗样本的迁移性。

**Method:** Structure

**Result:** {"tldr": "本文提出了一种新的对抗样本生成方法SAFER，通过在语义信息和抽象特征（如纹理和边缘）之间取得平衡，提高对抗样本的迁移性。", "motivation": "现有特征级别的对抗攻击主要集中在语义信息上，但CNN更倾向于关注高频组成部分。因此，本文希望通过同时扰乱语义和抽象特征来提升对抗样本的迁移性。", "method": "SAFER方法采用BLOCKMIX处理输入图像，并利用SELF-MIX在频率谱上执行操作来计算权重矩阵，以突出关键特征。", "result": "在ImageNet数据集上进行了大量实验，证明了SAFER方法能够显著提高对抗样本的迁移性。", "conclusion": "研究表明，同时扰乱图像中的语义和抽象特征可以有效提高对抗样本的迁移性，这对保障深度神经网络的安全性具有重要意义。"}

**Conclusion:** 研究表明，同时扰乱图像中的语义和抽象特征可以有效提高对抗样本的迁移性，这对保障深度神经网络的安全性具有重要意义。

**Abstract:** Adversarial examples pose significant threats to deep neural networks (DNNs),
and their property of transferability in the black-box setting has led to the
emergence of transfer-based attacks, making it feasible to target real-world
applications employing DNNs. Among them, feature-level attacks, where
intermediate features are perturbed based on feature importance weight matrix
computed from transformed images, have gained popularity. In this work, we find
that existing feature-level attacks primarily manipulate the semantic
information to derive the weight matrix. Inspired by several works that find
CNNs tend to focus more on high-frequency components (a.k.a. abstract features,
e.g., texture, edge, etc.), we validate that transforming images in the
high-frequency space also improves transferability. Based on this finding, we
propose a balanced approach called Semantic and Abstract FEatures disRuption
(SAFER). Specifically, SAFER conducts BLOCKMIX on the input image and SELF-MIX
on the frequency spectrum when computing the weight matrix to highlight crucial
features. By using such a weight matrix, we can direct the attacker to disrupt
both semantic and abstract features, leading to improved transferability.
Extensive experiments on the ImageNet dataset also demonstrate the
effectiveness of our method in boosting adversarial transferability.

</details>


### [30] [Improving Personalized Image Generation through Social Context Feedback](https://arxiv.org/abs/2507.16095)
*Parul Gupta,Abhinav Dhall,Thanh-Toan Do*

Main category: cs.CV

> 论文提出了一种基于反馈机制的个性化图像生成方法，改进了现有方法在复杂活动、身份保真度和注视点方面的不足，提高了生成图像的质量。

<details>
  <summary>Details</summary>

**Motivation:** 解决个性化图像生成中复杂活动不准确、参考人物身份不保真及人眼注视模式不合理的问题。

**Method:** 采用基于反馈的微调方法来改进现有个性化图像生成技术，使用姿态检测、人-物交互检测、面部识别和人眼注视点估计等先进检测器来优化扩散模型。此外，根据不同信号层次，在不同的时间步骤引入不同的反馈模块。

**Result:** 实验结果表明，提出的生成方法在人与物交互、面部识别和图像质量上优于基准数据集上的现有方法。

**Conclusion:** 本研究通过反馈机制增强了个性化图像生成的精度和质量，验证了该方法在三种基准数据集上的效能。

**Abstract:** Personalized image generation, where reference images of one or more subjects
are used to generate their image according to a scene description, has gathered
significant interest in the community. However, such generated images suffer
from three major limitations -- complex activities, such as $<$man, pushing,
motorcycle$>$ are not generated properly with incorrect human poses, reference
human identities are not preserved, and generated human gaze patterns are
unnatural/inconsistent with the scene description. In this work, we propose to
overcome these shortcomings through feedback-based fine-tuning of existing
personalized generation methods, wherein, state-of-art detectors of pose,
human-object-interaction, human facial recognition and human gaze-point
estimation are used to refine the diffusion model. We also propose
timestep-based inculcation of different feedback modules, depending upon
whether the signal is low-level (such as human pose), or high-level (such as
gaze point). The images generated in this manner show an improvement in the
generated interactions, facial identities and image quality over three
benchmark datasets.

</details>


### [31] [Stop-band Energy Constraint for Orthogonal Tunable Wavelet Units in Convolutional Neural Networks for Computer Vision problems](https://arxiv.org/abs/2507.16114)
*An D. Le,Hung Nguyen,Sungbal Seo,You-Suk Bae,Truong Q. Nguyen*

Main category: cs.CV

> A novel stop-band energy constraint for filters in CNNs improves image classification and anomaly detection, especially on texture-rich datasets.

<details>
  <summary>Details</summary>

**Motivation:** The aim is to improve image classification and anomaly detection performance, particularly on texture-rich datasets, by modifying the CNNs' convolution, pooling, and downsampling operations.

**Method:** The paper introduces a stop-band energy constraint for filters in orthogonal tunable wavelet units with a lattice structure to enhance image classification and anomaly detection in CNNs.

**Result:** The method improves accuracy by 2.48% on CIFAR-10 and 13.56% on the Describable Textures dataset when integrated into ResNet-18. Similar improvements are seen in ResNet-34. It also shows competitive results in segmentation and detection on the MVTec hazelnut anomaly detection task.

**Conclusion:** The stop-band energy constrained wavelet units effectively enhance performance in tasks such as image classification and anomaly detection, demonstrating significant improvements over existing methods.

**Abstract:** This work introduces a stop-band energy constraint for filters in orthogonal
tunable wavelet units with a lattice structure, aimed at improving image
classification and anomaly detection in CNNs, especially on texture-rich
datasets. Integrated into ResNet-18, the method enhances convolution, pooling,
and downsampling operations, yielding accuracy gains of 2.48% on CIFAR-10 and
13.56% on the Describable Textures dataset. Similar improvements are observed
in ResNet-34. On the MVTec hazelnut anomaly detection task, the proposed method
achieves competitive results in both segmentation and detection, outperforming
existing approaches.

</details>


### [32] [PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation](https://arxiv.org/abs/2507.16116)
*Yaofang Liu,Yumeng Ren,Aitor Artola,Yuxuan Hu,Xiaodong Cun,Xiaotong Zhao,Alan Zhao,Raymond H. Chan,Suiyun Zhang,Rui Liu,Dandan Tu,Jean-Michel Morel*

Main category: cs.CV

> Pusa是通过向量化时间步调整（VTA）实现视频生成的技术，其在SOTA模型上微调后，不仅在其主要任务上表现优越，而且还能解锁更多功能，同时大大减少了训练成本和数据量需求。

<details>
  <summary>Details</summary>

**Motivation:** 传统的标量时间步变量在视频扩散模型中的刚性同步限制了视频扩散模型的快速发展。虽然针对特定任务的调整和自回归模型试图解决这些问题，但仍受计算效率低下、灾难性遗忘以及适用性窄的限制。

**Method:** 该研究提出了一种名为Pusa的新范式，通过向量化时间步调整（VTA）来实现视频扩散模型中的精细时间控制。VTA是一种非破坏性调整，意味着它完全保留了基础模型的能力。通过对SOTA Wan2.1-T2V-14B模型进行VTA微调，实现了前所未有的效率。

**Result:** Pusa不仅为图像到视频（I2V）生成设定了新标准，达到了VBench-I2V的总分87.32%（优于Wan-I2V-14B的86.86%），还解锁了多种零样本多任务能力，如开始-结束帧和视频扩展。此外，Pusa还能进行文本到视频的生成。

**Conclusion:** 本工作建立了一种可扩展、高效、多功能的下一代视频合成范式，使高质量视频生成在研究和行业中都变得更加普及。代码开源于https://github.com/Yaofang-Liu/Pusa-VidGen。

**Abstract:** The rapid advancement of video diffusion models has been hindered by
fundamental limitations in temporal modeling, particularly the rigid
synchronization of frame evolution imposed by conventional scalar timestep
variables. While task-specific adaptations and autoregressive models have
sought to address these challenges, they remain constrained by computational
inefficiency, catastrophic forgetting, or narrow applicability. In this work,
we present Pusa, a groundbreaking paradigm that leverages vectorized timestep
adaptation (VTA) to enable fine-grained temporal control within a unified video
diffusion framework. Besides, VTA is a non-destructive adaptation, which means
it fully preserves the capabilities of the base model. By finetuning the SOTA
Wan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --
surpassing the performance of Wan-I2V-14B with $\leq$ 1/200 of the training
cost (\$500 vs. $\geq$ \$100,000) and $\leq$ 1/2500 of the dataset size (4K vs.
$\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V)
generation, achieving a VBench-I2V total score of 87.32\% (vs. 86.86\% of
Wan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as
start-end frames and video extension -- all without task-specific training.
Meanwhile, Pusa can still perform text-to-video generation. Mechanistic
analyses reveal that our approach preserves the foundation model's generative
priors while surgically injecting temporal dynamics, avoiding the combinatorial
explosion inherent to vectorized timesteps. This work establishes a scalable,
efficient, and versatile paradigm for next-generation video synthesis,
democratizing high-fidelity video generation for research and industry alike.
Code is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen

</details>


### [33] [Universal Wavelet Units in 3D Retinal Layer Segmentation](https://arxiv.org/abs/2507.16119)
*An D. Le,Hung Nguyen,Melanie Tran,Jesse Most,Dirk-Uwe G. Bartsch,William R Freeman,Shyamanga Borooah,Truong Q. Nguyen,Cheolhong An*

Main category: cs.CV

> 本文提出了一种新的方法，使用可调小波单元进行OCT卷的数据中的3D视网膜层分割，利用改进的波型下采样模块增强了图像的细节保留和结构一致性，提高了分割精度。

<details>
  <summary>Details</summary>

**Motivation:** 这是首次将可调小波单元应用于3D视网膜层分割中的研究。目的在于通过改进的波型下采样模块增强细部结构的一致性和精确度，克服传统池化方法的局限。

**Method:** 本研究将可调小波单元（UwUs）应用于3D视网膜层分割的光学相干断层扫描（OCT）数据中。为克服传统最大池化方法的限制，采用了三种基于小波的下采样模块，分别是OrthLattUwU、BiorthLattUwU和LS-BiorthLattUwU，集成了运动校正的MGU-Net架构。这些模块通过可学习的格滤波器组来保持低频和高频特征，增强空间细节和结构一致性。

**Result:** 在Jacobs视网膜中心（JRC）OCT数据集上进行了实验，结果表明，本框架在准确性、Dice分数上有了显著提高，特别是采用LS-BiorthLattUwU的情况下。

**Conclusion:** 研究结果证明了可调小波滤波器在体积医学图像分割中的优势。

**Abstract:** This paper presents the first study to apply tunable wavelet units (UwUs) for
3D retinal layer segmentation from Optical Coherence Tomography (OCT) volumes.
To overcome the limitations of conventional max-pooling, we integrate three
wavelet-based downsampling modules, OrthLattUwU, BiorthLattUwU, and
LS-BiorthLattUwU, into a motion-corrected MGU-Net architecture. These modules
use learnable lattice filter banks to preserve both low- and high-frequency
features, enhancing spatial detail and structural consistency. Evaluated on the
Jacobs Retina Center (JRC) OCT dataset, our framework shows significant
improvement in accuracy and Dice score, particularly with LS-BiorthLattUwU,
highlighting the benefits of tunable wavelet filters in volumetric medical
image segmentation.

</details>
