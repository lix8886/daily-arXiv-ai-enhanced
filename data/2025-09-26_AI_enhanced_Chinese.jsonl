{"id": "2509.20379", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20379", "abs": "https://arxiv.org/abs/2509.20379", "authors": ["Ofir Azachi", "Kfir Eliyahu", "Eyal El Ani", "Rom Himelstein", "Roi Reichart", "Yuval Pinter", "Nitay Calderon"], "title": "Leveraging NTPs for Efficient Hallucination Detection in VLMs", "comment": null, "summary": "Hallucinations of vision-language models (VLMs), which are misalignments\nbetween visual content and generated text, undermine the reliability of VLMs.\nOne common approach for detecting them employs the same VLM, or a different\none, to assess generated outputs. This process is computationally intensive and\nincreases model latency. In this paper, we explore an efficient on-the-fly\nmethod for hallucination detection by training traditional ML models over\nsignals based on the VLM's next-token probabilities (NTPs). NTPs provide a\ndirect quantification of model uncertainty. We hypothesize that high\nuncertainty (i.e., a low NTP value) is strongly associated with hallucinations.\nTo test this, we introduce a dataset of 1,400 human-annotated statements\nderived from VLM-generated content, each labeled as hallucinated or not, and\nuse it to test our NTP-based lightweight method. Our results demonstrate that\nNTP-based features are valuable predictors of hallucinations, enabling fast and\nsimple ML models to achieve performance comparable to that of strong VLMs.\nFurthermore, augmenting these NTPs with linguistic NTPs, computed by feeding\nonly the generated text back into the VLM, enhances hallucination detection\nperformance. Finally, integrating hallucination prediction scores from VLMs\ninto the NTP-based models led to better performance than using either VLMs or\nNTPs alone. We hope this study paves the way for simple, lightweight solutions\nthat enhance the reliability of VLMs.", "AI": {"tldr": "论文探索了一种基于NTP（下一个词概率）的轻量级方法以实现高效的幻觉检测，这种方法利用传统机器学习模型量化模型不确定性，并证明了高不确定性与幻觉有强相关性。实验表明此方法能够在不依赖强大VLM的情况下达到相似的检测效果。", "motivation": "传统的幻觉检测方法使用同一个或不同的VLM来评估生成的输出，这在计算上消耗大并增加模型延迟。本研究旨在探索高效的实时幻觉检测方法，以增强VLM的可靠性。", "method": "通过训练传统的机器学习模型处理视觉语言模型（VLM）的下一个词概率（NTPs）信号来实现实时幻觉检测。我们假设高不确定性（即低NTP值）与幻觉有很强的关系。", "result": "研究结果显示，基于NTP的特征是幻觉预测的有力指标，简单的机器学习模型能够达到与强大的VLM相媲美的性能。通过结合语言NTPs，能提高幻觉检测性能，并且将VLM的幻觉预测分数整合到NTP模型中，比单独使用VLM或NTP有更好效果。", "conclusion": "本研究为提高VLM的可靠性铺平了道路，提供了简单且轻量级的解决方案。通过利用NTPs和语言NTPs，我们可以创建高效的幻觉检测机制以改进VLM的准确性和可靠性。"}}
{"id": "2509.20420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20420", "abs": "https://arxiv.org/abs/2509.20420", "authors": ["Elias N. Zois", "Moises Diaz", "Salem Said", "Miguel A. Ferrer"], "title": "Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification", "comment": "9 pages, 3 figures", "summary": "Offline handwritten signature verification remains a challenging task,\nparticularly in writer-independent settings where models must generalize across\nunseen individuals. Recent developments have highlighted the advantage of\ngeometrically inspired representations, such as covariance descriptors on\nRiemannian manifolds. However, past or present, handcrafted or data-driven\nmethods usually depend on real-world signature datasets for classifier\ntraining. We introduce a quasi-synthetic data generation framework leveraging\nthe Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small\nset of genuine samples in the SPD space is the seed to a Riemannian Gaussian\nMixture which identifies Riemannian centers as synthetic writers and variances\nas their properties. Riemannian Gaussian sampling on each center generates\npositive as well as negative synthetic SPD populations. A metric learning\nframework utilizes pairs of similar and dissimilar SPD points, subsequently\ntesting it over on real-world datasets. Experiments conducted on two popular\nsignature datasets, encompassing Western and Asian writing styles, demonstrate\nthe efficacy of the proposed approach under both intra- and cross- dataset\nevaluation protocols. The results indicate that our quasi-synthetic approach\nachieves low error rates, highlighting the potential of generating synthetic\ndata in Riemannian spaces for writer-independent signature verification\nsystems.", "AI": {"tldr": "A method using Riemannian Gaussian mixtures to generate synthetic SPD data for training metric learning classifiers achieves low error rates in writer-independent offline signature verification.", "motivation": "The motivation is to address the challenge of writer-independent offline handwritten signature verification, where the model needs to generalize across unseen individuals. The method aims to leverage synthetic data generation in Riemannian space to improve the classifier's performance without relying heavily on real-world datasets for training.", "method": "The paper introduces a quasi-synthetic data generation framework based on Riemannian geometry of SPD matrices to generate synthetic positive and negative SPD populations. These are used in a metric learning framework to train a classifier for offline handwritten signature verification.", "result": "Experiments on two datasets with different writing styles show that the proposed method yields low error rates, indicating the effectiveness of using synthetic data for training signature verification systems.", "conclusion": "The conclusion is that using quasi-synthetic data generated in Riemannian space shows potential for improving the accuracy of writer-independent offline handwritten signature verification, with reduced dependence on real-world datasets for training."}}
{"id": "2509.20427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20427", "abs": "https://arxiv.org/abs/2509.20427", "authors": ["Team Seedream", "Yunpeng Chen", "Yu Gao", "Lixue Gong", "Meng Guo", "Qiushan Guo", "Zhiyao Guo", "Xiaoxia Hou", "Weilin Huang", "Yixuan Huang", "Xiaowen Jian", "Huafeng Kuang", "Zhichao Lai", "Fanshi Li", "Liang Li", "Xiaochen Lian", "Chao Liao", "Liyang Liu", "Wei Liu", "Yanzuo Lu", "Zhengxiong Luo", "Tongtong Ou", "Guang Shi", "Yichun Shi", "Shiqi Sun", "Yu Tian", "Zhi Tian", "Peng Wang", "Rui Wang", "Xun Wang", "Ye Wang", "Guofeng Wu", "Jie Wu", "Wenxu Wu", "Yonghui Wu", "Xin Xia", "Xuefeng Xiao", "Shuang Xu", "Xin Yan", "Ceyuan Yang", "Jianchao Yang", "Zhonghua Zhai", "Chenlin Zhang", "Heng Zhang", "Qi Zhang", "Xinyu Zhang", "Yuwei Zhang", "Shijia Zhao", "Wenliang Zhao", "Wenjia Zhu"], "title": "Seedream 4.0: Toward Next-generation Multimodal Image Generation", "comment": "Seedream 4.0 Technical Report", "summary": "We introduce Seedream 4.0, an efficient and high-performance multimodal image\ngeneration system that unifies text-to-image (T2I) synthesis, image editing,\nand multi-image composition within a single framework. We develop a highly\nefficient diffusion transformer with a powerful VAE which also can reduce the\nnumber of image tokens considerably. This allows for efficient training of our\nmodel, and enables it to fast generate native high-resolution images (e.g.,\n1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning\ndiverse taxonomies and knowledge-centric concepts. Comprehensive data\ncollection across hundreds of vertical scenarios, coupled with optimized\nstrategies, ensures stable and large-scale training, with strong\ngeneralization. By incorporating a carefully fine-tuned VLM model, we perform\nmulti-modal post-training for training both T2I and image editing tasks\njointly. For inference acceleration, we integrate adversarial distillation,\ndistribution matching, and quantization, as well as speculative decoding. It\nachieves an inference time of up to 1.8 seconds for generating a 2K image\n(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream\n4.0 can achieve state-of-the-art results on both T2I and multimodal image\nediting. In particular, it demonstrates exceptional multimodal capabilities in\ncomplex tasks, including precise image editing and in-context reasoning, and\nalso allows for multi-image reference, and can generate multiple output images.\nThis extends traditional T2I systems into an more interactive and\nmultidimensional creative tool, pushing the boundary of generative AI for both\ncreativity and professional applications. Seedream 4.0 is now accessible on\nhttps://www.volcengine.com/experience/ark?launch=seedream.", "AI": {"tldr": "本文介绍了Seedream 4.0，一种高效、高性能的多模态图像生成系统，该系统可以统一处理文本到图像的合成、图像编辑和多图像合成任务。Seedream 4.0 在大型数据集上进行了预训练，并通过各种优化技术实现了快速生成高分辨率图像的能力。", "motivation": "本研究旨在开发一个高效且高性能的多模态图像生成系统，它可以统一文本到图像合成、图像编辑以及多图像合成。", "method": "我们开发了一种高效的扩散变换器，并结合了强大的变分自编码器（VAE），以显著减少图像标记的数量。这使得我们的模型能够高效训练并快速生成原生高分辨率图像（例如，1K-4K）。我们还在数十亿个文本-图像对上对模型进行了预训练，并通过优化策略确保了大规模训练的稳定性和泛化能力。此外，通过引入仔细微调过的视觉语言模型（VLM），我们可以同时进行文本到图像合成和图像编辑任务的多模态后训练。", "result": "Seedream 4.0 在文本到图像合成和多模态图像编辑任务上实现了最新的技术水平，尤其是在复杂的图像编辑和上下文感知推理任务中表现卓越。", "conclusion": "Seedream 4.0 延伸了传统的文本到图像系统，使之成为一个更加交互和多维的创意工具，为创造力和专业应用领域的发展设定了新的标准。"}}
{"id": "2509.20474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20474", "abs": "https://arxiv.org/abs/2509.20474", "authors": ["Samia Saeed", "Khuram Naveed"], "title": "A Contrastive Learning Framework for Breast Cancer Detection", "comment": null, "summary": "Breast cancer, the second leading cause of cancer-related deaths globally,\naccounts for a quarter of all cancer cases [1]. To lower this death rate, it is\ncrucial to detect tumors early, as early-stage detection significantly improves\ntreatment outcomes. Advances in non-invasive imaging techniques have made early\ndetection possible through computer-aided detection (CAD) systems which rely on\ntraditional image analysis to identify malignancies. However, there is a\ngrowing shift towards deep learning methods due to their superior\neffectiveness. Despite their potential, deep learning methods often struggle\nwith accuracy due to the limited availability of large-labeled datasets for\ntraining. To address this issue, our study introduces a Contrastive Learning\n(CL) framework, which excels with smaller labeled datasets. In this regard, we\ntrain Resnet-50 in semi supervised CL approach using similarity index on a\nlarge amount of unlabeled mammogram data. In this regard, we use various\naugmentation and transformations which help improve the performance of our\napproach. Finally, we tune our model on a small set of labelled data that\noutperforms the existing state of the art. Specifically, we observed a 96.7%\naccuracy in detecting breast cancer on benchmark datasets INbreast and MIAS.", "AI": {"tldr": "研究提出了一种新的半监督对比学习方法，显著提高了基于ResNet-50的乳腺癌检测模型的性能，实现了96.7%的检测准确率。", "motivation": "尽管深度学习方法在乳腺癌检测方面具有巨大潜力，但由于大型标记数据集的缺乏，它们的准确性经常受到影响。我们的研究旨在解决这个问题。", "method": "我们的研究引入了一种对比学习（CL）框架，该框架在小规模标记数据集上表现出色。我们使用未标记的乳腺钼靶图像数据，通过半监督的CL方法训练Resnet-50。我们采用了各种数据增强和变换手段来提高模型性能，并最终在一小部分标记数据上对模型进行调优。", "result": "我们的方法在基准数据集INbreast和MIAS上达到了96.7%的乳腺癌检测准确率，超越了现有的最好结果。", "conclusion": "研究表明，通过使用对比学习框架和数据增强技术，可以显著提高乳腺癌检测的准确率，特别是在数据有限的情况下。"}}
{"id": "2509.20367", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.20367", "abs": "https://arxiv.org/abs/2509.20367", "authors": ["Leyi Ouyang"], "title": "Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models", "comment": "2 Figures, 7 Tables, 1 Algorithm", "summary": "Diplomatic events consistently prompt widespread public discussion and\ndebate. Public sentiment plays a critical role in diplomacy, as a good\nsentiment provides vital support for policy implementation, helps resolve\ninternational issues, and shapes a nation's international image. Traditional\nmethods for gauging public sentiment, such as large-scale surveys or manual\ncontent analysis of media, are typically time-consuming, labor-intensive, and\nlack the capacity for forward-looking analysis. We propose a novel framework\nthat identifies specific modifications for diplomatic event narratives to shift\npublic sentiment from negative to neutral or positive. First, we train a\nlanguage model to predict public reaction towards diplomatic events. To this\nend, we construct a dataset comprising descriptions of diplomatic events and\ntheir associated public discussions. Second, guided by communication theories\nand in collaboration with domain experts, we predetermined several textual\nfeatures for modification, ensuring that any alterations changed the event's\nnarrative framing while preserving its core facts.We develop a counterfactual\ngeneration algorithm that employs a large language model to systematically\nproduce modified versions of an original text. The results show that this\nframework successfully shifted public sentiment to a more favorable state with\na 70\\% success rate. This framework can therefore serve as a practical tool for\ndiplomats, policymakers, and communication specialists, offering data-driven\ninsights on how to frame diplomatic initiatives or report on events to foster a\nmore desirable public sentiment.", "AI": {"tldr": "本文提出了一种新的框架，用于通过修改外交事件文本描述来改善公众情绪。通过训练语言模型预测公众反应，并采用反事实生成算法来修改文本，该框架能将公众情绪正面转换的成功率达到了70%。", "motivation": "传统的方法，例如大规模调查或手动分析媒体内容，用于衡量公众情绪通常是费时、劳动密集型的，并且缺乏前瞻性分析的能力。因此，需要一种新的方法来快速有效地进行公众情绪分析和预测。", "method": "我们提出了一种新的框架，能够识别特定修改以改变外交事件叙事，从而将公众情绪从消极转变为中性或积极。该框架首先训练了一个语言模型来预测公众对外交事件的反应，为此构建了一个包含外交事件描述及其相关公众讨论的数据集。其次，依据交流理论和领域专家的指导，预先确定了几种文本特征进行修改，确保任何更改改变了事件的叙事框架，同时保留其核心事实。此外，我们开发了一种反事实生成算法，它使用大型语言模型系统地生成原始文本的修改版本。", "result": "实验结果表明，该框架能将公众情绪转变为更加有利的状态的成功率为70%。", "conclusion": "这一框架可以作为外交官、政策制定者和传播专家的实用工具，提供数据驱动的见解，以优化外交倡议或事件报道，促进更理想化的公众情绪。"}}
{"id": "2509.20479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20479", "abs": "https://arxiv.org/abs/2509.20479", "authors": ["Simon Baeuerle", "Pratik Khanna", "Nils Friederich", "Angelo Jovin Yamachui Sitcheu", "Damir Shakirov", "Andreas Steimer", "Ralf Mikut"], "title": "Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data", "comment": null, "summary": "Foundation Models (FMs) have shown impressive performance on various text and\nimage processing tasks. They can generalize across domains and datasets in a\nzero-shot setting. This could make them suitable for automated quality\ninspection during series manufacturing, where various types of images are being\nevaluated for many different products. Replacing tedious labeling tasks with a\nsimple text prompt to describe anomalies and utilizing the same models across\nmany products would save significant efforts during model setup and\nimplementation. This is a strong advantage over supervised Artificial\nIntelligence (AI) models, which are trained for individual applications and\nrequire labeled training data. We test multiple recent FMs on both custom\nreal-world industrial image data and public image data. We show that all of\nthose models fail on our real-world data, while the very same models perform\nwell on public benchmark datasets.", "AI": {"tldr": "研究了基础模型在工业自动质量检查中的应用潜力，发现它们在公共数据集上表现良好，但在真实工业数据上表现不佳。", "motivation": "研究基础模型（FMs）在系列制造过程中自动质量检查的潜力，以及它们在不同产品上使用时的跨域和跨数据集泛化能力，旨在减少繁琐的标注任务。", "method": "分析文本内容，提取关键信息。", "result": "尽管基础模型在公共基准数据集上表现出色，但它们在实际的工业图像数据上表现不佳。", "conclusion": "基础模型在没有标注数据的情况下实现了跨域和跨数据集的泛化，但在实际的工业应用中表现不如同类型模型在标准数据集上的表现。这表明基础模型在实际工业数据中的适应性和可靠性需要进一步提高。"}}
{"id": "2509.20373", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20373", "abs": "https://arxiv.org/abs/2509.20373", "authors": ["Shreya G. Upadhyay", "Carlos Busso", "Chi-Chun Lee"], "title": "Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition", "comment": null, "summary": "Cross-lingual speech emotion recognition (SER) remains a challenging task due\nto differences in phonetic variability and speaker-specific expressive styles\nacross languages. Effectively capturing emotion under such diverse conditions\nrequires a framework that can align the externalization of emotions across\ndifferent speakers and languages. To address this problem, we propose a\nspeaker-style aware phoneme anchoring framework that aligns emotional\nexpression at the phonetic and speaker levels. Our method builds\nemotion-specific speaker communities via graph-based clustering to capture\nshared speaker traits. Using these groups, we apply dual-space anchoring in\nspeaker and phonetic spaces to enable better emotion transfer across languages.\nEvaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)\ncorpora demonstrate improved generalization over competitive baselines and\nprovide valuable insights into the commonalities in cross-lingual emotion\nrepresentation.", "AI": {"tldr": "本文提出了一种通过构建基于图的说话人社区并利用双空间锚定技术来提升跨语言情感识别性能的方法，实验结果表明该方法有效提高了情感识别的跨语言泛化能力。", "motivation": "当前跨语言语音情感识别存在由于语言间发音变异性及说话人表达风格差异导致的挑战。", "method": "方法是基于图的聚类构建共享说话人特征的社区，并在说话人和音素空间应用双空间锚定技术。", "result": "在MSP-Podcast与BIIC-Podcast语料库上的评估结果显示，提出的方法比竞争基准方法具有更好的跨语言泛化性能。", "conclusion": "研究表明，该方法能够有效地提高跨语言情感表达的一致性和情感转移性能，为解决跨语言情感识别问题提供了一种有效方案。"}}
{"id": "2509.20481", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20481", "abs": "https://arxiv.org/abs/2509.20481", "authors": ["Jing Li", "Oskar Bartosz", "Chengyu Wang", "Michal Wnuczynski", "Dilshan Godaliyadda", "Michael Polley"], "title": "Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision", "comment": null, "summary": "The majority of AI models in imaging and vision are customized to perform on\nspecific high-precision task. However, this strategy is inefficient for\napplications with a series of modular tasks, since each requires a mapping into\na disparate latent domain. To address this inefficiency, we proposed a\nuniversal Neural Space (NS), where an encoder-decoder framework pre-computes\nfeatures across vision and imaging tasks. Our encoder learns transformation\naware, generalizable representations, which enable multiple downstream AI\nmodules to share the same feature space. This architecture reduces redundancy,\nimproves generalization across domain shift, and establishes a foundation for\neffecient multi-task vision pipelines. Furthermore, as opposed to larger\ntransformer backbones, our backbone is lightweight and CNN-based, allowing for\nwider across hardware. We furthur demonstrate that imaging and vision modules,\nsuch as demosaicing, denoising, depth estimation and semantic segmentation can\nbe performed efficiently in the NS.", "AI": {"tldr": "Introduces a universal Neural Space for pre-computing generalizable visual features across multiple tasks, leading to improved efficiency and hardware compatibility in AI vision pipelines.", "motivation": "To improve efficiency in multi-task applications by reducing the need for separate latent domains for each task and to support a wider range of hardware through a lightweight CNN-based backbone.", "method": "Proposes a universal Neural Space (NS) using an encoder-decoder framework for pre-computing features across various vision and imaging tasks, focusing on transformation-aware, generalizable representations to enable efficient multi-task pipelines with shared feature spaces.", "result": "The method demonstrates efficient performance across imaging and vision tasks, such as demosaicing, denoising, depth estimation, and semantic segmentation, while reducing redundancy and improving generalization across domain shifts.", "conclusion": "The universal Neural Space with a shared feature space improves efficiency and widens hardware compatibility in multi-task vision pipelines."}}
{"id": "2509.20374", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20374", "abs": "https://arxiv.org/abs/2509.20374", "authors": ["Nithin Somasekharan", "Ling Yue", "Yadi Cao", "Weichao Li", "Patrick Emami", "Pochinapeddi Sai Bhargav", "Anurag Acharya", "Xingyu Xie", "Shaowu Pan"], "title": "CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong performance across\ngeneral NLP tasks, but their utility in automating numerical experiments of\ncomplex physical system -- a critical and labor-intensive component -- remains\nunderexplored. As the major workhorse of computational science over the past\ndecades, Computational Fluid Dynamics (CFD) offers a uniquely challenging\ntestbed for evaluating the scientific capabilities of LLMs. We introduce\nCFDLLMBench, a benchmark suite comprising three complementary components --\nCFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM\nperformance across three key competencies: graduate-level CFD knowledge,\nnumerical and physical reasoning of CFD, and context-dependent implementation\nof CFD workflows. Grounded in real-world CFD practices, our benchmark combines\na detailed task taxonomy with a rigorous evaluation framework to deliver\nreproducible results and quantify LLM performance across code executability,\nsolution accuracy, and numerical convergence behavior. CFDLLMBench establishes\na solid foundation for the development and evaluation of LLM-driven automation\nof numerical experiments for complex physical systems. Code and data are\navailable at https://github.com/NREL-Theseus/cfdllmbench/.", "AI": {"tldr": "研究提出了CFDLLMBench，一个旨在全面评估大型语言模型在计算流体动力学（CFD）领域性能的基准测试套件。", "motivation": "尽管大型语言模型在自然语言处理任务中表现出色，但对于复杂物理系统自动化数值实验的能力却鲜有探索。鉴于CFD作为过去几十年计算科学的主要工作，它提供了一个独特的测试平台来评估大型语言模型的科学能力。", "method": "本研究引入了CFDLLMBench基准测试套件，该套件包含三个互补组件——CFDQuery、CFDCodeBench和FoamBench，旨在全面评估模型在CFD领域的三个关键能力：研究生层次的CFD知识、CFD的数值和物理推理，以及上下文相关的工作流实现能力。", "result": "基准测测试结合了详细的任务分类与严谨的评估框架，旨在提供可重复的结果，并量化大型语言模型在代码可执行性、解精度和数值收敛行为上的表现。", "conclusion": "CFDLLMBench为开发和评估全自动化复杂物理系统数值实验的大型语言模型奠定了坚实的基础。"}}
{"id": "2509.20484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20484", "abs": "https://arxiv.org/abs/2509.20484", "authors": ["Dani Manjah", "Tim Bary", "Benoît Gérin", "Benoît Macq", "Christophe de Vleeschouwer"], "title": "Data-Efficient Stream-Based Active Distillation for Scalable Edge Model Deployment", "comment": "6 pages, 3 figures, 2 algorithms, presented at SEEDS Workshop (ICIP\n  2025)", "summary": "Edge camera-based systems are continuously expanding, facing ever-evolving\nenvironments that require regular model updates. In practice, complex teacher\nmodels are run on a central server to annotate data, which is then used to\ntrain smaller models tailored to the edge devices with limited computational\npower. This work explores how to select the most useful images for training to\nmaximize model quality while keeping transmission costs low. Our work shows\nthat, for a similar training load (i.e., iterations), a high-confidence\nstream-based strategy coupled with a diversity-based approach produces a\nhigh-quality model with minimal dataset queries.", "AI": {"tldr": "研究探讨了如何在保持传输成本较低的情况下，通过选择最具代表性的图像进行训练，以提高边缘设备模型的质量。", "motivation": "随着环境不断变化，边缘摄像机系统需要频繁更新模型。处理此需求的一种方法是使用中央服务器上的复杂教师模型来标记数据，为计算能力有限的边缘设备训练更小的模型。", "method": "采用高置信度流策略结合多样性选择方法，从数据集中选取训练所需的图像。", "result": "实验表明，对于类似的训练负载（即迭代次数），上述策略可以以最小的数据集查询数量生成高质量的模型。", "conclusion": "高置信度流策略结合多样性选择方法，能够在保持传输成本低的同时提高模型质量。"}}
{"id": "2509.20375", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20375", "abs": "https://arxiv.org/abs/2509.20375", "authors": ["Sharanya Parimanoharan", "Ruwan D. Nawarathna"], "title": "Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text", "comment": null, "summary": "The rapid adoption of large language models (LLMs) such as ChatGPT has\nblurred the line between human and AI-generated texts, raising urgent questions\nabout academic integrity, intellectual property, and the spread of\nmisinformation. Thus, reliable AI-text detection is needed for fair assessment\nto safeguard human authenticity and cultivate trust in digital communication.\nIn this study, we investigate how well current machine learning (ML) approaches\ncan distinguish ChatGPT-3.5-generated texts from human-written texts employing\na labeled data set of 250 pairs of abstracts from a wide range of research\ntopics. We test and compare both classical (Logistic Regression armed with\nclassical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT\naugmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,\nand LSTM-based N-gram models) ML detection techniques. As we aim to assess each\nmodel's performance in detecting AI-generated research texts, we also aim to\ntest whether an ensemble of these models can outperform any single detector.\nResults show DistilBERT achieves the overall best performance, while Logistic\nRegression and BERT-Custom offer solid, balanced alternatives; LSTM- and\nBERT-N-gram approaches lag. The max voting ensemble of the three best models\nfails to surpass DistilBERT itself, highlighting the primacy of a single\ntransformer-based representation over mere model diversity. By comprehensively\nassessing the strengths and weaknesses of these AI-text detection approaches,\nthis work lays a foundation for more robust transformer frameworks with larger,\nricher datasets to keep pace with ever-improving generative AI models.", "AI": {"tldr": "研究比较了多种机器学习方法在识别AI生成文本上的效果，发现DistilBERT最优，集成方法未超越个体模型。", "motivation": "鉴于大型语言模型（如ChatGPT）带来的学术诚信、知识产权和传播错误信息等紧迫问题，此研究致力于可靠地检测AI生成的文本，以保障人类创作的真实性，并在数字通信中建立信任。", "method": "研究采用了经典机器学习方法（如套用了传统词袋模型、词性标注、TF-IDF特征的逻辑回归）和基于变压器的方法（如套用了N-grams的BERT、DistilBERT、带轻量级自定义分类器的BERT、基于LSTM的N-gram模型）来区分ChatGPT-3.5生成的文本与人类撰写的文本。", "result": "结果显示，DistilBERT整体表现最佳，逻辑回归和自定义BERT模型也有稳健均衡的表现；相比之下，基于BERT和LSTM的N-gram方法表现落后。最佳三模型的最大投票集成未能超越DistilBERT，这强调了单个基于变压器的方法的重要性。", "conclusion": "此研究全面评估了几种AI文本检测方法的优势和不足，为构建更强大的基于变压器框架奠定了基础，此类框架需要结合更大、更丰富的数据集以跟上持续改进的AI生成模型的步伐。"}}
{"id": "2509.20524", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20524", "abs": "https://arxiv.org/abs/2509.20524", "authors": ["Julien Han", "Shuwen Qiu", "Qi Li", "Xingzi Xu", "Mehmet Saygin Seyfioglu", "Kavosh Asadi", "Karim Bouyarmane"], "title": "InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On", "comment": "Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content\n  Creation workshop", "summary": "We present InstructVTON, an instruction-following interactive virtual try-on\nsystem that allows fine-grained and complex styling control of the resulting\ngeneration, guided by natural language, on single or multiple garments. A\ncomputationally efficient and scalable formulation of virtual try-on formulates\nthe problem as an image-guided or image-conditioned inpainting task. These\ninpainting-based virtual try-on models commonly use a binary mask to control\nthe generation layout. Producing a mask that yields desirable result is\ndifficult, requires background knowledge, might be model dependent, and in some\ncases impossible with the masking-based approach (e.g. trying on a long-sleeve\nshirt with \"sleeves rolled up\" styling on a person wearing long-sleeve shirt\nwith sleeves down, where the mask will necessarily cover the entire sleeve).\nInstructVTON leverages Vision Language Models (VLMs) and image segmentation\nmodels for automated binary mask generation. These masks are generated based on\nuser-provided images and free-text style instructions. InstructVTON simplifies\nthe end-user experience by removing the necessity of a precisely drawn mask,\nand by automating execution of multiple rounds of image generation for try-on\nscenarios that cannot be achieved with masking-based virtual try-on models\nalone. We show that InstructVTON is interoperable with existing virtual try-on\nmodels to achieve state-of-the-art results with styling control.", "AI": {"tldr": "InstructVTON通过结合视觉语言模型和图像分割模型实现自动化的二值掩码生成，进而控制虚拟试衣过程中的衣服样式，简化用户操作并提升试衣效果。", "motivation": "传统的基于二值掩码的虚拟试衣模型存在生成理想的试衣效果困难的问题，而且掩码的生成需要一定的背景知识，并且可能是特定模型依赖的，在某些情况下甚至是不可能的任务。", "method": "提出InstructVTON，一种基于指令随从的交互式虚拟试衣系统，该系统允许用户通过自然语言控制生成结果的精细和复杂的风格调整，适用于单个或多个衣物。这种方法将虚拟试衣问题定义为基于图像指导或约束的图像修复任务。", "result": "实验表明，InstructVTON能够实现与现有虚拟试衣模型的互操作，以达到具有样式控制能力的最先进的结果。", "conclusion": "InstructVTON简化了最终用户的体验，不仅去除了精确绘制掩码的必要性，还通过自动化执行多轮图像生成简化了试服装景，这些场景仅靠基于掩码的虚拟试衣模型是无法达到的。"}}
{"id": "2509.20376", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20376", "abs": "https://arxiv.org/abs/2509.20376", "authors": ["Haoxuan Li", "Zhen Wen", "Qiqi Jiang", "Chenxiao Li", "Yuwei Wu", "Yuchen Yang", "Yiyao Wang", "Xiuqi Huang", "Minfeng Zhu", "Wei Chen"], "title": "ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable performance across a\nwide range of natural language tasks. Understanding how LLMs internally\nrepresent knowledge remains a significant challenge. Despite Sparse\nAutoencoders (SAEs) have emerged as a promising technique for extracting\ninterpretable features from LLMs, SAE features do not inherently align with\nhuman-understandable concepts, making their interpretation cumbersome and\nlabor-intensive. To bridge the gap between SAE features and human concepts, we\npresent ConceptViz, a visual analytics system designed for exploring concepts\nin LLMs. ConceptViz implements a novel dentification => Interpretation =>\nValidation pipeline, enabling users to query SAEs using concepts of interest,\ninteractively explore concept-to-feature alignments, and validate the\ncorrespondences through model behavior verification. We demonstrate the\neffectiveness of ConceptViz through two usage scenarios and a user study. Our\nresults show that ConceptViz enhances interpretability research by streamlining\nthe discovery and validation of meaningful concept representations in LLMs,\nultimately aiding researchers in building more accurate mental models of LLM\nfeatures. Our code and user guide are publicly available at\nhttps://github.com/Happy-Hippo209/ConceptViz.", "AI": {"tldr": "ConceptViz是一种可视分析系统，通过探索LLMs中的概念来解决LLM特性和人类可理解概念之间的对齐问题。", "motivation": "虽然稀疏自动编码器已经作为提取LLMs中可解释特性的有前途的技术出现，但是这些特性并不与人类可理解的概念对齐，这使得它们的解释困难且劳动密集。为了弥合SAE特征与人类概念之间的差距。", "method": "ConceptViz是一种可视分析系统，旨在探索大语言模型（LLMs）中的概念。它实现了一个新颖的识别、解释和验证管道，允许用户用感兴趣的条款查询稀疏自动编码器（SAEs），互动探索概念与特性之间的对应，验证通过模型行为确认这些对应。", "result": "通过两个使用场景和一个用户研究，证明了ConceptViz的有效性，它可以通过流线型发现和验证在LLMs有意义的概念表示来提高解释性研究。", "conclusion": "ConceptViz增强了解释性研究，使研究人员可以建立更准确的关于LLM特性的心理模型，促进了LLM特性和人类概念之间对齐的理解。代码和用户指南公开可用。"}}
{"id": "2509.20537", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20537", "abs": "https://arxiv.org/abs/2509.20537", "authors": ["Dana A Abdullah", "Dana Rasul Hamad", "Bishar Rasheed Ibrahim", "Sirwan Abdulwahid Aula", "Aso Khaleel Ameen", "Sabat Salih Hamadamin"], "title": "Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition", "comment": null, "summary": "Altered fingerprint recognition (AFR) is challenging for biometric\nverification in applications such as border control, forensics, and fiscal\nadmission. Adversaries can deliberately modify ridge patterns to evade\ndetection, so robust recognition of altered prints is essential. We present\nDeepAFRNet, a deep learning recognition model that matches and recognizes\ndistorted fingerprint samples. The approach uses a VGG16 backbone to extract\nhigh-dimensional features and cosine similarity to compare embeddings. We\nevaluate on the SOCOFing Real-Altered subset with three difficulty levels\n(Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of\n96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A\nthreshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72\nsharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent,\nunderscoring the importance of threshold selection in biometric systems. By\nusing real altered samples and reporting per-level metrics, DeepAFRNet\naddresses limitations of prior work based on synthetic alterations or limited\nverification protocols, and indicates readiness for real-world deployments\nwhere both security and recognition resilience are critical.", "AI": {"tldr": "DeepAFRNet 提出了一种新的深度学习模型来识别修改过的指纹，显著提高了此类样本的识别准确率，并在一个包含三难度级别的数据集上进行了测试。", "motivation": "鉴于对手可以修改指纹以逃避检测，这项研究旨在开发一种能够有效识别修改后指纹的系统，以提高实际应用中的安全性。", "method": "DeepAFRNet 是一种深度学习识别模型，使用 VGG16 的骨干网络来提取高维特征，并利用余弦相似度来比较特征嵌入。", "result": "对于三个难度级别（简单，中等，困难），在严格的阈值下，DeepAFRNet 分别达到了 96.7%，98.76% 和 99.54% 的准确率。然而，阈值的放松会导致识别率显著下降。", "conclusion": "DeepAFRNet 通过对真实修改数据样本进行测试并报告分层指标的方法，解决了先前基于合成篡改或有限验证协议的研究的局限性，并表明了其在实际部署中的潜在应用价值。"}}
{"id": "2509.20377", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20377", "abs": "https://arxiv.org/abs/2509.20377", "authors": ["Tomoaki Isoda"], "title": "SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive tasks in\nrecent years. However, since retrieval systems may return irrelevant content,\nincorporating such information into the model often leads to hallucinations.\nThus, identifying and filtering out unhelpful retrieved content is a key\nchallenge for improving RAG performance.To better integrate the internal\nknowledge of the model with external knowledge from retrieval, it is essential\nto understand what the model \"knows\" and \"does not know\" (which is also called\n\"self-knowledge\"). Based on this insight, we propose SKILL-RAG (Self-Knowledge\nInduced Learning and Filtering for RAG), a novel method that leverages the\nmodel's self-knowledge to determine which retrieved documents are beneficial\nfor answering a given query. We design a reinforcement learning-based training\nframework to explicitly elicit self-knowledge from the model and employs\nsentence-level granularity to filter out irrelevant content while preserving\nuseful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several\nquestion answering benchmarks. Experimental results demonstrate that SKILL-RAG\nnot only improves generation quality but also significantly reduces the number\nof input documents, validating the importance of self-knowledge in guiding the\nselection of high-quality retrievals.", "AI": {"tldr": "研究提出了一种使用模型自知识来改进RAG方法的新技术SKILL-RAG，提高了生成质量并减少了输入文档数量。", "motivation": "由于检索系统可能会返回不相关的内容，将其纳入模型中可能导致幻觉。因此，识别和过滤出无用的检索内容是提高RAG性能的关键挑战。", "method": "提出了一种名为SKILL-RAG的新方法，该方法利用模型的自知识来确定哪些检索到的文档对回答给定的查询是有效的。设计了一个基于强化学习的训练框架，以明确地从模型中提取自知识，并使用句子级别的细粒度来过滤掉无关内容，同时保留有用的知识。", "result": "实验结果表明，SKILL-RAG不仅提高了生成质量，还显著减少了输入文档的数量，这验证了自知识在指导高质量检索选择方面的重要性。", "conclusion": "基于强化学习的SKILL-RAG方法通过利用模型的自知识来提高检索增强生成任务的性能，展示了在不需要大量输入文档的情况下提升生成质量的能力。"}}
{"id": "2509.20579", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.20579", "abs": "https://arxiv.org/abs/2509.20579", "authors": ["Hanna Yurchyk", "Wei-Di Chang", "Gregory Dudek", "David Meger"], "title": "Large Pre-Trained Models for Bimanual Manipulation in 3D", "comment": "Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid\n  Robots", "summary": "We investigate the integration of attention maps from a pre-trained Vision\nTransformer into voxel representations to enhance bimanual robotic\nmanipulation. Specifically, we extract attention maps from DINOv2, a\nself-supervised ViT model, and interpret them as pixel-level saliency scores\nover RGB images. These maps are lifted into a 3D voxel grid, resulting in\nvoxel-level semantic cues that are incorporated into a behavior cloning policy.\nWhen integrated into a state-of-the-art voxel-based policy, our\nattention-guided featurization yields an average absolute improvement of 8.2%\nand a relative gain of 21.9% across all tasks in the RLBench bimanual\nbenchmark.", "AI": {"tldr": "The research integrates attention maps from a pre-trained Vision Transformer (DINOv2) into voxel representations to improve the performance of a bimanual robotic manipulation policy, achieving significant performance gains on the RLBench benchmark.", "motivation": "The motivation is to enhance the capabilities of bimanual robotic manipulation through advanced visual attention mechanisms, leading to better semantic understanding in the policy’s decision-making process.", "method": "The method involves extracting attention maps from DINOv2 as pixel-level saliency scores over RGB images, then transferring these maps into a 3D voxel grid to be used as semantic cues in a behavior cloning policy.", "result": "The result is a notable improvement: the attention-guided approach improved the policy's performance by 8.2% on average and 21.9% relatively across all tasks in the RLBench bimanual benchmark.", "conclusion": "The conclusion is that integrating visual attention from a pre-trained Vision Transformer effectively enhances the performance of bimanual robotic policies by providing useful semantic information in voxel-based representations."}}
{"id": "2509.20378", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20378", "abs": "https://arxiv.org/abs/2509.20378", "authors": ["Sirui Wang", "Andong Chen", "Tiejun Zhao"], "title": "Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation", "comment": null, "summary": "Emotional text-to-speech (E-TTS) is central to creating natural and\ntrustworthy human-computer interaction. Existing systems typically rely on\nsentence-level control through predefined labels, reference audio, or natural\nlanguage prompts. While effective for global emotion expression, these\napproaches fail to capture dynamic shifts within a sentence. To address this\nlimitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework\nfor LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to\nwords to obtain word-level emotion annotations, and maps them through a\nFeature-wise Linear Modulation (FiLM) layer, enabling word-level emotion\ncontrol by directly modulating text embeddings. To support evaluation, we\nconstruct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed\nannotations of emotional transitions. Experiments show that Emo-FiLM\noutperforms existing approaches on both global and fine-grained tasks,\ndemonstrating its effectiveness and generality for expressive speech synthesis.", "AI": {"tldr": "本文介绍了Emo-FiLM，一种能够处理细粒度情感动态变化的文本转语音系统，实验结果显示其在全局和局部情感控制方面均优于现有技术。", "motivation": "传统的文本转语音系统通常依赖于预定义标签、参考音频或自然语言提示对整句进行情感控制，但这种方法无法捕捉到句内情感的瞬息变化。为了弥补这一不足，提出了Emo-FiLM框架。", "method": "Emo-FiLM, 一个基于LLM的TTS的细粒度情感建模框架，通过将emotion2vec的帧级特征与词汇对齐以获取词汇级情感注释，并通过特征级线性调制（FiLM）层直接调制文本嵌入，实现词汇级情感控制。", "result": "实验表明，Emo-FiLM在全局和细粒度任务上均优于现有方法，证明了其在表达性语音合成中的有效性和通用性。", "conclusion": "Emo-FiLM成功解决了现有系统无法捕捉句子内部情感变化的局限性，显著提升了情感文本转语音的质量和自然度。"}}
{"id": "2509.20580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20580", "abs": "https://arxiv.org/abs/2509.20580", "authors": ["Xinyang Mu", "Yuzhen Lu", "Boyang Deng"], "title": "A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management", "comment": "19 pages, 6 figures, 4 tables. Abstract abridged due to arXiv's 1920\n  character limit", "summary": "Blueberry detection in natural environments remains challenging due to\nvariable lighting, occlusions, and motion blur due to environmental factors and\nimaging devices. Deep learning-based object detectors promise to address these\nchallenges, but they demand a large-scale, diverse dataset that captures the\nreal-world complexities. Moreover, deploying these models in practical\nscenarios often requires the right accuracy/speed/memory trade-off in model\nselection. This study presents a novel comparative benchmark analysis of\nadvanced real-time object detectors, including YOLO (You Only Look Once)\n(v8-v12) and RT-DETR (Real-Time Detection Transformers) (v1-v2) families,\nconsisting of 36 model variants, evaluated on a newly curated dataset for\nblueberry detection. This dataset comprises 661 canopy images collected with\nsmartphones during the 2022-2023 seasons, consisting of 85,879 labelled\ninstances (including 36,256 ripe and 49,623 unripe blueberries) across a wide\nrange of lighting conditions, occlusions, and fruit maturity stages. Among the\nYOLO models, YOLOv12m achieved the best accuracy with a mAP@50 of 93.3%, while\nRT-DETRv2-X obtained a mAP@50 of 93.6%, the highest among all the RT-DETR\nvariants. The inference time varied with the model scale and complexity, and\nthe mid-sized models appeared to offer a good accuracy-speed balance. To\nfurther enhance detection performance, all the models were fine-tuned using\nUnbiased Mean Teacher-based semi-supervised learning (SSL) on a separate set of\n1,035 unlabeled images acquired by a ground-based machine vision platform in\n2024. This resulted in accuracy gains ranging from -1.4% to 2.9%, with\nRT-DETR-v2-X achieving the best mAP@50 of 94.8%. More in-depth research into\nSSL is needed to better leverage cross-domain unlabeled data. Both the dataset\nand software programs of this study are made publicly available to support\nfurther research.", "AI": {"tldr": "本研究对比分析了YOLO和RT-DETR系列的实时对象检测模型在蓝莓检测任务上的表现，并通过半监督学习进一步优化了模型性能。", "motivation": "蓝莓检测在自然环境下由于光照、遮挡和运动模糊等因素变得具有挑战性，本研究旨在通过深度学习方法来改进检测效果。", "method": "研究通过一个新构建的蓝莓检测数据集评估了36种YOLO和RT-DETR模型变体的检测性能。数据集包括661幅树冠图像和85,879个标记实例，并使用了半监督学习技术进一步优化模型。", "result": "YOLOv12m和RT-DETRv2-X分别以93.3%和93.6%的mAP@50值表现最佳，而半监督学习使检测性能在不同模型上有了-1.4%到2.9%的提高。", "conclusion": "实验证明，更复杂的模型如YOLOv12m和RT-DETRv2-X在蓝莓检测任务上具有较高的准确性，且通过半监督学习可以进一步提高检测性能。这些发现为实际应用中模型选择提供了依据。"}}
{"id": "2509.20381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20381", "abs": "https://arxiv.org/abs/2509.20381", "authors": ["Jianyu Wen", "Jingyun Wang", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Ying Zhang"], "title": "USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model", "comment": "Accepted by Recsys'25", "summary": "Recently, Large Language Models (LLMs) have been widely employed in\nConversational Recommender Systems (CRSs). Unlike traditional language model\napproaches that focus on training, all existing LLMs-based approaches are\nmainly centered around how to leverage the summarization and analysis\ncapabilities of LLMs while ignoring the issue of training. Therefore, in this\nwork, we propose an integrated training-inference framework,\nUser-Simulator-Based framework (USB-Rec), for improving the performance of LLMs\nin conversational recommendation at the model level. Firstly, we design a\nLLM-based Preference Optimization (PO) dataset construction strategy for RL\ntraining, which helps the LLMs understand the strategies and methods in\nconversational recommendation. Secondly, we propose a Self-Enhancement Strategy\n(SES) at the inference stage to further exploit the conversational\nrecommendation potential obtained from RL training. Extensive experiments on\nvarious datasets demonstrate that our method consistently outperforms previous\nstate-of-the-art methods.", "AI": {"tldr": "提出了一种基于用户模拟器的培训推理框架(USB-Rec)，用于提高LLMs在会话推荐中的性能。该框架包括偏好优化数据集构建策略和自我增强策略。", "motivation": "现有的基于大型语言模型(LLMs)的方法主要集中在如何利用LLMs的总结和分析能力，而忽视了训练问题。本文旨在解决这一问题，提高LLMs在会话推荐系统中的性能。", "method": "Structure", "result": "在各种数据集上的广泛实验表明，所提出的方法始终优于之前的方法。", "conclusion": "通过综合的培训推理框架，本文改进了LLMs在会话推荐系统中性能的方法是可取的。"}}
{"id": "2509.20585", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20585", "abs": "https://arxiv.org/abs/2509.20585", "authors": ["Farbod Bigdeli", "Mohsen Mohammadagha", "Ali Bigdeli"], "title": "Region-of-Interest Augmentation for Mammography Classification under Patient-Level Cross-Validation", "comment": "5 pages, 5 figures, 2 tables", "summary": "Breast cancer screening with mammography remains central to early detection\nand mortality reduction. Deep learning has shown strong potential for\nautomating mammogram interpretation, yet limited-resolution datasets and small\nsample sizes continue to restrict performance. We revisit the Mini-DDSM dataset\n(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest\n(ROI) augmentation strategy. During training, full images are probabilistically\nreplaced with random ROI crops sampled from a precomputed, label-free\nbounding-box bank, with optional jitter to increase variability. We evaluate\nunder strict patient-level cross-validation and report ROC-AUC, PR-AUC, and\ntraining-time efficiency metrics (throughput and GPU memory). Because ROI\naugmentation is training-only, inference-time cost remains unchanged. On\nMini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest\naverage ROC-AUC gains, with performance varying across folds; PR-AUC is flat to\nslightly lower. These results demonstrate that simple, data-centric ROI\nstrategies can enhance mammography classification in constrained settings\nwithout requiring additional labels or architectural modifications.", "AI": {"tldr": "研究通过Mini-DDSM数据集引入了一种轻量级的兴趣区域(ROI)增强策略，以提高基于深度学习的乳腺癌筛查的性能，该策略在训练阶段显示了一定的提升效果，但在不同折中性能差异较大。", "motivation": "受限于低分辨率数据集和小样本规模，研究旨在通过一种轻量级ROI增强策略提高基于深度学习的乳腺摄影解释的性能，特别是在约束环境下。", "method": "研究采用Mini-DDSM数据集，并在训练阶段引入了一个ROI增强策略，即以一定概率将完整图像替换为随机采样的ROI裁剪，通过这种方式增加训练的变异性。", "result": "实验结果表明，在不同折中，该ROI增强策略在ROC-AUC上的平均增益较为有限，PR-AUC则保持平坦甚至略有下降。", "conclusion": "这项研究表明，即使在没有增加标签或修改现有架构的情况下，简单的基于数据的ROI策略也能在约束环境下提高乳腺摄影的分类性能。"}}
{"id": "2509.20461", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20461", "abs": "https://arxiv.org/abs/2509.20461", "authors": ["Bruce Kuwahara", "Chen-Yuan Lin", "Xiao Shi Huang", "Kin Kwan Leung", "Jullian Arta Yapeter", "Ilya Stanevich", "Felipe Perez", "Jesse C. Cresswell"], "title": "Document Summarization with Conformal Importance Guarantees", "comment": "NeurIPS 2025. Code is available at\n  https://github.com/layer6ai-labs/conformal-importance-summarization", "summary": "Automatic summarization systems have advanced rapidly with large language\nmodels (LLMs), yet they still lack reliable guarantees on inclusion of critical\ncontent in high-stakes domains like healthcare, law, and finance. In this work,\nwe introduce Conformal Importance Summarization, the first framework for\nimportance-preserving summary generation which uses conformal prediction to\nprovide rigorous, distribution-free coverage guarantees. By calibrating\nthresholds on sentence-level importance scores, we enable extractive document\nsummarization with user-specified coverage and recall rates over critical\ncontent. Our method is model-agnostic, requires only a small calibration set,\nand seamlessly integrates with existing black-box LLMs. Experiments on\nestablished summarization benchmarks demonstrate that Conformal Importance\nSummarization achieves the theoretically assured information coverage rate. Our\nwork suggests that Conformal Importance Summarization can be combined with\nexisting techniques to achieve reliable, controllable automatic summarization,\npaving the way for safer deployment of AI summarization tools in critical\napplications. Code is available at\nhttps://github.com/layer6ai-labs/conformal-importance-summarization.", "AI": {"tldr": "研究介绍了一种名为Conformal Importance Summarization的框架，这种框架使用符合性预测来创建保持重要性摘要，可以在高风险领域提供必要的覆盖保证。", "motivation": "尽管大语言模型在自动摘要系统中取得了快速进步，但在医疗保健、法律和金融等高风险领域，它们仍缺乏可靠保证关键内容被包含在内。为了解决这一问题，提出了此框架。", "method": "本研究介绍了一种名为Conformal Importance Summarization的框架，该框架使用了符合性预测，可以在抽取文档摘要时提供严格的、无分布的覆盖保证。通过校准句子重要性评分的阈值，使得提取概括能够达到用户指定的关键内容覆盖和召回率。此方法与模型无关，仅需少量校准集，且可以无缝集成到现有的黑盒大语言模型中。", "result": "在已有的摘要基准测试中，Conformal Importance Summarization实现了理论上保证的信息覆盖率。", "conclusion": "本研究展示了Conformal Importance Summarization框架与现有技术结合，可以实现可靠的、可控的自动摘要，为在关键应用中部署AI自动摘要工具铺平了道路。"}}
{"id": "2509.20607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20607", "abs": "https://arxiv.org/abs/2509.20607", "authors": ["Jing Wu", "Zirui Wang", "Iro Laina", "Victor Adrian Prisacariu"], "title": "Reflect3r: Single-View 3D Stereo Reconstruction Aided by Mirror Reflections", "comment": null, "summary": "Mirror reflections are common in everyday environments and can provide stereo\ninformation within a single capture, as the real and reflected virtual views\nare visible simultaneously. We exploit this property by treating the reflection\nas an auxiliary view and designing a transformation that constructs a\nphysically valid virtual camera, allowing direct pixel-domain generation of the\nvirtual view while adhering to the real-world imaging process. This enables a\nmulti-view stereo setup from a single image, simplifying the imaging process,\nmaking it compatible with powerful feed-forward reconstruction models for\ngeneralizable and robust 3D reconstruction. To further exploit the geometric\nsymmetry introduced by mirrors, we propose a symmetric-aware loss to refine\npose estimation. Our framework also naturally extends to dynamic scenes, where\neach frame contains a mirror reflection, enabling efficient per-frame geometry\nrecovery. For quantitative evaluation, we provide a fully customizable\nsynthetic dataset of 16 Blender scenes, each with ground-truth point clouds and\ncamera poses. Extensive experiments on real-world data and synthetic data are\nconducted to illustrate the effectiveness of our method.", "AI": {"tldr": "本文提出了一种新的3D重建方法，可以利用镜子反射从单张图像生成多视图信息，进而构建虚拟相机并生成虚拟视图，适用于静态和动态场景。", "motivation": "镜子反射提供了一种从单张图像中获取立体信息的方法，可以从单张图像中构建多视图立体设置，从而简化成像过程，使其与强大的前馈重建模型兼容，适用于泛化和稳健的3D重建。", "method": "通过利用镜子反射属性，设计一个将反射视为辅助视图并构建有效虚拟相机的变换过程，使能够在像素域直接生成虚拟视图，保持与现实世界的成像过程一致。同时提出对称感知损失来优化姿态估计。该框架还能扩展到动态场景，提供高效的逐帧几何恢复。", "result": "进行了在真实数据和合成数据上的广泛实验，展示了该方法的有效性。合成数据集包含了16个Blender场景及其地面真实的点云和相机姿态。", "conclusion": "利用镜子反射从单张图像中获取多视图信息的方法能够简化3D重建过程，并且在真实和合成数据上进行了大量实验验证了方法的有效性。"}}
{"id": "2509.20467", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20467", "abs": "https://arxiv.org/abs/2509.20467", "authors": ["Henrik Vatndal", "Vinay Setty"], "title": "ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos", "comment": null, "summary": "Short-form video platforms like TikTok present unique challenges for\nmisinformation detection due to their multimodal, dynamic, and noisy content.\nWe present ShortCheck, a modular, inference-only pipeline with a user-friendly\ninterface that automatically identifies checkworthy short-form videos to help\nhuman fact-checkers. The system integrates speech transcription, OCR, object\nand deepfake detection, video-to-text summarization, and claim verification.\nShortCheck is validated by evaluating it on two manually annotated datasets\nwith TikTok videos in a multilingual setting. The pipeline achieves promising\nresults with F1-weighted score over 70\\%.", "AI": {"tldr": "研究开发了ShortCheck，一个能够自动识别需要核查的短视频的系统，旨在提高事实核查效率。", "motivation": "由于短视频平台（如TikTok）内容的多模态、动态和杂乱特性，它们为错误信息检测带来了独特的挑战。", "method": "ShortCheck, 一个模块化、仅推理的管道，包含语音转录、OCR、对象和深度伪造检测、视频到文本摘要和声明验证等功能，以识别值得核查的短视频，帮助人工事实核查员。", "result": "该系统在一个多语言设置的两个手动注释数据集上进行了验证，其F1加权评分超过70%。", "conclusion": "ShortCheck系统展示了在识别和验证值得核查的短视频方面的潜力，为处理短视频平台上的虚假信息提供了有力的工具。"}}
{"id": "2509.20628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20628", "abs": "https://arxiv.org/abs/2509.20628", "authors": ["Yiming Xiao", "Archit Gupta", "Miguel Esparza", "Yu-Hsuan Ho", "Antonia Sebastian", "Hannah Weas", "Rose Houck", "Ali Mostafavi"], "title": "Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery", "comment": "17 pages, 10 figures", "summary": "Building-level occupancy after disasters is vital for triage, inspections,\nutility re-energization, and equitable resource allocation. Overhead imagery\nprovides rapid coverage but often misses facade and access cues that determine\nhabitability, while street-view imagery captures those details but is sparse\nand difficult to align with parcels. We present FacadeTrack, a street-level,\nlanguage-guided framework that links panoramic video to parcels, rectifies\nviews to facades, and elicits interpretable attributes (for example, entry\nblockage, temporary coverings, localized debris) that drive two decision\nstrategies: a transparent one-stage rule and a two-stage design that separates\nperception from conservative reasoning. Evaluated across two post-Hurricane\nHelene surveys, the two-stage approach achieves a precision of 0.927, a recall\nof 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a\nprecision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond\naccuracy, intermediate attributes and spatial diagnostics reveal where and why\nresidual errors occur, enabling targeted quality control. The pipeline provides\nauditable, scalable occupancy assessments suitable for integration into\ngeospatial and emergency-management workflows.", "AI": {"tldr": "研究提出FacadeTrack框架，结合街道全景视频与地块信息，评估建筑物灾害后的占用状态，能够提供高精度的可居住性评估。", "motivation": "为了在灾害后快速评估建筑物的可居住性，作者研究了一种结合高层和街道视图优势的方法，以提供快速、全面且精准的占用评估。", "method": "FacadeTrack是一种街道级别的、语言引导框架，它将全景视频与地块链接，矫正视角到立面，并提取出可解释的属性（例如入口阻塞、临时覆盖物、局部残骸），这些属性驱动两种决策策略：一种是透明的一阶段规则，另一种是将感知与保守推理分离的两阶段设计。", "result": "在两次飓风Helene后的调查中，两阶段方法达到了0.927的精度、0.781的召回率，以及0.848的F1分数，相比之下，一阶段基线方法达到了0.943的精度、0.728的召回率，以及0.822的F1分数。", "conclusion": "该研究提出的方法不仅可以提高占用评估的准确性，还可以揭示错误出现的位置和原因，从而提高质量控制。这种方法适用于地理空间和紧急管理工作流程。"}}
{"id": "2509.20502", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20502", "abs": "https://arxiv.org/abs/2509.20502", "authors": ["Xiao Wang", "Jia Wang", "Yijie Wang", "Pengtao Dang", "Sha Cao", "Chi Zhang"], "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved impressive results in natural\nlanguage understanding, yet their reasoning capabilities remain limited when\noperating as single agents. Multi-Agent Debate (MAD) has been proposed to\naddress this limitation by enabling collaborative reasoning among multiple\nmodels in a round-table debate manner. While effective, MAD introduces\nsubstantial computational overhead due to the number of agents involved and the\nfrequent communication required. In this paper, we propose MARS (Multi-Agent\nReview System), a role-based collaboration framework inspired by the review\nprocess. In MARS, an author agent generates an initial solution, reviewer\nagents provide decisions and comments independently, and a meta-reviewer\nintegrates the feedback to make the final decision and guide further revision.\nThis design enhances reasoning quality while avoiding costly\nreviewer-to-reviewer interactions, thereby controlling token consumption and\ninference time. We compared MARS with both MAD and other state-of-the-art\nreasoning strategies across multiple benchmarks. Extensive experiments with\ndifferent LLMs show that MARS matches the accuracy of MAD while reducing both\ntoken usage and inference time by approximately 50\\%. Code is available at\nhttps://github.com/xwang97/MARS.", "AI": {"tldr": "本文提出了MARS系统，它通过减少代理之间的交互，提高了推理质量，并在保持准确性的同时减少了计算资源的消耗。", "motivation": "为了克服现有的多代理辩论方法在计算上的高开销，同时提高推理质量。", "method": "本文提出了一种基于角色的合作框架MARS，其中作者代理生成初始解决方案，审稿人代理独立提供决策和意见，元审稿人集成反馈以做出最终决策并指导进一步修订。", "result": "实验结果显示，MARS在多个基准上达到了与MAD相当的准确性，同时将token使用量和推理时间减少了大约一半。", "conclusion": "MARS是一种有效的方法，可以在减少计算成本的同时提高多代理系统的推理质量。"}}
{"id": "2509.20673", "categories": ["cs.CV", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20673", "abs": "https://arxiv.org/abs/2509.20673", "authors": ["Yiling Yun", "Hongjing Lu"], "title": "Human Semantic Representations of Social Interactions from Moving Shapes", "comment": null, "summary": "Humans are social creatures who readily recognize various social interactions\nfrom simple display of moving shapes. While previous research has often focused\non visual features, we examine what semantic representations that humans employ\nto complement visual features. In Study 1, we directly asked human participants\nto label the animations based on their impression of moving shapes. We found\nthat human responses were distributed. In Study 2, we measured the\nrepresentational geometry of 27 social interactions through human similarity\njudgments and compared it with model predictions based on visual features,\nlabels, and semantic embeddings from animation descriptions. We found that\nsemantic models provided complementary information to visual features in\nexplaining human judgments. Among the semantic models, verb-based embeddings\nextracted from descriptions account for human similarity judgments the best.\nThese results suggest that social perception in simple displays reflects the\nsemantic structure of social interactions, bridging visual and abstract\nrepresentations.", "AI": {"tldr": "研究发现人类在识别动画中的社交互动时不仅依赖视觉特征，还使用语义表征，尤其是在基于动词的语义模型中表现最好。", "motivation": "研究旨在探索人类在识别简单动画中的社交互动时除视觉特征外所使用的表征方式。", "method": "通过两个研究来探讨人类在识别简单动画中的社交互动时所使用的语义表征，并将其与视觉特征进行比较。研究1直接要求人类参与者根据移动形状的印象对动画进行标注。研究2测量了27种社交互动的表征几何，并通过人类相似性判断与基于视觉特征、标签和动画描述的语义嵌入的模型预测进行比较。", "result": "研究发现语义模型为解释人类判断提供了补充信息，特别是基于动词的嵌入从描述中提取出的信息最好地解释了人类的相似性判断。", "conclusion": "这些结果表明，简单动画中的社交感知反映出了社交互动的语义结构，将视觉和抽象表征联系了起来。"}}
{"id": "2509.20557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.20557", "abs": "https://arxiv.org/abs/2509.20557", "authors": ["Hannah Liu", "Junghyun Min", "Ethan Yue Heng Cheung", "Shou-Yi Hung", "Syed Mekael Wasti", "Runtong Liang", "Shiyao Qian", "Shizhao Zheng", "Elsie Chan", "Ka Ieng Charlotte Lo", "Wing Yu Yip", "Richard Tzong-Han Tsai", "En-Shiun Annie Lee"], "title": "SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages", "comment": "Work in progress. 14 pages, 4 figures, 5 tables", "summary": "Despite major advances in machine translation (MT) in recent years, progress\nremains limited for many low-resource languages that lack large-scale training\ndata and linguistic resources. Cantonese and Wu Chinese are two Sinitic\nexamples, although each enjoys more than 80 million speakers around the world.\nIn this paper, we introduce SiniticMTError, a novel dataset that builds on\nexisting parallel corpora to provide error span, error type, and error severity\nannotations in machine-translated examples from English to Mandarin, Cantonese,\nand Wu Chinese. Our dataset serves as a resource for the MT community to\nutilize in fine-tuning models with error detection capabilities, supporting\nresearch on translation quality estimation, error-aware generation, and\nlow-resource language evaluation. We report our rigorous annotation process by\nnative speakers, with analyses on inter-annotator agreement, iterative\nfeedback, and patterns in error type and severity.", "AI": {"tldr": "本文介绍了SiniticMTError数据集，该数据集对英语到三门不同中文方言的机器翻译错误进行全面标注，以支持低资源语言翻译研究和错误检测能力提升。", "motivation": "尽管近年来机器翻译取得了显著进展，但对于缺乏大规模训练数据和语言资源的低资源语言，进展仍然有限。因此，本文旨在通过引入SiniticMTError数据集来改善这一状况。", "method": "本文介绍了SiniticMTError数据集的创建方法，该数据集针对英语到粤语、吴语和普通话的机器翻译错误进行标注，包括错误跨度、错误类型和错误严重程度。", "result": "该数据集为机器翻译社区提供了资源，可用于微调具有错误检测能力的模型，支持翻译质量评估、错误感知生成和低资源语言评估的研究。", "conclusion": "通过母语者的严格标注过程，本文提供了关于注释者之间的一致性、迭代反馈以及错误类型和严重程度模式的分析。"}}
{"id": "2509.20684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20684", "abs": "https://arxiv.org/abs/2509.20684", "authors": ["Xiaowei Wang", "Di Wang", "Ke Li", "Yifeng Wang", "Chengjian Wang", "Libin Sun", "Zhihong Wu", "Yiming Zhang", "Quan Wang"], "title": "Enhancing Cross-View Geo-Localization Generalization via Global-Local Consistency and Geometric Equivariance", "comment": null, "summary": "Cross-view geo-localization (CVGL) aims to match images of the same location\ncaptured from drastically different viewpoints. Despite recent progress,\nexisting methods still face two key challenges: (1) achieving robustness under\nsevere appearance variations induced by diverse UAV orientations and fields of\nview, which hinders cross-domain generalization, and (2) establishing reliable\ncorrespondences that capture both global scene-level semantics and fine-grained\nlocal details. In this paper, we propose EGS, a novel CVGL framework designed\nto enhance cross-domain generalization. Specifically, we introduce an\nE(2)-Steerable CNN encoder to extract stable and reliable features under\nrotation and viewpoint shifts. Furthermore, we construct a graph with a virtual\nsuper-node that connects to all local nodes, enabling global semantics to be\naggregated and redistributed to local regions, thereby enforcing global-local\nconsistency. Extensive experiments on the University-1652 and SUES-200\nbenchmarks demonstrate that EGS consistently achieves substantial performance\ngains and establishes a new state of the art in cross-domain CVGL.", "AI": {"tldr": "EGS, a novel framework for cross-view geo-localization, uses an E(2)-Steerable CNN and a graph with a virtual super-node to enhance cross-domain generalization and establish global-local consistency.", "motivation": "To address the challenges of robustness under severe appearance variations and the establishment of reliable correspondences between global scene-level semantics and fine-grained local details in cross-view geo-localization.", "method": "The method introduces an E(2)-Steerable CNN encoder to extract stable features under rotation and viewpoint shifts, and constructs a graph with a virtual super-node connected to all local nodes to facilitate global semantics aggregation and redistribution.", "result": "Experiments on University-1652 and SUES-200 benchmarks show that EGS achieves significant performance improvements.", "conclusion": "EGS sets a new state-of-the-art in cross-domain cross-view geo-localization."}}
{"id": "2509.20567", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20567", "abs": "https://arxiv.org/abs/2509.20567", "authors": ["Ayan Sar", "Pranav Singh Puri", "Sumit Aich", "Tanupriya Choudhury", "Abhijit Kumar"], "title": "SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations", "comment": "Submitted to International Conference on Big Data 2025", "summary": "In multilingual healthcare environments, automatic disease diagnosis from\nclinical text remains a challenging task due to the scarcity of annotated\nmedical data in low-resource languages and the linguistic variability across\npopulations. This paper proposes SwasthLLM, a unified, zero-shot,\ncross-lingual, and multi-task learning framework for medical diagnosis that\noperates effectively across English, Hindi, and Bengali without requiring\nlanguage-specific fine-tuning. At its core, SwasthLLM leverages the\nmultilingual XLM-RoBERTa encoder augmented with a language-aware attention\nmechanism and a disease classification head, enabling the model to extract\nmedically relevant information regardless of the language structure. To align\nsemantic representations across languages, a Siamese contrastive learning\nmodule is introduced, ensuring that equivalent medical texts in different\nlanguages produce similar embeddings. Further, a translation consistency module\nand a contrastive projection head reinforce language-invariant representation\nlearning. SwasthLLM is trained using a multi-task learning strategy, jointly\noptimizing disease classification, translation alignment, and contrastive\nlearning objectives. Additionally, we employ Model-Agnostic Meta-Learning\n(MAML) to equip the model with rapid adaptation capabilities for unseen\nlanguages or tasks with minimal data. Our phased training pipeline emphasizes\nrobust representation alignment before task-specific fine-tuning. Extensive\nevaluation shows that SwasthLLM achieves high diagnostic performance, with a\ntest accuracy of 97.22% and an F1-score of 97.17% in supervised settings.\nCrucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and\n73.33% accuracy on Bengali medical text, demonstrating strong generalization in\nlow-resource contexts.", "AI": {"tldr": "SwasthLLM是一个用于医疗诊断的跨语言、零样本学习框架，旨在增强不同语言中的疾病分类任务性能。", "motivation": "由于低资源语言的标注医疗数据稀缺以及人群间语言差异性，自动疾病诊断仍然是一个具有挑战性的任务。本文旨在提出一种统一、零样本、跨语言和多任务学习框架，以解决这一问题。", "method": "SwasthLLM采用多语言XLM-RoBERTa编码器，结合语言敏感的注意力机制和疾病分类头。为了使跨语言的语义表示一致，引入了暹罗对比学习模块，并使用翻译一致性模块和对比投影头来增强语言不变的表示学习。模型通过多任务学习策略进行训练，同时优化疾病分类、翻译对齐和对比学习目标。此外，还采用了元学习（MAML）方法来使模型具备快速适应新语言或任务的能力。", "result": "SwasthLLM展示了高诊断性能，并在多种语言间表现出了强大的零样本推广能力。", "conclusion": "SwastLLM在监督设置中实现了97.22%的测试准确率和97.17%的F1得分。在零样本场景中，该模型在印地语和孟加拉语医学文本上的准确率分别为92.78%和73.33%，展示了在低资源环境中的强推广能力。"}}
{"id": "2509.20701", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20701", "abs": "https://arxiv.org/abs/2509.20701", "authors": ["Jiayi Zuo", "Songwei Pei", "Qian Li"], "title": "DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection", "comment": null, "summary": "Infrared small target detection is crucial for remote sensing applications\nlike disaster warning and maritime surveillance. However, due to the lack of\ndistinctive texture and morphological features, infrared small targets are\nhighly susceptible to blending into cluttered and noisy backgrounds. A\nfundamental challenge in designing deep models for this task lies in the\ninherent conflict between capturing high-resolution spatial details for minute\ntargets and extracting robust semantic context for larger targets, often\nleading to feature misalignment and suboptimal performance. Existing methods\noften rely on fixed gradient operators or simplistic attention mechanisms,\nwhich are inadequate for accurately extracting target edges under low contrast\nand high noise. In this paper, we propose a novel Dual-Path Edge Network that\nexplicitly addresses this challenge by decoupling edge enhancement and semantic\nmodeling into two complementary processing paths. The first path employs a\nBidirectional Interaction Module, which uses both Local Self-Attention and\nGlobal Self-Attention to capture multi-scale local and global feature\ndependencies. The global attention mechanism, based on a Transformer\narchitecture, integrates long-range semantic relationships and contextual\ninformation, ensuring robust scene understanding. The second path introduces\nthe Multi-Edge Refiner, which enhances fine-grained edge details using cascaded\nTaylor finite difference operators at multiple scales. This mathematical\napproach, along with an attention-driven gating mechanism, enables precise edge\nlocalization and feature enhancement for targets of varying sizes, while\neffectively suppressing noise. Our method provides a promising solution for\nprecise infrared small target detection and localization, combining structural\nsemantics and edge refinement in a unified framework.", "AI": {"tldr": "提出Dual-Path Edge Network解决红外小目标检测中高噪声和低对比度问题，结合结构语义和边缘细化提高检测精度。", "motivation": "本文提出Dual-Path Edge Network是为了应对红外小目标检测中的关键挑战，即在高噪声和低对比度条件下准确提取目标边缘。", "method": "我们提出了一种名为Dual-Path Edge Network的新方法，通过将边缘增强和语义建模分成两个互补的处理路径来解决这一挑战。第一条路径使用双向交互模块，结合局部自我注意力和全局自我注意力来捕捉多尺度局部和全局特征依赖关系。第二条路径引入多边缘精修器，利用级联的Taylor有限差分算子在多尺度上增强边缘细节，并通过注意力驱动的门控机制实现精确的边缘定位和特征增强。", "result": "本研究没有直接展示具体的实验结果，但表明提出的方法在捕获精细边缘细节和增强不同大小目标的特征时效果显著，有效抑制了噪声。", "conclusion": "我们的方法在红外小目标检测和定位方面提供了一个有前景的解决方案，将结构语义和边缘细化统一在一个框架内。"}}
{"id": "2509.20577", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20577", "abs": "https://arxiv.org/abs/2509.20577", "authors": ["Sampurna Roy", "Ayan Sar", "Anurag Kaushish", "Kanav Gupta", "Tanupriya Choudhury", "Abhijit Kumar"], "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures", "comment": "Submitted in IEEE International Conference on Big Data 2025", "summary": "Contemporary transformer architectures apply identical processing depth to\nall inputs, creating inefficiencies and limiting reasoning quality. Simple\nfactual queries are subjected to the same multilayered computation as complex\nlogical problems, wasting resources while constraining deep inference. To\novercome this, we came up with a concept of Dynamic Reasoning Chains through\nDepth Specialised Mixture of Experts (DS-MoE), a modular framework that extends\nthe Mixture of Experts paradigm from width-based to depth specialised\ncomputation. DS-MoE introduces expert modules optimised for distinct reasoning\ndepths, shallow pattern recognition, compositional reasoning, logical\ninference, memory integration, and meta-cognitive supervision. A learned\nrouting network dynamically assembles custom reasoning chains, activating only\nthe necessary experts to match input complexity. The dataset on which we\ntrained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse\ndomains such as scientific papers, legal texts, programming code, and web\ncontent, enabling systematic assessment across reasoning depths. Experimental\nresults demonstrate that DS-MoE achieves up to 16 per cent computational\nsavings and 35 per cent faster inference compared to uniform-depth\ntransformers, while delivering 2.8 per cent higher accuracy on complex\nmulti-step reasoning benchmarks. Furthermore, routing decisions yield\ninterpretable reasoning chains, enhancing transparency and scalability. These\nfindings establish DS-MoE as a significant advancement in adaptive neural\narchitectures, demonstrating that depth-specialised modular processing can\nsimultaneously improve efficiency, reasoning quality, and interpretability in\nlarge-scale language models.", "AI": {"tldr": "提出了深度专业化专家混合（DS-MoE）框架，通过动态组合不同深度的专家模块来优化推理过程，提高了计算效率、推理质量和可解释性。", "motivation": "传统的变压器架构对所有输入应用相同的处理深度，这导致了资源浪费并限制了推理质量。", "method": "提出了深度专业化专家混合（DS-MoE）框架，使用优化的不同推理深度的专家模块，并通过学习路由网络动态组合这些专家模块来适应输入的复杂性。", "result": "在800GB的The Pile数据集上进行训练和评估，DS-MoE框架相比统一深度的变压器实现了最高16%的计算节省，推理速度提高了35%，并且复杂多步骤推理基准测试准确度提高了2.8%。", "conclusion": "DS-MoE框架证明了通过深度专业化模块化处理可以同时提高计算效率、推理质量和可解释性，是适应性神经架构的一个重要进步。"}}
{"id": "2509.20715", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20715", "abs": "https://arxiv.org/abs/2509.20715", "authors": ["Ruixu Zhang", "Yuran Wang", "Xinyi Hu", "Chaoyu Mai", "Wenxuan Liu", "Danni Xu", "Xian Zhong", "Zheng Wang"], "title": "Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset", "comment": null, "summary": "Intention recognition has traditionally focused on individual intentions,\noverlooking the complexities of collective intentions in group settings. To\naddress this limitation, we introduce the concept of group intention, which\nrepresents shared goals emerging through the actions of multiple individuals,\nand Group Intention Forecasting (GIF), a novel task that forecasts when group\nintentions will occur by analyzing individual actions and interactions before\nthe collective goal becomes apparent. To investigate GIF in a specific\nscenario, we propose SHOT, the first large-scale dataset for GIF, consisting of\n1,979 basketball video clips captured from 5 camera views and annotated with 6\ntypes of individual attributes. SHOT is designed with 3 key characteristics:\nmulti-individual information, multi-view adaptability, and multi-level\nintention, making it well-suited for studying emerging group intentions.\nFurthermore, we introduce GIFT (Group Intention ForecasTer), a framework that\nextracts fine-grained individual features and models evolving group dynamics to\nforecast intention emergence. Experimental results confirm the effectiveness of\nSHOT and GIFT, establishing a strong foundation for future research in group\nintention forecasting. The dataset is available at\nhttps://xinyi-hu.github.io/SHOT_DATASET.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.20581", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.20581", "abs": "https://arxiv.org/abs/2509.20581", "authors": ["Ayan Sar", "Sampurna Roy", "Kanav Gupta", "Anurag Kaushish", "Tanupriya Choudhury", "Abhijit Kumar"], "title": "Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding", "comment": "Submitted in IEEE International Conference on Big Data 2025", "summary": "Transformer architectures have achieved state-of-the-art performance across\nnatural language tasks, yet they fundamentally misrepresent the hierarchical\nnature of human language by processing text as flat token sequences. This\nresults in quadratic computational cost, weak computational cost, weak\ncompositional generalization, and inadequate discourse-level modeling. We\npropose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired\nneural architecture that processes language simultaneously across multiple\nresolutions, from characters to discourse-level units. HRT constructs a\nmulti-resolution attention, enabling bottom-up composition and top-down\ncontextualization. By employing exponential sequence reduction across scales,\nHRT achieves O(nlogn) complexity, offering significant efficiency improvements\nover standard transformers. We evaluated HRT on a diverse suite of benchmarks,\nincluding GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results\ndemonstrated that HRT outperforms standard transformer baselines by an average\nof +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while\nreducing memory usage by 42% and inference latency by 37% compared to BERT and\nGPT style models of similar parameter count. Ablation studies confirm the\neffectiveness of cross-resolution attention and scale-specialized modules,\nshowing that each contributes independently to both efficiency and accuracy.\nOur findings establish HRT as the first architecture to align computational\nstructure with the hierarchical organization of human language, demonstrating\nthat multi-scale, wavelet-inspired processing yields both theoretical\nefficiency gains and practical improvements in language understanding.", "AI": {"tldr": "本文提出了一种新型架构HRT，它通过模拟多分辨率处理来更有效地捕捉语言的层级结构，从而在多个自然语言任务基准上优于标准的 Transformer 模型。", "motivation": "当前的 Transformer 架构在处理文本时将其视为平坦的令牌序列，从根本上误解了人类语言的层级特性。这导致了二次计算成本，弱组合推广能力，以及对话语层次建模不足。", "method": "提出了一种名为分层解析变换器（HRT）的新神经架构，该架构受小波启发，能够同时在多个分辨率上处理语言，从字符到话语级别单元。HRT 构建了多分辨率注意力机制，实现了自下而上的组合化和自上而下的语境化。通过在尺度间使用指数序列约简，HRT 达到了 O(nlogn) 的计算复杂度，从而在效率上显著优于标准的 Transformer。", "result": "HRT 在 GLUE、SuperGLUE、Long Range Arena 和 WikiText-103 等多样化基准测试中实现了卓越性能，平均优于标准 Transformer 基准3.8%至6.1%，同时减少了42%的内存使用和37%的推理延迟。", "conclusion": "HRT 作为首个将计算结构与人类语言的层级组织对齐的架构，证明了受小波启发的多尺度处理不仅在理论上减少了计算资源的需求，而且在实际语言理解任务中也带来了显著改善。"}}
{"id": "2509.20745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.20745", "abs": "https://arxiv.org/abs/2509.20745", "authors": ["Yu Guo", "Shengfeng He", "Yuxu Lu", "Haonan An", "Yihang Tao", "Huilin Zhu", "Jingxian Liu", "Yuguang Fang"], "title": "Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection", "comment": null, "summary": "Maritime object detection is essential for navigation safety, surveillance,\nand autonomous operations, yet constrained by two key challenges: the scarcity\nof annotated maritime data and poor generalization across various maritime\nattributes (e.g., object category, viewpoint, location, and imaging\nenvironment). % In particular, models trained on existing datasets often\nunderperform in underrepresented scenarios such as open-sea environments. To\naddress these challenges, we propose Neptune-X, a data-centric\ngenerative-selection framework that enhances training effectiveness by\nleveraging synthetic data generation with task-aware sample selection. From the\ngeneration perspective, we develop X-to-Maritime, a multi-modality-conditioned\ngenerative model that synthesizes diverse and realistic maritime scenes. A key\ncomponent is the Bidirectional Object-Water Attention module, which captures\nboundary interactions between objects and their aquatic surroundings to improve\nvisual fidelity. To further improve downstream tasking performance, we propose\nAttribute-correlated Active Sampling, which dynamically selects synthetic\nsamples based on their task relevance. To support robust benchmarking, we\nconstruct the Maritime Generation Dataset, the first dataset tailored for\ngenerative maritime learning, encompassing a wide range of semantic conditions.\nExtensive experiments demonstrate that our approach sets a new benchmark in\nmaritime scene synthesis, significantly improving detection accuracy,\nparticularly in challenging and previously underrepresented settings.The code\nis available at https://github.com/gy65896/Neptune-X.", "AI": {"tldr": "论文介绍了Neptune-X框架，通过合成数据和样本选择提高了海事目标检测的性能。", "motivation": "为了解决海事领域的两个主要挑战：标注数据的稀缺性和在不同海事属性之间泛化能力差的问题，特别是现有数据集训练的模型在开放海域等场景下的表现欠佳。", "method": "提出了Neptune-X框架，该框架通过多模态条件生成模型X-to-Maritime生成各种各样的现实海事场景，并利用双向物体-水注意力模块捕捉物体与其水环境之间的边界互动。此外，提出了基于任务相关性的主动采样策略以提高下游任务性能，并构建了专用于生成式海事学习的Maritime Generation Dataset数据集。", "result": "实验结果表明，该方法在海事场景合成中建立了新的基准，并显著提升了检测准确性，尤其是在以前代表性不足的场景中。", "conclusion": "Neptune-X框架通过结合生成的数据和任务相关的样本选择，增强了训练的有效性，并在海事目标检测任务中展现了显著的性能提升。"}}
{"id": "2509.20624", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20624", "abs": "https://arxiv.org/abs/2509.20624", "authors": ["Amin Karimi Monsefi", "Nikhil Bhendawade", "Manuel Rafael Ciosici", "Dominic Culver", "Yizhe Zhang", "Irina Belousova"], "title": "FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models", "comment": null, "summary": "Autoregressive language models (ARMs) deliver strong likelihoods, but are\ninherently serial: they generate one token per forward pass, which limits\nthroughput and inflates latency for long sequences. Diffusion Language Models\n(DLMs) parallelize across positions and thus appear promising for language\ngeneration, yet standard discrete diffusion typically needs hundreds to\nthousands of model evaluations to reach high quality, trading serial depth for\niterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A\ndiscrete flow-matching model designed for speed without sacrificing quality.\nThe core idea is simple: make the number of sampling steps an explicit\nparameter and train the model to be consistent across step budgets, so one big\nmove lands where many small moves would. We pair this with a reliable update\nrule that moves probability in the right direction without overshooting, and\nwith strong teacher guidance distilled from long-run trajectories. Together,\nthese choices make few-step sampling stable, accurate, and easy to control. On\nlanguage modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity\nparity with a 1,024-step discrete-flow baseline for generating 1,024 tokens\nusing a similar-size model, delivering up to 128 times faster sampling and\ncorresponding latency/throughput gains.", "AI": {"tldr": "FS-DFM is a method designed to speed up discrete diffusion sampling in language generation models, enabling fast yet accurate language generation.", "motivation": "The motivation is to address the limitations of autoregressive language models (ARMs) and standard discrete diffusion models in terms of throughput and latency for long sequences. ARMs generate tokens serially, limiting their efficiency, while discrete diffusion typically requires many model evaluations, making the process time-consuming.", "method": "FS-DFM (Few-Step Discrete Flow-Matching) is introduced, which aims to make language generation faster without compromising quality. It achieves this by treating the number of sampling steps as a variable and training the model to be consistent across different step budgets, ensuring that a single step can achieve what multiple steps would otherwise accomplish. Additionally, the method uses a reliable update rule to guide probability updates accurately and avoids overshooting by using teacher guidance from long-run trajectories.", "result": "FS-DFM achieves comparable perplexity to a 1,024-step discrete-flow baseline at generating 1,024 tokens, while using only 8 sampling steps with a similarly sized model. This significantly accelerates the sampling process, improving overall latency and throughput.", "conclusion": "FS-DFM, through its innovative approach to sampling and model training, successfully balances speed with quality, making it a promising advancement in the realm of language generation models."}}
