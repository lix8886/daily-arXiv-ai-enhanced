<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
*Jiahong Li,Yiwen Shao,Jianheng Zhuo,Chenda Li,Liliang Tang,Dong Yu,Yanmin Qian*

Main category: cs.CL

> 本文聚焦于多语种ASR中不同语言的干扰问题，提供了一种基于LoRA语言专家的高效微调方法，实验显示在特定语言和语言不可知场景下性能显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 尽管深度学习的进展提高了多语种自动语音识别（ASR）的效果，但不同语言之间的相互干扰使得ASR模型在共享模型容量的同时难以有效识别多种语言。

**Method:** 本论文提出了一种基于LoRA语言专家的高效微调框架，通过LoRA专家融合或知识蒸馏的方法来提升多语种语音识别的效果。

**Result:** 实验结果表明，在语言感知和语言不可知的场景下，所提出的模型分别实现了约10%和15%的相对性能提升。

**Conclusion:** 提出的微调框架通过LoRA专家和知识蒸馏技术，相对于标准微调方法，在目标语言上获得了更好的识别性能。

**Abstract:** Recent advancements in deep learning have significantly enhanced multilingual
automatic speech recognition (ASR) due to the development of advanced model
architectures and available large-scale multilingual datasets. Despite that,
multilingual ASR still suffers from the curse of multilinguality in that
different languages tend to interfere with each other, making it difficult for
the ASR model to identify multiple languages effectively while sharing model
capacity across them. This paper proposes an efficient finetuning framework for
customized multilingual ASR via prepared LoRA language experts based on
Whisper. Through LoRA expert fusion or knowledge distillation, our approach
achieves better recognition performance on target languages than standard
fine-tuning methods. Experimental results demonstrate that the proposed models
yield approximately 10\% and 15\% relative performance gains in language-aware
and language-agnostic scenarios, respectively.

</details>


### [2] [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
*Hyeongcheol Park,MinHyuk Jang,Ha Dam Baek,Gyusam Chang,Jiyoung Seo,Jiwan Park,Hogun Park,Sangpil Kim*

Main category: cs.CL

> 提出Visual-Audio-Text知识图谱（VAT-KG），这是一种覆盖视觉、音频和文本信息的概念中心型多模态知识图谱，通过严格的过滤和对齐步骤自动从任何多模态数据集中生成。实验表明VAT-KG在跨模态问答任务中有效支持多模态大语言模型。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态知识图谱受限于范围和模态，限制了知识的完整性和丰富性，尤其是在视频和音频等更丰富的模态日益重要的情况下。因此提出VAT-KG来扩展知识图谱的范围和适用性。

**Method:** 采用一系列严格的过滤和对齐步骤，构建跨模态知识对齐的管道，自动从任何多模态数据集中生成VAT-KG。

**Result:** 实验表明，VAT-KG在跨模态的问答任务中有效，证明了其在统一和利用多模态知识上的实用价值。

**Conclusion:** VAT-KG通过涵盖视觉、音频和文本信息并支持从任意模态检索详细的概念级别知识，证明了其在支持多模态大语言模型中的有效性。

**Abstract:** Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge
across multiple modalities, play a pivotal role by complementing the implicit
knowledge of Multimodal Large Language Models (MLLMs) and enabling more
grounded reasoning via Retrieval Augmented Generation (RAG). However, existing
MMKGs are generally limited in scope: they are often constructed by augmenting
pre-existing knowledge graphs, which restricts their knowledge, resulting in
outdated or incomplete knowledge coverage, and they often support only a narrow
range of modalities, such as text and visual information. These limitations
reduce their extensibility and applicability to a broad range of multimodal
tasks, particularly as the field shifts toward richer modalities such as video
and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text
Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive
multimodal knowledge graph that covers visual, audio, and text information,
where each triplet is linked to multimodal data and enriched with detailed
descriptions of concepts. Specifically, our construction pipeline ensures
cross-modal knowledge alignment between multimodal data and fine-grained
semantics through a series of stringent filtering and alignment steps, enabling
the automatic generation of MMKGs from any multimodal dataset. We further
introduce a novel multimodal RAG framework that retrieves detailed
concept-level knowledge in response to queries from arbitrary modalities.
Experiments on question answering tasks across various modalities demonstrate
the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical
value in unifying and leveraging multimodal knowledge.

</details>


### [3] [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
*Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu*

Main category: cs.CL

> 论文提出了一种结合反证知识改进假新闻检测方法的框架DIFND，在FakeSV和FVC数据集上其实验结果超越了现有技术方案并提供了可信的决策结果。

<details>
  <summary>Details</summary>

**Motivation:** 假新闻在多媒体平台上的快速传播对信息可信度构成了严重挑战。该论文旨在利用反击知识来提高假新闻检测的表现和可解释性。

**Method:** 提出了一种称作Debunk-and-Infer框架用于假新闻检测(DIFND)，该框架结合了条件扩散模型的生成能力和多模态大语言模型（MLLMs）的协作推理能力。具体而言，使用反证扩散生成基于新闻视频多模态内容的反驳或认证证据，丰富评估过程。为提高推理，提出了一种多重反证策略，其中多代理MLLM系统产生逻辑基础且多模态感知的推理内容并对最终真实性进行判断。

**Result:** 在假新闻检测的准确性方面取得了显著提升。在FakeSV和FVC数据集上的广泛实验证明，DIFND不仅超越了现有方法，而且提供了值得信赖的决策。

**Conclusion:** DIFND通过结合多模态特征、生成性反证线索和丰富的推理验证，在统一架构中实现了假新闻检测的显著改进。实验表明其在提高检测准确性的同时，还提供了可信的决策。

**Abstract:** The rapid spread of fake news across multimedia platforms presents serious
challenges to information credibility. In this paper, we propose a
Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages
debunking knowledge to enhance both the performance and interpretability of
fake news detection. DIFND integrates the generative strength of conditional
diffusion models with the collaborative reasoning capabilities of multimodal
large language models (MLLMs). Specifically, debunk diffusion is employed to
generate refuting or authenticating evidence based on the multimodal content of
news videos, enriching the evaluation process with diverse yet semantically
aligned synthetic samples. To improve inference, we propose a chain-of-debunk
strategy where a multi-agent MLLM system produces logic-grounded,
multimodal-aware reasoning content and final veracity judgment. By jointly
modeling multimodal features, generative debunking cues, and reasoning-rich
verification within a unified architecture, DIFND achieves notable improvements
in detection accuracy. Extensive experiments on the FakeSV and FVC datasets
show that DIFND not only outperforms existing approaches but also delivers
trustworthy decisions.

</details>


### [4] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

> BTF是一个新的“回溯”评估基准，通过高质量的已知结果问题和相关网页集合，为大模型提供了一个现实的、封闭的且可重复的预测测试环境，旨在成为持续更新的活跃基准。

<details>
  <summary>Details</summary>

**Motivation:** 当前没有一个预测基准能够提供一个现实的、封闭的且可重复的环境供大模型进行预测，而预测又需要大量的在线研究以及时间来验证结果，这使得开发预测基准极具挑战性。

**Method:** 介绍了一个名为Bench To the Future (BTF) 的“回溯”基准测试，包含数百个已知结果的高质量问题，每个问题都伴随着成千上万的相关网页的离线语料库，使大规模语言模型（LLMs）能够对过去事件进行实际的“预测”。

**Result:** 结果显示，该回溯环境所产生的结果与基于尚未解决的问题并使用互联网进行预测所产生的结果相当。使用几个大模型，包括最近发布的Claude 4模型，进行代理预测和链式思考预测方法的基准测试，并展示了BTF追踪预测能力随时间发展的能力。

**Conclusion:** BTF能够作为一个活的基准，会持续更新新的问题，以适应不断增加的数据训练截止日期。

**Abstract:** Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [5] [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
*Junze Chen,Cheng Yang,Shujie Li,Zhiqiang Zhang,Yawen Li,Junping Du,Chuan Shi*

Main category: cs.CL

> The paper introduces GraphLAMA, a method using a GNN backbone and efficient adaptation mechanism for GLMs, offering state-of-the-art performance and faster inference on graph tasks with few labeled examples compared to existing ICL and instruction tuning approaches.

<details>
  <summary>Details</summary>

**Motivation:** The authors address the limitations of in-context learning (ICL) and instruction tuning in the context of graph analysis, which include poor effectiveness, low efficiency, and the need for large labeled datasets. They aim to improve the adaptation of GLMs to unseen tasks with minimal labeled data.

**Method:** The paper proposes GraphLAMA, a method designed to efficiently tailor GLMs to new graph tasks using few labeled examples. The method utilizes a GNN backbone to transform nodes into LLM token representation space, allowing task instructions to be encoded as a mix of node and language tokens. Training involves a pre-training phase on general tasks and an efficient adaptation phase using few-shot examples.

**Result:** Experiments on few/zero-shot node classification and summary generation tasks demonstrate that GraphLAMA achieves state-of-the-art performance, with a 4.91% absolute improvement in accuracy. Compared to in-context learning, GraphLAMA offers a 10x speedup in inference under a 5-shot setting.

**Conclusion:** The GraphLAMA method is shown to be effective in improving prediction accuracy and inference speed in graph tasks with few labeled examples. This makes it a viable solution for real-world scenarios where extensive labeled data is not available.

**Abstract:** Large language models (LLMs) have demonstrated their strong capabilities in
various domains, and have been recently integrated for graph analysis as graph
language models (GLMs). With LLMs as the predictor, some GLMs can interpret
unseen tasks described by natural language, and learn from a few examples in
the prompts without parameter tuning, known as in-context learning (ICL).
Another subset of GLMs utilizes abundant training labels to enhance model
performance, known as instruction tuning. However, we argue that ICL on graphs
has effectiveness issues due to fixed parameters and efficiency issues due to
long context. Meanwhile, the large amount of labeled data required for
instruction tuning can be difficult to obtain in real-world scenarios. To this
end, we aim to introduce an extra parameter adaptation stage that can
efficiently tailor GLMs to an unseen graph and task with only a few labeled
examples, in exchange for better prediction accuracy and faster inference
speed. For implementation, in this paper we propose GraphLAMA method, with its
model backbone and learning schemes specialized for efficient tuning and
inference. Specifically, for model backbone, we use a graph neural network
(GNN) with several well-designed components to transform nodes into the
representation space of LLM tokens. Task instructions can then be represented
as a mixture of node and language tokens. In the pre-training stage, model
parameters except the LLM will be trained with different tasks to capture
general knowledge. In the adaptation stage, only a few pre-trained parameters
will be updated based on few-shot examples. Extensive experiments on
few/zero-shot node classification and summary generation show that our proposed
GraphLAMA achieves state-of-the-art performance with 4.91% absolution
improvement in accuracy. Compared with ICL, our inference speed can be 10 times
faster under 5-shot setting.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Main category: cs.CV

> The paper presents SpatialReasoner-R1 and innovative fDPO and M3CTS techniques, improving spatial reasoning in VLMs with significant gains over existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The main motivation is to enhance current Vision-Language Models (VLMs) by addressing their issues with fine-grained spatial reasoning, especially in situations involving multi-step logic and detailed spatial alignment.

**Method:** The paper introduces the SpatialReasoner-R1 model and utilizes a novel Multi-Model Monte Carlo Tree Search (M3CTS) method to generate high-quality supervision for spatial reasoning. A fine-grained Direct Preference Optimization (fDPO) technique is also proposed, which provides segment-specific preference granularity for descriptive grounding and logical reasoning.

**Result:** The experimental outcomes reveal that SpatialReasoner-R1, utilizing fDPO, enhances spatial quality task performance by an average of 4.1%, while achieving a 9.0% increase in spatial quantity tasks, and surpasses the current best model by 9.8% on the SPATIALRGPT-Bench.

**Conclusion:** SpatialReasoner-R1, incorporating fDPO, establishes a new state-of-the-art level of performance on the SPATIALRGPT-Bench, showing a major improvement over the previous best model.

**Abstract:** Current Vision-Language Models (VLMs) struggle with fine-grained spatial
reasoning, particularly when multi-step logic and precise spatial alignment are
required. In this work, we introduce SpatialReasoner-R1, a vision-language
reasoning model designed to address these limitations. To construct
high-quality supervision for spatial reasoning, we design a Multi-Model Monte
Carlo Tree Search (M3CTS) method that generates diverse, logically consistent
Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose
fine-grained Direct Preference Optimization (fDPO), which introduces
segment-specific preference granularity for descriptive grounding and logical
reasoning, guided by a spatial reward mechanism that evaluates candidate
responses based on visual consistency, spatial grounding, and logical
coherence. Experimental results demonstrate that fDPO achieves an average
improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%
gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a
new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in
average accuracy, while maintaining competitive performance on general
vision-language tasks.

</details>


### [7] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

> TanDiT提出了通过生成切平面图像来合成全景图像的新方法，并通过新的评估指标TangentIS和TangentFID，实现了高质量的全景图像生成。

<details>
  <summary>Details</summary>

**Motivation:** 当前的图像生成模型在处理全景图像生成时，面临着几何畸变变化和无缝循环一致性的独特挑战，而该方法旨在克服这些问题的同时，利用现有模型的优势。

**Method:** 一种名为TanDiT的方法被提出，用于通过生成覆盖整个360度视角的切平面图像网格来合成全景场景，不同于依赖多个扩散分支的先前方法，TanDiT使用单一统一扩散模型同时生成这些切平面图像。此外，作者提出了一种模型无关的后处理步骤旨在增强生成全景图像的全局一致性。

**Result:** 实验表明，该方法能够超越其训练数据泛化，准确解释详细的复杂文本提示，并与其他生成模型无缝集成，生成高质量多样的全景图像。

**Conclusion:** 引入了评估全景图像质量的两个专门指标TangentIS和TangentFID，以及包含标注过的全景数据集和标准化评估脚本的全面基准。

**Abstract:** Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>
