{"id": "2506.15747", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.15747", "abs": "https://arxiv.org/abs/2506.15747", "authors": ["Fangzhou Lin", "Zilin Dai", "Rigved Sanku", "Songlin Hou", "Kazunori D Yamada", "Haichong K. Zhang", "Ziming Zhang"], "title": "A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion", "comment": "6 pages, 2 figures", "summary": "The single-view image guided point cloud completion (SVIPC) task aims to\nreconstruct a complete point cloud from a partial input with the help of a\nsingle-view image. While previous works have demonstrated the effectiveness of\nthis multimodal approach, the fundamental necessity of image guidance remains\nlargely unexamined. To explore this, we propose a strong baseline approach for\nSVIPC based on an attention-based multi-branch encoder-decoder network that\nonly takes partial point clouds as input, view-free. Our hierarchical\nself-fusion mechanism, driven by cross-attention and self-attention layers,\neffectively integrates information across multiple streams, enriching feature\nrepresentations and strengthening the networks ability to capture geometric\nstructures. Extensive experiments and ablation studies on the ShapeNet-ViPC\ndataset demonstrate that our view-free framework performs superiorly to\nstate-of-the-art SVIPC methods. We hope our findings provide new insights into\nthe development of multimodal learning in SVIPC. Our demo code will be\navailable at https://github.com/Zhang-VISLab.", "AI": {"tldr": "本文探讨了单视图图像在点云补全任务中的必要性，提出了一种无需图像引导的点云补全方法，实验显示其优于现有方法。", "motivation": "探讨在单视图图像引导的点云补全任务中，图像引导的基本必要性，提出一种无视角的基线方法来检验其效果。", "method": "采用基于注意力机制的多分支编码-解码网络，该网络只使用部分点云作为输入，通过层次自融合机制，跨多个流融合信息，增强特征表示，提高捕捉几何结构的能力。", "result": "该研究旨在探讨基于单视图图像引导下的点云补全任务，其提出的无视角基线方法在无需单视角图像输入的情况下，基于注意力机制的多分支编码-解码网络，实现了优于当前最先进的单视图图像引导点云补全方法的性能。实验验证了这种方法在ShapeNet-ViPC数据集上的有效性。", "conclusion": "研究发现，提出的无视角框架在单视图图像引导的点云补全任务上表现出色，可能为这一领域的多模态学习提供新的见解。"}}
{"id": "2506.15755", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.15755", "abs": "https://arxiv.org/abs/2506.15755", "authors": ["Xiasi Wang", "Tianliang Yao", "Simin Chen", "Runqi Wang", "Lei YE", "Kuofeng Gao", "Yi Huang", "Yuan Yao"], "title": "VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service", "comment": "Accepted by ACL 2025", "summary": "Vision-Language Models (VLMs) have demonstrated great potential in real-world\napplications. While existing research primarily focuses on improving their\naccuracy, the efficiency remains underexplored. Given the real-time demands of\nmany applications and the high inference overhead of VLMs, efficiency\nrobustness is a critical issue. However, previous studies evaluate efficiency\nrobustness under unrealistic assumptions, requiring access to the model\narchitecture and parameters -- an impractical scenario in ML-as-a-service\nsettings, where VLMs are deployed via inference APIs. To address this gap, we\npropose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness\nin a realistic black-box setting. VLMInferSlow incorporates fine-grained\nefficiency modeling tailored to VLM inference and leverages zero-order\noptimization to search for adversarial examples. Experimental results show that\nVLMInferSlow generates adversarial images with imperceptible perturbations,\nincreasing the computational cost by up to 128.47%. We hope this research\nraises the community's awareness about the efficiency robustness of VLMs.", "AI": {"tldr": "研究提出了一种方法VLMInferSlow，用于在实际部署中有效评估视觉语言模型的效率鲁棒性，特别关注在对抗样本影响下的模型表现。", "motivation": "传统的研究主要集中在提高视觉语言模型的准确性，而效率问题较少受到关注。现有评估方法在机器学习作为服务的部署方式下存在不切实际的假设，因此需要一种新的评估方法。", "method": "提出了一种新的方法VLMInferSlow，用于在实际的黑盒环境下评估视觉语言模型的效率鲁棒性。该方法采用了针对视觉语言推理的精细效率建模，并利用零阶优化来搜索对抗样本。", "result": "实验结果表明，VLMInferSlow可以生成具有不明显扰动的对抗图像，将计算成本最多提高128.47%。", "conclusion": "此研究旨在引起社区对视觉语言模型效率鲁棒性的关注，并提供了在实际黑盒设置下评估其效率的新途径。"}}
