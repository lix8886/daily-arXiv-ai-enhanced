{"id": "2510.18888", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18888", "abs": "https://arxiv.org/abs/2510.18888", "authors": ["Daniel Vollmers", "Hamada M. Zahera", "Diego Moussallem", "Axel-Cyrille Ngonga Ngomo"], "title": "Contextual Augmentation for Entity Linking using Large Language Models", "comment": null, "summary": "Entity Linking involves detecting and linking entity mentions in natural\nlanguage texts to a knowledge graph. Traditional methods use a two-step process\nwith separate models for entity recognition and disambiguation, which can be\ncomputationally intensive and less effective. We propose a fine-tuned model\nthat jointly integrates entity recognition and disambiguation in a unified\nframework. Furthermore, our approach leverages large language models to enrich\nthe context of entity mentions, yielding better performance in entity\ndisambiguation. We evaluated our approach on benchmark datasets and compared\nwith several baselines. The evaluation results show that our approach achieves\nstate-of-the-art performance on out-of-domain datasets.", "AI": {"tldr": "本文提出了一种基于大型语言模型在统一框架中结合实体识别与消歧的方法，该方法在跨领域数据集上取得了最佳性能。", "motivation": "传统的实体链接方法效率较低，难以有效处理大规模数据。", "method": "提出一种在统一样本上同时进行实体识别与消歧的微调模型，并利用大型语言模型丰富实体提及的语境。", "result": "在基准数据集上进行评估，并与若干基线方法进行了比较，结果表明本文方法在跨领域数据集上达到了最先进的性能。", "conclusion": "结果显示，通过在统一框架中实现实体识别和消歧，并使用大型语言模型增强实体提及的上下文，可以得到更好的实体消歧效果。"}}
{"id": "2510.18890", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18890", "abs": "https://arxiv.org/abs/2510.18890", "authors": ["Jian Zhang"], "title": "Small Language Models Offer Significant Potential for Science Community", "comment": null, "summary": "Recent advancements in natural language processing, particularly with large\nlanguage models (LLMs), are transforming how scientists engage with the\nliterature. While the adoption of LLMs is increasing, concerns remain regarding\npotential information biases and computational costs. Rather than LLMs, I\ndeveloped a framework to evaluate the feasibility of precise, rapid, and\ncost-effective information retrieval from extensive geoscience literature using\nfreely available small language models (MiniLMs). A curated corpus of\napproximately 77 million high-quality sentences, extracted from 95 leading\npeer-reviewed geoscience journals such as Geophysical Research Letters and\nEarth and Planetary Science Letters published during years 2000 to 2024, was\nconstructed. MiniLMs enable a computationally efficient approach for extracting\nrelevant domain-specific information from these corpora through semantic search\ntechniques and sentence-level indexing. This approach, unlike LLMs such as\nChatGPT-4 that often produces generalized responses, excels at identifying\nsubstantial amounts of expert-verified information with established,\nmulti-disciplinary sources, especially for information with quantitative\nfindings. Furthermore, by analyzing emotional tone via sentiment analysis and\ntopical clusters through unsupervised clustering within sentences, MiniLM\nprovides a powerful tool for tracking the evolution of conclusions, research\npriorities, advancements, and emerging questions within geoscience communities.\nOverall, MiniLM holds significant potential within the geoscience community for\napplications such as fact and image retrievals, trend analyses, contradiction\nanalyses, and educational purposes.", "AI": {"tldr": "本文介绍了使用小型语言模型(MiniLM)进行地质科学文献高效信息检索的框架，展示了其在成本效益、信息精确度和领域特定信息提取方面的优势。", "motivation": "作者旨在解决大型语言模型（LLMs）在信息偏差和计算成本方面的关切，通过MiniLM提供一个成本效益、计算效率更高的信息检索方案。", "method": "作者构建了一个由7700万高质量句子组成的语料库，这些句子来自95种领先的同行评议地质科学期刊。采用语义搜索技术和句子级别索引，通过MiniLM框架提取相关领域信息。", "result": "相较于LLMs，MiniLM能够在提取经专家验证的信息方面表现更佳，特别是在数值结果信息方面。同时，通过情感分析和无监督聚类等技术，MiniLM能够追踪地学界结论、研究重点、进步和新问题的演变。", "conclusion": "MiniLM在地质科学社区具有巨大潜力，适用于事实和图像检索、趋势分析、矛盾分析和教育目的等多种应用。"}}
{"id": "2510.18892", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.18892", "abs": "https://arxiv.org/abs/2510.18892", "authors": ["Richard J. Young", "Brandon Gillins", "Alice M. Matthews"], "title": "When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs", "comment": "21 pages, 3 figures, 5 tables. Comprehensive evaluation of 256 LLMs\n  on instruction-following tasks", "summary": "Despite widespread deployment of Large Language Models, systematic evaluation\nof instruction-following capabilities remains challenging. While comprehensive\nbenchmarks exist, focused assessments that quickly diagnose specific\ninstruction adherence patterns are valuable. As newer models may be trained on\nexisting benchmarks, novel evaluation approaches are needed to assess genuine\ncapabilities rather than memorized performance. This paper presents a\nstreamlined evaluation framework using twenty carefully designed prompts to\nassess LLM instruction-following across diverse task categories. We demonstrate\nthis framework through a large-scale empirical study conducted on October 14,\n2025, testing 256 verified working models from 331 available via OpenRouter. To\nensure methodological rigor and prevent selection bias, we first verified each\nmodel's basic functionality before inclusion. Unlike large-scale benchmarks\nrequiring extensive computational resources, our approach offers a practical\ndiagnostic tool researchers and practitioners can readily apply. Our\nmethodology builds upon verifiable instructions while introducing a compact\ntest suite balancing comprehensiveness with efficiency. Each prompt targets\ndistinct aspects of instruction following, including format compliance, content\nconstraints, logical sequencing, and multi-step task execution. We evaluate\nmodels from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and\nemerging implementations (Qwen, DeepSeek, community models), providing\ncomparative performance analysis. Our findings reveal consistent failure modes\nand identify specific instruction types posing particular challenges. This work\ncontributes both a practical evaluation tool and one of the most comprehensive\nempirical analyses of instruction-following capabilities across the\ncontemporary LLM landscape.", "AI": {"tldr": "本文提出了一种评估大型语言模型（LLM）指令跟随能力的简化框架，并通过一项大规模研究进行了验证，该研究涵盖了多个知名提供商和新兴实现的模型。", "motivation": "虽然大型语言模型被广泛部署，但系统性评估指令跟随能力仍然具有挑战性。为了尽快诊断特定的指令遵循模式，需要新颖的评估方法，并提供一个实用的诊断工具，以供研究人员和从业者使用。", "method": "本研究提出了一种简化的评估框架，使用20个精心设计的提示来评估LLM在不同任务类别中的指令跟随能力。该框架通过一个大规模的经验研究进行演示，该研究于2025年10月14日进行，测试了来自OpenRouter的331个可用模型中的256个经过验证的运行模型。", "result": "研究结果揭示了一致的失败模式，并确定了对特定指令类型构成挑战的具体类型。", "conclusion": "这项工作不仅提供了一个实用的评估工具，还提供了对当代LLM指令跟随能力最全面的经验分析之一。"}}
{"id": "2510.18898", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.18898", "abs": "https://arxiv.org/abs/2510.18898", "authors": ["Mangsura Kabir Oni", "Tabia Tanzin Prama"], "title": "Transformer-Based Low-Resource Language Translation: A Study on Standard Bengali to Sylheti", "comment": null, "summary": "Machine Translation (MT) has advanced from rule-based and statistical methods\nto neural approaches based on the Transformer architecture. While these methods\nhave achieved impressive results for high-resource languages, low-resource\nvarieties such as Sylheti remain underexplored. In this work, we investigate\nBengali-to-Sylheti translation by fine-tuning multilingual Transformer models\nand comparing them with zero-shot large language models (LLMs). Experimental\nresults demonstrate that fine-tuned models significantly outperform LLMs, with\nmBART-50 achieving the highest translation adequacy and MarianMT showing the\nstrongest character-level fidelity. These findings highlight the importance of\ntask-specific adaptation for underrepresented languages and contribute to\nongoing efforts toward inclusive language technologies.", "AI": {"tldr": "研究表明，微调多语言Transformer模型在低资源语言如锡尔赫提语的机器翻译中优于零样本大型语言模型。", "motivation": "尽管神经网络方法在高资源语言的机器翻译方面取得了显著成果，但低资源语言，如锡尔赫提语，仍被较少研究。本研究旨在填补这一空白。", "method": "本研究通过微调多语言Transformer模型并将其与零样本大型语言模型（LLMs）进行比较，来研究孟加拉语到锡尔赫提语的翻译。", "result": "实验结果显示，微调模型在翻译充分性和字符级别保真度方面显著优于LLMs。mBART-50在翻译充分性方面表现最佳，而MarianMT在字符级别保真度方面表现最强。", "conclusion": "研究结果强调了针对被边缘化的语言进行任务特异性适应的重要性，并为促进语言技术的包容性做出了贡献。"}}
{"id": "2510.18904", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18904", "abs": "https://arxiv.org/abs/2510.18904", "authors": ["Shriyansh Agrawal", "Aidan Lau", "Sanyam Shah", "Ahan M R", "Kevin Zhu", "Sunishchal Dev", "Vasu Sharma"], "title": "DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code", "comment": "Accepted to 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025): 4th Workshop on Deep Learning for Code", "summary": "The prevalence of Large Language Models (LLMs) for generating multilingual\ntext and source code has only increased the imperative for machine-generated\ncontent detectors to be accurate and efficient across domains. Current\ndetectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or\nGPTZero, either incur high computational cost or lack sufficient accuracy,\noften with a trade-off between the two, leaving room for further improvement.\nTo address these gaps, we propose the fine-tuning of encoder-only Small\nLanguage Models (SLMs), in particular, the pre-trained models of RoBERTA and\nCodeBERTa using specialized datasets on source code and other natural language\nto prove that for the task of binary classification, SLMs outperform LLMs by a\nhuge margin whilst using a fraction of compute. Our encoders achieve AUROC $=\n0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by\n$8$-$12\\times$ and peak VRAM by $3$-$5\\times$ at $512$-token inputs. Under\ncross-generator shifts and adversarial transformations (paraphrase,\nback-translation; code formatting/renaming), performance retains $\\geq 92%$ of\nclean AUROC. We release training and evaluation scripts with seeds and configs;\na reproducibility checklist is also included.", "AI": {"tldr": "通过微调小型语言模型，研究解决了大型语言模型在机器生成内容检测准确性与计算成本之间的权衡问题，提高了效率并降低了计算资源使用。", "motivation": "鉴于大型语言模型（LLM）在生成多语言文本和源代码方面的流行，迫切需要高效的机器生成内容检测器。当前的检测方法，如Fast DetectGPT和GPTZero，大多数依赖零样本方法，存在成本高或准确性不足的问题。", "method": "本研究通过微调RoBERTA和CodeBERTa等小型语言模型（SLM），使用特定的数据集进行源代码和其他自然语言的训练，以证明在二分类任务中SLM的表现远超大型语言模型（LLM），同时计算成本更低。", "result": "所采用的编码器在AUROC达到了0.97到0.99，macro-F1达到了0.89到0.94，同时降低了8-12倍的延迟和3-5倍的VRAM消耗。在跨生成器变化和对抗变换中的表现仍保持相对较高的准确性。", "conclusion": "研究表明，通过微调小型语言模型，可以实现更高效且准确的机器生成内容检测，减少了计算资源的消耗，并且在不同的测试条件下仍然保持较高的性能。"}}
{"id": "2510.18908", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18908", "abs": "https://arxiv.org/abs/2510.18908", "authors": ["Wangjiaxuan Xin", "Shuhua Yin", "Shi Chen", "Yaorong Ge"], "title": "Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets", "comment": null, "summary": "Social media platforms such as Twitter (now X) provide rich data for\nanalyzing public discourse, especially during crises such as the COVID-19\npandemic. However, the brevity, informality, and noise of social media short\ntexts often hinder the effectiveness of traditional topic modeling, producing\nincoherent or redundant topics that are often difficult to interpret. To\naddress these challenges, we have developed \\emph{TM-Rephrase}, a\nmodel-agnostic framework that leverages large language models (LLMs) to\nrephrase raw tweets into more standardized and formal language prior to topic\nmodeling. Using a dataset of 25,027 COVID-19-related Twitter posts, we\ninvestigate the effects of two rephrasing strategies, general- and\ncolloquial-to-formal-rephrasing, on multiple topic modeling methods. Results\ndemonstrate that \\emph{TM-Rephrase} improves three metrics measuring topic\nmodeling performance (i.e., topic coherence, topic uniqueness, and topic\ndiversity) while reducing topic redundancy of most topic modeling algorithms,\nwith the colloquial-to-formal strategy yielding the greatest performance gains\nand especially for the Latent Dirichlet Allocation (LDA) algorithm. This study\ncontributes to a model-agnostic approach to enhancing topic modeling in public\nhealth related social media analysis, with broad implications for improved\nunderstanding of public discourse in health crisis as well as other important\ndomains.", "AI": {"tldr": "研究开发了\\emph{TM-Rephrase}框架，利用大语言模型提升社交媒体主题模型建模效果，尤其是在公共健康危机语境中的表现。", "motivation": "传统的主题建模方法在处理如推特（现为X）等社交媒体平台上的简短、非正式和充满噪音的文本时效果不佳，生成的主题通常难以解释，存在不连贯或冗余。为了解决这些问题，开展了这项研究。", "method": "开发了名为\\emph{TM-Rephrase}的模型无关框架，该框架利用大语言模型在主题建模前将原始推文重新表述为更标准化和正式的语言。研究了两种重新表述策略（通用型和通俗到正式语言的重新表述）对多种主题建模方法的影响。", "result": "\\emph{TM-Rephrase}提高了主题建模性能的三个指标（即主题连贯性、主题独特性和主题多样性），同时减少了大多数主题建模算法的主题冗余。特别是，以通俗到正式语言重新表述的策略为Latent Dirichlet Allocation (LDA)算法带来的性能提升最大。", "conclusion": "该研究提出了一种模型无关的方法，提升了公共健康相关社交媒体中的主题建模效果，该方法有助于更好地理解健康危机及其他重要领域中的公众讨论。"}}
{"id": "2510.18909", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18909", "abs": "https://arxiv.org/abs/2510.18909", "authors": ["Hongyi He", "Xiao Liu", "Zhenghao Lin", "Mingni Tang", "Yi Cheng", "Jintao Wang", "Wenjie Li", "Peng Cheng", "Yeyun Gong"], "title": "Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection", "comment": null, "summary": "High-quality pre-training data is crutial for large language models, where\nquality captures factual reliability and semantic value, and diversity ensures\nbroad coverage and distributional heterogeneity. Existing approaches typically\nrely on single or multiple-dimensional score-based selection. However, directly\nselecting top-scored data often degrades performance, and sampling from a\nbroader range is required to recover results. The above non-monotonicity\nbetween dataset scores and downstream benchmark results reveals a fundamental\nbias: score-based methods collapse correlated dimensions, causing top-scored\ndata to appear high-quality while systematically overlooking diversity. We\nargue that ensuring diversity requires decomposing correlated metrics into\northogonal feature dimensions, from which the top-scored data can be directly\nselected. Therefore, we proposed the Orthogonal Diversity-Aware Selection\n(ODiS) algorithm, which preserves both quality and diversity during data\nselection. First, ODiS evaluates data from multiple dimensions, covering\nlanguage quality, knowledge quality, and comprehension difficulty. The\nmulti-dimensional scores are then decorrelated via Principal Component Analysis\n(PCA), yielding orthogonal evaluation dimensions. For each dimension, a\nRoberta-based scorer is trained to regress the data onto PCA-projected scores,\nenabling scalable inference on large corpora. Finally, ODiS constructs the\ntraining dataset by selecting top-scored data within each orthogonal dimension,\nthereby ensuring both quality and diversity. Empirical results show that\nODiS-selected data exhibit less than 2\\% inter-dimension overlap, confirming\northogonality between dimensions. More importantly, models trained with\nODiS-selected data significantly outperform other baselines on downstream\nbenchmarks, highlighting the necessity of orthogonal, diversity-aware data\nselection for LLMs.", "AI": {"tldr": "本文提出了ODiS算法，该算法通过将相关的评价指标分解为正交特征维度，确保在数据选择过程中既保持质量又增加多样性。实验结果显示，使用ODiS算法选择的数据在下游评估基准上显著优于其他基准。", "motivation": "现有的基于分数的数据选择方法会因为压缩相关的维度而导致高质量但不够多样化的数据集，这种方法存在根本的偏差。因此，我们需要一种新的方法来确保数据的质量和多样性。", "method": "ODiS算法首先从语言质量，知识质量和理解难度等多个维度评估数据，然后利用主成分分析（PCA）去除多维分数的相关性，最后在每个正交维度中选择高分数据，确保既保留质量又增加多样性。", "result": "实验结果展示了使用ODiS算法选择的数据在不同维度间的重叠率不足2%，证明了维度间的正交性。更重要的是，使用ODiS数据训练的模型在下游评估基准上表现更优。", "conclusion": "ODiS算法显示了正交、多样性意识的数据选择对大型语言模型的重要性。"}}
{"id": "2510.18914", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18914", "abs": "https://arxiv.org/abs/2510.18914", "authors": ["Afrozah Nadeem", "Mark Dras", "Usman Naseem"], "title": "Context-aware Fairness Evaluation and Mitigation in LLMs", "comment": "PrePrint", "summary": "Large language models often display undesirable behaviors embedded in their\ninternal representations, undermining fairness, inconsistency drift,\namplification of harmful content, and the propagation of unwanted patterns\nduring extended dialogue and conversations. Although training-time or\ndata-centric methods attempt to reduce these effects, they are computationally\nexpensive, irreversible once deployed, and slow to adapt to new conversational\ncontexts. Pruning-based methods provide a flexible and transparent way to\nreduce bias by adjusting the neurons responsible for certain behaviors.\nHowever, most existing approaches are static; once a neuron is removed, the\nmodel loses the ability to adapt when the conversation or context changes. To\naddress this, we propose a dynamic, reversible, pruning-based framework that\ndetects context-aware neuron activations and applies adaptive masking to\nmodulate their influence during generation. Our inference-time solution\nprovides fine-grained, memory-aware mitigation with knowledge-preserved, more\ncoherent behavior across multilingual single- and multi-turn dialogues,\nenabling dynamic fairness control in real-world conversational AI.", "AI": {"tldr": "本文提出了一种新的动态可逆剪枝框架，用于在对话式AI中减少大型语言模型的偏见和其他不良行为，提供了记忆感知和细粒度控制，同时保持了多语言对话中的连贯性和知识性。", "motivation": "大型语言模型往往在其内部表示中表现出不公平性、不一致性漂移、有害内容的放大以及在长时间对话中扩散不良模式的不当行为。虽然可以通过训练时间或以数据为中心的方法来减少这些效应，但它们成本高，一旦部署就不可逆，并且难以快速适应新的对话场景。", "method": "本文提出了一种动态可逆的剪枝框架，该框架能够检测到与上下文相关的神经元激活，并在生成过程中应用自适应掩码来调节它们的影响。这样的方法可以在推理阶段提供细粒度、记忆感知的缓解措施，同时保持知识，使得多语言单轮和多轮对话都能表现出更加连贯的行为。", "result": "该方法能够在多语言单次或多轮对话中提供具有知识保持和更连贯行为的细致缓解；动态公平性控制在实际的会话式AI中得到了实现。", "conclusion": "研究展示了一种新的剪枝方法，能够在多语言对话中实现细粒度的公平控制，该方法是动态可逆的，并且能够保持知识连贯性，这对于实际应用中的对话式AI具有巨大潜力。"}}
{"id": "2510.18915", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.18915", "abs": "https://arxiv.org/abs/2510.18915", "authors": ["Chen Chen", "ZeYang Hu", "Fengjiao Chen", "Liya Ma", "Jiaxing Liu", "Xiaoyu Li", "Xuezhi Cao"], "title": "MMAO-Bench: MultiModal All in One Benchmark Reveals Compositional Law between Uni-modal and Omni-modal in OmniModels", "comment": "10 pages, 8 figures. Work in progress", "summary": "Multimodal Large Languages models have been progressing from uni-modal\nunderstanding toward unifying visual, audio and language modalities,\ncollectively termed omni models. However, the correlation between uni-modal and\nomni-modal remains unclear, which requires comprehensive evaluation to drive\nomni model's intelligence evolution. In this work, we propose a novel, high\nquality and diversity omni model benchmark, MultiModal All in One Benchmark\n(MMAO-Bench), which effectively assesses both uni-modal and omni-modal\nunderstanding capabilities. The benchmark consists of 1880 human curated\nsamples, across 44 task types, and a innovative multi-step open-ended question\ntype that better assess complex reasoning tasks. Experimental result shows the\ncompositional law between cross-modal and uni-modal performance and the\nomni-modal capability manifests as a bottleneck effect on weak models, while\nexhibiting synergistic promotion on strong models.", "AI": {"tldr": "提出MultiModal All in One Benchmark (MMAO-Bench)来评估单模态和全模态理解能力，发现全模态能力在弱模型上表现为瓶颈，在强模型上表现出促进作用。", "motivation": "大型多模态语言模型正从单一模态理解向统一视觉、音频和语言模态转变。然而，单一模态和全模态之间的相关性尚不清楚，需要全面的评估来推动全模态模型的智能进化。", "method": "提出了一种新型的高质量多样性全模态基准测试MultiModal All in One Benchmark (MMAO-Bench),该基准测试有效评估了单模态和全模态理解能力。基准测试包含1880个人工策划的样本，跨越44种任务类型，以及一种创新的多步骤开放式问题类型，更好地评估复杂推理任务。", "result": "实验结果展示了跨模态和单模态性能之间的组合规律，并且全模态能力在弱模型上表现为瓶颈效应，而在强模型上则表现出协同促进效应。", "conclusion": "研究表明单模态和全模态之间的相关性，提出了一个评估全模态理解能力和瓶颈效应及协同促进效应的基准测试。"}}
{"id": "2510.18918", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18918", "abs": "https://arxiv.org/abs/2510.18918", "authors": ["Jainee Patel", "Chintan Bhatt", "Himani Trivedi", "Thanh Thi Nguyen"], "title": "Misinformation Detection using Large Language Models with Explainability", "comment": "Accepted for publication in the Proceedings of the 8th International\n  Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2025)", "summary": "The rapid spread of misinformation on online platforms undermines trust among\nindividuals and hinders informed decision making. This paper shows an\nexplainable and computationally efficient pipeline to detect misinformation\nusing transformer-based pretrained language models (PLMs). We optimize both\nRoBERTa and DistilBERT using a two-step strategy: first, we freeze the backbone\nand train only the classification head; then, we progressively unfreeze the\nbackbone layers while applying layer-wise learning rate decay. On two\nreal-world benchmark datasets, COVID Fake News and FakeNewsNet GossipCop, we\ntest the proposed approach with a unified protocol of preprocessing and\nstratified splits. To ensure transparency, we integrate the Local Interpretable\nModel-Agnostic Explanations (LIME) at the token level to present token-level\nrationales and SHapley Additive exPlanations (SHAP) at the global feature\nattribution level. It demonstrates that DistilBERT achieves accuracy comparable\nto RoBERTa while requiring significantly less computational resources. This\nwork makes two key contributions: (1) it quantitatively shows that a\nlightweight PLM can maintain task performance while substantially reducing\ncomputational cost, and (2) it presents an explainable pipeline that retrieves\nfaithful local and global justifications without compromising performance. The\nresults suggest that PLMs combined with principled fine-tuning and\ninterpretability can be an effective framework for scalable, trustworthy\nmisinformation detection.", "AI": {"tldr": "This paper presents an explainable and computationally efficient pipeline for misinformation detection using transformer-based PLMs, showing that lightweight models like DistilBERT can achieve strong performance while being more resource-efficient than heavier ones like RoBERTa.", "motivation": "Given the spread of online misinformation that undermines trust and affects informed decision-making, the paper aims to present a scalable, trustworthy, and computationally efficient approach to detecting misinformation using transformer-based language models.", "method": "The paper proposes an explainable and computationally efficient pipeline for detecting misinformation using transformer-based pretrained language models (PLMs). The models, RoBERTa and DistilBERT, are optimized using a two-step strategy with freezing, unfreezing layers and applying layer-wise learning rate decay. Interpretability is enhanced with LIME for token-level rationales and SHAP for global feature attributions.", "result": "Experiments on the COVID Fake News and FakeNewsNet GossipCop datasets demonstrate that DistilBERT achieves accuracy comparable to RoBERTa with significantly less computational demand. The pipeline effectively maintains performance while offering interpretability without performance compromise.", "conclusion": "The study concludes that lightweight PLMs can achieve strong task performance while being more computationally efficient. Additionally, it emphasizes the benefit of using explainable pipelines that offer local and global justifications, enhancing the framework's scalability and trustworthiness for misinformation detection."}}
{"id": "2510.18932", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18932", "abs": "https://arxiv.org/abs/2510.18932", "authors": ["Hiroshi Nonaka", "K. E. Perry"], "title": "Evaluating LLM Story Generation through Large-scale Network Analysis of Social Structures", "comment": "This paper has 14 pages and 8 figures. To be presented at the NeurIPS\n  2025 Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent\n  Abilities, and Scaling", "summary": "Evaluating the creative capabilities of large language models (LLMs) in\ncomplex tasks often requires human assessments that are difficult to scale. We\nintroduce a novel, scalable methodology for evaluating LLM story generation by\nanalyzing underlying social structures in narratives as signed character\nnetworks. To demonstrate its effectiveness, we conduct a large-scale\ncomparative analysis using networks from over 1,200 stories, generated by four\nleading LLMs (GPT-4o, GPT-4o mini, Gemini 1.5 Pro, and Gemini 1.5 Flash) and a\nhuman-written corpus. Our findings, based on network properties like density,\nclustering, and signed edge weights, show that LLM-generated stories\nconsistently exhibit a strong bias toward tightly-knit, positive relationships,\nwhich aligns with findings from prior research using human assessment. Our\nproposed approach provides a valuable tool for evaluating limitations and\ntendencies in the creative storytelling of current and future LLMs.", "AI": {"tldr": "本文提出了一种新的方法，通过分析故事中的社会结构网络来评估大语言模型的故事生成能力，这方法是可扩展的，并适用于当前和未来的大语言模型。", "motivation": "评估大语言模型在复杂任务中的创意能力通常需要难以扩展的人类评估，因此提出了这种方法。", "method": "提出了一种可扩展的方法，通过将故事中的社会结构分析为有向字符网络来评估大语言模型的故事生成能力。", "result": "通过对4个领先的大语言模型和人类编写的故事进行大规模比较分析，发现大语言模型生成的故事显示出对紧密、积极关系的高度偏向。", "conclusion": "所提方法为评估当前和未来大语言模型的创意故事写作能力提供了一个有价值的工具。"}}
{"id": "2510.18939", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18939", "abs": "https://arxiv.org/abs/2510.18939", "authors": ["Howard Yen", "Ashwin Paranjape", "Mengzhou Xia", "Thejas Venkatesh", "Jack Hessel", "Danqi Chen", "Yuhao Zhang"], "title": "Lost in the Maze: Overcoming Context Limitations in Long-Horizon Agentic Search", "comment": "Code and data are available here: https://github.com/howard-yen/SLIM", "summary": "Long-horizon agentic search requires iteratively exploring the web over long\ntrajectories and synthesizing information across many sources, and is the\nfoundation for enabling powerful applications like deep research systems. In\nthis work, we show that popular agentic search frameworks struggle to scale to\nlong trajectories primarily due to context limitations-they accumulate long,\nnoisy content, hit context window and tool budgets, or stop early. Then, we\nintroduce SLIM (Simple Lightweight Information Management), a simple framework\nthat separates retrieval into distinct search and browse tools, and\nperiodically summarizes the trajectory, keeping context concise while enabling\nlonger, more focused searches. On long-horizon tasks, SLIM achieves comparable\nperformance at substantially lower cost and with far fewer tool calls than\nstrong open-source baselines across multiple base models. Specifically, with o3\nas the base model, SLIM achieves 56% on BrowseComp and 31% on HLE,\noutperforming all open-source frameworks by 8 and 4 absolute points,\nrespectively, while incurring 4-6x fewer tool calls. Finally, we release an\nautomated fine-grained trajectory analysis pipeline and error taxonomy for\ncharacterizing long-horizon agentic search frameworks; SLIM exhibits fewer\nhallucinations than prior systems. We hope our analysis framework and simple\ntool design inform future long-horizon agents.", "AI": {"tldr": "研究开发了SLIM框架，该框架能使长时段的网络搜索更有效率，通过精确切割搜索任务并简化上下文来降低成本和工具调用次数，同时保证搜索质量。", "motivation": "长时段代理搜索要求在长时间的过程中迭代探索网络并综合多个来源的信息，而流行代理搜索框架因上下文限制难以扩展。", "method": "引入SLIM（Simple Lightweight Information Management），一个简单的框架，将检索分为独立的搜索和浏览工具，并定期总结轨迹，保持上下文简洁，同时支持更长、更集中的搜索。", "result": "SLIM在长时段任务上实现了与强开源基线相当的性能，但成本大幅降低且工具调用次数减少了4-6倍。具体来说，使用o3作为基础模型，SLIM在BrowseComp上达到56%，在HLE上达到31%，分别超过所有开源框架8和4个百分点。", "conclusion": "SLIM展现出了较少的幻觉并希望能对未来长时段代理的分析框架和工具设计提供启示。"}}
{"id": "2510.18941", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18941", "abs": "https://arxiv.org/abs/2510.18941", "authors": ["Zhilin Wang", "Jaehun Jung", "Ximing Lu", "Shizhe Diao", "Ellie Evans", "Jiaqi Zeng", "Pavlo Molchanov", "Yejin Choi", "Jan Kautz", "Yi Dong"], "title": "ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge", "comment": "23 pages", "summary": "Evaluating progress in large language models (LLMs) is often constrained by\nthe challenge of verifying responses, limiting assessments to tasks like\nmathematics, programming, and short-form question-answering. However, many\nreal-world applications require evaluating LLMs in processing professional\ndocuments, synthesizing information, and generating comprehensive reports in\nresponse to user queries. We introduce ProfBench: a set of over 7000\nresponse-criterion pairs as evaluated by human-experts with professional\nknowledge across Physics PhD, Chemistry PhD, Finance MBA and Consulting MBA. We\nbuild robust and affordable LLM-Judges to evaluate ProfBench rubrics, by\nmitigating self-enhancement bias and reducing the cost of evaluation by 2-3\norders of magnitude, to make it fair and accessible to the broader community.\nOur findings reveal that ProfBench poses significant challenges even for\nstate-of-the-art LLMs, with top-performing models like GPT-5-high achieving\nonly 65.9\\% overall performance. Furthermore, we identify notable performance\ndisparities between proprietary and open-weight models and provide insights\ninto the role that extended thinking plays in addressing complex,\nprofessional-domain tasks. Data:\nhttps://huggingface.co/datasets/nvidia/ProfBench and Code:\nhttps://github.com/NVlabs/ProfBench", "AI": {"tldr": "介绍了ProfBench，这是一个评估专业领域大规模语言模型性能的数据集。该基准由超过7000个由专业领域专家评估的响应-标准对组成，同时通过LLM-Judges降低了评估的成本。即使是最先进的语言模型，在此基准测试中的表现也不尽如人意。", "motivation": "当前，大型语言模型（LLMs）的进展评估受限于验证响应的挑战，这限制了其应用范围。许多现实世界的应用场景需要评估LLMs在处理专业文档和生成报告方面的能力。为了改进这一现状，同时使评估更公平和更易获取，我们提出了ProfBench。", "method": "我们提出了ProfBench，这个基准包含超过7000个由具有专业背景的专家评估的响应-标准对。我们还开发了LLM-Judges来通过减轻自我增强偏差和将评估成本降低两级或三级，来公平且经济地评估这些标准。", "result": "测试发现即使是最先进的GPT-5-high模型，整体表现也只有65.9%。此外，我们还发现了专有模型和开源模型之间的性能差异，并提供了关于扩展思维在处理复杂专业任务中的作用的见解。", "conclusion": "ProfBench揭示了大型语言模型在处理专业文档和生成报告方面面临的挑战，并提供了一个公开的基准，使研究社区能够更好地评估这些模型的能力。"}}
{"id": "2510.18935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.18935", "abs": "https://arxiv.org/abs/2510.18935", "authors": ["Nathan Mankovich", "Kai-Hendrik Cohrs", "Homer Durand", "Vasileios Sitokonstantinou", "Tristan Williams", "Gustau Camps-Valls"], "title": "Dimensionality Reduction for Remote Sensing Data Analysis: A Systematic Review of Methods and Applications", "comment": null, "summary": "Earth observation involves collecting, analyzing, and processing an\never-growing mass of data. Automatically harvesting information is crucial for\naddressing significant societal, economic, and environmental challenges,\nranging from environmental monitoring to urban planning and disaster\nmanagement. However, the high dimensionality of these data poses challenges in\nterms of sparsity, inefficiency, and the curse of dimensionality, which limits\nthe effectiveness of machine learning models. Dimensionality reduction (DR)\ntechniques, specifically feature extraction, address these challenges by\npreserving essential data properties while reducing complexity and enhancing\ntasks such as data compression, cleaning, fusion, visualization, anomaly\ndetection, and prediction. This review provides a handbook for leveraging DR\nacross the RS data value chain and identifies opportunities for under-explored\nDR algorithms and their application in future research.", "AI": {"tldr": "文章综述了降维技术在遥感数据分析中的应用，以提高机器学习模型的有效性，并提出未来研究方向。", "motivation": "面对遥感数据量的快速增长，自动化信息采集对解决社会、经济和环境挑战至关重要。但高维数据带来的问题限制了机器学习模型的有效性，因此需要通过特征提取来简化数据复杂度。", "method": "本文通过综述数据分析中的降维技术，特别是特征提取方法，来解决高维遥感数据带来的稀缺性、效率低下和维度灾难等问题。", "result": "综述指出降维技术在数据压缩、清理、融合、可视化、异常检测和预测等方面具有应用潜力，并指出了未充分探索的降维算法的研究机会。", "conclusion": "文章强调了降维技术在遥感数据价值链条中的重要性，并对未来的降维算法研究和应用提出了指导建议。"}}
{"id": "2510.19005", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19005", "abs": "https://arxiv.org/abs/2510.19005", "authors": ["Sophia Xiao Pu", "Sitao Cheng", "Xin Eric Wang", "William Yang Wang"], "title": "Dynamic Evaluation for Oversensitivity in LLMs", "comment": "EMNLP-Findings 2025", "summary": "Oversensitivity occurs when language models defensively reject prompts that\nare actually benign. This behavior not only disrupts user interactions but also\nobscures the boundary between harmful and harmless content. Existing benchmarks\nrely on static datasets that degrade overtime as models evolve, leading to data\ncontamination and diminished evaluative power. To address this, we develop a\nframework that dynamically generates model-specific challenging datasets,\ncapturing emerging defensive patterns and aligning with each model's unique\nbehavior. Building on this approach, we construct OVERBENCH, a benchmark that\naggregates these datasets across diverse LLM families, encompassing 450,000\nsamples from 25 models. OVERBENCH provides a dynamic and evolving perspective\non oversensitivity, allowing for continuous monitoring of defensive triggers as\nmodels advance, highlighting vulnerabilities that static datasets overlook.", "AI": {"tldr": "引入OVERBENCH动态基准测试，解决静态基准数据集随时间贬值的问题，帮助识别和监控语言模型的过度敏感行为。", "motivation": "现有的基准依赖于静态数据集，随着时间的推移，模型的进化会导致数据污染和评估能力下降。", "method": "开发了一个动态生成模型特定挑战数据集的框架，捕捉新兴的防御模式并适应每个模型的独特行为。构建了OVERBENCH基准，综合了跨越不同LLM家族的数据集，包括来自25个模型的450,000个样本。", "result": "OVERBENCH提供了一个动态和不断演变的视角来看待过度敏感性，可以连续监控防御触发因素，揭露静态数据集忽视的漏洞。", "conclusion": "通过使用动态基准测试方法，可以更准确地识别语言模型过度敏感的情况，提高模型的行为理解和安全防御能力。"}}
{"id": "2510.18976", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18976", "abs": "https://arxiv.org/abs/2510.18976", "authors": ["Yuichiro Takeuchi", "Yusuke Imoto", "Shunya Kato"], "title": "Ninja Codes: Neurally Generated Fiducial Markers for Stealthy 6-DoF Tracking", "comment": "11 pages, 12 figures", "summary": "In this paper we describe Ninja Codes, neurally-generated fiducial markers\nthat can be made to naturally blend into various real-world environments. An\nencoder network converts arbitrary images into Ninja Codes by applying visually\nmodest alterations; the resulting codes, printed and pasted onto surfaces, can\nprovide stealthy 6-DoF location tracking for a wide range of applications\nincluding augmented reality, robotics, motion-based user interfaces, etc. Ninja\nCodes can be printed using off-the-shelf color printers on regular printing\npaper, and can be detected using any device equipped with a modern RGB camera\nand capable of running inference. Using an end-to-end process inspired by prior\nwork on deep steganography, we jointly train a series of network modules that\nperform the creation and detection of Ninja Codes. Through experiments, we\ndemonstrate Ninja Codes' ability to provide reliable location tracking under\ncommon indoor lighting conditions, while successfully concealing themselves\nwithin diverse environmental textures. We expect Ninja Codes to offer\nparticular value in scenarios where the conspicuous appearances of conventional\nfiducial markers make them undesirable for aesthetic and other reasons.", "AI": {"tldr": "本文提出了一种可以自然融入环境，并能进行隐蔽位置追踪的Ninja Codes，这些标记能够通过普通打印机打印并在常见室内条件下提供准确的位置追踪。", "motivation": "解决传统标记在外观显眼性方面的问题，以及在包括增强现实、机器人技术等各种应用中的使用限制。", "method": "开发并训练了用于Ninja Codes创建和检测的一系列网络模块，借鉴了深度隐形技术的端到端过程。", "result": "该论文介绍了Ninja Codes，这是一种神经生成的标记，可以自然融入各种真实环境。编码器网络可以将任意图像转化为Ninja Codes，只需进行适度的视觉修改；这些代码，当被打印并粘贴到表面上时，可以为包括增强现实、机器人技术、基于动作的用户界面等广泛的应用提供隐蔽的六自由度定位追踪功能。Ninja Codes可以使用普通的彩色打印机打印在普通的打印纸上，并且通过任何配备现代RGB相机并可进行推理的设备即可检测。研究者们通过借鉴深度隐形技术的端到端过程，联合训练了一系列网络模块以实现Ninja Codes的创建和检测。实验表明，Ninja Codes在常见室内照明条件下能够提供可靠的定位追踪功能，同时成功隐藏在各种环境纹理中。研究者们认为，Ninja Codes特别适用于传统标记因外观显眼而不可取的情况，特别是在审美和其他方面。", "conclusion": "Ninja Codes能够提供可靠的定位追踪服务，同时隐藏于环境内，可用于增强现实及机器人技术等场景。"}}
{"id": "2510.19028", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19028", "abs": "https://arxiv.org/abs/2510.19028", "authors": ["Eunsu Kim", "Junyeong Park", "Juhyun Oh", "Kiwoong Park", "Seyoung Song", "A. Seza Dogruoz", "Najoung Kim", "Alice Oh"], "title": "Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and Korean Dialogues", "comment": null, "summary": "As large language models (LLMs) are increasingly used in human-AI\ninteractions, their social reasoning capabilities in interpersonal contexts are\ncritical. We introduce SCRIPTS, a 1k-dialogue dataset in English and Korean,\nsourced from movie scripts. The task involves evaluating models' social\nreasoning capability to infer the interpersonal relationships (e.g., friends,\nsisters, lovers) between speakers in each dialogue. Each dialogue is annotated\nwith probabilistic relational labels (Highly Likely, Less Likely, Unlikely) by\nnative (or equivalent) Korean and English speakers from Korea and the U.S.\nEvaluating nine models on our task, current proprietary LLMs achieve around\n75-80% on the English dataset, whereas their performance on Korean drops to\n58-69%. More strikingly, models select Unlikely relationships in 10-25% of\ntheir responses. Furthermore, we find that thinking models and chain-of-thought\nprompting, effective for general reasoning, provide minimal benefits for social\nreasoning and occasionally amplify social biases. Our findings reveal\nsignificant limitations in current LLMs' social reasoning capabilities,\nhighlighting the need for efforts to develop socially-aware language models.", "AI": {"tldr": "大型语言模型在英语人际对话关系推理准确率为75-80%，但在韩语中的表现仅为58-69%，且在10-25%的回答中偏向不太可能的关系。进一步显示了这些模型在社会推理能力上的局限性，强调了开发社会意识语言模型的重要性。", "motivation": "由于大型语言模型被越来越多地用于人类和AI的交互，其在人际语境中推理对话说话者关系的社会推理能力变得尤为重要。", "method": "介绍了一种名为SCRIPTS的1000个对话数据集，该数据集源自电影剧本，包含英韩两种语言。任务是评估模型在推理对话中说话者之间人际关系方面（例如，朋友、姐妹、爱人）的社会推理能力。每个对话由韩国和美国的母语（或等价）韩英说话者标注了概率关系标签（高度可能、不太可能、不可能）。", "result": "在我们的任务评估中，当前专有的大型语言模型在英语数据集中实现约75-80%的准确率，而在韩语中的性能下降到58-69%。此外，模型在10-25%的回答中选择不太可能的关系。研究还发现，思考模型和链式思考提示在这种社会推理能力上几乎没有裨益，并有时放大社会偏见。", "conclusion": "研究揭示了当前大型语言模型在社会推理能力方面存在显著局限性，强调了开发具有社会意识的语言模型的需求。"}}
{"id": "2510.19001", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19001", "abs": "https://arxiv.org/abs/2510.19001", "authors": ["Seungjun Yu", "Junsung Park", "Youngsun Lim", "Hyunjung Shim"], "title": "Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts", "comment": null, "summary": "We present a two-phase vision-language QA system for autonomous driving that\nanswers high-level perception, prediction, and planning questions. In Phase-1,\na large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a\nshort temporal window of history, and a chain-of-thought prompt with few-shot\nexemplars. A self-consistency ensemble (multiple sampled reasoning chains)\nfurther improves answer reliability. In Phase-2, we augment the prompt with\nnuScenes scene metadata (object annotations, ego-vehicle state, etc.) and\ncategory-specific question instructions (separate prompts for perception,\nprediction, planning tasks). In experiments on a driving QA benchmark, our\napproach significantly outperforms the baseline Qwen2.5 models. For example,\nusing 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall\naccuracy (vs.62.61% with zero-shot); applying self-consistency raises this to\n66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96%\naccuracy under severe visual corruption. These results demonstrate that\ncarefully engineered prompts and contextual grounding can greatly enhance\nhigh-level driving QA with pretrained vision-language models.", "AI": {"tldr": "研究了一种新型的两阶段视觉-语言自动驾驶问答系统，结合多模态大语言模型和精心设计的提示词。实验显示，该方法在问答基准测试中表现优越，即使在视觉损伤情况下也能维持高水平的准确性。", "motivation": "这项研究的动机在于通过精心设计的提示词和上下文定位来显著提升高层次驾驶问答的能力，尤其是在结合预训练的视觉-语言模型时。", "method": "我们提出了一种针对自动驾驶的两阶段视觉-语言问答系统，能够回答高层次的感知、预测和规划问题。第一阶段，我们使用一个大型多模态大语言模型（Qwen2.5-VL-32B），结合六摄像头输入、一段简短的时间历史窗口和少量样本文本链的提示词组合来条件化模型。为了提高答案的可靠性，我们还采用了自一致性集成（多个采样推理链）。在第二阶段，我们通过nuScenes场景元数据（对象注释、自我车辆状态等）和针对不同类别的问题说明（为感知、预测和规划任务设置单独的提示词）来增强提示词。", "result": "在驾驶问答基准测试中，我们的方法显著优于基线的Qwen2.5模型。例如，在第一阶段使用5帧历史和10个少量样本的提示下，整体准确率为65.1%（零样本时为62.61%）；应用自一致性提升了这一数值到66.85%。第二阶段达到了67.37%的整体准确率。值得注意的是，该系统在严重视觉损坏情况下仍能保持96%的准确率。", "conclusion": "实验结果表明，针对自动驾驶的问答系统可以显著提高高阶任务的准确度，同时也证明了在视觉-语言模型中使用上下文定位和精细化提示词设计的策略的有效性。"}}
{"id": "2510.19030", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19030", "abs": "https://arxiv.org/abs/2510.19030", "authors": ["Zackary Rackauckas", "Nobuaki Minematsu", "Julia Hirschberg"], "title": "Re:Member: Emotional Question Generation from Personal Memories", "comment": "Accepted to HCI+NLP at ACL 2025", "summary": "We present Re:Member, a system that explores how emotionally expressive,\nmemory-grounded interaction can support more engaging second language (L2)\nlearning. By drawing on users' personal videos and generating stylized spoken\nquestions in the target language, Re:Member is designed to encourage affective\nrecall and conversational engagement. The system aligns emotional tone with\nvisual context, using expressive speech styles such as whispers or late-night\ntones to evoke specific moods. It combines WhisperX-based transcript alignment,\n3-frame visual sampling, and Style-BERT-VITS2 for emotional synthesis within a\nmodular generation pipeline. Designed as a stylized interaction probe,\nRe:Member highlights the role of affect and personal media in learner-centered\neducational technologies.", "AI": {"tldr": "Re:Member系统通过利用用户个人视频和生成目标语言中的风格化语音问题，鼓励情感回忆和交流互动，旨在提升第二语言学习的吸引力。", "motivation": "动机在于探索情感表达和基于个人记忆的交互如何支持更吸引人的第二语言学习方法。", "method": "Re:Member系统结合WhisperX的转录对齐、三帧视觉采样和Style-BERT-VITS2的情感合成，构建了一个模块化的生成管道，以支持具有情感表达的、基于记忆的人际交互，促进第二语言学习。", "result": "系统通过拟声说话风格，如低语或深夜语气来唤起特定情绪，并结合用户的个人视频，鼓励情感回忆和对话参与，从而设计出更吸引人的语言学习体验。", "conclusion": "Re:Member展示了情感和个人媒体在以学习者为中心的教育技术中的作用，强调了它在教育技术设计中的重要性。"}}
{"id": "2510.19003", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19003", "abs": "https://arxiv.org/abs/2510.19003", "authors": ["Zhengbo Zhou", "Dooman Arefan", "Margarita Zuley", "Shandong Wu"], "title": "$Δ$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction", "comment": null, "summary": "Longitudinal analysis of sequential radiological images is hampered by a\nfundamental data challenge: how to effectively model a sequence of\nhigh-resolution images captured at irregular time intervals. This data\nstructure contains indispensable spatial and temporal cues that current methods\nfail to fully exploit. Models often compromise by either collapsing spatial\ninformation into vectors or applying spatio-temporal models that are\ncomputationally inefficient and incompatible with non-uniform time steps. We\naddress this challenge with Time-Aware $\\Delta$t-Mamba3D, a novel state-space\narchitecture adapted for longitudinal medical imaging. Our model simultaneously\nencodes irregular inter-visit intervals and rich spatio-temporal context while\nremaining computationally efficient. Its core innovation is a continuous-time\nselective scanning mechanism that explicitly integrates the true time\ndifference between exams into its state transitions. This is complemented by a\nmulti-scale 3D neighborhood fusion module that robustly captures\nspatio-temporal relationships. In a comprehensive breast cancer risk prediction\nbenchmark using sequential screening mammogram exams, our model shows superior\nperformance, improving the validation c-index by 2-5 percentage points and\nachieving higher 1-5 year AUC scores compared to established variants of\nrecurrent, transformer, and state-space models. Thanks to its linear\ncomplexity, the model can efficiently process long and complex patient\nscreening histories of mammograms, forming a new framework for longitudinal\nimage analysis.", "AI": {"tldr": "论文提出了Time-Aware $\\Delta$t-Mamba3D，一个专为纵向医学影像分析设计的新状态空间框架。该模型能够在有效捕捉时空信息的同时支持计算效率，提升了乳腺癌风险预测的性能。", "motivation": "纵向分析序列放射影像受制于一个基本的数据挑战：如何有效建模在不规则时间间隔内捕获的高分辨率影像序列。当前的方法常常通过将空间信息简化为向量或将时空模型应用于计算效率不佳且无法处理非均匀时间步骤的解决方案，来妥协处理这一挑战。", "method": "提出了一种名为Time-Aware $\\Delta$t-Mamba3D的新状态空间架构，该架构特别针对纵向医学影像分析进行适应。其创新之处在于一种连续时间的选择性扫描机制，该机制明确将相邻检查之间的真实时间差纳入状态转换过程。此外，还结合了多尺度3D邻域融合模块，以稳健地捕捉时空关系。", "result": "在基于顺序筛查乳房X光片的乳腺癌风险预测基准测试中，该模型表现出更优的性能，验证c指标提高了2-5个百分点，并在1-5年的AUC得分上超过了现有多重、转换器和状态空间模型的现有变体。", "conclusion": "Time-Aware $\\Delta$t-Mamba3D模型通过线性复杂性实现了对长时间和复杂患者筛查历史的有效处理，为纵向影像分析奠定了新的框架。"}}
{"id": "2510.19032", "categories": ["cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19032", "abs": "https://arxiv.org/abs/2510.19032", "authors": ["Abeer Badawi", "Elahe Rahimi", "Md Tahmid Rahman Laskar", "Sheri Grach", "Lindsay Bertrand", "Lames Danok", "Jimmy Huang", "Frank Rudzicz", "Elham Dolatabadi"], "title": "When Can We Trust LLMs in Mental Health? Large-Scale Benchmarks for Reliable LLM Evaluation", "comment": null, "summary": "Evaluating Large Language Models (LLMs) for mental health support is\nchallenging due to the emotionally and cognitively complex nature of\ntherapeutic dialogue. Existing benchmarks are limited in scale, reliability,\noften relying on synthetic or social media data, and lack frameworks to assess\nwhen automated judges can be trusted. To address the need for large-scale\ndialogue datasets and judge reliability assessment, we introduce two benchmarks\nthat provide a framework for generation and evaluation. MentalBench-100k\nconsolidates 10,000 one-turn conversations from three real scenarios datasets,\neach paired with nine LLM-generated responses, yielding 100,000 response pairs.\nMentalAlign-70k}reframes evaluation by comparing four high-performing LLM\njudges with human experts across 70,000 ratings on seven attributes, grouped\ninto Cognitive Support Score (CSS) and Affective Resonance Score (ARS). We then\nemploy the Affective Cognitive Agreement Framework, a statistical methodology\nusing intraclass correlation coefficients (ICC) with confidence intervals to\nquantify agreement, consistency, and bias between LLM judges and human experts.\nOur analysis reveals systematic inflation by LLM judges, strong reliability for\ncognitive attributes such as guidance and informativeness, reduced precision\nfor empathy, and some unreliability in safety and relevance. Our contributions\nestablish new methodological and empirical foundations for reliable,\nlarge-scale evaluation of LLMs in mental health. We release the benchmarks and\ncodes at: https://github.com/abeerbadawi/MentalBench/", "AI": {"tldr": "通过引入MentalBench-100k 和 MentalAlign-70k 两个基准，本研究提供了评估大型语言模型在心理健康支持中的生成与评估框架，并揭示了自动评分者在评价情感共鸣时存在的系统偏差。", "motivation": "为了应对当前心理健康支持对话中测试数据集规模有限、可信度低以及缺乏评估自动评分者可靠性的框架问题。", "method": "提出了MentalBench-100k 与 MentalAlign-70k 两个基准，分别用于生成和评估大型语言模型产生的对话回复，并使用情感认知一致性框架来量化LLM评分者与人类专家之间的一致性、可靠性及偏差。", "result": "研究发现，自动评分者在评价时存在系统性高估现象，对于认知支持属性的评分可靠性高，但是对于情感属性如共情的评分准确性较低，安全性和相关性评价方面存在部分不可靠。", "conclusion": "本研究为在心理健康领域中进行大型语言模型的可靠、大规模评估奠定了新的方法论和实证基础。"}}
{"id": "2510.19022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19022", "abs": "https://arxiv.org/abs/2510.19022", "authors": ["Aritra Bhowmik", "Denis Korzhenkov", "Cees G. M. Snoek", "Amirhossein Habibian", "Mohsen Ghafoorian"], "title": "MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models", "comment": null, "summary": "Text-to-video diffusion models have enabled high-quality video synthesis, yet\noften fail to generate temporally coherent and physically plausible motion. A\nkey reason is the models' insufficient understanding of complex motions that\nnatural videos often entail. Recent works tackle this problem by aligning\ndiffusion model features with those from pretrained video encoders. However,\nthese encoders mix video appearance and dynamics into entangled features,\nlimiting the benefit of such alignment. In this paper, we propose a\nmotion-centric alignment framework that learns a disentangled motion subspace\nfrom a pretrained video encoder. This subspace is optimized to predict\nground-truth optical flow, ensuring it captures true motion dynamics. We then\nalign the latent features of a text-to-video diffusion model to this new\nsubspace, enabling the generative model to internalize motion knowledge and\ngenerate more plausible videos. Our method improves the physical commonsense in\na state-of-the-art video diffusion model, while preserving adherence to textual\nprompts, as evidenced by empirical evaluations on VideoPhy, VideoPhy2, VBench,\nand VBench-2.0, along with a user study.", "AI": {"tldr": "本文提出了一种新的框架，通过学习并优化一个动作子空间，改善了文本到视频扩散模型的动画合理性和时间一致性。实验表明，这种方法提高了视频的物理常识表现。", "motivation": "现有的文本到视频扩散模型在生成时间一致且物理上合理的动作方面存在不足，原因是这些模型对自然视频中往往包含的复杂动作理解不够。本研究希望能通过优化动作信息的处理来提高生成视频的质量。", "method": "本研究提出了一种以动作为中心的对齐框架，该框架从预训练的视频编码器中学习一个解缠的动作子空间。这个子空间针对真实光学流进行了优化，以捕捉真正的动作动态。然后，将文本到视频扩散模型的潜在特征与这个新的子空间对齐，使生成模型能够内化动作知识，生成更加合理的视频。", "result": "实验证明，该方法提高了当前最先进的视频扩散模型的物理常识，同时保持了对文本描述的依从性。这在VideoPhy, VideoPhy2, VBench和VBench-2.0等数据集，以及用户研究中得到了证实。", "conclusion": "通过学习一个优化的动作子空间并与文本到视频扩散模型的潜在特征进行对齐，本研究显著提高了生成视频的物理合理性和时间一致性，同时维持了对文本描述的忠实度。"}}
{"id": "2510.19036", "categories": ["cs.CL", "I.2"], "pdf": "https://arxiv.org/pdf/2510.19036", "abs": "https://arxiv.org/abs/2510.19036", "authors": ["Suswitha Pericharla", "Daniel B. Hier", "Tayo Obafemi-Ajayi"], "title": "From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization", "comment": "Submitted for publication to BMC BioData Mining", "summary": "Effective biomedical data integration depends on automated term\nnormalization, the mapping of natural language biomedical terms to standardized\nidentifiers. This linking of terms to identifiers is essential for semantic\ninteroperability. Large language models (LLMs) show promise for this task but\nperform unevenly across terminologies. We evaluated both memorization\n(training-term performance) and generalization (validation-term performance)\nacross multiple biomedical ontologies. Fine-tuning Llama 3.1 8B revealed marked\ndifferences by terminology. GO mappings showed strong memorization gains (up to\n77% improvement in term-to-identifier accuracy), whereas HPO showed minimal\nimprovement. Generalization occurred only for protein-gene (GENE) mappings\n(13.9% gain), while fine-tuning for HPO and GO yielded negligible transfer.\nBaseline accuracy varied by model scale, with GPT-4o outperforming both Llama\nvariants for all terminologies. Embedding analyses showed tight semantic\nalignment between gene symbols and protein names but weak alignment between\nterms and identifiers for GO or HPO, consistent with limited lexicalization.\nFine-tuning success depended on two interacting factors: identifier popularity\nand lexicalization. Popular identifiers were more likely encountered during\npretraining, enhancing memorization. Lexicalized identifiers, such as gene\nsymbols, enabled semantic generalization. By contrast, arbitrary identifiers in\nGO and HPO constrained models to rote learning. These findings provide a\npredictive framework for when fine-tuning enhances factual recall versus when\nit fails due to sparse or non-lexicalized identifiers.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.19060", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19060", "abs": "https://arxiv.org/abs/2510.19060", "authors": ["Amith Ananthram", "Elias Stengel-Eskin", "Lorena A. Bradford", "Julia Demarest", "Adam Purvis", "Keith Krut", "Robert Stein", "Rina Elster Pantalony", "Mohit Bansal", "Kathleen McKeown"], "title": "PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions", "comment": "24 pages, 9 figures. Metric/benchmark available at\n  https://github.com/amith-ananthram/posh", "summary": "While vision-language models (VLMs) have advanced into detailed image\ndescription, evaluation remains a challenge. Standard metrics (e.g. CIDEr,\nSPICE) were designed for short texts and tuned to recognize errors that are now\nuncommon, such as object misidentification. In contrast, long texts require\nsensitivity to attribute and relation attachments and scores that localize\nerrors to particular text spans. In this work, we introduce PoSh, a metric for\ndetailed image description that uses scene graphs as structured rubrics to\nguide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained\nerrors (e.g. mistakes in compositional understanding). PoSh is replicable,\ninterpretable and a better proxy for human raters than existing metrics\n(including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new\ndataset, DOCENT. This novel benchmark contains artwork, paired with\nexpert-written references, and model-generated descriptions, augmented with\ngranular and coarse judgments of their quality from art history students. Thus,\nDOCENT enables evaluating both detailed image description metrics and detailed\nimage description itself in a challenging new domain. We show that PoSh\nachieves stronger correlations (+0.05 Spearman $\\rho$) with the human judgments\nin DOCENT than the best open-weight alternatives, is robust to image type\n(using CapArena, an existing dataset of web imagery) and is a capable reward\nfunction, outperforming standard supervised fine-tuning. Then, using PoSh, we\ncharacterize the performance of open and closed models in describing the\npaintings, sketches and statues in DOCENT and find that foundation models\nstruggle to achieve full, error-free coverage of images with rich scene\ndynamics, establishing a demanding new task to gauge VLM progress. Through both\nPoSh and DOCENT, we hope to enable advances in important areas such as\nassistive text generation.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.19116", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19116", "abs": "https://arxiv.org/abs/2510.19116", "authors": ["Jaesung Bae", "Cameron Churchwell", "Mitchell Hermon", "Tsun-An Hsieh", "Jocelyn Xu", "Yekaterina Yegorova", "Mark Hasegawa-Johnson", "Heng Ji"], "title": "That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation", "comment": null, "summary": "This paper investigates how large language models (LLMs) behave when faced\nwith discrepancies between their parametric knowledge and conflicting\ninformation contained in a prompt. Building on prior question-answering (QA)\nresearch, we extend the investigation of knowledge conflicts to the realm of\ncode generation. We propose a domain-agnostic framework for constructing and\ninterpreting such conflicts, along with a novel evaluation method and dataset\ntailored to code conflict scenarios. Our experiments indicate that sufficiently\nlarge LLMs encode the notion of a knowledge conflict in their parameters,\nenabling us to detect knowledge conflicts with up to \\textbf{80.65\\%} accuracy.\nBuilding on these insights, we show that activation-level steering can achieve\nup to a \\textbf{12.6\\%} improvement in steering success over a random baseline.\nHowever, effectiveness depends critically on balancing model size, task domain,\nand steering direction. The experiment code and data will be made publicly\navailable after acceptance.", "AI": {"tldr": "本文研究了大语言模型在面对提示信息与其知识库之间冲突时的表现，并提出了一种适用于代码生成冲突场景的新框架、评估方法和数据集。", "motivation": "基于先前关于知识冲突的研究，我们希望将这种研究扩展到代码生成领域，特别是探索大规模语言模型(LLMs)在提示中存在与其参数化知识冲突的信息时的行为。", "method": "我们提出了一种领域无关的框架来构建和解释知识冲突，并为代码冲突场景设计了一种新的评估方法和数据集。", "result": "实验结果表明，足够大的LLMs可以在其参数中编码knowledge conflict的概念，使我们能够以高达80.65%的准确率检测到knowledge conflict。我们还展示了通过激活级指导可以实现比随机基准高出12.6%的成功率。", "conclusion": "研究结果揭示了在处理knowledge conflict时，平衡模型大小、任务领域和指导方向的重要性。"}}
{"id": "2510.19078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19078", "abs": "https://arxiv.org/abs/2510.19078", "authors": ["Zhongyu Jiang", "Wenhao Chai", "Lei Li", "Zhuoran Zhou", "Cheng-Yen Yang", "Jenq-Neng Hwang"], "title": "UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning", "comment": null, "summary": "In recent years, there has been a growing interest in developing effective\nalignment pipelines to generate unified representations from different\nmodalities for multi-modal fusion and generation. As an important component of\nHuman-Centric applications, Human Pose representations are critical in many\ndownstream tasks, such as Human Pose Estimation, Action Recognition,\nHuman-Computer Interaction, Object tracking, etc. Human Pose representations or\nembeddings can be extracted from images, 2D keypoints, 3D skeletons, mesh\nmodels, and lots of other modalities. Yet, there are limited instances where\nthe correlation among all of those representations has been clearly researched\nusing a contrastive paradigm. In this paper, we propose UniHPR, a unified Human\nPose Representation learning pipeline, which aligns Human Pose embeddings from\nimages, 2D and 3D human poses. To align more than two data representations at\nthe same time, we propose a novel singular value-based contrastive learning\nloss, which better aligns different modalities and further boosts performance.\nTo evaluate the effectiveness of the aligned representation, we choose 2D and\n3D Human Pose Estimation (HPE) as our evaluation tasks. In our evaluation, with\na simple 3D human pose decoder, UniHPR achieves remarkable performance metrics:\nMPJPE 49.9mm on the Human3.6M dataset and PA-MPJPE 51.6mm on the 3DPW dataset\nwith cross-domain evaluation. Meanwhile, we are able to achieve 2D and 3D pose\nretrieval with our unified human pose representations in Human3.6M dataset,\nwhere the retrieval error is 9.24mm in MPJPE.", "AI": {"tldr": "我们提出了UniHPR，一个用于统一人体姿态表示学习的管道，该管道使用一种基于奇异值的对比学习损失来实现不同模态之间的对齐，从而在多项评估任务中表现出优越的性能。", "motivation": "尽管人体姿态表示在许多下游任务中至关重要，但对于这些表示之间的相关性的研究却是有限的。通过提出UniHPR，我们的目的是填补这一研究空白，特别是在多模态融合和生成方面。", "method": "我们提出了UniHPR，一个统一的人体姿态表示学习管道，它能够将从图像、2D和3D人体姿态中提取的人体姿态嵌入对齐。为了同时对齐更多的数据表示，我们提出了一种基于奇异值的对比学习损失，它能够更好地对齐不同的模态并进一步提高性能。", "result": "我们在2D和3D人体姿态估计（HPE）任务中评估了该方法的有效性。实验结果显示，使用简单的3D人体姿态解码器，UniHPR在Human3.6M数据集上实现了49.9mm的MPJPE和在3DPW数据集上实现了51.6mm的PA-MPJPE。此外，在Human3.6M数据集中，我们使用统一的人体姿态表示实现了2D和3D姿态检索，其检索误差为9.24mm的MPJPE。", "conclusion": "研究结果表明，UniHPR在统一不同模态的人体姿态表示方面是有效的，并且可以应用于2D和3D人体姿态估计以及姿态检索任务中，显示出其在实际应用场景中的潜力。"}}
{"id": "2510.19117", "categories": ["cs.CL", "cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.19117", "abs": "https://arxiv.org/abs/2510.19117", "authors": ["Valentin Noël"], "title": "A Graph Signal Processing Framework for Hallucination Detection in Large Language Models", "comment": "Preprint under review (2025). 11 pages, 7 figures. Code and scripts:\n  to be released", "summary": "Large language models achieve impressive results but distinguishing factual\nreasoning from hallucinations remains challenging. We propose a spectral\nanalysis framework that models transformer layers as dynamic graphs induced by\nattention, with token embeddings as signals on these graphs. Through graph\nsignal processing, we define diagnostics including Dirichlet energy, spectral\nentropy, and high-frequency energy ratios, with theoretical connections to\ncomputational stability. Experiments across GPT architectures suggest universal\nspectral patterns: factual statements exhibit consistent \"energy mountain\"\nbehavior with low-frequency convergence, while different hallucination types\nshow distinct signatures. Logical contradictions destabilize spectra with large\neffect sizes ($g>1.0$), semantic errors remain stable but show connectivity\ndrift, and substitution hallucinations display intermediate perturbations. A\nsimple detector using spectral signatures achieves 88.75% accuracy versus 75%\nfor perplexity-based baselines, demonstrating practical utility. These findings\nindicate that spectral geometry may capture reasoning patterns and error\nbehaviors, potentially offering a framework for hallucination detection in\nlarge language models.", "AI": {"tldr": "通过谱分析框架，我们可以检测大型语言模型中的幻觉问题，该框架准确率为88.75%，优于现有方法。", "motivation": "大型语言模型在很多任务上取得了显著的成果，但如何区别事实推理与幻觉（错误生成）仍然是一个挑战。", "method": "我们提出了一种基于谱分析的框架，将transformer层建模为由注意力引起的动态图，将token嵌入作为图上的信号。通过图信号处理，定义了包括Dirichlet能量、谱熵和高频能量比例等诊断指标，并通过理论建立了与稳定性计算的联系。", "result": "实验结果表明了不同类型的幻觉具有独特的标志：逻辑矛盾导致频谱不稳定（效应值大于1.0），语义错误保持相对稳定但连接性有所变化，而替代性幻觉则表现出中等程度的干扰。使用频谱签名的简单检测器达到了88.75%的准确率，比基于困惑度的基线提高了约13.75%。", "conclusion": "这些发现表明，通过谱几何很可能捕捉到推理模式和错误行为，或许可以提供一个用于大型语言模型幻觉检测的框架。"}}
{"id": "2510.19109", "categories": ["cs.CV", "68U10, 68T07, 68T45", "I.4.6; I.2.10; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2510.19109", "abs": "https://arxiv.org/abs/2510.19109", "authors": ["Eyad Gad", "Seif Soliman", "M. Saeed Darweesh"], "title": "Advancing Brain Tumor Segmentation via Attention-based 3D U-Net Architecture and Digital Image Processing", "comment": null, "summary": "In the realm of medical diagnostics, rapid advancements in Artificial\nIntelligence (AI) have significantly yielded remarkable improvements in brain\ntumor segmentation. Encoder-Decoder architectures, such as U-Net, have played a\ntransformative role by effectively extracting meaningful representations in 3D\nbrain tumor segmentation from Magnetic resonance imaging (MRI) scans. However,\nstandard U-Net models encounter challenges in accurately delineating tumor\nregions, especially when dealing with irregular shapes and ambiguous\nboundaries. Additionally, training robust segmentation models on\nhigh-resolution MRI data, such as the BraTS datasets, necessitates high\ncomputational resources and often faces challenges associated with class\nimbalance. This study proposes the integration of the attention mechanism into\nthe 3D U-Net model, enabling the model to capture intricate details and\nprioritize informative regions during the segmentation process. Additionally, a\ntumor detection algorithm based on digital image processing techniques is\nutilized to address the issue of imbalanced training data and mitigate bias.\nThis study aims to enhance the performance of brain tumor segmentation,\nultimately improving the reliability of diagnosis. The proposed model is\nthoroughly evaluated and assessed on the BraTS 2020 dataset using various\nperformance metrics to accomplish this goal. The obtained results indicate that\nthe model outperformed related studies, exhibiting dice of 0.975, specificity\nof 0.988, and sensitivity of 0.995, indicating the efficacy of the proposed\nmodel in improving brain tumor segmentation, offering valuable insights for\nreliable diagnosis in clinical settings.", "AI": {"tldr": "This paper integrates an attention mechanism into a 3D U-Net model for brain tumor segmentation, improving accuracy on MRI scans.", "motivation": "To overcome issues with standard U-Net models in handling irregular tumor shapes, ambiguous boundaries, and class imbalance, while also considering the high computational requirements.", "method": "The attention mechanism and a tumor detection algorithm based on digital image processing are introduced to improve 3D U-Net's performance in brain tumor segmentation.", "result": "The model demonstrates superior results on the BraTS 2020 dataset, with a dice score of 0.975, specificity of 0.988, and sensitivity of 0.995.", "conclusion": "The proposed model enhances the reliability of brain tumor segmentation, providing valuable diagnostics in clinical environments."}}
{"id": "2510.19131", "categories": ["cs.CL", "cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.19131", "abs": "https://arxiv.org/abs/2510.19131", "authors": ["Valentin Noël"], "title": "Training-Free Spectral Fingerprints of Voice Processing in Transformers", "comment": "Preprint under review (2025). 12 pages, 8 figures", "summary": "Different transformer architectures implement identical linguistic\ncomputations via distinct connectivity patterns, yielding model imprinted\n``computational fingerprints'' detectable through spectral analysis. Using\ngraph signal processing on attention induced token graphs, we track changes in\nalgebraic connectivity (Fiedler value, $\\Delta\\lambda_2$) under voice\nalternation across 20 languages and three model families, with a prespecified\nearly window (layers 2--5). Our analysis uncovers clear architectural\nsignatures: Phi-3-Mini shows a dramatic English specific early layer disruption\n($\\overline{\\Delta\\lambda_2}_{[2,5]}\\!\\approx\\!-0.446$) while effects in 19\nother languages are minimal, consistent with public documentation that\npositions the model primarily for English use. Qwen2.5-7B displays small,\ndistributed shifts that are largest for morphologically rich languages, and\nLLaMA-3.2-1B exhibits systematic but muted responses. These spectral signatures\ncorrelate strongly with behavioral differences (Phi-3: $r=-0.976$) and are\nmodulated by targeted attention head ablations, linking the effect to early\nattention structure and confirming functional relevance. Taken together, the\nfindings are consistent with the view that training emphasis can leave\ndetectable computational imprints: specialized processing strategies that\nmanifest as measurable connectivity patterns during syntactic transformations.\nBeyond voice alternation, the framework differentiates reasoning modes,\nindicating utility as a simple, training free diagnostic for revealing\narchitectural biases and supporting model reliability analysis.", "AI": {"tldr": "通过图信号处理方法分析注意力诱导的标记图，该研究揭示了不同Transformer架构在执行语音变换任务时的内部计算指纹，揭示了模型内在的架构特征及训练偏向的影响。", "motivation": "研究动机是探索不同Transformer架构在执行特定任务时的内部计算模式，并通过这些模式揭示模型内部的架构特征及训练过程中留下的计算指纹。", "method": "研究采用了图信号处理方法，分析了注意力机制生成的标记图，在早期层（第2到5层）跟踪代号变换任务中的代数连通性变化。研究对象包括三种不同的模型家族，涉及20种语言。", "result": "结果显示，不同模型展现出不同的架构签名。Phi-3-Mini在处理英语时表现出明显的早期层干扰，而在处理其他语言时影响较小。Qwen2.5-7B在形态丰富的语言上表现出较大的分布变化，而LLaMA-3.2-1B则有规律但较弱的变化。这些谱签名与行为差异高度相关，通过注意力头的移除实验进一步确认了这些关联。", "conclusion": "研究结果支持了这样一个观点，即训练时的侧重点会留下可检测的计算指纹，体现为在句法转换过程中可测量到的连接模式。该框架还能够区分推理模式，表明其作为无监督诊断工具在揭示模型架构偏差和辅助模型可靠性分析方面的潜在应用价值。"}}
{"id": "2510.19118", "categories": ["cs.CV", "cs.AI", "68U10, 68T07, 68T45, 92C55", "I.4.6; I.2.10; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2510.19118", "abs": "https://arxiv.org/abs/2510.19118", "authors": ["Eyad Gad", "Mustafa Abou Khatwa", "Mustafa A. Elattar", "Sahar Selim"], "title": "A Novel Approach to Breast Cancer Segmentation using U-Net Model with Attention Mechanisms and FedProx", "comment": null, "summary": "Breast cancer is a leading cause of death among women worldwide, emphasizing\nthe need for early detection and accurate diagnosis. As such Ultrasound\nImaging, a reliable and cost-effective tool, is used for this purpose, however\nthe sensitive nature of medical data makes it challenging to develop accurate\nand private artificial intelligence models. A solution is Federated Learning as\nit is a promising technique for distributed machine learning on sensitive\nmedical data while preserving patient privacy. However, training on\nnon-Independent and non-Identically Distributed (non-IID) local datasets can\nimpact the accuracy and generalization of the trained model, which is crucial\nfor accurate tumour boundary delineation in BC segmentation. This study aims to\ntackle this challenge by applying the Federated Proximal (FedProx) method to\nnon-IID Ultrasonic Breast Cancer Imaging datasets. Moreover, we focus on\nenhancing tumour segmentation accuracy by incorporating a modified U-Net model\nwith attention mechanisms. Our approach resulted in a global model with 96%\naccuracy, demonstrating the effectiveness of our method in enhancing tumour\nsegmentation accuracy while preserving patient privacy. Our findings suggest\nthat FedProx has the potential to be a promising approach for training precise\nmachine learning models on non-IID local medical datasets.", "AI": {"tldr": "本研究通过应用FedProx方法与改进的U-Net模型，提高了乳腺癌非独立同分布超声影像数据的肿瘤边界分割准确性，同时保证了患者隐私。", "motivation": "研究动机在于解决乳腺癌早期检测和准确诊断过程中遇到的私人信息保护难题，特别是如何在保护患者隐私的前提下有效训练非独立同分布的本地医疗数据模型。", "method": "研究采用了Federated Proximal方法应对非独立同分布数据集的挑战，并结合拟改进的含注意力机制的U-Net模型进行肿瘤分割。", "result": "该研究针对乳腺癌检测中的隐私保护和非独立同分布数据挑战，应用Federated Proximal（FedProx）方法和改进的带注意力机制的U-Net模型提高肿瘤边界分割的准确性。实验结果显示，最终模型的准确率为96%，表明该方法在保护患者隐私的同时，能有效提高肿瘤分割精度。", "conclusion": "研究结论指出，FedProx方法在训练非独立同分布的本地医疗数据集上可能存在潜力，为精确的机器学习模型提供了一个有前景的方法。"}}
{"id": "2510.19144", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19144", "abs": "https://arxiv.org/abs/2510.19144", "authors": ["Cheng Huang", "Nyima Tashi", "Fan Gao", "Yutong Liu", "Jiahao Li", "Hao Tian", "Siyang Jiang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Jin Zhang", "Xiao Feng", "Hao Wang", "Jie Tang", "Guojie Tang", "Xiangxiang Wang", "Jia Zhang", "Tsengdar Lee", "Yongbin Yu"], "title": "Tibetan Language and AI: A Comprehensive Survey of Resources, Methods and Challenges", "comment": null, "summary": "Tibetan, one of the major low-resource languages in Asia, presents unique\nlinguistic and sociocultural characteristics that pose both challenges and\nopportunities for AI research. Despite increasing interest in developing AI\nsystems for underrepresented languages, Tibetan has received limited attention\ndue to a lack of accessible data resources, standardized benchmarks, and\ndedicated tools. This paper provides a comprehensive survey of the current\nstate of Tibetan AI in the AI domain, covering textual and speech data\nresources, NLP tasks, machine translation, speech recognition, and recent\ndevelopments in LLMs. We systematically categorize existing datasets and tools,\nevaluate methods used across different tasks, and compare performance where\npossible. We also identify persistent bottlenecks such as data sparsity,\northographic variation, and the lack of unified evaluation metrics.\nAdditionally, we discuss the potential of cross-lingual transfer, multi-modal\nlearning, and community-driven resource creation. This survey aims to serve as\na foundational reference for future work on Tibetan AI research and encourages\ncollaborative efforts to build an inclusive and sustainable AI ecosystem for\nlow-resource languages.", "AI": {"tldr": "论文综述了藏语AI的研究现状，包括数据资源、NLP任务、机器翻译、语音识别及大型语言模型的发展，并指出了发展的瓶颈及潜力。", "motivation": "论文旨在全面评估和发展藏语AI的研究状态，特别是在数据资源稀缺的情况下，通过解决数据稀疏性和正字法变化等瓶颈问题来促进该领域的发展。", "method": "此论文通过系统地整理和分类现有的藏语文本和语音数据资源、NLP任务、机器翻译、语音识别及大型语言模型的发展，并评估不同任务中使用的方法来综述藏语AI的现状。", "result": "论文系统地整理了现有数据集和工具，并评估了藏语AI在不同任务中的方法，同时比较了可能的性能，识别出数据稀疏性、正字法变化和缺乏统一评估指标等问题。", "conclusion": "论文作为藏语AI研究的参考基础，鼓励采取交叉语言迁移学习、多模态学习和社区驱动的资源创建等方式，以建立包容性和可持续发展的AI生态系统。"}}
{"id": "2510.19150", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19150", "abs": "https://arxiv.org/abs/2510.19150", "authors": ["Yunzhe Wang", "Soham Hans", "Volkan Ustun"], "title": "X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning", "comment": "8 pages, 5 figures", "summary": "Human team tactics emerge from each player's individual perspective and their\nability to anticipate, interpret, and adapt to teammates' intentions. While\nadvances in video understanding have improved the modeling of team interactions\nin sports, most existing work relies on third-person broadcast views and\noverlooks the synchronous, egocentric nature of multi-agent learning. We\nintroduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay\nfootage from 45 professional-level matches of the popular e-sports game\nCounter-Strike 2, designed to facilitate research on multi-agent\ndecision-making in complex 3D environments. X-Ego-CS provides cross-egocentric\nvideo streams that synchronously capture all players' first-person perspectives\nalong with state-action trajectories. Building on this resource, we propose\nCross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric\nvisual streams to foster team-level tactical situational awareness from an\nindividual's perspective. We evaluate CECL on a teammate-opponent location\nprediction task, demonstrating its effectiveness in enhancing an agent's\nability to infer both teammate and opponent positions from a single\nfirst-person view using state-of-the-art video encoders. Together, X-Ego-CS and\nCECL establish a foundation for cross-egocentric multi-agent benchmarking in\nesports. More broadly, our work positions gameplay understanding as a testbed\nfor multi-agent modeling and tactical learning, with implications for\nspatiotemporal reasoning and human-AI teaming in both virtual and real-world\ndomains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.", "AI": {"tldr": "提出X-Ego-CS数据集和CECL方法，用于在复杂3D环境中研究多智能体的团队战术理解，通过队友视角对比学习以增强战术意识。", "motivation": "现有的研究大多依赖于第三人称视角，忽略了多智能体学习中的同步自我中心视角特点。通过引入X-Ego-CS和CECL方法，旨在提升在这种复杂三维环境中的决策能力。", "method": "引入X-Ego-CS数据集和提出CECL方法，X-Ego-CS包含124小时的专业级别的电子竞技游戏《CS2》的游戏录像，CECL通过对比学习来对齐队友的视角以增强团队战术意识。", "result": "CECL方法在团队成员与对手位置预测任务上表现出色，证明了从单一第一视角中推测队友及对手位置的有效性。", "conclusion": "通过评估团队成员和对手位置预测任务，证明了CECL能有效改进单个自视图下智能体对队友和对手位置的理解，为电子竞技中多智能体基准测试和策略学习确立基础。"}}
{"id": "2510.19167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19167", "abs": "https://arxiv.org/abs/2510.19167", "authors": ["Dingjie Fu", "Dianxing Shi"], "title": "\"You Are Rejected!\": An Empirical Study of Large Language Models Taking Hiring Evaluations", "comment": "Technical Report, 14 pages, 8 figures", "summary": "With the proliferation of the internet and the rapid advancement of\nArtificial Intelligence, leading technology companies face an urgent annual\ndemand for a considerable number of software and algorithm engineers. To\nefficiently and effectively identify high-potential candidates from thousands\nof applicants, these firms have established a multi-stage selection process,\nwhich crucially includes a standardized hiring evaluation designed to assess\njob-specific competencies. Motivated by the demonstrated prowess of Large\nLanguage Models (LLMs) in coding and reasoning tasks, this paper investigates a\ncritical question: Can LLMs successfully pass these hiring evaluations? To this\nend, we conduct a comprehensive examination of a widely used professional\nassessment questionnaire. We employ state-of-the-art LLMs to generate responses\nand subsequently evaluate their performance. Contrary to any prior expectation\nof LLMs being ideal engineers, our analysis reveals a significant inconsistency\nbetween the model-generated answers and the company-referenced solutions. Our\nempirical findings lead to a striking conclusion: All evaluated LLMs fails to\npass the hiring evaluation.", "AI": {"tldr": "本文探讨了大型语言模型（LLMs）是否可以成功通过科技公司的专业评估问卷，并发现所有评估的LLMs未能通过雇佣评估。", "motivation": "受到大型语言模型（LLMs）在编程和推理任务中表现出的强大能力的启发，本文旨在研究LLMs能否成功通过这些雇佣评估。", "method": "本文采用了一种全面审查广泛使用的专业评估问卷的方法，使用了最先进的大型语言模型（LLMs）来生成回答，并对其表现进行了评估。", "result": "分析结果显示，模型生成的答案与公司参考答案之间存在显著差异。", "conclusion": "实证研究结果得出一个令人惊讶的结论：所有评估的LLMs未能通过雇佣评估。"}}
{"id": "2510.19170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19170", "abs": "https://arxiv.org/abs/2510.19170", "authors": ["Keaton Kraiger", "Jingjing Li", "Skanda Bharadwaj", "Jesse Scott", "Robert T. Collins", "Yanxi Liu"], "title": "FootFormer: Estimating Stability from Visual Input", "comment": "19 pages, 9 figures", "summary": "We propose FootFormer, a cross-modality approach for jointly predicting human\nmotion dynamics directly from visual input. On multiple datasets, FootFormer\nachieves statistically significantly better or equivalent estimates of foot\npressure distributions, foot contact maps, and center of mass (CoM), as\ncompared with existing methods that generate one or two of those measures.\nFurthermore, FootFormer achieves SOTA performance in estimating\nstability-predictive components (CoP, CoM, BoS) used in classic kinesiology\nmetrics. Code and data are available at\nhttps://github.com/keatonkraiger/Vision-to-Stability.git.", "AI": {"tldr": "FootFormer 是一种跨模态方法，可以从视觉输入直接预测人体运动动力学。在多个数据集上，FootFormer 在估计脚部压力分布、脚部接触图和质心方面的表现优于或相当于现有方法。此外，FootFormer 在估计稳定性预测组件方面的表现达到了 SOTA，代码和数据在指定的 GitHub 地址上公开。", "motivation": "旨在开发一种方法，能够直接从视觉数据预测人体运动中的动力学参数，提供更全面和准确的稳定性评估。", "method": "提出了一个名为 FootFormer 的跨模态方法，能够从视觉输入直接预测人体运动的多个动力学参数。", "result": "FootFormer 在脚部压力分布、接触图和质心估计上具有统计显著的优良表现。同时，该方法在估算稳定性预测组件 CoP（质心投影）、CoM（质心）和 BoS（稳定性边界）方面达到了 SOTA 性能。", "conclusion": "FootFormer 方法能够有效联合预测多项运动动力学参数，并在多个评估指标上优于现有方法。"}}
{"id": "2510.19171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19171", "abs": "https://arxiv.org/abs/2510.19171", "authors": ["Jihwan Bang", "Juntae Lee", "Seunghan Yang", "Sungha Choi"], "title": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop RAG", "comment": "Accepted at NeurIPS 2025 Workshop", "summary": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference.", "AI": {"tldr": "content", "motivation": "method", "method": "motivation", "result": "{\n  \"tldr\": \"TSSS是一种结构化的多跳检索增强生成框架，旨在提高效率。它通过模板化推理和检索者终止器来减少令牌生成成本并提供稳定的推理过程。TSSS在多个数据集上达到了最先进的准确率和竞争力的效率，尤其适用于设备端推理等受限场景。\",\n  \"motivation\": \"现有的多跳检索增强生成方法中的迭代提示策略存在低效率问题，如重复生成可预见的标记序列和依赖随机停止机制，导致了过多的令牌使用和不稳定的终止点。\",\n  \"method\": \"TSSS引入了基于模板的推理，可以缓存重复的前缀并锚定子查询以减少令牌生成成本。同时，引入了检索者终止器来确定性地停止推理过程以避免重复。\",\n  \"result\": \"TSSS在HotpotQA、2WikiMultiHop和MuSiQue数据集上达到较高的准确率和高效的性能，进一步证明了其在设备端等效率受限场景中的有效性。\",\n  \"conclusion\": \"TSSS通过结构化推理和终止控制机制，实现了更快速的推理和更可靠的答案，展示了其在检索增强生成策略中的优势。\"}\n}", "conclusion": "result"}}
{"id": "2510.19182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19182", "abs": "https://arxiv.org/abs/2510.19182", "authors": ["Warisa Nusrat", "Mostafijur Rahman", "Ayatullah Faruk Mollah"], "title": "Malaria Detection from Blood Cell Images Using XceptionNet", "comment": null, "summary": "Malaria, which primarily spreads with the bite of female anopheles mosquitos,\noften leads to death of people - specifically children in the age-group of 0-5\nyears. Clinical experts identify malaria by observing RBCs in blood smeared\nimages with a microscope. Lack of adequate professional knowledge and skills,\nand most importantly manual involvement may cause incorrect diagnosis.\nTherefore, computer aided automatic diagnosis stands as a preferred substitute.\nIn this paper, well-demonstrated deep networks have been applied to extract\ndeep intrinsic features from blood cell images and thereafter classify them as\nmalaria infected or healthy cells. Among the six deep convolutional networks\nemployed in this work viz. AlexNet, XceptionNet, VGG-19, Residual Attention\nNetwork, DenseNet-121 and Custom-CNN. Residual Attention Network and\nXceptionNet perform relatively better than the rest on a publicly available\nmalaria cell image dataset. They yield an average accuracy of 97.28% and 97.55%\nrespectively, that surpasses other related methods on the same dataset. These\nfindings highly encourage the reality of deep learning driven method for\nautomatic and reliable detection of malaria while minimizing direct manual\ninvolvement.", "AI": {"tldr": "本文利用深度网络分析血细胞图像，自动诊断疟疾，采用多种深层卷积网络，其中残差注意力网络和XceptionNet表现最好，分别达到97.28%和97.55%的准确率。", "motivation": "临床专家通过显微镜下观察血液涂片中的红细胞来诊断疟疾，但由于专业知识和技能不足以及人为判断可能存在的误差，计算机辅助自动诊断成为一种更优选择。", "method": "运用了六种深层卷积网络，包括AlexNet、XceptionNet、VGG-19、残差注意力网络、DenseNet-121和自定义CNN，从血细胞图像中提取深层次特征，并将其分类为疟疾感染或健康细胞。", "result": "在公开的疟疾细胞图像数据集中，残差注意力网络和XceptionNet表现优于其他模型，准确率分别为97.28%和97.55%，超越了该数据集上的其他相关方法。", "conclusion": "结果表明，基于深度学习的疟疾自动可靠检测方法是可行的，并且极大地减少了直接人工操作。"}}
{"id": "2510.19172", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19172", "abs": "https://arxiv.org/abs/2510.19172", "authors": ["Nishanth Sridhar Nakshatri", "Shamik Roy", "Manoj Ghuhan Arivazhagan", "Hanhan Zhou", "Vinayshekhar Bannihatti Kumar", "Rashmi Gangadharaiah"], "title": "When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA", "comment": "Under submission", "summary": "LLMs often fail to handle temporal knowledge conflicts--contradictions\narising when facts evolve over time within their training data. Existing\nstudies evaluate this phenomenon through benchmarks built on structured\nknowledge bases like Wikidata, but they focus on widely-covered,\neasily-memorized popular entities and lack the dynamic structure needed to\nfairly evaluate LLMs with different knowledge cut-off dates. We introduce\nevolveQA, a benchmark specifically designed to evaluate LLMs on temporally\nevolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS\nupdates, Azure changes, and WHO disease outbreak reports. Our framework\nidentifies naturally occurring knowledge evolution and generates questions with\ngold answers tailored to different LLM knowledge cut-off dates. Through\nextensive evaluation of 12 open and closed-source LLMs across 3 knowledge\nprobing formats, we demonstrate significant performance drops of up to 31% on\nevolveQA compared to static knowledge questions.", "AI": {"tldr": "我们提出了evolveQA，一个基于真实世界时间标注语料库设计的基准测试，展示在处理时间演变的知识时，LLMs的表现显著下降。", "motivation": "现有的研究关注于广泛覆盖、容易被记忆的流行实体，但缺乏用于公平评估具有不同知识截至日期的LLMs的动态结构。", "method": "我们提出了evolveQA，一个专门用于评估LLMs在时间演化知识方面表现的基准，该基准由三个带有时间戳的真实世界语料库构建：AWS更新、Azure变更和WHO疾病爆发报告。通过识别自然发生的知识演变并针对不同LLMs的知识截至日期生成问题及其标准答案。", "result": "对12个开源和闭源LLMs进行了广泛的评估，结果显示在evolveQA上的表现比静态知识问题下降了高达31%。", "conclusion": "这表明现有的LLMs在处理随着时间推移而演变的知识时存在显著困难。"}}
{"id": "2510.19183", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19183", "abs": "https://arxiv.org/abs/2510.19183", "authors": ["Fengyuan Sun", "Hui Chen", "Xinhao Xu", "Dandan Zheng", "Jingdong Chen", "Jun Zhou", "Jungong Han", "Guiguang Ding"], "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning", "comment": null, "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.", "AI": {"tldr": "Introduces PruneHal, a method to mitigate hallucinations in MLLMs by focusing on important visual tokens using adaptive KV cache pruning.", "motivation": "To address the challenge of hallucinations in MLLMs without incurring additional training costs or computational overhead.", "method": "PruneHal, a training-free method that leverages adaptive KV cache pruning to enhance focus on critical visual information.", "result": "Achieves robust and outstanding performance on hallucination evaluation benchmarks using four mainstream MLLMs, without requiring additional training.", "conclusion": "PruneHal demonstrates effectiveness and superiority in mitigating hallucinations in MLLMs while being model-agnostic and low-cost in terms of inference."}}
{"id": "2510.19181", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19181", "abs": "https://arxiv.org/abs/2510.19181", "authors": ["Kartikeya Aneja", "Manasvi Srivastava", "Subhayan Das", "Nagender Aneja"], "title": "Interpretable Question Answering with Knowledge Graphs", "comment": null, "summary": "This paper presents a question answering system that operates exclusively on\na knowledge graph retrieval without relying on retrieval augmented generation\n(RAG) with large language models (LLMs). Instead, a small paraphraser model is\nused to paraphrase the entity relationship edges retrieved from querying the\nknowledge graph. The proposed pipeline is divided into two main stages. The\nfirst stage involves pre-processing a document to generate sets of\nquestion-answer (QA) pairs. The second stage converts these QAs into a\nknowledge graph from which graph-based retrieval is performed using embeddings\nand fuzzy techniques. The graph is queried, re-ranked, and paraphrased to\ngenerate a final answer. This work includes an evaluation using LLM-as-a-judge\non the CRAG benchmark, which resulted in accuracies of 71.9% and 54.4% using\nLLAMA-3.2 and GPT-3.5-Turbo, respectively.", "AI": {"tldr": "This paper presents a QA system that operates without large language models, using a knowledge graph and a small paraphraser for generating answers. It achieves promising evaluation results on the CRAG benchmark.", "motivation": "The motivation is to create a question answering system that does not depend on retrieval augmented generation (RAG) with large language models (LLMs). The goal is to explore an alternative approach that focuses on the use of knowledge graphs and a smaller paraphraser model.", "method": "The paper introduces a QA system relying solely on knowledge graph retrieval. It includes two stages: first, preprocessing documents to create QA pairs; second, transforming these pairs into a knowledge graph and retrieving data via embeddings and fuzzy techniques. The system then refines and paraphrases retrieved information to produce answers.", "result": "The evaluation using LLM-as-a-judge on the CRAG benchmark resulted in accuracies of 71.9% with LLAMA-3.2 and 54.4% with GPT-3.5-Turbo.", "conclusion": "The paper concludes its exploration of an effective QA system using knowledge graphs instead of relying on large language models for retrieval and generation tasks."}}
{"id": "2510.19193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19193", "abs": "https://arxiv.org/abs/2510.19193", "authors": ["Takehiro Aoshima", "Yusuke Shinohara", "Park Byeongseon"], "title": "Video Consistency Distance: Enhancing Temporal Consistency for Image-to-Video Generation via Reward-Based Fine-Tuning", "comment": "17 pages", "summary": "Reward-based fine-tuning of video diffusion models is an effective approach\nto improve the quality of generated videos, as it can fine-tune models without\nrequiring real-world video datasets. However, it can sometimes be limited to\nspecific performances because conventional reward functions are mainly aimed at\nenhancing the quality across the whole generated video sequence, such as\naesthetic appeal and overall consistency. Notably, the temporal consistency of\nthe generated video often suffers when applying previous approaches to\nimage-to-video (I2V) generation tasks. To address this limitation, we propose\nVideo Consistency Distance (VCD), a novel metric designed to enhance temporal\nconsistency, and fine-tune a model with the reward-based fine-tuning framework.\nTo achieve coherent temporal consistency relative to a conditioning image, VCD\nis defined in the frequency space of video frame features to capture frame\ninformation effectively through frequency-domain analysis. Experimental results\nacross multiple I2V datasets demonstrate that fine-tuning a video generation\nmodel with VCD significantly enhances temporal consistency without degrading\nother performance compared to the previous method.", "AI": {"tldr": "A new metric, VCD, is introduced for reward-based fine-tuning of video diffusion models to improve temporal consistency in image-to-video generation, showing positive results on several datasets.", "motivation": "The conventional reward functions are not sufficient for enhancing temporal consistency in video generation, especially in image-to-video tasks.", "method": "Reward-based fine-tuning of video diffusion models is enhanced with Video Consistency Distance (VCD), a novel metric for improving temporal consistency in image-to-video generation tasks.", "result": "Experiments on various datasets show that incorporating VCD into reward-based fine-tuning significantly boosts temporal consistency without negatively impacting other aspects of video quality.", "conclusion": "VCD-based fine-tuning is an effective strategy for improving temporal consistency in video generation, while preserving other quality metrics."}}
{"id": "2510.19186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19186", "abs": "https://arxiv.org/abs/2510.19186", "authors": ["Zhaoyi Joey Hou", "Tanya Shourya", "Yingfan Wang", "Shamik Roy", "Vinayshekhar Bannihatti Kumar", "Rashmi Gangadharaiah"], "title": "Multi-Faceted Evaluation of Tool-Augmented Dialogue Systems", "comment": "The first two authors contributed equally. Manuscript under\n  submission", "summary": "Evaluating conversational AI systems that use external tools is challenging,\nas errors can arise from complex interactions among user, agent, and tools.\nWhile existing evaluation methods assess either user satisfaction or agents'\ntool-calling capabilities, they fail to capture critical errors in multi-turn\ntool-augmented dialogues-such as when agents misinterpret tool results yet\nappear satisfactory to users. We introduce TRACE, a benchmark of systematically\nsynthesized tool-augmented conversations covering diverse error cases, and\nSCOPE, an evaluation framework that automatically discovers diverse error\npatterns and evaluation rubrics in tool-augmented dialogues. Experiments show\nSCOPE significantly outperforms the baseline, particularly on challenging cases\nwhere user satisfaction signals are misleading.", "AI": {"tldr": "本文提出了一种新的评估框架SCOPE和一个系统合成的工具增强对话基准测试TRACE，以解决现有评估方法无法捕捉多轮对话中的关键错误问题。", "motivation": "现有的评估方法只能评估用户满意度或代理调用工具的能力，但无法捕捉多轮工具增强对话中关键的错误。因此，作者提出了新的评估框架和基准测试。", "method": "该论文介绍了TRACE，一个系统合成的工具增强对话基准测试，涵盖了各种错误案例。同时引入了SCOPE，一个能够自动发现工具增强对话中多种错误模式和评估标准的评估框架。", "result": "实验表明，SCOPE在评估表现上显著优于基线方法，特别是在用户满意度信号具有误导性的困难情况中。", "conclusion": "新框架SCOPE能够有效评估工具增强对话系统，特别是在用户反馈可能误导评估的情况下。"}}
{"id": "2510.19195", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19195", "abs": "https://arxiv.org/abs/2510.19195", "authors": ["Kai Zeng", "Zhanqian Wu", "Kaixin Xiong", "Xiaobao Wei", "Xiangyu Guo", "Zhenxin Zhu", "Kalok Ho", "Lijun Zhou", "Bohan Zeng", "Ming Lu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wentao Zhang"], "title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks", "comment": null, "summary": "Recent advancements in driving world models enable controllable generation of\nhigh-quality RGB videos or multimodal videos. Existing methods primarily focus\non metrics related to generation quality and controllability. However, they\noften overlook the evaluation of downstream perception tasks, which are\n$\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing\nmethods usually leverage a training strategy that first pretrains on synthetic\ndata and finetunes on real data, resulting in twice the epochs compared to the\nbaseline (real data only). When we double the epochs in the baseline, the\nbenefit of synthetic data becomes negligible. To thoroughly demonstrate the\nbenefit of synthetic data, we introduce Dream4Drive, a novel synthetic data\ngeneration framework designed for enhancing the downstream perception tasks.\nDream4Drive first decomposes the input video into several 3D-aware guidance\nmaps and subsequently renders the 3D assets onto these guidance maps. Finally,\nthe driving world model is fine-tuned to produce the edited, multi-view\nphotorealistic videos, which can be used to train the downstream perception\nmodels. Dream4Drive enables unprecedented flexibility in generating multi-view\ncorner cases at scale, significantly boosting corner case perception in\nautonomous driving. To facilitate future research, we also contribute a\nlarge-scale 3D asset dataset named DriveObj3D, covering the typical categories\nin driving scenarios and enabling diverse 3D-aware video editing. We conduct\ncomprehensive experiments to show that Dream4Drive can effectively boost the\nperformance of downstream perception models under various training epochs.\nProject: $\\href{https://wm-research.github.io/Dream4Drive/}{this\\ https\\ URL}$", "AI": {"tldr": "介绍了用于自动驾驶情境中增强感知任务的Dream4Drive合成数据生成框架，展示了其提高下游任务性能的能力。", "motivation": "当前的方法虽然在生成质量和可控性方面有所进展，但忽略了对自动驾驶至关重要的下游感知任务的评估。Dream4Drive旨在弥补这一不足，展示合成数据在下游任务上的益处。", "method": "Dream4Drive主要通过首先将输入视频分解为几个具有3D感知的指导图，然后在这些指导图上渲染3D资产来生成合成数据。最终对驾驶世界模型进行微调，以生成编辑后的多视角逼真视频，用于训练下游感知模型。", "result": "通过充分利用合成数据的优势，Dream4Drive能够在不同训练周期下显著提高下游感知模型的性能。这表明合成数据在自动驾驶应用中的重要性。", "conclusion": "实验表明，与仅使用真实数据的基线相比，在相同的训练周期下，Dream4Drive能有效提高下游感知模型的表现。"}}
{"id": "2510.19208", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19208", "abs": "https://arxiv.org/abs/2510.19208", "authors": ["Hang Zheng", "Hongshen Xu", "Yongkai Lin", "Shuai Fan", "Lu Chen", "Kai Yu"], "title": "DiSRouter: Distributed Self-Routing for LLM Selections", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has created a diverse\necosystem of models with highly varying performance and costs, necessitating\neffective query routing to balance performance and expense. Current routing\nsystems often rely on a centralized external router trained on a fixed set of\nLLMs, making them inflexible and prone to poor performance since the small\nrouter can not fully understand the knowledge boundaries of different LLMs. We\nintroduce DiSRouter (Distributed Self-Router), a novel paradigm that shifts\nfrom centralized control to distributed routing. In DiSRouter, a query\ntraverses a network of LLM agents, each independently deciding whether to\nanswer or route to other agents based on its own self-awareness, its ability to\njudge its competence. This distributed design offers superior flexibility,\nscalability, and generalizability. To enable this, we propose a two-stage\nSelf-Awareness Training pipeline that enhances each LLM's self-awareness.\nExtensive experiments demonstrate that DiSRouter significantly outperforms\nexisting routing methods in utility across various scenarios, effectively\ndistinguishes between easy and hard queries, and shows strong generalization to\nout-of-domain tasks. Our work validates that leveraging an LLM's intrinsic\nself-awareness is more effective than external assessment, paving the way for\nmore modular and efficient multi-agent systems.", "AI": {"tldr": "提出了一种分布式路由方案（DiSRouter），该方案将查询通过一组具有自我意识的LLM代理网络进行传输，显示出比现有路由方法更高的灵活性、扩展性和泛化能力。", "motivation": "大型语言模型（LLM）的多样性导致了性能和成本的差异，需要有效的查询路由来平衡性能和成本。现有的路由系统通常依赖于集中式的外部路由器，这种方法缺乏灵活性，并且由于小型路由器不能完全理解不同LLM的知识边界，因此性能往往不好。", "method": "介绍了DiSRouter（分布式自路由）这一新的范式，它从集中式控制转变为分布式路由。在DiSRouter中，一个查询会在一组LLM代理的网络中传输，每个代理基于自己的自我意识和判断能力决定是否回答或者将查询转发给其他代理。为此，研究者提出了一种两阶段的自我意识训练流水线来增强每个LLM的自我意识能力。", "result": "实验结果表明，在各种情况下，DiSRouter相对于现有的路由方法显示出了更高的实用性，能够有效地区分简单和复杂的查询，并且对于超出领域范围的任务也具有良好的泛化能力。", "conclusion": "这项工作验证了利用LLM固有的自我意识比外部评估更有效，为更模块化和高效的多代理系统铺平了道路。"}}
{"id": "2510.19210", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19210", "abs": "https://arxiv.org/abs/2510.19210", "authors": ["In-Hwan Jin", "Hyeongju Mun", "Joonsoo Kim", "Kugjin Yun", "Kyeongbo Kong"], "title": "MoE-GS: Mixture of Experts for Dynamic Gaussian Splatting", "comment": null, "summary": "Recent advances in dynamic scene reconstruction have significantly benefited\nfrom 3D Gaussian Splatting, yet existing methods show inconsistent performance\nacross diverse scenes, indicating no single approach effectively handles all\ndynamic challenges. To overcome these limitations, we propose Mixture of\nExperts for Dynamic Gaussian Splatting (MoE-GS), a unified framework\nintegrating multiple specialized experts via a novel Volume-aware Pixel Router.\nOur router adaptively blends expert outputs by projecting volumetric\nGaussian-level weights into pixel space through differentiable weight\nsplatting, ensuring spatially and temporally coherent results. Although MoE-GS\nimproves rendering quality, the increased model capacity and reduced FPS are\ninherent to the MoE architecture. To mitigate this, we explore two\ncomplementary directions: (1) single-pass multi-expert rendering and gate-aware\nGaussian pruning, which improve efficiency within the MoE framework, and (2) a\ndistillation strategy that transfers MoE performance to individual experts,\nenabling lightweight deployment without architectural changes. To the best of\nour knowledge, MoE-GS is the first approach incorporating Mixture-of-Experts\ntechniques into dynamic Gaussian splatting. Extensive experiments on the N3V\nand Technicolor datasets demonstrate that MoE-GS consistently outperforms\nstate-of-the-art methods with improved efficiency. Video demonstrations are\navailable at https://anonymous.4open.science/w/MoE-GS-68BA/.", "AI": {"tldr": "MoE-GS框架通过集成多种专门化的专家，解决了现有方法在动态场景重建中的性能一致性问题，并通过蒸馏策略解决了效率问题。", "motivation": "现有方法在处理多样化场景时表现不一致，无法有效应对所有动态挑战。", "method": "提出Mixture of Experts for Dynamic Gaussian Splatting (MoE-GS)框架，该框架集成多个通过新颖的Volume-aware Pixel Router进行专门化的专家，实现动态场景重建。通过将体积Gaussian级别的权重投影到像素空间并进行可微权重扩散，Router实现了专家输出的自适应混合，确保了空间和时间上的一致性。为了解决模型容量增加和FPS减少的问题，作者探索了两种互补的方向：单次遍历多专家渲染和gate-aware Gaussian剪枝，以及将MoE性能转移至个体专家的蒸馏策略。", "result": "在N3V和Technicolor数据集上进行的广泛实验表明，MoE-GS在提高效率方面优于最先进的方法。", "conclusion": "MoE-GS是首个将Mixture-of-Experts技术引入动态高斯点云领域的框架，实验结果证明了其有效性和效率的提升。"}}
{"id": "2510.19217", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19217", "abs": "https://arxiv.org/abs/2510.19217", "authors": ["York Hay Ng", "Aditya Khan", "Xiang Lu", "Matteo Salloum", "Michael Zhou", "Phuong H. Hoang", "A. Seza Doğruöz", "En-Shiun Annie Lee"], "title": "Modality Matching Matters: Calibrating Language Distances for Cross-Lingual Transfer in URIEL+", "comment": null, "summary": "Existing linguistic knowledge bases such as URIEL+ provide valuable\ngeographic, genetic and typological distances for cross-lingual transfer but\nsuffer from two key limitations. One, their one-size-fits-all vector\nrepresentations are ill-suited to the diverse structures of linguistic data,\nand two, they lack a principled method for aggregating these signals into a\nsingle, comprehensive score. In this paper, we address these gaps by\nintroducing a framework for type-matched language distances. We propose novel,\nstructure-aware representations for each distance type: speaker-weighted\ndistributions for geography, hyperbolic embeddings for genealogy, and a latent\nvariables model for typology. We unify these signals into a robust,\ntask-agnostic composite distance. In selecting transfer languages, our\nrepresentations and composite distances consistently improve performance across\na wide range of NLP tasks, providing a more principled and effective toolkit\nfor multilingual research.", "AI": {"tldr": "本文介绍了语言距离的类型匹配框架，并提出了新型结构感知表示方法以解决现有语言知识库的问题，这些方法在选择转移语言时，能提升NLP任务的表现。", "motivation": "现有的语言知识库如URIEL+存在两个关键限制：一是一刀切的向量表示不适合语言数据的多样性结构；二是缺乏一种原则性方法来将这些信号综合到一个单一的综合评分中。本文旨在解决这些问题。", "method": "我们提出了针对每种距离类型的新型、结构感知表示方法：地理距离使用说话人加权分布，谱系距离使用双曲嵌入，类型学距离使用潜在变量模型。我们将这些信号统一成一个健壯且任务无关的综合距离。", "result": "我们的表示方法和综合距离在广泛NLP任务中一致提高了性能，为多语言研究提供了一套更为完善且有效的工具。", "conclusion": "本研究提出的方法为多语言研究提供了一个更加原则化且有效的工具箱。"}}
{"id": "2510.19215", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19215", "abs": "https://arxiv.org/abs/2510.19215", "authors": ["Xiaozhi Li", "Huijun Di", "Jian Li", "Feng Liu", "Wei Liang"], "title": "SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion", "comment": "Submitted to Pattern Recognition", "summary": "3D object detection is essential for autonomous driving. As an emerging\nsensor, 4D imaging radar offers advantages as low cost, long-range detection,\nand accurate velocity measurement, making it highly suitable for object\ndetection. However, its sparse point clouds and low resolution limit object\ngeometric representation and hinder multi-modal fusion. In this study, we\nintroduce SFGFusion, a novel camera-4D imaging radar detection network guided\nby surface fitting. By estimating quadratic surface parameters of objects from\nimage and radar data, the explicit surface fitting model enhances spatial\nrepresentation and cross-modal interaction, enabling more reliable prediction\nof fine-grained dense depth. The predicted depth serves two purposes: 1) in an\nimage branch to guide the transformation of image features from perspective\nview (PV) to a unified bird's-eye view (BEV) for multi-modal fusion, improving\nspatial mapping accuracy; and 2) in a surface pseudo-point branch to generate\ndense pseudo-point cloud, mitigating the radar point sparsity. The original\nradar point cloud is also encoded in a separate radar branch. These two point\ncloud branches adopt a pillar-based method and subsequently transform the\nfeatures into the BEV space. Finally, a standard 2D backbone and detection head\nare used to predict object labels and bounding boxes from BEV features.\nExperimental results show that SFGFusion effectively fuses camera and 4D radar\nfeatures, achieving superior performance on the TJ4DRadSet and view-of-delft\n(VoD) object detection benchmarks.", "AI": {"tldr": "本文提出了SFGFusion，一种基于表面拟合指导的摄像头-4D成像雷达检测网络，提升了多模态融合及空间表示能力，从而提高三维物体检测性能。", "motivation": "4D成像雷达虽然具有低成本、远距离检测和准确速度测量等优势，但其稀疏点云和低分辨率限制了物体几何表示及多模态融合。", "method": "通过估计图像和雷达数据中物体的二次曲面参数，显式表面拟合模型增强了空间表示和跨模态相互作用，预测的深度用于指导图像特征转换和生成密集伪点云。", "result": "实验结果显示，SFGFusion在TJ4DRadSet和VoD物体检测基准数据集上实现了优越性能，有效融合了摄像头和4D雷达特征。", "conclusion": "SFGFusion通过表面拟合和深度预测技术有效解决了雷达点云稀疏和多模态融合的问题，提升了自动驾驶中的物体检测性能。"}}
{"id": "2510.19247", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19247", "abs": "https://arxiv.org/abs/2510.19247", "authors": ["Ziwei Wang", "Jiayuan Su", "Mengyu Zhou", "Huaxing Zeng", "Mengni Jia", "Xiao Lv", "Haoyu Dong", "Xiaojun Ma", "Shi Han", "Dongmei Zhang"], "title": "SheetBrain: A Neuro-Symbolic Agent for Accurate Reasoning over Complex and Large Spreadsheets", "comment": null, "summary": "Understanding and reasoning over complex spreadsheets remain fundamental\nchallenges for large language models (LLMs), which often struggle with\naccurately capturing the complex structure of tables and ensuring reasoning\ncorrectness. In this work, we propose SheetBrain, a neuro-symbolic dual\nworkflow agent framework designed for accurate reasoning over tabular data,\nsupporting both spreadsheet question answering and manipulation tasks.\nSheetBrain comprises three core modules: an understanding module, which\nproduces a comprehensive overview of the spreadsheet - including sheet summary\nand query-based problem insight to guide reasoning; an execution module, which\nintegrates a Python sandbox with preloaded table-processing libraries and an\nExcel helper toolkit for effective multi-turn reasoning; and a validation\nmodule, which verifies the correctness of reasoning and answers, triggering\nre-execution when necessary. We evaluate SheetBrain on multiple public tabular\nQA and manipulation benchmarks, and introduce SheetBench, a new benchmark\ntargeting large, multi-table, and structurally complex spreadsheets.\nExperimental results show that SheetBrain significantly improves accuracy on\nboth existing benchmarks and the more challenging scenarios presented in\nSheetBench. Our code is publicly available at\nhttps://github.com/microsoft/SheetBrain.", "AI": {"tldr": "SheetBrain是一种神经符号双工作流代理框架，用于对表格数据进行准确推理，支持电子表格问答和操控任务。", "motivation": "大型语言模型在捕获表格的复杂结构和确保推理正确性方面存在困难，因此需要SheetBrain这样专门设计的框架来进行准确的表格数据推理。", "method": "SheetBrain框架由三个核心模块组成：理解模块、执行模块和验证模块。理解模块生成表格的综合概述，执行模块使用Python沙盒和Excel工具包进行多轮推理，验证模块保证推理和答案的正确性。", "result": "在多个公开的表格问答和操控基准上的实验结果显示，SheetBrain显著提高了精度，尤其在新的SheetBench基准上面对大型、多表和结构复杂的电子表格挑战。", "conclusion": "通过集成处理表单库和Excel工具包，SheetBrain在保持推理准确性的同时，还能高效地处理复杂表格数据。代码已公开。"}}
{"id": "2510.19220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19220", "abs": "https://arxiv.org/abs/2510.19220", "authors": ["Xiaoqing Lan", "Biqiao Xin", "Bingshu Wang", "Han Zhang", "Laixian Zhang"], "title": "Space Object Detection using Multi-frame Temporal Trajectory Completion Method", "comment": null, "summary": "Space objects in Geostationary Earth Orbit (GEO) present significant\ndetection challenges in optical imaging due to weak signals, complex stellar\nbackgrounds, and environmental interference. In this paper, we enhance\nhigh-frequency features of GEO targets while suppressing background noise at\nthe single-frame level through wavelet transform. Building on this, we propose\na multi-frame temporal trajectory completion scheme centered on the Hungarian\nalgorithm for globally optimal cross-frame matching. To effectively mitigate\nmissing and false detections, a series of key steps including temporal matching\nand interpolation completion, temporal-consistency-based noise filtering, and\nprogressive trajectory refinement are designed in the post-processing pipeline.\nExperimental results on the public SpotGEO dataset demonstrate the\neffectiveness of the proposed method, achieving an F_1 score of 90.14%.", "AI": {"tldr": "本文提出了一种通过小波变换和多帧时序轨迹补全技术，在GEO轨道上进行有效目标检测的方法，该方法在公共数据库上取得了90.14%的F_1得分。", "motivation": "由于弱信号、复杂的星场背景和环境干扰，GEO轨道上空间物体的目标检测面临巨大挑战。", "method": "通过小波变换在单帧水平上增强GEO目标的高频特征并抑制背景噪声。进一步提出了基于匈牙利算法的多帧时序轨迹补全方案，实现全局最优跨帧匹配。在后处理管线中设计了一系列关键步骤，包括时序匹配和插值补全、基于时序一致性的噪声过滤和逐步轨迹优化，以有效缓解漏检和误检问题。", "result": "实验结果表明，本文提出的方法在公开的SpotGEO数据集上表现出较高的效果，达到了90.14%的F_1得分。", "conclusion": "实验结果证明了本文提出的方法在GEO轨道上空间物体的有效检测。"}}
{"id": "2510.19265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19265", "abs": "https://arxiv.org/abs/2510.19265", "authors": ["Yuto Tomikawa", "Masaki Uto"], "title": "Difficulty-Controllable Multiple-Choice Question Generation Using Large Language Models and Direct Preference Optimization", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Difficulty-controllable question generation for reading comprehension has\ngained significant attention in the field of education as a fundamental tool\nfor adaptive learning support. Although several neural question generation\nmethods have recently succeeded in controlling difficulty, conventional\napproaches still face two major limitations. First, they cannot directly\ngenerate multiple-choice questions, which are the most widely used question\ntype in educational contexts. Second, they are not explicitly trained to\noptimize the accuracy of difficulty control, leaving room for further\nimprovement in difficulty controllability. To address these limitations, this\nstudy proposes a novel difficulty-controllable multiple-choice question\ngeneration method for reading comprehension which leverages a large language\nmodel trained using a direct preference optimization technique to improve the\naccuracy of difficulty control.", "AI": {"tldr": "本文提出了一种基于大语言模型的难度可控的多项选择题生成方法，以改善阅读理解中的难度控制准确性。", "motivation": "此项研究的动机是解决现有神经网络问题生成方法在生成多项选择题和难度控制准确性上的两个限制。", "method": "使用直接偏好优化技术训练大语言模型来提高难度控制的准确度。", "result": "未提供具体结果，但提出了一个理论上能够改善现有方法不足的新方法。", "conclusion": "通过采用新的生成方法，有望在教育领域适应性学习支持中改善难度可控性。"}}
{"id": "2510.19250", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19250", "abs": "https://arxiv.org/abs/2510.19250", "authors": ["Yuheng Wu", "Xiangbo Gao", "Quang Tau", "Zhengzhong Tu", "Dongman Lee"], "title": "Background Fades, Foreground Leads: Curriculum-Guided Background Pruning for Efficient Foreground-Centric Collaborative Perception", "comment": null, "summary": "Collaborative perception enhances the reliability and spatial coverage of\nautonomous vehicles by sharing complementary information across vehicles,\noffering a promising solution to long-tail scenarios that challenge\nsingle-vehicle perception. However, the bandwidth constraints of vehicular\nnetworks make transmitting the entire feature map impractical. Recent methods,\ntherefore, adopt a foreground-centric paradigm, transmitting only predicted\nforeground-region features while discarding the background, which encodes\nessential context. We propose FadeLead, a foreground-centric framework that\novercomes this limitation by learning to encapsulate background context into\ncompact foreground features during training. At the core of our design is a\ncurricular learning strategy that leverages background cues early on but\nprogressively prunes them away, forcing the model to internalize context into\nforeground representations without transmitting background itself. Extensive\nexperiments on both simulated and real-world benchmarks show that FadeLead\noutperforms prior methods under different bandwidth settings, underscoring the\neffectiveness of context-enriched foreground sharing.", "AI": {"tldr": "FadeLead 是一种前景为中心的框架，通过课程学习策略，使模型在训练过程中学会将背景上下文内化到前景特征中，从而在带宽有限的情况下提高自主车辆的感知性能。", "motivation": "现有的方法虽然只传输预测的前景区域特征，但丢弃了背景中的重要上下文信息，而带宽限制使得传输整个特征图不可行。", "method": "FadeLead, 一个前景为中心的框架，学习在训练过程中将背景上下文封装到紧凑的前景特征中，采用课程学习策略，早期利用背景线索但逐渐去除，迫使模型将上下文内化到前景表示中而不需传输背景本身。", "result": "在模拟和实际的基准测试上，FadeLead 在不同的带宽设置下均优于先前的方法，证明了上下文化前景共享的有效性。", "conclusion": "实验表明，通过将背景上下文内化到前景特征，可以在带宽有限的情况下提高自主车辆的感知性能。"}}
{"id": "2510.19286", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19286", "abs": "https://arxiv.org/abs/2510.19286", "authors": ["Reza Esfandiarpoor", "Vishwas Suryanarayanan", "Stephen H. Bach", "Vishal Chowdhary", "Anthony Aue"], "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools", "comment": "Code: https://github.com/Reza-esfandiarpoor/the-mcp-company", "summary": "Since the introduction of the Model Context Protocol (MCP), the number of\navailable tools for Large Language Models (LLMs) has increased significantly.\nThese task-specific tool sets offer an alternative to general-purpose tools\nsuch as web browsers, while being easier to develop and maintain than GUIs.\nHowever, current general-purpose agents predominantly rely on web browsers for\ninteracting with the environment. Here, we introduce TheMCPCompany, a benchmark\nfor evaluating tool-calling agents on tasks that involve interacting with\nvarious real-world services. We use the REST APIs of these services to create\nMCP servers, which include over 18,000 tools. We also provide manually\nannotated ground-truth tools for each task. In our experiments, we use the\nground truth tools to show the potential of tool-calling agents for both\nimproving performance and reducing costs assuming perfect tool retrieval. Next,\nwe explore agent performance using tool retrieval to study the real-world\npracticality of tool-based agents. While all models with tool retrieval perform\nsimilarly or better than browser-based agents, smaller models cannot take full\nadvantage of the available tools through retrieval. On the other hand, GPT-5's\nperformance with tool retrieval is very close to its performance with\nground-truth tools. Overall, our work shows that the most advanced reasoning\nmodels are effective at discovering tools in simpler environments, but\nseriously struggle with navigating complex enterprise environments.\nTheMCPCompany reveals that navigating tens of thousands of tools and combining\nthem in non-trivial ways to solve complex problems is still a challenging task\nfor current models and requires both better reasoning and better retrieval\nmodels.", "AI": {"tldr": "介绍了TheMCPCompany，用于评估工具调用代理在现实世界服务互动任务上的性能。实验结果表明工具调用代理在提高性能和降低成本上具有潜力，特别是对于较复杂的环境，这需要更好的推理和检索模型。", "motivation": "当前通用代理主要依赖于网络浏览器与环境交互。然而，这些特定任务的工具集提供了一种替代通用工具（如网页浏览器）的方式，并且相较于图形用户界面来说更易于开发和维护。", "method": "我们引入了TheMCPCompany，这是一个用于评估工具调用代理在涉及与各种现实世界服务交互的任务上的基准。我们使用这些服务的REST API来创建MCP服务器，其中包括超过18,000个工具。我们还为每个任务提供了手动注释的地面真实工具。", "result": "实验表明，带有工具检索的所有模型的表现要么与基于浏览器的代理相仿，要么更好；尽管较小的模型无法完全利用可用的工具，但GPT-5在使用工具检索时的性能接近其使用地面真实工具时的表现。", "conclusion": "总体而言，这项工作显示了最先进的推理模型在发现较简单环境中的工具时有效，但在导航复杂的商业环境时则表现不佳。TheMCPCompany揭示了在解决复杂问题时，导航成千上万的工具并以非平凡的方式组合它们仍然是当前模型面临的挑战，这需要更好的推理和检索模型。"}}
{"id": "2510.19255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19255", "abs": "https://arxiv.org/abs/2510.19255", "authors": ["Mingrui Zhao", "Sauradip Nag", "Kai Wang", "Aditya Vora", "Guangda Ji", "Peter Chun", "Ali Mahdavi-Amiri", "Hao Zhang"], "title": "Advances in 4D Representation: Geometry, Motion, and Interaction", "comment": "21 pages. Project Page: https://mingrui-zhao.github.io/4DRep-GMI/", "summary": "We present a survey on 4D generation and reconstruction, a fast-evolving\nsubfield of computer graphics whose developments have been propelled by recent\nadvances in neural fields, geometric and motion deep learning, as well 3D\ngenerative artificial intelligence (GenAI). While our survey is not the first\nof its kind, we build our coverage of the domain from a unique and distinctive\nperspective of 4D representations\\/}, to model 3D geometry evolving over time\nwhile exhibiting motion and interaction. Specifically, instead of offering an\nexhaustive enumeration of many works, we take a more selective approach by\nfocusing on representative works to highlight both the desirable properties and\nensuing challenges of each representation under different computation,\napplication, and data scenarios. The main take-away message we aim to convey to\nthe readers is on how to select and then customize the appropriate 4D\nrepresentations for their tasks. Organizationally, we separate the 4D\nrepresentations based on three key pillars: geometry, motion, and interaction.\nOur discourse will not only encompass the most popular representations of\ntoday, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),\nbut also bring attention to relatively under-explored representations in the 4D\ncontext, such as structured models and long-range motions. Throughout our\nsurvey, we will reprise the role of large language models (LLMs) and video\nfoundational models (VFMs) in a variety of 4D applications, while steering our\ndiscussion towards their current limitations and how they can be addressed. We\nalso provide a dedicated coverage on what 4D datasets are currently available,\nas well as what is lacking, in driving the subfield forward. Project\npage:https://mingrui-zhao.github.io/4DRep-GMI/", "AI": {"tldr": "本文综述了4D生成与重建领域，特别关注4D表示的几何、运动与交互特征，分析代表性工作的优缺点，讨论大型语言模型和视频基础模型在该领域中的角色及其局限性，指出了当前数据集的不足。", "motivation": "旨在帮助读者了解如何选择和定制合适的4D表示来满足他们的任务需求，并提供了当前可用的4D数据集以及尚需完善的方面。", "method": "我们通过三个关键支柱来组织4D表示：几何、运动和交互，并专注于具有代表性的作品以突出每种表示在不同计算、应用和数据场景下的优缺点和挑战。", "result": "论文涵盖了当前流行的表示方法，如神经辐射场（NeRFs）和3D高斯撒点法（3DGS），以及较少被探索的表示方法，如结构化模型和长距离运动。同时，还讨论了大型语言模型（LLMs）和视频基础模型（VFMs）在4D应用中的角色及其局限性。", "conclusion": "通过对现有研究进行选择性回顾和分析，为4D生成和重建领域的发展提供了新的视角和方向。"}}
{"id": "2510.19310", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19310", "abs": "https://arxiv.org/abs/2510.19310", "authors": ["Fan Xu", "Huixuan Zhang", "Zhenliang Zhang", "Jiahao Wang", "Xiaojun Wan"], "title": "JointCQ: Improving Factual Hallucination Detection with Joint Claim and Query Generation", "comment": null, "summary": "Current large language models (LLMs) often suffer from hallucination issues,\ni,e, generating content that appears factual but is actually unreliable. A\ntypical hallucination detection pipeline involves response decomposition (i.e.,\nclaim extraction), query generation, evidence collection (i.e., search or\nretrieval), and claim verification. However, existing methods exhibit\nlimitations in the first two stages, such as context loss during claim\nextraction and low specificity in query generation, resulting in degraded\nperformance across the hallucination detection pipeline. In this work, we\nintroduce JointCQ https://github.com/pku0xff/JointCQ, a joint claim-and-query\ngeneration framework designed to construct an effective and efficient\nclaim-query generator. Our framework leverages elaborately designed evaluation\ncriteria to filter synthesized training data, and finetunes a language model\nfor joint claim extraction and query generation, providing reliable and\ninformative inputs for downstream search and verification. Experimental results\ndemonstrate that our method outperforms previous methods on multiple\nopen-domain QA hallucination detection benchmarks, advancing the goal of more\ntrustworthy and transparent language model systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.19272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19272", "abs": "https://arxiv.org/abs/2510.19272", "authors": ["Yun Kai Zhuang"], "title": "SCEESR: Semantic-Control Edge Enhancement for Diffusion-Based Super-Resolution", "comment": "10 pages, 5 figures, 3 tables", "summary": "Real-world image super-resolution (Real-ISR) must handle complex degradations\nand inherent reconstruction ambiguities. While generative models have improved\nperceptual quality, a key trade-off remains with computational cost. One-step\ndiffusion models offer speed but often produce structural inaccuracies due to\ndistillation artifacts. To address this, we propose a novel SR framework that\nenhances a one-step diffusion model using a ControlNet mechanism for semantic\nedge guidance. This integrates edge information to provide dynamic structural\ncontrol during single-pass inference. We also introduce a hybrid loss combining\nL2, LPIPS, and an edge-aware AME loss to optimize for pixel accuracy,\nperceptual quality, and geometric precision. Experiments show our method\neffectively improves structural integrity and realism while maintaining the\nefficiency of one-step generation, achieving a superior balance between output\nquality and inference speed. The results of test datasets will be published at\nhttps://drive.google.com/drive/folders/1amddXQ5orIyjbxHgGpzqFHZ6KTolinJF?usp=drive_link\nand the related code will be published at\nhttps://github.com/ARBEZ-ZEBRA/SCEESR.", "AI": {"tldr": "本文提出了一种新的超分辨率方法，通过引入ControlNet机制和混合损失函数，改善了图像的结构准确性和感知质量。", "motivation": "现有的超分辨率（ISR）方法面临计算成本与图像质量之间的权衡，本研究旨在通过引入新的模型和损失优化方法解决结构不准确的问题。", "method": "通过ControlNet机制进行边缘引导，结合L2、LPIPS和边缘感知AME损失来优化图像生成过程，达到增强结构准确性和整体质量的效果。", "result": "该研究提出了一种新的超分辨率框架，它通过ControlNet机制引导语义边缘信息，来提高单步扩散模型的结构准确性。此外，还引入了一个结合L2、LPIPS和边缘感知AME损失的混合损失函数，以优化像素精度、感知质量和几何精确度。实验结果显示，该方法在保持单步生成效率的同时，显著提升了结构完整性和真实性。", "conclusion": "该研究提出的方法在保持高效生成速度的同时，显著提升了超分辨率图像的质量。"}}
{"id": "2510.19316", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19316", "abs": "https://arxiv.org/abs/2510.19316", "authors": ["Kailin Jiang", "Hongbo Jiang", "Ning Jiang", "Zhi Gao", "Jinhe Bi", "Yuchen Ren", "Bin Li", "Yuntao Du", "Lei Liu", "Qing Li"], "title": "KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints", "comment": "project page: https://kore-lmm.github.io/", "summary": "Large Multimodal Models encode extensive factual knowledge in their\npre-trained weights. However, its knowledge remains static and limited, unable\nto keep pace with real-world developments, which hinders continuous knowledge\nacquisition. Effective knowledge injection thus becomes critical, involving two\ngoals: knowledge adaptation (injecting new knowledge) and knowledge retention\n(preserving old knowledge). Existing methods often struggle to learn new\nknowledge and suffer from catastrophic forgetting. To address this, we propose\nKORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints\nfor injecting new knowledge into large multimodal models while preserving old\nknowledge. Unlike general text or image data augmentation, KORE automatically\nconverts individual knowledge items into structured and comprehensive knowledge\nto ensure that the model accurately learns new knowledge, enabling accurate\nadaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix\nof LMM's linear layer activations and initializes the adapter by projecting the\noriginal weights into the matrix's null space, defining a fine-tuning direction\nthat minimizes interference with previous knowledge, enabling powerful\nretention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B,\nLLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new\nknowledge injection performance and effectively mitigates catastrophic\nforgetting.", "AI": {"tldr": "KORE方法实现了新知识的有效注入和旧知识的保留，解决了大规模多模态模型中的知识适应和避免灾难性遗忘的问题。", "motivation": "由于大规模多模态模型中的知识是静态和有限的，无法跟上现实世界的快速发展，新的知识注入方法变得至关重要，以实现知识适应和知识保留的目标。", "method": "KORE采用知识导向的增强和约束方法，以结构化和全面的方式将新知识注入大规模多模态模型，同时通过在协方差矩阵中存储先前的知识，并将原始权重初始化到矩阵的零空间中，最小化新旧知识之间的干扰。", "result": "在LLaVA-v1.5-7B, LLaVA-v1.5-13B, 和 Qwen2.5-VL-7B等多种大规模多模态模型上的大量实验证明了KORE的有效性。", "conclusion": "实验结果表明，KORE在多种大规模多模态模型中实现了优越的新知识注入性能，并有效缓解了灾难性遗忘问题。"}}
{"id": "2510.19273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19273", "abs": "https://arxiv.org/abs/2510.19273", "authors": ["Zhang Nengbo", "Ho Hann Woei"], "title": "MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation", "comment": null, "summary": "Accurate and efficient recognition of Micro Air Vehicle (MAV) motion is\nessential for enabling real-time perception and coordination in autonomous\naerial swarm. However, most existing approaches rely on large, computationally\nintensive models that are unsuitable for resource-limited MAV platforms, which\nresults in a trade-off between recognition accuracy and inference speed. To\naddress these challenges, this paper proposes a lightweight MAV action\nrecognition framework, MobiAct, designed to achieve high accuracy with low\ncomputational cost. Specifically, MobiAct adopts MobileNetV4 as the backbone\nnetwork and introduces a Stage-wise Orthogonal Knowledge Distillation (SOKD)\nstrategy to effectively transfer MAV motion features from a teacher network\n(ResNet18) to a student network, thereby enhancing knowledge transfer\nefficiency. Furthermore, a parameter-free attention mechanism is integrated\ninto the architecture to improve recognition accuracy without increasing model\ncomplexity. In addition, a hybrid loss training strategy is developed to\ncombine multiple loss objectives, which ensures stable and robust optimization\nduring training. Experimental results demonstrate that the proposed MobiAct\nachieves low-energy and low-computation MAV action recognition, while\nmaintaining the fastest action decoding speed among compared methods. Across\nall three self-collected datasets, MobiAct achieves an average recognition\naccuracy of 92.12%, while consuming only 136.16 pJ of energy and processing\nrecognition at a rate of 8.84 actions per second. Notably, MobiAct decodes\nactions up to 2 times faster than the leading method, with highly comparable\nrecognition accuracy, highlighting its superior efficiency in MAV action\nrecognition.", "AI": {"tldr": "本研究提出了一种适用于资源受限的微型空中车辆平台的轻量化动作识别框架MobiAct，能够实现高精度和低计算成本的动作识别，实验表明该框架具备低能耗（136.16 pJ）、高识别准确率（92.12%）和快速动作解码（8.84 actions per second）的特点。", "motivation": "解决现有动作识别方法计算成本高、资源占用大，在资源受限平台上应用不现实的问题。", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出了一种轻量级的无人机动作识别框架MobiAct，用于在资源受限的微型空中车辆平台上实现高精度和低计算成本的动作识别，实验结果表明MobiAct在三个自建数据集上达到了92.12%的平均识别准确率，同时能耗仅为136.16 pJ，识别速度达到了8.84 actions per second，速度快于当前领先方法两倍。\",\n  \"motivation\": \"传统的方法依赖于大型的、计算密集的模型，这在资源受限的微型空中车辆平台上是不合适，因而本文旨在解决这一问题，提高动作识别的准确率和推理速度。\",\n  \"method\": \"MobiAct采用MobileNetV4作为骨干网络，并引入了阶段正交知识迁移策略（SOKD），引入无参数注意力机制以及混合损失训练策略以优化训练过程。\",\n  \"result\": \"实验结果表明，MobiAct实现了低能耗且计算效率高的无人机动作识别，并且在动作解码速度上比同类方法快近两倍。\",\n  \"conclusion\": \"MobiAct在保持高识别准确率的同时，实现了在资源受限平台上的高效动作识别，显示出其在微型空中车辆动作识别领域的优越效率。\\n\"}\n}", "conclusion": "MobiAct在改进的资源受限平台上实现实时感知和协同作用的微型空中车辆动作识别中具有优异的性能。"}}
{"id": "2510.19318", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19318", "abs": "https://arxiv.org/abs/2510.19318", "authors": ["Fan Xu", "Xinyu Hu", "Zhenghan Yu", "Li Lin", "Xu Zhang", "Yang Zhang", "Wei Zhou", "Jinjie Gu", "Xiaojun Wan"], "title": "HAD: HAllucination Detection Language Models Based on a Comprehensive Hallucination Taxonomy", "comment": null, "summary": "The increasing reliance on natural language generation (NLG) models,\nparticularly large language models, has raised concerns about the reliability\nand accuracy of their outputs. A key challenge is hallucination, where models\nproduce plausible but incorrect information. As a result, hallucination\ndetection has become a critical task. In this work, we introduce a\ncomprehensive hallucination taxonomy with 11 categories across various NLG\ntasks and propose the HAllucination Detection (HAD) models\nhttps://github.com/pku0xff/HAD, which integrate hallucination detection,\nspan-level identification, and correction into a single inference process.\nTrained on an elaborate synthetic dataset of about 90K samples, our HAD models\nare versatile and can be applied to various NLG tasks. We also carefully\nannotate a test set for hallucination detection, called HADTest, which contains\n2,248 samples. Evaluations on in-domain and out-of-domain test sets show that\nour HAD models generally outperform the existing baselines, achieving\nstate-of-the-art results on HaluEval, FactCHD, and FaithBench, confirming their\nrobustness and versatility.", "AI": {"tldr": "文中提出一个幻觉分类体系以及名为HAD的模型，该模型在幻觉检测和纠正上具有优异的表现。", "motivation": "因为大语言模型存在产生合理但不正确的信息的挑战（幻觉问题），幻觉检测成为一个关键任务。", "method": "该研究引入了一个包含11类幻觉的全面分类体系，并提出了名为HAD（HAllucination Detection）的模型，它集成了幻觉检测、片段级识别和纠正功能。HAD模型在一个精心设计的合成数据集上进行训练，该数据集包含约90,000个样本。", "result": "使用HAD模型在内部和外部测试集上的评估表明，与现有基线相比，HAD模型表现更优异，在HaluEval, FactCHD 和 FaithBench基准测试上取得了最新成果。", "conclusion": "实验结果显示HAD模型在几个基准测试集上超越现有方法，证明了其鲁棒性和适用性。"}}
{"id": "2510.19278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19278", "abs": "https://arxiv.org/abs/2510.19278", "authors": ["Nobline Yoo", "Olga Russakovsky", "Ye Zhu"], "title": "D2D: Detector-to-Differentiable Critic for Improved Numeracy in Text-to-Image Generation", "comment": "24 pages, 14 figures", "summary": "Text-to-image (T2I) diffusion models have achieved strong performance in\nsemantic alignment, yet they still struggle with generating the correct number\nof objects specified in prompts. Existing approaches typically incorporate\nauxiliary counting networks as external critics to enhance numeracy. However,\nsince these critics must provide gradient guidance during generation, they are\nrestricted to regression-based models that are inherently differentiable, thus\nexcluding detector-based models with superior counting ability, whose\ncount-via-enumeration nature is non-differentiable. To overcome this\nlimitation, we propose Detector-to-Differentiable (D2D), a novel framework that\ntransforms non-differentiable detection models into differentiable critics,\nthereby leveraging their superior counting ability to guide numeracy\ngeneration. Specifically, we design custom activation functions to convert\ndetector logits into soft binary indicators, which are then used to optimize\nthe noise prior at inference time with pre-trained T2I models. Our extensive\nexperiments on SDXL-Turbo, SD-Turbo, and Pixart-DMD across four benchmarks of\nvarying complexity (low-density, high-density, and multi-object scenarios)\ndemonstrate consistent and substantial improvements in object counting accuracy\n(e.g., boosting up to 13.7% on D2D-Small, a 400-prompt, low-density benchmark),\nwith minimal degradation in overall image quality and computational overhead.", "AI": {"tldr": "The paper presents Detector-to-Differentiable (D2D), a framework that adapts non-differentiable detector models for enhancing the numerical accuracy of text-to-image diffusion models.", "motivation": "Despite the success of text-to-image diffusion models in semantic alignment, there's a need to improve the accuracy of object counting specified in prompts.", "method": "The paper proposes D2D, which converts detector models into differentiable critics using custom activation functions to improve numerical accuracy in T2I models.", "result": "Experiments across multiple benchmarks show improved object counting accuracy with little impact on image quality and with manageable computational costs.", "conclusion": "Detector-to-Differentiable provides a solution to improve the numeracy of text-to-image models without sacrificing the quality of the generated images."}}
{"id": "2510.19325", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19325", "abs": "https://arxiv.org/abs/2510.19325", "authors": ["Junjie Song", "Yiwen Liu", "Dapeng Li", "Yin Sun", "Shukun Fu", "Siqi Chen", "Yuji Cao"], "title": "Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization", "comment": null, "summary": "Text summarization is a crucial task that requires the simultaneous\noptimization of multiple objectives, including consistency, coherence,\nrelevance, and fluency, which presents considerable challenges. Although large\nlanguage models (LLMs) have demonstrated remarkable performance, enhanced by\nreinforcement learning (RL), few studies have focused on optimizing the\nmulti-objective problem of summarization through RL based on LLMs. In this\npaper, we introduce hypervolume optimization (HVO), a novel optimization\nstrategy that dynamically adjusts the scores between groups during the reward\nprocess in RL by using the hypervolume method. This method guides the model's\noptimization to progressively approximate the pareto front, thereby generating\nbalanced summaries across multiple objectives. Experimental results on several\nrepresentative summarization datasets demonstrate that our method outperforms\ngroup relative policy optimization (GRPO) in overall scores and shows more\nbalanced performance across different dimensions. Moreover, a 7B foundation\nmodel enhanced by HVO performs comparably to GPT-4 in the summarization task,\nwhile maintaining a shorter generation length. Our code is publicly available\nat https://github.com/ai4business-LiAuto/HVO.git", "AI": {"tldr": "本文提出一种新策略HVO，用于优化LLMs在文本总结中多目标协调的问题，并展示其在多个数据集上的优越性能。", "motivation": "虽然大型语言模型（LLMs）通过强化学习（RL）已经表现出色，但在LLMs基础上通过RL优化总结任务中的多目标问题的研究较少。本文致力于解决这一问题。", "method": "本文提出了一种新颖的优化策略——超体积优化（HVO），该策略在强化学习（RL）过程中通过超体积方法动态调整不同组之间的奖励分数，旨在生成多目标协调的摘要。", "result": "实验表明，该方法在多个总结数据集上的总体表现优于群组相对策略优化（GRPO），并在不同维度上表现更均衡。另外，经过HVO优化的7B基础模型在总结任务中的表现与GPT-4相当，同时生成长度更短。", "conclusion": "HVO方法在文本总结任务中展现了优于GRPO的整体表现，并且7B基础模型经过优化后能与GPT-4相匹敌。代码已开源。"}}
{"id": "2510.19282", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19282", "abs": "https://arxiv.org/abs/2510.19282", "authors": ["Safa Ben Atitallah", "Maha Driss", "Wadii Boulila", "Anis Koubaa"], "title": "Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning", "comment": null, "summary": "Alzheimer disease is a severe brain disorder that causes harm in various\nbrain areas and leads to memory damage. The limited availability of labeled\nmedical data poses a significant challenge for accurate Alzheimer disease\ndetection. There is a critical need for effective methods to improve the\naccuracy of Alzheimer disease detection, considering the scarcity of labeled\ndata, the complexity of the disease, and the constraints related to data\nprivacy. To address this challenge, our study leverages the power of big data\nin the form of pre-trained Convolutional Neural Networks (CNNs) within the\nframework of Few-Shot Learning (FSL) and ensemble learning. We propose an\nensemble approach based on a Prototypical Network (ProtoNet), a powerful method\nin FSL, integrating various pre-trained CNNs as encoders. This integration\nenhances the richness of features extracted from medical images. Our approach\nalso includes a combination of class-aware loss and entropy loss to ensure a\nmore precise classification of Alzheimer disease progression levels. The\neffectiveness of our method was evaluated using two datasets, the Kaggle\nAlzheimer dataset and the ADNI dataset, achieving an accuracy of 99.72% and\n99.86%, respectively. The comparison of our results with relevant\nstate-of-the-art studies demonstrated that our approach achieved superior\naccuracy and highlighted its validity and potential for real-world applications\nin early Alzheimer disease detection.", "AI": {"tldr": "The paper proposes an ensemble method using pre-trained CNNs within the Few-Shot Learning framework to improve Alzheimer's disease detection accuracy, achieving high results on two datasets.", "motivation": "The study aims to improve the accuracy of Alzheimer's disease detection given the limited availability of labeled data, the complexity of the disease, and privacy constraints.", "method": "Our approach combines Few-Shot Learning with an ensemble of pre-trained CNNs, using a Prototypical Network (ProtoNet) as the core, and integrates class-aware loss and entropy loss to enhance the classification of Alzheimer's disease stages.", "result": "The proposed method achieved an accuracy of 99.72% on the Kaggle Alzheimer dataset and 99.86% on the ADNI dataset, outperforming other state-of-the-art studies.", "conclusion": "The developed ensemble approach demonstrates superior accuracy and potential for real-world applications in the early detection of Alzheimer's disease."}}
{"id": "2510.19326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19326", "abs": "https://arxiv.org/abs/2510.19326", "authors": ["Kadri Hacioglu", "Manjunath K E", "Andreas Stolcke"], "title": "Slot Filling as a Reasoning Task for SpeechLLMs", "comment": null, "summary": "We propose integration of reasoning into speech large language models\n(speechLLMs) for the end-to-end slot-filling task. Inspired by the recent\ndevelopment of reasoning LLMs, we use a chain-of-thought framework to decompose\nthe slot-filling task into multiple reasoning steps, create a reasoning dataset\nand apply the supervised fine-tuning strategy to a speechLLM. We distinguish\nbetween regular and reasoning speechLLMs and experiment with different types\nand sizes of LLMs as their text foundation models. We demonstrate performance\nimprovements by introducing reasoning (intermediate) steps. However, we show\nthat a reasoning textual LLM developed mainly for math, logic and coding\ndomains might be inferior as a foundation model for a reasoning speechLLM. We\nfurther show that hybrid speechLLMs, built on a hybrid text foundation LLM and\nfine-tuned to preserve both direct and reasoning modes of operation, have\nbetter performance than those fine-tuned employing only one mode of operation.", "AI": {"tldr": "将推理机制融入语音语言模型，通过链式思维框架分解任务，并通过微调提升性能，发现特定领域的基础模型并不总是最优选择。", "motivation": "旨在将推理集成到语音大型语言模型中，以提高端到端槽填充任务的性能。", "method": "采用链式思维框架将槽填充任务分解为多个推理步骤，并创建推理数据集，采用监督微调策略对语音语言模型进行调整。", "result": "引入推理（中间）步骤可以提升性能。然而，专门针对数学、逻辑和编程领域的推理文本语言模型可能作为推理语音语言模型的基础模型时表现不佳。同时表明，基于混合文本基础语言模型构建的混合语音语言模型，并微调以保持直接和推理两种操作模式，其性能优于仅采用一种操作模式的模型。", "conclusion": "研究表明，采用推理步骤可以改进槽填充任务的性能，但需针对语音模型进行特定领域的优化。混合模式操作的语音语言模型性能更优。"}}
{"id": "2510.19292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19292", "abs": "https://arxiv.org/abs/2510.19292", "authors": ["Konstantinos Bacharidis", "Antonis A. Argyros"], "title": "Vision-Based Mistake Analysis in Procedural Activities: A Review of Advances and Challenges", "comment": "21pages, 6 figures, 2 tables", "summary": "Mistake analysis in procedural activities is a critical area of research with\napplications spanning industrial automation, physical rehabilitation, education\nand human-robot collaboration. This paper reviews vision-based methods for\ndetecting and predicting mistakes in structured tasks, focusing on procedural\nand executional errors. By leveraging advancements in computer vision,\nincluding action recognition, anticipation and activity understanding,\nvision-based systems can identify deviations in task execution, such as\nincorrect sequencing, use of improper techniques, or timing errors. We explore\nthe challenges posed by intra-class variability, viewpoint differences and\ncompositional activity structures, which complicate mistake detection.\nAdditionally, we provide a comprehensive overview of existing datasets,\nevaluation metrics and state-of-the-art methods, categorizing approaches based\non their use of procedural structure, supervision levels and learning\nstrategies. Open challenges, such as distinguishing permissible variations from\ntrue mistakes and modeling error propagation are discussed alongside future\ndirections, including neuro-symbolic reasoning and counterfactual state\nmodeling. This work aims to establish a unified perspective on vision-based\nmistake analysis in procedural activities, highlighting its potential to\nenhance safety, efficiency and task performance across diverse domains.", "AI": {"tldr": "这篇综述文章探讨了基于视觉的方法在检测和预测结构化任务中的执行错误和程序错误的应用，以及此领域当前的挑战与未来的研究方向。", "motivation": "错误分析在程序性活动中是一个关键的研究领域，其应用范围涵盖了工业自动化、物理康复、教育和人机协作。", "method": "本文综述了基于视觉的方法在检测和预测结构化任务中的错误方面的应用，重点在于程序性和执行性错误。通过利用计算机视觉领域的进展，包括动作识别、预期和活动理解，基于视觉的系统可以识别任务执行中的偏差，如不正确的顺序、使用不正确的技术或时间错误。", "result": "文章探讨了包括类内变异性、视角差异和组合活动结构等挑战难题，这些都会影响到错误检测的效果。此外，本综述还提供了现有数据集、评估指标及前沿方法的全面概述，并根据流程结构、监督级别和学习策略对这些方法进行了分类。", "conclusion": "本文旨在建立一种统一的关于程序性活动中基于视觉的错误分析的视角，并强调其有潜力提升各个领域的安全性、效率和任务表现。讨论的开放性挑战包括区分允许的变异性与真正的错误，以及建模错误传播。除此之外，还提出了未来研究方向，包括神经符号推理和反事实状态建模。"}}
{"id": "2510.19331", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.19331", "abs": "https://arxiv.org/abs/2510.19331", "authors": ["Ewelina Gajewska", "Arda Derbent", "Jaroslaw A Chudziak", "Katarzyna Budzynska"], "title": "Algorithmic Fairness in NLP: Persona-Infused LLMs for Human-Centric Hate Speech Detection", "comment": "This paper has been accepted for the upcoming 59th Hawaii\n  International Conference on System Sciences (HICSS-59), 2026, Hawaii, USA.\n  The final published version will appear in the official conference\n  proceedings", "summary": "In this paper, we investigate how personalising Large Language Models\n(Persona-LLMs) with annotator personas affects their sensitivity to hate\nspeech, particularly regarding biases linked to shared or differing identities\nbetween annotators and targets. To this end, we employ Google's Gemini and\nOpenAI's GPT-4.1-mini models and two persona-prompting methods: shallow persona\nprompting and a deeply contextualised persona development based on\nRetrieval-Augmented Generation (RAG) to incorporate richer persona profiles. We\nanalyse the impact of using in-group and out-group annotator personas on the\nmodels' detection performance and fairness across diverse social groups. This\nwork bridges psychological insights on group identity with advanced NLP\ntechniques, demonstrating that incorporating socio-demographic attributes into\nLLMs can address bias in automated hate speech detection. Our results highlight\nboth the potential and limitations of persona-based approaches in reducing\nbias, offering valuable insights for developing more equitable hate speech\ndetection systems.", "AI": {"tldr": "本研究通过给大型语言模型添加不同身份背景的注释者的人格轮廓来探索减少仇恨言论检测偏见的方法，发现该方法既有潜力也有局限性。", "motivation": "研究通过添加注释者的人格轮廓是否可以提高大型语言模型在仇恨言论检测中对偏见的敏感度，尤其是当注释者和目标共享或具有不同身份时。", "method": "使用Google的Gemini和OpenAI的GPT-4.1-mini模型，并采用两种人格化提示方法：浅层人格提示和基于检索增强生成（RAG）的深层次人格发展来整合更丰富的人格档案。", "result": "结果表明，使用内群和外群注释者的人格轮廓对模型的检测性能和公平性在不同社会群体之间产生了影响，展示出基于人格的方法在减少偏见方面的潜力和局限性。", "conclusion": "将社会人口统计学属性整合进入LLMs中可以解决自动仇恨言论检测中的偏差问题，为开发更公平的仇恨言论检测系统提供了有价值的见解。"}}
{"id": "2510.19307", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19307", "abs": "https://arxiv.org/abs/2510.19307", "authors": ["Byung-Kwan Lee", "Ryo Hachiuma", "Yong Man Ro", "Yu-Chiang Frank Wang", "Yueh-Hua Wu"], "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models", "comment": "NeurIPS 2025, Project page: https://byungkwanlee.github.io/RIL-page", "summary": "Vision-Language Models (VLMs) have achieved remarkable progress, yet their\nlarge scale often renders them impractical for resource-constrained\nenvironments. This paper introduces Unified Reinforcement and Imitation\nLearning (RIL), a novel and efficient training algorithm designed to create\npowerful, lightweight VLMs. RIL distinctively combines the strengths of\nreinforcement learning with adversarial imitation learning. This enables\nsmaller student VLMs not only to mimic the sophisticated text generation of\nlarge teacher models but also to systematically improve their generative\ncapabilities through reinforcement signals. Key to our imitation framework is\nan LLM-based discriminator that adeptly distinguishes between student and\nteacher outputs, complemented by guidance from multiple large teacher VLMs to\nensure diverse learning. This unified learning strategy, leveraging both\nreinforcement and imitation, empowers student models to achieve significant\nperformance gains, making them competitive with leading closed-source VLMs.\nExtensive experiments on diverse vision-language benchmarks demonstrate that\nRIL significantly narrows the performance gap with state-of-the-art open- and\nclosed-source VLMs and, in several instances, surpasses them.", "AI": {"tldr": "Unified Reinforcement and Imitation Learning (RIL) creates powerful, lightweight Vision-Language Models (VLMs) by combining reinforcement learning and adversarial imitation learning, leading to significant performance gains across various benchmarks.", "motivation": "To address the impracticality of large-scale Vision-Language Models (VLMs) in resource-constrained environments by developing an efficient training method for smaller models.", "method": "RIL combines reinforcement learning with adversarial imitation learning, using an LLM-based discriminator to guide the generative capabilities of student VLMs and multiple large teacher VLMs to ensure diverse learning.", "result": "Experiments show that RIL-trained models outperform or match the performance of state-of-the-art open- and closed-source VLMs on diverse vision-language tasks.", "conclusion": "RIL effectively bridges the performance gap between larger, more resource-demanding VLMs and smaller, more efficient models by enhancing their generative performance through a unified learning strategy."}}
{"id": "2510.19346", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19346", "abs": "https://arxiv.org/abs/2510.19346", "authors": ["Prakrithi Shivaprakash", "Lekhansh Shukla", "Animesh Mukherjee", "Prabhat Chand", "Pratima Murthy"], "title": "Local Obfuscation by GLINER for Impartial Context Aware Lineage: Development and evaluation of PII Removal system", "comment": "30 pages, 15 main text and 15 supplementary material", "summary": "Removing Personally Identifiable Information (PII) from clinical notes in\nElectronic Health Records (EHRs) is essential for research and AI development.\nWhile Large Language Models (LLMs) are powerful, their high computational costs\nand the data privacy risks of API-based services limit their use, especially in\nlow-resource settings. To address this, we developed LOGICAL (Local Obfuscation\nby GLINER for Impartial Context-Aware Lineage), an efficient, locally\ndeployable PII removal system built on a fine-tuned Generalist and Lightweight\nNamed Entity Recognition (GLiNER) model. We used 1515 clinical documents from a\npsychiatric hospital's EHR system. We defined nine PII categories for removal.\nA modern-gliner-bi-large-v1.0 model was fine-tuned on 2849 text instances and\nevaluated on a test set of 376 instances using character-level precision,\nrecall, and F1-score. We compared its performance against Microsoft Azure NER,\nMicrosoft Presidio, and zero-shot prompting with Gemini-Pro-2.5 and\nLlama-3.3-70B-Instruct. The fine-tuned GLiNER model achieved superior\nperformance, with an overall micro-average F1-score of 0.980, significantly\noutperforming Gemini-Pro-2.5 (F1-score: 0.845). LOGICAL correctly sanitised 95%\nof documents completely, compared to 64% for the next-best solution. The model\noperated efficiently on a standard laptop without a dedicated GPU. However, a\n2% entity-level false negative rate underscores the need for human-in-the-loop\nvalidation across all tested systems. Fine-tuned, specialised transformer\nmodels like GLiNER offer an accurate, computationally efficient, and secure\nsolution for PII removal from clinical notes. This \"sanitisation at the source\"\napproach is a practical alternative to resource-intensive LLMs, enabling the\ncreation of de-identified datasets for research and AI development while\npreserving data privacy, particularly in resource-constrained environments.", "AI": {"tldr": "LOGICAL系统使用微调过的GLiNER模型高效地移除EHR中的PII，性能超越了许多常见的模型，并特别适合资源受限的环境。", "motivation": "大型语言模型虽然强大，但其计算成本高且API服务的数据隐私风险使得它在低资源环境中难以应用。为了应对这些问题，本文提出了一种高效的、可本地部署的PII移除系统。", "method": "开发了LOGICAL系统，基于微调过的Generalist和轻量级命名实体识别(GLiNER)模型，用于有效且本地部署的个人识别信息(PII)移除系统。使用了精神病院EHR系统的1515份临床文档，并定义了九个需要移除的PII类别。使用modern-gliner-bi-large-v1.0模型对2849个文本实例进行了微调，并在包含376个实例的测试集上进行了评估，使用字符级别的精准度、召回率和F1分数。", "result": "与Microsoft Azure NER、Microsoft Presidio以及Gemini-Pro-2.5和Llama-3.3-70B-Instruct的零样本提示相比，微调后的GLiNER模型表现更佳，总体微平均F1值达到了0.980，相比Gemini-Pro-2.5（F1值: 0.845）有显著提高。LOGICAL系统完全准确地处理了95%的文档，比次优解决方案的准确率高出了超过30%。", "conclusion": "微调专用的转换器模型如GLiNER提供了一种准确、计算效率高且安全的解决方案，用于从临床记录中移除个人识别信息，特别是在资源有限的环境中，能够创建用于研究和AI开发的去标识数据集，同时保护数据隐私。"}}
{"id": "2510.19321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19321", "abs": "https://arxiv.org/abs/2510.19321", "authors": ["Hai-jie Yuan", "Heng Zhang", "Fei Yin"], "title": "Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer", "comment": null, "summary": "Handwritten signature verification is a crucial aspect of identity\nauthentication, with applications in various domains such as finance and\ne-commerce. However, achieving high accuracy in signature verification remains\nchallenging due to intra-user variability and the risk of forgery. This paper\nintroduces a novel approach for dynamic signature verification: the\nTemporal-Spatial Graph Attention Transformer (TS-GATR). TS-GATR combines the\nGraph Attention Network (GAT) and the Gated Recurrent Unit (GRU) to model both\nspatial and temporal dependencies in signature data. TS-GATR enhances\nverification performance by representing signatures as graphs, where each node\ncaptures dynamic features (e.g. position, velocity, pressure), and by using\nattention mechanisms to model their complex relationships. The proposed method\nfurther employs a Dual-Graph Attention Transformer (DGATR) module, which\nutilizes k-step and k-nearest neighbor adjacency graphs to model local and\nglobal spatial features, respectively. To capture long-term temporal\ndependencies, the model integrates GRU, thereby enhancing its ability to learn\ndynamic features during signature verification. Comprehensive experiments\nconducted on benchmark datasets such as MSDS and DeepSignDB show that TS-GATR\nsurpasses current state-of-the-art approaches, consistently achieving lower\nEqual Error Rates (EER) across various scenarios.", "AI": {"tldr": "提出了一种用于动态手写签名验证的新方法 TS-GATR，其结合了图注意力网络 (GAT) 和门控循环单元 (GRU)，在多个基准测试中显示出了优于当前最先进方法的性能。", "motivation": "解决手写签名验证中的高精度实现难题，尤其针对同一用户之间的变化性和伪造风险。", "method": "TS-GATR 结合图注意力网络 (GAT) 和门控循环单元 (GRU) 来建模签名数据中的空间和时间依赖性。该方法通过将签名表示为图，其中每个节点捕捉动态特征（如位置、速度、压力），并使用注意力机制来建模复杂的特征关系。进一步通过 Dual-Graph 注意力转换器 (DGATR) 模块，利用 k 步长和 k 最近邻邻接图来分别建模局部和全局的空间特征。为了捕捉长期时间依赖性，模型集成了 GRU。", "result": "在基准数据集如 MSDS 和 DeepSignDB 上进行的全面实验表明，TS-GATR 在多种情况下的一致性结果显示优于当前最先进的方法，实现更低的等错误率 (EER)。", "conclusion": "TS-GATR 方法在手写签名验证中展现出优于现有方法的性能表现。"}}
{"id": "2510.19350", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19350", "abs": "https://arxiv.org/abs/2510.19350", "authors": ["Varsha Suresh", "M. Hamza Mughal", "Christian Theobalt", "Vera Demberg"], "title": "Modeling Turn-Taking with Semantically Informed Gestures", "comment": null, "summary": "In conversation, humans use multimodal cues, such as speech, gestures, and\ngaze, to manage turn-taking. While linguistic and acoustic features are\ninformative, gestures provide complementary cues for modeling these\ntransitions. To study this, we introduce DnD Gesture++, an extension of the\nmulti-party DnD Gesture corpus enriched with 2,663 semantic gesture annotations\nspanning iconic, metaphoric, deictic, and discourse types. Using this dataset,\nwe model turn-taking prediction through a Mixture-of-Experts framework\nintegrating text, audio, and gestures. Experiments show that incorporating\nsemantically guided gestures yields consistent performance gains over\nbaselines, demonstrating their complementary role in multimodal turn-taking.", "AI": {"tldr": "研究证实在轮流对话预测中加入语义指导手势带来的性能提升。", "motivation": "人类在对话中利用多种模态线索（如语言、手势、注视）来管理轮流对话。尽管语言和声学特征是有用的，但手势提供了用于建模这些转换的互补线索。", "method": "使用扩展的多党DnD Gesture语料库，该语料库增添了2,663个手势的语义标注，通过融合文本、音频和手势的专家混合模型来研究轮流对话的预测。", "result": "研究通过引入DnD Gesture++语料库扩展，该语料库包含了2,663个手势的语义标注，涵盖象征性、比喻性、指示性和话语类型的手势，展示了手势在轮流对话预测中的补充作用。实验表明，与基线模型相比，融合语义指导手势的多通道方法取得了持续的性能提升。", "conclusion": "该研究证明了语义指导手势在多通道轮流对话中的互补作用，且其在预测模型中的引入带来了一致的性能改进。"}}
