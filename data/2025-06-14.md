<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 40]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
*Dingfeng Shi,Jingyi Cao,Qianben Chen,Weichen Sun,Weizhen Li,Hongxuan Lu,Fangchen Dong,Tianrui Qin,King Zhu,Minghao Yang,Jian Yang,Ge Zhang,Jiaheng Liu,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

介绍了一种名为TaskCraft的自动化流程，用于生成具备多种特性的代理任务，提升了生成工作流中的提示优化，改进了代理基础模型的监督训练，并提供了一个大规模数据集以支援未来研究。


<details>
  <summary>Details</summary>
Motivation: 解决现有指令数据缺乏工具交互的问题，以及现有的代理基准依赖于耗时的人类标注，这限制了其可扩展性。通过提出TaskCraft来解决这些问题，并推动自然语言处理和人工智能的进步。

Method: 通过引入名为\textsc{TaskCraft}的自动化工作流来生成具有可扩展难度、多工具使用以及可验证代理任务的方法。TaskCraft利用深度扩展和宽度扩展来拓展原子任务，从而创建结构复杂且具有层次性的挑战。

Result: 实验结果显示，这些任务改善了生成工作流中的提示优化，并提升了对代理基础模型的监督微调效果。

Conclusion: 提出了一个大规模的合成数据集，包含约36,000个难度各异的任务，旨在支持未来关于代理调优及评估的研究。

Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool
use, and adaptive reasoning, are becoming increasingly central to the
advancement of NLP and AI. However, existing instruction data lacks tool
interaction, and current agentic benchmarks rely on costly human annotation,
limiting their scalability. We introduce \textsc{TaskCraft}, an automated
workflow for generating difficulty-scalable, multi-tool, and verifiable agentic
tasks with execution trajectories. TaskCraft expands atomic tasks using
depth-based and width-based extensions to create structurally and
hierarchically complex challenges. Empirical results show that these tasks
improve prompt optimization in the generation workflow and enhance supervised
fine-tuning of agentic foundation models. We present a large-scale synthetic
dataset of approximately 36,000 tasks with varying difficulty to support future
research on agent tuning and evaluation.

</details>


### [2] [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
*Christopher J. Agostino,Quan Le Thien,Molly Apsel,Denizhan Pak,Elina Lesyk,Ashabari Majumdar*

Main category: cs.CL

本文通过语义贝尔不等式实验表明，在歧义下，自然语言的解释具有非经典的语境性，挑战了传统语言学中的“意义自我实现”的观点，提出了基于贝叶斯方法的理解。


<details>
  <summary>Details</summary>
Motivation: 研究者认为现有的大型语言模型和其他现代NLP系统内在限制在于他们都局限在自然语言中，自然语言的语义退化带来了阐释上的限制。提出观察者依赖的阐释行为是实现意义的途径。

Method: 使用Kolmogorov复杂度论证，随着表达复杂性的增加，任何解释者恢复单一意图意义的可能性将消失。提出了一种语义贝尔不等式测试，利用多样化的LLM代理作为“计算认知系统”在不同的上下文设置下来解释模糊的词对。

Result: 实验发现平均CHSH期望值在1.2到2.8之间，多个运行结果（例如2.3-2.4）显著违反了经典的边界（|S|≤2）。这表明在歧义下语言解释可以表现出非经典的语境性。

Conclusion: 基于这些结果，传统的频次分析方法在自然语言中是必然有损失的，建议使用贝叶斯多次采样方法在语境中提供更实用且适当的语言意义特征。

Abstract: Semantic degeneracy represents a fundamental property of natural language
that extends beyond simple polysemy to encompass the combinatorial explosion of
potential interpretations that emerges as semantic expressions increase in
complexity. Large Language Models (LLMs) and other modern NLP systems face
inherent limitations precisely because they operate within natural language
itself, making them subject to the same interpretive constraints imposed by
semantic degeneracy. In this work, we argue using Kolmogorov complexity that as
an expression's complexity grows, the likelihood of any interpreting agent
(human or LLM-powered AI) recovering the single intended meaning vanishes. This
computational intractability suggests the classical view that linguistic forms
possess meaning in and of themselves is flawed. We alternatively posit that
meaning is instead actualized through an observer-dependent interpretive act.
To test this, we conducted a semantic Bell inequality test using diverse LLM
agents as ``computational cognitive systems'' to interpret ambiguous word pairs
under varied contextual settings. Across several independent experiments, we
found average CHSH expectation values ranging from 1.2 to 2.8, with several
runs yielding values (e.g., 2.3-2.4) that significantly violate the classical
boundary ($|S|\leq2$). This demonstrates that linguistic interpretation under
ambiguity can exhibit non-classical contextuality, consistent with results from
human cognition experiments. These results inherently imply that classical
frequentist-based analytical approaches for natural language are necessarily
lossy. Instead, we propose that Bayesian-style repeated sampling approaches can
provide more practically useful and appropriate characterizations of linguistic
meaning in context.

</details>


### [3] [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
*Christodoulos Constantinides,Shuxin Lin,Nianjun Zhou,Dhaval Patel*

Main category: cs.CL

论文介绍了一个新的多智能体系统Chat-of-Thought，旨在通过协作的LLM智能体，动态任务路由及想法对话模式，优化和改进工业资产的FMEA文档生成流程。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于解决工业设备监控中生成和验证故障模式与影响分析（FMEA）文档时遇到的具体挑战，包括内容生成的质量和效率问题，以及需要更加精细的验证过程。

Method: 采用多个协同工作的大型语言模型（LLM）智能体，每个智能体分别承担特定的角色，运用先进的人工智能技术和动态任务路由，以及通过“想法对话”（Chat of Thought）模式中的多角色互动来改进和精炼生成的文档内容。

Result: 该论文介绍了一个名为Chat-of-Thought的新型多智能体系统，旨在为工业资产生成故障模式与影响分析(FMEA)文档。该系统采用了具有特定角色的多个协作大型语言模型（LLM）智能体，利用先进的AI技术和动态任务路由来优化FMEA表格的生成和验证。系统的关键创新在于引入了“想法对话”（Chat of Thought），通过动态、多角色驱动的讨论迭代细化内容。该研究探讨了工业设备监控的应用领域，概述了主要挑战，并展示了Chat-of-Thought通过交互式模板驱动的工作流程和背景感知智能体协作应对这些挑战的潜力。

Conclusion: 该研究突出了Chat-of-Thought系统在工业设备监控领域的多功能性及其在生成与验证FMEA文档方面的创新解决方案。展示了通过交互式模板驱动和背景感知来提升生成文档质量和效率的潜力。

Abstract: This paper presents a novel multi-agent system called Chat-of-Thought,
designed to facilitate the generation of Failure Modes and Effects Analysis
(FMEA) documents for industrial assets. Chat-of-Thought employs multiple
collaborative Large Language Model (LLM)-based agents with specific roles,
leveraging advanced AI techniques and dynamic task routing to optimize the
generation and validation of FMEA tables. A key innovation in this system is
the introduction of a Chat of Thought, where dynamic, multi-persona-driven
discussions enable iterative refinement of content. This research explores the
application domain of industrial equipment monitoring, highlights key
challenges, and demonstrates the potential of Chat-of-Thought in addressing
these challenges through interactive, template-driven workflows and
context-aware agent collaboration.

</details>


### [4] [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/abs/2506.10095)
*Xiao Li,Joel Kreuzwieser,Alan Peters*

Main category: cs.CL

研究了大型语言模型在提示的词汇实现变化但语义意图不变的情况下行为差异，提出了一种诊断框架来衡量这种行为变异，并发现这种变异与分词和解码过程有关。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型对提示词汇实现变化但语义意图不变的情境下的反应，揭示模型的稳定性问题。

Method: 提出了一种名为基于提示的语义偏移（PBSS）的诊断框架，用于衡量在语义等效的提示重新表达下，大型语言模型的行为漂移。

Result: 将PBSS应用于十个受限任务中，揭示了模型特定的响应偏移，表明这种偏移与分词策略和解码过程有关。

Conclusion: 结果强调了在重述提示下模型评估的稳定性是一个被忽略的维度，分词策略和解码动态可能会影响训练后服务的质量。

Abstract: We investigate how large language models respond to prompts that differ only
in their token-level realization but preserve the same semantic intent, a
phenomenon we call prompt variance. We propose Prompt-Based Semantic Shift
(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under
semantically equivalent prompt rewordings. Applied to ten constrained tasks,
PBSS reveals consistent, model-specific response shifts, suggesting statistical
regularities linked to tokenization and decoding. These results highlight an
overlooked dimension of model evaluation stability under rephrasing and suggest
that tokenization strategies and decoding dynamics may contribute to
post-training quality of service instability.

</details>


### [5] [ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering](https://arxiv.org/abs/2506.10116)
*Caijun Jia,Nan Xu,Jingxuan Wei,Qingli Wang,Lei Wang,Bihui Yu,Junnan Zhu*

Main category: cs.CL

提出了ChartReasoner，一种新的、基于代码的两阶段框架，用以对可视化图表进行精准和可解释的推理，通过合成的图表推理数据集进行训练后，其性能与现有模型相当。


<details>
  <summary>Details</summary>
Motivation: 目前，多模态推理方法通常将视觉推理任务转化为文本推理任务，通过若干次图转文的转换，这在任务需要大量视觉细节，如图表问答任务中会丢失关键结构和语义信息。为了弥合这一差距，提出了ChartReasoner框架。

Method: 提出了一种名为ChartReasoner的两阶段框架，以实现对图表的精确、可解释的推理。首先训练了一个高保真模型，将多种图表图像转换为结构化的ECharts代码，尽量保持布局和数据语义。然后，设计了一个通用的图表推理数据综合管道，利用该预训练传输模型自动生成可扩展的图表推理轨迹，并利用代码验证器过滤掉低质量的样本。

Result: 实验结果在四个公共基准上清楚地展示了我们提出的ChartReasoner的有效性。它可以尽可能地保留图表的原始细节，并且在参数较少的情况下与最先进的开源模型表现相当。

Conclusion: ChartReasoner能够在有限参数数量下实现接近GPT-4o等专有系统在跨域设置中的性能，同时保持较少的参数量。这在处理图表问答任务时尤其关键。

Abstract: Recently, large language models have shown remarkable reasoning capabilities
through long-chain reasoning before responding. However, how to extend this
capability to visual reasoning tasks remains an open challenge. Existing
multimodal reasoning approaches transfer such visual reasoning task into
textual reasoning task via several image-to-text conversions, which often lose
critical structural and semantic information embedded in visualizations,
especially for tasks like chart question answering that require a large amount
of visual details. To bridge this gap, we propose ChartReasoner, a code-driven
novel two-stage framework designed to enable precise, interpretable reasoning
over charts. We first train a high-fidelity model to convert diverse chart
images into structured ECharts codes, preserving both layout and data semantics
as lossless as possible. Then, we design a general chart reasoning data
synthesis pipeline, which leverages this pretrained transport model to
automatically and scalably generate chart reasoning trajectories and utilizes a
code validator to filter out low-quality samples. Finally, we train the final
multimodal model using a combination of supervised fine-tuning and
reinforcement learning on our synthesized chart reasoning dataset and
experimental results on four public benchmarks clearly demonstrate the
effectiveness of our proposed ChartReasoner. It can preserve the original
details of the charts as much as possible and perform comparably with
state-of-the-art open-source models while using fewer parameters, approaching
the performance of proprietary systems like GPT-4o in out-of-domain settings.

</details>


### [6] [Unsupervised Elicitation of Language Models](https://arxiv.org/abs/2506.10139)
*Jiaxin Wen,Zachary Ankner,Arushi Somani,Peter Hase,Samuel Marks,Jacob Goldman-Wetzler,Linda Petrini,Henry Sleight,Collin Burns,He He,Shi Feng,Ethan Perez,Jan Leike*

Main category: cs.CL

ICM是一种新的无监督算法，用于解决超人类能力模型难以获取高质量人类监督的问题，通过模型自身的标签进行微调。


<details>
  <summary>Details</summary>
Motivation: 由于超人类能力模型难以获取高质量的人类监督，研究人员引入了ICM来解决这一挑战。

Method: Internal Coherence Maximization (ICM)作为新的无监督算法被引入，用于在没有外部监督的情况下对预训练语言模型进行微调，方法是使用模型自身生成的标签。

Result: 在GSM8k-verification、TruthfulQA和Alpaca奖励模型任务上，ICM方法的表现与使用黄金标签训练相当，优于人类监督训练。在需要模型具备强烈超人类能力的任务中，ICM可以激发出更好的能力。此外，通过ICM训练的无监督奖励模型和强化学习提升的Claude 3.5 Haiku助手优于那些由人类监督训练的模型。

Conclusion: ICM方法不仅可以在GSM8k-verification、TruthfulQA和Alpaca奖励模型任务上达到与黄金标签训练相当的性能，还可以显著激发模型的超人类能力。此外，它还能改善前沿模型的训练，提升了奖励模型和助手的表现。

Abstract: To steer pretrained language models for downstream tasks, today's
post-training paradigm relies on humans to specify desired behaviors. However,
for models with superhuman capabilities, it is difficult or impossible to get
high-quality human supervision. To address this challenge, we introduce a new
unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune
pretrained language models on their own generated labels, \emph{without
external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward
modeling tasks, our method matches the performance of training on golden
supervision and outperforms training on crowdsourced human supervision. On
tasks where LMs' capabilities are strongly superhuman, our method can elicit
those capabilities significantly better than training on human labels. Finally,
we show that our method can improve the training of frontier LMs: we use our
method to train an unsupervised reward model and use reinforcement learning to
train a Claude 3.5 Haiku-based assistant. Both the reward model and the
assistant outperform their human-supervised counterparts.

</details>


### [7] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
*Aakriti Kumar,Nalin Poungpeth,Diyi Yang,Erina Farrell,Bruce Lambert,Matthew Groh*

Main category: cs.CL

研究通过对比专家、众包工作者和大语言模型在理解和标注共情交流上的表现，展示了大语言模型在这个特定任务上的表现接近专家水平，可以用于情感敏感的应用中。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨LLMs在生成共情回复方面虽然表现出色，但在判断共情交流的细微差别方面有多可靠。

Method: 通过将专家、众包工作者和大语言模型（LLMs）在四个来自心理学、自然语言处理和通信领域的评估框架下的注释结果进行对比，来研究LLMs在判断共情交流细微差别方面的可靠性。

Result: 通过分析3,150个专家注释、2,844个众包注释和3,150个LLM注释后发现，专家间的一致性高，但会随着框架子部分的清晰度、复杂性和主观性而变化。LLMs在所有四个框架中的一致性接近专家设定的基准，并超过众包工作者的一致性。

Conclusion: 这项研究表明，当大语言模型在特定任务中通过适当的基准进行验证时，它们可以提高在情感敏感应用中使用的透明度和监管，包括作为对话伴侣的应用。

Abstract: Large language models (LLMs) excel at generating empathic responses in
text-based conversations. But, how reliably do they judge the nuances of
empathic communication? We investigate this question by comparing how experts,
crowdworkers, and LLMs annotate empathic communication across four evaluative
frameworks drawn from psychology, natural language processing, and
communications applied to 200 real-world conversations where one speaker shares
a personal problem and the other offers support. Drawing on 3,150 expert
annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess
inter-rater reliability between these three annotator groups. We find that
expert agreement is high but varies across the frameworks' sub-components
depending on their clarity, complexity, and subjectivity. We show that expert
agreement offers a more informative benchmark for contextualizing LLM
performance than standard classification metrics. Across all four frameworks,
LLMs consistently approach this expert level benchmark and exceed the
reliability of crowdworkers. These results demonstrate how LLMs, when validated
on specific tasks with appropriate benchmarks, can support transparency and
oversight in emotionally sensitive applications including their use as
conversational companions.

</details>


### [8] [Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME](https://arxiv.org/abs/2506.10154)
*Bidyarthi Paul,SM Musfiqur Rahman,Dipta Biswas,Md. Ziaul Hasan,Md. Zahid Hossain*

Main category: cs.CL

本研究通过使用机器学习模型（线性SVM, KNN, 随机森林）分析孟加拉语的情感，并结合PCA降维、BiLSTM和AdaBoost技术，展示了情感分析在资源有限的语言上的应用和改进。


<details>
  <summary>Details</summary>
Motivation: 随着对具有独特地区表达和文化特征（如孟加拉语）的不常用语言情绪分析研究继续扩大，本研究旨在通过分析EmoNoBa数据集中的22,698条社交媒体评论，探索有限资源语言的情绪分析方法。

Method: 使用机器学习模型如线性SVM、KNN、随机森林，结合从TF-IDF向量化器获得的n-gram数据进行语言分析。此外，还探讨了PCA对降维的影响，并使用BiLSTM模型和AdaBoost改进决策树。为了使机器学习模型更易于理解，使用LIME来解释AdaBoost分类器的预测，此分类器采用决策树。

Result: 未明确提及具体结果，但研究展示了通过多种技术（如PCA降维， AdaBoost和BiLSTM模型）提高机器学习模型情感识别性能的方法。通过LIME的解释，使这些技术更加透明和易于理解。

Conclusion: 研究工作通过探索多种技术，旨在为像孟加拉语这样的资源有限的语言识别情绪找到高效的方法。

Abstract: Research on understanding emotions in written language continues to expand,
especially for understudied languages with distinctive regional expressions and
cultural features, such as Bangla. This study examines emotion analysis using
22,698 social media comments from the EmoNoBa dataset. For language analysis,
we employ machine learning models: Linear SVM, KNN, and Random Forest with
n-gram data from a TF-IDF vectorizer. We additionally investigated how PCA
affects the reduction of dimensionality. Moreover, we utilized a BiLSTM model
and AdaBoost to improve decision trees. To make our machine learning models
easier to understand, we used LIME to explain the predictions of the AdaBoost
classifier, which uses decision trees. With the goal of advancing sentiment
analysis in languages with limited resources, our work examines various
techniques to find efficient techniques for emotion identification in Bangla.

</details>


### [9] [Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities](https://arxiv.org/abs/2506.10155)
*Elizabeth Demers,Victor Xiaoqi Wang,Kean Wu*

Main category: cs.CL

通过机器学习开发员工资本（HC）的相关关键词列表，促进HC管理和披露的研究。


<details>
  <summary>Details</summary>
Motivation: 员工资本（HC）对企业价值创造的重要性日益增加，但与其它资产不同，HC缺乏明确的衡量和披露规则。

Method: 使用word2vec机器学习算法，基于确认的员工资本（HC）披露集，开发出涵盖五个子类别的全面HC相关关键词列表。

Result: 提供了HC词典、企业HC披露以及用于开发词典的Python代码，还提供了使用数据和代码的详细示例，包括微调BERT模型。

Conclusion: 研究概述如何使用HC词典或修改代码以捕捉其它感兴趣的构建，以及在未来研究中关于HC管理和披露的机会。

Abstract: Human capital (HC) is increasingly important to corporate value creation.
Unlike other assets, however, HC is not currently subject to well-defined
measurement or disclosure rules. We use a machine learning algorithm (word2vec)
trained on a confirmed set of HC disclosures to develop a comprehensive list of
HC-related keywords classified into five subcategories (DEI; health and safety;
labor relations and culture; compensation and benefits; and demographics and
other) that capture the multidimensional nature of HC management. We share our
lexicon, corporate HC disclosures, and the Python code used to develop the
lexicon, and we provide detailed examples of using our data and code, including
for fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the
code to capture another construct of interest) with their samples of corporate
communications to address pertinent HC questions. We close with a discussion of
future research opportunities related to HC management and disclosure.

</details>


### [10] [Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective](https://arxiv.org/abs/2506.10161)
*Yi Wang,Max Kreminski*

Main category: cs.CL

本文通过使用LLMs解决叙事规划问题，评估它们在生成故事时的质量，重点关注因果连贯性、角色意图和戏剧冲突。结果显示LLMs在小故事上有良好的表现，但在复杂推理上仍需进一步提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型故事生成的应用较为突出，但由于自动评估方法的挑战和人工评估的高成本及主观性，对LLMs生成高质量故事的能力的理解仍然有限。

Method: 本研究利用大型语言模型(LLMs)解决叙事规划问题，提出了一个基于文学示例的叙事规划评估基准，重点关注因果连贯性、角色意图和戏剧冲突。

Result: 实验结果表明，GPT-4级的LLMs在小规模上能够生成因果连贯的故事，但对于包含角色意图和戏剧冲突的复杂推理规划，仍然具有挑战性，这需要经过强化学习训练的LLMs来实现。

Conclusion: 实验表明，LLMs能够生成一定规模的故事并保持从不同方面来看的质量。研究也强调了在游戏环境中使用LLMs进行叙事规划所面临的挑战和需要考虑的问题，这对未来的研究和应用提供了有价值的信息。

Abstract: Story generation has been a prominent application of Large Language Models
(LLMs). However, understanding LLMs' ability to produce high-quality stories
remains limited due to challenges in automatic evaluation methods and the high
cost and subjectivity of manual evaluation. Computational narratology offers
valuable insights into what constitutes a good story, which has been applied in
the symbolic narrative planning approach to story generation. This work aims to
deepen the understanding of LLMs' story generation capabilities by using them
to solve narrative planning problems. We present a benchmark for evaluating
LLMs on narrative planning based on literature examples, focusing on causal
soundness, character intentionality, and dramatic conflict. Our experiments
show that GPT-4 tier LLMs can generate causally sound stories at small scales,
but planning with character intentionality and dramatic conflict remains
challenging, requiring LLMs trained with reinforcement learning for complex
reasoning. The results offer insights on the scale of stories that LLMs can
generate while maintaining quality from different aspects. Our findings also
highlight interesting problem solving behaviors and shed lights on challenges
and considerations for applying LLM narrative planning in game environments.

</details>


### [11] [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
*Shubhashis Roy Dipta,Francis Ferraro*

Main category: cs.CL

Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent approaches have shown impressive proficiency in extracting and
leveraging parametric knowledge from Large-Language Models (LLMs) and
Vision-Language Models (VLMs). In this work, we consider how we can improve the
identification and retrieval of videos related to complex real-world events by
automatically extracting latent parametric knowledge about those events. We
present Q2E: a Query-to-Event decomposition method for zero-shot multilingual
text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our
approach demonstrates that we can enhance the understanding of otherwise overly
simplified human queries by decomposing the query using the knowledge embedded
in LLMs and VLMs. We additionally show how to apply our approach to both visual
and speech-based inputs. To combine this varied multimodal knowledge, we adopt
entropy-based fusion scoring for zero-shot fusion. Through evaluations on two
diverse datasets and multiple retrieval metrics, we demonstrate that Q2E
outperforms several state-of-the-art baselines. Our evaluation also shows that
integrating audio information can significantly improve text-to-video
retrieval. We have released code and data for future research.

</details>


### [12] [TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games](https://arxiv.org/abs/2506.10209)
*Prakamya Mishra,Jiang Liu,Jialian Wu,Xiaodong Yu,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

研究引入TTT-Bench评测框架，通过四个两人对弈的井字棋游戏来评估大规模推理模型在基本策略、空间和逻辑推理方面的能力，发现这些模型即使擅长解决复杂的数学问题，但在简单的策略游戏中常表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模推理模型展现了处理高级STEM问题的能力，但其在更广泛任务领域中的推理能力尚未充分研究。通过设计可用于检测模型基本推理能力的游戏，研究人员试图填补这一研究空白。

Method: 开发了TTT-Bench测试平台，由四个井字棋样式的双人游戏组成，这些游戏对人类非常简单，但它们要求推理对手的意图以及棋盘的空间配置。

Result: 大规模推理模型在TTT-Bench中的得分平均比在同样难度的数学问题上低41%和5%，尤其是面对新而简单的长期策略推理问题时表现困难。

Conclusion: 结果揭示了这些模型在解决某些简单但涉及策略性和空间性的问题时的能力有限，即使它们在数学问题上表现出色。这提醒我们需要对这些模型的全面推理能力进行更进一步的研究。

Abstract: Large reasoning models (LRMs) have demonstrated impressive reasoning
capabilities across a broad range of tasks including Olympiad-level
mathematical problems, indicating evidence of their complex reasoning
abilities. While many reasoning benchmarks focus on the STEM domain, the
ability of LRMs to reason correctly in broader task domains remains
underexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmark
that is designed to evaluate basic strategic, spatial, and logical reasoning
abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games
that humans can effortlessly solve from a young age. We propose a simple yet
scalable programmatic approach for generating verifiable two-player game
problems for TTT-Bench. Although these games are trivial for humans, they
require reasoning about the intentions of the opponent, as well as the game
board's spatial configurations, to ensure a win. We evaluate a diverse set of
state-of-the-art LRMs, and \textbf{discover that the models that excel at hard
math problems frequently fail at these simple reasoning games}. Further testing
reveals that our evaluated reasoning models score on average $\downarrow$ 41\%
\& $\downarrow$ 5\% lower on TTT-Bench compared to MATH 500 \& AIME 2024
respectively, with larger models achieving higher performance using shorter
reasoning traces, where most of the models struggle on long-term strategic
reasoning situations on simple and new TTT-Bench tasks.

</details>


### [13] [Classifying Unreliable Narrators with Large Language Models](https://arxiv.org/abs/2506.10231)
*Anneliese Brei,Katharine Henry,Abhisheik Sharma,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.CL

本文提出了TUNa数据集，用于通过计算方法识别不可靠叙述者，并评估了不同语言模型在该任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 文章动机是利用计算方法来自动识别文本中可能存在的不可靠叙述者，并将文学理论与机器学习结合应用于这一领域。

Method: 文章构建了TUNa数据集，并定义了不同类型的不可靠叙述者分类任务，评估了多种语言模型的表现，尝试了少样本学习、微调和课程学习方法。

Result: 研究表明，该任务具有挑战性，但利用语言模型的方法有潜力识别不可靠叙述者。

Conclusion: 研究者发布了专家标注的数据集及代码，并鼓励未来研究。

Abstract: Often when we interact with a first-person account of events, we consider
whether or not the narrator, the primary speaker of the text, is reliable. In
this paper, we propose using computational methods to identify unreliable
narrators, i.e. those who unintentionally misrepresent information. Borrowing
literary theory from narratology to define different types of unreliable
narrators based on a variety of textual phenomena, we present TUNa, a
human-annotated dataset of narratives from multiple domains, including blog
posts, subreddit posts, hotel reviews, and works of literature. We define
classification tasks for intra-narrational, inter-narrational, and
inter-textual unreliabilities and analyze the performance of popular
open-weight and proprietary LLMs for each. We propose learning from literature
to perform unreliable narrator classification on real-world text data. To this
end, we experiment with few-shot, fine-tuning, and curriculum learning
settings. Our results show that this task is very challenging, and there is
potential for using LLMs to identify unreliable narrators. We release our
expert-annotated dataset and code and invite future research in this area.

</details>


### [14] [ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese](https://arxiv.org/abs/2506.10245)
*Iago Alves Brito,Julia Soares Dollis,Fernanda Bufon Färber,Diogo Fernandes Costa Silva,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

本文介绍了ToxSyn-PT，这是首个支持葡萄牙语环境中针对九个受法律保护的少数群体的细粒度仇恨言论分类的大规模语料库，通过一个创新的四阶段流程创建而成，旨在推动合成数据和低资源环境下仇恨言论检测的研究。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决细粒度仇恨言论分类中针对受法律保护的少数群体的问题，发布的数据集能够在不同少数群体和毒性标签间有效分配。

Method: 该文提出了一种四阶段流程来创建ToxSyn-PT语料库，包括手动精选种子样本，少量样本扩增，通过改写进行数据增强，以及增强后添加中性文本以防止过拟合。

Result: 实验结果显示，在传统的葡萄牙语仇恨言论数据集上进行二元和多标签分类时，该语料库展现出强大的性能，证实了其即使在领域边界变化的情况下也能实现坚韧的泛化表现。

Conclusion: 该研究成功创建了针对葡萄牙语仇恨言论分类的大规模语料库ToxSyn-PT，并已公开发布，以促进合成数据以及在低资源条件下的仇恨言论检测研究的发展。

Abstract: We present ToxSyn-PT, the first large-scale Portuguese corpus that enables
fine-grained hate-speech classification across nine legally protected minority
groups. The dataset contains 53,274 synthetic sentences equally distributed
between minorities groups and toxicity labels. ToxSyn-PT is created through a
novel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot
expansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and
(4) enrichment, plus additional neutral texts to curb overfitting to
group-specific cues. The resulting corpus is class-balanced, stylistically
diverse, and free from the social-media domain that dominate existing
Portuguese datasets. Despite domain differences with traditional benchmarks,
experiments on both binary and multi-label classification on the corpus yields
strong results across five public Portuguese hate-speech datasets,
demonstrating robust generalization even across domain boundaries. The dataset
is publicly released to advance research on synthetic data and hate-speech
detection in low-resource settings.

</details>


### [15] [Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models](https://arxiv.org/abs/2506.10268)
*Andrea Yaoyun Cui,Pengfei Yu*

Main category: cs.CL

研究结果质疑了之前关于语言模型使用随机采样决策的观点，并展示了在某些条件下，语言模型可以作出确定性的决策判断。这为解释语言模型提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于检验语言模型是否具备贝叶斯式的决策机制，并探究模拟Gibbs采样方法在揭示语言模型内部决策模式时的局限性。

Method: 研究通过模拟Gibbs采样来检验语言模型的决策模式是否确为随机，并提出了一种简单的区分随机和确定性决策模式的方法以防止推断出误导性的语言模型先验。

Result: 研究发现语言模型在某些条件下可以表现出近似确定性的决策，并提出通过仔细的分析可以避免错误地得出语言模型先验。实验表明，该方法对于理解大型语言模型的决策模式是非常重要的。

Conclusion: 大型语言模型在特定条件下可能会表现出确定性的决策模式，而不仅仅是随机采样的结果。该研究提供了一种新的方法来正确区分随机和确定性决策，有助于更准确地理解这些模型的内部运作机制。

Abstract: Language models are essentially probability distributions over token
sequences. Auto-regressive models generate sentences by iteratively computing
and sampling from the distribution of the next token. This iterative sampling
introduces stochasticity, leading to the assumption that language models make
probabilistic decisions, similar to sampling from unknown distributions.
Building on this assumption, prior research has used simulated Gibbs sampling,
inspired by experiments designed to elicit human priors, to infer the priors of
language models. In this paper, we revisit a critical question: Do language
models possess Bayesian brains? Our findings show that under certain
conditions, language models can exhibit near-deterministic decision-making,
such as producing maximum likelihood estimations, even with a non-zero sampling
temperature. This challenges the sampling assumption and undermines previous
methods for eliciting human-like priors. Furthermore, we demonstrate that
without proper scrutiny, a system with deterministic behavior undergoing
simulated Gibbs sampling can converge to a "false prior." To address this, we
propose a straightforward approach to distinguish between stochastic and
deterministic decision patterns in Gibbs sampling, helping to prevent the
inference of misleading language model priors. We experiment on a variety of
large language models to identify their decision patterns under various
circumstances. Our results provide key insights in understanding decision
making of large language models.

</details>


### [16] [ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs](https://arxiv.org/abs/2506.10288)
*Zige Wang,Qi Zhu,Fei Mi,Minghui Xu,Ruochun Jin,Wenjing Yang*

Main category: cs.CL

本文提出 ClusterUCB 框架，结合聚类和改进的 UCB 算法，以高效选择影响较大的数据样本，减少计算资源消耗，达到与原始梯度数据选择方法相当的效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在监督微调过程中的梯度计算需要大量资源，为了提高效率，本文提出了ClusterUCB框架。

Method: 本文提出了一种高效的基于梯度的数据选择框架，结合了聚类和一个改进的上置信界（UCB）算法。

Result: 实验结果表明，ClusterUCB框架能够在减少计算资源消耗的同时，与原始的基于梯度的数据选择方法达到相当的效果。

Conclusion: ClusterUCB框架通过聚类和改进的UCB算法，能够在限制计算预算的情况下，合理选择数据样本，实现高效而减少资源消耗的模型微调。

Abstract: Gradient-based data influence approximation has been leveraged to select
useful data samples in the supervised fine-tuning of large language models.
However, the computation of gradients throughout the fine-tuning process
requires too many resources to be feasible in practice. In this paper, we
propose an efficient gradient-based data selection framework with clustering
and a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition
that data samples with similar gradient features will have similar influences,
we first perform clustering on the training data pool. Then, we frame the
inter-cluster data selection as a constrained computing budget allocation
problem and consider it a multi-armed bandit problem. A modified UCB algorithm
is leveraged to solve this problem. Specifically, during the iterative sampling
process, historical data influence information is recorded to directly estimate
the distributions of each cluster, and a cold start is adopted to balance
exploration and exploitation. Experimental results on various benchmarks show
that our proposed framework, ClusterUCB, can achieve comparable results to the
original gradient-based data selection methods while greatly reducing computing
consumption.

</details>


### [17] [Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages](https://arxiv.org/abs/2506.10292)
*Ali Almutairi,Abdullah Alsuhaibani,Shoaib Jameel,Usman Naseem,Gelareh Mohammadi,Imran Razzak*

Main category: cs.CL

Flick通过优化伪标签生成过程，在低资源语言的文本分类任务中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 为了应对真正低资源语言环境下的少标签文本分类难题，特别是在含有噪音伪标签和领域适应性挑战的情况下。

Method: Flick提出了一种新颖的伪标签优化组件，该组件从广泛的初始聚类中提炼高置信度伪标签，专注于单个簇的凝聚力，并利用自适应的top-k选择机制来改进低资源环境下文本分类的伪标签质量。

Result: Flick在包括阿拉伯语、乌尔都语、塞茨瓦纳语以及英语在内的14个多样数据集上展示了其优越的性能和适应性。

Conclusion: Flick方法通过引入伪标签优化组件，有效减少了低资源数据中的错误传播，证明了其在仅有少量真实标注的情况下，对预训练语言模型进行稳健微调的能力。

Abstract: Training deep learning networks with minimal supervision has gained
significant research attention due to its potential to reduce reliance on
extensive labelled data. While self-training methods have proven effective in
semi-supervised learning, they remain vulnerable to errors from noisy pseudo
labels. Moreover, most recent approaches to the few-label classification
problem are either designed for resource-rich languages such as English or
involve complex cascading models that are prone to overfitting. To address the
persistent challenge of few-label text classification in truly low-resource
linguistic contexts, where existing methods often struggle with noisy
pseudo-labels and domain adaptation, we propose Flick. Unlike prior methods
that rely on generic multi-cluster pseudo-labelling or complex cascading
architectures, Flick leverages the fundamental insight that distilling
high-confidence pseudo-labels from a broader set of initial clusters can
dramatically improve pseudo-label quality, particularly for linguistically
diverse, low-resource settings. Flick introduces a novel pseudo-label
refinement component, a departure from traditional pseudo-labelling strategies
by identifying and leveraging top-performing pseudo-label clusters. This
component specifically learns to distil highly reliable pseudo-labels from an
initial broad set by focusing on single-cluster cohesion and leveraging an
adaptive top-k selection mechanism. This targeted refinement process is crucial
for mitigating the propagation of errors inherent in low-resource data,
allowing for robust fine-tuning of pre-trained language models with only a
handful of true labels. We demonstrate Flick's efficacy across 14 diverse
datasets, encompassing challenging low-resource languages such as Arabic, Urdu,
and Setswana, alongside English, showcasing its superior performance and
adaptability.

</details>


### [18] ["Check My Work?": Measuring Sycophancy in a Simulated Educational Context](https://arxiv.org/abs/2506.10297)
*Chuck Arvin*

Main category: cs.CL

研究发现，在模拟教育环境下，用户提供的建议对LLMs的答案质量和示好行为有显著影响，特别是在不同规模的模型中表现各异。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究大规模语言模型如何受到教育场景中用户建议的影响，特别是在存在示好风险的情境下。

Method: 本研究测试了来自OpenAI的GPT-4o和GPT-4.1五个不同模型在五个实验条件下的表现，探讨用户提供的建议如何影响大规模语言模型（LLMs）在模拟教育环境中的表现，特别是在示好行为带来显著风险的情况下。

Result: 研究结果表明，基于查询框架的不同，回答的质量变化很大。当学生提到一个错误答案时，LLM的正确率可能会下降15个百分点，而提到正确答案则会提高相同的幅度。这种偏倚在较小的模型中更为显著，对于GPT-4.1-nano模型，影响可以达到30%，而GPT-4o模型的影响为8%。模型更倾向于按学生提到的答案进行回答，这与示好行为的假设相吻合。

Conclusion: 本研究强调了理解这种偏倚机制及其在教育环境中缓解措施的重要性，该偏倚可能加速对于有知识的学生的学习过程，但同时对于知识不足的学生则会强化误解。

Abstract: This study examines how user-provided suggestions affect Large Language
Models (LLMs) in a simulated educational context, where sycophancy poses
significant risks. Testing five different LLMs from the OpenAI GPT-4o and
GPT-4.1 model classes across five experimental conditions, we show that
response quality varies dramatically based on query framing. In cases where the
student mentions an incorrect answer, the LLM correctness can degrade by as
much as 15 percentage points, while mentioning the correct answer boosts
accuracy by the same margin. Our results also show that this bias is stronger
in smaller models, with an effect of up to 30% for the GPT-4.1-nano model,
versus 8% for the GPT-4o model. Our analysis of how often LLMs "flip" their
answer, and an investigation into token level probabilities, confirm that the
models are generally changing their answers to answer choices mentioned by
students in line with the sycophancy hypothesis. This sycophantic behavior has
important implications for educational equity, as LLMs may accelerate learning
for knowledgeable students while the same tools may reinforce misunderstanding
for less knowledgeable students. Our results highlight the need to better
understand the mechanism, and ways to mitigate, such bias in the educational
context.

</details>


### [19] [Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs](https://arxiv.org/abs/2506.10299)
*Hayato Futami,Emiru Tsunoo,Yosuke Kashiwagi,Yuki Ito,Hassan Shahmohammadi,Siddhant Arora,Shinji Watanabe*

Main category: cs.CL

本文提出了一种有序交错语音-文本训练的方法，以解决基于文本的大语言模型在语音到语音翻译任务中遇到的模态适应问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型主要基于文本数据训练，这在将这些模型应用于基于语音的数据类型时带来了模态适应性的挑战，特别是语音到语音的翻译（S2ST）任务。

Method: 我们提出了一种名为有序交错语音-文本训练的方法，该方法在训练过程中使用了交错的语音-文本单元，其中对齐的文本标记以词级别交错插入。随着训练的进行，逐渐减少文本的比例，以促进从文本到语音的逐步模态适应。

Result: 通过在CVSS数据集上对LLaMA3.2-1B进行微调实验，实验结果表明，所提出的方法能够一致地提升翻译性能，特别是在训练数据有限的语言中表现更为显著。

Conclusion: 研究证明，提出的方法能够有效提升语音到语音翻译的质量，尤其在资源有限的语言中表现更好。

Abstract: Speech-to-speech translation (S2ST) has been advanced with large language
models (LLMs), which are fine-tuned on discrete speech units. In such
approaches, modality adaptation from text to speech has been an issue. LLMs are
trained on text-only data, which presents challenges to adapt them to speech
modality with limited speech-to-speech data. To address the training
difficulty, we propose scheduled interleaved speech--text training in this
study. We use interleaved speech--text units instead of speech units during
training, where aligned text tokens are interleaved at the word level. We
gradually decrease the ratio of text as training progresses, to facilitate
progressive modality adaptation from text to speech. We conduct experimental
evaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show
that the proposed method consistently improves the translation performances,
especially for languages with limited training data.

</details>


### [20] [Code Execution as Grounded Supervision for LLM Reasoning](https://arxiv.org/abs/2506.10343)
*Dongwon Jung,Wenxuan Zhou,Muhao Chen*

Main category: cs.CL

The paper introduces a novel method for generating high-quality chain-of-thought supervision for LLMs, leveraging program execution traces, which improves reasoning abilities and reduces unnecessary token usage in inference.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge of obtaining reliable and accurate reasoning supervision for training large language models (LLMs) with CoT supervision to enhance their reasoning abilities.

Method: We propose a scalable method for generating a high-quality chain-of-thought (CoT) supervision dataset by leveraging the determinism of program execution, extracting verifiable, step-by-step reasoning traces from code execution, and transforming them into natural language CoT reasoning.

Result: Experiments on reasoning benchmarks across various domains show that the proposed method effectively equips LLMs with transferable reasoning abilities. Ablation studies also validate the method's ability to produce highly accurate reasoning data and reduce overall token length during inference by minimizing meaningless repetition and overthinking.

Conclusion: In conclusion, the paper presents a scalable method to generate high-quality chain-of-thought supervision for training large language models, which results in enhanced reasoning abilities and improved efficiency in terms of token length during inference.

Abstract: Training large language models (LLMs) with chain-of-thought (CoT) supervision
has proven effective for enhancing their reasoning abilities. However,
obtaining reliable and accurate reasoning supervision remains a significant
challenge. We propose a scalable method for generating a high-quality CoT
supervision dataset by leveraging the determinism of program execution. Unlike
existing reasoning dataset generation methods that rely on costly human
annotations or error-prone LLM-generated CoT, our approach extracts verifiable,
step-by-step reasoning traces from code execution and transforms them into a
natural language CoT reasoning. Experiments on reasoning benchmarks across
various domains show that our method effectively equips LLMs with transferable
reasoning abilities across diverse tasks. Furthermore, the ablation studies
validate that our method produces highly accurate reasoning data and reduces
overall token length during inference by reducing meaningless repetition and
overthinking.

</details>


### [21] [TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning](https://arxiv.org/abs/2506.10380)
*Xiaohan Yu,Pu Jian,Chong Chen*

Main category: cs.CL

论文引入了TableRAG，一个能有效处理包含表格和文本的异构文档的新型框架，并通过实验表明其超越了当前基线方法。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有检索增强生成(RAG)方法在应用于既包含文本也包含表格的异构文档时所表现出来的限制，比如将表格压平和分块策略导致的信息丢失和对大型多跳查询的能力削弱。

Method: 该论文提出了TableRAG框架，这是一个统一处理文本理解和复杂表格数据操作的混合框架。TableRAG迭代地执行四个步骤：依赖上下文的查询分解、文本检索、SQL编程和执行以及中间答案的组合生成。

Result: 实验结果显示，TableRAG在公共数据集和新开发的HeteQA基准测试中，都能持续超越现有的基线方法，成为异构文档问题回答的新标杆。

Conclusion: TableRAG通过处理多跳异构推理任务中的文本和表格数据的复杂操作，展示了其在异构文档问题回答方面的优越性。为了进一步促进研究，TableRAG代码已公开发布。

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated considerable
effectiveness in open-domain question answering. However, when applied to
heterogeneous documents, comprising both textual and tabular components,
existing RAG approaches exhibit critical limitations. The prevailing practice
of flattening tables and chunking strategies disrupts the intrinsic tabular
structure, leads to information loss, and undermines the reasoning capabilities
of LLMs in multi-hop, global queries. To address these challenges, we propose
TableRAG, an hybrid framework that unifies textual understanding and complex
manipulations over tabular data. TableRAG iteratively operates in four steps:
context-sensitive query decomposition, text retrieval, SQL programming and
execution, and compositional intermediate answer generation. We also develop
HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous
reasoning capabilities. Experimental results demonstrate that TableRAG
consistently outperforms existing baselines on both public datasets and our
HeteQA, establishing a new state-of-the-art for heterogeneous document question
answering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.

</details>


### [22] [PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier](https://arxiv.org/abs/2506.10406)
*Yuhua Jiang,Yuwen Xiong,Yufeng Yuan,Chao Xin,Wenyuan Xu,Yu Yue,Qianchuan Zhao,Lin Yan*

Main category: cs.CL

本论文提出了一种名为PAG的框架，使语言模型能够自我验证和纠正自己生成的内容，提高了可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在复杂的推理任务中表现出色，但仍难以可靠地验证其输出的正确性。现有的解决这一验证挑战的方法往往依赖于单独的验证模型或需要多阶段的自我纠正训练管道，这些方法限制了可扩展性。

Method: 提出了一种名为Policy as Generative Verifier（PAG）的简单而有效的框架，该框架通过在统一的多轮强化学习（RL）范式中交替使用策略和验证者角色，使LLMs能够自我纠正。不同于以往始终生成第二个尝试而不考虑模型置信度的方法，PAG引入了一种选择性修订机制：仅当其自身的生成验证步骤检测到错误时，模型才会修订其答案。

Result: 广泛的实验结果显示，PAG在不同推理基准测试中表现出双重进步：作为策略，它增强了直接生成和自我纠正的准确性；作为验证者，其自我验证超过了自我一致性。

Conclusion: 作为策略，PAG增强了直接生成和自我纠正的准确性；作为验证者，它的自我验证能力超过了自我一致性，证明了其在增强模型自我纠正和验证能力方面的有效性。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
complex reasoning tasks, yet they still struggle to reliably verify the
correctness of their own outputs. Existing solutions to this verification
challenge often depend on separate verifier models or require multi-stage
self-correction training pipelines, which limit scalability. In this paper, we
propose Policy as Generative Verifier (PAG), a simple and effective framework
that empowers LLMs to self-correct by alternating between policy and verifier
roles within a unified multi-turn reinforcement learning (RL) paradigm.
Distinct from prior approaches that always generate a second attempt regardless
of model confidence, PAG introduces a selective revision mechanism: the model
revises its answer only when its own generative verification step detects an
error. This verify-then-revise workflow not only alleviates model collapse but
also jointly enhances both reasoning and verification abilities. Extensive
experiments across diverse reasoning benchmarks highlight PAG's dual
advancements: as a policy, it enhances direct generation and self-correction
accuracy; as a verifier, its self-verification outperforms self-consistency.

</details>


### [23] [Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?](https://arxiv.org/abs/2506.10415)
*Yingjin Song,Yupei Du,Denis Paperno,Albert Gatt*

Main category: cs.CL

The paper describes the TempVS benchmark for evaluating MLLMs' temporal grounding and reasoning in image sequences, showing a wide performance gap between these models and human capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to evaluate state-of-the-art MLLMs' ability to use both visual and linguistic information to understand temporal sequences of events, revealing limitations in these models and guiding future research.

Method: This paper introduces the TempVS benchmark to assess the temporal grounding and reasoning capabilities of Multimodal Large Language Models (MLLMs) when processing image sequences. It includes tests for event relation inference, sentence ordering, and image ordering, each with a basic grounding test.

Result: The evaluations of 38 state-of-the-art MLLMs on the TempVS benchmark revealed significant difficulties and a notable performance gap when compared to human capabilities.

Conclusion: The conclusion of the paper is that while significant gaps exist in MLLMs' ability to process temporal information in image sequences, the provided benchmark offers insights into improving these models' future development.

Abstract: This paper introduces the TempVS benchmark, which focuses on temporal
grounding and reasoning capabilities of Multimodal Large Language Models
(MLLMs) in image sequences. TempVS consists of three main tests (i.e., event
relation inference, sentence ordering and image ordering), each accompanied
with a basic grounding test. TempVS requires MLLMs to rely on both visual and
linguistic modalities to understand the temporal order of events. We evaluate
38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS,
with a substantial performance gap compared to human capabilities. We also
provide fine-grained insights that suggest promising directions for future
research. Our TempVS benchmark data and code are available at
https://github.com/yjsong22/TempVS.

</details>


### [24] [Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting](https://arxiv.org/abs/2506.10421)
*Avneet Kaur,Arnav Arora*

Main category: cs.CL

本研究通过计算方法分析新闻报道，揭示了更倾向于基于战争的报道，并显示了不同地区的媒体在报道中的显著差异和偏见。


<details>
  <summary>Details</summary>
Motivation: 新闻媒体在冲突时期的框架使用可能会对读者的观点产生重大影响，甚至加剧冲突本身。而目前关于冲突框架的研究由于其性质为定性分析或只关注表面水平的通用框架，因此缺乏深入的见解。

Method: 采用计算方法，结合框架语义学和大型语言模型，来识别新闻报道中的交流框架及其与语言框架的联系。

Result: 分析结果显示，在报道中更多地聚焦于基于战争的报道，而非和平。研究还揭示了美国、英国和中东新闻媒体在报道冲突中的袭击者和受害者时存在显著差异，反映出媒体中的偏见。

Conclusion: 研究结果强调了媒体在报道冲突时的框架使用的重要性，并揭示了媒体偏见的存在。

Abstract: Framing used by news media, especially in times of conflict, can have
substantial impact on readers' opinion, potentially aggravating the conflict
itself. Current studies on the topic of conflict framing have limited insights
due to their qualitative nature or only look at surface level generic frames
without going deeper. In this work, we identify indicators of war and peace
journalism, as outlined by prior work in conflict studies, in a corpus of news
articles reporting on the Israel-Palestine war. For our analysis, we use
computational approaches, using a combination of frame semantics and large
language models to identify both communicative framing and its connection to
linguistic framing. Our analysis reveals a higher focus on war based reporting
rather than peace based. We also show substantial differences in reporting
across the US, UK, and Middle Eastern news outlets in framing who the assailant
and victims of the conflict are, surfacing biases within the media.

</details>


### [25] [Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty](https://arxiv.org/abs/2506.10446)
*Zehui Ling,Deshu Chen,Hongwei Zhang,Yifeng Jiao,Xin Guo,Yuan Cheng*

Main category: cs.CL

研究通过引入新的奖励机制和输出长度惩罚来优化大型语言模型的推理效率，实现了在保持或提高准确性的同时减少输出长度。实验在三个数据集上显示了显著的效果，特别是对于更复杂的任务准确率得到了提高。


<details>
  <summary>Details</summary>
Motivation: 虽然链式思考等技术提升了大型语言模型的推理能力，但这些方法导致了较长的输出和较高的计算延迟。现有的一些减少推理输出长度的方法使用无差别的惩罚机制，这会损害模型对复杂问题推理的质量。为了解决这个问题，研究希望在保持复杂问题推理准确性的同时，简化简单问题的输出，从而优化模型的整体性能。

Method: Structure

Result: {
  "tldr": "研究通过引入新的奖励机制和输出长度惩罚来优化大型语言模型的推理效率，实现了在保持或提高准确性的同时减少输出长度。实验在三个数据集上显示了显著的效果，特别是对于更复杂的任务准确率得到了提高。", 
  "motivation": "虽然链式思考等技术提升了大型语言模型的推理能力，但这些方法导致了较长的输出和较高的计算延迟。现有的一些减少推理输出长度的方法使用无差别的惩罚机制，这会损害模型对复杂问题推理的质量。为了解决这个问题，研究希望在保持复杂问题推理准确性的同时，简化简单问题的输出，从而优化模型的整体性能。", 
  "method": "研究采用了方法来管理模型的推理效率，通过划分奖励函数和引入一个关于输出长度的新惩罚机制。这使得简单的任务输出变得更简短，同时确保复杂的任务中维持足够的推理质量。", 
  "result": "研究表明，在GSM8K、MATH500和AIME2024数据集上，研究的方法明显缩短了简单问题的输出长度且维持或改善了它的准确性，而在更复杂的AIME2024数据集上，研究的方法提高了模型的准确性。", 
  "conclusion": "提出的新方法在提高大型语言模型的推理效率和准确性上有显著效果，尤其在复杂问题上的准确性得到了提升，这为模型的应用提供了更好的性能支持。"}
}

Conclusion: 提出的新方法在提高大型语言模型的推理效率和准确性上有显著效果，尤其在复杂问题上的准确性得到了提升，这为模型的应用提供了更好的性能支持。

Abstract: Large language models (LLMs) have demonstrated significant advancements in
reasoning capabilities, performing well on various challenging benchmarks.
Techniques like Chain-of-Thought prompting have been introduced to further
improve reasoning. However, these approaches frequently generate longer
outputs, which in turn increase computational latency. Although some methods
use reinforcement learning to shorten reasoning, they often apply uniform
penalties without considering the problem's complexity, leading to suboptimal
outcomes. In this study, we seek to enhance the efficiency of LLM reasoning by
promoting conciseness for simpler problems while preserving sufficient
reasoning for more complex ones for accuracy, thus improving the model's
overall performance. Specifically, we manage the model's reasoning efficiency
by dividing the reward function and including a novel penalty for output
length. Our approach has yielded impressive outcomes in benchmark evaluations
across three datasets: GSM8K, MATH500, and AIME2024. For the comparatively
simpler datasets GSM8K and MATH500, our method has effectively shortened output
lengths while preserving or enhancing accuracy. On the more demanding AIME2024
dataset, our approach has resulted in improved accuracy.

</details>


### [26] [Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers](https://arxiv.org/abs/2506.10486)
*Xanh Ho,Sunisth Kumar,Yun-Ang Wu,Florian Boudin,Atsuhiro Takasu,Akiko Aizawa*

Main category: cs.CL

研究将表格-文本对齐重新定义为解释任务，用于改进声明验证方法的透明度和可解释性，并展示了即使正确预测标签，大语言模型的推理过程也未必合理。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于当前预测声明是否被数据支持或反驳的方法无法揭示模型的推理过程，且可解释性有限。因此，该研究通过改进数据集和方法以增加模型推理的透明度和可解释性。

Method: 该研究方法将表格-文本对齐重新定义为解释任务，要求模型识别出对声明验证至关重要的表格单元格，并通过扩展SciTab基准建立了一个带有人工注释单元格级别理由的新数据集。

Result: 实验结果显示，(i) 加入表格对齐信息可以提高声明验证性能，并且(ii) 大多数大语言模型尽管常常能正确预测标签，却无法恢复人类一致的理由，这表明它们的预测并非基于忠实推理。

Conclusion: 研究结论包括，加入表格对齐信息能够改进科学声明验证的性能，然而大语言模型即便能正确预测声明标签，其推理过程也未必忠实于人类的理解。

Abstract: Scientific claim verification against tables typically requires predicting
whether a claim is supported or refuted given a table. However, we argue that
predicting the final label alone is insufficient: it reveals little about the
model's reasoning and offers limited interpretability. To address this, we
reframe table-text alignment as an explanation task, requiring models to
identify the table cells essential for claim verification. We build a new
dataset by extending the SciTab benchmark with human-annotated cell-level
rationales. Annotators verify the claim label and highlight the minimal set of
cells needed to support their decision. After the annotation process, we
utilize the collected information and propose a taxonomy for handling ambiguous
cases. Our experiments show that (i) incorporating table alignment information
improves claim verification performance, and (ii) most LLMs, while often
predicting correct labels, fail to recover human-aligned rationales, suggesting
that their predictions do not stem from faithful reasoning.

</details>


### [27] [Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models](https://arxiv.org/abs/2506.10491)
*Aleksandra Sorokovikova,Pavel Chizhov,Iuliia Eremenko,Ivan P. Yamshchikov*

Main category: cs.CL

文章研究了大规模语言模型中的偏见问题，发现模型对用户个人身份的了解可能导致偏见加深，尤其是涉及薪资谈判建议时。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探讨大规模语言模型中的偏见现象，以及这些偏见如何影响模型的输出，特别是在涉及个人身份和记忆的情况下。

Method: 研究使用了预先设定的人物角色评估模型，并通过一个多主题基准（MMLU）测试来观察分数的差异，还通过让模型评估用户答案和提供薪资谈判建议来检测偏见。

Result: 预设定人物角色在多主题基准测试中导致的分数差异微乎其微，但在让模型评估用户答案时显示出更多偏见信号，特别是在薪资谈判建议中，偏见尤为明显。

Conclusion: 随着语言模型记忆和个人化的发展趋势，偏见问题变得更加复杂，因为模型可以通过记录的社会人口统计数据了解用户的个人身份。

Abstract: Modern language models are trained on large amounts of data. These data
inevitably include controversial and stereotypical content, which contains all
sorts of biases related to gender, origin, age, etc. As a result, the models
express biased points of view or produce different results based on the
assigned personality or the personality of the user. In this paper, we
investigate various proxy measures of bias in large language models (LLMs). We
find that evaluating models with pre-prompted personae on a multi-subject
benchmark (MMLU) leads to negligible and mostly random differences in scores.
However, if we reformulate the task and ask a model to grade the user's answer,
this shows more significant signs of bias. Finally, if we ask the model for
salary negotiation advice, we see pronounced bias in the answers. With the
recent trend for LLM assistant memory and personalization, these problems open
up from a different angle: modern LLM users do not need to pre-prompt the
description of their persona since the model already knows their
socio-demographics.

</details>


### [28] [Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models](https://arxiv.org/abs/2506.10504)
*Sangmin Song,Juhwan Choi,JungMin Yun,YoungBin Kim*

Main category: cs.CL

本文研究了大型语言模型（LLMs）在多用户对话状态跟踪（DST）中的鲁棒性，结果表明与单用户环境相比，LLMs在多用户环境中的性能有所下降，强调了改进LLMs在多用户场景下效能的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有DST基准主要关注结构化的用户代理对话，未能捕捉现实世界多用户交互的复杂性。本研究旨在评估LLMs在多用户DST中的鲁棒性，同时尽量减少数据集构建成本。

Method: 我们通过利用基于LLM的数据标注的最新进展，扩展了现有的DST数据集，通过基于言语行为理论生成第二个用户的发言，从而系统地将第二个用户的发言纳入对话中，以实现在多用户环境下的控制评估。

Result: 实验结果显示，相较于单用户的DST，LLMs在多用户DST中的性能显著下降。

Conclusion: 实验结果揭示了LLMs在多用户DST中的性能显著下降，这表明现有的LLMs在从多说话人中提取和跟踪对话状态方面存在局限。这突显了未来研究需要改进LLMs以适应多用户DST场景的必要性。

Abstract: Large language models (LLMs) have demonstrated remarkable performance in
zero-shot dialogue state tracking (DST), reducing the need for task-specific
training. However, conventional DST benchmarks primarily focus on structured
user-agent conversations, failing to capture the complexities of real-world
multi-user interactions. In this study, we assess the robustness of LLMs in
multi-user DST while minimizing dataset construction costs. Inspired by recent
advances in LLM-based data annotation, we extend an existing DST dataset by
generating utterances of a second user based on speech act theory. Our
methodology systematically incorporates a second user's utterances into
conversations, enabling a controlled evaluation of LLMs in multi-user settings.
Experimental results reveal a significant performance drop compared to
single-user DST, highlighting the limitations of current LLMs in extracting and
tracking dialogue states amidst multiple speakers. Our findings emphasize the
need for future research to enhance LLMs for multi-user DST scenarios, paving
the way for more realistic and robust DST models.

</details>


### [29] [Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs](https://arxiv.org/abs/2506.10508)
*Yilin Xiao,Chuang Zhou,Qinggang Zhang,Bo Li,Qing Li,Xiao Huang*

Main category: cs.CL

该研究提出了一种新的框架RRP，它结合了大语言模型和知识图谱的优势来提高对复杂问题的理解与解决能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型任务方面常常表现不佳，研究者希望通过将知识图谱与大语言模型集成来解决这些问题，特别是针对复杂问题的解决能力不足的问题。

Method: 该论文提出了一种名为RRP的框架，结合了大语言模型的语义优势与通过关系嵌入和双向分布学习获得的结构信息，用于从知识图谱中挖掘知识。此外，还引入了一个反思模块，根据其重要性评估和优化推理路径。

Result: 实验结果显示，RRP在两个公开数据集上的表现优于现有的基线方法，能够有效地生成针对特定问题的高质量推理路径，并且容易集成到各种大语言模型中，以增强其推理能力。

Conclusion: 通过生成高质量的推理路径，RRP可以为大语言模型提供有效的推理指导，从而提高它们处理复杂问题的能力。

Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks
due to a lack of background knowledge and a tendency to hallucinate. To address
these limitations, integrating knowledge graphs (KGs) with LLMs has been
intensively studied. Existing KG-enhanced LLMs focus on supplementary factual
knowledge, but still struggle with solving complex questions. We argue that
refining the relationships among facts and organizing them into a logically
consistent reasoning path is equally important as factual knowledge itself.
Despite their potential, extracting reliable reasoning paths from KGs poses the
following challenges: the complexity of graph structures and the existence of
multiple generated paths, making it difficult to distinguish between useful and
redundant ones. To tackle these challenges, we propose the RRP framework to
mine the knowledge graph, which combines the semantic strengths of LLMs with
structural information obtained through relation embedding and bidirectional
distribution learning. Additionally, we introduce a rethinking module that
evaluates and refines reasoning paths according to their significance.
Experimental results on two public datasets show that RRP achieves
state-of-the-art performance compared to existing baseline methods. Moreover,
RRP can be easily integrated into various LLMs to enhance their reasoning
abilities in a plug-and-play manner. By generating high-quality reasoning paths
tailored to specific questions, RRP distills effective guidance for LLM
reasoning.

</details>


### [30] [Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search](https://arxiv.org/abs/2506.10614)
*Promise Dodzi Kpoglu*

Main category: cs.CL

研究提出了一种无监督方法来重建原词形，该方法结合了数据驱动方法和基于规则的启发式方法，并在实验中展示了良好效果。


<details>
  <summary>Details</summary>
Motivation: 先前重建原词形的工作主要依赖于概率模型，这种方法主要由数据驱动，有一定局限性，因此需要一个更综合的方法来提高精确性和合理性。

Method: 提出了一种无监督的方法，用于重建原词形。这种方法整合了数据驱动推理和基于规则的启发式方法，在进化优化框架下工作。

Result: 在使用来自五种罗曼语的同源词数据集对拉丁词原形进行重建的任务中，实验结果显示，该方法在字符级准确性和音系合理性指标上均显著优于现有基线模型。

Conclusion: 通过整合数据驱动的方法和基于规则的启发式方法，提出了一个改进的原词形重建模型，该模型在字符级准确性和音系合理性方面取得了显著效果。

Abstract: We propose an unsupervised method for the reconstruction of protoforms i.e.,
ancestral word forms from which modern language forms are derived. While prior
work has primarily relied on probabilistic models of phonological edits to
infer protoforms from cognate sets, such approaches are limited by their
predominantly data-driven nature. In contrast, our model integrates data-driven
inference with rule-based heuristics within an evolutionary optimization
framework. This hybrid approach leverages on both statistical patterns and
linguistically motivated constraints to guide the reconstruction process. We
evaluate our method on the task of reconstructing Latin protoforms using a
dataset of cognates from five Romance languages. Experimental results
demonstrate substantial improvements over established baselines across both
character-level accuracy and phonological plausibility metrics.

</details>


### [31] [SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis](https://arxiv.org/abs/2506.10622)
*Sergio Burdisso,Esaú Villatoro-Tello,Petr Motlicek*

Main category: cs.CL

SDialog is a Python toolkit that uses LLMs to generate high-quality synthetic dialogues for the development and benchmarking of conversational AI systems.


<details>
  <summary>Details</summary>
Motivation: The motivation behind SDialog is to address the challenges of generating high-quality, flexible, and reproducible synthetic dialogues for training, evaluation, and benchmarking of conversational AI systems.

Method: SDialog is a modular, extensible Python toolkit that leverages instruction-tuned Large Language Models (LLMs) to provide abstractions for personas, orchestration, and scenario management, enabling the creation of realistic, diverse, and controllable conversational data.

Result: SDialog represents a step forward in the standardization of tools and frameworks for synthetic data generation, supporting workflows such as multi-agent simulation and scenario-driven generation.

Conclusion: SDialog facilitates the creation of high-quality synthetic dialogue data, contributing to the reproducibility and advancement of research in the fast-evolving field of conversational AI systems.

Abstract: The advancement of conversational AI systems relies on the availability of
high-quality, flexible, and reproducible synthetic dialogues for training,
evaluation, and benchmarking. SDialog is a modular, extensible Python toolkit
designed to address the challenges of synthetic dialogue generation and
analysis. By leveraging instruction-tuned Large Language Models (LLMs), SDialog
provides abstractions for personas, orchestration, and scenario management,
enabling the creation of realistic, diverse, and controllable conversational
data for research and development. SDialog supports workflows such as
multi-agent simulation and scenario-driven generation, and represents a step
forward in the standardization of tools and frameworks for synthetic data
generation, a crucial advancement for ensuring reproducibility in today's
fast-evolving research landscape.

</details>


### [32] [NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors](https://arxiv.org/abs/2506.10627)
*Numaan Naeem,Sarfraz Ahmad,Momina Ahsan,Hasan Iqbal*

Main category: cs.CL

本文提出了一个系统，用于评估一个AI驱动的辅导系统是否能正确识别学生数学推理中的错误，该系统通过四种方法的集成和策略，达到了总体最佳的性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估辅导员的回应是否能够正确识别学生数学推理中的错误，并利用AI技术来评估辅导系统的教学能力。

Method: 本文探讨了四种方法来解决AI驱动的辅导系统在识别学生数学推理中的错误时的表现评估问题：(1) 使用来自多个预训练语言模型的混合机器学习模型的集成方法；(2) 使用固定句向量的句子变压器叠加MLP分类器；(3) 含有多个注意力头的历史感知模型，用于处理在历史记录和回应嵌入之间的令牌水平交互；以及(4) 基于大型语言模型（如GPT 4o）的检索增强少样本提示系统。最终系统通过检索语义相似的例子，构建结构化的提示，并结合基于模式的输出解析来生成可解释的预测。

Result: 最终的系统表现优于所有基准，说明了将基于样例的提示与大型语言模型的推理相结合，在评估教学反馈方面的有效性和优势。

Conclusion: 本文开发了一种有效的方法，结合了不同AI技术手段，用于评估AI驱动的辅导系统在识别学生数学错误时的表现，表明此整合策略在该任务上能够取得最佳效果。

Abstract: This paper presents our system for Track 1: Mistake Identification in the BEA
2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The
task involves evaluating whether a tutor's response correctly identifies a
mistake in a student's mathematical reasoning. We explore four approaches: (1)
an ensemble of machine learning models over pooled token embeddings from
multiple pretrained language models (LMs); (2) a frozen sentence-transformer
using [CLS] embeddings with an MLP classifier; (3) a history-aware model with
multi-head attention between token-level history and response embeddings; and
(4) a retrieval-augmented few-shot prompting system with a large language model
(LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples,
constructs structured prompts, and uses schema-guided output parsing to produce
interpretable predictions. It outperforms all baselines, demonstrating the
effectiveness of combining example-driven prompting with LLM reasoning for
pedagogical feedback assessment. Our code is available at
https://github.com/NaumanNaeem/BEA_2025.

</details>


### [33] [Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters](https://arxiv.org/abs/2506.10641)
*Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

研究分析了大型语言模型在生成字符过程中的内部表征和字符级信息处理方式，揭示了这些模型采用不同于直观的拼写行为。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）可以以高精度逐字符生成标记，但它们在处理更复杂的字符级任务（如识别标记内的组成成分）时遇到困难。

Method: 通过使用探测分类器、识别知识神经元和检查注意力权重三种互补分析方法，研究了LLMs在生成字符过程中的内部表征和字符级别信息的使用方式。

Result: 研究发现，虽然对于人类来说拼写是一个简单任务，但对于LLMs来说并非如此，LLMs依赖于中间和更高层次的Transformer层来重建字符级知识。

Conclusion: 通过探测分类器、知识神经元的识别以及注意力权重的检查，验证了LLMs在字符级信息使用和拼写过程中的特定机制。

Abstract: Large language models (LLMs) can spell out tokens character by character with
high accuracy, yet they struggle with more complex character-level tasks, such
as identifying compositional subcomponents within tokens. In this work, we
investigate how LLMs internally represent and utilize character-level
information during the spelling-out process. Our analysis reveals that,
although spelling out is a simple task for humans, it is not handled in a
straightforward manner by LLMs. Specifically, we show that the embedding layer
does not fully encode character-level information, particularly beyond the
first character. As a result, LLMs rely on intermediate and higher Transformer
layers to reconstruct character-level knowledge, where we observe a distinct
"breakthrough" in their spelling behavior. We validate this mechanism through
three complementary analyses: probing classifiers, identification of knowledge
neurons, and inspection of attention weights.

</details>


### [34] [Large Language Models for Detection of Life-Threatening Texts](https://arxiv.org/abs/2506.10687)
*Thanh Thi Nguyen,Campbell Wilson,Janis Dalins*

Main category: cs.CL

使用大型语言模型（LLMs）识别潜在威胁生命的语言优于传统方法，并在不同类型的数据集上表现出色，尤其是在处理数据不平衡问题时。


<details>
  <summary>Details</summary>
Motivation: 识别生命威胁语言对于保护处于困境中的人、促进心理健康和预防潜在伤害至关重要。

Method: 研究采用Gemina、Mistral和Llama-2这些大型语言模型，并在类别平衡、不平衡和极度不平衡的数据集上进行微调。也对比了传统的词汇袋、词嵌入、主题建模和双向编码器表示方法。

Result: 实验结果显示大型语言模型（LLMs）整体表现优于传统方法，特别是在类别平衡和不平衡的数据上，Mistral和Llama-2模型表现最好，Gemma稍后。对于不平衡数据，上采样技术对传统方法有益，但对LLMs效果不大。

Conclusion: 这项研究揭示了大型语言模型在检测威胁生命的语言方面的强大潜力，并可能适用于现实世界的问题。

Abstract: Detecting life-threatening language is essential for safeguarding individuals
in distress, promoting mental health and well-being, and preventing potential
harm and loss of life. This paper presents an effective approach to identifying
life-threatening texts using large language models (LLMs) and compares them
with traditional methods such as bag of words, word embedding, topic modeling,
and Bidirectional Encoder Representations from Transformers. We fine-tune three
open-source LLMs including Gemma, Mistral, and Llama-2 using their 7B parameter
variants on different datasets, which are constructed with class balance,
imbalance, and extreme imbalance scenarios. Experimental results demonstrate a
strong performance of LLMs against traditional methods. More specifically,
Mistral and Llama-2 models are top performers in both balanced and imbalanced
data scenarios while Gemma is slightly behind. We employ the upsampling
technique to deal with the imbalanced data scenarios and demonstrate that while
this method benefits traditional approaches, it does not have as much impact on
LLMs. This study demonstrates a great potential of LLMs for real-world
life-threatening language detection problems.

</details>


### [35] [Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet](https://arxiv.org/abs/2506.10715)
*Lorenzo Augello,John P. McCrae*

Main category: cs.CL

本文探讨了形容词之间的上位关系，开发了新的形容词上位资源，并对大型语言模型进行了微调以预测形容词的上位关系。


<details>
  <summary>Details</summary>
Motivation: Open English Wordnet中缺乏许多链接，本文旨在探讨如何在形容词之间建立上位关系。

Method: 本文通过理论讨论了形容词之间的上位关系，并开发了一个新的形容词上位资源。同时，本文对大型语言模型进行微调以预测形容词的上位关系，展示了TaxoLLaMa方法可以适应这个任务。

Result: 本文成功开发了新的形容词上位资源，并通过微调大型语言模型验证了TaxoLLaMa方法的有效性。

Conclusion: 本文说明了TaxoLLaMa方法可以适应预测形容词上位关系的任务。

Abstract: Open English Wordnet is a key resource published in OntoLex-lemon as part of
the linguistic linked open data cloud. There are, however, many links missing
in the resource, and in this paper, we look at how we can establish hypernymy
between adjectives. We present a theoretical discussion of the hypernymy
relation and how it differs for adjectives in contrast to nouns and verbs. We
develop a new resource for adjective hypernymy and fine-tune large language
models to predict adjective hypernymy, showing that the methodology of
TaxoLLaMa can be adapted to this task.

</details>


### [36] [PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models](https://arxiv.org/abs/2506.10716)
*Ye Yu,Yaoning Yu,Haohan Wang*

Main category: cs.CL

PREMISE 是一种提示优化框架，能够减少大型推理模型在数学问题上的冗长推理，同时保持高准确率，显著降低了计算和成本。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在数学基准测试中表现出色，但它们的推理轨迹过于冗长，增加标记使用量和成本，限制了其在延迟敏感或API受限环境中的部署。因此，需要一种方法来减少推理开销，提高效率。

Method: PREMISE (PRompt-based Efficient Mathematical Inference with Strategic Evaluation) 是一种仅使用提示的方法，旨在减少长时间链式思考 (CoT) 推理中的冗余计算，同时保持答案准确性。它结合了跟踪级别诊断和梯度启发的提示优化，通过多目标文本搜索平衡标记长度和答案有效性。

Result: 在GSM8K，SVAMP 和 Math500 数据集上，PREMISE 与基线模型相比匹配或超过了准确性（Claude 从96%升至96%，Gemini 从91%升至92%），同时减少了高达87.5%的推理标记，且成本减少了69%-82%。

Conclusion: 这些结果表明，通过提示级别优化可以实现高效的大型推理模型推理，而不影响推理质量，是一种实用且可扩展的方法。

Abstract: Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve
strong performance on mathematical benchmarks using lengthy chain-of-thought
(CoT) reasoning, but the resulting traces are often unnecessarily verbose. This
inflates token usage and cost, limiting deployment in latency-sensitive or
API-constrained settings. We introduce PREMISE (PRompt-based Efficient
Mathematical Inference with Strategic Evaluation), a prompt-only framework that
reduces reasoning overhead without modifying model weights. PREMISE combines
trace-level diagnostics with gradient-inspired prompt optimization to minimize
redundant computation while preserving answer accuracy. The approach jointly
optimizes brevity and correctness through a multi-objective textual search that
balances token length and answer validity. Unlike prior work, PREMISE runs in a
single-pass black-box interface, so it can be applied directly to commercial
LLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy
($96\%\rightarrow96\%$ with Claude, $91\%\rightarrow92\%$ with Gemini) while
reducing reasoning tokens by up to $87.5\%$ and cutting dollar cost by
$69$--$82\%$. These results show that prompt-level optimization is a practical
and scalable path to efficient LRM inference without compromising reasoning
quality.

</details>


### [37] [Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims](https://arxiv.org/abs/2506.10728)
*Priyanka Kargupta,Runchu Tian,Jiawei Han*

Main category: cs.CL

论文提出了ClaimSpect框架，用于自动构建针对声明的方面层次结构，并通过检索增强生成技术来丰富这些方面，使其能提供更多的视角和数据支持，以全面评估声明的各个层面.


<details>
  <summary>Details</summary>
Motivation: 动机在于解决声明难以简单地划分为真或假的问题，特别适用于科学和政治声明，通过分解声明的不同方面来更全面准确地评估这些声明.

Method: 采用的是检索增强生成技术框架，构建声明的方面层次结构，以结构化的方式解析声明并发现其不同视角和数据支持情况.

Result: 应用ClaimSpect到多种实际的科学和政治声明上，展示了该框架在解析复杂和细微声明以及提供相关视角上的稳健性和准确性，并通过案例研究和人类评估验证了其有效性.

Conclusion: 通过提出和验证ClaimSpect，论文表明了结构化分解声明并提供多视角数据在评估声明有效性方面的优势.

Abstract: Claims made by individuals or entities are oftentimes nuanced and cannot be
clearly labeled as entirely "true" or "false" -- as is frequently the case with
scientific and political claims. However, a claim (e.g., "vaccine A is better
than vaccine B") can be dissected into its integral aspects and sub-aspects
(e.g., efficacy, safety, distribution), which are individually easier to
validate. This enables a more comprehensive, structured response that provides
a well-rounded perspective on a given problem while also allowing the reader to
prioritize specific angles of interest within the claim (e.g., safety towards
children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based
framework for automatically constructing a hierarchy of aspects typically
considered when addressing a claim and enriching them with corpus-specific
perspectives. This structure hierarchically partitions an input corpus to
retrieve relevant segments, which assist in discovering new sub-aspects.
Moreover, these segments enable the discovery of varying perspectives towards
an aspect of the claim (e.g., support, neutral, or oppose) and their respective
prevalence (e.g., "how many biomedical papers believe vaccine A is more
transportable than B?"). We apply ClaimSpect to a wide variety of real-world
scientific and political claims featured in our constructed dataset, showcasing
its robustness and accuracy in deconstructing a nuanced claim and representing
perspectives within a corpus. Through real-world case studies and human
evaluation, we validate its effectiveness over multiple baselines.

</details>


### [38] [TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora](https://arxiv.org/abs/2506.10737)
*Priyanka Kargupta,Nan Zhang,Yunyi Zhang,Rui Zhang,Prasenjit Mitra,Jiawei Han*

Main category: cs.CL

TaxoAdapt是一种新的分类框架，能够更细腻、连贯地分类科学文献，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在改进科学文献分类方法，以适应快速变化的科学环境。

Method: 提出了TaxoAdapt，通过迭代分层分类动态调整LLM生成的分类体系，适应不同维度的语料库。

Result: 本文提出了一种新的框架TaxoAdapt，旨在解决科学文献快速演变所带来组织和检索挑战。TaxoAdapt通过迭代分层分类，动态调整由大型语言模型（LLM）生成的分类体系，以适应不同维度的语料库，克服了传统专家分类的耗时、成本高昂以及现有自动分类技术过于依赖特定语料库或忽视科学领域动态性的局限。实验显示，与现有方法相比，TaxoAdapt在多个维度上的分类更细腻、更连贯，分别提高了26.51%和50.41%，展示了其在构建和捕捉科学领域演变方面的能力。

Conclusion: TaxoAdapt展示其在结构化和捕捉科学领域演进上的能力，表现出色。

Abstract: The rapid evolution of scientific fields introduces challenges in organizing
and retrieving scientific literature. While expert-curated taxonomies have
traditionally addressed this need, the process is time-consuming and expensive.
Furthermore, recent automatic taxonomy construction methods either (1)
over-rely on a specific corpus, sacrificing generalizability, or (2) depend
heavily on the general knowledge of large language models (LLMs) contained
within their pre-training datasets, often overlooking the dynamic nature of
evolving scientific domains. Additionally, these approaches fail to account for
the multi-faceted nature of scientific literature, where a single research
paper may contribute to multiple dimensions (e.g., methodology, new tasks,
evaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a
framework that dynamically adapts an LLM-generated taxonomy to a given corpus
across multiple dimensions. TaxoAdapt performs iterative hierarchical
classification, expanding both the taxonomy width and depth based on corpus'
topical distribution. We demonstrate its state-of-the-art performance across a
diverse set of computer science conferences over the years to showcase its
ability to structure and capture the evolution of scientific fields. As a
multidimensional method, TaxoAdapt generates taxonomies that are 26.51% more
granularity-preserving and 50.41% more coherent than the most competitive
baselines judged by LLMs.

</details>


### [39] [One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers](https://arxiv.org/abs/2506.10766)
*Diana Abagyan,Alejandro R. Salamanca,Andres Felipe Cruz-Salinas,Kris Cao,Hangyu Lin,Acyr Locatelli,Marzieh Fadaee,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

通过使用通用分词器，我们改善了多语言大型语言模型的可塑性，即便面对完全未见过的新语言也表现良好。这种方式使得模型能够更适应新语言，并在大多数预训练语言上保持性能。


<details>
  <summary>Details</summary>
Motivation: 预训练多语种大型语言模型（LLMs）对于许多语言来说具有挑战性，由于模型容量有限，高质量数据稀缺和计算约束。此外，分词器语言覆盖的缺乏使得在后期仅凭后训练阶段难以解决新语言的差距。

Method: 我们研究了在训练的早期阶段采取相对廉价的干预措施，以提高模型“语言可塑性”的方法，即模型在训练后适应新语言的能力。我们专注于分词器设计，并提出使用一种通用分词器，这种分词器针对比预训练语言更多的语言进行训练，以实现预训练后扩展语言覆盖范围的高效适应能力。

Result: 我们的系统实验涵盖了不同的语言组和不同的训练策略，结果显示，通用分词器在适应新语言方面表现出色，相较于专门针对预训练语言的分词器，其胜率提高了多达20.2%。此外，通用分词器还能够提高对分词器和预训练过程中完全未见过的语言的可塑性，胜率提高了5%。我们在扩展到更广泛的语言集的过程中，在大多数预训练语言的性能上几乎没有妥协。

Conclusion: 使用通用分词器显著增强了语言适应性，并可以为未见过的语言带来高达5%的胜率增加。这种提升没有显著牺牲预训练语言的质量。这种方法为语言模型扩展到更多语言提供了一种效率和性能兼顾的解决方案。

Abstract: Pretraining massively multilingual Large Language Models (LLMs) for many
languages at once is challenging due to limited model capacity, scarce
high-quality data, and compute constraints. Moreover, the lack of language
coverage of the tokenizer makes it harder to address the gap for new languages
purely at the post-training stage. In this work, we study what relatively cheap
interventions early on in training improve "language plasticity", or adaptation
capabilities of the model post-training to new languages. We focus on tokenizer
design and propose using a universal tokenizer that is trained for more
languages than the primary pretraining languages to enable efficient adaptation
in expanding language coverage after pretraining. Our systematic experiments
across diverse groups of languages and different training strategies show that
a universal tokenizer enables significantly higher language adaptation, with up
to 20.2% increase in win rates compared to tokenizers specific to pretraining
languages. Furthermore, a universal tokenizer also leads to better plasticity
towards languages that are completely unseen in the tokenizer and pretraining,
by up to 5% win rate gain. We achieve this adaptation to an expanded set of
languages with minimal compromise in performance on the majority of languages
included in pretraining.

</details>


### [40] [Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs](https://arxiv.org/abs/2506.10769)
*Alberto Testoni,Iacer Calixto*

Main category: cs.CL

研究对医疗领域多项选择题的不确定性估计方法进行了全面评估，发现简单方法接近复杂方法的性能，且更高效。强调了根据问题和模型特性进行选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 在临床决策支持等高风险领域部署大型语言模型时，准确且校准良好的不确定性估计至关重要。

Method: 本研究对临床多项选择题解答中的不确定性估计方法进行了细致的评估，涵盖了十个开源的大语言模型（包括通用、生物医学和推理模型），在两个数据集、十一个医学专科和六种问题类型上进行了测试。同时，它还将常用的一次生成法和基于采样的方法进行了比较，并通过一个案例研究，探讨了基于推理轨迹行为信号的简单单次通过估计器。

Result: 研究结果显示，简单的方法在性能上接近语义熵，但只需一次生成。此外，研究结果揭示了不同专科和问题类型之间的性能存在显著差异，强调了应根据问题性质和模型特定优势选择模型的重要性。

Conclusion: 研究表明，应根据问题性质和各个模型的特定优势来选择合适的模型，以确保临床决策支持等领域的应用模型能够提供准确可靠的不确定性估计。

Abstract: Accurate and well-calibrated uncertainty estimates are essential for
deploying large language models (LLMs) in high-stakes domains such as clinical
decision support. We present a fine-grained evaluation of uncertainty
estimation methods for clinical multiple-choice question answering, covering
ten open-source LLMs (general-purpose, biomedical, and reasoning models) across
two datasets, eleven medical specialties, and six question types. We compare
standard single-generation and sampling-based methods, and present a case study
exploring simple, single-pass estimators based on behavioral signals in
reasoning traces. These lightweight methods approach the performance of
Semantic Entropy while requiring only one generation. Our results reveal
substantial variation across specialties and question types, underscoring the
importance of selecting models based on both the nature of the question and
model-specific strengths.

</details>


### [41] [Improving Named Entity Transcription with Contextual LLM-based Revision](https://arxiv.org/abs/2506.10779)
*Viet Anh Trinh,Xinlu He,Jacob Whitehill*

Main category: cs.CL

通过引入大型语言模型(LLM)校正机制，利用LLM的推理能力和局部上下文环境（如讲座笔记），我们提出了一种改进自动语音识别(ASR)系统对专有名词识别准确性的方法，并展示了该方法在特定数据集上的显著效果。


<details>
  <summary>Details</summary>
Motivation: 尽管ASR系统在普通语音识别上取得了显著进步，但专有名词的误识率仍然偏高，这将影响到一系列下游应用，特别是ASR系统作为复杂系统前端时。因此，提出改进的动机是降低专有名词的误识率，提高整个系统的性能。

Method: 本文提出的方法是通过大型语言模型LLM的校正机制，利用LLM的推理能力和包含正确专有名词的局部上下文环境来校正ASR预测中的专有名词误识错误。

Result: 使用包含45小时MIT课程数据的NER-MIT-OpenCourseWare数据集进行开发和测试，结果表明，该技术对专有名词的WER能够降低高达30%。

Conclusion: 结论是，通过综合利用大型语言模型的推理能力和局部背景信息，能够有效提升ASR系统对专有名词的识别准确性，对于提高ASR系统的整体性能和下游应用效果具有显著意义。

Abstract: With recent advances in modeling and the increasing amount of supervised
training data, automatic speech recognition (ASR) systems have achieved
remarkable performance on general speech. However, the word error rate (WER) of
state-of-the-art ASR remains high for named entities. Since named entities are
often the most critical keywords, misrecognizing them can affect all downstream
applications, especially when the ASR system functions as the front end of a
complex system. In this paper, we introduce a large language model (LLM)
revision mechanism to revise incorrect named entities in ASR predictions by
leveraging the LLM's reasoning ability as well as local context (e.g., lecture
notes) containing a set of correct named entities. Finally, we introduce the
NER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses
for development and testing. On this dataset, our proposed technique achieves
up to 30\% relative WER reduction for named entities.

</details>


### [42] [Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints](https://arxiv.org/abs/2506.10800)
*Wei Sun,Tingyu Qu,Mingxiao Li,Jesse Davis,Marie-Francine Moens*

Main category: cs.CL

The paper presents LangEdit, a novel null-space constrained framework designed to perform precise multilingual knowledge updates in large language models, effectively addressing the issue of parameter interference.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to solve the challenge of integrating knowledge updates across multiple languages in a unified model without causing destructive parameter interference.

Method: LangEdit is a framework that projects parameter updates for each language onto the orthogonal complement of previous updated subspaces to achieve update independence and preserve multilingual generalization.

Result: The results of the evaluation demonstrate that LangEdit can effectively mitigate parameter interference and outperforms existing state-of-the-art editing methods.

Conclusion: The study concludes that LangEdit has the potential to enable efficient and accurate multilingual knowledge updates in large language models.

Abstract: Efficiently updating multilingual knowledge in large language models (LLMs),
while preserving consistent factual representations across languages, remains a
long-standing and unresolved challenge. While deploying separate editing
systems for each language might seem viable, this approach incurs substantial
costs due to the need to manage multiple models. A more efficient solution
involves integrating knowledge updates across all languages into a unified
model. However, performing sequential edits across languages often leads to
destructive parameter interference, significantly degrading multilingual
generalization and the accuracy of injected knowledge. To address this
challenge, we propose LangEdit, a novel null-space constrained framework
designed to precisely isolate language-specific knowledge updates. The core
innovation of LangEdit lies in its ability to project parameter updates for
each language onto the orthogonal complement of previous updated subspaces.
This approach mathematically guarantees update independence while preserving
multilingual generalization capabilities. We conduct a comprehensive evaluation
across three model architectures, six languages, and four downstream tasks,
demonstrating that LangEdit effectively mitigates parameter interference and
outperforms existing state-of-the-art editing methods. Our results highlight
its potential for enabling efficient and accurate multilingual knowledge
updates in LLMs. The code is available at
https://github.com/VRCMF/LangEdit.git.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

本文提出了一个基于先进人工智能算法的自动生成电影视频的方法，结果表明该方法在高质量视频生成、故事连贯性和效率方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能的进步，多媒体创作环境变得自动化。本文的动机是利用先进的技术改进来自文本的视频合成，提供更高质量的影视作品创建方式，未来将促进创意、教育和工业的应用。

Method: 此论文提出了一种使用稳定扩散(Stable Diffusion)生成高质量图像，GPT-2构建故事情节，结合gTTS和YouTube音源构建混合音频管道的方法，来实现从文本输入自动生成60秒的电影视频合成。整个过程基于一个五场景框架，通过线性帧插值、电影后的处理增强画质，并且实现了音频与视频的同步。

Result: 实验结果显示，该方法在图像质量、故事连贯性和效率方面表现优异，这表明对于创意内容的生产有着巨大的潜力和应用价值。

Conclusion: 通过结合不同的技术，本文实现了从文本输入到专业影视作品的自动化生成，为创意、教育和工业界带来了先进的解决方案。

Abstract: Advances in generative artificial intelligence have altered multimedia
creation, allowing for automatic cinematic video synthesis from text inputs.
This work describes a method for creating 60-second cinematic movies
incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for
narrative structuring, and a hybrid audio pipeline using gTTS and
YouTube-sourced music. It uses a five-scene framework, which is augmented by
linear frame interpolation, cinematic post-processing (e.g., sharpening), and
audio-video synchronization to provide professional-quality results. It was
created in a GPU-accelerated Google Colab environment using Python 3.11. It has
a dual-mode Gradio interface (Simple and Advanced), which supports resolutions
of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA
memory management and error handling ensure reliability. The experiments
demonstrate outstanding visual quality, narrative coherence, and efficiency,
furthering text-to-video synthesis for creative, educational, and industrial
applications.

</details>


### [44] [LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning](https://arxiv.org/abs/2506.10082)
*Chenjian Gao,Lihe Ding,Xin Cai,Zhanpeng Huang,Zibin Wang,Tianfan Xue*

Main category: cs.CV

The paper introduces a flexible video editing method using mask-based LoRA tuning for pre-trained I2V models and additional visual references.


<details>
  <summary>Details</summary>
Motivation: To improve flexibility in video editing beyond current limitations of large-scale pretraining and first-frame guidance.

Method: Our approach uses mask-based LoRA tuning to adapt pre-trained Image-to-Video models, enabling flexible edits while preserving the background. It also uses additional references like alternate viewpoints to guide the editing process.

Result: Experimental results show superior video editing performance compared to state-of-the-art methods.

Conclusion: The proposed method offers a flexible and efficient video editing approach by using mask-based LoRA tuning and additional references to guide the editing process.

Abstract: Video editing using diffusion models has achieved remarkable results in
generating high-quality edits for videos. However, current methods often rely
on large-scale pretraining, limiting flexibility for specific edits.
First-frame-guided editing provides control over the first frame, but lacks
flexibility over subsequent frames. To address this, we propose a mask-based
LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video
(I2V) models for flexible video editing. Our approach preserves background
regions while enabling controllable edits propagation. This solution offers
efficient and adaptable video editing without altering the model architecture.
To better steer this process, we incorporate additional references, such as
alternate viewpoints or representative scene states, which serve as visual
anchors for how content should unfold. We address the control challenge using a
mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model
to the editing context. The model must learn from two distinct sources: the
input video provides spatial structure and motion cues, while reference images
offer appearance guidance. A spatial mask enables region-specific learning by
dynamically modulating what the model attends to, ensuring that each area draws
from the appropriate source. Experimental results show our method achieves
superior video editing performance compared to state-of-the-art methods.

</details>


### [45] [DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding](https://arxiv.org/abs/2506.10084)
*Bin Guo,John H. L. Hansen*

Main category: cs.CV

介绍DeepTraverse，一种通过系统阐明和自适应精炼过程学习特征，受到算法搜索策略启发的新型视觉架构。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉骨干网络通过一系列相对均匀的操作构建特征，限制了自适应、迭代精炼的显式路径。这引发了提问：经典的搜索算法原则是否能为网络带来更算法性、结构化和逻辑化的处理流程，以通过更多可解释性的、类似推理的决策过程构建表示？

Method: DeepTraverse，一种直接受到算法搜索策略启发的新型视觉架构，通过两个关键协同组件实现特征学习：递归探索模块，在具有参数共享的效率下沿有前景的表示路径进行深入的特征分析；自适应校准模块，基于不断演变的全局上下文动态调整特征显著性。

Result: 在广泛的图像分类基准测试中的综合评估表明，DeepTraverse实现了高度竞争的分类准确性及强大的特征区分性，经常在参数数量相似或更大时优于传统模型。

Conclusion: 我们的研究表明，融合算法先验为构建更高效、性能更好且更结构化的视觉骨干网提供了一个原则性和有效的方法。

Abstract: Conventional vision backbones, despite their success, often construct
features through a largely uniform cascade of operations, offering limited
explicit pathways for adaptive, iterative refinement. This raises a compelling
question: can principles from classical search algorithms instill a more
algorithmic, structured, and logical processing flow within these networks,
leading to representations built through more interpretable, perhaps
reasoning-like decision processes? We introduce DeepTraverse, a novel vision
architecture directly inspired by algorithmic search strategies, enabling it to
learn features through a process of systematic elucidation and adaptive
refinement distinct from conventional approaches. DeepTraverse operationalizes
this via two key synergistic components: recursive exploration modules that
methodically deepen feature analysis along promising representational paths
with parameter sharing for efficiency, and adaptive calibration modules that
dynamically adjust feature salience based on evolving global context. The
resulting algorithmic interplay allows DeepTraverse to intelligently construct
and refine feature patterns. Comprehensive evaluations across a diverse suite
of image classification benchmarks show that DeepTraverse achieves highly
competitive classification accuracy and robust feature discrimination, often
outperforming conventional models with similar or larger parameter counts. Our
work demonstrates that integrating such algorithmic priors provides a
principled and effective strategy for building more efficient, performant, and
structured vision backbones.

</details>


### [46] [Test-Time Adaptation for Generalizable Task Progress Estimation](https://arxiv.org/abs/2506.10085)
*Christos Ziakas,Alessandra Russo*

Main category: cs.CV

本文提出了一种元学习策略培训的测试时适应方法，该方法通过自监督优化策略在线调整模型，从而改进了多样任务下的模型进展估计性能。


<details>
  <summary>Details</summary>
Motivation: 作者旨在开发一种有效的方法，使模型能够在线适应测试环境中的动态变化，提高对测试轨迹的估计准确性，特别是在面对多样化任务和环境时。

Method: 本文提出了一种测试时适应的方法，该方法使进度估计模型能够通过优化学习的自监督目标来在线适应测试轨迹的视觉和时间背景。为此，我们引入了一种基于梯度的元学习策略，该策略使用专家视觉轨迹及其自然语言任务描述来训练模型，使得测试时的适应可以依靠语义内容而不是单纯的时间顺序来进行进度估计。

Result: 提出的方法能够从单一训练环境推广到多样性的分布外任务、环境和实现形式，其表现优于当前最先进的上下文学习方法，后者使用自回归视觉-语言模型。

Conclusion: 研究展示了所提出的测试适应方法在提高模型对于新的多样化任务、环境和实现形式的适应能力上的优势。

Abstract: We propose a test-time adaptation method that enables a progress estimation
model to adapt online to the visual and temporal context of test trajectories
by optimizing a learned self-supervised objective. To this end, we introduce a
gradient-based meta-learning strategy to train the model on expert visual
trajectories and their natural language task descriptions, such that test-time
adaptation improves progress estimation relying on semantic content over
temporal order. Our test-time adaptation method generalizes from a single
training environment to diverse out-of-distribution tasks, environments, and
embodiments, outperforming the state-of-the-art in-context learning approach
using autoregressive vision-language models.

</details>


### [47] [EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models](https://arxiv.org/abs/2506.10100)
*Yantai Yang,Yuhao Wang,Zichen Wen,Luo Zhongwei,Chang Zou,Zhipeng Zhang,Chuan Wen,Linfeng Zhang*

Main category: cs.CV

EfficientVLA框架对标准VLA模型进行无训练的推理加速，显著增加速度，减少计算量，同时仅稍微降低成功率。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-行动（VLA）模型虽然展示了变革性潜力，但因其计算和内存需求高而受到限制，现有加速方法无法全面解决这一问题。

Method: EfficientVLA框架整合三种策略，分别为剪枝语言模块中的冗余层，优化视觉处理路径，缓存并重用关键中间特征以减轻扩散型动作头部的时间计算冗余。

Result: 分析论文摘要如下：
- **研究动机**：视觉-语言-行动（VLA）模型展示了对具身智能的变革潜力，但它受到高计算和内存需求的严重阻碍，这些需求源于其固有的和推理时的冗余。现有加速方法通常只能解决孤立的低效问题，未能有效解决整个VLA管道中的各种计算和内存瓶颈。
- **研究方法**：作者提出了一种名为EfficientVLA的无训练加速推断框架，该框架通过系统性地减少冗余来整合三种策略以减轻这些瓶颈。
1) 对语言模块中的冗余层进行剪枝；
2) 通过任务感知策略优化视觉处理路径；
3) 通过策略性地缓存和重用关键中间特征，在迭代扩散基础上的动作头部减轻时间计算冗余。
- **研究结果**：该方法应用于标准的VLA模型CogACT，获得了1.93倍的推理加速，将FLOPs减少到28.9%，在SIMPLER基准上的成功率仅降低了0.6%。
- **结论**：EfficientVLA能够系统地解决VLA模型的冗余问题，显著提升计算效率并保持良好的性能水平。

Conclusion: EfficientVLA通过整合多种策略成功减轻了VLA模型中的计算和内存问题，提高了计算效率且保留了较好的性能。

Abstract: Vision-Language-Action (VLA) models, particularly diffusion-based
architectures, demonstrate transformative potential for embodied intelligence
but are severely hampered by high computational and memory demands stemming
from extensive inherent and inference-time redundancies. While existing
acceleration efforts often target isolated inefficiencies, such piecemeal
solutions typically fail to holistically address the varied computational and
memory bottlenecks across the entire VLA pipeline, thereby limiting practical
deployability. We introduce EfficientVLA, a structured and training-free
inference acceleration framework that systematically eliminates these barriers
by cohesively exploiting multifaceted redundancies. EfficientVLA
synergistically integrates three targeted strategies: (1) pruning of
functionally inconsequential layers from the language module, guided by an
analysis of inter-layer redundancies; (2) optimizing the visual processing
pathway through a task-aware strategy that selects a compact, diverse set of
visual tokens, balancing task-criticality with informational coverage; and (3)
alleviating temporal computational redundancy within the iterative
diffusion-based action head by strategically caching and reusing key
intermediate features. We apply our method to a standard VLA model CogACT,
yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%
success rate drop in the SIMPLER benchmark.

</details>


### [48] [A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild](https://arxiv.org/abs/2506.10117)
*Klim Kireev,Ana-Maria Creţu,Raphael Meier,Sarah Adel Bargal,Elissa Redmiles,Carmela Troncoso*

Main category: cs.CV

本研究发布了一个名为'Image-Caption Children in the Wild Dataset (ICCWD)'的数据集，以解决目前在多模态环境中缺乏评估未成年人内容检测方法的基准和数据集的问题。该数据集由10,000对图像-字幕组成，研究显示儿童检测仍具挑战，即使在最佳方法的情况下，其真正阳性率也只有75.3%。


<details>
  <summary>Details</summary>
Motivation: 目前还没有现成的基准或数据集，用于评估在多模态环境中识别未成年人的方法。鉴于这一需求，本研究旨在填补空白，通过建立一个新的数据集，即'Image-Caption Children in the Wild Dataset (ICCWD)'，以助于提升检测儿童图像的质量和准确性。

Method: 不同的平台和法律对未成年人数字内容的管制与其他类型的内容不同。鉴于需要评估的内容量巨大，基于机器学习的自动化工具常被用来检测涉及未成年人的内容。为了填补这一领域的空白，研究人员发布了一个名为'Image-Caption Children in the Wild Dataset (ICCWD)'的图像-字幕数据集，该数据集用于评估检测儿童形象的工具。ICCWD包含了10,000对图像-字幕配对，这些图像在多个上下文中包含了儿童，包括虚构描绘和部分可见的身体。

Result: 研究人员测试了三种不同检测方法的有效性，包括一款应用在图像上的商用年龄估计系统。研究结果表明，儿童检测是一项极具挑战性的任务，最好的方法在真阳性率上达到了75.3%。

Conclusion: 尽管如此，检测儿童图像仍然是一项具有挑战性的任务。通过基准测试，最好的方法达到了75.3%的真正阳性率。出版这个数据集是为了帮助设计更好的检测未成年人的方法，以适用于各种场景。

Abstract: Platforms and the law regulate digital content depicting minors (defined as
individuals under 18 years of age) differently from other types of content.
Given the sheer amount of content that needs to be assessed, machine
learning-based automation tools are commonly used to detect content depicting
minors. To our knowledge, no dataset or benchmark currently exists for
detecting these identification methods in a multi-modal environment. To fill
this gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an
image-caption dataset aimed at benchmarking tools that detect depictions of
minors. Our dataset is richer than previous child image datasets, containing
images of children in a variety of contexts, including fictional depictions and
partially visible bodies. ICCWD contains 10,000 image-caption pairs manually
labeled to indicate the presence or absence of a child in the image. To
demonstrate the possible utility of our dataset, we use it to benchmark three
different detectors, including a commercial age estimation system applied to
images. Our results suggest that child detection is a challenging task, with
the best method achieving a 75.3% true positive rate. We hope the release of
our dataset will aid in the design of better minor detection methods in a wide
range of scenarios.

</details>


### [49] [Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers](https://arxiv.org/abs/2506.10119)
*Natanael Lucena,Fábio S. da Silva,Ricardo Rios*

Main category: cs.CV

论文对比了CNNs和ViTs在银屑病与其他相似疾病图像的多分类任务上的性能，发现ViTs表现更佳，尤其是DaViT-B模型，其f1得分为96.4%，推荐用于银屑病检测。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估和对比CNNs和ViTs在医学图像分类任务中的表现，特别是对于银屑病的自动检测。

Method: 该研究比较了卷积神经网络（CNNs）和视觉变换器（ViTs）在多分类含有银屑病病变和相似疾病图像任务上的性能。模型首先在ImageNet上预训练，然后适应于特定数据集。

Result: 实验结果表明，ViTs在较小模型下表现出了更优的性能。其中，双注意力视觉变换器-基础（DaViT-B）获得了最佳结果，f1得分为96.4%。

Conclusion: 研究表明，ViTs在医学图像分类任务中具有巨大潜力，尤其是在银屑病检测上，推荐使用DaViT-B架构。

Abstract: This paper presents a comparison of the performance of Convolutional Neural
Networks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying
images containing lesions of psoriasis and diseases similar to it. Models
pre-trained on ImageNet were adapted to a specific data set. Both achieved high
predictive metrics, but the ViTs stood out for their superior performance with
smaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the
best results, with an f1-score of 96.4%, and is recommended as the most
efficient architecture for automated psoriasis detection. This article
reinforces the potential of ViTs for medical image classification tasks.

</details>


### [50] [ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs](https://arxiv.org/abs/2506.10128)
*Xiyao Wang,Zhengyuan Yang,Chao Feng,Yongyuan Liang,Yuhang Zhou,Xiaoyu Liu,Ziyi Zang,Ming Li,Chung-Ching Lin,Kevin Lin,Linjie Li,Furong Huang,Lijuan Wang*

Main category: cs.CV

Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning (RL) has shown great effectiveness for fine-tuning
large language models (LLMs) using tasks that are challenging yet easily
verifiable, such as math reasoning or code generation. However, extending this
success to visual perception in vision-language models (VLMs) has been impeded
by the scarcity of vision-centric tasks that are simultaneously challenging and
unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption
Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle,
synthetic visual hallucination injected into paragraphs of human-written image
captions. Starting from a 200-word captions, we inject a single, subtle visual
description error-altering a few words on objects, attributes, counts, or
spatial relations-and task the model to pinpoint the corrupted span given the
image and the modified caption. This formulation preserves the full perceptual
difficulty while providing a binary, exact-match reward that is easy to compute
and unambiguous. Models trained with the ViCrit Task exhibit substantial gains
across a variety of VL benchmarks. Crucially, the improvements transfer beyond
natural-image training data to abstract image reasoning and visual math,
showing promises of learning to perceive rather than barely memorizing seen
objects. To facilitate evaluation, we further introduce ViCrit-Bench, a
category-balanced diagnostic benchmark that systematically probes perception
errors across diverse image domains and error types. Together, our results
demonstrate that fine-grained hallucination criticism is an effective and
generalizable objective for enhancing visual perception in VLMs.

</details>


### [51] [RoCA: Robust Cross-Domain End-to-End Autonomous Driving](https://arxiv.org/abs/2506.10145)
*Rajeev Yasarla,Shizhong Han,Hsin-Pai Cheng,Litian Liu,Shweta Mahajan,Apratim Bhattacharyya,Yunxiao Shi,Risheek Garrepalli,Hong Cai,Fatih Porikli*

Main category: cs.CV

本文提出了一种名为RoCA的框架，用于解决端到端自动驾驶在跨不同城市部署时遇到的挑战。RoCA提高了基础模型的泛化能力和在新域的适应性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决端到端自动驾驶在跨域部署（如不同城市）面临的实际挑战，特别是大型语言模型（LLMs）在这些方面提供的性能无法保证且可能引起高昂的重训练成本。

Method: RoCA提出了一种新颖的框架，通过联合概率分布处理端到端自动驾驶中自我车辆和周围车辆信息的编码。通过使用高斯过程（GP），RoCA能够学习一组基元token及其对应的轨迹，从而覆盖多样的驾驶场景。对于任何驾驶场景，该框架可以概率性地推断出未来的轨迹。

Result: 与在源域进行训练的基础模型配合使用时，RoCA提高了模型的泛化能力，而无需额外的推理计算。此外，RoCA在新的目标域上表现出强大的稳健适应性，显著优于直接微调的表现。

Conclusion: 研究表明，RoCA在多种跨域场景中具有显著的领域泛化和适应性表现。

Abstract: End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,
offering significant potential. However, few studies have looked into the
practical challenge of deployment across domains (e.g., cities). Although
several works have incorporated Large Language Models (LLMs) to leverage their
open-world knowledge, LLMs do not guarantee cross-domain driving performance
and may incur prohibitive retraining costs during domain adaptation. In this
paper, we propose RoCA, a novel framework for robust cross-domain E2E
autonomous driving. RoCA formulates the joint probabilistic distribution over
the tokens that encode ego and surrounding vehicle information in the E2E
pipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of
basis tokens with corresponding trajectories, which span diverse driving
scenarios. Then, given any driving scene, it is able to probabilistically infer
the future trajectory. By using RoCA together with a base E2E model in
source-domain training, we improve the generalizability of the base model,
without requiring extra inference computation. In addition, RoCA enables robust
adaptation on new target domains, significantly outperforming direct
finetuning. We extensively evaluate RoCA on various cross-domain scenarios and
show that it achieves strong domain generalization and adaptation performance.

</details>


### [52] [SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score](https://arxiv.org/abs/2506.10173)
*Mohammad Jalali,Haoyu Lei,Amin Gohari,Farzan Farnia*

Main category: cs.CV

The SPARKE method improves prompt-aware diversity in diffusion models while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the challenge of ensuring adequate diversity in generated samples from prompt-guided diffusion models, especially across semantically similar prompts.

Method: The paper introduces SPARKE, a method utilizing conditional entropy for prompt-aware diversity guidance in diffusion models, focusing on reducing computational complexity to O(n).

Result: Numerical testing on text-to-image models shows that SPARKE improves prompt-aware diversity without significant increase in computational costs.

Conclusion: The SPARKE method effectively enhances diversity in generated data for diffusion models, maintaining low computational complexity for large-scale generation.

Abstract: Diffusion models have demonstrated remarkable success in high-fidelity image
synthesis and prompt-guided generative modeling. However, ensuring adequate
diversity in generated samples of prompt-guided diffusion models remains a
challenge, particularly when the prompts span a broad semantic spectrum and the
diversity of generated data needs to be evaluated in a prompt-aware fashion
across semantically similar prompts. Recent methods have introduced guidance
via diversity measures to encourage more varied generations. In this work, we
extend the diversity measure-based approaches by proposing the Scalable
Prompt-Aware R\'eny Kernel Entropy Diversity Guidance (SPARKE) method for
prompt-aware diversity guidance. SPARKE utilizes conditional entropy for
diversity guidance, which dynamically conditions diversity measurement on
similar prompts and enables prompt-aware diversity control. While the
entropy-based guidance approach enhances prompt-aware diversity, its reliance
on the matrix-based entropy scores poses computational challenges in
large-scale generation settings. To address this, we focus on the special case
of Conditional latent RKE Score Guidance, reducing entropy computation and
gradient-based optimization complexity from the $O(n^3)$ of general entropy
measures to $O(n)$. The reduced computational complexity allows for
diversity-guided sampling over potentially thousands of generation rounds on
different prompts. We numerically test the SPARKE method on several
text-to-image diffusion models, demonstrating that the proposed method improves
the prompt-aware diversity of the generated data without incurring significant
computational costs. We release our code on the project page:
https://mjalali.github.io/SPARKE

</details>


### [53] [Retrieval of Surface Solar Radiation through Implicit Albedo Recovery from Temporal Context](https://arxiv.org/abs/2506.10174)
*Yael Frischholz,Devis Tuia,Michael Lehning*

Main category: cs.CV

本研究提出了一种基于时空视觉变压器的注意力机制模拟器，用于从原始卫星图像中学习无云天空条件下地表反射率，从而改进太阳辐射的估算，特别适用于复杂地形区。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常使用月度统计来近似背景反射率，这种方法在山区等复杂地形区域失效，因为这些区域积雪覆盖和雪面变化频繁。

Method: 提出了一种基于注意力机制的模拟器，用于从原始卫星图像序列中隐式学习并推断无云天空下的地表反射率。该方法基于时空视觉变压器，无需手工制作特征（如显式反照率图或云掩模）。

Result: 在瑞士复杂地形和动态积雪覆盖区域进行训练后，该模型能够在提供足够长的时间背景时，匹配反照率知情模型的性能。该模型特别在山区表现出色，提高了在简单和复杂地形中的泛化能力。

Conclusion: 该研究证明了所提出的方法可以有效地学习和利用地表反射率的动力学特征，尤其在复杂地形区域中表现优异，提高了太阳辐射估算的性能。

Abstract: Accurate retrieval of surface solar radiation (SSR) from satellite imagery
critically depends on estimating the background reflectance that a spaceborne
sensor would observe under clear-sky conditions. Deviations from this baseline
can then be used to detect cloud presence and guide radiative transfer models
in inferring atmospheric attenuation. Operational retrieval algorithms
typically approximate background reflectance using monthly statistics, assuming
surface properties vary slowly relative to atmospheric conditions. However,
this approach fails in mountainous regions where intermittent snow cover and
changing snow surfaces are frequent. We propose an attention-based emulator for
SSR retrieval that implicitly learns to infer clear-sky surface reflectance
from raw satellite image sequences. Built on the Temporo-Spatial Vision
Transformer, our approach eliminates the need for hand-crafted features such as
explicit albedo maps or cloud masks. The emulator is trained on instantaneous
SSR estimates from the HelioMont algorithm over Switzerland, a region
characterized by complex terrain and dynamic snow cover. Inputs include
multi-spectral SEVIRI imagery from the Meteosat Second Generation platform,
augmented with static topographic features and solar geometry. The target
variable is HelioMont's SSR, computed as the sum of its direct and diffuse
horizontal irradiance components, given at a spatial resolution of 1.7 km. We
show that, when provided a sufficiently long temporal context, the model
matches the performances of albedo-informed models, highlighting the model's
ability to internally learn and exploit latent surface reflectance dynamics.
Our geospatial analysis shows this effect is most powerful in mountainous
regions and improves generalization in both simple and complex topographic
settings. Code and datasets are publicly available at
https://github.com/frischwood/HeMu-dev.git

</details>


### [54] [Attention, Please! Revisiting Attentive Probing for Masked Image Modeling](https://arxiv.org/abs/2506.10178)
*Bill Psomas,Dionysis Christopoulos,Eirini Baltzi,Ioannis Kakogeorgiou,Tilemachos Aravanis,Nikos Komodakis,Konstantinos Karantzalos,Yannis Avrithis,Giorgos Tolias*

Main category: cs.CV

Efficient probing (EP) is introduced as an improved probing method, offering reduced parameters, faster speed, and better performance compared to existing techniques across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: The distributed nature of patch tokens in Masked Image Modeling (MIM) limits the effectiveness of traditional linear probing (LP). There is a need for an efficient and effective alternative that can better assess the capabilities of these models.

Method: We introduce efficient probing (EP), a multi-query cross-attention mechanism that reduces redundancies in existing attentive probing methods, leading to fewer trainable parameters and up to a 10x speed-up compared to conventional multi-head attention.

Result: EP outperforms linear probing (LP) and prior attentive probing approaches across seven benchmarks, and generalizes well beyond MIM to diverse pre-training paradigms, producing interpretable attention maps with strong gains in low-shot and layer-wise settings.

Conclusion: Efficient probing emerges as a powerful yet streamlined technique for evaluating self-supervised learning models, offering significant performance and efficiency improvements over linear and prior attentive probing methods.

Abstract: As fine-tuning (FT) becomes increasingly impractical at scale, probing is
emerging as the preferred evaluation protocol for self-supervised learning
(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the
potential of models trained with Masked Image Modeling (MIM), due to the
distributed nature of patch tokens. This motivates the need for attentive
probing, an alternative that uses attention to selectively aggregate
patch-level features. Despite its growing adoption, attentive probing remains
under-explored, with existing methods suffering from excessive parameterization
and poor computational efficiency.
  In this work, we revisit attentive probing through the lens of the
accuracy-efficiency trade-off. We conduct a systematic study of existing
methods, analyzing their mechanisms and benchmarking their performance. We
introduce efficient probing (EP), a multi-query cross-attention mechanism that
eliminates redundant projections, reduces the number of trainable parameters,
and achieves up to a 10$\times$ speed-up over conventional multi-head
attention. Despite its simplicity, EP outperforms LP and prior attentive
probing approaches across seven benchmarks, generalizes well beyond MIM to
diverse pre-training paradigms, produces interpretable attention maps, and
achieves strong gains in low-shot and layer-wise settings. Code available at
https://github.com/billpsomas/efficient-probing.

</details>


### [55] [Improving Personalized Search with Regularized Low-Rank Parameter Updates](https://arxiv.org/abs/2506.10182)
*Fiona Ryan,Josef Sivic,Fabian Caba Heilbron,Judy Hoffman,James M. Rehg,Bryan Russell*

Main category: cs.CV

本文提出了一种个性化视觉-语言检索方法，通过正则化的低秩调整视觉-语言双编码器的参数，以识别个人概念并保持一般知识。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是解决个性化视觉-语言检索任务，它需要从少量实例中学习新概念并将其与个人和通用知识相结合以在不同环境中识别概念。

Method: 我们的方法是通过正则化的低秩适应来调整视觉-语言双编码器模型的语言编码器的最后一层，以识别个人概念并保留一般知识。我们进一步探讨了将多个学习的个人概念的参数组合起来的策略，发现参数相加是有效的。

Result: 我们的方法在两个个性化图像检索基准数据集DeepFashion2和ConCon-Chi上实现了最先进的准确性，比现有方法提高了4%-22%的个性化检索性能。

Conclusion: 本文提出的方法能够有效适应视觉-语言双编码器模型的内部表示形式以进行个性化视觉-语言检索，并展示了在保持一般知识的同时识别个人概念的能力。

Abstract: Personalized vision-language retrieval seeks to recognize new concepts (e.g.
"my dog Fido") from only a few examples. This task is challenging because it
requires not only learning a new concept from a few images, but also
integrating the personal and general knowledge together to recognize the
concept in different contexts. In this paper, we show how to effectively adapt
the internal representation of a vision-language dual encoder model for
personalized vision-language retrieval. We find that regularized low-rank
adaption of a small set of parameters in the language encoder's final layer
serves as a highly effective alternative to textual inversion for recognizing
the personal concept while preserving general knowledge. Additionally, we
explore strategies for combining parameters of multiple learned personal
concepts, finding that parameter addition is effective. To evaluate how well
general knowledge is preserved in a finetuned representation, we introduce a
metric that measures image retrieval accuracy based on captions generated by a
vision language model (VLM). Our approach achieves state-of-the-art accuracy on
two benchmarks for personalized image retrieval with natural language queries -
DeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal
retrievals.

</details>


### [56] [ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators](https://arxiv.org/abs/2506.10226)
*Parsa Rahimi,Sebastien Marcel*

Main category: cs.CV

提出ScoreMix，一种新的数据增强策略，通过凸组合分数提升判别器性能，尤其在数据有限的情况下表现突出。实验表明，混合嵌入空间距离远的类可以获得更大的性能提升，这种方法有效解决了大规模数据集收集的问题。


<details>
  <summary>Details</summary>
Motivation: 解决有限数据集里的判别模型性能问题，特别是在有大量未标记数据的场景下，通过数据增强来提升模型表现。

Method: 提出ScoreMix，一种新的数据增强策略，利用扩散模型的分数组成属性来提升判别器性能，特别是在标记数据有限的情况下。通过在扩散采样过程中凸组合来自不同类条件轨迹的分数，生成具有挑战性的合成样本，以显著提高区分能力。系统地探究了混合的类选择策略，发现将判别器嵌入空间中距离较远的类组合可以获得更大的性能提升，而不是在生成器条件空间中距离较近的类。此外，我们经验性地展示了，在标准度量下，生成器习得的条件空间与判别器嵌入空间的相关性最小。

Result: 方法在所有研究的基准测试中均显著提升了判别能力，而无需进行复杂的参数搜索，展示了训练判别模型的实际优势，同时有效缓解了大规模数据集收集的问题。

Conclusion: ScoreMix方法在有限标签数据的情况下显著改进了判别模型性能，无需大量的参数调优，显示出其在训练判别性模型时的实际应用价值，并且有效解决了大规模数据集收集的问题。

Abstract: In this paper, we propose ScoreMix, a novel yet simple data augmentation
strategy leveraging the score compositional properties of diffusion models to
enhance discriminator performance, particularly under scenarios with limited
labeled data. By convexly mixing the scores from different class-conditioned
trajectories during diffusion sampling, we generate challenging synthetic
samples that significantly improve discriminative capabilities in all studied
benchmarks. We systematically investigate class-selection strategies for mixing
and discover that greater performance gains arise when combining classes
distant in the discriminator's embedding space, rather than close in the
generator's condition space. Moreover, we empirically show that, under standard
metrics, the correlation between the generator's learned condition space and
the discriminator's embedding space is minimal. Our approach achieves notable
performance improvements without extensive parameter searches, demonstrating
practical advantages for training discriminative models while effectively
mitigating problems regarding collections of large datasets. Paper website:
https://parsa-ra.github.io/scoremix

</details>


### [57] [California Crop Yield Benchmark: Combining Satellite Image, Climate, Evapotranspiration, and Soil Data Layers for County-Level Yield Forecasting of Over 70 Crops](https://arxiv.org/abs/2506.10228)
*Hamid Kamangir,Mona Hajiesmaeeli,Mason Earles*

Main category: cs.CV

我们开发了一种多模态深度学习模型，用于县一级、作物特异性的产量预测。该模型涵盖了加州所有70多种作物，从2008年到2022年的历史数据。模型成功预测了不同农业区域的作物产量。完整数据集和代码已公开。


<details>
  <summary>Details</summary>
Motivation: 尽管有USDA国家农业统计服务提供的大量历史数据，由于环境、气候和土壤相关因素之间复杂的相互作用，准确而及时地作物产量预测仍是一个挑战。

Method: 我们开发了一种多模态深度学习模型，该模型能够处理包括Landsat卫星图像、每日气候记录、每月蒸散量和高分辨率土壤属性等在内的异构输入，用于县一级、作物特异性的产量预测。模型采用分层特征提取和时间序列编码来捕捉生长季内的空间和时间动态。静态输入如土壤特性、作物身份信息可用于解释长期的变异性。

Result: 我们的方法在整个未见测试数据集的所有作物中实现了0.76的总体R2评分，显示出在加州多样化农业区域中强大的预测性能。

Conclusion: 该基准数据集和建模框架为推进农业预测、气候适应和精准农业提供了宝贵的基石。完整数据集和代码库已经公开发布在我们的GitHub仓库中。

Abstract: California is a global leader in agricultural production, contributing 12.5%
of the United States total output and ranking as the fifth-largest food and
cotton supplier in the world. Despite the availability of extensive historical
yield data from the USDA National Agricultural Statistics Service, accurate and
timely crop yield forecasting remains a challenge due to the complex interplay
of environmental, climatic, and soil-related factors. In this study, we
introduce a comprehensive crop yield benchmark dataset covering over 70 crops
across all California counties from 2008 to 2022. The benchmark integrates
diverse data sources, including Landsat satellite imagery, daily climate
records, monthly evapotranspiration, and high-resolution soil properties. To
effectively learn from these heterogeneous inputs, we develop a multi-modal
deep learning model tailored for county-level, crop-specific yield forecasting.
The model employs stratified feature extraction and a timeseries encoder to
capture spatial and temporal dynamics during the growing season. Static inputs
such as soil characteristics and crop identity inform long-term variability.
Our approach achieves an overall R2 score of 0.76 across all crops of unseen
test dataset, highlighting strong predictive performance across California
diverse agricultural regions. This benchmark and modeling framework offer a
valuable foundation for advancing agricultural forecasting, climate adaptation,
and precision farming. The full dataset and codebase are publicly available at
our GitHub repository.

</details>


### [58] [DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos](https://arxiv.org/abs/2506.10242)
*Rajeev Yasarla,Shizhong Han,Hong Cai,Fatih Porikli*

Main category: cs.CV

本文提出了DySS，采用状态空间学习和动态查询，以在高效推理的同时实现优秀的3D物体检测性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服基于密集鸟瞰图特征和稀疏查询法在自驾车3D物体检测上的成本和效率问题，本文提出了一种新的检测方法。

Method: 本文提出了一种新的方法DySS，采用状态空间学习和动态查询。具体来说，DySS利用状态空间模型（SSM）对时间步长中的采样特征进行顺序处理。为了鼓励模型更好地捕捉潜在的运动和对应信息，引入了对未来预测和掩码重建的辅助任务来优化SSM的训练。通过SSM的状态，提供了一个信息丰富且高效的场景总结。基于状态空间学习特征，通过合并、移除和分割操作动态更新查询，帮助在整个网络中保持一个有用且精简的检测查询集。

Result: DySS在检测性能和推理效率方面表现出色。具体而言，在nuScenes测试分割上，DySS实现了65.31的NDS和57.4的mAP，超越了最新的最先进方法。在val分割上，DySS实现了56.2的NDS和46.2的mAP，以及实时推理速度为33 FPS。

Conclusion: 本文提出的方法DySS不仅在性能上优于现有的最先进方法，而且在推理速度上也表现出色，实现了实时检测的能力。

Abstract: Camera-based 3D object detection in Bird's Eye View (BEV) is one of the most
important perception tasks in autonomous driving. Earlier methods rely on dense
BEV features, which are costly to construct. More recent works explore sparse
query-based detection. However, they still require a large number of queries
and can become expensive to run when more video frames are used. In this paper,
we propose DySS, a novel method that employs state-space learning and dynamic
queries. More specifically, DySS leverages a state-space model (SSM) to
sequentially process the sampled features over time steps. In order to
encourage the model to better capture the underlying motion and correspondence
information, we introduce auxiliary tasks of future prediction and masked
reconstruction to better train the SSM. The state of the SSM then provides an
informative yet efficient summarization of the scene. Based on the state-space
learned features, we dynamically update the queries via merge, remove, and
split operations, which help maintain a useful, lean set of detection queries
throughout the network. Our proposed DySS achieves both superior detection
performance and efficient inference. Specifically, on the nuScenes test split,
DySS achieves 65.31 NDS and 57.4 mAP, outperforming the latest state of the
art. On the val split, DySS achieves 56.2 NDS and 46.2 mAP, as well as a
real-time inference speed of 33 FPS.

</details>


### [59] [HalLoc: Token-level Localization of Hallucinations for Vision Language Models](https://arxiv.org/abs/2506.10286)
*Eunkyu Park,Minyeong Kim,Gunhee Kim*

Main category: cs.CV

论文提出了HalLoc数据集和基线模型解决视觉语言模型中的幻觉问题，实现低开销的幻觉检测，提高模型的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 鉴于大视觉语言模型中的幻觉现象对可靠性构成的挑战及现有检测方法存在的高延迟和高资源消耗问题，该研究旨在提供一个更高效的解决方案。

Method: 该论文提出了HalLoc数据集，用于高效的概率幻觉检测，包含150K个标记级别的样本，并介绍了基于HalLoc训练的基线模型，能够实现在生成过程中进行低开销的并发幻觉检测。

Result: 提出了HalLoc数据集，该数据集支持开发能以分级置信度检测幻觉的模型，并且提供了一个基线模型，能够实现在生成过程中的低开销幻觉检测。

Conclusion: 通过HalLoc数据集和基线模型的研究，论文展示了在保持效率的同时增强视觉语言模型可靠性和可信度的可能性，为真实应用场景打开了新途径。

Abstract: Hallucinations pose a significant challenge to the reliability of large
vision-language models, making their detection essential for ensuring accuracy
in critical applications. Current detection methods often rely on
computationally intensive models, leading to high latency and resource demands.
Their definitive outcomes also fail to account for real-world scenarios where
the line between hallucinated and truthful information is unclear. To address
these issues, we propose HalLoc, a dataset designed for efficient,
probabilistic hallucination detection. It features 150K token-level annotated
samples, including hallucination types, across Visual Question Answering (VQA),
instruction-following, and image captioning tasks. This dataset facilitates the
development of models that detect hallucinations with graded confidence,
enabling more informed user interactions. Additionally, we introduce a baseline
model trained on HalLoc, offering low-overhead, concurrent hallucination
detection during generation. The model can be seamlessly integrated into
existing VLMs, improving reliability while preserving efficiency. The prospect
of a robust plug-and-play hallucination detection module opens new avenues for
enhancing the trustworthiness of vision-language models in real-world
applications. The HalLoc dataset and code are publicly available at:
https://github.com/dbsltm/cvpr25_halloc.

</details>


### [60] [Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation](https://arxiv.org/abs/2506.10302)
*Hamzeh Asgharnezhad,Pegah Tabarisaadi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharya*

Main category: cs.CV

通过使用多种预训练模型和不确定性量化技术（如MCD、集成学习和EMCD），研究发现CLIP变体和SVM结合提供最高的分类性能。集成方法在准确性和不确定性处理之间提供了良好的平衡，而EMCD对不确定预测更为敏感。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌的精准诊断对早期治疗和改善患者预后至关重要。深度学习模型在自动化皮肤癌分类方面显示出潜力，但其性能受限于数据不足和缺乏不确定性的认识。

Method: 本研究分为两个阶段。第一阶段，使用多种预训练的特征提取器（如CLIP变体、ResNet50、DenseNet121、VGG16和EfficientNet-V2-Large）结合传统的分类器（如SVM、XGBoost和逻辑回归），对皮肤病变分类进行了基准测试。第二阶段，采用Monte Carlo Dropout (MCD)、集成学习(Ensemble)和集成Monte Carlo Dropout (EMCD)等不确定性量化(UQ)的方法，不仅评估预测的准确性，还评估模型输出的可靠性。

Result: CLIP-based vision transformers，特别是LAION CLIP ViT-H/14与SVM结合，提供了最高的分类性能。不确定性量化方法显示集成方法提供了较好的准确性和不确定性处理的平衡，而EMCD对不确定预测更敏感。

Conclusion: 本研究强调了在深度学习为基础的医学诊断中整合不确定量化以提高真实世界临床应用中的性能和可靠性的重要性。

Abstract: Accurate and reliable skin cancer diagnosis is critical for early treatment
and improved patient outcomes. Deep learning (DL) models have shown promise in
automating skin cancer classification, but their performance can be limited by
data scarcity and a lack of uncertainty awareness. In this study, we present a
comprehensive evaluation of DL-based skin lesion classification using transfer
learning and uncertainty quantification (UQ) on the HAM10000 dataset. In the
first phase, we benchmarked several pre-trained feature extractors-including
Contrastive Language-Image Pretraining (CLIP) variants, Residual Network-50
(ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual
Geometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range
of traditional classifiers such as Support Vector Machine (SVM), eXtreme
Gradient Boosting (XGBoost), and logistic regression. Our results show that
CLIP-based vision transformers, particularly LAION CLIP ViT-H/14 with SVM,
deliver the highest classification performance. In the second phase, we
incorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte
Carlo Dropout (EMCD) to assess not only prediction accuracy but also the
reliability of model outputs. We evaluated these models using uncertainty-aware
metrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen),
uncertainty specificity(USpe), and uncertainty precision(UPre). The results
demonstrate that ensemble methods offer a good trade-off between accuracy and
uncertainty handling, while EMCD is more sensitive to uncertain predictions.
This study highlights the importance of integrating UQ into DL-based medical
diagnosis to enhance both performance and trustworthiness in real-world
clinical applications.

</details>


### [61] [Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework](https://arxiv.org/abs/2506.10328)
*Sadia Kamal,Tim Oates,Joy Wan*

Main category: cs.CV

本文介绍了一种新的弱监督多模态框架，可以减少生成SOAP记录所需的手动工作量，并通过新设计的评估指标显示了高质量的临床相关性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是全球最常见的癌症形式，每年的医疗保健支出超过80亿美元。在临床环境中，手工生成SOAP记录非常费时费力，导致临床医生压力增大。

Method: 提出了一种弱监督多模态框架，可以从有限的输入（包括病灶图像和稀疏的临床文本）生成有结构的SOAP记录。该方法减少了对人工标注的依赖，使得临床文档的生成具有可扩展性，并减轻了临床医生的工作负担。

Result: 该研究方法在关键的临床相关性指标上的表现与GPT-4o、Claude和DeepSeek Janus Pro相当。引入了两个新的评估临床质量的指标MedConceptEval和临床连贯性得分（CCS），分别评估了与专家医疗概念的语义对齐度和对输入特征的一致性。

Conclusion: 研究展示了一种能够减少人工输入依赖的SOAP记录生成方法，从而降低了临床医生的工作压力，并减少了对大规模标注数据的需求。

Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for
over $8 billion in annual healthcare expenditures. In clinical settings,
physicians document patient visits using detailed SOAP (Subjective, Objective,
Assessment, and Plan) notes. However, manually generating these notes is
labor-intensive and contributes to clinician burnout. In this work, we propose
a weakly supervised multimodal framework to generate clinically structured SOAP
notes from limited inputs, including lesion images and sparse clinical text.
Our approach reduces reliance on manual annotations, enabling scalable,
clinically grounded documentation while alleviating clinician burden and
reducing the need for large annotated data. Our method achieves performance
comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical
relevance metrics. To evaluate clinical quality, we introduce two novel metrics
MedConceptEval and Clinical Coherence Score (CCS) which assess semantic
alignment with expert medical concepts and input features, respectively.

</details>


### [62] [Research on Audio-Visual Quality Assessment Dataset and Method for User-Generated Omnidirectional Video](https://arxiv.org/abs/2506.10331)
*Fei Zhao,Da Pan,Zelu Qi,Ping Shi*

Main category: cs.CV

研究针对全向视频(ODV)的视听质量评估(AVQA)，构建用户生成内容的全向音频和视频数据集，并基于该数据集开发了一个AVQA基准模型，实验结果显示模型性能良好。


<details>
  <summary>Details</summary>
Motivation: 为了解决全向视频ODV的视听质量评估(AVQA)的局限性。

Method: 构建了一个包含用户生成内容的全向音频和视频内容的数据集。视频由五个人使用两种不同类型的全向摄像机拍摄，共拍摄了300个视频，涉及10种不同的场景类型。此外，通过主观音频-视频质量评估实验获得了音频-视频序列的平均意见得分(MOS)。最后，基于提出的数据集构建了一个有效的AVQA基准模型，该模型由视频特征提取模块、音频特征提取模块和音频-视频融合模块组成。

Result: 实验结果表明，该模型在所提出的数据库上表现出优异的性能。

Conclusion: 通过构建用户生成的全向音频和视频数据集以及相应的AVQA基准模型，研究者们成功地解决了全向视频的AVQA挑战。

Abstract: In response to the rising prominence of the Metaverse, omnidirectional videos
(ODVs) have garnered notable interest, gradually shifting from
professional-generated content (PGC) to user-generated content (UGC). However,
the study of audio-visual quality assessment (AVQA) within ODVs remains
limited. To address this, we construct a dataset of UGC omnidirectional audio
and video (A/V) content. The videos are captured by five individuals using two
different types of omnidirectional cameras, shooting 300 videos covering 10
different scene types. A subjective AVQA experiment is conducted on the dataset
to obtain the Mean Opinion Scores (MOSs) of the A/V sequences. After that, to
facilitate the development of UGC-ODV AVQA fields, we construct an effective
AVQA baseline model on the proposed dataset, of which the baseline model
consists of video feature extraction module, audio feature extraction and
audio-visual fusion module. The experimental results demonstrate that our model
achieves optimal performance on the proposed dataset.

</details>


### [63] [Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions](https://arxiv.org/abs/2506.10334)
*Deliang Wang,Chao Yang,Gaowei Chen*

Main category: cs.CV

本研究采用零样本提示技术评估了两种视觉-语言模型识别学生学术情绪的能力。结果显示，Qwen2.5-VL-7B-Instruct优于Llama-3.2-11B-Vision-Instruct，并在困惑表情识别方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习方法在情绪分析的泛化能力上存在局限性，而视觉-语言模型（VLMs）的出现为这类问题提供了一种新的解决方案。研究旨在探索VLMs在基于在线学习环境中的面部表情分析学生学习情绪的可能性。

Method: 本研究使用了两种视觉-语言模型（VLMs），Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct，通过零样本提示技术分析了5,000张不同表情（困惑、分心、快乐、中性、疲倦）的学生面部图像。

Result: 初步结果显示，两种模型在学术面部表情识别中表现出中等性能，但Qwen2.5-VL-7B-Instruct模型优于Llama-3.2-11B-Vision-Instruct模型。两模型在识别快乐情绪方面表现出色，但在识别分心行为方面效果欠佳。Qwen2.5-VL-7B-Instruct在识别困惑表情方面表现出色。

Conclusion: 视觉-语言模型具有识别学生表情并反映学术情绪的潜力，特别是Qwen2.5-VL-7B-Instruct模型在辨识困惑表情方面表现出较高的准确度，这为在线教育中实时识别引起学生困惑的内容提供了可能。

Abstract: Students' academic emotions significantly influence their social behavior and
learning performance. Traditional approaches to automatically and accurately
analyze these emotions have predominantly relied on supervised machine learning
algorithms. However, these models often struggle to generalize across different
contexts, necessitating repeated cycles of data collection, annotation, and
training. The emergence of Vision-Language Models (VLMs) offers a promising
alternative, enabling generalization across visual recognition tasks through
zero-shot prompting without requiring fine-tuning. This study investigates the
potential of VLMs to analyze students' academic emotions via facial expressions
in an online learning environment. We employed two VLMs,
Llama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct, to analyze 5,000
images depicting confused, distracted, happy, neutral, and tired expressions
using zero-shot prompting. Preliminary results indicate that both models
demonstrate moderate performance in academic facial expression recognition,
with Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct.
Notably, both models excel in identifying students' happy emotions but fail to
detect distracted behavior. Additionally, Qwen2.5-VL-7B-Instruct exhibits
relatively high performance in recognizing students' confused expressions,
highlighting its potential for practical applications in identifying content
that causes student confusion.

</details>


### [64] [PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting](https://arxiv.org/abs/2506.10335)
*Lintao Xiang,Hongpei Zheng,Yating Huang,Qijun Yang,Hujun Yin*

Main category: cs.CV

本文提出一种新的3D高斯分布渲染框架，通过改进特征编码和优化互动网络，可以实现在少量视图下仍然高质量的实时渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯分布渲染（3DGS）是一种创新的渲染技术，它在渲染速度和视觉质量上超越了神经辐射场（NeRF），但现有的3DGS方法需要大量的校准视图来生成一致的完整的场景表示。输入视图受限时，3DGS容易过度拟合训练视图，导致渲染质量显著下降。为了解决这个局限性，我们提出了一个基于点特征感知的高斯分布渲染框架。

Method: 我们的方法是一种基于点特征感知的高斯分布渲染框架，用于从稀疏训练视图中实现实时高质量渲染。首先，我们使用最新的立体基础模型来估计准确的相机姿态并重建密集点云以初始化高斯分布。然后，我们通过采样和聚合多尺度2D外观特征来编码每个3D高斯的颜色属性。为了增强点式的外观表示，我们设计了一个基于自注意力机制的点交互网络，使每个高斯点能够与其最近邻点进行交互。这些丰富的特征随后通过两个轻量级多层感知器（MLPs）解码成高斯参数以进行最终渲染。

Result: 在各种基准测试上的大量实验表明，我们的方法在少量视图设置下的性能不仅显著优于NeRF基础方法，而且与其他最先进的3DGS方法相比，也具有竞争力。

Conclusion: 本文提出了一种基于点特征感知的高斯分布渲染方法，从稀疏视图中实现实时高质量渲染，并在多种测试环境下展示了该方法相对于NeRF和现有3DGS方法的优势。

Abstract: 3D Gaussian splatting (3DGS) is an innovative rendering technique that
surpasses the neural radiance field (NeRF) in both rendering speed and visual
quality by leveraging an explicit 3D scene representation. Existing 3DGS
approaches require a large number of calibrated views to generate a consistent
and complete scene representation. When input views are limited, 3DGS tends to
overfit the training views, leading to noticeable degradation in rendering
quality. To address this limitation, we propose a Point-wise Feature-Aware
Gaussian Splatting framework that enables real-time, high-quality rendering
from sparse training views. Specifically, we first employ the latest stereo
foundation model to estimate accurate camera poses and reconstruct a dense
point cloud for Gaussian initialization. We then encode the colour attributes
of each 3D Gaussian by sampling and aggregating multiscale 2D appearance
features from sparse inputs. To enhance point-wise appearance representation,
we design a point interaction network based on a self-attention mechanism,
allowing each Gaussian point to interact with its nearest neighbors. These
enriched features are subsequently decoded into Gaussian parameters through two
lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive
experiments on diverse benchmarks demonstrate that our method significantly
outperforms NeRF-based approaches and achieves competitive performance under
few-shot settings compared to the state-of-the-art 3DGS methods.

</details>


### [65] [GeoCAD: Local Geometry-Controllable CAD Generation](https://arxiv.org/abs/2506.10337)
*Zhanwei Zhang,Kaiyuan Liu,Junjie Liu,Wenxiao Wang,Binbin Lin,Liang Xie,Chen Shen,Deng Cai*

Main category: cs.CV

GeoCAD 是一种用于局部几何控制的计算机辅助设计生成方法，通过提出一种标注策略来生成针对局部部分的几何指令，解决了现有方法在遵循文本指令和局部部分控制方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前方法在实现自动修改CAD模型局部部分并遵循用户特定几何指令的目标时面临挑战。GeoCAD旨在解决现有方法无法遵循文本指令或者无法专注于局部部分的问题，以此提升设计效率。

Method: GeoCAD, 一种用户友好且可控制局部几何的CAD生成方法，首先提出了一种补充性标注策略来生成局部部分的几何指令，此策略包含基于顶点的和基于VLLM的标注方法，适用于简单和复杂部分的系统化标注。此外，训练阶段会对CAD模型随机遮蔽局部部分，利用几何指令和剩余部分作为输入，通过大型语言模型预测被遮蔽的部分。推理阶段，用户可以指定任何局部部分进行修改，并遵循各种预定义的几何指令。

Result: 广泛的实验表明，GeoCAD 在生成质量、有效性和文本到CAD的一致性方面均表现出色。

Conclusion: GeoCAD通过其独特的补充分区策略和使用大型语言模型预测被遮蔽部分的方法，有效地提高了CAD模型的生成质量和遵循几何指令的能力。

Abstract: Local geometry-controllable computer-aided design (CAD) generation aims to
modify local parts of CAD models automatically, enhancing design efficiency. It
also ensures that the shapes of newly generated local parts follow
user-specific geometric instructions (e.g., an isosceles right triangle or a
rectangle with one corner cut off). However, existing methods encounter
challenges in achieving this goal. Specifically, they either lack the ability
to follow textual instructions or are unable to focus on the local parts. To
address this limitation, we introduce GeoCAD, a user-friendly and local
geometry-controllable CAD generation method. Specifically, we first propose a
complementary captioning strategy to generate geometric instructions for local
parts. This strategy involves vertex-based and VLLM-based captioning for
systematically annotating simple and complex parts, respectively. In this way,
we caption $\sim$221k different local parts in total. In the training stage,
given a CAD model, we randomly mask a local part. Then, using its geometric
instruction and the remaining parts as input, we prompt large language models
(LLMs) to predict the masked part. During inference, users can specify any
local part for modification while adhering to a variety of predefined geometric
instructions. Extensive experiments demonstrate the effectiveness of GeoCAD in
generation quality, validity and text-to-CAD consistency. Code will be
available at https://github.com/Zhanwei-Z/GeoCAD.

</details>


### [66] [UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models](https://arxiv.org/abs/2506.10342)
*Jun Yin,Jing Zhong,Peilin Li,Pengyu Zeng,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CV

研究提出了UrbanSense，一种基于视觉语言模型的多模态框架，用于城市街景分析，并通过实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统城市文化研究通常依赖专家解读和历史文献，难以在不同情境中标准化。为了克服这一问题，本研究提出了一个新的分析方法。

Method: 本研究提出了一种基于视觉语言模型的多模态研究框架UrbanSense，该框架可以实现对城市街景风格差异的自动化和可扩展分析。

Result: 实验结果显示，超过80%的生成描述通过了T检验（p小于0.05）。主观评价的高Phi评分（城市0.912，时期0.833）证实了此方法能够捕捉细微的风格差异。

Conclusion: 这个方法展示了量化解释城市风格演变的潜力，为未来设计提供了科学依据。

Abstract: Urban cultures and architectural styles vary significantly across cities due
to geographical, chronological, historical, and socio-political factors.
Understanding these differences is essential for anticipating how cities may
evolve in the future. As representative cases of historical continuity and
modern innovation in China, Beijing and Shenzhen offer valuable perspectives
for exploring the transformation of urban streetscapes. However, conventional
approaches to urban cultural studies often rely on expert interpretation and
historical documentation, which are difficult to standardize across different
contexts. To address this, we propose a multimodal research framework based on
vision-language models, enabling automated and scalable analysis of urban
streetscape style differences. This approach enhances the objectivity and
data-driven nature of urban form research. The contributions of this study are
as follows: First, we construct UrbanDiffBench, a curated dataset of urban
streetscapes containing architectural images from different periods and
regions. Second, we develop UrbanSense, the first vision-language-model-based
framework for urban streetscape analysis, enabling the quantitative generation
and comparison of urban style representations. Third, experimental results show
that Over 80% of generated descriptions pass the t-test (p less than 0.05).
High Phi scores (0.912 for cities, 0.833 for periods) from subjective
evaluations confirm the method's ability to capture subtle stylistic
differences. These results highlight the method's potential to quantify and
interpret urban style evolution, offering a scientifically grounded lens for
future design.

</details>


### [67] [RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration](https://arxiv.org/abs/2506.10344)
*Mina C. Moghadam,Alan Q. Wang,Omer Taub,Martin R. Prince,Mert R. Sabuncu*

Main category: cs.CV

Introduces RealKeyMorph (RKM), a resolution-agnostic image registration method avoiding resampling artifacts, working on raw data in real-world coordinates.


<details>
  <summary>Details</summary>
Motivation: Resampling in previous registration techniques can introduce artifacts. RKM avoids this by working on raw data in real-world coordinates.

Method: RealKeyMorph (RKM), an extension of KeyMorph, learns corresponding keypoints in real-world coordinates without resampling images, enabling resolution-agnostic image registration.

Result: RKM demonstrates advantages for registering orthogonal 2D stacks of abdominal MRIs and 3D brain datasets with varying resolutions.

Conclusion: RealKeyMorph effectively addresses the limitations of traditional image registration methods by operating on the raw data without resampling.

Abstract: Many real-world settings require registration of a pair of medical images
that differ in spatial resolution, which may arise from differences in image
acquisition parameters like pixel spacing, slice thickness, and field-of-view.
However, all previous machine learning-based registration techniques resample
images onto a fixed resolution. This is suboptimal because resampling can
introduce artifacts due to interpolation. To address this, we present
RealKeyMorph (RKM), a resolution-agnostic method for image registration. RKM is
an extension of KeyMorph, a registration framework which works by training a
network to learn corresponding keypoints for a given pair of images, after
which a closed-form keypoint matching step is used to derive the transformation
that aligns them. To avoid resampling and enable operating on the raw data, RKM
outputs keypoints in real-world coordinates of the scanner. To do this, we
leverage the affine matrix produced by the scanner (e.g., MRI machine) that
encodes the mapping from voxel coordinates to real world coordinates. By
transforming keypoints into real-world space and integrating this into the
training process, RKM effectively enables the extracted keypoints to be
resolution-agnostic. In our experiments, we demonstrate the advantages of RKM
on the registration task for orthogonal 2D stacks of abdominal MRIs, as well as
3D volumes with varying resolutions in brain datasets.

</details>


### [68] [Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation](https://arxiv.org/abs/2506.10353)
*Runqi Ouyang,Haoyun Li,Zhenyuan Zhang,Xiaofeng Wang,Zheng Zhu,Guan Huang,Xingang Wang*

Main category: cs.CV

本文提出了Motion-R1框架，该框架能够处理复杂的文本指令，生成更具控制性、一致性和多样性的动作。实验结果表明其相比最先进的方法有更强的语义理解和长时连贯性。


<details>
  <summary>Details</summary>
Motivation: 一般来说，现有的方法虽然在语义对齐和动作合成上取得了一定的进展，但往往依赖于端到端映射策略，未能捕捉到深层次的语言结构和逻辑推理。因此，生成的动作往往缺乏控制力、一致性和多样性。本文提出Motion-R1来解决这些问题。

Method: Motion-R1框架整合了Chain-of-Thought机制，将复杂的文本指令分解为逻辑结构化的动作路径，为动作生成提供高级语义指导，显著提升了模型理解和执行多步、长时间范围和复杂组合命令的能力。

Result: 在多个基准数据集上的大量实验表明，Motion-R1相较于最先进的方法可以实现竞争或更优的性能，特别是在需要细致的语义理解和长期时间连贯性的场景中。

Conclusion: Motion-R1通过整合Chain-of-Thought机制，在提高动作生成的控制力、一致性和多样性的同时，有效应对了现有方法的局限，展现了其在复杂多步、长时间范围和综合命令中的优秀表现。

Abstract: Recent advances in large language models, especially in natural language
understanding and reasoning, have opened new possibilities for text-to-motion
generation. Although existing approaches have made notable progress in semantic
alignment and motion synthesis, they often rely on end-to-end mapping
strategies that fail to capture deep linguistic structures and logical
reasoning. Consequently, generated motions tend to lack controllability,
consistency, and diversity. To address these limitations, we propose Motion-R1,
a unified motion-language modeling framework that integrates a Chain-of-Thought
mechanism. By explicitly decomposing complex textual instructions into
logically structured action paths, Motion-R1 provides high-level semantic
guidance for motion generation, significantly enhancing the model's ability to
interpret and execute multi-step, long-horizon, and compositionally rich
commands. To train our model, we adopt Group Relative Policy Optimization, a
reinforcement learning algorithm designed for large models, which leverages
motion quality feedback to optimize reasoning chains and motion synthesis
jointly. Extensive experiments across multiple benchmark datasets demonstrate
that Motion-R1 achieves competitive or superior performance compared to
state-of-the-art methods, particularly in scenarios requiring nuanced semantic
understanding and long-term temporal coherence. The code, model and data will
be publicly available.

</details>


### [69] [FaceLiVT: Face Recognition using Linear Vision Transformer with Structural Reparameterization For Mobile Device](https://arxiv.org/abs/2506.10361)
*Novendra Setyawan,Chi-Chia Sun,Mao-Hsiu Hsu,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

本文提出了一种名为FaceLiVT的轻量级人脸识别模型，结合了CNN-Transformer架构和创新的MHLA机制，在保留高精度的同时显著提高了推理速度，在多个基准测试中表现优于当前最佳的轻量级模型。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是设计一个在保持高精度的同时，减少计算复杂度的轻量级人脸识别模型。通过引入创新的MHLA机制，FaceLiVT旨在优化移动设备上的推理速度。

Method: 该论文介绍了一种名为FaceLiVT的轻量级且强大的人脸识别模型，该模型结合了混合卷积神经网络（CNN）-Transformer架构和创新的轻量级多头线性注意力（MHLA）机制。通过结合MHLA和重新参数化令牌混合器，FaceLiVT有效减少了计算复杂度，同时保持了竞争优势。

Result: 该模型在多个挑战性的基准测试中展现了卓越的表现，包括LFW, CFP-FP, AgeDB-30, IJB-B和IJB-C。相比于当前其他轻量级模型，FaceLiVT的推理速度更快，例如比优化边缘设备的EdgeFace快8.6倍，比基于纯ViT的模型快21.2倍。

Conclusion: FaceLiVT提供了一种在资源受限平台上实现高效实时人脸识别的解决方案，提出了在移动设备上实现低延迟高精度人脸识别的方法。

Abstract: This paper introduces FaceLiVT, a lightweight yet powerful face recognition
model that integrates a hybrid Convolution Neural Network (CNN)-Transformer
architecture with an innovative and lightweight Multi-Head Linear Attention
(MHLA) mechanism. By combining MHLA alongside a reparameterized token mixer,
FaceLiVT effectively reduces computational complexity while preserving
competitive accuracy. Extensive evaluations on challenging benchmarks;
including LFW, CFP-FP, AgeDB-30, IJB-B, and IJB-C; highlight its superior
performance compared to state-of-the-art lightweight models. MHLA notably
improves inference speed, allowing FaceLiVT to deliver high accuracy with lower
latency on mobile devices. Specifically, FaceLiVT is 8.6 faster than EdgeFace,
a recent hybrid CNN-Transformer model optimized for edge devices, and 21.2
faster than a pure ViT-Based model. With its balanced design, FaceLiVT offers
an efficient and practical solution for real-time face recognition on
resource-constrained platforms.

</details>


### [70] [FSATFusion: Frequency-Spatial Attention Transformer for Infrared and Visible Image Fusion](https://arxiv.org/abs/2506.10366)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui,Yuhan Lyu*

Main category: cs.CV

本研究提出了一种名为FSATFusion的红外与可见光图像融合方法，通过引入频率-空间注意力Transformer模块，提高了图像特征提取的质量和效率，并展现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在图像融合中存在信息损失的问题，因此，本文旨在提出一种新的方法来提升图像融合的性能。

Method: FSATFusion使用频率-空间注意力Transformer（FSAT）模块来捕捉特征图中的显著特征，并采用改进的Transformer模块（ITM）来增强全局上下文信息的提取能力。

Result: 实验结果表明，FSATFusion在图像融合质量和效率上优于其他最先进的方法，并且在其他两个无需修改的任务上表现出色。

Conclusion: FSATFusion不仅提升了图像融合的效果，还展示了良好的泛化能力和在下游视觉任务中的优越性。

Abstract: The infrared and visible images fusion (IVIF) is receiving increasing
attention from both the research community and industry due to its excellent
results in downstream applications. Existing deep learning approaches often
utilize convolutional neural networks to extract image features. However, the
inherently capacity of convolution operations to capture global context can
lead to information loss, thereby restricting fusion performance. To address
this limitation, we propose an end-to-end fusion network named the
Frequency-Spatial Attention Transformer Fusion Network (FSATFusion). The
FSATFusion contains a frequency-spatial attention Transformer (FSAT) module
designed to effectively capture discriminate features from source images. This
FSAT module includes a frequency-spatial attention mechanism (FSAM) capable of
extracting significant features from feature maps. Additionally, we propose an
improved Transformer module (ITM) to enhance the ability to extract global
context information of vanilla Transformer. We conducted both qualitative and
quantitative comparative experiments, demonstrating the superior fusion quality
and efficiency of FSATFusion compared to other state-of-the-art methods.
Furthermore, our network was tested on two additional tasks without any
modifications, to verify the excellent generalization capability of FSATFusion.
Finally, the object detection experiment demonstrated the superiority of
FSATFusion in downstream visual tasks. Our code is available at
https://github.com/Lmmh058/FSATFusion.

</details>


### [71] [Revisiting Transformers with Insights from Image Filtering](https://arxiv.org/abs/2506.10371)
*Laziz U. Abdullaev,Maksim Tkachenko,Tan M. Nguyen*

Main category: cs.CV

本文旨在通过一个统一的图像处理框架解释自注意力机制及其变体，并探讨位置编码和残差连接等组件的作用，同时提出了两种架构修改方法，这些修改方法不仅能增强解释性，还能提升模型在语言和视觉任务中的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于自注意力机制缺乏理论基础，难以解释，本文希望通过图像处理框架来深入探讨其内在机制。

Method: 采用统一的图像处理框架来解释自注意力机制，包括原始形式及其变体，并专门分析诸如位置编码和残差连接等组件的作用。

Result: 提出了两种能够提升Transformer架构解释性的修改方法，同时还发现了这些修改可以带来更好的准确性和鲁棒性。

Conclusion: 通过一个统一的图像处理视角加深了对自注意力机制的理解，并且初步验证了基于图像处理的架构修改对于提高模型性能的潜力。

Abstract: The self-attention mechanism, a cornerstone of Transformer-based
state-of-the-art deep learning architectures, is largely heuristic-driven and
fundamentally challenging to interpret. Establishing a robust theoretical
foundation to explain its remarkable success and limitations has therefore
become an increasingly prominent focus in recent research. Some notable
directions have explored understanding self-attention through the lens of image
denoising and nonparametric regression. While promising, existing frameworks
still lack a deeper mechanistic interpretation of various architectural
components that enhance self-attention, both in its original formulation and
subsequent variants. In this work, we aim to advance this understanding by
developing a unifying image processing framework, capable of explaining not
only the self-attention computation itself but also the role of components such
as positional encoding and residual connections, including numerous later
variants. We also pinpoint potential distinctions between the two concepts
building upon our framework, and make effort to close this gap. We introduce
two independent architectural modifications within transformers. While our
primary objective is interpretability, we empirically observe that image
processing-inspired modifications can also lead to notably improved accuracy
and robustness against data contamination and adversaries across language and
vision tasks as well as better long sequence understanding.

</details>


### [72] [Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial](https://arxiv.org/abs/2506.10386)
*Jerry Yan,Chinmay Talegaonkar,Nicholas Antipa,Eric Terrill,Sophia Merrifield*

Main category: cs.CV

The paper introduces a novel computer vision pipeline named PoseIDON that estimates the burial depth of objects on the seafloor, helping to assess ecological risks and plan pollution mitigation strategies with a mean error of about 10 centimeters.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge of accurately estimating the burial depth of anthropogenic objects, which are critical for understanding localized sedimentation dynamics, assessing ecological risks, potential pollutant transport, and planning recovery or mitigation strategies for hazardous materials such as munitions.

Method: This paper presents a computer vision pipeline called PoseIDON, which uses deep foundation model features combined with multiview photogrammetry to estimate the six degrees of freedom object pose and the orientation of the seafloor from remote operated vehicle (ROV) video footage. The burial depth of objects is inferred by aligning CAD models of the objects with the observed imagery and fitting a local planar approximation of the seafloor.

Result: The PoseIDON model was validated using footage of 54 objects, including barrels and munitions, recorded at a historic ocean dumpsite in the San Pedro Basin. The model achieved a mean error of approximately 10 centimeters and was able to resolve spatial burial patterns that reflect underlying sediment transport processes and ecological risks.

Conclusion: The PoseIDON approach is capable of scalable, non-invasive mapping of seafloor burial and supports environmental assessment at contaminated sites, offering a valuable tool for assessing ecological risks and supporting decision-making for pollution mitigation and recovery efforts.

Abstract: The burial state of anthropogenic objects on the seafloor provides insight
into localized sedimentation dynamics and is also critical for assessing
ecological risks, potential pollutant transport, and the viability of recovery
or mitigation strategies for hazardous materials such as munitions. Accurate
burial depth estimation from remote imagery remains difficult due to partial
occlusion, poor visibility, and object degradation. This work introduces a
computer vision pipeline, called PoseIDON, which combines deep foundation model
features with multiview photogrammetry to estimate six degrees of freedom
object pose and the orientation of the surrounding seafloor from ROV video.
Burial depth is inferred by aligning CAD models of the objects with observed
imagery and fitting a local planar approximation of the seafloor. The method is
validated using footage of 54 objects, including barrels and munitions,
recorded at a historic ocean dumpsite in the San Pedro Basin. The model
achieves a mean burial depth error of approximately 10 centimeters and resolves
spatial burial patterns that reflect underlying sediment transport processes.
This approach enables scalable, non-invasive mapping of seafloor burial and
supports environmental assessment at contaminated sites.

</details>


### [73] [DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Transformer and Mamba](https://arxiv.org/abs/2506.10390)
*Shicheng Yin,Kaixuan Yin,Yang Liu,Weixing Chen,Liang Lin*

Main category: cs.CV

研究提出了一种名为DART的自适应区域令牌生成器，该方法对信息丰富的区域进行更细粒度的编码，提升了多个模型的性能。


<details>
  <summary>Details</summary>
Motivation: 该研究针对非卷积模型在处理稀疏分布的信息对象时存在的背景区域过度编码和关键局部细节遗漏的问题，提出了一种解决方案。

Method: DART方法通过结合可学习的区域评分与分段可微分的分位数操作来实现自适应分区，对信息丰富的区域分配更密集的令牌，从而克服了传统固定大小patch的局限性。

Result: 实验表明，DART在DeiT、Vim和VideoMamba模型上均提高了准确性，同时对计算开销的影响较小或甚至减少。

Conclusion: DART通过引入大约1百万额外参数提高了2.1%的准确性，并且在保持性能的同时降低了45%的FLOPs，显示出更高的效率。

Abstract: Recently, non-convolutional models such as the Vision Transformer (ViT) and
Vision Mamba (Vim) have achieved remarkable performance in computer vision
tasks. However, their reliance on fixed-size patches often results in excessive
encoding of background regions and omission of critical local details,
especially when informative objects are sparsely distributed. To address this,
we introduce a fully differentiable Dynamic Adaptive Region Tokenizer (DART),
which adaptively partitions images into content-dependent patches of varying
sizes. DART combines learnable region scores with piecewise differentiable
quantile operations to allocate denser tokens to information-rich areas.
Despite introducing only approximately 1 million (1M) additional parameters,
DART improves accuracy by 2.1% on DeiT (ImageNet-1K). Unlike methods that
uniformly increase token density to capture fine-grained details, DART offers a
more efficient alternative, achieving 45% FLOPs reduction with superior
performance. Extensive experiments on DeiT, Vim, and VideoMamba confirm that
DART consistently enhances accuracy while incurring minimal or even reduced
computational overhead. Code is available at
https://github.com/HCPLab-SYSU/DART.

</details>


### [74] [ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion](https://arxiv.org/abs/2506.10391)
*Yuanyi Song,Pumeng Lyu,Ben Fei,Fenghua Ling,Wanli Ouyang,Lei Bai*

Main category: cs.CV

ReconMOST, a novel guided diffusion model for ocean temperature reconstruction that leverages historical data and sparse observational data, achieves high accuracy even in areas with missing data.


<details>
  <summary>Details</summary>
Motivation: The motivation for the research is to overcome the challenges posed by sparse data, complex algorithms, and high computational costs associated with conventional ocean reconstruction methods, as well as to extend machine learning-based methods beyond sea surface and local regions.

Method: A data-driven guided diffusion model framework named ReconMOST for multi-layer sea temperature reconstruction is proposed. Firstly, an unconditional diffusion model is pre-trained using a large collection of historical numerical simulation data. Then, during the generation phase, sparse, high-accuracy in-situ observational data are used as guidance points for the reverse diffusion process. This process generates accurate reconstruction results, even in regions lacking direct observational data, due to learned spatial distribution patterns.

Result: The ReconMOST model, when tested using CMIP6 and EN4 data sets, demonstrated a mean squared error of 0.049 under guidance, 0.680 for reconstruction, and overall 0.633, demonstrating effective global, multi-layer ocean temperature reconstruction, especially in areas with as much as 92.5% missing data.

Conclusion: This research effectively demonstrates the capability of the ReconMOST model to reconstruct ocean temperatures with high accuracy, even in areas with sparse data, showing the potential future use in broader climate dynamics and marine meteorological research.

Abstract: Accurate reconstruction of ocean is essential for reflecting global climate
dynamics and supporting marine meteorological research. Conventional methods
face challenges due to sparse data, algorithmic complexity, and high
computational costs, while increasing usage of machine learning (ML) method
remains limited to reconstruction problems at the sea surface and local
regions, struggling with issues like cloud occlusion. To address these
limitations, this paper proposes ReconMOST, a data-driven guided diffusion
model framework for multi-layer sea temperature reconstruction. Specifically,
we first pre-train an unconditional diffusion model using a large collection of
historical numerical simulation data, enabling the model to attain physically
consistent distribution patterns of ocean temperature fields. During the
generation phase, sparse yet high-accuracy in-situ observational data are
utilized as guidance points for the reverse diffusion process, generating
accurate reconstruction results. Importantly, in regions lacking direct
observational data, the physically consistent spatial distribution patterns
learned during pre-training enable implicitly guided and physically plausible
reconstructions. Our method extends ML-based SST reconstruction to a global,
multi-layer setting, handling over 92.5% missing data while maintaining
reconstruction accuracy, spatial resolution, and superior generalization
capability. We pre-train our model on CMIP6 numerical simulation data and
conduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The
results of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on
reconstruction, and 0.633 on total, respectively, demonstrating the
effectiveness and robustness of the proposed framework. Our source code is
available at https://github.com/norsheep/ReconMOST.

</details>


### [75] [Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation](https://arxiv.org/abs/2506.10395)
*Zhiyang Xu,Jiuhai Chen,Zhaojiang Lin,Xichen Pan,Lifu Huang,Tianyi Zhou,Madian Khabsa,Qifan Wang,Di Jin,Michihiro Yasunaga,Lili Yu,Xi Victoria Lin,Shaoliang Nie*

Main category: cs.CV

介绍了Pisces，一种自回归多模态基础模型，通过新颖的解耦视觉编码架构和定制训练技术解决了统一多模态模型在图像理解和生成上的挑战，展示了强大的跨任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决统一模型在图像理解和生成方面表现不如专门模型的问题，尤其是在视觉特性需求和训练过程不同的情况下。

Method: Pisces采用了一种创新的解耦视觉编码架构和专门的训练技术来优化多模态生成，结合了精心的数据整理、预训练和微调。

Result: 在超过20个公开的图像理解基准测试中显示了强大的性能，并且在GenEval上展示了稳健的生成能力。

Conclusion: Pisces在图像理解和生成方面都展现出了竞争力的表现，并通过广泛的分析揭示了图像理解和生成之间的协同关系，推进了统一多模态模型领域的研究。

Abstract: Recent advances in large language models (LLMs) have enabled multimodal
foundation models to tackle both image understanding and generation within a
unified framework. Despite these gains, unified models often underperform
compared to specialized models in either task. A key challenge in developing
unified models lies in the inherent differences between the visual features
needed for image understanding versus generation, as well as the distinct
training processes required for each modality. In this work, we introduce
Pisces, an auto-regressive multimodal foundation model that addresses this
challenge through a novel decoupled visual encoding architecture and tailored
training techniques optimized for multimodal generation. Combined with
meticulous data curation, pretraining, and finetuning, Pisces achieves
competitive performance in both image understanding and image generation. We
evaluate Pisces on over 20 public benchmarks for image understanding, where it
demonstrates strong performance across a wide range of tasks. Additionally, on
GenEval, a widely adopted benchmark for image generation, Pisces exhibits
robust generative capabilities. Our extensive analysis reveals the synergistic
relationship between image understanding and generation, and the benefits of
using separate visual encoders, advancing the field of unified multimodal
models.

</details>


### [76] [It's Not the Target, It's the Background: Rethinking Infrared Small Target Detection via Deep Patch-Free Low-Rank Representations](https://arxiv.org/abs/2506.10425)
*Guoyi Zhang,Guangsheng Xu,Siyang Chen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

本文提出了一种名为LRRNet的红外小目标检测（IRSTD）端到端框架，通过压缩-重构-相减（CRS）范式直接学习红外图像背景的低秩结构。LRRNet优于现有的多个先进方法，展现出了高的检测精度、鲁棒性和计算效率，同时能够实时处理。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测（IRSTD）在复杂背景下的挑战在于低信杂比（SCR）、多样的目标形态以及缺乏显著的视觉线索。尽管深度学习方法试图学习区别性表示，但小目标固有的变异性和弱先验知识往往导致性能不稳定。

Method: LRRNet方法利用红外图像背景的低秩特性，通过压缩-重构-相减（CRS）范式直接在图像域中建模结构感知的低秩背景表示。不同于基于补丁的处理或显式矩阵分解，本方法采用深度神经网络以端到端的方式学习低秩背景结构。

Result: LRRNet在多个公开数据集上进行的广泛实验表明，它在检测准确性、鲁棒性和计算效率方面优于38种最先进的方法，并实现了实时性能，平均速度达到82.34 FPS。在具有挑战性的NoisySIRST数据集上的测试进一步证明了该模型对传感器噪声的抵抗能力。

Conclusion: 该研究表明，LRRNet是一种有效的红外小目标检测方法，能够在复杂背景下稳定检测目标，同时具备实时处理能力和对传感器噪声的抵抗能力。

Abstract: Infrared small target detection (IRSTD) remains a long-standing challenge in
complex backgrounds due to low signal-to-clutter ratios (SCR), diverse target
morphologies, and the absence of distinctive visual cues. While recent deep
learning approaches aim to learn discriminative representations, the intrinsic
variability and weak priors of small targets often lead to unstable
performance. In this paper, we propose a novel end-to-end IRSTD framework,
termed LRRNet, which leverages the low-rank property of infrared image
backgrounds. Inspired by the physical compressibility of cluttered scenes, our
approach adopts a compression--reconstruction--subtraction (CRS) paradigm to
directly model structure-aware low-rank background representations in the image
domain, without relying on patch-based processing or explicit matrix
decomposition. To the best of our knowledge, this is the first work to directly
learn low-rank background structures using deep neural networks in an
end-to-end manner. Extensive experiments on multiple public datasets
demonstrate that LRRNet outperforms 38 state-of-the-art methods in terms of
detection accuracy, robustness, and computational efficiency. Remarkably, it
achieves real-time performance with an average speed of 82.34 FPS. Evaluations
on the challenging NoisySIRST dataset further confirm the model's resilience to
sensor noise. The source code will be made publicly available upon acceptance.

</details>


### [77] [MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment](https://arxiv.org/abs/2506.10430)
*Shuo wang,Jihao Zhang*

Main category: cs.CV

论文介绍了MF2Summ模型，一种基于多模态内容理解的视频摘要方法，整合视觉和听觉信息，实验结果表明MF2Summ在现有的视频摘要任务上取得了提升。


<details>
  <summary>Details</summary>
Motivation: 在线视频内容的快速增加对有效的视频摘要技术提出了需求。传统方法依赖单一模态（通常是视觉），难以捕捉视频的全部语义丰富性。因此，该论文提出了MF2Summ，这是一种基于多模态内容理解的视频摘要模型，融合了视觉和听觉信息。

Method: MF2Summ采用五阶段过程：特征提取、跨模态注意力交互、特征融合、片段预测和关键镜头选择。视觉特征使用预训练的GoogLeNet模型提取，而听觉特征使用SoundNet获取。融合机制的核心是一个跨模态Transformer和一个对齐指导自注意力Transformer，旨在有效地建模跨模态依赖和时间对应关系。

Result: 实验结果表明，MF2Summ在SumMe和TVSum数据集上取得了具有竞争力的表现，F1分数分别比DSNet模型提高了1.9%和0.6%，并优于其他最先进的方法。

Conclusion: MF2Summ通过跨模态信息融合，显著提高了视频摘要的质量和性能，证明了其在当前研究中的价值。

Abstract: The rapid proliferation of online video content necessitates effective video
summarization techniques. Traditional methods, often relying on a single
modality (typically visual), struggle to capture the full semantic richness of
videos. This paper introduces MF2Summ, a novel video summarization model based
on multimodal content understanding, integrating both visual and auditory
information. MF2Summ employs a five-stage process: feature extraction,
cross-modal attention interaction, feature fusion, segment prediction, and key
shot selection. Visual features are extracted using a pre-trained GoogLeNet
model, while auditory features are derived using SoundNet. The core of our
fusion mechanism involves a cross-modal Transformer and an alignment-guided
self-attention Transformer, designed to effectively model inter-modal
dependencies and temporal correspondences. Segment importance, location, and
center-ness are predicted, followed by key shot selection using Non-Maximum
Suppression (NMS) and the Kernel Temporal Segmentation (KTS) algorithm.
Experimental results on the SumMe and TVSum datasets demonstrate that MF2Summ
achieves competitive performance, notably improving F1-scores by 1.9\% and
0.6\% respectively over the DSNet model, and performing favorably against other
state-of-the-art methods.

</details>


### [78] [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/abs/2506.10452)
*Guowei Zhong,Ruohong Huan,Mingzhen Wu,Ronghua Liang,Peng Chen*

Main category: cs.CV

本文提出了一种名为Causal Inference Distiller (CIDer)的新型稳健多模态情感识别框架，它包括一种新的任务定义Randome Modality Feature Missing (RMFM)和两个关键组件Model-Specific Self-Distillation (MSSD)以及Model-Agnostic Causal Inference (MACI)模块。CIDer解决了多模态数据缺失和分布外数据识别的问题，并通过实验验证了其在较少参数和较快训练速度下的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别在面对数据缺失和分布外数据时存在挑战，现有方法通常依赖于特定模型或引入额外参数，这限制了其在实践中的应用。为了克服这些问题并提高系统的实用性和性能，作者提出了CIDer。

Method: CIDer通过MSSD模块增强了在RMFM任务的鲁棒性，同时通过MACI模块来解决分布外数据问题。MACI使用特定的因果图来减少标签和语言偏差，并可独立提高分布外数据的泛化能力。此外，文章引入了一个名为WSAM的模块来减少计算复杂度，以及Multimodal Composite Transformer (MCT)来促进高效的多模态融合。

Result: CIDer不仅能更好地处理多模态特征缺失和分布外数据问题，而且其鲁棒性能明显优于目前最先进的方法，同时拥有更少的参数量和更快的训练速度。

Conclusion: CIDer在处理多模态情感识别中遇到的多模态数据缺失和分布外数据问题上表现出色，它提供了更少参数和更快训练速度的鲁棒模型。

Abstract: Recent advancements in Multimodal Emotion Recognition (MER) face challenges
in addressing both modality missing and Out-Of-Distribution (OOD) data
simultaneously. Existing methods often rely on specific models or introduce
excessive parameters, which limits their practicality. To address these issues,
we propose a novel robust MER framework, Causal Inference Distiller (CIDer),
and introduce a new task, Random Modality Feature Missing (RMFM), to generalize
the definition of modality missing. CIDer integrates two key components: a
Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal
Inference (MACI) module. MSSD enhances robustness under the RMFM task through a
weight-sharing self-distillation approach applied across low-level features,
attention maps, and high-level representations. Additionally, a Word-level
Self-aligned Attention Module (WSAM) reduces computational complexity, while a
Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.
To tackle OOD challenges, MACI employs a tailored causal graph to mitigate
label and language biases using a Multimodal Causal Module (MCM) and
fine-grained counterfactual texts. Notably, MACI can independently enhance OOD
generalization with minimal additional parameters. Furthermore, we also
introduce the new repartitioned MER OOD datasets. Experimental results
demonstrate that CIDer achieves robust performance in both RMFM and OOD
scenarios, with fewer parameters and faster training compared to
state-of-the-art methods. The implementation of this work is publicly
accessible at https://github.com/gw-zhong/CIDer.

</details>


### [79] [Rethinking Generative Human Video Coding with Implicit Motion Transformation](https://arxiv.org/abs/2506.10453)
*Bolin Chen,Ru-Ling Liao,Jie Chen,Yan Ye*

Main category: cs.CV

该论文提出了一种隐式运动转化IMT方法来改善人在身体视频编码中的性能，并通过实验验证了IMT的有效性，实现了高效压缩和高质量合成。


<details>
  <summary>Details</summary>
Motivation: 由于人体视频相对于面部视频具有更复杂的运动模式，采用显式运动指导的人体视频生成编码（GHVC）可能会导致严重的失真和不准确的运动。因此，论文强调了显式运动引导方法在人体视频编码方面的局限性，并探索如何改进GHVC的性能。

Method: 此论文提出了一种使用隐式运动转化（IMT）的方法来改善人的身体视频编码性能。该方法将复杂的信号转化为紧凑的视觉特征，并利用这些特征为信号重建提供隐式运动指导。

Result: 实验结果表明，提出的IMT方法能有效地提高GHVC的性能，实现了高效的压缩和高质量的合成。

Conclusion: 该研究通过引入隐式运动转化（IMT）的方法，成功提高了人体视频生成编码（GHVC）的性能，为高效压缩和高保真合成提供了有效的解决方案。

Abstract: Beyond traditional hybrid-based video codec, generative video codec could
achieve promising compression performance by evolving high-dimensional signals
into compact feature representations for bitstream compactness at the encoder
side and developing explicit motion fields as intermediate supervision for
high-quality reconstruction at the decoder side. This paradigm has achieved
significant success in face video compression. However, compared to facial
videos, human body videos pose greater challenges due to their more complex and
diverse motion patterns, i.e., when using explicit motion guidance for
Generative Human Video Coding (GHVC), the reconstruction results could suffer
severe distortions and inaccurate motion. As such, this paper highlights the
limitations of explicit motion-based approaches for human body video
compression and investigates the GHVC performance improvement with the aid of
Implicit Motion Transformation, namely IMT. In particular, we propose to
characterize complex human body signal into compact visual features and
transform these features into implicit motion guidance for signal
reconstruction. Experimental results demonstrate the effectiveness of the
proposed IMT paradigm, which can facilitate GHVC to achieve high-efficiency
compression and high-fidelity synthesis.

</details>


### [80] [Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Intermediate Feature Distance](https://arxiv.org/abs/2506.10459)
*Chun Liu,Bingqian Zhu,Tao Xu,Zheng Zheng,Zheng Li,Wei Yang,Zhigang Han,Jiayao Wang*

Main category: cs.CV

A novel method to boost the transferability of adversarial examples in hyperspectral image(DNN-based) classification models is proposed through block-based transformations and a novel loss mechanism targeting intermediate features.


<details>
  <summary>Details</summary>
Motivation: To address the security vulnerabilities of DNNs in HSI classification due to their unique high-dimensional and spectral characteristics by improving the methodology of adversarial attacks specifically for HSIs.

Method: The method involves a novel approach to enhance adversarial example transferability for HSI classification models by randomly dividing HSIs into blocks and applying transformations to these blocks. A feature distancing loss for intermediate layers is also designed to enhance transferability.

Result: Extensive experiments show effective transferability of the adversarial examples to black-box models on two public HSI datasets, and the method maintains robustness under defensive measures.

Conclusion: The proposed method successfully enhances the transferability of adversarial examples in HSI classification models, overcoming the challenges posed by the high-dimensionality and rich spectral information in HSIs.

Abstract: Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, which pose
security challenges to hyperspectral image (HSI) classification technologies
based on DNNs. In the domain of natural images, numerous transfer-based
adversarial attack methods have been studied. However, HSIs differ from natural
images due to their high-dimensional and rich spectral information. Current
research on HSI adversarial examples remains limited and faces challenges in
fully utilizing the structural and feature information of images. To address
these issues, this paper proposes a novel method to enhance the transferability
of the adversarial examples for HSI classification models. First, while keeping
the image structure unchanged, the proposed method randomly divides the image
into blocks in both spatial and spectral dimensions. Then, various
transformations are applied on a block by block basis to increase input
diversity and mitigate overfitting. Second, a feature distancing loss targeting
intermediate layers is designed, which measures the distance between the
amplified features of the original examples and the features of the adversarial
examples as the primary loss, while the output layer prediction serves as the
auxiliary loss. This guides the perturbation to disrupt the features of the
true class in adversarial examples, effectively enhancing transferability.
Extensive experiments demonstrate that the adversarial examples generated by
the proposed method achieve effective transferability to black-box models on
two public HSI datasets. Furthermore, the method maintains robust attack
performance even under defense strategies.

</details>


### [81] [Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization](https://arxiv.org/abs/2506.10463)
*Stone Yun,Alexander Wong*

Main category: cs.CV

研究显示不同权重初始化方法对量化CNN的鲁棒性有显著影响，并提出GHN-QAT方法，利用图超网络预测量化DNN参数，提高量化精度。


<details>
  <summary>Details</summary>
Motivation: 旨在深入了解量化感知的DNN权重初始化，并提供了一种新颖的量化DNN模型设计方法，以改善量化深度神经网络的准确性。

Method: 使用Graph Hypernetworks (GHN) 预测量化DNN参数的方法，着眼于DNN训练初始条件对量化鲁棒性的影响。特别地，他们探讨了GHN预测参数的量化鲁棒性，并提出了一种新的方法——GHN-QAT，通过微调GHN来预测量化图的参数。

Result: 研究表明，不同的权重初始化方法可以显著影响训练模型的量化鲁棒性，并且GHN-QAT对于即使是4位量化，也显示出显著的准确性提高，对于2位也表现出比随机初始化更好的性能。

Conclusion: GHN-QAT提供了一种新的量化DNN模型设计方法，开辟了量化感知训练的可能，可以进一步简化DNN量化流程。

Abstract: Deep neural network (DNN) quantization for fast, efficient inference has been
an important tool in limiting the cost of machine learning (ML) model
inference. Quantization-specific model development techniques such as
regularization, quantization-aware training, and quantization-robustness
penalties have served to greatly boost the accuracy and robustness of modern
DNNs. However, very little exploration has been done on improving the initial
conditions of DNN training for quantization. Just as random weight
initialization has been shown to significantly impact test accuracy of floating
point models, it would make sense that different weight initialization methods
impact quantization robustness of trained models. We present an extensive study
examining the effects of different weight initializations on a variety of CNN
building blocks commonly used in efficient CNNs. This analysis reveals that
even with varying CNN architectures, the choice of random weight initializer
can significantly affect final quantization robustness. Next, we explore a new
method for quantization-robust CNN initialization -- using Graph Hypernetworks
(GHN) to predict parameters of quantized DNNs. Besides showing that
GHN-predicted parameters are quantization-robust after regular float32
pretraining (of the GHN), we find that finetuning GHNs to predict parameters
for quantized graphs (which we call GHN-QAT) can further improve quantized
accuracy of CNNs. Notably, GHN-QAT shows significant accuracy improvements for
even 4-bit quantization and better-than-random accuracy for 2-bits. To the best
of our knowledge, this is the first in-depth study on quantization-aware DNN
weight initialization. GHN-QAT offers a novel approach to quantized DNN model
design. Future investigations, such as using GHN-QAT-initialized parameters for
quantization-aware training, can further streamline the DNN quantization
process.

</details>


### [82] [MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models](https://arxiv.org/abs/2506.10465)
*Yu Huang,Zelin Peng,Yichen Zhao,Piao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.CV

本文提出MedSeg-R框架，通过多模态大规模语言模型解析复杂医学指令，以提高医学图像分割的精度和解析能力。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于解决现有医学图像分割模型依赖显式人类指令且缺乏理解和复杂临床问题推理能力的问题。

Method: 本文提出了一种名为MedSeg-R的端到端框架，该框架利用多模态大规模语言模型（MLLMs）的推理能力来解析临床问题，并生成精确的医学图像分割掩码。框架以两个核心组件为基础：1) 全局上下文理解模块，该模块解析图像并理解复杂的医学指令，生成多模态中间标记；2) 像素级定位模块，将这些标记解码以生成精确的分割掩码和文本响应。

Result: 实验结果显示MedSeg-R在多个基准上的优越性能，实现了高分割准确度，并使医学图像的可解释性文本分析成为可能。

Conclusion: 本文为医学图像推理分割任务引入了新的方法和大型数据集MedSeg-QA。实验表明MedSeg-R在多个基准上表现优异，提高了医学图像分割的精准度，并实现了医学图像文字分析的可解释性。

Abstract: Medical image segmentation is crucial for clinical diagnosis, yet existing
models are limited by their reliance on explicit human instructions and lack
the active reasoning capabilities to understand complex clinical questions.
While recent advancements in multimodal large language models (MLLMs) have
improved medical question-answering (QA) tasks, most methods struggle to
generate precise segmentation masks, limiting their application in automatic
medical diagnosis. In this paper, we introduce medical image reasoning
segmentation, a novel task that aims to generate segmentation masks based on
complex and implicit medical instructions. To address this, we propose
MedSeg-R, an end-to-end framework that leverages the reasoning abilities of
MLLMs to interpret clinical questions while also capable of producing
corresponding precise segmentation masks for medical images. It is built on two
core components: 1) a global context understanding module that interprets
images and comprehends complex medical instructions to generate multi-modal
intermediate tokens, and 2) a pixel-level grounding module that decodes these
tokens to produce precise segmentation masks and textual responses.
Furthermore, we introduce MedSeg-QA, a large-scale dataset tailored for the
medical image reasoning segmentation task. It includes over 10,000 image-mask
pairs and multi-turn conversations, automatically annotated using large
language models and refined through physician reviews. Experiments show
MedSeg-R's superior performance across several benchmarks, achieving high
segmentation accuracy and enabling interpretable textual analysis of medical
images.

</details>
