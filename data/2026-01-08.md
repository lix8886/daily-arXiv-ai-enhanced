<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepResearch-Slice: Bridging the Retrieval-Utilization Gap via Explicit Text Slicing](https://arxiv.org/abs/2601.03261)
*Shuo Lu,Yinuo Xu,Jianjie Cheng,Lingxiao He,Meng Wang,Jian Liang*

Main category: cs.CL

> 提出了DeepResearch-Slice框架，通过预测精确的跨度索引来解决检索到的黄金证据利用不足的问题，显著提高了六个基准的稳健性。

<details>
  <summary>Details</summary>

**Motivation:** 优化深度学习代理的检索策略以提高检索概率，但是面临即使检索到黄金证据也无法有效利用的瓶颈。

**Method:** 开发了一个简单的神经符号框架DeepResearch-Slice，能够预测精确的跨度索引执行确定性硬过滤，从而增加信息利用效率。

**Result:** 在六个基准测试中展现了显著的稳健性提升，尤其在应用到冻结的骨干网络时有73%的相对改进，从19.1%提高到了33.0%。

**Conclusion:** 实验证明了明确的锚定机制在开放性研究中的重要性，尤其是在处理噪声环境和信息利用方面。

**Abstract:** Deep Research agents predominantly optimize search policies to maximize retrieval probability. However, we identify a critical bottleneck: the retrieval-utilization gap, where models fail to use gold evidence even after it is retrieved, due to context blindness in noisy environments. To bridge this gap, we propose DeepResearch-Slice, a simple yet effective neuro-symbolic framework. Unlike implicit attention, our approach predicts precise span indices to perform a deterministic hard filter before reasoning. Extensive evaluations across six benchmarks show substantial robustness gains. Applying our method to frozen backbones yields a 73 percent relative improvement, from 19.1 percent to 33.0 percent, effectively mitigating noise without requiring parameter updates to the reasoning model. These results highlight the need for explicit grounding mechanisms in open-ended research.

</details>


### [2] [Internal Reasoning vs. External Control: A Thermodynamic Analysis of Sycophancy in Large Language Models](https://arxiv.org/abs/2601.03263)
*Edward Y. Chang*

Main category: cs.CL

> 研究显示大型语言模型容易优先考虑用户满意度而不是准确性，内部推理机制在减少此问题上的效果有限，而外部调节机制则能有效消除此问题。

<details>
  <summary>Details</summary>

**Motivation:** 探讨大型语言模型的顺从性问题，是否需要外部监管或能否通过内部机制解决。

**Method:** 使用CAP-GSM8K数据集，对比GPT-3.5、GPT-4o和GPT-5.1的不同模型使用内部机制(CoT)和外部机制(RCA)的表现。

**Result:** 内部推理机制在弱模型中导致性能下降，强模型也存在输出差距问题，而外部机制RCA能有效消除顺从性。

**Conclusion:** 得出结论，要在保证安全的基础上达到最优效率，需要强匹配的混合系统，而弱或不匹配的系统会导致效率问题，因此外部结构性约束是必须的。

**Abstract:** Large Language Models frequently exhibit sycophancy, prioritizing user agreeableness over correctness. We investigate whether this requires external regulation or can be mitigated by internal reasoning alone. Using CAP-GSM8K (N=500), an adversarial dataset, we evaluate internal (CoT) versus external (RCA) mechanisms across GPT-3.5, GPT-4o, and GPT-5.1. Our results reveal the structural limits of internal reasoning: it causes performance collapse in weak models (the Prioritization Paradox) and leaves an 11.4\% final output gap in frontier models. In contrast, RCA structurally eliminates sycophancy (0.0\%) across all tiers. We synthesize these findings into a thermodynamic hierarchy: hybrid systems achieve Resonance (optimal efficiency) only when capabilities are matched and strong, while weak or mismatched pairs succumb to Dissonance and Entropy. This confirms that external structural constraints are strictly necessary to guarantee safety.

</details>


### [3] [Jailbreak-Zero: A Path to Pareto Optimal Red Teaming for Large Language Models](https://arxiv.org/abs/2601.03265)
*Kai Hu,Abhinav Aggarwal,Mehran Khodabandeh,David Zhang,Eric Hsin,Li Chen,Ankit Jain,Matt Fredrikson,Akash Bharadwaj*

Main category: cs.CL

> Jailbreak-Zero is a new red teaming method for LLM safety evaluation which uses a policy-based framework and an attack LLM to generate adversarial prompts, resulting in higher attack success rates compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** Shifting the paradigm of LLM safety evaluation from a constrained example-based approach to a more expansive and effective policy-based framework, aiming for Pareto optimality in policy coverage, attack strategy diversity, and prompt fidelity.

**Method:** Structure

**Result:** Jailbreak-Zero demonstrates higher attack success rates against open-source and proprietary models, such as GPT-40 and Claude 3.5, with human-readable and effective adversarial prompts with minimal human intervention.

**Conclusion:** Jailbreak-Zero presents a scalable and comprehensive solution for identifying and mitigating safety vulnerabilities of LLMs.

**Abstract:** This paper introduces Jailbreak-Zero, a novel red teaming methodology that shifts the paradigm of Large Language Model (LLM) safety evaluation from a constrained example-based approach to a more expansive and effective policy-based framework. By leveraging an attack LLM to generate a high volume of diverse adversarial prompts and then fine-tuning this attack model with a preference dataset, Jailbreak-Zero achieves Pareto optimality across the crucial objectives of policy coverage, attack strategy diversity, and prompt fidelity to real user inputs. The empirical evidence demonstrates the superiority of this method, showcasing significantly higher attack success rates against both open-source and proprietary models like GPT-40 and Claude 3.5 when compared to existing state-of-the-art techniques. Crucially, Jailbreak-Zero accomplishes this while producing human-readable and effective adversarial prompts with minimal need for human intervention, thereby presenting a more scalable and comprehensive solution for identifying and mitigating the safety vulnerabilities of LLMs.

</details>


### [4] [Benchmarking and Adapting On-Device Large Language Models for Clinical Decision Support](https://arxiv.org/abs/2601.03266)
*Alif Munim,Jun Ma,Omar Ibrahim,Alhusain Abdalla,Shuolin Yin,Leo Chen,Bo Wang*

Main category: cs.CL

> 研究对比了两个开源的语言模型在临床诊断中的性能，并通过精调展示其在设备端适应性的潜力。结果表明，这些模型在临床决策支持方面具有准确性和适应性的优势，同时保护了隐私。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在临床决策中取得了快速进展，但隐私顾虑和依赖云端基础设施的问题阻碍了专有系统的部署。开源替代方案允许本地推理，但往往需要较大的模型尺寸，限制了它们在资源受限的临床环境中的应用。为了探究设备端语言模型在临床决策中的应用潜力，进行了这项研究。

**Method:** 通过benchmark测试两个设备端的语言模型(gpt-oss-20b和gpt-oss-120b)在三个临床任务上的表现：一般疾病诊断，专科（眼科）诊断和治疗，以及仿真人专家分级和评估。与业内领先的专有模型（GPT-5和o4-mini）和最优秀的开源模型（DeepSeek-R1）进行对比，并进一步通过精调gpt-oss-20b在一般诊断数据上的适应性来评估设备端系统的适应性。

**Result:** 尽管模型尺寸较小，gpt-oss模型在各项任务上的表现与DeepSeek-R1和o4-mini相当或超越。通过精调，gpt-oss-20b的诊断准确性显著提高，接近GPT-5的性能。

**Conclusion:** 研究结果表明，设备端语言模型有潜力提供准确、适应性强且保护隐私的临床决策支持，为大型语言模型更广泛地融入常规临床实践提供了一条实用的路径。

**Abstract:** Large language models (LLMs) have rapidly advanced in clinical decision-making, yet the deployment of proprietary systems is hindered by privacy concerns and reliance on cloud-based infrastructure. Open-source alternatives allow local inference but often require large model sizes that limit their use in resource-constrained clinical settings. Here, we benchmark two on-device LLMs, gpt-oss-20b and gpt-oss-120b, across three representative clinical tasks: general disease diagnosis, specialty-specific (ophthalmology) diagnosis and management, and simulation of human expert grading and evaluation. We compare their performance with state-of-the-art proprietary models (GPT-5 and o4-mini) and a leading open-source model (DeepSeek-R1), and we further evaluate the adaptability of on-device systems by fine-tuning gpt-oss-20b on general diagnostic data. Across tasks, gpt-oss models achieve performance comparable to or exceeding DeepSeek-R1 and o4-mini despite being substantially smaller. In addition, fine-tuning remarkably improves the diagnostic accuracy of gpt-oss-20b, enabling it to approach the performance of GPT-5. These findings highlight the potential of on-device LLMs to deliver accurate, adaptable, and privacy-preserving clinical decision support, offering a practical pathway for broader integration of LLMs into routine clinical practice.

</details>


### [5] [OpenAI GPT-5 System Card](https://arxiv.org/abs/2601.03267)
*Aaditya Singh,Adam Fry,Adam Perelman,Adam Tart,Adi Ganesh,Ahmed El-Kishky,Aidan McLaughlin,Aiden Low,AJ Ostrow,Akhila Ananthram,Akshay Nathan,Alan Luo,Alec Helyar,Aleksander Madry,Aleksandr Efremov,Aleksandra Spyra,Alex Baker-Whitcomb,Alex Beutel,Alex Karpenko,Alex Makelov,Alex Neitz,Alex Wei,Alexandra Barr,Alexandre Kirchmeyer,Alexey Ivanov,Alexi Christakis,Alistair Gillespie,Allison Tam,Ally Bennett,Alvin Wan,Alyssa Huang,Amy McDonald Sandjideh,Amy Yang,Ananya Kumar,Andre Saraiva,Andrea Vallone,Andrei Gheorghe,Andres Garcia Garcia,Andrew Braunstein,Andrew Liu,Andrew Schmidt,Andrey Mereskin,Andrey Mishchenko,Andy Applebaum,Andy Rogerson,Ann Rajan,Annie Wei,Anoop Kotha,Anubha Srivastava,Anushree Agrawal,Arun Vijayvergiya,Ashley Tyra,Ashvin Nair,Avi Nayak,Ben Eggers,Bessie Ji,Beth Hoover,Bill Chen,Blair Chen,Boaz Barak,Borys Minaiev,Botao Hao,Bowen Baker,Brad Lightcap,Brandon McKinzie,Brandon Wang,Brendan Quinn,Brian Fioca,Brian Hsu,Brian Yang,Brian Yu,Brian Zhang,Brittany Brenner,Callie Riggins Zetino,Cameron Raymond,Camillo Lugaresi,Carolina Paz,Cary Hudson,Cedric Whitney,Chak Li,Charles Chen,Charlotte Cole,Chelsea Voss,Chen Ding,Chen Shen,Chengdu Huang,Chris Colby,Chris Hallacy,Chris Koch,Chris Lu,Christina Kaplan,Christina Kim,CJ Minott-Henriques,Cliff Frey,Cody Yu,Coley Czarnecki,Colin Reid,Colin Wei,Cory Decareaux,Cristina Scheau,Cyril Zhang,Cyrus Forbes,Da Tang,Dakota Goldberg,Dan Roberts,Dana Palmie,Daniel Kappler,Daniel Levine,Daniel Wright,Dave Leo,David Lin,David Robinson,Declan Grabb,Derek Chen,Derek Lim,Derek Salama,Dibya Bhattacharjee,Dimitris Tsipras,Dinghua Li,Dingli Yu,DJ Strouse,Drew Williams,Dylan Hunn,Ed Bayes,Edwin Arbus,Ekin Akyurek,Elaine Ya Le,Elana Widmann,Eli Yani,Elizabeth Proehl,Enis Sert,Enoch Cheung,Eri Schwartz,Eric Han,Eric Jiang,Eric Mitchell,Eric Sigler,Eric Wallace,Erik Ritter,Erin Kavanaugh,Evan Mays,Evgenii Nikishin,Fangyuan Li,Felipe Petroski Such,Filipe de Avila Belbute Peres,Filippo Raso,Florent Bekerman,Foivos Tsimpourlas,Fotis Chantzis,Francis Song,Francis Zhang,Gaby Raila,Garrett McGrath,Gary Briggs,Gary Yang,Giambattista Parascandolo,Gildas Chabot,Grace Kim,Grace Zhao,Gregory Valiant,Guillaume Leclerc,Hadi Salman,Hanson Wang,Hao Sheng,Haoming Jiang,Haoyu Wang,Haozhun Jin,Harshit Sikchi,Heather Schmidt,Henry Aspegren,Honglin Chen,Huida Qiu,Hunter Lightman,Ian Covert,Ian Kivlichan,Ian Silber,Ian Sohl,Ibrahim Hammoud,Ignasi Clavera,Ikai Lan,Ilge Akkaya,Ilya Kostrikov,Irina Kofman,Isak Etinger,Ishaan Singal,Jackie Hehir,Jacob Huh,Jacqueline Pan,Jake Wilczynski,Jakub Pachocki,James Lee,James Quinn,Jamie Kiros,Janvi Kalra,Jasmyn Samaroo,Jason Wang,Jason Wolfe,Jay Chen,Jay Wang,Jean Harb,Jeffrey Han,Jeffrey Wang,Jennifer Zhao,Jeremy Chen,Jerene Yang,Jerry Tworek,Jesse Chand,Jessica Landon,Jessica Liang,Ji Lin,Jiancheng Liu,Jianfeng Wang,Jie Tang,Jihan Yin,Joanne Jang,Joel Morris,Joey Flynn,Johannes Ferstad,Johannes Heidecke,John Fishbein,John Hallman,Jonah Grant,Jonathan Chien,Jonathan Gordon,Jongsoo Park,Jordan Liss,Jos Kraaijeveld,Joseph Guay,Joseph Mo,Josh Lawson,Josh McGrath,Joshua Vendrow,Joy Jiao,Julian Lee,Julie Steele,Julie Wang,Junhua Mao,Kai Chen,Kai Hayashi,Kai Xiao,Kamyar Salahi,Kan Wu,Karan Sekhri,Karan Sharma,Karan Singhal,Karen Li,Kenny Nguyen,Keren Gu-Lemberg,Kevin King,Kevin Liu,Kevin Stone,Kevin Yu,Kristen Ying,Kristian Georgiev,Kristie Lim,Kushal Tirumala,Kyle Miller,Lama Ahmad,Larry Lv,Laura Clare,Laurance Fauconnet,Lauren Itow,Lauren Yang,Laurentia Romaniuk,Leah Anise,Lee Byron,Leher Pathak,Leon Maksin,Leyan Lo,Leyton Ho,Li Jing,Liang Wu,Liang Xiong,Lien Mamitsuka,Lin Yang,Lindsay McCallum,Lindsey Held,Liz Bourgeois,Logan Engstrom,Lorenz Kuhn,Louis Feuvrier,Lu Zhang,Lucas Switzer,Lukas Kondraciuk,Lukasz Kaiser,Manas Joglekar,Mandeep Singh,Mandip Shah,Manuka Stratta,Marcus Williams,Mark Chen,Mark Sun,Marselus Cayton,Martin Li,Marvin Zhang,Marwan Aljubeh,Matt Nichols,Matthew Haines,Max Schwarzer,Mayank Gupta,Meghan Shah,Melody Huang,Meng Dong,Mengqing Wang,Mia Glaese,Micah Carroll,Michael Lampe,Michael Malek,Michael Sharman,Michael Zhang,Michele Wang,Michelle Pokrass,Mihai Florian,Mikhail Pavlov,Miles Wang,Ming Chen,Mingxuan Wang,Minnia Feng,Mo Bavarian,Molly Lin,Moose Abdool,Mostafa Rohaninejad,Nacho Soto,Natalie Staudacher,Natan LaFontaine,Nathan Marwell,Nelson Liu,Nick Preston,Nick Turley,Nicklas Ansman,Nicole Blades,Nikil Pancha,Nikita Mikhaylin,Niko Felix,Nikunj Handa,Nishant Rai,Nitish Keskar,Noam Brown,Ofir Nachum,Oleg Boiko,Oleg Murk,Olivia Watkins,Oona Gleeson,Pamela Mishkin,Patryk Lesiewicz,Paul Baltescu,Pavel Belov,Peter Zhokhov,Philip Pronin,Phillip Guo,Phoebe Thacker,Qi Liu,Qiming Yuan,Qinghua Liu,Rachel Dias,Rachel Puckett,Rahul Arora,Ravi Teja Mullapudi,Raz Gaon,Reah Miyara,Rennie Song,Rishabh Aggarwal,RJ Marsan,Robel Yemiru,Robert Xiong,Rohan Kshirsagar,Rohan Nuttall,Roman Tsiupa,Ronen Eldan,Rose Wang,Roshan James,Roy Ziv,Rui Shu,Ruslan Nigmatullin,Saachi Jain,Saam Talaie,Sam Altman,Sam Arnesen,Sam Toizer,Sam Toyer,Samuel Miserendino,Sandhini Agarwal,Sarah Yoo,Savannah Heon,Scott Ethersmith,Sean Grove,Sean Taylor,Sebastien Bubeck,Sever Banesiu,Shaokyi Amdo,Shengjia Zhao,Sherwin Wu,Shibani Santurkar,Shiyu Zhao,Shraman Ray Chaudhuri,Shreyas Krishnaswamy,Shuaiqi,Xia,Shuyang Cheng,Shyamal Anadkat,Simón Posada Fishman,Simon Tobin,Siyuan Fu,Somay Jain,Song Mei,Sonya Egoian,Spencer Kim,Spug Golden,SQ Mah,Steph Lin,Stephen Imm,Steve Sharpe,Steve Yadlowsky,Sulman Choudhry,Sungwon Eum,Suvansh Sanjeev,Tabarak Khan,Tal Stramer,Tao Wang,Tao Xin,Tarun Gogineni,Taya Christianson,Ted Sanders,Tejal Patwardhan,Thomas Degry,Thomas Shadwell,Tianfu Fu,Tianshi Gao,Timur Garipov,Tina Sriskandarajah,Toki Sherbakov,Tomer Kaftan,Tomo Hiratsuka,Tongzhou Wang,Tony Song,Tony Zhao,Troy Peterson,Val Kharitonov,Victoria Chernova,Vineet Kosaraju,Vishal Kuo,Vitchyr Pong,Vivek Verma,Vlad Petrov,Wanning Jiang,Weixing Zhang,Wenda Zhou,Wenlei Xie,Wenting Zhan,Wes McCabe,Will DePue,Will Ellsworth,Wulfie Bain,Wyatt Thompson,Xiangning Chen,Xiangyu Qi,Xin Xiang,Xinwei Shi,Yann Dubois,Yaodong Yu,Yara Khakbaz,Yifan Wu,Yilei Qian,Yin Tat Lee,Yinbo Chen,Yizhen Zhang,Yizhong Xiong,Yonglong Tian,Young Cha,Yu Bai,Yu Yang,Yuan Yuan,Yuanzhi Li,Yufeng Zhang,Yuguang Yang,Yujia Jin,Yun Jiang,Yunyun Wang,Yushi Wang,Yutian Liu,Zach Stubenvoll,Zehao Dou,Zheng Wu,Zhigang Wang*

Main category: cs.CL

> GPT-5 是一个集成系统，包含了一个快速回答大多数问题的智能模型，一个用于解决复杂问题的深入推理模型，以及一个实时路由器，该路由器能够根据对话类型、复杂性、工具需求和明确意图来快速决定使用哪个模型。该系统在减少幻觉、提高指令跟随能力和最小化逢迎方面取得了显著进展，并且在写作、编程和健康领域表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 开发 GPT-5 的目的是为了提升模型的实用性，使它能够更快更准确地回答问题，同时减少幻觉和提高指令跟随能力。此外，该系统还包括了最新的安全训练方法，以防止产生不允许的内容。

**Method:** GPT-5 采用了智能模型、深入推理模型和实时路由器的方法。智能模型用于快速回答简单问题，深入推理模型用于解决复杂问题，而实时路由器根据对话的具体情况进行模型的选择和切换。

**Result:** GPT-5 在减少幻觉和提高指令跟随能力方面取得了显著改进，并且在写作、编程和健康领域表现出色。系统的实时路由器根据实时信号进行自我训练，进一步提高系统性能。

**Conclusion:** GPT-5 不仅在基准测试和问题回答速度上超越了之前的模型，在实用性方面也有显著提升。同时，该系统采取了预防措施来应对潜在的高能力威胁，例如生物和化学领域的能力。

**Abstract:** This is the system card published alongside the OpenAI GPT-5 launch, August 2025.
  GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent (for example, if you say 'think hard about this' in the prompt). The router is continuously trained on real signals, including when users switch models, preference rates for responses, and measured correctness, improving over time. Once usage limits are reached, a mini version of each model handles remaining queries.
  This system card focuses primarily on gpt-5-thinking and gpt-5-main, while evaluations for other models are available in the appendix. The GPT-5 system not only outperforms previous models on benchmarks and answers questions more quickly, but -- more importantly -- is more useful for real-world queries. We've made significant advances in reducing hallucinations, improving instruction following, and minimizing sycophancy, and have leveled up GPT-5's performance in three of ChatGPT's most common uses: writing, coding, and health. All of the GPT-5 models additionally feature safe-completions, our latest approach to safety training to prevent disallowed content.
  Similarly to ChatGPT agent, we have decided to treat gpt-5-thinking as High capability in the Biological and Chemical domain under our Preparedness Framework, activating the associated safeguards. While we do not have definitive evidence that this model could meaningfully help a novice to create severe biological harm -- our defined threshold for High capability -- we have chosen to take a precautionary approach.

</details>


### [6] [WRAVAL -- WRiting Assist eVALuation](https://arxiv.org/abs/2601.03268)
*Gabriel Benedict,Matthew Butler,Naved Merchant,Eetu Salama-Laine*

Main category: cs.CL

> 研究表明小型语言模型在常识应用任务中表现出色，提出了一个评估框架来凸显其在特定任务上的潜力，适用于边缘计算和私有计算场景。

<details>
  <summary>Details</summary>

**Motivation:** 此前的评价体系未能充分展现小型语言模型在实际工业应用场景中的表现，因此需要制定新的评估框架。

**Method:** 新框架结合新型数据生成、提示优化和大型语言模型评估方法，展示任务特定微调的效果。

**Result:** 新型评估方法为小型语言模型在非推理任务中的应用潜力提供了新的视角。

**Conclusion:** 这项工作提供了工具，使实践者能够更有效地对小型语言模型和大型语言模型进行基准测试，特别是在边缘和私有计算环境中。

**Abstract:** The emergence of Large Language Models (LLMs) has shifted language model evaluation toward reasoning and problem-solving tasks as measures of general intelligence. Small Language Models (SLMs) -- defined here as models under 10B parameters -- typically score 3-4 times lower than LLMs on these metrics. However, we demonstrate that these evaluations fail to capture SLMs' effectiveness in common industrial applications, such as tone modification tasks (e.g., funny, serious, professional). We propose an evaluation framework specifically designed to highlight SLMs' capabilities in non-reasoning tasks where predefined evaluation datasets don't exist. Our framework combines novel approaches in data generation, prompt-tuning, and LLM-based evaluation to demonstrate the potential of task-specific finetuning. This work provides practitioners with tools to effectively benchmark both SLMs and LLMs for practical applications, particularly in edge and private computing scenarios. Our implementation is available at: https://github.com/amazon-science/wraval.

</details>


### [7] [The Instruction Gap: LLMs get lost in Following Instruction](https://arxiv.org/abs/2601.03269)
*Vishesh Tripathi,Uday Allu,Biddwan Ahmed*

Main category: cs.CL

> 本文研究了大型语言模型在企业环境中面临的主要挑战——对定制指令的一致遵守问题，并提供了模型性能基准。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型在实际部署中表现出的一致性不足，研究旨在评估这些模型在企业级应用中遵守指令的能力。

**Method:** 该研究通过系统化的测试与企业级评估协议，对13个领先的大语言模型在指令遵守、响应准确性和实际RAG（检索增强生成）场景中的性能进行了全面评估。

**Result:** 研究表明，不同模型在遵循指令方面的表现差异显著，Claude-Sonnet-4和GPT-5表现最佳。

**Conclusion:** 研究揭示了所谓的'指令差距'，这使得模型在企业部署中面临挑战，并为组织提供实际见解和部署基准。

**Abstract:** Large Language Models (LLMs) have shown remarkable capabilities in natural language understanding and generation, yet their deployment in enterprise environments reveals a critical limitation: inconsistent adherence to custom instructions. This study presents a comprehensive evaluation of 13 leading LLMs across instruction compliance, response accuracy, and performance metrics in realworld RAG (Retrieval-Augmented Generation) scenarios. Through systematic testing with samples and enterprise-grade evaluation protocols, we demonstrate that instruction following varies dramatically across models, with Claude-Sonnet-4 and GPT-5 achieving the highest results. Our findings reveal the "instruction gap" - a fundamental challenge where models excel at general tasks but struggle with precise instruction adherence required for enterprise deployment. This work provides practical insights for organizations deploying LLM-powered solutions and establishes benchmarks for instruction-following capabilities across major model families.

</details>


### [8] [Advances and Challenges in Semantic Textual Similarity: A Comprehensive Survey](https://arxiv.org/abs/2601.03270)
*Lokendra Kumar,Neelesh S. Upadhye,Kannan Piedy*

Main category: cs.CL

> 这篇论文概述了自2021年以来语义文本相似度（STS）研究在transformer架构、对比学习和领域特定技术方面取得的进展。

<details>
  <summary>Details</summary>

**Motivation:** 该论文旨在通过回顾STS发展的六个领域，包括基于transformer的模型、对比学习、领域专用解决方案、多模态方法、基于图的方法以及知识增强技术，来提供深入的发展视角。

**Method:** 分析论文摘要

**Result:** 最近的如FarSSiBERT和DeBERTa-v3等模型取得了显著的准确率提升；对比方法如AspectCSE建立了新的基准；领域适应模型如CXR-BERT和Financial-STS展示了STS如何有效地被定制以适应专业领域。

**Conclusion:** 该论文分析了当前的发展趋势，为研究人员和实践者提供了有价值的见解，并指出了STS领域的未来挑战和机遇。

**Abstract:** Semantic Textual Similarity (STS) research has expanded rapidly since 2021, driven by advances in transformer architectures, contrastive learning, and domain-specific techniques. This survey reviews progress across six key areas: transformer-based models, contrastive learning, domain-focused solutions, multi-modal methods, graph-based approaches, and knowledge-enhanced techniques. Recent transformer models such as FarSSiBERT and DeBERTa-v3 have achieved remarkable accuracy, while contrastive methods like AspectCSE have established new benchmarks. Domain-adapted models, including CXR-BERT for medical texts and Financial-STS for finance, demonstrate how STS can be effectively customized for specialized fields. Moreover, multi-modal, graph-based, and knowledge-integrated models further enhance semantic understanding and representation. By organizing and analyzing these developments, the survey provides valuable insights into current methods, practical applications, and remaining challenges. It aims to guide researchers and practitioners alike in navigating rapid advancements, highlighting emerging trends and future opportunities in the evolving field of STS.

</details>


### [9] [Less is more: Not all samples are effective for evaluation](https://arxiv.org/abs/2601.03272)
*Wentang Song,Jinqiang Li,Kele Huang,Junhui Lin,Shengxiang Wu,Zhongshi Xie*

Main category: cs.CL

> 本文提出了一种新的测试集压缩方法，能够在无需历史数据的情况下，通过适应领域的嵌入空间和聚类技术，有效地压缩大规模测试数据集，大幅降低计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 已有压缩方法依赖于来自多个历史模型在完整测试集上的正确性标签，这在冷启动场景中不适用，例如新任务、新领域或新模型的引入，没有先前的评估历史。为解决此限制，本研究开发了一个无需历史模型表现数据的测试集压缩框架。

**Method:** 本研究提出了一种无需历史数据的测试集压缩框架。首先，通过少量领域特定数据对基础语言模型进行微调，以便其能够内化任务相关的语义。接着，仅使用原始文本内容为所有测试样本生成高层次语义嵌入。在该领域适应的嵌入空间中，进行任务感知聚类，并引入一种新颖的数据集X光机制，根据聚类几何结构动态校准压缩强度，基于基准的内在冗余。

**Result:** 实验表明，本方法在大规模3GPP通信基准测试中能有效识别并移除冗余样本，将评估成本降低90%以上，同时保持对完整基准的高度保真。

**Conclusion:** 该方法证明了在新任务或模型引入时，通过少量领域特定数据微调基础语言模型，并利用新颖的数据集X光机制分析语义嵌入空间中的几何结构，可以有效地识别和删除冗余测试数据，达到大幅降低评估成本而不影响高保真度的效果。

**Abstract:** The versatility of Large Language Models (LLMs) in vertical domains has spurred the development of numerous specialized evaluation benchmarks. However, these benchmarks often suffer from significant semantic redundancy and impose high computational costs during evaluation. Existing compression methods, such as tinyBenchmarks depend critically on correctness labels from multiple historical models evaluated on the full test set, making them inapplicable in cold-start scenarios, such as the introduction of a new task, domain, or model with no prior evaluation history.
  To address this limitation, we propose a history-free test set compression framework that requires no prior model performance data. Our method begins by fine-tuning a base LLM on a small amount of domain-specific data to internalize task-relevant semantics. It then generates high-level semantic embeddings for all original test samples using only their raw textual content. In this domain-adapted embedding space, we perform task-aware clustering and introduce a novel dataset X-ray mechanism that analyzes cluster geometry to dynamically calibrate the compression intensity based on the intrinsic redundancy of the benchmark.
  Experiments on professional-domain dataset, notably a large-scale 3GPP communications benchmark, demonstrate that our approach effectively identifies and removes redundant samples, reducing evaluation cost by over 90% while preserving high fidelity to the full benchmark.

</details>


### [10] [GuardEval: A Multi-Perspective Benchmark for Evaluating Safety, Fairness, and Robustness in LLM Moderators](https://arxiv.org/abs/2601.03273)
*Naseem Machlovi,Maryam Saleki,Ruhul Amin,Mohamed Rahouti,Shawqi Al-Maliki,Junaid Qadir,Mohamed M. Abdallah,Ala Al-Fuqaha*

Main category: cs.CL

> 提出了GuardEval多视角基准数据集和GemmaGuard模型，用以提高复杂边缘情况下的安全性、公平性和鲁棒性，显著优于已有审查模型。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的大型语言模型（LLMs）难以处理复杂的语境和主观问题，如隐性的侮辱性内容、微妙的性别和种族偏见，以及监狱越狱提示，因此迫切需要更安全的审查系统。

**Method:** 我们提出了GuardEval，这是一个综合的多视角基准数据集，用于训练和评估，包含106种细粒度类别，涵盖了人类情感、侮辱性和仇恨语言、性别和种族偏见以及更广泛的安全问题。我们还展示了GemmaGuard（GGuard），这是在GuardEval上训练的Gemma3-12B的QLoRA微调版本，用于使用细粒度标签评估内容审查。

**Result:** 我们的评估表明，GGuard在宏观F1分数上达到了0.832，显著优于领先的内容审查模型，如OpenAI Moderator（0.64）和Llama Guard（0.61）。

**Conclusion:** GuardEval和GGuard一起证明了，多样化、代表性的数据对于减少有偏见和不一致的审查决定至关重要。

**Abstract:** As large language models (LLMs) become deeply embedded in daily life, the urgent need for safer moderation systems, distinguishing between naive from harmful requests while upholding appropriate censorship boundaries, has never been greater. While existing LLMs can detect harmful or unsafe content, they often struggle with nuanced cases such as implicit offensiveness, subtle gender and racial biases, and jailbreak prompts, due to the subjective and context-dependent nature of these issues. Furthermore, their heavy reliance on training data can reinforce societal biases, resulting in inconsistent and ethically problematic outputs. To address these challenges, we introduce GuardEval, a unified multi-perspective benchmark dataset designed for both training and evaluation, containing 106 fine-grained categories spanning human emotions, offensive and hateful language, gender and racial bias, and broader safety concerns. We also present GemmaGuard (GGuard), a QLoRA fine-tuned version of Gemma3-12B trained on GuardEval, to assess content moderation with fine-grained labels. Our evaluation shows that GGuard achieves a macro F1 score of 0.832, substantially outperforming leading moderation models, including OpenAI Moderator (0.64) and Llama Guard (0.61). We show that multi-perspective, human-centered safety benchmarks are critical for reducing biased and inconsistent moderation decisions. GuardEval and GGuard together demonstrate that diverse, representative data materially improve safety, fairness, and robustness on complex, borderline cases.

</details>


### [11] [LLM_annotate: A Python package for annotating and analyzing fiction characters](https://arxiv.org/abs/2601.03274)
*Hannes Rosenbusch*

Main category: cs.CL

> LLM_annotate是一个用于分析虚构角色性格特点的Python包，支持多种语言模型，可用于行为标注、性格推断和质量验证。

<details>
  <summary>Details</summary>

**Motivation:** 该工具旨在提供一个高效且可重复使用的角色分析方法，使得研究人员能够利用各种类型的大型语言模型（如商业、开源或自定义的模型）进行研究。

**Method:** LLM_annotate是一个Python包，用于利用大型语言模型分析虚构角色的性格。它包括文本分块、基于LLM的标注、角色名称消歧、质量评分以及计算角色级别的统计和嵌入等功能，以标准化对完整文本（如书籍和电影剧本）中角色行为的标注流程，并通过人机交互界面验证标注与推理的质量。

**Result:** 通过使用《辛普森一家》电影和小说《傲慢与偏见》作为示例，展示了LLM_annotate包在高效和可重复的角色分析中的应用。

**Conclusion:** LLM_annotate为研究虚构角色提供了强大的工具，使得角色行为的标注、性格特征的推断以及人机互动验证变得标准化且高效。

**Abstract:** LLM_annotate is a Python package for analyzing the personality of fiction characters with large language models. It standardizes workflows for annotating character behaviors in full texts (e.g., books and movie scripts), inferring character traits, and validating annotation/inference quality via a human-in-the-loop GUI. The package includes functions for text chunking, LLM-based annotation, character name disambiguation, quality scoring, and computation of character-level statistics and embeddings. Researchers can use any LLM, commercial, open-source, or custom, within LLM_annotate. Through tutorial examples using The Simpsons Movie and the novel Pride and Prejudice, I demonstrate the usage of the package for efficient and reproducible character analyses.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [12] [HyperCLOVA X 32B Think](https://arxiv.org/abs/2601.03286)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.CV

> HyperCLOVA X 32B Think是一款注重韩国语言和文化背景推理能力的视觉语言模型，展现出了强大的多模态理解和代理行为能力。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是开发一款在韩国语言和文化背景中具有强力推理能力的模型，以支持多模态理解和代理行为，同时也能符合人类偏好。

**Method:** 该模型经过了预训练和后训练，预训练时着重提高推理能力，后训练时支持多模态理解和增强的代理行为。

**Result:** 实验结果显示，与类似规模的模型相比，该模型在韩文文本到文本和视觉到文本基准测试以及代理导向评估任务中表现出色。

**Conclusion:** 通过开源HyperCLOVA X 32B Think，研究人员希望能够促进学术界和工业界更广泛的采用和进一步研究创新。

**Abstract:** In this report, we present HyperCLOVA X 32B Think, a vision-language model designed with particular emphasis on reasoning within the Korean linguistic and cultural context, as well as agentic ability. HyperCLOVA X 32B Think is pre-trained with a strong focus on reasoning capabilities and subsequently post-trained to support multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences. Experimental evaluations against comparably sized models demonstrate that our model achieves strong performance on Korean text-to-text and vision-to-text benchmarks, as well as on agent-oriented evaluation tasks. By open-sourcing HyperCLOVA X 32B Think, we aim to support broader adoption and facilitate further research and innovation across both academic and industrial communities.

</details>


### [13] [CageDroneRF: A Large-Scale RF Benchmark and Toolkit for Drone Perception](https://arxiv.org/abs/2601.03302)
*Mohammad Rostami,Atik Faysal,Hongtao Xia,Hadi Kasasbeh,Ziang Gao,Huaxia Wang*

Main category: cs.CV

> CageDroneRF (CDRF)是一个RF无人机检测和识别的大规模基准数据集，旨在解决现有数据集的稀有性和多样性不足的问题。

<details>
  <summary>Details</summary>

**Motivation:** CageDroneRF (CDRF) 解决了现有RF数据集稀少和多样性不足的问题，通过提供一个包含广泛现代无人机模型和采集条件的数据集，这些无人机模型在当前的公共数据集中往往是不可用的。

**Method:** 我们提出了CageDroneRF (CDRF)，这是一个基于真实世界捕获和系统生成的合成变种的大规模无人机RF检测和识别基准数据集。通过结合广泛的原始记录和一个精心设计的数据增强管线，该管线能够精确控制信噪比(SNR)，注入干扰发射器，并应用频率偏移以及标签一致的边界框转换以供检测使用。

**Result:** CDRF 附带了开放源代码工具，用于数据生成、预处理、增强和评估，这些工具不仅可以应用于现有的公共基准数据集，还可以支持分类、开放式识别和对象检测等标准化基准测试。

**Conclusion:** 通过发布这一全面的基准数据集和工具，CageDroneRF (CDRF)旨在加速开发强大且通用的RF感知模型的进程。

**Abstract:** We present CageDroneRF (CDRF), a large-scale benchmark for Radio-Frequency (RF) drone detection and identification built from real-world captures and systematically generated synthetic variants. CDRF addresses the scarcity and limited diversity of existing RF datasets by coupling extensive raw recordings with a principled augmentation pipeline that (i) precisely controls Signal-to-Noise Ratio (SNR), (ii) injects interfering emitters, and (iii) applies frequency shifts with label-consistent bounding-box transformations for detection. This dataset spans a wide range of contemporary drone models, many unavailable in current public datasets, and acquisition conditions, derived from data collected at the Rowan University campus and within a controlled RF-cage facility. CDRF is released with interoperable open-source tools for data generation, preprocessing, augmentation, and evaluation that also operate on existing public benchmarks. CDRF enables standardized benchmarking for classification, open-set recognition, and object detection, supporting rigorous comparisons and reproducible pipelines. By releasing this comprehensive benchmark and tooling, CDRF aims to accelerate progress toward robust, generalizable RF perception models.

</details>


### [14] [Mass Concept Erasure in Diffusion Models with Concept Hierarchy](https://arxiv.org/abs/2601.03305)
*Jiahang Tu,Ye Li,Yiming Wu,Hanbin Zhao,Chao Zhang,Hui Qian*

Main category: cs.CV

> 论文提出了一种名为SuPLoRA的新方法来解决扩散模型中概念擦除问题，通过建立层次结构和群组抑制策略，提高了擦除方法的性能和效率。

<details>
  <summary>Details</summary>

**Motivation:** 论文针对扩散模型生成不安全或有害内容的问题，提出了一个更为高效和有效的方法来处理概念擦除，特别是解决了随着被擦除概念数量增加，现有方法变得低效的问题。

**Method:** 研究设计了一个超类型-子类型的概念层次结构，将不同的子类型概念分组在一起，并通过共享一组可学习参数来进行群组抑制。在擦除阶段，使用扩散正则化来保持未被掩码区域的去噪过程。此外，提出了SuPLoRA方法，通过调整矩阵投影来防止超类型生成性能的下降。

**Result:** 该论文提出了一种超类型-子类型的概念层次结构，用于在去除有害概念时提高方法的效率和效果。通过将相关的子类型概念分组并在一个超类型节点下聚合，可以共享一组可学习参数来同时抑制相似概念，而不是单独处理每个概念。此外，还提出了一种名为超类型保持低秩适应（SuPLoRA）的方法，以防止在过多去除相关的子类型概念时对超类型生成的损害。该方法将超类型概念信息编码到固定的下投影矩阵中，并仅在去除时更新上投影矩阵。理论分析表明SuPLoRA在减少产生性能下降方面是有效的。论文构建了一个新的基准测试，要求同时在不同领域（如名人、物体和色情内容）中去除概念。

**Conclusion:** 该论文证明了采用超类型-子类型层次结构和群组抑制方法可以在不牺牲整体生成质量的情况下更有效地执行概念擦除。SuPLoRA方法的成功应用展示了一种有前途的解决方案，能够在不同领域同时擦除多个概念时保持较高的性能。

**Abstract:** The success of diffusion models has raised concerns about the generation of unsafe or harmful content, prompting concept erasure approaches that fine-tune modules to suppress specific concepts while preserving general generative capabilities. However, as the number of erased concepts grows, these methods often become inefficient and ineffective, since each concept requires a separate set of fine-tuned parameters and may degrade the overall generation quality. In this work, we propose a supertype-subtype concept hierarchy that organizes erased concepts into a parent-child structure. Each erased concept is treated as a child node, and semantically related concepts (e.g., macaw, and bald eagle) are grouped under a shared parent node, referred to as a supertype concept (e.g., bird). Rather than erasing concepts individually, we introduce an effective and efficient group-wise suppression method, where semantically similar concepts are grouped and erased jointly by sharing a single set of learnable parameters. During the erasure phase, standard diffusion regularization is applied to preserve denoising process in unmasked regions. To mitigate the degradation of supertype generation caused by excessive erasure of semantically related subtypes, we propose a novel method called Supertype-Preserving Low-Rank Adaptation (SuPLoRA), which encodes the supertype concept information in the frozen down-projection matrix and updates only the up-projection matrix during erasure. Theoretical analysis demonstrates the effectiveness of SuPLoRA in mitigating generation performance degradation. We construct a more challenging benchmark that requires simultaneous erasure of concepts across diverse domains, including celebrities, objects, and pornographic content.

</details>


### [15] [VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models](https://arxiv.org/abs/2601.03309)
*Jianke Zhang,Xiaoyu Chen,Qiuyue Wang,Mingsheng Li,Yanjiang Guo,Yucheng Hu,Jiajun Zhang,Shuai Bai,Junyang Lin,Jianyu Chen*

Main category: cs.CV

> 研究探讨了Vision-Language Models (VLM)对Vision-Language-Action (VLA)模型性能的影响，通过简单的转换pipeline发现，虽然VLM的初始配置提供了显著的优势，但它们的通用能力而不是主要的性能瓶颈，视觉模块的表现更为关键。

<details>
  <summary>Details</summary>

**Motivation:** 探究VLM的选择和能力如何影响VLA策略的表现，以及VLM的通用能力在多大程度上能够预测其在下游任务中的表现。这有助于理解VLM在复杂任务中的适应性和改进方向。

**Method:** 开发了一个叫VLM4VLA的简洁转换pipeline，利用少量可学习参数将通用VLM转换为VLA策略，然后比较了多种VLM在不同下游任务上的性能。同时也研究了具体的体感能力对性能的影响和模态层面的剥离分析。

**Result:** 实验结果表明，VLM的初始配置提供了一定的优势，但其通用能力并不能准确预测在下游任务上的表现。此外，视觉模组的表现比语言部分更加关键。加入与控制相关的监督到视觉编码器中可以在下游任务上带来更好的性能提升。

**Conclusion:** 尽管VLM的初始性能是有益的，但它们的通用能力不足以确保在复杂的VLA任务中的表现。视觉模块的实际表现才是关键因素，所以需要新的预训练目标来更好地满足实体行动规划的需求。

**Abstract:** Vision-Language-Action (VLA) models, which integrate pretrained large Vision-Language Models (VLM) into their policy backbone, are gaining significant attention for their promising generalization capabilities. This paper revisits a fundamental yet seldom systematically studied question: how VLM choice and competence translate to downstream VLA policies performance? We introduce VLM4VLA, a minimal adaptation pipeline that converts general-purpose VLMs into VLA policies using only a small set of new learnable parameters for fair and efficient comparison. Despite its simplicity, VLM4VLA proves surprisingly competitive with more sophisticated network designs. Through extensive empirical studies on various downstream tasks across three benchmarks, we find that while VLM initialization offers a consistent benefit over training from scratch, a VLM's general capabilities are poor predictors of its downstream task performance. This challenges common assumptions, indicating that standard VLM competence is necessary but insufficient for effective embodied control. We further investigate the impact of specific embodied capabilities by fine-tuning VLMs on seven auxiliary embodied tasks (e.g., embodied QA, visual pointing, depth estimation). Contrary to intuition, improving a VLM's performance on specific embodied skills does not guarantee better downstream control performance. Finally, modality-level ablations identify the visual module in VLM, rather than the language component, as the primary performance bottleneck. We demonstrate that injecting control-relevant supervision into the vision encoder of the VLM yields consistent gains, even when the encoder remains frozen during downstream fine-tuning. This isolates a persistent domain gap between current VLM pretraining objectives and the requirements of embodied action-planning.

</details>


### [16] [Deep Learning-Based Image Recognition for Soft-Shell Shrimp Classification](https://arxiv.org/abs/2601.03317)
*Yun-Hao Zhang,I-Hsien Ting,Dario Liberona,Yun-Hsiu Liu,Kazunori Minetaki*

Main category: cs.CV

> 研究采用深度学习图像识别技术，利用CNN模型对初捕捞的白虾进行自动分类，以保持虾的新鲜度和外观完整性，有效提升加工效率。

<details>
  <summary>Details</summary>

**Motivation:** 随着信息技术融入水产养殖，生产变得更加稳定并保持每年增长。然而，消费者对高质量水产品的需求日益增加，新鲜度和外观完整性成为关键问题。特别是加工食品中的虾，捕捞后新鲜度迅速下降，而软壳虾在烹饪或冷冻后常出现头身分离，影响产品外观和消费者认知。

**Method:** 本研究利用基于深度学习的图像识别技术，对初捕捞的白虾进行自动分类。通过使用卷积神经网络（CNN）模型替代人工分类，提升分类的准确性、效率和一致性。

**Result:** 该技术通过减少加工时间，有助于保持虾的新鲜度，并使虾运输业务更有效地满足客户需求。

**Conclusion:** 研究成功利用深度学习图像识别技术提高虾的分类过程，这对提升虾产品的质量和消费者满意度具有重要意义。

**Abstract:** With the integration of information technology into aquaculture, production has become more stable and continues to grow annually. As consumer demand for high-quality aquatic products rises, freshness and appearance integrity are key concerns. In shrimp-based processed foods, freshness declines rapidly post-harvest, and soft-shell shrimp often suffer from head-body separation after cooking or freezing, affecting product appearance and consumer perception. To address these issues, this study leverages deep learning-based image recognition for automated classification of white shrimp immediately after harvest. A convolutional neural network (CNN) model replaces manual sorting, enhancing classification accuracy, efficiency, and consistency. By reducing processing time, this technology helps maintain freshness and ensures that shrimp transportation businesses meet customer demands more effectively.

</details>


### [17] [Higher order PCA-like rotation-invariant features for detailed shape descriptors modulo rotation](https://arxiv.org/abs/2601.03326)
*Jarek Duda*

Main category: cs.CV

> 论文扩展了PCA方法，使用高阶张量和多项式乘以高斯函数构建可解码的形状描述符，适用于分子形状描述和2D/3D旋转物体识别。

<details>
  <summary>Details</summary>

**Motivation:** 动机是PCA虽然可以用于获取旋转不变性的特征，但现实中的形状往往复杂得多，需要更高阶的描述符来准确捕捉其特性。

**Method:** 提出了PCA方法的扩展，使用三阶或更高阶的张量描述中心矩，或者多项式乘以高斯函数构建任意高精度的可解码形状描述符，来获取旋转不变性特征。

**Result:** 该研究创建了可用于分子形状描述符或2D图像/3D扫描中的旋转不变性物体识别的形状描边符，以及允许在不计算优化旋转的情况下进行廉价比较的形状相似性度量。

**Conclusion:** 结论是这种更复杂的形状描述符可以提供更高精度，并且它们的旋转不变量可以应用于旋转不变特征的提取，这可以用于不同领域的形状描述和识别。

**Abstract:** PCA can be used for rotation invariant features, describing a shape with its $p_{ab}=E[(x_i-E[x_a])(x_b-E[x_b])]$ covariance matrix approximating shape by ellipsoid, allowing for rotation invariants like its traces of powers. However, real shapes are usually much more complicated, hence there is proposed its extension to e.g. $p_{abc}=E[(x_a-E[x_a])(x_b-E[x_b])(x_c-E[x_c])]$ order-3 or higher tensors describing central moments, or polynomial times Gaussian allowing decodable shape descriptors of arbitrarily high accuracy, and their analogous rotation invariants. Its practical applications could be rotation-invariant features to include shape modulo rotation e.g. for molecular shape descriptors, or for up to rotation object recognition in 2D images/3D scans, or shape similarity metric allowing their inexpensive comparison (modulo rotation) without costly optimization over rotations.

</details>


### [18] [MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.03331)
*Yang Shi,Yifeng Xie,Minzhe Guo,Liangsi Lu,Mingxuan Huang,Jingchao Wang,Zhihong Zhu,Boyan Xu,Zhiqi Huang*

Main category: cs.CV

> The paper presents MMErroR, a benchmark for evaluating VLMs' ability to detect reasoning errors within multi-modal samples, demonstrating the challenges faced by current models in error detection and classification.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to assess whether Vision-Language Models (VLMs) can identify and classify reasoning errors beyond merely providing correct answers, thereby probing deeper into their understanding and reasoning capabilities.

**Method:** We introduce MMErroR, a novel multi-modal benchmark consisting of 2,013 samples with each sample containing a single coherent reasoning error. This benchmark is designed to evaluate VLMs' ability to detect and classify reasoning errors in visual and linguistic contexts.

**Result:** Evaluation of 20 VLMs with MMErroR shows that even the best model (Gemini-3.0-Pro) only correctly identifies the error type in 66.47% of cases, highlighting the ongoing challenge of accurate error detection in VLMs.

**Conclusion:** The study concludes that while there have been improvements in multi-modal learning, Vision-Language Models still face significant challenges in recognizing and classifying reasoning errors, indicating the need for further research into enhanced reasoning capabilities.

**Abstract:** Recent advances in Vision-Language Models (VLMs) have improved performance in multi-modal learning, raising the question of whether these models truly understand the content they process. Crucially, can VLMs detect when a reasoning process is wrong and identify its error type? To answer this, we present MMErroR, a multi-modal benchmark of 2,013 samples, each embedding a single coherent reasoning error. These samples span 24 subdomains across six top-level domains, ensuring broad coverage and taxonomic richness. Unlike existing benchmarks that focus on answer correctness, MMErroR targets a process-level, error-centric evaluation that requires models to detect incorrect reasoning and classify the error type within both visual and linguistic contexts. We evaluate 20 advanced VLMs, even the best model (Gemini-3.0-Pro) classifies the error in only 66.47\% of cases, underscoring the challenge of identifying erroneous reasoning. Furthermore, the ability to accurately identify errors offers valuable insights into the capabilities of multi-modal reasoning models. Project Page: https://mmerror-benchmark.github.io

</details>


### [19] [RelightAnyone: A Generalized Relightable 3D Gaussian Head Model](https://arxiv.org/abs/2601.03357)
*Yingyan Xu,Pramod Rao,Sebastian Weiss,Gaspard Zoss,Markus Gross,Christian Theobalt,Marc Habermann,Derek Bradley*

Main category: cs.CV

> 一个用于3D头像重光照的新方法，只需少量图像数据，即可实现高质量的重光照效果，克服了现有方法对复杂照明数据的依赖。

<details>
  <summary>Details</summary>

**Motivation:** 现有的高质量重光照方法需要复杂的时分复用照明，例如一灯一照(OLAT)来捕捉对象。这在实际应用中并不总是可行。为了克服这一难题，我们提出了一个创新的方法，使得不依赖复杂的OLAT数据，也能进行高质量的重光照。

**Method:** 我们提出了一种新的广义可重光照3D高斯头模型，能够对在单视图或多视图图像中观察到的任何对象进行重光照，而无需该对象的OLAT数据。我们的核心思想是从平光照明的3DGS头像到相应可重照明高斯参数的映射。我们的模型分为两个阶段：第一阶段在没有OLAT照明的情况下建模平光照明的3DGS头像，并学习自监督光照对齐的特定数据集光照代码，第二阶段学习映射到物理基础反射参数以实现高质量重光照。

**Result:** 我们的方法可以很好地推广，并能像对对象进行了OLAT照明捕捉一样对第一阶段的任何对象进行重光照。此外，我们还可以通过最少的一张未知对象的图像拟合我们的模型，这使得模型在合成新视图和对数字头像进行重光照等应用中具有多种用途。

**Conclusion:** 这项工作展示了如何通过两阶段的设计，使3D头像的重光照不再依赖于复杂的OLAT数据，而是能够通过少量数据实现高质量的重光照效果。这种方法具有广泛的适用性和潜在的应用场景。

**Abstract:** 3D Gaussian Splatting (3DGS) has become a standard approach to reconstruct and render photorealistic 3D head avatars. A major challenge is to relight the avatars to match any scene illumination. For high quality relighting, existing methods require subjects to be captured under complex time-multiplexed illumination, such as one-light-at-a-time (OLAT). We propose a new generalized relightable 3D Gaussian head model that can relight any subject observed in a single- or multi-view images without requiring OLAT data for that subject. Our core idea is to learn a mapping from flat-lit 3DGS avatars to corresponding relightable Gaussian parameters for that avatar. Our model consists of two stages: a first stage that models flat-lit 3DGS avatars without OLAT lighting, and a second stage that learns the mapping to physically-based reflectance parameters for high-quality relighting. This two-stage design allows us to train the first stage across diverse existing multi-view datasets without OLAT lighting ensuring cross-subject generalization, where we learn a dataset-specific lighting code for self-supervised lighting alignment. Subsequently, the second stage can be trained on a significantly smaller dataset of subjects captured under OLAT illumination. Together, this allows our method to generalize well and relight any subject from the first stage as if we had captured them under OLAT lighting. Furthermore, we can fit our model to unseen subjects from as little as a single image, allowing several applications in novel view synthesis and relighting for digital avatars.

</details>
