{"id": "2510.13827", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13827", "abs": "https://arxiv.org/abs/2510.13827", "authors": ["Ashish Kattamuri", "Ishita Prasad", "Meetu Malhotra", "Arpita Vats", "Rahul Raja", "Albert Lie"], "title": "Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL", "comment": "20th International Workshop on Semantic and Social Media Adaptation &\n  Personalization", "summary": "Current Text-to-SQL methods are evaluated and only focused on executable\nqueries, overlooking the semantic alignment challenge -- both in terms of the\nsemantic meaning of the query and the correctness of the execution results.\nEven execution accuracy itself shows significant drops when moving from English\nto other languages, with an average decline of 6 percentage points across\nnon-English languages. We address these challenges by presenting a new\nframework that combines Group Relative Policy Optimization (GRPO) within a\nmultilingual contrastive reward signal to enhance both task efficiency and\nsemantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method\nteaches models to obtain better correspondence between SQL generation and user\nintent by combining a reward signal based on semantic similarity. On the\nseven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO\nimproved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and\nsemantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive\nreward signal in the GRPO framework further improved the average semantic\naccuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our\nexperiments showcase that a smaller, parameter-efficient 3B LLaMA model\nfine-tuned with our contrastive reward signal outperforms a much larger\nzero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from\n81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly\nmatches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using\njust 3,000 reinforcement learning training examples. These results demonstrate\nhow we can improve the performance of Text-to-SQL systems with contrastive\nrewards for directed semantic alignment, without requiring large-scale training\ndatasets.", "AI": {"tldr": "本文提出的结合对比奖励信号的Group Relative Policy Optimization (GRPO)框架显著提高了Text-to-SQL系统的跨语言性能，尤其是在语义准确性和执行准确性方面。", "motivation": "当前的Text-to-SQL方法在评估过程中仅专注于可执行查询，而忽略了语义对齐的挑战，包括查询的语义含义和执行结果的正确性。甚至执行准确性在从英语转换到其他语言时也显著下降，非英语语言的平均下降幅度为6个百分点。", "method": "提出了一种新的框架，结合了多语言对比奖励信号中的集团相对策略优化（GRPO），以提高Text-to-SQL系统在跨语言场景中的任务效率和语义准确性。该方法通过基于语义相似性的奖励信号教导模型以获得更好的SQL生成和用户意图之间的对应关系。", "result": "{", "conclusion": "实验表明，在没有大规模训练数据的情况下，利用对比奖励信号进行定向语义对齐可以有效提高Text-to-SQL系统的性能。"}}
{"id": "2510.13828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13828", "abs": "https://arxiv.org/abs/2510.13828", "authors": ["Ratna Kandala", "Akshata Kishore Moharir", "Divya Arvinda Nayak"], "title": "From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening", "comment": null, "summary": "Explainable Artificial Intelligence (XAI) has been presented as the critical\ncomponent for unlocking the potential of machine learning in mental health\nscreening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI\ntechniques, such as SHAP and LIME, excel at producing technically faithful\noutputs such as feature importance scores, but fail to deliver clinically\nrelevant, actionable insights that can be used by clinicians or understood by\npatients. This disconnect between technical transparency and human utility is\nthe primary barrier to real-world adoption. This paper argues that this gap is\na translation problem and proposes the Generative Operational Framework, a\nnovel system architecture that leverages Large Language Models (LLMs) as a\ncentral translation engine. This framework is designed to ingest the raw,\ntechnical outputs from diverse XAI tools and synthesize them with clinical\nguidelines (via RAG) to automatically generate human-readable, evidence-backed\nclinical narratives. To justify our solution, we provide a systematic analysis\nof the components it integrates, tracing the evolution from intrinsic models to\ngenerative XAI. We demonstrate how this framework directly addresses key\noperational barriers, including workflow integration, bias mitigation, and\nstakeholder-specific communication. This paper also provides a strategic\nroadmap for moving the field beyond the generation of isolated data points\ntoward the delivery of integrated, actionable, and trustworthy AI in clinical\npractice.", "AI": {"tldr": "文章提出了生成操作框架，使用大型语言模型作为核心，将XAI工具的原始技术输出转化为可操作的临床叙述，解决XAI技术在临床应用中的可操作性问题。", "motivation": "目前的XAI技术，如SHAP和LIME，虽然在生成技术上精确的输出（例如特征重要性分数）方面表现出色，但未能提供对临床相关且具有操作性的见解，这些见解可以被临床医生使用或被患者理解。文章旨在解决这一技术透明度与人类实用性之间的脱节问题。", "method": "提出了一种名为生成操作框架的新系统架构，该框架使用大型语言模型（LLMs）作为核心翻译引擎。该框架设计用于吸收来自各种XAI工具的原始技术输出，并与临床指南结合（通过RAG），自动生成可读的、基于证据的临床叙述。", "result": "文章详细分析了它集成的各个组件，并展示了该框架如何直接解决关键操作障碍，包括工作流程整合、偏见缓解和利益相关者特定的交流。", "conclusion": "该论文提供了一条战略路线图，旨在将领域从生成孤立的数据点转向在临床实践中提供整合的、可操作的和值得信赖的AI。"}}
{"id": "2510.13829", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13829", "abs": "https://arxiv.org/abs/2510.13829", "authors": ["Shinwoo Park", "Hyejin Park", "Hyeseon Ahn", "Yo-Sub Han"], "title": "A Linguistics-Aware LLM Watermarking via Syntactic Predictability", "comment": null, "summary": "As large language models (LLMs) continue to advance rapidly, reliable\ngovernance tools have become critical. Publicly verifiable watermarking is\nparticularly essential for fostering a trustworthy AI ecosystem. A central\nchallenge persists: balancing text quality against detection robustness. Recent\nstudies have sought to navigate this trade-off by leveraging signals from model\noutput distributions (e.g., token-level entropy); however, their reliance on\nthese model-specific signals presents a significant barrier to public\nverification, as the detection process requires access to the logits of the\nunderlying model. We introduce STELA, a novel framework that aligns watermark\nstrength with the linguistic degrees of freedom inherent in language. STELA\ndynamically modulates the signal using part-of-speech (POS) n-gram-modeled\nlinguistic indeterminacy, weakening it in grammatically constrained contexts to\npreserve quality and strengthen it in contexts with greater linguistic\nflexibility to enhance detectability. Our detector operates without access to\nany model logits, thus facilitating publicly verifiable detection. Through\nextensive experiments on typologically diverse languages-analytic English,\nisolating Chinese, and agglutinative Korean-we show that STELA surpasses prior\nmethods in detection robustness. Our code is available at\nhttps://github.com/Shinwoo-Park/stela_watermark.", "AI": {"tldr": "本文介绍了STELA框架，通过词性n元语法模型的语义不确定性来动态调制水印信号，实现了无需访问模型logits的公开可验证检测，提高了检测鲁棒性。", "motivation": "随着大型语言模型（LLMs）的快速发展，需要可靠的治理工具，特别是可公共验证的水印技术，来促进一个可信赖的AI生态系统，以平衡文本质量和检测鲁棒性之间的挑战。", "method": "STELA框架通过利用词性(POS)n元语法模型的语义不确定性来动态调制信号，从而在语法约束较大的语境中减弱水印强度，在语义灵活性较大的语境中增强水印强度，实现了文本质量和检测鲁棒性间的平衡。检测器无需访问模型的logits，实现了公开可验证的检测。", "result": "通过对分析英语、孤立汉语和黏着语韩国语等类型不同的语言的广泛实验，结果表明，STELA在检测鲁棒性方面超越了以往的方法。", "conclusion": "通过广泛实验，STELA证明在不同类型的语言中能够超越以往方法，从而在公开可验证性和检测鲁棒性上提供了优异的表现。"}}
{"id": "2510.13830", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13830", "abs": "https://arxiv.org/abs/2510.13830", "authors": ["Zhongze Cai", "Xiaocheng Li"], "title": "Users as Annotators: LLM Preference Learning from Comparison Mode", "comment": null, "summary": "Pairwise preference data have played an important role in the alignment of\nlarge language models (LLMs). Each sample of such data consists of a prompt,\ntwo different responses to the prompt, and a binary label indicating which of\nthe two responses is better. The labels are usually annotated by professional\nhuman annotators. In this paper, we consider an alternative approach to collect\npairwise preference data -- user annotation from comparison mode. With the\nincreasingly wider adoption of LLMs among the population, users are\ncontributing more and more of their preference labels through their daily\ninteractions with the LLMs. The upside of such labels is that users are the\nbest experts in judging the responses to their own queries/prompts, but the\ndownside is the lack of quality control in these labels. In this paper, we\nconsider a new idea of generating two responses from two different models or\ntwo different versions of the same model. The asymmetry allows us to make an\ninference of the user's data quality through our proposed user behavior model.\nWe develop an expectation-maximization algorithm to estimate a latent quality\nfactor of the user, and filter users' annotation data accordingly. The\ndownstream task shows the effectiveness of our approach in both capturing the\nuser behavior and data filtering for LLM alignment.", "AI": {"tldr": "论文提出了一种新的方法，通过用户行为模型来评估并过滤成对偏好评注数据的质量，以便更好地对齐大型语言模型。", "motivation": "论文的研究动机在于探索一种新的方式来收集成对的偏好评注数据，以解决专业标注者的人力资源限制，并利用用户在使用LLM时提供的反馈。", "method": "本论文提出了一种利用用户行为模型评估和过滤用户标注的成对偏好数据的方法。具体而言，通过对不同的模型或同一模型的不同版本生成两组响应，并通过期望最大化算法估计用户的潜在质量因素，从而过滤掉低质量的用户标注数据。", "result": "实验结果表明，该方法在捕捉用户行为和过滤数据以对齐语言模型方面是有效的。", "conclusion": "论文得出结论称，利用用户的日常交互来收集偏好评注数据可以帮助改进模型的对齐过程，尽管存在质量问题，但通过过滤机制可以提高数据的质量。"}}
{"id": "2510.13889", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13889", "abs": "https://arxiv.org/abs/2510.13889", "authors": ["Yue Hu", "Guohang Zhuang"], "title": "MultiFoodhat: A potential new paradigm for intelligent food quality inspection", "comment": null, "summary": "Food image classification plays a vital role in intelligent food quality\ninspection, dietary assessment, and automated monitoring. However, most\nexisting supervised models rely heavily on large labeled datasets and exhibit\nlimited generalization to unseen food categories. To overcome these challenges,\nthis study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning\nframework for zero-shot food recognition. The framework integrates\nvision-language models (VLMs) and large language models (LLMs) to enable\ncollaborative reasoning through multi-round visual-textual dialogues. An Object\nPerception Token (OPT) captures fine-grained visual attributes, while an\nInteractive Reasoning Agent (IRA) dynamically interprets contextual cues to\nrefine predictions. This multi-agent design allows flexible and human-like\nunderstanding of complex food scenes without additional training or manual\nannotations. Experiments on multiple public food datasets demonstrate that\nMultiFoodChat achieves superior recognition accuracy and interpretability\ncompared with existing unsupervised and few-shot methods, highlighting its\npotential as a new paradigm for intelligent food quality inspection and\nanalysis.", "AI": {"tldr": "本研究提出了一种名为MultiFoodChat的框架，用于零样本食品识别。通过视觉-语言模型（VLMs）和大型语言模型（LLMs）的协作推理，在不需额外训练或手动标注的情况下实现了灵活、类似人类的复杂食品场景理解，实验验证了其优越的识别准确性和可解释性。", "motivation": "目前大多数监督模型严重依赖大规模的标注数据，并且对于未见过的食品种类泛化能力有限。为了克服这些挑战，本研究提出了MultiFoodChat框架。", "method": "本研究介绍了MultiFoodChat，这是一个基于多智能体推理的对话驱动框架，用于零样本食品识别。框架集成了视觉-语言模型（VLM）和大型语言模型（LLM），通过多轮视觉文本对话实现协作推理。其中，对象感知标记（OPT）捕捉精细的视觉属性，而交互推理代理（IRA）动态解读情境线索以优化预测。", "result": "实验结果表明，MultiFoodChat在多个公开食品数据集上相比现有的无监督和少样本方法展现出更高的识别准确率和可解释性。", "conclusion": "实验表明，MultiFoodChat在多个公开的食品数据集上实现了优越的识别准确性和可解释性，与现有的无监督和少量样本方法相比表现出色，凸显了其作为智能食品质量检测和分析新范式的潜力。"}}
{"id": "2510.13831", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13831", "abs": "https://arxiv.org/abs/2510.13831", "authors": ["Chao Han", "Yijuan Liang", "Zihao Xuan", "Daokuan Wu", "Wei Zhang", "Xiaoyu Shen"], "title": "Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference", "comment": null, "summary": "The deployment of large language models (LLMs) in real-world applications is\nincreasingly limited by their high inference cost. While recent advances in\ndynamic token-level computation allocation attempt to improve efficiency by\nselectively activating model components per token, existing methods rely on\ngreedy routing--a myopic execute-or-skip mechanism that often leads to\nirreversible information loss and suboptimal token selection. This paper\nintroduces informed routing, a new paradigm that proactively addresses these\nissues. The key insight is to assess not only a token's immediate importance\nbut also its recoverability, i.e., how well its transformation can be\napproximated. To this end, we propose the Lightweight Feature Forecaster (LFF),\na small predictive module that estimates a unit's output before routing\ndecisions are made. This enables a flexible execute-or-approximate policy that\npreserves model fidelity while drastically reducing computation. Extensive\nexperiments on both language modeling and reasoning tasks show that informed\nrouting achieves state-of-the-art efficiency-performance trade-offs across\nmultiple sparsity levels. Notably, even without final LoRA fine-tuning, our\nmethod matches or surpasses strong baselines that require full fine-tuning, all\nwhile reducing training time by over 50%. The code is available at:\nhttps://github.com/EIT-NLP/informed-routing", "AI": {"tldr": "本文提出了 informed routing 新方法，通过评估 token 的即刻重要性和可恢复性，引入 Lightweight Feature Forecaster (LFF) 模块优化 token 的执行或近似策略，显著提升大型语言模型的推理效率。", "motivation": "解决现有技术依赖贪心路由策略导致的信息不可逆损失和次优 token 选择的问题，提高大型语言模型在实际应用中的推理效率。", "method": "介绍了名为 informed routing 的新范式，通过评估 token 的即刻重要性和可恢复性来提高效率。提出了 Lightweight Feature Forecaster (LFF) 模块，在路由决策前预测单元输出，实现精确或近似执行策略，从而减少计算同时保持模型保真度。", "result": "实验显示，informed routing 在多个稀疏度水平上实现了最先进的效率-性能权衡，包括语言建模和推理任务。即使不进行最终的 LoRA 微调，该方法也能匹配甚至超越需要完全微调的强基线方法，并且将训练时间减少超过 50%。", "conclusion": "新提出的 informed routing 方法显著改进了动态 token 级计算分配的效率和性能，且无需复杂微调，展现了优越的资源利用能力。"}}
{"id": "2510.13899", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.13899", "abs": "https://arxiv.org/abs/2510.13899", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann", "Jörg Keckstein", "Simon Keckstein"], "title": "Post-surgical Endometriosis Segmentation in Laparoscopic Videos", "comment": "This is a demo paper that was already published\n  https://ieeexplore.ieee.org/document/9461900 but a preprint/author's copy is\n  needed for the funding agency", "summary": "Endometriosis is a common women's condition exhibiting a manifold visual\nappearance in various body-internal locations. Having such properties makes its\nidentification very difficult and error-prone, at least for laymen and\nnon-specialized medical practitioners. In an attempt to provide assistance to\ngynecologic physicians treating endometriosis, this demo paper describes a\nsystem that is trained to segment one frequently occurring visual appearance of\nendometriosis, namely dark endometrial implants. The system is capable of\nanalyzing laparoscopic surgery videos, annotating identified implant regions\nwith multi-colored overlays and displaying a detection summary for improved\nvideo browsing.", "AI": {"tldr": "此论文介绍了一个系统，用于分析腹腔镜手术视频中的深色子宫内膜异位植片，并通过多色覆盖物标注这些区域，帮助医生改善视频浏览和诊断效果。", "motivation": "由于子宫内膜异位症在体内不同部位的表现形式多样，导致其识别难度大且容易出错，尤其是对于非医学专家而言。此系统旨在辅助妇科医生进行子宫内膜异位症的诊疗。", "method": "系统经过训练可以分割出一种常见的子宫内膜异位症表现形式——深色子宫内膜植入物，并在腹腔镜手术视频中标注出这些区域。", "result": "系统能够分析腹腔镜视频，以多色覆盖物标注识别的植入物区域，并提供检测摘要，供改善视频浏览使用。", "conclusion": "通过此系统，有望提高子宫内膜异位症的识别准确度，减少误诊，从而提高医生对子宫内膜异位症的诊疗效率。"}}
{"id": "2510.13832", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13832", "abs": "https://arxiv.org/abs/2510.13832", "authors": ["Minsik Choi", "Hyegang Son", "Changhoon Kim", "Young Geun Kim"], "title": "Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning", "comment": "32 pages", "summary": "Transformer-based models have achieved remarkable performance in NLP tasks.\nHowever, their structural characteristics-multiple layers and attention\nheads-introduce efficiency challenges in inference and deployment. To address\nthese challenges, various pruning methods have recently been proposed. Notably,\ngradient-based methods using Head Importance Scores (HIS) have gained traction\nfor interpretability, efficiency, and ability to identify redundant heads.\nHowever, HIS alone has limitations as it captures only the gradient-driven\ncontribution, overlooking the diversity of attention patterns. To overcome\nthese limitations, we introduce a novel pruning criterion, HIES (Head\nImportance-Entropy Score), which integrates head importance scores with\nattention entropy, providing complementary evidence on per-head contribution.\nEmpirically, HIES-based pruning yields up to 15.2% improvement in model quality\nand 2.04x improvement in stability over HIS-only methods, enabling substantial\nmodel compression without sacrificing either accuracy or stability. Code will\nbe released upon publication.", "AI": {"tldr": "本文提出了HIES方法，通过结合头部重要性评分与注意力熵来改进Transformer模型，实验表明这种方法能提高模型质量和稳定性，同时实现了模型压缩。", "motivation": "多层和注意力头的结构特征带来了推理和部署的效率挑战，本文旨在通过剪枝方法解决这些问题。", "method": "引入了一种新的剪枝标准HIES（Head Importance-Entropy Score），该标准结合了头部重要性评分与注意力熵，提供了关于每个头部贡献度的补充证据。", "result": "经验结果显示，基于HIES的剪枝方法相比只使用HIS的方法，能提高最多15.2%的模型质量，同时稳定性的提升可达2.04倍。", "conclusion": "HIES的引入使得模型压缩得以实现，同时保证了准确性和稳定性不受损失。"}}
{"id": "2510.13993", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13993", "abs": "https://arxiv.org/abs/2510.13993", "authors": ["Jia Yun Chua", "Argyrios Zolotas", "Miguel Arana-Catania"], "title": "Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models", "comment": "11 pages, 7 figures, 8 tables. To be published in Applied AI Letters", "summary": "Remote sensing has become a vital tool across sectors such as urban planning,\nenvironmental monitoring, and disaster response. While the volume of data\ngenerated has increased significantly, traditional vision models are often\nconstrained by the requirement for extensive domain-specific labelled data and\ntheir limited ability to understand the context within complex environments.\nVision Language Models offer a complementary approach by integrating visual and\ntextual data; however, their application to remote sensing remains\nunderexplored, particularly given their generalist nature. This work\ninvestigates the combination of vision models and VLMs to enhance image\nanalysis in remote sensing, with a focus on aircraft detection and scene\nunderstanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and\nGemini aims to achieve more accurate and contextually aware image\ninterpretation. Performance is evaluated on both labelled and unlabelled remote\nsensing data, as well as degraded image scenarios which are crucial for remote\nsensing. The findings show an average MAE improvement of 48.46% across models\nin the accuracy of aircraft detection and counting, especially in challenging\nconditions, in both raw and degraded scenarios. A 6.17% improvement in\nCLIPScore for comprehensive understanding of remote sensing images is obtained.\nThe proposed approach combining traditional vision models and VLMs paves the\nway for more advanced and efficient remote sensing image analysis, especially\nin few-shot learning scenarios.", "AI": {"tldr": "研究结合视觉模型和视觉语言模型（VLMs）以改进遥感图像分析，特别是飞机检测和场景理解，展示了显著的性能提升，平均MAE提高48.46%，CLIPScore提升6.17%。", "motivation": "传统的视觉模型受限于需要大量的领域特定标注数据和在复杂环境中的理解能力不足。而视觉语言模型（VLMs）由于其互补的特性，特别是将视觉与文本数据融合，其在遥感领域的应用还未充分探索。研究旨在通过使用VLMs来增强遥感图像的分析能力，特别是在少样本学习场景下的表现。", "method": "通过将YOLO视觉模型与VLM如LLaVA, ChatGPT, 和Gemini集成来提升遥感图像的解析准确性。模型首先通过YOLO进行目标检测，然后由VLM评估图像的理解质量，最后结合结果进行图像分析。", "result": "实验在有标注和无标注的遥感数据，以及降质图像场景中进行，证明了方法的有效性。研究表明，该方法在飞机检测精度和数量估计中平均MAE提高了48.46%。在遥感图像的全面理解上，CLIPScore提升了6.17%。", "conclusion": "结合传统视觉模型与VLM的混合模型，在提升遥感图像分析的准确性、对复杂场景的理解能力以及在少样本学习场景中的适应性方面具有很大潜力。这种方法为遥感图像分析的发展开辟了更高级、更高效的路径。"}}
{"id": "2510.13835", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13835", "abs": "https://arxiv.org/abs/2510.13835", "authors": ["Avik Dutta", "Priyanshu Gupta", "Hosein Hasanbeig", "Rahul Pratap Singh", "Harshit Nigam", "Sumit Gulwani", "Arjun Radhakrishna", "Gustavo Soares", "Ashish Tiwari"], "title": "ConDABench: Interactive Evaluation of Language Models for Data Analysis", "comment": null, "summary": "Real-world data analysis tasks often come with under-specified goals and\nunclean data. User interaction is necessary to understand and disambiguate a\nuser's intent, and hence, essential to solving these complex tasks. Existing\nbenchmarks for evaluating LLMs on data analysis tasks do not capture these\ncomplexities or provide first-class support for interactivity. We introduce\nConDABench, a framework for generating conversational data analysis (ConDA)\nbenchmarks and evaluating external tools on the generated benchmarks. \\bench\nconsists of (a) a multi-agent workflow for generating realistic benchmarks from\narticles describing insights gained from public datasets, (b) 1,420 ConDA\nproblems generated using this workflow, and (c) an evaluation harness that, for\nthe first time, makes it possible to systematically evaluate conversational\ndata analysis tools on the generated ConDA problems. Evaluation of\nstate-of-the-art LLMs on the benchmarks reveals that while the new generation\nof models are better at solving more instances, they are not necessarily better\nat solving tasks that require sustained, long-form engagement. ConDABench is an\navenue for model builders to measure progress towards truly collaborative\nmodels that can complete complex interactive tasks.", "AI": {"tldr": "ConDABench是一个生成对话式数据分析基准并评估交互性工具的框架，显示了其在复杂数据分析任务上的必要性。", "motivation": "现有的基准无法捕捉现实世界数据中分析任务的复杂性，尤其是用户交互的重要性。因此，需要一个框架来生成对话式数据分析任务，并提供第一个全面支持交互性的评估基准。", "method": "介绍了一个名为ConDABench的框架，用于生成对话式数据分析基准，并评估外部工具在这些基准上的表现。该框架包括一个多智能体的工作流，用于从描述公共数据集获得的见解的文章中生成现实的基准，以及包含1,420个对话式数据分析问题。还有用于系统评估对话式数据分析工具的评估工具。", "result": "评估表明最新模型在解决对话式数据分析问题上表现更好，但面对需要长时间交互的任务则不尽如人意。", "conclusion": "ConDABench为模型构建者提供了一个量化进步的途径，目标是创建能完成复杂交互任务的协作模型。"}}
