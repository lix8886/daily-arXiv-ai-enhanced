{"id": "2511.00010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable profi-\nciency in code generation. However, their ability to create complex visualiza-\ntions for scaled and structured data remains largely unevaluated and\nunderdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark\nfeaturing 1k challenging visualization tasks that cover a wide range of topics,\nsuch as fi- nance, scientific research, and sociology. The benchmark is\nstructured around seven high-level visualization tasks and encompasses 48\ndistinct chart types. Cru- cially, it is the first to systematically evaluate\nboth single-turn generation and multi-turn refinement across a diverse spectrum\nof task complexities. Our com- prehensive evaluation of 23 leading LLMs on\nPlotCraft reveals obvious per- formance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent frame- work. Building upon this dataset, we develope\nPlotCraftor, a novel code gener- ation model that achieves strong capabilities\nin complex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading propri- etary approaches. Especially, on hard\ntask, Our model achieves over 50% per- formance improvement. We will release\nthe benchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark.", "AI": {"tldr": "介绍了PlotCraft，一个新的包含1000个复杂数据可视化任务的基准测试，以及为了提高处理复杂可视化任务的能力而开发的SynthVis-30K数据集和PlotCraftor模型，该模型在复杂数据可视化方面表现优异。", "motivation": "现有的大型语言模型在生成代码方面表现出色，但它们在创建复杂数据可视化方面的性能尚未充分开发和评估。", "method": "开发了PlotCraft基准测试，包括1000个挑战性可视化任务，并通过综合数据集SynthVis-30K和模型PlotCraftor来评估和改进复杂数据可视化的性能。", "result": "PlotCraftor在多个基准测试上表现良好，特别是在困难任务上，性能提高了50%以上。", "conclusion": "PlotCraft或能有效突出大型语言模型在数据可视化领域的不足，而PlotCraftor模型在复杂数据可视化方面显示出卓越的能力。"}}
{"id": "2511.00115", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00115", "abs": "https://arxiv.org/abs/2511.00115", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "comment": null, "summary": "Personality recognition from text is typically cast as hard-label\nclassification, which obscures the graded, prototype-like nature of human\npersonality judgments. We present ProtoMBTI, a cognitively aligned framework\nfor MBTI inference that operationalizes prototype theory within an LLM-based\npipeline. First, we construct a balanced, quality-controlled corpus via\nLLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).\nNext, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative\nembeddings and to standardize a bank of personality prototypes. At inference,\nwe retrieve top-k prototypes for a query post and perform a\nretrieve--reuse--revise--retain cycle: the model aggregates prototype evidence\nvia prompt-based voting, revises when inconsistencies arise, and, upon correct\nprediction, retains the sample to continually enrich the prototype library.\nAcross Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both\nthe four MBTI dichotomies and the full 16-type task, and exhibits robust\ncross-dataset generalization. Our results indicate that aligning the inference\nprocess with psychological prototype reasoning yields gains in accuracy,\ninterpretability, and transfer for text-based personality modeling.", "AI": {"tldr": "本研究提出了一个认知对齐的框架ProtoMBTI，用于MBTI的推理任务，并证明了这种方法在准确性、解释性、和迁移性上的改进。", "motivation": "典型的方法将个性识别视为硬标签分类，这掩盖了人类个性判断的分级和原型特征。因此，本研究旨在通过将原型理论应用于基于语言模型的流程来创建一个认知对齐的框架。", "method": "我们提出了ProtoMBTI框架，该框架通过LLM引导的多维增强（语义、语言、情感）来构建平衡且质量控制的语料库，然后通过LoRA微调一个轻量级的编码器来学习有判别力的嵌入并标准化个性原型库。在推理阶段，模型通过检索-利用-修订-保留循环来处理查询帖子，并通过提示式的投票聚合原型证据，修正不一致，并保留正确的预测以丰富原型库。", "result": "在Kaggle和Pandora基准测试中，ProtoMBTI在MBTI的四个二元维度和完整16型任务上均优于基线模型，并表现出强大的跨数据集泛化能力。", "conclusion": "研究结果表明，将推理过程与心理学原型推理对齐，提高了基于文本个性建模的准确性、可解释性和迁移性。"}}
{"id": "2511.00180", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00180", "abs": "https://arxiv.org/abs/2511.00180", "authors": ["Nicky Pochinkov", "Yulia Volkova", "Anna Vasileva", "Sai V R Chereddy"], "title": "ParaScopes: What do Language Models Activations Encode About Future Text?", "comment": "Main paper: 9 pages, 10 figures. Total 24 pages", "summary": "Interpretability studies in language models often investigate forward-looking\nrepresentations of activations. However, as language models become capable of\ndoing ever longer time horizon tasks, methods for understanding activations\noften remain limited to testing specific concepts or tokens. We develop a\nframework of Residual Stream Decoders as a method of probing model activations\nfor paragraph-scale and document-scale plans. We test several methods and find\ninformation can be decoded equivalent to 5+ tokens of future context in small\nmodels. These results lay the groundwork for better monitoring of language\nmodels and better understanding how they might encode longer-term planning\ninformation.", "AI": {"tldr": "研究提出了一种名为残差流解码器的框架，用于检测和理解大型语言模型中关于段落乃至文件级别的规划信息，揭示了模型中可能编码的长期规划信息的解码方法。", "motivation": "随着语言模型的时间跨度变得越来越长，对于理解这些模型中的信息处理和长期规划能力的方法显得相对有限。该项研究的动机在于提供更深入的方法来理解语言模型的长跨度任务处理能力。", "method": "采用了残差流解码器框架，研究了小型语言模型中等效于5个以上词汇未来上下文信息的解码方法。", "result": "研究发现，通过该框架可以在小型模型中解码出相当于5个以上未来词汇上下文的信息。", "conclusion": "这项研究为更好地监控语言模型和理解其如何编码长期规划信息奠定了基础。"}}
{"id": "2511.00198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00198", "abs": "https://arxiv.org/abs/2511.00198", "authors": ["Chun-Hao Yang", "Bo-Han Feng", "Tzu-Yuan Lai", "Yan Yu Chen", "Yin-Kai Dean Huang", "Shou-De Lin"], "title": "Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap", "comment": null, "summary": "Optimizing training performance in large language models (LLMs) remains an\nessential challenge, particularly in improving model performance while\nmaintaining computational costs. This work challenges the conventional approach\nof training LLMs using next-token prediction (NTP), arguing that by predicting\ninformation-rich tokens during training, there is a more effective way to train\nLLMs. We investigate the impact of the proposed solution in three kinds of\ntasks for LLMs: arithmetic, multi-label classification of text, and\nnatural-language generation. This work offers a principled approach to\noptimizing LLM training, advancing both model performance and theoretical\nunderstanding of the target-token selection strategies.", "AI": {"tldr": "该研究提出了一种通过预测信息丰富的标记来优化大型语言模型（LLM）训练性能的方法，验证了这种方法在三种任务中的有效性，并推进了对目标标记选择策略的理论理解。", "motivation": "当前大型语言模型在提高模型性能的同时保持计算成本是一个关键挑战。该研究旨在提出一种更有效的方法来提高LLM的训练效率。", "method": "该研究提出了一种新的方法——通过预测信息丰富的标记而不是常规的下一个标记来训练大型语言模型，以此改进训练效果。", "result": "本文在三种任务——算术运算、文本的多标签分类和自然语言生成中，验证了该方法的有效性。", "conclusion": "该研究提供了一种优化大型语言模型训练的方法，既提高了模型性能，也推进了对目标标记选择策略的理论理解。"}}
{"id": "2511.00011", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00011", "abs": "https://arxiv.org/abs/2511.00011", "authors": ["Alexander Okupnik", "Johannes Schneider", "Kyriakos Flouris"], "title": "Generative human motion mimicking through feature extraction in denoising diffusion settings", "comment": null, "summary": "Recent success with large language models has sparked a new wave of verbal\nhuman-AI interaction. While such models support users in a variety of creative\ntasks, they lack the embodied nature of human interaction. Dance, as a primal\nform of human expression, is predestined to complement this experience. To\nexplore creative human-AI interaction exemplified by dance, we build an\ninteractive model based on motion capture (MoCap) data. It generates an\nartificial other by partially mimicking and also \"creatively\" enhancing an\nincoming sequence of movement data. It is the first model, which leverages\nsingle-person motion data and high level features in order to do so and, thus,\nit does not rely on low level human-human interaction data. It combines ideas\nof two diffusion models, motion inpainting, and motion style transfer to\ngenerate movement representations that are both temporally coherent and\nresponsive to a chosen movement reference. The success of the model is\ndemonstrated by quantitatively assessing the convergence of the feature\ndistribution of the generated samples and the test set which serves as\nsimulating the human performer. We show that our generations are first steps to\ncreative dancing with AI as they are both diverse showing various deviations\nfrom the human partner while appearing realistic.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.00222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00222", "abs": "https://arxiv.org/abs/2511.00222", "authors": ["Marwa Abdulhai", "Ryan Cheng", "Donovan Clay", "Tim Althoff", "Sergey Levine", "Natasha Jaques"], "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to simulate human users in\ninteractive settings such as therapy, education, and social role-play. While\nthese simulations enable scalable training and evaluation of AI agents,\noff-the-shelf LLMs often drift from their assigned personas, contradict earlier\nstatements, or abandon role-appropriate behavior. We introduce a unified\nframework for evaluating and improving persona consistency in LLM-generated\ndialogue. We define three automatic metrics: prompt-to-line consistency,\nline-to-line consistency, and Q&A consistency, that capture different types of\npersona drift and validate each against human annotations. Using these metrics\nas reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs\nfor three user roles: a patient, a student, and a social chat partner. Our\nmethod reduces inconsistency by over 55%, resulting in more coherent and\nfaithful simulated users.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.00021", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00021", "abs": "https://arxiv.org/abs/2511.00021", "authors": ["Julio Jerison E. Macrohon", "Gordon Hung"], "title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets", "comment": "15 pages, 10 figures", "summary": "Coral reefs support numerous marine organisms and are an important source of\ncoastal protection from storms and floods, representing a major part of marine\necosystems. However coral reefs face increasing threats from pollution, ocean\nacidification, and sea temperature anomalies, making efficient protection and\nmonitoring heavily urgent. Therefore, this study presents a novel\nmachine-learning-based coral bleaching classification system based on a diverse\nglobal dataset with samples of healthy and bleached corals under varying\nenvironmental conditions, including deep seas, marshes, and coastal zones. We\nbenchmarked and compared three state-of-the-art models: Residual Neural Network\n(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).\nAfter comprehensive hyperparameter tuning, the CNN model achieved the highest\naccuracy of 88%, outperforming existing benchmarks. Our findings offer\nimportant insights into autonomous coral monitoring and present a comprehensive\nanalysis of the most widely used computer vision models.", "AI": {"tldr": "本研究开发了一种基于机器学习的珊瑚白化分类系统，通过比较三种顶尖模型，最终确定CNN模型的性能最佳，准确率达到88%，为珊瑚自动监测提供重要见解。", "motivation": "珊瑚礁支撑着众多海洋生物，是防风暴和洪水的重要海岸保护来源，是海洋生态系统的重要组成部分。然而，珊瑚礁面临着污染、海洋酸化和海水温度异常等不断增加的威胁，因此有效保护和监测珊瑚礁显得尤为重要。", "method": "本研究提出了一种基于机器学习的珊瑚白化分类系统，该系统基于全球多样化的数据集，包括健康和白化的珊瑚样本，以及不同的环境条件，如深海、沼泽和沿海区域。我们评估并对比了三种最先进的模型：残差神经网络（ResNet）、视觉变压器（ViT）和卷积神经网络（CNN）。", "result": "经过全面的超参数调整后，CNN模型达到了最高的88%准确率，超过了现有的基准。", "conclusion": "研究结果为自动化的珊瑚监测提供了重要的见解，并对广泛使用的计算机视觉模型进行了全面分析。"}}
{"id": "2511.00265", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00265", "abs": "https://arxiv.org/abs/2511.00265", "authors": ["Arman Anwar", "Zefang Liu"], "title": "AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding", "comment": null, "summary": "Traditional cybersecurity tabletop exercises (TTXs) provide valuable training\nbut are often scripted, resource-intensive, and difficult to scale. We\nintroduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches\ngame that integrates large language model teammates with a Bloom-aligned,\nretrieval-augmented copilot (C2D2). The system expands a curated corpus into\nfactual, conceptual, procedural, and metacognitive snippets, delivering\non-demand, cognitively targeted hints. Prompt-engineered agents employ a\nscaffolding ladder that gradually fades as learner confidence grows. In a\nsolo-player pilot with four graduate students, participants reported greater\nintention to use the agent-based version compared to the physical card deck and\nviewed it as more scalable, though a ceiling effect emerged on a simple\nknowledge quiz. Despite limitations of small sample size, single-player focus,\nand narrow corpus, these early findings suggest that large language model\naugmented TTXs can provide lightweight, repeatable practice without the\nlogistical burden of traditional exercises. Planned extensions include\nmulti-player modes, telemetry-driven coaching, and comparative studies with\nlarger cohorts.", "AI": {"tldr": "AgentBnB是一种基于浏览器的网络安全桌面演练解决方案，利用大型语言模型，为学习者提供按需认知性提示，进行单人试点研究取得积极反馈，未来计划添加更多功能扩展。", "motivation": "传统的网络安全桌面演练提供有价值的培训，但往往是脚本化的、资源密集的，并且难以扩展。", "method": "引入了AgentBnB，这是一种基于浏览器的Backdoors & Breaches游戏的重新设计版本。它结合了大型语言模型队员和与Bloom对齐的检索增强助手（C2D2）。该系统扩展了精心策划的语料库，提供即时、认知目标性的提示。通过提示工程，代理会根据学习者的信心逐步减少帮助。", "result": "在一项涉及四名研究生的单人试点研究中，参与者表示他们更倾向于使用基于代理版本，而不是物理卡片版，并认为它更具可扩展性，尽管在简单的知识测试中有天花板效应出现。", "conclusion": "尽管存在样本量小、仅关注单人模式和狭窄语料库的限制，这些初步发现表明，通过大型语言模型增强的桌面演练可以提供轻量级、可重复的练习，而无需传统演练的后勤负担。计划扩展包括多玩家模式、以遥测为基础的教练以及与更大群体的比较研究。"}}
{"id": "2511.00022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00022", "abs": "https://arxiv.org/abs/2511.00022", "authors": ["Jules Gerard", "Leandro Di Bella", "Filip Huyghe", "Marc Kochzius"], "title": "Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline", "comment": "Accepted to EUVIP2025, student session", "summary": "Coral reef monitoring in the Western Indian Ocean is limited by the labor\ndemands of underwater visual censuses. This work evaluates a YOLOv8-based deep\nlearning pipeline for automating family-level fish identification from video\ntransects collected in Kenya and Tanzania. A curated dataset of 24 families was\ntested under different configurations, providing the first region-specific\nbenchmark for automated reef fish monitoring in the Western Indian Ocean. The\nbest model achieved mAP@0.5 of 0.52, with high accuracy for abundant families\nbut weaker detection of rare or complex taxa. Results demonstrate the potential\nof deep learning as a scalable complement to traditional monitoring methods.", "AI": {"tldr": "本文通过基于YOLOv8的深度学习技术，展示了它在西印度洋地区自动鱼类识别上的潜力，作为对传统监控方法的补充。", "motivation": "由于珊瑚礁监测在西印度洋受到水下目视调查所需的劳动需求限制，本文的目的是评估基于深度学习的方法对这一地区进行自动珊瑚礁鱼类监控的潜力。", "method": "本文提出了一种基于YOLOv8的深度学习流水线，用于从肯尼亚和坦桑尼亚收集的视频横断面中自动识别鱼类家庭级别的物种。", "result": "最佳模型在mAP@0.5指标上达到了0.52，对于数量较多的鱼类家族来说，识别准确度较高，但对于稀有或复杂的物种识别效果相对较弱。", "conclusion": "结果表明，深度学习可以作为一种规模化的传统监测方法的补充。"}}
{"id": "2511.00268", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00268", "abs": "https://arxiv.org/abs/2511.00268", "authors": ["Shounak Paul", "Dhananjay Ghumare", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Identifying/retrieving relevant statutes and prior cases/precedents for a\ngiven legal situation are common tasks exercised by law practitioners.\nResearchers to date have addressed the two tasks independently, thus developing\ncompletely different datasets and models for each task; however, both retrieval\ntasks are inherently related, e.g., similar cases tend to cite similar statutes\n(due to similar factual situation). In this paper, we address this gap. We\npropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),\nwhich is a unique corpus that provides a common testbed for developing models\nfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit\nthe dependence between the two. We experiment extensively with several baseline\nmodels on the tasks, including lexical models, semantic models and ensemble\nbased on GNNs. Further, to exploit the dependence between the two tasks, we\ndevelop an LLM-based re-ranking approach that gives the best performance.", "AI": {"tldr": "本文提出了IL-PCR，一个用于法律判例和法规检索的综合语料库，以结合两者间的相关性进行模型开发。实验表明，基于LLM的重排序方法性能最佳。", "motivation": "现有的法律文本检索研究分别处理法规和判例检索任务，缺少一个同时涵盖两者且能利用两者间相关性的综合平台。", "method": "开发IL-PCR语料库，使用多种基准模型进行实验，包括词汇模型、语义模型和基于GNN的集成模型，并提出一种基于LLM的重排序方法利用两项任务间的依赖关系。", "result": "实验表明，基于LLM的重排序方法在法规和判例检索任务上具有最佳性能。", "conclusion": "IL-PCR语料库为相关性高的法律文本检索提供了一个整合的平台，基于LLM的重排序方法证明是一种有效的方法。"}}
{"id": "2511.00028", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00028", "abs": "https://arxiv.org/abs/2511.00028", "authors": ["Hanyang Chen", "Yanchao Yang"], "title": "Mutual Information guided Visual Contrastive Learning", "comment": "Tech Report - Undergraduate Thesis - 2023", "summary": "Representation learning methods utilizing the InfoNCE loss have demonstrated\nconsiderable capacity in reducing human annotation effort by training invariant\nneural feature extractors. Although different variants of the training\nobjective adhere to the information maximization principle between the data and\nlearned features, data selection and augmentation still rely on human\nhypotheses or engineering, which may be suboptimal. For instance, data\naugmentation in contrastive learning primarily focuses on color jittering,\naiming to emulate real-world illumination changes. In this work, we investigate\nthe potential of selecting training data based on their mutual information\ncomputed from real-world distributions, which, in principle, should endow the\nlearned features with better generalization when applied in open environments.\nSpecifically, we consider patches attached to scenes that exhibit high mutual\ninformation under natural perturbations, such as color changes and motion, as\npositive samples for learning with contrastive loss. We evaluate the proposed\nmutual-information-informed data augmentation method on several benchmarks\nacross multiple state-of-the-art representation learning frameworks,\ndemonstrating its effectiveness and establishing it as a promising direction\nfor future research.", "AI": {"tldr": "研究提出了一种基于场景中自然扰动下高互信息补丁的对比学习数据增强方法，通过评价多个基准测试，展示了该方法的有效性，为未来的表征学习方向提供了新思路。", "motivation": "尽管使用InfoNCE损失的表示学习方法在减少人工标注方面表现出色，但如果数据选择和增强仍高度依赖人类假设或工程，这种假设可能是次优的。因此，研究提出了基于场景中补丁之间高互信息的数据选择策略，以改善模型在开放环境中的泛化能力。", "method": "该方法通过计算自然扰动（如颜色变化和运动）下场景中补丁之间的互信息，选择具有高互信息的补丁作为对学习过程中有用的正样本，利用对比损失实现这一目标。", "result": "实验结果展示了该互信息导向的数据增强方法在多个基准上和多种最先进的表示学习框架中的有效性，增强了学习到的特征能够更好的泛化。", "conclusion": "此研究确定了提出的数据增强策略作为未来研究的有希望的方向，展示了在开放环境下提高模型表现的潜力。"}}
{"id": "2511.00270", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00270", "abs": "https://arxiv.org/abs/2511.00270", "authors": ["Abhinav Joshi", "Vaibhav Sharma", "Sanjeet Singh", "Ashutosh Modi"], "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Sign language translation remains a challenging task due to the scarcity of\nlarge-scale, sentence-aligned datasets. Prior arts have focused on various\nfeature extraction and architectural changes to support neural machine\ntranslation for sign languages. We propose POSESTITCH-SLT, a novel pre-training\nscheme that is inspired by linguistic-templates-based sentence generation\ntechnique. With translation comparison on two sign language datasets, How2Sign\nand iSign, we show that a simple transformer-based encoder-decoder architecture\noutperforms the prior art when considering template-generated sentence pairs in\ntraining. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign\nand from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for\npose-based gloss-free translation. The results demonstrate the effectiveness of\ntemplate-driven synthetic supervision in low-resource sign language settings.", "AI": {"tldr": "POSESTITCH-SLT is a novel pre-training method inspired by linguistic-template-based sentence generation, which significantly improves the performance of sign language translation tasks using a simple transformer architecture and achieves state-of-the-art BLEU-4 score improvements on two datasets.", "motivation": "The scarcity of large-scale, sentence-aligned datasets for sign language translation presents a significant challenge. The aim is to overcome this issue with a pre-training method that relies on synthetic data generated from templates.", "method": "POSESTITCH-SLT leverages a template-driven synthetic supervision approach to create sentence pairs for training a transformer-based encoder-decoder architecture designed for sign language translation.", "result": "The method achieves notable improvements in BLEU-4 scores on the How2Sign (1.97 to 4.56) and iSign (0.55 to 3.43) datasets, surpassing the performance of previous state-of-the-art methods.", "conclusion": "The study shows that incorporating template-generated sentence pairs into the training set can significantly enhance translation performance in low-resource sign language settings."}}
