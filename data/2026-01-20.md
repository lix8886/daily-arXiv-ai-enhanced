<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT Reasoning](https://arxiv.org/abs/2601.10775)
*Tommaso Felice Banfi,Sashenka Gamage*

Main category: cs.CL

> 研究提出了一种新型基于大语言模型的框架，用于在离散博弈任务中进行推理，并通过井字游戏进行了示例说明。基于熵的自适应推理方法提高了LLM在决策树中的性能，研究结果表明该方法具有显著统计学意义。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于通过引入基于熵的自适应推理方式，提高LLM在离散推理任务中的表现。通过实验，研究者旨在证明这种方法可以对决策质量产生实质性的正面影响。

**Method:** 我们提出了一种基于大语言模型（LLM）的框架，用于执行离散和博弈理论任务中的推理，用井字游戏（Tic-Tac-Toe）进行了示范。该方法将上下文学习与基于熵的推理链（CoT）相结合，并具有自适应上下文检索功能。模型能够根据令牌级别的不确定性动态调整所检索示例的数量和推理路径：低不确定性情况下使用简短推理和少量上下文，而高不确定性会触发扩展多路径CoT探索。

**Result:** 实验结果表明，与次优算法对手相比，基于熵自适应推理的方法提高了决策质量。具体而言，平均游戏结果从无熵感知基线方法的-11.6%提升到了+9.5%，同时保持了较低的数量级的每游戏大语言模型查询次数。统计验证确认了这种改进具有显著性。

**Conclusion:** 不确定性引导的自适应推理有助于提高LLM在序列决策环境中的性能。通过对令牌级别熵与行为优劣之间的相关性分析，研究证实了不确定性指导的推理机制确实提升了决策质量。

**Abstract:** We propose a novel LLM-based framework for reasoning in discrete, game-theoretic tasks, illustrated with \emph{Tic-Tac-Toe}. The method integrates in-context learning with entropy-guided chain-of-thought (CoT) reasoning and adaptive context retrieval. The model dynamically adjusts both the number of retrieved examples and reasoning paths according to token-level uncertainty: concise reasoning with minimal context is used when uncertainty is low, whereas higher uncertainty triggers expanded multi-path CoT exploration. Experimental evaluation against a sub-optimal algorithmic opponent shows that entropy-aware adaptive reasoning substantially improves decision quality, increasing the average game outcome from \(-11.6\%\) with the baseline LLM to \(+9.5\%\) with entropy-guided adaptive reasoning over 100 games (win = +1, tie = 0, loss = -1), while maintaining a relatively low number of LLM queries per game. Statistical validation confirms that the improvement is significant, and correlation analysis reveals a negative association between token-level entropy and move optimality. These findings demonstrate that uncertainty-guided adaptive reasoning effectively enhances LLM performance in sequential decision-making environments.

</details>


### [2] [BYOL: Bring Your Own Language Into LLMs](https://arxiv.org/abs/2601.10804)
*Syed Waqas Zamir,Wassim Hamidouche,Boulbaba Ben Amor,Luana Marotti,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

> BYOL框架针对不同资源水平的语言提出了特定的集成路径，包括数据精炼和扩展流程，实现了在低资源语言上超过12%的表现提升，同时通过机器翻译系统解决了极端低资源语言的问题。

<details>
  <summary>Details</summary>

**Motivation:** 语言资源的全球不平衡导致了大量的低资源和极端低资源语言的系统性表现不佳、文化偏差和有限可达性。为了解决这一问题，BYOL框架力求基于每种语言的数字足迹，提供一种可扩展且语言相关的LLM开发统一框架。

**Method:** 该研究提出了一种全栈数据精炼和扩展管道，用于低资源语言，包括语料库清理、合成文本生成、持续预训练和监督微调。同时，对于极端低资源语言，提出了一个翻译介导的包容途径，通过专用的机器翻译系统来提供高精度语言模型访问。

**Result:** 通过应用于Chichewa和Maori，BYOL框架在12个基准测试中实现了约12%的平均改进，同时通过权重空间模型合并保留了英语和多语言能力。在极端低资源语言如Inuktitut上的测试显示，特定的机器翻译系统能使表现超越商用基线4个BLEU值。此外，还发布了这三个语言的人类翻译版本的Global MMLU-Lite基准测试，并开源了代码和模型。

**Conclusion:** Bring Your Own Language (BYOL)框架被证明能够有效地提高低资源和极端低资源语言背景下LLMs的表现，同时形成一种新的途径来推动全球语言数字包容性。

**Abstract:** Large Language Models (LLMs) exhibit strong multilingual capabilities, yet remain fundamentally constrained by the severe imbalance in global language resources. While over 7,000 languages are spoken worldwide, only a small subset (fewer than 100) has sufficient digital presence to meaningfully influence modern LLM training. This disparity leads to systematic underperformance, cultural misalignment, and limited accessibility for speakers of low-resource and extreme-low-resource languages. To address this gap, we introduce Bring Your Own Language (BYOL), a unified framework for scalable, language-aware LLM development tailored to each language's digital footprint. BYOL begins with a language resource classification that maps languages into four tiers (Extreme-Low, Low, Mid, High) using curated web-scale corpora, and uses this classification to select the appropriate integration pathway. For low-resource languages, we propose a full-stack data refinement and expansion pipeline that combines corpus cleaning, synthetic text generation, continual pretraining, and supervised finetuning. Applied to Chichewa and Maori, this pipeline yields language-specific LLMs that achieve approximately 12 percent average improvement over strong multilingual baselines across 12 benchmarks, while preserving English and multilingual capabilities via weight-space model merging. For extreme-low-resource languages, we introduce a translation-mediated inclusion pathway, and show on Inuktitut that a tailored machine translation system improves over a commercial baseline by 4 BLEU, enabling high-accuracy LLM access when direct language modeling is infeasible. Finally, we release human-translated versions of the Global MMLU-Lite benchmark in Chichewa, Maori, and Inuktitut, and make our codebase and models publicly available at https://github.com/microsoft/byol .

</details>


### [3] [A Concise Agent is Less Expert: Revealing Side Effects of Using Style Features on Conversational Agents](https://arxiv.org/abs/2601.10809)
*Young-Min Cho,Yuan Yuan,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.CL

> 研究展示了大型语言模型对话代理中风格特征引导的跨特征副作用，揭示了这些特征之间的复杂相互作用，并提出了一种数据集以支持未来研究。

<details>
  <summary>Details</summary>

**Motivation:** 旨在理解风格特征引导大语言模型对话代理行为时未被充分了解的意外副作用，并期望为未来的对话代理设计提供有效数据集。

**Method:** 通过使用涵盖任务导向型和开放领域的控制合成对话，使用成对的LLM作为评估框架来量化引导一种风格特征如何因果地影响其它特征，进行了跨特征风格副作用的系统研究。

**Result:** 研究结果揭示了一致且有结构的副作用，例如，为了简洁而提示显著降低了感知的专业性，说明风格特征是深度纠缠而不是正交的。

**Conclusion:** 研究发现质疑了关于LLM风格控制的假设，并强调了需要多目标和更系统的方法来安全、针对性地引导对话代理的风格。

**Abstract:** Style features such as friendly, helpful, or concise are widely used in prompts to steer the behavior of Large Language Model (LLM) conversational agents, yet their unintended side effects remain poorly understood. In this work, we present the first systematic study of cross-feature stylistic side effects. We conduct a comprehensive survey of 127 conversational agent papers from ACL Anthology and identify 12 frequently used style features. Using controlled, synthetic dialogues across task-oriented and open domain settings, we quantify how prompting for one style feature causally affects others via a pairwise LLM as a Judge evaluation framework. Our results reveal consistent and structured side effects, such as prompting for conciseness significantly reduces perceived expertise. They demonstrate that style features are deeply entangled rather than orthogonal. To support future research, we introduce CASSE (Conversational Agent Stylistic Side Effects), a dataset capturing these complex interactions. We further evaluate prompt based and activation steering based mitigation strategies and find that while they can partially restore suppressed traits, they often degrade the primary intended style. These findings challenge the assumption of faithful style control in LLMs and highlight the need for multi-objective and more principled approaches to safe, targeted stylistic steering in conversational agents.

</details>


### [4] [Reasoning Models Generate Societies of Thought](https://arxiv.org/abs/2601.10825)
*Junsol Kim,Shiyang Lai,Nino Scherrer,Blaise Agüera y Arcas,James Evans*

Main category: cs.CL

> 研究发现，推理增强不仅来自延长的计算链，还来自模拟多代理互动，这使得内部认知视角更加多样化。通过定量分析，发现推理模型比指令调优模型在推理过程中展现了更大的视角多样性和特征冲突，这解释了它们在推理任务中的精度优势。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在各个域中展现了卓越的能力，但复杂的推理机制仍然模糊不清。研究揭示延长的计算并不是唯一原因，多代理交互模拟在推理过程中的作用值得探讨。

**Method:** 通过定量分析和机制可解释方法应用于推理轨迹，研究者发现像DeepSeek-R1和QwQ-32B这样的推理模型展现出比指令调优模型更大的视角多样性，在推理过程中激活了与个性和专长相关的特征之间的更广范围的冲突。

**Result:** 推理模型中多代理结构体现在诸如问答、视角转换和冲突观点的调和等会话行为中，并且显现出社会化的情感角色，这解释了它们在推理任务中的精度优势。

**Conclusion:** 研究结果表明，思维的社会化组织能够有效地探索解决方案空间，并提出群体中多样性和系统化结构的集体智慧可以被计算模型模拟，从而为代理组织开辟新的机会。

**Abstract:** Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.

</details>


### [5] [EncodeRec: An Embedding Backbone for Recommendation Systems](https://arxiv.org/abs/2601.10837)
*Guy Hadad,Neomi Rabaev,Bracha Shapira*

Main category: cs.CL

> 本文提出了EncodeRec方法，解决了预训练语言模型在推荐系统中的嵌入问题，提高了推荐性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前推荐系统越来越多地利用大型预训练语言模型（PLMs）的嵌入，但这些嵌入存在两个关键限制：PLMs不是专门优化以生成结构化和判别性嵌入空间的；它们的表示仍然过于通用，常常无法捕捉推荐任务中关键的领域特定语义。

**Method:** 我们提出了一种名为EncodeRec的方法，它将文本表示与推荐目标对齐，同时直接从项目描述中学得紧凑且具有信息量的嵌入。EncodeRec在推荐系统训练过程中保持语言模型参数不变，从而在不牺牲语义保真度的情况下保持计算效率。

**Result:** 实验结果显示EncodeRec在核心推荐基准上的有效性，无论是作为序列推荐模型的骨干模型还是语义ID标记化。与基于PLM和嵌入模型的基线相比，EncodeRec显示出显著的改进。

**Conclusion:** 这些结果强调了嵌入适应在弥合通用语言模型和实用推荐系统之间的差距所起的关键作用。

**Abstract:** Recent recommender systems increasingly leverage embeddings from large pre-trained language models (PLMs). However, such embeddings exhibit two key limitations: (1) PLMs are not explicitly optimized to produce structured and discriminative embedding spaces, and (2) their representations remain overly generic, often failing to capture the domain-specific semantics crucial for recommendation tasks. We present EncodeRec, an approach designed to align textual representations with recommendation objectives while learning compact, informative embeddings directly from item descriptions. EncodeRec keeps the language model parameters frozen during recommender system training, making it computationally efficient without sacrificing semantic fidelity. Experiments across core recommendation benchmarks demonstrate its effectiveness both as a backbone for sequential recommendation models and for semantic ID tokenization, showing substantial gains over PLM-based and embedding model baselines. These results underscore the pivotal role of embedding adaptation in bridging the gap between general-purpose language models and practical recommender systems.

</details>


### [6] [DialDefer: A Framework for Detecting and Mitigating LLM Dialogic Deference](https://arxiv.org/abs/2601.10896)
*Parisa Rabbani,Priyam Sahoo,Ruben Mathew,Aishee Mondal,Harshita Ketharaman,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

> 研究发现，LLMs在不同的对话框架下对相同内容进行评估时，产生的判断结果有所不同，研究者提出了一种名为DialDefer的框架来检测和缓解这种问题，发现对话中人类与AI的归因主要驱动了较大的判断偏移变化。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大型语言模型（LLMs）越来越多地被用作第三方评审员，但它们在评估对话中的发言者时的可靠性仍然知之甚少。本文旨在研究LLMs在评估对话内容时由于上下文框架不同而产生的不同判断。

**Method:** 该研究提出了一种名为DialDefer的框架，用于检测和缓解对话中由于框架引起的判断变化。该框架引入了一个评估标准：对话偏向得分(DDS)，用以捕捉框架变化引起的准确率稳定情况下的方向性偏移。

**Result:** 在九个不同领域、超过3000个实例和四种不同模型中，研究发现对话框架可以引起显著的判断变化(|DDS|高达87pp，p < .0001)，而准确率变化微乎其微(<2pp)。这种效应在自然的Reddit对话中被放大了2-4倍。不同领域中，模型可能会偏移至同意或怀疑，同一模型在科学和社交判断中的评分可以相差-53到+58点。

**Conclusion:** 研究表明，对话框架对LLMs的判断影响重大，人类与LLM的归因驱动了最大的偏移变化。初步缓解措施可以减少偏向，但也可能过度调整，使得问题变成校准问题，而不仅仅是准确率优化。

**Abstract:** LLMs are increasingly used as third-party judges, yet their reliability when evaluating speakers in dialogue remains poorly understood. We show that LLMs judge identical claims differently depending on framing: the same content elicits different verdicts when presented as a statement to verify ("Is this statement correct?") versus attributed to a speaker ("Is this speaker correct?"). We call this dialogic deference and introduce DialDefer, a framework for detecting and mitigating these framing-induced judgment shifts. Our Dialogic Deference Score (DDS) captures directional shifts that aggregate accuracy obscures. Across nine domains, 3k+ instances, and four models, conversational framing induces large shifts (|DDS| up to 87pp, p < .0001) while accuracy remains stable (<2pp), with effects amplifying 2-4x on naturalistic Reddit conversations. Models can shift toward agreement (deference) or disagreement (skepticism) depending on domain -- the same model ranges from DDS = -53 on graduate-level science to +58 on social judgment. Ablations reveal that human-vs-LLM attribution drives the largest shifts (17.7pp swing), suggesting models treat disagreement with humans as more costly than with AI. Mitigation attempts reduce deference but can over-correct into skepticism, framing this as a calibration problem beyond accuracy optimization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [7] [Future Optical Flow Prediction Improves Robot Control & Video Generation](https://arxiv.org/abs/2601.10781)
*Kanchana Ranasinghe,Honglu Zhou,Yu Fang,Luyu Yang,Le Xue,Ran Xu,Caiming Xiong,Silvio Savarese,Michael S Ryoo,Juan Carlos Niebles*

Main category: cs.CV

> 提出了一种新的光流预测模型FOFPred，结合视觉语言模型(VLM)和扩散架构，能从大规模的网络人类活动数据中学习并进行多模态推理，以高生成保真度预测未来的运动。

<details>
  <summary>Details</summary>

**Motivation:** 光流等未来运动表示对于控制和生成任务具有巨大的价值，然而，从嘈杂的真实世界数据中学习通用的、空间密集的运动表示仍然是一个关键的挑战。

**Method:** 提出了一种新的基于语言条件的光流预测模型FOFPred，该模型结合了视觉语言模型(VLM)和扩散架构，能够进行多模态推理并以像素级别的生成保真度预测未来的运动。

**Result:** 该模型在机器人操作和语言驱动的视频生成任务中表现出跨域的灵活性，并在评估中证明了其有效性。

**Conclusion:** FOFPred模型成功展示了跨域应用的能力，证实了统一VLM-Diffusion架构和大规模网络数据学习对未来光流预测的价值。

**Abstract:** Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.

</details>


### [8] [ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research](https://arxiv.org/abs/2601.10802)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

> 本文介绍了ICONIC-444数据集，用于OOD检测研究，包含超过310万张图像，444个类别，并提供了基准任务和评估方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏大规模、高质量的OOD数据集，现有的OOD检测研究受到了限制。ICONIC-444的引入解决了这一问题，提供了用于OOD检测的综合性数据集。

**Method:** 我们引入了ICONIC-444，这是一个大型的工业图像数据集，包含超过310万张RGB图像，涵盖了444个类别，专门为OOD检测研究设计。该数据集支持从精细到粗糙的不同计算机视觉任务，并提供了四个基准任务来评估和推进OOD检测研究。

**Result:** 提供了22种最先进的OOD检测方法的基线结果，展示了数据集的有效性。

**Conclusion:** ICONIC-444数据集的引入为OOD检测研究提供了全面、多样化的数据来源，有助于推进该领域的研究。

**Abstract:** Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.

</details>


### [9] [A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems](https://arxiv.org/abs/2601.10819)
*Yizhou Wang,Sameer Pusegaonkar,Yuxing Wang,Anqi Li,Vishal Kumar,Chetan Sethi,Ganapathy Aiyer,Yun He,Kartikay Thakkar,Swapnil Rathi,Bhushan Rupde,Zheng Tang,Sujit Biswas*

Main category: cs.CV

> 本文提出了一种用于大规模基础设施环境的Sparse4D框架，通过引入考虑遮挡的ReID嵌入模块和生成数据增强策略，解决了异构相机部署和遮挡问题，并在AI City Challenge 2025基准测试中达到了顶尖性能，实现了实时部署所需的高速度。

<details>
  <summary>Details</summary>

**Motivation:** 准确的3D物体感知和多目标多相机（MTMC）跟踪是工业基础设施数字化转型的基础。然而，将“inside-out”自动驾驶模型转换为“outside-in”静止相机网络面临异构相机部署和极端遮挡的挑战。

**Method:** 本研究提出了一种专门针对大规模基础设施环境的Sparse4D框架，该框架利用绝对世界坐标的几何先验并引入了一个考虑遮挡的ReID嵌入模块，以保持分布式传感器网络中的身份稳定性。为了弥合Sim2Real域差距且无需人工标注，我们采用了一种使用NVIDIA COSMOS框架的生成数据增强策略，创建多种环境样式以增强模型的外观不变性。此外，为了解决实时部署的约束条件，我们还开发了一个优化的TensorRT插件，用于多尺度可变形聚合（MSDA），实现了在现代GPU架构上的2.15倍加速。

**Result:** 经过AI City Challenge 2025基准测试，我们的仅采用相机框架达到了HOTA 45.22的顶尖性能。此外，硬件加速的实现使得单个Blackwell类别GPU能够支持超过64个并发相机流。

**Conclusion:** 本文强调了Sparse4D框架在处理大型基础设施中复杂分布传感器环境方面的有效性和创新性，尤其是在遮挡的情况下。它展示了一个成功的Sim2Real泛化方法和一种实时处理多GPU架构的优化技术。

**Abstract:** Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning "inside-out" autonomous driving models to "outside-in" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.

</details>


### [10] [Can Vision-Language Models Understand Construction Workers? An Exploratory Study](https://arxiv.org/abs/2601.10835)
*Hieu Bui,Nathaniel E. Chodosh,Arash Tavakoli*

Main category: cs.CV

> 本研究评估了三种领先视觉语言模型（GPT-4o、Florence 2 和 LLaVa-1.5）在识别建筑工人动作和情感方面的性能，使用一个精心挑选的1,000张图片的数据集。结果显示GPT-4o在两个任务中表现最佳，而其他模型表现较差。尽管如此，研究表明这些通用视觉语言模型为建筑环境中的人类行为识别提供了基准能力，但仍需进一步改进以实现现实世界的可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 随着机器人技术在施工工作流程中的应用越来越广泛，使其能够理解和响应人类行为对于安全有效的协作至关重要。视觉语言模型（VLMs）由于其无需大量领域特定培训就能识别人类行为的能力，在施工领域尤为有吸引力。

**Method:** 使用一个包含1,000张图片的数据集，这些图片被标注为10个动作类别和10个情感类别。通过标准化的推理管道评估每个模型的表现，并采用多种评估指标。

**Result:** GPT-4o在识别动作和情感方面得分最高，平均F1分为0.756和0.712，准确率为0.799和0.773；Florence 2和LLaVa-1.5则表现较弱，F1分分别仅为0.497/0.414和0.466/0.461。

**Conclusion:** 研究表明，尽管通用视觉语言模型为建筑环境中的人类行为识别提供了一定的基础能力，但在实际应用中仍需通过领域适应、时态建模或多模态传感器等进一步改进。

**Abstract:** As robotics become increasingly integrated into construction workflows, their ability to interpret and respond to human behavior will be essential for enabling safe and effective collaboration. Vision-Language Models (VLMs) have emerged as a promising tool for visual understanding tasks and offer the potential to recognize human behaviors without extensive domain-specific training. This capability makes them particularly appealing in the construction domain, where labeled data is scarce and monitoring worker actions and emotional states is critical for safety and productivity. In this study, we evaluate the performance of three leading VLMs, GPT-4o, Florence 2, and LLaVa-1.5, in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, we assess each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 for action and 0.414 for emotion, while LLaVa-1.5 showed the lowest overall performance, with F1-scores of 0.466 for action and 0.461 for emotion. Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, such as collaborating in teams versus communicating with supervisors. While the results indicate that general-purpose VLMs can offer a baseline capability for human behavior recognition in construction environments, further improvements, such as domain adaptation, temporal modeling, or multimodal sensing, may be needed for real-world reliability.

</details>


### [11] [One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection](https://arxiv.org/abs/2601.10836)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

> 对于出分布(OOD)检测的研究，发现训练策略和OOD检测性能之间存在非单调关系，且训练策略的选择与检测器的选择相互影响。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在OOD检测方面取得了稳步进展，但这些检测器与最大化内部分布准确性和泛化能力的现代训练流程之间的相互作用仍然没有得到充分探索。因此，这项研究旨在填补这一知识缺口。

**Method:** 我们通过广泛的实证研究，调查了现有的训练策略与出分布（OOD）检测之间的联系。研究中采用了广泛采用的ResNet-50网络架构，并测试了21种最先进的OOD检测方法。

**Result:** 研究表明，随着准确性的提高，初始OOD性能有所提升，但当训练方法将准确性提高到超过基线水平时，OOD性能开始下降。此外，训练策略、检测器的选择以及最后的OOD性能之间存在强烈的相互依赖关系。

**Conclusion:** 实验结果表明，目前没有单一的OOD检测方法能在所有情况下都表现出最优性能，这表明需要根据特定的训练策略来选择相应的检测方法。

**Abstract:** Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.

</details>


### [12] [Effects of Different Attention Mechanisms Applied on 3D Models in Video Classification](https://arxiv.org/abs/2601.10854)
*Mohammad Rasras,Iuliana Marin,Serban Radu,Irina Mocanu*

Main category: cs.CV

> 研究了减少时间数据的影响同时提升帧分辨率对3D Resnet模型性能的影响，实验表明添加注意力模块后模型精度提升，特别是多头注意力机制在一个变种中表现最佳。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探究减少时间数据捕获的信息量，同时增加帧分辨率对模型性能的影响。

**Method:** 该研究通过在原始的MC3, R3D 和 R(2+1)D模型基础上添加dropout层，并开发了含有注意力机制模块（如CBAM、TCN及多头注意力机制）的十个新变种模型来观察这些模块对模型表现的影响。

**Result:** 实验数据显示，在UCF101数据集上添加多头注意力机制的R(2+1)D模型变体的准确率达到88.98%。不同变种在类别级别的准确性上有不同的表现。

**Conclusion:** 这项研究得出结论，缺少时间特征会对新创建的高分辨率模型的性能有显著影响。

**Abstract:** Human action recognition has become an important research focus in computer vision due to the wide range of applications where it is used. 3D Resnet-based CNN models, particularly MC3, R3D, and R(2+1)D, have different convolutional filters to extract spatiotemporal features. This paper investigates the impact of reducing the captured knowledge from temporal data, while increasing the resolution of the frames. To establish this experiment, we created similar designs to the three originals, but with a dropout layer added before the final classifier. Secondly, we then developed ten new versions for each one of these three designs. The variants include special attention blocks within their architecture, such as convolutional block attention module (CBAM), temporal convolution networks (TCN), in addition to multi-headed and channel attention mechanisms. The purpose behind that is to observe the extent of the influence each of these blocks has on performance for the restricted-temporal models. The results of testing all the models on UCF101 have shown accuracy of 88.98% for the variant with multiheaded attention added to the modified R(2+1)D. This paper concludes the significance of missing temporal features in the performance of the newly created increased resolution models. The variants had different behavior on class-level accuracy, despite the similarity of their enhancements to the overall performance.

</details>


### [13] [Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation](https://arxiv.org/abs/2601.10880)
*Chongcong Jiang,Tianxingjian Ding,Chuhan Song,Jiachen Tu,Ziyang Yan,Yihua Shao,Zhenyi Wang,Yuzhang Shang,Tianyu Han,Yu Tian*

Main category: cs.CV

> 为了克服直接使用SAM3模型进行医学图像分割中的领域转移问题，研究人员开发了Medical SAM3，通过在大规模医学数据上对SAM3进行全模型微调，显著提高了在复杂医学结构中的分割准确性和灵活性。

<details>
  <summary>Details</summary>

**Motivation:** 发现简单使用SAM3在医学影像数据上的表现会大幅度下降，其优秀表现主要依赖于强几何先验，如人工标注的边界框，因此需要更全面的模型调整，而不仅仅是提示工程的改变。

**Method:** 通过在大规模、异构的二维和三维医学成像数据集上对SAM3进行全模型微调而获得Medical SAM3，该数据集配有分割掩码和文本提示。

**Result:** 实验显示在涉及器官、成像模式和维度的各种情况下，Medical SAM3在语义模糊、复杂形态和长距离三维环境中的表现均有显著提高。

**Conclusion:** Medical SAM3被确立为一种适用于医学影像、基于文本引导的分割基础模型，并突出了在严重领域迁移下实现稳健提示驱动分割的全面模型调整的重要性。

**Abstract:** Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.

</details>


### [14] [FrankenMotion: Part-level Human Motion Generation and Composition](https://arxiv.org/abs/2601.10909)
*Chuqiao Li,Xianghui Xie,Yong Cao,Andreas Geiger,Gerard Pons-Moll*

Main category: cs.CV

> 研究基于大型语言模型（LLMs）创建了一个新的高质量运动数据集，并引入了FrankenMotion框架，使得人类动作生成模型可以同时在空间（身体部位）和时间（原子动作）上进行控制。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文本提示生成人类动作的方法由于缺少细粒度、部位级别的运动标注，导致不能很好控制每一个身体部位，因此，研究者们创建了一个高质量的动作数据集，该数据集具有原子级别的、时间感知的、部位级别的文本标注。

**Method:** 介绍了一种基于扩散模式的身体部位感知运动生成框架FrankenMotion，该框架允许基于时空结构化文本提示对每个身体部位进行指导。

**Result:** 实验结果显示，FrankenMotion不仅优于经过改编和重新训练以适应此项研究的数据集的所有先前基线模型，而且能够合成训练期间未见的运动。

**Conclusion:** 实验表明，FrankenMotion的表现优于所有以前的基线模型，并且可以生成训练过程中未见过的动作组合。

**Abstract:** Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.

</details>


### [15] [Classification of Chest XRay Diseases through image processing and analysis techniques](https://arxiv.org/abs/2601.10913)
*Santiago Martínez Novoa,María Catalina Ibáñez,Lina Gómez Mesa,Jeremias Kramer*

Main category: cs.CV

> 研究首先概述并比较了用于多分类胸部X光图像诊断的不同方法，包括DenseNet121，并通过一个开源的基于网络的应用程序进行测试。研究还分析了方法的缺点并提出了改进建议。

<details>
  <summary>Details</summary>

**Motivation:** 胸部X光图像是诊断胸腔疾病最常用的放射学检查形式之一。本研究旨在概述并比较用于处理此任务的不同方法。

**Method:** 本研究采用了多种方法对胸部X光图像进行多分类诊断，其中包括DenseNet121。此外，研究还使用了一个开源的基于网络的应用程序。

**Result:** 研究通过测试比较了不同方法的效果，并仔细分析了所采用方法的缺点。

**Conclusion:** 本研究提出了未来改进方法的建议，代码已开源。

**Abstract:** Multi-Classification Chest X-Ray Images are one of the most prevalent forms of radiological examination used for diagnosing thoracic diseases. In this study, we offer a concise overview of several methods employed for tackling this task, including DenseNet121. In addition, we deploy an open-source web-based application. In our study, we conduct tests to compare different methods and see how well they work. We also look closely at the weaknesses of the methods we propose and suggest ideas for making them better in the future. Our code is available at: https://github.com/AML4206-MINE20242/Proyecto_AML

</details>


### [16] [Self-learned representation-guided latent diffusion model for breast cancer classification in deep ultraviolet whole surface images](https://arxiv.org/abs/2601.10917)
*Pouya Afshin,David Helminiak,Tianling Niu,Julie M. Jorns,Tina Yen,Bing Yu,Dong Hye Ye*

Main category: cs.CV

> 本文提出了一种利用自我监督学习和潜在扩散模型生成高质量的合成训练数据的方法，以提高用于乳腺保留手术中边界精确评估的深度学习模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于标注的DUV数据稀缺，阻碍了深度学习模型的训练。为了克服这一难题，我们提出了利用自我监督学习方法生成合成数据以增强模型训练的数据质量。

**Method:** 我们提出了一种基于自我监督学习（SSL）引导的潜在扩散模型（LDM）生成高质量伪训练补丁的方法。通过使用从微调的DINO教师模型中得到的嵌入来引导LDM，我们能够将细胞结构的丰富语义细节注入到合成数据中。我们将真实和合成的补丁结合起来，微调一个视觉变换器（ViT），并利用补丁预测聚合来进行WSI级别的分类。

**Result:** 实验结果表明，我们的方法在五折交叉验证中达到了96.47%的准确率，并且将FID分数降低到45.72，大大优于基线的类别条件方法。

**Conclusion:** 研究结果证明，利用自我监督学习指导的潜在扩散模型生成的数据有效地提升了乳腺癌边界精确评估技术的性能。

**Abstract:** Breast-Conserving Surgery (BCS) requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning (SSL)-guided Latent Diffusion Model (LDM) to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer (ViT), utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47 % accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.

</details>
