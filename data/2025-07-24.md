<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 30]
- [cs.CV](#cs.CV) [Total: 36]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
*Shmuel Amar,Ori Shapira,Aviv Slobodkin,Ido Dagan*

Main category: cs.CL

> 本文提出了一种统一的NLP内容选择框架IGCS，创建了一个大型通用合成数据集，并展示了通过迁移学习可以改善模型在内容选择任务中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 这篇文章旨在通过一个统一的框架来解决NLP领域的内容选择任务，这些任务通常各自为政，缺乏统一的方法论、数据集和评价标准。通过引入IGCS框架，希望可以将这些任务的研究结合起来，促进跨任务的学习和优化。

**Method:** 本文提出了基于指令的内容选择（IGCS）框架，该框架将任务定义和实例特定的请求封装为语言模型的指令，作为NLP任务中广泛存在的从给定源文本中选择相关文本片段问题的统一解决方案。此外，作者创建了一个大型的通用合成数据集，用于多种内容选择任务，并展示了这些数据集在没有特定任务训练时也能提升性能的迁移学习能力。

**Result:** 研究引入了首个统一的内容选择基准测试	extit{igcsbench}，并且发现迁移学习从大型通用合成数据集中往往可以获得更好的性能。同时，文章还解决了一些在基于大规模语言模型的内容选择时出现的推理时间问题。

**Conclusion:** 总的来说，研究提出了基于指令的内容选择框架IGCS，并引入了相关的资源和方法，为未来的内容选择模型提供了有用的工具和思路。

**Abstract:** A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [2] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
*Robert Korom,Sarah Kiptinness,Najib Adan,Kassim Said,Catherine Ithuli,Oliver Rotich,Boniface Kimani,Irene King'ori,Stellah Kamau,Elizabeth Atemba,Muna Aden,Preston Bowman,Michael Sharman,Rebecca Soskin Hicks,Rebecca Distler,Johannes Heidecke,Rahul K. Arora,Karan Singhal*

Main category: cs.CL

> 通过在肯尼亚诊所的真实环境中使用AI Consult，研究发现这款基于大型语言模型的临床决策工具能够减少诊断和治疗错误，提高医疗质量。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于探索AI支持的临床决策工具在真实世界中的应用效果，具体来说，通过质量改进研究来评估AI Consult如何帮助减少临床决策和文档制定中的错误。

**Method:** 本研究通过与肯尼亚内罗毕的Penda Health初级诊所网络合作，评估了大型语言模型支持的临床决策工具—AI Consult在实时照护环境中的影响。AI Consult工具旨在识别潜在的文档和临床决策错误，只在必要时激活，保护了临床医生的自主性。

**Result:** 在比较了15个诊所的39,849次患者访问后发现，使用AI Consult的临床医生出现的错误较少，即诊断错误减少了16%，治疗错误减少了13%。此外，所有使用AI Consult的临床医生认为该工具提高了他们提供的医疗质量，其中75%认为效果显著。

**Conclusion:** 研究表明，与临床工作流程相适应并积极部署的AI Consult可以提高医疗质量，并且减少诊断和治疗错误，为负责地采纳基于大型语言模型的临床决策支持工具提供了一个实用框架。

**Abstract:** We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

</details>


### [3] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
*Shuyuan Lin,Lei Duan,Philip Hughes,Yuxuan Sheng*

Main category: cs.CL

> SALU, a novel method integrating unanswerability detection into LLMs, uses multi-task learning and RLHF to enhance reliability in conversational information retrieval.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of handling unanswerable questions in CIR systems and avoid generating misleading content.

**Method:** SALU employs a multi-task learning framework and confidence-score-guided reinforcement learning with human feedback to train LLMs for QA and abstention.

**Result:** SALU outperforms baselines, demonstrating high accuracy in answering questions or abstaining when necessary and significantly reducing hallucination.

**Conclusion:** SALU improves the reliability of CIR systems by training LLMs to recognize their knowledge boundaries and appropriately abstain from unanswerable questions.

**Abstract:** Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [4] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
*Aleksandr Perevalov,Andreas Both*

Main category: cs.CL

> 论文介绍了一种名为mKGQAgent的框架，旨在通过分解任务来将自然语言问题转化为SPARQL查询，特别适用于多语言知识图谱问答任务。该框架在Text2SPARQL挑战中取得了第一的成绩。

<details>
  <summary>Details</summary>

**Motivation:** 目前的方法多结合成多个组件来解决下游任务，缺乏对任务的分解和模块化处理。因此，提出了mKGQAgent来提升处理多语言知识图谱问答的能力。

**Method:** Structure

**Result:** {
  "tldr": "论文介绍了一种名为mKGQAgent的框架，旨在通过分解任务来将自然语言问题转化为SPARQL查询，特别适用于多语言知识图谱问答任务。该框架在Text2SPARQL挑战中取得了第一的成绩。",
  "motivation": "目前的方法多结合成多个组件来解决下游任务，缺乏对任务的分解和模块化处理。因此，提出了mKGQAgent来提升处理多语言知识图谱问答的能力。",
  "method": "mKGQAgent框架使用一个协调的LLM代理工作流程（包括规划、实体链接和查询优化）来处理任务。通过一个策略池进行上下文学习来指导多个子任务。",
  "result": "在DBpedia和Corporate基准上的KGQA任务上，mKGQAgent在Text2SPARQL挑战2025中取得了最好的成绩。",
  "conclusion": "这项工作为开发具有人类相似推理能力的多语言语义解析系统开辟了新的可能性。">
}


**Conclusion:** 这项工作为开发具有人类相似推理能力的多语言语义解析系统开辟了新的可能性。

**Abstract:** Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

</details>


### [5] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
*Rishemjit Kaur,Arshdeep Singh Bhankhar,Surangika Ranathunga,Jashanpreet Singh Salh,Sudhir Rajput,Vidhi,Kashish Mahendra,Bhavika Berwal,Ritesh Kumar*

Main category: cs.CL

> 研究生成多语言合成农业数据集并对语言特定的LLMs进行微调，显著提高了LLMs在农业领域尤其是多语言和低资源环境下的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的公共通用大型语言模型在农业领域通常缺乏精确的地方性和多语言上下文建议，且在农业特定训练和高质量、地区特定数据方面不足。

**Method:** 通过生成多语言的合成农业数据集（英语、印地语、旁遮普语）来自农业特定文档，并对语言特定的大型语言模型（LLMs）进行微调。

**Result:** 在精心策划的多语言数据集上的评估显示，微调模型在事实准确性、相关性和农业共识方面比基线模型有显著提高。

**Conclusion:** 该研究通过合成数据驱动的、语言特定的微调策略，大幅提升了LLMs在农业领域表现，特别是在多语言和低资源设置中的性能。

**Abstract:** Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

</details>


### [6] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
*Giulio Pelosio,Devesh Batra,Noémie Bovey,Robert Hankache,Cristovao Iglesias,Greig Cowan,Raad Khraishi*

Main category: cs.CL

> 研究展示了小型和大型语言模型在潜在国籍偏见和准确性方面的差异，强调了偏见问题的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 研究大型语言模型在没有显式的人口统计标记时仍然可能存在的国籍偏见问题。

**Method:** 提出了一种基于名字的基准测试方法，该方法源自于BBQ数据集，用于研究用文化指示性名字代替显式的国籍标签对偏见和准确性的影响。

**Result:** 小型模型在准确性方面表现较差且表现出更多的偏见。例如，在模糊的背景下，Claude Haiku的刻板印象偏见评分为9%，而更大的模型Claude Sonnet的评分为3.5%，且在准确性上高出117.7%。此外，小模型在这些模糊背景下保留了更大比例的现有错误。

**Conclusion:** 研究表明，即使使用了文化指示性名字代替国籍标签，大型语言模型中的偏见仍然存在，这强调了这种偏见对于开发和部署全球范围内的AI系统的重要影响。

**Abstract:** Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

</details>


### [7] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
*Ming Huang,Zehan Li,Yan Hu,Wanjing Wang,Andrew Wen,Scott Lane,Salih Selek,Lokesh Shahani,Rodrigo Machado-Vieira,Jair Soares,Hua Xu,Hongfang Liu*

Main category: cs.CL

> 研究使用GPT-3.5和GPT-4.5对自杀相关因素进行多标签分类，从EHRs中提取信息，展示了生成式AI在临床分类任务中的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是应对忽视自杀相关因素复杂性的二分类任务问题，提出了一种新的端到端生成式MLC流水线，并引入了先进的评估方法，如标签集级评估指标和多标签混淆矩阵进行错误分析。

**Method:** 该研究采用GPT-3.5和GPT-4.5等生成式大型语言模型（LLMs），用于从精神科电子健康记录（EHRs）中进行自杀相关因素（SrFs）的多标签分类（MLC）。

**Result:** 微调后的GPT-3.5达到了0.94的部分匹配准确率和0.91的F1分数，而GPT-4.5在引导提示下展示出了跨越多个标签集的优越性能，包括罕见或少数标签集，显示了更平衡和稳健的表现。

**Conclusion:** 该研究发现系统性错误模式，如自杀观念(SI)与自杀企图(SA)之间的混淆，并指出模型存在谨慎的过度标注倾向。这不仅展示了生成式AI在复杂临床分类任务中的可行性，还提供了一个结构化无结构EHR数据的蓝图，以支持大规模临床研究和基于证据的医学。

**Abstract:** Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

</details>


### [8] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
*Arduin Findeis,Floris Weers,Guoli Yin,Ke Ye,Ruoming Pang,Tom Gunter*

Main category: cs.CL

> 本研究提出了一种工具辅助的标注系统，通过引入网络搜索和代码执行工具，以提高在长篇事实性、数学和编程任务上的标注质量。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在解决在某些领域难以获得高质量成对比较标注的问题，特别是针对含有大量事实陈述的响应。在这种情况下，标注者可能过多地重视写作质量而非内容的真实性。

**Method:** 本研究通过引入基于网络搜索和代码执行的工具，以提高在长篇事实性、数学和编程任务上的人工标注系统的性能。这些工具独立于语言模型的内部知识和偏见，旨在为这些特定领域提供更高质量的反馈。

**Result:** 实验结果显示，这些外部工具确实可以在许多情况下提升标注性能，但并非在所有情况下均有效。此外，实验还强调了性能对简单参数（如提示语）的敏感性和需要改进的标注基准。

**Conclusion:** 本研究的结果支持在特定任务中使用外部工具可以改善人工标注的质量，但也表明标注质量高度依赖于实验设计中的具体参数和非饱和标注基准。

**Abstract:** Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

</details>


### [9] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
*Soumen Sinha,Shahryar Rahnamayan,Azam Asilian Bidgoli*

Main category: cs.CL

> This paper explores a method to improve the efficiency of text embeddings in NLP by using binary representations with feature-specific thresholds, demonstrating its superiority over traditional methods in terms of accuracy.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the issues of storage and computational efficiency in large-scale NLP applications by using more efficient binary representations for text embeddings, which are typically real-valued.

**Method:** The paper introduces a Coordinate Search-based optimization framework for identifying optimal thresholds to convert continuous embeddings, such as those from BERT, into binary representations. This method applies feature-specific thresholds to improve the performance of binary encoding.

**Result:** The optimal barcode representations proposed in the paper showed improved performance in accuracy when compared to traditional binarization methods. The experiments encompassed various NLP tasks and datasets, indicating that the method can achieve better binary encoding and is superior in diverse application scenarios.

**Conclusion:** The conclusion is that using feature-specific optimal thresholds in the Coordinate Search-based framework produces higher quality binary representations, which can lead to improved performance in NLP tasks and potentially across other domains in machine learning.

**Abstract:** Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

</details>


### [10] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
*Cheng Liu,Yifei Lu,Fanghua Ye,Jian Li,Xingyu Chen,Feiliang Ren,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

> 本文提出CogDual，一种新的角色扮演语言代理，它通过认知-响应推理范式，结合情景意识和自我意识，使用强化学习优化性能。实验中，CogDual展现了卓越的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的角色扮演语言代理（RPLAs）侧重于模仿特定场景中的角色行为，但忽略了这些行为背后的认知机制。本文旨在通过引入CogDual填补这一空白，并提高RPLAs的表现。

**Method:** CogDual, 采用认知-响应推理范式，结合外部情景意识和内部自我意识来生成回答，以提高人物一致性和情境一致性。同时使用了强化学习和两个通用奖励机制来优化性能。

**Result:** 实验结果表明，CogDual在CoSER基准和Cross-MR以及LifeChoice数据集上的表现优于现有基线，并且在不同的角色扮演任务中展现了良好的泛化能力。

**Conclusion:** CogDual的有效性和泛化能力表明，通过引入认知机制，角色扮演语言代理能够获得更好的表现。此方法提供了一个新的视角来改进大型语言模型的行为模拟。

**Abstract:** Role-Playing Language Agents (RPLAs) have emerged as a significant
application direction for Large Language Models (LLMs). Existing approaches
typically rely on prompt engineering or supervised fine-tuning to enable models
to imitate character behaviors in specific scenarios, but often neglect the
underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by
cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a
\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external
situational awareness and internal self-awareness, CogDual generates responses
with improved character consistency and contextual alignment. To further
optimize the performance, we employ reinforcement learning with two
general-purpose reward schemes designed for open-domain text generation.
Extensive experiments on the CoSER benchmark, as well as Cross-MR and
LifeChoice, demonstrate that CogDual consistently outperforms existing
baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [11] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
*Zhiqiang Liu,Enpei Niu,Yin Hua,Mengshu Sun,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

> 本文介绍了SKA-Bench，这是一个包含四种常见的结构化知识形式的增强型QA基准测试，使用一种三阶段的管道来构建数据集，并评估出了LLMs在处理结构化知识时的能力和不足。

<details>
  <summary>Details</summary>

**Motivation:** 现有的针对结构化知识理解的评估方法是非严格的，缺乏对特定能力的评估，并且只侧重于单一类型的结构化知识。因此，我们旨在提出一个更为全面和严格的结构化知识理解基准测试，以诊断LLMs的不足之处。

**Method:** 我们引入了SKA-Bench，这是一个涵盖了四种常用的结构化知识形式的增强型QA基准测试：知识图谱(KG)、表格(Table)、KG+文本、Table+文本。我们采用三阶段管道来构建SKA-Bench实例，这些实例包括问题、答案、正向知识单元和噪声知识单元。为了细粒度地评估LLMs在理解结构化知识方面的能力，我们将其扩展为四个基本能力测试床：噪声鲁棒性、次序不敏感性、信息整合能力和否定拒绝能力。

**Result:** 在8种代表性的LLMs上进行的实验证明，现有的LLMs在理解结构化知识方面仍然面临重大挑战，其表现受到噪声量、知识单元顺序、幻觉现象等因素的影响。

**Conclusion:** 我们的数据集和代码可在GitHub上获取：https://github.com/Lza12a/SKA-Bench。实验结果表明，现有的LLMs在理解结构化知识方面还有很大的提升空间，研究这一领域仍有许多工作要做。

**Abstract:** Although large language models (LLMs) have made significant progress in
understanding Structured Knowledge (SK) like KG and Table, existing evaluations
for SK understanding are non-rigorous (i.e., lacking evaluations of specific
capabilities) and focus on a single type of SK. Therefore, we aim to propose a
more comprehensive and rigorous structured knowledge understanding benchmark to
diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a
Structured Knowledge Augmented QA Benchmark that encompasses four widely used
structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a
three-stage pipeline to construct SKA-Bench instances, which includes a
question, an answer, positive knowledge units, and noisy knowledge units. To
evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we
expand the instances into four fundamental ability testbeds: Noise Robustness,
Order Insensitivity, Information Integration, and Negative Rejection. Empirical
evaluations on 8 representative LLMs, including the advanced DeepSeek-R1,
indicate that existing LLMs still face significant challenges in understanding
structured knowledge, and their performance is influenced by factors such as
the amount of noise, the order of knowledge units, and hallucination
phenomenon. Our dataset and code are available at
https://github.com/Lza12a/SKA-Bench.

</details>


### [12] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
*Lingfeng Zeng,Fangqi Lou,Zixuan Wang,Jiajie Xu,Jinyi Niu,Mengping Li,Yifan Dong,Qi Qi,Wei Zhang,Ziwei Yang,Jun Han,Ruilun Feng,Ruiqi Hu,Lejie Zhang,Zhengbo Feng,Yicheng Ren,Xin Guo,Zhaowei Liu,Dongpo Cheng,Weige Cai,Liwen Zhang*

Main category: cs.CL

> This paper presents FinGAIA, a benchmark for evaluating AI agents in the financial sector. Though the best agent, ChatGPT, performed better than non-professionals, it still falls short of human experts, indicating room for improvement in AI capabilities in finance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to explore the multi-step, multi-tool collaboration capabilities of AI agents in the financial sector, which remains underexplored.

**Method:** This paper introduces FinGAIA, an end-to-end benchmark for evaluating AI agents' practical abilities in the financial domain, which includes 407 tasks across seven major financial sub-domains.

**Result:** The best-performing AI agent, ChatGPT, achieved an overall accuracy of 48.9% on FinGAIA, which is better than non-professionals but still significantly lower than financial experts.

**Conclusion:** The research identifies five recurring failure patterns in the tested AI agents, pointing to directions for future research. The work aims to provide an objective assessment and promotion of AI agents in the financial domain.

**Abstract:** The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [13] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
*Giuseppe Russo,Debora Nozza,Paul Röttger,Dirk Hovy*

Main category: cs.CL

> 研究发现，LLMs在道德判断上仅在人类高度一致时与人类相一致，否则一致性较差。LLMs对道德价值观的依赖范围较狭窄。提出Dynamic Moral Profiling（DMP）方法，显著提高了模型的一致性和道德价值观的多样性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于人们越来越多地依赖大型语言模型（LLMs）来获取道德建议，但尚不清楚LLMs的评估结果与人类道德判断有多一致，我们开展了一项研究来解决这一问题。

**Method:** 我们构建了Moral Dilemma Dataset，包含1,618个真实世界道德困境，配以人类道德判断的分布（包括二元判断和自由文本理由）。我们将问题视为多元分布一致性任务，比较模型和人类在各个道德困境上的判断分布。我们还构建了一个60价值观的分类法，基于从3,783个价值表达中提取的道德价值，分析LLMs对人类道德价值观的依赖程度。

**Result:** 我们发现，只有在高度一致的情况下，模型才与人类判断相一致，当人类判断不一致时，一致性急剧下降。此外，LLMs依赖的道德价值观的范围比人类狭窄。我们提出了一种方法，Dynamic Moral Profiling（DMP）来解决这个问题，这极大地提高了一致性（提高了64.3%），并增强了道德价值观的多样性。

**Conclusion:** 研究揭示了LLMs在道德判断上的局限性以及与人类道德判断的一致性问题，并提出了一种新方法来改进模型的一致性和道德价值观的多样性。

**Abstract:** People increasingly rely on Large Language Models (LLMs) for moral advice,
which may influence humans' decisions. Yet, little is known about how closely
LLMs align with human moral judgments. To address this, we introduce the Moral
Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a
distribution of human moral judgments consisting of a binary evaluation and a
free-text rationale. We treat this problem as a pluralistic distributional
alignment task, comparing the distributions of LLM and human judgments across
dilemmas. We find that models reproduce human judgments only under high
consensus; alignment deteriorates sharply when human disagreement increases. In
parallel, using a 60-value taxonomy built from 3,783 value expressions
extracted from rationales, we show that LLMs rely on a narrower set of moral
values than humans. These findings reveal a pluralistic moral gap: a mismatch
in both the distribution and diversity of values expressed. To close this gap,
we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method
that conditions model outputs on human-derived value profiles. DMP improves
alignment by 64.3% and enhances value diversity, offering a step toward more
pluralistic and human-aligned moral guidance from LLMs.

</details>


### [14] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
*Kyeongkyu Lee,Seonghwan Yoon,Hongki Lim*

Main category: cs.CL

> CLARIFID框架通过模拟专家的两步工作流实现诊断正确性，包括逻辑流程学习、策略优化微调、推理感知解码以及多视图融合，其在临床准确性上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在提出一种优化诊断准确性的自动放射学报告生成功能框架，通过更合理的工作流来确保报告的事实准确性，并通过多视图图像以提高诊断的全面性。

**Method:** CLARIFID采用先通过部分感知预训练学习从发现到印象的逻辑流程，接着利用CheXbert F1分数作为奖励信号通过近似策略优化进行微调，然后强制先完成“发现”部分再合成“印象”部分，最后通过基于视觉变换器的多视图编码器融合多个胸部X光视图。

**Result:** 实验结果表明，该方法在MIMIC-CXR数据集上的临床效果优于现有的基线方法。

**Conclusion:** 实验结果证明了CLARIFID在提高放射学报告生成的事实准确性及诊断全面性上的优越性。

**Abstract:** Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

</details>


### [15] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
*Miaomiao Gao,Xiaoxiao Xiang,Yiwen Guo*

Main category: cs.CL

> 本文描述了一个名为Triple X的语音识别系统，该系统在Multi-Lingual Conversational Speech Language Modeling挑战赛中表现出色，通过创新的架构和训练策略实现多语种对话的准确识别。

<details>
  <summary>Details</summary>

**Motivation:** 本文的工作重点是在多语种对话场景中优化语音识别的准确性。

**Method:** 此论文采用了一种创新的编码器-适配器-大语言模型框架，利用文本基础大型语言模型的强大推理能力，并结合领域特定的调整。同时，通过精心设计的多阶段训练策略和广泛的多语种音频数据集进一步提高多语种识别性能。

**Result:** 实验结果表明，该方法在开发集和测试集上的词错误率（WER）达到了具有竞争力的表现，并在挑战赛中排名第二。

**Conclusion:** 通过使用编码器-适配器-大型语言模型的框架及多阶段训练策略，该研究成功提高了多语种语音识别的性能。

**Abstract:** This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

</details>


### [16] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
*Zhili Shen,Chenxin Diao,Pascual Merita,Pavlos Vougiouklis,Jeff Z. Pan*

Main category: cs.CL

> 研究者们研究了通过适应$\text{GeAR}$这一最先进的基于图的RAG解决方案，来探讨其在SIIGR 2025 LiveRAG挑战中的可行性和局限性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管目前的研究通过利用从文档中提取的实体及其关系等结构化或半结构化信息来改进检索，这些方法通常被设计用来解决特定任务，缺少证明其在更广泛数据集上的通用性的证据。

**Method:** 本研究旨在通过适应最先进的基于图的RAG（检索增强生成）解决方案：$\text{GeAR}$来研究其在SIGIR 2025 LiveRAG挑战上的性能和局限性。

**Result:** 尚未提供具体研究结果。

**Conclusion:** 尚未提供具体结论。

**Abstract:** Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

</details>


### [17] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
*Carlotta Quensel,Neele Falk,Gabriella Lapesa*

Main category: cs.CL

> 本研究通过回归分析，量化了情绪、叙述和缓和对论据质量的影响，并发现这些因素在客观和主观论据质量上的影响存在差异。

<details>
  <summary>Details</summary>

**Motivation:** 尽管存在对个别主观特征的研究，但仍缺乏大规模地分析这些特征与论据强度之间关系的研究。

**Method:** 通过回归分析量化情绪、叙述和缓和等主观因素对客观论据质量和主观说服力的影响力。

**Result:** 研究结果表明，叙述和缓和在客观和主观的论据质量上具有相反的影响效果，而情绪的影响则取决于修辞利用方式，而非领域。

**Conclusion:** 该研究展示了不同主观特征对论据强度的不同影响模式，并对比评估了自动化标注方法。

**Abstract:** In assessing argument strength, the notions of what makes a good argument are
manifold. With the broader trend towards treating subjectivity as an asset and
not a problem in NLP, new dimensions of argument quality are studied. Although
studies on individual subjective features like personal stories exist, there is
a lack of large-scale analyses of the relation between these features and
argument strength. To address this gap, we conduct regression analysis to
quantify the impact of subjective factors $-$ emotions, storytelling, and
hedging $-$ on two standard datasets annotated for objective argument quality
and subjective persuasion. As such, our contribution is twofold: at the level
of contributed resources, as there are no datasets annotated with all studied
dimensions, this work compares and evaluates automated annotation methods for
each subjective feature. At the level of novel insights, our regression
analysis uncovers different patterns of impact of subjective features on the
two facets of argument strength encoded in the datasets. Our results show that
storytelling and hedging have contrasting effects on objective and subjective
argument quality, while the influence of emotions depends on their rhetoric
utilization rather than the domain.

</details>


### [18] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
*Shiting Chen,Zijian Zhao,Jinsong Chen*

Main category: cs.CL

> 本文提出两种方法增强 Retrieval-Augmented Generation (RAG) 的效果，分别是 Mixture-Embedding RAG 和 Confident RAG。结果显示 Confident RAG 相比于基本 RAG 和 LLM 在不同情况下均有明显的提升。

<details>
  <summary>Details</summary>

**Motivation:** 由于在 RAG 中使用不同的变体嵌入模型会导致相似度计算结果不同以及 LLM 产出的回答质量不一，因此研究者希望结合多个嵌入模型的优势来改善 RAG。

**Method:** 我们提出并评估了两种方法来改善 Retrieval-Augmented Generation (RAG)，即 Mixture-Embedding RAG 和 Confident RAG。Mixture-Embedding RAG 通过标准化相似度来排序并选择来自多个嵌入模型的检索结果，但其表现并未优于基本的 RAG。而 Confident RAG 则使用不同的嵌入模型多次生成响应，并选择置信度最高的响应。

**Result:** 实验结果显示，相比于基本的 LLM 和 RAG，Confident RAG 平均提升了约 10% 和 5% 的效果。其结果在不同 LLM 和嵌入模型下的一致性表明，Confident RAG 是一个有效的即插即用方法。

**Conclusion:** Confident RAG 方法提供了一种有效的即插即用解决方案，以改善 RAG 不同领域应用时的回答质量。

**Abstract:** Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

</details>


### [19] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
*Alexander R. Fabbri,Diego Mares,Jorge Flores,Meher Mankikar,Ernesto Hernandez,Dean Lee,Bing Liu,Chen Xing*

Main category: cs.CL

> 本文介绍了Multilingual Native Reasoning Challenge（MultiNRC），一个评估大型语言模型在法语、西班牙语和中文中本土推理能力的新基准，分析结果显示当前LLMs在此方面表现不佳，特别是在文化相关的推理任务上。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多语言推理基准通常通过翻译现有的英文推理基准得到，这导致评估偏向英文文化内容。因此，本文设计了一个全新的多语言推理基准测试，以全面评估语言模型在不同语言及文化背景下的推理能力。

**Method:** 引入了MultiNRC，一个由母语者设计的基准，涵盖了法语、西班牙语和中文中超过1,000个本土、语言和文化相关的推理问题。MultiNRC评估了语言特定的推理、文字游戏与谜语、文化/传统推理以及时带有文化关联的数学推理。对于某些题目，提供英文翻译版本让模型英文推理能力与其他语言对比。

**Result:** 对当下14种领先的大语言模型进行评估，在MultiNRC基准测试中，没有一种模型的得分超过50%。结果显示，模型在语言特定、文化相关的任务上存在显著差异，且在英文数学推理方面表现优于原始语言。

**Conclusion:** 当前的大语言模型在处理多语言背景下的本土推理任务上仍然面临挑战，尤其是文化和逻辑推理任务方面。这表明在跨语言和跨文化知识的理解上仍需进一步研究。

**Abstract:** Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

</details>


### [20] [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)
*Shanbo Cheng,Yu Bao,Zhichao Huang,Yu Lu,Ningxin Peng,Lu Xu,Runsheng Yu,Rong Cao,Ting Han,Zeyang Li,Sitong Liu,Shengtao Ma,Shiguang Pan,Jiongchen Xiao,Nuo Xu,Meng Yang,Rong Ye,Yiming Yu,Ruofei Zhang,Wanyi Zhang,Wenhao Zhu,Liehao Zou,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

> 本文介绍了Seed-LiveInterpret 2.0系统，通过创新的双工语音理解和生成框架实现了同声传译的高质量和低延迟，超出商业解决方案显著。

<details>
  <summary>Details</summary>

**Motivation:** 目标是开发一种能够提供高质量且超低延迟语音到语音转换的同声传译系统，并通过语音克隆技术解决现有系统中存在的问题。

**Method:** 介绍了一种名为Seed-LiveInterpret 2.0的端到端实时同声传译模型，该模型通过创新的双工语音理解和生成框架解决了同声传译中常见的问题，包括次优的转换和翻译质量、缺乏实时语音生成、多方讲话者的混淆以及翻译中的语音膨胀。

**Result:** 实验证明，该模型通过大规模预训练和强化学习，成功实现了翻译准确性和延迟之间的更好平衡，据人类译员验证，在复杂场景下的正确率超过了70%。与现有的商业解决方案相比，Seed-LiveInterpret 2.0的翻译质量显著提高，同时将克隆语音的平均延迟从近10秒降低到了几乎实时的3秒，提高了近70%，极大地增强了实际使用性。

**Conclusion:** 通过引入Seed-LiveInterpret 2.0，本研究证明了在现有同声传译系统存在的挑战中取得显著进步的可能性，特别是在翻译质量和延迟方面。

**Abstract:** Simultaneous Interpretation (SI) represents one of the most daunting
frontiers in the translation industry, with product-level automatic systems
long plagued by intractable challenges: subpar transcription and translation
quality, lack of real-time speech generation, multi-speaker confusion, and
translated speech inflation, especially in long-form discourses. In this study,
we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers
high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning
capabilities. As a fully operational product-level solution, Seed-LiveInterpret
2.0 tackles these challenges head-on through our novel duplex speech-to-speech
understanding-generating framework. Experimental results demonstrate that
through large-scale pretraining and reinforcement learning, the model achieves
a significantly better balance between translation accuracy and latency,
validated by human interpreters to exceed 70% correctness in complex scenarios.
Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by
significant margins in translation quality, while slashing the average latency
of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is
around a near 70% reduction that drastically enhances practical usability.

</details>


### [21] [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)
*Brian DeRenzi,Anna Dixon,Mohamed Aymane Farhi,Christian Resch*

Main category: cs.CL

> 这项研究展示了非洲地区八种语言的合成语音语料库的创建方法，并通过结合真实和合成数据提升了这些语言的自动语音识别性能，所有数据和模型均公开。

<details>
  <summary>Details</summary>

**Motivation:** 这篇论文的动机是评估非洲语言大规模合成语音语料库的能力，目前这些地区大部分语言的语音技术尚未普及。

**Method:** 我们采用了三步流程来生成非洲语言的合成语音语料库：由LLM驱动的文本创建、TTS语音合成和针对ASR的微调。

**Result:** 对于被评估的三种语言（豪萨语、多鲁阿语、奇切瓦语），我们生成了超过2500小时的合成语音数据，其成本还不到真实数据成本的1%。Hausa语言的细调模型显示，合成数据与真实数据相结合可以达到甚至超越仅使用真实数据的效果。对于资源非常有限的语言，如奇切瓦语，合成数据能带来约6.5%的相对改善; 多鲁阿语在某些评估数据上的改进也非常相似。

**Conclusion:** 研究指出需要更稳健的审稿人协议和更准确的评估数据，并且，所有数据和模型都已公开发布，以促进后续关于非洲语言的合成数据研究工作。

**Abstract:** Speech technology remains out of reach for most of the over 2300 languages in
Africa. We present the first systematic assessment of large-scale synthetic
voice corpora for African ASR. We apply a three-step process: LLM-driven text
creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages
for which we create synthetic text achieved readability scores above 5 out of
7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created
more than 2,500 hours of synthetic voice data at below 1% of the cost of real
data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h
synthetic Hausa matched a 500h real-data-only baseline, while 579h real and
450h to 993h synthetic data created the best performance. We also present
gender-disaggregated ASR performance evaluation. For very low-resource
languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2
real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on
some evaluation data, but not on others. Investigating intercoder reliability,
ASR errors and evaluation datasets revealed the need for more robust reviewer
protocols and more accurate evaluation data. All data and models are publicly
released to invite further work to improve synthetic data for African
languages.

</details>


### [22] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
*Bowen Zheng,Ming Ma,Zhongqiao Lin,Tianming Yang*

Main category: cs.CL

> 本文提出了一种新的解码方法SPADE，结合优化的早期退出决策，提升了解码准确性，降低了计算成本，适合大规模语言模型的部署。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型由于其深层次结构而计算成本高昂。虽然先前的研究表明中间层含有足够信息生成准确答案，并提出了减少计算成本的早期退出算法，但这些方法因中间层与输出层表示不对齐而导致解码不准确。

**Method:** 我们提出了SPADE（SPace Alignment DEcoding），一种新的解码方法，通过传播仅包含开始标记和答案标记的最简序列来对齐中间层与输出层表示。此外，我们通过训练SPADE的线性近似来优化早期退出决策过程，该近似使用基于熵的信心度量。

**Result:** 我们的方法显著减少了推理成本，同时不牺牲准确性，为在实际应用中部署大规模语言模型提供了可扩展和高效的解决方案。

**Conclusion:** 通过使用SPADE进行高质量输出生成，并通过监控信心水平在中间层停止推理，我们成功地解决了现有早期退出算法性能不佳的问题。

**Abstract:** Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

</details>


### [23] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
*Changxin Tian,Jiapeng Wang,Qian Zhao,Kunlong Chen,Jia Liu,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

> The paper introduces Warmup-Stable and Merge (WSM), a framework that connects learning rate decay with model merging, showing that merge duration is the key to improved model performance.

<details>
  <summary>Details</summary>

**Motivation:** To provide a unified theoretical foundation for learning rate scheduling that can emulate various decay strategies while being compatible with diverse optimization methods.

**Method:** WSM framework which merges the concept of learning rate decay with model averaging techniques.

**Result:** WSM outperforms traditional approaches with significant improvements in multiple benchmark tests and highlights the importance of merge duration.

**Conclusion:** WSM is a promising method that shows potential for enhancing model performance and could be valuable for long-term model refinement efforts.

**Abstract:** Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

</details>


### [24] [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)
*Victor Hartman,Petter Törnberg*

Main category: cs.CL

> 本研究利用大型语言模型对跨国政治推文负面竞选活动进行分类，展现了执政党和极激进右翼政党在使用负面信息上的差异及LLMs未来在政治数据分析方面所带来的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 负面竞选活动是政治竞争中的核心特征，但是现有的分类方法成本高且扩展性有限，限制了实证研究的发展。该研究旨在克服这些限制，通过大规模的跨国家研究提供新的见解。

**Method:** 本研究引入零样本大型语言模型（LLMs）作为跨语言分类负面竞选活动的新型方法。通过包含十种语言的基准数据集，证明了LLMs的性能可与懂本地语言的人类编码者媲美，并优于传统的监督机器学习方法。此外，研究利用这一新方法，对2017年至2022年间19个欧洲国家议会成员发布的1800万条推文进行分析。

**Result:** 结果显示，执政党使用负面信息的可能性较低；而激进右翼和民粹主义政党表现出更高的负面信息比例。

**Conclusion:** 研究结果进一步深化了我们对政党品牌特性如何塑造多党制系统中战略沟通的理解。总体而言，该研究证明了LLMs在未来能够推动跨语言和文化背景下的政治沟通研究的发展。

**Abstract:** Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (LLMs) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that LLMs achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic communication
in multiparty systems. More broadly, the study demonstrates the potential of
LLMs to enable scalable, transparent, and replicable research in political
communication across linguistic and cultural contexts.

</details>


### [25] [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
*Changxin Tian,Kunlong Chen,Jia Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

> 研究通过引入EL度量标准和大规模实证研究，揭示了MoE模型配置与其计算优势之间的关系，并开发了一个准确预测EL的统一缩放法则，证实了法则的准确性。

<details>
  <summary>Details</summary>

**Motivation:** MoE架构通过解耦参数规模和计算成本来高效扩展大型语言模型，然而，如何预测给定配置的MoE模型的能力仍然是一个问题。

**Method:** 引入了一个新的度量标准Efficiency Leverage (EL)，用于量化MoE模型相对于密集型等效模型的计算优势。通过大规模实证研究，系统地调查了几百个不同配置的MoE模型（参数规模高达28B）及其EL之间的关系。

**Result:** 研究结果表明，EL主要由专家激活比例和计算预算总量决定，这两者都遵循可预测的幂律关系，而专家粒度作为一个非线性调节因子，在一定范围内有明显的最优值。一个统一的缩放法则被整合，能够根据MoE架构配置准确预测其EL。通过对比训练量为0.85B的Ling-mini-beta和6.1B的密集型模型，验证了该缩放法则的准确性。

**Conclusion:** 这项工作提供了高效MoE模型扩展的原理基础和实证依据，有助于更好地理解和提升模型架构设计。

**Abstract:** Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large
Language Models (LLMs) efficiently by decoupling total parameters from
computational cost. However, this decoupling creates a critical challenge:
predicting the model capacity of a given MoE configurations (e.g., expert
activation ratio and granularity) remains an unresolved problem. To address
this gap, we introduce Efficiency Leverage (EL), a metric quantifying the
computational advantage of an MoE model over a dense equivalent. We conduct a
large-scale empirical study, training over 300 models up to 28B parameters, to
systematically investigate the relationship between MoE architectural
configurations and EL. Our findings reveal that EL is primarily driven by the
expert activation ratio and the total compute budget, both following
predictable power laws, while expert granularity acts as a non-linear modulator
with a clear optimal range. We integrate these discoveries into a unified
scaling law that accurately predicts the EL of an MoE architecture based on its
configuration. To validate our derived scaling laws, we designed and trained
Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active
parameters, alongside a 6.1B dense model for comparison. When trained on an
identical 1T high-quality token dataset, Ling-mini-beta matched the performance
of the 6.1B dense model while consuming over 7x fewer computational resources,
thereby confirming the accuracy of our scaling laws. This work provides a
principled and empirically-grounded foundation for the scaling of efficient MoE
models.

</details>


### [26] [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)
*Parker Riley,Siamak Shakeri,Waleed Ammar,Jonathan H. Clark*

Main category: cs.CL

> TyDi QA-WANA是一个旨在评估模型在西亚和北非的10种语言变体中利用大文本上下文回答问题能力的问答数据集，包含28K个信息寻求类问题。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于提供一个多语言问答数据集，用于评估模型在未经过翻译直接在各种语言变体中处理问题和回答的能力，并特别考虑了文化相关性问题。

**Method:** TyDi QA-WANA是一个问答数据集，包含28K个示例，涵盖了西亚和北非的10种语言变体。这个数据集的收集过程旨在激发知识探索类型的问题，即提问者真正在乎问题的答案。每个问题都与一篇文章配对，这篇文章可能包含也可能不包含答案。为了评估模型利用大量文本上下文回答问题的能力，文章的篇幅较大。此外，数据直接在每种语言变体中收集，避免了文化相关性的问题。

**Result:** 文中呈现了两个基线模型的性能，并且公开了代码和数据，以便研究社区进一步改进。

**Conclusion:** 该研究创建了一个具有挑战性的多语言问答数据集，并向社区发布了相关数据和模型代码，以促进该领域的进一步研究和发展。

**Abstract:** We present TyDi QA-WANA, a question-answering dataset consisting of 28K
examples divided among 10 language varieties of western Asia and northern
Africa. The data collection process was designed to elicit information-seeking
questions, where the asker is genuinely curious to know the answer. Each
question in paired with an entire article that may or may not contain the
answer; the relatively large size of the articles results in a task suitable
for evaluating models' abilities to utilize large text contexts in answering
questions. Furthermore, the data was collected directly in each language
variety, without the use of translation, in order to avoid issues of cultural
relevance. We present performance of two baseline models, and release our code
and data to facilitate further improvement by the research community.

</details>


### [27] [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
*Karen Zhou,John Giorgi,Pranav Mani,Peng Xu,Davis Liang,Chenhao Tan*

Main category: cs.CL

> 本研究针对AI生成的临床笔记质量评估提出了基于系统化的用户反馈检查表方法，该方法能够更好地反映临床医生的真实偏好，并在多个方面优于现有的自动化评估方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的自动化指标往往未能与真正的临床医生偏好相一致，因此，研究目的是解决AI生成的临床记录的质量评价难题。

**Method:** 提出了一种将真实用户反馈系统地提炼成结构化检查表的流水线方法。该检查表旨在可解释、基于人类反馈、并通过基于大规模语言模型（LLM）的评估者来强制执行。

**Result:** 利用来自超过21,000次临床会诊的去标识化数据进行的离线评估表明，与基线方法相比，研究生成的反馈检查表在覆盖范围、多样性和预测力方面更加出色。

**Conclusion:** 实验表明，该检查表对质量退化扰动有较强的鲁棒性，与临床医生偏好高度一致，证明了作为评估方法的有效性。离线研究环境中，该检查表有助于识别质量可能低于设定阈值的笔记。

**Abstract:** AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.

</details>


### [28] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
*Danny D. Leybzon,Shreyas Tirumala,Nishant Jain,Summer Gillen,Michael Jackson,Cameron McPhee,Jennifer Schmidt*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

</details>


### [29] [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)
*Boxun Li,Yadong Li,Zhiyuan Li,Congyi Liu,Weilin Liu,Guowei Niu,Zheyue Tan,Haiyang Xu,Zhuyu Yao,Tao Yuan,Dong Zhou,Yueqing Zhuang,Bo Zhao,Guohao Dai,Yu Wang*

Main category: cs.CL

> Megrez2-Preview：参数量少但性能优秀的轻量级语言模型，适用于资源受限的应用。

<details>
  <summary>Details</summary>

**Motivation:** 开发一种轻量级且高性能的语言模型架构，适用于设备端，能够在保持模型性能的同时降低资源需求。

**Method:** 引入跨层专家共享机制和预门控路由技术，减少参数量，提升推理速度并降低内存消耗。

**Result:** Megrez2 是一种轻量且高性能的语言模型架构，专为设备本地部署优化。通过引入跨层专家共享机制和预门控路由，Megrez2-Preview 版本在参数量显著减少的同时保持了模型大部分能力，并展现了在多种任务上的竞争力或优越性能。

**Conclusion:** Megrez2-Preview 在语言理解、指令跟随、数学推理和代码生成等任务中表现出色，证明了 Megrez2 架构在准确性、效率和部署性之间的良好平衡。

**Abstract:** We present Megrez2, a novel lightweight and high-performance language model
architecture optimized for device native deployment. Megrez2 introduces a novel
cross-layer expert sharing mechanism, which significantly reduces total
parameter count by reusing expert modules across adjacent transformer layers
while maintaining most of the model's capacity. It also incorporates pre-gated
routing, enabling memory-efficient expert loading and faster inference. As the
first instantiation of the Megrez2 architecture, we introduce the
Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and
further enhanced through supervised fine-tuning and reinforcement learning with
verifiable rewards. With only 3B activated and 7.5B stored parameters,
Megrez2-Preview demonstrates competitive or superior performance compared to
larger models on a wide range of tasks, including language understanding,
instruction following, mathematical reasoning, and code generation. These
results highlight the effectiveness of the Megrez2 architecture to achieve a
balance between accuracy, efficiency, and deployability, making it a strong
candidate for real-world, resource-constrained applications.

</details>


### [30] [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
*Linbo Cao,Jinman Zhao*

Main category: cs.CL

> 本文提出了一种新的基于辩论的评估方法，通过让模型进行对抗性的多轮辩论来评估其性能，这种方法可以有效减少数据污染和记忆效应，并降低了数据集创建的成本。

<details>
  <summary>Details</summary>

**Motivation:** 随着前沿语言模型在标准问答基准上变得愈发普及，关于数据污染、记忆效应和不断上升的数据集成本的问题依然存在。本文试图通过新的评估范式解决这些问题，提出了一种更有效、低成本的评估方法。

**Method:** 本文提出了一种基于辩论的评估范式，将现有的问答数据集转换为结构化的对抗辩论，其中一端模型要辩护官方答案，另一端模型则构建并防守一个替代答案，这些辩论由一个对正确答案不知情的裁判模型判定。通过这种多轮论辩的方式，评估方法显著增加了难度，并且对浅层次记忆进行了惩罚，同时利用问答项目减少了策划成本。

**Result:** 实验结果证明了该方法的鲁棒性和其在抵御数据污染方面的有效性。即使对测试问题进行微调的Llama 3.1模型其准确性大幅提高，但辩论中的表现却更差。此外，即使是较弱的裁判也能可靠地区分较强的辩论者，证明了基于辩论的评估方法更适用于未来更强大的系统，同时其成本较低。

**Conclusion:** 本文提出的技术框架强调了“测试集上的预训练不再是衡量高级语言模型真正推理能力的唯一方法”，并且提供了一条可持续的道路来衡量先进语言模型的真正确推理能力。

**Abstract:** As frontier language models increasingly saturate standard QA benchmarks,
concerns about data contamination, memorization, and escalating dataset
creation costs persist. We propose a debate-driven evaluation paradigm that
transforms any existing QA dataset into structured adversarial debates--where
one model is given the official answer to defend, and another constructs and
defends an alternative answer--adjudicated by a judge model blind to the
correct solution. By forcing multi-round argumentation, this approach
substantially increases difficulty while penalizing shallow memorization, yet
reuses QA items to reduce curation overhead. We make two main contributions:
(1) an evaluation pipeline to systematically convert QA tasks into debate-based
assessments, and (2) a public benchmark that demonstrates our paradigm's
effectiveness on a subset of MMLU-Pro questions, complete with standardized
protocols and reference models. Empirical results validate the robustness of
the method and its effectiveness against data contamination--a Llama 3.1 model
fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)
but performed worse in debates. Results also show that even weaker judges can
reliably differentiate stronger debaters, highlighting how debate-based
evaluation can scale to future, more capable systems while maintaining a
fraction of the cost of creating new benchmarks. Overall, our framework
underscores that "pretraining on the test set is no longer all you need,"
offering a sustainable path for measuring the genuine reasoning ability of
advanced language models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [31] [Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery](https://arxiv.org/abs/2507.16849)
*Yi-Shan Chu,Hsuan-Cheng Wei*

Main category: cs.CV

> 提出了一种基于视觉变换器的深度学习框架，用于从遥感图像中精炼灾害影响区域分割，支持台湾太空局开发的应急增值服务。使用少量手动标注区域，通过主成分分析和置信度指数扩展标签，训练多波段输入的视觉变换器模型，提高分割结果的平滑度和可靠性。案例研究证明了其在灾害制图中的可扩展性和有效性。

<details>
  <summary>Details</summary>

**Motivation:** 旨在改进遥感图像中灾害影响区域的分割，特别是在准确的实地数据不可用时支持更可靠的灾害测绘分析。

**Method:** Structure

**Result:** {
  "tldr": "提出了一种基于视觉变换器的深度学习框架，用于从遥感图像中精炼灾害影响区域分割，支持台湾太空局开发的应急增值服务。使用少量手动标注区域，通过主成分分析和置信度指数扩展标签，训练多波段输入的视觉变换器模型，提高分割结果的平滑度和可靠性。案例研究证明了其在灾害制图中的可扩展性和有效性。", 
  "motivation": "旨在改进遥感图像中灾害影响区域的分割，特别是在准确的实地数据不可用时支持更可靠的灾害测绘分析。", 
  "method": "采用视觉变换器框架配合主成分分析以及置信度指数方法进行弱监督学习，使用Sentinel-2和Formosat-5的多波段数据训练多解码器的视觉变换器模型，并采用多阶段损失策略。", 
  "result": "实验结果应用于2022年鄱阳湖干旱和2023年罗得岛火灾的案例研究中，提高了分割结果的空间一致性和可靠性。", 
  "conclusion": "提出的方法可以为灾害影响区域的分割提供一个有效的解决方案，特别是在准确的地面实况无法获得时提供了一种可扩展且可靠的灾害制图方法。"}
}

**Conclusion:** 提出的方法可以为灾害影响区域的分割提供一个有效的解决方案，特别是在准确的地面实况无法获得时提供了一种可扩展且可靠的灾害制图方法。

**Abstract:** We propose a vision transformer (ViT)-based deep learning framework to refine
disaster-affected area segmentation from remote sensing imagery, aiming to
support and enhance the Emergent Value Added Product (EVAP) developed by the
Taiwan Space Agency (TASA). The process starts with a small set of manually
annotated regions. We then apply principal component analysis (PCA)-based
feature space analysis and construct a confidence index (CI) to expand these
labels, producing a weakly supervised training set. These expanded labels are
then used to train ViT-based encoder-decoder models with multi-band inputs from
Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder
variants and multi-stage loss strategies to improve performance under limited
supervision. During the evaluation, model predictions are compared with
higher-resolution EVAP output to assess spatial coherence and segmentation
consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes
wildfire demonstrate that our framework improves the smoothness and reliability
of segmentation results, offering a scalable approach for disaster mapping when
accurate ground truth is unavailable.

</details>


### [32] [Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors](https://arxiv.org/abs/2507.16850)
*Mohamed Adjel*

Main category: cs.CV

> 提出一种结合实时2D关键点检测和几何感知的2D到3D提升方法，使用相机内在参数和个体特定的解剖学先验知识来改善单目3D人体姿态估计的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 单目3D人体姿态估计算法在实时设置和不受限制的环境中仍然是一个具有挑战性和病态的问题。直接的图像到3D方法需要大量的标注数据集和重型模型，而2D到3D的提升提供了一个更加轻量级和灵活的替代方案，尤其是在增强先验知识的情况下。

**Method:** 本研究提出了一种框架，结合实时2D关键点检测与考虑几何的2D到3D提升方法，并明确利用已知的相机内在参数和个体特定的解剖学先验。这种方法基于自校准和生物力学约束逆运动学的最新进展，从MoCap和合成数据集中生成大规模、合理的2D-3D训练样本对。

**Result:** 这种方法可以实现从单目图像中快速、个性化且准确的3D姿态估计，而无需专门的硬件。

**Conclusion:** 该提案旨在促进以数据驱动的学习和模型基础的先验知识相结合的方法，以提高3D人体运动捕捉的准确性、可解释性和部署能力，特别是在边缘设备上的应用。

**Abstract:** Monocular 3D human pose estimation remains a challenging and ill-posed
problem, particularly in real-time settings and unconstrained environments.
While direct imageto-3D approaches require large annotated datasets and heavy
models, 2D-to-3D lifting offers a more lightweight and flexible
alternative-especially when enhanced with prior knowledge. In this work, we
propose a framework that combines real-time 2D keypoint detection with
geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics
and subject-specific anatomical priors. Our approach builds on recent advances
in self-calibration and biomechanically-constrained inverse kinematics to
generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic
datasets. We discuss how these ingredients can enable fast, personalized, and
accurate 3D pose estimation from monocular images without requiring specialized
hardware. This proposal aims to foster discussion on bridging data-driven
learning and model-based priors to improve accuracy, interpretability, and
deployability of 3D human motion capture on edge devices in the wild.

</details>


### [33] [Coarse-to-fine crack cue for robust crack detection](https://arxiv.org/abs/2507.16851)
*Zelong Liu,Yuliang Gu,Zhichao Sun,Huachao Zhu,Xin Xiao,Bo Du,Laurent Najman,Yongchao Xu*

Main category: cs.CV

> CrackCue是一种新的裂缝检测方法，它利用裂缝的细长结构特性来生成更精确的裂缝提示，并通过整合到高级检测网络中显著提高裂缝检测的泛化能力和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管深度学习方法在数据集内表现优异，但在未见领域仍存在泛化能力不足的问题，且之前的裂缝检测方法通常忽视了裂缝的细长结构特性，因此该论文旨在解决这些问题，提高裂缝检测的泛化能力和鲁棒性。

**Method:** 本论文提出的方法CrackCue首先采用了简单最大的池化和上采样操作来处理裂缝图像，以生成一个粗糙的无裂缝背景。随后，通过重建网络获取一个精细的无裂缝背景。原始图像与精细无裂缝背景之间的差异形成精细的裂缝提示，该裂缝提示嵌入了稳定的裂缝先验信息，不受复杂背景等影响。

**Result:** 主要内容：本篇论文提出了一种新的裂缝检测方法CrackCue，基于粗到细的裂缝提示生成。该方法利用裂缝的细长结构特性，首先使用简单的最大池化和上采样操作对裂缝图像进行处理，生成一个粗糙的无裂缝背景，然后通过一个重建网络获得一个更精细的无裂缝背景。原始图像与精细无裂缝背景之间的差异提供了一个精细的裂缝提示，这个提示包含稳定的裂缝先验信息，不受复杂背景、阴影和变化的光照的影响。作为即插即用的方法，CrackCue被整合到三个高级裂缝检测网络中，实验结果表明，CrackCue显著提高了基准方法的泛化能力和鲁棒性。该源代码将会公开。

**Conclusion:** 实验显示，通过将CrackCue整合到三个高级裂缝检测网络中，其显著提升了方法的泛化能力和鲁棒性。

**Abstract:** Crack detection is an important task in computer vision. Despite impressive
in-dataset performance, deep learning-based methods still struggle in
generalizing to unseen domains. The thin structure property of cracks is
usually overlooked by previous methods. In this work, we introduce CrackCue, a
novel method for robust crack detection based on coarse-to-fine crack cue
generation. The core concept lies on leveraging the thin structure property to
generate a robust crack cue, guiding the crack detection. Specifically, we
first employ a simple max-pooling and upsampling operation on the crack image.
This results in a coarse crack-free background, based on which a fine
crack-free background can be obtained via a reconstruction network. The
difference between the original image and fine crack-free background provides a
fine crack cue. This fine cue embeds robust crack prior information which is
unaffected by complex backgrounds, shadow, and varied lighting. As a
plug-and-play method, we incorporate the proposed CrackCue into three advanced
crack detection networks. Extensive experimental results demonstrate that the
proposed CrackCue significantly improves the generalization ability and
robustness of the baseline methods. The source code will be publicly available.

</details>


### [34] [CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.16854)
*Xiaoqiang He*

Main category: cs.CV

> The paper presents CLAMP, a new framework for multimodal aspect-based sentiment analysis, which improves cross-modal alignment and reduces noise, outperforming many existing methods in benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the paper is to address the limitations of current multimodal aspect-based sentiment analysis methods, particularly challenges related to cross-modal alignment noise and inconsistent fine-grained representations.

**Method:** The paper introduces CLAMP, which includes three modules: Progressive Attention Fusion network, Multi-task Contrastive Learning, and Adaptive Multi-loss Aggregation. This framework aims to enhance fine-grained alignment and consistency between textual and visual data, mitigating issues such as irrelevant visual noise and gradient interference.

**Result:** Evaluation results on standard public benchmarks indicate that CLAMP outperforms most existing state-of-the-art methods in multimodal aspect-based sentiment analysis.

**Conclusion:** The conclusion of the paper is that CLAMP, an end-to-end framework with adaptive multi-loss and progressive attention fusion, effectively enhances cross-modal representation consistency and mitigates irrelevant visual noise, leading to improved performance in multimodal aspect-based sentiment analysis.

**Abstract:** Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect
terms within paired image-text data and determine their fine grained sentiment
polarities, representing a fundamental task for improving the effectiveness of
applications such as product review systems and public opinion monitoring.
Existing methods face challenges such as cross modal alignment noise and
insufficient consistency in fine-grained representations. While global modality
alignment methods often overlook the connection between aspect terms and their
corresponding local visual regions, bridging the representation gap between
text and images remains a challenge. To address these limitations, this paper
introduces an end to end Contrastive Learning framework with Adaptive
Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed
of three novel modules: Progressive Attention Fusion network, Multi-task
Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive
Attention Fusion network enhances fine-grained alignment between textual
features and image regions via hierarchical, multi-stage cross modal
interactions, effectively suppressing irrelevant visual noise. Secondly,
multi-task contrastive learning combines global modal contrast and local
granularity alignment to enhance cross modal representation consistency.
Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting
mechanism to calibrate loss contributions according to each task's uncertainty,
thereby mitigating gradient interference. Evaluation on standard public
benchmarks demonstrates that CLAMP consistently outperforms the vast majority
of existing state of the art methods.

</details>


### [35] [SIA: Enhancing Safety via Intent Awareness for Vision-Language Models](https://arxiv.org/abs/2507.16856)
*Youngjin Na,Sangheon Jeong,Youngwan Lee*

Main category: cs.CV

> The paper introduces SIA, a dynamic, training-free framework for detecting and mitigating harmful intent in vision-language models by analyzing the implicit intent of image-text inputs.

<details>
  <summary>Details</summary>

**Motivation:** To address the safety risks arising from the interplay between images and text in vision-language models, which cannot be adequately mitigated by previous approaches based on post hoc filtering or static refusal prompts.

**Method:** SIA (Safety via Intent Awareness), a training-free prompt engineering framework that employs a three-stage reasoning process: (1) visual abstraction via captioning, (2) intent inference through few-shot chain-of-thought prompting, and (3) intent-conditioned response refinement.

**Result:** SIA achieves substantial safety improvements on benchmarks like SIUO, MM-SafetyBench, and HoliSafe, while showing a minor reduction in general reasoning accuracy on MMStar compared to prior methods.

**Conclusion:** SIA outperforms prior methods on safety-critical benchmarks, demonstrating the value of intent-aware reasoning in aligning vision-language models with human-centric values, despite a minor reduction in general reasoning accuracy.

**Abstract:** As vision-language models (VLMs) are increasingly deployed in real-world
applications, new safety risks arise from the subtle interplay between images
and text. In particular, seemingly innocuous inputs can combine to reveal
harmful intent, leading to unsafe model responses. Despite increasing attention
to multimodal safety, previous approaches based on post hoc filtering or static
refusal prompts struggle to detect such latent risks, especially when
harmfulness emerges only from the combination of inputs. We propose SIA (Safety
via Intent Awareness), a training-free prompt engineering framework that
proactively detects and mitigates harmful intent in multimodal inputs. SIA
employs a three-stage reasoning process: (1) visual abstraction via captioning,
(2) intent inference through few-shot chain-of-thought prompting, and (3)
intent-conditioned response refinement. Rather than relying on predefined rules
or classifiers, SIA dynamically adapts to the implicit intent inferred from the
image-text pair. Through extensive experiments on safety-critical benchmarks
including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves
substantial safety improvements, outperforming prior methods. Although SIA
shows a minor reduction in general reasoning accuracy on MMStar, the
corresponding safety gains highlight the value of intent-aware reasoning in
aligning VLMs with human-centric values.

</details>


### [36] [Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection](https://arxiv.org/abs/2507.16861)
*Xiang Li*

Main category: cs.CV

> 通过使用2D物体先验进行跨模态特征预对齐，该论文提出的方法在nuScenes数据集上实现了71.5%的mAP和73.6%的NDS，显著提升了3D感知性能。

<details>
  <summary>Details</summary>

**Motivation:** 论文的主要动机是使用2D物体先验来在跨模态特征融合之前预对齐激光雷达和摄像头输出，以解决因为投影误差导致的特征融合问题，这些误差主要出现在物体-背景边界。

**Method:** 论文的方法包括两个组成部分：Prior Guided Depth Calibration (PGDC)用于修正局部对齐中的误差，而Discontinuity Aware Geometric Fusion (DAGF)用于处理经过PGDC修正的结果，以增强边缘清晰度并减少噪声。

**Result:** 该论文提出了一种新的方法来解决在结合激光雷达和摄像头输入生成鸟瞰图（BEV）表示时出现的对齐问题，这些问题通常会导致融合过程中的深度监督不准确。提出的方法包括两步：首先利用2D物体先验来预对齐跨模态特征，解决局部对齐问题；其次通过一种新的几何融合方法解决全局对齐问题，以处理那些经过预对齐的特征，同时增强物体-背景边界上的明显过渡。实验结果表明，该方法在nuScenes验证数据集上达到了最先进的性能，mAP和NDS分别达到了71.5%和73.6%。

**Conclusion:** 这篇论文证明了通过先验引导深度校准和间断敏感几何融合的方法可以改善激光雷达和摄像头特征融合中的对齐问题，在nuScenes数据集上实现了state-of-the-art的性能。

**Abstract:** Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV)
representation is crucial for enhancing 3D perception capabilities of
autonomous vehicles. However, current methods are often affected by
misalignment between camera and LiDAR features. This misalignment leads to
inaccurate depth supervision in camera branch and erroneous fusion during
cross-modal feature aggregation. The root cause of this misalignment lies in
projection errors, stemming from minor extrinsic calibration inaccuracies and
rolling shutter effect of LiDAR during vehicle motion. In this work, our key
insight is that these projection errors are predominantly concentrated at
object-background boundaries, which are readily identified by 2D detectors.
Based on this, our main motivation is to utilize 2D object priors to pre-align
cross-modal features before fusion. To address local misalignment, we propose
Prior Guided Depth Calibration (PGDC), which leverages 2D priors to correct
local misalignment and preserve correct cross-modal feature pairs. To resolve
global misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF)
to process calibrated results from PGDC, suppressing noise and explicitly
enhancing sharp transitions at object-background boundaries. To effectively
utilize these transition-aware depth representations, we incorporate Structural
Guidance Depth Modulator (SGDM), using a gated attention mechanism to
efficiently fuse aligned depth and image features. Our proposed method achieves
state-of-the-art performance on nuScenes validation dataset, with its mAP and
NDS reaching 71.5% and 73.6% respectively.

</details>


### [37] [Pixels, Patterns, but No Poetry: To See The World like Humans](https://arxiv.org/abs/2507.16863)
*Hongcheng Gao,Zihao Huang,Lin Xu,Jingyi Tang,Xinhao Li,Yue Liu,Haoyang Li,Taihang Hu,Minhua Lin,Xinlong Yang,Ge Wu,Balong Bi,Hongyu Chen,Wentao Zhang*

Main category: cs.CV

> 本篇论文提出了Turing Eye Test (TET)，一个新的用于评估多模态语言模型视觉感知能力的基准测试，并揭示了当前模型在处理这些任务上的巨大差距。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在转向多模态大型语言模型的感知能力研究，而不是以往主要关注的推理能力。特别是，研究关注模型是否能像人类一样“看到”世界，并揭示了当前最先进模型在这方面的严重不足。

**Method:** 本论文介绍了Turing Eye Test (TET)，一个旨在评估大型多模态语言模型视觉感知能力的新基准测试。TET包含了四个诊断任务，这些任务涉及的是人类可以直观处理的合成图像。

**Result:** 研究结果揭示，即使是当前最先进的MLLMs也在TET的感知任务上表现出灾难性的失败，这些任务对于人类来说却十分简单。进一步分析显示，对于语言主干训练或在上下文中的学习并不能改善在此任务上的表现。

**Conclusion:** 研究表明，视觉塔的微调能够快速提升在这些任务上的性能，暗示这些任务更多挑战的是视觉塔的泛化能力，而非语言主干的知识和推理能力。这揭示了当前MLLMs与人类感知能力之间的关键差距。

**Abstract:** Achieving human-like perception and reasoning in Multimodal Large Language
Models (MLLMs) remains a central challenge in artificial intelligence. While
recent research has primarily focused on enhancing reasoning capabilities in
MLLMs, a fundamental question persists: Can Multimodal Large Language Models
truly perceive the world as humans do? This paper shifts focus from reasoning
to perception. Rather than constructing benchmarks specifically for reasoning,
we introduce the Turing Eye Test (TET), a challenging perception-oriented
benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on
synthetic images that humans process intuitively. Our findings reveal that
state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks
trivial for humans. Both in-context learning and training on language
backbone-effective for previous benchmarks-fail to improve performance on our
tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting
that our benchmark poses challenges for vision tower generalization rather than
for the knowledge and reasoning capabilities of the language backbone-a key gap
between current MLLMs and human perception. We release a representative subset
of TET tasks in this version, and will introduce more diverse tasks and methods
to enhance visual generalization in future work.

</details>


### [38] [HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting](https://arxiv.org/abs/2507.16873)
*Jeongeun Lee,Youngjae Yu,Dongha Lee*

Main category: cs.CV

> 本文提出了HIPPO-Video数据集和HiPHer方法，利用基于LLM的用户模拟器生成的观看历史预测个性化视频片段的显著性评分，实验结果表明其超越了现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视频数据集通常缺乏个性化，依赖于孤立的视频或简单的文本查询，无法捕捉到用户行为的复杂性。因此，为了应对用户个性化需求，文章提出了HIPPO-Video数据集和HiPHer方法。

**Method:** 利用基于LLM的用户模拟器生成反映多样化用户喜好的现实观看历史，以此创建了一个名为HIPPO-Video的新数据集，数据集中包含2040对（观看历史，显著性评分），涉及20400个视频，覆盖170个语义类别。此外，提出了HiPHer方法，利用这些个性化的观看历史来预测条件偏好下的分段显著性评分。

**Result:** 通过广泛的实验表明，相对于现有的通用和基于查询的方法，该方法表现更优，展示了其在现实场景中高度用户中心的视频高亮处理中的潜力。

**Conclusion:** 该研究展示了一种新的数据集和方法，能够更好地理解和预测用户对视频内容的个性化偏好，从而实现更高效的个性化视频高亮处理。

**Abstract:** The exponential growth of video content has made personalized video
highlighting an essential task, as user preferences are highly variable and
complex. Existing video datasets, however, often lack personalization, relying
on isolated videos or simple text queries that fail to capture the intricacies
of user behavior. In this work, we introduce HIPPO-Video, a novel dataset for
personalized video highlighting, created using an LLM-based user simulator to
generate realistic watch histories reflecting diverse user preferences. The
dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400
videos across 170 semantic categories. To validate our dataset, we propose
HiPHer, a method that leverages these personalized watch histories to predict
preference-conditioned segment-wise saliency scores. Through extensive
experiments, we demonstrate that our method outperforms existing generic and
query-based approaches, showcasing its potential for highly user-centric video
highlighting in real-world scenarios.

</details>


### [39] [ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension](https://arxiv.org/abs/2507.16877)
*Yizhi Hu,Zezhao Tian,Xingqun Qi,Chen Su,Bingkun Yang,Junhui Yin,Muyi Sun,Man Zhang,Zhenan Sun*

Main category: cs.CV

> 为了解决现有方法在处理多实体引用表达式理解中忽略复杂实体间关系的问题，本研究提出了一种新的方法ReMeREC，该方法结合视觉和文本信息进行多实体定位，并建模实体间的关系。实验结果表明该方法具有显著的性能优势。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法在处理单实体定位时忽略了多实体场景中复杂的实体间关系，这对准确性和可靠性造成了限制。此外，缺乏有高质量细粒度的、成对的图像-文本-关系注释的数据集阻碍了进一步的进展。

**Method:** 构建了一个关系感知的多实体引用表达式理解（REC）数据集称为ReMeX，包含了详细的关系和文本注释。提出了一种新的框架ReMeREC，该框架结合视觉和文本线索的同时建模实体间关系。提出了文本自适应多实体感知机（TMP）来从细粒度文本线索中动态推断实体的数量和范围，并引入了实体间关系推理器（EIR）来增强关系推理和全局场景理解。

**Result:** 实验结果显示ReMeREC在多实体定位和关系预测上取得了比现有方法更优越的性能。

**Conclusion:** ReMeREC结合了视觉和文本线索来定位多个实体，同时建模这些实体之间的关系，从而解决了语言中由隐式实体边界引起的语义歧义问题，显著提高了多实体引用表达式理解的性能。

**Abstract:** Referring Expression Comprehension (REC) aims to localize specified entities
or regions in an image based on natural language descriptions. While existing
methods handle single-entity localization, they often ignore complex
inter-entity relationships in multi-entity scenes, limiting their accuracy and
reliability. Additionally, the lack of high-quality datasets with fine-grained,
paired image-text-relation annotations hinders further progress. To address
this challenge, we first construct a relation-aware, multi-entity REC dataset
called ReMeX, which includes detailed relationship and textual annotations. We
then propose ReMeREC, a novel framework that jointly leverages visual and
textual cues to localize multiple entities while modeling their
inter-relations. To address the semantic ambiguity caused by implicit entity
boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron
(TMP), which dynamically infers both the quantity and span of entities from
fine-grained textual cues, producing distinctive representations. Additionally,
our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and
global scene understanding. To further improve language comprehension for
fine-grained prompts, we also construct a small-scale auxiliary dataset,
EntityText, generated using large language models. Experiments on four
benchmark datasets show that ReMeREC achieves state-of-the-art performance in
multi-entity grounding and relation prediction, outperforming existing
approaches by a large margin.

</details>


### [40] [CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos](https://arxiv.org/abs/2507.16878)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang,Wentao Zhang*

Main category: cs.CV

> 本文提出CausalStep作为评估视频中因果推理能力的严格基准测试。

<details>
  <summary>Details</summary>

**Motivation:** 虽然LLMs在文本和图像领域提高了推理能力，但在视频推理上仍然面临巨大挑战。现有的视频基准测试主要评估浅层理解和推理，允许模型利用全局上下文，未能严格评估真正的因果和逐步推理。

**Method:** 本文提出了CausalStep基准测试，该测试专门设计用于评估视频中的显式逐步因果推理能力。它将视频分割成因果相关的单元，并采用严格的问答协议，要求按顺序作答并避免捷径解决方案。每个问题均包含基于错误类型分类精心构建的干扰项，以确保诊断价值。

**Result:** 实验使用了领先的专有和开源模型以及人类基线，揭示了当前模型和人级别的逐步推理能力之间存在显著差距。

**Conclusion:** CausalStep为促进稳健和可解释的视频推理的进步提供了一个严格基准。

**Abstract:** Recent advances in large language models (LLMs) have improved reasoning in
text and image domains, yet achieving robust video reasoning remains a
significant challenge. Existing video benchmarks mainly assess shallow
understanding and reasoning and allow models to exploit global context, failing
to rigorously evaluate true causal and stepwise reasoning. We present
CausalStep, a benchmark designed for explicit stepwise causal reasoning in
videos. CausalStep segments videos into causally linked units and enforces a
strict stepwise question-answer (QA) protocol, requiring sequential answers and
preventing shortcut solutions. Each question includes carefully constructed
distractors based on error type taxonomy to ensure diagnostic value. The
benchmark features 100 videos across six categories and 1,852 multiple-choice
QA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,
enabling precise diagnosis of causal reasoning capabilities. Experiments with
leading proprietary and open-source models, as well as human baselines, reveal
a significant gap between current models and human-level stepwise reasoning.
CausalStep provides a rigorous benchmark to drive progress in robust and
interpretable video reasoning.

</details>


### [41] [Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed](https://arxiv.org/abs/2507.16880)
*Antoni Kowalczuk,Dominik Hintersdorf,Lukas Struppek,Kristian Kersting,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

> 该研究展示了现有的剪枝方法在防止文本到图像扩散模型中的数据复制方面存在的脆弱性，并提出了一种新的对抗微调方法来增强模型的鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在评估剪枝方法在防止数据复制方面的有效性，并挑战记忆局部性的假设。

**Method:** 研究人员分析了剪枝后模型的脆弱性，并提出了一个新的对抗微调方法来增强模型的鲁棒性。

**Result:** 研究发现即使剪枝后，文本嵌入的一些小调整可以重新触发数据复制，并且复制可以从文本嵌入空间的多个位置触发。

**Conclusion:** 现有的缓解策略不足以防止数据复制，需要开发能够真正去除记忆内容的方法，研究提供了一个对抗微调方法作为第一步。

**Abstract:** Text-to-image diffusion models (DMs) have achieved remarkable success in
image generation. However, concerns about data privacy and intellectual
property remain due to their potential to inadvertently memorize and replicate
training data. Recent mitigation efforts have focused on identifying and
pruning weights responsible for triggering replication, based on the assumption
that memorization can be localized. Our research assesses the robustness of
these pruning-based approaches. We demonstrate that even after pruning, minor
adjustments to text embeddings of input prompts are sufficient to re-trigger
data replication, highlighting the fragility of these defenses. Furthermore, we
challenge the fundamental assumption of memorization locality, by showing that
replication can be triggered from diverse locations within the text embedding
space, and follows different paths in the model. Our findings indicate that
existing mitigation strategies are insufficient and underscore the need for
methods that truly remove memorized content, rather than attempting to suppress
its retrieval. As a first step in this direction, we introduce a novel
adversarial fine-tuning method that iteratively searches for replication
triggers and updates the model to increase robustness. Through our research, we
provide fresh insights into the nature of memorization in text-to-image DMs and
a foundation for building more trustworthy and compliant generative AI.

</details>


### [42] [Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning](https://arxiv.org/abs/2507.16886)
*Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

> 本文提出S2S-ST框架，通过稀疏采样空间转录组数据和自然图像共同训练，实现准确的空间转录组插补，从而减轻高分辨率数据成本高昂的问题。

<details>
  <summary>Details</summary>

**Motivation:** 空间转录组学虽然在生物医学研究中发挥了革命性的作用，但高分辨率空间转录组数据的成本高昂且稀缺，成为重要挑战。因此，作者提出S2S-ST框架来解决这一问题。

**Method:** S2S-ST框架，包含三个关键创新：(1) 使用稀疏到稀疏的自监督学习策略，利用空间转录组数据中的内在空间模式；(2) 借助自然图像进行跨域协同学习以增强特征表示；(3) 提出级联数据一致性的插补网络（CDCIN）来迭代精炼预测结果同时保持采样基因数据的准确性。

**Result:** 在多种组织类型上的广泛实验表明，该方法在插补准确性方面优于最先进的方法。通过利用稀疏输入实现稳健的空间转录组重建，显著减少了对高成本高分辨率数据的依赖。

**Conclusion:** S2S-ST框架能够以更低的成本实现高质量的空间转录组重建，促进在生物医学研究和临床应用中的更广泛应用。

**Abstract:** Spatial transcriptomics (ST) has revolutionized biomedical research by
enabling high resolution gene expression profiling within tissues. However, the
high cost and scarcity of high resolution ST data remain significant
challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel
framework for accurate ST imputation that requires only a single and low-cost
sparsely sampled ST dataset alongside widely available natural images for
co-training. Our approach integrates three key innovations: (1) a
sparser-to-sparse self-supervised learning strategy that leverages intrinsic
spatial patterns in ST data, (2) cross-domain co-learning with natural images
to enhance feature representation, and (3) a Cascaded Data Consistent
Imputation Network (CDCIN) that iteratively refines predictions while
preserving sampled gene data fidelity. Extensive experiments on diverse tissue
types, including breast cancer, liver, and lymphoid tissue, demonstrate that
our method outperforms state-of-the-art approaches in imputation accuracy. By
enabling robust ST reconstruction from sparse inputs, our framework
significantly reduces reliance on costly high resolution data, facilitating
potential broader adoption in biomedical research and clinical applications.

</details>


### [43] [AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation](https://arxiv.org/abs/2507.16940)
*Nima Fathi,Amar Kumar,Tal Arbel*

Main category: cs.CV

> AURA是一个专为医学图像分析设计的新型视觉语言解释代理，使用了Qwen-32B大型语言模型架构，并结合了分段、反事实图像生成和评估模块，以适应复杂的临床任务。

<details>
  <summary>Details</summary>

**Motivation:** 当前基于大型语言模型的代理系统在各个领域已经显示出潜力，但在医学成像中的应用仍然有限。AURA旨在填补这一空白，提供透明、灵活且符合临床需求的人工智能系统。

**Method:** 通过使用Qwen-32B大型语言模型架构，AURA结合了一个包括分段套件、反事实图像生成模块和评估工具集的模块化工具箱，以支持对医学图像的综合分析、解释和评估。

**Result:** AURA作为第一个用于医学图像全面分析、解释和评估的视觉语言性解释代理，展示了其在将医学图像分析从静态预测转变为交互式决策支持方面的潜力。

**Conclusion:** AURA代表了更透明、更适应临床需求的AI系统的重要进步，并展示了代理AI在医学图像分析领域从静态预测到交互式决策支持的转变潜力。

**Abstract:** Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm
shift from static prediction systems to agentic AI agents capable of reasoning,
interacting with tools, and adapting to complex tasks. While LLM-based agentic
systems have shown promise across many domains, their application to medical
imaging remains in its infancy. In this work, we introduce AURA, the first
visual linguistic explainability agent designed specifically for comprehensive
analysis, explanation, and evaluation of medical images. By enabling dynamic
interactions, contextual explanations, and hypothesis testing, AURA represents
a significant advancement toward more transparent, adaptable, and clinically
aligned AI systems. We highlight the promise of agentic AI in transforming
medical image analysis from static predictions to interactive decision support.
Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular
toolbox comprising: (i) a segmentation suite with phase grounding, pathology
segmentation, and anatomy segmentation to localize clinically meaningful
regions; (ii) a counterfactual image-generation module that supports reasoning
through image-level explanations; and (iii) a set of evaluation tools including
pixel-wise difference-map analysis, classification, and advanced
state-of-the-art components to assess diagnostic relevance and visual
interpretability.

</details>


### [44] [Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts](https://arxiv.org/abs/2507.16946)
*Chiao-An Yang,Kuan-Chuan Peng,Raymond A. Yeh*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Anomaly detection (AD) identifies the defect regions of a given image. Recent
works have studied AD, focusing on learning AD without abnormal images, with
long-tailed distributed training data, and using a unified model for all
classes. In addition, online AD learning has also been explored. In this work,
we expand in both directions to a realistic setting by considering the novel
task of long-tailed online AD (LTOAD). We first identified that the offline
state-of-the-art LTAD methods cannot be directly applied to the online setting.
Specifically, LTAD is class-aware, requiring class labels that are not
available in the online setting. To address this challenge, we propose a
class-agnostic framework for LTAD and then adapt it to our online learning
setting. Our method outperforms the SOTA baselines in most offline LTAD
settings, including both the industrial manufacturing and the medical domain.
In particular, we observe +4.63% image-AUROC on MVTec even compared to methods
that have access to class labels and the number of classes. In the most
challenging long-tailed online setting, we achieve +0.53% image-AUROC compared
to baselines. Our LTOAD benchmark is released here:
https://doi.org/10.5281/zenodo.16283852 .

</details>


### [45] [Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks](https://arxiv.org/abs/2507.17000)
*Jacob Piland,Chris Sweet,Adam Czajka*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Existing saliency-guided training approaches improve model generalization by
incorporating a loss term that compares the model's class activation map (CAM)
for a sample's true-class ({\it i.e.}, correct-label class) against a human
reference saliency map. However, prior work has ignored the false-class CAM(s),
that is the model's saliency obtained for incorrect-label class. We hypothesize
that in binary tasks the true and false CAMs should diverge on the important
classification features identified by humans (and reflected in human saliency
maps). We use this hypothesis to motivate three new saliency-guided training
methods incorporating both true- and false-class model's CAM into the training
strategy and a novel post-hoc tool for identifying important features. We
evaluate all introduced methods on several diverse binary close-set and
open-set classification tasks, including synthetic face detection, biometric
presentation attack detection, and classification of anomalies in chest X-ray
scans, and find that the proposed methods improve generalization capabilities
of deep learning models over traditional (true-class CAM only) saliency-guided
training approaches. We offer source codes and model weights\footnote{GitHub
repository link removed to preserve anonymity} to support reproducible
research.

</details>


### [46] [Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models](https://arxiv.org/abs/2507.17008)
*Gaston Gustavo Rios,Pedro Dal Bianco,Franco Ronchetti,Facundo Quiroga,Oscar Stanchi,Santiago Ponte Ahón,Waldo Hasperué*

Main category: cs.CV

> 我们使用EfficientNet和两种GAN（ReACGAN与SPADE）生成合成数据来解决手语手势数据集的小规模和不平衡问题，提高了分类准确率并增强了跨数据集的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 大多数手语手势数据集的规模有限且分布不均，这给模型训练带来了挑战。本文探讨了通过生成合成数据来增强训练集的效果，以解决此问题。

**Method:** 我们使用EfficientNet分类器，并结合两种不同的生成对抗网络（GAN）架构来生成合成数据以增强训练集：ReACGAN和SPADE。ReACGAN通过辅助分类器使用标签信息来条件生成过程，而SPADE利用空间自适应归一化来限制手部形状配置。

**Result:** 我们的方法在RWTH数据集上将当前最优准确率提升了5%，并且通过利用基于姿态的生成器模型（训练于庞大的HaGRID数据集）可以跨不同的手语数据集进行泛化。我们无需重新训练生成器即可与单源训练的分类器达到相当的性能。

**Conclusion:** 我们的技术解决了小规模和不平衡数据集的限制，提高了手语手势识别的准确度，并展示了跨数据集的泛化能力。这表明，通过合成数据增强可以有效改善手势识别模型的性能。

**Abstract:** Most sign language handshape datasets are severely limited and unbalanced,
posing significant challenges to effective model training. In this paper, we
explore the effectiveness of augmenting the training data of a handshape
classifier by generating synthetic data. We use an EfficientNet classifier
trained on the RWTH German sign language handshape dataset, which is small and
heavily unbalanced, applying different strategies to combine generated and real
images. We compare two Generative Adversarial Networks (GAN) architectures for
data generation: ReACGAN, which uses label information to condition the data
generation process through an auxiliary classifier, and SPADE, which utilizes
spatially-adaptive normalization to condition the generation on pose
information. ReACGAN allows for the generation of realistic images that align
with specific handshape labels, while SPADE focuses on generating images with
accurate spatial handshape configurations. Our proposed techniques improve the
current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the
limitations of small and unbalanced datasets. Additionally, our method
demonstrates the capability to generalize across different sign language
datasets by leveraging pose-based generation trained on the extensive HaGRID
dataset. We achieve comparable performance to single-source trained classifiers
without the need for retraining the generator.

</details>


### [47] [Transformer Based Building Boundary Reconstruction using Attraction Field Maps](https://arxiv.org/abs/2507.17038)
*Muhammad Kamran,Mohammad Moein Sheikholeslami,Andreas Wichmann,Gunho Sohn*

Main category: cs.CV

> 本文介绍了一种新的深度学习方法——Decoupled-PolyGCN，用于从卫星图像中自动提取建筑物边界，其性能优于现有方法，精度提高了6%的平均精度（AP）和10%的平均召回率（AR）。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在物体检测和表示方面取得了显著进展，但基于结构元素的对象表示仍然是计算机视觉的一个挑战。当前高质量的空间图往往依赖于耗时的手动过程，而本文提出的方法旨在解决这一问题。

**Method:** 本研究提出了一种基于图卷积网络（GCN）的新型深度学习方法，以解决从卫星图像中重建建筑物边界的问题。该方法通过集成多尺度和多分辨率特征，并在网络中嵌入吸引场图，利用几何规则化增强建筑物边界的精度。

**Result:** 实验结果显示，与现有方法相比，Decoupled-PolyGCN在平均精度（AP）和平均召回率（AR）上分别提高了6%和10%。

**Conclusion:** Decoupled-PolyGCN方法在自动建筑物边界提取上展现出优越的精度和规律性，为城市规划、灾害管理和大规模空间分析提供了强有力的支持。

**Abstract:** In recent years, the number of remote satellites orbiting the Earth has grown
significantly, streaming vast amounts of high-resolution visual data to support
diverse applications across civil, public, and military domains. Among these
applications, the generation and updating of spatial maps of the built
environment have become critical due to the extensive coverage and detailed
imagery provided by satellites. However, reconstructing spatial maps from
satellite imagery is a complex computer vision task, requiring the creation of
high-level object representations, such as primitives, to accurately capture
the built environment. While the past decade has witnessed remarkable
advancements in object detection and representation using visual data,
primitives-based object representation remains a persistent challenge in
computer vision. Consequently, high-quality spatial maps often rely on
labor-intensive and manual processes. This paper introduces a novel deep
learning methodology leveraging Graph Convolutional Networks (GCNs) to address
these challenges in building footprint reconstruction. The proposed approach
enhances performance by incorporating geometric regularity into building
boundaries, integrating multi-scale and multi-resolution features, and
embedding Attraction Field Maps into the network. These innovations provide a
scalable and precise solution for automated building footprint extraction from
a single satellite image, paving the way for impactful applications in urban
planning, disaster management, and large-scale spatial analysis. Our model,
Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,
demonstrating its ability to deliver accurate and regularized building
footprints across diverse and challenging scenarios.

</details>


### [48] [Controllable Hybrid Captioner for Improved Long-form Video Understanding](https://arxiv.org/abs/2507.17047)
*Kuleen Sasse,Efsun Sarioglu Kayi,Arun Reddy*

Main category: cs.CV

> 本文通过结合视频字幕生成和视觉语言模型来构建文本记忆，改进了视频内容的理解和查询能力。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于视频数据尤其是长视频的高度密集和高维特性，本文旨在通过文本摘要方式提供一种更紧凑的查询相关视频内容表示方法。此外，文本表示便于现有大型语言模型（LLMs）处理，支持对视频内容进行复杂自然语言查询的推理。

**Method:** 本文提出了一种基于文本的记忆构建方法，结合视频字幕生成器和视觉语言模型（VLM），通过将视频分割成更小的片段以进行时空建模，并使用VLM来丰富静态场景描述。这种方法旨在改进单纯基于短视频片段的活动日志质量。同时，通过微调LaViLa视频字幕生成器，使其能够生成动作和场景描述，从而提高字幕生成流程的效率。

**Result:** 提出的可控制混合字幕生成器能够根据视频中的场景变化及时地交替生成不同类型的描述，提高了视频内容描述的准确性和丰富性，从而增强了视频理解系统的性能。

**Conclusion:** 本文证明了使用改进的视频字幕生成方法结合LLMs可以有效回答有关视频的问题，特别是在增加了静态场景描述后，显著扩展了可以从文本记忆中回答的问题范围。

**Abstract:** Video data, especially long-form video, is extremely dense and
high-dimensional. Text-based summaries of video content offer a way to
represent query-relevant content in a much more compact manner than raw video.
In addition, textual representations are easily ingested by state-of-the-art
large language models (LLMs), which enable reasoning over video content to
answer complex natural language queries. To solve this issue, we rely on the
progressive construction of a text-based memory by a video captioner operating
on shorter chunks of the video, where spatio-temporal modeling is
computationally feasible. We explore ways to improve the quality of the
activity log comprised solely of short video captions. Because the video
captions tend to be focused on human actions, and questions may pertain to
other information in the scene, we seek to enrich the memory with static scene
descriptions using Vision Language Models (VLMs). Our video understanding
system relies on the LaViLa video captioner in combination with a LLM to answer
questions about videos. We first explored different ways of partitioning the
video into meaningful segments such that the textual descriptions more
accurately reflect the structure of the video content. Furthermore, we
incorporated static scene descriptions into the captioning pipeline using LLaVA
VLM, resulting in a more detailed and complete caption log and expanding the
space of questions that are answerable from the textual memory. Finally, we
have successfully fine-tuned the LaViLa video captioner to produce both action
and scene captions, significantly improving the efficiency of the captioning
pipeline compared to using separate captioning models for the two tasks. Our
model, controllable hybrid captioner, can alternate between different types of
captions according to special input tokens that signals scene changes detected
in the video.

</details>


### [49] [Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models](https://arxiv.org/abs/2507.17050)
*Tz-Ying Wu,Tahani Trigui,Sharath Nittur Sridhar,Anand Bodas,Subarna Tripathi*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Result:** {
  "tldr": "VideoNarrator 是一个无需训练的视频描述生成管道，它致力于生成详尽且时间精确的视频描述以准确捕获视频内容。实验表明，它有效减少了描述中所产生的不准确性，并改善了时间上的同步性。", 
  "motivation": "现有的多模态大型语言模型在视频理解方面取得了进展，但在时间对齐的描述生成方面仍然存在困难，并且在不熟悉的场景中容易产生不准确的描述。为了克服这些问题，提出了VideoNarrator。", 
  "method": "VideoNarrator 使用了一个灵活的管道，其中包括现成的多模态语言模型和视觉语言模型，它们可以作为描述生成器、背景提供者或描述验证器发挥作用。", 
  "result": "实验结果表明管道中的组件协同工作有效增强了视频描述的质量和准确性，并且减少了不准确性的产生且提高了同步性。", 
  "conclusion": "该方法不仅提高了视频理解的质量，也促进了下游任务如视频摘要生成和视频问答的发展，且可能在广告和市场营销领域进行扩展。 " 
}

**Conclusion:** 

**Abstract:** In this paper, we introduce VideoNarrator, a novel training-free pipeline
designed to generate dense video captions that offer a structured snapshot of
video content. These captions offer detailed narrations with precise
timestamps, capturing the nuances present in each segment of the video. Despite
advancements in multimodal large language models (MLLMs) for video
comprehension, these models often struggle with temporally aligned narrations
and tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator
addresses these challenges by leveraging a flexible pipeline where
off-the-shelf MLLMs and visual-language models (VLMs) can function as caption
generators, context providers, or caption verifiers. Our experimental results
demonstrate that the synergistic interaction of these components significantly
enhances the quality and accuracy of video narrations, effectively reducing
hallucinations and improving temporal alignment. This structured approach not
only enhances video understanding but also facilitates downstream tasks such as
video summarization and video question answering, and can be potentially
extended for advertising and marketing applications.

</details>


### [50] [Few-Shot Learning in Video and 3D Object Detection: A Survey](https://arxiv.org/abs/2507.17079)
*Md Meftahul Ferdaus,Kendall N. Niles,Joe Tom,Mahdi Abdelguerfi,Elias Ioup*

Main category: cs.CV

> This paper reviews recent advancements in few-shot learning for video and 3D object detection, emphasizing efficient use of limited labeled data.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of high labeling costs by utilizing few-shot learning techniques in video and 3D detection, thereby minimizing the need for extensive annotated data.

**Method:** The paper conducts a survey of recent techniques that leverage few-shot learning for object detection in dynamic scenes and 3D environments.

**Result:** Few-shot learning shows significant potential in reducing the amount of labeled data required for model training, improving efficiency in both video and 3D detection scenarios.

**Conclusion:** Through integrating few-shot learning approaches, it is possible to enhance the real-world applicability of object detection models in various scenarios, particularly where data annotation is laborious or costly.

**Abstract:** Few-shot learning (FSL) enables object detection models to recognize novel
classes given only a few annotated examples, thereby reducing expensive manual
data labeling. This survey examines recent FSL advances for video and 3D object
detection. For video, FSL is especially valuable since annotating objects
across frames is more laborious than for static images. By propagating
information across frames, techniques like tube proposals and temporal matching
networks can detect new classes from a couple examples, efficiently leveraging
spatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces
challenges like sparsity and lack of texture. Solutions integrate FSL with
specialized point cloud networks and losses tailored for class imbalance.
Few-shot 3D detection enables practical autonomous driving deployment by
minimizing costly 3D annotation needs. Core issues in both domains include
balancing generalization and overfitting, integrating prototype matching, and
handling data modality properties. In summary, FSL shows promise for reducing
annotation requirements and enabling real-world video, 3D, and other
applications by efficiently leveraging information across feature, temporal,
and data modalities. By comprehensively surveying recent advancements, this
paper illuminates FSL's potential to minimize supervision needs and enable
deployment across video, 3D, and other real-world applications.

</details>


### [51] [SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction](https://arxiv.org/abs/2507.17083)
*Zaipeng Duan,Chenxu Dang,Xuzhong Hu,Pei An,Junfeng Ding,Jie Zhan,Yunbiao Xu,Jie Ma*

Main category: cs.CV

> This paper introduces SDG-OCC, a multimodal occupancy prediction model that improves 3D occupancy prediction for autonomous driving by integrating depth and semantic information from multimodal data, achieving real-time processing and state-of-the-art performance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this work is to overcome the limitations of existing single-modality occupancy prediction methods, which either lack depth information (camera-based) or struggle with occlusions (LiDAR-based). The goal is to improve the accuracy and robustness of 3D occupancy prediction for autonomous driving scenarios.

**Method:** The proposed method, SDG-OCC, integrates a joint semantic and depth-guided view transformation with a fusion-to-occupancy-driven active distillation. It aims to construct accurate depth distributions and extract rich semantic information from multimodal data to enhance occupancy prediction.

**Result:** The proposed method achieves state-of-the-art performance with real-time processing on the Occ3D-nuScenes dataset and shows comparable performance on the more challenging SurroundOcc-nuScenes dataset, indicating its effectiveness and robustness.

**Conclusion:** The research concludes that the SDG-OCC network effectively enhances 3D occupancy prediction through multimodal data fusion and active distillation, surpassing existing methods' performance and providing a real-time solution. The code will also be released for further research and application.

**Abstract:** Multimodal 3D occupancy prediction has garnered significant attention for its
potential in autonomous driving. However, most existing approaches are
single-modality: camera-based methods lack depth information, while LiDAR-based
methods struggle with occlusions. Current lightweight methods primarily rely on
the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth
estimation and fails to fully exploit the geometric and semantic information of
3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction
network called SDG-OCC, which incorporates a joint semantic and depth-guided
view transformation coupled with a fusion-to-occupancy-driven active
distillation. The enhanced view transformation constructs accurate depth
distributions by integrating pixel semantics and co-point depth through
diffusion and bilinear discretization. The fusion-to-occupancy-driven active
distillation extracts rich semantic information from multimodal data and
selectively transfers knowledge to image features based on LiDAR-identified
regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses
fusion alone, and SDG-KL, which integrates both fusion and distillation for
faster inference. Our method achieves state-of-the-art (SOTA) performance with
real-time processing on the Occ3D-nuScenes dataset and shows comparable
performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating
its effectiveness and robustness. The code will be released at
https://github.com/DzpLab/SDGOCC.

</details>


### [52] [FedVLM: Scalable Personalized Vision-Language Models through Federated Learning](https://arxiv.org/abs/2507.17088)
*Arkajyoti Mitra,Afia Anjum,Paul Agbaje,Mert Pesé,Habeeb Olufowobi*

Main category: cs.CV

> 提出FedVLM和pLoRA新技术以解决视觉语言模型在异构数据和分布式环境中微调的挑战。实验表明新的方法显著提升了模型在非独立同分布环境中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型（VLMs）虽然具有出色的零样本和小样本学习能力，但在大规模微调特别是在分布式环境中面临着挑战。现有的参数高效微调方法如LoRA在处理异构客户端数据时效果不佳，导致泛化能力较差。因此需提出一种新的解决方案来解决这些问题。

**Method:** 我们提出了一种名为FedVLM的联邦LoRA微调框架，该框架允许在保持模型隐私的同时在去中心化的环境中适应视觉语言模型。此外，我们引入了个性化LoRA（pLoRA），以动态适应每个客户端的独特数据分布，从而在保持全局模型聚合的同时提高了本地适应性。

**Result:** 在RLAIF-V数据集上的实验表明，pLoRA将客户端特定性能提升了24.5%，展示了它在非独立同分布环境中的优越适应性。

**Conclusion:** FedVLM提供了一种在联邦设置中微调视觉语言模型的可扩展且高效的解决方案，并推进了分布式学习场景中的个性化适应。

**Abstract:** Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot
learning capabilities, making them essential for several downstream tasks.
However, fine-tuning these models at scale remains challenging, particularly in
federated environments where data is decentralized and non-iid across clients.
Existing parameter-efficient tuning methods like LoRA (Low-Rank Adaptation)
reduce computational overhead but struggle with heterogeneous client data,
leading to suboptimal generalization. To address these challenges, we propose
FedVLM, a federated LoRA fine-tuning framework that enables decentralized
adaptation of VLMs while preserving model privacy and reducing reliance on
centralized training. To further tackle data heterogeneity, we introduce
personalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each
client's unique data distribution, significantly improving local adaptation
while maintaining global model aggregation. Experiments on the RLAIF-V dataset
show that pLoRA improves client-specific performance by 24.5% over standard
LoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a
scalable and efficient solution for fine-tuning VLMs in federated settings,
advancing personalized adaptation in distributed learning scenarios.

</details>


### [53] [IONext: Unlocking the Next Era of Inertial Odometry](https://arxiv.org/abs/2507.17089)
*Shanshan Zhang,Siyue Wang,Tianshui Wen,Qi Zhang,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.CV

> 本文提出了一个名为IONext的CNN基础模型，通过引入DADM和STGU组件，能更好地捕捉全局和局部运动特征，从而改进了惯性里程计的性能，表现出超越现有方法的结果。

<details>
  <summary>Details</summary>

**Motivation:** 尽管Transformer模型在建模长距离依赖方面表现出色，但这类模型对局部细粒度运动变化的敏感性有限，缺乏先天的归纳偏置，从而影响了定位精度和泛化能力。因此，本文旨在通过一个新的CNN模块来克服这些问题。

**Method:** 本文提出了一个新的基于CNN的模块，称为双翼自适应动态混合器（DADM），能够自适应地捕获动态输入中的全局运动模式和局部细粒度的运动特征。该模块能够根据输入动态生成选择性权重，提高多尺度特征聚集的效率。此外，还引入了一个名为时空门控单元（STGU）的组件，改进了时间建模。基于DADM和STGU，构建了一个名为IONext的新CNN惯性里程计骨干模型。

**Result:** 实验结果表明，IONext在六个公共数据集上的性能始终优于最先进的Transformer和CNN方法。例如，在RNIN数据集上，IONext相比代表性模型iMOT将平均ATE减少了10%，平均RTE减少了12%。

**Conclusion:** 实验验证了本文提出的IONext模型在惯性里程计任务上具有卓越的性能，尤其是在局部和全局运动特征的捕捉方面，证明了其作为高级CNN方法的有效性和优越性。

**Abstract:** Researchers have increasingly adopted Transformer-based models for inertial
odometry. While Transformers excel at modeling long-range dependencies, their
limited sensitivity to local, fine-grained motion variations and lack of
inherent inductive biases often hinder localization accuracy and
generalization. Recent studies have shown that incorporating large-kernel
convolutions and Transformer-inspired architectural designs into CNN can
effectively expand the receptive field, thereby improving global motion
perception. Motivated by these insights, we propose a novel CNN-based module
called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures
both global motion patterns and local, fine-grained motion features from
dynamic inputs. This module dynamically generates selective weights based on
the input, enabling efficient multi-scale feature aggregation. To further
improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),
which selectively extracts representative and task-relevant motion features in
the temporal domain. This unit addresses the limitations of temporal modeling
observed in existing CNN approaches. Built upon DADM and STGU, we present a new
CNN-based inertial odometry backbone, named Next Era of Inertial Odometry
(IONext). Extensive experiments on six public datasets demonstrate that IONext
consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based
methods. For instance, on the RNIN dataset, IONext reduces the average ATE by
10% and the average RTE by 12% compared to the representative model iMOT.

</details>


### [54] [Robust Five-Class and binary Diabetic Retinopathy Classification Using Transfer Learning and Data Augmentation](https://arxiv.org/abs/2507.17121)
*Faisal Ahmed,Mohammad Alfrad Nobel Bhuiyan*

Main category: cs.CV

> 论文提出了一种用于DR分类的深度学习框架，利用迁移学习和数据增强技术，实现了在APTOS 2019数据集上的二分类和五分类任务的高准确率，展示了在DR诊断中的实际应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** DR是全球视力丧失的主要原因，通过自动化视网膜图像分析进行早期诊断可以显著降低失明的风险。因此，研究高效准确的DR诊断方法是非常有意义的。

**Method:** 本论文提出了一种基于深度学习的框架，用于糖尿病视网膜病变（DR）的二分类和五分类，采用迁移学习和大量的数据增强来应对类别不平衡和训练数据有限的挑战。详细来说，他们评估了包括ResNet和EfficientNet在内的预训练卷积神经网络架构在APTOS 2019数据集上的表现。

**Result:** 在二分类任务中，该模型达到了98.9%的最高准确率，精确率为98.6%，召回率为99.3%，F1分数为98.9%，AUC为99.4%。在更具有挑战性的五分类严重程度分类任务中，模型获得了84.6%的准确率和94.1%的AUC，优于几种现有的方法。同时发现EfficientNet-B0和ResNet34在精确率和计算效率之间提供了最优的平衡。

**Conclusion:** 结合类别平衡增强和迁移学习的框架，展现了在DR诊断中的高效性能。该框架提供了一种可供扩展且准确的DR筛查方案，有可能在实际临床环境中部署。

**Abstract:** Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and
early diagnosis through automated retinal image analysis can significantly
reduce the risk of blindness. This paper presents a robust deep learning
framework for both binary and five-class DR classification, leveraging transfer
learning and extensive data augmentation to address the challenges of class
imbalance and limited training data. We evaluate a range of pretrained
convolutional neural network architectures, including variants of ResNet and
EfficientNet, on the APTOS 2019 dataset.
  For binary classification, our proposed model achieves a state-of-the-art
accuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of
98.9%, and an AUC of 99.4%. In the more challenging five-class severity
classification task, our model obtains a competitive accuracy of 84.6% and an
AUC of 94.1%, outperforming several existing approaches. Our findings also
demonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between
accuracy and computational efficiency across both tasks.
  These results underscore the effectiveness of combining class-balanced
augmentation with transfer learning for high-performance DR diagnosis. The
proposed framework provides a scalable and accurate solution for DR screening,
with potential for deployment in real-world clinical environments.

</details>


### [55] [ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation](https://arxiv.org/abs/2507.17149)
*Bo Fang,Jianan Fan,Dongnan Liu,Hang Chang,Gerald J. Shami,Filip Braet,Weidong Cai*

Main category: cs.CV

> 提出ScSAM方法以克服亚细胞场景下的学习偏差和细粒度细节丢失，通过融合SAM和MAE知识，在各种亚细胞图像数据集中表现出优于现有最先进方法的性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决亚细胞成分形态和分布的显著变异性所带来的挑战，这种变异性增加了基于学习的细胞器分割模型产生偏差特征学习的风险。

**Method:** 引入了ScSAM方法，通过将预训练的SAM与Masked Autoencoder (MAE)引导的细胞先验知识融合，以减轻由于数据不平衡带来的训练偏差。具体来说，设计了一个特征对齐和融合模块，将预训练嵌入对齐到同一个特征空间，并有效结合不同的表示形式。此外，提出了一种基于余弦相似度矩阵的分类提示编码器，激活类别特定特征以识别亚细胞类别。

**Result:** 在多种亚细胞图像数据集上的广泛实验表明，ScSAM优于现有最先进的方法。

**Conclusion:** ScSAM通过融合SAM和MAE引导的知识，解决了SAM在亚细胞场景应用上的两个关键挑战：特征偏差学习和细粒度空间细节的忽略。这种方法在多个亚细胞图像数据集上表现出色，超越了当前最先进的方法。

**Abstract:** The significant morphological and distributional variability among
subcellular components poses a long-standing challenge for learning-based
organelle segmentation models, significantly increasing the risk of biased
feature learning. Existing methods often rely on single mapping relationships,
overlooking feature diversity and thereby inducing biased training. Although
the Segment Anything Model (SAM) provides rich feature representations, its
application to subcellular scenarios is hindered by two key challenges: (1) The
variability in subcellular morphology and distribution creates gaps in the
label space, leading the model to learn spurious or biased features. (2) SAM
focuses on global contextual understanding and often ignores fine-grained
spatial details, making it challenging to capture subtle structural alterations
and cope with skewed data distributions. To address these challenges, we
introduce ScSAM, a method that enhances feature robustness by fusing
pre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge
to alleviate training bias from data imbalance. Specifically, we design a
feature alignment and fusion module to align pre-trained embeddings to the same
feature space and efficiently combine different representations. Moreover, we
present a cosine similarity matrix-based class prompt encoder to activate
class-specific features to recognize subcellular categories. Extensive
experiments on diverse subcellular image datasets demonstrate that ScSAM
outperforms state-of-the-art methods.

</details>


### [56] [UNICE: Training A Universal Image Contrast Enhancer](https://arxiv.org/abs/2507.17157)
*Ruodai Cui,Lei Zhang*

Main category: cs.CV

> UNICE方法利用HDR图像生成多曝光序列并融合，实现了高效的图像对比度增强，展现出优于特定任务模型的泛化性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的图像对比度增强方法通常为特定任务设计，模型在不同任务或同一任务的不同数据集上的泛化能力较弱。研究的目的在于探索是否可以学习到一种普遍适用和富有泛化能力的对比度增强模型。

**Method:** 我们提出了名为UNICE的方法，通过收集46,928张HDR原始图像，并渲染出328,496张sRGB图像来构建多曝光序列（MES）以及相应的伪sRGB真实值，以此训练网络从一张sRGB图像生成MES，再融合成增强图像。

**Result:** 所提出的方法在不同的对比度增强任务中展示了比现有方法更强的泛化性能，甚至在无参考的图像质量指标中超过了手动创建的真实值。

**Conclusion:** UNICE方法证明了通过学习一种普遍的对比度增强模型能够在不同任务间提供显著的性能提升，而无需耗费人力标注。

**Abstract:** Existing image contrast enhancement methods are typically designed for
specific tasks such as under-/over-exposure correction, low-light and backlit
image enhancement, etc. The learned models, however, exhibit poor
generalization performance across different tasks, even across different
datasets of a specific task. It is important to explore whether we can learn a
universal and generalized model for various contrast enhancement tasks. In this
work, we observe that the common key factor of these tasks lies in the need of
exposure and contrast adjustment, which can be well-addressed if high-dynamic
range (HDR) inputs are available. We hence collect 46,928 HDR raw images from
public sources, and render 328,496 sRGB images to build multi-exposure
sequences (MES) and the corresponding pseudo sRGB ground-truths via
multi-exposure fusion. Consequently, we train a network to generate an MES from
a single sRGB image, followed by training another network to fuse the generated
MES into an enhanced image. Our proposed method, namely UNiversal Image
Contrast Enhancer (UNICE), is free of costly human labeling. However, it
demonstrates significantly stronger generalization performance than existing
image contrast enhancement methods across and within different tasks, even
outperforming manually created ground-truths in multiple no-reference image
quality metrics. The dataset, code and model are available at
https://github.com/BeyondHeaven/UNICE.

</details>


### [57] [DOOMGAN:High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing](https://arxiv.org/abs/2507.17158)
*Bharath Krishnamurthy,Ajita Rattani*

Main category: cs.CV

> 本文针对可见光谱下的眼部生物识别中的形态攻击问题，提出DOOMGAN模型，并发布了首个眼部形态数据集。

<details>
  <summary>Details</summary>

**Motivation:** 形态攻击对生物识别系统的完整性构成威胁，尤其是对于可见光谱下的眼部生物特征，这一领域尚未得到充分研究。

**Method:** 我们提出了DOOMGAN模型，该模型通过使用基于地标点的可见光眼部解剖结构编码、注意力引导的生成方法以合成真实的形态特征，并采用多方面损失的动态加权优化收敛。

**Result:** DOOMGAN在严格的阈值条件下，比基准方法的攻击成功率高出20%以上，椭圆形虹膜结构生成更优（提高了20%），以及更好的凝视一致性（提高了30%）。

**Conclusion:** 通过引入DOOMGAN和首个全面的眼部形态数据集，本研究填补了可见光谱眼部生物识别领域形态攻击研究的空白。

**Abstract:** Ocular biometrics in the visible spectrum have emerged as a prominent
modality due to their high accuracy, resistance to spoofing, and non-invasive
nature. However, morphing attacks, synthetic biometric traits created by
blending features from multiple individuals, threaten biometric system
integrity. While extensively studied for near-infrared iris and face
biometrics, morphing in visible-spectrum ocular data remains underexplored.
Simulating such attacks demands advanced generation models that handle
uncontrolled conditions while preserving detailed ocular features like iris
boundaries and periocular textures. To address this gap, we introduce DOOMGAN,
that encompasses landmark-driven encoding of visible ocular anatomy,
attention-guided generation for realistic morph synthesis, and dynamic
weighting of multi-faceted losses for optimized convergence. DOOMGAN achieves
over 20% higher attack success rates than baseline methods under stringent
thresholds, along with 20% better elliptical iris structure generation and 30%
improved gaze consistency. We also release the first comprehensive ocular
morphing dataset to support further research in this domain.

</details>


### [58] [Multi-Scale PCB Defect Detection with YOLOv8 Network Improved via Pruning and Lightweight Network](https://arxiv.org/abs/2507.17176)
*Li Pingzhen,Xu Sheng,Chen Jing,Su Chengyue*

Main category: cs.CV

> 本文提出了一种基于YOLOv8的改进多尺度PCB缺陷检测方法，通过优化网络结构和损失函数，实现了在高精度检测方面的提升，特别是在检测小缺陷方面具有明显优势。

<details>
  <summary>Details</summary>

**Motivation:** 传统的PCB缺陷检测模型难以兼顾准确性和计算成本，在高密度、高速生产的要求下无法满足高精度实时检测小缺陷的需求。

**Method:** 提出了一种基于YOLOv8的多尺度PCB缺陷检测方法，采用了小目标敏感策略、网络轻量化和自适应剪枝的综合策略，优化了骨干网络、颈部网络和检测头、损失函数以及自适应剪枝率。具体来说，在骨干网络中使用了参数较少的Ghost-HGNetv2结构，利用多级特征提取图像语义特征；在颈部部分集成了参数较少的C2f-Faster，增强多级特征融合；在检测头部分，设计了一种新的GCDetect检测头，利用组卷积共享权重，减少了参数量，同时保留了检测准确性，设计了内部-MPDIoU边界损失函数以改善小目标的检测和定位；最后通过优化的自适应剪枝率进一步减少了模型复杂度。

**Result:** 实验结果表明，该模型在精确度和速度方面表现出优势，在公开的PCB缺陷数据集上，mAP0.5达到了99.32%，mAP0.5:0.9达到了75.18%，比YOLOv8n高出10.13%。

**Conclusion:** 改进的多尺度PCB缺陷检测方法通过优化网络架构和损失函数设计，在保持高检测精度的同时，提高了检测速度，并成功应用于实际的PCB缺陷检测任务中。

**Abstract:** With the high density of printed circuit board (PCB) design and the high
speed of production, the traditional PCB defect detection model is difficult to
take into account the accuracy and computational cost, and cannot meet the
requirements of high accuracy and real-time detection of tiny defects.
Therefore, in this paper, a multi-scale PCB defect detection method is improved
with YOLOv8 using a comprehensive strategy of tiny target sensitivity strategy,
network lightweighting and adaptive pruning, which is able to improve the
detection speed and accuracy by optimizing the backbone network, the neck
network and the detection head, the loss function and the adaptive pruning
rate. Firstly, a Ghost-HGNetv2 structure with fewer parameters is used in the
backbone network, and multilevel features are used to extract image semantic
features to discover accurate defects. Secondly, we integrate C2f-Faster with
small number of parameters in the neck section to enhance the ability of
multi-level feature fusion. Next, in the Head part, we design a new GCDetect
detection head, which allows the prediction of bounding boxes and categories to
share the weights of GroupConv, and uses a small number of grouping
convolutions to accomplish the regression and classification tasks, which
significantly reduces the number of parameters while maintaining the accuracy
of detection. We also design the Inner-MPDIoU boundary loss function to improve
the detection and localization of tiny targets. Finally, the model was pruned
by an optimized adaptive pruning rate to further reduce the complexity of the
model. Experimental results show that the model exhibits advantages in terms of
accuracy and speed. On the publicly available PCB defect dataset, mAP0.5
reaches 99.32% and mAP0.5:0.9 reaches 75.18%, which is 10.13% higher compared
to YOLOv8n.

</details>


### [59] [Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment](https://arxiv.org/abs/2507.17182)
*Linghe Meng,Jiarun Song*

Main category: cs.CV

> 论文提出了一个针对AI生成内容质量评估的多级视觉表示范式，开发了两种新网络，并在实验证明其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法通常依赖单级视觉特征，不能充分捕捉AIGC图像中的复杂失真。为解决这种局限性，提出了多级视觉表示范式。

**Method:** 提出了一种多级视觉表示范式，包含三个阶段：多级特征提取、层级融合和联合聚合。基于此范式，开发了两种网络：一种为多级全局局部融合网络（MGLF-Net），用于感知质量评估；另一种为多级提示嵌入融合网络（MPEF-Net），用于文本到图像的对应评估。该范式在实验中展示了出色的表现。

**Result:** 在基准测试上的实验表明，所开发的网络在感知质量和文本到图像的对应任务中均表现出色。

**Conclusion:** 实验证明该多级视觉评估范式在感知质量和文本到图像对应评定任务中都表现出色，证实了其有效性。

**Abstract:** The quality assessment of AI-generated content (AIGC) faces multi-dimensional
challenges, that span from low-level visual perception to high-level semantic
understanding. Existing methods generally rely on single-level visual features,
limiting their ability to capture complex distortions in AIGC images. To
address this limitation, a multi-level visual representation paradigm is
proposed with three stages, namely multi-level feature extraction, hierarchical
fusion, and joint aggregation. Based on this paradigm, two networks are
developed. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net)
is designed for the perceptual quality assessment, extracting complementary
local and global features via dual CNN and Transformer visual backbones. The
Multi-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image
correspondence by embedding prompt semantics into the visual feature fusion
process at each feature level. The fused multi-level features are then
aggregated for final evaluation. Experiments on benchmarks demonstrate
outstanding performance on both tasks, validating the effectiveness of the
proposed multi-level visual assessment paradigm.

</details>


### [60] [TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition](https://arxiv.org/abs/2507.17335)
*Guangzhu Xu,Zhi Ke,Pengcheng Zuo,Bangjun Lei*

Main category: cs.CV

> 本文提出了一种结合轻量级视觉编码器和文本解码器的方法来解决开放环境下车牌识别的挑战，实现了在多种条件下的高精度识别，并提高了处理速度。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决当前CNN和CRNN方法在车牌识别中遇到的局限性，特别是在开放环境下的多类型车牌和成像条件所带来的挑战。

**Method:** 本文提出了一种结合轻量级视觉编码器和文本解码器的统一解决方案，并在单行和双行中文车牌识别上进行了预训练。为了解决双行车牌数据集稀缺的问题，本文通过合成图像、纹理映射到真实场景并融合真实车牌图像的方式构建了一个单行/双行车牌数据集。此外，为了提高系统的识别精度，本文还引入了一个视角校正网络(PTN)，该网络利用车牌角点坐标回归作为隐变量，并使用车牌视角分类信息进行监督。

**Result:** 所提出的算法在粗略定位扰动下的CCPD测试集上达到了99.34%的平均识别精度，在精细定位扰动下进一步提升到了99.58%。在双行车牌测试集上，也达到了98.70%的平均识别精度，处理速度达到了每秒167帧。

**Conclusion:** 提出的解决方案不仅提高了车牌识别的稳定性、可解释性和降低了标注成本，而且在实际应用中表现出了强大的适用性。

**Abstract:** License plate recognition in open environments is widely applicable across
various domains; however, the diversity of license plate types and imaging
conditions presents significant challenges. To address the limitations
encountered by CNN and CRNN-based approaches in license plate recognition, this
paper proposes a unified solution that integrates a lightweight visual encoder
with a text decoder, within a pre-training framework tailored for single and
double-line Chinese license plates. To mitigate the scarcity of double-line
license plate datasets, we constructed a single/double-line license plate
dataset by synthesizing images, applying texture mapping onto real scenes, and
blending them with authentic license plate images. Furthermore, to enhance the
system's recognition accuracy, we introduce a perspective correction network
(PTN) that employs license plate corner coordinate regression as an implicit
variable, supervised by license plate view classification information. This
network offers improved stability, interpretability, and low annotation costs.
The proposed algorithm achieves an average recognition accuracy of 99.34% on
the corrected CCPD test set under coarse localization disturbance. When
evaluated under fine localization disturbance, the accuracy further improves to
99.58%. On the double-line license plate test set, it achieves an average
recognition accuracy of 98.70%, with processing speeds reaching up to 167
frames per second, indicating strong practical applicability.

</details>


### [61] [Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification](https://arxiv.org/abs/2507.17185)
*M. A. Rasel,Sameem Abdul Kareem,Zhenli Kwan,Nik Aimee Azizah Faheem,Winn Hui Han,Rebecca Kai Jan Choong,Shin Shen Yong,Unaizah Obaidellah*

Main category: cs.CV

> 该研究利用监督学习和CNN技术对皮肤病变的形状特征进行分类分析，通过几何方法和CNN方法分别达到很高的检测率和分类准确率。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是帮助非专家理解皮肤病变诊断中的重要标准——病变形状的不对称性，并且在分类性能上超越现有的方法。

**Method:** 研究的第一步是标注皮肤镜图像数据中的对称信息；然后提出了一个支持性技术，即监督学习图像处理算法，用于分析病变形状；最后利用预训练的CNN提取特征并用SVM分类器进行多类分类。

**Result:** 该研究基于皮肤镜图像，通过标注数据集中的对称性信息，并提出一种监督学习图像处理算法来分析病变形状的几何模式，辅助非专家理解不对称病变的诊断标准。同时，利用预训练的卷积神经网络(CNN)提取形状、颜色和纹理特征，训练多类支持向量机(SVM)分类器对皮肤病变形状进行分类。几何方法的不对称病变检测率为99.00%，CNN方法的最佳性能指标为94% Kappa值，95% Macro F1得分和97% Weighted F1得分。

**Conclusion:** 研究证实了提出的几何分析方法和基于CNN的多类分类器在皮肤病变形状分类中的高性能，为非专家理解和诊断提供了强有力的支持。

**Abstract:** In dermoscopic images, which allow visualization of surface skin structures
not visible to the naked eye, lesion shape offers vital insights into skin
diseases. In clinically practiced methods, asymmetric lesion shape is one of
the criteria for diagnosing melanoma. Initially, we labeled data for a
non-annotated dataset with symmetrical information based on clinical
assessments. Subsequently, we propose a supporting technique, a supervised
learning image processing algorithm, to analyze the geometrical pattern of
lesion shape, aiding non-experts in understanding the criteria of an asymmetric
lesion. We then utilize a pre-trained convolutional neural network (CNN) to
extract shape, color, and texture features from dermoscopic images for training
a multiclass support vector machine (SVM) classifier, outperforming
state-of-the-art methods from the literature. In the geometry-based experiment,
we achieved a 99.00% detection rate for dermatological asymmetric lesions. In
the CNN-based experiment, the best performance is found with 94% Kappa Score,
95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes
(Asymmetric, Half-Symmetric, and Symmetric).

</details>


### [62] [URPO: A Unified Reward & Policy Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.17515)
*Songshuo Lu,Hua Wang,Zhi Chen,Yaohua Tang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large-scale alignment pipelines typically pair a policy model with a
separately trained reward model whose parameters remain frozen during
reinforcement learning (RL). This separation creates a complex,
resource-intensive pipeline and suffers from a performance ceiling due to a
static reward signal. We propose a novel framework, Unified Reward & Policy
Optimization (URPO), that unifies instruction-following ("player") and reward
modeling ("referee") within a single model and a single training phase. Our
method recasts all alignment data-including preference pairs, verifiable
reasoning, and open-ended instructions-into a unified generative format
optimized by a single Group-Relative Policy Optimization (GRPO) loop. This
enables the model to learn from ground-truth preferences and verifiable logic
while simultaneously generating its own rewards for open-ended tasks.
Experiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified
model significantly outperforms a strong baseline using a separate generative
reward model, boosting the instruction-following score on AlpacaEval from 42.24
to 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,
URPO cultivates a superior internal evaluator as a byproduct of training,
achieving a RewardBench score of 85.15 and surpassing the dedicated reward
model it replaces (83.55). By eliminating the need for a separate reward model
and fostering a co-evolutionary dynamic between generation and evaluation, URPO
presents a simpler, more efficient, and more effective path towards robustly
aligned language models.

</details>


### [63] [Vec2Face+ for Face Dataset Generation](https://arxiv.org/abs/2507.17192)
*Haiyu Wu,Jaskirat Singh,Sicong Tian,Liang Zheng,Kevin W. Bowyer*

Main category: cs.CV

> 提出Vec2Face+模型，通过三个策略生成具有适当类内一致性和类间可分性的高质量人脸合成数据集，并验证了合成数据集在人脸识别中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法在增加类内变异时忽视维护类内身份一致性的问题，生成高质量的人脸训练数据。

**Method:** Vec2Face+模型直接从图像特征生成图像，使用三种策略来维持适当的身份一致性和类间可分性以及增加类内变异。

**Result:** 生成的VFace10K、VFace100K和VFace300K数据集在多个真实测试集上取得了更好的结果，证明了合成数据集的有效性。

**Conclusion:** 首次合成的数据集在平均准确率上超过了真实数据集CASIA-WebFace，但也指出合成身份训练的模型可能更加偏见。

**Abstract:** When synthesizing identities as face recognition training data, it is
generally believed that large inter-class separability and intra-class
attribute variation are essential for synthesizing a quality dataset. % This
belief is generally correct, and this is what we aim for. However, when
increasing intra-class variation, existing methods overlook the necessity of
maintaining intra-class identity consistency. % To address this and generate
high-quality face training data, we propose Vec2Face+, a generative model that
creates images directly from image features and allows for continuous and easy
control of face identities and attributes. Using Vec2Face+, we obtain datasets
with proper inter-class separability and intra-class variation and identity
consistency using three strategies: 1) we sample vectors sufficiently different
from others to generate well-separated identities; 2) we propose an AttrOP
algorithm for increasing general attribute variations; 3) we propose LoRA-based
pose control for generating images with profile head poses, which is more
efficient and identity-preserving than AttrOP. % Our system generates VFace10K,
a synthetic face dataset with 10K identities, which allows an FR model to
achieve state-of-the-art accuracy on seven real-world test sets. Scaling the
size to 4M and 12M images, the corresponding VFace100K and VFace300K datasets
yield higher accuracy than the real-world training dataset, CASIA-WebFace, on
five real-world test sets. This is the first time a synthetic dataset beats the
CASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11
synthetic datasets outperforms random guessing (\emph{i.e., 50\%}) in twin
verification and that models trained with synthetic identities are more biased
than those trained with real identities. Both are important aspects for future
investigation.

</details>


### [64] [Dual-branch Prompting for Multimodal Machine Translation](https://arxiv.org/abs/2507.17588)
*Jie Wang,Zhendong Yang,Liansong Zong,Xiaobo Zhang,Dexian Wang,Ji Zhang*

Main category: cs.CV

> 提出D2P-MMT，一种基于扩散的双分支提示框架，用于提升视觉引导翻译的稳健性和性能

<details>
  <summary>Details</summary>

**Motivation:** 解决现有的多模态机器翻译方法对无关视觉噪音敏感和对齐图片文本输入依赖性问题，提升多模态机器翻译的鲁棒性和实际应用性

**Method:** D2P-MMT, 一个基于扩散的双分支提示框架，旨在实现稳健的视觉引导翻译。该框架仅需源文本和由预训练扩散模型生成的重建图像即可工作，能够自然过滤掉分散注意力的视觉细节同时保留语义线索

**Result:** 在Multi30K数据集上的实验表明，D2P-MMT相比于现有的最先进技术实现了更好的翻译性能

**Conclusion:** 研究展示了D2P-MMT在保持翻译质量的同时减少了对无关视觉信息的依赖，证明其在多模态翻译中的有效性和应用潜力

**Abstract:** Multimodal Machine Translation (MMT) typically enhances text-only translation
by incorporating aligned visual features. Despite the remarkable progress,
state-of-the-art MMT approaches often rely on paired image-text inputs at
inference and are sensitive to irrelevant visual noise, which limits their
robustness and practical applicability. To address these issues, we propose
D2P-MMT, a diffusion-based dual-branch prompting framework for robust
vision-guided translation. Specifically, D2P-MMT requires only the source text
and a reconstructed image generated by a pre-trained diffusion model, which
naturally filters out distracting visual details while preserving semantic
cues. During training, the model jointly learns from both authentic and
reconstructed images using a dual-branch prompting strategy, encouraging rich
cross-modal interactions. To bridge the modality gap and mitigate
training-inference discrepancies, we introduce a distributional alignment loss
that enforces consistency between the output distributions of the two branches.
Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves
superior translation performance compared to existing state-of-the-art
approaches.

</details>


### [65] [DesignLab: Designing Slides Through Iterative Detection and Correction](https://arxiv.org/abs/2507.17202)
*Jooyeol Yun,Heng Wang,Yotaro Shimose,Jaegul Choo,Shingo Takamatsu*

Main category: cs.CV

> 我们提出了一种名为 DesignLab 的方法，通过将设计过程分解为设计审查者和设计贡献者两个角色，以实现设计的迭代优化，这种方法优于现有的设计生成方法。

<details>
  <summary>Details</summary>

**Motivation:** 针对非设计专家在设计高质量演示文稿时面临的复杂设计选择，以及现有自动化工具无法有效优化其输出的问题，我们提出了一种新的解决方案。

**Method:** 我们将设计过程分解为设计审查者和设计贡献者两个角色，设计审查者识别设计相关问题，设计贡献者修正这些问题。我们对大型语言模型进行微调以实现这两个角色，并通过引入控制扰动来模拟中间草稿，使设计审查者学习设计错误，设计贡献者学习如何修复这些错误。

**Result:** 我们的实验表明，DesignLab 在生成设计方面优于现有方法，包括商业工具，这得益于其采纳设计的迭代性质，能够生成精致、专业的幻灯片。

**Conclusion:** 我们得出结论，通过将设计过程分解成两个角色并通过迭代解决设计问题，DesignLab 能生成更高质量的幻灯片，比现有工具更出色。

**Abstract:** Designing high-quality presentation slides can be challenging for non-experts
due to the complexity involved in navigating various design choices. Numerous
automated tools can suggest layouts and color schemes, yet often lack the
ability to refine their own output, which is a key aspect in real-world
workflows. We propose DesignLab, which separates the design process into two
roles, the design reviewer, who identifies design-related issues, and the
design contributor who corrects them. This decomposition enables an iterative
loop where the reviewer continuously detects issues and the contributor
corrects them, allowing a draft to be further polished with each iteration,
reaching qualities that were unattainable. We fine-tune large language models
for these roles and simulate intermediate drafts by introducing controlled
perturbations, enabling the design reviewer learn design errors and the
contributor learn how to fix them. Our experiments show that DesignLab
outperforms existing design-generation methods, including a commercial tool, by
embracing the iterative nature of designing which can result in polished,
professional slides.

</details>


### [66] [VBCD: A Voxel-Based Framework for Personalized Dental Crown Design](https://arxiv.org/abs/2507.17205)
*Linda Wei,Chang Liu,Wenran Zhang,Zengji Zhang,Shaoting Zhang,Hongsheng Li*

Main category: cs.CV

> 本文提出了一种基于体素的全自动牙冠设计框架VBCD，该框架通过体素化的口腔扫描并利用距离感知监督和位置提示，提升了牙冠设计的准确性和效率，并且在大规模数据集上的表现优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的牙冠设计流程对于牙科技术人员来说既费时又费力。本研究旨在通过自动化设计来解决这一问题，以提高设计效率和准确性。

**Method:** 我们提出了一种基于体素的全自动牙冠设计框架VBCD，该框架通过体素化的口腔扫描生成粗略的牙冠，然后使用带有距离感知监督的细化器来提高精确度和质量。在训练阶段，使用曲率和边界线罚则损失（CMPL）来增强生成牙冠与边界线的对齐，并引入了基于FDI牙编号系统的定位提示以进一步提高生成牙冠的准确度。

**Result:** 在大规模口腔扫描数据集上的评估表明，我们的方法优于现有的方法，为个性化的牙冠设计提供了强大的解决方案。

**Conclusion:** 我们的方法通过综合使用VBCD框架、基于距离感知的细化技术和定位提示，显著提高了牙冠设计的准确性和效率，展示了一个全新的牙冠设计途径。

**Abstract:** The design of restorative dental crowns from intraoral scans is
labor-intensive for dental technicians. To address this challenge, we propose a
novel voxel-based framework for automated dental crown design (VBCD). The VBCD
framework generates an initial coarse dental crown from voxelized intraoral
scans, followed by a fine-grained refiner incorporating distance-aware
supervision to improve accuracy and quality. During the training stage, we
employ the Curvature and Margin line Penalty Loss (CMPL) to enhance the
alignment of the generated crown with the margin line. Additionally, a
positional prompt based on the FDI tooth numbering system is introduced to
further improve the accuracy of the generated dental crowns. Evaluation on a
large-scale dataset of intraoral scans demonstrated that our approach
outperforms existing methods, providing a robust solution for personalized
dental crown design.

</details>
