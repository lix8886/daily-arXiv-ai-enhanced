<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 20]
- [cs.CV](#cs.CV) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering](https://arxiv.org/abs/2511.17559)
*Gyubok Lee,Woosog Chay,Edward Choi*

Main category: cs.CL

> 本文引入了SCARE，一个评估EHR问答系统中生成的SQL查询的独立后验证机制的基准。SCARE通过分类问题的可回答性并验证或纠正候选SQL查询来评估这些机制的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决在临床环境中部署文本到SQL模型的安全问题，文章提出了一个统一的基准SCARE，用于评估后验证机制的有效性。

**Method:** SCARE包含了4,200个问题-候选SQL查询-预期模型输出的组合，并基于MIMIC-III、MIMIC-IV和eICU数据库进行评估。涵盖了七种不同的文本到SQL模型生成的问题和SQL查询。

**Result:** 实验表明，在问题分类和SQL错误纠正之间存在显著的权衡，强调了未来研究的关键挑战。

**Conclusion:** SCARE为评估EHR问答系统中的后验证机制提供了一个新颖且重要的基准，揭示了该领域的当前不足并设定了未来研究的方向。

**Abstract:** Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.

</details>


### [2] [$A^3$: Attention-Aware Accurate KV Cache Fusion for Fast Large Language Model Serving](https://arxiv.org/abs/2511.17560)
*Yuechi Zhou,Yi Su,Jianxin Zhang,Juntao Li,Qingrong Xia,Zhefeng Wang,Xinyu Duan,Baoxing Huai*

Main category: cs.CL

> Presents A^3, a novel KV Cache integration method, for large language models, decreasing decoding latency and memory overhead significantly.

<details>
  <summary>Details</summary>

**Motivation:** To reduce decoding latency and memory overhead in large language models which handle long sequences efficiently but suffer in real-world deployments due to high computational costs despite recent advances in KV Cache reuse methods.

**Method:** Attention-Aware Accurate KV Cache Fusion algorithm (A^3), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question.

**Result:** Achieved the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2x.

**Conclusion:** The A^3 method is effective in improving the efficiency and performance of large language models in dealing with long contexts, providing significant speedup with minimal computational overhead.

**Abstract:** Large language models (LLMs) have demonstrated strong capabilities in processing long contexts, enabling them to tackle tasks involving long textual inputs such as multi-turn conversations, legal documents, or retrieved documents in Retrieval-Augmented Generation (RAG) systems. However, despite their ability to handle long sequences, the resulting decoding latency and memory overhead remain substantial, posing challenges for real-world deployment. Recent advances in KV Cache reuse have shown potential to mitigate these costs, but still suffer from notable performance degradation. To address this issue, we conduct an in-depth investigation of recomputation-based reuse methods and observe that the recomputed tokens often fail to align with the context segments most relevant to the question. This misalignment hinders proper updates to the critical contextual representations. Therefore, we propose the $\textbf{A}$ttention-$\textbf{A}$ware $\textbf{A}$ccurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question, achieving accurate integration with minimal computational overhead. Extensive experiments on various benchmarks and LLMs demonstrate that $A^3$ achieves the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2$\times$.

</details>


### [3] [LexInstructEval: Lexical Instruction Following Evaluation for Large Language Models](https://arxiv.org/abs/2511.17561)
*Huimin Ren,Yan Liang,Baiqiao Su,Chaobo Sun,Hengtong Lu,Kaike Zhang,Chen Wei*

Main category: cs.CL

> 介绍LexInstructEval，一个用于评估大型语言模型遵循精细语言指令能力的新框架，通过规则语法和多阶段人工参与生成数据集，并进行透明验证。

<details>
  <summary>Details</summary>

**Motivation:** 当前评估LLMs遵循复杂精细语言指令能力的方法存在主观性和成本高昂的问题，或者存在固有偏见和不可靠性的自动系统，现有程序化基准测试也缺乏测试精细组合约束的能力。

**Method:** 提出了LexInstructEval，这是一个新的评估框架，基于形式化的规则语法，将复杂指令分解为<Procedure, Relation, Value>三元组，通过多阶段人工参与的流程系统生成多样化数据集，并通过透明的程序引擎进行客观验证。

**Result:** 该框架通过系统生成多样化数据集并通过透明的程序引擎进行验证，解决了现有方法中的问题。

**Conclusion:** 该框架和数据集的发布有助于进一步研究LLMs的可控性和可靠性。

**Abstract:** The ability of Large Language Models (LLMs) to precisely follow complex and fine-grained lexical instructions is a cornerstone of their utility and controllability. However, evaluating this capability remains a significant challenge. Current methods either rely on subjective and costly human evaluation or on automated LLM-as-a-judge systems, which suffer from inherent biases and unreliability. Existing programmatic benchmarks, while objective, often lack the expressiveness to test intricate, compositional constraints at a granular level. To address these limitations, we introduce LexInstructEval, a new benchmark and evaluation framework for fine-grained lexical instruction following. Our framework is built upon a formal, rule-based grammar that deconstructs complex instructions into a canonical <Procedure, Relation, Value> triplet. This grammar enables the systematic generation of a diverse dataset through a multi-stage, human-in-the-loop pipeline and facilitates objective verification via a transparent, programmatic engine. We release our dataset and open-source evaluation tools to facilitate further research into the controllability and reliability of LLMs.

</details>


### [4] [ChineseErrorCorrector3-4B: State-of-the-Art Chinese Spelling and Grammar Corrector](https://arxiv.org/abs/2511.17562)
*Wei Tian,YuhaoZhou*

Main category: cs.CL

> ChineseErrorCorrector3-4B, 基于Qwen3-4B的统一模型，用于中文拼写和语法错误纠正，在多个权威基准数据集上取得最高F1和F0.5评分。

<details>
  <summary>Details</summary>

**Motivation:** 开发一个能够同时处理中文拼写和语法错误纠正的高效模型，以改进现有模型的不足。

**Method:** 使用Qwen3-4B为基础，构建ChineseErrorCorrector3-4B模型来实现中文拼写和语法错误纠正。

**Result:** 模型在SIGHAN-2015, EC-LAW, MCSC, 和NaCGEC等多个数据集上取得了显著更高的F1和F0.5评分，超越了当前公开的模型，在拼写和语法错误纠正任务中均排名第一。

**Conclusion:** ChineseErrorCorrector3-4B在中文拼写和语法错误纠正任务中展现出卓越性能，达到了新的研究水平。

**Abstract:** This paper introduces ChineseErrorCorrector3-4B, a unified model for Chinese spelling and grammatical error correction based on Qwen3-4B. The model demonstrates outstanding performance in general text correction tasks and achieves state-of-the-art results in both spelling correction (CSC) and grammatical correction (CGC). On several authoritative benchmark datasets -- including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC -- the model's F1 and F0.5 scores significantly surpass existing publicly available models, ranking first in both spelling and grammatical error correction tasks.

</details>


### [5] [Generative Caching for Structurally Similar Prompts and Responses](https://arxiv.org/abs/2511.17565)
*Sarthak Chakraborty,Suman Nath,Xuchao Zhang,Chetan Bansal,Indranil Gupta*

Main category: cs.CL

> 本文介绍了一种名为\

<details>
  <summary>Details</summary>

**Motivation:** 在重复工作流和代理场景下，虽然提示相似但存在细微差别的重复使用，精确提示匹配无效，而语义缓存可能会忽略关键差异导致错误响应。

**Method:** 提出了一种生成型缓存方法，可以识别相似结构提示之间的可重用响应模式，并为新请求合成定制输出。

**Result:** 该方法在没有提示重复的数据集中实现 83printf{错误：缺少合法的百分比值，根据上下文应为83	extbackslash	extbackslash n	extbackslash	extbackslash t正确的表述应为83\%}的缓存命中率，并且在代理工作流中将缓存命中率提高约20printf{错误：缺少合法的百分比值，根据上下文应为20	extbackslash	extbackslash n	extbackslash	extbackslash t正确的表述应为20\%}，并将端到端执行延迟减少约34printf{错误：缺少合法的百分比值，根据上下文应为34	extbackslash	extbackslash n	extbackslash	extbackslash t正确的表述应为34\%}。

**Conclusion:** 该方法适用于大型语言模型在重复性任务中的优化，提高缓存命中率，减少执行延迟。

**Abstract:** Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \ourmethod{} achieves 83\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\sim$20\% and reduces end-to-end execution latency by $\sim$34\% compared to standard prompt matching.

</details>


### [6] [Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs](https://arxiv.org/abs/2511.17572)
*Patrick Gerard,Aiden Chang,Svitlana Volkova*

Main category: cs.CL

> 通过测试追踪知识删除后的响应模式，表明对齐的LLM能维持特定社区的行为模式，即使在除去大量事实后依然如此。这为开发更安全透明的模型提供证据和框架。

<details>
  <summary>Details</summary>

**Motivation:** 探究当大型语言模型（LLM）与特定在线社区对齐时，它们是否表现出可泛化的行为模式，这些行为模式反映该社区的态度和应对新不确定性的方式，还是仅仅重复训练数据中的模式？

**Method:** 提出框架以验证LLM在特定社区对不确定性的处理模式是否可转移。框架包括：从训练数据中有针对性地删除事件知识，使用多个探针进行验证，评估LLM在无知情况下是否再现社区有机反应模式。

**Result:** 即使在去除大量事实之后，与社区对齐的LLM仍然能保持稳定、特定于社区的行为模式来处理不确定性。这表明了对齐编码了超越表面模拟能力的结构化、可泛化行为。

**Conclusion:** 框架提供了一种系统的方式，用于检测在无知情况下持续存在的行为偏见，这促进了更加安全和透明的LLM应用。

**Abstract:** When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.

</details>


### [7] [Random Text, Zipf's Law, Critical Length,and Implications for Large Language Models](https://arxiv.org/abs/2511.17575)
*Vladimir Berman*

Main category: cs.CL

> 研究了一个简单的、非语言模型的文字序列，通过几何分布和组合学推导出词语统计、词汇增长等结果，并发现Zipf-like模式可在无需语言学原理的情况下出现。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于提供一种结构坚实的基础模型，用于解释自然语言单词统计和大语言模型中的标记统计。

**Method:** 我们研究了一个简单、完全非语言的文字模型：从包含有限字母集和一个空格符号的集合中连续进行独立抽取。在此基础之上，我们推导出了几个结构性的结果。

**Result:** 我们得到了几个结果：词语长度服从几何分布，且仅由空格符号的概率决定；给定长度的词语数量及其不同词语数量的期望值可以使用优惠券收集者论证得到；结合长度为k的字符串数量的指数增长和每个字符串概率的指数衰减，我们得到了一种遵循Zipf类型幂律的秩-频率关系。

**Conclusion:** 结果表明，Zipf-like模式可以单纯从组合学和分段中产生，无需优化原则或语言组织，并有助于解释哪些现象需要用比随机文本结构更深层次的理论来解释。

**Abstract:** We study a deliberately simple, fully non-linguistic model of text: a sequence of independent draws from a finite alphabet of letters plus a single space symbol. A word is defined as a maximal block of non-space symbols. Within this symbol-level framework, which assumes no morphology, syntax, or semantics, we derive several structural results. First, word lengths follow a geometric distribution governed solely by the probability of the space symbol. Second, the expected number of words of a given length, and the expected number of distinct words of that length, admit closed-form expressions based on a coupon-collector argument. This yields a critical word length k* at which word types transition from appearing many times on average to appearing at most once. Third, combining the exponential growth of the number of possible strings of length k with the exponential decay of the probability of each string, we obtain a Zipf-type rank-frequency law p(r) proportional to r^{-alpha}, with an exponent determined explicitly by the alphabet size and the space probability.
  Our contribution is twofold. Mathematically, we give a unified derivation linking word lengths, vocabulary growth, critical length, and rank-frequency structure in a single explicit model. Conceptually, we argue that this provides a structurally grounded null model for both natural-language word statistics and token statistics in large language models. The results show that Zipf-like patterns can arise purely from combinatorics and segmentation, without optimization principles or linguistic organization, and help clarify which phenomena require deeper explanation beyond random-text structure.

</details>


### [8] [Computational frame analysis revisited: On LLMs for studying news coverage](https://arxiv.org/abs/2511.17746)
*Sharaj Kunjar,Alyssa Hasegawa Smith,Tyler R Mckenzie,Rushali Mohbe,Samuel V Scarpino,Brooke Foucault Welles*

Main category: cs.CL

> 通过对生成式LLM和传统方法的性能对比，研究指出了在媒体框架分析中LLM的潜力和局限，并提倡采用多元化方法论。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨并评估生成式LLM在识别新闻报道中媒体框架的有效性，与传统方法对比以发现最佳的分析方法。

**Method:** 通过系统性评估生成式LLM（如GPT和Claude）在识别媒体框架方面的效果，将其与计算预处理器（词袋模型和编码器独占型变换模型）及传统手动编码程序进行对比。研究基于新开发的黄金标准数据集，该数据集是通过一套研究在2022年美国Mpox疫情六个月的新闻报道中形成的。

**Result:** 虽然发现生成式LLM在某些潜在应用中有用途，但它们的效果始终逊色于手动编码者，且在某些情况下甚至输给了较小的语言模型。研究还揭示了人类验证在决定适当模型选择中的必要性。此外，通过考察不同方法在分析过程中的适用性，为研究者如何结合这些方法提供了见解。

**Conclusion:** 研究结果支持多元化的方法论并制定了未来研究者可遵循的计算框架分析指南。

**Abstract:** Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.

</details>


### [9] [PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese](https://arxiv.org/abs/2511.17808)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

> 本文介绍了PoETa v2基准，用于评估多种大型语言模型在葡萄牙语上的表现，揭示了计算投入和语言适应对葡萄牙语性能的影响。

<details>
  <summary>Details</summary>

**Motivation:** 为了系统地评估大型语言模型在多样语言和文化背景下的性能表现，特别是在葡萄牙语上的表现。

**Method:** 使用新引入的PoETa v2基准测试，涵盖了40多个葡萄牙语任务，评估了超过20种模型。

**Result:** 研究揭示了计算资源投资和语言特定适应对葡萄牙语性能的影响，并分析了与英语相当任务的性能差距。

**Conclusion:** PoETa v2为未来葡萄牙语文本建模和评估的研究奠定了基础，相关基准测试也已开放源代码。

**Abstract:** Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.

</details>


### [10] [Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation](https://arxiv.org/abs/2511.17813)
*Scott Merrill,Shashank Srivastava*

Main category: cs.CL

> 研究开发了一种将Zoom录音转换为包含发言者身份信息和语用行动标签的转录文本的方法，并应用于模拟真实的多方位讨论。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型虽然提供了模拟多方讨论的机会，但现实主义建模受制于缺乏发言者归属的数据。这项工作希望解决数据短缺问题，提升模型的模拟能力。

**Method:** 通过将公共Zoom录音转换成带有发言者归属标签、人物档案和语用行动标签的转录文本，引入了一个可复现的管道。

**Result:** 使用这种方法细调的大型语言模型在特定参与者建模中表现出色，困惑度降低了67%，分类器性能几乎翻倍，模拟的讨论在人类评估中几乎与真实讨论无法区分。

**Conclusion:** 这种方法为复杂且真实的市民模拟提供了一种可行和可扩展的方法。

**Abstract:** Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this "action-aware" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.

</details>


### [11] [A superpersuasive autonomous policy debating system](https://arxiv.org/abs/2511.17854)
*Allen Roush,Devin Gonier,John Hines,Judah Goldfeder,Philippe Martin Wyder,Sanjay Basu,Ravid Shwartz Ziv*

Main category: cs.CL

> 提出了DeepDebater系统，该系统能够参与完整未经修改的竞赛性政策辩论，并取得胜利。系统使用了一个多智能体的工作流架构，并能在AI vs AI以及混合人类和AI的操作模式下运行。

<details>
  <summary>Details</summary>

**Motivation:** 当前的人工智能在复杂、基于证据和策略适应的辩论能力上还有很大的提升空间。先前的研究多集中在简化和短化的辩论格式上。DeepDebater旨在解决这一挑战，展示在完整、未经修改的团队比赛中获胜的能力。

**Method:** 采用分层的多智能体工作流架构，团队内的LLM智能体协作并相互批评，以完成离散的辩论任务。工作流使用迭代检索、综合和自我纠正的方法，利用大规模的政策辩论证据语料库（OpenDebateEvidence），并生成完整的演讲文稿、交叉询问和反驳。系统还提供了一个实时互动的端到端展示管道，将文字转化为声音和动画。

**Result:** 初步评估显示，相比于人类编写的案例，DeepDebater生成了质量更高的辩论组件，并在模拟的辩论中胜出，这得到了独立自主裁判的认可。即使专家人类辩论教练也更偏好DeepDebater构建的论点、证据和案例。

**Conclusion:** DeepDebater不仅在自主操作的辩论中表现出色，还支持人类和AI之间的混合操作模式，展现了先进的辩论能力。所有代码和生成的资源都已开源。

**Abstract:** The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main

</details>


### [12] [Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction](https://arxiv.org/abs/2511.17908)
*Debashish Chakraborty,Eugene Yang,Daniel Khashabi,Dawn Lawrie,Kevin Duh*

Main category: cs.CL

> 本文提出了一种基于符合性预测的上下文工程方法，通过控制覆盖率，更可靠地进行上下文缩减，提高了大语言模型在检索增强生成任务中的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的预生成过滤器依赖于启发式方法或未经校准的大模型置信度得分，无法对保留的证据提供统计控制。为了克服这一问题，本文提出了使用一种具有覆盖控制的过滤框架，即符合性预测，来改进检索增强生成模型中的上下文工程。

**Method:** 通过运用符合性预测（conformal prediction）框架对检索到的证据进行过滤，该框架在移除不相关内容的同时保留了支持证据的召回率。本文测试了基于嵌入和大模型评分函数的方法，并在NeuCLIR和RAGTIME数据集上进行了验证。

**Result:** 实验证明，符合性预测过滤器能够一致地达到其覆盖率目标，确保指定比例的相关片段被保留，并且相对于未过滤的检索，保留的上下文减少了2-3倍。在NeuCLIR上，严格的筛选改进了下游事实的准确性，而适度的覆盖则保持了事实的准确性稳定，表明被丢弃的大部分材料是冗余或不相关的。

**Conclusion:** 这些结果显示，符合性预测为RAG中的上下文缩减提供了可靠的、具有覆盖控制的方法，这是一种与模型无关且符合原则的方法。

**Abstract:** Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.

</details>


### [13] [L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention](https://arxiv.org/abs/2511.17910)
*Yuliang Zhan,Xinyu Tang,Han Wan,Jian Li,Ji-Rong Wen,Hao Sun*

Main category: cs.CL

> 本研究开发了一种名为L2V-CoT的无需训练的方法，用于通过提取和重新抽样低频CoT表示，将CoT推理从LLMs转移至VLMs，这种方法提高了VLMs的推理能力并超越了现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 研究人员发现，虽然思维链（CoT）推理显著提升了大规模语言模型（LLMs）的能力，但视觉-语言模型（VLMs）在多步推理任务中仍因缺乏多模态推理数据而表现不佳。现有的将CoT推理从LLMs转移到VLMs的方法要么需要高昂的训练成本，要么需要架构上的对齐，因此，研究动机是寻找一种更有效的方法来弥合这一差距。

**Method:** 研究者利用线性人工断层扫描技术（LAT）实验证明，尽管架构不同，LLMs和VLMs在低频部分共享相似的CoT推理隐含表示。基于这一发现，他们提出了L2V-CoT，这是一种无需训练的潜在干预方法，用于从LLMs向VLMs转移CoT推理。这种方法通过提取并重新抽样LLMs中的低频CoT表示，在频率域中实现维度匹配，并在推理过程中注入到VLMs中以增强其推理能力。

**Result:** <tool_call>
ingenvalue
<tool_call>
{

**Conclusion:** 实验结果表明，本研究的方法一致优于无需训练的基准，并且甚至超过了监督方法。

**Abstract:** Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.

</details>


### [14] [Towards Efficient LLM-aware Heterogeneous Graph Learning](https://arxiv.org/abs/2511.17923)
*Wenda Li,Tongya Zheng,Shunyu Liu,Yu Wang,Kaixuan Chen,Hanyang Yuan,Bingde Hu,Zujie Ren,Mingli Song,Gang Chen*

Main category: cs.CL

> 本文提出了ELLA框架，旨在通过使用LLM捕获异构图中的复杂关系语义，并通过创新的方法减少计算复杂度，同时缓解预训练和微调任务之间的语义差距，实验证明该方法在性能和效率上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机在于解决异构图中复杂关系语义建模的挑战。这包括克服预定义语义依赖的局限性以及大规模语言模型（LLMs）在异构图中的应用受限于计算复杂度的问题。

**Method:** 研究提出了一个名为ELLA的高效LLM感知框架，引入了LLM感知的关系分词器和跳级关系图转换器，并使用细化的任务感知的思考链提示来降低计算复杂度和减小语义差距。

**Result:** ELLA框架在四种异构图上的实验结果表明，它不仅提高了性能，而且实现了效率的提升，相比于现有的LLM方法有达4倍的速度提升。

**Conclusion:** 该研究所提出的ELLA框架证明了在异构图中有效利用LLM进行复杂关系语义建模的潜力，其不仅表现出色，而且比现有方法更高效。

**Abstract:** Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.

</details>


### [15] [SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization](https://arxiv.org/abs/2511.17938)
*Jianghao Wu,Yasmeen George,Jin Ye,Yicheng Wu,Daniel F. Schmidt,Jianfei Cai*

Main category: cs.CL

> 提出了SPINE框架，用于提高大型语言模型和多模态语言模型在测试阶段的推理能力，避免了响应变短和Pass@1下降的问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有测试时间强化学习方法中的分布变化问题和缺乏验证性监督问题，这些问题导致方法经常崩溃，响应变短，Pass@1下降。

**Method:** 提出SPINE框架，这是一种选择性标记的测试时间强化学习框架，该框架（i）仅更新分支标记，这些标记是从前向传递统计中确定的高熵分支点，（ii）在这些标记上使用熵带正则化器，在熵过低时维持探索，在熵过高时抑制噪声监督。

**Result:** 在十个涵盖多模式VQA、普通和专业QA、数学推理和医疗QA的基准测试中，SPINE在避免响应长度崩溃和实现更稳定的训练动态的同时，提高了Pass@1。

**Conclusion:** 将更新与推理分支点对齐是一种简单且无需标签的机制，可在不进行额外标注或奖励模型的情况下，实现稳定和有效的测试时间适应。

**Abstract:** Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.

</details>


### [16] [Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models](https://arxiv.org/abs/2511.17946)
*Shuo Zhang,Fabrizio Gotti,Fengran Mo,Jian-Yun Nie*

Main category: cs.CL

> 研究探讨了词汇训练数据覆盖对幻觉检测的影响，发现其提供的信号在结合其他特征时有助于提高检测效果。

<details>
  <summary>Details</summary>

**Motivation:** 先前的工作主要通过模型内部信号来检测幻觉，但忽略了预训练数据暴露与幻觉之间的联系。我们想要探讨词汇训练数据覆盖是否可以作为幻觉检测的信号。

**Method:** 我们通过构建可扩展的后缀数组来检索问题和生成答案的$n$-gram统计信息，以此评估词汇训练数据覆盖对幻觉检测的影响。

**Result:** 研究发现单独使用出现频率特征作为预测因子效果较弱，但与对数概率结合使用时可以取得适度的改进，特别是在模型不确定性更高的数据集上。

**Conclusion:** 这些发现表明词汇覆盖率特征可以作为幻觉检测的有效补充信号。

**Abstract:** Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.

</details>


### [17] [MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok](https://arxiv.org/abs/2511.17955)
*Dat Thanh Nguyen,Nguyen Hung Lam,Anh Hoang-Thi Nguyen,Trong-Hop Do*

Main category: cs.CL

> This paper presents MTikGuard, a system using multimodal classification to detect harmful TikTok content in real-time, with high performance achieved through expansion of a dataset to 4,723 labeled videos and a scalable streaming architecture.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the paper is to address the issue of harmful content on TikTok, which is difficult to moderate due to the platform's high volume and real-time nature of short-form video uploads.

**Method:** The paper introduces MTikGuard, a multimodal content detection system designed for TikTok, featuring a multimodal framework that integrates visual, audio, and textual features, achieving high accuracy with a scalable real-time deployment architecture.

**Result:** MTikGuard demonstrates state-of-the-art performance with an accuracy of 89.37% and an F1-score of 89.45%, indicating its effectiveness in detecting harmful content in real-time.

**Conclusion:** The paper concludes that an advanced multimodal detection system with a robust deployment architecture is effective in handling the challenges of real-time content moderation on platforms like TikTok.

**Abstract:** With the rapid rise of short-form videos, TikTok has become one of the most influential platforms among children and teenagers, but also a source of harmful content that can affect their perception and behavior. Such content, often subtle or deceptive, challenges traditional moderation methods due to the massive volume and real-time nature of uploads. This paper presents MTikGuard, a real-time multimodal harmful content detection system for TikTok, with three key contributions: (1) an extended TikHarm dataset expanded to 4,723 labeled videos by adding diverse real-world samples, (2) a multimodal classification framework integrating visual, audio, and textual features to achieve state-of-the-art performance with 89.37% accuracy and 89.45% F1-score, and (3) a scalable streaming architecture built on Apache Kafka and Apache Spark for real-time deployment. The results demonstrate the effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation. The dataset is available at https://github.com/ntdat-8324/MTikGuard-System.git.

</details>


### [18] [Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets](https://arxiv.org/abs/2511.18054)
*Gowtham,Sai Rupesh,Sanjay Kumar,Saravanan,Venkata Chaithanya*

Main category: cs.CL

> Blu-WERP 是一个先进的数据预处理管道，专门优化 Common Crawl WARC 文件用于大语言模型 (LLM) 的训练，其在不同规模模型和评估基准上显著优于现有方法如 DCLM。

<details>
  <summary>Details</summary>

**Motivation:** 改善大语言模型训练数据的质量，解决现有预处理管线难以有效去除噪音和无结构内容的问题。

**Method:** Blu-WERP 管道处理 CC WARC 数据，运用高级过滤和质量评估机制。

**Result:** 在 1B 参数尺度的模型上，Blu-WERP 总体性能相比 DCLM 和 Fineweb 分别提升 4.0% 和 9.5%，具体类别：世界知识与推理 2.4%，语言理解 6.2%，常识推理 4.2%，此结果展示了数据预处理技术对 LLM 性能的重要影响。

**Conclusion:** Blu-WERP 是一种先进且高效的预处理管道，在提升大语言模型训练数据的质量和模型性能方面有所建树。

**Abstract:** High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.

</details>


### [19] [GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set](https://arxiv.org/abs/2511.18146)
*Yomal De Mel,Nisansa de Silva*

Main category: cs.CL

> 本研究介绍了GeeSanBhava数据集，即通过手动标注从YouTube提取的高质量僧伽罗语歌曲评论，展示了基于评论的情感分布，并提出了一种多层感知器模型，以提高音乐情感识别的效果。

<details>
  <summary>Details</summary>

**Motivation:** 解决基于评论的情感与基于歌曲的情感之间的对比的挑战，同时还减轻用户生成内容中固有的偏见影响。本研究贡献了一套宝贵的数据集，提供了僧伽罗语自然语言处理和音乐情感识别的见解。

**Method:** 使用Russells Valence-Arousal模型手动标记从YouTube提取的僧伽罗语歌曲评论，使用三个独立的人类注释者进行标注。同时也使用机器学习和深度学习模型在相关的僧伽罗语新闻评论数据集上进行预训练，以报告零样本结果。

**Result:** 三个注释者之间的实质性注释者一致性很高（Fleiss kappa=84.96%）。经过广泛的超参数调整的优化后的多层感知器模型在ROC-AUC上得分0.887。

**Conclusion:** 本研究提供了一个有价值的注释语料库，并为未来的僧伽罗语自然语言处理和音乐情感识别工作提供了见解。

**Abstract:** This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.

</details>


### [20] [Vector Arithmetic in Concept and Token Subspaces](https://arxiv.org/abs/2511.18162)
*Sheridan Feucht,Byron Wallace,David Bau*

Main category: cs.CL

> 研究探讨了如何利用概念和词头注意力转换隐藏状态，以在语义和表面词信息层面上提高语言模型的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 为了预测下一个标记，需要模型表示当前词的语义和表面信息。研究试图使用概念诱导头和标记诱导头来揭示这种信息。

**Method:** 通过使用概念头和词头注意力权重对隐藏状态进行转换，研究展示了在Llama-2-7b模型激活子空间中存在有连贯语义结构的现象。

**Result:** 研究结果表明，通过概念头转换后的隐藏状态在进行平行四边形算术运算时更准确（80%），此方法比直接使用原始隐藏状态要好得多（47%）。类似地，标记头的转换也揭示了隐藏状态中的表面词信息。

**Conclusion:** 概念头和词头的注意力权重能够用于揭示模型激活中潜在的语义结构，提高了在特定任务上的准确性。

**Abstract:** In order to predict the next token, LLMs must represent semantic and surface-level information about the current word. Previous work identified two types of attention heads that disentangle this information: (i) Concept induction heads, which copy word meanings, and (ii) Token induction heads, which copy literal token representations (Feucht et al., 2025). We show that these heads can be used to identify subspaces of model activations that exhibit coherent semantic structure in Llama-2-7b. Specifically, when we transform hidden states using the attention weights of concept heads, we are able to more accurately perform parallelogram arithmetic (Mikolov et al., 2013) on the resulting hidden states, e.g., showing that "Athens" - "Greece" + "China" = "Beijing". This transformation allows for much higher nearest-neighbor accuracy (80%) than direct use of raw hidden states (47%). Analogously, we show that token heads allow for transformations that reveal surface-level word information in hidden states, allowing for operations like "coding" - "code" + "dance" = "dancing".

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [21] [Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks](https://arxiv.org/abs/2511.17576)
*Rayan Aldajani*

Main category: cs.CV

> 本研究展示了使用AI模型通过人体图像和人体测量数据估算身体脂肪百分比的可行性，这为健康和健身领域的消费者应用提供了低成本的解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 由于金标准方法如DEXA扫描对于大多数人来说成本高昂且难以获得，因此本研究旨在评估基于人工智能（AI）的低成本替代方案的可行性。

**Method:** 本研究开发了两种方法来估算身体脂肪百分比：一种是基于ResNet的图像模型，另一种是基于人体测量数据的回归模型。同时，还概述了一个多模态融合框架，以供未来扩展使用。

**Result:** 图像模型取得了4.44%的均方根误差（RMSE）和0.807的决定系数（R^2）。

**Conclusion:** 这些发现表明，AI辅助模型可以提供成本较低且易于获取的身体脂肪百分比估算，这类模型支持未来在健康和健身领域的消费者应用。

**Abstract:** Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.

</details>


### [22] [Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding](https://arxiv.org/abs/2511.17596)
*Yassir Benhammou,Suman Kalyan,Sujay Kumar*

Main category: cs.CV

> 论文提出了一种多模态自编码器（MMAE），可在不同模态间学习统一的表示，从而实现元数据的自动化提取和处理，为进一步开发自动化元数据生成及多模态检索技术奠定了基础。

<details>
  <summary>Details</summary>

**Motivation:** 虽然现有的AI系统通常在单一模态（如视频、音频或文本）上运行，但它们理解广播材料中的复杂跨模态关系的能力受到限制。本工作旨在解决这一问题，以提高元数据生成和跨模态检索的效率。

**Method:** 提出了一种多模态自编码器（MMAE），其可以在文本、音频和视觉数据之间学习统一表示，从而实现元数据提取和语义聚类的端到端自动化。该模型利用最近引入的LUMA数据集进行训练，这是一个完全对齐的多模态三元组基准，具有代表性的现实世界媒体内容。通过最小化跨模态的联合重构损失，MMAE能够在不依赖大型配对或对比数据集的情况下发现模态不变的语义结构。

**Result:** 在聚类和对齐指标（轮廓、ARI、NMI）上的结果显示，与线性基线相比有显著改善，表明基于重构的多模态嵌入可以作为可扩展元数据生成和跨模态检索的基础。

**Conclusion:** 这些结果显示，基于重构的多模态学习能够增强自动化、搜索性以及内容管理效率在现代广播工作流程中的作用。

**Abstract:** Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.

</details>


### [23] [BCWildfire: A Long-term Multi-factor Dataset and Deep Learning Benchmark for Boreal Wildfire Risk Prediction](https://arxiv.org/abs/2511.17597)
*Zhengsen Xu,Sibo Cheng,Hongjie He,Lanying Wang,Wentao Sun,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

> 本文提出了一套全面的25年每日分辨率野火数据集，用于评估和研究多种时间序列预测模型在野火风险预测中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有公开的基准数据集在支持长期时间建模、大范围空间覆盖和多模态驱动因素方面较少，为此提出了一整套长期且高分辨率的数据集。

**Method:** 使用了一个25年的每日分辨率野火数据集，涵盖不列颠哥伦比亚省及周边2.4亿公顷的地区。该数据集包含了38个共变量，包括活跃火情检测、气象变量、燃料条件、地形特征和人类活动因素。使用这个基准数据集评估了多种时间序列预测模型，包括基于CNN、线性模型、Transformer和Mamba的架构。同时研究了位置嵌入的有效性和不同火情驱动因素的相对重要性。

**Result:** 未直接提供结果，但提到通过此数据集评估了多种模型的性能，并研究了位置嵌入和不同因素的重要性。

**Conclusion:** 虽然没有具体结论描述，但可以推测研究通过模型评估和因素分析旨在为野火风险预测提供更全面的数据和方法支持。

**Abstract:** Wildfire risk prediction remains a critical yet challenging task due to the complex interactions among fuel conditions, meteorology, topography, and human activity. Despite growing interest in data-driven approaches, publicly available benchmark datasets that support long-term temporal modeling, large-scale spatial coverage, and multimodal drivers remain scarce. To address this gap, we present a 25-year, daily-resolution wildfire dataset covering 240 million hectares across British Columbia and surrounding regions. The dataset includes 38 covariates, encompassing active fire detections, weather variables, fuel conditions, terrain features, and anthropogenic factors. Using this benchmark, we evaluate a diverse set of time-series forecasting models, including CNN-based, linear-based, Transformer-based, and Mamba-based architectures. We also investigate effectiveness of position embedding and the relative importance of different fire-driving factors. The dataset and the corresponding code can be found at https://github.com/SynUW/mmFire

</details>


### [24] [Robustness of Structured Data Extraction from Perspectively Distorted Documents](https://arxiv.org/abs/2511.17607)
*Hyakka Nakada,Yoshiyasu Tanaka*

Main category: cs.CV

> 该研究检查了几何变换对 Gemini-1.5-pro 从文档图像中提取数据准确性的影响，特别是透视变形对字符识别和结构识别精度的影响。研究发现，通过简单的旋转校正可以提升结构识别精度，为多模态大语言模型的实际应用提供了见解。

<details>
  <summary>Details</summary>

**Motivation:** 考虑真实世界文档图像中常见的透视变形及旋转问题，研究其对最新模型 Gemini-1.5-pro 可视数据提取性能的影响。

**Method:** 研究使用了ISO梯形转化模型来简化文档图像的变形参数化，并通过调整旋转角度与变形比例来评价它们对数据提取精度的影响。评估标准包括字符识别准确率和结构识别准确率。

**Result:** 实验结果表明，文档的透视变形严重影响了结构识别的正确性，而简单的旋转校正能够改进这一问题。

**Conclusion:** 研究说明了透视效应对于文档数据提取任务的确切影响，发现透视变形对数据提取模型的影响及其潜在的解决方案，对实际应用有启示作用。

**Abstract:** Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.

</details>


### [25] [3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF](https://arxiv.org/abs/2511.17609)
*Linh Van Ma,Unse Fatima,Tepy Sokun Chriv,Haroon Imran,Moongu Jeon*

Main category: cs.CV

> 本文提出了一种新的使用UKF将多个摄像机的2D注释转化为3D真实的世界坐标的算法，该算法不仅提高了3D定位的精度，还具有完整的3D形状输出和遮挡应对能力。

<details>
  <summary>Details</summary>

**Motivation:** 精确的3D真实估计对于自主导航、监视和机器人等应用至关重要。现有的方法往往只能提供地面平面信息，而本文的方法不仅能够给出物体的3D位置，还能输出物体的完整3D形状。这展示了本文方法在多摄像机系统中提供可扩展和全自动解决方案的能力，只使用2D图像注释。

**Method:** 本文提出了一种新型方法，该方法利用无迹卡尔曼滤波器（UKF）融合多个校准摄像机的2D边界框或姿态关键点的真实注释，以获得准确的3D真实估计。通过利用人类标注的2D真实数据，所提方法——一种多摄像机单目标跟踪算法，能够将2D图像坐标转化为通过单应性投影和UKF融合得到的鲁棒3D世界坐标。该算法处理多视角数据来估计物体的位置和形状，同时有效应对遮挡等挑战。

**Result:** 本文的方法在CMC、Wildtrack和Panoptic这些数据集上进行了验证，与其他方法进行对比，展示了在3D定位上的高精度。

**Conclusion:** 本文提出的方法通过融合来自多个摄像机的2D注释，成功地实现了物体到3D世界坐标的转换，不仅处理了单一目标的跟踪问题，还通过多视角数据解决了物体形状估计和遮挡处理的挑战。

**Abstract:** Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.

</details>


### [26] [Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression](https://arxiv.org/abs/2511.17612)
*Siddiqua Namrah*

Main category: cs.CV

> 本文提出了一种无监督的多阶段深度学习框架，用于增强低光照交通图像，该框架包含三阶段模块，分别解决亮度调整、反射恢复和过曝补偿问题，并在多个数据集上表现出优于现有方法的性能。

<details>
  <summary>Details</summary>

**Motivation:** 提升低光环境下的交通图像识别性能，为自动驾驶和智能交通提供更可靠的感知系统。

**Method:** 使用无监督学习的方法，将图像分解为光照和反射分量，并通过三个专门模块进行逐步优化：光照适应、反射恢复和过曝补偿。

**Result:** 在一般数据集和交通特定数据集上进行的实验显示，该方法在定量指标（如PSNR, SSIM, LPIPS, NIQE）和定性视觉质量上均优于最先进的方法。

**Conclusion:** 所提方法能增强低光照环境下交通图像的能见度，保持结构细节，并提高实际场景中下游感知任务的可靠性。

**Abstract:** Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.

</details>


### [27] [HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation](https://arxiv.org/abs/2511.17614)
*Danyang Sun,Fadi Dornaika,Nagore Barrena*

Main category: cs.CV

> HSMix is a novel data augmentation approach using hard and soft mixing of superpixels to improve medical image segmentation by addressing data scarcity without requiring complex setups.

<details>
  <summary>Details</summary>

**Motivation:** Due to data scarcity in medical imaging, this paper aims to leverage data augmentation to improve segmentation performance without the complications of self-supervised or semi-supervised learning paradigms.

**Method:** HSMix combines homogeneous regions (superpixels) from two source images with both hard and soft (brightness mixing) techniques based on locally aggregated pixel-wise saliency coefficients to generate augmented images and their associated masks for training.

**Result:** The method effectively enriches the augmentation space with more varied data, preserving local semantic information, and is shown to be effective across different medical segmentation tasks without being model-specific.

**Conclusion:** HSMix is a simple yet powerful data augmentation technique for medical image segmentation that can be easily integrated into existing models to address the challenge of data scarcity.

**Abstract:** Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in https://github.com/DanielaPlusPlus/HSMix.

</details>


### [28] [Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis](https://arxiv.org/abs/2511.17615)
*Young-Beom Woo*

Main category: cs.CV

> This paper introduces PnP-MIX, a tuning-free method for high-fidelity text-to-image synthesis that seamlessly integrates multiple personalized concepts, maintaining non-personalized regions' integrity and preventing concept leakage with multiple innovative techniques.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to improve the performance of Text-to-Image generation in complex scenes by focusing on integrating multiple personalized concepts while preserving the semantic consistency and structural integrity of the generated image.

**Method:** Our method, PnP-MIX, uses guided appearance attention and a mask-guided noise mixing strategy to integrate multiple personalized concepts into a T2I synthesis, while maintaining the fidelity of non-personalized regions and preventing concept leakage with background dilution++.

**Result:** Experimental results show that the proposed method outperforms existing methods in both single- and multi-concept personalization settings without requiring additional model tuning, indicating its robustness and effectiveness.

**Conclusion:** The paper concludes that PnP-MIX, which includes several innovative techniques like guided appearance attention, mask-guided noise mixing, and background dilution++, offers superior performance in the integration of personalized concepts in T2I synthesis with high fidelity preservation.

**Abstract:** Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.

</details>


### [29] [Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach](https://arxiv.org/abs/2511.17618)
*Ju-Young Oh*

Main category: cs.CV

> This paper introduces FIQ, a method that enriches VQA datasets with scene-level Q&A pairs and introduces a VQ-CAlign module to enhance the generalizability and reasoning performance of VQA models, achieving state-of-the-art performance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the restrictive nature of event-centric annotations in existing VQA datasets and to improve the generalization and reasoning capabilities of VQA models by providing richer foundational information.

**Method:** In this paper, the authors propose FIQ, a framework that generates Q&A pairs directly from video content to enhance the foundational comprehension of VQA models. Additionally, a VQ-CAlign module is introduced to align question embeddings with visual features, preserving essential contextual cues.

**Result:** The experimental results on the SUTD-TrafficQA dataset indicate that the proposed FIQ method outperforms existing baseline approaches.

**Conclusion:** The research concludes that generating Q&A pairs from core scene-level attributes and aligning task-specific question embeddings with visual features significantly enhances the reasoning capability and generalizability of VQA models.

**Abstract:** Conventional VQA approaches primarily rely on question-answer (Q&A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.

</details>


### [30] [Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds](https://arxiv.org/abs/2511.17619)
*Qinghao Meng,Junbo Yin,Jianbing Shen,Yunde Jia*

Main category: cs.CV

> 本文提出了基于角点的回归方法来提高3D物体检测的稳定性和准确性。该方法在KITTI数据集上的性能优于基于中心的基线方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于中心的回归方法在3D物体检测中主导地位显著，但它由于LiDAR点云的前表面偏向性质，可能导致边界框预测不稳定和不准确。为此，我们重新审视了边界框的表示方法。

**Method:** 我们提出了一种基于角点的回归方法，该方法将预测目标从不稳定的中心转移到几何信息丰富的角点上。通过利用角点之间的几何约束和图像2D边界框，部分3D边界框参数可以从角点标注中恢复，从而支持弱监督学习，无需完整的3D标签。

**Result:** 实验结果显示，我们的方法在KITTI数据集上比基于中心的方法提高了3.5%的AP，仅使用BEV角点标注就能达到全监督精度的83%。

**Conclusion:** 实验结果证明了我们基于角点的回归策略的有效性。

**Abstract:** Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.

</details>


### [31] [BD-Net: Has Depth-Wise Convolution Ever Been Applied in Binary Neural Networks?](https://arxiv.org/abs/2511.17633)
*DoYoung Kim,Jin-Seop Lee,Noo-ri Kim,SungJoon Lee,Jee-Hyong Lee*

Main category: cs.CV

> This paper proposes a 1.58-bit convolution and a pre-BN residual connection to enhance performance in BNNs, achieving a new state-of-the-art on various datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to tackle the limitations of BNNs caused by extreme quantization, improving representational capacity and training stability, especially for lightweight networks with depth-wise convolutions.

**Method:** Our method introduces a 1.58-bit convolution to improve the expressiveness of binary neural networks (BNNs) and a pre-BN residual connection to enhance the optimization stability by improving the Hessian condition number.

**Result:** The proposed method achieves 33M OPs on ImageNet with MobileNet V1, surpassing previous BNNs with the same OPs. It also consistently achieves higher accuracy across various datasets, with improvements of up to 9.3 percentage points.

**Conclusion:** The paper introduces a significant advancement in the binarization of depth-wise convolutions in BNNs, establishing a new state-of-the-art performance, with notable improvements in accuracy and efficiency across diverse datasets.

**Abstract:** Recent advances in model compression have highlighted the potential of low-bit precision techniques, with Binary Neural Networks (BNNs) attracting attention for their extreme efficiency. However, extreme quantization in BNNs limits representational capacity and destabilizes training, posing significant challenges for lightweight architectures with depth-wise convolutions. To address this, we propose a 1.58-bit convolution to enhance expressiveness and a pre-BN residual connection to stabilize optimization by improving the Hessian condition number. These innovations enable, to the best of our knowledge, the first successful binarization of depth-wise convolutions in BNNs. Our method achieves 33M OPs on ImageNet with MobileNet V1, establishing a new state-of-the-art in BNNs by outperforming prior methods with comparable OPs. Moreover, it consistently outperforms existing methods across various datasets, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements of up to 9.3 percentage points.

</details>


### [32] [Efficient Score Pre-computation for Diffusion Models via Cross-Matrix Krylov Projection](https://arxiv.org/abs/2511.17634)
*Kaikwan Lau,Andrew S. Na,Justin W. L. Wan*

Main category: cs.CV

> This paper proposes a novel framework using cross-matrix Krylov projection to accelerate score-based diffusion models, achieving significant time savings and better performance than DDPMs in resource-constrained scenarios.

<details>
  <summary>Details</summary>

**Motivation:** To improve the computational efficiency of training score-based diffusion models by addressing the high computational cost associated with solving large linear systems for each image.

**Method:** The paper introduces a cross-matrix Krylov projection technique that leverages shared subspaces built from seed matrices to efficiently solve for subsequent target matrices, addressing the computational challenges in score-based diffusion models.

**Result:** Experiments indicate a time reduction of 15.8% to 43.7% over standard sparse solvers and a speedup of up to 115x when compared to DDPM baselines in denoising tasks.

**Conclusion:** The proposed approach not only accelerates training and generation processes but also produces high-quality images under resource limitations, outperforming DDPM methods.

**Abstract:** This paper presents a novel framework to accelerate score-based diffusion models. It first converts the standard stable diffusion model into the Fokker-Planck formulation which results in solving large linear systems for each image. For training involving many images, it can lead to a high computational cost. The core innovation is a cross-matrix Krylov projection method that exploits mathematical similarities between matrices, using a shared subspace built from ``seed" matrices to rapidly solve for subsequent ``target" matrices. Our experiments show that this technique achieves a 15.8\% to 43.7\% time reduction over standard sparse solvers. Additionally, we compare our method against DDPM baselines in denoising tasks, showing a speedup of up to 115$\times$. Furthermore, under a fixed computational budget, our model is able to produce high-quality images while DDPM fails to generate recognizable content, illustrating our approach is a practical method for efficient generation in resource-limited settings.

</details>


### [33] [Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification](https://arxiv.org/abs/2511.17635)
*Max A. Nelson,Elif Keles,Eminenur Sen Tasci,Merve Yazol,Halil Ertugrul Aktas,Ziliang Hong,Andrea Mia Bejar,Gorkem Durak,Oznur Leman Boyunaga,Ulas Bagci*

Main category: cs.CV

> 本文介绍了UPMI方法，通过在低维的元特征空间中进行数据增强，提升了机器学习在儿童胰腺炎诊断中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本论文旨在通过新型的数据增强技术来解决儿童胰腺炎在诊断过程中遇到的挑战。特别是，由于小样本量和多模态影像的复杂性，机器学习方法在诊断时遇到了困难。

**Method:** 本研究提出了一种上游概率元插补法（UPMI），这是一种轻量级的数据增强策略。该方法通过多模态影像（T1加权和T2加权MRI）的逻辑回归产生概率输出，并将其转化为7维元特征向量。随后，在每个交叉验证折叠中，采用条件高斯混合模型（GMM）进行元特征的合成采样。合成的元特征与真实元特征一起用于训练随机森林（RF）元分类器。

**Result:** 在包含67名儿童患者的数据库中，UPMI方法达到了平均AUC为0.908的准确率，比未使用数据增强的基线（AUC为0.864）提高了约5%。

**Conclusion:** 研究表明，UPMI作为一种元学习框架中的数据增强策略，有效提升了儿童胰腺炎影像诊断的准确性，为临床提供了一种新的辅助诊断工具。

**Abstract:** Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\pm$ 0.072, a $\sim$5% relative gain over a real-only baseline (AUC 0.864 $\pm$ 0.061).

</details>


### [34] [TSRE: Channel-Aware Typical Set Refinement for Out-of-Distribution Detection](https://arxiv.org/abs/2511.17636)
*Weijun Gao,Rundong He,Jinyang Dong,Yongshun Gong*

Main category: cs.CV

> The paper addresses limitations in existing activation-based OOD detection methods and proposes a refinement method that considers channel characteristics and distributional skewness, resulting in state-of-the-art performance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of existing methods that often overlook channel characteristics and distributional skewness, leading to inaccurate typical set estimation and improper inclusion of anomalous activations.

**Method:** Our method refines the typical set of activations by introducing a refinement based on discriminability and activity, which considers channel characteristics, and a skewness-based refinement that mitigates distributional bias, aiming to improve the accuracy and reliability of OOD detection.

**Result:** Experiments on ImageNet-1K and CIFAR-100 benchmarks demonstrate that our method outperforms existing methods and generalizes well across different model backbones and score functions.

**Conclusion:** The conclusion is that the proposed method significantly improves OOD detection by refining the typical set of activations, achieving state-of-the-art performance and good generalization across benchmarks, backbones, and score functions.

**Abstract:** Out-of-Distribution (OOD) detection is a critical capability for ensuring the safe deployment of machine learning models in open-world environments, where unexpected or anomalous inputs can compromise model reliability and performance. Activation-based methods play a fundamental role in OOD detection by mitigating anomalous activations and enhancing the separation between in-distribution (ID) and OOD data. However, existing methods apply activation rectification while often overlooking channel's intrinsic characteristics and distributional skewness, which results in inaccurate typical set estimation. This discrepancy can lead to the improper inclusion of anomalous activations across channels. To address this limitation, we propose a typical set refinement method based on discriminability and activity, which rectifies activations into a channel-aware typical set. Furthermore, we introduce a skewness-based refinement to mitigate distributional bias in typical set estimation. Finally, we leverage the rectified activations to compute the energy score for OOD detection. Experiments on the ImageNet-1K and CIFAR-100 benchmarks demonstrate that our method achieves state-of-the-art performance and generalizes effectively across backbones and score functions.

</details>


### [35] [SWITCH: Benchmarking Modeling and Handling of Tangible Interfaces in Long-horizon Embodied Scenarios](https://arxiv.org/abs/2511.17649)
*Jieru Lin,Zhiwei Yu,Börje F. Karlsson*

Main category: cs.CV

> 提出了一个名为SWITCH的任务驱动型基准测试，用来检验自主智能在真实环境控制接口中多方面的能力，发现现有模型在多步交互中表现不稳定，依赖文本线索多于视觉或视频信息，并提供了数据和代码以促进社区贡献和未来更复杂版本的基准测试开发。

<details>
  <summary>Details</summary>

**Motivation:** 为了弥补现有基准测试中关于自主智能在真实环境中的操作能力不足，特别是涉及物理常识推理、因果预测和结果验证方面，提出了一个新基准以全面评估这些能力。

**Method:** 通过迭代发布的方式构建了一个名为SWITCH的基准测试，其第一个版本SWITCH-Basic重点评估任务意识的VQA、语义UI定位、动作生成、状态变迁预测、结果验证等五种能力，在第一人称视图视频输入和设备多样性下进行测试。

**Result:** 在351个涵盖98种真实设备和家电的任务中，商业和开源的大规模多模态模型在单步交互中也表现不稳定，常常过于依赖文本线索，而低估了视觉或视频证据的重要性，总体高分可能掩盖了这些缺陷。

**Conclusion:** SWITCH提供了一个评估和促进自主智能系统在真实世界控制接口任务中操作能力发展的平台。

**Abstract:** Autonomous intelligence requires not only perception and reasoning, but critically, effective interaction with the existing world and its infrastructure. Everyday environments are rich in tangible control interfaces (TCIs), e.g., light switches, appliance panels, and embedded GUIs, that demand commonsense and physics reasoning, but also causal prediction and outcome verification in time and space (e.g., delayed heating, remote lights). Moreover, failures here have potential safety implications, yet current benchmarks rarely test grounding, partial observability (video), or post-hoc verification in situated settings. We introduce SWITCH (Semantic World Interface Tasks for Control and Handling), an embodied, task-driven benchmark created through iterative releases to probe these gaps. Its first iteration, SWITCH-Basic, evaluates five complementary abilities:task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification, under egocentric RGB video input and device diversity. Across 351 tasks spanning 98 real devices and appliances, commercial and open LMMMs exhibit inconsistent performance even on single-step interactions, often over-relying on textual cues and under-using visual or video evidence (and high aggregate scores can mask such failures). SWITCH provides data, code, and held-out splits to enable reproducible evaluation and community contributions toward more challenging future iterations of the benchmark and the creation of training datasets. Benchmark resources are available at: https://github.com/BAAI-Agents/SWITCH.

</details>


### [36] [Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment](https://arxiv.org/abs/2511.17655)
*Md. Mohaiminul Islam,Md. Mofazzal Hossen,Maher Ali Rusho,Nahiyan Nazah Ridita,Zarin Tasnia Shanta,Md. Simanto Haider,Ahmed Faizul Haque Dhrubo,Md. Khurshid Jahan,Mohammad Abdul Qayum*

Main category: cs.CV

> 本研究提供了一整套用于自动分类脑肿瘤的深度学习系统，基于MRI图像，系统涵盖了六种基准架构，并提出了一种轻量级的CNN模型，该模型能实现实时推理，同时保持高精度，适合资源有限的环境部署。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在标准化评估过程，提高模型的解释性与部署可行性，开发轻量级模型以适应多场景的应用需求。

**Method:** 采用五种预训练模型和一个自建的轻量级CNN模型，使用标准的预处理、训练集及评估体系，同时使用Grad-CAM和GradientShap以增强模型的解释能力。

**Result:** Inception-ResNet V2达到了99.53%的测试准确率；轻量级模型取得了96.49%的测试准确率，模型体积是Inception-ResNet V2的1/100，能在375ms内实现实时推理。

**Conclusion:** 提出了一种适合资源有限环境的轻量级模型，并且展示了深度学习在医疗影像领域中的潜力，包括精确分类、准确解释和快速部署。

**Abstract:** Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.

</details>
