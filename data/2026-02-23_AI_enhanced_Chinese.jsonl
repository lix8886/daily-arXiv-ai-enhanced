{"id": "2602.17768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17768", "abs": "https://arxiv.org/abs/2602.17768", "authors": ["Boda Lin", "Yongjie Zhu", "Xiaocheng Gong", "Wenyu Qin", "Meng Wang"], "title": "KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding", "comment": "26 pages", "summary": "Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.", "AI": {"tldr": "研究提出了一种自动化注释流程以及KPM-Bench数据集，以促进细粒度运动理解和改进现有模型，同时引入了MoPE算法及幻觉评估指标，显著提高了运动中心视频标注模型的可靠性。", "motivation": "尽管视频标注模型在技术上有所进步，但仍存在描述精细运动细节的不足和严重幻象问题，特别是在处理动作中心视频时，因此，该研究旨在解决这些差距，并提高动作中心视频标注模型的可靠性。", "method": "本研究提出了一种自动化注释流程，将基于动力学的运动计算和语言解析相结合，以详细分解和描述复杂的人类运动。基于此流程，构建并发布了Kinematic Parsing Motion Benchmark (KPM-Bench)，该数据集旨在促进细粒度运动理解，并为评估运动描述中的幻觉现象提供了一个细致策划的评估集合。此外，还提出了语言基础的运动解析和提取（MoPE）算法，能够从文本描述中准确提取运动特定属性，并基于MoPE引入了一个独立于大规模视觉语言或仅限语言模型的精确幻觉评估指标。", "result": "MoPE算法与GRPO后训练框架的结合，有效缓解了幻象问题，显著提升了运动中心视频描述的可靠性。", "conclusion": "本研究通过MoPE算法和KPM-Bench数据集，解决了视频标注中的幻象问题，并提供了进一步研究这类问题的基础和方法。"}}
{"id": "2602.17770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17770", "abs": "https://arxiv.org/abs/2602.17770", "authors": ["Balamurugan Thambiraja", "Omid Taheri", "Radek Danecek", "Giorgio Becherini", "Gerard Pons-Moll", "Justus Thies"], "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild", "comment": "ICLR2026; Project page: https://balamuruganthambiraja.github.io/CLUTCH/", "summary": "Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.", "AI": {"tldr": "本文介绍了'3D Hands in the Wild'(3D-HIW)数据集和CLUTCH系统，通过结合视觉语言模型和先进的3D手部追踪技术，解决了现有手部动作生成方法的局限问题，提高了文本与动作的对齐准确性和生成的多样性和质量。", "motivation": "鉴于现有方法在生成自然手部动作时使用受限的动作和场景，成本较高，且难以与真实世界的应用场景相匹配，本文旨在开发一个更高效、更全面的数据集和系统来突破这些局限。", "method": "文中提出了3D-HIW数据集，包含32K个手部动作序列及匹配的文本描述，并设计了CLUTCH系统，该系统具有SHIFT算法及几何优化阶段，以通过分部分别的VQ-VAE架构改善泛化能力与重建保真度。", "result": "实验结果显示，本文方法在文本至动作生成与动作至文字描述任务上达到最先进水平，并首次建立了用于可扩展的、真实世界的3D手部动作建模的基准。", "conclusion": "该研究为手部动作生成提供了一个强大的数据集和系统，能有效应对真实场景中的动作生成问题，填补了当前研究的一大空白。"}}
{"id": "2602.17785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17785", "abs": "https://arxiv.org/abs/2602.17785", "authors": ["Xinwei Ju", "Rema Daher", "Danail Stoyanov", "Sophia Bano", "Francisco Vasconcelos"], "title": "Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision", "comment": "14 pages, 6 figures; early accepted by IPCAI2026", "summary": "Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.", "AI": {"tldr": "This paper introduces PRISM, a self-supervised learning framework for monocular depth and pose estimation in colonoscopy-assisted navigation. PRISM utilizes anatomical and illumination priors to improve depth and pose estimation under challenging conditions.", "motivation": "The aim is to enhance the precision and reliability of colonoscopy by overcoming issues like texture-less surfaces, complex illumination, and deformation that complicate depth and pose estimation.", "method": "PRISM employs a strategy that integrates edge detection and luminance decoupling for guidance, where edge maps and intrinsic decomposition modules help to capture thin structures and separate shading and reflectance.", "result": "Experiments show that PRISM achieves state-of-the-art performance on both real and synthetic colonoscopy datasets.", "conclusion": "The study concludes that self-supervised learning on real data and the correct sampling of video frames are critical for enhancing the performance of the model in the context of colonoscopy depth and pose estimation."}}
{"id": "2602.17793", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17793", "abs": "https://arxiv.org/abs/2602.17793", "authors": ["Peide Zhu", "Linbin Lu", "Zhiqin Chen", "Xiong Chen"], "title": "LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge", "comment": null, "summary": "It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.", "AI": {"tldr": "LGD-Net通过跨模态特征幻化方法，无需显式生成像素级图像，而是直接从H&E切片特征映射到HER2表达水平的潜在空间，不仅在公共数据集上性能优秀，还大幅提高了推理效率。", "motivation": "由于传统的多步IHC染色法资源密集、昂贵且耗时，且在很多地区不可用，研究提出了一种直接从H&E切片预测HER2水平的方法，目的是代替传统的评估HER2表达水平的方法。", "method": "LGD-Net采用跨模态特征幻化而非显式的像素级图像生成来解决虚拟染色方法的计算成本高昂及重构误差问题，通过教师IHC编码器引导训练，实现了从形态学H&E特征到分子潜在空间的直接映射，并利用核分布和膜染色强度等轻量级辅助正则化任务确保幻化的特征可以捕获临床相关的表型。", "result": "在BCI公有数据集上的实验表明，LGD-Net达到了最先进的性能，相较基准方法具有显著优势。", "conclusion": "研究展示了一种新的框架，通过轻量级辅助任务正则化，LGD-Net能更有效和准确地预测HER2水平，且仅需单模态H&E输入，为乳腺癌诊断提供了高效的解决方案。"}}
{"id": "2602.17784", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17784", "abs": "https://arxiv.org/abs/2602.17784", "authors": ["Meng Ye", "Xiao Lin", "Georgina Lukoczki", "Graham W. Lederer", "Yi Yao"], "title": "QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration", "comment": null, "summary": "Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.", "AI": {"tldr": "QueryPlot框架通过结合地质文献和地图数据，使用NLP技术进行矿产潜力区绘图，有效提升了地质勘查预测的效率和准确性。", "motivation": "地质勘查预测需要整合异构的地质知识，这一过程是传统的手动知识密集型的。QueryPlot旨在通过自动化和语义化检索过程来提高这一过程的效率和准确性。", "method": "提出了QueryPlot框架，它结合了大规模地质文本语料库和地质图数据，使用现代自然语言处理技术进行语义检索和制图。该框架将描述性沉积模型和地质地图的多边形转换为结构化文本表示，通过预训练的嵌入模型编码并计算语义相似度得分，以空间可视化的方式对区域进行排名。", "result": "QueryPlot不仅能够通过语义检索进行高效的地质勘查预测，还可以作为额外的特征提升在监督学习中的分类性能。", "conclusion": "在钨钙质角岩的研究案例中，该研究显示基于嵌入的检索系统能够实现已知矿点的高召回率，并且与专家定义的潜在地区紧密相关。此外，相似度得分还可以作为附加特性用于监督学习流程，从而提高分类性能。"}}
{"id": "2602.17799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17799", "abs": "https://arxiv.org/abs/2602.17799", "authors": ["Jose Sosa", "Danila Rukhovich", "Anis Kacem", "Djamila Aouada"], "title": "Enabling Training-Free Text-Based Remote Sensing Segmentation", "comment": null, "summary": "Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.", "AI": {"tldr": "介绍了一种完全不需要额外训练即可实现高质量的遥感图像分割方法", "motivation": "激发读者对于无需额外训练的分割方法的兴趣及认识到现有方法的问题", "method": "Structure", "result": "{\n    \"tldr\": \"研究展示了完全不需要额外训练即可实现基于文本的遥感图像分割的方法。通过结合对比和生成视觉语言模型与Segment Anything Model (SAM)，在无需训练或仅轻量调参的情况下，实现了先进的开放词汇语义分割及其他任务的分割。\", \n    \"motivation\": \"现有方法依赖额外的可训练组件，限制了其泛化能力和实际应用性。研究旨在探讨仅依赖现有基础模型是否可以实现基于文本的遥感图像分割。\", \n    \"method\": \"提出了一个集成对比和生成视觉语言模型与Segment Anything Model (SAM)的方法，采用CLIP作为分割提案的选择器，并使用GPT-5和调优后的Qwen-VL生成点击提示用于分割。\", \n    \"result\": \"在19个遥感基准测试中，展示了其在开放词汇、指代分割、和推理任务上的强大分割能力。\", \n    \"conclusion\": \"结果显示，该方法在完全不需要训练的设置下即可实现高质量的分割，即使在轻量调参后，也能达到更好的性能。代码将开源，供进一步研究。\")", "conclusion": "该研究证明了无额外训练的分割方法的有效性，并计划开源代码"}}
{"id": "2602.17815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17815", "abs": "https://arxiv.org/abs/2602.17815", "authors": ["Zhining Zhang", "Wentao Zhu", "Chi Han", "Yizhou Wang", "Heng Ji"], "title": "Neural Synchrony Between Socially Interacting Language Models", "comment": "Accepted at ICLR 2026", "summary": "Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the \"social minds\" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.", "AI": {"tldr": "本文研究了大型语言模型（LLMs）在社会互动中的神经同步现象，表明LLMs之间的神经同步与社会表现相关，揭示了人类和LLM社交互动的内部动态存在相似性。", "motivation": "尽管大语言模型（LLMs）被广泛认为是人类行为的强大近似，但是否可以将它们与人类的社会心智进行有意义的比较仍然是一个有争议的问题。本文旨在探讨在社会互动中的LLMs之间的神经同步作为这一争论的实证证据。", "method": "通过精心设计的实验，研究了大型语言模型（LLMs）在社会模拟过程中的神经同步现象，将其作为分析LLMs社会性的新型代理。", "result": "实验证明，神经同步可靠地反映了LLMs之间的社会参与和时间同步。", "conclusion": "研究结果表明，LLMs之间的神经同步与它们的社会表现密切相关，这为理解LLMs的“社会心智”提供了一个新视角，揭示了人类和LLM社交互动背后的内部动态存在惊人相似性。"}}
{"id": "2602.17807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17807", "abs": "https://arxiv.org/abs/2602.17807", "authors": ["Narges Norouzi", "Idil Esen Zulfikar", "Niccol`o Cavagnero", "Tommie Kerssies", "Bastian Leibe", "Gijs Dubbelman", "Daan de Geus"], "title": "VidEoMT: Your ViT is Secretly Also a Video Segmentation Model", "comment": null, "summary": "Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/", "AI": {"tldr": "本文提出了一个名为VidEoMT的简单视频分割模型，该模型通过轻量级查询传播和融合机制实现了时间建模，无需专门跟踪模块，实现了精度与速度的平衡。", "motivation": "现有的在线视频分割模型通常采用每帧分割器和复杂的专门跟踪模块，尽管它们非常有效，但也引入了大量架构复杂性和计算开销。研究表明，经过充分大容量训练的无特定模块的Vision Transformer (ViT)编码器同样能够实现准确的图像分割。因此，该研究旨在简化架构，减少计算开销，同时保持高效准确度。", "method": "提出了一个名为VidEoMT的简单编码器视频分割模型，该模型没有特定的跟踪模块。为了在只有编码器的ViT中实现时间建模，VidEoMT引入了一种轻量级查询传播机制，可以在帧之间重复使用查询进行信息传递，并结合一种查询融合策略来平衡新内容的适应性。", "result": "VidEoMT在不增加复杂性的情况下实现了追踪器的效益，实现了竞争力强的精度，运行速度比同类模型快5到10倍，峰值速度可达160 FPS。", "conclusion": "VidEoMT是有效地视频分割模型，它通过使用轻量级查询传播机制和查询融合策略，在无额外复杂性情况下达到了较高的精度和速度。"}}
{"id": "2602.17848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17848", "abs": "https://arxiv.org/abs/2602.17848", "authors": ["Cassandra L. Jacobs", "Morgan Grobol"], "title": "On the scaling relationship between cloze probabilities and language model next-token prediction", "comment": null, "summary": "Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.", "AI": {"tldr": "研究显示，更大规模的语言模型在预测人类阅读行为时有更好的表现，主要是因为它们在语义方面与人类响应有更好的对齐，尽管它们对低级信息的敏感性较低。", "motivation": "研究动机在于探索语言模型规模与预测人类阅读行为之间关系，尤其是模型规模如何影响其对语义和低级信息的敏感度。", "method": "本文通过比较不同规模的语言模型在预测眼动和阅读时间数据上的表现，分析了模型大小对预测人类响应质量的影响。", "result": "研究发现，虽然最好的模型在分配概率质量给人类响应时仍有不足，但较大的模型在预测下一个词汇及其产生可能性时能提供更高质量的估计，因为它对词汇共现统计的依赖性较低，同时在语义上与人类闭合测试响应有更好的对齐。", "conclusion": "研究结果支持了较大模型通过更大的记忆容量增强了猜测语义上更合适的词汇的能力，但对词识别相关的低级信息的敏感度降低这一观点。"}}
{"id": "2602.17814", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17814", "abs": "https://arxiv.org/abs/2602.17814", "authors": ["Adrian Catalin Lutu", "Eduard Poesina", "Radu Tudor Ionescu"], "title": "VQPP: Video Query Performance Prediction Benchmark", "comment": null, "summary": "Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.", "AI": {"tldr": "本文提出了第一个用于视频查询性能预测（VQPP）的基准，包含两个文本到视频检索数据集和两个内容基础视频检索系统，适用于未来的视频领域查询性能预测研究。", "motivation": "虽然查询性能预测（QPP）在文本和图像检索方面已经有较多研究，但对于基于内容的视频检索（CBVR）的QPP研究较少。为了填补这一空白，本文提出了一个针对视频检索的查询性能预测基准。", "method": "本文构建了一个包含56,000个文本查询和51,000个视频的VQPP基准，包括官方的训练集、验证集和测试集。此外，还测试了多种预检索和后检索性能预测器。", "result": "研究结果显示预检索预测器达到了具有竞争力的表现，这意味着无需实际执行检索步骤就可以实现应用。", "conclusion": "本研究不仅建立了VQPP基准，也为利用性能预测器优化大语言模型（LLM）在查询重述任务上的训练提供了可能。"}}
{"id": "2602.17881", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17881", "abs": "https://arxiv.org/abs/2602.17881", "authors": ["Joschka Braun"], "title": "Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations", "comment": "Master's Thesis, University of Tübingen. 89 pages, 34 figures. Portions of this work were published at the ICLR 2025 Workshop on Foundation Models in the Wild (see arXiv:2505.22637)", "summary": "Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.", "AI": {"tldr": "本文通过分析引导矢量的训练数据，研究了其在不同行为上作用的可靠性，发现引导矢量受到训练数据集中激活差异的余弦相似性以及激活的分离程度的影响，并提出了发展更稳健引导方法的动机。", "motivation": "作者希望通过研究引导矢量在不同行为上表现的不稳定性以及其如何受到训练数据的影响来解释为什么会存在这种差异性。", "method": "作者进行了一系列研究，包括观察训练激活差异的余弦相似性如何影响引导的可靠性，以及激活在行为数据集上的分离程度如何影响引导的可靠性，并且还研究了引导矢量在不同提示变化上的训练情况。", "result": "作者发现，当训练激活差异之间的余弦相似性更高时，引导效果更可靠；行为数据集中正负激活沿引导方向上分离度较高时，引导效果更可靠；而在不同提示变化上训练的引导矢量虽然在方向上有所区别，但在所有数据集上的表现相似，并且效果相关。", "conclusion": "研究结果表明，当潜在目标行为表示无法被线性控制方向有效近似时，引导矢量可能会变得不可靠。这些发现为诊断引导矢量的不稳定性提供了一个实用的方法，并且推动了开发能够显式考虑潜在行为表示非线性的更稳健引导方法的发展。"}}
{"id": "2602.17854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17854", "abs": "https://arxiv.org/abs/2602.17854", "authors": ["Domonkos Varga"], "title": "On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective", "comment": null, "summary": "This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.", "AI": {"tldr": "该论文对Liu和Szirányi提出的姿势识别方法进行了方法论分析，重点在于其评估协议的有效性。作者展示了所报告的近乎完美的准确度指标是由不可避免地混合了同一主题样本的帧级随机训练-测试分割造成的，这导致了严重的数据泄露。通过检查公布的混淆矩阵、学习曲线和数据集构建，作者表明该评估并未测量对未知个体的泛化能力。该发现强调了在视觉姿势识别研究中进行独立于主题的数据划分的重要性，特别是在诸如无人机人间交互等需要可靠识别由先前未见过的人完成的姿势的应用中。", "motivation": "研究动机在于揭示Liu和Szirányi提出的姿势识别方法中的评估协议中存在的问题，以准确衡量模型对未曾见过个体的泛化能力。", "method": "作者通过仔细检查发表的混淆矩阵、学习曲线以及数据集的构造，分析了Liu和Szirányi的评估协议中的潜在问题。", "result": "研究结果表明由于数据泄露，他们的精度指标并不能反映模型对新个体的姿势识别能力。", "conclusion": "该研究强调了在基于视觉的姿势识别中，采用独立于个体的数据划分的重要性，尤其是在需要可靠识别新个体手势的应用中。"}}
{"id": "2602.17907", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17907", "abs": "https://arxiv.org/abs/2602.17907", "authors": ["Raymond Li", "Amirhossein Abaskohi", "Chuyuan Li", "Gabriel Murray", "Giuseppe Carenini"], "title": "Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions", "comment": "20 pages, 5 figures", "summary": "Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.", "AI": {"tldr": "本文提出了一种新的神经主题模型方法，利用语言模型生成软标签目标来训练，从而提高主题的质量和语义相似文档的识别能力。", "motivation": "传统神经主题模型通过重建文档的词袋表示进行优化，忽略了上下文信息并难以处理数据稀疏问题。", "method": "通过语言模型将特定提示下的下一个标记概率投影到预定义词汇表上来获取上下文化监督信号，然后训练主题模型重构这些软标签。", "result": "实验结果表明，该方法在主题连贯性和纯度方面大幅优于现有基线，并且在识别语义相似文档方面表现突出。", "conclusion": "该方法通过生成上下文丰富的软标签目标，提高了主题模型的质量，特别是在语义相似文档检索方面有显著提升。"}}
{"id": "2602.17869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17869", "abs": "https://arxiv.org/abs/2602.17869", "authors": ["Yuxiao Chen", "Jue Wang", "Zhikang Zhang", "Jingru Yi", "Xu Zhang", "Yang Zou", "Zhaowei Cai", "Jianbo Yuan", "Xinyu Li", "Hao Yang", "Davide Modolo"], "title": "Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models", "comment": null, "summary": "With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.", "AI": {"tldr": "研究提出了一个结合AVS、SVC和MLLM的框架，用于长视频理解，具备适应性、高效性，并在不同基准上表现良好。", "motivation": "解决长视频分析中的挑战，包括在内存约束下高效处理大量帧，以及从大量输入数据中提取判别信息。", "method": "提出了一种用于长视频理解的端到端新架构，包括基于信息密度的自适应视频采样器（AVS）和基于自动编码器的时空视频压缩器（SVC），并与多模态大语言模型（MLLM）集成。", "result": "提出的框架在长视频理解任务和标准视频理解基准上表现出色。", "conclusion": "该框架展示了其在处理长时间视频序列复杂性方面的多功能性和有效性。"}}
{"id": "2602.17911", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17911", "abs": "https://arxiv.org/abs/2602.17911", "authors": ["Jash Rajesh Parekh", "Wonbin Kweon", "Joey Chan", "Rezarta Islamaj", "Robert Leaman", "Pengcheng Jiang", "Chih-Hsuan Wei", "Zhizheng Wang", "Zhiyong Lu", "Jiawei Han"], "title": "Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering", "comment": null, "summary": "Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.", "AI": {"tldr": "提出了CondMedQA基准测试和Condition-Gated Reasoning框架，用于解决现有生物医学问答系统不考虑条件依赖性的问题。", "motivation": "现有的生物医学问答系统和基准测试没有评估条件因果推理，并且检索增强或基于图的方法缺乏机制来确保检索到的知识适用于给定的上下文。", "method": "提出了Condition-Gated Reasoning框架，该框架基于查询条件选择性地激活或剪枝推理路径。", "result": "研究表明Condition-Gated Reasoning框架更可靠地选择条件适当的答案，在生物医学问答基准上的性能与现有最先进技术相当或超过。", "conclusion": "研究强调了明确建模条件性对于健壯的医学推理的重要性。"}}
{"id": "2602.17871", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.17871", "abs": "https://arxiv.org/abs/2602.17871", "authors": ["Dhruba Ghosh", "Yuhui Zhang", "Ludwig Schmidt"], "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.", "AI": {"tldr": "研究发现视觉语言模型在细粒度图像分类任务上表现不佳，并通过一系列消融实验找到了可能的原因，包括更好的语言模型和视觉编码器的影响，以及预训练阶段的重要性，特别是当语言模型权重不冻结时。这些发现为提高细粒度视觉理解和视觉中心能力提供了方向。", "motivation": "文章的动机是探讨视觉语言模型在细粒度视觉知识任务上表现不佳的原因，特别是在传统的图像分类基准测试中。", "method": "通过测试大量的视觉语言模型在细粒度分类基准上的表现，并进行一系列的消融实验来分析模型的不同部分对结果的影响。", "result": "研究表明改进的语言模型均等改善了所有基准分数，但更好的视觉编码器特别提升了细粒度分类表现。同时，预训练阶段对于提升细粒度视觉理解非常重要，尤其是在语言模型权重不冻结的情况下。", "conclusion": "这些实验结果揭示了提升视觉语言模型在细粒度视觉理解和以视觉为中心能力方面的潜在增强途径。"}}
{"id": "2602.17937", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17937", "abs": "https://arxiv.org/abs/2602.17937", "authors": ["Xiaotang Du", "Giwon Hong", "Wai-Chung Kwan", "Rohit Saxena", "Ivan Titov", "Pasquale Minervini", "Emily Allaway"], "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification", "comment": null, "summary": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.", "AI": {"tldr": "本文通过使用DSPy优化框架对比了四种提示技术，并验证了指令优化在提高语言模型事实验证性能上的效果，尤其是在合理选择优化器和提示技术的情况下。", "motivation": "提供一种轻量级、模型不可知的方法来提升大型语言模型的推理能力。", "method": "本论文采用DSPy优化框架，对比了四种即插即用的提示技术（直接预测、CoT链条思维、带有SQL工具的ReAct、带有Python执行的CodeAct），并通过三种优化器（COPRO、MiPROv2、SIMBA）在四个基准和三个模型族上进行了评估。", "result": "指令优化在事实验证准确性上始终有所提升，MiPROv2对CoT的提升最稳定，SIMBA对于ReAct代理在较大模型规模上提供了最大的好处。行为分析显示，SIMBA通过应用启发式方法鼓励了更直接的推理路径，提高了数值比较能力和减少了不必要的工具调用。", "conclusion": "CoT仍然适合表格事实检查，尤其是在使用较小模型时。然而，使用大模型构建的ReAct代理虽然可以取得有竞争力的表现，但需要精心的指令优化。"}}
{"id": "2602.17909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17909", "abs": "https://arxiv.org/abs/2602.17909", "authors": ["Amirhosein Javadi", "Chi-Shiang Gau", "Konstantinos D. Polyzos", "Tara Javidi"], "title": "A Single Image and Multimodality Is All You Need for Novel View Synthesis", "comment": null, "summary": "Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.", "AI": {"tldr": "This paper demonstrates that integrating sparse multimodal depth measurements into diffusion-based generative models improves the quality and consistency of single-image novel view synthesis, particularly in challenging real-world conditions.", "motivation": "To enhance the robustness of novel view synthesis by addressing the limitations of monocular depth estimation under poor conditions like low texture or heavy occlusions, through the use of sparse multimodal depth measurements.", "method": "The paper introduces a framework that uses sparse, multimodal range sensing data to produce reliable dense depth maps for conditioning diffusion models, employing a localized Gaussian Process for depth modeling that also quantifies uncertainty.", "result": "Experiments on driving scenes show improved geometric consistency and visual quality in novel views when using the proposed method versus relying solely on monocular depth estimations.", "conclusion": "The research underscores the potential of multimodal sensing data, even when extremely sparse, to provide more reliable geometric priors for diffusion-based novel view synthesis, leading to superior synthesis quality."}}
