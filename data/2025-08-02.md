<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.CV](#cs.CV) [Total: 31]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Large Language Models in the Travel Domain: An Industrial Experience](https://arxiv.org/abs/2507.22910)
*Sergio Di Meglio,Aniello Somma,Luigi Libero Lucio Starace,Fabio Scippacercola,Giancarlo Sperlì,Sergio Di Martino*

Main category: cs.CL

> The paper discusses the use of Large Language Models (LLMs) to improve the consistency and reliability of accommodation data on property booking platforms by addressing issues with external data sources.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance user experience and prevent market loss due to inconsistent or incomplete accommodation information from third-party providers on booking platforms.

**Method:** The researchers tested two LLMs, Mistral 7B (fine-tuned with QLoRA) and Mixtral 8x7B (with a refined system prompt), on their capacity to generate consistent and unbiased descriptions.

**Result:** Mixtral 8x7B performed better than Mistral 7B in terms of description completeness, precision, and lower hallucination rate, albeit at a higher computational cost.

**Conclusion:** The research offers practical guidance on the trade-offs in deploying LLMs in production environments to achieve higher model quality in accommodation data consistency.

**Abstract:** Online property booking platforms are widely used and rely heavily on
consistent, up-to-date information about accommodation facilities, often
sourced from third-party providers. However, these external data sources are
frequently affected by incomplete or inconsistent details, which can frustrate
users and result in a loss of market. In response to these challenges, we
present an industrial case study involving the integration of Large Language
Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by
FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,
fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.
Both models were assessed based on their ability to generate consistent and
homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B
outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision
(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet
more concise content (249 vs. 277 words on average). However, this came at a
significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB
and $0.16/hour for Mistral 7B. Our findings provide practical insights into the
trade-offs between model quality and resource efficiency, offering guidance for
deploying LLMs in production environments and demonstrating their effectiveness
in enhancing the consistency and reliability of accommodation data.

</details>


### [2] [ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing](https://arxiv.org/abs/2507.22911)
*Jinzhi Wang,Qingke Peng,Haozhou Li,Zeyuan Zeng,Qinfeng Song,Kaixuan Yang,Jiangbo Zhang,Yaoying Wang,Ruimeng Li,Biyi Zhou*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Electric power marketing customer service plays a critical role in addressing
inquiries, complaints, and service requests. However, current systems, such as
China's 95598 hotline, often struggle with slow response times, inflexible
procedures, and limited accuracy in domain-specific tasks. While large language
models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,
they lack the domain expertise and empathy required in this field. To bridge
this gap, we introduce ElectriQ, the first benchmark designed to evaluate and
enhance LLMs in electric power marketing scenarios. ElectriQ consists of a
dialogue dataset covering six key service categories and introduces four
evaluation metrics: professionalism, popularity, readability, and
user-friendliness. We further incorporate a domain-specific knowledge base and
propose a knowledge augmentation method to boost model performance. Experiments
on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and
augmented, can surpass GPT-4o in terms of professionalism and
user-friendliness. ElectriQ establishes a comprehensive foundation for
developing LLMs tailored to the needs of power marketing services.

</details>


### [3] [A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms](https://arxiv.org/abs/2507.22912)
*Navid Yazdanjue,Morteza Rakhshaninejad,Hossein Yazdanjouei,Mohammad Sadegh Khorshidi,Mikko S. Niemela,Fang Chen,Amir H. Gandomi*

Main category: cs.CL

> 提出了一种高效的非法市场内容分类模型，并在多个数据集上进行了验证，证明了模型的有效性和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 由于标注数据有限、非法语言不断变化以及在线来源结构的异质性，对非法市场内容进行检测和分类极具挑战。因此，开发一种新的方法来有效地解决这一问题是必要的。

**Method:** 提出了一种结合微调语言模型和半监督集成学习策略的分层分类框架，用于检测和分类来自不同平台的非法市场内容。框架使用了在深度和暗网页面、Telegram频道、Subreddits和Pastebin帖子上微调的ModernBERT语言模型来提取语义表示，同时，框架还结合了手工特征如文档结构、嵌入式模式（包括比特币地址、电子邮件和IP）和元数据。分类流水线分为两个阶段：第一阶段使用XGBoost、随机森林和SVM的半监督集成通过基于熵的加权投票来检测与销售相关的文档；第二阶段进一步将这些文档分类为药物、武器或凭证销售。

**Result:** 模型在多数据集上的表现超出了多个现有模型，显示出了强劲的泛化能力、在有限监督下的鲁棒性和在现实世界中检测非法内容的有效性。

**Conclusion:** 实验结果表明，该模型在多个数据集上的表现优于包括BERT、ModernBERT、DarkBERT、ALBERT、Longformer和BigBird在内的多个基线模型，在准确率、F1分数和TMCC方面分别达到了0.96489、0.93467和0.95388，展示了模型在有限监督下的强泛化能力和现实世界非法内容检测的有效性。

**Abstract:** Illegal marketplaces have increasingly shifted to concealed parts of the
internet, including the deep and dark web, as well as platforms such as
Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of
illicit goods including drugs, weapons, and stolen credentials. Detecting and
categorizing such content remains challenging due to limited labeled data, the
evolving nature of illicit language, and the structural heterogeneity of online
sources. This paper presents a hierarchical classification framework that
combines fine-tuned language models with a semi-supervised ensemble learning
strategy to detect and classify illicit marketplace content across diverse
platforms. We extract semantic representations using ModernBERT, a transformer
model for long documents, finetuned on domain-specific data from deep and dark
web pages, Telegram channels, Subreddits, and Pastebin pastes to capture
specialized jargon and ambiguous linguistic patterns. In addition, we
incorporate manually engineered features such as document structure, embedded
patterns including Bitcoin addresses, emails, and IPs, and metadata, which
complement language model embeddings. The classification pipeline operates in
two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random
Forest, and SVM with entropy-based weighted voting to detect sales-related
documents. The second stage further classifies these into drug, weapon, or
credential sales. Experiments on three datasets, including our multi-source
corpus, DUTA, and CoDA, show that our model outperforms several baselines,
including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The
model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of
0.95388, demonstrating strong generalization, robustness under limited
supervision, and effectiveness in real-world illicit content detection.

</details>


### [4] [A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models](https://arxiv.org/abs/2507.22913)
*Jinyu Liu,Xiaoying Song,Diana Zhang,Jason Thomale,Daqing He,Lingzi Hong*

Main category: cs.CL

> 研究提出了一种混合框架，通过结合机器学习模型和大型语言模型，改善图书主题分析，获得了更好的预测和词汇一致性结果。

<details>
  <summary>Details</summary>

**Motivation:** 旨在利用传统机器学习模型和大型语言模型的优势来改善图书主题分析，解决LLM过度生成和幻觉的问题。

**Method:** 提出了一种结合基于嵌入的传统机器学习模型与大型语言模型（LLM）的混合框架。该方法使用机器学习模型（1）预测最优的LCSH标签数量以指导LLM预测，以及（2）使用实际的LCSH术语修正LLM生成的预测术语，以减少生成错误。

**Result:** 实验结果表明，通过提供初始预测以引导LLM生成和实施后期编辑，可以获得控制良好、词汇一致的结果。

**Conclusion:** 混合框架通过结合ML和LLM的优势改善了LLM在图书主题分析任务中的性能。

**Abstract:** Providing subject access to information resources is an essential function of
any library management system. Large language models (LLMs) have been widely
used in classification and summarization tasks, but their capability to perform
subject analysis is underexplored. Multi-label classification with traditional
machine learning (ML) models has been used for subject analysis but struggles
with unseen cases. LLMs offer an alternative but often over-generate and
hallucinate. Therefore, we propose a hybrid framework that integrates
embedding-based ML models with LLMs. This approach uses ML models to (1)
predict the optimal number of LCSH labels to guide LLM predictions and (2)
post-edit the predicted terms with actual LCSH terms to mitigate
hallucinations. We experimented with LLMs and the hybrid framework to predict
the subject terms of books using the Library of Congress Subject Headings
(LCSH). Experiment results show that providing initial predictions to guide LLM
generations and imposing post-edits result in more controlled and
vocabulary-aligned outputs.

</details>


### [5] [Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs](https://arxiv.org/abs/2507.22914)
*Victor Eiti Yamamoto,Hideaki Takeda*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Knowledge graphs (KGs) are powerful tools for representing and reasoning over
structured information. Their main components include schema, identity, and
context. While schema and identity matching are well-established in ontology
and entity matching research, context matching remains largely unexplored. This
is particularly important because real-world KGs often vary significantly in
source, size, and information density - factors not typically represented in
the datasets on which current entity matching methods are evaluated. As a
result, existing approaches may fall short in scenarios where diverse and
complex contexts need to be integrated.
  To address this gap, we propose a novel KG integration method consisting of
label matching and triple matching. We use string manipulation, fuzzy matching,
and vector similarity techniques to align entity and predicate labels. Next, we
identify mappings between triples that convey comparable information, using
these mappings to improve entity-matching accuracy. Our approach demonstrates
competitive performance compared to leading systems in the OAEI competition and
against supervised methods, achieving high accuracy across diverse test cases.
Additionally, we introduce a new dataset derived from the benchmark dataset to
evaluate the triple-matching step more comprehensively.

</details>


### [6] [Theoretical Foundations and Mitigation of Hallucination in Large Language Models](https://arxiv.org/abs/2507.22915)
*Esmail Gumaan*

Main category: cs.CL

> 论文区分了内在和外在幻觉，提出了幻觉风险的定义，并探讨了幻觉的检测和缓解策略，如检索增强生成、幻觉感知微调、logit校准和事实验证模块的引入。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLM）中幻觉问题的存在，即生成的内容与输入或现实世界事实不符，需要得到严格的处理，包括形式定义和理论分析。

**Method:** 本论文采用学习理论框架（例如PAC-Bayes和Rademacher复杂度）来界定和分析大型语言模型中的幻觉风险，并探讨了幻觉的检测和缓解策略。

**Result:** 论文提出了一个综合的幻觉检测和缓解工作流程，并推荐了一些评估协议以量化和减少幻觉。

**Conclusion:** 这项工作为解决大型语言模型中的幻觉问题提供了理论基础和实践指南。

**Abstract:** Hallucination in Large Language Models (LLMs) refers to the generation of
content that is not faithful to the input or the real-world facts. This paper
provides a rigorous treatment of hallucination in LLMs, including formal
definitions and theoretical analyses. We distinguish between intrinsic and
extrinsic hallucinations, and define a \textit{hallucination risk} for models.
We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes
and Rademacher complexity). We then survey detection strategies for
hallucinations, such as token-level uncertainty estimation, confidence
calibration, and attention alignment checks. On the mitigation side, we discuss
approaches including retrieval-augmented generation, hallucination-aware
fine-tuning, logit calibration, and the incorporation of fact-verification
modules. We propose a unified detection and mitigation workflow, illustrated
with a diagram, to integrate these strategies. Finally, we outline evaluation
protocols for hallucination, recommending datasets, metrics, and experimental
setups to quantify and reduce hallucinations. Our work lays a theoretical
foundation and practical guidelines for addressing the crucial challenge of
hallucination in LLMs.

</details>


### [7] [Reading Between the Timelines: RAG for Answering Diachronic Questions](https://arxiv.org/abs/2507.22917)
*Kwun Hang Lau,Ruiyuan Zhang,Weijie Shi,Xiaofang Zhou,Xiaojun Cheng*

Main category: cs.CL

> 本文提出了一个能够在检索增强生成系统中融合时间逻辑的新框架，适用于处理长期的查询。实验表明，我们的方法显着提高了问答任务中的答案准确率。

<details>
  <summary>Details</summary>

**Motivation:** 现有基于语义检索的方法无法有效处理长期查询，因为它们无法收集在指定时间段内主题相关且时间连贯的证据。我们提出了一个新的框架来解决这一问题。

**Method:** 我们的方法首先将用户的查询分解为核心主题和时间窗口。然后，使用一种专门的检索器，它在语义匹配和时间相关性之间进行校准，确保收集到的证据集在整个查询时间段内连贯一致。

**Result:** 通过ADQAB实验结果显示，我们的方法比标准RAG实现的准确率提高了13%到27%。

**Conclusion:** 研究为RAG系统处理复杂、现实世界问题提供了有效途径，可以通过对不同时间段内的信息进行细致和演化的分析来补充静态知识。

**Abstract:** While Retrieval-Augmented Generation (RAG) excels at injecting static,
factual knowledge into Large Language Models (LLMs), it exhibits a critical
deficit in handling longitudinal queries that require tracking entities and
phenomena across time. This blind spot arises because conventional,
semantically-driven retrieval methods are not equipped to gather evidence that
is both topically relevant and temporally coherent for a specified duration. We
address this challenge by proposing a new framework that fundamentally
redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by
disentangling a user's query into its core subject and its temporal window. It
then employs a specialized retriever that calibrates semantic matching against
temporal relevance, ensuring the collection of a contiguous evidence set that
spans the entire queried period. To enable rigorous evaluation of this
capability, we also introduce the Analytical Diachronic Question Answering
Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus
of real and synthetic financial news. Empirical results on ADQAB show that our
approach yields substantial gains in answer accuracy, surpassing standard RAG
implementations by 13% to 27%. This work provides a validated pathway toward
RAG systems capable of performing the nuanced, evolutionary analysis required
for complex, real-world questions. The dataset and code for this study are
publicly available at https://github.com/kwunhang/TA-RAG.

</details>


### [8] [Semantic Convergence: Investigating Shared Representations Across Scaled LLMs](https://arxiv.org/abs/2507.22918)
*Daniel Son,Sanjana Rathore,Andrew Rufail,Adrian Simon,Daniel Zhang,Soham Dave,Cole Blondin,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

> 研究Gemma-2语言模型的特征普遍性，发现不同规模的模型在某些层面上能够收敛到相似的内部概念，表明大规模语言模型的普遍性有助于跨模型解释。

<details>
  <summary>Details</summary>

**Motivation:** 研究Gemma-2语言模型（Gemma-2-2B和Gemma-2-9B）中的特征普遍性，探讨规模相差四倍的模型是否仍然汇聚到相似的内部概念。

**Method:** 利用稀疏自编码器(SAE)字典学习流程，对每个模型的残差流激活进行SAE处理，通过激活关联对所得的单义特征进行对齐，并使用SVCCA和RSA比较匹配的特征空间。

**Result:** 中间层的特征重叠最强，而早期和晚期层则显示出较少的相似性。初步实验将分析从单个标记扩展到多个标记子空间，发现语义相似的子空间与语言模型的交互方式相似。

**Conclusion:** 这些结果强化了大规模语言模型能够将世界分割成广泛相似且可解释特征的观点，即使存在规模差异，这加强了普遍性作为跨模型可解释性的基础。

**Abstract:** We investigate feature universality in Gemma-2 language models (Gemma-2-2B
and Gemma-2-9B), asking whether models with a four-fold difference in scale
still converge on comparable internal concepts. Using the Sparse Autoencoder
(SAE) dictionary-learning pipeline, we utilize SAEs on each model's
residual-stream activations, align the resulting monosemantic features via
activation correlation, and compare the matched feature spaces with SVCCA and
RSA. Middle layers yield the strongest overlap, while early and late layers
show far less similarity. Preliminary experiments extend the analysis from
single tokens to multi-token subspaces, showing that semantically similar
subspaces interact similarly with language models. These results strengthen the
case that large language models carve the world into broadly similar,
interpretable features despite size differences, reinforcing universality as a
foundation for cross-model interpretability.

</details>


### [9] [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
*Qixuan Hu,Xumou Zhang,Jinman Kim,Florence Bourgeois,Adam G. Dunn*

Main category: cs.CL

> 研究通过仅使用注册前信息预测临床试验中的严重不良事件（SAE）结果，开发了一种基于预训练语言模型的方法，取得了较好的预测准确性，表明该方法在改善试验设计和警示预期与报告的安全结果之间的不一致性方面具有潜力。

<details>
  <summary>Details</summary>

**Motivation:** 准确估计预期的安全结果，可以帮助临床试验的设计避免终止，并限制参与者的风险暴露。研究的动机是评估仅使用注册前信息来预测临床试验中严重不良事件（SAE）结果的方法。

**Method:** 该研究使用了转移学习方法，利用预训练语言模型（如ClinicalT5，BioBERT）进行特征抽取，并结合下游模型进行预测。为了在超过本地化语言模型输入限制的长试验文本中保持语义表示，开发了一种滑动窗口方法进行嵌入抽取。研究中开发了两个预测模型：一个分类器用来预测实验组是否有更高的严重不良事件（SAE）发生率，另一个回归模型用来预测对照组的严重不良事件发生率比例。

**Result:** 使用ClinicalT5+Transformer+MLP的最佳模型预测哪个实验组有更高的严重不良事件发生率的比例时，AUC达到了77.6%。预测对照组研究参与者中经历严重不良事件比例时，相同的模型达到了18.6%的RMSE。滑动窗口方法在性能上始终优于没有使用滑动窗口的方法；在12个分类器中，平均AUC绝对值增加了2.00%；在12个回归模型中，平均RMSE绝对值减少了1.58%。

**Conclusion:** ClinicalTrials.gov上可用的总结结果数据仍未充分利用。在试验开始前估计结果的能力为改进试验设计和警示预期与报告的安全结果之间的不一致性提供了机会。

**Abstract:** Objectives: With accurate estimates of expected safety results, clinical
trials could be designed to avoid terminations and limit exposing participants
to unnecessary risks. We evaluated methods for predicting serious adverse event
(SAE) results in clinical trials using information only from their
registrations prior to the trial. Material and Methods: We analysed 22,107
two-arm parallel interventional clinical trials from ClinicalTrials.gov with
structured summary results. Two prediction models were developed: a classifier
predicting will experimental arm have higher SAE rates (area under the receiver
operating characteristic curve; AUC) than control arm, and a regression model
to predict the proportion of SAEs in control arms (root mean squared error;
RMSE). A transfer learning approach using pretrained language models (e.g.,
ClinicalT5, BioBERT) was used for feature extraction, combined with downstream
model for prediction. To maintain semantic representation in long trial texts
exceeding localised language model input limits, a sliding window method was
developed for embedding extraction. Results: The best model
(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a
higher proportion of patients with SAEs. When predicting proportion of
participants experiencing SAE in the control arm, the same model achieved RMSE
of 18.6%. The sliding window approach consistently outperformed methods without
it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across
12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:
Summary results data available at ClinicalTrials.gov remains underutilised. The
potential to estimate results of trials before they start is an opportunity to
improve trial design and flag discrepancies between expected and reported
safety results.

</details>


### [10] [Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey](https://arxiv.org/abs/2507.22920)
*Jindong Li,Yali Fu,Jiahong Liu,Linxiao Cao,Wei Ji,Menglin Yang,Irwin King,Ming-Hsuan Yang*

Main category: cs.CL

> The paper presents a comprehensive analysis of vector quantization techniques for LLMs, covering algorithmic principles, challenges, and impacts on LLM performance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is the lack of a systematic study that analyses vector quantization techniques for transforming continuous multimodal data into discrete representations compatible with LLM architectures, to leverage computational efficiency.

**Method:** This paper introduces a structured taxonomy and analysis of discrete tokenization methods for LLMs, focusing on 8 vector quantization (VQ) techniques. It explores the algorithmic principles, training dynamics, and integration challenges, as well as discussing the impact of these techniques on alignment, reasoning, and generation performance in LLM-based systems.

**Result:** Identifies key challenges such as codebook collapse and unstable gradient estimation, and explores emerging research directions like dynamic and task-adaptive quantization methods.

**Conclusion:** This survey serves as a foundational reference for developing efficient and generalizable multimodal systems by bridging the gap between traditional vector quantization and LLM applications.

**Abstract:** The rapid advancement of large language models (LLMs) has intensified the
need for effective mechanisms to transform continuous multimodal data into
discrete representations suitable for language-based processing. Discrete
tokenization, with vector quantization (VQ) as a central approach, offers both
computational efficiency and compatibility with LLM architectures. Despite its
growing importance, there is a lack of a comprehensive survey that
systematically examines VQ techniques in the context of LLM-based systems. This
work fills this gap by presenting the first structured taxonomy and analysis of
discrete tokenization methods designed for LLMs. We categorize 8 representative
VQ variants that span classical and modern paradigms and analyze their
algorithmic principles, training dynamics, and integration challenges with LLM
pipelines. Beyond algorithm-level investigation, we discuss existing research
in terms of classical applications without LLMs, LLM-based single-modality
systems, and LLM-based multimodal systems, highlighting how quantization
strategies influence alignment, reasoning, and generation performance. In
addition, we identify key challenges including codebook collapse, unstable
gradient estimation, and modality-specific encoding constraints. Finally, we
discuss emerging research directions such as dynamic and task-adaptive
quantization, unified tokenization frameworks, and biologically inspired
codebook learning. This survey bridges the gap between traditional vector
quantization and modern LLM applications, serving as a foundational reference
for the development of efficient and generalizable multimodal systems. A
continuously updated version is available at:
https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.

</details>


### [11] [Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers](https://arxiv.org/abs/2507.22921)
*Lee Harris*

Main category: cs.CL

> 本文提出了语言模型链（LMC）算法，通过多阶段级联集合语言模型提高了预测的速度和准确性，并减少了幻觉现象，尤其在从医疗文档中提取患者出生日期的应用中效果显著。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决语言模型捕捉文本中复杂关系时存在的两大问题：计算成本高昂以及生成不存在信息（幻觉）。特别是在信息不正确的情况下，前期投入的资源将被浪费。为解决这一问题，作者提出了LMC算法。

**Method:** 本文提出了一种称为语言模型链（LMC）的新算法，通过多阶段级联集合语言模型来提高预测速度和准确性，并减少幻觉现象。具体来说，给定提示下的语言模型响应只有在其候选答案集中存在时才是正确的。对于错误响应对应的文字，则将其输入到一个更具有预测能力但速度较慢的语言模型中。此过程针对多个语言模型重复进行，直到关于文本的所有预测都正确为止。

**Result:** 通过使用LMC算法从医疗文档中提取患者的出生日期，研究发现与单独使用语言模型相比，多阶段级联集合语言模型显著提高了预测的速度和准确性，同时大幅减少了相应的幻觉现象。

**Conclusion:** 本文认为LMC算法对知识抽取领域有重大贡献，其进一步研究的可能性非常值得探索。

**Abstract:** Language models can capture complex relationships in given text, but these
are notorious for being costly and for producing information that does not
exist (i.e., hallucinations). Furthermore, the resources invested into
producing this information would be wasted if it were incorrect. We address
these issues by proposing, implementing, and applying the Language Model Chain
(LMC) algorithm. In this, a language model's response to a given prompt about
given text is only correct if it exists in the collection of possible (i.e.,
candidate) answers, and text corresponding to incorrect responses is fed into a
more predictive (but slower) language model. This process is repeated for a
collection of language models, or until all predictions about the text are
correct. We used the LMC algorithm to extract patient dates of birth from
medical documents, and combining a collection of language models in a
multi-stage cascade significantly increased prediction speed and accuracy over
individual language models, while greatly reducing the number of corresponding
hallucinations. We believe that the novel LMC algorithm significantly
contributes to the knowledge extraction field, and that this should be explored
much further in the future.

</details>


### [12] [Predicting stock prices with ChatGPT-annotated Reddit sentiment](https://arxiv.org/abs/2507.22922)
*Mateusz Kmak,Kamil Chmurzyński,Kamil Matejuk,Paweł Kotzbach,Jan Kocoń*

Main category: cs.CL

> 本研究分析了两个公司GameStop和AMC在Reddit上的讨论情绪，并使用了三种不同的情绪分析模型。结果表明，社交媒体情绪与股价的相关性较弱，但评论量和谷歌搜索趋势显示出更强的预测信号。这表明零售投资者行为的复杂性，传统的情绪分析可能无法完全捕捉市场移动的在线讨论中的细微差别。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于2021年GameStop短挤压事件引发的零售投资者在社交媒体上的活动激增，这篇论文探讨了社交媒体讨论中提取的情绪是否能有意义地预测股市走势。

**Method:** 本研究聚焦于Reddit的r/wallstreetbets板块，分析了GameStop (GME) 和AMC Entertainment (AMC)两家公司的社交媒体情绪。研究采用了两种现有文本情绪分析方法，并引入了一种基于ChatGPT注释和微调的RoBERTa模型，以更好地解读社交媒体中普遍存在的非正式语言和表情符号。使用相关性和因果性指标来确定这些模型的预测能力。

**Result:** 研究结果表明，社交媒体情绪与股价仅有弱相关性。与此相反，如评论量和谷歌搜索趋势等更简单的指标显示出更强的预测信号。

**Conclusion:** 这些结果强调了零售投资者行为的复杂性，并表明传统的情感分析可能无法完全捕捉到市场走势的在线讨论中的细微差异。

**Abstract:** The surge of retail investor activity on social media, exemplified by the
2021 GameStop short squeeze, raised questions about the influence of online
sentiment on stock prices. This paper explores whether sentiment derived from
social media discussions can meaningfully predict stock market movements. We
focus on Reddit's r/wallstreetbets and analyze sentiment related to two
companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's
role, we employ two existing text-based sentiment analysis methods and
introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model
designed to better interpret the informal language and emojis prevalent in
social media discussions. We use correlation and causality metrics to determine
these models' predictive power. Surprisingly, our findings suggest that social
media sentiment has only a weak correlation with stock prices. At the same
time, simpler metrics, such as the volume of comments and Google search trends,
exhibit stronger predictive signals. These results highlight the complexity of
retail investor behavior and suggest that traditional sentiment analysis may
not fully capture the nuances of market-moving online discussions.

</details>


### [13] [How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting](https://arxiv.org/abs/2507.22923)
*Aman Gupta,Yingying Zhuang,Zhou Yu,Ziji Zhang,Anurag Beniwal*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Despite advances in the multilingual capabilities of Large Language Models
(LLMs), their performance varies substantially across different languages and
tasks. In multilingual retrieval-augmented generation (RAG)-based systems,
knowledge bases (KB) are often shared from high-resource languages (such as
English) to low-resource ones, resulting in retrieved information from the KB
being in a different language than the rest of the context. In such scenarios,
two common practices are pre-translation to create a mono-lingual prompt and
cross-lingual prompting for direct inference. However, the impact of these
choices remains unclear. In this paper, we systematically evaluate the impact
of different prompt translation strategies for classification tasks with
RAG-enhanced LLMs in multilingual systems. Experimental results show that an
optimized prompting strategy can significantly improve knowledge sharing across
languages, therefore improve the performance on the downstream classification
task. The findings advocate for a broader utilization of multilingual resource
sharing and cross-lingual prompt optimization for non-English languages,
especially the low-resource ones.

</details>


### [14] [Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers](https://arxiv.org/abs/2507.22924)
*Brittney Exline,Melanie Duffin,Brittany Harbison,Chrissa da Gomez,David Joyner*

Main category: cs.CL

> 本论文探讨在线美国计算机课程中，英语母语者与非母语者在同行反馈体验三个指标上的差异，并揭示了语言背景在同行反馈体验中的作用。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于越来越多的国际学生参加在线课程，并且很多学生在学习语言非母语的英语课程，作者希望通过研究语言背景在同行反馈体验中的作用，来优化在线教育中的互动和反馈机制。

**Method:** 本论文采用基于Twitter-roBERTa的模型分析来自和给随机抽取的500名学生的同行评审的语气，并将语气评分和同行反馈评价与学生的语言背景相关联。

**Result:** 研究结果表明，母语为英语的学生对反馈的评价较低，而非英语背景的学生写出的评价更加积极但接收到的反馈语气不太积极。当控制性别和年龄因素后，语言背景显示出一定程度的复杂且微妙的作用。

**Conclusion:** 语言背景对在线同行反馈体验具有微妙但复杂的影响。非英语背景学生写给同伴的反馈更积极，但其收到的反馈语气评分较低，这表明语言因素在同行反馈过程中扮演了重要角色。

**Abstract:** Graduate-level CS programs in the U.S. increasingly enroll international
students, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.
students. Many of these students take online courses, where peer feedback is
used to engage students and improve pedagogy in a scalable manner. Since these
courses are conducted in English, many students study in a language other than
their first. This paper examines how native versus non-native English speaker
status affects three metrics of peer feedback experience in online U.S.-based
computing courses. Using the Twitter-roBERTa-based model, we analyze the
sentiment of peer reviews written by and to a random sample of 500 students. We
then relate sentiment scores and peer feedback ratings to students' language
background. Results show that native English speakers rate feedback less
favorably, while non-native speakers write more positively but receive less
positive sentiment in return. When controlling for sex and age, significant
interactions emerge, suggesting that language background plays a modest but
complex role in shaping peer feedback experiences.

</details>


### [15] [Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents](https://arxiv.org/abs/2507.22925)
*Haoran Sun,Shaoning Zeng*

Main category: cs.CL

> 本文提出了一种层次记忆结构，通过多层次的组织和位置索引编码，提高了大型语言模型的长期记忆推理能力，资源利用更高效，比五种基准方法有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 长期记忆力是影响大型语言模型代理（LLM Agent）推理能力的关键因素之一。尽管在记忆存储和检索方面已经取得了进展，但现有的方法在结构化记忆组织和检索效率上仍存在不足。为了应对这些问题，我们提出了这种新架构。

**Method:** 我们提出了一种层次记忆（H-MEM）架构，该架构根据语义抽象的程度以多层次方式组织和更新记忆。每个记忆向量都嵌入了一个位置索引编码，指向其在下一层的语义相关子记忆。在推理阶段，索引路由机制使得可以在不进行详尽相似性计算的情况下进行高效、逐层检索。

**Result:** 我们在LoCoMo数据集上的五个任务设置中进行了实验，结果表明，我们的方法在长期对话场景下，其表现优于五种基准方法，证明了其有效性。

**Conclusion:** 提出的H-MEM架构在长期记忆的组织和检索方面，为大型语言模型提供了更有效的解决方案，在具体对话任务中，其性能优于现有方法。

**Abstract:** Long-term memory is one of the key factors influencing the reasoning
capabilities of Large Language Model Agents (LLM Agents). Incorporating a
memory mechanism that effectively integrates past interactions can
significantly enhance decision-making and contextual coherence of LLM Agents.
While recent works have made progress in memory storage and retrieval, such as
encoding memory into dense vectors for similarity-based search or organizing
knowledge in the form of graph, these approaches often fall short in structured
memory organization and efficient retrieval. To address these limitations, we
propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that
organizes and updates memory in a multi-level fashion based on the degree of
semantic abstraction. Each memory vector is embedded with a positional index
encoding pointing to its semantically related sub-memories in the next layer.
During the reasoning phase, an index-based routing mechanism enables efficient,
layer-by-layer retrieval without performing exhaustive similarity computations.
We evaluate our method on five task settings from the LoCoMo dataset.
Experimental results show that our approach consistently outperforms five
baseline methods, demonstrating its effectiveness in long-term dialogue
scenarios.

</details>


### [16] [Multi-Relation Extraction in Entity Pairs using Global Context](https://arxiv.org/abs/2507.22926)
*Nilesh,Atul Gupta,Avinash C Panday*

Main category: cs.CL

> This paper proposes a new approach to document-level relation extraction by capturing entities' positions across the entire document, rather than focusing only on their immediate context, improving relational accuracy and offering both theoretical advancements and practical applications in NLP.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitation of previous approaches in document-level relation extraction where entities are considered only in the sentences they are mentioned, failing to capture the complete document context necessary for accurate relation extraction.

**Method:** The proposed method introduces a novel input embedding approach that captures the positions of mentioned entities throughout the document, treating entities as standalone segments independent of their positions, thus leveraging global relationships and multi-sentence reasoning.

**Result:** The experimental results on datasets like DocRED and Re-DocRED showed improved performance in predicting the relationships between entities across an entire document.

**Conclusion:** The research not only advances the theoretical understanding of global context modeling in document-level relation extraction but also has practical implications for enhancing real-world NLP applications that require comprehensive entity-level insights.

**Abstract:** In document-level relation extraction, entities may appear multiple times in
a document, and their relationships can shift from one context to another.
Accurate prediction of the relationship between two entities across an entire
document requires building a global context spanning all relevant sentences.
Previous approaches have focused only on the sentences where entities are
mentioned, which fails to capture the complete document context necessary for
accurate relation extraction. Therefore, this paper introduces a novel input
embedding approach to capture the positions of mentioned entities throughout
the document rather than focusing solely on the span where they appear. The
proposed input encoding approach leverages global relationships and
multi-sentence reasoning by representing entities as standalone segments,
independent of their positions within the document. The performance of the
proposed method has been tested on three benchmark relation extraction
datasets, namely DocRED, Re-DocRED, and REBEL. The experimental results
demonstrated that the proposed method accurately predicts relationships between
entities in a document-level setting. The proposed research also has
theoretical and practical implications. Theoretically, it advances global
context modeling and multi-sentence reasoning in document-level relation
extraction. Practically, it enhances relationship detection, enabling improved
performance in real-world NLP applications requiring comprehensive entity-level
insights and interpretability.

</details>


### [17] [PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation](https://arxiv.org/abs/2507.22927)
*Zhehao Tan,Yihan Jiao,Dan Yang,Lei Liu,Jie Feng,Duolin Sun,Yue Shen,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

> 该研究引入了Placeholder-RAG-Benchmark，这是一个多层级细致的评估框架，用于系统化地评估RAG系统中大语言模型对文档的利用能力。通过创新的占位符方法，该框架能更细致地理解LLM在其处理外部知识时的作用。该研究揭示了代表性LLM在RAG系统中的生成能力存在的局限，特别是在错误弹性和上下文忠实度方面。

<details>
  <summary>Details</summary>

**Motivation:** 该论文的动机是改进对RAG系统中大语言模型能力的评估，特别是在文档利用能力方面的系统化和细分评估，现有的基准测试往往没有全面考虑这些方面。

**Method:** 提出了Placeholder-RAG-Benchmark，通过多层级的评估维度，包括多层级过滤能力、组合能力和参考推理能力，以及一种创新的占位符方法，来分解LLM的参数化知识和外部知识的贡献。

**Result:** 实验表明，代表性LLMs在RAG系统生成能力方面存在局限性，特别是在错误弹性和保持上下文忠实度方面。

**Conclusion:** Placeholder-RAG-Benchmark提供了一个可复制的框架，用于更可靠和高效地开发RAG系统。

**Abstract:** Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating external knowledge, where the LLM's ability to generate responses
based on the combination of a given query and retrieved documents is crucial.
However, most benchmarks focus on overall RAG system performance, rarely
assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects
such as noise robustness, but lack a systematic and granular evaluation
framework on document utilization. To this end, we introduce
\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,
emphasizing the following progressive dimensions: (1) multi-level filtering
abilities, (2) combination abilities, and (3) reference reasoning. To provide a
more nuanced understanding of LLMs' roles in RAG systems, we formulate an
innovative placeholder-based approach to decouple the contributions of the
LLM's parametric knowledge and the external knowledge. Experiments demonstrate
the limitations of representative LLMs in the RAG system's generation
capabilities, particularly in error resilience and context faithfulness. Our
benchmark provides a reproducible framework for developing more reliable and
efficient RAG systems. Our code is available in
https://github.com/Alipay-Med/PRGB.

</details>


### [18] [How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding](https://arxiv.org/abs/2507.22928)
*Xi Chen,Aske Plaat,Niki van Stein*

Main category: cs.CL

> 通过分析chain-of-thought (CoT)提示对不同规模模型的影响，研究揭示了CoT提示对大型语言模型内部推理过程的影响，并验证了该提示作为一种结构化提示方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 探究chain-of-thought (CoT)提示生成的思维链条是否真实反映了大型语言模型内部的推理过程。

**Method:** 结合稀疏自编码器与激活修补技术，从处理GSM8K数学问题的Pythia-70M和Pythia-2.8B模型中提取单义特征，并对比chain-of-thought (CoT)提示和普通提示下的表现。通过交换少量CoT推理特征到普通提示运行中来测试其对模型性能的影响。

**Result:** 向2.8B模型的小批量普通提示运行中引入少量CoT推理特征会大幅提高答案的对数概率，但在70M模型中则没有显著效果，表明了明显的规模阈值。CoT提示在较大的模型中引发了更高的激活稀疏性和特征可解释性得分，显示出更加模块化的内部计算。例如，模型生成正确答案的信心从1.2提升至4.3。

**Conclusion:** 研究结果表明，CoT提示可以在高容量的大型语言模型中诱导出更可解释的内部结构，证实其作为结构化提示方法的有效性。

**Abstract:** Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on
multi-step tasks, yet whether the generated "thoughts" reflect the true
internal reasoning process is unresolved. We present the first feature-level
causal study of CoT faithfulness. Combining sparse autoencoders with activation
patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B
while they tackle GSM8K math problems under CoT and plain (noCoT) prompting.
Swapping a small set of CoT-reasoning features into a noCoT run raises answer
log-probabilities significantly in the 2.8B model, but has no reliable effect
in 70M, revealing a clear scale threshold. CoT also leads to significantly
higher activation sparsity and feature interpretability scores in the larger
model, signalling more modular internal computation. For example, the model's
confidence in generating correct answers improves from 1.2 to 4.3. We introduce
patch-curves and random-feature patching baselines, showing that useful CoT
information is not only present in the top-K patches but widely distributed.
Overall, our results indicate that CoT can induce more interpretable internal
structures in high-capacity LLMs, validating its role as a structured prompting
method.

</details>


### [19] [EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow](https://arxiv.org/abs/2507.22929)
*Xiaoyu Pan,Yang Bai,Ke Zou,Yang Zhou,Jun Zhou,Huazhu Fu,Yih-Chung Tham,Yong Liu*

Main category: cs.CL

> 本研究聚焦于眼科领域的大型语言模型，介绍了一种新的基准——EH-Benchmark，用以评估和减轻这些模型中的误解。

<details>
  <summary>Details</summary>

**Motivation:** 现有的医疗基准无法有效评估大型语言模型中的各种误解，也无法提供有效的解决方案。为了解决这些问题，我们引入了EH-Benchmark，一个新型的、旨在评估眼科领域大型语言模型误解的基准。

**Method:** 我们提出了一种基于多智能体的三阶段框架，分别是知识水平检索阶段、任务水平案例研究阶段和结果水平验证阶段。由于大多数医疗大型语言模型主要依赖语言推理而非视觉处理，这种框架特别适用于减少模型中的误解。

**Result:** 实验结果表明，引入的多阶段框架能够显著提高大型语言模型在眼科疾病诊断中的准确性和可靠性。

**Conclusion:** 实验结果表明，这种多智能体框架显著减少了视觉理解和逻辑推理两种类型的误解，提高了模型的准确性、可解释性和可靠性。

**Abstract:** Medical Large Language Models (MLLMs) play a crucial role in ophthalmic
diagnosis, holding significant potential to address vision-threatening
diseases. However, their accuracy is constrained by hallucinations stemming
from limited ophthalmic knowledge, insufficient visual localization and
reasoning capabilities, and a scarcity of multimodal ophthalmic data, which
collectively impede precise lesion detection and disease diagnosis.
Furthermore, existing medical benchmarks fail to effectively evaluate various
types of hallucinations or provide actionable solutions to mitigate them. To
address the above challenges, we introduce EH-Benchmark, a novel ophthalmology
benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'
hallucinations based on specific tasks and error types into two primary
classes: Visual Understanding and Logical Composition, each comprising multiple
subclasses. Given that MLLMs predominantly rely on language-based reasoning
rather than visual processing, we propose an agent-centric, three-phase
framework, including the Knowledge-Level Retrieval stage, the Task-Level Case
Studies stage, and the Result-Level Validation stage. Experimental results show
that our multi-agent framework significantly mitigates both types of
hallucinations, enhancing accuracy, interpretability, and reliability. Our
project is available at https://github.com/ppxy1/EH-Benchmark.

</details>


### [20] [Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection](https://arxiv.org/abs/2507.22930)
*Shalini Jangra,Suparna De,Nishanth Sastry,Saeed Fadaei*

Main category: cs.CL

> 我们为在线社交媒体中的PII隐私风险研究提供了一个用于检测PII揭示文本的合成数据集。

<details>
  <summary>Details</summary>

**Motivation:** 研究中识别和检索这些风险自我披露的PII数据受制于缺乏开源标签数据集。为了促进关于PII揭示文本检测的可重复研究，我们开发了一种新方法来创建合成数据。

**Method:** 我们开发了一种新方法来创建可以安全共享的PII披露数据的合成等价物，包括建立一个包含19个PII披露类别的分类法，并使用Llama2-7B, Llama3-8B和zephyr-7b-beta三个大型语言模型生成合成PII标注多文本跨度数据集。

**Result:** 我们创建了一个可以用于评估模型性能的合成数据集，该数据集在可重复性、与原始用户无关以及与原始数据不可区分性这三个指标上得到了验证。

**Conclusion:** 我们发布了一个可以用于研究在线社交媒体中PII隐私风险的合成数据集和代码。

**Abstract:** Social platforms such as Reddit have a network of communities of shared
interests, with a prevalence of posts and comments from which one can infer
users' Personal Information Identifiers (PIIs). While such self-disclosures can
lead to rewarding social interactions, they pose privacy risks and the threat
of online harms. Research into the identification and retrieval of such risky
self-disclosures of PIIs is hampered by the lack of open-source labeled
datasets. To foster reproducible research into PII-revealing text detection, we
develop a novel methodology to create synthetic equivalents of PII-revealing
data that can be safely shared. Our contributions include creating a taxonomy
of 19 PII-revealing categories for vulnerable populations and the creation and
release of a synthetic PII-labeled multi-text span dataset generated from 3
text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and
zephyr-7b-beta, with sequential instruction prompting to resemble the original
Reddit posts. The utility of our methodology to generate this synthetic dataset
is evaluated with three metrics: First, we require reproducibility equivalence,
i.e., results from training a model on the synthetic data should be comparable
to those obtained by training the same models on the original posts. Second, we
require that the synthetic data be unlinkable to the original users, through
common mechanisms such as Google Search. Third, we wish to ensure that the
synthetic data be indistinguishable from the original, i.e., trained humans
should not be able to tell them apart. We release our dataset and code at
https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster
reproducible research into PII privacy risks in online social media.

</details>


### [21] [Enhancing RAG Efficiency with Adaptive Context Compression](https://arxiv.org/abs/2507.22931)
*Shuyu Guo,Zhaochun Ren*

Main category: cs.CL

> 提出了一种自适应上下文压缩框架(ACC-RAG)，该框架可以根据查询复杂度动态调整压缩率，优化大语言模型RAG的推理效率而不降低准确性，在保持或改善准确性的同时，推理速度提高了4倍以上。

<details>
  <summary>Details</summary>

**Motivation:** 虽然上下文压缩可以减轻RAG推理成本过高的问题，但是现有的方法使用固定的压缩率，这会导致简单查询过度压缩或复杂查询压缩不足。为了改善这一状况，提出了自适应上下文压缩的方法来优化RAG的效率。

**Method:** 提出了一种名为自适应上下文压缩RAG(ACC-RAG)的框架，该框架能够根据输入复杂度动态调整压缩率，从而在不牺牲准确性的情况下优化推断效率。ACC-RAG结合了用于多粒度嵌入的分层压缩器和上下文选择器，以保持最小的必要信息，类似于人类的略读过程。

**Result:** 在Wikipedia和五个问答数据集上评估ACC-RAG，结果显示其优于固定比率压缩方法，并提升了4倍以上的推理效率，同时保持或提升了准确性。

**Conclusion:** 评估结果表明，ACC-RAG优于固定比率的方法，并在与标准RAG对比时，推理速度至少快了四倍，同时保持或提高了准确性。

**Abstract:** Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external knowledge but incurs significant inference costs due to lengthy
retrieved contexts. While context compression mitigates this issue, existing
methods apply fixed compression rates, over-compressing simple queries or
under-compressing complex ones. We propose Adaptive Context Compression for RAG
(ACC-RAG), a framework that dynamically adjusts compression rates based on
input complexity, optimizing inference efficiency without sacrificing accuracy.
ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with
a context selector to retain minimal sufficient information, akin to human
skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms
fixed-rate methods and matches/unlocks over 4 times faster inference versus
standard RAG while maintaining or improving accuracy.

</details>


### [22] [FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification](https://arxiv.org/abs/2507.22932)
*Baptiste Lefort,Eric Benhamou,Beatrice Guez,Jean-Jacques Ohana,Ethan Setrouk,Alban Etienne*

Main category: cs.CL

> 提出了一种结合LLMs和DRL的层级框架，用于处理混合金融数据，提升投资组合优化效果。该框架实现了优于市场基准的投资回报。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在通过混合金融新闻中的情感信号和传统的市场指标来改进投资组合优化方法。

**Method:** 本论文提出了一种新颖的层级框架，用于结合轻量级大型语言模型（LLMs）和深度强化学习（DRL）进行投资组合优化。这一三层架构利用基础RL代理来处理混合数据，元代理来汇总决策，超级代理根据市场数据和情感分析合并决策。

**Result:** 该框架在2018年至2024年的数据上进行了评估，其在2000年至2017年的训练后，实现了26%的年化回报率和1.2的夏普比率，优于等权重和S&P 500基准。

**Conclusion:** 关键贡献包括可扩展的跨模式集成，用于增强稳定性的层级RL结构，以及开源可复现性。

**Abstract:** This paper presents a novel hierarchical framework for portfolio
optimization, integrating lightweight Large Language Models (LLMs) with Deep
Reinforcement Learning (DRL) to combine sentiment signals from financial news
with traditional market indicators. Our three-tier architecture employs base RL
agents to process hybrid data, meta-agents to aggregate their decisions, and a
super-agent to merge decisions based on market data and sentiment analysis.
Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework
achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming
equal-weighted and S&P 500 benchmarks. Key contributions include scalable
cross-modal integration, a hierarchical RL structure for enhanced stability,
and open-source reproducibility.

</details>


### [23] [Augmented Vision-Language Models: A Systematic Review](https://arxiv.org/abs/2507.22933)
*Anthony C Davis,Burhan Sadiq,Tianmin Shu,Chien-Ming Huang*

Main category: cs.CL

> 本文提出了一种通过结合神经网络和外部符号信息系统来提升视觉语言理解的方法，并通过系统性文献回顾分类了相关技术。

<details>
  <summary>Details</summary>

**Motivation:** 当前的视觉语言机器学习模型虽然表现出色，但存在解释能力差、需要重新训练以整合新信息、资源消耗大以及逻辑推理能力不足的问题。为了解决这些问题，文章提出将神经网络与外部符号信息系统结合，形成神经符号系统，以提升推理和记忆能力。

**Method:** 本文通过系统性文献回顾，旨在分类能够通过与外部符号信息系统交互来提升视觉语言理解的技术。

**Result:** 暂无具体实验结果，因为这是一篇文献综述。

**Conclusion:** 文章确定了一个有希望的方法论，即将强大的预训练视觉语言模型作为核心神经组件，并通过外部系统增强，以实现神经符号集成的优势。

**Abstract:** Recent advances in visual-language machine learning models have demonstrated
exceptional ability to use natural language and understand visual scenes by
training on large, unstructured datasets. However, this training paradigm
cannot produce interpretable explanations for its outputs, requires retraining
to integrate new information, is highly resource-intensive, and struggles with
certain forms of logical reasoning. One promising solution involves integrating
neural networks with external symbolic information systems, forming neural
symbolic systems that can enhance reasoning and memory abilities. These neural
symbolic systems provide more interpretable explanations to their outputs and
the capacity to assimilate new information without extensive retraining.
Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural
component, augmented by external systems, offers a pragmatic approach to
realizing the benefits of neural-symbolic integration. This systematic
literature review aims to categorize techniques through which visual-language
understanding can be improved by interacting with external symbolic information
systems.

</details>


### [24] [Deep Learning Approaches for Multimodal Intent Recognition: A Survey](https://arxiv.org/abs/2507.22934)
*Jingwei Zhao,Yuhua Wen,Qifei Li,Minchi Hu,Yingying Zhou,Jingyao Xue,Junyang Wu,Yingming Gao,Zhengqi Wen,Jianhua Tao,Ya Li*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Intent recognition aims to identify users' underlying intentions,
traditionally focusing on text in natural language processing. With growing
demands for natural human-computer interaction, the field has evolved through
deep learning and multimodal approaches, incorporating data from audio, vision,
and physiological signals. Recently, the introduction of Transformer-based
models has led to notable breakthroughs in this domain. This article surveys
deep learning methods for intent recognition, covering the shift from unimodal
to multimodal techniques, relevant datasets, methodologies, applications, and
current challenges. It provides researchers with insights into the latest
developments in multimodal intent recognition (MIR) and directions for future
research.

</details>


### [25] [Trusted Knowledge Extraction for Operations and Maintenance Intelligence](https://arxiv.org/abs/2507.22935)
*Kathleen Mealey,Jonathan A. Karr Jr.,Priscila Saboia Moreira,Paul R. Brenner,Charles F. Vardeman II*

Main category: cs.CL

> 本文评估了面向航空领域的NLP工具和LLM对设备故障或维护需求的知识提取性能，并讨论了其在关键任务行业应用中的技术成熟度以及面临的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 由于数据保密与数据整合目标的二分法以及NLP工具相对于特定领域知识结构的功能局限性，从组织数据仓库中获取运营情报是一个关键挑战。

**Method:** 论文探讨了知识图谱构建，并将知识提取过程分解为命名实体识别、共指消解、实体链接和关系抽取等组件。作者评估了16种NLP工具与大型语言模型（LLMs）在零样本性能上的表现，并重点关注航空工业中运营和维护情报的可信应用。数据集来源于美国联邦航空管理局关于设备故障或维护需求的公开数据。

**Result:** 观察到NLP和LLM工具在特定专业领域有显著性能限制。

**Conclusion:** 论文最后提供了增强信任的建议，并开源了经过精心整理的数据集以支持进一步的基准测试和评估。

**Abstract:** Deriving operational intelligence from organizational data repositories is a
key challenge due to the dichotomy of data confidentiality vs data integration
objectives, as well as the limitations of Natural Language Processing (NLP)
tools relative to the specific knowledge structure of domains such as
operations and maintenance. In this work, we discuss Knowledge Graph
construction and break down the Knowledge Extraction process into its Named
Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation
Extraction functional components. We then evaluate sixteen NLP tools in concert
with or in comparison to the rapidly advancing capabilities of Large Language
Models (LLMs). We focus on the operational and maintenance intelligence use
case for trusted applications in the aircraft industry. A baseline dataset is
derived from a rich public domain US Federal Aviation Administration dataset
focused on equipment failures or maintenance requirements. We assess the
zero-shot performance of NLP and LLM tools that can be operated within a
controlled, confidential environment (no data is sent to third parties). Based
on our observation of significant performance limitations, we discuss the
challenges related to trusted NLP and LLM tools as well as their Technical
Readiness Level for wider use in mission-critical industries such as aviation.
We conclude with recommendations to enhance trust and provide our open-source
curated dataset to support further baseline testing and evaluation.

</details>


### [26] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)
*Md Talha Mohsin*

Main category: cs.CL

> 本研究比较了五种主流大语言模型（GPT、Claude、Perplexity、Gemini 和 DeepSeek）在金融自然语言处理任务上的性能，发现GPT表现最佳，而Gemini和DeepSeek的输出更具变异性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大语言模型（LLMs）在金融自然语言处理（FinNLP）中展示了卓越的能力，但关于主流LLMs的系统性对比研究仍然较少。鉴于LLMs在金融分析中的快速发展和影响力，此研究旨在填补这一空白。

**Method:** 本研究对五种主流大语言模型（GPT、Claude、Perplexity、Gemini 和 DeepSeek）进行了全面的对比评估。研究采用了 'Magnificent Seven' 技术公司10-K文件，设计了一组领域特定的提示词，并使用三种方法来评估模型性能：人工标注、自动化词汇语义度量（ROUGE、余弦相似度、Jaccard），以及模型行为诊断（提示层级变异和跨模型相似度）。

**Result:** 评估结果显示，GPT提供的答案在连贯性、语义一致性和情境相关性方面表现最佳；接着是Claude和Perplexity。相比之下，Gemini和DeepSeek表现更不稳定，一致性较差。此外，输出的相似度和稳定性在不同的公司和时间段中有所不同，显示出它们对所使用的提示词和源材料更敏感。

**Conclusion:** 尽管GPT在连贯性、语义一致性和情境相关性表现最佳，但模型的表现会因为不同的提示词和源材料而有所不同，这显示出它们在处理金融自然语言处理任务时的具体能力。因此，在选择和使用大语言模型时，需要考虑其对不同公司和时间的敏感性。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide variety of Financial Natural Language Processing (FinNLP) tasks.
However, systematic comparisons among widely used LLMs remain underexplored.
Given the rapid advancement and growing influence of LLMs in financial
analysis, this study conducts a thorough comparative evaluation of five leading
LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the
'Magnificent Seven' technology companies. We create a set of domain-specific
prompts and then use three methodologies to evaluate model performance: human
annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,
Jaccard), and model behavior diagnostics (prompt-level variance and
across-model similarity). The results show that GPT gives the most coherent,
semantically aligned, and contextually relevant answers; followed by Claude and
Perplexity. Gemini and DeepSeek, on the other hand, have more variability and
less agreement. Also, the similarity and stability of outputs change from
company to company and over time, showing that they are sensitive to how
prompts are written and what source material is used.

</details>


### [27] [CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering](https://arxiv.org/abs/2507.22937)
*Jinkun Zhao,Yuanshuai Wang,Xingjian Zhang,Ruibo Chen,Xingchuang Liao,Junle Wang,Lei Huang,Kui Zhang,Wenjun Wu*

Main category: cs.CL

> 本文提出了一种名为CoE-Ops的任务协作框架，该框架在AIOps领域内进行了实现，并在DevOps-EVAL数据集上进行了广泛实验。实验结果显示，CoE-Ops在处理高级AIOps任务时的路由准确性相较于现有方法提升了72%，在DevOps问题解决上的准确率也比单一模型提高了8%，并比大规模的混合专家模型(MoE)准确度高14%。

<details>
  <summary>Details</summary>

**Motivation:** 随着人工智能的快速发展，AIOps作为DevOps中的一个显著范式已经出现。尽管有许多工作专注于提高AIOps不同阶段的性能，但受限于领域的专业知识，单一模型只能处理特定的操作需求。本论文旨在通过结合多个模型提高效率，受到了集成学习和最近LLM训练领域的启发。

**Method:** 本论文提出了一种名为CoE-Ops的专家协作框架，该框架结合了通用大型语言模型的任务分类器。为了提高框架处理问答任务（包括高级任务如代码构建测试等和低级任务如故障分析异常检测等）的能力，引入了检索增强生成机制。

**Result:** 实验结果表明，与现有方法相比，CoE-Ops在高级AIOps任务的路由准确性上提高了72%，在DevOps问题解决上的准确率提高了8%，并且在准确度上比大规模混合专家模型(MoE)高出了14%。

**Conclusion:** 基于实验结果，CoE-Ops显著提升了AIOps领域中任务处理的效率和准确性，特别是在高级任务的路由选择和DevOps问题解决上表现出色。

**Abstract:** With the rapid evolution of artificial intelligence, AIOps has emerged as a
prominent paradigm in DevOps. Lots of work has been proposed to improve the
performance of different AIOps phases. However, constrained by domain-specific
knowledge, a single model can only handle the operation requirement of a
specific task,such as log parser,root cause analysis. Meanwhile, combining
multiple models can achieve more efficient results, which have been proved in
both previous ensemble learning and the recent LLM training domain. Inspired by
these works,to address the similar challenges in AIOPS, this paper first
proposes a collaboration-of-expert framework(CoE-Ops) incorporating a
general-purpose large language model task classifier. A retrieval-augmented
generation mechanism is introduced to improve the framework's capability in
handling both Question-Answering tasks with high-level(Code,build,Test,etc.)
and low-level(fault analysis,anomaly detection,etc.). Finally, the proposed
method is implemented in the AIOps domain, and extensive experiments are
conducted on the DevOps-EVAL dataset. Experimental results demonstrate that
CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps
tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement
over single AIOps models in DevOps problem resolution, and outperforms
larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.

</details>


### [28] [A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents](https://arxiv.org/abs/2507.22938)
*Sumit Soman,H. G. Ranjani,Sujoy Roychowdhury,Venkata Dharma Surya Narayana Sastry,Akshat Jain,Pranav Gangrade,Ayaaz Khan*

Main category: cs.CL

> 本文通过结合图形表示和基于文本的RAG系统，以提高技术文档中以流程图为答案的问答能力，实验显示该方法有效并具有成本效益。

<details>
  <summary>Details</summary>

**Motivation:** 文本基于的RAG系统在处理技术文档中涉及图表的答案时往往表现不佳，本文旨在通过引入流程图的图形表示来解决这一问题，特别是在电信领域中的问答任务。

**Method:** 本文提出了一种结合图形表示和基于文本的检索增强生成(RAG)系统的端到端方法，以提高从技术文档中进行问答的能力。具体来说，该方法包括处理技术文档、分类图像类型、构建图形表示，并将这些表示融入文本嵌入管道中，从而实现高效检索。

**Result:** 实验结果表明，使用微调的视觉大语言模型(VLM)获得的图形表示相对于地面真相的编辑距离较低，这说明了这些表示对于流程图图像的鲁棒性。此外，使用这些表示进行问答的检索性能良好，包括适应电信领域的文本嵌入模型。

**Conclusion:** 本文的方法可以有效解决基于文本的RAG系统在处理流程图或流程图类型图像相关的问答问题上的不足，并且在推理阶段无需使用VLM，这对部署的问答系统来说是一个重要的成本优势。

**Abstract:** Question-Answering (QA) from technical documents often involves questions
whose answers are present in figures, such as flowcharts or flow diagrams.
Text-based Retrieval Augmented Generation (RAG) systems may fail to answer such
questions. We leverage graph representations of flowcharts obtained from Visual
large Language Models (VLMs) and incorporate them in a text-based RAG system to
show that this approach can enable image retrieval for QA in the telecom
domain. We present the end-to-end approach from processing technical documents,
classifying image types, building graph representations, and incorporating them
with the text embedding pipeline for efficient retrieval. We benchmark the same
on a QA dataset created based on proprietary telecom product information
documents. Results show that the graph representations obtained using a
fine-tuned VLM model have lower edit distance with respect to the ground truth,
which illustrate the robustness of these representations for flowchart images.
Further, the approach for QA using these representations gives good retrieval
performance using text-based embedding models, including a telecom-domain
adapted one. Our approach also alleviates the need for a VLM in inference,
which is an important cost benefit for deployed QA systems.

</details>


### [29] [PARROT: An Open Multilingual Radiology Reports Dataset](https://arxiv.org/abs/2507.22939)
*Bastien Le Guellec,Kokou Adambounou,Lisa C Adams,Thibault Agripnidis,Sung Soo Ahn,Radhia Ait Chalal,Tugba Akinci D Antonoli,Philippe Amouyel,Henrik Andersson,Raphael Bentegeac,Claudio Benzoni,Antonino Andrea Blandino,Felix Busch,Elif Can,Riccardo Cau,Armando Ugo Cavallo,Christelle Chavihot,Erwin Chiquete,Renato Cuocolo,Eugen Divjak,Gordana Ivanac,Barbara Dziadkowiec Macek,Armel Elogne,Salvatore Claudio Fanni,Carlos Ferrarotti,Claudia Fossataro,Federica Fossataro,Katarzyna Fulek,Michal Fulek,Pawel Gac,Martyna Gachowska,Ignacio Garcia Juarez,Marco Gatti,Natalia Gorelik,Alexia Maria Goulianou,Aghiles Hamroun,Nicolas Herinirina,Krzysztof Kraik,Dominik Krupka,Quentin Holay,Felipe Kitamura,Michail E Klontzas,Anna Kompanowska,Rafal Kompanowski,Alexandre Lefevre,Tristan Lemke,Maximilian Lindholz,Lukas Muller,Piotr Macek,Marcus Makowski,Luigi Mannacio,Aymen Meddeb,Antonio Natale,Beatrice Nguema Edzang,Adriana Ojeda,Yae Won Park,Federica Piccione,Andrea Ponsiglione,Malgorzata Poreba,Rafal Poreba,Philipp Prucker,Jean Pierre Pruvo,Rosa Alba Pugliesi,Feno Hasina Rabemanorintsoa,Vasileios Rafailidis,Katarzyna Resler,Jan Rotkegel,Luca Saba,Ezann Siebert,Arnaldo Stanzione,Ali Fuat Tekin,Liz Toapanta Yanchapaxi,Matthaios Triantafyllou,Ekaterini Tsaoulia,Evangelia Vassalou,Federica Vernuccio,Johan Wasselius,Weilang Wang,Szymon Urban,Adrian Wlodarczak,Szymon Wlodarczak,Andrzej Wysocki,Lina Xu,Tomasz Zatonski,Shuhang Zhang,Sebastian Ziegelmayer,Gregory Kuchcinski,Keno K Bressem*

Main category: cs.CL

> PARROT数据集由多语言的、虚构的放射学报告组成，可用于测试自然语言处理应用，数据集包含2,658份多样化的放射学报告，研究显示参与者能以53.9%的准确率区分人类与AI生成的报告，其中放射科医生表现最好。

<details>
  <summary>Details</summary>

**Motivation:** 开发和验证PARROT（多语言注释放射检查报告的开放测试数据集），用于测试放射学中的自然语言处理应用。这是一个大型的、多中心的、可公开访问的数据集，包含多语种的虚构放射学报告。

**Method:** 从2024年5月至9月，邀请放射科医生按照他们的标准报告惯例提供虚构的放射学报告。贡献者至少提供了20份包含元数据的报告，包括解剖区域、成像方式、临床背景等，对于非英语报告还需提供英语翻译。所有报告都被分配了ICD-10编码。进行了一个人类与AI报告区分研究，参与者包括154名放射科医生、医疗专业人员和非医疗专业人员，以确定报告是人类编写还是AI生成的。

**Result:** 该数据集包含来自21个国家76名作者的2,658份放射学报告，涵盖13种语言。涉及多种成像方式（CT: 36.1%，MRI: 22.8%，放射摄影: 19.0%，超声: 16.8%）和多个解剖区域，胸部、腹部、头部和骨盆是最主要的部分。在区分研究中，参与者以53.9%的准确率区分了人类和AI生成的报告（95% CI: 50.7%-57.1%），放射科医生表现更好（56.9%，95% CI: 53.3%-60.6%，p<0.05）。

**Conclusion:** PARROT是最大的开放多语言放射学报告数据集，它支持开发和验证在语言、地理位置和临床界限上的自然语言处理应用，且无隐私约束。

**Abstract:** Rationale and Objectives: To develop and validate PARROT (Polyglottal
Annotated Radiology Reports for Open Testing), a large, multicentric,
open-access dataset of fictional radiology reports spanning multiple languages
for testing natural language processing applications in radiology. Materials
and Methods: From May to September 2024, radiologists were invited to
contribute fictional radiology reports following their standard reporting
practices. Contributors provided at least 20 reports with associated metadata
including anatomical region, imaging modality, clinical context, and for
non-English reports, English translations. All reports were assigned ICD-10
codes. A human vs. AI report differentiation study was conducted with 154
participants (radiologists, healthcare professionals, and non-healthcare
professionals) assessing whether reports were human-authored or AI-generated.
Results: The dataset comprises 2,658 radiology reports from 76 authors across
21 countries and 13 languages. Reports cover multiple imaging modalities (CT:
36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical
regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)
being most prevalent. In the differentiation study, participants achieved 53.9%
accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated
reports, with radiologists performing significantly better (56.9%, 95% CI:
53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the
largest open multilingual radiology report dataset, enabling development and
validation of natural language processing applications across linguistic,
geographic, and clinical boundaries without privacy constraints.

</details>


### [30] [Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes](https://arxiv.org/abs/2507.22940)
*Rui Jiao,Yue Zhang,Jinku Li*

Main category: cs.CL

> RELICENSE 是一个新框架，旨在解决大型语言模型 (LLMs) 中的一个关键漏洞：尽管最终答案是正确的，中间推理步骤中的事实不准确。该框架包含三个核心组件：专门的事实检查分类器、多维奖励的 GRPO 强化学习方法和机制可解释性模块。通过实验评估，RELICENSE 显著提高了事实准确性，同时在挑战性基准测试中保持或提高了性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决在高风险领域，如医疗保健、法律分析和科学研究中，即使最终答案正确，中间推理步骤中的事实不准确也可能误导用户的危险情况，对这些领域造成潜在风险。

**Method:** 包含三个部分：专门的事实检查分类器（检测推理链中的事实不一致性），GRPO 强化学习（通过多维奖励来平衡事实性、连贯性和结构正确性），以及机制可解释性模块（研究事实性改进如何在推理过程中反映在模型激活上）。

**Result:** 经过对十个最先进的模型的广泛评估，揭示了重要的模式：即使是领先的模型，如 Claude-3.7 和 GPT-o1，在推理事实准确性方面也仅为 81.93% 和 82.57%。相比之下，RELICENSE 显著提高了事实准确性（最高达 49.90% 的提升），并在数学、AIME 和 GPQA 等挑战性基准测试中保持或提高了性能。

**Conclusion:** RELICENSE 不仅显著增强了事实准确性，还提供了有关如何通过激活指导优化来改进未来模型训练方法的洞察。

**Abstract:** We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy
for Confidence Enhancement), a novel framework addressing a critical
vulnerability in Large Language Models (LLMs): the prevalence of factual
inaccuracies within intermediate reasoning steps despite correct final answers.
This phenomenon poses substantial risks in high-stakes domains including
healthcare, legal analysis, and scientific research, where erroneous yet
confidently presented reasoning can mislead users into dangerous decisions. Our
framework integrates three core components: (1) a specialized fact-checking
classifier trained on counterfactually augmented data to detect subtle factual
inconsistencies within reasoning chains; (2) a Group Relative Policy
Optimization (GRPO) reinforcement learning approach that balances factuality,
coherence, and structural correctness through multi-dimensional rewards; and
(3) a mechanistic interpretability module examining how factuality improvements
manifest in model activations during reasoning processes. Extensive evaluation
across ten state-of-the-art models reveals concerning patterns: even leading
models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of
only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual
robustness (up to 49.90% improvement) while maintaining or improving
performance on challenging benchmarks including Math-500, AIME-2024, and GPQA.
Furthermore, our activation-level analysis provides actionable insights into
how factual enhancements reshape reasoning trajectories within model
architectures, establishing foundations for future training methodologies that
explicitly target factual robustness through activation-guided optimization.

</details>


### [31] [SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology](https://arxiv.org/abs/2507.22941)
*Paul Minchella,Loïc Verlingue,Stéphane Chrétien,Rémi Vaucher,Guillaume Metzler*

Main category: cs.CL

> SigBERT is introduced as a new survival analysis framework that processes clinical reports to estimate patient-specific risk, leveraging advanced embedding and extraction techniques to capture temporal dynamics effectively.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind SigBERT is to address the challenge of handling the complexity of textual data in electronic medical reports, especially its sequential nature, for the purpose of improving survival analysis in the healthcare domain.

**Method:** SigBERT is a novel temporal survival analysis framework designed to handle a large volume of clinical reports per patient. It processes timestamped medical reports by converting them into sentence embeddings and then using signature extraction from rough path theory to capture temporal dynamics, resulting in geometric features that are used with a LASSO-penalized Cox model for estimating patient-specific risk scores.

**Result:** The model was evaluated on a real-world oncology dataset from the Léon Bérard Center, achieving a C-index score of 0.75 (with a standard deviation of 0.014) on the independent test set, indicating a significant improvement in survival model performance.

**Conclusion:** SigBERT integrates sequential medical data through a series of transformation and reduction techniques to enhance risk estimation, thereby advancing the field of narrative-based survival analysis.

**Abstract:** Electronic medical reports (EHR) contain a vast amount of information that
can be leveraged for machine learning applications in healthcare. However,
existing survival analysis methods often struggle to effectively handle the
complexity of textual data, particularly in its sequential form. Here, we
propose SigBERT, an innovative temporal survival analysis framework designed to
efficiently process a large number of clinical reports per patient. SigBERT
processes timestamped medical reports by extracting and averaging word
embeddings into sentence embeddings. To capture temporal dynamics from the time
series of sentence embedding coordinates, we apply signature extraction from
rough path theory to derive geometric features for each patient, which
significantly enhance survival model performance by capturing complex temporal
dynamics. These features are then integrated into a LASSO-penalized Cox model
to estimate patient-specific risk scores. The model was trained and evaluated
on a real-world oncology dataset from the L\'eon B\'erard Center corpus, with a
C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT
integrates sequential medical data to enhance risk estimation, advancing
narrative-based survival analysis.

</details>


### [32] [A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies](https://arxiv.org/abs/2507.22943)
*Shirley V Wang,Georg Hahn,Sushama Kattinakere Sreedhara,Mufaddal Mahesri,Haritha S. Pillai,Rajendra Aldis,Joyce Lii,Sarah K. Dutcher,Rhoda Eniafe,Jamal T. Jones,Keewan Kim,Jiwei He,Hana Lee,Sengwee Toh,Rishi J Desai,Jie Yang*

Main category: cs.CL

> The paper presents an expedited validation process for claims-based algorithms using NLP to decrease manual review time and adaptive sampling to optimize the number of reviewed charts, which was tested with an algorithm for identifying intentional self-harm in obese patients.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the efficiency of validating code-based algorithms used in large claims databases, which can enhance the reliability of findings from database studies without extensive time and resource allocation.

**Method:** Content describes a method that uses natural language processing (NLP) to expedite the validation of claims-based outcome algorithms and a multi-wave adaptive sampling approach for identifying the performance characteristics of these algorithms with sufficient precision, focusing on reducing the manual review time required for the reference-standard label creation process.

**Result:** The study results showed that the NLP-assisted annotation process reduced the review time per chart by 40%, and using the pre-defined stopping rule with multi-wave samples would have prevented 77% of patient charts from being reviewed, while still maintaining the precision of the derived measurement characteristics.

**Conclusion:** The conclusion is that this NLP-assisted and multi-wave adaptive sampling method can make the validation of key study parameters more routine, ultimately aiding in the better understanding of the reliability of findings from database studies.

**Abstract:** Background: One of the ways to enhance analyses conducted with large claims
databases is by validating the measurement characteristics of code-based
algorithms used to identify health outcomes or other key study parameters of
interest. These metrics can be used in quantitative bias analyses to assess the
robustness of results for an inferential study given potential bias from
outcome misclassification. However, extensive time and resource allocation are
typically re-quired to create reference-standard labels through manual chart
review of free-text notes from linked electronic health records. Methods: We
describe an expedited process that introduces efficiency in a validation study
us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to
reduce time spent by human reviewers to review each chart, and 2) a multi-wave
adaptive sampling approach with pre-defined criteria to stop the validation
study once performance characteristics are identified with sufficient
precision. We illustrate this process in a case study that validates the
performance of a claims-based outcome algorithm for intentional self-harm in
patients with obesity. Results: We empirically demonstrate that the
NLP-assisted annotation process reduced the time spent on review per chart by
40% and use of the pre-defined stopping rule with multi-wave samples would have
prevented review of 77% of patient charts with limited compromise to precision
in derived measurement characteristics. Conclusion: This approach could
facilitate more routine validation of code-based algorithms used to define key
study parameters, ultimately enhancing understanding of the reliability of
find-ings derived from database studies.

</details>


### [33] [Opacity as Authority: Arbitrariness and the Preclusion of Contestation](https://arxiv.org/abs/2507.22944)
*Naomi Omeonga wa Kayembe*

Main category: cs.CL

> 该文重新定义了随意性，认为它是人类系统和互动中的功能性机制，而非规范性的缺陷或支配的标志。通过引入一系列链式结构分析随意性，并提出其理论模型，说明随意性在法律和社会动态中的普遍适用性。

<details>
  <summary>Details</summary>

**Motivation:** 文章试图纠正批评传统对随意性的理解误区，即随意性并不等同于不公，而是作为一种语义特征，使系统能够有效地运作，同时保留其内部理据的私密性。

**Method:** 基于索绪尔的语言符号随意性理论，文章提出了“动机->可验证性->可争议性”的链条机制来分析随意性的操作过程，并结合香农的信息熵模型定义了随意性的计算模型 A = H(L|M)。

**Result:** 通过分析发现，随意性原理不是无理性的表现，而是设计上的有意为之，用以保护权威免受问责。同时，这一理论框架也为分析人工智能系统的解释性提供了新途径。

**Conclusion:** 随意性作为中立的控制与关照的机制，在人际关系中是一个被忽视的维度。文章提出一套理论模型来解释这一机制在不同领域的应用方式。

**Abstract:** This article redefines arbitrariness not as a normative flaw or a symptom of
domination, but as a foundational functional mechanism structuring human
systems and interactions. Diverging from critical traditions that conflate
arbitrariness with injustice, it posits arbitrariness as a semiotic trait: a
property enabling systems - linguistic, legal, or social - to operate
effectively while withholding their internal rationale. Building on Ferdinand
de Saussure's concept of l'arbitraire du signe, the analysis extends this
principle beyond language to demonstrate its cross-domain applicability,
particularly in law and social dynamics. The paper introduces the "Motivation
-> Constatability -> Contestability" chain, arguing that motivation functions
as a crucial interface rendering an act's logic vulnerable to intersubjective
contestation. When this chain is broken through mechanisms like
"immotivization" or "Conflict Lateralization" (exemplified by "the blur of the
wolf drowned in the fish"), acts produce binding effects without exposing their
rationale, thus precluding justiciability. This structural opacity, while
appearing illogical, is a deliberate design protecting authority from
accountability. Drawing on Shannon's entropy model, the paper formalizes
arbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern
theory of arbitrariness as a neutral operator central to control as well as
care, an overlooked dimension of interpersonal relations. While primarily
developed through human social systems, this framework also illuminates a new
pathway for analyzing explainability in advanced artificial intelligence
systems.

</details>


### [34] [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968)
*Chengqian Ma,Wei Tao,Yiwen Guo*

Main category: cs.CL

> 本文讨论了Spoken Dialogue Models（SDM）在理解和模拟人类对话方面的发展，并提出了一套基准数据集及评估方法以全面探索SDMs的实际效能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管SDMs因其能够直接生成语音响应而在近期受到了越来越大的重视，但在对其理解和模拟人类对话的实际效果的研究上仍存在差距，尤其是与得到了广泛基准测试的基于文本的大语言模型相比。这些差距主要是由于语音对话特有的复杂性，包括语义和语音学方面的挑战，以及上下文依赖性。

**Method:** 本文提出了一套基准数据集，包含1,079个英语和中文实例，以及一种基于大语言模型的评估方法，旨在全面探索SDMs在解决实际挑战方面的表现。

**Result:** 通过引入新的基准数据集和评估方法，本文有望照亮SDM发展的当前状态，并解决语音对话模型面临的挑战。

**Conclusion:** 本文通过构建一个新的基准数据集和配套的基于大语言模型的评估方法，为全面探索SDMs在处理实际语音对话挑战方面的能力提供了新的视角。

**Abstract:** Spoken Dialogue Models (SDMs) have recently attracted significant attention
for their ability to generate voice responses directly to users' spoken
queries. Despite their increasing popularity, there exists a gap in research
focused on comprehensively understanding their practical effectiveness in
comprehending and emulating human conversations. This is especially true
compared to text-based Large Language Models (LLMs), which benefit from
extensive benchmarking. Human voice interactions are inherently more complex
than text due to characteristics unique to spoken dialogue. Ambiguity poses one
challenge, stemming from semantic factors like polysemy, as well as
phonological aspects such as heterograph, heteronyms, and stress patterns.
Additionally, context-dependency, like omission, coreference, and multi-turn
interaction, adds further complexity to human conversational dynamics. To
illuminate the current state of SDM development and to address these
challenges, we present a benchmark dataset in this paper, which comprises 1,079
instances in English and Chinese. Accompanied by an LLM-based evaluation method
that closely aligns with human judgment, this dataset facilitates a
comprehensive exploration of the performance of SDMs in tackling these
practical challenges.

</details>


### [35] [Math Natural Language Inference: this should be easy!](https://arxiv.org/abs/2507.23063)
*Valeria de Paiva,Qiyue Gao,Hai Hu,Pavel Kovalev,Yikang Liu,Lawrence S. Moss,Zhiheng Qian*

Main category: cs.CL

> 论文探讨了当前LLMs在数学自然语言推理（Math NLI）任务上的表现，结果显示混合效果，既有积极发现也有挑战。同时也为该领域的未来研究提供了数据支持。

<details>
  <summary>Details</summary>

**Motivation:** 探究当前的大规模语言模型（LLMs）是否能执行数学文本上的自然语言推理（NLI）任务。这项任务称为Math NLI问题。

**Method:** 构建了一个数学自然语言推理（Math NLI）的数据集，数据集的前设来自于现存的数学文本，而假设和黄金标签是由既具有数学研究水平又熟悉NLI领域的人提供的。同时，也调查了由LLM生成假设的语料库的质量。

**Result:** 研究结果是混合的。正面发现包括：在某些设置下，使用多个LLM的多数投票结果与使用人类标注的数据在Math NLI领域中的效果相当。负面发现包括：LLM在处理数学语言方面仍存在困难，偶尔会在基本推理上出错。相对于上一代模型，当前模型更少依赖于假设的独立“推理”。

**Conclusion:** 该研究不仅提供了正面和负面的发现，还提供了支持未来Math NLI研究的语料库数据。

**Abstract:** We ask whether contemporary LLMs are able to perform natural language
inference (NLI) tasks on mathematical texts. We call this the Math NLI problem.
We construct a corpus of Math NLI pairs whose premises are from extant
mathematical text and whose hypotheses and gold labels were provided by people
with experience in both research-level mathematics and also in the NLI field.
We also investigate the quality of corpora using the same premises but whose
hypotheses are provided by LLMs themselves. We not only investigate the
performance but also the inter-group consistency of the diverse group of LLMs.
We have both positive and negative findings. Among our positive findings: in
some settings, using a majority vote of LLMs is approximately equivalent to
using human-labeled data in the Math NLI area. On the negative side: LLMs still
struggle with mathematical language. They occasionally fail at even basic
inferences. Current models are not as prone to hypothesis-only "inference" in
our data the way the previous generation had been. In addition to our findings,
we also provide our corpora as data to support future work on Math NLI.

</details>


### [36] [Exploring In-Context Learning for Frame-Semantic Parsing](https://arxiv.org/abs/2507.23082)
*Diego Garat,Guillermo Moncecchi,Dina Wonsever*

Main category: cs.CL

> The paper investigates the use of In-Context Learning (ICL) with large language models to perform Frame Semantic Parsing (FSP), achieving competitive results on violent event-related frames without the need for model fine-tuning.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of this paper is to explore a practical and effective alternative to the traditional fine-tuning process in domain-specific Frame Semantic Parsing tasks, leveraging In-Context Learning (ICL) and Large Language Models (LLMs).

**Method:** The paper employs In-Context Learning (ICL) technique with Large Language Models (LLMs) to perform Frame Semantic Parsing (FSP). It proposes an automatic generation of task-specific prompts for Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) using FrameNet database without the need for fine-tuning the LLMs.

**Result:** The proposed method achieves high F1 scores of 94.3% for FI and 77.4% for FSRL on a subset of violent events-related frames.

**Conclusion:** The study concludes that ICL with automatically generated prompts is a viable and effective approach for FSP, especially without the need for extensive model fine-tuning, demonstrating competitive performance in the context of violent event-related frames.

**Abstract:** Frame Semantic Parsing (FSP) entails identifying predicates and labeling
their arguments according to Frame Semantics. This paper investigates the use
of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP
without model fine-tuning. We propose a method that automatically generates
task-specific prompts for the Frame Identification (FI) and Frame Semantic Role
Labeling (FSRL) subtasks, relying solely on the FrameNet database. These
prompts, constructed from frame definitions and annotated examples, are used to
guide six different LLMs. Experiments are conducted on a subset of frames
related to violent events. The method achieves competitive results, with F1
scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers
a practical and effective alternative to traditional fine-tuning for
domain-specific FSP tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [37] [CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam](https://arxiv.org/abs/2507.22958)
*Ruslan Khrulev*

Main category: cs.CV

> 该论文提出了一个新的基准测试，用于评估VLMs在评估手写数学解答方面的能力，揭示了现有模型在数学推理和与人类评分标准对齐方面的局限性，指出AI辅助评估的新研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 与现有的专注于问题解决的基准测试不同，该研究关注理解学生解决方案、识别错误和根据固定标准分配分数。

**Method:** 该论文引入了EGE-Math Solutions Assessment Benchmark，这是一个新的基准测试，用于评估视觉语言模型（VLMs）评估手写数学解答的能力。基准测试包括来自俄罗斯国家统一考试的122个解答，附带官方专家评分，并在三种推理模式下评估了来自Google、OpenAI、Arcee AI和阿里云的七种现代VLMs。

**Result:** 研究表明当前模型在数学推理和与人类评分标准对齐方面存在局限性。

**Conclusion:** 该研究揭示了AI辅助评估的新研究方向。

**Abstract:** This paper introduces a novel benchmark, EGE-Math Solutions Assessment
Benchmark, for evaluating Vision-Language Models (VLMs) on their ability to
assess hand-written mathematical solutions. Unlike existing benchmarks that
focus on problem solving, our approach centres on understanding student
solutions, identifying mistakes, and assigning grades according to fixed
criteria. We compile 122 scanned solutions from the Russian Unified State Exam
(EGE) together with official expert grades, and evaluate seven modern VLMs from
Google, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The
results reveal current limitations in mathematical reasoning and human-rubric
alignment, opening new research avenues in AI-assisted assessment. You can find
code in https://github.com/Karifannaa/Auto-check-EGE-math

</details>


### [38] [Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction](https://arxiv.org/abs/2507.23006)
*Zhensheng Yuan,Haozhi Huang,Zhen Xiong,Di Wang,Guanghua Yang*

Main category: cs.CV

> 本文提出了一种能够快速重建和实时渲染大规模城市场景的框架，同时解决了多视角捕获中的外观变化问题。

<details>
  <summary>Details</summary>

**Motivation:** 解决大规模城市场景重建中的效率和质量问题，特别是在多视角捕获中由于外观变化带来的挑战。

**Method:** 框架包括场景划分和并行训练，基于可见性的图像选择策略，可控范围的细节层次（LOD）策略和外观转换模块。此外，框架还包括深度正则化，尺度正则化和抗锯齿等模块以提高重建质量。

**Result:** 实验结果表明该方法有效地重建了大规模城市场景，并在效率和质量上优于先前的方法。

**Conclusion:** 提出的方法成功地解决了大规模城市场景重建中的效率和外观变化问题，为实际应用提供了强有力的工具。

**Abstract:** We present a framework that enables fast reconstruction and real-time
rendering of urban-scale scenes while maintaining robustness against appearance
variations across multi-view captures. Our approach begins with scene
partitioning for parallel training, employing a visibility-based image
selection strategy to optimize training efficiency. A controllable
level-of-detail (LOD) strategy explicitly regulates Gaussian density under a
user-defined budget, enabling efficient training and rendering while
maintaining high visual fidelity. The appearance transformation module
mitigates the negative effects of appearance inconsistencies across images
while enabling flexible adjustments. Additionally, we utilize enhancement
modules, such as depth regularization, scale regularization, and antialiasing,
to improve reconstruction fidelity. Experimental results demonstrate that our
method effectively reconstructs urban-scale scenes and outperforms previous
approaches in both efficiency and quality. The source code is available at:
https://yzslab.github.io/REUrbanGS.

</details>


### [39] [Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction](https://arxiv.org/abs/2507.23021)
*Giuseppe Cartella,Vittorio Cuculo,Alessandro D'Amelio,Marcella Cornia,Giuseppe Boccignone,Rita Cucchiara*

Main category: cs.CV

> 这篇论文介绍了 ScanDiff ，一种结合扩散模型和视觉变压器的新型架构，以生成多样的、真实的扫描路径，并在实验中展示了其超越现有方法的能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管深度学习模型在扫描路径预测方面取得了进展，但大多数现有方法产生的都是平均行为，未能捕捉到人类视觉探索的变异性。这项工作旨在解决这一问题。

**Method:** 我们的方法提出了一种新的架构 ScanDiff，它结合了扩散模型和视觉变压器，以生成多样且真实的扫描路径。该方法通过利用扩散模型的随机性质显式地建模扫描路径的变异性，并引入了文本条件，以实现任务驱动的扫描路径生成，使模型能够适应不同的视觉搜索目标。

**Result:** 实验结果显示，ScanDiff 在基准数据集上的自由视图和任务驱动场景中均超越了最先进的方法，生成了更多样化和准确的扫描路径。

**Conclusion:** 这些结果强调了 ScanDiff 在捕捉人类视觉行为复杂性方面的能力，推进了凝视预测研究的发展。

**Abstract:** Predicting human gaze scanpaths is crucial for understanding visual
attention, with applications in human-computer interaction, autonomous systems,
and cognitive robotics. While deep learning models have advanced scanpath
prediction, most existing approaches generate averaged behaviors, failing to
capture the variability of human visual exploration. In this work, we present
ScanDiff, a novel architecture that combines diffusion models with Vision
Transformers to generate diverse and realistic scanpaths. Our method explicitly
models scanpath variability by leveraging the stochastic nature of diffusion
models, producing a wide range of plausible gaze trajectories. Additionally, we
introduce textual conditioning to enable task-driven scanpath generation,
allowing the model to adapt to different visual search objectives. Experiments
on benchmark datasets show that ScanDiff surpasses state-of-the-art methods in
both free-viewing and task-driven scenarios, producing more diverse and
accurate scanpaths. These results highlight its ability to better capture the
complexity of human visual behavior, pushing forward gaze prediction research.
Source code and models are publicly available at
https://aimagelab.github.io/ScanDiff.

</details>


### [40] [Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging](https://arxiv.org/abs/2507.23027)
*Krishan Agyakari Raja Babu,Om Prabhu,Annu,Mohanasankar Sivaprakasam*

Main category: cs.CV

> 本研究通过使用超分辨率技术改善了低质量二维超声图像的分类准确度，在资源受限环境中具有实用价值。

<details>
  <summary>Details</summary>

**Motivation:** 自动心脏解释在资源受限的环境下常被低质量的超声心动图图像所限制，影响了下游诊断模型的效果。虽然超分辨率技术已在MRI和CT上显示出潜力，但其在超声心动图上的应用尚未被充分探索。本文旨在探讨深度学习超分辨技术是否能改善低质量二维超声图的分类准确度。

**Method:** 本研究使用了公开的CAMUS数据集，将样本按照图像质量进行分层，并评估了两种临床相关的任务：简单的二腔心与四腔心视图分类，以及复杂的舒张末期与收缩末期分类。研究采用了两种广泛应用的超分辨率模型SRGAN与SRResNet来增强低质量图像，并观察到性能指标上的显著提高，尤其是计算效率更高的SRResNet模型。

**Result:** 研究发现超分辨率技术能有效恢复低质量超声图像的诊断价值，特别是在使用计算效率较高的SRResNet模型时效果更佳。

**Conclusion:** 本研究证明了超分辨率技术在提高低质量超声心动图的诊断准确性方面具有潜力，可以作为AI辅助诊断在资源受限环境中的一种有效工具。

**Abstract:** Automated cardiac interpretation in resource-constrained settings (RCS) is
often hindered by poor-quality echocardiographic imaging, limiting the
effectiveness of downstream diagnostic models. While super-resolution (SR)
techniques have shown promise in enhancing magnetic resonance imaging (MRI) and
computed tomography (CT) scans, their application to echocardiography-a widely
accessible but noise-prone modality-remains underexplored. In this work, we
investigate the potential of deep learning-based SR to improve classification
accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS
dataset, we stratify samples by image quality and evaluate two clinically
relevant tasks of varying complexity: a relatively simple Two-Chamber vs.
Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole
vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR
models-Super-Resolution Generative Adversarial Network (SRGAN) and
Super-Resolution Residual Network (SRResNet), to enhance poor-quality images
and observe significant gains in performance metric-particularly with SRResNet,
which also offers computational efficiency. Our findings demonstrate that SR
can effectively recover diagnostic value in degraded echo scans, making it a
viable tool for AI-assisted care in RCS, achieving more with less.

</details>


### [41] [Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields](https://arxiv.org/abs/2507.23033)
*Ranxi Lin,Canming Yao,Jiayi Li,Weihang Liu,Xin Lou,Pingqiang Zhou*

Main category: cs.CV

> 提出了一种基于脉冲神经网络的NeRF框架PATA，该框架可以通过调整时间步长来平衡渲染质量和计算资源消耗，在保持渲染质量的同时减少推理时间和功耗。

<details>
  <summary>Details</summary>

**Motivation:** 现有的NeRF模型在训练和推理过程中需要密集的点采样，这会导致大量的浮点运算，不适用于资源受限的环境。SNN由于其能量效率，被认为是可行的替代方案。同时，针对场景尺度和纹理复杂度的变化，提出一种适合场景自适应推理的方法。

**Method:** 提出名为PATA的脉冲NeRF框架，利用动态时间步长训练策略自动探索渲染质量和时间步长之间的平衡，以适应不同场景的自适应推理并减少计算资源消耗。建立在Instant-NGP架构上对方法进行评估。

**Result:** 实验结果显示，PATA可以保持渲染质量的同时减少推理时间步骤64%，运行功率降低61.55%。

**Conclusion:** PATA框架有助于解决资源受限场景下的NeRF模型应用问题，提高了能量效率。

**Abstract:** Neural Radiance Fields (NeRF)-based models have achieved remarkable success
in 3D reconstruction and rendering tasks. However, during both training and
inference, these models rely heavily on dense point sampling along rays from
multiple viewpoints, resulting in a surge in floating-point operations and
severely limiting their use in resource-constrained scenarios like edge
computing. Spiking Neural Networks (SNNs), which communicate via binary spikes
over discrete time steps, offer a promising alternative due to their
energy-efficient nature. Given the inherent variability in scene scale and
texture complexity in neural rendering and the prevailing practice of training
separate models per scene, we propose a spike-based NeRF framework with a
dynamic time step training strategy, termed Pretrain-Adaptive Time-step
Adjustment (PATA). This approach automatically explores the trade-off between
rendering quality and time step length during training. Consequently, it
enables scene-adaptive inference with variable time steps and reduces the
additional consumption of computational resources in the inference process.
Anchoring to the established Instant-NGP architecture, we evaluate our method
across diverse datasets. The experimental results show that PATA can preserve
rendering fidelity while reducing inference time steps by 64\% and running
power by 61.55\%.

</details>


### [42] [Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving](https://arxiv.org/abs/2507.23042)
*Santosh Patapati,Trisanth Srinivasan*

Main category: cs.CV

> NovaDrive 是一种改进的自动驾驶技术，它通过一种单分支视觉-语言架构，集合了图像、高精度地图、LiDAR 深度和路径点信息，高效地提升了自动驾驶的安全性、路径效率以及能源使用效率。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高自动驾驶车辆在快速反应和复杂情况下的导航功能，同时提升驾驶安全性和效率，研究人员提出了 NovaDrive。这个系统试图通过整合多个输入源并优化交叉注意力过程来改进现有多传感器融合的自动驾驶技术。

**Method:** NovaDrive 采用了一个轻量级的两阶段交叉注意力模块，首先将路径点与高精度地图对齐，然后通过细化对图像和深度补丁的注意力来增强整个系统的决策过程。项目团队还研发了一个专门用于平滑转弯和速度变化的损失函数，从而实现更加平稳的驾驶效果。最终，这个模型是通过微调 LLaMA-3.2 视觉语言骨干网络的顶层来实现高效实时推理的。

**Result:** NovaDrive 是一种单分支视觉语言架构，专门设计用于处理自动驾驶所需的多种传感器数据输入，包括前向相机图像、高精度地图、LiDAR 深度和文字路径点。它通过轻量级的两阶段交叉注意力机制使路径点与高精度地图对齐，并进一步细化图像和深度补丁上的注意力。研究团队还包括一个新颖的平滑损失函数，以减少突然的方向和速度变化。实验结果显示，NovaDrive 在 nuScenes / Waymo 子集上提高了成功率、路径效率，并降低碰撞频率。此外，其较短的路径和能耗优化特性为油耗或电池使用提供了改进。

**Conclusion:** NovaDrive 成功地将自动驾驶的成功率、路径效率以及降低碰撞频率进一步提升，证明了其在复杂驾驶情况下的有效性和可靠性。此外，研究还发现了路径优化带来的能源使用效率提升，这对于车辆的安全性、效率和成本节约具有积极意义。

**Abstract:** Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive's
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.

</details>


### [43] [Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation](https://arxiv.org/abs/2507.23058)
*Alexandru Buburuzan*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Safety-critical applications, such as autonomous driving and medical image
analysis, require extensive multimodal data for rigorous testing. Synthetic
data methods are gaining prominence due to the cost and complexity of gathering
real-world data, but they demand a high degree of realism and controllability
to be useful. This work introduces two novel methods for synthetic data
generation in autonomous driving and medical image analysis, namely MObI and
AnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal
Object Inpainting that leverages a diffusion model to produce realistic and
controllable object inpaintings across perceptual modalities, demonstrated
simultaneously for camera and lidar. Given a single reference RGB image, MObI
enables seamless object insertion into existing multimodal scenes at a
specified 3D location, guided by a bounding box, while maintaining semantic
consistency and multimodal coherence. Unlike traditional inpainting methods
that rely solely on edit masks, this approach uses 3D bounding box conditioning
to ensure accurate spatial positioning and realistic scaling. AnydoorMed
extends this paradigm to the medical imaging domain, focusing on
reference-guided inpainting for mammography scans. It leverages a
diffusion-based model to inpaint anomalies with impressive detail preservation,
maintaining the reference anomaly's structural integrity while semantically
blending it with the surrounding tissue. Together, these methods demonstrate
that foundation models for reference-guided inpainting in natural images can be
readily adapted to diverse perceptual modalities, paving the way for the next
generation of systems capable of constructing highly realistic, controllable
and multimodal counterfactual scenarios.

</details>


### [44] [Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints](https://arxiv.org/abs/2507.23064)
*Santosh Patapati,Trisanth Srinivasan,Murari Ambati*

Main category: cs.CV

> 论文介绍了XYZ-Drive，一个单一的视觉语言模型，用于处理自动驾驶汽车的几何精度和语义理解。该模型结合了摄像头画面、俯视地图和下一个路径点信息，能够输出车辆的转向和速度，从而在复杂环境中导航。

<details>
  <summary>Details</summary>

**Motivation:** 现有的自动驾驶系统通常独立处理几何精度和语义理解。该研究的目标是展示通过视觉语言模型的联合处理方法可以提高性能，简化系统架构，并降低碰撞率。

**Method:** XYZ-Drive模型使用了一种较轻量级的目标导向交叉注意力层，只需一次计算即可在模型的不同部分之间进行数据融合，包括摄像头画面、地图和车辆路径点位置。整个模型以一个部分微调过的LLaMA-3.2 11B为基础。

**Result:** 在MD-NEX Outdoor-Driving基准测试中，XYZ-Drive达到了95%的成功率和0.80的成功率加权路径长度（SPL），其性能超越了PhysNav-DG 15%，并将碰撞率降低了一半。多功能测试表明移除任何一种模态将导致高达11%的成功率下降；简单地将数据堆叠降低模型3%的表现，而冻结变压器则降低5%的性能。

**Conclusion:** 实验结果表明，意图和地图布局的早期、基于标记的融合可以实现准确、透明、实时的驾驶。这证明了单一模型处理多种模态信息的可行性及其增强自动驾驶性能的优点。

**Abstract:** Autonomous cars need geometric accuracy and semantic understanding to
navigate complex environments, yet most stacks handle them separately. We
present XYZ-Drive, a single vision-language model that reads a front-camera
frame, a 25m $\times$ 25m overhead map, and the next waypoint, then outputs
steering and speed. A lightweight goal-centered cross-attention layer lets
waypoint tokens highlight relevant image and map patches, supporting both
action and textual explanations, before the fused tokens enter a partially
fine-tuned LLaMA-3.2 11B model.
  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and
0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and
halving collisions, all while significantly improving efficiency by using only
a single branch. Sixteen ablations explain the gains. Removing any modality
(vision, waypoint, map) drops success by up to 11%, confirming their
complementary roles and rich connections. Replacing goal-centered attention
with simple concatenation cuts 3% in performance, showing query-based fusion
injects map knowledge more effectively. Keeping the transformer frozen loses
5%, showing the importance of fine-tuning when applying VLMs for specific tasks
such as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs
lane edges and raises crash rate.
  Overall, these results demonstrate that early, token-level fusion of intent
and map layout enables accurate, transparent, real-time driving.

</details>


### [45] [Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model](https://arxiv.org/abs/2507.23070)
*Dmitry Demidov,Zaigham Zaheer,Omkar Thawakar,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

> 本文提出了一种无训练方法Enriched-FineR(E-FineR)，通过综合利用大型语言模型和视觉语言模型的潜力，在细粒度图像分类中取得了SOTA结果，同时提高了可解释性，适用于缺乏专家注释的新领域。该方法还展示了在零样本和少样本分类中的出色表现。

<details>
  <summary>Details</summary>

**Motivation:** 当前细粒度图像分类方法依赖固定词汇和闭集分类，限制了其在新类别不断出现的实际设置中的可扩展性和适应能力。本文旨在改进现有方法，在分类阶段更好地利用大型语言模型，避免过度依赖未经详尽分析的语言模型预测类别名称。

**Method:** 本文提出了E-FineR，一种无训练的方法，通过结合大型语言模型和视觉语言模型，在不依赖预定义类别标签的情况下实现细粒度视觉识别。

**Result:** E-FineR展示了在细粒度图像分类中的先进性能，特别是在零样本和少样本分类任务中表现优秀，展现了其在真实世界应用中的潜力。

**Conclusion:** 本文介绍的方法支持图像分类从刚性的标签预测转向灵活的语言驱动理解，为实际应用提供了可扩展和通用的系统。

**Abstract:** Fine-grained image classification, the task of distinguishing between
visually similar subcategories within a broader category (e.g., bird species,
car models, flower types), is a challenging computer vision problem.
Traditional approaches rely heavily on fixed vocabularies and closed-set
classification paradigms, limiting their scalability and adaptability in
real-world settings where novel classes frequently emerge. Recent research has
demonstrated that combining large language models (LLMs) with vision-language
models (VLMs) makes open-set recognition possible without the need for
predefined class labels. However, the existing methods are often limited in
harnessing the power of LLMs at the classification phase, and also rely heavily
on the guessed class names provided by an LLM without thorough analysis and
refinement. To address these bottlenecks, we propose our training-free method,
Enriched-FineR (or E-FineR for short), which demonstrates state-of-the-art
results in fine-grained visual recognition while also offering greater
interpretability, highlighting its strong potential in real-world scenarios and
new domains where expert annotations are difficult to obtain. Additionally, we
demonstrate the application of our proposed approach to zero-shot and few-shot
classification, where it demonstrated performance on par with the existing SOTA
while being training-free and not requiring human interventions. Overall, our
vocabulary-free framework supports the shift in image classification from rigid
label prediction to flexible, language-driven understanding, enabling scalable
and generalizable systems for real-world applications. Well-documented code is
available on https://github.com/demidovd98/e-finer.

</details>


### [46] [Details Matter for Indoor Open-vocabulary 3D Instance Segmentation](https://arxiv.org/abs/2507.23134)
*Sanghun Jung,Jingjing Zheng,Ke Zhang,Nan Qiao,Albert Y. C. Chen,Lu Xia,Chi Liu,Yuyin Sun,Xiao Zeng,Hsiang-Wei Huang,Byron Boots,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

> The paper presents a new, state-of-the-art method for open-vocabulary 3D instance segmentation through a two-stage process of 3D proposal generation and instance classification, which outperforms existing approaches.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve upon existing open-vocabulary 3D instance segmentation (OV-3DIS) approaches by synthesizing various concepts into a cohesive solution to address key challenges.

**Method:** Our solution includes a two-stage scheme: first, robust 3D tracking-based proposal aggregation is used for generating 3D proposals, with iterative merging/removal of overlaps and partials. Second, Alpha-CLIP, incorporating object masks to reduce background noise, is employed for classification, along with the use of the standardized maximum similarity (SMS) score to improve precision.

**Result:** The framework attains state-of-the-art performance on ScanNet200 and S3DIS in all AP and AR metrics, even surpassing an end-to-end closed-vocabulary method.

**Conclusion:** The proposed method effectively combines various concepts into a novel, high-performing framework for OV-3DIS, demonstrating its superiority over current methods on standard benchmarks.

**Abstract:** Unlike closed-vocabulary 3D instance segmentation that is often trained
end-to-end, open-vocabulary 3D instance segmentation (OV-3DIS) often leverages
vision-language models (VLMs) to generate 3D instance proposals and classify
them. While various concepts have been proposed from existing research, we
observe that these individual concepts are not mutually exclusive but
complementary. In this paper, we propose a new state-of-the-art solution for
OV-3DIS by carefully designing a recipe to combine the concepts together and
refining them to address key challenges. Our solution follows the two-stage
scheme: 3D proposal generation and instance classification. We employ robust 3D
tracking-based proposal aggregation to generate 3D proposals and remove
overlapped or partial proposals by iterative merging/removal. For the
classification stage, we replace the standard CLIP model with Alpha-CLIP, which
incorporates object masks as an alpha channel to reduce background noise and
obtain object-centric representation. Additionally, we introduce the
standardized maximum similarity (SMS) score to normalize text-to-proposal
similarity, effectively filtering out false positives and boosting precision.
Our framework achieves state-of-the-art performance on ScanNet200 and S3DIS
across all AP and AR metrics, even surpassing an end-to-end closed-vocabulary
method.

</details>


### [47] [X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention](https://arxiv.org/abs/2507.23143)
*Xiaochen Zhao,Hongyi Xu,Guoxian Song,You Xie,Chenxu Zhang,Xiu Li,Linjie Luo,Jinli Suo,Yebin Liu*

Main category: cs.CV

> 本文介绍了一种改进的零样本扩散式肖像动画技术X-NeMo，该技术能够有效减少身份泄露并提高表达程度，生成高度相似的动画。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决当前方法中的关键问题，包括身份泄露和捕捉细微与极端表情的难度，以改进肖像动画生成技术。

**Method:** 本文提出了X-NeMo，这是一种新颖的零样本扩散式肖像动画生成管线，可以使用来自不同个体的驱动视频，对静态肖像进行动画处理。为了克服身份泄露和捕捉细微及极端表情的难题，本文引入了一个全流程训练框架，该框架能够从驱动图像中提取身份无关的1D潜藏运动描述符，并通过交叉注意力机制控制图像生成过程中的运动。此外，本文通过一个双生成对抗网络（Dual GAN）解码器监督学习来增强表达效果和分离运动潜藏与身份线索，同时还采用了空间和色彩增强技术。通过将驱动运动嵌入到1D潜藏向量中，通过对交叉注意力机制而非空间引导来进行运动控制，本方法显著减轻了身份泄露的问题。

**Result:** 大量实验结果表明，X-NeMo在生成高度表达的动画方面超越了现有的最高基线方法，同时保持了优秀的身份相似度。

**Conclusion:** 本文提出的X-NeMo通过一个完全新的训练框架，有效解决了身份泄露和表情细节捕捉的问题，在生成动画方面表现出卓越的性能。

**Abstract:** We propose X-NeMo, a novel zero-shot diffusion-based portrait animation
pipeline that animates a static portrait using facial movements from a driving
video of a different individual. Our work first identifies the root causes of
the key issues in prior approaches, such as identity leakage and difficulty in
capturing subtle and extreme expressions. To address these challenges, we
introduce a fully end-to-end training framework that distills a 1D
identity-agnostic latent motion descriptor from driving image, effectively
controlling motion through cross-attention during image generation. Our
implicit motion descriptor captures expressive facial motion in fine detail,
learned end-to-end from a diverse video dataset without reliance on pretrained
motion detectors. We further enhance expressiveness and disentangle motion
latents from identity cues by supervising their learning with a dual GAN
decoder, alongside spatial and color augmentations. By embedding the driving
motion into a 1D latent vector and controlling motion via cross-attention
rather than additive spatial guidance, our design eliminates the transmission
of spatial-aligned structural clues from the driving condition to the diffusion
backbone, substantially mitigating identity leakage. Extensive experiments
demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly
expressive animations with superior identity resemblance. Our code and models
are available for research.

</details>


### [48] [Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues](https://arxiv.org/abs/2507.23162)
*Xu Cao,Takafumi Taketomi*

Main category: cs.CV

> 本文提出了一种神经逆渲染方法，可用于从多视角图像中重建几何结构、空间变化反射率和光照条件，超越现有方法，处理更复杂场景。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动力在于提出一种不需要光校准或中间提示（如每个视图的法线图）即可从原始图像中一次性优化所有场景参数的方法，从而改进之前多视角光度立体方法的局限性。

**Method:** 我们的方法是神经逆渲染技术，可以从多视角图像中同时重建几何图形、空间变化的反射率和光照条件。这种方法将几何图形和反射率表示为神经隐式场，并使用阴影感知的体积渲染技术。首先，一个空间网络预测每个场景点的符号距离和反射潜码，然后反射网络根据潜码和角度编码的表面法线、视线和光源方向来估计反射率值。

**Result:** 提出的方法在形状和光照估计准确度上优于现有的基于法线的先进技术，可以推广到视角不一致的多光源图像，能够处理具有挑战性的几何图形和反射率的对象。

**Conclusion:** 这种方法展示了一种有效的方式，提升了在复杂光照条件下从多视角图像重建场景质量和精度的能力。

**Abstract:** We propose a neural inverse rendering approach that jointly reconstructs
geometry, spatially varying reflectance, and lighting conditions from
multi-view images captured under varying directional lighting. Unlike prior
multi-view photometric stereo methods that require light calibration or
intermediate cues such as per-view normal maps, our method jointly optimizes
all scene parameters from raw images in a single stage. We represent both
geometry and reflectance as neural implicit fields and apply shadow-aware
volume rendering. A spatial network first predicts the signed distance and a
reflectance latent code for each scene point. A reflectance network then
estimates reflectance values conditioned on the latent code and angularly
encoded surface normal, view, and light directions. The proposed method
outperforms state-of-the-art normal-guided approaches in shape and lighting
estimation accuracy, generalizes to view-unaligned multi-light images, and
handles objects with challenging geometry and reflectance.

</details>


### [49] [CNN-based solution for mango classification in agricultural environments](https://arxiv.org/abs/2507.23174)
*Beatriz Díaz Peón,Jorge Torres Gómez,Ariel Fajardo Márquez*

Main category: cs.CV

> 该论文展示了一种使用卷积神经网络设计的水果检测和分类系统，着重介绍了芒果分类方法，采用了Resnet-18及级联检测器，提升了农业中水果质量评估的准确性和效率，并进一步开发了交互界面，证明了其在农业生产中的应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于为农场库存管理开发了一个可以自动评估水果质量的系统。目的是利用先进的计算机视觉技术来提高水果分类的准确性和效率，并在农业生产质量控制中启用潜在的应用。

**Method:** 该论文采用卷积神经网络（CNN）设计了一种水果检测与分类系统，特别是使用图像处理技术开发了一种芒果分类方法。选择了Resnet-18作为初步分类架构，同时为了平衡执行速度和计算资源消耗，使用了级联检测器进行检测。成果通过在MatLab App Designer中开发的图形界面展示，简化了系统交互。

**Result:** 该系统实现了水果的精准分类，确保了效率的同时探测到水果的质量问题，证明了算法的有效性。通过图形界面使系统操作更加简单，为实际应用提供了新思路。

**Conclusion:** 研究结论认为，卷积神经网络与级联检测器的结合提供了可靠的方法来解决水果的分类和检测问题，这为农业中的品质控制提供了有力的支持，证明了其在实际应用中的可行性。

**Abstract:** This article exemplifies the design of a fruit detection and classification
system using Convolutional
  Neural Networks (CNN). The goal is to develop a system that automatically
assesses fruit quality for
  farm inventory management. Specifically, a method for mango fruit
classification was developed using
  image processing, ensuring both accuracy and efficiency. Resnet-18 was
selected as the preliminary
  architecture for classification, while a cascade detector was used for
detection, balancing execution speed
  and computational resource consumption. Detection and classification results
were displayed through a
  graphical interface developed in MatLab App Designer, streamlining system
interaction. The integration
  of convolutional neural networks and cascade detectors proffers a reliable
solution for fruit classification
  and detection, with potential applications in agricultural quality control.

</details>


### [50] [Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network](https://arxiv.org/abs/2507.23185)
*Jongwook Si,Sungyoung Kim*

Main category: cs.CV

> A new restoration network with Corner Loss and R-CBAM for rain streak removal, achieving better performance on specific datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the quality of images affected by rain streaks while preserving fine structural details, which goes beyond simple noise suppression.

**Method:** We propose a novel image restoration network for single-image rain streak removal that includes a Corner Loss to preserve object boundaries and a Residual Convolutional Block Attention Module (R-CBAM) to adjust feature importance dynamically.

**Result:** Quantitative evaluations on Rain100L and Rain100H datasets show significant improvement over previous methods with PSNRs of 33.29 dB and 26.16 dB, respectively.

**Conclusion:** The proposed method, utilizing a Corner Loss and R-CBAM in the network, demonstrates superior performance in removing rain streaks from images while preserving the fine details and overall quality.

**Abstract:** The problem of single-image rain streak removal goes beyond simple noise
suppression, requiring the simultaneous preservation of fine structural details
and overall visual quality. In this study, we propose a novel image restoration
network that effectively constrains the restoration process by introducing a
Corner Loss, which prevents the loss of object boundaries and detailed texture
information during restoration. Furthermore, we propose a Residual
Convolutional Block Attention Module (R-CBAM) Block into the encoder and
decoder to dynamically adjust the importance of features in both spatial and
channel dimensions, enabling the network to focus more effectively on regions
heavily affected by rain streaks. Quantitative evaluations conducted on the
Rain100L and Rain100H datasets demonstrate that the proposed method
significantly outperforms previous approaches, achieving a PSNR of 33.29 dB on
Rain100L and 26.16 dB on Rain100H.

</details>


### [51] [Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space](https://arxiv.org/abs/2507.23188)
*Shiyao Yu,Zi-An Wang,Kangning Yin,Zheng Tian,Mingyuan Zhang,Weixin Si,Shihao Zou*

Main category: cs.CV

> This paper introduces a novel framework that leverages a sequence-level contrastive learning approach to align four modalities (text, audio, video, and motion) in a fine-grained embedding space, achieving superior performance in motion retrieval tasks compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the limitations of existing methods for motion retrieval which lack an intuitive user interaction mode and overlook the sequential nature of the modalities involved.

**Method:** The paper proposes a framework that creates a fine-grained joint embedding space for text, audio, video, and motion modalities through a sequence-level contrastive learning approach. This is the first time audio is incorporated into motion retrieval frameworks, aiming to improve the user's immersive experience and make motion retrieval more intuitive.

**Result:** The framework's effectiveness is validated on two newly created multi-modal motion retrieval datasets, demonstrating significant performance improvements over state-of-the-art methods in R@1 and R@10 retrieval metrics.

**Conclusion:** The study concludes that the proposed multi-modal (text, audio, video, motion) framework, utilizing sequence-level contrastive learning, not only outperforms existing 3-modal approaches but also highlights the potential of multi-modal frameworks in advancing motion acquisition tasks.

**Abstract:** Motion retrieval is crucial for motion acquisition, offering superior
precision, realism, controllability, and editability compared to motion
generation. Existing approaches leverage contrastive learning to construct a
unified embedding space for motion retrieval from text or visual modality.
However, these methods lack a more intuitive and user-friendly interaction mode
and often overlook the sequential representation of most modalities for
improved retrieval performance. To address these limitations, we propose a
framework that aligns four modalities -- text, audio, video, and motion --
within a fine-grained joint embedding space, incorporating audio for the first
time in motion retrieval to enhance user immersion and convenience. This
fine-grained space is achieved through a sequence-level contrastive learning
approach, which captures critical details across modalities for better
alignment. To evaluate our framework, we augment existing text-motion datasets
with synthetic but diverse audio recordings, creating two multi-modal motion
retrieval datasets. Experimental results demonstrate superior performance over
state-of-the-art methods across multiple sub-tasks, including an 10.16%
improvement in R@10 for text-to-motion retrieval and a 25.43% improvement in
R@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, our
results show that our 4-modal framework significantly outperforms its 3-modal
counterpart, underscoring the potential of multi-modal motion retrieval for
advancing motion acquisition.

</details>


### [52] [A Novel Dataset for Flood Detection Robust to Seasonal Changes in Satellite Imagery](https://arxiv.org/abs/2507.23193)
*Youngsun Jang,Dongyoun Kim,Chulwoo Pack,Kwanghee Won*

Main category: cs.CV

> 该研究介绍了一个针对卫星图像中洪水区域分割的新数据集，旨在填补现有数据集在此任务中的空白。数据集基于2019年美国中西部洪水事件，共有10个卫星图像数据，涵盖了五个州的不同地点，用于评估语义分割性能。模型显示了良好的初步结果，但需要未来采用多模式和时间学习策略以提高效果。此数据集将在GitHub上公开。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于发现现有数据集在处理卫星图像中特定的洪水区域分割任务时存在不足，因此构建了一个新数据集以适应这一挑战。

**Method:** 通过Planet Explorer收集了2019年美国中西部5个州共10个地点的卫星图像，保证数据集中图像具有统一的分辨率和尺寸。使用计算机视觉和遥感领域的现有模型进行语义分割，同时进行了改变窗口大小用于捕捉时间特性的消融研究。

**Result:** 研究中的模型在新数据集上展示了中等程度的结果，表明在特定任务下现有模型仅能获得有限的性能。

**Conclusion:** 基于研究结果，表明未来的研究需要在多模式和时间学习策略上下功夫，以突破当前模型在卫星图像洪水区域分割上的局限性。数据集将在GitHub公开，供其他研究人员使用。

**Abstract:** This study introduces a novel dataset for segmenting flooded areas in
satellite images. After reviewing 77 existing benchmarks utilizing satellite
imagery, we identified a shortage of suitable datasets for this specific task.
To fill this gap, we collected satellite imagery of the 2019 Midwestern USA
floods from Planet Explorer by Planet Labs (Image \c{opyright} 2024 Planet Labs
PBC). The dataset consists of 10 satellite images per location, each containing
both flooded and non-flooded areas. We selected ten locations from each of the
five states: Iowa, Kansas, Montana, Nebraska, and South Dakota. The dataset
ensures uniform resolution and resizing during data processing. For evaluating
semantic segmentation performance, we tested state-of-the-art models in
computer vision and remote sensing on our dataset. Additionally, we conducted
an ablation study varying window sizes to capture temporal characteristics.
Overall, the models demonstrated modest results, suggesting a requirement for
future multimodal and temporal learning strategies. The dataset will be
publicly available on
<https://github.com/youngsunjang/SDSU_MidWest_Flood_2019>.

</details>


### [53] [Adversarial-Guided Diffusion for Multimodal LLM Attacks](https://arxiv.org/abs/2507.23202)
*Chengwei Xia,Fan Ma,Ruijie Quan,Kun Zhan,Yi Yang*

Main category: cs.CV

> This paper introduces an adversarial-guided diffusion (AGD) approach that effectively generates adversarial images to attack MLLMs, ensuring less noticeable image distortion and robustness to defenses by incorporating adversarial signals into the noise component during reverse diffusion.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to develop a method that can generate adversarial images to deceive MLLMs into generating targeted responses with less distortion to the original image and a higher resistance to common defenses against adversarial attacks.

**Method:** The paper proposes an adversarial-guided diffusion (AGD) approach for attacking multimodal large language models (MLLMs). AGD introduces adversarial-guided noise that injects target semantics into the noise component of reverse diffusion, unlike traditional adversarial attacks that embed high-frequency perturbations directly into the clean image. This ensures attack effectiveness and robustness against defenses like low-pass filtering, as the adversarial signal spans the entire frequency spectrum.

**Result:** Experiments show AGD outperforms state-of-the-art methods in terms of attack performance and robustness against specific defenses.

**Conclusion:** AGD presents a novel way to conduct adversarial attacks on MLLMs, which not only maintains the quality of the original image but also demonstrates superior robustness to certain defensive measures compared to existing approaches.

**Abstract:** This paper addresses the challenge of generating adversarial image using a
diffusion model to deceive multimodal large language models (MLLMs) into
generating the targeted responses, while avoiding significant distortion of the
clean image. To address the above challenges, we propose an adversarial-guided
diffusion (AGD) approach for adversarial attack MLLMs. We introduce
adversarial-guided noise to ensure attack efficacy. A key observation in our
design is that, unlike most traditional adversarial attacks which embed
high-frequency perturbations directly into the clean image, AGD injects target
semantics into the noise component of the reverse diffusion. Since the added
noise in a diffusion model spans the entire frequency spectrum, the adversarial
signal embedded within it also inherits this full-spectrum property.
Importantly, during reverse diffusion, the adversarial image is formed as a
linear combination of the clean image and the noise. Thus, when applying
defenses such as a simple low-pass filtering, which act independently on each
component, the adversarial image within the noise component is less likely to
be suppressed, as it is not confined to the high-frequency band. This makes AGD
inherently robust to variety defenses. Extensive experiments demonstrate that
our AGD outperforms state-of-the-art methods in attack performance as well as
in model robustness to some defenses.

</details>


### [54] [Confidence-aware agglomeration classification and segmentation of 2D microscopic food crystal images](https://arxiv.org/abs/2507.23206)
*Xiaoyu Ji,Ali Shakouri,Fengqing Zhu*

Main category: cs.CV

> The paper proposes a combination of a supervised baseline model for generating segmentation pseudo-labels and an instance classification model for pixel-wise segmentation to improve the accuracy of agglomeration classification in 2D microscopic images of food crystals, dealing with the challenges posed by water transparency and limited perspective.

<details>
  <summary>Details</summary>

**Motivation:** The research aims to address the difficulties in manually annotating agglomeration in 2D microscopic images of food crystals due to water transparency and the limited perspective of the images.

**Method:** PaperAnalysis

**Result:** The proposed method improves the true positive classification accuracy and size distribution predictions of agglomeration in comparison to existing methods, and it effectively classifies potential agglomerated instances under two confidence levels of manual annotations.

**Conclusion:** The study concludes that the developed model, incorporating both classification and segmentation capabilities alongside a post-processing module, successfully enhances the precision and reliability of detecting food crystal agglomeration in microscopic images, providing a robust solution to manual annotation challenges in this field.

**Abstract:** Food crystal agglomeration is a phenomenon occurs during crystallization
which traps water between crystals and affects food product quality. Manual
annotation of agglomeration in 2D microscopic images is particularly difficult
due to the transparency of water bonding and the limited perspective focusing
on a single slide of the imaged sample. To address this challenge, we first
propose a supervised baseline model to generate segmentation pseudo-labels for
the coarsely labeled classification dataset. Next, an instance classification
model that simultaneously performs pixel-wise segmentation is trained. Both
models are used in the inference stage to combine their respective strengths in
classification and segmentation. To preserve crystal properties, a post
processing module is designed and included to both steps. Our method improves
true positive agglomeration classification accuracy and size distribution
predictions compared to other existing methods. Given the variability in
confidence levels of manual annotations, our proposed method is evaluated under
two confidence levels and successfully classifies potential agglomerated
instances.

</details>


### [55] [YOLO-ROC: A High-Precision and Ultra-Lightweight Model for Real-Time Road Damage Detection](https://arxiv.org/abs/2507.23225)
*Zicheng Lin,Weichao Pan*

Main category: cs.CV

> 本文提出了一种高精度轻量级模型YOLO-ROC，设计了BMS-SPPF模块以增强多尺度特征提取，并实施了层次通道压缩策略以降低计算复杂度，实验表明该模型在小尺度目标检测方面有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 为解决现有深度学习模型在道路损伤检测中多尺度特征提取不足和计算需求大的问题。

**Method:** 提出一种名为YOLO-ROC的模型，设计了BMS-SPPF模块并实现了层次通道压缩策略。

**Result:** 实验表明该模型在小目标类别D40的mAP50提升了16.8%，模型大小仅为2.0MB，同时在RDD2022_China_Motorbike数据集上表现良好。

**Conclusion:** YOLO-ROC在道路损伤检测中表现出高精度和低计算需求的优越性。

**Abstract:** Road damage detection is a critical task for ensuring traffic safety and
maintaining infrastructure integrity. While deep learning-based detection
methods are now widely adopted, they still face two core challenges: first, the
inadequate multi-scale feature extraction capabilities of existing networks for
diverse targets like cracks and potholes, leading to high miss rates for
small-scale damage; and second, the substantial parameter counts and
computational demands of mainstream models, which hinder their deployment for
efficient, real-time detection in practical applications. To address these
issues, this paper proposes a high-precision and lightweight model, YOLO - Road
Orthogonal Compact (YOLO-ROC). We designed a Bidirectional Multi-scale Spatial
Pyramid Pooling Fast (BMS-SPPF) module to enhance multi-scale feature
extraction and implemented a hierarchical channel compression strategy to
reduce computational complexity. The BMS-SPPF module leverages a bidirectional
spatial-channel attention mechanism to improve the detection of small targets.
Concurrently, the channel compression strategy reduces the parameter count from
3.01M to 0.89M and GFLOPs from 8.1 to 2.6. Experiments on the
RDD2022_China_Drone dataset demonstrate that YOLO-ROC achieves a mAP50 of
67.6%, surpassing the baseline YOLOv8n by 2.11%. Notably, the mAP50 for the
small-target D40 category improved by 16.8%, and the final model size is only
2.0 MB. Furthermore, the model exhibits excellent generalization performance on
the RDD2022_China_Motorbike dataset.

</details>


### [56] [Toward Safe, Trustworthy and Realistic Augmented Reality User Experience](https://arxiv.org/abs/2507.23226)
*Yanming Xiu*

Main category: cs.CV

> 研究开发了两个系统来检测有害AR内容，并提出了未来工作的三个方向。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决AR内容可能造成的任务损害问题，特别是在遮挡关键信息或微妙操纵用户感知方面。

**Method:** 本研究开发了两个系统ViDDAR和VIM-Sense，利用视觉语言模型（VLMs）和多模态推理模块来检测有害的AR内容。

**Result:** 未给出具体的结果，但提出了未来工作的三个方向：自动感知对齐的质量评估、多模态攻击检测、以及在AR设备上实现高效的VLMs部署。

**Conclusion:** 研究旨在建立一个可扩展且符合人类感知的框架来保障AR体验，并寻求关于感知建模、多模态AR内容实现和轻量级模型适配的反馈。

**Abstract:** As augmented reality (AR) becomes increasingly integrated into everyday life,
ensuring the safety and trustworthiness of its virtual content is critical. Our
research addresses the risks of task-detrimental AR content, particularly that
which obstructs critical information or subtly manipulates user perception. We
developed two systems, ViDDAR and VIM-Sense, to detect such attacks using
vision-language models (VLMs) and multimodal reasoning modules. Building on
this foundation, we propose three future directions: automated, perceptually
aligned quality assessment of virtual content; detection of multimodal attacks;
and adaptation of VLMs for efficient and user-centered deployment on AR
devices. Overall, our work aims to establish a scalable, human-aligned
framework for safeguarding AR experiences and seeks feedback on perceptual
modeling, multimodal AR content implementation, and lightweight model
adaptation.

</details>


### [57] [Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.23237)
*Fan Lyu,Linglan Zhao,Chengyan Liu,Yinying Mei,Zhang Zhang,Jian Zhang,Fuyuan Hu,Liang Wang*

Main category: cs.CV

> 我们提出了GSemi-FSCIL框架和ALDC策略，纳入基础类别和所有先前见过的新类别，在未标记样本中进行动态特征校正，实验结果表明该方法优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 之前的Semi-FSCIL研究仅假设未标记数据来自当前会话的新类别，这种观点过于狭隘且不符合实际情况。为了更好地反映现实场景，我们将Semi-FSCIL重新定义为Generalized Semi-FSCIL (GSemi-FSCIL)，并纳入了基础类别和所有之前见过的新类别。

**Method:** 我们提出了一个名为Ambiguity-guided Learnable Distribution Calibration (ALDC) 的策略，该策略动态使用大量的基础样本校正少量新类别样本的特征分布偏差。

**Result:** 在三个基准数据集上的实验表明，我们提出的方法优于现有工作，并取得了新的最优结果。

**Conclusion:** 我们通过在未标记集内引入基础类别和所有先前见过的新类别，构建了GSemi-FSCIL框架，并通过ALDC策略取得了显著性能提升，证明了其在实际应用中的潜力。

**Abstract:** Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new
concepts from limited data while retaining knowledge of previous classes.
Recently, many studies have started to leverage unlabeled samples to assist
models in learning from few-shot samples, giving rise to the field of
Semi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However,
these studies often assume that the source of unlabeled data is only confined
to novel classes of the current session, which presents a narrow perspective
and cannot align well with practical scenarios. To better reflect real-world
scenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by
incorporating both base and all the ever-seen novel classes in the unlabeled
set. This change in the composition of unlabeled samples poses a new challenge
for existing methods, as they struggle to distinguish between unlabeled samples
from base and novel classes. To address this issue, we propose an
Ambiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC
dynamically uses abundant base samples to correct biased feature distributions
for few-shot novel classes. Experiments on three benchmark datasets show that
our method outperforms existing works, setting new state-of-the-art results.

</details>


### [58] [Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents](https://arxiv.org/abs/2507.23242)
*Sungguk Cha,DongWook Kim,Taeseung Hahn,Mintae Kim,Youngsub Han,Byoung-Ki Jeon*

Main category: cs.CV

> 提出RL-QR框架，用于检索增强生成系统中的查询重写，提升了多模态和文本检索性能，但对语义检索尚存挑战。

<details>
  <summary>Details</summary>

**Motivation:** 检索增强生成(RAG)系统依赖于有效的查询形成来利用外部知识。然而，针对多样化和非结构化的现实世界文档优化查询仍然是一个挑战。

**Method:** 本论文提出了一种名为RL-QR的强化学习框架，用于针对特定检索器的查询重写，该框架能够消除对人类注释数据集的需求，并扩展至纯文本和多模态数据库的应用。通过合成场景-问题对和利用广义奖励策略优化（GRPO），RL-QR对特定检索器进行了定制化的查询重写训练，增强了跨领域检索性能。

**Result:** 实验展示了在工业内部数据上的显著改善，其中RL-QR的多模态版本在多模态RAG中的NDCG@3指标相对提升了11%，而相对纯文本检索器而言提升了9%。然而，对于语义和混合检索器，重写器未能提升性能，可能是由于训练不匹配。

**Conclusion:** 本论文展示了RL-QR对RAG系统查询优化的巨大潜力，提供了一种可扩展且无需注释的数据解决方案用于实际检索任务，同时指出了在语义检索上下文中需要进一步改进的领域。

**Abstract:** Retrieval-Augmented Generation (RAG) systems rely heavily on effective query
formulation to unlock external knowledge, yet optimizing queries for diverse,
unstructured real-world documents remains a challenge. We introduce
\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query
rewriting that eliminates the need for human-annotated datasets and extends
applicability to both text-only and multi-modal databases. By synthesizing
scenario-question pairs and leveraging Generalized Reward Policy Optimization
(GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing
retrieval performance across varied domains. Experiments on industrial in-house
data demonstrate significant improvements, with
$\text{RL-QR}_{\text{multi-modal}}$ achieving an 11\% relative gain in NDCG@3
for multi-modal RAG and $\text{RL-QR}_{\text{lexical}}$ yielding a 9\% gain for
lexical retrievers. However, challenges persist with semantic and hybrid
retrievers, where rewriters failed to improve performance, likely due to
training misalignments. Our findings highlight RL-QR's potential to
revolutionize query optimization for RAG systems, offering a scalable,
annotation-free solution for real-world retrieval tasks, while identifying
avenues for further refinement in semantic retrieval contexts.

</details>


### [59] [Automated Mapping the Pathways of Cranial Nerve II, III, V, and VII/VIII: A Multi-Parametric Multi-Stage Diffusion Tractography Atlas](https://arxiv.org/abs/2507.23245)
*Lei Xie,Jiahao Huang,Jiawei Zhang,Jianzhong He,Yiang Pan,Guoqiang Xie,Mengjun Li,Qingrun Zeng,Mingchu Li,Yuanjing Feng*

Main category: cs.CV

> 本文介绍了一种新的多阶段纤维聚类方法，实现了颅神经通路的自动化、全面映射，这对于改善手术前对颅神经与关键组织间空间关系的理解十分有益。实验表明，该方法生成的图谱与手动标注具有高度空间一致性。

<details>
  <summary>Details</summary>

**Motivation:** 由于颅神经独特的解剖结构和颅底环境的复杂性，创建一个全面且详细的颅神经图谱具有挑战性。因此，本研究旨在开发首个用于人类大脑中颅神经通路的自动化映射的全面扩散图谱。

**Method:** 本研究通过使用多参数纤维束成像技术生成每对颅神经的流线，并采用多阶段纤维聚类的新策略来生成一个全面的扩散图谱，以实现颅神经通路的自动化映射。这一策略应用于人类连接组计划（HCP）中的50名受试者生成的大约1,000,000个流线。

**Result:** 定量和视觉实验显示，该颅神经图谱在包括HCP、多壳扩散MRI（MDM）数据集以及两名垂体腺瘤患者的病例数据等多采集站点上，与专家手动注释具有高度的空间一致性。该图谱可以自动识别与5对颅神经相关的8个纤维束。

**Conclusion:** 此工作通过更高效和自动化的成对颅神经路径映射，促进了扩散成像领域的技术进步，从而通过可视化其与周围解剖结构的空间关系来增强对复杂大脑结构的分析和理解。

**Abstract:** Cranial nerves (CNs) play a crucial role in various essential functions of
the human brain, and mapping their pathways from diffusion MRI (dMRI) provides
valuable preoperative insights into the spatial relationships between
individual CNs and key tissues. However, mapping a comprehensive and detailed
CN atlas is challenging because of the unique anatomical structures of each CN
pair and the complexity of the skull base environment.In this work, we present
what we believe to be the first study to develop a comprehensive diffusion
tractography atlas for automated mapping of CN pathways in the human brain. The
CN atlas is generated by fiber clustering by using the streamlines generated by
multi-parametric fiber tractography for each pair of CNs. Instead of disposable
clustering, we explore a new strategy of multi-stage fiber clustering for
multiple analysis of approximately 1,000,000 streamlines generated from the 50
subjects from the Human Connectome Project (HCP). Quantitative and visual
experiments demonstrate that our CN atlas achieves high spatial correspondence
with expert manual annotations on multiple acquisition sites, including the HCP
dataset, the Multi-shell Diffusion MRI (MDM) dataset and two clinical cases of
pituitary adenoma patients. The proposed CN atlas can automatically identify 8
fiber bundles associated with 5 pairs of CNs, including the optic nerve CN II,
oculomotor nerve CN III, trigeminal nerve CN V and facial-vestibulocochlear
nerve CN VII/VIII, and its robustness is demonstrated experimentally. This work
contributes to the field of diffusion imaging by facilitating more efficient
and automated mapping the pathways of multiple pairs of CNs, thereby enhancing
the analysis and understanding of complex brain structures through
visualization of their spatial relationships with nearby anatomy.

</details>


### [60] [A Deep Dive into Generic Object Tracking: A Survey](https://arxiv.org/abs/2507.23251)
*Fereshteh Aghaee Meibodi,Shadi Alijani,Homayoun Najjaran*

Main category: cs.CV

> 本文综述了目标跟踪的三大类方法，并重点关注基于Transformer的方法，提供了一种新的技术分类框架以及定性和定量比较。

<details>
  <summary>Details</summary>

**Motivation:** 目标是提供一个全面的综述，尤其是针对快速发展的基于Transformer的方法，为理解现存的追踪技术及其优缺点提供参考。

**Method:** Content分析涉及三个主要领域的目标跟踪技术综述：基于Siamese的追踪器、判别式追踪器、以及最近流行的基于Transformer的方法。文章着重讨论了这些技术的核心设计原则、创新点及局限性，并通过定性和定量的比较进行了分析。此外，研究提供了一个用于比较代表方法的新分类法，并从多个角度整理现有追踪器，概述了主要的评估基准。

**Result:** 通过对比分析不同方法的核心设计原则、创新点及局限性，研究为理解和评估现有目标追踪技术提供了新的见解和框架。

**Conclusion:** 基于Transformer的目标跟踪技术因其强大的时空建模能力正快速发展，该综述提供了全面的技术分类和比较，为未来研究提供了基础。

**Abstract:** Generic object tracking remains an important yet challenging task in computer
vision due to complex spatio-temporal dynamics, especially in the presence of
occlusions, similar distractors, and appearance variations. Over the past two
decades, a wide range of tracking paradigms, including Siamese-based trackers,
discriminative trackers, and, more recently, prominent transformer-based
approaches, have been introduced to address these challenges. While a few
existing survey papers in this field have either concentrated on a single
category or widely covered multiple ones to capture progress, our paper
presents a comprehensive review of all three categories, with particular
emphasis on the rapidly evolving transformer-based methods. We analyze the core
design principles, innovations, and limitations of each approach through both
qualitative and quantitative comparisons. Our study introduces a novel
categorization and offers a unified visual and tabular comparison of
representative methods. Additionally, we organize existing trackers from
multiple perspectives and summarize the major evaluation benchmarks,
highlighting the fast-paced advancements in transformer-based tracking driven
by their robust spatio-temporal modeling capabilities.

</details>


### [61] [Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality](https://arxiv.org/abs/2507.23253)
*Mingyang Yu,Xiahui Guo,Peng chen,Zhenkai Li,Yang Shu*

Main category: cs.CV

> 本文提出了一种新的时间序列评估指标TGSI和多组成部分的损失函数SATL，以解决传统评估指标无法评估时间序列几何结构的问题，实验结果表明采用SATL训练的模型性能优于基线方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的诸如均方误差(MSE)这样的数值指标可以量化点精确度，但是它们无法评估时间序列数据的几何结构，该结构对于理解时间动态至关重要。

**Method:** 我们提出了一种新的评估指标时间序列几何结构指数(TGSI)，它将时间序列转换为图像以利用其固有的二维几何表示。为了解决图像转换过程中非可微的问题，我们还引入了Shape-Aware Temporal Loss (SATL)，它是一个多组成部分的损失函数，包括一阶差分损失、频域损失和感知特征损失。

**Result:** 实验结果表明，采用SATL训练的模型在多个数据集上，在MSE和提出的TGSI指标上都优于基线方法，并且在推理时没有额外的计算成本。

**Conclusion:** 通过引入SATL，我们能够在训练过程中增强时间序列的结构建模，并通过实验证明了SATL的有效性。

**Abstract:** Time Series forecasting is critical in diverse domains such as weather
forecasting, financial investment, and traffic management. While traditional
numerical metrics like mean squared error (MSE) can quantify point-wise
accuracy, they fail to evaluate the geometric structure of time series data,
which is essential to understand temporal dynamics. To address this issue, we
propose the time series Geometric Structure Index (TGSI), a novel evaluation
metric that transforms time series into images to leverage their inherent
two-dimensional geometric representations. However, since the image
transformation process is non-differentiable, TGSI cannot be directly
integrated as a training loss. We further introduce the Shape-Aware Temporal
Loss (SATL), a multi-component loss function operating in the time series
modality to bridge this gap and enhance structure modeling during training.
SATL combines three components: a first-order difference loss that measures
structural consistency through the MSE between first-order differences, a
frequency domain loss that captures essential periodic patterns using the Fast
Fourier Transform while minimizing noise, and a perceptual feature loss that
measures geometric structure difference in time-series by aligning temporal
features with geometric structure features through a pre-trained temporal
feature extractor and time-series image autoencoder. Experiments across
multiple datasets demonstrate that models trained with SATL achieve superior
performance in both MSE and the proposed TGSI metrics compared to baseline
methods, without additional computational cost during inference.

</details>


### [62] [Learning Semantic-Aware Threshold for Multi-Label Image Recognition with Partial Labels](https://arxiv.org/abs/2507.23263)
*Haoxian Ruan,Zhihua Xu,Zhijing Yang,Guang Ma,Jieming Xie,Changxiang Fan,Tianshui Chen*

Main category: cs.CV

> 针对多标签图像识别中使用部分标签训练模型的问题，本文提出了一种新的算法SATL，该算法能够更准确地生成伪标签并提高模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的多标签图像识别方法依赖于语义或特征相关性为未知标签生成伪标签，这往往忽视了各个类别得分分布的差异，导致伪标签不准确和不完整，从而影响了性能。

**Method:** 引入了名为Semantic-Aware Threshold Learning (SATL) 的算法。该方法通过计算每个类别中正样本和负样本的得分分布来确定类特定阈值，并在整个学习过程中动态更新这些分布和阈值。此外，通过实施差异排名损失来显著拉开正样本和负样本得分分布的差距，提高阈值的判别性。

**Result:** 在大规模多标签数据集上进行了详尽的实验和分析，如Microsoft COCO和VG-200，结果展示了我们的方法在有限标签场景下显著提高了性能。

**Conclusion:** 实验证明，我们的方法在具有有限标签的场景中能够显著提升多标签图像识别的性能。

**Abstract:** Multi-label image recognition with partial labels (MLR-PL) is designed to
train models using a mix of known and unknown labels. Traditional methods rely
on semantic or feature correlations to create pseudo-labels for unidentified
labels using pre-set thresholds. This approach often overlooks the varying
score distributions across categories, resulting in inaccurate and incomplete
pseudo-labels, thereby affecting performance. In our study, we introduce the
Semantic-Aware Threshold Learning (SATL) algorithm. This innovative approach
calculates the score distribution for both positive and negative samples within
each category and determines category-specific thresholds based on these
distributions. These distributions and thresholds are dynamically updated
throughout the learning process. Additionally, we implement a differential
ranking loss to establish a significant gap between the score distributions of
positive and negative samples, enhancing the discrimination of the thresholds.
Comprehensive experiments and analysis on large-scale multi-label datasets,
such as Microsoft COCO and VG-200, demonstrate that our method significantly
improves performance in scenarios with limited labels.

</details>


### [63] [PixNerd: Pixel Neural Field Diffusion](https://arxiv.org/abs/2507.23268)
*Shuai Wang,Ziteng Gao,Chenhui Zhu,Weilin Huang,Limin Wang*

Main category: cs.CV

> 本文提出了PixelNerd，通过神经场在像素空间上进行逐块解码，解决了现有扩散变压器的错误和伪影问题，取得了优异的图像合成性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决依赖预训练变分自编码器的扩散变压器所引入的累积误差和解码伪影问题。

**Method:** 本文提出了名为PixelNerd的单尺度、单阶段、高效且端到端的解决方案，通过神经场建模逐块解码，以解决现有扩散变压器依赖预训练变分自编码器引入的累积误差和解码伪影等问题。

**Result:** 在ImageNet 256x256数据集上达到了2.15的FID，在ImageNet 512x512数据集上达到了2.84的FID，无需任何复杂的级联流水线或变分自编码器。在文生图应用中，PixNerd-XXL/16在GenEval基准测试中达到了0.73的总体评分，在DPG基准测试中达到了80.9的总体评分。

**Conclusion:** 实验结果表明，PixelNerd能够在无复杂级联流水线或变分自编码器的情况下，实现高质量的图像生成，并且在文生图应用中也展现出了强大的竞争力。

**Abstract:** The current success of diffusion transformers heavily depends on the
compressed latent space shaped by the pre-trained variational autoencoder(VAE).
However, this two-stage training paradigm inevitably introduces accumulated
errors and decoding artifacts. To address the aforementioned problems,
researchers return to pixel space at the cost of complicated cascade pipelines
and increased token complexity. In contrast to their efforts, we propose to
model the patch-wise decoding with neural field and present a single-scale,
single-stage, efficient, end-to-end solution, coined as pixel neural field
diffusion~(PixelNerd). Thanks to the efficient neural field representation in
PixNerd, we directly achieved 2.15 FID on ImageNet $256\times256$ and 2.84 FID
on ImageNet $512\times512$ without any complex cascade pipeline or VAE. We also
extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16
achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9
overall score on the DPG benchmark.

</details>


### [64] [Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2](https://arxiv.org/abs/2507.23272)
*Solha Kang,Eugene Kim,Joris Vankerschaver,Utku Ozbulak*

Main category: cs.CV

> 本研究通过切片标注传播的方式在SAM2模型上实现了低成本的乳腺MRI肿瘤分割任务，尤其在中心向外的策略下获得了较好的分割效果，展示了通用模型在资源受限条件下应用的可能性。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于，随着AI技术在加速医学影像分析中的应用，商业医学AI产品的高昂授权成本、专有软件和基础设施需求限制了其在中低收入国家的采用。

**Method:** 本研究探讨了Segment Anything Model 2 (SAM2) 是否可以被用于低成本、低输入的乳腺MRI三维肿瘤分割。通过在单一切片上使用单一边界框标注，将分割预测传播至整个3D体积，采用三种不同的一维追踪策略：从上到下、从下到上以及从中心向外。

**Result:** 研究结果表明，从中心向外的传播策略在患者大样本量上产生了最一致且准确的分割。尽管SAM2作为一个零样本模型并未针对体积医学数据进行训练，但在极低的监督下仍能实现强大的分割性能。

**Conclusion:** 研究结论强调，诸如SAM2这样的通用基础模型可以在最小监督下支持三维医学影像分析，为资源匮乏的环境提供了可访问和经济的替代方案。

**Abstract:** Breast MRI provides high-resolution volumetric imaging critical for tumor
assessment and treatment planning, yet manual interpretation of 3D scans
remains labor-intensive and subjective. While AI-powered tools hold promise for
accelerating medical image analysis, adoption of commercial medical AI products
remains limited in low- and middle-income countries due to high license costs,
proprietary software, and infrastructure demands. In this work, we investigate
whether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,
minimal-input 3D tumor segmentation in breast MRI. Using a single bounding box
annotation on one slice, we propagate segmentation predictions across the 3D
volume using three different slice-wise tracking strategies: top-to-bottom,
bottom-to-top, and center-outward. We evaluate these strategies across a large
cohort of patients and find that center-outward propagation yields the most
consistent and accurate segmentations. Despite being a zero-shot model not
trained for volumetric medical data, SAM2 achieves strong segmentation
performance under minimal supervision. We further analyze how segmentation
performance relates to tumor size, location, and shape, identifying key failure
modes. Our results suggest that general-purpose foundation models such as SAM2
can support 3D medical image analysis with minimal supervision, offering an
accessible and affordable alternative for resource-constrained settings.

</details>


### [65] [iLRM: An Iterative Large 3D Reconstruction Model](https://arxiv.org/abs/2507.23277)
*Gyeongjin Kang,Seungtae Nam,Xiangyu Sun,Sameh Khamis,Abdelrahman Mohamed,Eunbyung Park*

Main category: cs.CV

> 提出了一个迭代的大型3D重建模型(iLRM)，解决了基于变压器架构的方法在处理大量视图输入时计算成本高的问题，通过分解全注意力机制，实现高效率和高质量的3D重建。

<details>
  <summary>Details</summary>

**Motivation:** 当前的3D重建方法，尤其是基于变压器的方法，受限于全注意力机制导致在处理多视图和高分辨率图像时计算成本过高。

**Method:** iLRM主要通过三个核心原则来改进：(1)将场景表示从输入视图图像中解耦，使3D表示更加紧凑；(2)将全注意力多视图互动分解成两阶段注意力模式，减少计算成本；(3)在每一层注入高分辨率信息，实现高保真度重建。

**Result:** 在RE10K和DL3DV等常用数据集上的实验结果显示，iLRM在重建质量和速度上优于现有方法，特别是显示出更优的可扩展性，在与现有方法计算成本相当的情况下能有效利用更多输入视图达到更高的重建质量。

**Conclusion:** iLRM模型通过引入迭代细化机制和高分辨率信息的逐层注入，解决了现有3D重建方法的计算成本和可扩展性问题，表明其是一个高效的解决方案。

**Abstract:** Feed-forward 3D modeling has emerged as a promising approach for rapid and
high-quality 3D reconstruction. In particular, directly generating explicit 3D
representations, such as 3D Gaussian splatting, has attracted significant
attention due to its fast and high-quality rendering, as well as numerous
applications. However, many state-of-the-art methods, primarily based on
transformer architectures, suffer from severe scalability issues because they
rely on full attention across image tokens from multiple input views, resulting
in prohibitive computational costs as the number of views or image resolution
increases. Toward a scalable and efficient feed-forward 3D reconstruction, we
introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D
Gaussian representations through an iterative refinement mechanism, guided by
three core principles: (1) decoupling the scene representation from input-view
images to enable compact 3D representations; (2) decomposing fully-attentional
multi-view interactions into a two-stage attention scheme to reduce
computational costs; and (3) injecting high-resolution information at every
layer to achieve high-fidelity reconstruction. Experimental results on widely
used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms
existing methods in both reconstruction quality and speed. Notably, iLRM
exhibits superior scalability, delivering significantly higher reconstruction
quality under comparable computational cost by efficiently leveraging a larger
number of input views.

</details>


### [66] [UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing](https://arxiv.org/abs/2507.23278)
*Hao Tang,Chenwei Xie,Xiaoyi Bao,Tingyu Weng,Pandeng Li,Yun Zheng,Liwei Wang*

Main category: cs.CV

> 本文提出了一种名为UniLIP的新模型，它基于CLIP，能够进行图像理解、重建、生成和编辑，其在各方面均表现出色，超过了许多现有的模型。

<details>
  <summary>Details</summary>

**Motivation:** 之前的基于CLIP的统一方法通常需要额外的扩散解码器或量化技术来支持重建和生成任务，这导致了重建不一致或原始理解性能下降。因此，我们提出UniLIP，旨在改进这些方面，即保持原始理解性能的同时，实现有效的图像重建。

**Method:** 我们提出了一种名为UniLIP的方法，它扩展了CLIP的功能，使其不仅能够理解和识别图像，还能进行图像重建、生成和编辑。UniLIP通过两阶段的训练方案和自蒸馏策略来实现这一目标，同时引入了双条件架构来更好地整合多模态大规模语言模型（MLLM）和扩散变换器（diffusion transformer）的功能，从而提高了生成任务中的推理能力和编辑任务中的特征利用效率。

**Result:** 在文本到图像生成任务中，UniLIP在GenEval和WISE基准上分别获得了0.87和0.53的分数，超过了同类规模的所有先前统一模型。在图像编辑任务上，UniLIP在ImgEdit Benchmark上的得分达到3.62，超越了如BAGEL和UniWorld-V1等近期的最先进模型。

**Conclusion:** UniLIP大大扩展了CLIP的应用领域，使得连续的CLIP特征不仅是理解任务的最佳选择，还能在生成和编辑任务中达到高度竞争的表现。

**Abstract:** In this paper, we propose UniLIP, which extends CLIP to reconstruction,
generation and editing, thereby building a unified tokenizer upon its
exceptional comprehension capabilities. Previous CLIP-based unified methods
often require additional diffusion decoders or quantization to support
reconstruction and generation tasks, leading to inconsistent reconstruction or
degradation of original comprehension performance.In contrast, we introduce a
two-stage training scheme and a self-distillation strategy that progressively
integrates reconstruction capabilities into CLIP, allowing it to maintain
original comprehension performance while achieving effective image
reconstruction. Furthermore, we propose a dual-condition architecture to
connect the MLLM and diffusion transformer, using both learnable queries and
the last layer multimodal hidden states as joint conditions. This method not
only enables the utilization of the MLLM's strong reasoning capabilities in
generation tasks, but also maximizes the exploitation of the rich information
in UniLIP features during editing tasks. In text-to-image generation tasks,
UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark
respectively, surpassing all previous unified models of similar scale. In image
editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark,
surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP
effectively expand the application scope of CLIP, enabling continuous CLIP
features to not only serve as the optimal choice for understanding tasks but
also achieve highly competitive performance in generation and editing tasks.

</details>


### [67] [Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval](https://arxiv.org/abs/2507.23284)
*Dohwan Ko,Ji Soo Lee,Minhyuk Choi,Zihang Meng,Hyunwoo J. Kim*

Main category: cs.CV

> 本文提出的一种基于多模态语言模型的新的文本-视频检索框架BLiM，通过引入候选者先验归一化CPN模块来减少候选者先验偏见，改善了检索的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 观察到直接使用多模态语言模型进行检索会导致候选者先验偏见，即更有利于具有较高固有先验概率的候选者。希望设计一个既能在给定视频的情况下生成文本，也能在给定文本的情况下生成视频特征的模型，从而更注重查询-候选者相关性而非候选者的固有概率。

**Method:** 提出了一种新的检索框架BLiM，利用双向似然估计来解决基于候选者似然性直接检索时引入的候选者先验偏见问题。此外，还引入了候选者先验归一化(CPN)模块，这是一种简单而有效的训练无关得分校准模块，旨在减少候选者先验偏见。

**Result:** 在四个文本-视频检索基准测试上，BLiM加上CPN的性能要比之前的最先进模型平均高出6.4 R@1，有效地减轻了候选者先验偏见，并强调了查询-候选者的相关性。

**Conclusion:** 该研究不仅证明了在文本-视频检索任务上的有效性，也展示了CPN在减少对文本先验依赖以增强视觉理解方面广泛的适用性。

**Abstract:** Text-Video Retrieval aims to find the most relevant text (or video) candidate
given a video (or text) query from large-scale online databases. Recent work
leverages multi-modal large language models (MLLMs) to improve retrieval,
especially for long or complex query-candidate pairs. However, we observe that
the naive application of MLLMs, i.e., retrieval based on candidate likelihood,
introduces candidate prior bias, favoring candidates with inherently higher
priors over those more relevant to the query. To this end, we propose a novel
retrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),
which leverages both query and candidate likelihoods by training the model to
generate text from a given video as well as video features from a given text.
Furthermore, we introduce Candidate Prior Normalization (CPN), a simple yet
effective training-free score calibration module designed to mitigate candidate
prior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,
our BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4
R@1 on average, effectively alleviating candidate prior bias and emphasizing
query-candidate relevance. Our in-depth analysis across various multi-modal
tasks beyond retrieval highlights the broad applicability of CPN which enhances
visual understanding by reducing reliance on textual priors. Code is available
at https://github.com/mlvlab/BLiM.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [68] [Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation](https://arxiv.org/abs/2507.23110)
*Zheyuan Zhang,Linkai Peng,Wanying Dou,Cuiling Sun,Halil Ertugrul Aktas,Andrea M. Bejar,Elif Keles,Gorkem Durak,Ulas Bagci*

Main category: eess.IV

> 提出了PancreasDG数据集用于研究医学影像中的领域泛化问题，揭示了领域泛化的一些见解，并提出了一种利用解剖不变性的半监督方法，在跨序列分割上显著优于最先进的领域泛化技术。

<details>
  <summary>Details</summary>

**Motivation:** 现有的领域泛化基准主要集中在跨中心转换上，忽略了T1和T2序列之间的主导变异性。胰腺分割在腹部成像中仍然是一个重大挑战，现有的深度网络在胰腺分割上的表现仍不尽如人意。

**Method:** 通过构建大规模多中心3D MRI胰腺分割数据集PancreasDG，研究医学影像中的领域泛化问题。该数据集包含6个机构的563个MRI扫描，覆盖静脉相和反相序列。

**Result:** 研究结果揭示了领域泛化在胰腺分割中的三个见解：有限采样引入的显著方差可能被误认为是分布转换，跨中心性能与相同序列的源领域性能相关，而跨序列转换需要专门的解决方案。此外，提出的半监督方法在跨序列分割中Dice分数提高了61.63%。

**Conclusion:** PancreasDG为医学影像中的领域泛化设立了新的基准。

**Abstract:** Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences
whose appearance differs more than the acquisition sites that produce them.
Existing domain-generalization benchmarks focus almost on cross-center shifts
and overlook this dominant source of variability. Pancreas segmentation remains
a major challenge in abdominal imaging: the gland is small, irregularly,
surrounded by organs and fat, and often suffers from low T1 contrast.
State-of-the-art deep networks that already achieve >90% Dice on the liver or
kidneys still miss 20-30% of the pancreas. The organ is also systematically
under-represented in public cross-domain benchmarks, despite its clinical
importance in early cancer detection, surgery, and diabetes research. To close
this gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas
segmentation dataset for investigating domain generalization in medical
imaging. The dataset comprises 563 MRI scans from six institutions, spanning
both venous phase and out-of-phase sequences, enabling study of both
cross-center and cross-sequence variations with pixel-accurate pancreas masks
created by a double-blind, two-pass protocol. Through comprehensive analysis,
we reveal three insights: (i) limited sampling introduces significant variance
that may be mistaken for distribution shifts, (ii) cross-center performance
correlates with source domain performance for identical sequences, and (iii)
cross-sequence shifts require specialized solutions. We also propose a
semi-supervised approach that leverages anatomical invariances, significantly
outperforming state-of-the-art domain generalization techniques with 61.63%
Dice score improvements and 87.00% on two test centers for cross-sequence
segmentation. PancreasDG sets a new benchmark for domain generalization in
medical imaging. Dataset, code, and models will be available at
https://pancreasdg.netlify.app.

</details>


### [69] [Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery](https://arxiv.org/abs/2507.23150)
*Philip Wootaek Shin,Vishal Gaur,Rahul Ramachandran,Manil Maskey,Jack Sampson,Vijaykrishnan Narayanan,Sujit Roy*

Main category: eess.IV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** High-resolution satellite imagery is essential for geospatial analysis, yet
differences in spatial resolution across satellite sensors present challenges
for data fusion and downstream applications. Super-resolution techniques can
help bridge this gap, but existing methods rely on artificially downscaled
images rather than real sensor data and are not well suited for heterogeneous
satellite sensors with differing spectral, temporal characteristics. In this
work, we develop a preliminary framework to align and Harmonized Landsat
Sentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a
reference from the HLS dataset. Our approach aims to bridge the resolution gap
between these sensors and improve the quality of super-resolved Landsat
imagery. Quantitative and qualitative evaluations demonstrate the effectiveness
of our method, showing its potential for enhancing satellite-based sensing
applications. This study provides insights into the feasibility of
heterogeneous satellite image super-resolution and highlights key
considerations for future advancements in the field.

</details>
