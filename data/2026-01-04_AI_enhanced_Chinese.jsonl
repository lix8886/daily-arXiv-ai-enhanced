{"id": "2512.23786", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.23786", "abs": "https://arxiv.org/abs/2512.23786", "authors": ["Ankan Aich", "Yangming Lee"], "title": "Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments", "comment": null, "summary": "Accurate Monocular Depth Estimation (MDE) is critical for robotic surgery but remains fragile in specular, fluid-filled endoscopic environments. Existing self-supervised methods, typically relying on foundation models trained with noisy real-world pseudo-labels, often suffer from boundary collapse on thin surgical tools and transparent surfaces. In this work, we address this by leveraging the high-fidelity synthetic priors of the Depth Anything V2 architecture, which inherently captures precise geometric details of thin structures. We efficiently adapt these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA), minimizing the parameter budget while bridging the synthetic-to-real gap. Additionally, we introduce a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes often masked by aggregate metrics. Our approach establishes a new state-of-the-art, achieving an accuracy (< 1.25) of 98.1% and reducing Squared Relative Error by over 17% compared to established baselines, demonstrating superior robustness in adverse surgical lighting.", "AI": {"tldr": "论文提出了一种新的单目深度估计方法，解决了现有方法在富含光泽的内窥镜手术环境中的不足，通过结合高保真合成先验和动态适应技术，实现了更高的准确性和鲁棒性。", "motivation": "该论文旨在解决在内窥镜环境下，现有单目深度估计方法在薄手术工具和透明表面上边界坍塌的问题。这种方法在富含光泽的环境中的准确性较低，这限制了其在机器人手术中的应用。", "method": "本研究利用Depth Anything V2架构的高保真合成先验，该架构能够捕捉薄结构的精确几何细节，并通过动态向量低秩适应（DV-LORA）将其有效应用于医学领域，以缩小合成与现实之间的差距，同时最小化参数预算。此外，还引入了在SCARED数据集上的物理分层评估协议，以量化高光泽度条件下的性能。", "result": "实验结果展示了新方法在高光泽度环境下的卓越性能，达到了98.1%的高精确度，并将平方相对误差降低了17%以上。", "conclusion": "研究方法在SCARED数据集的评估中建立了新的领先水平，实现了98.1%的高精确度（＜1.25）和比现有基线降低超过17%的平方相对误差，尤其是在不利的手术照明条件下表现更为优异。"}}
{"id": "2512.23819", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23819", "abs": "https://arxiv.org/abs/2512.23819", "authors": ["Surya Rayala", "Marcos Quinones-Grueiro", "Naveeduddin Mohammed", "Ashwin T S", "Benjamin Goldberg", "Randall Spain", "Paige Lawton", "Gautam Biswas"], "title": "Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments", "comment": "14 pages, 9 figures, I/ITSEC-2025", "summary": "Effective urban warfare training requires situational awareness and muscle memory, developed through repeated practice in realistic yet controlled environments. A key drill, Enter and Clear the Room (ECR), demands threat assessment, coordination, and securing confined spaces. The military uses Synthetic Training Environments that offer scalable, controlled settings for repeated exercises. However, automatic performance assessment remains challenging, particularly when aiming for objective evaluation of cognitive, psychomotor, and teamwork skills. Traditional methods often rely on costly, intrusive sensors or subjective human observation, limiting scalability and accuracy. This paper introduces a video-based assessment pipeline that derives performance analytics from training videos without requiring additional hardware. By utilizing computer vision models, the system extracts 2D skeletons, gaze vectors, and movement trajectories. From these data, we develop task-specific metrics that measure psychomotor fluency, situational awareness, and team coordination. These metrics feed into an extended Cognitive Task Analysis (CTA) hierarchy, which employs a weighted combination to generate overall performance scores for teamwork and cognition. We demonstrate the approach with a case study of real-world ECR drills, providing actionable, domain specific metrics that capture individual and team performance. We also discuss how these insights can support After Action Reviews with interactive dashboards within Gamemaster and the Generalized Intelligent Framework for Tutoring (GIFT), providing intuitive and understandable feedback. We conclude by addressing limitations, including tracking difficulties, ground-truth validation, and the broader applicability of our approach. Future work includes expanding analysis to 3D video data and leveraging video analysis to enable scalable evaluation within STEs.", "AI": {"tldr": "该研究提出了一种基于视频的评估方法，利用计算机视觉技术从训练视频中提取信息，进而评估士兵在城市作战训练中的心理运动、认知及团队合作能力。", "motivation": "传统的方法往往依赖于成本高昂、侵入性的传感器或主观的人类观察，限制了可扩展性和准确性。因此，该研究旨在介绍一个基于视频的评估管道，可以从训练视频中导出绩效分析，而无需额外的硬件。", "method": "通过利用计算机视觉模型，该系统能从训练视频中提取2D人体骨架、注视向量和运动轨迹，进而开发特定于任务的指标来度量心理运动流畅性、态势感知和团队协作，并将这些指标输入扩展的认知任务分析（CTA）层次结构，以此生成团队合作和认知的总体绩效分数。", "result": "该方法能够在现实中对进入并扫清房间（ECR）的训练演习进行案例研究，提供实际而具体的绩效度量。", "conclusion": "研究讨论了如动作跟踪难题、基准验证以及方法应用范围等限制，并指出了未来在扩展3D视频数据分析和利用视频分析系统的可扩展性进行评估方面的研究方向。"}}
{"id": "2512.23851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23851", "abs": "https://arxiv.org/abs/2512.23851", "authors": ["Lvmin Zhang", "Shengqu Cai", "Muyang Li", "Chong Zeng", "Beijia Lu", "Anyi Rao", "Song Han", "Gordon Wetzstein", "Maneesh Agrawala"], "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression", "comment": "https://github.com/lllyasviel/PFP", "summary": "We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.", "AI": {"tldr": "PFP是一种神经网络结构，可以压缩长视频至简短的上下文，同时保存单帧的高频细节。该模型可以压缩20秒的视频为约5k长度的上下文，任一时间点的随机帧可以以感知上保持一致的外观被检索到。预训练模型可以直接微调为自回归视频模型的记忆编码器，使保存长历史记忆时具有较低的上下文成本和保真度损失。", "motivation": "动机是提出一种能够压缩视频同时保持帧间高频率细节的方法，使得长视频可以被有效压缩，并且能够以较低的成本和保真度在自回归视频模型中使用。", "method": "方法包括一个神经网络结构PFP，该结构目的是压缩视频至简短的上下文的同时，保留高频率的细节，并通过预训练目标明确地保留单帧的细节。", "result": "研究结果表明，该模型可以将20秒的视频压缩到5k长度的上下文内，而且可以以感知上一致的外观检索出随机帧。预训练模型也可以直接被调优为自回归视频模型的记忆编码器，从而降低上下文成本和保真度损失。", "conclusion": "结论是PFP作为一种压缩视频的方法，可以有效地保持帧间的高频细节，并通过低上下文成本和低保真度损失进一步支持自回归视频的记忆性，展示了其在视频压缩与处理方面的潜力。"}}
{"id": "2512.23860", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23860", "abs": "https://arxiv.org/abs/2512.23860", "authors": ["Qucheng Peng", "Hongfei Xue", "Pu Wang", "Chen Chen"], "title": "Lifelong Domain Adaptive 3D Human Pose Estimation", "comment": "Accepted by AAAI 2026", "summary": "3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.", "AI": {"tldr": "提出了一种新的任务设置，即终身域适应3D人体姿态估计任务，通过一种生成对抗网络框架和一种新的3D姿势生成器范式来解决面向现实世界3D人体姿态估计中的适应和知识保留难题。", "motivation": "传统的3D人体姿态估计方法和现有的领域自适应策略在面对多样化的开放式环境时，无法很好地推广到现实世界的各种场景。此外，现有的域适应方法忽视了目标域数据集的非零散性问题。", "method": "提出了一种新颖的任务，即终身域适应3D人体姿态估计（Lifelong Domain Adaptive 3D HPE），其中姿态估计器在预先训练完成后，逐步适应不同的目标域。在这个框架中，除了对抗网络框架外，还构建了一个包括姿态感知、时间感知和域感知的知识集成的3D姿态生成器范式。", "result": "通过在多种域适应的3D人体姿态估计算法数据集上进行验证，该方法展示了优越的成绩，有效地削弱了领域差异，同时实现了目标域的适应平衡和已有知识的保持。", "conclusion": "所提出的方法为解决3D人体姿态估计中面临的现实世界适应性和知识保持挑战提供了一个创新的解决方案，通过新的任务设置和框架成功地克服了领域偏移问题。"}}
{"id": "2512.23710", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23710", "abs": "https://arxiv.org/abs/2512.23710", "authors": ["Zahra Abedi", "Richard M. K. van Dijk", "Gijs Wijnholds", "Tessa Verhoef"], "title": "Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration", "comment": null, "summary": "This research digitizes and analyzes the Leidse hoogleraren en lectoren 1575-1815 books written between 1983 and 1985, which contain biographic data about professors and curators of Leiden University. It addresses the central question: how can we design an automated pipeline that integrates OCR, LLM-based interpretation, and database linking to harmonize data from historical document images with existing high-quality database records? We applied OCR techniques, generative AI decoding constraints that structure data extraction, and database linkage methods to process typewritten historical records into a digital format. OCR achieved a Character Error Rate (CER) of 1.08 percent and a Word Error Rate (WER) of 5.06 percent, while JSON extraction from OCR text achieved an average accuracy of 63 percent and, based on annotated OCR, 65 percent. This indicates that generative AI somewhat corrects low OCR performance. Our record linkage algorithm linked annotated JSON files with 94% accuracy and OCR-derived JSON files with 81%. This study contributes to digital humanities research by offering an automated pipeline for interpreting digitized historical documents, addressing challenges like layout variability and terminology differences, and exploring the applicability and strength of an advanced generative AI model.", "AI": {"tldr": "研究提出了一种自动化管道，集成OCR技术、LLM数据提取和数据库链接方法，处理获得了高质量的数字化历史传记数据。", "motivation": "本研究旨在设计一种自动化的管道，集成OCR技术，基于大语言模型的解释，以及数据库链接，实现历史文献图像数据与现有高质量数据库记录的整合。", "method": "本研究采用OCR技术、基于大语言模型的数据提取约束以及数据库链接方法，处理了1983年至1985年间出版的关于莱顿大学教授和管理者的传记数据书籍，将其转换为数字化格式。", "result": "OCR技术达到了1.08%的字符错误率和5.06%的单词错误率，JSON数据提取的平均准确率为63%，基于OCR标注的准确率为65%。而链接算法分别以94%和81%的精度，链接了标注的JSON文件和通过OCR提取的JSON文件。", "conclusion": "本研究提出了一种自动化处理数字化历史文档的管道，解决了布局变异和术语差异的挑战，探索了先进生成式AI模型的应用和效果，对数字人文研究做出了贡献。"}}
{"id": "2512.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23894", "abs": "https://arxiv.org/abs/2512.23894", "authors": ["Krithika Iyer", "Austin Tapp", "Athelia Paulli", "Gabrielle Dickerson", "Syed Muhammad Anwar", "Natasha Lepore", "Marius George Linguraru"], "title": "MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework", "comment": null, "summary": "Quantifying normative pediatric cranial development and suture ossification is crucial for diagnosing and treating growth-related cephalic disorders. Computed tomography (CT) is widely used to evaluate cranial and sutural deformities; however, its ionizing radiation is contraindicated in children without significant abnormalities. Magnetic resonance imaging (MRI) offers radiation free scans with superior soft tissue contrast, but unlike CT, MRI cannot elucidate cranial sutures, estimate skull bone density, or assess cranial vault growth. This study proposes a deep learning driven pipeline for transforming T1 weighted MRIs of children aged 0.2 to 2 years into synthetic CTs (sCTs), predicting detailed cranial bone segmentation, generating suture probability heatmaps, and deriving direct suture segmentation from the heatmaps. With our in-house pediatric data, sCTs achieved 99% structural similarity and a Frechet inception distance of 1.01 relative to real CTs. Skull segmentation attained an average Dice coefficient of 85% across seven cranial bones, and sutures achieved 80% Dice. Equivalence of skull and suture segmentation between sCTs and real CTs was confirmed using two one sided tests (TOST p < 0.05). To our knowledge, this is the first pediatric cranial CT synthesis framework to enable suture segmentation on sCTs derived from MRI, despite MRI's limited depiction of bone and sutures. By combining robust, domain specific variational autoencoders, our method generates perceptually indistinguishable cranial sCTs from routine pediatric MRIs, bridging critical gaps in non invasive cranial evaluation.", "AI": {"tldr": "本研究提出一种基于深度学习的方法，用于从MRI生成合成CT，以解决MRI无法清晰显示儿童颅骨及缝合线的问题。", "motivation": "量化规范的儿科颅骨发育和缝合骨化对于诊断和治疗生长相关的头颅疾病至关重要。但是，广泛用于评估颅缝畸形的计算机断层扫描（CT）由于其电离辐射对于没有明显异常的儿童并不适用。尽管磁共振成像（MRI）提供的软组织对比度更优且无辐射，但不能清晰显示颅缝，或评估颅骨密度和颅骨生长。", "method": "本研究提出了一种基于深度学习的管道，用于将0.2至2岁儿童的T1加权MRI转换为合成CT图像，预测详细的颅骨分割，生成缝线概率热图，并从热图中直接生成缝线分割。", "result": "研究表明，通过引用的儿科数据集，合成CT达到了99%的结构相似性和1.01的自由度生成距离，且颅骨分割在七个颅骨中平均dice系数为85%，缝合线分割 Dice系数为80%。", "conclusion": "该研究首次开发了一种儿童颅骨CT合成框架，实现了在基于MRI的合成CT图像上进行缝合线分割。该方法填补了无创颅骨评估中的关键空白。"}}
{"id": "2512.23711", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23711", "abs": "https://arxiv.org/abs/2512.23711", "authors": ["Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Claudio Pinhanez", "Yago Primerano"], "title": "CAT: A Metric-Driven Framework for Analyzing the Consistency-Accuracy Relation of LLMs under Controlled Input Variations", "comment": null, "summary": "We introduce \\textsc{CAT}, a framework designed to evaluate and visualize the \\emph{interplay} of \\emph{accuracy} and \\emph{response consistency} of Large Language Models (LLMs) under controllable input variations, using multiple-choice (MC) benchmarks as a case study. Current evaluation practices primarily focus on model capabilities such as accuracy or benchmark scores and, more recently, measuring consistency is being considered an essential property for deploying LLMs in high-stake, real-world applications. We argue in this paper that although both dimensions should still be evaluated independently, their inter-dependency also need to be considered for a more nuanced evaluation of LLMs. At the core of \\textsc{CAT} are the \\emph{Consistency-Accuracy Relation (CAR)} curves, which visualize how model accuracy varies with increasing consistency requirements, as defined by the \\emph{Minimum-Consistency Accuracy (MCA)} metric. We further propose the \\emph{Consistency-Oriented Robustness Estimate (CORE)} index, a global metric that combines the area and shape of the CAR curve to quantify the trade-off between accuracy and consistency. We present a practical demonstration of our framework across a diverse set of generalist and domain-specific LLMs, evaluated on multiple MC benchmarks. We also outline how \\textsc{CAT} can be extended beyond MC tasks to support long-form, open-ended evaluations through adaptable scoring functions.", "AI": {"tldr": "本文介绍了一种名为CAT的框架，用于评估和可视化LLMs在可控输入变化下的准确性和一致性之间的相互关系，提出了一种新的度量方法'一致性导向稳健性估计（CORE）指数'。", "motivation": "当前的评估方法主要关注模型能力如准确性或基准分数，最近也开始考虑一致性这一属性，尤其是在高风险、实际应用部署中。本文认为，虽然应该继续独立评估这两个维度，但考虑到它们之间的相互依赖性对于LLMs更加细致的评估是必要的。", "method": "本文提出了一个名为CAT的框架，用于评估和可视化在可控输入变化下，大规模语言模型（LLMs）的准确性与响应一致性之间的交互作用。框架的核心是'一致性-准确性关系（CAR）曲线'，通过定义的最小一致性准确性（MCA）指标，展示模型准确性随着增加的一致性要求的变化情况。此外，本文还提出了'一致性导向稳健性估计（CORE）指数'，作为结合CAR曲线面积和形状的全局度量，量化准确性和一致性之间的权衡。", "result": "本文通过在一系列普遍和特定领域的LLMs上使用多个多项选择性基准测试，展示了框架的实际应用。同时，还概述了如何将CAT扩展到非多项选择性任务，以支持通过可适应的评分函数进行长篇和开放性评估。", "conclusion": "本文提供了一种新的框架，即CAT，用于综合评估LLMs的准确性和一致性，以及它们在不同条件下的相互关系，并通过一些例子展示了框架的实用性和扩展性。"}}
{"id": "2512.23903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23903", "abs": "https://arxiv.org/abs/2512.23903", "authors": ["Charith Wickrema", "Eliza Mace", "Hunter Brown", "Heidys Cabrera", "Nick Krall", "Matthew O'Neill", "Shivangi Sarkar", "Lowell Weissman", "Eric Hughes", "Guido Zarrella"], "title": "Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale", "comment": null, "summary": "We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.", "AI": {"tldr": "研究了高分辨率电光数据集上训练基础模型的实际技术，发现性能受限于数据规模而非模型参数，为数据收集、计算资源配置和优化方案提供了见解。", "motivation": "探索人工智能的扩展行为，建立在高分辨率电光（EO）数据集上训练基础模型的实际技术。这些数据集的规模超过了当前最先进的水平几个数量级。", "method": "我们使用了超过一千万亿像素的商业卫星电光数据和MITRE联邦AI沙箱来训练逐步增加规模的视觉变压器（ViT）骨干模型，并报告了在petascale下的成功和失败模式，同时分析了这些模式对跨越其他遥感模态的领域差距的桥梁的影响。", "result": "我们观察到，即使在这一规模下，性能也符合数据受限模式，而不是模型参数受限模式。", "conclusion": "这些实用见解旨在指导数据收集策略、计算预算和优化时间表，以推进前沿规模遥感基础模型的未来发展。"}}
{"id": "2512.23712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23712", "abs": "https://arxiv.org/abs/2512.23712", "authors": ["Guanghui Wang", "Jinze Yu", "Xing Zhang", "Dayuan Jiang", "Yin Song", "Tomal Deb", "Xuefeng Liu", "Peiyang He"], "title": "STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed for structured data generation, yet output consistency remains critical for production applications. We introduce a comprehensive framework for evaluating and improving consistency in LLM-generated structured outputs. Our approach combines: (1) STED (Semantic Tree Edit Distance), a novel similarity metric balancing semantic flexibility with structural strictness when comparing JSON outputs, and (2) a consistency scoring framework aggregating multiple STED measurements across repeated generations to quantify reliability. Through systematic experiments on synthetic datasets with controlled schema, expression, and semantic variations, we demonstrate STED achieves superior performance ($0.86-0.90$ similarity for semantic equivalents, $0.0$ for structural breaks) compared to existing metrics including TED, BERTScore, and DeepDiff. Applying our framework to benchmark six LLMs reveals significant variations: Claude-3.7-Sonnet demonstrates exceptional consistency, maintaining near-perfect structural reliability even at high temperatures ($T=0.9$), while models like Claude-3-Haiku and Nova-Pro exhibit substantial degradation requiring careful tuning. Our framework enables practical applications including targeted model selection for structured tasks, iterative prompt refinement for reproducible results, and diagnostic analysis to identify inconsistency root causes. This work provides theoretical foundations and practical tools for ensuring reliable structured output generation in LLM-based production systems.", "AI": {"tldr": "本文提出了STED和一个连续性评分框架来衡量和提高大语言模型生成结构化数据的输出一致性，并展示了这些工具在实际应用中的有效性。", "motivation": "随着大语言模型被越来越多地部署来生成结构化数据，其输出的一致性对于生产应用来说变得很重要。因此，该论文旨在解决这一问题，确保在生产系统中生成可靠的结构化数据。", "method": "该论文提出了一个评估和提高大语言模型（LLMs）生成的结构化数据一致性的综合框架，包括STED（语义树编辑距离）——一种比较JSON输出时在语义灵活性和结构严格性之间取得平衡的新相似度度量，以及一个通过多次生成的STED度量来量化可靠性的连续性评分框架。", "result": "通过系统实验，论文显示STED在合成数据集上表现优越，能够准确度量语义等价和结构断裂的相似度。应用该框架评估六种大语言模型显示出一致性上的显著差异。", "conclusion": "该论文为大语言模型生成结构化数据的可靠性提供了理论基础和实用工具，包括模型选择、提示优化和诊断分析等实际应用。"}}
{"id": "2512.23920", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23920", "abs": "https://arxiv.org/abs/2512.23920", "authors": ["Yipei Wang", "Qianye Yang", "Lior Drukker", "Aris T. Papageorghiou", "Yipeng Hu", "J. Alison Noble"], "title": "Learning to learn skill assessment for fetal ultrasound scanning", "comment": null, "summary": "Traditionally, ultrasound skill assessment has relied on expert supervision and feedback, a process known for its subjectivity and time-intensive nature. Previous works on quantitative and automated skill assessment have predominantly employed supervised learning methods, often limiting the analysis to predetermined or assumed factors considered influential in determining skill levels. In this work, we propose a novel bi-level optimisation framework that assesses fetal ultrasound skills by how well a task is performed on the acquired fetal ultrasound images, without using manually predefined skill ratings. The framework consists of a clinical task predictor and a skill predictor, which are optimised jointly by refining the two networks simultaneously. We validate the proposed method on real-world clinical ultrasound videos of scanning the fetal head. The results demonstrate the feasibility of predicting ultrasound skills by the proposed framework, which quantifies optimised task performance as a skill indicator.", "AI": {"tldr": "本文提出了一种新颖的双层优化框架，用于评估胎儿超声技能，通过任务执行情况自动评估技能，避免了主观性和耗时性。", "motivation": "传统的超声技能评估依赖于专家监督和反馈，这种过程主观且耗时。此前的工作主要集中在使用监督学习方法的定量和自动化技能评估上，常常限制于预定或假设的因素影响技能水平评估。因此，需要一个更客观、自动化的技能评估方法。", "method": "提出了一种新的双层优化框架，通过获取的胎儿超声图像任务执行情况来评估胎儿超声技能，无需使用手动预定义的技能评分。该框架由一个临床任务预测器和一个技能预测器组成，通过同时优化两个网络来实现联合优化。", "result": "在真实世界临床超声视频的胎儿头部扫描上验证了提出的方法，结果证明了所提框架预测超声技能的可行性，通过优化的任务表现量化技能指标。", "conclusion": "研究展示了通过优化的任务表现来量化超声技能是可行的。新型双层优化框架为超声技能的自动评估提供了一种有效方法。"}}
{"id": "2512.23713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23713", "abs": "https://arxiv.org/abs/2512.23713", "authors": ["Jahidul Islam", "Md Ataullha", "Saiful Azad"], "title": "PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents", "comment": "6 Pages", "summary": "LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\\% on the development set and 71.6\\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at github.com/jahidulzaid/PyBanglaCodeActAgent.", "AI": {"tldr": "论文讨论了一种新的基于代理的框架BanglaCodeAct，用于从孟加拉语生成Python代码。实验结果显示，应用BanglaCodeAct框架的Qwen3-8B在mHumanEval孟加拉语自然语言到代码数据集上表现最佳，实现了94.0%的开发集通过率和71.6%的盲测通过率。", "motivation": "这项研究的动机在于，尽管大语言模型（LLMs）在英语提示生成代码方面表现出色，但这种进展并不适用于低资源语言。研究者希望通过引入一种基于代理的框架（BanglaCodeAct）解决孟加拉语到Python代码生成的问题。", "method": "研究提出了BanglaCodeAct，一种基于代理的框架，通过多代理提示和迭代自我纠正来生成从孟加拉语到Python的代码。不同于以前的方法依赖于特定任务的微调，BanglaCodeAct利用开源多语言LLM并采用Thought-Code-Observation循环，实现了从孟加拉语指令中动态生成、测试和改进代码的能力。", "result": "研究结果表明，使用BanglaCodeAct框架时，Qwen3-8B在开发集上的pass@1准确率为94.0%，在盲测试集上的准确率为71.6%，显示出在低资源语言（如孟加拉语）到Python的代码生成方面的显著性能。这一成果为孟加拉语到Python的翻译设立了新的基准，并突显了基于代理推理在低资源语言中生成可靠代码的潜力。", "conclusion": "实验结果为孟加拉语到Python的代码生成任务建立了新的基准，并强调了基于代理的方法在处理低资源语言中进行代码生成的有效性。"}}
