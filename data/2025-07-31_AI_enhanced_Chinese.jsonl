{"id": "2507.22159", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22159", "abs": "https://arxiv.org/abs/2507.22159", "authors": ["Vanessa Rebecca Wiyono", "David Anugraha", "Ayu Purwarianti", "Genta Indra Winata"], "title": "IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian", "comment": "Preprint", "summary": "Over 200 million people speak Indonesian, yet the language remains\nsignificantly underrepresented in preference-based research for large language\nmodels (LLMs). Most existing multilingual datasets are derived from English\ntranslations, often resulting in content that lacks cultural and linguistic\nauthenticity. To address this gap, we introduce IndoPref, the first fully\nhuman-authored and multi-domain Indonesian preference dataset specifically\ndesigned to evaluate the naturalness and quality of LLM-generated text. All\nannotations are natively written in Indonesian and evaluated using\nKrippendorff's alpha, demonstrating strong inter-annotator agreement.\nAdditionally, we benchmark the dataset across multiple LLMs and assess the\noutput quality of each model.", "AI": {"tldr": "本研究提出了IndoPref数据集，解决了印尼语在大规模语言模型偏好研究中的代表性不足问题，它是一个完全由人类撰写的、多领域的印尼语偏好数据集。", "motivation": "印尼语使用者超过2亿人，但在大型语言模型（LLM）的偏好研究中，该语言严重不足。现有的多语种数据集大多来源于英语翻译，导致缺乏文化与语言的真实性。本研究旨在填补这一空白。", "method": "本研究介绍了IndoPref，这是首个完全由人类撰写且多领域的印尼语偏好数据集，专门用于评估大型语言模型（LLM）生成文本的自然度和质量。所有标注均为原生印尼语，并使用Krippendorff's alpha评估了强的注释者间一致性。", "result": "研究对多个大型语言模型进行了基准测试，并评估了每个模型的输出质量。", "conclusion": "IndoPref数据集为评估LLM在印尼语生成文本的自然度和质量提供了一个新的基准，有助于提升模型在印尼语环境中的表现。"}}
{"id": "2507.22168", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22168", "abs": "https://arxiv.org/abs/2507.22168", "authors": ["Kimberly Le Truong", "Riccardo Fogliato", "Hoda Heidari", "Zhiwei Steven Wu"], "title": "Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles", "comment": null, "summary": "Current benchmarks for evaluating Large Language Models (LLMs) often do not\nexhibit enough writing style diversity, with many adhering primarily to\nstandardized conventions. Such benchmarks do not fully capture the rich variety\nof communication patterns exhibited by humans. Thus, it is possible that LLMs,\nwhich are optimized on these benchmarks, may demonstrate brittle performance\nwhen faced with \"non-standard\" input. In this work, we test this hypothesis by\nrewriting evaluation prompts using persona-based LLM prompting, a low-cost\nmethod to emulate diverse writing styles. Our results show that, even with\nidentical semantic content, variations in writing style and prompt formatting\nsignificantly impact the estimated performance of the LLM under evaluation.\nNotably, we identify distinct writing styles that consistently trigger either\nlow or high performance across a range of models and tasks, irrespective of\nmodel family, size, and recency. Our work offers a scalable approach to augment\nexisting benchmarks, improving the external validity of the assessments they\nprovide for measuring LLM performance across linguistic variations.", "AI": {"tldr": "研究表明，写作风格变化显著影响LLMs的表现，提供了一种扩展现有基准测试的可扩展方法。", "motivation": "当前评估大型语言模型（LLMs）的基准测试往往缺乏写作风格的多样性，这可能导致这些模型在面对非标准输入时表现不佳。", "method": "通过使用基于角色的LLM提示重写评估提示，以模仿多样的写作风格，从而测试大型语言模型（LLMs）在非标准输入下的表现是否薄弱。", "result": "即使语义内容相同，写作风格和提示格式的变化也会显著影响被评估的LLM的表现。我们发现某些写作风格会一致地触发低或高表现，无论模型的家族、大小和新旧。", "conclusion": "这项工作提出了一种扩展现有基准测试的方法，可以提高评估LLMs在语言变化中表现的外部有效性。"}}
{"id": "2507.22187", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22187", "abs": "https://arxiv.org/abs/2507.22187", "authors": ["Adam M. Morgan", "Adeen Flinker"], "title": "A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models", "comment": null, "summary": "We present an automated pipeline for estimating Verb Frame Frequencies\n(VFFs), the frequency with which a verb appears in particular syntactic frames.\nVFFs provide a powerful window into syntax in both human and machine language\nsystems, but existing tools for calculating them are limited in scale,\naccuracy, or accessibility. We use large language models (LLMs) to generate a\ncorpus of sentences containing 476 English verbs. Next, by instructing an LLM\nto behave like an expert linguist, we had it analyze the syntactic structure of\nthe sentences in this corpus. This pipeline outperforms two widely used\nsyntactic parsers across multiple evaluation datasets. Furthermore, it requires\nfar fewer resources than manual parsing (the gold-standard), thereby enabling\nrapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF\ndatabase with broader verb coverage, finer-grained syntactic distinctions, and\nexplicit estimates of the relative frequencies of structural alternates\ncommonly studied in psycholinguistics. The pipeline is easily customizable and\nextensible to new verbs, syntactic frames, and even other languages. We present\nthis work as a proof of concept for automated frame frequency estimation, and\nrelease all code and data to support future research.", "AI": {"tldr": "研究介绍了一种估算动词框架频率（VFFs）的自动化管道，该管道利用大型语言模型（LLMs）生成语料库并分析句法结构，比传统的句法分析器表现更好，且资源消耗较少，能够进行快速大规模的VFF估算。", "motivation": "传统的VFF估算工具在规模、准确性和可访问性方面存在局限性。", "method": "通过大型语言模型生成包含476个英文动词的句子语料库，并指导模型像专家语言学家一样分析这些句子的句法结构。", "result": "该管道在多个评估数据集上优于广泛使用的句法分析器，且所需资源远少于手动解析。", "conclusion": "研究展示了自动帧频率估算的可行性，并发布了所有代码和数据以支持未来研究。"}}
{"id": "2507.22201", "categories": ["cs.CL", "cs.SI", "physics.soc-ph", "I.2.7; J.4; H.4.0"], "pdf": "https://arxiv.org/pdf/2507.22201", "abs": "https://arxiv.org/abs/2507.22201", "authors": ["L. Toschi", "S. Torrisi", "A. Fronzetti Colladon"], "title": "The role of media memorability in facilitating startups' access to venture capital funding", "comment": null, "summary": "Media reputation plays an important role in attracting venture capital\ninvestment. However, prior research has focused too narrowly on general media\nexposure, limiting our understanding of how media truly influences funding\ndecisions. As informed decision-makers, venture capitalists respond to more\nnuanced aspects of media content. We introduce the concept of media\nmemorability - the media's ability to imprint a startup's name in the memory of\nrelevant investors. Using data from 197 UK startups in the micro and\nnanotechnology sector (funded between 1995 and 2004), we show that media\nmemorability significantly influences investment outcomes. Our findings suggest\nthat venture capitalists rely on detailed cues such as a startup's\ndistinctiveness and connectivity within news semantic networks. This\ncontributes to research on entrepreneurial finance and media legitimation. In\npractice, startups should go beyond frequent media mentions to strengthen brand\nmemorability through more targeted, meaningful coverage highlighting their\nuniqueness and relevance within the broader industry conversation.", "AI": {"tldr": "研究揭示了一种新的媒体影响力指标，即“媒体记忆力”，通过分析197家英国微技术和纳米技术领域的初创企业数据，发现媒体记忆力对融资结果有重大影响。", "motivation": "过去的媒介曝光研究过于狭隘地集中在一般性媒介曝光上，限制了我们对媒体如何真正影响融资决策的理解。本研究的动机是深入探索媒体在投资决策中的更细微影响，尤其是通过引入“媒体记忆力”的概念。", "method": "使用了来自197家英国微技术和纳米技术领域的初创企业（1995年至2004年间获得融资）的数据，分析媒体的记忆力如何影响投资结果。", "result": "研究结果显示，媒体记忆力对投资结果有显著影响。风险投资家依赖于详细的线索，如企业的独特性及其在新闻语义网络中的连通性。", "conclusion": "此研究为创业金融和媒体合法化领域的研究做出了贡献，并建议企业家不仅要频繁地被媒体报道，还应通过更有针对性、更有意义的报道来强化品牌的认知度。"}}
{"id": "2507.22099", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22099", "abs": "https://arxiv.org/abs/2507.22099", "authors": ["Shuqing Li", "Qiang Chen", "Xiaoxue Ren", "Michael R. Lyu"], "title": "Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?", "comment": null, "summary": "Physics Engines (PEs) are fundamental software frameworks that simulate\nphysical interactions in applications ranging from entertainment to\nsafety-critical systems. Despite their importance, PEs suffer from physics\nfailures, deviations from expected physical behaviors that can compromise\nsoftware reliability, degrade user experience, and potentially cause critical\nfailures in autonomous vehicles or medical robotics. Current testing approaches\nfor PE-based software are inadequate, typically requiring white-box access and\nfocusing on crash detection rather than semantically complex physics failures.\nThis paper presents the first large-scale empirical study characterizing\nphysics failures in PE-based software. We investigate three research questions\naddressing the manifestations of physics failures, the effectiveness of\ndetection techniques, and developer perceptions of current detection practices.\nOur contributions include: (1) a taxonomy of physics failure manifestations;\n(2) a comprehensive evaluation of detection methods including deep learning,\nprompt-based techniques, and large multimodal models; and (3) actionable\ninsights from developer experiences for improving detection approaches. To\nsupport future research, we release PhysiXFails, code, and other materials at\nhttps://sites.google.com/view/physics-failure-detection.", "AI": {"tldr": "本文研究了物理引擎中的物理失效问题，提出失效分类法，评估了多种检测技术，并根据开发者反馈提供改进建议。", "motivation": "物理引擎中的物理失效可能会影响软件的可靠性、用户体验，甚至在自动驾驶车辆或医疗机器人中造成关键故障，当前测试方法无法有效检测这些复杂的物理失效。", "method": "通过大规模实证研究来定义和检测物理引擎中的物理失效，包括深度学习、基于提示的技术和大型多模态模型的综合评估，并从开发者经验中提取可操作的见解。", "result": "提出了物理失效表现的分类法，评估了多种检测方法的有效性，并通过开发者经验提供了改进检测方法的实际见解。", "conclusion": "本研究通过对物理引擎中物理失效的大规模实证研究，揭示了这些失效的表现形式，评估了不同检测技术的有效性，并基于开发者反馈提供了改进物理失效检测的建议。"}}
{"id": "2507.22209", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22209", "abs": "https://arxiv.org/abs/2507.22209", "authors": ["Christian Clark", "Byung-Doh Oh", "William Schuler"], "title": "How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?", "comment": null, "summary": "Contextual entropy is a psycholinguistic measure capturing the anticipated\ndifficulty of processing a word just before it is encountered. Recent studies\nhave tested for entropy-related effects as a potential complement to well-known\neffects from surprisal. For convenience, entropy is typically estimated based\non a language model's probability distribution over a word's first subword\ntoken. However, this approximation results in underestimation and potential\ndistortion of true word entropy. To address this, we generate Monte Carlo (MC)\nestimates of word entropy that allow words to span a variable number of tokens.\nRegression experiments on reading times show divergent results between\nfirst-token and MC word entropy, suggesting a need for caution in using\nfirst-token approximations of contextual entropy.", "AI": {"tldr": "研究通过蒙特卡洛方法评估单词的上下文熵，发现其与仅基于单词首标记的估计方法之间存在显著差异，提示应谨慎使用首标记近似法。", "motivation": "传统的通过语言模型概率分布仅基于单词首子词标记来估计熵的方法存在低估和潜在失真的问题，本研究试图解决这一问题。", "method": "本研究使用蒙特卡洛（MC）方法估计单词熵，允许单词跨越可变数量的标记，以解决基于首标记近似单词熵导致的低估和潜在失真问题。", "result": "回归实验表明，基于首标记和MC方法的单词熵之间的阅读时间结果存在分歧，表明对首标记法近似上下文熵应谨慎对待。", "conclusion": "研究结果建议在估计上下文熵时应避免使用首标记近似法，转而采用更精确的方法如蒙特卡洛估计。"}}
{"id": "2507.22100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22100", "abs": "https://arxiv.org/abs/2507.22100", "authors": ["Sicheng Zhang", "Binzhu Xie", "Zhonghao Yan", "Yuli Zhang", "Donghao Zhou", "Xiaofei Chen", "Shi Qiu", "Jiaqi Liu", "Guoyang Xie", "Zhichao Lu"], "title": "Trade-offs in Image Generation: How Do Different Dimensions Interact?", "comment": "Accepted in ICCV 2025, Codebase: https://github.com/fesvhtr/TRIG", "summary": "Model performance in text-to-image (T2I) and image-to-image (I2I) generation\noften depends on multiple aspects, including quality, alignment, diversity, and\nrobustness. However, models' complex trade-offs among these dimensions have\nrarely been explored due to (1) the lack of datasets that allow fine-grained\nquantification of these trade-offs, and (2) the use of a single metric for\nmultiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in\nImage Generation), which spans 10 dimensions (Realism, Originality, Aesthetics,\nContent, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains\n40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we\ndevelop TRIGScore, a VLM-as-judge metric that automatically adapts to various\ndimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I\nand I2I tasks. In addition, we propose the Relation Recognition System to\ngenerate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among\nmodel-specific capabilities. Our experiments demonstrate that DTM consistently\nprovides a comprehensive understanding of the trade-offs between dimensions for\neach type of generative model. Notably, we show that the model's\ndimension-specific weaknesses can be mitigated through fine-tuning on DTM to\nenhance overall performance. Code is available at:\nhttps://github.com/fesvhtr/TRIG", "AI": {"tldr": "研究提出了TRIG-Bench和TRIGScore来全面评估生成模型的性能，并通过维度权衡图揭示了不同模型间的权衡关系。", "motivation": "为了评估和分析文本到图像(Text-to-Image, T2I)和图像到图像(Image-to-Image, I2I)生成模型的性能，特别是解决现有方法在评估模型的多维度性能（如现实性、原创性等）方面的不足。", "method": "引入了TRIG-Bench来涵盖10个不同的维度，包含40,200个样本，并开发了TRIGScore来自动适应不同维度的评估。进行了14个模型的评估，并提出了关系识别系统生成维度权衡图。", "result": "bstract介绍了TRIG-Bench和TRIGScore，用于评估文本到图像和图像到图像生成模型的性能，并讨论了这些模型之间的维度权衡。通过开发维度权衡图来更全面地理解各种生成模型的性能。", "conclusion": "实验表明，维度权衡图可以一致性且全面地提供各种生成模型之间维度权衡的理解。还能通过在维度权衡图上进行微调来减轻模型在维度上的弱点，从而改善整体性能。"}}
{"id": "2507.22219", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22219", "abs": "https://arxiv.org/abs/2507.22219", "authors": ["Dongyub Jude Lee", "Zhenyi Ye", "Pengcheng He"], "title": "RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation", "comment": null, "summary": "Preference-learning methods for machine translation (MT)--such as Direct\nPreference Optimization (DPO)--have achieved impressive gains but depend\nheavily on large, carefully curated triplet datasets and often struggle to\ngeneralize beyond their tuning domains. We propose Reinforcement Learning from\nTeacher-Model Refinement (RLfR), a novel framework that removes reliance on\nstatic triplets by leveraging continuous, high-quality feedback from an\nexternal teacher model (GPT-4o). RLfR frames each translation step as a\nmicro-tutorial: the actor generates a hypothesis, the teacher refines it, and\nthe actor is rewarded based on how closely it aligns with the teacher's\nrefinement. Guided by two complementary signals--(i) negative edit distance,\npromoting lexical and structural fidelity, and (ii) COMET score, ensuring\nsemantic adequacy--the actor progressively learns to emulate the teacher,\nmirroring a human learning process through incremental, iterative improvement.\nOn the FLORES-200 benchmark (English to and from German, Spanish, Chinese,\nKorean, and Japanese), RLfR consistently outperforms both MT-SFT and\npreference-based baselines, significantly improving COMET (semantic adequacy)\nand M-ETA (entity preservation) scores.", "AI": {"tldr": "文章提出了一种更依赖于教师模型连续反馈的新框架--RLfR，它提高了机器翻译的质量和泛化能力，特别在语义恰当性和实体保持性方面表现优异。", "motivation": "现有的偏好学习方法虽然在机器翻译（MT）领域取得了显著的成绩，但严重依赖于大量精心整理的三元组数据集，并且常常无法推广到其调优领域之外。因此，提出了RLfR框架希望能够克服这些限制，提高翻译的质量和泛化能力。", "method": "RLfR是一个新的框架，它通过利用外部教师模型（如GPT-4o）提供的连续、高质量的反馈来减少对静态三元组数据集的依赖。每个翻译步骤都被视为微型教程：演员生成一个假设，教师对其进行改进，演员的奖励基于它与教师改进的接近程度。主要使用两种互补信号：(i) 负编辑距离，促进词汇和结构上的保真度；(ii) COMET得分，确保语义上的准确性。", "result": "在多语言FLORES-200基准测试（包括英语到和自德语、西班牙语、中文、韩语、日语）上，RLfR框架在多个指标上都优于MT-SFT和偏好模型基线，显著提升了COMET（语义恰当性）和M-ETA（实体保持性）得分。", "conclusion": "RLfR框架通过连续反馈和奖励机制，模拟了人类学习过程中的逐步和迭代改进，从而提高了翻译质量和泛化能力。"}}
{"id": "2507.22101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22101", "abs": "https://arxiv.org/abs/2507.22101", "authors": ["Umair Nawaz", "Muhammad Zaigham Zaheer", "Fahad Shahbaz Khan", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "title": "AI in Agriculture: A Survey of Deep Learning Techniques for Crops, Fisheries and Livestock", "comment": null, "summary": "Crops, fisheries and livestock form the backbone of global food production,\nessential to feed the ever-growing global population. However, these sectors\nface considerable challenges, including climate variability, resource\nlimitations, and the need for sustainable management. Addressing these issues\nrequires efficient, accurate, and scalable technological solutions,\nhighlighting the importance of artificial intelligence (AI). This survey\npresents a systematic and thorough review of more than 200 research works\ncovering conventional machine learning approaches, advanced deep learning\ntechniques (e.g., vision transformers), and recent vision-language foundation\nmodels (e.g., CLIP) in the agriculture domain, focusing on diverse tasks such\nas crop disease detection, livestock health management, and aquatic species\nmonitoring. We further cover major implementation challenges such as data\nvariability and experimental aspects: datasets, performance evaluation metrics,\nand geographical focus. We finish the survey by discussing potential open\nresearch directions emphasizing the need for multimodal data integration,\nefficient edge-device deployment, and domain-adaptable AI models for diverse\nfarming environments. Rapid growth of evolving developments in this field can\nbe actively tracked on our project page:\nhttps://github.com/umair1221/AI-in-Agriculture", "AI": {"tldr": "这篇综述提供了关于农业领域中机器学习和深度学习技术应用的全面回顾，并讨论了实施挑战和未来研究方向。", "motivation": "农业部门面临如气候变动、资源限制以及可持续管理需求等重大挑战。解决这些问题需要高效、精准且可扩展的技术解决方案，突显了人工智能的重要性。", "method": "本综述系统地回顾了超过200项研究，涵盖了传统的机器学习方法、先进的深度学习技术（如视觉转换器）和最近的视觉语言基础模型（如CLIP）在农业领域中的应用，重点关注诸如作物病害检测、牲畜健康管理以及水生生物监测等多样化任务。", "result": "综述还讨论了主要的实施挑战，包括数据变动性和实验方面的问题：数据集、性能评估指标和地理重点。", "conclusion": "我们通过讨论潜在的开放研究方向，强调了多模态数据集成、高效边缘设备部署和适用于多样化农业环境的领域自适应AI模型的必要性。"}}
{"id": "2507.22286", "categories": ["cs.CL", "cs.AI", "68T50"], "pdf": "https://arxiv.org/pdf/2507.22286", "abs": "https://arxiv.org/abs/2507.22286", "authors": ["Supantho Rakshit", "Adele Goldberg"], "title": "Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs", "comment": "5 pages, 3 figures, Accepted for publication at the Second\n  International Workshop on Construction Grammars and NLP at the 16th\n  International Conference for Computational Semantics (IWCS) 2025", "summary": "The usage-based constructionist (UCx) approach posits that language comprises\na network of learned form-meaning pairings (constructions) whose use is largely\ndetermined by their meanings or functions, requiring them to be graded and\nprobabilistic. This study investigates whether the internal representations in\nLarge Language Models (LLMs) reflect the proposed function-infused gradience.\nWe analyze the neural representations of the English dative constructions\n(Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of\n$5000$ sentence pairs systematically varied for human-rated preference\nstrength. A macro-level geometric analysis finds that the separability between\nconstruction representations, as measured by Energy Distance or Jensen-Shannon\nDivergence, is systematically modulated by gradient preference strength. More\nprototypical exemplars of each construction occupy more distinct regions in the\nactivation space of LLMs. These results provide strong evidence that LLMs learn\nrich, meaning-infused, graded representations of constructions and offer\nsupport for geometric measures of basic constructionist principles in LLMs.", "AI": {"tldr": "研究通过分析大型语言模型（LLMs）中英语施为结构（双宾结构和介词宾语结构）的神经表示，发现这些表示反映了用法构建主义提出的基于功能的渐变性，表明LLMs学习了富含意义的渐变结构表示。", "motivation": "本研究旨在探索大型语言模型的内部表示是否反映出用法构建主义所主张的功能渐变性。", "method": "研究分析了Pythia-1.4B中英语施为结构的神经表示，使用了一个含有5000个系统变化的句子对的数据集，根据人类评估的偏好强度进行分析。", "result": "宏观几何分析显示，构造表示的可分离性（通过能量距离或杰森-香农散度测量）受到渐变偏好强度的系统性调节。更具代表性的结构实例在LLMs的激活空间中占据更独立的区域。", "conclusion": "研究结果强有力地证明了LLMs学习了丰富的、富含意义的、渐变的构造表示，并支持了LLMs中基本构建主义原则的几何测量。"}}
{"id": "2507.22136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22136", "abs": "https://arxiv.org/abs/2507.22136", "authors": ["Chaofei Qi", "Zhitai Liu", "Jianbin Qiu"], "title": "Color as the Impetus: Transforming Few-Shot Learner", "comment": null, "summary": "Humans possess innate meta-learning capabilities, partly attributable to\ntheir exceptional color perception. In this paper, we pioneer an innovative\nviewpoint on few-shot learning by simulating human color perception mechanisms.\nWe propose the ColorSense Learner, a bio-inspired meta-learning framework that\ncapitalizes on inter-channel feature extraction and interactive learning. By\nstrategically emphasizing distinct color information across different channels,\nour approach effectively filters irrelevant features while capturing\ndiscriminative characteristics. Color information represents the most intuitive\nvisual feature, yet conventional meta-learning methods have predominantly\nneglected this aspect, focusing instead on abstract feature differentiation\nacross categories. Our framework bridges the gap via synergistic color-channel\ninteractions, enabling better intra-class commonality extraction and larger\ninter-class differences. Furthermore, we introduce a meta-distiller based on\nknowledge distillation, ColorSense Distiller, which incorporates prior teacher\nknowledge to augment the student network's meta-learning capacity. We've\nconducted comprehensive coarse/fine-grained and cross-domain experiments on\neleven few-shot benchmarks for validation. Numerous experiments reveal that our\nmethods have extremely strong generalization ability, robustness, and\ntransferability, and effortless handle few-shot classification from the\nperspective of color perception.", "AI": {"tldr": "本研究通过模仿人类的色彩感知能力，提出了ColorSense Learner，一种结合生物启发的元学习框架和基于知识蒸馏的ColorSense Distiller，以增强少样本学习的通用性、鲁棒性和跨域分类能力。", "motivation": "人类具有先天的元学习能力，这部分归因于其出色的色彩感知能力。以往的元学习方法主要集中在抽象特征的分类上，忽视了色彩信息这一最直观的视觉特征，本文旨在通过模仿人类的色彩感知，增强少样本学习的性能。", "method": "模拟人类颜色感知机制，提出了一种创新的少样本学习方法——ColorSense Learner。该方法利用通道间特征提取和互动学习，通过强调不同通道的色彩信息来过滤无关特征，捕捉辨别性特征。此外，还引入了基于知识蒸馏的元蒸馏器ColorSense Distiller，利用教师网络知识提高学生网络的元学习能力。", "result": "通过在11个少样本数据集上的粗细粒度和跨域实验验证，实验结果表明本文方法具备强大的泛化能力、鲁棒性和转移性，且在色彩感知角度下的少样本分类任务上表现出色。", "conclusion": "我们的方法成功展示了颜色感知在少样本学习中的作用，证明了ColorSense Learner框架结合ColorSense Distiller蒸馏器能有效增强少样本学习的性能。"}}
{"id": "2507.22289", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22289", "abs": "https://arxiv.org/abs/2507.22289", "authors": ["Galo Castillo-López", "Gaël de Chalendar", "Nasredine Semmar"], "title": "Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations", "comment": "Accepted for publication at SIGDIAL 2025", "summary": "Intent recognition is a fundamental component in task-oriented dialogue\nsystems (TODS). Determining user intents and detecting whether an intent is\nOut-of-Scope (OOS) is crucial for TODS to provide reliable responses. However,\ntraditional TODS require large amount of annotated data. In this work we\npropose a hybrid approach to combine BERT and LLMs in zero and few-shot\nsettings to recognize intents and detect OOS utterances. Our approach leverages\nLLMs generalization power and BERT's computational efficiency in such\nscenarios. We evaluate our method on multi-party conversation corpora and\nobserve that sharing information from BERT outputs to LLMs leads to system\nperformance improvement.", "AI": {"tldr": "本研究提出了一种结合BERT和大语言模型的方法，在零样本和少样本设置下提高任务导向对话系统识别用户意图和检测出界话语的性能。", "motivation": "任务导向对话系统(TODS)需要大量的标注数据，而在零样本和少样本设置下，识别用户意图和检测OOS话语是保持系统可靠性的关键。", "method": "本研究提出了一种结合BERT和LLMs的方法，在零样本和少样本设置下识别用户的意图并检测出界(OOS)的话语。这种方法利用了LLMs的一般化能力和BERT的计算效率。", "result": "实验结果表明，在多党对话语料库上，这种方法能通过在BERT输出和LLMs之间的信息共享来提高系统性能。", "conclusion": "通过结合BERT和LLMs，可以在零样本和少样本设置下有效提高任务导向对话系统的性能。"}}
{"id": "2507.22152", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.22152", "abs": "https://arxiv.org/abs/2507.22152", "authors": ["A. Piffer", "J. A. Buchner", "A. G. Gennari", "P. Grehten", "S. Sirin", "E. Ross", "I. Ezhov", "M. Rosier", "J. C. Peeken", "M. Piraud", "B. Menze", "A. Guerreiro Stücklin", "A. Jakab", "F. Kofler"], "title": "Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset", "comment": "A. Jakab and F. Kofler have shared last authorship", "summary": "Background Brain tumours are the most common solid malignancies in children,\nencompassing diverse histological, molecular subtypes and imaging features and\noutcomes. Paediatric brain tumours (PBTs), including high- and low-grade\ngliomas (HGG, LGG), medulloblastomas (MB), ependymomas, and rarer forms, pose\ndiagnostic and therapeutic challenges. Deep learning (DL)-based segmentation\noffers promising tools for tumour delineation, yet its performance across\nheterogeneous PBT subtypes and MRI protocols remains uncertain. Methods A\nretrospective single-centre cohort of 174 paediatric patients with HGG, LGG,\nmedulloblastomas (MB), ependymomas, and other rarer subtypes was used. MRI\nsequences included T1, T1 post-contrast (T1-C), T2, and FLAIR. Manual\nannotations were provided for four tumour subregions: whole tumour (WT),\nT2-hyperintensity (T2H), enhancing tumour (ET), and cystic component (CC). A 3D\nnnU-Net model was trained and tested (121/53 split), with segmentation\nperformance assessed using the Dice similarity coefficient (DSC) and compared\nagainst intra- and inter-rater variability. Results The model achieved robust\nperformance for WT and T2H (mean DSC: 0.85), comparable to human annotator\nvariability (mean DSC: 0.86). ET segmentation was moderately accurate (mean\nDSC: 0.75), while CC performance was poor. Segmentation accuracy varied by\ntumour type, MRI sequence combination, and location. Notably, T1, T1-C, and T2\nalone produced results nearly equivalent to the full protocol. Conclusions DL\nis feasible for PBTs, particularly for T2H and WT. Challenges remain for ET and\nCC segmentation, highlighting the need for further refinement. These findings\nsupport the potential for protocol simplification and automation to enhance\nvolumetric assessment and streamline paediatric neuro-oncology workflows.", "AI": {"tldr": "本研究评估了一种基于3D nnU-Net的深度学习模型在儿科脑肿瘤不同子区域分割中的表现，发现其在全肿瘤和T2高信号区域的分割中表现稳健，但对增强肿瘤和囊性成分的分割准确性较低，支持了MRI协议简化和自动化的潜力。", "motivation": "鉴于儿科脑肿瘤的组织学、分子亚型和影像特征各异，其诊断和治疗具有挑战性。基于深度学习的分割技术为肿瘤区分提供了可能的工具，但其在儿科脑肿瘤（PBTs）不同亚型和MRI协议下的表现尚不确定，本研究旨在评估其可行性和准确性。", "method": "使用了单中心回顾性队列，包含174名患有高-低级别胶质瘤（HGG、LGG）、髓母细胞瘤（MB）、室管膜瘤及其他罕见类型的儿科脑肿瘤（PBTs）儿童患者。MRI序列包括T1、T1增强（T1-C）、T2和FLAIR。研究采用手动注释的方式对四个肿瘤子区域进行标注：全肿瘤（WT）、T2高信号（T2H）、增强肿瘤（ET）及囊性成分（CC）。通过3D nnU-Net模型进行训练和测试（训练集/测试集比例：121/53），并使用Dice相似系数（DSC）评估了其分割性能，同时对比了人工注释的一致性和变异性。", "result": "模型在WT和T2H分割方面的表现稳健，平均DSC值为0.85，与人类注释者的一致性相当（平均DSC值为0.86）。ET的分割准确度中等（平均DSC值为0.75），而CC的分割性能较差。分割准确性受肿瘤类型、MRI序列组合及肿瘤位置的影响。值得注意的是，仅使用T1、T1-C和T2也可以达到几乎与完整协议相当的结果。", "conclusion": "对于PBTs，特别是T2H和WT的分割，DL是可行的。但对于ET和CC的分割仍面临挑战，这表明需要进一步改进。这些发现支持了MRI协议简化及自动化操作的潜力，有助于增强儿科神经肿瘤学的体积评估，简化工作流程。"}}
{"id": "2507.22337", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.22337", "abs": "https://arxiv.org/abs/2507.22337", "authors": ["Roxana Petcu", "Samarth Bhargav", "Maarten de Rijke", "Evangelos Kanoulas"], "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers", "comment": null, "summary": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation.", "AI": {"tldr": "论文通过分类法和数据集的创建，提高神经信息检索模型在处理否定信息时的性能。", "motivation": "解决神经模型在处理包含否定信息的任务时表现不佳的问题。", "method": "1) 提出否定句的分类法；2) 创建两个基准数据集；3) 提出基于逻辑的分类机制。", "result": "该论文探讨了传统神经信息检索模型和基于LLM（大规模语言模型）的模型在处理包含否定句的信息搜索任务中的表现，并揭示了这些模型在处理否定句时存在的问题。研究通过引入一种基于哲学、语言学和逻辑学定义的否定分类法，创建了两个用于评估模型性能和增强性能的基准数据集，并提出了一种逻辑分类机制来分析现有数据集上的检索模型表现。该分类法能产生各否定类型分布均衡的数据集，有助于神经模型在NevIR数据集上更快收敛。此外，论文提出了一个分类方案，以揭示现有数据集中否定类型的覆盖率，为改进模型泛化能力提供了见解。", "conclusion": "论文提出的方法和数据集有助于神经信息检索模型更好地处理否定信息，提高模型性能。"}}
{"id": "2507.22194", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.22194", "abs": "https://arxiv.org/abs/2507.22194", "authors": ["Christian Ellis", "Maggie Wigness", "Craig Lennon", "Lance Fiondella"], "title": "Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception", "comment": null, "summary": "Rapid progress in terrain-aware autonomous ground navigation has been driven\nby advances in supervised semantic segmentation. However, these methods rely on\ncostly data collection and labor-intensive ground truth labeling to train deep\nmodels. Furthermore, autonomous systems are increasingly deployed in\nunrehearsed, unstructured environments where no labeled data exists and\nsemantic categories may be ambiguous or domain-specific. Recent zero-shot\napproaches to unsupervised segmentation have shown promise in such settings but\ntypically operate on individual frames, lacking temporal consistency-a critical\nproperty for robust perception in unstructured environments. To address this\ngap we introduce Frontier-Seg, a method for temporally consistent unsupervised\nsegmentation of terrain from mobile robot video streams. Frontier-Seg clusters\nsuperpixel-level features extracted from foundation model\nbackbones-specifically DINOv2-and enforces temporal consistency across frames\nto identify persistent terrain boundaries or frontiers without human\nsupervision. We evaluate Frontier-Seg on a diverse set of benchmark\ndatasets-including RUGD and RELLIS-3D-demonstrating its ability to perform\nunsupervised segmentation across unstructured off-road environments.", "AI": {"tldr": "论文介绍了Frontier-Seg，这是一种用于从移动机器人视频流中获得时间一致的无监督地形分割的方法。", "motivation": "现有的监督语义分割方法依赖于昂贵的数据收集和劳动密集型的地面真相标记来训练深度模型。而在未彩排、非结构化的环境中，没有标记数据，并且语义类别可能模糊或特定于领域。此外，现有的无监督分割方法通常在单个帧上操作，缺乏时间一致性，而这对于非结构化环境中的鲁棒感知至关重要。", "method": "Frontier-Seg采用从基础模型骨干（特别是DINOv2）提取的超像素级特征进行聚类，并在帧间强制时间一致性，以无监督的方式识别持久的地形边界或前沿。", "result": "在包括RUGD和RELLIS-3D在内的多样化的基准数据集上的评估显示，Frontier-Seg能够在非结构化的越野环境中进行无监督分割。", "conclusion": "Frontier-Seg展示了其在非结构化越野环境中进行有效无监督分割的能力。"}}
{"id": "2507.22367", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.22367", "abs": "https://arxiv.org/abs/2507.22367", "authors": ["Jia Li", "Yichao He", "Jiacheng Xu", "Tianhao Luo", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors", "comment": "8 pages, 3 figures, ACM MM 2025", "summary": "Accurate and reliable personality assessment plays a vital role in many\nfields, such as emotional intelligence, mental health diagnostics, and\npersonalized education. Unlike fleeting emotions, personality traits are\nstable, often subconsciously leaked through language, facial expressions, and\nbody behaviors, with asynchronous patterns across modalities. It was hard to\nmodel personality semantics with traditional superficial features and seemed\nimpossible to achieve effective cross-modal understanding. To address these\nchallenges, we propose a novel personality assessment framework called\n\\textit{\\textbf{Traits Run Deep}}. It employs\n\\textit{\\textbf{psychology-informed prompts}} to elicit high-level\npersonality-relevant semantic representations. Besides, it devises a\n\\textit{\\textbf{Text-Centric Trait Fusion Network}} that anchors rich text\nsemantics to align and integrate asynchronous signals from other modalities. To\nbe specific, such fusion module includes a Chunk-Wise Projector to decrease\ndimensionality, a Cross-Modal Connector and a Text Feature Enhancer for\neffective modality fusion and an ensemble regression head to improve\ngeneralization in data-scarce situations. To our knowledge, we are the first to\napply personality-specific prompts to guide large language models (LLMs) in\nextracting personality-aware semantics for improved representation quality.\nFurthermore, extracting and fusing audio-visual apparent behavior features\nfurther improves the accuracy. Experimental results on the AVI validation set\nhave demonstrated the effectiveness of the proposed components, i.e.,\napproximately a 45\\% reduction in mean squared error (MSE). Final evaluations\non the test set of the AVI Challenge 2025 confirm our method's superiority,\nranking first in the Personality Assessment track. The source code will be made\navailable at https://github.com/MSA-LMC/TraitsRunDeep.", "AI": {"tldr": "本文提出了一种称为Traits Run Deep的新性格评估框架，通过心理学导向提示来提取高层次性格相关信息，并整合异步多模态数据，显著提高了性格评估的准确度。在AVI挑战2025上，该方法在性格评估中排名第一。源代码将在指定链接提供。", "motivation": "准确可靠的性格评估在情感智能、心理健康诊断和个人化教育等领域至关重要。与短暂的情绪不同，性格特征是稳定的，并且往往通过语言、面部表情和身体行为等渠道无意识地泄露出来，每个模态之间的模式通常是异步的。传统的浅层特征难以建模性格语义，实现有效的跨模态理解似乎是不可能的。", "method": "提出了一种名为Traits Run Deep的新性格评估框架。该框架使用心理学启发的提示来获取高层次的性格相关语义表示。还设计了一个以文本为中心的性格融合网络，用于降维、跨模态信号对齐和融合，并提高数据稀缺情况下的泛化能力。此外，通过提取和融合视听表观行为特征进一步提高了准确性。", "result": "实验结果表明，AVI验证集上所述组件的有效性，即均方误差（MSE）大约降低了45%。在AVI挑战2025测试集上的最终评估证实了我们方法的优越性，我们的方法在性格评估赛道中排名第一。", "conclusion": "本研究首次将性格特定的提示应用于引导大型语言模型（LLMs）从文本、视觉和语音等多模态信号中提取性格感知的语义表示，从而提高了性格评估的准确性。"}}
{"id": "2507.22264", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22264", "abs": "https://arxiv.org/abs/2507.22264", "authors": ["Shaoan Xie", "Lingjing Kong", "Yujia Zheng", "Yu Yao", "Zeyu Tang", "Eric P. Xing", "Guangyi Chen", "Kun Zhang"], "title": "SmartCLIP: Modular Vision-language Alignment with Identification Guarantees", "comment": "CVPR2025", "summary": "Contrastive Language-Image Pre-training (CLIP)~\\citep{radford2021learning}\nhas emerged as a pivotal model in computer vision and multimodal learning,\nachieving state-of-the-art performance at aligning visual and textual\nrepresentations through contrastive learning. However, CLIP struggles with\npotential information misalignment in many image-text datasets and suffers from\nentangled representation. On the one hand, short captions for a single image in\ndatasets like MSCOCO may describe disjoint regions in the image, leaving the\nmodel uncertain about which visual features to retain or disregard. On the\nother hand, directly aligning long captions with images can lead to the\nretention of entangled details, preventing the model from learning\ndisentangled, atomic concepts -- ultimately limiting its generalization on\ncertain downstream tasks involving short prompts.\n  In this paper, we establish theoretical conditions that enable flexible\nalignment between textual and visual representations across varying levels of\ngranularity. Specifically, our framework ensures that a model can not only\n\\emph{preserve} cross-modal semantic information in its entirety but also\n\\emph{disentangle} visual representations to capture fine-grained textual\nconcepts. Building on this foundation, we introduce \\ours, a novel approach\nthat identifies and aligns the most relevant visual and textual representations\nin a modular manner. Superior performance across various tasks demonstrates its\ncapability to handle information misalignment and supports our identification\ntheory. The code is available at https://github.com/Mid-Push/SmartCLIP.", "AI": {"tldr": "本文提出了一种名为\\ours的新方法，该方法能够灵活地在不同粒度级别上对齐文本和视觉表示，并能解耦视觉信息以捕捉细粒度的文本概念，从而提高了模型在处理信息对齐问题上的性能。", "motivation": "本文旨在解决CLIP模型在处理图像-文本数据集时存在的潜在信息对齐问题，以及由长文本描述引起的纠缠表示问题。这些问题限制了模型在使用短提示的特定下游任务上的泛化能力。", "method": "本文提出了一种新的方法\\ours，该方法能够在不同的粒度级别上建立灵活的文本和视觉表示对齐。这种方法可以确保模型不仅能够完整地保留跨模态的语义信息，还能解耦视觉表示，以捕捉细粒度的文本概念。", "result": "实验结果表明，所提出的方法能够很好地处理信息对齐问题，并在各种任务上表现出了优越的性能，验证了本文的识别理论。", "conclusion": "所提出的方法\\ours成功解决了CLIP模型存在的视觉和文本表示纠缠问题，证明了其提高模型在处理下游任务泛化能力的潜力。"}}
{"id": "2507.22387", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22387", "abs": "https://arxiv.org/abs/2507.22387", "authors": ["Homaira Huda Shomee", "Suman Kalyan Maity", "Sourav Medya"], "title": "PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs", "comment": null, "summary": "Large language models (LLMs) have emerged as transformative approaches in\nseveral important fields. This paper aims for a paradigm shift for patent\nwriting by leveraging LLMs to overcome the tedious patent-filing process. In\nthis work, we present PATENTWRITER, the first unified benchmarking framework\nfor evaluating LLMs in patent abstract generation. Given the first claim of a\npatent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a\nconsistent setup spanning zero-shot, few-shot, and chain-of-thought prompting\nstrategies to generate the abstract of the patent. Our benchmark PATENTWRITER\ngoes beyond surface-level evaluation: we systematically assess the output\nquality using a comprehensive suite of metrics -- standard NLP measures (e.g.,\nBLEU, ROUGE, BERTScore), robustness under three types of input perturbations,\nand applicability in two downstream patent classification and retrieval tasks.\nWe also conduct stylistic analysis to assess length, readability, and tone.\nExperimental results show that modern LLMs can generate high-fidelity and\nstylistically appropriate patent abstracts, often surpassing domain-specific\nbaselines. Our code and dataset are open-sourced to support reproducibility and\nfuture research.", "AI": {"tldr": "本文介绍了一种新的方法，使用大型语言模型(Large Language Models, LLMs)来革新专利写作流程，特别是在专利摘要生成方面。通过对比六种主流LLMs在不同条件下的表现，提出了一套综合性的评价框架PATENTWRITER。实验结果显示，现代LLMs能够生成高保真、风格恰当的专利摘要文本，整体性能优于特定领域的基准模型。", "motivation": "专利撰写的流程通常是繁琐且耗时的。通过使用LLMs，论文希望能加快并优化这一流程，特别是通过自动化生成专利摘要来实现。", "method": "论文开发了初步的综合评估框架PATENTWRITER，用来评测六种不同的LLMs生成专利摘要的能力。评估框架涵盖了零样本学习、少样本学习和链式思考方法在内的多种评测策略。此外，还考量了生成质量的多个维度，如文本长度、易读性和语气。", "result": "实验证明，现代LLMs在生成专利摘要方面表现出色，普遍优于专门领域的基线模型。", "conclusion": "研究结果表明LLMs可以有效地贯通于专利撰写，特别是在生成高质量专利摘要上有巨大潜力。论文提供了代码和数据集支持，为进一步研究打开了大门。"}}
{"id": "2507.22274", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22274", "abs": "https://arxiv.org/abs/2507.22274", "authors": ["Faisal Ahmed"], "title": "HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional Neural Networks for Retinal Image Classification", "comment": "13 pages; 5 figures", "summary": "The analysis of fundus images is critical for the early detection and\ndiagnosis of retinal diseases such as Diabetic Retinopathy (DR), Glaucoma, and\nAge-related Macular Degeneration (AMD). Traditional diagnostic workflows,\nhowever, often depend on manual interpretation and are both time- and\nresource-intensive. To address these limitations, we propose an automated and\ninterpretable clinical decision support framework based on a hybrid feature\nextraction model called HOG-CNN. Our key contribution lies in the integration\nof handcrafted Histogram of Oriented Gradients (HOG) features with deep\nconvolutional neural network (CNN) representations. This fusion enables our\nmodel to capture both local texture patterns and high-level semantic features\nfrom retinal fundus images. We evaluated our model on three public benchmark\ndatasets: APTOS 2019 (for binary and multiclass DR classification), ORIGA (for\nGlaucoma detection), and IC-AMD (for AMD diagnosis); HOG-CNN demonstrates\nconsistently high performance. It achieves 98.5\\% accuracy and 99.2 AUC for\nbinary DR classification, and 94.2 AUC for five-class DR classification. On the\nIC-AMD dataset, it attains 92.8\\% accuracy, 94.8\\% precision, and 94.5 AUC,\noutperforming several state-of-the-art models. For Glaucoma detection on ORIGA,\nour model achieves 83.9\\% accuracy and 87.2 AUC, showing competitive\nperformance despite dataset limitations. We show, through comprehensive\nappendix studies, the complementary strength of combining HOG and CNN features.\nThe model's lightweight and interpretable design makes it particularly suitable\nfor deployment in resource-constrained clinical environments. These results\nposition HOG-CNN as a robust and scalable tool for automated retinal disease\nscreening.", "AI": {"tldr": "本研究提出了一种基于HOG-CNN混合特征提取模型的自动且可解释的临床决策支持框架，用于视网膜疾病的筛查。模型在三个公开数据集上展现出了高精度和高AUC值，表现优于现有方法，并且适合资源受限的临床环境。", "motivation": "研究动机在于解决传统视网膜疾病诊断流程依赖人工分析以及耗费时间与资源的问题。通过自动化流程来提高效率和准确性。", "method": "研究方法是开发一种结合手工直方图（HOG）特征和深度卷积神经网络（CNN）表示的混合模型（HOG-CNN），以捕获视网膜图像中的局部纹理特征和高层次语义特征。", "result": "研究发现该模型在APTOS 2019、ORIGA和IC-AMD三个数据集上的表现优异，如在二分类DR诊断中达到了98.5%的准确率和99.2的AUC值，展示了其优越性和可行性。", "conclusion": "结论是HOG-CNN模型不仅是有效的，而且因其轻量级和可解释性特性，非常适用于资源有限的临床环境。"}}
{"id": "2507.22410", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22410", "abs": "https://arxiv.org/abs/2507.22410", "authors": ["Xiaocheng Yang", "Sumuk Shashidhar", "Dilek Hakkani-Tur"], "title": "Question Generation for Assessing Early Literacy Reading Comprehension", "comment": "2 pages, 1 figure, accepted by SLaTE 2025", "summary": "Assessment of reading comprehension through content-based interactions plays\nan important role in the reading acquisition process. In this paper, we propose\na novel approach for generating comprehension questions geared to K-2 English\nlearners. Our method ensures complete coverage of the underlying material and\nadaptation to the learner's specific proficiencies, and can generate a large\ndiversity of question types at various difficulty levels to ensure a thorough\nevaluation. We evaluate the performance of various language models in this\nframework using the FairytaleQA dataset as the source material. Eventually, the\nproposed approach has the potential to become an important part of autonomous\nAI-driven English instructors.", "AI": {"tldr": "", "motivation": "", "method": "Structure", "result": "{\"tldr\": \"本文提出了一种针对K-2年级英语学习者的阅读理解题目生成的新方法，能全面覆盖学习材料并适应学习者特定的技能水平。使用FairytaleQA数据集进行了评估，有潜力成为自动AI驱动英语教学的一部分。\", \"motivation\": \"评估通过基于内容的互动进行的阅读理解在阅读能力培养过程中扮演重要角色。\", \"method\": \"提出的新方法确保了对底层材料的完全覆盖，并且能够适应学习者的特定技能，生成不同类型的题目，以不同难度水平进行全面评估。\", \"result\": \"使用FairytaleQA数据集评估了各种语言模型的表现。\", \"conclusion\": \"该提议的方法有可能成为自动AI驱动英语教学的重要组成部分。\"}", "conclusion": ""}}
{"id": "2507.22291", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22291", "abs": "https://arxiv.org/abs/2507.22291", "authors": ["Christopher F. Brown", "Michal R. Kazmierski", "Valerie J. Pasquarella", "William J. Rucklidge", "Masha Samsikova", "Chenhui Zhang", "Evan Shelhamer", "Estefania Lahera", "Olivia Wiles", "Simon Ilyushchenko", "Noel Gorelick", "Lihui Lydia Zhang", "Sophia Alj", "Emily Schechter", "Sean Askay", "Oliver Guinan", "Rebecca Moore", "Alexis Boukouvalas", "Pushmeet Kohli"], "title": "AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data", "comment": null, "summary": "Unprecedented volumes of Earth observation data are continually collected\naround the world, but high-quality labels remain scarce given the effort\nrequired to make physical measurements and observations. This has led to\nconsiderable investment in bespoke modeling efforts translating sparse labels\ninto maps. Here we introduce AlphaEarth Foundations, an embedding field model\nyielding a highly general, geospatial representation that assimilates spatial,\ntemporal, and measurement contexts across multiple sources, enabling accurate\nand efficient production of maps and monitoring systems from local to global\nscales. The embeddings generated by AlphaEarth Foundations are the only to\nconsistently outperform all previous featurization approaches tested on a\ndiverse set of mapping evaluations without re-training. We will release a\ndataset of global, annual, analysis-ready embedding field layers from 2017\nthrough 2024.", "AI": {"tldr": "介绍了AlphaEarth Foundations，一种新型嵌入场模型，能够生成准确且高效的地理空间表示图，超越了以往的所有特征化方法。", "motivation": "由于地球观测数据的标注稀缺，需要大量的物理测量和观察工作，为了改进地图生成和监测系统，提出了AlphaEarth Foundations。", "method": "通过整合空间、时间和测量上下文，AlphaEarth Foundations生成了一种高度通用的地理空间表示法。", "result": "AlphaEarth Foundations在多样化制图评估中超越了所有之前测试的特征化方法，且无需重新训练。", "conclusion": "该模型可支持从局部到全球范围的地图和监测系统的高效生成，且计划发布2017-2024年的全球年度分析准备嵌入字段图层数据集。"}}
{"id": "2507.22411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22411", "abs": "https://arxiv.org/abs/2507.22411", "authors": ["Hyeonseok Moon", "Heuiseok Lim"], "title": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models", "comment": "13 pages", "summary": "The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large\nLanguage Models' (LLMs) ability to understand long contexts (LC). It evaluates\nthe capability to identify query-relevant context within extensive\nquery-irrelevant passages. Although this method serves as a widely accepted\nstandard for evaluating long-context understanding, our findings suggest it may\noverestimate the true LC capability of LLMs. We demonstrate that even\nstate-of-the-art models such as GPT-4o struggle to intactly incorporate given\ncontexts made up of solely query-relevant ten sentences. In response, we\nintroduce a novel benchmark, \\textbf{NeedleChain}, where the context consists\nentirely of query-relevant information, requiring the LLM to fully grasp the\ninput to answer correctly. Our benchmark allows for flexible context length and\nreasoning order, offering a more comprehensive analysis of LLM performance.\nAdditionally, we propose an extremely simple yet compelling strategy to improve\nLC understanding capability of LLM: ROPE Contraction. Our experiments with\nvarious advanced LLMs reveal a notable disparity between their ability to\nprocess large contexts and their capacity to fully understand them. Source code\nand datasets are available at https://github.com/hyeonseokk/NeedleChain", "AI": {"tldr": "我们引入了 NeedleChain 基准测试和 ROPE Contraction 策略，以更准确地评估和提升 LLM 对长上下文的理解能力。", "motivation": "尽管 Needle-in-a-Haystack (NIAH) 基准测试被广泛用于评估大语言模型 (LLM) 在理解长上下文方面的表现，但研究表明这一方法可能过高估计了 LLM 的真实能力。因此有必要提出一种新的评估方法。", "method": "我们提出了一种新的基准测试 NeedleChain，它仅包含与查询相关的上下文信息，并提出了一种名为 ROPE Contraction 的简单策略来提高 LLM 对长上下文的理解能力。", "result": "实验结果显示，尽管先进的 LLM 模型如 GPT-4o 在处理长上下文方面有较好的表现，但其完全理解上下文的能力仍有较大的提升空间。", "conclusion": "NeedleChain 基准测试和 ROPE Contraction 策略为评估和提升 LLM 的长上下文理解能力提供了一个新的视角。"}}
{"id": "2507.22316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22316", "abs": "https://arxiv.org/abs/2507.22316", "authors": ["Chi Ding", "Qingchao Zhang", "Ge Wang", "Xiaojing Ye", "Yunmei Chen"], "title": "LAMA-Net: A Convergent Network Architecture for Dual-Domain Reconstruction", "comment": "arXiv admin note: substantial text overlap with arXiv:2410.21111", "summary": "We propose a learnable variational model that learns the features and\nleverages complementary information from both image and measurement domains for\nimage reconstruction. In particular, we introduce a learned alternating\nminimization algorithm (LAMA) from our prior work, which tackles two-block\nnonconvex and nonsmooth optimization problems by incorporating a residual\nlearning architecture in a proximal alternating framework. In this work, our\ngoal is to provide a complete and rigorous convergence proof of LAMA and show\nthat all accumulation points of a specified subsequence of LAMA must be Clarke\nstationary points of the problem. LAMA directly yields a highly interpretable\nneural network architecture called LAMA-Net. Notably, in addition to the\nresults shown in our prior work, we demonstrate that the convergence property\nof LAMA yields outstanding stability and robustness of LAMA-Net in this work.\nWe also show that the performance of LAMA-Net can be further improved by\nintegrating a properly designed network that generates suitable initials, which\nwe call iLAMA-Net. To evaluate LAMA-Net/iLAMA-Net, we conduct several\nexperiments and compare them with several state-of-the-art methods on popular\nbenchmark datasets for Sparse-View Computed Tomography.", "AI": {"tldr": "本文提出了一种利用图像与测量域互补信息进行图像重建的可学习变分模型，并证明了LAMA算法的收敛性，从而提升了模型的稳定性和鲁棒性。通过实验验证了LAMA-Net/iLAMA-Net的性能优势。", "motivation": "本研究的动机是在图像重建过程中，结合图像和测量域的互补信息，提出一种高效且鲁棒的可学习变分模型。", "method": "我们提出了一种可学习的变分模型，该模型从图像和测量域中学习特征并利用互补信息来进行图像重建。我们引入了一种名为LAMA的学习交替最小化算法，该算法通过对抗凸和非光滑优化问题进行处理，通过在邻近交替框架中结合残差学习架构来实现。在这个工作中，我们的目标是提供一个完整的关于LAMA的收敛性证明，并展示LAMA的指定子序列的所有累积点必须是问题的Clarke平稳点。LAMA直接产生一个高度可解释的神经网络架构，称为LAMA-Net。此外，我们证明LAMA的收敛性属性可以大大提高LAMA-Net的稳定性和鲁棒性。我们还展示了通过结合一个适当设计的生成合适初始值的网络（我们称之为iLAMA-Net），可以进一步提升LAMA-Net的性能。", "result": "我们的实验对比了几种最新方法在稀疏视图计算机断层扫描的数据集上测试了LAMA-Net/iLAMA-Net，证明了其卓越的性能。", "conclusion": "通过引入LAMA和开发LAMA-Net，我们提供了一个高度解释性的神经网络架构，结合了收敛性证明和实验验证，显示了其在稳定性和性能上的提升。"}}
{"id": "2507.22445", "categories": ["cs.CL", "cs.AI", "H.1.2; I.2.4; I.2.0; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22445", "abs": "https://arxiv.org/abs/2507.22445", "authors": ["Jill Walker Rettberg", "Hermann Wigers"], "title": "AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini", "comment": "This project has received funding from the European Union's Horizon\n  2020 research and innovation programme under grant agreement number\n  101142306. The project is also supported by the Center for Digital Narrative,\n  which is funded by the Research Council of Norway through its Centres of\n  Excellence scheme, project number 332643", "summary": "Can a language model trained largely on Anglo-American texts generate stories\nthat are culturally relevant to other nationalities? To find out, we generated\n11,800 stories - 50 for each of 236 countries - by sending the prompt \"Write a\n1500 word potential {demonym} story\" to OpenAI's model gpt-4o-mini. Although\nthe stories do include surface-level national symbols and themes, they\noverwhelmingly conform to a single narrative plot structure across countries: a\nprotagonist lives in or returns home to a small town and resolves a minor\nconflict by reconnecting with tradition and organising community events.\nReal-world conflicts are sanitised, romance is almost absent, and narrative\ntension is downplayed in favour of nostalgia and reconciliation. The result is\na narrative homogenisation: an AI-generated synthetic imaginary that\nprioritises stability above change and tradition above growth. We argue that\nthe structural homogeneity of AI-generated narratives constitutes a distinct\nform of AI bias, a narrative standardisation that should be acknowledged\nalongside the more familiar representational bias. These findings are relevant\nto literary studies, narratology, critical AI studies, NLP research, and\nefforts to improve the cultural alignment of generative AI.", "AI": {"tldr": "研究利用语言模型生成有关不同国家的故事，发现这些故事在叙事结构上具有同质性，反映出AI偏见——叙事标准化。", "motivation": "研究使用主要基于英美文本训练的语言模型能否生成对其他国家和地区有文化相关性的故事。", "method": "通过向OpenAI的gpt-4o-mini模型发送特定提示，生成了11,800个故事，每个国家50个故事，涵盖了236个国家。提示内容为“写一个潜在的{地名}故事，长度为1500字。”", "result": "生成的故事虽然包含了一些表面的国家象征和主题，但它们普遍呈现出同一种叙事结构：主人公生活在或重返一个小城镇，并通过重新连接传统和组织社区活动来解决一个轻微的冲突。真实的冲突被净化，浪漫几乎不存在，叙事紧张程度也被降低。这导致了叙事的同质化现象。", "conclusion": "AI生成的叙事在结构上的同质性构成了一种新型的AI偏见——叙事标准化，这应该被文学研究、叙事学、批判性AI研究、自然语言处理研究以及改进生成AI文化对齐的努力所关注。"}}
{"id": "2507.22321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22321", "abs": "https://arxiv.org/abs/2507.22321", "authors": ["Yuzhen Gao", "Qianqian Wang", "Yongheng Sun", "Cui Wang", "Yongquan Liang", "Mingxia Liu"], "title": "Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment", "comment": null, "summary": "Accurate identification of late-life depression (LLD) using structural brain\nMRI is essential for monitoring disease progression and facilitating timely\nintervention. However, existing learning-based approaches for LLD detection are\noften constrained by limited sample sizes (e.g., tens), which poses significant\nchallenges for reliable model training and generalization. Although\nincorporating auxiliary datasets can expand the training set, substantial\ndomain heterogeneity, such as differences in imaging protocols, scanner\nhardware, and population demographics, often undermines cross-domain\ntransferability. To address this issue, we propose a Collaborative Domain\nAdaptation (CDA) framework for LLD detection using T1-weighted MRIs. The CDA\nleverages a Vision Transformer (ViT) to capture global anatomical context and a\nConvolutional Neural Network (CNN) to extract local structural features, with\neach branch comprising an encoder and a classifier. The CDA framework consists\nof three stages: (a) supervised training on labeled source data, (b)\nself-supervised target feature adaptation and (c) collaborative training on\nunlabeled target data. We first train ViT and CNN on source data, followed by\nself-supervised target feature adaptation by minimizing the discrepancy between\nclassifier outputs from two branches to make the categorical boundary clearer.\nThe collaborative training stage employs pseudo-labeled and augmented\ntarget-domain MRIs, enforcing prediction consistency under strong and weak\naugmentation to enhance domain robustness and generalization. Extensive\nexperiments conducted on multi-site T1-weighted MRI data demonstrate that the\nCDA consistently outperforms state-of-the-art unsupervised domain adaptation\nmethods.", "AI": {"tldr": "The paper proposes a Collaborative Domain Adaptation (CDA) framework for accurate late-life depression (LLD) detection using T1-weighted MRIs, combining a Vision Transformer (ViT) and a Convolutional Neural Network (CNN) to handle limited sample sizes and domain heterogeneity.", "motivation": "To improve the accuracy of LLD detection in MRI scans by overcoming the challenges of limited sample sizes and domain heterogeneity between datasets while enabling reliable model training and generalization.", "method": "The CDA framework for LLD detection uses a combination of a Vision Transformer (ViT) for global anatomical context and a Convolutional Neural Network (CNN) for local structural features. It includes stages of supervised training, self-supervised target feature adaptation, and collaborative training with pseudo-labeled and augmented data.", "result": "Experiments on multi-site T1-weighted MRI data show that the CDA framework outperforms existing unsupervised domain adaptation methods in LLD detection.", "conclusion": "The CDA framework offers a robust solution for LLD detection, demonstrating better performance and generalization capabilities compared to current methods, which is crucial for improving diagnosis and monitoring of LLD in clinical practice."}}
{"id": "2507.22448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22448", "abs": "https://arxiv.org/abs/2507.22448", "authors": ["Jingwei Zuo", "Maksim Velikanov", "Ilyas Chahed", "Younes Belkada", "Dhia Eddine Rhayem", "Guillaume Kunsch", "Hakim Hacid", "Hamza Yous", "Brahim Farhat", "Ibrahim Khadraoui", "Mugariya Farooq", "Giulia Campesan", "Ruxandra Cojocaru", "Yasser Djilali", "Shi Hu", "Iheb Chaabane", "Puneesh Khanna", "Mohamed El Amine Seddik", "Ngoc Dung Huynh", "Phuc Le Khac", "Leen AlQadi", "Billel Mokeddem", "Mohamed Chami", "Abdalgader Abubaker", "Mikhail Lubinets", "Kacper Piskorski", "Slim Frikha"], "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance", "comment": "Technical report of Falcon-H1 model series", "summary": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research.", "AI": {"tldr": "Falcon-H1 is a new series of hybrid architecture large language models featuring high performance and efficiency. It combines Transformer-based attention with State Space Models to improve long-context memory and computational efficiency. Falcon-H1 models outperform many larger models while using fewer parameters.", "motivation": "To improve performance and efficiency of large language models across diverse use cases by combining different architectural approaches.", "method": "Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention mechanisms with State Space Models for better long-context memory and efficiency. Multiple configurations are available, including base and instruction-tuned variants with varying parameter sizes.", "result": "Falcon-H1 models show state-of-the-art performance, especially in reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge, often matching or outperforming larger models with fewer parameters.", "conclusion": "Falcon-H1 is a highly efficient and performant series of large language models with various configurations, released with open-source licenses for accessible AI research."}}
{"id": "2507.22342", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22342", "abs": "https://arxiv.org/abs/2507.22342", "authors": ["Yuki Fujimura", "Takahiro Kushida", "Kazuya Kitano", "Takuya Funatomi", "Yasuhiro Mukaigawa"], "title": "UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views", "comment": "Project page: https://yfujimura.github.io/UFV-Splatter_page/", "summary": "This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS)\nframework designed to handle unfavorable input views. A common rendering setup\nfor training feed-forward approaches places a 3D object at the world origin and\nrenders it from cameras pointed toward the origin -- i.e., from favorable\nviews, limiting the applicability of these models to real-world scenarios\ninvolving varying and unknown camera poses. To overcome this limitation, we\nintroduce a novel adaptation framework that enables pretrained pose-free\nfeed-forward 3DGS models to handle unfavorable views. We leverage priors\nlearned from favorable images by feeding recentered images into a pretrained\nmodel augmented with low-rank adaptation (LoRA) layers. We further propose a\nGaussian adapter module to enhance the geometric consistency of the Gaussians\nderived from the recentered inputs, along with a Gaussian alignment method to\nrender accurate target views for training. Additionally, we introduce a new\ntraining strategy that utilizes an off-the-shelf dataset composed solely of\nfavorable images. Experimental results on both synthetic images from the Google\nScanned Objects dataset and real images from the OmniObject3D dataset validate\nthe effectiveness of our method in handling unfavorable input views.", "AI": {"tldr": "A novel adaptation of pose-free, feed-forward 3D Gaussian Splatting (3DGS) method is introduced to effectively process unfavorable input views, enhancing the model's applicability in varying real-world conditions.", "motivation": "The motivation behind this research is to address a limitation of common feed-forward approaches in deep learning, which have been trained with an assumption of favorable input views, making them less effective for real-world scenarios where camera poses are unknown or varying.", "method": "This paper introduces a pose-free, feed-forward 3D Gaussian Splatting (3DGS) framework adapted for handling unfavorable input views. It leverages previously learned priors from favorable images and integrates low-rank adaptation (LoRA) layers and a Gaussian adapter module to enhance training models for rendering less favorable views accurately.", "result": "The proposed method was found to be effective in handling unfavorable input views, as validated by experiments on images from the Google Scanned Objects and OmniObject3D datasets.", "conclusion": "The novel approach enhances the applicability of feed-forward 3DGS models by adding a capability to handle less favorable camera poses, expanding potential applications to real-world scenarios."}}
{"id": "2507.22457", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22457", "abs": "https://arxiv.org/abs/2507.22457", "authors": ["Tian Yun", "Chen Sun", "Ellie Pavlick"], "title": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments about Large Language Models", "comment": "CONLL 2025. Project webpage: https://abstract-reasoner-llm.github.io/", "summary": "Recent work has argued that large language models (LLMs) are not \"abstract\nreasoners\", citing their poor zero-shot performance on a variety of challenging\ntasks as evidence. We revisit these experiments in order to add nuance to the\nclaim. First, we show that while LLMs indeed perform poorly in a zero-shot\nsetting, even tuning a small subset of parameters for input encoding can enable\nnear-perfect performance. However, we also show that this finetuning does not\nnecessarily transfer across datasets. We take this collection of empirical\nresults as an invitation to (re-)open the discussion of what it means to be an\n\"abstract reasoner\", and why it matters whether LLMs fit the bill.", "AI": {"tldr": "尽管大型语言模型（LLMs）在零样本设置中表现不佳，但对其输入编码参数进行少量微调可以显著提升性能，不过这种微调并不一定会跨数据集转移。这一结果有助于重新审视什么是“抽象推理者”及其对LLMs的意义。", "motivation": "回应关于LLMs是否为“抽象推理者”的质疑，通过实验表明这些模型在零样本设置下虽然表现不佳，但通过参数微调可以极大地提升性能，并探讨这结果对未来研究的重要性。", "method": "重新审视先前的实验，重点研究在零样本设置下LLMs的表现，并探索仅微调输入编码参数对其性能的影响。", "result": "发现即使只对一小部分输入编码参数进行微调，也可以使LLMs在某些任务上达到近乎完美的性能，但这种性能提升不能很好地迁移到其他数据集。", "conclusion": "这些发现扩展了关于LLMs作为“抽象推理者”的讨论，进一步阐明了对细微调参如何影响模型能力和跨任务迁移的重要性。"}}
{"id": "2507.22346", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.22346", "abs": "https://arxiv.org/abs/2507.22346", "authors": ["Pei Deng", "Wenqian Zhou", "Hanlin Wu"], "title": "DeltaVLM: Interactive Remote Sensing Image Change Analysis via Instruction-guided Difference Perception", "comment": "12 pages, 5 figures. Submitted to IEEE Transactions on Geoscience and\n  Remote Sensing (TGRS). Code and dataset are available at\n  https://github.com/hanlinwu/DeltaVLM", "summary": "Accurate interpretation of land-cover changes in multi-temporal satellite\nimagery is critical for real-world scenarios. However, existing methods\ntypically provide only one-shot change masks or static captions, limiting their\nability to support interactive, query-driven analysis. In this work, we\nintroduce remote sensing image change analysis (RSICA) as a new paradigm that\ncombines the strengths of change detection and visual question answering to\nenable multi-turn, instruction-guided exploration of changes in bi-temporal\nremote sensing images. To support this task, we construct ChangeChat-105k, a\nlarge-scale instruction-following dataset, generated through a hybrid\nrule-based and GPT-assisted process, covering six interaction types: change\ncaptioning, classification, quantification, localization, open-ended question\nanswering, and multi-turn dialogues. Building on this dataset, we propose\nDeltaVLM, an end-to-end architecture tailored for interactive RSICA. DeltaVLM\nfeatures three innovations: (1) a fine-tuned bi-temporal vision encoder to\ncapture temporal differences; (2) a visual difference perception module with a\ncross-semantic relation measuring (CSRM) mechanism to interpret changes; and\n(3) an instruction-guided Q-former to effectively extract query-relevant\ndifference information from visual changes, aligning them with textual\ninstructions. We train DeltaVLM on ChangeChat-105k using a frozen large\nlanguage model, adapting only the vision and alignment modules to optimize\nefficiency. Extensive experiments and ablation studies demonstrate that\nDeltaVLM achieves state-of-the-art performance on both single-turn captioning\nand multi-turn interactive change analysis, outperforming existing multimodal\nlarge language models and remote sensing vision-language models. Code, dataset\nand pre-trained weights are available at https://github.com/hanlinwu/DeltaVLM.", "AI": {"tldr": "本文提出了遥感图像变化分析（RSICA）这一新范式，结合变化检测和视觉问答，以支持双向遥感图像中多轮、指令引导变化探索，并提出了DeltaVLM架构，实现了在变化分析数据集上的出色表现。", "motivation": "动机在于提高对多时态卫星影图像中土地覆盖变化的精确解释能力，并克服现有方法提供单一变化掩码或静态描述的限制，以支持互动、查询驱动的分析。", "method": "该论文提出了一种新的范式，称为遥感图像变化分析（RSICA），它结合了变化检测和视觉问答的优势，使用户能够在双向遥感图像中进行多轮、指令引导的变化探索。其主要创新点包括：1. 细调后的双向视觉编码器，用于捕捉时间差异；2. 具有跨语义关系测量（CSRM）机制的视觉差异感知模块，用于解释变化；3. 指令引导的Q-former，用于从视觉变化中有效提取与查询相关的信息，并与文本指令对齐。", "result": "通过广泛的实验和消融研究，DeltaVLM在单轮描述和多轮交互变化分析方面达到了最先进的性能，超越了现有的多模态大规模语言模型和遥感视觉-语言模型。", "conclusion": "研究表明，DeltaVLM能够有效地支持对双向遥感图像进行多轮、指令引导的变化分析，表现优于当前的方法，推动了该领域的技术进步。"}}
{"id": "2507.22462", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22462", "abs": "https://arxiv.org/abs/2507.22462", "authors": ["Jian Yang", "Wei Zhang", "Shukai Liu", "Linzheng Chai", "Yingshui Tan", "Jiaheng Liu", "Ge Zhang", "Wangchunshu Zhou", "Guanglin Niu", "Zhoujun Li", "Binyuan Hui", "Junyang Lin"], "title": "IFEvalCode: Controlled Code Generation", "comment": "10 pages", "summary": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions.", "AI": {"tldr": "文章提出了一种用于提高代码生成大模型指令跟随能力的方法，并引入了一个新的多语言基准IFEvalCode来评估模型的性能。实验结果表明闭源模型在这一方面表现优越。", "motivation": "现有的代码生成大模型虽然可以将自然语言描述转化为功能性代码，但在实际应用中还需要严格遵循一些详细的规范要求，诸如编码风格、行数限制和结构约束等。", "method": "通过引入正向和反向约束生成方法来提高代码生成大模型在受控代码生成任务中的指令跟随能力，使输出更能符合人类定义的规范。", "result": "引入了IFEvalCode，这是一个包含七种编程语言（Python、Java、JavaScript、TypeScript、Shell、C++和C#）且每种语言都有中英两种查询类型的多语言基准，总共包含了1.6K个测试样本。它将评估分成正确性(Corr.)和指令跟随性(Instr.)两个指标，提供了更加细化的评估方法。", "conclusion": "实验结果表明，闭源模型在可控代码生成方面优于开源模型，并揭示了模型生成正确代码的能力与其生成精准遵循指令的代码能力之间存在显著差距。"}}
{"id": "2507.22353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22353", "abs": "https://arxiv.org/abs/2507.22353", "authors": ["Yunseok Oh", "Dong-Wan Choi"], "title": "FaceGCD: Generalized Face Discovery via Dynamic Prefix Generation", "comment": "BMVC 2025 Accepted", "summary": "Recognizing and differentiating among both familiar and unfamiliar faces is a\ncritical capability for face recognition systems and a key step toward\nartificial general intelligence (AGI). Motivated by this ability, this paper\nintroduces generalized face discovery (GFD), a novel open-world face\nrecognition task that unifies traditional face identification with generalized\ncategory discovery (GCD). GFD requires recognizing both labeled and unlabeled\nknown identities (IDs) while simultaneously discovering new, previously unseen\nIDs. Unlike typical GCD settings, GFD poses unique challenges due to the high\ncardinality and fine-grained nature of face IDs, rendering existing GCD\napproaches ineffective. To tackle this problem, we propose FaceGCD, a method\nthat dynamically constructs instance-specific feature extractors using\nlightweight, layer-wise prefixes. These prefixes are generated on the fly by a\nHyperNetwork, which adaptively outputs a set of prefix generators conditioned\non each input image. This dynamic design enables FaceGCD to capture subtle\nidentity-specific cues without relying on high-capacity static models.\nExtensive experiments demonstrate that FaceGCD significantly outperforms\nexisting GCD methods and a strong face recognition baseline, ArcFace, achieving\nstate-of-the-art results on the GFD task and advancing toward open-world face\nrecognition.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.22478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22478", "abs": "https://arxiv.org/abs/2507.22478", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "title": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL", "comment": "16 pages, 2 figures, work in progress", "summary": "Large language models (LLMs) have demonstrated strong performance in\ntranslating natural language questions into SQL queries (Text-to-SQL). In\ncontrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters\ncurrently underperform on Text-to-SQL tasks due to their limited logical\nreasoning capabilities. However, SLMs offer inherent advantages in inference\nspeed and suitability for edge deployment. To explore their potential in\nText-to-SQL applications, we leverage recent advancements in post-training\ntechniques. Specifically, we used the open-source SynSQL-2.5M dataset to\nconstruct two derived datasets: SynSQL-Think-916K for SQL generation and\nSynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised\nfine-tuning and reinforcement learning-based post-training to the SLM, followed\nby inference using a corrective self-consistency approach. Experimental results\nvalidate the effectiveness and generalizability of our method, SLM-SQL. On the\nBIRD development set, the five evaluated models achieved an average improvement\nof 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy\n(EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset,\nmodel, and code to github: https://github.com/CycloneBoy/slm_sql.", "AI": {"tldr": "研究介绍了如何通过先进的后训练技术改小型语言模型，提升其在Text-to-SQL任务的表现。模型和代码将在GitHub上公开。", "motivation": "尽管小型语言模型(SLM)目前在Text-to-SQL任务上表现不佳，但由于这些模型的优势在于推理速度和更适合边缘部署，所以探索将这些模型运用到Text-to-SQL应用中的潜力是有意义的。", "method": "利用开源的SynSQL-2.5M数据集构建了两个派生数据集：SynSQL-Think-916K用于SQL生成，SynSQL-Merge-Think-310K用于SQL合并修订。通过监督精细调整和基于强化学习的后训练技术，以及使用纠正自一致性方法进行推断，对小型语言模型进行了改进。", "result": "实验结果验证了所提出方法SLM-SQL的有效性和可推广性。在BIRD开发集上，五个评估模型平均提升了31.4分。特别是，0.5B模型达到了56.87%的执行准确性（EX），而1.5B模型达到了67.08%的EX。", "conclusion": "该研究展示了通过先进的后训练技术如何改进小型语言模型，以增强其在SQL生成任务中的性能。"}}
{"id": "2507.22360", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22360", "abs": "https://arxiv.org/abs/2507.22360", "authors": ["Kunyang Li", "Jeffrey A Chan Santiago", "Sarinda Dhanesh Samarasinghe", "Gaowen Liu", "Mubarak Shah"], "title": "GVD: Guiding Video Diffusion Model for Scalable Video Distillation", "comment": null, "summary": "To address the larger computation and storage requirements associated with\nlarge video datasets, video dataset distillation aims to capture spatial and\ntemporal information in a significantly smaller dataset, such that training on\nthe distilled data has comparable performance to training on all of the data.\nWe propose GVD: Guiding Video Diffusion, the first diffusion-based video\ndistillation method. GVD jointly distills spatial and temporal features,\nensuring high-fidelity video generation across diverse actions while capturing\nessential motion information. Our method's diverse yet representative\ndistillations significantly outperform previous state-of-the-art approaches on\nthe MiniUCF and HMDB51 datasets across 5, 10, and 20 Instances Per Class (IPC).\nSpecifically, our method achieves 78.29 percent of the original dataset's\nperformance using only 1.98 percent of the total number of frames in MiniUCF.\nAdditionally, it reaches 73.83 percent of the performance with just 3.30\npercent of the frames in HMDB51. Experimental results across benchmark video\ndatasets demonstrate that GVD not only achieves state-of-the-art performance\nbut can also generate higher resolution videos and higher IPC without\nsignificantly increasing computational cost.", "AI": {"tldr": "Introduces GVD, a novel method for video dataset distillation that captures essential spatial and temporal features with high fidelity, outperforming previous methods while reducing computational requirements.", "motivation": "To address the issue of large computation and storage requirements associated with large video datasets, by creating a smaller, yet high-performance, distilled dataset.", "method": "GVD: Guiding Video Diffusion, a first-ever diffusion-based video distillation method that jointly distills spatial and temporal features for high-fidelity video generation.", "result": "Outperforms previous state-of-the-art methods on MiniUCF and HMDB51 datasets with significantly fewer instances and frames while maintaining high performance and generating higher resolution videos. Achieves 78.29% performance with 1.98% frames on MiniUCF and 73.83% with 3.30% frames on HMDB51.", "conclusion": "GVD not only reduces computational and storage needs but also achieves state-of-the-art performance and generates higher resolution videos without increasing computational cost significantly."}}
{"id": "2507.22533", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22533", "abs": "https://arxiv.org/abs/2507.22533", "authors": ["Dongchen Li", "Jitao Liang", "Wei Li", "Xiaoyu Wang", "Longbing Cao", "Kun Yu"], "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records", "comment": null, "summary": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists.", "AI": {"tldr": "提出CliCARE框架以解决大型语言模型在癌症电子健康记录中处理长周期数据、多语言支持和临床幻觉问题，通过将不结构化的数据转化为患者特定的时间知识图来提供临床决策支持，并在大规模数据集上进行了验证，显示优于基线方法。", "motivation": "解决大型语言模型处理复杂、长期癌症电子健康记录的能力不足，以及临床幻觉风险增加的问题。", "method": "通过将病历转化为患者特定的时间知识图，结合规范指南知识图进行临床决策支持。", "result": "在私人中文癌症数据集和公共英文MIMIC-IV数据集上，CliCARE有显著优于基线方法的表现。", "conclusion": "CliCARE能够提供基于证据的临床决策支持，临床有效性通过与专家肿瘤学家的评估高度相关得到验证。"}}
{"id": "2507.22361", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22361", "abs": "https://arxiv.org/abs/2507.22361", "authors": ["Aria Salari", "Abtin Djavadifar", "Xiangrui Liu", "Homayoun Najjaran"], "title": "Object Recognition Datasets and Challenges: A Review", "comment": null, "summary": "Object recognition is among the fundamental tasks in the computer vision\napplications, paving the path for all other image understanding operations. In\nevery stage of progress in object recognition research, efforts have been made\nto collect and annotate new datasets to match the capacity of the\nstate-of-the-art algorithms. In recent years, the importance of the size and\nquality of datasets has been intensified as the utility of the emerging deep\nnetwork techniques heavily relies on training data. Furthermore, datasets lay a\nfair benchmarking means for competitions and have proved instrumental to the\nadvancements of object recognition research by providing quantifiable\nbenchmarks for the developed models. Taking a closer look at the\ncharacteristics of commonly-used public datasets seems to be an important first\nstep for data-driven and machine learning researchers. In this survey, we\nprovide a detailed analysis of datasets in the highly investigated object\nrecognition areas. More than 160 datasets have been scrutinized through\nstatistics and descriptions. Additionally, we present an overview of the\nprominent object recognition benchmarks and competitions, along with a\ndescription of the metrics widely adopted for evaluation purposes in the\ncomputer vision community. All introduced datasets and challenges can be found\nonline at github.com/AbtinDjavadifar/ORDC.", "AI": {"tldr": "本文综述了对象识别领域的数据集，分析了超过160个数据集，并概述了重要基准和竞赛，以及计算机视觉社区使用的评估指标。所有提及的数据集和挑战可以在指定GitHub地址找到。", "motivation": "随着深度网络技术的发展，对象识别领域对数据集的规模和质量提出了更高要求。研究者希望通过详细分析常用数据集的特征，为驱动数据和机器学习研究提供重要第一步。", "method": "本论文主要通过对超过160个常用数据集的统计和描述来进行详细分析。此外，还概述了显著的对象识别基准和竞赛，并描述了计算机视觉社区广泛采用的评估指标。", "result": "论文详细分析了160多个常用对象识别数据集，并总结了计算机视觉社区广泛使用的评估指标。此外，研究还概括了重要的对象识别基准和竞赛。", "conclusion": "该研究强调了合适的数据集对对象识别研究的重要性，并为研究人员提供了一份详细的数据集和竞赛综述，有助于推动领域的进一步发展。"}}
{"id": "2507.22542", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22542", "abs": "https://arxiv.org/abs/2507.22542", "authors": ["Long S. T. Nguyen", "Truong P. Hua", "Thanh M. Nguyen", "Toan Q. Pham", "Nam K. Ngo", "An X. Nguyen", "Nghi D. M. Pham", "Nghia H. Nguyen", "Tho T. Quan"], "title": "A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support", "comment": "Under review at ICCCI 2025", "summary": "With the rapid growth of Artificial Intelligence, Large Language Models\n(LLMs) have become essential for Question Answering (QA) systems, improving\nefficiency and reducing human workload in customer service. The emergence of\nVietnamese LLMs (ViLLMs) highlights lightweight open-source models as a\npractical choice for their accuracy, efficiency, and privacy benefits. However,\ndomain-specific evaluations remain limited, and the absence of benchmark\ndatasets reflecting real customer interactions makes it difficult for\nenterprises to select suitable models for support applications. To address this\ngap, we introduce the Customer Support Conversations Dataset (CSConDa), a\ncurated benchmark of over 9,000 QA pairs drawn from real interactions with\nhuman advisors at a large Vietnamese software company. Covering diverse topics\nsuch as pricing, product availability, and technical troubleshooting, CSConDa\nprovides a representative basis for evaluating ViLLMs in practical scenarios.\nWe further present a comprehensive evaluation framework, benchmarking 11\nlightweight open-source ViLLMs on CSConDa with both automatic metrics and\nsyntactic analysis to reveal model strengths, weaknesses, and linguistic\npatterns. This study offers insights into model behavior, explains performance\ndifferences, and identifies key areas for improvement, supporting the\ndevelopment of next-generation ViLLMs. By establishing a robust benchmark and\nsystematic evaluation, our work enables informed model selection for customer\nservice QA and advances research on Vietnamese LLMs. The dataset is publicly\navailable at\nhttps://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.", "AI": {"tldr": "本文介绍了CSConDa数据集，用于评估越南大语言模型在客户服务问答系统中的性能，并提出了一个综合评估框架，帮助改进下一代ViLLMs。", "motivation": "由于领域特性的评估数据集缺乏，很难为支持应用程序选择合适的越南大型语言模型，本文为了解决这个问题。", "method": "通过创建一个基于真实客户互动的越南软件公司编纂的CSConDa数据集，并评估11个轻量级开源ViLLMs，使用自动度量和句法分析。", "result": "研究揭示了性能差异并指出了关键改进领域，有助于客户服务质量问答模型的选择和发展。", "conclusion": "通过建立一个强大的基准测试和系统评估，本研究为改进客户服务问答系统中的ViLLMs奠定了基础。"}}
{"id": "2507.22369", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22369", "abs": "https://arxiv.org/abs/2507.22369", "authors": ["Sinh Trong Vu", "Hieu Trung Pham", "Dung Manh Nguyen", "Hieu Minh Hoang", "Nhu Hoang Le", "Thu Ha Pham", "Tai Tan Mai"], "title": "Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring", "comment": null, "summary": "Classroom behavior monitoring is a critical aspect of educational research,\nwith significant implications for student engagement and learning outcomes.\nRecent advancements in Visual Question Answering (VQA) models offer promising\ntools for automatically analyzing complex classroom interactions from video\nrecordings. In this paper, we investigate the applicability of several\nstate-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, and\nNVILA, in the context of classroom behavior analysis. To facilitate rigorous\nevaluation, we introduce our BAV-Classroom-VQA dataset derived from real-world\nclassroom video recordings at the Banking Academy of Vietnam. We present the\nmethodology for data collection, annotation, and benchmark the performance of\nthe selected VQA models on this dataset. Our initial experimental results\ndemonstrate that all four models achieve promising performance levels in\nanswering behavior-related visual questions, showcasing their potential in\nfuture classroom analytics and intervention systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.22545", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22545", "abs": "https://arxiv.org/abs/2507.22545", "authors": ["Sung-Min Lee", "Siyoon Lee", "Juyeon Kim", "Kyungmin Roh"], "title": "ControlMed: Adding Reasoning Control to Medical Language Model", "comment": "13 pages", "summary": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis.", "AI": {"tldr": "ControlMed是一种医疗语言模型，它允许用户在推理过程中通过细粒度控制标记来灵活控制推理过程的长度，这有助于提升计算效率和响应速度。", "motivation": "尽管现有推理语言模型在医疗领域得到了应用，但它们产生的推理过程过长，导致计算负担和响应延迟增加，限制了它们在真实临床环境中的部署。为了解决这些问题，提出了ControlMed模型。", "method": "通过三阶段的训练流程来开发ControlMed模型：1) 在大规模合成医疗指令数据集上进行预训练，涵盖直接和推理响应；2) 使用多长度推理数据和明确的长度控制标记进行监督微调；3) 通过基于模型的奖励信号进行强化学习，以提高事实准确性及响应质量。", "result": "实验结果显示，与最先进的模型相比，我们的模型在一系列英文和韩文医疗基准测试中取得了相似或更好的表现。", "conclusion": "ControlMed作为一个实用且灵活的解决方案，可以在保持推理准确性的同时根据不同需求调整计算效率，从而适用于临床问题回答和医疗信息分析。"}}
{"id": "2507.22393", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22393", "abs": "https://arxiv.org/abs/2507.22393", "authors": ["Anubhav Kataria", "Surbhi Madan", "Shreya Ghosh", "Tom Gedeon", "Abhinav Dhall"], "title": "Gems: Group Emotion Profiling Through Multimodal Situational Understanding", "comment": null, "summary": "Understanding individual, group and event level emotions along with\ncontextual information is crucial for analyzing a multi-person social\nsituation. To achieve this, we frame emotion comprehension as the task of\npredicting fine-grained individual emotion to coarse grained group and event\nlevel emotion. We introduce GEMS that leverages a multimodal swin-transformer\nand S3Attention based architecture, which processes an input scene, group\nmembers, and context information to generate joint predictions. Existing\nmulti-person emotion related benchmarks mainly focus on atomic interactions\nprimarily based on emotion perception over time and group level. To this end,\nwe extend and propose VGAF-GEMS to provide more fine grained and holistic\nanalysis on top of existing group level annotation of VGAF dataset. GEMS aims\nto predict basic discrete and continuous emotions (including valence and\narousal) as well as individual, group and event level perceived emotions. Our\nbenchmarking effort links individual, group and situational emotional responses\nholistically. The quantitative and qualitative comparisons with adapted\nstate-of-the-art models demonstrate the effectiveness of GEMS framework on\nVGAF-GEMS benchmarking. We believe that it will pave the way of further\nresearch. The code and data is available at:\nhttps://github.com/katariaak579/GEMS", "AI": {"tldr": "GEMS利用多模态技术预测个体、群体和事件情绪，扩展了数据集，展示了框架的有效性。", "motivation": "理解个体、群体和事件的情绪及背景信息在分析社交情况时至关重要。现存数据集主要关注原子性情感识别，研究人员扩展了VGAF-GEMS数据集，提供一个更精细和全面的基准测试环境。", "method": "GEMS采用了多模态swin-transformer和S3Attention框架，通过输入场景、群体成员和背景信息来生成联合预测结果，涵盖基础情绪和连续情绪的预测。", "result": "GEMS框架在扩展后的VGAF-GEMS数据集上，通过结合多模态swin-transformer和S3Attention架构，实现了对个体、群体及事件层面情绪的精细预测和整体分析，包括基本离散情绪及连续情绪（如效价和唤醒度）的预测，展示了其有效性和未来研究潜力。", "conclusion": "GEMS框架通过其多模态处理能力，对个体、群体及事件层面的情绪进行了全面捕捉和预测，展示了其在情感理解领域的优越性和前瞻性。研究结果支持了GEMS作为该领域进一步研究的基础。"}}
{"id": "2507.22564", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22564", "abs": "https://arxiv.org/abs/2507.22564", "authors": ["Xikang Yang", "Biyu Zhou", "Xuehai Tang", "Jizhong Han", "Songlin Hu"], "title": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs", "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet their safety mechanisms remain susceptible to\nadversarial attacks that exploit cognitive biases -- systematic deviations from\nrational judgment. Unlike prior jailbreaking approaches focused on prompt\nengineering or algorithmic manipulation, this work highlights the overlooked\npower of multi-bias interactions in undermining LLM safeguards. We propose\nCognitiveAttack, a novel red-teaming framework that systematically leverages\nboth individual and combined cognitive biases. By integrating supervised\nfine-tuning and reinforcement learning, CognitiveAttack generates prompts that\nembed optimized bias combinations, effectively bypassing safety protocols while\nmaintaining high attack success rates. Experimental results reveal significant\nvulnerabilities across 30 diverse LLMs, particularly in open-source models.\nCognitiveAttack achieves a substantially higher attack success rate compared to\nthe SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations\nin current defense mechanisms. These findings highlight multi-bias interactions\nas a powerful yet underexplored attack vector. This work introduces a novel\ninterdisciplinary perspective by bridging cognitive science and LLM safety,\npaving the way for more robust and human-aligned AI systems.", "AI": {"tldr": "研究提出了CognitiveAttack，一种利用认知偏差多交互来攻击大型语言模型的新框架。实验结果显示该方法有效且成功率高，提示需要关注LLM的安全性。", "motivation": "大型语言模型的安全机制容易受到利用认知偏差的对抗攻击。这项工作旨在展示多偏差交互在破坏LLM安全保障中的未被重视的力量，而非以前的越狱方法主要集中在提示工程或算法操控上。", "method": "CognitiveAttack,一种新颖的对抗框架，利用个体及组合认知偏差来产生嵌入优化偏差组合的提示，从而有效绕过安全协议同时保持高攻击成功率，通过监督微调和强化学习实现。", "result": "实验结果显示了30种多样化的LLM的显著漏洞，尤其是开源模型。CognitiveAttack的攻击成功率显著高于最先进的黑盒方法PAP（60.1%对比31.6%）。", "conclusion": "这些发现突出了多偏差交互作为一个强大而未被充分探索的攻击向量。这项工作通过连接认知科学和LLM安全，引入了一种新颖的跨学科视角，为更强大且更符合人类需求的AI系统铺平了道路。"}}
{"id": "2507.22398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22398", "abs": "https://arxiv.org/abs/2507.22398", "authors": ["Jordan Vice", "Naveed Akhtar", "Yansong Gao", "Richard Hartley", "Ajmal Mian"], "title": "On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations", "comment": "Keywords: Vision-Language Models, Frequency-Domain Perturbations,\n  Adversarial Robustness, Image Authenticity, Reliability", "summary": "Vision-Language Models (VLMs) are increasingly used as perceptual modules for\nvisual content reasoning, including through captioning and DeepFake detection.\nIn this work, we expose a critical vulnerability of VLMs when exposed to\nsubtle, structured perturbations in the frequency domain. Specifically, we\nhighlight how these feature transformations undermine authenticity/DeepFake\ndetection and automated image captioning tasks. We design targeted image\ntransformations, operating in the frequency domain to systematically adjust VLM\noutputs when exposed to frequency-perturbed real and synthetic images. We\ndemonstrate that the perturbation injection method generalizes across five\nstate-of-the-art VLMs which includes different-parameter Qwen2/2.5 and BLIP\nmodels. Experimenting across ten real and generated image datasets reveals that\nVLM judgments are sensitive to frequency-based cues and may not wholly align\nwith semantic content. Crucially, we show that visually-imperceptible spatial\nfrequency transformations expose the fragility of VLMs deployed for automated\nimage captioning and authenticity detection tasks. Our findings under\nrealistic, black-box constraints challenge the reliability of VLMs,\nunderscoring the need for robust multimodal perception systems.", "AI": {"tldr": "本研究发现，视觉语言模型（VLMs）在面对频率域中的细微结构化扰动时，其输出存在显著的脆弱性，并对这些模型在DeepFake检测和图像描述任务中的可用性提出了质疑。", "motivation": "该研究发现了视觉语言模型（VLMs）在面对频率域中的细微结构化扰动时存在的关键脆弱性。研究旨在揭示这些模型在自动化图像标注和DeepFake检测任务中所面临的挑战。", "method": "本研究设计了在频率域中操作的有针对性的图像转换，这些转换可以使五个最先进的视觉语言模型（包括不同参数的Qwen2/2.5和BLIP模型）的输出发生系统性变化。这些模型用于各种图像数据集（包括真实图像和生成图像），以验证它们在面对频率域扰动时的脆弱性。", "result": "实验表明，这些视觉语言模型的判断容易受到基于频率的线索的影响，而不完全与语义内容一致。研究在现实的黑盒约束条件下，证明了这些模型用于自动图像标注和真实性检测任务时的脆弱性。", "conclusion": "该研究揭示了视觉语言模型在处理频率扰动时的脆弱性，这表明这些模型可能无法在所有情况下提供可靠的多模态感知。实验结果强调了开发鲁棒的多模态感知系统的必要性。"}}
{"id": "2507.22581", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22581", "abs": "https://arxiv.org/abs/2507.22581", "authors": ["Inaya Rahmanisa", "Lyzander Marciano Andrylie", "Krisna Mahardika Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "title": "Unveiling the Influence of Amplifying Language-Specific Neurons", "comment": "Our code and dataset are made available at\n  https://github.com/tauimbz/lang-task-neuron", "summary": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer.", "AI": {"tldr": "研究通过放大特定语言的神经元，测试了其对18种语言的影响，结果表明这种干预能有效提高某些语言的性能，但对跨语言性能带来负面影响。", "motivation": "虽然已经观察到通过停用特定语言的神经元能够影响模型的行为，但它们在放大多种语言中的作用尚不明确。这项研究旨在探究放大多语言模型中特定语言神经元的效果。", "method": "本研究通过干预放大特定语言的神经元来研究其对模型行为的影响，实验涉及18种语言，包括资源较少的语言，并使用三种主要使用不同语言训练的模型进行测试。", "result": "研究发现，最优的放大因子能够将输出导向几乎所有测试的语言。在下游任务中使用该放大因子可以提高某些语言自模型的性能，但通常会降低跨语言的结果。", "conclusion": "放大特定语言神经元对多语言模型的行为有显著影响，特别是对资源较少的语言有明显好处，但在跨语言迁移方面优势不大。"}}
{"id": "2507.22404", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22404", "abs": "https://arxiv.org/abs/2507.22404", "authors": ["Sua Lee", "Joonhun Lee", "Myungjoo Kang"], "title": "MINR: Implicit Neural Representations with Masked Image Modelling", "comment": "Accepted to the ICCV 2023 workshop on Out-of-Distribution\n  Generalization in Computer Vision", "summary": "Self-supervised learning methods like masked autoencoders (MAE) have shown\nsignificant promise in learning robust feature representations, particularly in\nimage reconstruction-based pretraining task. However, their performance is\noften strongly dependent on the masking strategies used during training and can\ndegrade when applied to out-of-distribution data. To address these limitations,\nwe introduce the masked implicit neural representations (MINR) framework that\nsynergizes implicit neural representations with masked image modeling. MINR\nlearns a continuous function to represent images, enabling more robust and\ngeneralizable reconstructions irrespective of masking strategies. Our\nexperiments demonstrate that MINR not only outperforms MAE in in-domain\nscenarios but also in out-of-distribution settings, while reducing model\ncomplexity. The versatility of MINR extends to various self-supervised learning\napplications, confirming its utility as a robust and efficient alternative to\nexisting frameworks.", "AI": {"tldr": "本文提出了MINR框架，结合隐式神经表示与遮罩图像建模技术，展示出在图像重建任务中优于MAE的方法，并减少了模型复杂性。MINR在领域内及领域外数据上的表现均优于MAE。", "motivation": "尽管自监督学习方法如掩码自动编码器（MAE）在学习鲁棒特征表示方面展现了巨大的潜力，尤其是在基于图像重建的预训练任务中，但它们的性能往往强烈依赖于训练期间使用的掩码策略，并且在应用于分布外数据时性能会下降。为解决这些问题，我们提出了MINR框架。", "method": "我们提出了一种称为遮罩隐式神经表示（MINR）的框架，该框架将隐式神经表示与遮罩图像建模相结合。MINR 学习连续函数来表示图像，这使得在不同遮罩策略下也能得到更鲁棒和泛化的重建效果。", "result": "实验表明，MINR 不仅在领域内场景中优于 MAE，还在分布外设置中表现出色，同时减少了模型的复杂性。", "conclusion": "MINR 的灵活性延伸到各种自监督学习应用中，确认了其作为现有框架的稳健和有效替代方案的实用性。"}}
{"id": "2507.22603", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22603", "abs": "https://arxiv.org/abs/2507.22603", "authors": ["Rawan Al-Matham", "Kareem Darwish", "Raghad Al-Rasheed", "Waad Alshammari", "Muneera Alhoshan", "Amal Almazrua", "Asma Al Wazrah", "Mais Alheraki", "Firoj Alam", "Preslav Nakov", "Norah Alzahrani", "Eman alBilali", "Nizar Habash", "Abdelrahman El-Sheikh", "Muhammad Elmallah", "Haonan Li", "Hamdy Mubarak", "Mohamed Anwar", "Zaid Alyafeai", "Ahmed Abdelali", "Nora Altwairesh", "Maram Hasanain", "Abdulmohsen Al Thubaity", "Shady Shehata", "Bashar Alhafni", "Injy Hamed", "Go Inoue", "Khalid Elmadani", "Ossama Obeid", "Fatima Haouari", "Tamer Elsayed", "Emad Alghamdi", "Khalid Almubarak", "Saied Alshahrani", "Ola Aljarrah", "Safa Alajlan", "Areej Alshaqarawi", "Maryam Alshihri", "Sultana Alghurabi", "Atikah Alzeghayer", "Afrah Altamimi", "Abdullah Alfaifi", "Abdulrahman AlOsaimy"], "title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models", "comment": null, "summary": "The impressive advancement of Large Language Models (LLMs) in English has not\nbeen matched across all languages. In particular, LLM performance in Arabic\nlags behind, due to data scarcity, linguistic diversity of Arabic and its\ndialects, morphological complexity, etc. Progress is further hindered by the\nquality of Arabic benchmarks, which typically rely on static, publicly\navailable data, lack comprehensive task coverage, or do not provide dedicated\nplatforms with blind test sets. This makes it challenging to measure actual\nprogress and to mitigate data contamination. Here, we aim to bridge these gaps.\nIn particular, we introduce BALSAM, a comprehensive, community-driven benchmark\naimed at advancing Arabic LLM development and evaluation. It includes 78 NLP\ntasks from 14 broad categories, with 52K examples divided into 37K test and 15K\ndevelopment, and a centralized, transparent platform for blind evaluation. We\nenvision BALSAM as a unifying platform that sets standards and promotes\ncollaborative research to advance Arabic LLM capabilities.", "AI": {"tldr": "提出了BALSAM基准测试，以解决阿拉伯语大语言模型的数据稀缺、语言多样性和形态复杂性等问题，推动模型的发展与评估。", "motivation": "阿拉伯语大语言模型的发展落后于英语模型，这主要是由于阿拉伯语和其方言的数据稀缺，语言多样性和形态复杂性等原因。为了推进阿拉伯语大语言模型的发展和评估，引入了BALSAM作为解决方案。", "method": "通过建立全面的基准测试BALSAM，改善阿拉伯语语言模型的发展与评估问题。BALSAM涵盖了14个广泛类别的78个NLP任务，具有大量的数据集，并采用透明的平台进行盲测。", "result": "该论文提出BALSAM，这是一个全面的、由社区驱动的基准测试，旨在推动阿拉伯语大语言模型的发展与评估。它包括来自14个广泛类别的78个NLP任务，共有52,000个示例，其中包括37,000个测试集和15,000个开发集，并提供了一个集中且透明的平台，以实现盲测。BALSAM的目标是设立标准并促进共同研究，以提高阿拉伯语大语言模型的能力。", "conclusion": "BALSAM将作为一个统一的平台，设立标准并促进协作研究，以提升阿拉伯语大语言模型的能力。"}}
{"id": "2507.22407", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.22407", "abs": "https://arxiv.org/abs/2507.22407", "authors": ["Seungryong Lee", "Woojeong Baek", "Younghyun Kim", "Eunwoo Kim", "Haru Moon", "Donggon Yoo", "Eunbyung Park"], "title": "Moiré Zero: An Efficient and High-Performance Neural Architecture for Moiré Removal", "comment": "Project page: https://sngryonglee.github.io/MoireZero", "summary": "Moir\\'e patterns, caused by frequency aliasing between fine repetitive\nstructures and a camera sensor's sampling process, have been a significant\nobstacle in various real-world applications, such as consumer photography and\nindustrial defect inspection. With the advancements in deep learning\nalgorithms, numerous studies-predominantly based on convolutional neural\nnetworks-have suggested various solutions to address this issue. Despite these\nefforts, existing approaches still struggle to effectively eliminate artifacts\ndue to the diverse scales, orientations, and color shifts of moir\\'e patterns,\nprimarily because the constrained receptive field of CNN-based architectures\nlimits their ability to capture the complex characteristics of moir\\'e\npatterns. In this paper, we propose MZNet, a U-shaped network designed to bring\nimages closer to a 'Moire-Zero' state by effectively removing moir\\'e patterns.\nIt integrates three specialized components: Multi-Scale Dual Attention Block\n(MSDAB) for extracting and refining multi-scale features, Multi-Shape Large\nKernel Convolution Block (MSLKB) for capturing diverse moir\\'e structures, and\nFeature Fusion-Based Skip Connection for enhancing information flow. Together,\nthese components enhance local texture restoration and large-scale artifact\nsuppression. Experiments on benchmark datasets demonstrate that MZNet achieves\nstate-of-the-art performance on high-resolution datasets and delivers\ncompetitive results on lower-resolution dataset, while maintaining a low\ncomputational cost, suggesting that it is an efficient and practical solution\nfor real-world applications. Project page:\nhttps://sngryonglee.github.io/MoireZero", "AI": {"tldr": "提出了MZNet，一种U形神经网络，用于高效去除图像中的莫尔条纹，同时保持低计算成本，展现出在现实应用中的潜力。", "motivation": "莫尔条纹由于精细重复结构与摄像机传感器采样过程之间的频率混叠而生成，已成为摄影和工业缺陷检测等领域的一个重大障碍。随着深度学习算法的进步，基于卷积神经网络的方法已被广泛用于解决这一问题。然而，现有方法由于莫尔条纹的多样化尺度、方向和色彩变化，仍然难以有效消除图像伪影。这是因为CNN架构受限制的感受域限制了它们捕捉莫尔条纹复杂特性的能力。", "method": "研究提出了一种U形网络，集成多尺度双注意力块（MSDAB）、多形状大卷积核块（MSLKB）和基于特征融合的跳层连接三个专业组件来克服这一问题。", "result": "本研究提出了一种名为MZNet的U形网络，旨在通过有效地去除莫尔条纹，使图像接近'无莫尔条纹'的状态。该网络整合了三个专业组件，包括用于提取和提炼多尺度特征的多尺度双注意力块（MSDAB）、用于捕捉多样莫尔结构的多形状大卷积核块（MSLKB），以及用于增强信息流的基于特征融合的跳层连接。这使MZNet在恢复局部纹理和抑制大规模伪影方面表现出色。在基准数据集上的实验表明，MZNet在高分辨率数据集上达到了最先进的性能，并在较低分辨率数据集上也表现出竞争力，与此同时保持了低计算成本，显示出其作为实际应用有效且实用解决方案的潜力。", "conclusion": "实验结果表明，MZNet在去除莫尔条纹方面实现了最先进的性能，并且在不同分辨率的数据集上均有良好表现，具备低计算成本，展示了其在实际应用中的高效性和实用性。"}}
{"id": "2507.22608", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22608", "abs": "https://arxiv.org/abs/2507.22608", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Yusser Al Ghussin", "Tanja Baeumel", "Josef van Genabith", "Simon Ostermann"], "title": "Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation", "comment": "preprint", "summary": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation.", "AI": {"tldr": "研究指出了大语言模型中特定语言处理的神经机制，并展示了通过操控这些神经元提高多语言性能的方法。", "motivation": "探究大型语言模型（LLMs）处理特定语言的神经机制，以及如何通过操控这些神经元改善模型的多语言能力。", "method": "通过LAPE方法分析神经元在不同语言中的分布，并使用语言算术系统地激活和去激活语言模型中的特定语言神经元。", "result": "发现控制语言行为的神经元主要集中在更深的层中，而非拉丁文字则表现为更高的专业化。相近语言之间存在重叠神经元，这反映了它们内部语言相近的关系。操作这些神经元在五个多语言任务中表现出更优的效果，尤其在资源丰富的语言上更显著。此外，跨语言的神经元控制能增强下游性能。", "conclusion": "调控特定语言的神经元可以显著改善模型的多语言任务表现，对于资源丰富的语言尤其有效。还揭示了在逐步去激活神经元时，模型内部的语言选择回退机制。代码公开发布在GitHub上。"}}
{"id": "2507.22412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22412", "abs": "https://arxiv.org/abs/2507.22412", "authors": ["Sijie Wang", "Siqi Li", "Yawei Zhang", "Shangshu Yu", "Shenghai Yuan", "Rui She", "Quanjiang Guo", "JinXuan Zheng", "Ong Kang Howe", "Leonrich Chandra", "Shrivarshann Srijeyan", "Aditya Sivadas", "Toshan Aggarwal", "Heyuan Liu", "Hongming Zhang", "Chujie Chen", "Junyu Jiang", "Lihua Xie", "Wee Peng Tay"], "title": "UAVScenes: A Multi-Modal Dataset for UAVs", "comment": "Accepted by ICCV 2025", "summary": "Multi-modal perception is essential for unmanned aerial vehicle (UAV)\noperations, as it enables a comprehensive understanding of the UAVs'\nsurrounding environment. However, most existing multi-modal UAV datasets are\nprimarily biased toward localization and 3D reconstruction tasks, or only\nsupport map-level semantic segmentation due to the lack of frame-wise\nannotations for both camera images and LiDAR point clouds. This limitation\nprevents them from being used for high-level scene understanding tasks. To\naddress this gap and advance multi-modal UAV perception, we introduce\nUAVScenes, a large-scale dataset designed to benchmark various tasks across\nboth 2D and 3D modalities. Our benchmark dataset is built upon the\nwell-calibrated multi-modal UAV dataset MARS-LVIG, originally developed only\nfor simultaneous localization and mapping (SLAM). We enhance this dataset by\nproviding manually labeled semantic annotations for both frame-wise images and\nLiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses.\nThese additions enable a wide range of UAV perception tasks, including\nsegmentation, depth estimation, 6-DoF localization, place recognition, and\nnovel view synthesis (NVS). Our dataset is available at\nhttps://github.com/sijieaaa/UAVScenes", "AI": {"tldr": "介绍了一个大型多模态无人机数据集UAVScenes，以支持包括分割、深度估计和新视图合成等多项高级感知任务。", "motivation": "现有数据集在多模态无人机操作方面存在局限性，主要偏向于定位和三维重建任务，仅能支持地图级别的语义分割。为了弥补这一缺口并推进多模态无人机感知能力，构建了这一新的数据集。", "method": "多模态数据集UAVScenes的构建，基于MARS-LVIG数据集，增加了针对相机图像和LiDAR点云的手动标注的语义注释及精准的六自由度位姿注释，以支持更高层次的场景理解任务。", "result": "数据集支持广泛的无人机感知任务，包括分割、深度估计、六自由度定位、地点识别和新视图合成。", "conclusion": "UAVScenes将促进多模态感知技术的发展，尤其适合高精度无人机操作和环境理解相关的研究。数据集可在GitHub上获取。"}}
{"id": "2507.22623", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22623", "abs": "https://arxiv.org/abs/2507.22623", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Ivan Vykopal", "Josef van Genabith", "Simon Ostermann", "Roberto Zamparelli"], "title": "Multilingual Political Views of Large Language Models: Identification and Steering", "comment": "pre-print", "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs.", "AI": {"tldr": "研究通过对7个开源指令调整的大型语言模型进行多语言的政治倾向评价，发现模型倾向左翼政治立场，且可以通过具体技术干预来调整这些立场。", "motivation": "现有的大多数研究仅评价了少数模型和语言，因此存在政治偏见在不同架构、规模和多语言设置中的一般性问题尚待解答。少有研究探讨这些偏见是否可以被有效控制。本研究旨在填补这些空白。", "method": "通过对7个现代开源指令调整的大型语言模型（包括LLaMA-3.1，Qwen-3，和Aya-Expanse）进行大规模研究来探究政治倾向，这些模型在14种语言中应用了政治指南测试，每个陈述都有11个语义等效的措辞以确保测量的稳健性。", "result": "研究结果显示，较大的模型在政治立场上更倾向于自由主义-左翼，各国语言之间和模型家族间存在显著差异。使用质心激活干预技术测试立场可操控性方面显示，该技术可以有效引导模型反应向不同的意识形态位置转变。", "conclusion": "研究表明，模型的政治立场有可操控性，同时较大的模型倾向于某一特定政治立场。使用质心激活干预技术，模型的反应可以被引导到替代的意识形态位置。"}}
{"id": "2507.22418", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22418", "abs": "https://arxiv.org/abs/2507.22418", "authors": ["Phi Van Nguyen", "Ngoc Huynh Trinh", "Duy Minh Lam Nguyen", "Phu Loc Nguyen", "Quoc Long Tran"], "title": "Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching", "comment": null, "summary": "Quantifying aleatoric uncertainty in medical image segmentation is critical\nsince it is a reflection of the natural variability observed among expert\nannotators. A conventional approach is to model the segmentation distribution\nusing the generative model, but current methods limit the expression ability of\ngenerative models. While current diffusion-based approaches have demonstrated\nimpressive performance in approximating the data distribution, their inherent\nstochastic sampling process and inability to model exact densities limit their\neffectiveness in accurately capturing uncertainty. In contrast, our proposed\nmethod leverages conditional flow matching, a simulation-free flow-based\ngenerative model that learns an exact density, to produce highly accurate\nsegmentation results. By guiding the flow model on the input image and sampling\nmultiple data points, our approach synthesizes segmentation samples whose\npixel-wise variance reliably reflects the underlying data distribution. This\nsampling strategy captures uncertainties in regions with ambiguous boundaries,\noffering robust quantification that mirrors inter-annotator differences.\nExperimental results demonstrate that our method not only achieves competitive\nsegmentation accuracy but also generates uncertainty maps that provide deeper\ninsights into the reliability of the segmentation outcomes. The code for this\npaper is freely available at https://github.com/huynhspm/Data-Uncertainty", "AI": {"tldr": "提出了一种基于条件流匹配的方法，以精确地建模医学图像分割中的不确定性，并展示了该方法的有效性。", "motivation": "量化医学图像分割中的偶然不确定性是关键的，因为它反映了专家注释者之间自然的变异性。使用生成模型建模分割分布的常规方法存在局限性，而基于扩散的方法在近似数据分布方面表现出色，但在准确捕捉不确定性方面存在不足。", "method": "通过条件流匹配，一种基于流的生成模型，该模型能够学习精确的概率密度，以生成高度准确的分割结果。通过在输入图像上引导流模型并采样多个数据点，我们的方法可以合成分割样本，这些样本的像素级方差可靠地反映了底层数据分布。这种采样策略可以捕捉具有模糊边界的区域的不确定性，提供与注释者之间差异相呼应的稳健量化。", "result": "实验结果表明，该方法不仅实现了有竞争力的分割准确性，而且生成的不确定性地图为分割结果的可靠性提供了更深入的见解。", "conclusion": "该方法利用条件流匹配生成准确分割结果，并通过采样策略捕捉不确定性区域，生成的不确定性地图提供了可靠性的深入见解。"}}
{"id": "2507.22676", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.22676", "abs": "https://arxiv.org/abs/2507.22676", "authors": ["Jia Li", "Yang Wang", "Wenhao Qian", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "title": "Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment", "comment": "8 pages, 4 figures, ACM MM 2025.\n  github:https://github.com/MSA-LMC/365Aspects", "summary": "Interview performance assessment is essential for determining candidates'\nsuitability for professional positions. To ensure holistic and fair\nevaluations, we propose a novel and comprehensive framework that explores\n``365'' aspects of interview performance by integrating \\textit{three}\nmodalities (video, audio, and text), \\textit{six} responses per candidate, and\n\\textit{five} key evaluation dimensions. The framework employs\nmodality-specific feature extractors to encode heterogeneous data streams and\nsubsequently fused via a Shared Compression Multilayer Perceptron. This module\ncompresses multimodal embeddings into a unified latent space, facilitating\nefficient feature interaction. To enhance prediction robustness, we incorporate\na two-level ensemble learning strategy: (1) independent regression heads\npredict scores for each response, and (2) predictions are aggregated across\nresponses using a mean-pooling mechanism to produce final scores for the five\ntarget dimensions. By listening to the unspoken, our approach captures both\nexplicit and implicit cues from multimodal data, enabling comprehensive and\nunbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our\nframework secured first place in the AVI Challenge 2025, demonstrating its\neffectiveness and robustness in advancing automated and multimodal interview\nperformance assessment. The full implementation is available at\nhttps://github.com/MSA-LMC/365Aspects.", "AI": {"tldr": "该论文提出了一种用于多模态面试表现评估的框架，综合考虑视频、音频和文本信息，并从五个关键维度进行评估，实现了高度准确和公平的评估结果。", "motivation": "论文动机在于提高面试评估的全面性和公平性，通过一个全面的框架来更加客观地评估候选人的面试表现。", "method": "该论文提出了一个综合框架，用于评估候选人面试表现，通过融合理解视频、音频和文本三种模态的信息，提取六种候选人的回答，并从五个关键评估维度进行分析。框架采用了特定模态的特征提取器来编码异构数据流，并通过共享压缩多层感知器融合这些模态。此外，该框架使用了两级集成学习策略，通过独立回归头预测每个回答得分，然后使用平均池化机制聚合这些预测以生成最终的五个目标维度得分。", "result": "该框架在AVI Challenge 2025中获得了第一名，并实现了多维平均均方误差为0.1824，有效验证了框架的有效性和鲁棒性。", "conclusion": "通过捕捉到明确和隐含的多模态数据中的线索，该框架能够进行全面和无偏的面试表现评估，且证实了其在自动化和多模态面试评估方面的优越性和鲁棒性。"}}
{"id": "2507.22421", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22421", "abs": "https://arxiv.org/abs/2507.22421", "authors": ["Shahla John"], "title": "Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking", "comment": null, "summary": "Real-time video analysis remains a challenging problem in computer vision,\nrequiring efficient processing of both spatial and temporal information while\nmaintaining computational efficiency. Existing approaches often struggle to\nbalance accuracy and speed, particularly in resource-constrained environments.\nIn this work, we present a unified framework that leverages advanced\nspatial-temporal modeling techniques for simultaneous action recognition and\nobject tracking. Our approach builds upon recent advances in parallel sequence\nmodeling and introduces a novel hierarchical attention mechanism that\nadaptively focuses on relevant spatial regions across temporal sequences. We\ndemonstrate that our method achieves state-of-the-art performance on standard\nbenchmarks while maintaining real-time inference speeds. Extensive experiments\non UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action\nrecognition accuracy and 2.8% in tracking precision compared to existing\nmethods, with 40% faster inference time.", "AI": {"tldr": "本文提出了一种结合高级时空建模技术的统一框架，用于动作识别和目标跟踪，实现了实时分析的高性能。", "motivation": "文章旨在解决实时视频分析中空间和时间信息处理效率以及计算效率之间的平衡难题，特别是在资源受限的环境中。", "method": "该方法基于并行序列建模的最新进展，引入了一种新的分层注意力机制，可以自适应地关注跨时间序列的相关空间区域。", "result": "在UCF-101、HMDB-51和MOT17数据集上的实验展示了动作识别精度提高了3.2%，跟踪精度提高了2.8%，推理速度提高了40%。", "conclusion": "研究展示了该方法在标准基准上的实时推理速度和性能指标达到最新水平。"}}
{"id": "2507.22716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22716", "abs": "https://arxiv.org/abs/2507.22716", "authors": ["Jie He", "Victor Gutierrez Basulto", "Jeff Z. Pan"], "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs", "comment": null, "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.", "AI": {"tldr": "TIRESRAG-R1 framework is proposed for improving RAG models' reasoning and stability through an advanced reward system and a structured think-retrieve-reflect process.", "motivation": "The motivation is to address the limitations of existing reinforcement learning (RL) based RAG models, which mainly rely on rewards for the final answer and ignore intermediate reasoning quality.", "method": "The paper proposes TIRESRAG-R1, a novel framework incorporating a think-retrieve-reflect process, a multi-dimensional reward system, a difficulty-aware reweighting strategy, and training sample filtering to improve reasoning and response stability in RAG models.", "result": "Experiments show TIRESRAG-R1 outperforms previous RAG methods on four multi-hop QA datasets and generalizes well to single-hop reasoning tasks.", "conclusion": "The TIRESRAG-R1 framework enhances RAG models' reasoning capabilities by addressing three key failure modes: information insufficiency, faulty reasoning, and answer-reasoning inconsistency, leading to improved performance on complex tasks."}}
{"id": "2507.22431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22431", "abs": "https://arxiv.org/abs/2507.22431", "authors": ["Zhixiang Wei", "Guangting Wang", "Xiaoxiao Ma", "Ke Mei", "Huaian Chen", "Yi Jin", "Fengyun Rao"], "title": "HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models", "comment": null, "summary": "Large-scale but noisy image-text pair data have paved the way for the success\nof Contrastive Language-Image Pretraining (CLIP). As the foundation vision\nencoder, CLIP in turn serves as the cornerstone for most large vision-language\nmodels (LVLMs). This interdependence naturally raises an interesting question:\nCan we reciprocally leverage LVLMs to enhance the quality of image-text pair\ndata, thereby opening the possibility of a self-reinforcing cycle for\ncontinuous improvement? In this work, we take a significant step toward this\nvision by introducing an LVLM-driven data refinement pipeline. Our framework\nleverages LVLMs to process images and their raw alt-text, generating four\ncomplementary textual formulas: long positive descriptions, long negative\ndescriptions, short positive tags, and short negative tags. Applying this\npipeline to the curated DFN-Large dataset yields VLM-150M, a refined dataset\nenriched with multi-grained annotations. Based on this dataset, we further\npropose a training paradigm that extends conventional contrastive learning by\nincorporating negative descriptions and short tags as additional supervised\nsignals. The resulting model, namely HQ-CLIP, demonstrates remarkable\nimprovements across diverse benchmarks. Within a comparable training data\nscale, our approach achieves state-of-the-art performance in zero-shot\nclassification, cross-modal retrieval, and fine-grained visual understanding\ntasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models\ntrained on the DFN-2B dataset, which contains 10$\\times$ more training data\nthan ours. All code, data, and models are available at\nhttps://zxwei.site/hqclip.", "AI": {"tldr": "通过使用大语言-视觉模型（LVLMs）来优化图像-文本配对数据，作者提出了一个改进数据质量的方法，提升了视觉对比学习的性能，从而达到了state-of-the-art的零样本分类、跨模态检索和细粒度视觉理解能力。", "motivation": "随着对比语言-图像预训练模型的成功，提出了一种利用LVLMs来优化图像-文本数据质量的方法，旨在实现数据质量和模型能力之间的自我强化循环。", "method": "引入了一个由LVLM驱动的数据精炼流程，对图像及其原始描述进行处理，生成四种互补的文本描述形式。这种方法被应用于DFN-Large数据集，生成了带有多级别注释VLM-150M数据集。", "result": "基于该数据集提出的训练范式在多种基准测试中表现强劲，尤其是HQ-CLIP模型在零样本分类、跨模态检索和细粒度视觉理解中取得了新的最佳成绩。", "conclusion": "我们发现通过利用LVLMs来提高图像-文本对数据的质量，可以显著提升对比学习模型的性能，展示了LVLMs在数据增强中的巨大潜力。"}}
{"id": "2507.22720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22720", "abs": "https://arxiv.org/abs/2507.22720", "authors": ["Amit Das", "Md. Najib Hasan", "Souvika Sarkar", "Zheng Zhang", "Fatemeh Jamshidi", "Tathagata Bhattacharya", "Nilanjana Raychawdhury", "Dongji Feng", "Vinija Jain", "Aman Chadha"], "title": "Investigating Hallucination in Conversations for Low Resource Languages", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.", "AI": {"tldr": "本研究分析了六种大语言模型在印地语、波斯语和汉语中的语言和事实错误。结果发现，这些模型在汉语中的幻觉回应极少，但在印地语和波斯语中有较多。", "motivation": "尽管许多研究关注于英语中的幻觉问题，本研究扩展了这一调查，关注大语言模型在除英语外的语言中的事实准确性，力求提高模型的可靠性和有效性。", "method": "本研究通过分析包含三种语言（印地语、波斯语和汉语）对话数据的综合数据集，来检查GPT-3.5、GPT-4o、Llama-3.1、Gemma-2.0、DeepSeek-R1和Qwen-3这些大语言模型在事实和语言错误方面的表现。", "result": "研究发现，这些语言模型在汉语中产生幻觉性回应的数量非常少，但在印地语和波斯语中则显著增多。", "conclusion": "该研究揭示了大语言模型在不同语言环境中出现幻觉性回应的差异，突显了在非英语背景下进行类似研究的重要性。"}}
{"id": "2507.22438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22438", "abs": "https://arxiv.org/abs/2507.22438", "authors": ["Youngho Kim", "Hoonhee Cho", "Kuk-Jin Yoon"], "title": "From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras", "comment": null, "summary": "Human pose estimation is critical for applications such as rehabilitation,\nsports analytics, and AR/VR systems. However, rapid motion and low-light\nconditions often introduce motion blur, significantly degrading pose estimation\ndue to the domain gap between sharp and blurred images. Most datasets assume\nstable conditions, making models trained on sharp images struggle in blurred\nenvironments. To address this, we introduce a novel domain adaptation approach\nthat leverages event cameras, which capture high temporal resolution motion\ndata and are inherently robust to motion blur. Using event-based augmentation,\nwe generate motion-aware blurred images, effectively bridging the domain gap\nbetween sharp and blurred domains without requiring paired annotations.\nAdditionally, we develop a student-teacher framework that iteratively refines\npseudo-labels, leveraging mutual uncertainty masking to eliminate incorrect\nlabels and enable more effective learning. Experimental results demonstrate\nthat our approach outperforms conventional domain-adaptive human pose\nestimation methods, achieving robust pose estimation under motion blur without\nrequiring annotations in the target domain. Our findings highlight the\npotential of event cameras as a scalable and effective solution for domain\nadaptation in real-world motion blur environments. Our project codes are\navailable at https://github.com/kmax2001/EvSharp2Blur.", "AI": {"tldr": "利用事件相机和师生框架解决运动模糊引起的人体姿态估计问题，不需目标域标注，提高了实际场景下的姿态估计性能。", "motivation": "解决快速运动和低光条件下由于域差距造成的姿态估计性能下降问题。现有数据集假设了稳定的拍摄条件，导致模型在模糊环境下表现不佳。", "method": "提出了一种利用事件相机进行领域适应的新方法，这些相机能捕捉高时间分辨率运动数据并且对运动模糊具有鲁棒性。通过基于事件的增强生成运动感知模糊图像以弥合清晰和模糊域间的差距，不需配对标注。此外，还开发了一个师生框架，以迭代地细化伪标签，利用互不确定性遮罩消除错误标签并促进更有效的学习。", "result": "实验结果表明，该方法超过了传统的领域自适应人体姿态估计方法，能够在不使用目标域标注的情况下实现鲁棒的姿态估计。", "conclusion": "研究结果突出了事件相机作为大规模和有效解决方案的潜力，以适应真实世界中的运动模糊环境。项目代码可在https://github.com/kmax2001/EvSharp2Blur获取。"}}
{"id": "2507.22729", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22729", "abs": "https://arxiv.org/abs/2507.22729", "authors": ["Benedikt Roth", "Stephan Rappensperger", "Tianming Qiu", "Hamza Imamović", "Julian Wörmann", "Hao Shen"], "title": "Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning", "comment": null, "summary": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields state-of-the-art performance on the English clustering track\nof the Massive Text Embedding Benchmark (MTEB). An analysis of the attention\nmap further shows that fine-tuning shifts focus from prompt tokens to\nsemantically relevant words, indicating more effective compression of meaning\ninto the final hidden state. Our experiments demonstrate that LLMs can be\neffectively adapted as text embedding models through a combination of prompt\nengineering and resource-efficient contrastive fine-tuning on synthetically\ngenerated positive pairs.", "AI": {"tldr": "This paper focuses on developing techniques to adapt decoder-only LLMs for non-generative tasks by generating high-quality text embeddings using a combination of token embedding aggregation, prompt engineering, and contrastive fine-tuning, achieving top results on a clustering benchmark.", "motivation": "The motivation behind this research is to develop a method to transform LLMs into generators of effective sentence or document-level embeddings that can be used for various non-generative NLP tasks. This is important because simply pooling token vectors into text embeddings discards important information that could be crucial for these tasks.", "method": "Our study involves using several strategies to adapt pre-trained, decoder-only LLMs for creating text embeddings that are suitable for non-generative tasks, such as clustering, classification, or retrieval. These strategies include different token embedding aggregation techniques, task-specific prompt engineering, and text-level augmentation through contrastive fine-tuning.", "result": "The research outcome reveals that the proposed methods lead to state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark. Analysis of the attention maps supports the effectiveness of the attention mechanism in focusing on semantically relevant parts of the text.", "conclusion": "The conclusion of the paper is that pre-trained decoder-only LLMs can be effectively fine-tuned for creating high-quality text embeddings suitable for non-generative tasks, through prompt engineering and contrastive fine-tuning, with notable improvements in performance and semantic focus."}}
{"id": "2507.22454", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.22454", "abs": "https://arxiv.org/abs/2507.22454", "authors": ["Jiuming Liu", "Zheng Huang", "Mengmeng Liu", "Tianchen Deng", "Francesco Nex", "Hao Cheng", "Hesheng Wang"], "title": "TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation", "comment": "Accepted by IROS 2025. Code:https://github.com/IRMVLab/TopoLiDM", "summary": "LiDAR scene generation is critical for mitigating real-world LiDAR data\ncollection costs and enhancing the robustness of downstream perception tasks in\nautonomous driving. However, existing methods commonly struggle to capture\ngeometric realism and global topological consistency. Recent LiDAR Diffusion\nModels (LiDMs) predominantly embed LiDAR points into the latent space for\nimproved generation efficiency, which limits their interpretable ability to\nmodel detailed geometric structures and preserve global topological\nconsistency. To address these challenges, we propose TopoLiDM, a novel\nframework that integrates graph neural networks (GNNs) with diffusion models\nunder topological regularization for high-fidelity LiDAR generation. Our\napproach first trains a topological-preserving VAE to extract latent graph\nrepresentations by graph construction and multiple graph convolutional layers.\nThen we freeze the VAE and generate novel latent topological graphs through the\nlatent diffusion models. We also introduce 0-dimensional persistent homology\n(PH) constraints, ensuring the generated LiDAR scenes adhere to real-world\nglobal topological structures. Extensive experiments on the KITTI-360 dataset\ndemonstrate TopoLiDM's superiority over state-of-the-art methods, achieving\nimprovements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower\nMinimum Matching Distance (MMD). Notably, our model also enables fast\ngeneration speed with an average inference time of 1.68 samples/s, showcasing\nits scalability for real-world applications. We will release the related codes\nat https://github.com/IRMVLab/TopoLiDM.", "AI": {"tldr": "提出新的激光雷达生成框架TopoLiDM，结合GNNs和扩散模型，引入拓扑保持和PH约束，提升了生成的激光雷达场景的几何现实性和拓扑一致性，且生成速度快，效果优于现有方法。", "motivation": "现有方法在捕获几何现实性和全局拓扑一致性方面存在困难。虽然近年来的激光雷达扩散模型(LiDMs)嵌入了激光雷达点以提高生成效率，但这限制了它们在建模详细几何结构和保持全局拓扑一致性方面的可解释性。", "method": "提出了一种名为TopoLiDM的新框架，该框架结合了图神经网络(GNNs)和扩散模型，并在拓扑正则化下进行高保真激光雷达场景生成。首先使用拓扑保持的变分自编码器(VAE)通过图构建和多个图卷积层提取潜在图表示。接着冻结VAE并通过潜在扩散模型生成新的潜在拓扑图。引入了0维持续同调(PH)约束，确保生成的激光雷达场景符合现实世界中的全局拓扑结构。", "result": "在KITTI-360数据集上进行的大量实验表明，TopoLiDM优于最先进方法，实现了22.6%的Frechet Range Image Distance (FRID)和9.2%的Minimum Matching Distance (MMD)的降低。此外，该模型还具备快速生成能力，平均推理时间为1.68样本/秒。", "conclusion": "实验证明了TopoLiDM在生成高保真激光雷达场景方面的能力，并具备快速生成速度，适用于实际应用。"}}
{"id": "2507.22744", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22744", "abs": "https://arxiv.org/abs/2507.22744", "authors": ["Praveenkumar Katwe", "Rakesh Chandra", "Balabantaray Kali", "Prasad Vittala"], "title": "Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index", "comment": "8", "summary": "Reducing hallucinations in abstractive summarization remains a critical\nchallenge for deploying language models (LMs) in real-world settings. In this\nwork, we introduce a rewarddriven fine-tuning framework that explicitly\noptimizes for Entity Hallucination Index (EHI), a metric designed to quantify\nthe presence, correctness, and grounding of named entities in generated\nsummaries. Given a corpus of meeting transcripts, we first generate baseline\nsummaries using a pre-trained LM and compute EHI scores via automatic entity\nextraction and matching. We then apply reinforcement learning to fine-tune the\nmodel parameters, using EHI as a reward signal to bias generation toward\nentity-faithful outputs. Our approach does not rely on human-written factuality\nannotations, enabling scalable fine-tuning. Experiments demonstrate consistent\nimprovements in EHI across datasets, with qualitative analysis revealing a\nsignificant reduction in entity-level hallucinations without degradation in\nfluency or informativeness. We release a reproducible Colab pipeline,\nfacilitating further research on hallucination-aware model fine-tuning using\nlightweight, hallucintion metrics like EHI.", "AI": {"tldr": "本篇论文提出了一种使用奖励驱动框架优化实体幻觉指数（EHI）的方法，以减少语言模型在摘要生成时的实体幻觉现象，提升了生成摘要的质量。", "motivation": "该研究旨在减少抽取式摘要中的幻觉现象，这是一个在现实世界中部署语言模型时的关键挑战。通过优化EHI，可以减少实体级别的幻觉，同时保持流畅性和信息量不受影响。", "method": "该论文介绍了一种基于奖励驱动的微调框架，用于优化实体幻觉指数（EHI），这是一种衡量生成摘要中名称实体存在性、正确性和基础程度的指标。该框架首先使用一个预训练语言模型生成基线摘要，并通过自动实体提取和匹配计算EHI分数。然后应用强化学习来微调模型参数，使用EHI作为奖励信号，以偏向于生成忠实于实体的输出。", "result": "实验结果显示提出的框架在不同数据集上一致改善了EHI，定性分析表明在没有降低流畅性和信息度的情况下明显减少了实体级别的幻觉。", "conclusion": "该方法展示了对摘要生成中的幻觉现象进行量化并减少的能力，并且可以不依赖于人类编写的真实性注释，实现了可扩展的微调。此外，作者提供了可复制的Colab工具，以供进一步研究使用。"}}
{"id": "2507.22459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.22459", "abs": "https://arxiv.org/abs/2507.22459", "authors": ["Jaeha Kim", "Junghun Oh", "Kyoung Mu Lee"], "title": "Exploiting Diffusion Prior for Task-driven Image Restoration", "comment": "Accepted to ICCV 2025", "summary": "Task-driven image restoration (TDIR) has recently emerged to address\nperformance drops in high-level vision tasks caused by low-quality (LQ) inputs.\nPrevious TDIR methods struggle to handle practical scenarios in which images\nare degraded by multiple complex factors, leaving minimal clues for\nrestoration. This motivates us to leverage the diffusion prior, one of the most\npowerful natural image priors. However, while the diffusion prior can help\ngenerate visually plausible results, using it to restore task-relevant details\nremains challenging, even when combined with recent TDIR methods. To address\nthis, we propose EDTR, which effectively harnesses the power of diffusion prior\nto restore task-relevant details. Specifically, we propose directly leveraging\nuseful clues from LQ images in the diffusion process by generating from\npixel-error-based pre-restored LQ images with mild noise added. Moreover, we\nemploy a small number of denoising steps to prevent the generation of redundant\ndetails that dilute crucial task-related information. We demonstrate that our\nmethod effectively utilizes diffusion prior for TDIR, significantly enhancing\ntask performance and visual quality across diverse tasks with multiple complex\ndegradations.", "AI": {"tldr": "Propose EDTR which effectively harnesses the power of diffusion prior to restore task-relevant details in task-driven image restoration settings with multiple complex degradations.", "motivation": "This motivates us to leverage the diffusion prior, one of the most powerful natural image priors. However, while the diffusion prior can help generate visually plausible results, using it to restore task-relevant details remains challenging, even when combined with recent TDIR methods.", "method": "Specifically, we propose directly leveraging useful clues from LQ images in the diffusion process by generating from pixel-error-based pre-restored LQ images with mild noise added. Moreover, we employ a small number of denoising steps to prevent the generation of redundant details that dilute crucial task-related information.", "result": "We demonstrate that our method effectively utilizes diffusion prior for TDIR, significantly enhancing task performance and visual quality across diverse tasks with multiple complex degradations.", "conclusion": "It indicates the approach is effective in restoring task-relevant details and improving task performance and visual quality."}}
{"id": "2507.22752", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22752", "abs": "https://arxiv.org/abs/2507.22752", "authors": ["Jindřich Libovický", "Jindřich Helcl", "Andrei Manea", "Gianluca Vico"], "title": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset", "comment": null, "summary": "We introduce a benchmark for open-ended regional question answering that\nencompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. As a baseline, we evaluate\nstate-of-the-art LLMs through prompting and complement this with human\njudgments of answer correctness. Using these human evaluations, we analyze the\nreliability of existing automatic evaluation metrics. Our baseline results\nhighlight a significant gap in regional knowledge among current LLMs. Moreover,\napart from LLM-based evaluation, there is minimal correlation between automated\nmetrics and human judgment. We release this dataset as a resource to (1) assess\nregional knowledge in LLMs, (2) study cross-lingual generation consistency in a\nchallenging setting, and (3) advance the development of evaluation metrics for\nopen-ended question answering.", "AI": {"tldr": "该论文提出了一个结合文本和视觉模态的开放性区域问答基准，强调了大型语言模型在区域知识方面的短缺，并指出现存自动评估指标与人工判断之间存在差距。", "motivation": "该研究旨在评估当前大型语言模型在区域知识方面的不足，并通过人的判断来改进现有自动评估指标的可靠性。同时，也为跨语言生成一致性和开放式问题回答的评估指标的发展提供了数据资源。", "method": "该论文创建了一个结合文本和视觉模态的开放性区域问答基准，并使用了最先进的大型语言模型（LLMs）作为基线对其进行评估。数据集由来自捷克、斯洛伐克和乌克兰的母语者基于维基百科手动编写，包含英文翻译。问题类型包括纯文本和需要视觉理解的问题。评估方法除了模型自动生成的答案外，还加入了人工判断。", "result": "研究结果展示了现有LLMs在区域知识方面的大量信息空白，并且除了基于LLM的评估外，自动评估指标和人类判断之间存在极小的相关性。", "conclusion": "该论文发布了此数据集，旨在评估LLMs的区域知识、研究跨语言生成一致性以及促进开放式问题回答评估指标的发展。"}}
{"id": "2507.22465", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22465", "abs": "https://arxiv.org/abs/2507.22465", "authors": ["Zheng Xiangyu", "He Songcheng", "Li Wanyun", "Li Xiaoqiang", "Zhang Wei"], "title": "Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation", "comment": "Accepted to ACM MM'25: The 33rd ACM International Conference on\n  Multimedia Proceedings", "summary": "Unsupervised Video Object Segmentation (UVOS) aims to predict pixel-level\nmasks for the most salient objects in videos without any prior annotations.\nWhile memory mechanisms have been proven critical in various video segmentation\nparadigms, their application in UVOS yield only marginal performance gains\ndespite sophisticated design. Our analysis reveals a simple but fundamental\nflaw in existing methods: over-reliance on memorizing high-level semantic\nfeatures. UVOS inherently suffers from the deficiency of lacking fine-grained\ninformation due to the absence of pixel-level prior knowledge. Consequently,\nmemory design relying solely on high-level features, which predominantly\ncapture abstract semantic cues, is insufficient to generate precise\npredictions. To resolve this fundamental issue, we propose a novel hierarchical\nmemory architecture to incorporate both shallow- and high-level features for\nmemory, which leverages the complementary benefits of pixel and semantic\ninformation. Furthermore, to balance the simultaneous utilization of the pixel\nand semantic memory features, we propose a heterogeneous interaction mechanism\nto perform pixel-semantic mutual interactions, which explicitly considers their\ninherent feature discrepancies. Through the design of Pixel-guided Local\nAlignment Module (PLAM) and Semantic-guided Global Integration Module (SGIM),\nwe achieve delicate integration of the fine-grained details in shallow-level\nmemory and the semantic representations in high-level memory. Our Hierarchical\nMemory with Heterogeneous Interaction Network (HMHI-Net) consistently achieves\nstate-of-the-art performance across all UVOS and video saliency detection\nbenchmarks. Moreover, HMHI-Net consistently exhibits high performance across\ndifferent backbones, further demonstrating its superiority and robustness.\nProject page: https://github.com/ZhengxyFlow/HMHI-Net .", "AI": {"tldr": "We address the limitation of existing UVOS methods by introducing a hierarchical memory architecture that integrates both shallow and high-level features, leading to improved segmentation performance across various benchmarks.", "motivation": "The motivation for this paper is to improve the performance of Unsupervised Video Object Segmentation (UVOS) beyond the marginal gains achieved with current memory mechanisms, which rely heavily on high-level semantic features.", "method": "Our method introduces a hierarchical memory architecture that integrates both shallow- and high-level features to enhance the precision of UVOS. This is achieved through a Pixel-guided Local Alignment Module (PLAM) and a Semantic-guided Global Integration Module (SGIM) to address the issue of over-reliance on high-level features.", "result": "The proposed method achieves state-of-the-art performance across all evaluated UVOS and video saliency detection benchmarks, and exhibits consistent high performance across different backbones.", "conclusion": "The newly proposed hierarchical memory with heterogeneous interaction network (HMHI-Net) significantly enhances the performance of UVOS by effectively integrating pixel and semantic information, demonstrating state-of-the-art results and high robustness."}}
{"id": "2507.22753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22753", "abs": "https://arxiv.org/abs/2507.22753", "authors": ["Sowmya Vajjala", "Bashar Alhafni", "Stefano Bannò", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "title": "Opportunities and Challenges of LLMs in Education: An NLP Perspective", "comment": "Pre-print", "summary": "Interest in the role of large language models (LLMs) in education is\nincreasing, considering the new opportunities they offer for teaching,\nlearning, and assessment. In this paper, we examine the impact of LLMs on\neducational NLP in the context of two main application scenarios: {\\em\nassistance} and {\\em assessment}, grounding them along the four dimensions --\nreading, writing, speaking, and tutoring. We then present the new directions\nenabled by LLMs, and the key challenges to address. We envision that this\nholistic overview would be useful for NLP researchers and practitioners\ninterested in exploring the role of LLMs in developing language-focused and\nNLP-enabled educational applications of the future.", "AI": {"tldr": "本文探讨了大型语言模型在教育中的角色，特别是在辅助和评估两个应用场景中的影响，并提出了未来发展的新方向和面临的挑战。", "motivation": "随着大型语言模型为教育领域带来了新的机遇，本文旨在研究其在教育NLP中的影响，特别是在辅助教学和评估方面的作用。", "method": "本文围绕阅读、写作、口语和辅导四个维度，探讨了大型语言模型在教育应用中的影响，以及对未来发展方向的展望和面临的挑战。", "result": "本文为NLP研究者和实践者提供了关于大型语言模型在教育领域中未来角色的全方位概述。", "conclusion": "本文侧重于语言学习和NLP支持的教育应用，为研究者和实践者提供了有价值的指导，以开发未来的教育应用。"}}
{"id": "2507.22469", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22469", "abs": "https://arxiv.org/abs/2507.22469", "authors": ["Viacheslav Pirogov"], "title": "Visual Language Models as Zero-Shot Deepfake Detectors", "comment": "Accepted to the ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "summary": "The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models\nfor face swapping, presents a substantial and evolving threat in digital media,\nidentity verification, and a multitude of other systems. The majority of\nexisting methods for detecting deepfakes rely on training specialized\nclassifiers to distinguish between genuine and manipulated images, focusing\nonly on the image domain without incorporating any auxiliary tasks that could\nenhance robustness. In this paper, inspired by the zero-shot capabilities of\nVision Language Models, we propose a novel VLM-based approach to image\nclassification and then evaluate it for deepfake detection. Specifically, we\nutilize a new high-quality deepfake dataset comprising 60,000 images, on which\nour zero-shot models demonstrate superior performance to almost all existing\nmethods. Subsequently, we compare the performance of the best-performing\narchitecture, InstructBLIP, on the popular deepfake dataset DFDC-P against\ntraditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our\nresults demonstrate the superiority of VLMs over traditional classifiers.", "AI": {"tldr": "This paper introduces a new method for deepfake detection using Vision Language Models, achieving better results than traditional classifiers in both zero-shot and fine-tuned settings.", "motivation": "Aims to address the limitations of existing deepfake detection methods, which do not incorporate auxiliary tasks and thus may lack robustness.", "method": "Proposes a VLM-based approach for deepfake detection, utilizing a new high-quality deepfake dataset with 60,000 images and comparing the performance of InstructBLIP against traditional methods.", "result": "Demonstrates the superiority of VLM-based models, particularly InstructBLIP, in both zero-shot and in-domain fine-tuning scenarios.", "conclusion": "Highlights the potential of Vision Language Models in deepfake detection, showing they outperform current methods and can be effectively used in zero-shot settings."}}
