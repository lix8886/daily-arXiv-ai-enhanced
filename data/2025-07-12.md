<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 16]
- [cs.CV](#cs.CV) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

> 本研究通过实验方法揭示了大型语言模型中认知偏见的来源主要由预训练影响，而非微调或训练随机性。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在理解大型语言模型中认知偏见的原因，这些偏见可能来源于预训练、微调，或是训练的随机性。

**Method:** 本研究提出了一个两步的因果实验方法来厘清这些因素。首先，通过使用不同的随机种子多次微调模型来研究训练随机性如何影响30多种认知偏见。其次，引入了"交叉微调"--交换微调数据集之间的模型以孤立偏见来源。这种交换使用了导致不同偏见模式的数据集，直接测试偏见是否依赖于数据集。

**Result:** 研究结果表明，训练随机性引起一定的变异，但偏见主要由预训练决定：具有相同预训练模型的模型比仅共享微调数据集的模型表现出更相似的偏见模式。

**Conclusion:** 理解微调模型中的偏见需要考虑其预训练的根源，而不仅仅是微调的影响。这种观点可以指导未来开发评估和缓解大型语言模型偏见的原则性策略。

**Abstract:** Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [2] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

> The paper examines LLMs' responses to survey questions, revealing vulnerabilities and a consistent recency bias, and highlights the need for prompt design and robustness testing in generating synthetic survey data.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to understand the reliability and susceptibility to known response biases of LLMs when used as proxies for human subjects in social science surveys.

**Method:** This paper investigates the response robustness of LLMs in normative survey contexts by testing nine diverse LLMs on questions from the World Values Survey (WVS), applying a comprehensive set of 11 perturbations to both question phrasing and answer option structure.

**Result:** The research reveals that all tested models exhibit a consistent recency bias varying in intensity, favoring the last-presented answer option, and remain sensitive to semantic variations like paraphrasing and combined perturbations.

**Conclusion:** The study underscores the critical importance of prompt design and robustness testing when using LLMs to generate synthetic survey data.

**Abstract:** Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [3] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

> SynthTextEval工具包用于评估合成文本在下游应用中的实用性、系统公平性、隐私泄露风险、与其他文本分布差异等方面，特别适用于健康和法律领域的合成数据。这有助于提升合成文本的实用性和加强隐私保护。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型（LLM）输出的流畅性，合成文本在减少AI系统开发和部署中的隐私泄露风险方面具有潜在价值，特别是在高风险领域。

**Method:** 本文介绍了SynthTextEval工具包，该工具包旨在对合成文本进行全面评估，涵盖了合成数据的多个维度：其在下游系统中的应用效果、系统的公平性、隐私泄露风险、与源文本在分布上的差异以及领域专家的定性反馈。

**Result:** 该工具包使得用户能够评估他们上传或使用其生成模块生成的数据的各个维度，并且特别强调了健康和法律两个高风险领域中数据集的功能和有效性。

**Conclusion:** 通过整合和标准化评价指标，该研究旨在提升合成文本的实用性和保持AI开发中的隐私保护。

**Abstract:** We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [4] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

> 本文为医疗领域语言模型专门设计了一种安全评估协议，并构建了PatientSafetyBench数据集，进行红队测试，从患者、临床医生和普通用户三个视角评估模型的安全性，首次定义了医疗领域LLMs的安全评估标准。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型（LLMs）在医学领域的应用越来越广泛，从不同角色的用户使用的安全问题变得日益关键。此前的安全评估主要集中在通用安全基准上，少有针对医疗领域安全的具体设计。

**Method:** 本文提出了一种专门针对医疗领域语言模型的安全评估协议，该协议从患者视角和临床医生视角出发，除了通用安全评估之外，还进行了定量分析。此外，构建了包含466个样本的PatientSafetyBench数据集，覆盖5个关键安全类别，用于从患者角度衡量安全性。并通过MediPhi模型集合进行案例研究，应用了其红队测试协议。

**Result:** 通过应用具体的红队测试协议对MediPhi模型集合进行分析，本文填补了这一领域的空白，首次定义了医疗领域语言模型的安全评估标准，此种标准是从患者、临床医生和普通用户三个不同的角度进行。

**Conclusion:** 本文的工作为在医学领域更加安全地部署语言模型奠定了基础。

**Abstract:** As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [5] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

> 研究开发了一种先进方法，用于在教室这种包含重叠语音的多小组对话环境中识别中断，并揭示了中断在合作学习中是如何表现的。

<details>
  <summary>Details</summary>

**Motivation:** 虽然先前关于中断检测和解释的研究大多在单一对话环境中进行，本研究旨在解决在包含多个并行对话的教室场景中，AI如何从重叠语音中识别中断的问题。

**Method:** 本研究分析了在单一对话环境和多小组对话环境中中断检测的差异，并开发了一种能够应对重叠语音的先进中断识别方法，使得该方法可以应用于教室场景。此外，研究还探讨了合作小组互动中断中具有意义的语用和韵律信息。

**Result:** 开发了一种先进的中断识别方法，这种方法对于重叠语音具有鲁棒性，因此能够应用于教育领域，特别是小组协作学习的教室环境。

**Conclusion:** 本研究提供了一个方法来识别重叠语音中的中断，为未来跟踪小组对话和考虑多重重叠语音影响的研究铺平了道路。

**Abstract:** Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [6] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [7] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

> 本文提出了一种新型的模型架构，结合了CNNs和GNNs，用于处理文本以提高效率。该模型通过实时生成图来处理字符级的输入，并在多个文本分类任务中显示出良好的效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于，处理长文本时，时间和能量效率是深度学习（DL）中的关键问题。转换器模型存在计算复杂度较高问题，尤其是相对于输入长度的二次复杂度，这使得它们在处理较长文档时效率低下。因此，提出了一种新的模型架构以解决此问题。

**Method:** 该研究提出了一种结合图神经网络（GNNs）和卷积神经网络（CNNs）的新模型架构，它与实时端到端图生成机制集成。模型处理字符级输入的小批次，无需填充或截断，并通过高效字典查找整合大型语言模型中的信息，如标记嵌入和情感极性。使用CNN捕捉局部上下文模式，利用基于晶格的图结构扩大局部接受场，并采用小世界图聚合文档级别的信息。

**Result:** 实验结果证实了所提出的模型在效率和性能上的竞争力。模型在多个文本分类任务（包括情感分析和新闻分类）上进行了评估，并与最先进的模型进行了比较。

**Conclusion:** 研究结论表明，所提出的结合CNNs和GNNs，并集成实时端到端图生成机制的模型架构，虽然在处理长文本时可以更高效，但也展示了竞争性的性能表现。

**Abstract:** Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [8] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

> 研究介绍MedReadCtrl框架，提升了生成式AI在医疗领域中人机沟通的个性化与可理解性，特别是在临床任务和低学历水平阅读需求上的表现优于GPT-4。

<details>
  <summary>Details</summary>

**Motivation:** 提高生成式AI在医疗领域应用时的人机沟通效率，特别是在个性化和易理解内容的生成上。

**Method:** 引入了MedReadCtrl，这是一种可调难度而不影响意义的可读性控制指令调优框架。

**Result:** 在九个数据集和三个任务中的评估显示MedReadCtrl比GPT-4有更低的可读性指令遵循错误，并且在未见的临床任务上有了巨大的提升。专家更倾向于MedReadCtrl，尤其在低学历水平用户中。

**Conclusion:** MedReadCtrl能够将临床内容重组为易于理解且符合可读性要求的语言，同时保留医疗意图，为患者教育和支持AI赋能的医疗提供了可扩展解决方案。

**Abstract:** Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [9] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

> 开发了SynthEHR-Eviction管道，结合LLM、人机协作注释和APO，从临床记录中有效抽取驱逐状态，创建了大规模数据集，显著提高了数据标注效率和模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 驱逐是健康的社会决定因素（SDoH），但在结构化EHR字段中很少编码。这项工作旨在解决这一问题，提供了一个可以大规模应用的解决方案。

**Method:** 介绍了一种称为SynthEHR-Eviction的管道，结合了大语言模型（LLM）、人机协作注释和自动提示优化（APO），用于从临床记录中抽取驱逐状态。

**Result:** 使用该管道创建了迄今为止最大的公共驱逐相关SDoH数据集，含有14个细粒度类别。调优后的LLM在人类验证的数据上实现了高达88.8%（驱逐）和90.3%（其他SDoH）的Macro-F1分数。

**Conclusion:** 该方法减少了80%以上的标注工作，加速了数据集的创建，实现了可扩展的驱逐检测，并可以推广到其他信息抽取任务。

**Abstract:** Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [10] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [11] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

> 本文提出Self-taught ActioN Deliberation（SAND）框架，通过审议候选行动的方式来改进现有大型语言模型的行为优化方法，从而提高代理在两个代表性交互任务上的性能，平均改进20%。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型代理通常通过监督微调或成对回放的偏好优化来调优，这可能导致代理过度依赖某些看似合理但实际上次优的行为。本文提出了一种新的方法来解决这个问题。

**Method:** SAND框架被提出，以解决在大型行动空间探索不足导致的选择次优行动的问题。该框架通过对候选行动进行明确的审议，避免了简单模仿专家行为的方法可能导致的过度承诺次优行动的问题。通过自我一致性行动采样和执行导向行动批评来辅助综合逐行动审议思维。这些审议轨迹在迭代过程中被用来微调LLM代理本身。

**Result:** 在两个代表性交互代理任务中，SAND方法比初始的监督微调提高了20%，并且超越了最新的代理微调方法。

**Conclusion:** SAND框架提高了LLM代理在行动审议和选择上的能力，并且展示了在实际任务中的有效性和优越性。

**Abstract:** Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [12] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

> RLEP框架通过重放高质量轨迹，避免无果探索，专注于有望的推理路径，加速收敛并提升最终性能，在多个数学问题数据集上超越基线性能。

<details>
  <summary>Details</summary>

**Motivation:** 强化学习在大规模语言模型中的训练是能耗高的任务：训练可能不稳定，策略可能逐渐偏离其预训练权重。

**Method:** 提出了一种名为RLEP（基于经验重放的强化学习）的两阶段框架，该框架首先收集验证过的轨迹，然后在后续训练中重放这些经验。在每个更新步骤中，策略都在由新生成轨迹和重放的成功经验混合而成的小批量数据上进行优化。通过重放高质量样本，RLEP引导模型远离无果的探索，专注于有希望的推理路径，从而实现更快的收敛和更优的最终性能。

**Result:** 在Qwen2.5-Math-7B基础模型上，RLEP用更少的更新次数达到基线峰值准确率，并最终超越它，在AIME-2024、AIME-2025、AMC-2023数据集上的准确率分别提高到了39.9%、22.3%、82.2%。

**Conclusion:** 实验结果表明RLEP框架提高了在数学问题任务上的准确率，并显著减少了训练所需的更新次数，提供代码、数据集和检查点以促进可复制性和进一步研究。

**Abstract:** Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [13] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [14] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

> PLAN-TUNING, a post-training method, fine-tunes smaller LLMs to better mimic planning strategies of larger models, thereby boosting reasoning performance and generalization capabilities.

<details>
  <summary>Details</summary>

**Motivation:** The primary motivation is to apply the effective problem decomposition and planning strategies used by large LLMs to smaller LLMs to boost their performance and generalization.

**Method:** PLAN-TUNING is a post-training framework that first distills planning trajectories from large-scale LLMs and then fine-tunes smaller models with these trajectories to enhance reasoning skills through supervised and reinforcement learning objectives.

**Result:** The plan-tuned models showed improvements of ~7% on GSM8k and MATH benchmarks and even better generalization to out-of-domain data, with gains of ~10% on OlympiadBench and ~12% on AIME 2024.

**Conclusion:** PLAN-TUNING is an effective approach for enhancing smaller LLMs' performance on specific tasks by incorporating the planning strategies of larger models.

**Abstract:** Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [15] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

> TeaR improves LLM reasoning capabilities by teaching models to better navigate code execution and derive outputs, resulting in performance gains across numerous benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the reasoning capabilities of large language models by avoiding overfitting to complex algorithms and focusing on core reasoning structures.

**Method:** TeaR uses careful data curation and reinforcement learning to guide LLMs in discovering optimal reasoning paths through code-related tasks.

**Result:** The results show significant performance improvements across various benchmarks and model sizes, with a notable 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

**Conclusion:** TeaR effectively enhances the reasoning skills of LLMs through its methodological approach to data curation and learning, creating a more adaptable and capable model for various reasoning tasks.

**Abstract:** Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [16] [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
*Hein Htet,Amgad Ahmed Ali Ibrahim,Yutaka Sasaki,Ryoji Asahi*

Main category: cs.CL

> 本研究提出使用DyGIE++结合多种预训练的BERT变体，包括MatSciBERT和PubMedBERT，从科学文献中提取与氧还原反应（ORR）催化剂相关的信息。实验表明，经过微调的PubMedBERT模型在命名实体识别（NER）中达到最高的F1值82.19%，而MatSciBERT在关系抽取（RE）中表现最佳，F1值为66.10%。这表明领域特定的BERT模型在ORR催化剂提取方面优于通用的科学模型。

<details>
  <summary>Details</summary>

**Motivation:** ORR催化剂对于提高燃料电池效率至关重要，而如何从大量科学文献中结构化提取ORR催化剂信息是一个重大挑战。此研究旨在解决这一问题。

**Method:** 研究中提出了一种使用DyGIE++结合多种预训练BERT变体（包括MatSciBERT和PubMedBERT）的方法，手动构建了包含12个关键实体和两种关系类型的综合性数据集，通过数据标注、集成和模型微调来提高信息提取准确性。

**Result:** 实验显示，微调后的PubMedBERT模型在NER中获得了82.19%的最高F1值，MatSciBERT则在RE中得到最佳的66.10% F1值，与人工注释比较表明，微调后的模型具有可靠的ORR催化剂提取能力。

**Conclusion:** 研究结果表明，领域特定的BERT模型（如MatSciBERT和PubMedBERT）在ORR催化剂信息提取方面优于通用科学模型，预示着它们在自动化文献分析中的巨大潜力。

**Abstract:** The oxygen reduction reaction (ORR) catalyst plays a critical role in
enhancing fuel cell efficiency, making it a key focus in material science
research. However, extracting structured information about ORR catalysts from
vast scientific literature remains a significant challenge due to the
complexity and diversity of textual data. In this study, we propose a named
entity recognition (NER) and relation extraction (RE) approach using DyGIE++
with multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,
to extract ORR catalyst-related information from the scientific literature,
which is compiled into a fuel cell corpus for materials informatics
(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12
critical entities and two relationship types between pairs of the entities. Our
methodology involves data annotation, integration, and fine-tuning of
transformer-based models to enhance information extraction accuracy. We assess
the impact of different BERT variants on extraction performance and investigate
the effects of annotation consistency. Experimental evaluations demonstrate
that the fine-tuned PubMedBERT model achieves the highest NER F1-score of
82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.
Furthermore, the comparison with human annotators highlights the reliability of
fine-tuned models for ORR catalyst extraction, demonstrating their potential
for scalable and automated literature analysis. The results indicate that
domain-specific BERT models outperform general scientific models like BlueBERT
for ORR catalyst extraction.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [17] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

> The paper proposes a Multi-level Mixture of Experts (MMoE) model for Multimodal Entity Linking (MEL) to address issues of mention ambiguity and dynamic selection of modal content.

<details>
  <summary>Details</summary>

**Motivation:** Existing approaches to MEL do not address problems such as mention ambiguity caused by brief textual context and the lack of dynamic selection of important modal content.

**Method:** MMoE includes a description-aware mention enhancement module, a multimodal feature extraction module, and intra-level and inter-level mixture of experts modules that use a switch mixture of experts mechanism to select features.

**Result:** Experiments show that MMoE outperforms state-of-the-art approaches in MEL.

**Conclusion:** The proposed MMoE model significantly improves the performance of multimodal entity linking by dynamically selecting and enhancing relevant features.

**Abstract:** Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [18] [CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings](https://arxiv.org/abs/2507.07125)
*Cristina Mata,Kanchana Ranasinghe,Michael S. Ryoo*

Main category: cs.CV

> 本文提出了一种新的无监督领域适应（UDA）方法CoPT，通过利用领域无关的文本嵌入来提升图像分割领域适应的效果，实验表明该方法在四个基准数据集上达到了新的性能水平。

<details>
  <summary>Details</summary>

**Motivation:** 传统的无监督领域适应（UDA）方法在分割任务上效果有限，尽管在大规模视觉语言表征学习方面取得了进展，但这些进展尚未用于分割任务中的UDA方法。为了利用文本的领域无关特性，提出了一种新的方法。

**Method:** 提出了一种基于协方差的像素-文本损失（CoPT），利用领域无关的文本嵌入来学习图像分割编码器中的领域不变特征。文本嵌入是通过我们的大型语言模型（LLM）领域模板过程生成，该过程使用LLM生成源领域和目标领域的描述，并将其输入到冻结的CLIP模型中进行组合。

**Result:** 在四个基准数据集上的实验表明，使用CoPT的方法显著提升了UDA在分割任务上的性能，达到了新的SOTA水平。

**Conclusion:** 实验结果表明，使用CoPT训练的模型在四个基准数据集上实现了分割任务UDA的新SOTA性能，证明了所提出方法的有效性。

**Abstract:** Unsupervised domain adaptation (UDA) involves learning class semantics from
labeled data within a source domain that generalize to an unseen target domain.
UDA methods are particularly impactful for semantic segmentation, where
annotations are more difficult to collect than in image classification. Despite
recent advances in large-scale vision-language representation learning, UDA
methods for segmentation have not taken advantage of the domain-agnostic
properties of text. To address this, we present a novel Covariance-based
Pixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn
domain-invariant features in an image segmentation encoder. The text embeddings
are generated through our LLM Domain Template process, where an LLM is used to
generate source and target domain descriptions that are fed to a frozen CLIP
model and combined. In experiments on four benchmarks we show that a model
trained using CoPT achieves the new state of the art performance on UDA for
segmentation. The code can be found at https://github.com/cfmata/CoPT.

</details>


### [19] [Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)
*Renyang Liu,Guanlin Li,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

> 研究表明现有图像生成模型的机器未学习技术存在脆弱性，提出一个名为Recall的新对抗框架来突出这些漏洞。

<details>
  <summary>Details</summary>

**Motivation:** 现有的机器未学习技术的鲁棒性和有效性在多模态对抗性输入的情况下尚未得到充分探索。这项研究旨在填补该研究空白，并揭示当前未学习机制中的关键漏洞。

**Method:** 提出一种名为Recall的新对抗框架，专门设计来破坏已去除不良概念的图像生成模型的鲁棒性。Recall不同于现有的主要依赖于对抗性文本提示的方法，而是利用扩散模型内在的多模态条件能力，通过从单个语义相关的参考图像中获取指导来有效优化对抗性图像提示。

**Result:** 广泛的实验结果表明，Recall在对抗有效性、计算效率和与原始文本提示的语义保真度方面始终优于现有的基线方法。

**Conclusion:** 这些发现揭示了当前未学习机制中的关键脆弱性，并强调了确保生成模型安全性和可靠性的更强大解决方案的需求。代码和数据已经公开获取。

**Abstract:** Recent advances in image generation models (IGMs), particularly
diffusion-based architectures such as Stable Diffusion (SD), have markedly
enhanced the quality and diversity of AI-generated visual content. However,
their generative capability has also raised significant ethical, legal, and
societal concerns, including the potential to produce harmful, misleading, or
copyright-infringing content. To mitigate these concerns, machine unlearning
(MU) emerges as a promising solution by selectively removing undesirable
concepts from pretrained models. Nevertheless, the robustness and effectiveness
of existing unlearning techniques remain largely unexplored, particularly in
the presence of multi-modal adversarial inputs.
  To bridge this gap, we propose Recall, a novel adversarial framework
explicitly designed to compromise the robustness of unlearned IGMs. Unlike
existing approaches that predominantly rely on adversarial text prompts, Recall
exploits the intrinsic multi-modal conditioning capabilities of diffusion
models by efficiently optimizing adversarial image prompts with guidance from a
single semantically relevant reference image. Extensive experiments across ten
state-of-the-art unlearning methods and diverse tasks show that Recall
consistently outperforms existing baselines in terms of adversarial
effectiveness, computational efficiency, and semantic fidelity with the
original textual prompt. These findings reveal critical vulnerabilities in
current unlearning mechanisms and underscore the need for more robust solutions
to ensure the safety and reliability of generative models. Code and data are
publicly available at \textcolor{blue}{https://github.com/ryliu68/RECALL}.

</details>


### [20] [Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey](https://arxiv.org/abs/2507.07148)
*Getamesay Haile Dagnaw,Yanming Zhu,Muhammad Hassan Maqsood,Wencheng Yang,Xingshuai Dong,Xuefei Yin,Alan Wee-Chung Liew*

Main category: cs.CV

> 本文系统性地审查并分类解释性人工智能（XAI）方法，特别关注生物医学图像分析领域。文章提出了基于成像模式的分类标准，强调了多模式学习和视觉语言模型的重要性，并讨论了该领域的评估指标、开源框架、挑战及未来方向。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的XAI技术综述往往缺乏基于成像模式的视角，忽视了多模式和视觉语言范式的最新进展，并且提供了有限的实用指南，因此本文旨在从全面和结构化的角度来综合XAI方法，特别针对生物医学图像分析。

**Method:** 本文通过系统地分类解释性人工智能（XAI）方法，并分析这些方法在生物医学图像分析中的基本原理、优势与局限性，弥补了现有综述的不足。特别是，本文提出了基于成像模式的分类方法，指出了跨模式解释性挑战，并探讨了多模式学习和视觉语言模型在可解释生物医学AI中的新兴作用。

**Result:** 该研究提供了一个基于成像模式的分类方法，讨论了广泛应用的评估指标和开源框架，并批判性地讨论了持续存在的挑战和未来的研究方向。该综述为推进生物医学图像分析中的可解释深度学习提供了及时且深入的基础。

**Conclusion:** 综上所述，本文不仅填补了现有XAI综述的空白，还为理解和实施生物医学图像分析中的解释性深度学习提供了全面的指导，并指出了未来的研究方向。

**Abstract:** Explainable artificial intelligence (XAI) has become increasingly important
in biomedical image analysis to promote transparency, trust, and clinical
adoption of DL models. While several surveys have reviewed XAI techniques, they
often lack a modality-aware perspective, overlook recent advances in multimodal
and vision-language paradigms, and provide limited practical guidance. This
survey addresses this gap through a comprehensive and structured synthesis of
XAI methods tailored to biomedical image analysis.We systematically categorize
XAI methods, analyzing their underlying principles, strengths, and limitations
within biomedical contexts. A modality-centered taxonomy is proposed to align
XAI methods with specific imaging types, highlighting the distinct
interpretability challenges across modalities. We further examine the emerging
role of multimodal learning and vision-language models in explainable
biomedical AI, a topic largely underexplored in previous work. Our
contributions also include a summary of widely used evaluation metrics and
open-source frameworks, along with a critical discussion of persistent
challenges and future directions. This survey offers a timely and in-depth
foundation for advancing interpretable DL in biomedical image analysis.

</details>


### [21] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

> 本文探讨了多模态大型语言模型在视觉-语言任务中的模态冲突问题，构建了Multimodal Modality Conflict（MMMC）数据集，并提出了三种缓解模态冲突引起的幻觉的方法，实验结果表明强化学习方法表现最优。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大型语言模型在视觉-语言任务中表现出强大的能力，但在实际应用中它们往往会产生幻觉。目前的研究工作大多是关注模型响应和输入之间的冲突，而本文则着重研究来自不同模态输入之间的内在冲突，这些问题导致模型不具备稳定性。

**Method:** 本文提出了三种基于prompt工程、有监督微调和强化学习的方法来缓解由模态冲突引起的问题，并在Multimodal Modality Conflict数据集上进行实验以评估这些方法的有效性。

**Result:** 实验结果显示，基于强化学习的方法在减轻模态冲突造成的幻觉方面表现出最好的性能，而有监督微调方法则表现出非常有潜力和稳定的结果。

**Conclusion:** 本文揭示了一个值得注意的问题：模态冲突导致多模态大型语言模型产生误解，同时提供了关于增强模型鲁棒性的新见解。

**Abstract:** Despite the impressive capabilities of multimodal large language models
(MLLMs) in vision-language tasks, they are prone to hallucinations in
real-world scenarios. This paper investigates the hallucination phenomenon in
MLLMs from the perspective of modality conflict. Unlike existing works focusing
on the conflicts between model responses and inputs, we study the inherent
conflicts in inputs from different modalities that place MLLMs in a dilemma and
directly lead to hallucinations. We formally define the modality conflict and
construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this
phenomenon in vision-language tasks. Three methods based on prompt engineering,
supervised fine-tuning, and reinforcement learning are proposed to alleviate
the hallucination caused by modality conflict. Extensive experiments are
conducted on the MMMC dataset to analyze the merits and demerits of these
methods. Our results show that the reinforcement learning method achieves the
best performance in mitigating the hallucination under modality conflict, while
the supervised fine-tuning method shows promising and stable performance. Our
work sheds light on the unnoticed modality conflict that leads to
hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [22] [Aerial Maritime Vessel Detection and Identification](https://arxiv.org/abs/2507.07153)
*Antonella Barisic Kulas,Frano Petric,Stjepan Bogdan*

Main category: cs.CV

> 本文开发了一种利用视觉线索识别并定位目标船只的方法，特别适用于没有GNSS信号的环境中，该方法已通过现实世界实验进行验证。

<details>
  <summary>Details</summary>

**Motivation:** 在没有GNSS信号的环境下，自主海上监视和目标船只识别对搜索和救援及威胁检测等应用至关重要。该研究旨在解决仅凭视觉线索且无目标船只的最后已知位置时，无人机如何在严格的计算约束下扫描大面积区域的问题。

**Method:** 本文提出了一种使用YOLOv8模型检测所有可视船只，并通过特征匹配和色调直方图距离分析来识别目标船只的方法。当目标船只被识别后，使用简单的几何原理对目标进行定位。

**Result:** 该方法在MBZIRC2023竞赛的现实世界实验中进行了验证，并与理想情况进行了比较，评估了视角对检测准确性和定位精度的影响。

**Conclusion:** 研究结果表明，采用这种方法即使在GNSS受限的条件下，也能有效实现船只的识别和定位，但视角的不同会影响检测和定位的准确性。

**Abstract:** Autonomous maritime surveillance and target vessel identification in
environments where Global Navigation Satellite Systems (GNSS) are not available
is critical for a number of applications such as search and rescue and threat
detection. When the target vessel is only described by visual cues and its last
known position is not available, unmanned aerial vehicles (UAVs) must rely
solely on on-board vision to scan a large search area under strict
computational constraints. To address this challenge, we leverage the YOLOv8
object detection model to detect all vessels in the field of view. We then
apply feature matching and hue histogram distance analysis to determine whether
any detected vessel corresponds to the target. When found, we localize the
target using simple geometric principles. We demonstrate the proposed method in
real-world experiments during the MBZIRC2023 competition, integrated into a
fully autonomous system with GNSS-denied navigation. We also evaluate the
impact of perspective on detection accuracy and localization precision and
compare it with the oracle approach.

</details>


### [23] [CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation](https://arxiv.org/abs/2507.07154)
*Desheng Li,Chaoliang Liu,Zhiyong Xiao*

Main category: cs.CV

> 本文提出了CL-Polyp，一种基于对比学习的结肠息肉分割网络，通过对比正负样本对来提高特征提取能力，并引入了两个轻量级模块来提升多尺度特征融合和边界重建效果，在多个数据集上表现优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有息肉分割方法依赖辅助任务框架，需要额外标注数据，且受限于任务相似性，这限制了其泛化能力。本文旨在解决这些问题，提出无需额外标注的自我监督策略以改善视觉表示。

**Method:** 本文采用对比学习增强的编码器解码器架构，引入对比学习方法提高特征提取能力，并设计了MASPP和CA模块来提升分割效果。

**Result:** 实验显示，CL-Polyp在Kvasir-SEG和CVC-ClinicDB数据集上分别提高了0.011和0.020的IoU指标，表现优于现有技术。

**Conclusion:** CL-Polyp展示了在无须额外标注的情况下改善息肉分割性能的能力，证明了其在临床息肉分割任务中的有效性。

**Abstract:** Accurate segmentation of polyps from colonoscopy images is crucial for the
early diagnosis and treatment of colorectal cancer. Most existing deep
learning-based polyp segmentation methods adopt an Encoder-Decoder
architecture, and some utilize multi-task frameworks that incorporate auxiliary
tasks such as classification to enhance segmentation performance. However,
these approaches often require additional labeled data and rely on task
similarity, which can limit their generalizability. To address these
challenges, we propose CL-Polyp, a contrastive learning-enhanced polyp
segmentation network. Our method leverages contrastive learning to improve the
encoder's ability to extract discriminative features by contrasting positive
and negative sample pairs derived from polyp images. This self-supervised
strategy enhances visual representation without requiring additional
annotations. In addition, we introduce two lightweight and effective modules:
the Modified Atrous Spatial Pyramid Pooling (MASPP) module for better
multi-scale feature fusion, and the Channel Concatenate and Element Add (CA)
module to fuse low-level and upsampled features for improved boundary
reconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,
CVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp
consistently outperforms state-of-the-art methods. Specifically, it improves
the IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,
respectively, validating its effectiveness in clinical polyp segmentation
tasks.

</details>


### [24] [Interpretable EEG-to-Image Generation with Semantic Prompts](https://arxiv.org/abs/2507.07157)
*Arshak Rezvani,Ali Akbari,Kosar Sanjar Arani,Maryam Mirian,Emad Arasteh,Martin J. McKeown*

Main category: cs.CV

> 研究构建了一个通过多层级语义描述媒介解码EEG信号的视觉体验的方法，实现了图像的生成，并揭示了语义在EEG信号中的分布。

<details>
  <summary>Details</summary>

**Motivation:** 旨在克服EEG在空间细节上的限制，通过文本中介框架实现从EEG信号到图像的解码，结合神经科学与可解释的AI。

**Method:** 利用大规模语言模型生成多层级语义描述（从物体级别到抽象主题），并通过对比学习将EEG信号对齐到这些描述上。在推理阶段，通过投影头检索到的描述嵌入条件化预训练的潜扩散模型进行图像生成。

**Result:** 该方法在EEGCVPR数据集上达到了最先进的视觉解码效果，并揭示了感知图像中不同语义级别的EEG-描述关联性的显着性。

**Conclusion:** 此模型展示了结构化语义中介如何实现认知对齐的视觉从EEG信号的解码，从而提供了对感知图像中语义重要性的新见解。

**Abstract:** Decoding visual experience from brain signals offers exciting possibilities
for neuroscience and interpretable AI. While EEG is accessible and temporally
precise, its limitations in spatial detail hinder image reconstruction. Our
model bypasses direct EEG-to-image generation by aligning EEG signals with
multilevel semantic captions -- ranging from object-level to abstract themes --
generated by a large language model. A transformer-based EEG encoder maps brain
activity to these captions through contrastive learning. During inference,
caption embeddings retrieved via projection heads condition a pretrained latent
diffusion model for image generation. This text-mediated framework yields
state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable
alignment to known neurocognitive pathways. Dominant EEG-caption associations
reflected the importance of different semantic levels extracted from perceived
images. Saliency maps and t-SNE projections reveal semantic topography across
the scalp. Our model demonstrates how structured semantic mediation enables
cognitively aligned visual decoding from EEG.

</details>


### [25] [A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality](https://arxiv.org/abs/2507.07202)
*Mohamed Elmoghany,Ryan Rossi,Seunghyun Yoon,Subhojyoti Mukherjee,Eslam Bakr,Puneet Mathur,Gang Wu,Viet Dac Lai,Nedim Lipka,Ruiyi Zhang,Varun Manjunatha,Chien Nguyen,Daksh Dangi,Abel Salinas,Mohammad Taesiri,Hongjie Chen,Xiaolei Huang,Joe Barrow,Nesreen Ahmed,Hoda Eldardiry,Namyong Park,Yu Wang,Jaemin Cho,Anh Totti Nguyen,Zhengzhong Tu,Thien Nguyen,Dinesh Manocha,Mohamed Elhoseiny,Franck Dernoncourt*

Main category: cs.CV

> 本文研究了32篇生成高质量长视频的论文，揭示了关键架构与策略，构建了新分类法，并提供对比表。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视频生成模型在产生长视频时，尤其是在多角色视频中，难以保持角色一致性和动作连贯性。本研究旨在通过综合分析现有文献，解决这些问题，提升视频生成的质量与多样性。

**Method:** 通过对32篇视频生成论文的全面研究，我们确定了能够持续产生高质量视频的关键架构组件和训练策略。我们还构建了一个现有方法的详尽新分类法，并提供了分类表，根据其架构设计和性能特点对论文进行分类。

**Result:** 提出了一种关于视频生成方法的新分类法，并通过对比表分类现有研究，提高了理解和开发高质量视频生成模型的能力。

**Conclusion:** 我们总结了视频生成研究的关键组件和策略，提出了一个详尽的新分类法，并提供了对比表格。这些工作有助于理解如何持续生成高质量的视频。

**Abstract:** Despite the significant progress that has been made in video generative
models, existing state-of-the-art methods can only produce videos lasting 5-16
seconds, often labeled "long-form videos". Furthermore, videos exceeding 16
seconds struggle to maintain consistent character appearances and scene layouts
throughout the narrative. In particular, multi-subject long videos still fail
to preserve character consistency and motion coherence. While some methods can
generate videos up to 150 seconds long, they often suffer from frame redundancy
and low temporal diversity. Recent work has attempted to produce long-form
videos featuring multiple characters, narrative coherence, and high-fidelity
detail. We comprehensively studied 32 papers on video generation to identify
key architectural components and training strategies that consistently yield
these qualities. We also construct a comprehensive novel taxonomy of existing
methods and present comparative tables that categorize papers by their
architectural designs and performance characteristics.

</details>


### [26] [Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement](https://arxiv.org/abs/2507.07230)
*Priyank Pathak,Yogesh S. Rawat*

Main category: cs.CV

> 提出了CSCI方法，利用颜色信息减轻重识别模型中的外观偏差，显示出比基线更高的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法依赖额外模型或标注的问题，探索使用颜色作为轻量级、无需标注的代理方法。

**Method:** CSCI方法和S2A自注意力机制，直接从原始图像或视频帧中利用颜色信息，以缓解外观偏差。

**Result:** 在四个CC-ReID数据集上进行了广泛实验，提高了图像和视频重识别的准确性。

**Conclusion:** 颜色是一种有效的、成本效益高的解决重识别中外观偏差的代理方法。

**Abstract:** Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals
across different locations and times, irrespective of clothing. Existing
methods often rely on additional models or annotations to learn robust,
clothing-invariant features, making them resource-intensive. In contrast, we
explore the use of color - specifically foreground and background colors - as a
lightweight, annotation-free proxy for mitigating appearance bias in ReID
models. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that
leverages color information directly from raw images or video frames. CSCI
efficiently captures color-related appearance bias ('Color See') while
disentangling it from identity-relevant ReID features ('Color Ignore'). To
achieve this, we introduce S2A self-attention, a novel self-attention to
prevent information leak between color and identity cues within the feature
space. Our analysis shows a strong correspondence between learned color
embeddings and clothing attributes, validating color as an effective proxy when
explicit clothing labels are unavailable. We demonstrate the effectiveness of
CSCI on both image and video ReID with extensive experiments on four CC-ReID
datasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for
image-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID
without relying on additional supervision. Our results highlight the potential
of color as a cost-effective solution for addressing appearance bias in
CC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.

</details>


### [27] [Automated Video Segmentation Machine Learning Pipeline](https://arxiv.org/abs/2507.07242)
*Johannes Merz,Lucien Fostier*

Main category: cs.CV

> The paper introduces an efficient automated video segmentation pipeline leveraging machine learning, which has been successfully adopted in VFX production, cutting down manual mask creation time and boosting efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the slow and resource-intensive process of mask generation in VFX production, aiming to reduce manual effort, speed up the initial compositing stages, and improve the overall efficiency of VFX production.

**Method:** This paper proposes an automated video segmentation pipeline that uses machine learning for object detection with text prompts, per-frame segmentation, and video tracking to maintain temporal stability. The pipeline is deployed using containerization.

**Result:** The pipeline significantly reduces manual labor, expedites the creation of preliminary composites, and provides thorough segmentation data, leading to improved VFX production efficiency.

**Conclusion:** The automated video segmentation pipeline effectively enhances the VFX production process by automating mask generation, thereby saving time and resources.

**Abstract:** Visual effects (VFX) production often struggles with slow, resource-intensive
mask generation. This paper presents an automated video segmentation pipeline
that creates temporally consistent instance masks. It employs machine learning
for: (1) flexible object detection via text prompts, (2) refined per-frame
image segmentation and (3) robust video tracking to ensure temporal stability.
Deployed using containerization and leveraging a structured output format, the
pipeline was quickly adopted by our artists. It significantly reduces manual
effort, speeds up the creation of preliminary composites, and provides
comprehensive segmentation data, thereby enhancing overall VFX production
efficiency.

</details>


### [28] [DisenQ: Disentangling Q-Former for Activity-Biometrics](https://arxiv.org/abs/2507.07262)
*Shehreen Azad,Yogesh S Rawat*

Main category: cs.CV

> 本文提出了DisenQ，这是一种通过结构化语言指导来解耦特征的查询转换器，用于解决活动生物识别问题，优于依赖额外视觉数据的方法并达到了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决活动生物识别领域的问题，即在一系列不同活动中对个体进行识别。这个问题在身份线索与运动动力学和外观变化纠缠在一起时变得更加复杂。

**Method:** 提出了一种多模态语言引导框架，使用结构化文本指导来解耦生物特征、运动和非生物特征特征，而不是依赖额外的视觉数据。核心是一个统一的查询转换器DisenQ，它通过结构化语言指导来分离这些特征。

**Result:** 在三个基于活动的视频基准测试中，该方法达到了最先进的性能。此外，在一个传统的基于视频的身份认证基准测试中也展示了强大的通用性和竞争力。

**Conclusion:** 该框架通过结构化语言指导确保身份线索独立于外观和运动变化，防止错误识别，适合复杂的真实世界场景，表明了框架的有效性。

**Abstract:** In this work, we address activity-biometrics, which involves identifying
individuals across diverse set of activities. Unlike traditional person
identification, this setting introduces additional challenges as identity cues
become entangled with motion dynamics and appearance variations, making
biometrics feature learning more complex. While additional visual data like
pose and/or silhouette help, they often struggle from extraction inaccuracies.
To overcome this, we propose a multimodal language-guided framework that
replaces reliance on additional visual data with structured textual
supervision. At its core, we introduce \textbf{DisenQ} (\textbf{Disen}tangling
\textbf{Q}-Former), a unified querying transformer that disentangles
biometrics, motion, and non-biometrics features by leveraging structured
language guidance. This ensures identity cues remain independent of appearance
and motion variations, preventing misidentifications. We evaluate our approach
on three activity-based video benchmarks, achieving state-of-the-art
performance. Additionally, we demonstrate strong generalization to complex
real-world scenario with competitive performance on a traditional video-based
identification benchmark, showing the effectiveness of our framework.

</details>


### [29] [LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation](https://arxiv.org/abs/2507.07274)
*Ananya Raval,Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

> 本文介绍了LinguaMark，一个用于评估大型多模态模型在多语言视觉问答任务上的基准，测试结果显示Qwen2.5在多语言泛化方面表现突出。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型多模态模型通常在大量的图像-文本数据上进行训练，但它们往往在语言覆盖范围上存在限制，导致跨语言输出存在偏差和不公平问题。以前的工作虽然探索了多模态评估，但较少关注多语言能力的评估。因此，本工作旨在开发一个针对多语言VQA任务的评估基准。

**Method:** 介绍了一个名为LinguaMark的新基准，用于评估先进的大型多模态模型在多语言视觉问答（VQA）任务上的表现。该数据集包含6,875个跨越11种语言和五种社会属性的图像-文本对，并使用偏见、回答相关性和忠实度三个关键指标评估模型。

**Result:** 在LinguaMark基准测试中，闭源模型如GPT-4o和Gemini2.5，以及开源模型如Gemma3和Qwen2.5，在偏见、回答相关性和忠实度这三个关键指标上表现良好。Qwen2.5尤其在不同语言之间表现出强大的泛化能力。

**Conclusion:** 研究发现闭源模型总体表现最佳，闭源模型（GPT-4o 和 Gemini2.5）和开源模型（Gemma3，Qwen2.5）在社会属性方面表现均较强，特别地，Qwen2.5 在多种语言中表现出强大的泛化能力。提供基准和评估代码鼓励可重复性和进一步的研究。

**Abstract:** Large Multimodal Models (LMMs) are typically trained on vast corpora of
image-text data but are often limited in linguistic coverage, leading to biased
and unfair outputs across languages. While prior work has explored multimodal
evaluation, less emphasis has been placed on assessing multilingual
capabilities. In this work, we introduce LinguaMark, a benchmark designed to
evaluate state-of-the-art LMMs on a multilingual Visual Question Answering
(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages
and five social attributes. We evaluate models using three key metrics: Bias,
Answer Relevancy, and Faithfulness. Our findings reveal that closed-source
models generally achieve the highest overall performance. Both closed-source
(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform
competitively across social attributes, and Qwen2.5 demonstrates strong
generalization across multiple languages. We release our benchmark and
evaluation code to encourage reproducibility and further research.

</details>


### [30] [MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning](https://arxiv.org/abs/2507.07297)
*Chengfei Wu,Ronald Seoh,Bingxuan Li,Liqiang Zhang,Fengrong Han,Dan Goldwasser*

Main category: cs.CV

> 研究者们发布了MagiC基准测试，用于评估视觉语言模型在视觉推理任务中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型视觉语言模型已经在视觉问题回答和多模态推理方面表现出色，但仍不清楚这些模型是否真正进行了基于视觉的推理，还是仅仅依赖于表面模式和数据集偏差。本研究旨在评估这些模型进行基于视觉的多模态认知的能力。

**Method:** 本研究介绍了一个名为MagiC的全面基准测试，旨在评估基于视觉的多模态认知。该基准不仅评估答案的准确性，还评估逐步推理的质量及其与相关视觉证据的一致性。基准包含大约5,500个弱监督QA样本和900个人类策划样本，这些样本具有细粒度的注释，包括答案、推理依据和边界框定位。

**Result:** 研究对15个参数量从7B到70B的视觉语言模型进行了评估，从最终答案的准确性、推理的有效性、定位的真实性和自我校正能力四个维度进行。此外，还包含了诊断设置来探测模型在对抗性视觉线索下的鲁棒性，并评估其内省错误纠正能力。提出了新的评估指标MagiScore和StepSense。

**Conclusion:** 全面的分析揭示了当前方法在基于视觉的推理方面的关键限制和机遇。本研究献不仅提供了一个评估基准，还引入了新的评估指标，为未来的研究指明了方向。

**Abstract:** Recent advances in large vision-language models have led to impressive
performance in visual question answering and multimodal reasoning. However, it
remains unclear whether these models genuinely perform grounded visual
reasoning or rely on superficial patterns and dataset biases. In this work, we
introduce MagiC, a comprehensive benchmark designed to evaluate grounded
multimodal cognition, assessing not only answer accuracy but also the quality
of step-by-step reasoning and its alignment with relevant visual evidence. Our
benchmark includes approximately 5,500 weakly supervised QA examples generated
from strong model outputs and 900 human-curated examples with fine-grained
annotations, including answers, rationales, and bounding box groundings. We
evaluate 15 vision-language models ranging from 7B to 70B parameters across
four dimensions: final answer correctness, reasoning validity, grounding
fidelity, and self-correction ability. MagiC further includes diagnostic
settings to probe model robustness under adversarial visual cues and assess
their capacity for introspective error correction. We introduce new metrics
such as MagiScore and StepSense, and provide comprehensive analyses that reveal
key limitations and opportunities in current approaches to grounded visual
reasoning.

</details>


### [31] [ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation](https://arxiv.org/abs/2507.07317)
*Sherry X. Chen,Yi Wei,Luowei Zhou,Suren Kumar*

Main category: cs.CV

> 提出了ADIEE方法来构建一个大规模数据集，并基于该数据集训练了一种新的评分模型，用于指令引导图像编辑的自动评估，该模型的性能优于所有开源VLM和Gemini-Pro 1.5。

<details>
  <summary>Details</summary>

**Motivation:** 当前的方法在对指令引导的图像编辑进行自动评估时面临挑战，开源VLM模型在对齐方面表现不佳，而专有模型因为缺乏透明度和高昂的成本效率不足。此外，公开的训练数据集的缺乏限制了模型的使用和改进。因此，研究团队引入了ADIEE来应对这些挑战。

**Method:** 我们提出了一种名为ADIEE的自动数据集创建方法来解决现有的评估难题。该方法用于创建包含超过10万样本的大规模数据集，并利用这个数据集对LLaVA-NeXT-8B模型进行微调，使其能够解码来自自定义标记的数值评分。

**Result:** 训练出的新模型在与人类评分的相关性上提高了17.24%，在对MagicBrush模型的评估提升了8.98%。

**Conclusion:** 提出的评分模型可以作为一个奖励模型，用于自动选择最佳图像编辑和模型微调，展示了其在提高图像编辑评估准确性方面的潜力。

**Abstract:** Recent advances in instruction-guided image editing underscore the need for
effective automated evaluation. While Vision-Language Models (VLMs) have been
explored as judges, open-source models struggle with alignment, and proprietary
models lack transparency and cost efficiency. Additionally, no public training
datasets exist to fine-tune open-source VLMs, only small benchmarks with
diverse evaluation schemes. To address this, we introduce ADIEE, an automated
dataset creation approach which is then used to train a scoring model for
instruction-guided image editing evaluation. We generate a large-scale dataset
with over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified
to decode a numeric score from a custom token. The resulting scorer outperforms
all open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a
0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,
and improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench
and 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the
state-of-the-art. The scorer can act as a reward model, enabling automated best
edit selection and model fine-tuning. Notably, the proposed scorer can boost
MagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43
(+8.98%).

</details>


### [32] [Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory](https://arxiv.org/abs/2507.07333)
*Hui Pang,Sunil Hadap,Violetta Shevchenko,Rahul Suresh,Amin Banitalebi-Dehkordi*

Main category: cs.CV

> 研究提出了一种新的方法，通过近似Kubelka-Munk理论实现了快速且逼真的粉底试妆，构建了一个可扩展的端到端框架，并在实际应用中证明了其有效性和优势。

<details>
  <summary>Details</summary>

**Motivation:** 这项工作的动机在于解决美容行业中虚拟试妆应用的关键技术挑战，特别是准确地合成粉底与皮肤色调的融合，并保持该方法在不同产品范围内的可扩展性。

**Method:** 我们提出了一种新颖的方法来近似已建立的Kubelka-Munk理论，以实现更快速的图像合成，同时保留粉底与皮肤色调的逼真融合。此外，我们构建了一个仅依赖于电子商务网站上产品信息的可扩展端到端框架，用于实现逼真的粉底试妆应用。

**Result:** 我们使用现实生活中的彩妆图像验证了我们的方法，证明我们的框架优于其他技术。

**Conclusion:** 我们的研究为实现快速且逼真的粉底试妆虚拟应用提供了一个高效的解决方案。

**Abstract:** Augmented reality is revolutionizing beauty industry with virtual try-on
(VTO) applications, which empowers users to try a wide variety of products
using their phones without the hassle of physically putting on real products. A
critical technical challenge in foundation VTO applications is the accurate
synthesis of foundation-skin tone color blending while maintaining the
scalability of the method across diverse product ranges. In this work, we
propose a novel method to approximate well-established Kubelka-Munk (KM) theory
for faster image synthesis while preserving foundation-skin tone color blending
realism. Additionally, we build a scalable end-to-end framework for realistic
foundation makeup VTO solely depending on the product information available on
e-commerce sites. We validate our method using real-world makeup images,
demonstrating that our framework outperforms other techniques.

</details>


### [33] [Entity Re-identification in Visual Storytelling via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.07340)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

> 提出了对比强化学习方法，以改进视觉叙事模型在不同帧之间建立连贯实体连接的能力，通过微调Qwen Storyteller模型，显著提升了故事连贯性、目标检测和角色物体持续性。

<details>
  <summary>Details</summary>

**Motivation:** 视觉叙事系统，尤其是大规模视觉语言模型，难以在不同帧中保持角色和对象身份的一致性，导致参考不一致和参考幻象。这主要是因为模型缺乏在不同帧之间建立实体连接的明确训练。

**Method:** 使用对比强化学习方法训练模型，以区分连贯的图像序列和不相关的图像。通过扩展Story Reasoning数据集并引入合成负面样本，训练模型正确连接实体。采用直接偏好优化，使用双组件奖励函数，推广实体在真实故事中的定位和重新识别，并对合成上下文中不正确的实体连接进行惩罚。

**Result:** 通过对比框架微调Qwen Storyteller模型（基于Qwen2.5-VL 7B），评估结果显示：目标检测mAP从0.27提高到0.31，F1分数从0.35提升至0.41。除“its”之外，各类代词的地改善显著，跨帧角色和物体持续性在所有帧计数中增加，特别是在5帧以上的实体持续性从29.3%增加到33.3%。结构良好的故事比例从79.1%提升至97.5%。

**Conclusion:** 研究显示，对比强化学习方法能够有效提升视觉叙事模型在处理跨帧实体连接方面的能力，并且在多项评估指标上均有显著改善。这为未来的视觉叙事系统的实体一致性提供了新的解决方案。

**Abstract:** Visual storytelling systems, particularly large vision-language models,
struggle to maintain character and object identity across frames,
  often failing to recognize when entities in different images represent the
same individuals or objects,
  leading to inconsistent references and referential hallucinations.
  This occurs because models lack explicit training on when to establish entity
connections across frames.
  We propose a contrastive reinforcement learning approach that trains models
to discriminate between coherent image sequences
  and stories from unrelated images.
  We extend the Story Reasoning dataset with synthetic negative examples to
teach appropriate entity connection behavior.
  We employ Direct Preference Optimization with a dual-component reward
function that promotes grounding and re-identification of entities
  in real stories while penalizing incorrect entity connections in synthetic
contexts.
  Using this contrastive framework, we fine-tune Qwen Storyteller (based on
Qwen2.5-VL 7B).
  Evaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1
from 0.35 to 0.41 (+17.1%).
  Pronoun grounding accuracy improved across all pronoun types except ``its'',
  and cross-frame character and object persistence increased
  across all frame counts, with entities appearing in 5 or more frames
advancing from 29.3% to 33.3% (+13.7%).
  Well-structured stories, containing the chain-of-thought and grounded story,
increased from 79.1% to 97.5% (+23.3%).

</details>


### [34] [PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency](https://arxiv.org/abs/2507.07374)
*Haotian Wang,Aoran Xiao,Xiaoqin Zhang,Meng Yang,Shijian Lu*

Main category: cs.CV

> 提出PacGDC技术，通过合成几何体的多尺度版本来改进深度完成的泛化能力，减少了收集训练所需的大规模度量深度标签数据集的负担。

<details>
  <summary>Details</summary>

**Motivation:** 目的是通过提出PacGDC技术，以极小的注释工作量增强数据多样性，解决训练广义深度完成模型所需的具有度量深度标签的大规模数据集的收集问题。

**Method:** PacGDC技术利用对2D到3D投影中物体形状和位置固有歧义和一致性的新见解来合成同一视觉场景的多个伪几何体，扩大深度图的尺度和多样性。采用多个深度基础模型作为尺度调节器，并结合插值、重定位策略和无标签图像以进一步提高几何多样性。

**Result:** 实验表明，PacGDC在多个基准上表现出显著的泛化能力，在各种场景语义、尺度和深度稀疏模式下表现优异，无论是零样本还是少样本设置下均出众。

**Conclusion:** PacGDC技术有效解决了广义深度完成任务中注释工作量大、数据多样性不足的问题，显示了其在不同条件下的泛化能力。

**Abstract:** Generalizable depth completion enables the acquisition of dense metric depth
maps for unseen environments, offering robust perception capabilities for
various downstream tasks. However, training such models typically requires
large-scale datasets with metric depth labels, which are often labor-intensive
to collect. This paper presents PacGDC, a label-efficient technique that
enhances data diversity with minimal annotation effort for generalizable depth
completion. PacGDC builds on novel insights into inherent ambiguities and
consistencies in object shapes and positions during 2D-to-3D projection,
allowing the synthesis of numerous pseudo geometries for the same visual scene.
This process greatly broadens available geometries by manipulating scene scales
of the corresponding depth maps. To leverage this property, we propose a new
data synthesis pipeline that uses multiple depth foundation models as scale
manipulators. These models robustly provide pseudo depth labels with varied
scene scales, affecting both local objects and global layouts, while ensuring
projection consistency that supports generalization. To further diversify
geometries, we incorporate interpolation and relocation strategies, as well as
unlabeled images, extending the data coverage beyond the individual use of
foundation models. Extensive experiments show that PacGDC achieves remarkable
generalizability across multiple benchmarks, excelling in diverse scene
semantics/scales and depth sparsity/patterns under both zero-shot and few-shot
settings. Code: https://github.com/Wang-xjtu/PacGDC.

</details>


### [35] [Adaptive Particle-Based Shape Modeling for Anatomical Surface Correspondence](https://arxiv.org/abs/2507.07379)
*Hong Xu,Shireen Y. Elhabian*

Main category: cs.CV

> The paper presents a novel approach to improve adaptivity in particle-based shape modeling by introducing a new neighborhood correspondence loss and a geodesic correspondence algorithm, which enhance the model's ability to accurately represent complex anatomical structures.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the lack of self-adaptivity in particle-based shape modeling for anatomical structures, aiming to more accurately represent complex anatomical variability.

**Method:** This paper introduces two mechanisms to enhance adaptivity in PSM: a new neighborhood correspondence loss and a geodesic correspondence algorithm. These improvements aim to better adjust particle configurations to local geometric features without losing consistency.

**Result:** The approach is evaluated on challenging datasets and shows improved adaptivity while maintaining surface representation accuracy and correspondence metrics compared to existing methods.

**Conclusion:** The results suggest that the proposed methods offer a balance in adaptivity and consistency, making them suitable for representing complex anatomical variability in shape modeling.

**Abstract:** Particle-based shape modeling (PSM) is a family of approaches that
automatically quantifies shape variability across anatomical cohorts by
positioning particles (pseudo landmarks) on shape surfaces in a consistent
configuration. Recent advances incorporate implicit radial basis function
representations as self-supervised signals to better capture the complex
geometric properties of anatomical structures. However, these methods still
lack self-adaptivity -- that is, the ability to automatically adjust particle
configurations to local geometric features of each surface, which is essential
for accurately representing complex anatomical variability. This paper
introduces two mechanisms to increase surface adaptivity while maintaining
consistent particle configurations: (1) a novel neighborhood correspondence
loss to enable high adaptivity and (2) a geodesic correspondence algorithm that
regularizes optimization to enforce geodesic neighborhood consistency. We
evaluate the efficacy and scalability of our approach on challenging datasets,
providing a detailed analysis of the adaptivity-correspondence trade-off and
benchmarking against existing methods on surface representation accuracy and
correspondence metrics.

</details>


### [36] [Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos](https://arxiv.org/abs/2507.07381)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.CV

> 为了改进现有精确事件检测模型的时间和空间局限性，作者提出了多尺度注意门移模块（MSAGSM），并在多个基准测试中验证了其优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有精确事件检测模型在处理视频中精细动作识别时的能力有限，尤其是在时间感受野和空间适应性方面。为了改进这些局限，作者提出了新的模块MSAGSM。

**Method:** 我们提出了一种多尺度注意门移模块（MSAGSM），该模块通过引入多尺度时间扩张和多头空间注意力机制增强了传统的Gate Shift Module（GSM），以更高效地建模短期和长期依赖关系并聚焦于显著区域。MSAGSM是一个轻量级的模块，可以轻松集成到各种二维主干网络中。

**Result:** 通过在五个精确事件检测基准上进行的广泛实验表明，MSAGSM在性能上实现了一致的提升，同时保持了计算上的效率，并设定了新的行业标准结果。

**Conclusion:** 我们提出了一个轻量级插件式模块MSAGSM，该模块通过引入多尺度时间扩张和多头空间注意力机制，提高了GSM的能力。该模块在多个基准上验证了其有效性和性能提升，并在乒乓球澳大利亚数据集等数据集上达到了新的SOTA结果。

**Abstract:** Precise Event Spotting (PES) in sports videos requires frame-level
recognition of fine-grained actions from single-camera footage. Existing PES
models typically incorporate lightweight temporal modules such as Gate Shift
Module (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with
temporal context. However, these modules are limited in both temporal receptive
field and spatial adaptability. We propose a Multi-Scale Attention Gate Shift
Module (MSAGSM) that enhances GSM with multi-scale temporal dilations and
multi-head spatial attention, enabling efficient modeling of both short- and
long-term dependencies while focusing on salient regions. MSAGSM is a
lightweight plug-and-play module that can be easily integrated with various 2D
backbones. To further advance the field, we introduce the Table Tennis
Australia (TTA) dataset-the first PES benchmark for table tennis-containing
over 4800 precisely annotated events. Extensive experiments across five PES
benchmarks demonstrate that MSAGSM consistently improves performance with
minimal overhead, setting new state-of-the-art results.

</details>
