{"id": "2601.14258", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.14258", "abs": "https://arxiv.org/abs/2601.14258", "authors": ["Ho Yin Au", "Junkun Jiang", "Jie Chen"], "title": "SOSControl: Enhancing Human Motion Generation through Saliency-Aware Symbolic Orientation and Timing Control", "comment": "Accepted by AAAI 2026", "summary": "Traditional text-to-motion frameworks often lack precise control, and existing approaches based on joint keyframe locations provide only positional guidance, making it challenging and unintuitive to specify body part orientations and motion timing. To address these limitations, we introduce the Salient Orientation Symbolic (SOS) script, a programmable symbolic framework for specifying body part orientations and motion timing at keyframes. We further propose an automatic SOS extraction pipeline that employs temporally-constrained agglomerative clustering for frame saliency detection and a Saliency-based Masking Scheme (SMS) to generate sparse, interpretable SOS scripts directly from motion data. Moreover, we present the SOSControl framework, which treats the available orientation symbols in the sparse SOS script as salient and prioritizes satisfying these constraints during motion generation. By incorporating SMS-based data augmentation and gradient-based iterative optimization, the framework enhances alignment with user-specified constraints. Additionally, it employs a ControlNet-based ACTOR-PAE Decoder to ensure smooth and natural motion outputs. Extensive experiments demonstrate that the SOS extraction pipeline generates human-interpretable scripts with symbolic annotations at salient keyframes, while the SOSControl framework outperforms existing baselines in motion quality, controllability, and generalizability with respect to motion timing and body part orientation control.", "AI": {"tldr": "提出了一种基于Salient Orientation Symbolic (SOS)脚本的框架，用于更精确的控制动作中的身体部位方向和动作时间，并提出了一种自动提取SOS脚本的管道和一个名为SOSControl的框架来优化动作生成。实验表明该方法在动作质量、可控制性和泛化能力方面优于现有方法。", "motivation": "解决传统文本到动作转换框架中精确控制不足的问题，特别是在指定身体部位方向和动作时间方面。现有基于关键帧定位的方法只能提供位置指导，无法直观地控制身体部位方向和动作时间。", "method": "提出SOS脚本框架，用于在关键帧上精确指定身体部位方向和动作时间；设计了一个自动SOS提取管道，其中包含基于时间约束的凝聚式聚类方法进行关键帧重要性检测和基于此类检测的掩模生成；提出了SOSControl框架，优先考虑满足稀疏SOS脚本中的方向符号约束，并通过基于SMS的数据扩增和梯度迭代优化来增强运动的用户控制一致性；采用ControlNet基础的ACTOR-PAE解码器来保证运动输出的平滑自然性。", "result": "实验表明SOS提取管道可以生成人类可理解的关键帧符号注释脚本，而SOSControl框架在动作质量、控制能力和泛化能力方面优于现有方法，特别是在动作时间控制和身体部位方向控制方面。", "conclusion": "通过采用SOS脚本和SOSControl框架，可以实现对动作生成更精确和直观的控制，提高了动作质量、可控制性和泛化能力。"}}
{"id": "2601.14259", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14259", "abs": "https://arxiv.org/abs/2601.14259", "authors": ["Ziwen Zhong", "Zhitao Shu", "Yue Zhao"], "title": "A Cloud-Based Cross-Modal Transformer for Emotion Recognition and Adaptive Human-Computer Interaction", "comment": null, "summary": "Emotion recognition is a fundamental component of next-generation human-computer interaction (HCI), enabling machines to perceive, understand, and respond to users' affective states. However, existing systems often rely on single-modality analysis such as facial expressions, speech tone, or textual sentiment, resulting in limited robustness and poor generalization in real-world environments. To address these challenges, this study proposes a Cloud-Based Cross-Modal Transformer (CMT) framework for multimodal emotion recognition and adaptive human-computer interaction. The proposed model integrates visual, auditory, and textual signals using pretrained encoders (Vision Transformer, Wav2Vec2, and BERT) and employs a cross-modal attention mechanism to capture complex interdependencies among heterogeneous features. By leveraging cloud computing infrastructure with distributed training on Kubernetes and TensorFlow Serving, the system enables scalable, low-latency emotion recognition for large-scale user interactions. Experiments conducted on benchmark datasets including IEMOCAP, MELD, and AffectNet demonstrate that the CMT achieves state-of-the-art performance, improving the F1-score by 3.0 percent and reducing cross-entropy loss by 12.9 percent compared to strong multimodal baselines. Additionally, cloud deployment evaluations show an average response latency of 128 ms, representing a 35 percent reduction compared with conventional transformer-based fusion systems. These results confirm that the proposed framework enables efficient, real-time emotion recognition and adaptive feedback in applications such as intelligent customer service, virtual tutoring systems, and affective computing interfaces, marking an important step toward cloud-native affective computing and emotionally intelligent interactive systems.", "AI": {"tldr": "A Cloud-Based Cross-Modal Transformer (CMT) framework for multimodal emotion recognition with state-of-the-art performance and low latency is proposed.", "motivation": "Existing systems in emotion recognition are often reliant on single-modality analysis which limits their robustness and generalization capabilities in real-world scenarios. The study aims to overcome these limitations by presenting a more versatile and scalable multi-modal approach and cloud-based solution.", "method": "The paper proposes a Cloud-Based Cross-Modal Transformer (CMT) framework that integrates visual, auditory, and textual signals using pretrained encoders (Vision Transformer, Wav2Vec2, and BERT) and employs a cross-modal attention mechanism to handle multimodal emotion recognition.", "result": "The CMT framework has been tested on multiple benchmark datasets showing improved F1-score by 3.0% and a 12.9% reduction in cross-entropy loss compared to existing baselines. Additionally, cloud deployment achieved an average response latency of 128ms, reducing latency by 35% compared to conventional models.", "conclusion": "The proposed CMT framework represents a significant advancement in cloud-native affective computing and emotionally intelligent interactive systems, enabling efficient, real-time emotion recognition in diverse applications such as customer service and virtual tutoring systems."}}
{"id": "2601.14261", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14261", "abs": "https://arxiv.org/abs/2601.14261", "authors": ["Taoliang Tan", "Chengwei Ma", "Zhen Tian", "Zhao Lin", "Dongdong Li", "Si Shi"], "title": "Intelligent Power Grid Design Review via Active Perception-Enabled Multimodal Large Language Models", "comment": null, "summary": "The intelligent review of power grid engineering design drawings is crucial for power system safety. However, current automated systems struggle with ultra-high-resolution drawings due to high computational demands, information loss, and a lack of holistic semantic understanding for design error identification. This paper proposes a novel three-stage framework for intelligent power grid drawing review, driven by pre-trained Multimodal Large Language Models (MLLMs) through advanced prompt engineering. Mimicking the human expert review process, the first stage leverages an MLLM for global semantic understanding to intelligently propose domain-specific semantic regions from a low-resolution overview. The second stage then performs high-resolution, fine-grained recognition within these proposed regions, acquiring detailed information with associated confidence scores. In the final stage, a comprehensive decision-making module integrates these confidence-aware results to accurately diagnose design errors and provide a reliability assessment. Preliminary results on real-world power grid drawings demonstrate our approach significantly enhances MLLM's ability to grasp macroscopic semantic information and pinpoint design errors, showing improved defect discovery accuracy and greater reliability in review judgments compared to traditional passive MLLM inference. This research offers a novel, prompt-driven paradigm for intelligent and reliable power grid drawing review.", "AI": {"tldr": "本文提出了一种新的三阶段框架，用于电网设计图纸的智能审查，通过多模态大语言模型（MLLMs）和高级提示工程来提高语义理解和设计错误识别的准确性。", "motivation": "传统的自动化系统在处理超高清电网设计图纸时面临计算需求高、信息丢失和缺乏整体语义理解等问题，这使得识别设计错误具有挑战性。", "method": "本研究提出了一种三阶段框架，利用预训练的多模态大语言模型（MLLMs）并通过先进的提示工程驱动。第一阶段利用MLLM进行全局语义理解，从低分辨率概述中智能地提出特定领域的语义区域；第二阶段在这些区域进行高分辨率的细粒度识别，获得详细信息及其置信度分数；第三阶段综合这些置信度感知的结果，准确诊断设计错误并提供可靠性评估。", "result": "初步结果表明，本方法显著提高了MLLM理解全局语义信息及识别设计错误的能力，在缺陷发现准确性和审查判断的可靠性方面超过了传统被动的MLLM推断。", "conclusion": "这项研究提供了一种新型的、由提示驱动的电网设计图纸智能审查模式。"}}
{"id": "2601.14330", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14330", "abs": "https://arxiv.org/abs/2601.14330", "authors": ["Mengyu Sun", "Ziyuan Yang", "Andrew Beng Jin Teoh", "Junxu Liu", "Haibo Hu", "Yi Zhang"], "title": "LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models", "comment": null, "summary": "Concept erasure aims to suppress sensitive content in diffusion models, but recent studies show that erased concepts can still be reawakened, revealing vulnerabilities in erasure methods. Existing reawakening methods mainly rely on prompt-level optimization to manipulate sampling trajectories, neglecting other generative factors, which limits a comprehensive understanding of the underlying dynamics. In this paper, we model the generation process as an implicit function to enable a comprehensive theoretical analysis of multiple factors, including text conditions, model parameters, and latent states. We theoretically show that perturbing each factor can reawaken erased concepts. Building on this insight, we propose a novel concept reawakening method: Latent space Unblocking for concept REawakening (LURE), which reawakens erased concepts by reconstructing the latent space and guiding the sampling trajectory. Specifically, our semantic re-binding mechanism reconstructs the latent space by aligning denoising predictions with target distributions to reestablish severed text-visual associations. However, in multi-concept scenarios, naive reconstruction can cause gradient conflicts and feature entanglement. To address this, we introduce Gradient Field Orthogonalization, which enforces feature orthogonality to prevent mutual interference. Additionally, our Latent Semantic Identification-Guided Sampling (LSIS) ensures stability of the reawakening process via posterior density verification. Extensive experiments demonstrate that LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods.", "AI": {"tldr": "A novel method, LURE, is proposed to reawaken erased concepts in diffusion models by comprehensively analyzing and manipulating generative factors, proving effective across various scenarios.", "motivation": "To address the vulnerabilities in erasure methods of diffusion models where erased concepts can still be reawakened. Seeks a comprehensive understanding of the underlying dynamics beyond prompt-level optimizations, considering multiple generative factors.", "method": "Content aims to suppress sensitive content in diffusion models but shows that erased concepts can still be reawakened. Analyzes generation process as implicit function, examining text conditions, model parameters, and latent states to theoretically demonstrate perturbing these factors can reawaken erased concepts. Proposes Latent space Unblocking for concept REawakening (LURE), reconstructing latent space and guiding sampling trajectory through semantic re-binding, which aligns denoising predictions with target distributions. Introduces Gradient Field Orthogonalization to prevent gradient conflicts and feature entanglement in multi-concept scenarios, and Latent Semantic Identification-Guided Sampling (LSIS) for stability.", "result": "LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods, as verified by extensive experiments.", "conclusion": "The paper demonstrates the theoretical and practical effectiveness of the LURE method in reawakening erased concepts in diffusion models by addressing the multi-faceted challenges in the generation process."}}
{"id": "2601.14267", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14267", "abs": "https://arxiv.org/abs/2601.14267", "authors": ["Pouria Mortezaagha", "Joseph Shaw", "Bowen Sun", "Arya Rahgozar"], "title": "From Chaos to Clarity: Schema-Constrained AI for Auditable Biomedical Evidence Extraction from Full-Text PDFs", "comment": null, "summary": "Biomedical evidence synthesis relies on accurate extraction of methodological, laboratory, and outcome variables from full-text research articles, yet these variables are embedded in complex scientific PDFs that make manual abstraction time-consuming and difficult to scale. Existing document AI systems remain limited by OCR errors, long-document fragmentation, constrained throughput, and insufficient auditability for high-stakes synthesis. We present a schema-constrained AI extraction system that transforms full-text biomedical PDFs into structured, analysis-ready records by explicitly restricting model inference through typed schemas, controlled vocabularies, and evidence-gated decisions. Documents are ingested using resume-aware hashing, partitioned into caption-aware page-level chunks, and processed asynchronously under explicit concurrency controls. Chunk-level outputs are deterministically merged into study-level records using conflict-aware consolidation, set-based aggregation, and sentence-level provenance to support traceability and post-hoc audit. Evaluated on a corpus of studies on direct oral anticoagulant level measurement, the pipeline processed all documents without manual intervention, maintained stable throughput under service constraints, and exhibited strong internal consistency across document chunks. Iterative schema refinement substantially improved extraction fidelity for synthesis-critical variables, including assay classification, outcome definitions, follow-up duration, and timing of measurement. These results demonstrate that schema-constrained, provenance-aware extraction enables scalable and auditable transformation of heterogeneous scientific PDFs into structured evidence, aligning modern document AI with the transparency and reliability requirements of biomedical evidence synthesis.", "AI": {"tldr": "本文提出了一种新的基于模式约束的AI抽取系统，该系统能够转化为结构化的、可分析的数据记录，并在一次评估中展示了其稳定性和精确性，证明了它能够提供可扩展且可靠的生物医学证据合成所需的透明性和可靠性。", "motivation": "生物医学证据合成依赖于从全文研究文章中准确提取方法学、实验室和结果变量，然而这些变量嵌入在复杂的科学PDF中，使得手动提取过程费时且难以扩展。现有的文档AI系统在光学字符识别错误、长文档片段化、吞吐量受限以及高风险合成的审计不足方面仍存在限制。", "method": "本文提出了一种基于模式约束的AI抽取系统，该系统通过使用类型化模式、受控词汇表和证据门控决策来显式限制模型推断，从而将全文生物医学PDF转化为结构化的、可供分析的数据记录。文档采用简历感知哈希的方式被摄取，以标题感知的方式被分割成页面级别的块，并在显式并发控制下异步处理。这些块级别的输出通过冲突感知整合、集合聚合以及句子级别的溯源进行确定性合并，以支持可追溯性和事后审计。", "result": "对直接口服抗凝剂水平测量的研究语料库进行评估后，该流程在没有人工干预的情况下处理了所有文档，并在线服务限制下保持了稳定的吞吐量。迭代模式优化显著提升了关键合成变量的提取准确性。", "conclusion": "结果表明，基于模式约束和证据溯源的抽取系统能将异质性的科学PDF转化为结构化的证据，符合现代文档AI与生物医学证据合成的透明性和可靠性要求。"}}
{"id": "2601.14339", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14339", "abs": "https://arxiv.org/abs/2601.14339", "authors": ["Haotian Xu", "Yue Hu", "Zhengqiu Zhu", "Chen Gao", "Ziyou Wang", "Junreng Rao", "Wenhao Lu", "Weishi Li", "Quanjun Yin", "Yong Li"], "title": "CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments", "comment": null, "summary": "Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.", "AI": {"tldr": "本研究提出了一个新的基准CityCube，用于评估视觉语言模型在城市环境中的跨视角推理能力，并发现大规模模型的表现不佳，同时表明了小规模模型在特定任务上的潜力。", "motivation": "现有的基准大多集中在室内或街道，忽视了开放城市空间的独特挑战。CityCube旨在解决这些问题。", "method": "本研究提出了CityCube，一个用于评估视觉语言模型在城市环境中跨视角推理能力的系统性基准。CityCube涵盖了多种视角动态变化，并且包含了精心标注的5,022个多视角问答对，分为五个认知维度和三种空间关系表达。", "result": "评估了33个视觉语言模型，发现即使是大规模模型也难以超过54.1%的准确率，比人类性能低34.2%。而经过微调的小规模模型能达到超过60.0%的准确率。", "conclusion": "CityCube评估显示大规模模型在跨视角推理任务中的表现存在显著差异，也揭示了视觉语言模型与人类推理之间的根本认知差异。"}}
{"id": "2601.14269", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14269", "abs": "https://arxiv.org/abs/2601.14269", "authors": ["Youyou Cheng", "Zhuangwei Kang", "Kerry Jiang", "Chenyu Sun", "Qiyang Pan"], "title": "The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues", "comment": null, "summary": "Large language models (LLMs) have been widely used for mental health support. However, current safety evaluations in this field are mostly limited to detecting whether LLMs output prohibited words in single-turn conversations, neglecting the gradual erosion of safety boundaries in long dialogues. Examples include making definitive guarantees, assuming responsibility, and playing professional roles. We believe that with the evolution of mainstream LLMs, words with obvious safety risks are easily filtered by their underlying systems, while the real danger lies in the gradual transgression of boundaries during multi-turn interactions, driven by the LLM's attempts at comfort and empathy.\n  This paper proposes a multi-turn stress testing framework and conducts long-dialogue safety tests on three cutting-edge LLMs using two pressure methods: static progression and adaptive probing. We generated 50 virtual patient profiles and stress-tested each model through up to 20 rounds of virtual psychiatric dialogues. The experimental results show that violations are common, and both pressure modes produced similar violation rates. However, adaptive probing significantly advanced the time at which models crossed boundaries, reducing the average number of turns from 9.21 in static progression to 4.64. Under both mechanisms, making definitive or zero-risk promises was the primary way in which boundaries were breached. These findings suggest that the robustness of LLM safety boundaries cannot be inferred solely through single-turn tests; it is necessary to fully consider the wear and tear on safety boundaries caused by different interaction pressures and characteristics in extended dialogues.", "AI": {"tldr": "本文发现大型语言模型在心理健康支持的多轮对话中存在边界漂移的问题，提出并通过多轮压力测试框架验证，发现自适应探测显著提升违规出现的速度。", "motivation": "当前在心理健康支持领域，大型语言模型（LLMs）的安全评估主要集中在单轮对话中检测是否输出禁止词汇，忽视了多轮对话中安全边界的逐渐侵蚀。这种侵蚀体现在语言模型尝试提供安慰和共情时，可能会做出明确的保证、承担责任或扮演专业角色。随着主流LLMs的发展，明显有安全风险的词汇容易被系统过滤，但真正的危险在于多轮互动中边界的渐进式跨越。", "method": "提出了一种多轮压力测试框架，并对三个前沿的LLMs进行了长时间对话的安全测试。采用了两种压力方法：静态进展和自适应探测。通过50个虚拟患者概况，对每个模型进行了最多20轮的虚拟精神病对话的压力测试。", "result": "实验结果显示，违规情况普遍存在，两种压力模式产生了相似的违规率。然而，自适应探测显著提前了模型跨越边界的时机，将平均轮次从静态进展的9.21轮减少到4.64轮。在两种机制下，做出明确或零风险的承诺是边界被侵犯的主要方式。", "conclusion": "研究发现，LLMs的安全边界坚固性不能仅通过单轮测试来推断，必须全面考虑不同互动压力和特点引起的长时间对话中的安全边界磨损。"}}
{"id": "2601.14406", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.14406", "abs": "https://arxiv.org/abs/2601.14406", "authors": ["Yixiong Chen", "Zongwei Zhou", "Wenxuan Li", "Alan Yuille"], "title": "Large-Scale Label Quality Assessment for Medical Segmentation via a Vision-Language Judge and Synthetic Data", "comment": "ISBI 2026 accepted", "summary": "Large-scale medical segmentation datasets often combine manual and pseudo-labels of uneven quality, which can compromise training and evaluation. Low-quality labels may hamper performance and make the model training less robust. To address this issue, we propose SegAE (Segmentation Assessment Engine), a lightweight vision-language model (VLM) that automatically predicts label quality across 142 anatomical structures. Trained on over four million image-label pairs with quality scores, SegAE achieves a high correlation coefficient of 0.902 with ground-truth Dice similarity and evaluates a 3D mask in 0.06s. SegAE shows several practical benefits: (I) Our analysis reveals widespread low-quality labeling across public datasets; (II) SegAE improves data efficiency and training performance in active and semi-supervised learning, reducing dataset annotation cost by one-third and quality-checking time by 70% per label. This tool provides a simple and effective solution for quality control in large-scale medical segmentation datasets. The dataset, model weights, and codes are released at https://github.com/Schuture/SegAE.", "AI": {"tldr": "我们提出了SegAE，这是一个轻量级的模型，用于预测医疗图像中的标签质量，它提高了数据效率和训练性能，降低了注释成本和质量检查时间。", "motivation": "大规模的医疗分割数据集往往结合了手动和伪标签，而这些标签的质量不一，可能导致训练和评估出现问题。低质量的标签可能会损害性能，并使模型训练变得更不稳健。为了应对这一问题，提出了SegAE。", "method": "我们提出了SegAE（分割评估引擎），这是一个轻量级的视觉-语言模型，可以自动预测142种解剖结构的标签质量。这个模型在超过400万张带有质量评分的图像-标签对上进行了训练。", "result": "SegAE与真实Dice相似性具有高度相关性（0.902），并且可以以每3D蒙版0.06秒的速度进行评估。它显式了公共数据集中广泛存在的低质量标签问题，提高了数据效率和训练性能，降低了三分之一的注释成本和70%的质量检查时间。", "conclusion": "SegAE提供了一种简单有效的解决方案，用于大规模医疗分割数据集中的质量控制。模型权重和代码已经在GitHub上发布。"}}
{"id": "2601.14270", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14270", "abs": "https://arxiv.org/abs/2601.14270", "authors": ["Liangming Pan", "Jason Liang", "Jiaran Ye", "Minglai Yang", "Xinyuan Lu", "Fengbin Zhu"], "title": "Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models", "comment": "Technical Report", "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.", "AI": {"tldr": "此综述侧重于大语言模型多步推理背后的机制，而非性能提升的工程方法。", "motivation": "现有的调查主要集中在提高性能的工程方法上，而此次调查旨在全面概述多步推理的大语言模型（LLMs）背后的机制。", "method": "组织综述围绕一个概念框架，包含七个相互关联的研究问题，从大语言模型（LLMs）如何在隐藏激活中执行隐式多跳推理到语言化的显式推理如何重塑内部计算。", "result": "此次综述强调了未来机制研究的五个研究方向。", "conclusion": "此次调查强调了理解多步推理的大语言模型（LLMs）内部机制的重要性，并指出了未来研究的潜在方向。"}}
{"id": "2601.14438", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14438", "abs": "https://arxiv.org/abs/2601.14438", "authors": ["Danial Sadrian Zadeh", "Otman A. Basir", "Behzad Moshiri"], "title": "Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation", "comment": "Under review at Computer Vision and Image Understanding (submitted July 25, 2025)", "summary": "Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.", "AI": {"tldr": "本文提出了一种新型框架，其能够将单一前视摄像头图像转换为能够捕捉空间布局、语义关系和驾驶相关线索的简洁自然语言描述，并展示了该模型在新数据集上的强大性能。", "motivation": "本文旨在解决自动驾驶汽车环境感知和解释的关键问题，通过提供一种新的框架将摄像头图像转化为自然语言描述，来帮助汽车更好地理解交通场景。", "method": "框架采用了一种混合注意力机制来提升空间和语义特征的提取，并将这些特征相结合以生成内容丰富详细的场景描述。", "result": "通过CIDEr和SPICE等评估指标和人类判断评估，该模型表现出色，证明了其在新开发的数据集上的有效性和性能。", "conclusion": "本文提出了一种能够有效完成其目标的模型，在新开发的数据集上实现了强大的性能，并提供了全面的评估指标讨论。"}}
{"id": "2601.14280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14280", "abs": "https://arxiv.org/abs/2601.14280", "authors": ["Nicholas X. Wang", "Aggelos K. Katsaggelos"], "title": "Hallucination-Free Automatic Question & Answer Generation for Intuitive Learning", "comment": null, "summary": "Hallucinations in large language models (LLMs), defined as fluent yet incorrect or incoherent outputs, pose a significant challenge to the automatic generation of educational multiple-choice questions (MCQs). We identified four key hallucination types in MCQ generation: reasoning inconsistencies, insolvability, factual errors, and mathematical errors. To address this, we propose a hallucination-free multi-agent generation framework that breaks down MCQ generation into discrete, verifiable stages. Our framework utilizes both rule-based and LLM-based detection agents, as well as hallucination scoring metrics to optimize question quality. We redefined MCQ generation as an optimization task minimizing hallucination risk while maximizing validity, answerability, and cost-efficiency. We also introduce an agent-led refinement process that uses counterfactual reasoning and chain-of-thought (CoT) to iteratively improve hallucination in question generation. We evaluated a sample of AP- aligned STEM questions, where our system reduced hallucination rates by over 90% compared to baseline generation while preserving the educational value and style of questions. Our results demonstrate that structured multi-agent collaboration can mitigate hallucinations in educational content creation at scale, paving the way for more reliable LLM-powered learning tools.", "AI": {"tldr": "提出了一种无幻觉多代理生成框架，用于优化教育多选题的生成，减少幻觉率的同时保持教育价值和风格。", "motivation": "解决大语言模型生成教育多选题时出现幻觉的问题，提高教育内容生成的可靠性。", "method": "将多选题生成分解为多个可验证阶段，利用基于规则和大语言模型的检测代理以及幻觉评分指标进行优化。", "result": "在AP对齐的STEM问题样本中，新系统将幻觉率降低了超过90%，同时保持了教育价值和问题风格。", "conclusion": "结构化的多代理协作可以减少教育内容生成中的幻觉问题，适用于大规模可靠的LLM教育工具的开发。"}}
{"id": "2601.14448", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14448", "abs": "https://arxiv.org/abs/2601.14448", "authors": ["A. Enes Doruk"], "title": "Gaussian Based Adaptive Multi-Modal 3D Semantic Occupancy Prediction", "comment": "Master Thesis", "summary": "The sparse object detection paradigm shift towards dense 3D semantic occupancy prediction is necessary for dealing with long-tail safety challenges for autonomous vehicles. Nonetheless, the current voxelization methods commonly suffer from excessive computation complexity demands, where the fusion process is brittle, static, and breaks down under dynamic environmental settings. To this end, this research work enhances a novel Gaussian-based adaptive camera-LiDAR multimodal 3D occupancy prediction model that seamlessly bridges the semantic strengths of camera modality with the geometric strengths of LiDAR modality through a memory-efficient 3D Gaussian model. The proposed solution has four key components: (1) LiDAR Depth Feature Aggregation (LDFA), where depth-wise deformable sampling is employed for dealing with geometric sparsity, (2) Entropy-Based Feature Smoothing, where cross-entropy is employed for handling domain-specific noise, (3) Adaptive Camera-LiDAR Fusion, where dynamic recalibration of sensor outputs is performed based on model outputs, and (4) Gauss-Mamba Head that uses Selective State Space Models for global context decoding that enjoys linear computation complexity.", "AI": {"tldr": "A novel Gaussian-based camera-LiDAR 3D occupancy prediction model is introduced to improve the sparse object detection method for vehicle safety in dynamic settings by integrating the strengths of both sensor modalities.", "motivation": "The goal is to enhance the sparse object detection model by predicting dense 3D semantic occupancy to address safety issues in autonomous vehicles. Traditional voxelization methods have high computational demands and are not robust in dynamic environments. This paper aims to overcome these limitations using a more efficient and adaptive approach.", "method": "The paper introduces a Gaussian-based adaptive camera-LiDAR multimodal 3D occupancy prediction model. This model integrates the semantic strengths of cameras with the geometric strengths of LiDARs using a memory-efficient 3D Gaussian framework. Key components include LiDAR Depth Feature Aggregation (LDFA) with depth-wise deformable sampling, Entropy-Based Feature Smoothing for noise management, an Adaptive Camera-LiDAR Fusion mechanism for dynamic recalibration, and a Gauss-Mamba Head for global context decoding with linear computational complexity.", "result": "Not explicitly detailed in the abstract, though the method is designed to enhance the density and accuracy of 3D semantic occupancy predictions in real-world, dynamic scenarios compared to existing voxelization techniques.", "conclusion": "The proposed model is expected to offer a more efficient and adaptive solution for 3D semantic occupancy prediction, which is a critical aspect for advancing autonomous vehicle safety."}}
{"id": "2601.14289", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14289", "abs": "https://arxiv.org/abs/2601.14289", "authors": ["Yelin Chen", "Fanjin Zhang", "Suping Sun", "Yunhe Pang", "Yuanchun Wang", "Jian Song", "Xiaoyan Li", "Lei Hou", "Shu Zhao", "Jie Tang", "Juanzi Li"], "title": "RPC-Bench: A Fine-grained Benchmark for Research Paper Comprehension", "comment": "11 pages, 21 appendix pages", "summary": "Understanding research papers remains challenging for foundation models due to specialized scientific discourse and complex figures and tables, yet existing benchmarks offer limited fine-grained evaluation at scale. To address this gap, we introduce RPC-Bench, a large-scale question-answering benchmark built from review-rebuttal exchanges of high-quality computer science papers, containing 15K human-verified QA pairs. We design a fine-grained taxonomy aligned with the scientific research flow to assess models' ability to understand and answer why, what, and how questions in scholarly contexts. We also define an elaborate LLM-human interaction annotation framework to support large-scale labeling and quality control. Following the LLM-as-a-Judge paradigm, we develop a scalable framework that evaluates models on correctness-completeness and conciseness, with high agreement to human judgment. Experiments reveal that even the strongest models (GPT-5) achieve only 68.2% correctness-completeness, dropping to 37.46% after conciseness adjustment, highlighting substantial gaps in precise academic paper understanding. Our code and data are available at https://rpc-bench.github.io/.", "AI": {"tldr": "研究工作创建了一个大型问答基准测试RPC-Bench来解决现有基准无法提供精准学术论文理解评估的问题，并展示了最强语言模型在精确理解学术论文时存在的显著差距。", "motivation": "现有基准测试在大规模细粒度评估论文理解能力方面存在局限性。为了填补这一空白，研究提出了RPC-Bench，以改善模型对专有科学术语、复杂图表和精准学术理解的能力评测。", "method": "通过构建一个大规模的问答基准测试（RPC-Bench）来解决现有基准无法提供细粒度评测的问题。该基准测试基于高质量计算机科学论文的审稿-反驳交流，包含15000个人工验证的问答对。", "result": "最强模型（GPT-5）在正确完整的评价标准下仅能实现68.2%的准确度，调整为简洁度标准后，准确度下降到37.46%，表明现有模型在精确理解和回答学术性论文方面存在明显不足。", "conclusion": "研究引入的RPC-Bench为评估大型语言模型对学术论文的深度理解和回答能力提供了一种新的方法。尽管最强模型表现尚可，但仍有较大提升空间。代码和数据已开放。"}}
{"id": "2601.14475", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14475", "abs": "https://arxiv.org/abs/2601.14475", "authors": ["Yajvan Ravan", "Aref Malek", "Chester Dolph", "Nikhil Behari"], "title": "Real-Time Wildfire Localization on the NASA Autonomous Modular Sensor using Deep Learning", "comment": "16 pages, 9 figures, published at AIAA SciTech 2026", "summary": "High-altitude, multi-spectral, aerial imagery is scarce and expensive to acquire, yet it is necessary for algorithmic advances and application of machine learning models to high-impact problems such as wildfire detection. We introduce a human-annotated dataset from the NASA Autonomous Modular Sensor (AMS) using 12-channel, medium to high altitude (3 - 50 km) aerial wildfire images similar to those used in current US wildfire missions. Our dataset combines spectral data from 12 different channels, including infrared (IR), short-wave IR (SWIR), and thermal. We take imagery from 20 wildfire missions and randomly sample small patches to generate over 4000 images with high variability, including occlusions by smoke/clouds, easily-confused false positives, and nighttime imagery.\n  We demonstrate results from a deep-learning model to automate the human-intensive process of fire perimeter determination. We train two deep neural networks, one for image classification and the other for pixel-level segmentation. The networks are combined into a unique real-time segmentation model to efficiently localize active wildfire on an incoming image feed. Our model achieves 96% classification accuracy, 74% Intersection-over-Union(IoU), and 84% recall surpassing past methods, including models trained on satellite data and classical color-rule algorithms. By leveraging a multi-spectral dataset, our model is able to detect active wildfire at nighttime and behind clouds, while distinguishing between false positives. We find that data from the SWIR, IR, and thermal bands is the most important to distinguish fire perimeters. Our code and dataset can be found here: https://github.com/nasa/Autonomous-Modular-Sensor-Wildfire-Segmentation/tree/main and https://drive.google.com/drive/folders/1-u4vs9rqwkwgdeeeoUhftCxrfe_4QPTn?=usp=drive_link", "AI": {"tldr": "论文引入了一套人类标注的NASA自主模块化传感器数据集，用于训练深度学习模型以自动识别和分割野火边界。模型在夜间和云层后也能有效工作，并优于现有方法。", "motivation": "高海拔、多光谱的航空影像数据稀缺且昂贵，但对推进算法和机器学习模型的应用于像野火检测这样的高影响问题至关重要。", "method": "采用深度学习模型来自动化人工密集型的火灾边界确定过程。训练了两个深度神经网络，一个用于图像分类，另一个用于像素级分割。将这两个网络结合成一个独特的实时分割模型，以高效定位传入图像流中的活跃野火。", "result": "模型实现了96%的分类准确率，74%的交并比(IoU)，以及84%的召回率，超过了以往的方法，包括基于卫星数据的模型和经典的色彩规则算法。", "conclusion": "通过利用多光谱数据集，该模型可以检测夜间和云层后的活跃野火，并区分误报。数据发现短波红外（SWIR）、红外（IR）和热红外（thermal）波段的数据对于区分火灾边界最重要。"}}
{"id": "2601.14290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14290", "abs": "https://arxiv.org/abs/2601.14290", "authors": ["Aradhya Dixit", "Tianxi Liang", "Jai Telang"], "title": "Project Aletheia: Verifier-Guided Distillation of Backtracking for Small Language Models", "comment": null, "summary": "Small Language Models (SLMs, under 10B parameters) are attractive for private, on-device deployment, yet they frequently fail on strict constraint-satisfaction problems due to linear, overconfident reasoning traces that do not recover from early mistakes. We introduce Verifier-Guided Distillation, a training protocol that transfers the process of error repair - explicit conflict detection and backtracking - rather than only correct final answers. By training a 7B model on verified reasoning traces that include mistakes and self-corrections, we show that latent verification behavior can emerge in small models, enabling them to occasionally stop, detect contradictions, and revise earlier assumptions.", "AI": {"tldr": "The paper presents Verifier-Guided Distillation to improve the reasoning ability of Small Language Models by teaching them to detect and correct errors.", "motivation": "To enable small language models (SLMs) to recover from early mistakes and improve their performance on strict constraint-satisfaction problems.", "method": "Verifier-Guided Distillation, a training protocol transferring the process of error repair, including explicit conflict detection and backtracking.", "result": "Demonstrates that latent verification behavior can emerge in small models, allowing them to detect contradictions and revise earlier assumptions.", "conclusion": "Verifier-Guided Distillation can enhance the reasoning ability of SLMs."}}
