{"id": "2509.10466", "categories": ["cs.CV", "cs.HC", "H.5.1; H.5.2; I.4.6; I.4.4; I.2.10; I.4.9; K.4.1"], "pdf": "https://arxiv.org/pdf/2509.10466", "abs": "https://arxiv.org/abs/2509.10466", "authors": ["Christian Fane"], "title": "A Real-Time Diminished Reality Approach to Privacy in MR Collaboration", "comment": "50 pages, 12 figures | Demo video: https://youtu.be/udBxj35GEKI?t=499\n  | Code: https://github.com/c1h1r1i1s1 (multiple repositories)", "summary": "Diminished reality (DR) refers to the digital removal of real-world objects\nby compositing background content in their place. This thesis presents a\nreal-time, inpainting-based DR system designed to enable privacy control in\nshared-space mixed reality (MR) meetings. The system allows a primary headset\nuser to selectively remove personal or sensitive items from their environment,\nensuring that those objects are no longer visible to other participants.\nRemoval is achieved through semantic segmentation and precise object selection,\nfollowed by real-time inpainting from the viewpoint of a secondary observer,\nimplemented using a mobile ZED 2i depth camera. The solution is designed to be\nportable and robust, requiring neither a fixed secondary viewpoint nor prior 3D\nscanning of the environment. The system utilises YOLOv11 for object detection\nand a modified Decoupled Spatial-Temporal Transformer (DSTT) model for\nhigh-quality video inpainting. At 720p resolution, the pipeline sustains frame\nrates exceeding 20 fps, demonstrating the feasibility of real-time diminished\nreality for practical privacy-preserving MR applications.", "AI": {"tldr": "本研究设计了一种实时虚化现实系统，通过语义分割、精确对象选择以及高度优化的视频修复算法，以支持共享空间混合现实会议中的隐私保护。系统无需固定观察视角或预先进行3D扫描，实现了实时处理并展示了其在隐私保护领域的应用潜力。", "motivation": "论文动机是提供一种隐私控制的方法，尤其是在共享空间的混合现实会议中，允许主要头戴设备用户选择性地移除自己环境中的个人或敏感物品，从而确保这些物品不再对其他参与者可见。", "method": "本论文提出了一种基于实时图像修复的虚化现实系统，该系统利用语义分割和精确对象选择来移除头戴设备用户环境中的个人或敏感物品，并使用移动ZED 2i深度摄像头从第二观察者的视角进行实时图像修复。该系统利用YOLOv11进行对象检测，并采用改进的Decoupled Spatial-Temporal Transformer (DSTT)模型进行高质量的视频修复。", "result": "在720p分辨率下，该系统的管道维持每秒20帧以上的帧率，证明了实时虚化现实技术在实际隐私保护的混合现实应用中的可行性。", "conclusion": "研究结论表明，所提出的系统是便携且稳健的，不需要固定的第二视角或环境的先验3D扫描，可以在实时混合现实会议中实现隐私控制。"}}
{"id": "2509.10555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10555", "abs": "https://arxiv.org/abs/2509.10555", "authors": ["Alejandra Perez", "Chinedu Nwoye", "Ramtin Raji Kermani", "Omid Mohareri", "Muhammad Abdullah Jamal"], "title": "SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning", "comment": null, "summary": "Vision-language pre-training (VLP) offers unique advantages for surgery by\naligning language with surgical videos, enabling workflow understanding and\ntransfer across tasks without relying on expert-labeled datasets. However,\nprogress in surgical VLP remains constrained by the limited scale, procedural\ndiversity, semantic quality, and hierarchical structure of existing datasets.\nIn this work, we present SurgLaVi, the largest and most diverse surgical\nvision-language dataset to date, comprising nearly 240k clip-caption pairs from\nmore than 200 procedures, and comprising hierarchical levels at phase-, step-,\nand task-level. At the core of SurgLaVi lies a fully automated pipeline that\nsystematically generates fine-grained transcriptions of surgical videos and\nsegments them into coherent procedural units. To ensure high-quality\nannotations, it applies dual-modality filtering to remove irrelevant and noisy\nsamples. Within this framework, the resulting captions are enriched with\ncontextual detail, producing annotations that are both semantically rich and\neasy to interpret. To ensure accessibility, we release SurgLaVi-\\b{eta}, an\nopen-source derivative of 113k clip-caption pairs constructed entirely from\npublic data, which is over four times larger than existing surgical VLP\ndatasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,\na CLIP-style video-text contrastive framework with dual encoders, as a\nrepresentative base model. SurgCLIP achieves consistent improvements across\nphase, step, action, and tool recognition, surpassing prior state-of-the-art\nmethods, often by large margins. These results validate that large-scale,\nsemantically rich, and hierarchically structured datasets directly translate\ninto stronger and more generalizable representations, establishing SurgLaVi as\na key resource for developing surgical foundation models.", "AI": {"tldr": "SurgLaVi 是一个大规模、多样化的手术视觉语言数据集，包含近24万个片段-标题对，涵盖了超过200种手术过程，含有层次结构，包括阶段、步骤和任务级别。此外，还引入了 SurgCLIP，一个基于CLIP样式的视频文本对比框架，该模型在多个识别任务上表现优于现有方法。", "motivation": "现有的手术视觉语言预训练数据集在规模、多样性、语义质量和层次结构方面存在局限，因此需要一个新的大规模、多样的数据集 SurgLaVi 来克服这些限制，并开发手术基础模型。", "method": "Structure", "result": "{\n  \"tldr\": \"SurgLaVi 是一个大规模、多样化的手术视觉语言数据集，包含近24万个片段-标题对，涵盖了超过200种手术过程，含有层次结构，包括阶段、步骤和任务级别。此外，还引入了 SurgCLIP，一个基于CLIP样式的视频文本对比框架，该模型在多个识别任务上表现优于现有方法。\", \n  \"motivation\": \"现有的手术视觉语言预训练数据集在规模、多样性、语义质量和层次结构方面存在局限，因此需要一个新的大规模、多样的数据集 SurgLaVi 来克服这些限制，并开发手术基础模型。\", \n  \"method\": \"SurgLaVi 采用全自动管道生成外科视频的详细转录，并将它们分割成连贯的过程单元，同时使用双模态过滤来筛选和去除无关和噪音样本。\", \n  \"result\": \"SurgCLIP 在阶段、步骤、动作和工具识别任务上取得了显著的性能提升，超过了现有的最先进方法。\", \n  \"conclusion\": \"大规模、语义丰富、层次结构分明的数据集直接增强了模型的表现力和通用性，SurgLaVi 数据集成为开发手术基础模型的关键资源。\" \n}", "conclusion": "大规模、语义丰富、层次结构分明的数据集直接增强了模型的表现力和通用性，SurgLaVi 数据集成为开发手术基础模型的关键资源。"}}
{"id": "2509.10620", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10620", "abs": "https://arxiv.org/abs/2509.10620", "authors": ["Emily Kaczmarek", "Justin Szeto", "Brennan Nichyporuk", "Tal Arbel"], "title": "Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses", "comment": "Accepted to ICCV 2025 Workshop CVAMD", "summary": "3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly\nacquired in clinical settings to monitor a wide range of neurological\nconditions, including neurodegenerative disorders and stroke. While deep\nlearning models have shown promising results analyzing 3D MRI across a number\nof brain imaging tasks, most are highly tailored for specific tasks with\nlimited labeled data, and are not able to generalize across tasks and/or\npopulations. The development of self-supervised learning (SSL) has enabled the\ncreation of large medical foundation models that leverage diverse, unlabeled\ndatasets ranging from healthy to diseased data, showing significant success in\n2D medical imaging applications. However, even the very few foundation models\nfor 3D brain MRI that have been developed remain limited in resolution, scope,\nor accessibility. In this work, we present a general, high-resolution\nSimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on\n18,759 patients (44,958 scans) from 11 publicly available datasets spanning\ndiverse neurological diseases. We compare our model to Masked Autoencoders\n(MAE), as well as two supervised baselines, on four diverse downstream\nprediction tasks in both in-distribution and out-of-distribution settings. Our\nfine-tuned SimCLR model outperforms all other models across all tasks. Notably,\nour model still achieves superior performance when fine-tuned using only 20% of\nlabeled training samples for predicting Alzheimer's disease. We use publicly\navailable code and data, and release our trained model at\nhttps://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly\napplicable and accessible foundation model for clinical brain MRI analysis.", "AI": {"tldr": "A general, high-resolution SSL model for 3D brain MRI scans, trained using SimCLR on a large dataset, outperforms other models in diverse prediction tasks with limited labeled data.", "motivation": "To develop a generalizable, high-resolution foundation model for analyzing 3D brain MRI scans across different populations and tasks, overcoming limitations of specialized models with limited labeled data.", "method": "SimCLR-based self-supervised learning model for 3D brain MRI, pretrained on a large dataset of 18,759 patients and 44,958 scans.", "result": "The fine-tuned SimCLR model outperforms Masked Autoencoders (MAE) and supervised baselines in four downstream prediction tasks, including using only 20% labeled training samples for Alzheimer's disease prediction.", "conclusion": "The SimCLR-based model is a broadly applicable and accessible foundation for clinical analysis of 3D brain MRI, demonstrating superior performance with lower amounts of labeled data compared to existing methods."}}
{"id": "2509.10651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10651", "abs": "https://arxiv.org/abs/2509.10651", "authors": ["Xiaoyang Ma", "Yiyang Chai", "Xinran Qu", "Hong Sun"], "title": "USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction", "comment": null, "summary": "Reconstructing hyperspectral images (HSIs) from a single RGB image is\nill-posed and can become physically inconsistent when the camera spectral\nsensitivity (CSS) and scene illumination are misspecified. We formulate\nRGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by\na nuclear norm in a learnable transform domain, and we explicitly estimate CSS\nand illumination to define the forward operator embedded in each iteration,\nensuring colorimetric consistency. To avoid the cost and instability of full\nsingular-value decompositions (SVDs) required by singular-value thresholding\n(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on\nthese components, we develop USCTNet, a deep unfolding solver tailored to HSI\nthat couples a parameter estimation module with learnable proximal updates.\nExtensive experiments on standard benchmarks show consistent improvements over\nstate-of-the-art RGB-based methods in reconstruction accuracy. Code:\nhttps://github.com/psykheXX/USCTNet-Code-Implementation.git", "AI": {"tldr": "该论文提出了一种名为USCTNet的新方法，将RGB图像转换为高光谱图像（HSI）并在标准基准测试中展示了比现有方法更好的重建精度。", "motivation": "该研究的动机在于，将单个RGB图像重建为高光谱图像（HSIs）是一个不适定问题，如相机光谱灵敏度（CSS）和场景照明不准确时则会变得物理上不一致。", "method": "该论文提出了一种名为USCTNet的方法，该方法将RGB图像转换为高光谱图像（HSI）视为一个基于物理的逆问题，通过核范数正则化在一个可学习的变换域中解决。该方法显式估计了相机光谱灵敏度（CSS）和照明，以避开每个迭代中全奇异值分解（SVD）的高成本和不稳定性，引入了一种数据自适应的低秩子空间奇异性阈值（SVT）操作符。USCTNet结合了参数估计模块与可学习的邻近更新，形成一个针对HSI的深层展开求解器。", "result": "论文通过广泛实验展示了USCTNet在标准基准数据集上的一致性改进，相较于最先进的RGB方法，恢复了更好的HSI。", "conclusion": "实验结果表明，相较于先进的RGB方法，USCTNet在高光谱图像重建的精度上显示出一致的改进。"}}
{"id": "2509.10546", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10546", "abs": "https://arxiv.org/abs/2509.10546", "authors": ["Gang Cheng", "Haibo Jin", "Wenbin Zhang", "Haohan Wang", "Jun Zhuang"], "title": "Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment", "comment": "Preprint, under review. TL;DR: We propose a multi-turn red-teaming\n  framework, RCA, that reveals critical regulatory vulnerabilities in financial\n  LLMs, achieving over 93% attack success on a proposed new benchmark,\n  FIN-Bench", "summary": "Large Language Models (LLMs) are increasingly integrated into financial\napplications, yet existing red-teaming research primarily targets harmful\ncontent, largely neglecting regulatory risks. In this work, we aim to\ninvestigate the vulnerability of financial LLMs through red-teaming approaches.\nWe introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that\niteratively conceals regulatory risks to provoke seemingly compliant yet\nregulatory-violating responses from LLMs. To enable systematic evaluation, we\nconstruct FIN-Bench, a domain-specific benchmark for assessing LLM safety in\nfinancial contexts. Extensive experiments on FIN-Bench demonstrate that RCA\neffectively bypasses nine mainstream LLMs, achieving an average attack success\nrate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.\nThese findings reveal a critical gap in current alignment techniques and\nunderscore the urgent need for stronger moderation mechanisms in financial\ndomains. We hope this work offers practical insights for advancing robust and\ndomain-aware LLM alignment.", "AI": {"tldr": "引入风险隐蔽攻击（RCA）来评估金融LLMs的监管风险，FIN-Bench基准测试显示了对多个主流LLM的高度成功率，强调了金融领域中更强的审查机制需求。", "motivation": "大型语言模型（LLMs）越来越多地被集成到金融应用中，而现有的对抗性测试研究主要集中在有害内容上，大多数忽略了监管风险。我们旨在通过对抗性测试方法来研究金融LLMs的脆弱性。", "method": "引入了风险隐蔽攻击（RCA），这是一种多轮框架，旨在逐步隐藏监管风险，诱使LLM产生看似合规但实际上违规的响应。为了实现系统性评估，构建了FIN-Bench，这是一个特定于金融领域的基准，用于评估LLM在金融环境中的安全性。", "result": "在FIN-Bench上的广泛实验表明，RCA有效地绕过了包括GPT-4.1和OpenAI o1在内的九个主流LLMs，平均攻击成功率（ASR）为93.18%，GPT-4.1的成功率为98.28%，OpenAI o1的成功率为97.56%。", "conclusion": "这些发现揭示了当前对齐技术中的关键差距，并强调了在金融领域中迫切需要更强的审查机制。我们希望这项工作能够提供实用的见解，以提高LLM的安全性和领域适应性。"}}
{"id": "2509.10683", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10683", "abs": "https://arxiv.org/abs/2509.10683", "authors": ["Felicia Liu", "Jay J. Yoo", "Farzad Khalvati"], "title": "A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI", "comment": null, "summary": "Large Language Models (LLMs) have shown strong performance in text-based\nhealthcare tasks. However, their utility in image-based applications remains\nunexplored. We investigate the effectiveness of LLMs for medical imaging tasks,\nspecifically glioma classification and segmentation, and compare their\nperformance to that of traditional convolutional neural networks (CNNs). Using\nthe BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a\ngeneral-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after\nfine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma\nclassification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and\nbalanced precision and recall. The general LLM reached 76% accuracy but\nsuffered from a specificity of only 18%, often misclassifying Low-Grade tumors.\nFine-tuning improved specificity to 55%, but overall performance declined\n(e.g., accuracy dropped to 72%). For segmentation, three methods - center\npoint, bounding box, and polygon extraction, were implemented. CNNs accurately\nlocalized gliomas, though small tumors were sometimes missed. In contrast, LLMs\nconsistently clustered predictions near the image center, with no distinction\nof glioma size, location, or placement. Fine-tuning improved output formatting\nbut failed to meaningfully enhance spatial accuracy. The bounding polygon\nmethod yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in\nboth tasks. LLMs showed limited spatial understanding and minimal improvement\nfrom fine-tuning, indicating that, in their current form, they are not\nwell-suited for image-based tasks. More rigorous fine-tuning or alternative\ntraining strategies may be needed for LLMs to achieve better performance,\nrobustness, and utility in the medical space.", "AI": {"tldr": "研究发现，尽管大型语言模型（LLMs）在文本处理的医疗任务中表现出色，但在医学影像任务（如胶质瘤的分类和分割）中，其性能不如传统卷积神经网络（CNNs），特别是在空间理解上存在局限性。", "motivation": "探讨大型语言模型在医学影像任务中的潜力，尤其是在胶质瘤分类和分割上的表现，并将其与传统的CNNs进行对比。", "method": "使用BraTS 2020多模态脑MRI数据集，评估未微调和微调过的LLaMA 3.2 Instruct LLM，并使用3D CNNs作为基准模型。", "result": "在胶质瘤分类中，CNN达到了80%的准确率；LLM达到了76%准确率，但特异性仅为18%。微调后LLM的特异性提高到55%，但整体性能下降。对于分割任务，CNN们能够较好地定位肿瘤；而LLMs的输出靠近图像中心，无显著提升。", "conclusion": "研究结果显示，CNN们在分类和分割任务上优于LLMs，后者在空间理解上存在局限且微调效果不佳，表明LLMs需要更严格地微调或替代训练策略才能在医疗领域获得更好的性能。"}}
{"id": "2509.10625", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10625", "abs": "https://arxiv.org/abs/2509.10625", "authors": ["Iván Vicente Moreno Cencerrado", "Arnau Padrés Masdemont", "Anton Gonzalvez Hawthorne", "David Demitri Africa", "Lorenzo Pacchiardi"], "title": "No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes", "comment": null, "summary": "Do large language models (LLMs) anticipate when they will answer correctly?\nTo study this, we extract activations after a question is read but before any\ntokens are generated, and train linear probes to predict whether the model's\nforthcoming answer will be correct. Across three open-source model families\nranging from 7 to 70 billion parameters, projections on this \"in-advance\ncorrectness direction\" trained on generic trivia questions predict success in\ndistribution and on diverse out-of-distribution knowledge datasets,\noutperforming black-box baselines and verbalised predicted confidence.\nPredictive power saturates in intermediate layers, suggesting that\nself-assessment emerges mid-computation. Notably, generalisation falters on\nquestions requiring mathematical reasoning. Moreover, for models responding \"I\ndon't know\", doing so strongly correlates with the probe score, indicating that\nthe same direction also captures confidence. By complementing previous results\non truthfulness and other behaviours obtained with probes and sparse\nauto-encoders, our work contributes essential findings to elucidate LLM\ninternals.", "AI": {"tldr": "研究发现，大语言模型能在读取问题后预测其答案是否正确，且这种自我评估能力在中间层形成，但对需要数学推理的问题预测能力有限。", "motivation": "这项研究的动机是探讨大语言模型在生成答案之前是否能够预知其正确性，从而增进对大规模语言模型内部运作的理解。", "method": "通过在模型读取问题但尚未生成任何令牌之前提取激活模式，并训练线性探测器来预测模型的答案是否正确，从而研究大语言模型是否能够预知其答案的正确性。", "result": "实验结果表明，在不同大小的三种开源模型家族上训练的探测器能够在分布内和分布外的各种知识数据集上成功预测模型的答案正确性，优于黑盒基线和口头表达的预测置信度。", "conclusion": "研究结果显示，自我评估在中间层计算中形成，针对需要数学推理的问题，预测能力失败。对于回答“I don’t know”的模型，其与探测器分数密切相关，表明同一方向也捕捉了置信度，该工作通过补充先前关于真实性及其他行为的研究结果，为阐明大语言模型的内部运作提供了重要的发现。"}}
{"id": "2509.10687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10687", "abs": "https://arxiv.org/abs/2509.10687", "authors": ["Hao Zhang", "Chun-Han Yao", "Simon Donné", "Narendra Ahuja", "Varun Jampani"], "title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation", "comment": "Page: https://stablepartdiffusion4d.github.io/", "summary": "We present Stable Part Diffusion 4D (SP4D), a framework for generating paired\nRGB and kinematic part videos from monocular inputs. Unlike conventional part\nsegmentation methods that rely on appearance-based semantic cues, SP4D learns\nto produce kinematic parts - structural components aligned with object\narticulation and consistent across views and time. SP4D adopts a dual-branch\ndiffusion model that jointly synthesizes RGB frames and corresponding part\nsegmentation maps. To simplify the architecture and flexibly enable different\npart counts, we introduce a spatial color encoding scheme that maps part masks\nto continuous RGB-like images. This encoding allows the segmentation branch to\nshare the latent VAE from the RGB branch, while enabling part segmentation to\nbe recovered via straightforward post-processing. A Bidirectional Diffusion\nFusion (BiDiFuse) module enhances cross-branch consistency, supported by a\ncontrastive part consistency loss to promote spatial and temporal alignment of\npart predictions. We demonstrate that the generated 2D part maps can be lifted\nto 3D to derive skeletal structures and harmonic skinning weights with few\nmanual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,\na curated dataset of over 20K rigged objects selected and processed from\nObjaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part\nvideo sequences. Experiments show that SP4D generalizes strongly to diverse\nscenarios, including real-world videos, novel generated objects, and rare\narticulated poses, producing kinematic-aware outputs suitable for downstream\nanimation and motion-related tasks.", "AI": {"tldr": "SP4D是一种框架，用于从单目输入生成配对的RGB和运动部件视频，采用双分支扩散模型，增强了部件预测的一致性，适用于动画和运动相关任务。", "motivation": "传统的部件分割方法依赖于基于外观的语义线索，而SP4D旨在学习生成与物体运动部件对齐且视图间一致的部件。", "method": "SP4D 采用双分支扩散模型，同步合成RGB帧及其对应的部件分割图。通过引入空间颜色编码方案，SP4D简化了架构，并允许不同的部件数量。BiDiFuse模块增强了跨分支一致性，同时使用对比部件一致性损失以促进部件预测的空间和时间对齐。", "result": "实验展示了SP4D在广泛场景中的强大泛化能力，包括真实世界的视频、新生成的对象及罕见的运动姿态。", "conclusion": "SP4D生成的2D部件图可以提升到3D以获得骨骼结构和和谐的皮肤权重，需要轻微的手动调整即可。"}}
{"id": "2509.10644", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10644", "abs": "https://arxiv.org/abs/2509.10644", "authors": ["Enora Rice", "Katharina von der Wense", "Alexis Palmer"], "title": "Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation", "comment": "Accepted to EMNLP 2025", "summary": "Computational morphology has the potential to support language documentation\nthrough tasks like morphological segmentation and the generation of Interlinear\nGlossed Text (IGT). However, our research outputs have seen limited use in\nreal-world language documentation settings. This position paper situates the\ndisconnect between computational morphology and language documentation within a\nbroader misalignment between research and practice in NLP and argues that the\nfield risks becoming decontextualized and ineffectual without systematic\nintegration of User-Centered Design (UCD). To demonstrate how principles from\nUCD can reshape the research agenda, we present a case study of GlossLM, a\nstate-of-the-art multilingual IGT generation model. Through a small-scale user\nstudy with three documentary linguists, we find that despite strong metric\nbased performance, the system fails to meet core usability needs in real\ndocumentation contexts. These insights raise new research questions around\nmodel constraints, label standardization, segmentation, and personalization. We\nargue that centering users not only produces more effective tools, but surfaces\nricher, more relevant research directions", "AI": {"tldr": "本文提出计算形态学的研究成果在实际语言记录领域应用有限，强调了以用户为中心的设计对于开发更有效工具的重要性。", "motivation": "本文旨在解决计算形态学和语言记录之间的脱节问题，提出如果不系统地整合用户中心设计，该领域可能会变得脱离背景并且无效。", "method": "本文通过一个案例研究来展示用户中心设计（UCD）原则如何重塑研究议程，案例研究的对象是GlossLM模型，这是一个最先进的多语言IGT生成模型。", "result": "通过一项小型用户研究，研究发现尽管模型在指标上表现良好，但它并未满足实际语言记录环境中的核心可用性需求。", "conclusion": "本文认为，以用户为中心不仅可以产生更有效的工具，还能揭示更丰富、更有针对性的研究方向。"}}
{"id": "2509.10710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10710", "abs": "https://arxiv.org/abs/2509.10710", "authors": ["Sven Schreiber", "Noha Sarhan", "Simone Frintrop", "Christian Wilms"], "title": "SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition", "comment": "Accepted at GCPR 2025", "summary": "Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB\ndata or signer pose information. However, combining these modalities often\nresults in the loss of crucial details, such as hand shape and orientation, due\nto imprecise representations like bounding boxes. Therefore, we propose the\nISLR system SegSLR, which combines RGB and pose information through promptable\nzero-shot video segmentation. Given the rough localization of the hands and the\nsigner's body from pose information, we segment the respective parts through\nthe video to maintain all relevant shape information. Subsequently, the\nsegmentations focus the processing of the RGB data on the most relevant body\nparts for ISLR. This effectively combines RGB and pose information. Our\nevaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR\noutperforms state-of-the-art methods. Furthermore, ablation studies indicate\nthat SegSLR strongly benefits from focusing on the signer's body and hands,\njustifying our design choices.", "AI": {"tldr": "提出了一种名为SegSLR的新ISLR系统，结合了RGB数据和姿态信息，通过具体分割保持关键细节，实验显示其超越现有方法。", "motivation": "当前的孤立手语识别(ISLR)方法主要依赖于RGB数据或签名人姿态信息，但结合这些模态往往导致关键细节如手势形状和方向的丢失。", "method": "通过可提示的零样本视频分割结合RGB和姿态信息，以保持所有相关形状信息，并专注于处理与ISLR最相关的身体部位，从而有效结合RGB和姿态信息。", "result": "在复杂的ChaLearn249 IsoGD数据集上的评估表明，SegSLR优于最先进的方法。进一步的消融研究表明，SegSLR从专注于签名人身体和手部中受益，这证明了我们的设计选择。", "conclusion": "SegSLR系统通过视频分割结合了RGB和姿态信息，显著提升了ISLR的性能，凸显了其在复杂背景下的优越性。"}}
{"id": "2509.10663", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10663", "abs": "https://arxiv.org/abs/2509.10663", "authors": ["Zineddine Tighidet", "Andrea Mogini", "Hedi Ben-younes", "Jiali Mei", "Patrick Gallinari", "Benjamin Piwowarski"], "title": "Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts", "comment": "Accepted at EMNLP 2025", "summary": "The behavior of Large Language Models (LLMs) when facing contextual\ninformation that conflicts with their internal parametric knowledge is\ninconsistent, with no generally accepted explanation for the expected outcome\ndistribution. Recent work has identified in autoregressive transformer models a\nclass of neurons -- called entropy neurons -- that produce a significant effect\non the model output entropy while having an overall moderate impact on the\nranking of the predicted tokens. In this paper, we investigate the preliminary\nclaim that these neurons are involved in inhibiting context copying behavior in\ntransformers by looking at their role in resolving conflicts between contextual\nand parametric information. We show that entropy neurons are responsible for\nsuppressing context copying across a range of LLMs, and that ablating them\nleads to a significant change in the generation process. These results enhance\nour understanding of the internal dynamics of LLMs when handling conflicting\ninformation.", "AI": {"tldr": "本文研究了在解决上下文信息与参数化信息冲突中，熵神经元在抑制变压器中上下文复制行为的作用。结果表明，熵神经元对抑制上下文复制行为是负责的，并且消除它们会导致生成过程发生显著变化。", "motivation": "大型语言模型在面临与内部参数化知识冲突的上下文信息时的行为是不一致的，而且没有普遍接受的解释来说明预期结果的分布。因此，本文的研究动机在于理解熵神经元在解决冲突信息中的作用。", "method": "本文通过观察这些神经元在解决上下文信息和参数化信息冲突中的作用，来初步验证这些神经元参与抑制变压器中的上下文复制行为的假设。", "result": "研究表明，熵神经元负责抑制各种大型语言模型中的上下文复制行为，并且消除它们会导致生成过程发生显著变化。", "conclusion": "这些结果加深了我们对大型语言模型在处理冲突信息时内部动态的理解。"}}
{"id": "2509.10748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10748", "abs": "https://arxiv.org/abs/2509.10748", "authors": ["Jecia Z. Y. Mao", "Francis X Creighton", "Russell H Taylor", "Manish Sahu"], "title": "SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation", "comment": null, "summary": "Accurate segmentation and tracking of relevant elements of the surgical scene\nis crucial to enable context-aware intraoperative assistance and decision\nmaking. Current solutions remain tethered to domain-specific, supervised models\nthat rely on labeled data and required domain-specific data to adapt to new\nsurgical scenarios and beyond predefined label categories. Recent advances in\nprompt-driven vision foundation models (VFM) have enabled open-set, zero-shot\nsegmentation across heterogeneous medical images. However, dependence of these\nmodels on manual visual or textual cues restricts their deployment in\nintroperative surgical settings. We introduce a speech-guided collaborative\nperception (SCOPE) framework that integrates reasoning capabilities of large\nlanguage model (LLM) with perception capabilities of open-set VFMs to support\non-the-fly segmentation, labeling and tracking of surgical instruments and\nanatomy in intraoperative video streams. A key component of this framework is a\ncollaborative perception agent, which generates top candidates of VFM-generated\nsegmentation and incorporates intuitive speech feedback from clinicians to\nguide the segmentation of surgical instruments in a natural human-machine\ncollaboration paradigm. Afterwards, instruments themselves serve as interactive\npointers to label additional elements of the surgical scene. We evaluated our\nproposed framework on a subset of publicly available Cataract1k dataset and an\nin-house ex-vivo skull-base dataset to demonstrate its potential to generate\non-the-fly segmentation and tracking of surgical scene. Furthermore, we\ndemonstrate its dynamic capabilities through a live mock ex-vivo experiment.\nThis human-AI collaboration paradigm showcase the potential of developing\nadaptable, hands-free, surgeon-centric tools for dynamic operating-room\nenvironments.", "AI": {"tldr": "开发了一种结合语言模型与视觉基础模型的框架，以实现手术场景中的实时分割、标注和追踪。", "motivation": "手术场景中的准确元素分割和追踪对于提供语境感知的术中辅助和决策至关重要，但当前解决方案依赖特定领域的标签数据，限制新场景的应用。SCOPE框架旨在解决这一问题。", "method": "提出了一种语音引导的协作感知框架（SCOPE），将大型语言模型（LLM）的推理能力与开集视觉基础模型（VFM）的感知能力结合，支持手术视频流中手术器械和解剖结构的实时分割、标注和追踪。", "result": "在公开的Cataract1k数据集和内部的离体颅底数据集上进行了框架评估，并通过建模离体实验展示了其动态能力。", "conclusion": "这个人类-人工智能协作范式展示了开发适应性强、免手持和医生中心的手术环境工具的潜力。"}}
{"id": "2509.10685", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10685", "abs": "https://arxiv.org/abs/2509.10685", "authors": ["Jiayou Zhong", "Anudeex Shetty", "Chao Jia", "Xuanrui Lin", "Usman Naseem"], "title": "Pluralistic Alignment for Healthcare: A Role-Driven Framework", "comment": "Accepted to EMNLP 2025 (Main Proceedings)", "summary": "As large language models are increasingly deployed in sensitive domains such\nas healthcare, ensuring their outputs reflect the diverse values and\nperspectives held across populations is critical. However, existing alignment\napproaches, including pluralistic paradigms like Modular Pluralism, often fall\nshort in the health domain, where personal, cultural, and situational factors\nshape pluralism. Motivated by the aforementioned healthcare challenges, we\npropose a first lightweight, generalizable, pluralistic alignment approach,\nEthosAgents, designed to simulate diverse perspectives and values. We\nempirically show that it advances the pluralistic alignment for all three modes\nacross seven varying-sized open and closed models. Our findings reveal that\nhealth-related pluralism demands adaptable and normatively aware approaches,\noffering insights into how these models can better respect diversity in other\nhigh-stakes domains.", "AI": {"tldr": "本文针对医疗领域中的语言模型多元化问题提出了EthosAgents方法，以适应多样化的价值观和视角，并证明了其有效性。", "motivation": "动机来源于现有对齐方法在医疗领域中的不足，特别是在考虑个人、文化和情境因素时。", "method": "提出了一种名为EthosAgents的轻量级、通用的、多元化的对齐方法，旨在模拟多样的观点和价值观。", "result": "实验证明，该方法在七种不同规模的开放和封闭模型上提升了全体三种模式的多元化对齐。", "conclusion": "研究发现，医疗卫生相关多元化需要适应性强且具有规范意识的方法，并为如何在其他高权重领域更好地尊重多样性提供了见解。"}}
{"id": "2509.10759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10759", "abs": "https://arxiv.org/abs/2509.10759", "authors": ["Yi-Ruei Liu", "You-Zhe Xie", "Yu-Hsiang Hsu", "I-Sheng Fang", "Yu-Lun Liu", "Jun-Cheng Chen"], "title": "Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation", "comment": null, "summary": "Common computer vision systems typically assume ideal pinhole cameras but\nfail when facing real-world camera effects such as fisheye distortion and\nrolling shutter, mainly due to the lack of learning from training data with\ncamera effects. Existing data generation approaches suffer from either high\ncosts, sim-to-real gaps or fail to accurately model camera effects. To address\nthis bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage\npipeline that combines 4D Gaussian Splatting with physically-based ray tracing\nfor camera effect simulation. Given multi-view videos, 4D-GRT first\nreconstructs dynamic scenes, then applies ray tracing to generate videos with\ncontrollable, physically accurate camera effects. 4D-GRT achieves the fastest\nrendering speed while performing better or comparable rendering quality\ncompared to existing baselines. Additionally, we construct eight synthetic\ndynamic scenes in indoor environments across four camera effects as a benchmark\nto evaluate generated videos with camera effects.", "AI": {"tldr": "4D-GRT is a novel pipeline for simulating real-world camera effects in computer vision systems, offering fast and high-quality rendering compared to existing methods.", "motivation": "To address the limitations of current computer vision systems in handling real-world camera effects such as fisheye distortion and rolling shutter, due to the lack of learning from training data with these camera effects. Existing approaches face challenges in cost, sim-to-real gaps, or accuracy in modeling these effects.", "method": "4D Gaussian Ray Tracing (4D-GRT), a two-stage pipeline that first reconstructs dynamic scenes from multi-view videos and then applies ray tracing to generate videos with controlled, physically accurate camera effects.", "result": "4D-GRT achieves the fastest rendering speed while maintaining better or comparable rendering quality compared to existing methods. A benchmark with eight synthetic dynamic scenes across four camera effects is used to evaluate generated videos.", "conclusion": "The proposed 4D-GRT method efficiently simulates real-world camera effects and provides a benchmark for evaluating generated videos with these effects."}}
{"id": "2509.10696", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10696", "abs": "https://arxiv.org/abs/2509.10696", "authors": ["Shuaiqi Wang", "Vikas Raunak", "Arturs Backurs", "Victor Reis", "Pei Zhou", "Sihao Chen", "Longqi Yang", "Zinan Lin", "Sergey Yekhanin", "Giulia Fanti"], "title": "Struct-Bench: A Benchmark for Differentially Private Structured Text Generation", "comment": null, "summary": "Differentially private (DP) synthetic data generation is a promising\ntechnique for utilizing private datasets that otherwise cannot be exposed for\nmodel training or other analytics. While much research literature has focused\non generating private unstructured text and image data, in enterprise settings,\nstructured data (e.g., tabular) is more common, often including natural\nlanguage fields or components. Existing synthetic data evaluation techniques\n(e.g., FID) struggle to capture the structural properties and correlations of\nsuch datasets. In this work, we propose Struct-Bench, a framework and benchmark\nfor evaluating synthetic datasets derived from structured datasets that contain\nnatural language data. The Struct-Bench framework requires users to provide a\nrepresentation of their dataset structure as a Context-Free Grammar (CFG). Our\nbenchmark comprises 5 real-world and 2 synthetically generated datasets, each\nannotated with CFGs. We show that these datasets demonstrably present a great\nchallenge even for state-of-the-art DP synthetic data generation methods.\nStruct-Bench also includes reference implementations of different metrics and a\nleaderboard, thereby providing researchers a standardized evaluation platform\nto benchmark and investigate privacy-preserving synthetic data generation\nmethods. Further, we also present a case study showing how to use Struct-Bench\nto improve the synthetic data quality of Private Evolution (PE) on structured\ndata. The benchmark and the leaderboard have been publicly made available at\nhttps://struct-bench.github.io.", "AI": {"tldr": "本文提出Struct-Bench框架，用于评估从包含自然语言数据的结构化数据集中生成的合成数据集，并提供了一个基准测试平台。", "motivation": "现有的合成数据评估技术难以捕捉到包含自然语言数据的结构化数据集的结构特性和相关性。", "method": "我们提出了Struct-Bench框架，它需要用户提供一种用上下文无关语法(CFG)表示的数据集结构。该基准包含了5个实际数据集和2个人工生成的数据集，每个都带有CFG注释。", "result": "这些数据集对最先进的DP合成数据生成方法提供了显著的挑战，并且通过案例研究展示了如何使用Struct-Bench来提高Private Evolution生成的结构化数据合成数据质量。", "conclusion": "Struct-Bench为研究者提供了一个标准化的评估平台，用以衡量和研究隐私保护的合成数据生成方法。"}}
{"id": "2509.10761", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10761", "abs": "https://arxiv.org/abs/2509.10761", "authors": ["Marcelo Sandoval-Castaneda", "Bryan Russell", "Josef Sivic", "Gregory Shakhnarovich", "Fabian Caba Heilbron"], "title": "EditDuet: A Multi-Agent System for Video Non-Linear Editing", "comment": "SIGGRAPH 2025", "summary": "Automated tools for video editing and assembly have applications ranging from\nfilmmaking and advertisement to content creation for social media. Previous\nvideo editing work has mainly focused on either retrieval or user interfaces,\nleaving actual editing to the user. In contrast, we propose to automate the\ncore task of video editing, formulating it as sequential decision making\nprocess. Ours is a multi-agent approach. We design an Editor agent and a Critic\nagent. The Editor takes as input a collection of video clips together with\nnatural language instructions and uses tools commonly found in video editing\nsoftware to produce an edited sequence. On the other hand, the Critic gives\nnatural language feedback to the editor based on the produced sequence or\nrenders it if it is satisfactory. We introduce a learning-based approach for\nenabling effective communication across specialized agents to address the\nlanguage-driven video editing task. Finally, we explore an LLM-as-a-judge\nmetric for evaluating the quality of video editing system and compare it with\ngeneral human preference. We evaluate our system's output video sequences\nqualitatively and quantitatively through a user study and find that our system\nvastly outperforms existing approaches in terms of coverage, time constraint\nsatisfaction, and human preference.", "AI": {"tldr": "提出了一种新的视频编辑方法，利用多智能体系统和语言驱动的方式，实现了视频的自动编辑，并且在多个指标上优于现有方法。", "motivation": "当前视频编辑工具主要集中在检索或用户界面，而缺乏自动化编辑的核心技术。本研究旨在自动化视频编辑的核心任务，通过多智能体协作实现语言驱动的自主编辑。", "method": "采用多智能体方法，设计了一个编辑器智能体和一个评论家智能体。编辑器智能体接收视频片段集合和自然语言指令，并通过使用视频编辑软件中的工具来生成编辑后的序列。评论家智能体基于生成的序列给出自然语言反馈或渲染最终结果。通过基于学习的方法实现专智能体之间的有效沟通，以应对语言驱动的视频编辑任务。", "result": "通过用户研究对生成的视频序列进行定性和定量评价，发现在覆盖率、时间约束满足和人类偏好方面，该系统远超现有方法。", "conclusion": "提出了一种基于多层次智能体协作的视频自动编辑方法，通过语言驱动的方式提高了编辑质量和效率。"}}
{"id": "2509.10697", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10697", "abs": "https://arxiv.org/abs/2509.10697", "authors": ["Pengcheng Jiang", "Siru Ouyang", "Yizhu Jiao", "Ming Zhong", "Runchu Tian", "Jiawei Han"], "title": "A Survey on Retrieval And Structuring Augmented Generation with Large Language Models", "comment": "KDD'25 survey track", "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nwith their remarkable capabilities in text generation and reasoning. However,\nthese models face critical challenges when deployed in real-world applications,\nincluding hallucination generation, outdated knowledge, and limited domain\nexpertise. Retrieval And Structuring (RAS) Augmented Generation addresses these\nlimitations by integrating dynamic information retrieval with structured\nknowledge representations. This survey (1) examines retrieval mechanisms\nincluding sparse, dense, and hybrid approaches for accessing external\nknowledge; (2) explore text structuring techniques such as taxonomy\nconstruction, hierarchical classification, and information extraction that\ntransform unstructured text into organized representations; and (3) investigate\nhow these structured representations integrate with LLMs through prompt-based\nmethods, reasoning frameworks, and knowledge embedding techniques. It also\nidentifies technical challenges in retrieval efficiency, structure quality, and\nknowledge integration, while highlighting research opportunities in multimodal\nretrieval, cross-lingual structures, and interactive systems. This\ncomprehensive overview provides researchers and practitioners with insights\ninto RAS methods, applications, and future directions.", "AI": {"tldr": "该论文通过调查分析了RAS增强生成方法在改进大型语言模型（LLMs）应用性能方面的应用及其面临的挑战和潜在的研究机会。", "motivation": "鉴于大型语言模型（LLMs）在文本生成和推理方面表现出色，但部署到实际应用时存在幻觉生成、知识过时、领域专业知识有限等挑战，该研究旨在通过检索和结构化生成增强（RAS）方法解决这些问题。", "method": "该论文采用调查方法，探讨了检索机制、文本结构化技术和结构化表示与大型语言模型（LLMs）的集成方法。", "result": "该研究深入分析了稀疏、密集和混合检索方法，探讨了文本结构化技术，如分类法构建、层级分类和信息抽取，还研究了通过提示方法、推理框架和知识嵌入技术将结构化表示与LLMs集成的方式。", "conclusion": "该调查不仅指出了检索效率、结构质量、知识融合等技术挑战，还提出了多模态检索、跨语言结构和交互式系统等研究机会，为研究人员和实践者提供了RAS方法、应用和未来方向的见解。"}}
{"id": "2509.10767", "categories": ["cs.CV", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.10767", "abs": "https://arxiv.org/abs/2509.10767", "authors": ["Sajad Amiri", "Shahram Taeb", "Sara Gharibi", "Setareh Dehghanfard", "Somayeh Sadat Mehrnia", "Mehrdad Oveisi", "Ilker Hacihaliloglu", "Arman Rahmim", "Mohammad R. Salmanpour"], "title": "Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging", "comment": "14 Pages, 1 Figure, and 6 Tables", "summary": "Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but\nraise safety, cost, and accessibility concerns. Predicting contrast enhancement\nfrom non-contrast MRI using machine learning (ML) offers a safer alternative,\nas enhancement reflects tumor aggressiveness and informs treatment planning.\nYet scanner and cohort variability hinder robust model selection. We propose a\nstability-aware framework to identify reproducible ML pipelines for multicenter\nprediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases\nfrom four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).\nNon-contrast T1WI served as input, with enhancement derived from paired\npost-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were\nextracted and combined with 48 dimensionality reduction methods and 25\nclassifiers, yielding 1,200 pipelines. Rotational validation was trained on\nthree datasets and tested on the fourth. Cross-validation prediction accuracies\nranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),\n0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,\nprecision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more\nwidely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr\npipeline consistently ranked highest, balancing accuracy and stability. This\nframework demonstrates that stability-aware model selection enables reliable\nprediction of contrast enhancement from non-contrast glioma MRI, reducing\nreliance on GBCAs and improving generalizability across centers. It provides a\nscalable template for reproducible ML in neuro-oncology and beyond.", "AI": {"tldr": "通过机器学习预测非对比MRI的对比增强能提供更安全、经济、易获得的胶质瘤成像方式。研究提出了一种稳定性框架，并使用多种数据集验证了这种方法的有效性和稳定性。", "motivation": "钆基对比剂(GBCAs)在胶质瘤成像中至关重要，但存在安全隐患、成本问题和获取障碍。通过机器学习预测非对比MRI的对比增强是一种更安全的替代方案，因为它能反映肿瘤的侵袭性并指导治疗计划的制定。扫描仪和队列差异阻碍了模型的选择。", "method": "提出了一种稳定性优先的框架，用于识别可重复的机器学习管道，以多中心预测胶质瘤MRI对比增强。", "result": "使用了4个TCIA数据集中的1,446例胶质瘤病例(UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG)。非对比T1WI作为输入，增強信号从配对的对比后T1WI得出。使用PyRadiomics在IBSI标准下提取了108个特征，并结合了48种降维方法和25种分类器，生成了1,200个管道。旋转验证在三个数据集上进行训练并在第四个数据集上进行测试。交叉验证的预测准确度范围为0.91到0.96，外部测试的准确度分别为UCSF-PDGM的0.87，UPENN-GB的0.98，BRATS-Africa的0.95，平均准确度为0.93。F1、精度和召回在0.87到0.96之间保持稳定，而ROC-AUC差异较大，反映队列异质性。", "conclusion": "MI与ETr管道表现出最高的稳定性和准确性，表明稳定性优先的模型选择可实现可靠的胶质瘤MRI对比增强预测，减少对GBCA的依赖，增强多中心间的泛化能力，为神经肿瘤的机器学习提供了可扩展、可复制的模板。"}}
{"id": "2509.10708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10708", "abs": "https://arxiv.org/abs/2509.10708", "authors": ["Iman Barati", "Mostafa Amiri", "Heshaam Faili"], "title": "SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation", "comment": null, "summary": "Supervised Fine-Tuning (SFT) is essential for training large language models\n(LLMs), significantly enhancing critical capabilities such as instruction\nfollowing and in-context learning. Nevertheless, creating suitable training\ndatasets tailored for specific domains remains challenging due to unique domain\nconstraints and data scarcity. In this paper, we propose SearchInstruct, an\ninnovative method explicitly designed to construct high quality instruction\ndatasets for SFT. Our approach begins with a limited set of domain specific,\nhuman generated questions, which are systematically expanded using a large\nlanguage model. Subsequently, domain relevant resources are dynamically\nretrieved to generate accurate and contextually appropriate answers for each\naugmented question. Experimental evaluation demonstrates that SearchInstruct\nenhances both the diversity and quality of SFT datasets, leading to measurable\nimprovements in LLM performance within specialized domains. Additionally, we\nshow that beyond dataset generation, the proposed method can also effectively\nfacilitate tasks such as model editing, enabling efficient updates to existing\nmodels. To facilitate reproducibility and community adoption, we provide full\nimplementation details, the complete set of generated instruction response\npairs, and the source code in a publicly accessible Git repository:\n[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)", "AI": {"tldr": "论文提出了SearchInstruct，一种用于生成高质量监督微调指令数据集的新方法，解决了特定领域训练数据集创建难题，提高了某些领域内大型语言模型性能。", "motivation": "旨在解决为特定领域创建合适训练数据集的挑战，由于领域约束和数据稀缺，这一过程变得很困难。", "method": "SearchInstruct, 一种专门设计用于构造高质量指令数据集的方法，用于监督微调（SFT）。此方法首先使用有限的领域特定且由人类生成的问题，然后通过大型语言模型系统地扩展这些问题，并动态检索领域相关的资源以生成准确且上下文适当的答案。", "result": "该研究提高了特定领域训练数据集的质量和多样性，增强了大型语言模型在这些领域的性能。另外，此方法也促进了模型编辑任务。", "conclusion": "实验评估表明，SearchInstruct能增强SFT数据集的多样性和质量，从而在特定领域内提高大型语言模型的性能。此外，该方法还可促进如模型编辑等任务。"}}
{"id": "2509.10779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10779", "abs": "https://arxiv.org/abs/2509.10779", "authors": ["Yilun Xiao"], "title": "Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection", "comment": "8 pages, 7 figures", "summary": "Dense small objects in UAV imagery are often missed due to long-range\nviewpoints, occlusion, and clutter[cite: 5]. This paper presents a\ndetector-agnostic post-processing framework that converts overlap-induced\nredundancy into group evidence[cite: 6]. Overlapping tiling first recovers\nlow-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)\nand a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group\nevidence[cite: 7]. Validated groups receive controlled confidence reweighting\nbefore class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall\nincrease from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to\n0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per\nimage[cite: 10]. These results indicate recall-first, precision-trade-off\nbehavior that benefits recall-sensitive applications such as far-field counting\nand monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,\nspatial clustering stabilizes geometry, semantic clustering enforces appearance\ncoherence, and reweighting provides calibrated integration with the\nbaseline[cite: 11]. The framework requires no retraining and integrates with\nmodern detectors[cite: 12]. Future work will reduce semantic gating cost and\nextend the approach with temporal cues[cite: 13].", "AI": {"tldr": "论文介绍了一种用于检测无人机图像中密集小目标的后处理框架，通过合并重叠造成的冗余，提高了检测的召回率，尽管以稍微降低精确率为代价。", "motivation": "无人机图像中的密集小目标常因远距离视野、遮挡和杂乱而丢失。为了提高这些目标的检测率，提出了这种框架。", "method": "此论文提出了一种与检测器无关的后处理框架，该框架首先通过重叠切片恢复低置信度候选对象，然后使用空间门（DBSCAN应用于框质心）和语义门（DBSCAN应用于ResNet-18嵌入）来验证组证据。经过验证的组在类感知NMS融合之前会接受精确的置信度重新加权。", "result": "实验结果显示在VisDrone数据集上的召回率从0.685提升至0.778，尽管精确率下降到0.595，但F1值为0.669，表明该框架具有优先提高召回率的性质，符合远距离计数和监控等应用的需求。", "conclusion": "框架无需重新训练现有检测器，可以与现代检测器集成。未来的研究将集中于降低语义门的成本并引入时间线索。"}}
{"id": "2509.10737", "categories": ["cs.CL", "cs.LG", "68T50, 68T07", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.10737", "abs": "https://arxiv.org/abs/2509.10737", "authors": ["Zaur Gouliev", "Jennifer Waters", "Chengqian Wang"], "title": "PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models", "comment": "11 pages, 5 figures, 4 tables. Submitted to arXiv in Computation and\n  Language", "summary": "Disinformation spreads rapidly across linguistic boundaries, yet most AI\nmodels are still benchmarked only on English. We address this gap with a\nsystematic comparison of five multilingual transformer models: mBERT, XLM,\nXLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning\nclassification task. While transformer-based language models have demonstrated\nnotable success in detecting disinformation in English, their effectiveness in\nmultilingual contexts still remains up for debate. To facilitate evaluation, we\nintroduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs\n(false claim vs. factual correction) spanning over twenty five languages that\ncollectively cover five language families and a broad topical range from\npolitics, health, climate, finance, and conspiracy, half of which are\nfact-checked disinformation claims verified by an augmented MindBugs Discovery\ndataset. Our experiments revealed performance variations. Models such as\nRemBERT achieved better overall accuracy, particularly excelling in\nlow-resource languages, whereas models like mBERT and XLM exhibit considerable\nlimitations when training data is scarce. We provide a discussion of these\nperformance patterns and implications for real-world deployment. The dataset is\npublicly available on our GitHub repository to encourage further\nexperimentation and advancement. Our findings illuminate both the potential and\nthe current limitations of AI systems for multilingual disinformation\ndetection.", "AI": {"tldr": "研究分析了多语言模型（mBERT, XLM, XLM-RoBERTa, RemBERT, mT5）在跨语言虚假信息检测中的性能，创建了PolyTruth Disinfo数据集包含多语言的假真陈述对。结果显示，RemBERT在语言资源较少的情况下表现更好，而mBERT和XLM则受限于稀缺训练数据。", "motivation": "现有的AI模型大多只在英语环境中进行基准测试，但虚假信息传播迅速跨越了语言的界限。本论文旨在填补这一空白。", "method": "对五个多语言变压器模型（mBERT、XLM、XLM-RoBERTa、RemBERT 和 mT5）进行了系统的比较分析，使用包含多种语言的真假陈述对的机器学习分类任务。", "result": "实验结果显示了性能的差异。例如，RemBERT在总体准确性上表现更好，特别是在资源较少的语言中；而mBERT和XLM在训练数据稀缺时显示出局限性。", "conclusion": "讨论了这些性能模式及其对实际部署的含义，并公开提供数据集以鼓励进一步的实验和进步。发现揭示了AI系统在多语言虚假信息检测方面的潜力及其当前限制。"}}
{"id": "2509.10813", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10813", "abs": "https://arxiv.org/abs/2509.10813", "authors": ["Weipeng Zhong", "Peizhou Cao", "Yichen Jin", "Li Luo", "Wenzhe Cai", "Jingli Lin", "Hanqing Wang", "Zhaoyang Lyu", "Tai Wang", "Bo Dai", "Xudong Xu", "Jiangmiao Pang"], "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts", "comment": null, "summary": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D\nscene datasets characterized by scene diversity and realistic layouts. However,\nexisting datasets typically suffer from limitations in data scale or diversity,\nsanitized layouts lacking small items, and severe object collisions. To address\nthese shortcomings, we introduce \\textbf{InternScenes}, a novel large-scale\nsimulatable indoor scene dataset comprising approximately 40,000 diverse scenes\nby integrating three disparate scene sources, real-world scans, procedurally\ngenerated scenes, and designer-created scenes, including 1.96M 3D objects and\ncovering 15 common scene types and 288 object classes. We particularly preserve\nmassive small items in the scenes, resulting in realistic and complex layouts\nwith an average of 41.5 objects per region. Our comprehensive data processing\npipeline ensures simulatability by creating real-to-sim replicas for real-world\nscans, enhances interactivity by incorporating interactive objects into these\nscenes, and resolves object collisions by physical simulations. We demonstrate\nthe value of InternScenes with two benchmark applications: scene layout\ngeneration and point-goal navigation. Both show the new challenges posed by the\ncomplex and realistic layouts. More importantly, InternScenes paves the way for\nscaling up the model training for both tasks, making the generation and\nnavigation in such complex scenes possible. We commit to open-sourcing the\ndata, models, and benchmarks to benefit the whole community.", "AI": {"tldr": "提出了InternScenes，一个大规模可模拟的室内场景数据集，通过整合多种场景源，解决了现有数据集规模和多样性不足、布局简化的缺陷，增强了场景的交互性和真实感，展示了其在场景布局生成和点到点导航上的应用潜力。", "motivation": "现有3D场景数据集在数据规模、多样性、布局真实性以及对象碰撞方面存在局限性，这些局限性阻碍了Embodied AI的发展。", "method": "通过整合真实世界扫描、程序生成场景和设计创建的场景，创建了包含约40,000个多样化场景的InternScenes数据集，并通过对数据的全面处理来确保数据可用于模拟和增强交互性。", "result": "展示了InternScenes在场景布局生成和点目标导航上的应用潜力，并解决了复杂和真实布局带来的新挑战。", "conclusion": "InternScenes为这两种任务的模型训练规模化提供了可能，并计划开源数据、模型和基准测试以惠及整个社区。"}}
{"id": "2509.10739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10739", "abs": "https://arxiv.org/abs/2509.10739", "authors": ["Mobina Pournemat", "Keivan Rezaei", "Gaurang Sriramanan", "Arman Zarei", "Jiaxiang Fu", "Yang Wang", "Hamid Eghbalzadeh", "Soheil Feizi"], "title": "Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs", "comment": "25 pages, 4 figures, 6 tables", "summary": "Despite widespread success in language understanding and generation, large\nlanguage models (LLMs) exhibit unclear and often inconsistent behavior when\nfaced with tasks that require probabilistic reasoning. In this work, we present\nthe first comprehensive study of the reasoning capabilities of LLMs over\nexplicit discrete probability distributions. Given observations from a\nprobability distribution, we evaluate models on three carefully designed tasks,\nmode identification, maximum likelihood estimation, and sample generation, by\nprompting them to provide responses to queries about either the joint\ndistribution or its conditionals. These tasks thus probe a range of\nprobabilistic skills, including frequency analysis, marginalization, and\ngenerative behavior. Through comprehensive empirical evaluations, we\ndemonstrate that there exists a clear performance gap between smaller and\nlarger models, with the latter demonstrating stronger inference and surprising\ncapabilities in sample generation. Furthermore, our investigations reveal\nnotable limitations, including sensitivity to variations in the notation\nutilized to represent probabilistic outcomes and performance degradation of\nover 60% as context length increases. Together, our results provide a detailed\nunderstanding of the probabilistic reasoning abilities of LLMs and identify key\ndirections for future improvement.", "AI": {"tldr": "本文评估了大型语言模型（LLMs）在面临概率推理任务时的能力，发现虽然大型模型在推理和生成样本方面表现优异，但仍存在局限性。", "motivation": "尽管在语言理解和生成方面取得了广泛的成功，LLMs在需要概率推理的任务中表现出不清楚且经常不一致的行为。本研究旨在全面研究LLMs在显式离散概率分布上的推理能力。", "method": "本文通过精心设计的任务评估大型语言模型（LLMs）在面对显式离散概率分布时的推理能力。这些任务包括模式识别、最大似然估计和样本生成，通过提示模型对联合分布或其条件分布的查询做出响应来进行评估。", "result": "实验证明存在小模型和大模型之间的性能差距，大型模型在推理和样本生成方面表现出较强的能力。然而，也发现其存在对概率结果表示符号变化敏感和随着上下文长度增加性能下降超过60%的局限性。", "conclusion": "研究结果提供了关于大型语言模型概率推理能力的深入了解，并指出了未来改进的关键方向。"}}
{"id": "2509.10815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10815", "abs": "https://arxiv.org/abs/2509.10815", "authors": ["Robert M. Corless", "Deepak Singh Kalhan", "Stephen M. Watt"], "title": "Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition", "comment": null, "summary": "Previous work has made use of a parameterized plane curve polynomial\nrepresentation for mathematical handwriting, with the polynomials represented\nin a Legendre or Legendre-Sobolev graded basis. This provides a compact\ngeometric representation for the digital ink. Preliminary results have also\nbeen shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the\ntrade-offs between basis choice and polynomial degree to achieve accurate\nmodeling with a low computational cost. To do this, we consider the condition\nnumber for polynomial evaluation in these bases and bound how the various inner\nproducts give norms for the variations between symbols.", "AI": {"tldr": "文章探讨了不同基函数与多项式度数之间的权衡关系，以减少计算成本并提高模型精准度。", "motivation": "之前的研究使用参数化平面曲线多项式表示数学手写内容，使用Legendre或Legendre-Sobolev分级基进行表示，给出了数字墨迹的紧凑几何表示。还需要探索不同基的选择和多项式度数之间的权衡，以在保持计算效率的同时，实现精准建模。", "method": "本文研究了不同基函数（如Legendre，Legendre-Sobolev，Chebyshev，Chebyshev-Sobolev）与多项式度数之间的权衡，以实现准确建模并降低计算成本。为此，文章考察了这些基函数中的多项式评估的条件数，并计算了不同内积给定的符号变化的范数。", "result": "文章分析了不同基函数的多项式评估条件数，并评估了不同内积给定的符号变化的范数，以寻找最优的基函数和多项式度数组合。", "conclusion": "基于研究结果，论文为选择合适的基和多项式度数提供了有价值的指导，这对于数学手写建模非常重要。"}}
{"id": "2509.10744", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.10744", "abs": "https://arxiv.org/abs/2509.10744", "authors": ["Ozan Gokdemir", "Neil Getty", "Robert Underwood", "Sandeep Madireddy", "Franck Cappello", "Arvind Ramanathan", "Ian T. Foster", "Rick L. Stevens"], "title": "Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models", "comment": "This manuscript has been accepted for publication at the\n  Supercomputing 25 (SC '25) Conference (Frontiers in Generative AI for HPC\n  Science and Engineering: Foundations, Challenges, and Opportunities Workshop)\n  in St. Louis, MO, USA on November 16th, 2025. It will appear in the SC25\n  Workshop Proceedings after that date", "summary": "As scientific knowledge grows at an unprecedented pace, evaluation benchmarks\nmust evolve to reflect new discoveries and ensure language models are tested on\ncurrent, diverse literature. We propose a scalable, modular framework for\ngenerating multiple-choice question-answering (MCQA) benchmarks directly from\nlarge corpora of scientific papers. Our pipeline automates every stage of MCQA\ncreation, including PDF parsing, semantic chunking, question generation, and\nmodel evaluation. As a case study, we generate more than 16,000 MCQs from\n22,000 open-access articles in radiation and cancer biology. We then evaluate a\nsuite of small language models (1.1B-14B parameters) on these questions,\ncomparing baseline accuracy with retrieval-augmented generation (RAG) from\npaper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.\nWe find that reasoning-trace retrieval consistently improves performance on\nboth synthetic and expert-annotated benchmarks, enabling several small models\nto surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.", "AI": {"tldr": "研究提出了一种新的MCQA生成框架，并通过实验验证了该框架的有效性，尤其在基于推理轨迹的检索情况下，一些小型模型的表现甚至优于较大的GPT-4模型。", "motivation": "随着科学知识以前所未有的速度增长，评估基准需要发展以反映新的发现，并确保语言模型能够接受当前多样化的文献测试。", "method": "我们提出了一种可扩展、模块化的框架，用于从大量科学论文中自动生成多选题（MCQA）基准。该框架自动化了MCQA创作的每一个阶段，包括PDF解析、语义切块、问题生成和模型评估。", "result": "作为案例研究，我们从22,000篇开放获取的放射性和癌症生物学文章中生成了超过16,000个多选题。我们评估了一系列小型语言模型（1.1B-14B参数）在这些问题上的表现，并将基线准确度与基于论文衍生语义切块和GPT-4.1提取的推理轨迹的检索增强生成（RAG）进行比较。", "conclusion": "结果发现，基于推理轨迹的检索连贯性地提高了在合成和专家标注基准上的性能，使几个小型模型在2023年天体放射和癌症生物学考试中的表现优于GPT-4。"}}
{"id": "2509.10824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10824", "abs": "https://arxiv.org/abs/2509.10824", "authors": ["Aghiles Kebaili", "Romain Modzelewski", "Jérôme Lapuyade-Lahorgue", "Maxime Fontanilles", "Sébastien Thureau", "Su Ruan"], "title": "Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression", "comment": null, "summary": "Glioma, an aggressive brain malignancy characterized by rapid progression and\nits poor prognosis, poses significant challenges for accurate evolution\nprediction. These challenges are exacerbated by sparse, irregularly acquired\nlongitudinal MRI data in clinical practice, where incomplete follow-up\nsequences create data imbalances and make reliable modeling difficult. In this\npaper, we present a multitask diffusion framework for time-agnostic, pixel-wise\nprediction of glioma progression. The model simultaneously generates future\nFLAIR sequences at any chosen time point and estimates spatial probabilistic\ntumor evolution maps derived using signed distance fields (SDFs), allowing\nuncertainty quantification. To capture temporal dynamics of tumor evolution\nacross arbitrary intervals, we integrate a pretrained deformation module that\nmodels inter-scan changes using deformation fields. Regarding the common\nclinical limitation of data scarcity, we implement a targeted augmentation\npipeline that synthesizes complete sequences of three follow-up scans and\nimputes missing MRI modalities from available patient studies, improving the\nstability and accuracy of predictive models. Based on merely two follow-up\nscans at earlier timepoints, our framework produces flexible time-depending\nprobability maps, enabling clinicians to interrogate tumor progression risks at\nany future temporal milestone. We further introduce a radiotherapy-weighted\nfocal loss term that leverages radiation dose maps, as these highlight regions\nof greater clinical importance during model training. The proposed method was\ntrained on a public dataset and evaluated on an internal private dataset,\nachieving promising results in both cases", "AI": {"tldr": "本文提出了一种多任务扩散框架，用于预测胶质瘤的像素级进展，并通过数据增强和预训练的形变模块有效处理数据稀缺的问题。", "motivation": "该研究的动机来源于胶质瘤这种进展迅速且预后不良的恶性脑肿瘤，其准确的进展预测带来了显著的挑战，特别是在临床实践中，由于纵向MRI数据稀疏且不规则获取，进一步增加了建模的难度。", "method": "本文提出了一个多任务扩散框架，用于时间无关的像素级胶质瘤进展预测。该模型同时生成未来任意时间点的FLAIR序列，并使用有符号距离场（SDF）估算空间概率肿瘤演变图，允许不确定性量化。为了捕捉任意时间间隔内的肿瘤演变动态，作者整合了一个经过预训练的形变模块，该模块使用形变场来建模扫描之间的变化。针对临床数据稀缺的常见问题，作者实施了一个有针对性的数据增强管道，该管道从现有的病人研究中合成完整的三份随访扫描序列并补全缺失的MRI模式，提升了预测模型的稳定性和准确性。", "result": "所提出的方法在公开数据集上进行了训练，并在内部私有数据集上进行了评估，在两种情况下都取得了有希望的结果。", "conclusion": "基于仅有的两个早期时间点的随访扫描，该框架可以生成灵活的时间依赖的概率图，允许临床医生在任意未来的时点询问肿瘤进展风险。"}}
{"id": "2509.10746", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10746", "abs": "https://arxiv.org/abs/2509.10746", "authors": ["Adarsh Srinivasan", "Jacob Dineen", "Muhammad Umar Afzal", "Muhammad Uzair Sarfraz", "Irbaz B. Riaz", "Ben Zhou"], "title": "RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems", "comment": null, "summary": "Large language models in healthcare often miss critical emotional cues,\ndelivering medically sound but emotionally flat advice. This is especially\nproblematic in clinical contexts where patients are distressed and vulnerable,\nand require empathic communication to support safety, adherence, and trust. We\npresent RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time\nframework that adds structured emotional reasoning without retraining. By\ndecomposing empathy into transparent appraisal-theoretic stages and exposing\nper-dimension Likert signals, RECAP produces nuanced, auditable responses.\nAcross EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by\n22-28% on 8B models and 10-13% on larger models over zero-shot baselines.\nClinician evaluations further confirm superior empathetic communication. RECAP\nshows that modular, theory-grounded prompting can systematically enhance\nemotional intelligence in medical AI while preserving the accountability\nrequired for deployment.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.10841", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10841", "abs": "https://arxiv.org/abs/2509.10841", "authors": ["Simone Mosco", "Daniel Fusaro", "Wanmeng Li", "Emanuele Menegatti", "Alberto Pretto"], "title": "Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios", "comment": "Submitted to Computer Vision and Image Understanding", "summary": "LiDAR point cloud semantic segmentation is essential for interpreting 3D\nenvironments in applications such as autonomous driving and robotics. Recent\nmethods achieve strong performance by exploiting different point cloud\nrepresentations or incorporating data from other sensors, such as cameras or\nexternal datasets. However, these approaches often suffer from high\ncomputational complexity and require large amounts of training data, limiting\ntheir generalization in data-scarce scenarios. In this paper, we improve the\nperformance of point-based methods by effectively learning features from 2D\nrepresentations through point-plane projections, enabling the extraction of\ncomplementary information while relying solely on LiDAR data. Additionally, we\nintroduce a geometry-aware technique for data augmentation that aligns with\nLiDAR sensor properties and mitigates class imbalance. We implemented and\nevaluated our method that applies point-plane projections onto multiple\ninformative 2D representations of the point cloud. Experiments demonstrate that\nthis approach leads to significant improvements in limited-data scenarios,\nwhile also achieving competitive results on two publicly available standard\ndatasets, as SemanticKITTI and PandaSet. The code of our method is available at\nhttps://github.com/SiMoM0/3PNet", "AI": {"tldr": "本文提出了一种点-平面投影结合几何感知数据增强的技术，用于LiDAR点云语义分割，有效利用2D表示，降低计算复杂性，提高数据稀缺场景下的泛化能力。", "motivation": "尽管现有方法通过使用不同的点云表示或结合来自其他传感器的数据取得了良好的性能，但这些方法通常计算复杂性高且需要大量训练数据，限制了在数据稀缺场景中的泛化能力。本研究旨在改进基于点的方法，通过点-平面投影有效学习特征，并进行几何感知的数据增强。", "method": "点-平面投影和几何感知数据增强技术，利用2D表示从LiDAR点云中有效学习特征，缓解类别不平衡问题。", "result": "实验表明，该方法在数据有限的场景下表现显著提升，并在两个公开的标准数据集SemanticKITTI和PandaSet上取得了有竞争力的结果。", "conclusion": "该方法通过点-平面投影技术和几何感知数据增强，改善了LiDAR点云语义分割的效果，尤其在数据有限的情况下表现突出，同时在标准数据集上也表现良好。"}}
{"id": "2509.10798", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10798", "abs": "https://arxiv.org/abs/2509.10798", "authors": ["Yijun Liu", "Yixuan Wang", "Yuzhuang Xu", "Shiyu Ji", "Yang Xu", "Qingfu Zhu", "Wanxiang Che"], "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction", "comment": "preprint", "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.", "AI": {"tldr": "本论文提出Judge Q方法解决大语言模型KV缓存替换问题，通过软标记列表训练捕获全局信息，保持解码质量，并在实验中证明了其有效性。", "motivation": "大语言模型的键值缓存大小随序列增长呈现线性增长，这对内存使用和解码效率造成负面影响。现有的缓存替换方法通常过度关注局部信息，可能忽略关键的全局信息。", "method": "Judge Q 方法通过在输入序列末尾连接软标记列表来训练语言模型的嵌入层，以较低的成本调整模型，使软标记的关注图与实际解码标记的接近，从而在缓存替换时捕获全局信息，准确评估键值对的重要性。", "result": "实验结果表明，与现有方法相比，在相同的替换预算下，该方法在LongBench基准测试中性能下降大约1个点，而在RULER基准测试中性能改善超过3个点。", "conclusion": "所提出的判别性Q方法能够有效地结合全局信息来评估KV缓存中键值对的重要性，在缓存替换时维持解码质量，且可轻松集成到现有的开源模型中，同时只需少量的训练开销。"}}
{"id": "2509.10842", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10842", "abs": "https://arxiv.org/abs/2509.10842", "authors": ["Chongyu Wang", "Kunlei Jing", "Jihua Zhu", "Di Wang"], "title": "OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds", "comment": null, "summary": "Open-vocabulary semantic segmentation enables models to recognize and segment\nobjects from arbitrary natural language descriptions, offering the flexibility\nto handle novel, fine-grained, or functionally defined categories beyond fixed\nlabel sets. While this capability is crucial for large-scale urban point clouds\nthat support applications such as digital twins, smart city management, and\nurban analytics, it remains largely unexplored in this domain. The main\nobstacles are the frequent absence of high-quality, well-aligned multi-view\nimagery in large-scale urban point cloud datasets and the poor generalization\nof existing three-dimensional (3D) segmentation pipelines across diverse urban\nenvironments with substantial variation in geometry, scale, and appearance. To\naddress these challenges, we present OpenUrban3D, the first 3D open-vocabulary\nsemantic segmentation framework for large-scale urban scenes that operates\nwithout aligned multi-view images, pre-trained point cloud segmentation\nnetworks, or manual annotations. Our approach generates robust semantic\nfeatures directly from raw point clouds through multi-view, multi-granularity\nrendering, mask-level vision-language feature extraction, and sample-balanced\nfusion, followed by distillation into a 3D backbone model. This design enables\nzero-shot segmentation for arbitrary text queries while capturing both semantic\nrichness and geometric priors. Extensive experiments on large-scale urban\nbenchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves\nsignificant improvements in both segmentation accuracy and cross-scene\ngeneralization over existing methods, demonstrating its potential as a flexible\nand scalable solution for 3D urban scene understanding.", "AI": {"tldr": "介绍OpenUrban3D，一种针对大规模城市场景的三维开放词汇语义分割框架，它可实现对任意文本查询的零样本分割，实验表明其在分割准确性和跨场景泛化性上优于现有方法。", "motivation": "提出OpenUrban3D，一个无需对齐的多视角图像、预训练的点云分割网络或手动注释，适用于大规模城市场景的三维开放词汇语义分割框架。", "method": "通过多视角、多粒度渲染、掩码级别的视觉语言特征提取以及样本平衡融合，直接从原始点云生成鲁棒的语义特征，这些特征随后被蒸馏到三维基础模型中，以实现对任意文本查询的零样本分割，同时捕捉语义丰富性和几何先验。", "result": "在SensatUrban和SUM等大规模城市基准上的实验表明，OpenUrban3D在分割准确性和跨场景泛化性上显著优于现有方法。", "conclusion": "展示了OpenUrban3D作为一个灵活和可扩展的解决方案，在三维城市场景理解方面的潜力。"}}
{"id": "2509.10833", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10833", "abs": "https://arxiv.org/abs/2509.10833", "authors": ["Dominic Petrak", "Thy Thy Tran", "Iryna Gurevych"], "title": "Towards Automated Error Discovery: A Study in Conversational AI", "comment": "Accepted to EMNLP 2025 main conference", "summary": "Although LLM-based conversational agents demonstrate strong fluency and\ncoherence, they still produce undesirable behaviors (errors) that are\nchallenging to prevent from reaching users during deployment. Recent research\nleverages large language models (LLMs) to detect errors and guide\nresponse-generation models toward improvement. However, current LLMs struggle\nto identify errors not explicitly specified in their instructions, such as\nthose arising from updates to the response-generation model or shifts in user\nbehavior. In this work, we introduce Automated Error Discovery, a framework for\ndetecting and defining errors in conversational AI, and propose SEEED (Soft\nClustering Extended Encoder-Based Error Detection), as an encoder-based\napproach to its implementation. We enhance the Soft Nearest Neighbor Loss by\namplifying distance weighting for negative samples and introduce Label-Based\nSample Ranking to select highly contrastive examples for better representation\nlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --\nacross multiple error-annotated dialogue datasets, improving the accuracy for\ndetecting unknown errors by up to 8 points and demonstrating strong\ngeneralization to unknown intent detection.", "AI": {"tldr": "本文提出了SEEED方法，通过增强Soft Nearest Neighbor Loss和引入Label-Based Sample Ranking来提高未知错误检测的准确度。", "motivation": "尽管大型语言模型(LLMs)在检测错误和引导响应生成模型改进方面表现出色，但它们在识别未在指令中明确指定的错误时仍存在困难。本文旨在解决这一问题，提出自动错误发现框架和SEEED方法。", "method": "本文介绍了自动化错误发现框架，用于检测和定义对话AI中的错误，并提出了SEEED（Soft Clustering Extended Encoder-Based Error Detection），这是一种基于编码器的方法。SEEED通过增强Soft Nearest Neighbor Loss来放大负样本的距离权重，并引入了基于标签的样本排名以选择对比度高的样本用于更好的表示学习。", "result": "SEEED在多个标注错误的对话数据集上击败了改编的基线方法，包括GPT-4o和Phi-4，未知错误检测准确性提高了8个百分点。", "conclusion": "SEEED方法展示了在未来检测对话AI系统中未知错误的强大潜力，证明了其在未知意图检测中具有良好的泛化能力。"}}
{"id": "2509.10887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10887", "abs": "https://arxiv.org/abs/2509.10887", "authors": ["Aryan Kashyap Naveen", "Bhuvanesh Singla", "Raajan Wankhade", "Shreesha M", "Ramu S", "Ram Mohana Reddy Guddeti"], "title": "AutoOEP -- A Multi-modal Framework for Online Exam Proctoring", "comment": "8 pages, 6 figures", "summary": "The burgeoning of online education has created an urgent need for robust and\nscalable systems to ensure academic integrity during remote examinations.\nTraditional human proctoring is often not feasible at scale, while existing\nautomated solutions can be intrusive or fail to detect a wide range of cheating\nbehaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a\ncomprehensive, multi-modal framework that leverages computer vision and machine\nlearning to provide effective, automated proctoring. The system utilizes a\ndual-camera setup to capture both a frontal view of the examinee and a side\nview of the workspace, minimizing blind spots. Our approach integrates several\nparallel analyses: the Face Module performs continuous identity verification\nusing ArcFace, along with head pose estimation, gaze tracking, and mouth\nmovement analysis to detect suspicious cues. Concurrently, the Hand Module\nemploys a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile\nphones, notes) and tracks hand proximity to these objects. Features from these\nmodules are aggregated and fed into a Long Short-Term Memory (LSTM) network\nthat analyzes temporal patterns to calculate a real-time cheating probability\nscore. We evaluate AutoOEP on a custom-collected dataset simulating diverse\nexam conditions. Our system achieves an accuracy of 90.7% in classifying\nsuspicious activities. The object detection component obtains a mean Average\nPrecision (mAP@.5) of 0.57 for prohibited items, and the entire framework\nprocesses video streams at approximately 2.4 frames per second without a GPU.\nThe results demonstrate that AutoOEP is an effective and resource-efficient\nsolution for automated proctoring, significantly reducing the need for human\nintervention and enhancing the integrity of online assessments.", "AI": {"tldr": "This paper presents AutoOEP, an automated proctoring system designed to enhance online exam integrity, achieving high accuracy and efficiency in detecting suspicious activities and prohibited items.", "motivation": "The motivation behind this paper is to address the challenges in ensuring academic integrity in online education, where traditional human proctoring is not scalable, and existing automated solutions are either intrusive or inadequate in detecting various forms of cheating.", "method": "This paper introduces AutoOEP, an automated multi-modal proctoring framework that combines computer vision and machine learning techniques. It employs a dual-camera setup for comprehensive surveillance, integrating several modules such as Face Module for identity verification and suspicious behavior detection, and Hand Module for detecting prohibited items and tracking hand proximity. The system further employs an LSTM network to analyze temporal patterns and compute a real-time cheating probability score.", "result": "AutoOEP demonstrates an accuracy of 90.7% in detecting suspicious activities and achieves a mean Average Precision of 0.57 for prohibited item detection. The system processes video at 2.4 frames per second without GPU support, highlighting its efficiency and feasibility in real-world applications.", "conclusion": "The conclusion of the paper is that AutoOEP stands out as an efficient, resource-efficient, and effective solution for automated proctoring, significantly reducing the requirement for human proctoring and ensuring academic honesty in online exams."}}
{"id": "2509.10843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10843", "abs": "https://arxiv.org/abs/2509.10843", "authors": ["Can Wang", "Yiqun Chen"], "title": "Evaluating Large Language Models for Evidence-Based Clinical Question Answering", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated substantial progress in\nbiomedical and clinical applications, motivating rigorous evaluation of their\nability to answer nuanced, evidence-based questions. We curate a multi-source\nbenchmark drawing from Cochrane systematic reviews and clinical guidelines,\nincluding structured recommendations from the American Heart Association and\nnarrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe\nconsistent performance patterns across sources and clinical domains: accuracy\nis highest on structured guideline recommendations (90%) and lower on narrative\nguideline and systematic review questions (60--70%). We also find a strong\ncorrelation between accuracy and the citation count of the underlying\nsystematic reviews, where each doubling of citations is associated with roughly\na 30% increase in the odds of a correct answer. Models show moderate ability to\nreason about evidence quality when contextual information is supplied. When we\nincorporate retrieval-augmented prompting, providing the gold-source abstract\nraises accuracy on previously incorrect items to 0.79; providing top 3 PubMed\nabstracts (ranked by semantic relevance) improves accuracy to 0.23, while\nrandom abstracts reduce accuracy (0.10, within temperature variation). These\neffects are mirrored in GPT-4o-mini, underscoring that source clarity and\ntargeted retrieval -- not just model size -- drive performance. Overall, our\nresults highlight both the promise and current limitations of LLMs for\nevidence-based clinical question answering. Retrieval-augmented prompting\nemerges as a useful strategy to improve factual accuracy and alignment with\nsource evidence, while stratified evaluation by specialty and question type\nremains essential to understand current knowledge access and to contextualize\nmodel performance.", "AI": {"tldr": "研究评估了大型语言模型在回答基于证据的临床问题时的表现，发现结构化指南建议的准确性最高，获取相关信息澄清来源和检索策略能够显著提高模型表现。", "motivation": "随着大型语言模型在生物医学和临床应用中取得的进步，需要对其提供基于证据答案的能力进行严格的评估。", "method": "我们使用GPT-4o-mini和GPT-5模型对一个从Cochrane系统综述和临床指南中汇集的多源基准测试进行了评估。该测试涵盖了美国心脏协会的结构化建议和保险公司使用的叙述指导。我们还使用了检索增强的提示方法，提供黄金来源摘要和PubMed摘要，以测试它们对模型准确性的影响。", "result": "研究发现，模型在结构化指南建议上的准确性最高（90%），而在文献指南和系统综述问题上的准确性较低（60-70%）。准确性与底层系统综述的引文数量呈强相关性，每次引文数量翻倍，正确答案的概率约提高30%。提供黄金来源摘要可将先前错误项目的准确性提高到0.79；提供基于语义相似度排名的前3篇PubMed摘要可提高准确性到0.23，而随机摘要则会降低准确性（0.10）。这些影响同样体现在GPT-4o-mini上，突显出来源清晰度和有针对性的检索性能的重要性。", "conclusion": "研究结果表明，尽管存在局限性，大型语言模型在基于证据的临床问答方面具有巨大潜力。检索增强的提示法是提高事实准确性和对源证据对齐的有效策略。然而，需要分专业的评估和问题类型以理解当前的知识获取和模型表现的具体情况。"}}
{"id": "2509.10897", "categories": ["cs.CV", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.10897", "abs": "https://arxiv.org/abs/2509.10897", "authors": ["Weiqiang Zhao", "Tianzhu Liu", "Yuzhe Gui", "Yanfeng Gu"], "title": "Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System", "comment": null, "summary": "Spectral imaging technology has long-faced fundamental challenges in\nbalancing spectral, spatial, and temporal resolutions. While compressive\nsensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this\ntrade-off through optical encoding, high compression ratios result in ill-posed\nreconstruction problems. Traditional model-based methods exhibit limited\nperformance due to reliance on handcrafted inherent image priors, while deep\nlearning approaches are constrained by their black-box nature, which\ncompromises physical interpretability. To address these limitations, we propose\na dual-camera CASSI reconstruction framework that integrates total variation\n(TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical\nmodel, we reduce the computational complexity of solving the inverse problem\nand provide a mathematically well-founded framework for analyzing multi-camera\nsystems. A dynamic regularization strategy is introduced, incorporating\nnormalized gradient constraints from RGB/panchromatic-derived reference images,\nwhich constructs a TV subgradient similarity function with strict convex\noptimization guarantees. Leveraging spatial priors from auxiliary cameras, an\nadaptive reference generation and updating mechanism is designed to provide\nsubgradient guidance. Experimental results demonstrate that the proposed method\neffectively preserves spatial-spectral structural consistency. The theoretical\nframework establishes an interpretable mathematical foundation for\ncomputational spectral imaging, demonstrating robust performance across diverse\nreconstruction scenarios. The source code is available at\nhttps://github.com/bestwishes43/ADMM-TVDS.", "AI": {"tldr": "本研究提出了一种结合全变差（TV）子梯度理论的双摄像头CASSI重建框架，改进了传统的模型依赖手工图像先验的方法和深度学习方法的黑盒特性。通过动态正则化策略和自适应参考生成机制，该框架提供了严格的数学基础和优化保证，提高了图像的空间-光谱结构连贯性。", "motivation": "本研究旨在解决传统基于模型的方法和深度学习方法中存在的光谱、空间和时间分辨率之间的平衡问题。这些问题源于压缩感知理论带来的冗余表达问题以及深度学习方法的黑盒属性和物理可解释性的缺乏。", "method": "本研究提出了一种结合全变差（TV）子梯度理论的双摄像头编码孔径快照光谱成像（CASSI）重建框架。通过建立端到端SD-CASSI数学模型，该框架降低了求解逆问题的计算复杂性，并为多摄像头系统的分析提供了坚实的数学基础。此外，还引入了一种动态正则化策略，该策略采用从RGB/全色导出的参考图像中的归一化梯度约束，构造成一个具有严格凸优化保证的TV子梯度相似函数。依据辅助相机提供的空间先验，设计了一个自适应参考生成和更新机制，提供了子梯度指导。", "result": "实验结果表明，该方法能够有效保存空间-光谱结构的连贯性，并且该理论框架为计算光谱成像提供了一个可解释的数学基础，在多种重建场景中展示了强大的性能。", "conclusion": "研究展示了通过引入双摄像头系统和动态正则化策略来改进CASSI重建的效果。最终，该研究工作不仅提高了成像质量，还为计算光谱成像提供了一个坚实的数学分析框架。"}}
{"id": "2509.10844", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10844", "abs": "https://arxiv.org/abs/2509.10844", "authors": ["Yixuan Tang", "Yi Yang"], "title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings", "comment": "https://github.com/yixuantt/GAPrune", "summary": "Domain-specific embedding models have shown promise for applications that\nrequire specialized semantic understanding, such as coding agents and financial\nretrieval systems, often achieving higher performance gains than general\nmodels. However, state-of-the-art embedding models are typically based on LLMs,\nwhich contain billions of parameters, making deployment challenging in\nresource-constrained environments. Model compression through pruning offers a\npromising solution, but existing pruning methods treat all parameters\nuniformly, failing to distinguish between general semantic representations and\ndomain-specific patterns, leading to suboptimal pruning decisions. Thus, we\npropose GAPrune, a pruning framework that addresses this challenge by\nconsidering both domain importance and preserving general linguistic\nfoundation. Our method uses Fisher Information to measure importance and\ngeneral-domain gradient alignment to assess parameter behavior, then combines\nthese signals using our Domain Alignment Importance (DAI) scoring. Lower DAI\nscores indicate that the parameter is either less important for the domain task\nor creates conflicts between domain and general objectives. Experiments on two\ndomain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance\nwithin 2.5% of dense models in one-shot pruning at 50% sparsity, while\noutperforming all baselines. With retraining in 100 steps, GAPrune achieves\n+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our\npruning strategy not only preserves but enhances domain-specific capabilities.\nOur findings demonstrate that principled pruning strategies can achieve model\ncompression and enhanced domain specialization, providing the research\ncommunity with a new approach for development.", "AI": {"tldr": "Propose GAPrune, a pruning framework that considers domain importance and general linguistic foundation in large models, demonstrating better performance and domain specialization after pruning compared to baselines.", "motivation": "State-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions.", "method": "We propose GAPrune, a pruning framework that considers both domain importance and preserves general linguistic foundation. It uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring.", "result": "Experiments on FinMTEB and ChemTEB show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining, GAPrune achieves +4.51% on FinMTEB and +1.73% on ChemTEB.", "conclusion": "Experiments on two domain benchmarks show that GAPrune maintains or even enhances domain-specific capabilities after pruning, demonstrating the effectiveness of our approach for model compression and enhanced domain specialization."}}
{"id": "2509.10919", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10919", "abs": "https://arxiv.org/abs/2509.10919", "authors": ["Mohanad Albughdadi"], "title": "Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation", "comment": null, "summary": "Recent advances in Earth Observation have focused on large-scale foundation\nmodels. However, these models are computationally expensive, limiting their\naccessibility and reuse for downstream tasks. In this work, we investigate\ncompact architectures as a practical pathway toward smaller general-purpose EO\nmodels. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder\n(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing\nwith geo-temporal conditioning, incorporating imagery alongside\nlatitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE\non the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen\nencoder using linear probes. Despite its small size, the model competes with\nmuch larger architectures, demonstrating that metadata-aware pretraining\nimproves transfer and label efficiency. To further assess generalization, we\nevaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and\nstill observe competitive performance compared to models with hundreds of\nmillions of parameters. These results suggest that compact, metadata-aware\nMoE-MAEs are an efficient and scalable step toward future EO foundation models.", "AI": {"tldr": "本文研究了紧凑的EO模型架构，提出了一种只有2.5M参数的模型，其在多个数据集上的表现优于或可与大型模型相媲美，强调了元数据作用和小规模模型的潜力。", "motivation": "随着地球观测领域的进步，大型基础模型受到了关注。然而，这些模型计算成本高，限制了它们在下游任务中的可访问性和重用性。本工作研究了紧凑架构作为向更小通用EO模型的一种实际途径。", "method": "提出了一种带有元数据感知混合专家掩码自动编码器(MoE-MAE)，只有2.5M参数。该模型结合了稀疏专家路由与地理时间校准，结合图像以及纬度/经度和季节/日循环编码。", "result": "在BigEarthNet-Landsat数据集上预训练MoE-MAE，并对其冻结编码器的嵌入使用线性探针进行评估。尽管模型规模小，它与更大模型表现相当，表明元数据感知预训练可以提高迁移和标签效率。在缺乏明确元数据的EuroSAT-Landsat数据集上，仍观察到与数亿参数模型相当的性能。", "conclusion": "结果表明，紧凑的、元数据感知的MoE-MAE是未来EO基础模型高效和可扩展步骤。"}}
{"id": "2509.10845", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.10845", "abs": "https://arxiv.org/abs/2509.10845", "authors": ["Liqian Feng", "Lintao Wang", "Kun Hu", "Dehui Kong", "Zhiyong Wang"], "title": "Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production", "comment": null, "summary": "Sign language production (SLP) aims to translate spoken language sentences\ninto a sequence of pose frames in a sign language, bridging the communication\ngap and promoting digital inclusion for deaf and hard-of-hearing communities.\nExisting methods typically rely on gloss, a symbolic representation of sign\nlanguage words or phrases that serves as an intermediate step in SLP. This\nlimits the flexibility and generalization of SLP, as gloss annotations are\noften unavailable and language-specific. Therefore, we present a novel\ndiffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for\ngloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed\nto generate sign language sequences from noisy latent sign codes and spoken\ntext jointly, reducing the potential error accumulation through a\nnon-autoregressive iterative denoising process. We also design a cross-modal\nsigning aligner that learns a shared latent space to bridge visual and textual\ncontent in sign and spoken languages. This alignment supports the conditioned\ndiffusion-based process, enabling more accurate and contextually relevant sign\nlanguage generation without gloss. Extensive experiments on the commonly used\nPHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,\nachieving the state-of-the-art performance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.10961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10961", "abs": "https://arxiv.org/abs/2509.10961", "authors": ["Farhan Sadik", "Christopher L. Newman", "Stuart J. Warden", "Rachel K. Surowiec"], "title": "Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging", "comment": null, "summary": "Rigid-motion artifacts, such as cortical bone streaking and trabecular\nsmearing, hinder in vivo assessment of bone microstructures in high-resolution\nperipheral quantitative computed tomography (HR-pQCT). Despite various motion\ngrading techniques, no motion correction methods exist due to the lack of\nstandardized degradation models. We optimize a conventional sinogram-based\nmethod to simulate motion artifacts in HR-pQCT images, creating paired datasets\nof motion-corrupted images and their corresponding ground truth, which enables\nseamless integration into supervised learning frameworks for motion correction.\nAs such, we propose an Edge-enhanced Self-attention Wasserstein Generative\nAdversarial Network with Gradient Penalty (ESWGAN-GP) to address motion\nartifacts in both simulated (source) and real-world (target) datasets. The\nmodel incorporates edge-enhancing skip connections to preserve trabecular edges\nand self-attention mechanisms to capture long-range dependencies, facilitating\nmotion correction. A visual geometry group (VGG)-based perceptual loss is used\nto reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean\nsignal-to-noise ratio (SNR) of 26.78, structural similarity index measure\n(SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source\ndataset, while showing improved performance on the target dataset with an SNR\nof 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a\nsimplified representation of real-world motion that may not fully capture the\ncomplexity of in vivo motion artifacts. Nevertheless, because motion artifacts\npresent one of the foremost challenges to more widespread adoption of this\nmodality, these methods represent an important initial step toward implementing\ndeep learning-based motion correction in HR-pQCT.", "AI": {"tldr": "Develop a method to simulate and correct motion artifacts in HR-pQCT using machine learning, improving image quality significantly.", "motivation": "Rigid-motion artifacts hinder in vivo assessment of bone microstructures in HR-pQCT, no motion correction methods exist due to the lack of standardized degradation models", "method": "optimize a conventional sinogram-based method to simulate motion artifacts in HR-pQCT images, Edge-enhanced Self-attention Wasserstein Generative Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion artifacts", "result": "achieves a mean SNR of 26.78, SSIM of 0.81, VIF of 0.76 for the source dataset, and SNR of 29.31, SSIM of 0.87, VIF of 0.81 for the target dataset", "conclusion": "the methods represent an important initial step toward implementing deep learning-based motion correction in HR-pQCT despite not fully capturing the complexity of in vivo motion artifacts"}}
{"id": "2509.10847", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10847", "abs": "https://arxiv.org/abs/2509.10847", "authors": ["Xiaohui Rao", "Hanlin Wu", "Zhenguang G. Cai"], "title": "A funny companion: Distinct neural responses to perceived AI- versus human- generated humor", "comment": null, "summary": "As AI companions become capable of human-like communication, including\ntelling jokes, understanding how people cognitively and emotionally respond to\nAI humor becomes increasingly important. This study used electroencephalography\n(EEG) to compare how people process humor from AI versus human sources.\nBehavioral analysis revealed that participants rated AI and human humor as\ncomparably funny. However, neurophysiological data showed that AI humor\nelicited a smaller N400 effect, suggesting reduced cognitive effort during the\nprocessing of incongruity. This was accompanied by a larger Late Positive\nPotential (LPP), indicating a greater degree of surprise and emotional\nresponse. This enhanced LPP likely stems from the violation of low initial\nexpectations regarding AI's comedic capabilities. Furthermore, a key temporal\ndynamic emerged: human humor showed habituation effects, marked by an\nincreasing N400 and a decreasing LPP over time. In contrast, AI humor\ndemonstrated increasing processing efficiency and emotional reward, with a\ndecreasing N400 and an increasing LPP. This trajectory reveals how the brain\ncan dynamically update its predictive model of AI capabilities. This process of\ncumulative reinforcement challenges \"algorithm aversion\" in humor, as it\ndemonstrates how cognitive adaptation to AI's language patterns can lead to an\nintensified emotional reward. Additionally, participants' social attitudes\ntoward AI modulated these neural responses, with higher perceived AI\ntrustworthiness correlating with enhanced emotional engagement. These findings\nindicate that the brain responds to AI humor with surprisingly positive and\nintense reactions, highlighting humor's potential for fostering genuine\nengagement in human-AI social interaction.", "AI": {"tldr": "研究使用EEG技术对比人类和AI幽默的处理方式，发现虽然参与者对AI和人类幽默的评价相似，但AI幽默引发了更少的认知努力和更多的惊喜情绪反应。", "motivation": "随着AI同伴的交流能力越来越接近人类，研究人们如何认知和情感上回应AI的幽默能力变得越来越重要。", "method": "研究使用EEG技术比较人们对来自AI和人类的幽默的处理方式。", "result": "行为分析显示参与者认为AI和人类的幽默一样有趣，但神经生理学数据显示AI幽默引发了较少的认知努力和更多的惊喜情绪反应。", "conclusion": "研究结果揭示了大脑如何动态更新对AI能力的预测模型，以及在这种情况下情绪奖励的增强，表明大脑对AI幽默有积极且强烈的反应。"}}
