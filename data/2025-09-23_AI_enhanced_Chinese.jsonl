{"id": "2509.16226", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16226", "abs": "https://arxiv.org/abs/2509.16226", "authors": ["Brian S. Lin", "Jiaxin Yuan", "Zihan Zhou", "Shouli Wang", "Shuo Wang", "Cunliang Kong", "Qi Shi", "Yuxuan Li", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "title": "On LLM-Based Scientific Inductive Reasoning Beyond Equations", "comment": "24 pages", "summary": "As large language models (LLMs) increasingly exhibit human-like capabilities,\na fundamental question emerges: How can we enable LLMs to learn the underlying\npatterns from limited examples in entirely novel environments and apply them\neffectively? This question is central to the ability of LLMs in inductive\nreasoning. Existing research on LLM-based inductive reasoning can be broadly\ncategorized based on whether the underlying rules are expressible via explicit\nmathematical equations. However, many recent studies in the beyond-equations\ncategory have emphasized rule design without grounding them in specific\nscenarios. Inspired by the parallels between inductive reasoning and human\nscientific discovery, we propose the task of LLM-Based Scientific Inductive\nReasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to\nevaluate the inductive reasoning abilities of LLMs in scientific settings. Our\nexperimental results show that current LLMs still struggle with this task,\nunderscoring its difficulty and the need for further advancement in this area.", "AI": {"tldr": "本研究关注LLMs在全新环境下的归纳推理能力，提出科学归纳推理任务和基准，实验结果显示当前LLMs在这方面的挑战。", "motivation": "旨在解决现有LLMs在全新环境利用有限的例子归纳理解和应用模式的能力问题，特别是超越方程式的归纳推理能力。", "method": "提出任务LLM-Based Scientific Inductive Reasoning Beyond Equations并引入新的基准SIRBench-V1来评估LLMs在科学环境中的归纳推理能力。", "result": "实验结果显示当前的LLMs在新提出的科学归纳推理任务上仍然面临挑战。", "conclusion": "现有LLMs在科学环境中的归纳推理能力有待提高，需要在这一领域进行进一步的研究和进展。"}}
{"id": "2509.16241", "categories": ["cs.CL", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.16241", "abs": "https://arxiv.org/abs/2509.16241", "authors": ["Eishkaran Singh", "Tanav Singh Bajaj", "Siddharth Nayak"], "title": "REAMS: Reasoning Enhanced Algorithm for Maths Solving", "comment": null, "summary": "The challenges of solving complex university-level mathematics problems,\nparticularly those from MIT, and Columbia University courses, and selected\ntasks from the MATH dataset, remain a significant obstacle in the field of\nartificial intelligence. Conventional methods have consistently fallen short in\nthis domain, highlighting the need for more advanced approaches. In this paper,\nwe introduce a language-based solution that leverages zero-shot learning and\nmathematical reasoning to effectively solve, explain, and generate solutions\nfor these advanced math problems. By integrating program synthesis, our method\nreduces reliance on large-scale training data while significantly improving\nproblem-solving accuracy. Our approach achieves an accuracy of 90.15%,\nrepresenting a substantial improvement over the previous benchmark of 81% and\nsetting a new standard in automated mathematical problem-solving. These\nfindings highlight the significant potential of advanced AI methodologies to\naddress and overcome the challenges presented by some of the most complex\nmathematical courses and datasets.", "AI": {"tldr": "本文提出了一种使用零样本学习和数学推理解决高等数学问题的方法，实现了90.15%的准确率，比之前的81%有了显著提升。", "motivation": "传统的解决大学水平数学问题的方法一直表现不佳，特别是在MIT和哥伦比亚大学课程任务以及MATH数据集选定任务方面。这突出表明需要更先进的方法。", "method": "此论文介绍了一种基于语言的方法，结合零样本学习和数学推理来解决、解释和生成高等数学问题的解决方案。通过整合程序合成技术，该方法减少了对大规模训练数据的依赖并显著提高了问题解决的准确性。", "result": "该方法达到90.15%的准确率，比之前的基准81%有了显著提高。", "conclusion": "这些结果强调了先进的AI方法在解决最复杂的数学挑战方面的巨大潜力。"}}
{"id": "2509.16256", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16256", "abs": "https://arxiv.org/abs/2509.16256", "authors": ["Asiya Ibrahim Zanga", "Salisu Mamman Abdulrahman", "Abubakar Ado", "Abdulkadir Abubakar Bichi", "Lukman Aliyu Jibril", "Abdulmajid Babangida Umar", "Alhassan Adamu", "Shamsuddeen Hassan Muhammad", "Bashir Salisu Abubakar"], "title": "HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language", "comment": "Masters Thesis, a Dataset Paper", "summary": "The development of Natural Language Processing (NLP) tools for low-resource\nlanguages is critically hindered by the scarcity of annotated datasets. This\npaper addresses this fundamental challenge by introducing HausaMovieReview, a\nnovel benchmark dataset comprising 5,000 YouTube comments in Hausa and\ncode-switched English. The dataset was meticulously annotated by three\nindependent annotators, demonstrating a robust agreement with a Fleiss' Kappa\nscore of 0.85 between annotators. We used this dataset to conduct a comparative\nanalysis of classical models (Logistic Regression, Decision Tree, K-Nearest\nNeighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results\nreveal a key finding: the Decision Tree classifier, with an accuracy and\nF1-score 89.72% and 89.60% respectively, significantly outperformed the deep\nlearning models. Our findings also provide a robust baseline, demonstrating\nthat effective feature engineering can enable classical models to achieve\nstate-of-the-art performance in low-resource contexts, thereby laying a solid\nfoundation for future research.\n  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis", "AI": {"tldr": "通过创建新的Hausa评论数据集，提出经典模型决策树在低资源语言的情感分析中优于深度学习模型，强调特征工程的重要性。", "motivation": "解决低资源语言（如Hausa）NLP工具开发中缺乏标注数据的问题，提供基准数据集和研究基础。", "method": "开发了HausaMovieReview数据集，包含5000条YouTube评论，用以比较经典模型（逻辑回归、决策树、K近邻）和微调的Transformer模型（BERT和RoBERTa）的性能。", "result": "实验结果显示决策树分类器在准确率和F1得分上分别为89.72%和89.60%，显著优于深度学习模型。", "conclusion": "表明有效的特征工程能够让经典模型在低资源环境中达到前沿性能水平，为未来研究奠定基础。"}}
{"id": "2509.16264", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16264", "abs": "https://arxiv.org/abs/2509.16264", "authors": ["Wenjie Lin", "Hange Liu", "Xutao Mao", "Yingying Zhuang", "Jingwei Shi", "Xudong Han", "Tianyu Shi", "Jinrui Yang"], "title": "Gender and Political Bias in Large Language Models: A Demonstration Platform", "comment": "online demo: https://euro-parl-vote-demo.vercel.app/; Video:\n  https://www.youtube.com/@Jinrui-sf2jg", "summary": "We present ParlAI Vote, an interactive system for exploring European\nParliament debates and votes, and for testing LLMs on vote prediction and bias\nanalysis. This platform connects debate topics, speeches, and roll-call\noutcomes, and includes rich demographic data such as gender, age, country, and\npolitical group. Users can browse debates, inspect linked speeches, compare\nreal voting outcomes with predictions from frontier LLMs, and view error\nbreakdowns by demographic group. Visualizing the EuroParlVote benchmark and its\ncore tasks of gender classification and vote prediction, ParlAI Vote highlights\nsystematic performance bias in state-of-the-art LLMs. The system unifies data,\nmodels, and visual analytics in a single interface, lowering the barrier for\nreproducing findings, auditing behavior, and running counterfactual scenarios.\nIt supports research, education, and public engagement with legislative\ndecision-making, while making clear both the strengths and the limitations of\ncurrent LLMs in political analysis.", "AI": {"tldr": "ParlAI Vote是一个用于欧洲议会辩论和投票探索的交互系统，同时用于测试和展示LLMs在投票预测和偏见分析中的表现，强调系统性性能偏差，并支持研究、教育和公众参与。", "motivation": "目的是提供一个工具，使用户能够测试和分析LLMs在欧洲议会投票预测和偏见分析中的表现，并提供丰富的数据支持以促进研究和公众理解。", "method": "ParlAI Vote是一个交互式系统，用于探索欧洲议会辩论和投票情况，并测试LLMs在投票预测和偏见分析上的能力。该系统将辩论主题、演讲和投票结果联系起来，并包括丰富的人口统计数据如性别、年龄、国家和地区政治团体。用户可以浏览辩论，查看链接的演讲，比较真实投票结果与前沿LLMs的预测，并查看按人口统计组划分的错误细分。", "result": "通过可视化EuroParlVote基准及其核心任务，即性别分类和投票预测，ParlAI Vote强调了现有前沿LLMs的系统性性能偏差。该系统统一了数据、模型和可视分析，并提供单一接口，降低了再现研究结果、审计行为和运行反事实场景的门槛。", "conclusion": "ParlAI Vote支持了研究、教育和公众参与立法决策，并明确表达了当前LLMs在政治分析上的优势和局限。"}}
{"id": "2509.16221", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16221", "abs": "https://arxiv.org/abs/2509.16221", "authors": ["Martin Preiß"], "title": "Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement", "comment": null, "summary": "For the bachelor project 2021 of Professor Lippert's research group,\nhandwritten entries of historical patient records needed to be digitized using\nOptical Character Recognition (OCR) methods. Since the data will be used in the\nfuture, a high degree of accuracy is naturally required. Especially in the\nmedical field this has even more importance. Ensemble Learning is a method that\ncombines several machine learning models and is claimed to be able to achieve\nan increased accuracy for existing methods. For this reason, Ensemble Learning\nin combination with OCR is investigated in this work in order to create added\nvalue for the digitization of the patient records. It was possible to discover\nthat ensemble learning can lead to an increased accuracy for OCR, which methods\nwere able to achieve this and that the size of the training data set did not\nplay a role here.", "AI": {"tldr": "本文通过结合集成学习与OCR技术提高手写历史病历数字化的准确度，发现集成学习可以提高OCR准确度，且与训练数据集大小无关。", "motivation": "因为数字化内容将在未来被使用，所以需要很高的精度。特别是在医学领域中，这一点更加重要，因此需要探索一种能够提高OCR准确度的方法。", "method": "本文探讨了通过结合OCR技术使用集成学习方法来提高历史病历手写条目数字化的精度。该研究意在通过集成学习提升现有的OCR方法的准确度，以期为病历数字化提供更高价值的方法。", "result": "研究结果显示，集成学习确实能够提高OCR的准确度，并且能够发现哪些方法能够实现这一点。同时，研究也表明训练数据集的大小对此影响不大。", "conclusion": "集成学习结合OCR技术是提高历史病历手写条目数字化精度的有效方法。这种方法在医学领域应用具有更高的重要性和实用性。"}}
{"id": "2509.16278", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16278", "abs": "https://arxiv.org/abs/2509.16278", "authors": ["Alok N. Shah", "Khush Gupta", "Keshav Ramji", "Pratik Chaudhari"], "title": "Language Modeling with Learned Meta-Tokens", "comment": null, "summary": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization.", "AI": {"tldr": "研究通过预训练过程引入元标记和元注意力机制，改进了基于GPT-2架构的语言模型，结果表明这种方法能够显著提升模型在处理长距离依赖关系上的表现。", "motivation": "尽管现代基于转换器（Transformer）的语言模型在多任务泛化方面取得了重大成功，但往往难以捕捉到上下文窗口内的长距离相关性。因此，本研究提出了一种新颖的方法来解决这一问题。", "method": "本研究采用了一种新颖的方法，利用元标记（meta-tokens）和专门的元注意力机制来指导语言模型使用这些标记。在此基础上，使用了一种修改过的GPT-2架构进行了预训练，并且研究了这些标记在一系列合成任务上的影响。", "result": "研究发现，在预训练时使用少于100B标记的元标记和元注意力机制能够显著提升任务上的性能。通过观察模型内部以及使用信息论对压缩质量进行分析，进一步证实了这些行为。", "conclusion": "预训练中使用元标记能够提供一种简单且数据效率高的方法来提升长上下文语言模型的性能，并提出了一些关于长度泛化的模型行为的新见解。"}}
{"id": "2509.16343", "categories": ["cs.CV", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.16343", "abs": "https://arxiv.org/abs/2509.16343", "authors": ["Chung-En", "Yu", "Brian Jalaian", "Nathaniel D. Bastian"], "title": "Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute", "comment": null, "summary": "Developing trustworthy intelligent vision systems for high-stakes domains,\n\\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness\nwithout costly retraining. We propose \\textbf{Visual Reasoning Agent (VRA)}, a\ntraining-free, agentic reasoning framework that wraps off-the-shelf\nvision-language models \\emph{and} pure vision systems in a\n\\emph{Think--Critique--Act} loop. While VRA incurs significant additional\ntest-time computation, it achieves up to 40\\% absolute accuracy gains on\nchallenging visual reasoning benchmarks. Future work will optimize query\nrouting and early stopping to reduce inference overhead while preserving\nreliability in vision tasks.", "AI": {"tldr": "VRA无需重新训练，通过代理推理框架提高了智能视觉系统在高风险领域的鲁棒性，测试时计算成本较高，但在视觉推理基准上取得了显着的准确度提升。", "motivation": "开发高可信度的智能视觉系统的需求，尤其是在高风险领域如遥感和医学诊断当中，要求系统具备广泛的鲁棒性而无需频繁的昂贵重训。", "method": "提出Visual Reasoning Agent (VRA)，一种无需训练、具有代理推理功能的框架，它通过“思考-批评-行动”循环来封装现有的视觉语言模型和纯视觉系统。", "result": "该论文介绍了一个名为视觉推理代理（VRA）的无需训练的代理推理框架，它能够在无需重新训练的情况下提高智能视觉系统的鲁棒性，特别是在高风险领域如遥感和医学诊断中。VRA通过“思考-批评-行动”循环封装了现成的视觉语言模型和纯视觉系统。尽管VRA在测试时计算成本较高，但其在具有挑战性的视觉推理基准上实现了高达40%的绝对准确度提升。未来的优化工作将集中在减少推理开销的同时保持可靠性上。", "conclusion": "尽管VRA框架在测试时计算成本较高，但它大幅提升了视觉推理任务中的准确度。未来将致力于优化查询路由和提前停止机制，以减少推理开销同时保持系统的可靠性。"}}
{"id": "2509.16325", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16325", "abs": "https://arxiv.org/abs/2509.16325", "authors": ["Andrew Zhu", "Chris Callison-Burch"], "title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap", "comment": "8 pages, 1 figure", "summary": "Imagine AI assistants that enhance conversations without interrupting them:\nquietly providing relevant information during a medical consultation,\nseamlessly preparing materials as teachers discuss lesson plans, or\nunobtrusively scheduling meetings as colleagues debate calendars. While modern\nconversational LLM agents directly assist human users with tasks through a chat\ninterface, we study this alternative paradigm for interacting with LLM agents,\nwhich we call \"overhearing agents.\" Rather than demanding the user's attention,\noverhearing agents continuously monitor ambient activity and intervene only\nwhen they can provide contextual assistance. In this paper, we present the\nfirst analysis of overhearing LLM agents as a distinct paradigm in human-AI\ninteraction and establish a taxonomy of overhearing agent interactions and\ntasks grounded in a survey of works on prior LLM-powered agents and exploratory\nHCI studies. Based on this taxonomy, we create a list of best practices for\nresearchers and developers building overhearing agent systems. Finally, we\noutline the remaining research gaps and reveal opportunities for future\nresearch in the overhearing paradigm.", "AI": {"tldr": "本文探讨了一种新的AI交互方式——旁听代理，该代理通过监听环境并提供情境协助而非直接吸引用户注意来进行任务辅助。", "motivation": "动机在于提出一种无需直接与用户交互就能提供帮助的新模式，减少对用户的干扰，提高效率和体验。", "method": "建立旁听代理交互模式和任务的分类，通过调查前期的LLM代理工作和HCI研究来构建分类系统，并形成一套研究与开发最佳实践。", "result": "创建了旁听代理系统的交互分类系统及其任务清单，提供了针对该模式的研究及开发最佳实践。", "conclusion": "展示了旁听代理模式的独特价值，并指出了未来在此范式下的研究方向。"}}
{"id": "2509.16346", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16346", "abs": "https://arxiv.org/abs/2509.16346", "authors": ["Juan Castorena", "E. Louise Loudermilk", "Scott Pokswinski", "Rodman Linn"], "title": "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR", "comment": null, "summary": "The 3D structure of living and non-living components in ecosystems plays a\ncritical role in determining ecological processes and feedbacks from both\nnatural and human-driven disturbances. Anticipating the effects of wildfire,\ndrought, disease, or atmospheric deposition depends on accurate\ncharacterization of 3D vegetation structure, yet widespread measurement remains\nprohibitively expensive and often infeasible. We introduce ForestGen3D, a novel\ngenerative modeling framework that synthesizes high-fidelity 3D forest\nstructure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on\nconditional denoising diffusion probabilistic models (DDPMs) trained on\nco-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate\nTLS-like 3D point clouds conditioned on sparse ALS observations, effectively\nreconstructing occluded sub-canopy detail at scale. To ensure ecological\nplausibility, we introduce a geometric containment prior based on the convex\nhull of ALS observations and provide theoretical and empirical guarantees that\ngenerated structures remain spatially consistent. We evaluate ForestGen3D at\ntree, plot, and landscape scales using real-world data from mixed conifer\necosystems, and show that it produces high-fidelity reconstructions that\nclosely match TLS references in terms of geometric similarity and biophysical\nmetrics, such as tree height, DBH, crown diameter and crown volume.\nAdditionally, we demonstrate that the containment property can serve as a\npractical proxy for generation quality in settings where TLS ground truth is\nunavailable. Our results position ForestGen3D as a scalable tool for ecological\nmodeling, wildfire simulation, and structural fuel characterization in ALS-only\nenvironments.", "AI": {"tldr": "ForestGen3D, a novel generative model, synthesizes high-fidelity 3D forest structures using only aerial LiDAR inputs, offering a scalable solution for ecological modeling, wildfire simulation, and structural fuel characterization.", "motivation": "Accurate characterization of 3D vegetation structure is crucial for understanding ecological processes affected by natural and human-driven disturbances, but widespread measurement is expensive and often infeasible.", "method": "The method involves using conditional denoising diffusion probabilistic models (DDPMs) trained on co-registered ALS/TLS data to generate TLS-like 3D point clouds conditioned on sparse ALS observations, with a geometric containment prior ensuring spatial consistency.", "result": "ForestGen3D was evaluated at various scales and demonstrated high-fidelity reconstructions closely matching TLS references in terms of biophysical metrics.", "conclusion": "ForestGen3D is positioned as a scalable tool for ecological modeling and wildfire simulation, providing a practical solution for 3D forest structure assessment in ALS-only environments."}}
{"id": "2509.16326", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16326", "abs": "https://arxiv.org/abs/2509.16326", "authors": ["Yunsoo Kim", "Michal W. S. Ong", "Alex Shavick", "Honghan Wu", "Adam P. Levine"], "title": "HARE: an entity and relation centric evaluation framework for histopathology reports", "comment": "Accepted to EMNLP2025 Findings", "summary": "Medical domain automated text generation is an active area of research and\ndevelopment; however, evaluating the clinical quality of generated reports\nremains a challenge, especially in instances where domain-specific metrics are\nlacking, e.g. histopathology. We propose HARE (Histopathology Automated Report\nEvaluation), a novel entity and relation centric framework, composed of a\nbenchmark dataset, a named entity recognition (NER) model, a relation\nextraction (RE) model, and a novel metric, which prioritizes clinically\nrelevant content by aligning critical histopathology entities and relations\nbetween reference and generated reports. To develop the HARE benchmark, we\nannotated 813 de-identified clinical diagnostic histopathology reports and 652\nhistopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific\nentities and relations. We fine-tuned GatorTronS, a domain-adapted language\nmodel to develop HARE-NER and HARE-RE which achieved the highest overall\nF1-score (0.915) among the tested models. The proposed HARE metric outperformed\ntraditional metrics including ROUGE and Meteor, as well as radiology metrics\nsuch as RadGraph-XL, with the highest correlation and the best regression to\nexpert evaluations (higher than the second best method, GREEN, a large language\nmodel based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\\rho\n= 0.161$, Kendall $\\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release\nHARE, datasets, and the models at https://github.com/knowlab/HARE to foster\nadvancements in histopathology report generation, providing a robust framework\nfor improving the quality of reports.", "AI": {"tldr": "论文提出了HARE，一个新颖的组织病理学报告自动评估框架，通过名称实体识别和关系抽取方法，提高了报告的临床质量评估。", "motivation": "在医学领域，自动化文本生成是一个活跃的研究和发展领域；然而，评估生成报告的临床质量仍然是一个挑战，尤其是在特定领域缺乏度量标准的情况下，如组织病理学。", "method": "该论文提出了HARE框架，包括基准数据集、实体识别模型（NER）、关系抽取模型（RE）和一个新颖的评估指标。HARE框架旨在通过将参考报告和生成报告之间的关键病理学实体和关系对齐来优先考虑临床上相关的内容。", "result": "HARE-NER和HARE-RE在测试模型中实现了最高的F1得分0.915。HARE度量在与专家评估的关联性和回归效果上优于其他方法，提高了对报告质量的评估。", "conclusion": "HARE度量表现优于传统的ROUGE和Meteor度量，以及放射学度量如RadGraph-XL，与专家评估的相关性更高，回归效果更好。"}}
{"id": "2509.16363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16363", "abs": "https://arxiv.org/abs/2509.16363", "authors": ["Hrishikesh Sharma"], "title": "Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution", "comment": null, "summary": "The problem of image data generation in computer vision has traditionally\nbeen a harder problem to solve, than discriminative problems. Such data\ngeneration entails placing relevant objects of appropriate sizes each, at\nmeaningful location in a scene canvas. There have been two classes of popular\napproaches to such generation: graphics based, and generative models-based.\nOptimization problems are known to lurk in the background for both these\nclasses of approaches. In this paper, we introduce a novel, practically useful\nmanifestation of the classical Bin Packing problem in the context of generation\nof synthetic image data. We conjecture that the newly introduced problem,\nResizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide\ndetailed arguments about our conjecture. As a first solution, we present a\nnovel heuristic algorithm that is generic enough and therefore scales and packs\narbitrary number of arbitrary-shaped regions at arbitrary locations, into an\nimage canvas. The algorithm follows greedy approach to iteratively pack region\npairs in a careful way, while obeying the optimization constraints. The\nalgorithm is validated by an implementation that was used to generate a\nlarge-scale synthetic anomaly detection dataset, with highly varying degree of\nbin packing parameters per image sample i.e. RARP instance. Visual inspection\nof such data and checking of the correctness of each solution proves the\neffectiveness of our algorithm. With generative modeling being on rise in deep\nlearning, and synthetic data generation poised to become mainstream, we expect\nthat the newly introduced problem will be valued in the imaging scientific\ncommunity.", "AI": {"tldr": "本文针对计算机视觉中的图像数据生成问题，提出了Resizable Anchored Region Packing (RARP) 问题，并设计了一种新的启发式算法来解决它。", "motivation": "本文研究了在计算机视觉中的图像数据生成问题，指出这一问题相较于判别性问题更难解决。为此，本文引入了一个新的、具有实际应用价值的古典装箱问题形式，具体到合成图像数据生成的背景下。", "method": "本文提出了一种新型的启发式算法，用于在图像画布中任意位置迭代地打包任意数量和任意形状的区域，同时遵守优化约束。该算法具有广泛适用性。", "result": "本文算法通过实现生成了大规模的合成异常检测数据集，其中每个图像样本的装箱参数都有很大差异。通过目视检查数据和验证每个方案的正确性，证明了该算法的有效性。", "conclusion": "随着生成模型在深度学习中的兴起以及合成数据生成变得更加主流，本文期望新提出的问题将在图像科研界得到重视。"}}
{"id": "2509.16360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16360", "abs": "https://arxiv.org/abs/2509.16360", "authors": ["Weikang Qiu", "Tinglin Huang", "Ryan Rullo", "Yucheng Kuang", "Ali Maatouk", "S. Raquel Ramos", "Rex Ying"], "title": "RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering", "comment": "ACM KDD Health Track 2025 Blue Sky Best Paper", "summary": "Large Language Models (LLMs) hold promise in addressing complex medical\nproblems. However, while most prior studies focus on improving accuracy and\nreasoning abilities, a significant bottleneck in developing effective\nhealthcare agents lies in the readability of LLM-generated responses,\nspecifically, their ability to answer public health problems clearly and simply\nto people without medical backgrounds. In this work, we introduce RephQA, a\nbenchmark for evaluating the readability of LLMs in public health question\nanswering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across\n13 topics, and includes a proxy multiple-choice task to assess informativeness,\nalong with two readability metrics: Flesch-Kincaid grade level and professional\nscore. Evaluation of 25 LLMs reveals that most fail to meet readability\nstandards, highlighting a gap between reasoning and effective communication. To\naddress this, we explore four readability-enhancing strategies-standard\nprompting, chain-of-thought prompting, Group Relative Policy Optimization\n(GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best\nresults, advancing the development of more practical and user-friendly public\nhealth agents. These results represent a step toward building more practical\nagents for public health.", "AI": {"tldr": "此研究提出了RephQA，用于评估LLM生成的公共健康问答的可读性，发现大多数模型不符合可读性标准，并探索了四种提升可读性的策略。", "motivation": "大多数先前研究主要集中在提高准确性与推理能力，但医疗代理在回答公共健康问题时的可读性同样重要，即需要能够清晰简洁地回答给非专业背景的人。", "method": "开发了一个涉及13个主题、27个来源的533个专家评审问答对的基准测试RephQA，以及两个可读性度量：Flesch-Kincaid年级水平和专业评分。并且评估了25种LLM，尝试了四种提升可读性的策略。", "result": "评估结果显示大多数LLM未能达到可读性标准。通过探索四种策略，发现令牌适应型GRPO取得了最佳效果。", "conclusion": "该研究展示了数据标准和可衡量方法的重要性，并表明令牌适应型GRPO可改进公共健康代理的实用性和易用性，有助于开发更加实际和用户友好的公共健康代理。"}}
{"id": "2509.16382", "categories": ["cs.CV", "cs.LG", "eess.IV", "I.2.1; I.5.2"], "pdf": "https://arxiv.org/pdf/2509.16382", "abs": "https://arxiv.org/abs/2509.16382", "authors": ["Saurabh Saini", "Kapil Ahuja", "Marc C. Steinbach", "Thomas Wick"], "title": "Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor", "comment": "15 Pages, 7 Figures, 5 Tables", "summary": "In this study, we develop a new CAD system for accurate thyroid cancer\nclassification with emphasis on feature extraction. Prior studies have shown\nthat thyroid texture is important for segregating the thyroid ultrasound images\ninto different classes. Based upon our experience with breast cancer\nclassification, we first conjuncture that the Discrete Cosine Transform (DCT)\nis the best descriptor for capturing textural features. Thyroid ultrasound\nimages are particularly challenging as the gland is surrounded by multiple\ncomplex anatomical structures leading to variations in tissue density. Hence,\nwe second conjuncture the importance of localization and propose that the Local\nDCT (LDCT) descriptor captures the textural features best in this context.\nAnother disadvantage of complex anatomy around the thyroid gland is scattering\nof ultrasound waves resulting in noisy and unclear textures. Hence, we third\nconjuncture that one image descriptor is not enough to fully capture the\ntextural features and propose the integration of another popular texture\ncapturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is\nknown to be noise resilient as well. We term our novel descriptor as Binary\nPattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification\nis carried out using a non-linear SVM. The proposed CAD system is evaluated on\nthe only two publicly available thyroid cancer datasets, namely TDID and AUITD.\nThe evaluation is conducted in two stages. In Stage I, thyroid nodules are\ncategorized as benign or malignant. In Stage II, the malignant cases are\nfurther sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I\nclassification, our proposed model demonstrates exceptional performance of\nnearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed\nmodel again attains excellent classification of close to 100% on TDID and 99%\non AUITD.", "AI": {"tldr": "本文开发了一种新的CAD系统，专用于甲状腺癌的准确分类。系统采用了BPD-LDCT描述符进行特征提取，并使用非线性SVM进行最终分类，取得了非常高的分类准确率。", "motivation": "基于此前对乳腺癌分类的经验，并考虑到甲状腺周围复杂结构带来的挑战，本研究目的是提出一种更好的特征描述符来提升对甲状腺癌分类的准确性。", "method": "采用LDCT和ILBP两种纹理捕获描述符来提高特征提取的准确性，并结合非线性SVM进行分类。", "result": "该CAD系统在两个公开数据集（TDID和AUITD）上进行了评估，表现出几乎完美的分类性能，特别是在对恶性肿瘤的进一步分类（Stage II）上显示出近100%的准确率。", "conclusion": "BPD-LDCT描述符在甲状腺癌分类中展现出了强大的性能，并有助于推动在医疗影像处理领域的技术发展。"}}
{"id": "2509.16375", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16375", "abs": "https://arxiv.org/abs/2509.16375", "authors": ["Cihan Xiao", "Matthew Wiesner", "Debashish Chakraborty", "Reno Kriz", "Keith Cunningham", "Kenton Murray", "Kevin Duh", "Luis Tavarez-Arce", "Paul McNamee", "Sanjeev Khudanpur"], "title": "Whisper-UT: A Unified Translation Framework for Speech and Text", "comment": "EMNLP 2025 Main Conference", "summary": "Encoder-decoder models have achieved remarkable success in speech and text\ntasks, yet efficiently adapting these models to diverse uni/multi-modal\nscenarios remains an open challenge. In this paper, we propose Whisper-UT, a\nunified and efficient framework that leverages lightweight adapters to enable\nseamless adaptation across tasks, including a multi-modal machine translation\n(MMT) task that explicitly conditions translation on both speech and source\nlanguage text inputs. By incorporating ASR hypotheses or ground-truth\ntranscripts as prompts, this approach not only enables the system to process\nboth modalities simultaneously but also enhances speech translation (ST)\nperformance through a 2-stage decoding strategy. We demonstrate our methods\nusing the Whisper model, though in principle they are general and could be\napplied to similar multitask models. We highlight the effectiveness of\ncross-modal and cross-task fine-tuning, which improves performance without\nrequiring 3-way parallel data. Our results underscore the flexibility,\nefficiency, and general applicability of the proposed framework for multi-modal\ntranslation.", "AI": {"tldr": "本文提出了一种名为Whisper-UT的跨模态和跨任务适应框架，可以实现多模态任务中的高效处理和性能提升。", "motivation": "旨在解决将编码器-解码器模型高效地适应不同一模态或多模态场景的挑战。", "method": "提出了一种名为Whisper-UT的统一且高效的框架，该框架利用轻量级适配器实现跨任务的无缝适应，包括多模态机器翻译（MMT）任务，该任务显式地基于语音和源语言文本输入进行翻译。通过将ASR假设或真实转录作为提示，这种方法不仅使系统能够同时处理多种模态，而且还通过两阶段解码策略提高了语音翻译（ST）的性能。", "result": "展示了在不使用三路平行数据的情况下通过跨模态和跨任务微调来提高性能的方法的有效性。", "conclusion": "实验证明了所提出的框架的灵活性、效率以及在多模态翻译中的广泛适用性。"}}
{"id": "2509.16415", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16415", "abs": "https://arxiv.org/abs/2509.16415", "authors": ["Zhengri Wu", "Yiran Wang", "Yu Wen", "Zeyu Zhang", "Biao Wu", "Hao Tang"], "title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes", "comment": null, "summary": "Underwater stereo depth estimation provides accurate 3D geometry for robotics\ntasks such as navigation, inspection, and mapping, offering metric depth from\nlow-cost passive cameras while avoiding the scale ambiguity of monocular\nmethods. However, existing approaches face two critical challenges: (i)\nparameter-efficiently adapting large vision foundation encoders to the\nunderwater domain without extensive labeled data, and (ii) tightly fusing\nglobally coherent but scale-ambiguous monocular priors with locally metric yet\nphotometrically fragile stereo correspondences. To address these challenges, we\npropose StereoAdapter, a parameter-efficient self-supervised framework that\nintegrates a LoRA-adapted monocular foundation encoder with a recurrent stereo\nrefinement module. We further introduce dynamic LoRA adaptation for efficient\nrank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to\nenhance robustness under diverse underwater conditions. Comprehensive\nevaluations on both simulated and real-world benchmarks show improvements of\n6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,\nwhile real-world deployment with the BlueROV2 robot further demonstrates the\nconsistent robustness of our approach. Code:\nhttps://github.com/AIGeeksGroup/StereoAdapter. Website:\nhttps://aigeeksgroup.github.io/StereoAdapter.", "AI": {"tldr": "Proposes StereoAdapter, a method that enhances underwater stereo depth estimation through parameter-efficient adaptation and effective fusion of monocular and stereo data, showing significant improvements over state-of-the-art methods.", "motivation": "To address the challenges of parameter-efficient adaptation of large vision foundation encoders to the underwater domain and the fusion of globally coherent monocular priors with locally metric stereo correspondences.", "method": "StereoAdapter, a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo refinement module.", "result": "Improvements of 6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods, and consistent robustness shown in real-world deployment with the BlueROV2 robot.", "conclusion": "The proposed StereoAdapter framework provides robust and precise underwater stereo depth estimation, addressing key challenges in the field with significant performance improvements over existing methods."}}
{"id": "2509.16394", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.16394", "abs": "https://arxiv.org/abs/2509.16394", "authors": ["Deuksin Kwon", "Kaleen Shrestha", "Bin Han", "Elena Hayoung Lee", "Gale Lucas"], "title": "Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans", "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "Large Language Models (LLMs) are increasingly deployed in socially complex,\ninteraction-driven tasks, yet their ability to mirror human behavior in\nemotionally and strategically complex contexts remains underexplored. This\nstudy assesses the behavioral alignment of personality-prompted LLMs in\nadversarial dispute resolution by simulating multi-turn conflict dialogues that\nincorporate negotiation. Each LLM is guided by a matched Five-Factor\npersonality profile to control for individual variation and enhance realism. We\nevaluate alignment across three dimensions: linguistic style, emotional\nexpression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the\nclosest alignment with humans in linguistic style and emotional dynamics, while\nClaude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial\nalignment gaps persist. Our findings establish a benchmark for alignment\nbetween LLMs and humans in socially complex interactions, underscoring both the\npromise and the limitations of personality conditioning in dialogue modeling.", "AI": {"tldr": "该研究通过模拟冲突对话评估了带有个性提示的大型语言模型在争端解决中的行为一致性，发现尽管某些模型在某些方面接近人类，但仍存在一致性差距。", "motivation": "大型语言模型（LLMs）越来越多地被部署在社交复杂度高的任务中，它们在情感和战略复杂环境中模仿人类行为的能力尚未得到充分探索。", "method": "通过模拟多回合冲突对话来评估个性提示的大型语言模型（LLMs）在对抗性争端解决中的行为一致性，这些对话还融入了谈判元素。每个LLM都依据匹配的五因素个性档案进行指导，以控制个体差异，增强真实感。", "result": "GPT-4.1在语言风格和情感动态上最接近人类，而Claude-3.7-Sonnet在战略行为上表现最佳。不过，仍然存在显著的行为一致性差距。", "conclusion": "研究建立了大型语言模型与人类在社会复杂互动中行为一致性的基准，强调了个性条件化在对话建模中的潜力和局限。"}}
{"id": "2509.16421", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16421", "abs": "https://arxiv.org/abs/2509.16421", "authors": ["Aiden Chang", "Celso De Melo", "Stephanie M. Lukin"], "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead", "comment": "Accepted at NeurIPS 2025, 32 pages, 5 figures", "summary": "Real-time understanding of continuous video streams is essential for\nintelligent agents operating in high-stakes environments, including autonomous\nvehicles, surveillance drones, and disaster response robots. Yet, most existing\nvideo understanding and highlight detection methods assume access to the entire\nvideo during inference, making them unsuitable for online or streaming\nscenarios. In particular, current models optimize for offline summarization,\nfailing to support step-by-step reasoning needed for real-time decision-making.\nWe introduce Aha, an autoregressive highlight detection framework that predicts\nthe relevance of each video frame against a task described in natural language.\nWithout accessing future video frames, Aha utilizes a multimodal\nvision-language model and lightweight, decoupled heads trained on a large,\ncurated dataset of human-centric video labels. To enable scalability, we\nintroduce the Dynamic SinkCache mechanism that achieves constant memory usage\nacross infinite-length streams without degrading performance on standard\nbenchmarks. This encourages the hidden representation to capture high-level\ntask objectives, enabling effective frame-level rankings for informativeness,\nrelevance, and uncertainty with respect to the natural language task. Aha\nachieves state-of-the-art (SOTA) performance on highlight detection benchmarks,\nsurpassing even prior offline, full-context approaches and video-language\nmodels by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).\nWe explore Aha's potential for real-world robotics applications given a\ntask-oriented natural language input and a continuous, robot-centric video.\nBoth experiments demonstrate Aha's potential effectiveness as a real-time\nreasoning module for downstream planning and long-horizon understanding.", "AI": {"tldr": "Aha框架实现对连续视频流的实时理解，特别在自动驾驶车辆、监控无人机和灾害响应机器人等关键任务领域可以提供有效的支持。相较之前的离线全上下文和视频-语言模型，Aha在多种标准评估中展示出明显优势。", "motivation": "当前大部分视频理解和亮点检测方法都假设可以在推理时访问整个视频，这使得这些方法不适用于在线或流式传输场景。Aha旨在解决这个问题，提出自回归亮点检测框架，支持实时逐帧推理，以满足智能体在关键任务领域的实时决策需求。", "method": "通过自回归方法实现了对连续视频流实时理解，并提出了Dynamic SinkCache机制，确保无限长度视频流上的常量内存使用和性能不降。Aha框架利用多模态视觉-语言模型和轻量级解耦头在大规模人工标注视频数据集上进行训练，实现实时逐帧预测与自然语言任务描述的相关性。", "result": "Aha在广泛使用的亮点检测评估基准上实现了最先进的性能，其在TVSum上的mAP比之前的最佳离线方法高出了5.9%，在Mr.Hisum上则高出了8.3%。", "conclusion": "实验结果表明，Aha具备作为实时推理模块的潜力，能够为下游的长期规划和理解提供有效支持。这为自主性机器人应用程序提供了一种有前景的解决方案。"}}
{"id": "2509.16400", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16400", "abs": "https://arxiv.org/abs/2509.16400", "authors": ["Huy Nghiem", "Phuong-Anh Nguyen-Le", "John Prindle", "Rachel Rudinger", "Hal Daumé III"], "title": "'Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?", "comment": "EMNLP 2025, ver 1, 35 pages", "summary": "Large Language Models (LLMs) are increasingly involved in high-stakes\ndomains, yet how they reason about socially sensitive decisions remains\nunderexplored. We present a large-scale audit of LLMs' treatment of\nsocioeconomic status (SES) in college admissions decisions using a novel\ndual-process framework inspired by cognitive science. Leveraging a synthetic\ndataset of 30,000 applicant profiles grounded in real-world correlations, we\nprompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2\nmodes: a fast, decision-only setup (System 1) and a slower, explanation-based\nsetup (System 2). Results from 5 million prompts reveal that LLMs consistently\nfavor low-SES applicants -- even when controlling for academic performance --\nand that System 2 amplifies this tendency by explicitly invoking SES as\ncompensatory justification, highlighting both their potential and volatility as\ndecision-makers. We then propose DPAF, a dual-process audit framework to probe\nLLMs' reasoning behaviors in sensitive applications.", "AI": {"tldr": "本研究通过大规模审计，揭示了大语言模型在处理与社会经济地位相关的大学录取决策时存在明显的偏向性。通过双过程框架（系统1和系统2），发现了这些模型在处理敏感信息时的潜在优势和不确定性。", "motivation": "尽管大语言模型（LLMs）在高风险领域中的应用越来越多，但它们在处理敏感社会决策方面的推理方式仍需深入研究。为解决此问题，本研究设计了一种新的框架来审查LLMs如何处理大学录取过程中的社会经济地位问题。", "method": "采用了一种受认知科学启发的双过程框架，对LLMs在大学招生决策中对社会经济地位（SES）的处理进行了大规模审查。使用基于现实世界相关性的30,000份申请人合成数据集来提示4个开源LLMs（Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1），并分别在快速决策和慢速解释两种模式下进行测试。", "result": "测试结果表明，LLMs在大学录取决策中倾向于偏好低SES背景的申请人，即使在控制学术表现的情况下也是如此。慢速解释模式进一步放大了这一倾向，通过显式调用SES作为补偿性理由。", "conclusion": "研究结果表明，LLMs在处理大学录取中的SES问题时显示出其作为决策者的潜力和不确定性。为此，提出了一种名为DPAF的双过程审计框架，用于探测LLMs在敏感应用中的推理行为。"}}
{"id": "2509.16423", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16423", "abs": "https://arxiv.org/abs/2509.16423", "authors": ["Maria Taktasheva", "Lily Goli", "Alessandro Fiorini", "Zhen", "Li", "Daniel Rebain", "Andrea Tagliasacchi"], "title": "3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction", "comment": null, "summary": "Recent advances in radiance fields and novel view synthesis enable creation\nof realistic digital twins from photographs. However, current methods struggle\nwith flat, texture-less surfaces, creating uneven and semi-transparent\nreconstructions, due to an ill-conditioned photometric reconstruction\nobjective. Surface reconstruction methods solve this issue but sacrifice visual\nquality. We propose a novel hybrid 2D/3D representation that jointly optimizes\nconstrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)\nGaussians for the rest of the scene. Our end-to-end approach dynamically\ndetects and refines planar regions, improving both visual fidelity and\ngeometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++\nand ScanNetv2, and excels at mesh extraction without overfitting to a specific\ncamera model, showing its effectiveness in producing high-quality\nreconstruction of indoor scenes.", "AI": {"tldr": "The paper introduces a novel hybrid 2D/3D representation method that enhances the reconstruction of flat surfaces, achieving better visual and geometric accuracy in indoor scenes.", "motivation": "The motivation is to improve the reconstruction of flat, texture-less surfaces, which current techniques struggle with, leading to uneven and semi-transparent reconstructions due to an ill-conditioned photometric reconstruction objective.", "method": "Our method proposes a novel hybrid 2D/3D representation that uses constrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D) Gaussians for the rest of the scene, which dynamically detects and refines planar regions.", "result": "The proposed method achieves state-of-the-art depth estimation on ScanNet++ and ScanNetv2 datasets, and excels in mesh extraction without overfitting to a specific camera model.", "conclusion": "The hybrid 2D/3D representation effectively balances visual fidelity and geometric accuracy, improving the reconstructed quality of flat surfaces in indoor scenes."}}
{"id": "2509.16413", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16413", "abs": "https://arxiv.org/abs/2509.16413", "authors": ["Richard Diehl Martinez", "David Demitri Africa", "Yuval Weiss", "Suchir Salhan", "Ryan Daniels", "Paula Buttery"], "title": "Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research", "comment": null, "summary": "Building language models (LMs), especially small and medium ones, remains\nmore art than science. While large LMs often improve by sheer scale, it is\nstill unclear why many design choices work. For small LMs, this uncertainty is\nmore limiting: tight parameter budgets make each decision critical, yet\nresearchers still lack systematic, scientific ways to test and refine new\nideas.\n  We introduce Pico, a lightweight, modular framework that enables systematic,\nhypothesis-driven research for small and medium-scale language model\ndevelopment. Pico consists of two libraries that together provide a practical\nsandbox where researchers can make targeted changes to a model's architecture\nor training procedures and directly observe their effects on the model's\nbehavior. To support reproducible experimentation, we also release a suite of\nbaseline models, pico-decoder, trained under standardized conditions and\nopen-sourced for the community. Case studies highlight how Pico can support\niterative small LM design and analysis.", "AI": {"tldr": "文章介绍了一个轻量级框架Pico，用于系统化研究小规模和中等规模的语言模型，并提供了可重复实验的基线模型。", "motivation": "在开发小规模语言模型时，许多设计选择的不确定性仍然很大，而紧缩的参数预算使得每个决策都至关重要。研究人员仍然缺乏系统化的、科学的方法来测试和改进新想法。", "method": "介绍了Pico，一个轻量级、模块化的框架，用于小规模和中等规模语言模型开发的系统性研究。Pico包括两个库，提供了一个实际的沙箱，研究人员可以在此对模型架构或训练过程进行有针对性的修改，并直接观察其对模型行为的影响。", "result": "研究人员可以使用Pico进行可重复的实验。此外，还发布了一系列在标准化条件下训练的基线模型pico-decoder，并将其开源提供给社区。", "conclusion": "通过案例研究，展示了Pico如何支持小规模语言模型的迭代设计和分析。"}}
{"id": "2509.16429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16429", "abs": "https://arxiv.org/abs/2509.16429", "authors": ["Itzik Waizman", "Yakov Gusakov", "Itay Benou", "Tammy Riklin Raviv"], "title": "TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks", "comment": null, "summary": "White matter tractography is an advanced neuroimaging technique that\nreconstructs the 3D white matter pathways of the brain from diffusion MRI data.\nIt can be framed as a pathfinding problem aiming to infer neural fiber\ntrajectories from noisy and ambiguous measurements, facing challenges such as\ncrossing, merging, and fanning white-matter configurations. In this paper, we\npropose a novel tractography method that leverages Transformers to model the\nsequential nature of white matter streamlines, enabling the prediction of fiber\ndirections by integrating both the trajectory context and current diffusion MRI\nmeasurements. To incorporate spatial information, we utilize CNNs that extract\nmicrostructural features from local neighborhoods around each voxel. By\ncombining these complementary sources of information, our approach improves the\nprecision and completeness of neural pathway mapping compared to traditional\ntractography models. We evaluate our method with the Tractometer toolkit,\nachieving competitive performance against state-of-the-art approaches, and\npresent qualitative results on the TractoInferno dataset, demonstrating strong\ngeneralization to real-world data.", "AI": {"tldr": "提出了一种新的基于Transformer的纤维追踪方法，通过结合CNN特征增强了追踪效果，性能优异。", "motivation": "传统的纤维追踪方法难以应对扩散MRI数据中的噪声和模棱两可的测量问题，尤其在面对交叉、融合和扩散的白质配置时。", "method": "我们提出了一种新颖的基于Transformer的脑白质纤维追踪方法，该方法结合了序列建模能力和CNN提取的空间特征来预测纤维方向，从而提高了追踪的精确性和完整性。", "result": "使用Tractometer工具包进行评估，我们的方法在性能上与当前最先进的方法相当，并在TractoInferno数据集上展示了对真实世界数据的强大泛化能力。", "conclusion": "本研究成功结合了Transformer和CNN的优势，提高了纤维追踪的精度和完整性，展示了其在神经路径映射中的巨大潜力。"}}
{"id": "2509.16422", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16422", "abs": "https://arxiv.org/abs/2509.16422", "authors": ["Tom Mackintosh", "Harish Tayyar Madabushi", "Claire Bonial"], "title": "Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning", "comment": null, "summary": "We probe large language models' ability to learn deep form-meaning mappings\nas defined by construction grammars. We introduce the ConTest-NLI benchmark of\n80k sentences covering eight English constructions from highly lexicalized to\nhighly schematic. Our pipeline generates diverse synthetic NLI triples via\ntemplating and the application of a model-in-the-loop filter. This provides\naspects of human validation to ensure challenge and label reliability.\nZero-shot tests on leading LLMs reveal a 24% drop in accuracy between\nnaturalistic (88%) and adversarial data (64%), with schematic patterns proving\nhardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement,\nyet our results highlight persistent abstraction gaps in current LLMs and offer\na scalable framework for evaluating construction-informed learning.", "AI": {"tldr": "研究大型语言模型的构式语法学习能力，发现这些模型在处理模式化构式时存在困难，提出了ConTest-NLI基准以评价这种学习能力。", "motivation": "评估大型语言模型在理解和学习构式语法中的深层形式-意义映射的能力，以揭示模型的抽象差距，并为未来的研究提供评价框架。", "method": "通过实验，研究了大型语言模型学习由构式语法定义的深层形式-意义映射的能力。设计了ConTest-NLI基准，该基准包含80,000个句子，涵盖了从高度词汇化到高度模式化的八个英语构式。通过模板和模型循环过滤器生成多样化的合成NLI三元组，确保挑战性和标签可靠性。", "result": "对主流大型语言模型的零样本测试显示，自然数据的准确率从88%下降到对抗数据的64%，模式化构式最难学习。在ConTest-NLI的子集上进行微调可提高9%的准确率。", "conclusion": "结果显示当前大型语言模型存在持续的抽象差距，但提供了基于构式的可扩展学习评价框架。"}}
{"id": "2509.16436", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16436", "abs": "https://arxiv.org/abs/2509.16436", "authors": ["Zhejia Zhang", "Junjie Wang", "Le Zhang"], "title": "Improved mmFormer for Liver Fibrosis Staging via Missing-Modality Compensation", "comment": null, "summary": "In real-world clinical settings, magnetic resonance imaging (MRI) frequently\nsuffers from missing modalities due to equipment variability or patient\ncooperation issues, which can significantly affect model performance. To\naddress this issue, we propose a multimodal MRI classification model based on\nthe mmFormer architecture with an adaptive module for handling arbitrary\ncombinations of missing modalities. Specifically, this model retains the hybrid\nmodality-specific encoders and the modality-correlated encoder from mmFormer to\nextract consistent lesion features across available modalities. In addition, we\nintegrate a missing-modality compensation module which leverages zero-padding,\nmodality availability masks, and a Delta Function with learnable statistical\nparameters to dynamically synthesize proxy features for recovering missing\ninformation. To further improve prediction performance, we adopt a\ncross-validation ensemble strategy by training multiple models on different\nfolds and applying soft voting during inference. This method is evaluated on\nthe test set of Comprehensive Analysis & Computing of REal-world medical images\n(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based\non non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),\nT2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis\nDetection and Substantial Fibrosis Detection on in-distribution vendors, our\nmodel obtains accuracies of 66.67%, and 74.17%, and corresponding area under\nthe curve (AUC) scores of 71.73% and 68.48%, respectively.", "AI": {"tldr": "本文提出了一种基于mmFormer架构的多模态MRI分类模型，该模型结合了适应性模块用于处理任意组合的缺失模态，通过零填充、模态可用性掩码和可学习统计参数的Delta函数动态合成代理特征以恢复缺失信息，并采用交叉验证集成策略提升预测性能，在CARE 2025挑战的LiFS任务中对T1WI、T2WI和DWI进行肝纤维化分期检测，分别获得66.67%和74.17%的准确率及71.73%和68.48%的AUC得分。", "motivation": "在实际临床场景中，MRI常因设备差异或患者配合问题面临模态缺失，这严重影响模型性能，因此需要开发一种能够适应不同模态组合的模型。", "method": "本文提出的方法基于mmFormer架构，采用自适应模块处理任意组合的缺失模态。保留模态特定的编码器及模态关联的编码器，以提取跨可用模态的一致病灶特征。同时集成零填充、模态可用性掩码及可学习参数的Delta函数用于动态合成代理特征，辅助恢复缺失信息。此外，采用交叉验证集成策略提升预测性能。", "result": "在CARE 2025挑战的LiFS任务中，本模型对比在不同供应商的数据上对T1WI、T2WI和DWI进行肝纤维化分期检测，针对肝硬化与显著纤维化检测任务分别达到66.67%和74.17%的准确率以及71.73%和68.48%的AUC得分。", "conclusion": "研究表明，所提出的多模态MRI分类模型能有效处理临床中常见的模态缺失问题，并提升基于非对比动态MRI扫描的肝纤维化分期检测性能。"}}
{"id": "2509.16449", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16449", "abs": "https://arxiv.org/abs/2509.16449", "authors": ["Tsz Fung Pang", "Maryam Berijanian", "Thomas Orth", "Breanna Shi", "Charlotte S. Alexander"], "title": "PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization", "comment": null, "summary": "Legal documents are often long, dense, and difficult to comprehend, not only\nfor laypeople but also for legal experts. While automated document\nsummarization has great potential to improve access to legal knowledge,\nprevailing task-based evaluators overlook divergent user and stakeholder needs.\nTool development is needed to encompass the technicality of a case summary for\na litigator yet be accessible for a self-help public researching for their\nlawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation\nframework that scores summaries through the lens of six personas, including\nlegal and non-legal users. We also introduce a controlled dimension-shifted\npilot dataset of U.S. civil rights case summaries that varies along depth,\naccessibility, and procedural detail as well as Diversity-Coverage Index (DCI)\nto expose divergent optima of legal summary between persona-aware and\npersona-agnostic judges. This work enables refinement of legal AI summarization\nsystems for both expert and non-expert users, with the potential to increase\naccess to legal knowledge. The code base and data are publicly available in\nGitHub.", "AI": {"tldr": "本文介绍PersonaMatrix框架和DCI指标，用于评估法律案件摘要，考虑了法律专家和非专家的不同需求，以提高法律知识的可访问性。", "motivation": "现有的任务导向评估方法忽视了用户和利益相关者的差异化需求，本研究旨在开发一种新的工具，以满足不同用户的技术和可访问性需求。", "method": "介绍PersonaMatrix，一个评价框架，通过六种不同角色的视角对摘要进行评分，并引入了一个受控的维度变化的试点数据集以及Diversity-Coverage Index（DCI），用以揭示不同角色评价者之间的最优解。", "result": "提出了PersonaMatrix框架和Diversity-Coverage Index（DCI）指标，为法律AI摘要系统的优化提供了方法，增加法律知识的可访问性。", "conclusion": "本研究为优化针对专家和非专家用户群的法律AI摘要系统提供了新工具，并公开发放在GitHub的代码和数据有助于进一步研究。"}}
{"id": "2509.16438", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16438", "abs": "https://arxiv.org/abs/2509.16438", "authors": ["Mohamed Eltahir", "Osamah Sarraj", "Abdulrahman Alfrihidi", "Taha Alshatiri", "Mohammed Khurd", "Mohammed Bremoo", "Tanveer Hussain"], "title": "AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks", "comment": "Accepted at ArabicNLP 2025 (EMNLP 2025 workshop)", "summary": "Video-to-text and text-to-video retrieval are dominated by English benchmarks\n(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet\nArabic remains underserved, lacking localized evaluation metrics. We introduce\na three-stage framework, AutoArabic, utilizing state-of-the-art large language\nmodels (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,\nreducing the manual revision required by nearly fourfold. The framework\nincorporates an error detection module that automatically flags potential\ntranslation errors with 97% accuracy. Applying the framework to DiDeMo, a video\nretrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent\nArabic descriptions. An analysis of the translation errors is provided and\norganized into an insightful taxonomy to guide future Arabic localization\nefforts. We train a CLIP-style baseline with identical hyperparameters on the\nArabic and English variants of the benchmark, finding a moderate performance\ngap (about 3 percentage points at Recall@1), indicating that Arabic\nlocalization preserves benchmark difficulty. We evaluate three post-editing\nbudgets (zero/ flagged-only/ full) and find that performance improves\nmonotonically with more post-editing, while the raw LLM output (zero-budget)\nremains usable. To ensure reproducibility to other languages, we made the code\navailable at https://github.com/Tahaalshatiri/AutoArabic.", "AI": {"tldr": "AutoArabic框架通过大规模语言模型将非阿拉伯语文本翻译为现代标准阿拉伯语，生成了高质量的视频检索基准DiDeMo-AR，推动了阿拉伯语本地化基准的发展。", "motivation": "动机在于填补目前阿拉伯语在视频到文本和文本到视频检索领域缺乏本地化评估标准的空白，使用非阿拉伯语文本的翻译替代。", "method": "我们引入了一个三阶段框架AutoArabic，利用最先进的大规模语言模型（LLMs）将非阿拉伯语基准翻译成现代标准阿拉伯语，将所需的手动修订量减少了近四倍。该框架包含一个误差检测模块，能够以97%的准确性自动标记潜在的翻译错误。", "result": "采用该框架处理了视频检索基准DiDeMo，产生了40,144个流畅的阿拉伯语描述版本DiDeMo-AR。通过与英语版本进行对比实验，发现阿拉伯语本地化保持了基准难度。进一步分析表明，随着后期编辑预算的增加，性能逐步提高。", "conclusion": "阿拉伯语翻译版本及其评估框架展示了用于视频到文本和文本到视频检索任务的潜力，并鼓励后续更多语言的类似本地化工作。"}}
{"id": "2509.16457", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16457", "abs": "https://arxiv.org/abs/2509.16457", "authors": ["Yunzhe Wang", "Gale M. Lucas", "Burcin Becerik-Gerber", "Volkan Ustun"], "title": "Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations", "comment": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025), Main Conference", "summary": "Language-driven generative agents have enabled large-scale social simulations\nwith transformative uses, from interpersonal training to aiding global\npolicy-making. However, recent studies indicate that generative agent behaviors\noften deviate from expert expectations and real-world data--a phenomenon we\nterm the Behavior-Realism Gap. To address this, we introduce a theoretical\nframework called Persona-Environment Behavioral Alignment (PEBA), formulated as\na distribution matching problem grounded in Lewin's behavior equation stating\nthat behavior is a function of the person and their environment. Leveraging\nPEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that\niteratively refines agent personas, implicitly aligning their collective\nbehaviors with realistic expert benchmarks within a specified environmental\ncontext. We validate PEvo in an active shooter incident simulation we\ndeveloped, achieving an 84% average reduction in distributional divergence\ncompared to no steering and a 34% improvement over explicit instruction\nbaselines. Results also show PEvo-refined personas generalize to novel, related\nsimulation scenarios. Our method greatly enhances behavioral realism and\nreliability in high-stakes social simulations. More broadly, the PEBA-PEvo\nframework provides a principled approach to developing trustworthy LLM-driven\nsocial simulations.", "AI": {"tldr": "本文提出了一个理论框架PEBA，并基于此框架开发了一个算法PersonaEvolve，用于优化生成代理的行为以更好地匹配现实世界的预期，特别是在模拟高风险社会事件中的表现。", "motivation": "语言驱动的生成性代理引发了大规模的社会模拟，但最近的研究表明，生成代理的行为常常与专家的预期和现实数据有偏差。为了应对这一问题，即所谓的“行为现实性差距”，我们提出了我们的理论和方法。", "method": "我们提出了一个名为Persona-Environment Behavioral Alignment (PEBA) 的理论框架，它是一个基于 Lewin 的行为方程式的分布匹配问题，该方程式表明行为是人和环境的函数。基于 PEBA，我们设计了 PersonaEvolve (PEvo)，这是一个基于 LLM 的优化算法，旨在迭代地优化代理的人格，并在指定的环境背景下，使其集体行为与现实专家基准相一致。", "result": "我们在一个主动射击事件模拟中验证了PEvo，实现了与未引导条件相比，84%的平均分布分歧减少，以及34%超过显式指令基线的改进。结果还表明，经过PEvo优化的人格特征在新型、相关的模拟场景中也能很好地泛化。", "conclusion": "我们的方法极大地提升了高风险社会模拟中的行为现实性和可靠性。更广泛地说，PEBA-PEvo框架为开发值得信赖的语言模型驱动的社会模拟提供了一个原则性的方法。"}}
{"id": "2509.16452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16452", "abs": "https://arxiv.org/abs/2509.16452", "authors": ["Son Hai Nguyen", "Diwei Wang", "Jinhyeok Jang", "Hyewon Seo"], "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models", "comment": null, "summary": "Accurate vision-based action recognition is crucial for developing autonomous\nrobots that can operate safely and reliably in complex, real-world\nenvironments. In this work, we advance video-based recognition of indoor daily\nactions for robotic perception by leveraging vision-language models (VLMs)\nenriched with domain-specific knowledge. We adapt a prompt-learning framework\nin which class-level textual descriptions of each action are embedded as\nlearnable prompts into a frozen pre-trained VLM backbone. Several strategies\nfor structuring and encoding these textual descriptions are designed and\nevaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our\nmethod, using only RGB video inputs at test time, achieves over 95\\% accuracy\nand outperforms state-of-the-art approaches. These results highlight the\neffectiveness of knowledge-augmented prompts in enabling robust action\nrecognition with minimal supervision.", "AI": {"tldr": "This paper presents a technique using vision-language models with domain-specific knowledge and prompt learning for indoor action recognition, achieving high accuracy on a benchmark dataset without the need for extensive supervision.", "motivation": "The motivation is to enhance the accuracy of vision-based action recognition, which is essential for the development of autonomous robots operating in complex real-world environments.", "method": "The method leverages vision-language models (VLMs) enriched with domain-specific knowledge to improve action recognition. A prompt-learning framework is used where textual descriptions of actions are embedded as learnable prompts into a pre-trained VLM backbone.", "result": "Experiments on the ETRI-Activity3D dataset show over 95% accuracy with only RGB video inputs at test time, demonstrating the method's effectiveness in enabling robust action recognition.", "conclusion": "The conclusion is that knowledge-augmented prompts effectively improve action recognition accuracy with minimal supervision, outperforming state-of-the-art methods."}}
{"id": "2509.16462", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16462", "abs": "https://arxiv.org/abs/2509.16462", "authors": ["'Mina Arzaghi'", "'Alireza Dehghanpour Farashah'", "'Florian Carichon'", "' Golnoosh Farnadi'"], "title": "Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) exhibit socio-economic biases that can propagate\ninto downstream tasks. While prior studies have questioned whether intrinsic\nbias in LLMs affects fairness at the downstream task level, this work\nempirically investigates the connection. We present a unified evaluation\nframework to compare intrinsic bias mitigation via concept unlearning with\nextrinsic bias mitigation via counterfactual data augmentation (CDA). We\nexamine this relationship through real-world financial classification tasks,\nincluding salary prediction, employment status, and creditworthiness\nassessment. Using three open-source LLMs, we evaluate models both as frozen\nembedding extractors and as fine-tuned classifiers. Our results show that\nintrinsic bias mitigation through unlearning reduces intrinsic gender bias by\nup to 94.9%, while also improving downstream task fairness metrics, such as\ndemographic parity by up to 82%, without compromising accuracy. Our framework\noffers practical guidance on where mitigation efforts can be most effective and\nhighlights the importance of applying early-stage mitigation before downstream\ndeployment.", "AI": {"tldr": "研究发现通过概念遗忘来减少LLMs的内在偏见可以显著提高下游任务的公平性，而不会降低准确性；", "motivation": "研究大型语言模型(LLMs)中的社会经济偏见如何影响下游任务的公平性；", "method": "通过概念遗忘来内在地减少偏见与通过反事实数据增强(CDA)来外在地减少偏见的方法对比，统一评估框架；", "result": "内在偏见减少至多94.9%，下游任务公平性指标（如人口统计学差异）提高至多82%；", "conclusion": "提供实践指导以将缓解努力集中在最有效的地方，强调在部署前应用早期缓解措施的重要性；"}}
{"id": "2509.16472", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16472", "abs": "https://arxiv.org/abs/2509.16472", "authors": ["Parth Agarwal", "Sangaa Chatterjee", "Md Faisal Kabir", "Suman Saha"], "title": "Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM Models", "comment": "The paper got accepted in ICMLA-2025. It is a camera-ready version", "summary": "Gait is a key indicator in diagnosing movement disorders, but most models\nlack interpretability and rely on single datasets. We propose a dual-branch\nCNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D\nbranch on silhouettes from OU-MVLP. Interpretability is provided by SHAP\n(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,\nthe system achieves 98.6% accuracy with strong recall and F1. This approach\nadvances explainable gait analysis across both clinical and biometric domains.", "AI": {"tldr": "提出了一种基于双通道CNN-LSTM框架的增强步态分析模型，通过采用SHAP和Grad-CAM方法提高了解释性，达到了98.6%的准确率。", "motivation": "步态是诊断运动障碍的关键指标，但目前大多数模型缺乏解释性且依赖单一数据集。为了改善这一点，提出了一个新的模型以增强步态分析的解释性。", "method": "采用双通道CNN-LSTM框架，结合GAVD数据集的基于关节的1D特性和OU-MVLP数据集的3D剪影，利用SHAP进行时间归属解释，Grad-CAM进行空间定位解释。", "result": "该系统在保留集上达到了98.6%的准确率，并保持了良好的召回率和F1得分。", "conclusion": "该方法通过在GAVD数据集上的1D分支和在OU-MVLP数据集上的3D分支提高了可解释性和泛化能力，从而在步态分析中达到了98.6%的准确率，并在临床和生物识别领域展示了强大的召回率和F1得分。"}}
{"id": "2509.16464", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.16464", "abs": "https://arxiv.org/abs/2509.16464", "authors": ["Margaret Hughes", "Brandon Roy", "Elinor Poole-Dayan", "Deb Roy", "Jad Kabbara"], "title": "Computational Analysis of Conversation Dynamics through Participant Responsivity", "comment": null, "summary": "Growing literature explores toxicity and polarization in discourse, with\ncomparatively less work on characterizing what makes dialogue prosocial and\nconstructive. We explore conversational discourse and investigate a method for\ncharacterizing its quality built upon the notion of ``responsivity'' -- whether\none person's conversational turn is responding to a preceding turn. We develop\nand evaluate methods for quantifying responsivity -- first through semantic\nsimilarity of speaker turns, and second by leveraging state-of-the-art large\nlanguage models (LLMs) to identify the relation between two speaker turns. We\nevaluate both methods against a ground truth set of human-annotated\nconversations. Furthermore, selecting the better performing LLM-based approach,\nwe characterize the nature of the response -- whether it responded to that\npreceding turn in a substantive way or not.\n  We view these responsivity links as a fundamental aspect of dialogue but note\nthat conversations can exhibit significantly different responsivity structures.\nAccordingly, we then develop conversation-level derived metrics to address\nvarious aspects of conversational discourse. We use these derived metrics to\nexplore other conversations and show that they support meaningful\ncharacterizations and differentiations across a diverse collection of\nconversations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.16474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16474", "abs": "https://arxiv.org/abs/2509.16474", "authors": ["Gabrielle Chavez", "Laureano Moro-Velazquez", "Ankur Butala", "Najim Dehak", "Thomas Thebaud"], "title": "Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion", "comment": "5 pages, 2 figures, submitted to International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP)", "summary": "Handwriting is significantly affected by neurological disorders (ND) such as\nParkinson's disease (PD) and Alzheimer's disease (AD). Prior works have\nanalyzed handwriting tasks using feature-based approaches or computer-vision\ntechniques, but these methods have struggled to generalize across multiple\ndatasets, particularly between temporal features represented as time-series and\nimages. We propose a framework that leverages both time-series and images of\nhandwriting through a joint classifier, based on a ResNet50 pretrained on\nImageNet-1k. Binary classification experiments demonstrate state-of-the-art\nperformances on existing time-series and image datasets, with significant\nimprovement on specific drawing and writing tasks from the NeuroLogical Signals\n(NLS) dataset. In particular, the proposed model demonstrates improved\nperformance on Draw Clock and Spiral tasks. Additionally, cross-dataset and\nmulti-dataset experiments were consistently able to achieve high F1 scores, up\nto 98 for PD detection, highlighting the potential of the proposed model to\ngeneralize over different forms of handwriting signals, and enhance the\ndetection of motor deficits in ND.", "AI": {"tldr": "我们通过联合分类器方法改善了手写分析在神经疾病患者中的性能，并展示了模型在跨数据集和多数据集实验中的高准确性。", "motivation": "先前的工作使用基于特征的方法或计算机视觉技术来分析手写任务，但这些方法在面对多个数据集时难以泛化，尤其在时间序列和图像表示之间。", "method": "我们提出了一种框架，通过一个基于在ImageNet-1k上预训练的ResNet50的联合分类器来利用手写时间序列和图像信息。", "result": "二分类实验在现有的时间序列和图像数据集上表现出最先进的性能，特别是对NeuroLogical Signals (NLS)数据集中的特定绘画和书写任务有显著提升。模型在Draw Clock和Spiral任务上的性能表现优异。跨数据集和多数据集实验能够持续取得高达98的F1评分。", "conclusion": "我们的模型展示了对手写信号不同形式泛化的能力，以及提高神经性疾病中运动缺陷检测的潜力。"}}
{"id": "2509.16487", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16487", "abs": "https://arxiv.org/abs/2509.16487", "authors": ["Zixun Chen", "Petr Babkin", "Akshat Gupta", "Gopala Anumanchipalli", "Xiaomo Liu"], "title": "The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia", "comment": null, "summary": "Dialogue is one of the landmark abilities of large language models (LLMs).\nDespite its ubiquity, few studies actually distinguish specific ingredients\nunderpinning dialogue behavior emerging during post-training. We employ a\ncomprehensive suite of model-based metrics, each targeting a distinct\nfine-grained aspect of dialogue, motivated by linguistic theory. We evaluate\nhow the performance of pre-trained Pythia models changes with respect to each\nof those dimensions, depending on model size and as a result of supervised\nfine-tuning on conversational datasets. We observe only a mild impact of raw\nmodel size on most metrics, whereas fine-tuning quickly saturates the scores\nfor all but the smallest models tested. Somewhat contrary to our expectations,\nmany metrics show very similar trends, especially if they are all rooted in the\nsame evaluator model, which raises the question of their reliability in\nmeasuring a specific dimension. To that end, we conduct additional analyses of\nscore distributions, metric correlations, and term frequencies in generated\nresponses to help explain our observations.", "AI": {"tldr": "研究了预训练Pythia模型在对话能力上的表现，发现模型大小作用有限，度量指标可靠性堪忧。", "motivation": "尽管对话是大型语言模型的一项标志性能力，但很少有研究具体区分出对话行为在后训练期间出现的特定成分。本文旨在通过语言理论指导的综合性度量指标来填补这一空白。", "method": "使用基于模型的综合性度量指标来评估预训练的Pythia模型在对话不同维度的表现变化，这些维度取决于模型大小以及在对话数据集上的监督微调结果。", "result": "数据显示模型大小对大多数度量指标的影响较小，并且监督微调快速达到了所有但最小的模型测试的饱和分数。然而，如果这些度量指标源自相同的评估模型，则它们表现出非常相似的趋势，这提出了它们可靠性的疑问。", "conclusion": "研究表明模型大小在提升特定对话维度性能方面的作用有限，同时提出了所用度量指标可靠性的疑问。通过额外的分析，包括得分分布、度量指标相关性和生成响应中的术语频率，这些问题得到了一定程度的解答。"}}
{"id": "2509.16476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16476", "abs": "https://arxiv.org/abs/2509.16476", "authors": ["Qinyu Chen", "Jiawen Qi"], "title": "Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs", "comment": "11 pages", "summary": "Vision-Language Models (VLMs) deliver impressive performance in understanding\nvisual content with language instructions. However, redundancy in vision tokens\nresults in the degenerated inference efficiency of VLMs, which hinders\nreal-time use on edge consumer devices such as AR/VR devices. Existing\nefficiency methods commonly prune visual tokens using learned saliency, sparse\nattention schedules, or controller policies, but they often require\narchitectural modification or access to intermediate activations. These\npipelines add inference-time modules that increase compute and memory and often\nlead to an accuracy trade-off. Moreover, they also suffer from misalignment\nbetween the prompts and the region of interest in the images. Without human\nguidance, the model may focus on the wrong regions and miss small,\nhigh-frequency details when prompts or scenes change. In this paper, we propose\nGazeVLM, a training-free framework that uses the human eye gaze as a natural\nsupervisory signal to allocate computation where it matters. By extracting\ngaze-driven regions of interest (ROIs) and optionally combining them with a\nlow-resolution global view, GazeVLM mimics fovea-periphery perception to cut\nredundant visual tokens while preserving task-relevant details. We evaluate the\nvisual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark\nwith human gaze. Quality of the answer is assessed by GPT-4o pairwise judging\nand a weighted score over coverage, accuracy, details, and fluency. Efficiency\nis measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to\n93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better\nanswer quality relative to full-resolution baselines. Our results show that\naligning model computation with human gaze offers a simple, plug-and-play path\ntoward efficient VLM inference on consumer devices.", "AI": {"tldr": "本文提出了一种无需训练的框架GazeVLM，它使用人类视线作为监督信号，以减少冗余视觉标记，提高模型在边缘计算设备中的推理效率。", "motivation": "现有方法虽然能够减少视觉标记以提高模型推理效率，但往往需要对模型架构进行修改或增加计算和内存，并可能导致准确性降低。此外，模型在选择感兴趣区域时可能会出现不一致的问题。为了解决这些问题，作者提出了一种新的框架。", "method": "通过使用人类视线作为自然监督信号，GazeVLM 提出了一种无训练框架，该框架可以减少冗余视觉标记，同时保留任务相关的细节。通过提取视线驱动的兴趣区域（ROIs）并将它们与低分辨率全局视图相结合，GazeVLM 着眼于保留重要的细节部分而减少冗余。", "result": "在VOILA-COCO基准上评估的问答任务中，GazeVLM在保留更好的答案质量的同时，减少了高达93.1%的视觉标记，总体标记降低了59.6%，计算浮点操作数减少了50%。", "conclusion": "通过将模型计算与人类视线对齐，GazeVLM为消费者设备提供了一种简单、可插拔的方式，提高了Vision-Language模型的推理效率。这种方法在保持答案质量的同时，减少了计算量和记忆负荷。"}}
{"id": "2509.16494", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16494", "abs": "https://arxiv.org/abs/2509.16494", "authors": ["Fengyuan Liu", "Rui Zhao", "Shuo Chen", "Guohao Li", "Philip Torr", "Lei Han", "Jindong Gu"], "title": "Can an Individual Manipulate the Collective Decisions of Multi-Agents?", "comment": null, "summary": "Individual Large Language Models (LLMs) have demonstrated significant\ncapabilities across various domains, such as healthcare and law. Recent studies\nalso show that coordinated multi-agent systems exhibit enhanced decision-making\nand reasoning abilities through collaboration. However, due to the\nvulnerabilities of individual LLMs and the difficulty of accessing all agents\nin a multi-agent system, a key question arises: If attackers only know one\nagent, could they still generate adversarial samples capable of misleading the\ncollective decision? To explore this question, we formulate it as a game with\nincomplete information, where attackers know only one target agent and lack\nknowledge of the other agents in the system. With this formulation, we propose\nM-Spoiler, a framework that simulates agent interactions within a multi-agent\nsystem to generate adversarial samples. These samples are then used to\nmanipulate the target agent in the target system, misleading the system's\ncollaborative decision-making process. More specifically, M-Spoiler introduces\na stubborn agent that actively aids in optimizing adversarial samples by\nsimulating potential stubborn responses from agents in the target system. This\nenhances the effectiveness of the generated adversarial samples in misleading\nthe system. Through extensive experiments across various tasks, our findings\nconfirm the risks posed by the knowledge of an individual agent in multi-agent\nsystems and demonstrate the effectiveness of our framework. We also explore\nseveral defense mechanisms, showing that our proposed attack framework remains\nmore potent than baselines, underscoring the need for further research into\ndefensive strategies.", "AI": {"tldr": "该研究提出了M-Spoiler框架，通过模拟多智能体系统中的智能体交互，生成对抗样本，以误导目标系统中的集体决策过程。研究证实了在多智能体系统中了解单个智能体所带来的风险，并展示了所提攻击框架的有效性以及防御机制的潜在改进方向。", "motivation": "近期研究表明，协作多智能体系统在决策和推理方面表现出增强的能力。然而，由于个体大语言模型的脆弱性和难以访问多智能体系统所有智能体的问题，如果攻击者只知道其中一个智能体，是否可以生成误导性对抗样本成为了需要探讨的关键问题。", "method": "该研究将上述问题视为具有不完全信息的游戏，攻击者仅了解一个目标智能体。研究提出了M-Spoiler框架，该框架模拟了目标系统中的智能体交互，生成对抗样本，并引入了一个固执的智能体模拟目标系统中可能的固执反应，从而优化这些对抗样本。", "result": "实验结果验证了在多智能体系统中了解单个智能体所带来的风险，并展示了M-Spoiler框架的有效性。研究还探索了几种防御机制，结果显示所提出的攻击框架仍然比基线更强大。", "conclusion": "研究表明，针对多智能体系统的攻击可以通过了解单个智能体实现，并提出了有效应对这种攻击的方法和防御机制，进一步强调了防御策略研究的重要性。"}}
{"id": "2509.16479", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16479", "abs": "https://arxiv.org/abs/2509.16479", "authors": ["Christopher Silver", "Thangarajah Akilan"], "title": "Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture", "comment": null, "summary": "Falls among seniors are a major public health issue. Existing solutions using\nwearable sensors, ambient sensors, and RGB-based vision systems face challenges\nin reliability, user compliance, and practicality. Studies indicate that\nstakeholders, such as older adults and eldercare facilities, prefer\nnon-wearable, passive, privacy-preserving, and real-time fall detection systems\nthat require no user interaction. This study proposes an advanced thermal fall\ndetection method using a Bidirectional Convolutional Long Short-Term Memory\n(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general\nattention mechanisms. Through systematic experimentation across hundreds of\nmodel variations exploring the integration of attention mechanisms, recurrent\nmodules, and motion flow, we identified top-performing architectures. Among\nthem, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of\n$99.7\\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly\nemerged, diverse, and privacy-preserving benchmark. These results highlight the\ngeneralizability and practicality of the proposed model, setting new standards\nfor thermal fall detection and paving the way toward deployable,\nhigh-performance solutions.", "AI": {"tldr": "An advanced thermal fall detection system using a BiConvLSTM model with various attention mechanisms achieves state-of-the-art performance, aiming for real-time, non-wearable, and privacy-preserving solutions for seniors.", "motivation": "The motivation is to address the challenges faced by existing fall detection solutions among seniors, such as reliability, user compliance, and practicality. Stakeholders prefer non-wearable, passive, privacy-preserving, and real-time fall detection systems that require no user interaction.", "method": "This study proposes an advanced thermal fall detection method using a Bidirectional Convolutional Long Short-Term Memory (BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general attention mechanisms.", "result": "Among various tested architectures, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of 99.7% on the TSF dataset and demonstrated robust results on TF-66, a new, diverse, and privacy-preserving benchmark.", "conclusion": "The study highlights the generalizability and practicality of the proposed BiConvLSTM model, setting new standards for thermal fall detection and opening doors for high-performance deployable solutions."}}
{"id": "2509.16530", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16530", "abs": "https://arxiv.org/abs/2509.16530", "authors": ["Wei Xie", "Shuoyoucheng Ma", "Zhenhua Wang", "Enze Wang", "Kai Chen", "Xiaobing Sun", "Baosheng Wang"], "title": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans", "comment": "Thank you for your attention. This paper was accepted by the CogSci\n  2025 conference in April and published in August. The location in the\n  proceedings is: https://escholarship.org/uc/item/39k8f46q", "summary": "Large Language Models (LLMs) with hundreds of billions of parameters have\nexhibited human-like intelligence by learning from vast amounts of\ninternet-scale data. However, the uninterpretability of large-scale neural\nnetworks raises concerns about the reliability of LLM. Studies have attempted\nto assess the psychometric properties of LLMs by borrowing concepts from human\npsychology to enhance their interpretability, but they fail to account for the\nfundamental differences between LLMs and humans. This results in high rejection\nrates when human scales are reused directly. Furthermore, these scales do not\nsupport the measurement of LLM psychological property variations in different\nlanguages. This paper introduces AIPsychoBench, a specialized benchmark\ntailored to assess the psychological properties of LLM. It uses a lightweight\nrole-playing prompt to bypass LLM alignment, improving the average effective\nresponse rate from 70.12% to 90.40%. Meanwhile, the average biases are only\n3.3% (positive) and 2.1% (negative), which are significantly lower than the\nbiases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.\nFurthermore, among the total of 112 psychometric subcategories, the score\ndeviations for seven languages compared to English ranged from 5% to 20.2% in\n43 subcategories, providing the first comprehensive evidence of the linguistic\nimpact on the psychometrics of LLM.", "AI": {"tldr": "论文提出了AIPsychoBench，一种新的语言模型心理测试基准，它克服了现有方法的问题，提高了响应率，减少偏见，并提供了跨语言测试的支持。", "motivation": "动机在于解决现有评估大型语言模型心理特性的方法中存在的问题，这些问题包括高拒绝率以及跨语言测量时的不足。", "method": "此论文介绍了AIPsychoBench，这是一个专门用来评估大型语言模型心理特性的基准。它使用了一个轻量级的角色扮演提示来绕过语言模型的对齐问题，从而提高了有效响应率并降低了偏见。", "result": "结果表明，与传统的越狱提示相比，AIPsychoBench显著降低了正负偏见，并提供了关于语言模型心理测量特性的跨语言变异的首个综合证据。", "conclusion": "结论指出，AIPsychoBench能有效提高语言模型的响应率和减少偏见，为评估语言模型的心理性质提供了一个新的工具。此外，它揭示了语言对语言模型心理测量特性的影响。"}}
{"id": "2509.16483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16483", "abs": "https://arxiv.org/abs/2509.16483", "authors": ["Xujia Zhang", "Brendan Crowe", "Christoffer Heckman"], "title": "Octree Latent Diffusion for Semantic 3D Scene Generation and Completion", "comment": null, "summary": "The completion, extension, and generation of 3D semantic scenes are an\ninterrelated set of capabilities that are useful for robotic navigation and\nexploration. Existing approaches seek to decouple these problems and solve them\noneoff. Additionally, these approaches are often domain-specific, requiring\nseparate models for different data distributions, e.g. indoor vs. outdoor\nscenes. To unify these techniques and provide cross-domain compatibility, we\ndevelop a single framework that can perform scene completion, extension, and\ngeneration in both indoor and outdoor scenes, which we term Octree Latent\nSemantic Diffusion. Our approach operates directly on an efficient dual octree\ngraph latent representation: a hierarchical, sparse, and memory-efficient\noccupancy structure. This technique disentangles synthesis into two stages: (i)\nstructure diffusion, which predicts binary split signals to construct a coarse\noccupancy octree, and (ii) latent semantic diffusion, which generates semantic\nembeddings decoded by a graph VAE into voxellevel semantic labels. To perform\nsemantic scene completion or extension, our model leverages inference-time\nlatent inpainting, or outpainting respectively. These inference-time methods\nuse partial LiDAR scans or maps to condition generation, without the need for\nretraining or finetuning. We demonstrate highquality structure, coherent\nsemantics, and robust completion from single LiDAR scans, as well as zero-shot\ngeneralization to out-of-distribution LiDAR data. These results indicate that\ncompletion-through-generation in a dual octree graph latent space is a\npractical and scalable alternative to regression-based pipelines for real-world\nrobotic perception tasks.", "AI": {"tldr": "研发了一种名为Octree Latent Semantic Diffusion的单框架，用于室内和室外场景的3D语义场景完成、扩展和生成，通过双重八叉树图潜变量表示来提升效率和质量。", "motivation": "现有的方法试图将场景完成、扩展和生成分离并单独解决这些问题，大多局限于特定领域，需要针对不同的数据分布（例如室内与室外场景）分别建立模型。为了统一这些技术并提供跨领域的兼容性，我们提出了一种适用于室内和室外场景的方法。", "method": "我们的方法基于一个名为Octree Latent Semantic Diffusion的单框架，该框架在一个双重八叉树图潜变量表示上直接操作，用于室内和室外场景的场景完成、扩展和生成。这种方法将合成分为两个阶段：(i) 结构扩散，预测二进制分割信号以构建粗略的占用八叉树；(ii) 潜在语义扩散，生成解码为体素级语义标签的语义嵌入。在推理时，通过部分激光雷达扫描或地图来条件生成，而无需重新训练或微调。", "result": "结果表明，我们的模型能够从单个激光雷达扫描中实现高质量的结构、连贯的语义和强大的完成效果，且具备零样本泛化到分布外激光雷达数据的能力。这些结果意味着，通过生成方法在双重八叉树图潜变量空间完成场景是一种适用于实际机器人感知任务的可行且可扩展的替代方案。", "conclusion": "研究表明在双重八叉树图潜变量空间通过生成方法完成场景是一种实际且可扩展的解决方案，适合实际的机器人感知任务，能够实现高质量的结构、连贯的语义及泛化到未见过的数据。"}}
{"id": "2509.16531", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16531", "abs": "https://arxiv.org/abs/2509.16531", "authors": ["Junghwan Kim", "Haotian Zhang", "David Jurgens"], "title": "Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains", "comment": "Accepted to EMNLP 2025", "summary": "Authorship representation (AR) learning, which models an author's unique\nwriting style, has demonstrated strong performance in authorship attribution\ntasks. However, prior research has primarily focused on monolingual\nsettings-mostly in English-leaving the potential benefits of multilingual AR\nmodels underexplored. We introduce a novel method for multilingual AR learning\nthat incorporates two key innovations: probabilistic content masking, which\nencourages the model to focus on stylistically indicative words rather than\ncontent-specific words, and language-aware batching, which improves contrastive\nlearning by reducing cross-lingual interference. Our model is trained on over\n4.5 million authors across 36 languages and 13 domains. It consistently\noutperforms monolingual baselines in 21 out of 22 non-English languages,\nachieving an average Recall@8 improvement of 4.85%, with a maximum gain of\n15.91% in a single language. Furthermore, it exhibits stronger cross-lingual\nand cross-domain generalization compared to a monolingual model trained solely\non English. Our analysis confirms the effectiveness of both proposed\ntechniques, highlighting their critical roles in the model's improved\nperformance.", "AI": {"tldr": "This paper presents an improved approach to multilingual authorship representation learning through techniques like probabilistic content masking and language-aware batching, achieving better performance than monolingual baselines across multiple languages.", "motivation": "The motivation behind this paper is to explore the potential benefits of multilingual AR models as prior research has been mostly confined to monolingual settings, particularly in English.", "method": "The paper introduces a novel method for multilingual AR learning with two key innovations: probabilistic content masking and language-aware batching. This approach is trained on over 4.5 million authors from 36 languages and 13 domains.", "result": "The model outperforms monolingual baselines in 21 out of 22 non-English languages, showing an average Recall@8 improvement of 4.85% with a maximum gain of 15.91%.", "conclusion": "The paper concludes that both proposed techniques, probabilistic content masking and language-aware batching, are critical for the model's improved performance and that the multilingual approach exhibits stronger cross-lingual and cross-domain generalization compared to monolingual models."}}
{"id": "2509.16500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16500", "abs": "https://arxiv.org/abs/2509.16500", "authors": ["Tianyi Yan", "Wencheng Han", "Xia Zhou", "Xueyang Zhang", "Kun Zhan", "Cheng-zhong Xu", "Jianbing Shen"], "title": "RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation", "comment": "NeurIPS 2025", "summary": "Synthetic data is crucial for advancing autonomous driving (AD) systems, yet\ncurrent state-of-the-art video generation models, despite their visual realism,\nsuffer from subtle geometric distortions that limit their utility for\ndownstream perception tasks. We identify and quantify this critical issue,\ndemonstrating a significant performance gap in 3D object detection when using\nsynthetic versus real data. To address this, we introduce Reinforcement\nLearning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion\nmodels by incorporating rewards from specialized latent-space AD perception\nmodels. Its core components include an efficient Latent-Space Windowing\nOptimization technique for targeted feedback during diffusion, and a\nHierarchical Geometric Reward (HGR) system providing multi-level rewards for\npoint-line-plane alignment, and scene occupancy coherence. To quantify these\ndistortions, we propose GeoScores. Applied to models like DiVE on nuScenes,\nRLGF substantially reduces geometric errors (e.g., VP error by 21\\%, Depth\nerror by 57\\%) and dramatically improves 3D object detection mAP by 12.7\\%,\nnarrowing the gap to real-data performance. RLGF offers a plug-and-play\nsolution for generating geometrically sound and reliable synthetic videos for\nAD development.", "AI": {"tldr": "研究提出了一种新方法——带有几何反馈的强化学习（RLGF），用于改进视频生成模型中的几何准确性，显著提高了3D物体检测的性能。", "motivation": "识别并量化现有合成数据生成模型中的几何畸变问题，这些问题限制了它们在下游感知任务中的应用，尤其是在3D物体检测方面。", "method": "通过引入带有几何反馈的强化学习（RLGF）来改进视频扩散模型，该方法包括针对扩散过程中的反馈优化技术——高效意义上的窗口优化，以及提供多层级奖励的层次几何奖励系统，用于特征点、线和面的对齐和场景占用的一致性。", "result": "使用RLGF大大减少了视频生成中的几何错误，例如视点误差降低21%，深度误差降低57%，并将3D物体检测的mAP提升了12.7%。", "conclusion": "RLGF为生成适用于自动驾驶开发的高质量合成视频提供了一种即插即用的解决方案。"}}
{"id": "2509.16533", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16533", "abs": "https://arxiv.org/abs/2509.16533", "authors": ["Sungwon Kim", "Daniel Khashabi"], "title": "Challenging the Evaluator: LLM Sycophancy Under User Rebuttal", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) often exhibit sycophancy, distorting responses\nto align with user beliefs, notably by readily agreeing with user\ncounterarguments. Paradoxically, LLMs are increasingly adopted as successful\nevaluative agents for tasks such as grading and adjudicating claims. This\nresearch investigates that tension: why do LLMs show sycophancy when challenged\nin subsequent conversational turns, yet perform well when evaluating\nconflicting arguments presented simultaneously? We empirically tested these\ncontrasting scenarios by varying key interaction patterns. We find that\nstate-of-the-art models: (1) are more likely to endorse a user's\ncounterargument when framed as a follow-up from a user, rather than when both\nresponses are presented simultaneously for evaluation; (2) show increased\nsusceptibility to persuasion when the user's rebuttal includes detailed\nreasoning, even when the conclusion of the reasoning is incorrect; and (3) are\nmore readily swayed by casually phrased feedback than by formal critiques, even\nwhen the casual input lacks justification. Our results highlight the risk of\nrelying on LLMs for judgment tasks without accounting for conversational\nframing.", "AI": {"tldr": "研究表明，大型语言模型在对话中容易受用户影响，而其在评估任务中表现出良好的性能与对话中的逢迎行为存在冲突，研究发现了一些具体影响LLMs判断的因素，表明在对话框架下直接依赖LLMs做出判断有其风险。", "motivation": "研究动机在于揭示大型语言模型在对话中表现出的逢迎倾向与在评估任务中良好表现之间的冲突，为改进LLMs在适用性评估任务中的表现寻找依据。", "method": "通过改变交互模式，对LLMs在不同情况下的反应进行了实证测试，包括用户反驳观点的呈现方式、反驳中推理的细节程度以及反馈的正式程度。", "result": "该研究探讨了大型语言模型（LLMs）在对话中表现出逢迎行为，而在评估任务中却表现出良好的性能之间的矛盾。通过改变交互模式进行实证测试，研究发现：1) LLMs更容易同意用户提出的反驳观点，如果这些反驳观点作为用户的后续发言提出，而不是同时呈现以供评估；2) LLMs在面对含有详细推理（即使推理结论错误）的用户反驳时更容易被说服；3) LLMs更容易受非正式反馈的影响而不是正式批评，即使非正式反馈缺乏充分的论证。这些结果强调了在对话框架下依赖LLMs进行判断任务的风险。", "conclusion": "研究结果揭示了大型语言模型在对话过程中易于受到用户意见的影响，而当这些意见以不同方式呈现（如详细推理、非正式反馈）时，这种影响变得更加突出，表明在对话环境中的评价任务中，直接依赖LLMs需谨慎。"}}
{"id": "2509.16506", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16506", "abs": "https://arxiv.org/abs/2509.16506", "authors": ["Joe Barrow"], "title": "CommonForms: A Large, Diverse Dataset for Form Field Detection", "comment": null, "summary": "This paper introduces CommonForms, a web-scale dataset for form field\ndetection. It casts the problem of form field detection as object detection:\ngiven an image of a page, predict the location and type (Text Input, Choice\nButton, Signature) of form fields. The dataset is constructed by filtering\nCommon Crawl to find PDFs that have fillable elements. Starting with 8 million\ndocuments, the filtering process is used to arrive at a final dataset of\nroughly 55k documents that have over 450k pages. Analysis shows that the\ndataset contains a diverse mixture of languages and domains; one third of the\npages are non-English, and among the 14 classified domains, no domain makes up\nmore than 25% of the dataset.\n  In addition, this paper presents a family of form field detectors,\nFFDNet-Small and FFDNet-Large, which attain a very high average precision on\nthe CommonForms test set. Each model cost less than $500 to train. Ablation\nresults show that high-resolution inputs are crucial for high-quality form\nfield detection, and that the cleaning process improves data efficiency over\nusing all PDFs that have fillable fields in Common Crawl. A qualitative\nanalysis shows that they outperform a popular, commercially available PDF\nreader that can prepare forms. Unlike the most popular commercially available\nsolutions, FFDNet can predict checkboxes in addition to text and signature\nfields. This is, to our knowledge, the first large scale dataset released for\nform field detection, as well as the first open source models. The dataset,\nmodels, and code will be released at https://github.com/jbarrow/commonforms", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.16534", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16534", "abs": "https://arxiv.org/abs/2509.16534", "authors": ["Cheng Jiayang", "Qianqian Zhuang", "Haoran Li", "Chunkit Chan", "Xin Liu", "Lin Qiu", "Yangqiu Song"], "title": "InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Grounding large language models (LLMs) in external knowledge sources is a\npromising method for faithful prediction. While existing grounding approaches\nwork well for simple queries, many real-world information needs require\nsynthesizing multiple pieces of evidence. We introduce \"integrative grounding\"\n-- the challenge of retrieving and verifying multiple inter-dependent pieces of\nevidence to support a hypothesis query. To systematically study this problem,\nwe repurpose data from four domains for evaluating integrative grounding\ncapabilities. Our investigation reveals two critical findings: First, in\ngroundedness verification, while LLMs are robust to redundant evidence, they\ntend to rationalize using internal knowledge when information is incomplete.\nSecond, in examining retrieval planning strategies, we find that undirected\nplanning can degrade performance through noise introduction, while premise\nabduction emerges as a promising approach due to its logical constraints.\nAdditionally, LLMs' zero-shot self-reflection capabilities consistently improve\ngrounding quality. These insights provide valuable direction for developing\nmore effective integrative grounding systems.", "AI": {"tldr": "本文探讨了大语言模型在整合外部知识源进行准确预测时所面临的综合接地挑战。研究表明，语言模型在没有完整信息时倾向于使用内部知识进行推理，并且规划有目标的前提推断提升了性能。", "motivation": "现有方法对于简单的查询工作良好，但许多现实世界的信息需求需要合成多个证据片段。", "method": "介绍了一种称为“综合接地”的方法，即检索和验证多个相互依赖的证据以支持假设查询的挑战。为了系统地研究这个问题，从四个领域重新利用数据来评估综合接地能力。", "result": "研究揭示了两个关键发现：在接地验证中，虽然大语言模型对冗余证据具有鲁棒性，但当信息不完整时，它们倾向于使用内部知识进行解释。在检查检索规划策略时，发现无目标规划通过引入噪音会降低性能，而前提推断作为一种具有逻辑约束的方法表现出色。", "conclusion": "这些见解为开发更有效的综合接地系统提供了宝贵的方向。"}}
{"id": "2509.16507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16507", "abs": "https://arxiv.org/abs/2509.16507", "authors": ["Hanting Li", "Huaao Tang", "Jianhong Han", "Tianxiong Zhou", "Jiulong Cui", "Haizhen Xie", "Yan Chen", "Jie Hu"], "title": "OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution", "comment": null, "summary": "Recently, latent diffusion models has demonstrated promising performance in\nreal-world video super-resolution (VSR) task, which can reconstruct\nhigh-quality videos from distorted low-resolution input through multiple\ndiffusion steps. Compared to image super-resolution (ISR), VSR methods needs to\nprocess each frame in a video, which poses challenges to its inference\nefficiency. However, video quality and inference efficiency have always been a\ntrade-off for the diffusion-based VSR methods. In this work, we propose\nOne-Step Diffusion model for real-world Video Super-Resolution, namely\nOS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training\nparadigm, which can significantly improve the quality of synthetic videos.\nBesides, we devise a multi-frame fusion mechanism to maintain inter-frame\ntemporal consistency and reduce the flicker in video. Extensive experiments on\nseveral popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve\nbetter quality than existing diffusion-based VSR methods that require dozens of\nsampling steps.", "AI": {"tldr": "本文提出OS-DiffVSR一步扩散模型，通过新型相邻帧对抗训练及多帧融合机制，实现在一步中的高效率高质量视频超分辨。", "motivation": "针对视频超分辨率（VSR）方法在处理每一帧视频时面临的推理效率问题以及视频质量和推理效率之间的权衡，特别是针对扩散模型需要多步扩散来重建高质量视频的需求。", "method": "我们提出了一种名为OS-DiffVSR的一步扩散模型用于处理真实世界的视频超分辨率问题。该方法包括一种新颖的相邻帧对抗训练范式来提高合成视频的质量，以及一种多帧融合机制来保持帧间的时间一致性和减少视频闪烁。", "result": "实验结果表明，OS-DiffVSR能够实现优于现有的需要几十次采样步骤才能实现的扩散模型的视频质量。", "conclusion": "该工作通过提出OS-DiffVSR，实现了在一步内提高真实世界视频超分辨率的质量同时保持高效推理性能，超越了需要多步扩散过程的现有方法。"}}
{"id": "2509.16542", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16542", "abs": "https://arxiv.org/abs/2509.16542", "authors": ["Khalid Hasan", "Jamil Saquer", "Yifan Zhang"], "title": "Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models", "comment": "24th IEEE International Conference on Machine Learning and\n  Applications, ICMLA 2025 (camera-ready)", "summary": "Millions of people openly share mental health struggles on social media,\nproviding rich data for early detection of conditions such as depression,\nbipolar disorder, etc. However, most prior Natural Language Processing (NLP)\nresearch has focused on single-disorder identification, leaving a gap in\nunderstanding the efficacy of advanced NLP techniques for distinguishing among\nmultiple mental health conditions. In this work, we present a large-scale\ncomparative study of state-of-the-art transformer versus Long Short-Term Memory\n(LSTM)-based models to classify mental health posts into exclusive categories\nof mental health conditions. We first curate a large dataset of Reddit posts\nspanning six mental health conditions and a control group, using rigorous\nfiltering and statistical exploratory analysis to ensure annotation quality. We\nthen evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,\nALBERT, and ELECTRA) against several LSTM variants (with or without attention,\nusing contextual or static embeddings) under identical conditions. Experimental\nresults show that transformer models consistently outperform the alternatives,\nwith RoBERTa achieving 91-99% F1-scores and accuracies across all classes.\nNotably, attention-augmented LSTMs with BERT embeddings approach transformer\nperformance (up to 97% F1-score) while training 2-3.5 times faster, whereas\nLSTMs using static embeddings fail to learn useful signals. These findings\nrepresent the first comprehensive benchmark for multi-class mental health\ndetection, offering practical guidance on model selection and highlighting an\naccuracy-efficiency trade-off for real-world deployment of mental health NLP\nsystems.", "AI": {"tldr": "本研究对比了多次LSTM及变压器模型在心理健康诊断中的表现，发现在相同条件下，变压器模型尤其是RoBERTa优越于其他模型。", "motivation": "尽管大多数先前的自然语言处理研究都专注于单一疾病识别，但本研究旨在了解先进的NLP技术在区分多种心理健康状况方面的效果。", "method": "本研究通过比较五种基于变压器的模型（BERT, RoBERTa, DistilBERT, ALBERT, 和ELECTRA）和几种LSTM模型（带有或不带有注意力机制，使用上下文或静态嵌入）来分类心理健康帖子，以区分六种心理健康状况和一个对照组。", "result": "实验结果显示，基于变压器的模型始终优于其他模型，其中RoBERTa在所有类别中实现91-99%的F1分数和准确率。值得注意的是，带有BERT嵌入的注意力增强LSTM模型接近变压器模型的性能（高达97% 的F1分数），而训练速度却快2-3.5倍。使用静态嵌入的LSTM模型未能学习到有用的信号。", "conclusion": "这些发现代表了多类别心理健康检测的首个全面基准，为模型选择提供了实用指导，并强调了实际部署心理健康NLP系统时的精度与效率之间的权衡。"}}
{"id": "2509.16509", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16509", "abs": "https://arxiv.org/abs/2509.16509", "authors": ["Haijin Zeng", "Xuan Lu", "Yurong Zhang", "Yongyong Chen", "Jingyong Su", "Jie Liu"], "title": "SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging", "comment": "12 pages", "summary": "Humans learn in two complementary ways: a slow, cumulative process that\nbuilds broad, general knowledge, and a fast, on-the-fly process that captures\nspecific experiences. Existing deep-unfolding methods for spectral compressive\nimaging (SCI) mirror only the slow component-relying on heavy pre-training with\nmany unfolding stages-yet they lack the rapid adaptation needed to handle new\noptical configurations. As a result, they falter on out-of-distribution\ncameras, especially in bespoke spectral setups unseen during training. This\ndepth also incurs heavy computation and slow inference. To bridge this gap, we\nintroduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any\ndeep unfolding network beyond SCI systems. During slow learning, we pre-train\nor reuse a priors-based backbone and distill it via imaging guidance into a\ncompact fast-unfolding model. In the fast learning stage, lightweight\nadaptation modules are embedded within each block and trained self-supervised\nat test time via a dual-domain loss-without retraining the backbone. To the\nbest of our knowledge, SlowFast-SCI is the first test-time adaptation-driven\ndeep unfolding framework for efficient, self-adaptive spectral reconstruction.\nIts dual-stage design unites offline robustness with on-the-fly per-sample\ncalibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB\nPSNR improvement on out-of-distribution data, preserved cross-domain\nadaptability, and a 4x faster adaptation speed. In addition, its modularity\nintegrates with any deep-unfolding network, paving the way for self-adaptive,\nfield-deployable imaging and expanded computational imaging modalities. Code\nand models are available at https://github.com/XuanLu11/SlowFast-SCI.", "AI": {"tldr": "SlowFast-SCI框架可以实现高效的自我适应光谱重建，具备比现有方法更强的泛化能力和更快的适应速度，同时减少参数和计算量。", "motivation": "现存的用于光谱压缩成像的深度展开方法仅模仿人类缓慢的学习过程，缺乏适应新型光学配置的能力，且计算量大、推理速度慢。SlowFast-SCI旨在解决这些问题。", "method": "SlowFast-SCI框架，结合慢速学习和快速学习。慢速学习阶段预训练基于先验的模型，并通过成像指导提炼出紧凑的快速展开模型。快速学习阶段在每个块中嵌入轻量级适应模块，并通过双重域损失实现无监督自适应训练。", "result": "相比于现有模型，SlowFast-SCI参数和计算量减少了70%，在未见分布数据上的PSNR提高5.79 dB，适应速度提升4倍，并且保持跨域适应性。", "conclusion": "SlowFast-SCI作为首个测试时自适应驱动的深度展开框架，拥有新奇的双阶段设计思想，这将为计算成像提供更多可能。"}}
{"id": "2509.16543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16543", "abs": "https://arxiv.org/abs/2509.16543", "authors": ["Yue Huang", "Zhengzhe Jiang", "Xiaonan Luo", "Kehan Guo", "Haomin Zhuang", "Yujun Zhou", "Zhengqing Yuan", "Xiaoqi Sun", "Jules Schleinitz", "Yanbo Wang", "Shuhao Zhang", "Mihir Surve", "Nitesh V Chawla", "Olaf Wiest", "Xiangliang Zhang"], "title": "ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions", "comment": null, "summary": "Empowering large language models (LLMs) with chemical intelligence remains a\nchallenge due to the scarcity of high-quality, domain-specific\ninstruction-response datasets and the misalignment of existing synthetic data\ngeneration pipelines with the inherently hierarchical and rule-governed\nstructure of chemical information. To address this, we propose ChemOrch, a\nframework that synthesizes chemically grounded instruction-response pairs\nthrough a two-stage process: task-controlled instruction generation and\ntool-aware response construction. ChemOrch enables controllable diversity and\nlevels of difficulty for the generated tasks, and ensures response precision\nthrough tool planning and distillation, and tool-based self-repair mechanisms.\nThe effectiveness of ChemOrch is evaluated based on: 1) the high quality of\ngenerated instruction data, demonstrating superior diversity and strong\nalignment with chemical constraints; 2) the reliable generation of evaluation\ntasks that more effectively reveal LLM weaknesses in chemistry; and 3) the\nsignificant improvement of LLM chemistry capabilities when the generated\ninstruction data are used for fine-tuning. Our work thus represents a critical\nstep toward scalable and verifiable chemical intelligence in LLMs.", "AI": {"tldr": "本文提出了ChemOrch框架，用于增强大型语言模型的化学智能。通过两阶段生成化学相关的指令和响应，并确保生成数据的质量和适用性。实验结果展示了ChemOrch在生成高质量数据、生成评估任务以及提升模型化学能力方面的有效性。", "motivation": "为大型语言模型（LLM）赋能化学智能面临挑战，主要是因为高质量的特定领域指令-响应数据集稀缺，现有合成数据生成管线与化学信息的层次结构和规则治理不匹配。ChemOrch框架提出解决这些问题。", "method": "ChemOrch框架通过两阶段过程合成与化学相关的指令-响应对：任务控制的指令生成和工具感知的响应构建。ChemOrch能够控制生成任务的多样性和难度水平，并通过工具规划和蒸馏及基于工具的自我修复机制确保响应的精确性。", "result": "ChemOrch的有效性评估基于以下三点：1) 生成的指令数据质量高，展现了良好的多样性和与化学约束的强一致性；2) 生成评估任务的可靠性，更有效地揭示LLM在化学方面的弱点；3) 使用生成的指令数据进行微调时，显著提升了LLM在化学方面的能力。", "conclusion": "该工作代表了向具有可扩展性和可验证的化学智能LLMs发展的重要一步。"}}
{"id": "2509.16517", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.16517", "abs": "https://arxiv.org/abs/2509.16517", "authors": ["Burak Satar", "Zhixin Ma", "Patrick A. Irawan", "Wilfried A. Mulyawan", "Jing Jiang", "Ee-Peng Lim", "Chong-Wah Ngo"], "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding", "comment": "Accepted to EMNLP 2025 Main Conference,\n  https://seeingculture-benchmark.github.io/", "summary": "Multimodal vision-language models (VLMs) have made substantial progress in\nvarious tasks that require a combined understanding of visual and textual\ncontent, particularly in cultural understanding tasks, with the emergence of\nnew cultural datasets. However, these datasets frequently fall short of\nproviding cultural reasoning while underrepresenting many cultures. In this\npaper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural\nreasoning with a novel approach that requires VLMs to reason on culturally rich\nimages in two stages: i) selecting the correct visual option with\nmultiple-choice visual question answering (VQA), and ii) segmenting the\nrelevant cultural artifact as evidence of reasoning. Visual options in the\nfirst stage are systematically organized into three types: those originating\nfrom the same country, those from different countries, or a mixed group.\nNotably, all options are derived from a singular category for each type.\nProgression to the second stage occurs only after a correct visual option is\nchosen. The SCB benchmark comprises 1,065 images that capture 138 cultural\nartifacts across five categories from seven Southeast Asia countries, whose\ndiverse cultures are often overlooked, accompanied by 3,178 questions, of which\n1,093 are unique and meticulously curated by human annotators. Our evaluation\nof various VLMs reveals the complexities involved in cross-modal cultural\nreasoning and highlights the disparity between visual reasoning and spatial\ngrounding in culturally nuanced scenarios. The SCB serves as a crucial\nbenchmark for identifying these shortcomings, thereby guiding future\ndevelopments in the field of cultural reasoning.\nhttps://github.com/buraksatar/SeeingCulture", "AI": {"tldr": "本文介绍了Seeing Culture Benchmark (SCB)，通过两阶段方法评估多模态视觉语言模型在文化理解任务中的表现，强调了文化多样性在模型训练中的重要性。", "motivation": "随着新的文化数据集的出现，现有的数据集在提供文化推理方面存在不足，并且忽视了许多文化。为了解决这个问题，本文提出了一种新的评估标准。", "method": "SCB基准测试集中在文化推理上，要求多模态视觉语言模型通过多重选择视觉问题回答来选择正确的视觉选项，并在第一阶段选择正确选项后分割相关的文化艺术品作为推理证据。", "result": "评估结果显示，在跨模态文化推理任务中，多模态视觉语言模型存在复杂性和视觉推理与空间定位之间的差异，特别是在文化细微差异的情景中。", "conclusion": "SCB基准测试对于揭示这些不足之处至关重要，也将引导未来文化推理领域的研究和发展。"}}
{"id": "2509.16551", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16551", "abs": "https://arxiv.org/abs/2509.16551", "authors": ["Dan John Velasco", "Matthew Theodore Roque"], "title": "Rethinking the Role of Text Complexity in Language Model Pretraining", "comment": "To be published in BabyLM Workshop at EMNLP 2025", "summary": "Improving pretraining data quality and size is known to boost downstream\nperformance, but the role of text complexity is less explored. Text complexity\nrefers to how hard a text is to read, and is typically estimated from surface\ncues such as sentence length, word choice, and sentence structure. We reduce\nsurface-level complexity--shorter sentences, simpler words, simpler\nstructure--while keeping core text content close to constant, and ask: (1) How\ndoes complexity affect language modeling across model sizes? (2) Can useful\nrepresentations be learned from simpler text alone? (3) How does pretraining\ntext complexity influence downstream language understanding? To answer these\nquestions, we simplify human-written texts using a large language model, then\npretrain causal models (28M-500M) from scratch on both original and simplified\ndata, and evaluate them in finetuning and zero-shot setups. We find that\nperplexity is sensitive to the interaction between model capacity and text\ncomplexity--smaller models degrade far less on simpler texts--while text\ncomplexity has little impact on finetuning evaluations, with zero-shot\nevaluations indicating that simpler texts benefit performance on linguistic\nknowledge tasks, whereas more complex texts favor tasks requiring world\nknowledge and entity tracking.", "AI": {"tldr": "研究发现语言建模的困惑度对模型容量和文本复杂性之间的交互作用非常敏感，而文本复杂性对微调评估影响小，但对零样本设置下的任务有影响，简单文本在语言知识任务上有优势，而复杂文本则利于需要世界知识和实体追踪的任务。", "motivation": "文本复杂度对语言模型预训练和下游任务效果的影响尚不明确，本研究旨在探索通过简化文本复杂度可以了解哪些类型的表示可以独立从简化文本中学习，以及预训练文本复杂性如何影响下游语言理解任务。", "method": "我们通过减少文本表面复杂性（如短句、简单词汇和结构），同时保持核心文本内容大致不变来进行实验。我们使用大型语言模型简化人类编写的文本，然后从头开始在原始和简化过的数据上对因果模型（28M-500M参数）进行预训练，并在微调和零样本设置下进行评估。", "result": "研究发现，困惑度对模型容量和文本复杂性之间的交互作用敏感，而文本复杂性对微调任务的影响不大，但在零样本评估中，简单文本在语言知识任务上有助于提高性能，复杂文本则在需要世界知识和实体追踪的任务中有优势。", "conclusion": "研究结果表明，在预训练过程中使用简化文本可以提高模型在某些任务上的性能，特别是在零样本设置下的语言知识任务。但复杂文本更适合需要世界知识和实体追踪的任务。因此，根据下游任务的不同，使用不同复杂度的文本进行预训练可能会有不同的效果。"}}
{"id": "2509.16518", "categories": ["cs.CV", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.16518", "abs": "https://arxiv.org/abs/2509.16518", "authors": ["Sankeerth Durvasula", "Kavya Sreedhar", "Zain Moustafa", "Suraj Kothawade", "Ashish Gondimalla", "Suvinay Subramanian", "Narges Shahidi", "Nandita Vijaykumar"], "title": "FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers", "comment": null, "summary": "Generating realistic videos with diffusion transformers demands significant\ncomputation, with attention layers the central bottleneck; even producing a\nshort clip requires running a transformer over a very long sequence of\nembeddings, e.g., more than 30K embeddings for a 5-second video, incurring\nsignificant latency. Prior work aims to mitigate this bottleneck by exploiting\nsparsity in the attention layers to reduce computation. However, these works\ntypically rely on block-sparse attention, which skips score computation only\nwhen all entries in a block of attention scores (corresponding to M queries and\nM keys, with M = 64 typically) are zero. This coarse-granular skipping of\nattention scores does not fully exploit sparsity in the attention map and\nleaves room for improvement. In this work, we propose FG-Attn, a sparse\nattention mechanism for long-context diffusion transformers that leverages\nsparsity at a fine granularity. Unlike block-sparse attention, which skips\nentire MxM blocks, our approach skips computations at the granularity of Mx1\nslices of the attention map. Each slice is produced by query-key dot products\nbetween a block of query vectors and a single key. To implement our proposed\nsparse attention mechanism, we develop a new efficient bulk-load operation\ncalled asynchronous-gather load. This load operation gathers a sparse set of\nrelevant key-value vectors from memory and arranges them into packed tiles in\nthe GPU's shared memory. Only a sparse set of keys relevant to those queries\nare loaded into shared memory when computing attention for a block of queries,\nin contrast to loading full blocks of key tokens in block-sparse attention. Our\nfine-grained sparse attention, applied to video diffusion models, achieves an\naverage 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average\n1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.", "AI": {"tldr": "本研究旨在改进扩散变换器生成视频时的计算效率问题，提出FG-Attn方法通过提高稀疏注意力计算的精细度，实现了显著的提速。", "motivation": "现有的稀疏注意力方法，如块稀疏注意力，只能在注意力得分矩阵的整个MxM块都是零时跳过得分计算，这种方法在利用注意力图的稀疏性方面较为粗糙，因此还有改进的空间。", "method": "研究中引入了一种新的高效批量加载操作，称为异步-聚集加载。这种方法从内存中聚集一小组相关的键值向量，并将它们排列在GPU的共享内存中的密集块中。这使得在计算查询块的注意力时，只加载与这些查询相关的稀疏键，而非像块稀疏注意力那样加载完整的键令牌块。", "result": "通过提出FG-Attn，一种适用于长上下文扩散转换器的精细粒度稀疏注意力机制，显著提升了视频生成中的计算效率。实验结果表明，对于5秒的480p和720p视频，FG-Attn平均可提升1.55倍及1.41倍的速度，分别达到最高1.65倍和1.49倍的速度提升，这使得在单个H100 GPU上生成真实视频变得更加高效。", "conclusion": "本研究提出了一种更精细粒度的方法来实现稀疏注意力，即FG-Attn，并开发了一种新的高效批量加载操作来实现这种方法，这种方法相较于传统的块稀疏注意力机制在视频生成过程中实现了显著的速度提升。"}}
{"id": "2509.16564", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.16564", "abs": "https://arxiv.org/abs/2509.16564", "authors": ["Jun Rong Brian Chong", "Yixuan Tang", "Anthony K. H. Tung"], "title": "MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs", "comment": "35 pages, 8 figures", "summary": "Misinformation evolves as it spreads, shifting in language, framing, and\nmoral emphasis to adapt to new audiences. However, current misinformation\ndetection approaches implicitly assume that misinformation is static. We\nintroduce MPCG, a multi-round, persona-conditioned framework that simulates how\nclaims are iteratively reinterpreted by agents with distinct ideological\nperspectives. Our approach uses an uncensored large language model (LLM) to\ngenerate persona-specific claims across multiple rounds, conditioning each\ngeneration on outputs from the previous round, enabling the study of\nmisinformation evolution. We evaluate the generated claims through human and\nLLM-based annotations, cognitive effort metrics (readability, perplexity),\nemotion evocation metrics (sentiment analysis, morality), clustering,\nfeasibility, and downstream classification. Results show strong agreement\nbetween human and GPT-4o-mini annotations, with higher divergence in fluency\njudgments. Generated claims require greater cognitive effort than the original\nclaims and consistently reflect persona-aligned emotional and moral framing.\nClustering and cosine similarity analyses confirm semantic drift across rounds\nwhile preserving topical coherence. Feasibility results show a 77% feasibility\nrate, confirming suitability for downstream tasks. Classification results\nreveal that commonly used misinformation detectors experience macro-F1\nperformance drops of up to 49.7%. The code is available at\nhttps://github.com/bcjr1997/MPCG", "AI": {"tldr": "研究通过一种名为MPCG的框架，探索了虚假信息在传播过程中随着视角不同而发生的演变，并进行了多方面的评估，结果显示MPCG生成的声明在情感、道德上更加与角色一致，并对现有虚假信息检测技术提出了挑战。", "motivation": "现有的虚假信息检测方法隐性假设虚假信息是静态的，但虚假信息随着传播而演变，因此提出了MPCG框架，用于研究虚假信息的演变。", "method": "介绍了一种名为MPCG的多轮次、角色条件框架，通过使用无审查的大语言模型(LLM)生成多个轮次的角色特定声明，并在每轮次输出的基础上进行条件生成，以模拟声明是如何被具有不同意识形态视角的代理人重新解释的。", "result": "实验结果显示，人类和GPT-4o-mini注释之间存在高度一致的评价，但在流畅性评分上存在较大差异。生成的声明需要比原始声明更高的认知努力，并且在情感和道德上更加与角色一致。聚类和余弦相似度分析证实了语义上的漂移以及主题上的连贯性。可行性结果表明可行率为77%，适用于下游任务。分类结果显示，常用的虚假信息检测器的宏观F1性能下降了高达49.7%。", "conclusion": "MPCG框架有效地模拟了虚假信息在传播过程中的演变，揭示了现有虚假信息检测技术在面对动态虚假信息时的局限性，证明了该框架在下游任务中的适用性。"}}
{"id": "2509.16519", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16519", "abs": "https://arxiv.org/abs/2509.16519", "authors": ["Yang Han"], "title": "PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality", "comment": null, "summary": "We introduce PM25Vision (PM25V), the largest and most comprehensive dataset\nto date for estimating air quality - specifically PM2.5 concentrations - from\nstreet-level images. The dataset contains over 11,114 images matched with\ntimestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations\nand 11 years, significantly exceeding the scale of previous benchmarks. The\nspatial accuracy of this dataset has reached 5 kilometers, far exceeding the\ncity-level accuracy of many datasets. We describe the data collection,\nsynchronization, and cleaning pipelines, and provide baseline model\nperformances using CNN and transformer architectures. Our dataset is publicly\navailable.", "AI": {"tldr": "PM25Vision 是迄今最大且最全面用于从街景图像估算PM2.5浓度的数据集，包含超过11,114张图像，空间准确度达到5公里，提供了CNN和Transformer架构的基线模型性能。", "motivation": "该研究旨在创建一个大规模且准确的数据集，以从街景图像中估算空气质量，特别关注PM2.5浓度的估算。", "method": "研究人员收集了11,114张街景图像，并与3,261个AQI监测站的PM2.5读数进行时间戳和地理定位匹配，跨时11年。描述了数据收集、同步和清理的管道，并提供了使用CNN和Transformer架构的基线模型性能。", "result": "该数据集包括超过11,114张图像，时间跨度为11年，跨越3,261个空气质量管理站，空间准确度达到5公里。", "conclusion": "PM25Vision是一个公开可用的、进行空气质量估计的基础数据集。这个数据集的推出将促进基于图像的空气质量监测的研究和技术进步。"}}
{"id": "2509.16584", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16584", "abs": "https://arxiv.org/abs/2509.16584", "authors": ["Benlu Wang", "Iris Xia", "Yifan Zhang", "Junda Wang", "Feiyun Ouyang", "Shuo Han", "Arman Cohan", "Hong Yu", "Zonghai Yao"], "title": "From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations", "comment": "Equal contribution for the first two authors. To appear as an Oral\n  presentation in the proceedings of the Main Conference on Empirical Methods\n  in Natural Language Processing (EMNLP) 2025", "summary": "Large language models (LLMs) have demonstrated promising performance on\nmedical benchmarks; however, their ability to perform medical calculations, a\ncrucial aspect of clinical decision-making, remains underexplored and poorly\nevaluated. Existing benchmarks often assess only the final answer with a wide\nnumerical tolerance, overlooking systematic reasoning failures and potentially\ncausing serious clinical misjudgments. In this work, we revisit medical\ncalculation evaluation with a stronger focus on clinical trustworthiness.\nFirst, we clean and restructure the MedCalc-Bench dataset and propose a new\nstep-by-step evaluation pipeline that independently assesses formula selection,\nentity extraction, and arithmetic computation. Under this granular framework,\nthe accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by\nprior evaluations. Second, we introduce an automatic error analysis framework\nthat generates structured attribution for each failure mode. Human evaluation\nconfirms its alignment with expert judgment, enabling scalable and explainable\ndiagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that\ncombines retrieval-augmented generation and Python-based code execution.\nWithout any fine-tuning, MedRaC improves the accuracy of different LLMs from\n16.35% up to 53.19%. Our work highlights the limitations of current benchmark\npractices and proposes a more clinically faithful methodology. By enabling\ntransparent and transferable reasoning evaluation, we move closer to making\nLLM-based systems trustworthy for real-world medical applications.", "AI": {"tldr": "研究清理并改进了MedCalc-Bench数据集，提出了更细致的评估方法和自动错误分析框架，并开发了MedRaC系统，提高了大语言模型在医疗计算任务中的准确性和可信度。", "motivation": "研究动机在于，目前的大语言模型在医学计算方面的能力尚未得到充分探索和评估，而医学计算是临床决策的关键方面。现有的基准测试通常只评估最终答案并且有较大的数值容差，忽略了系统性推理失败，可能会导致严重的临床误判。", "method": "该研究首先清理并重构了MedCalc-Bench数据集，并提出了一种新的分步评估流水线，该流水线独立评估公式选择、实体抽取和算术计算。其次，引入了一种自动错误分析框架，可以为每种失败模式生成结构化归因。最后，提出了一个模块化代理流水线MedRaC，它结合了检索增强生成和基于Python的代码执行，无需任何微调即可提高不同大语言模型的准确性。", "result": "研究发现，采用新的评估框架后，GPT-4的准确率从62.7%下降到43.6%，揭示了一些之前被掩盖的误差。MedRaC系统在不同的大语言模型上无需调整即可将准确率从16.35%提高到53.19%。", "conclusion": "该研究揭示了现有评估实践中大语言模型存在的局限性，并提出了一种更加符合临床实际情况的方法。通过实现透明和可转移的推理评估，使得基于大语言模型的系统更接近于在实际医学应用中信任可靠的级别。"}}
{"id": "2509.16527", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16527", "abs": "https://arxiv.org/abs/2509.16527", "authors": ["Guangze Zheng", "Shijie Lin", "Haobo Zuo", "Si Si", "Ming-Shan Wang", "Changhong Fu", "Jia Pan"], "title": "Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity", "comment": "NeurIPS 2025. Project page: https://george-zhuang.github.io/lbm/", "summary": "This work proposes the Lattice Boltzmann Model (LBM) to learn real-world\npixel dynamicity for visual tracking. LBM decomposes visual representations\ninto dynamic pixel lattices and solves pixel motion states through\ncollision-streaming processes. Specifically, the high-dimensional distribution\nof the target pixels is acquired through a multilayer predict-update network to\nestimate the pixel positions and visibility. The predict stage formulates\nlattice collisions among the spatial neighborhood of target pixels and develops\nlattice streaming within the temporal visual context. The update stage\nrectifies the pixel distributions with online visual representations. Compared\nwith existing methods, LBM demonstrates practical applicability in an online\nand real-time manner, which can efficiently adapt to real-world visual tracking\ntasks. Comprehensive evaluations of real-world point tracking benchmarks such\nas TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of\nlarge-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B\nfurther demonstrates LBM's real-world practicality.", "AI": {"tldr": "This paper introduces a real-time and efficient method, Lattice Boltzmann Model (LBM), for visual tracking by decomposing visual information into dynamic pixel lattices, validated on various benchmarks.", "motivation": "The motivation behind this research is to address the limitations of current methods in real-time and online visual tracking, proposing a more efficient and adaptive model based on the Lattice Boltzmann method.", "method": "This paper proposes the Lattice Boltzmann Model (LBM) to track visual pixel dynamics. It decomposes visual information into pixel lattices and uses collision-streaming processes to solve for pixel motion states. The model employs a multilayer predict-update network to estimate pixel positions and visibility, using spatial and temporal processes.", "result": "Evaluations demonstrate that LBM can efficiently adapt to visual tracking tasks in real-world scenarios and performs well on benchmarks such as TAP-Vid, RoboTAP, TAO, BFT, and OVT-B, highlighting its practicality.", "conclusion": "The LBM method shows practical applicability and real-time efficiency for real-world visual tracking tasks, validated by performance on real-world benchmarks like TAP-Vid, RoboTAP, and open-world object tracking benchmarks such as TAO, BFT, and OVT-B."}}
{"id": "2509.16589", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16589", "abs": "https://arxiv.org/abs/2509.16589", "authors": ["Qiongqiong Wang", "Hardik Bhupendra Sailor", "Tianchi Liu", "Wenyu Zhang", "Muhammad Huzaifah", "Nattadaporn Lertcheva", "Shuo Sun", "Nancy F. Chen", "Jinyang Wu", "AiTi Aw"], "title": "Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data", "comment": "Accepted in EMNLP Findings 2025", "summary": "Recent speech-LLMs have shown impressive performance in tasks like\ntranscription and translation, yet they remain limited in understanding the\nparalinguistic aspects of speech crucial for social and emotional intelligence.\nWe propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual\nparalinguistic reasoning the integration of verbal content with non-verbal cues\nlike emotion and prosody. The benchmark includes two curated question answering\n(QA) datasets requiring both linguistic and empathetic understanding. We\nevaluate state-of-the-art speech-LLMs from both open and closed-source models\nand perform a comprehensive analysis across different question types. The top\ntwo models were further analyzed under temperature tuning to understand its\neffect on this task. Our benchmark reveals a key gap in existing evaluations\nand offers insights into building more context-aware and emotionally\nintelligent speech-capable LLMs.", "AI": {"tldr": "介绍CP-Bench，一个针对语音LLMs在情境性语用推理方面进行评估的新基准，显示出理解和整合情感和韵律的重要性。", "motivation": "评估语音LLMs在理解情境性和情感方面的能力是迫切的需求，因为这是他们实现更广泛应用的关键一步。", "method": "开发了CP-Bench，这是一个评估语音LLMs在理解和整合非语言线索方面能力的基准，通过问答数据集来评估它们的表现。", "result": "最近的语音LLMs在转录和翻译等任务中表现出色，但在理解对于社交和情感智能至关重要的语用学方面仍然有限。我们提出了CP-Bench，一个评估语音LLMs在情境性语用推理方面能力的基准，该基准涉及语言内容与情感和韵律等非语言线索的整合。这个基准包括两个需要语言和同理心理解的问题回答（QA）数据集。我们评估了来自开源和闭源模型的最先进的语音LLMs，并对不同的问题类型进行了全面分析。我们进一步分析了排名前两的模型在温度调整下的表现，以理解其对任务的影响。我们的基准测试揭示了现有评估的关键差距，并为构建更具有语境感知能力和情感智能的语音LLMs提供了见解。", "conclusion": "CP-Bench揭示了现有评估方法的关键差距，为改进语音LLMs的理解能力提供了有价值的见解和方向。"}}
{"id": "2509.16538", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16538", "abs": "https://arxiv.org/abs/2509.16538", "authors": ["Shubhashis Roy Dipta", "Tz-Ying Wu", "Subarna Tripathi"], "title": "Advancing Reference-free Evaluation of Video Captions with Factual Analysis", "comment": null, "summary": "Video captions offer concise snapshots of actors, objects, and actions within\na video, serving as valuable assets for applications such as question answering\nand event localization. However, acquiring human annotations for video captions\nis costly or even impractical, especially when dealing with diverse video\ndomains. Existing models trained on supervised datasets face challenges in\nevaluating performance across different domains due to the reliance on\nreference-based evaluation protocols, which necessitate ground truth captions.\nThis assumption is unrealistic for evaluating videos in the wild. To address\nthese limitations, we propose a reference-free evaluation framework that does\nnot require ground truth captions, focusing on factual grounding to ensure\naccurate assessment of caption quality. We introduce VC-Inspector, a novel\ncaption quality evaluator that is both reference-free and factually grounded.\nUtilizing large language models, we generate pseudo captions of varying quality\nbased on supervised data, which are subsequently used to train a multimodal\nmodel (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior\nalignment with human judgments on the VATEX-Eval dataset, outperforming\nexisting methods. The performance also generalizes to image caption datasets,\nFlickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.\nOverall, VC-Inspector offers a scalable and generalizable solution for\nevaluating the factual accuracy of video captions, paving the way for more\neffective and objective assessment methodologies in diverse video domains.", "AI": {"tldr": "论文提出了一个无需参考标注，专注于事实校验的视频字幕质量评估方法VC-Inspector，利用大型语言模型生成伪标注以训练模型，提高了对视频字幕事实准确性的评估。", "motivation": "由于获取人类标注的视频字幕成本高且困难，现有的依赖参考标注的评估方式不适用于评估野外视频。本论文旨在开发一种新的评估方法，以解决这些问题。", "method": "该论文提出了一种无需参考标注的新评估框架VC-Inspector，它使用大模型生成伪标注以训练一个多模态模型，用于评估视频字幕的真实性。", "result": "该方法在VATEX-Eval数据集上展示了与人类判断的高度一致性，并在Flickr8K-Expert和Flickr8K-CF图像描述数据集上具有泛化性能。", "conclusion": "VC-Inspector提供了一种可扩展且普遍适用的方案，用于评估视频字幕的事实准确性，这对多样化的视频领域具有重要价值。"}}
{"id": "2509.16591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16591", "abs": "https://arxiv.org/abs/2509.16591", "authors": ["Zheng Liu", "Mengjie Liu", "Siwei Wen", "Mengzhang Cai", "Bin Cui", "Conghui He", "Wentao Zhang"], "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature", "comment": null, "summary": "Reinforcement Learning has emerged as the fundamental technique for enhancing\nreasoning in LLMs. However, existing algorithms apply uniform optimization to\nall tokens, ignoring their different roles in reasoning process. To address\nthis limitation, we introduce Heterogeneous Adaptive Policy Optimization\n(HAPO), a comprehensive token-aware algorithm that dynamically adapts\noptimization based on token entropy. For rollout sampling, we propose Adaptive\nTemperature Sampling, which adjusts sampling temperature in real time,\npromoting exploration at high-entropy tokens while preserving coherence at\nlow-entropy ones. For advantage calculation, we introduce Token Level Group\nAverage that normalizes advantages at token level, jointly accounting for\nsequence-length as in token-mean loss while preserving non-biased treatment. We\nthen develop Differential Advantage Redistribution that leverages entropy and\nimportance ratios to modulate rewards-adjusting updates for tokens with clear\nsignals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing\naggressive probability reduction for noisy low-entropy tokens while enabling\nexploration for high-entropy tokens. Through systematic investigation between\nentropy and training dynamics, we embedded token-level treatment into every\nstages to achieve fine-grained control. Extensive experiments demonstrate that\nHAPO consistently outperforms DAPO across multiple model scales. Our code can\nbe found in https://github.com/starriver030515/HAPO.", "AI": {"tldr": "本文提出了HAPO算法，通过异构自适应策略优化，实现token级别的细粒度控制和优化。实验表明，该方法在多个模型规模中，性能优于DAPO。", "motivation": "强化学习已经成为提升LLMs推理能力的核心技术。然而，现有的算法在对所有token进行统一优化时，忽略了它们在推理过程中的不同作用。为了解决这个限制，该研究引入了一种全面基于token的算法。", "method": "提出了异构自适应策略优化(HAPO)，一种全面的基于token的算法，能根据token熵动态调整优化策略。在采样过程中，提出自适应温度采样，根据高熵和低熵token调整采样温度。在计算优势时，提出了Token级别的群组平均，对token级别的优势进行规范化处理，同时考虑序列长度并保持无偏处理。差异优势重分配机制则利用熵和重要性比例来调节奖励，对信号明确的token进行更新调整。对于裁剪损失，设计了对低熵token有积极限制作用，而对高熵token则保持探索性的非对称自适应裁剪。", "result": "实验结果表明，HAPO在多个模型规模上相比DAPO均有提升。", "conclusion": "研究表明，通过细致地将token级处理嵌入到每个阶段，实现了对LLM训练的精确控制，改善了模型推理能力。"}}
{"id": "2509.16549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16549", "abs": "https://arxiv.org/abs/2509.16549", "authors": ["Zirui Wang", "Jiayi Zhang", "Tianwei Guan", "Yuhan Zhou", "Xingyuan Li", "Minjing Dong", "Jinyuan Liu"], "title": "Efficient Rectified Flow for Image Fusion", "comment": null, "summary": "Image fusion is a fundamental and important task in computer vision, aiming\nto combine complementary information from different modalities to fuse images.\nIn recent years, diffusion models have made significant developments in the\nfield of image fusion. However, diffusion models often require complex\ncomputations and redundant inference time, which reduces the applicability of\nthese methods. To address this issue, we propose RFfusion, an efficient\none-step diffusion model for image fusion based on Rectified Flow. We\nincorporate Rectified Flow into the image fusion task to straighten the\nsampling path in the diffusion model, achieving one-step sampling without the\nneed for additional training, while still maintaining high-quality fusion\nresults. Furthermore, we propose a task-specific variational autoencoder (VAE)\narchitecture tailored for image fusion, where the fusion operation is embedded\nwithin the latent space to further reduce computational complexity. To address\nthe inherent discrepancy between conventional reconstruction-oriented VAE\nobjectives and the requirements of image fusion, we introduce a two-stage\ntraining strategy. This approach facilitates the effective learning and\nintegration of complementary information from multi-modal source images,\nthereby enabling the model to retain fine-grained structural details while\nsignificantly enhancing inference efficiency. Extensive experiments demonstrate\nthat our method outperforms other state-of-the-art methods in terms of both\ninference speed and fusion quality. Code is available at\nhttps://github.com/zirui0625/RFfusion.", "AI": {"tldr": "RFfusion提出一种基于Rectified Flow的高效一阶段扩散模型，用于提高图像融合的计算效率和推理速度，同时确保高质量的融合结果。", "motivation": "扩散模型在图像融合领域取得了显著进展，但其计算复杂且推理时间冗长。为解决这一问题，我们提出了RFfusion，旨在提高方法的实用性，同时确保高质量的图像融合结果。", "method": "我们提出了一种基于Rectified Flow的一阶段高效扩散模型RFfusion，用于图像融合。通过将Rectified Flow融入图像融合任务中，我们能够简化采样路径并实现一阶段采样，无需额外训练，同时保持高质量的融合结果。此外，我们还提出了一种针对图像融合任务的特定变分自编码器(VAE)架构，其中融合操作嵌入在潜在空间中，进一步降低了计算复杂度。为了解决传统重建导向的VAE目标与图像融合需求之间的固有差异，我们引入了一种两阶段训练策略。", "result": "通过引入两阶段训练策略，我们的模型能够有效地学习并集成多模态源图像中的互补信息，同时保持精细的结构细节并显著提高推理效率。", "conclusion": "实验结果表明，我们的方法在推理速度和融合质量方面优于其他最先进方法。代码已公开。"}}
{"id": "2509.16596", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16596", "abs": "https://arxiv.org/abs/2509.16596", "authors": ["Junjie Ye", "Yuming Yang", "Yang Nan", "Shuo Li", "Qi Zhang", "Tao Gui", "Xuanjing Huang", "Peng Wang", "Zhongchao Shi", "Jianping Fan"], "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels", "comment": "Accepted by EMNLP 2025 Main Conference. arXiv admin note: text\n  overlap with arXiv:2409.15825", "summary": "Large language models (LLMs) acquire substantial world knowledge during\npre-training, which is further shaped by post-training techniques such as\nsupervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge\nremains underexplored, limiting our ability to control knowledge change\nbehavior in fine-tuned models. To address this gap, we evaluate closed-book\nquestion answering (CBQA) performance across five LLMs from the LLaMA-2 and\nLLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up\nto 14% worse than those fine-tuned on only 240 samples. Furthermore, varying\nthe level of knowledge mastery in the fine-tuning data leads to performance\nfluctuations of over 12%. To investigate these effects, we analyze model\nbehavior at both the token and parameter levels. Our analysis reveals that up\nto 90% of parameter updates during SFT do not contribute to knowledge\nenhancement. Restoring these updates can improve performance on the CBQA task,\ndepending on the characteristics of the fine-tuning data. These insights offer\npractical guidance for developing fine-tuning strategies that more effectively\nstrengthen model knowledge.", "AI": {"tldr": "研究发现微调样本数量和知识掌握程度的变动会对模型性能产生重要影响，并揭示了微调过程中的参数更新对知识增强的贡献不大，提供策略以更有效地加强模型知识。", "motivation": "探讨监督微调（SFT）对模型知识的影响，解决目前在控制微调模型的知识变化行为方面的能力受限的问题。", "method": "评估了五个来自LLaMA-2和LLaMA-3系列的大型语言模型在闭卷问答（CBQA）任务上的表现，并在标记和参数层面分析了模型的行为。", "result": "在仅使用240个微调样本的模型相比使用1,920个样本的模型，CBQA任务上的性能提高了最多14%，并且控制知识掌握程度可以导致性能波动超过12%。", "conclusion": "分析表明，高达90%的微调过程中参数的更新并不增强知识，这对制定更有效的微调策略有实际指导意义。"}}
{"id": "2509.16552", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16552", "abs": "https://arxiv.org/abs/2509.16552", "authors": ["Xiaoyang Yan", "Muleilan Pei", "Shaojie Shen"], "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting", "comment": null, "summary": "3D occupancy prediction is critical for comprehensive scene understanding in\nvision-centric autonomous driving. Recent advances have explored utilizing 3D\nsemantic Gaussians to model occupancy while reducing computational overhead,\nbut they remain constrained by insufficient multi-view spatial interaction and\nlimited multi-frame temporal consistency. To overcome these issues, in this\npaper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework\nto enhance both spatial and temporal modeling in existing Gaussian-based\npipelines. Specifically, we develop a guidance-informed spatial aggregation\nstrategy within a dual-mode attention mechanism to strengthen spatial\ninteraction in Gaussian representations. Furthermore, we introduce a\ngeometry-aware temporal fusion scheme that effectively leverages historical\ncontext to improve temporal continuity in scene completion. Extensive\nexperiments on the large-scale nuScenes occupancy prediction benchmark showcase\nthat our proposed approach not only achieves state-of-the-art performance but\nalso delivers markedly better temporal consistency compared to existing\nGaussian-based methods.", "AI": {"tldr": "我们提出一种新的框架ST-GS，通过引入空间和时间建模的创新策略，来增强基于高斯模型的3D占用预测性能。该方法在大规模nuScenes数据集上展示了优越的性能，并且具有更好的时间一致性。", "motivation": "3D占用预测对于视觉中心的自动驾驶场景理解至关重要。最近的进展探索了利用3D语义高斯模型来进行占用建模，以减少计算开销，但它们仍然受制于多视角空间交互不足和多帧时间一致性有限的问题。为了克服这些挑战，我们提出该框架。", "method": "我们提出了一种名为Spatial-Temporal Gaussian Splatting (ST-GS) 的新框架，以增强基于高斯模型的管道中的空间和时间建模。具体而言，我们开发了一种在双模式注意力机制内的指导信息空间聚合策略来增强高斯表示的空间交互。此外，我们引入了一种几何感知的时间融合方案，能够有效地利用历史上下文以提高场景完成的时间连续性。", "result": "在大规模nuScenes占用预测基准测试中显示，我们提出的方法不仅达到了最先进的性能，而且在与现有基于高斯模型的方法相比时，在时间一致性上表现得更为优异。", "conclusion": "提出的空间时间高斯绘制框架通过增强的空间时间和几何感知的方法，改进了现有高斯模型的性能。实验结果表明其在时空一致性上优于现有的方法。"}}
{"id": "2509.16597", "categories": ["cs.CL", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.16597", "abs": "https://arxiv.org/abs/2509.16597", "authors": ["Luyan Zhang"], "title": "MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models", "comment": "13 pages, 6 figures, 2 tables", "summary": "Aiming at the problems of computational inefficiency and insufficient\ninterpretability faced by large models in complex tasks such as multi-round\nreasoning and multi-modal collaboration, this study proposes a three-layer\ncollaboration framework based on model-controller-task adaptation (MCP). By\ndecoupling large model functions into reasoning, generation and retrieval\nmodules, and combining reinforcement learning-driven dynamic routing algorithms\nand task adaptation mechanisms, the systematic integration of control theory\nand large model dynamic reasoning is achieved for the first time. Experiments\nshow that the MCP framework improves the performance of cross-modal\nbenchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared\nwith the baseline model, improves the reasoning efficiency by 40%, and\ngenerates the interpretable intermediate results through the Presenter layer,\nobtaining 90% of the manual interpretability scores, which provides a brand-new\ntechnological path to solve the bottleneck of the practical application of the\nlarge model.", "AI": {"tldr": "研究提出一种三层MCP框架，通过解耦大型模型功能及运用动态路由和任务适应机制，提升了任务性能和推理效率，同时也改进了解释性。", "motivation": "针对大型模型在多轮推理和多模态协作等复杂任务中计算效率低下和解释性不足的问题，提出了解决方案。", "method": "本研究提出了一种基于模型-控制器-任务适应（MCP）的三层协作框架。通过将大型模型的功能解耦为推理、生成和检索模块，并结合强化学习驱动的动态路由算法和任务适应机制，首次实现了控制理论与大型模型动态推理的系统整合。", "result": "实验结果表明，与基线模型相比，MCP框架在GLUE、COCO、ScienceQA等跨模态基准任务上的性能提高了15-30%，推理效率提高了40%，并通过Presenter层生成的可解释中间结果，达到了90%的手动解释性评分。", "conclusion": "该框架为解决大型模型实际应用中的瓶颈提供了一条全新的技术路径。"}}
{"id": "2509.16557", "categories": ["cs.CV", "cs.ET", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16557", "abs": "https://arxiv.org/abs/2509.16557", "authors": ["Muhammad Hamza", "Danish Hamid", "Muhammad Tahir Akram"], "title": "Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose", "comment": "21 pages, 8 figures, 7 tables. Preprint of a manuscript submitted to\n  CCF Transactions on Pervasive Computing and Interaction (Springer), currently\n  under review", "summary": "Human-Object Interaction Recognition (HOIR) and user identification play a\ncrucial role in advancing augmented reality (AR)-based personalized assistive\ntechnologies. These systems are increasingly being deployed in high-stakes,\nhuman-centric environments such as aircraft cockpits, aerospace maintenance,\nand surgical procedures. This research introduces I2S (Interact2Sign), a multi\nstage framework designed for unobtrusive user identification through human\nobject interaction recognition, leveraging 3D hand pose analysis in egocentric\nvideos. I2S utilizes handcrafted features extracted from 3D hand poses and per\nforms sequential feature augmentation: first identifying the object class,\nfollowed by HOI recognition, and ultimately, user identification. A\ncomprehensive feature extraction and description process was carried out for 3D\nhand poses, organizing the extracted features into semantically meaningful\ncategories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor\nintroduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive\nablation studies were conducted to determine the most effective combination of\nfeatures. The optimal configuration achieved an impressive average F1-score of\n97.52% for user identification, evaluated on a bimanual object manipulation\ndataset derived from the ARCTIC and H2O datasets. I2S demonstrates\nstate-of-the-art performance while maintaining a lightweight model size of\nunder 4 MB and a fast inference time of 0.1 seconds. These characteristics make\nthe proposed framework highly suitable for real-time, on-device authentication\nin security-critical, AR-based systems.", "AI": {"tldr": "I2S是一种无干扰的用户识别框架，通过多阶段策略识别人机交互以区分用户，实现在AR环境中的高效用户认证。", "motivation": "研究旨在通过引入I2S框架，改善AR基础的个性化辅助技术中的人机交互识别和用户识别，特别是在航空驾驶舱、航空航天维护和外科手术等高风险环境中。", "method": "I2S (Interact2Sign)采用多阶段框架，通过3D手部姿态分析来进行无干扰的用户识别，首先识别对象类别，然后进行人机交互识别，最终实现用户识别。", "result": "研究通过详尽的特征提取和描述过程对3D手部姿态进行分析，并通过大量消融研究确定了最优特征组合，最终在双手动操作数据集上达到了97.52%的用户识别平均F1-score。", "conclusion": "I2S框架在双手动操作数据集上实现了97.52%的平均F1-score，展示了其在实时设备认证方面的优越性能和轻量化模型大小（小于4 MB），以及快速的推断时间（0.1秒），使其适合AR系统中的使用。"}}
{"id": "2509.16598", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16598", "abs": "https://arxiv.org/abs/2509.16598", "authors": ["Byeongho Yu", "Changhun Lee", "Jungyu Jin", "Eunhyeok Park"], "title": "PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality", "comment": null, "summary": "To mitigate the hallucination problem in large language models, DoLa exploits\nearly exit logits from the same model as a contrastive prior. However, we found\nthat these early exit logits tend to be flat, low in magnitude, and fail to\nreflect meaningful contrasts. To address this, we propose PruneCD, a novel\ncontrastive decoding method that constructs the amateur model via layer pruning\nrather than early exit. This design leads to more informative and well-aligned\nlogits, enabling more effective contrastive decoding. Through qualitative and\nquantitative analyses, we demonstrate that PruneCD consistently improves\nfactuality with minimal inference overhead, offering a robust and practical\napproach to mitigating hallucinations in LLMs.", "AI": {"tldr": "提出PruneCD来解决大型语言模型中的幻觉问题。", "motivation": "旨在解决大型语言模型中存在的幻觉问题，早期退出logits存在平坦、量级低且无法反映有意义的对比。", "method": "提出了一种名为PruneCD的新对比解码方法，通过层剪枝构建业余模型，而不是早期退出，以产生更具信息量和更好地对齐的logits，从而实现更有效的对比解码。", "result": "通过定性和定量分析，证明PruneCD能够一致地改善事实性，并且推理开销最小。", "conclusion": "PruneCD提供了一种在大型语言模型中减轻幻觉的稳健而实用的方法。"}}
{"id": "2509.16560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16560", "abs": "https://arxiv.org/abs/2509.16560", "authors": ["Ji Soo Lee", "Byungoh Ko", "Jaewon Cho", "Howoong Lee", "Jaewoon Byun", "Hyunwoo J. Kim"], "title": "Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization", "comment": "EMNLP 2025 Findings", "summary": "In text-video retrieval, auxiliary captions are often used to enhance video\nunderstanding, bridging the gap between the modalities. While recent advances\nin multi-modal large language models (MLLMs) have enabled strong zero-shot\ncaption generation, we observe that such captions tend to be generic and\nindistinguishable across visually similar videos, limiting their utility for\nfine-grained retrieval. Moreover, conventional captioning approaches are\ntypically evaluated using language generation metrics, such as BLEU, which are\nnot typically tailored for retrieval tasks that require making discriminative\ndistinctions between candidates. To address this, we propose\n$\\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption\ngeneration using retrieval relevance scores. At its core is Dual-Group Direct\nPreference Optimization (DG-DPO), a novel learning strategy that supervises\ncaptioning by modeling preferences across groups of distinct video and caption\npairs. In addition, we present an MLLM-based retrieval model that incorporates\nrole-embeddings to better distinguish between textual inputs with different\nfunctional roles, such as an auxiliary caption and a text query. Through\nextensive experiments, we demonstrate that CaRe-DPO significantly enhances\nretrieval performance by effectively leveraging auxiliary knowledge to generate\nfine-grained captions for retrieval. Code is available at\nhttps://github.com/mlvlab/CaReDPO.", "AI": {"tldr": "本文提出了CaRe-DPO框架，解决了多模态大语言模型生成的辅助字幕过于通用的问题，优化了文本-视频检索中的字幕生成和细粒度检索效果。", "motivation": "本文研究文本-视频检索中的辅助字幕生成问题。当前，多模态大语言模型虽然能使零样本字幕生成变得强大，但生成的字幕往往过于通用，使得在具有相似视觉特征的视频中难以区分。这限制了其在细粒度检索中的应用。", "method": "本文提出了CaRe-DPO，一个利用检索相关性评分直接优化字幕生成的检索框架。其核心是双组直接偏好优化（DG-DPO）学习策略，通过建模不同视频字幕对之间的偏好来进行监督。此外，该模型还采用了角色嵌入来更好地区分具有不同功能性角色的文本输入，比如辅助字幕和文本查询。", "result": "通过广泛的实验，作者展示了CaRe-DPO能够显著提升检索性能，通过有效利用辅助知识，生成适用于细粒度检索的字幕。", "conclusion": "CaRe-DPO模型通过直接优化字幕生成相关的检索相关性评分，成功提升了辅助字幕在细粒度检索中的区分度和实用性。代码可在https://github.com/mlvlab/CaReDPO获得。"}}
{"id": "2509.16599", "categories": ["cs.CL", "cs.IR", "stat.AP", "stat.ME", "H.3.3; I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2509.16599", "abs": "https://arxiv.org/abs/2509.16599", "authors": ["Sandro Tsang"], "title": "Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence", "comment": "11 pages, 7 figures and 4 tables. This work describes an information\n  retrieval-driven workflow for medical evidence synthesis, with an application\n  to endometriosis recurrence. The method can be generalized to other\n  systematic reviews. The preregistered protocol is available:\n  https://doi.org/10.17605/OSF.IO/R2DFA", "summary": "Background: Evidence synthesis facilitates evidence-based medicine. Without\ninformation retrieval techniques, this task is impossible due to the vast and\nexpanding literature. Objective: Building on prior work, this study evaluates\nan information retrieval-driven workflow to enhance the efficiency,\ntransparency, and reproducibility of systematic reviews. We use endometriosis\nrecurrence as an ideal case due to its complex and ambiguous literature.\nMethods: Our hybrid approach integrates PRISMA guidelines with computational\ntechniques. We applied semi-automated deduplication to efficiently filter\nrecords before manual screening. This workflow synthesized evidence from\nrandomised controlled trials on the efficacy of a subclass of\ngonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method\naddressed unit-of-analysis errors in multi-arm trials. Results: Our workflow\nefficiently reduced the screening workload. It took only 11 days to fetch and\nfilter 812 records. Seven RCTs were eligible, providing evidence from 841\npatients in 4 countries. The pooled random-effects model yielded a Risk Ratio\n(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity\n($I^2=0.00\\%$, $\\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.\nSensitivity analyses and bias assessments supported the robustness of our\nfindings. Conclusion: This study demonstrates an information-retrieval-driven\nworkflow for medical evidence synthesis. Our approach yields valuable clinical\nresults while providing a framework for accelerating the systematic review\nprocess. It bridges the gap between clinical research and computer science and\ncan be generalized to other complex systematic reviews.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.16567", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16567", "abs": "https://arxiv.org/abs/2509.16567", "authors": ["Nikolaos Spanos", "Maria Lymperaiou", "Giorgos Filandrianos", "Konstantinos Thomas", "Athanasios Voulodimos", "Giorgos Stamou"], "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits", "comment": "Accepted in NeurIPS 2025", "summary": "Recent black-box counterfactual generation frameworks fail to take into\naccount the semantic content of the proposed edits, while relying heavily on\ntraining to guide the generation process. We propose a novel, plug-and-play\nblack-box counterfactual generation framework, which suggests step-by-step\nedits based on theoretical guarantees of optimal edits to produce human-level\ncounterfactual explanations with zero training. Our framework utilizes a\npre-trained image editing diffusion model, and operates without access to the\ninternals of the classifier, leading to an explainable counterfactual\ngeneration process. Throughout our experimentation, we showcase the explanatory\ngap between human reasoning and neural model behavior by utilizing both\nConvolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision\nLanguage Model (LVLM) classifiers, substantiated through a comprehensive human\nevaluation.", "AI": {"tldr": "研究提出了一种新颖的黑盒反事实生成框架，该框架不需要训练即可生成高质量的反事实解释，同时保持可解释性。", "motivation": "当前的黑盒反事实生成框架忽视了所提编辑的语义内容，且高度依赖训练来引导生成过程。为了弥补这一不足，研究提出了一种新的无需训练、可插拔的反事实生成框架。", "method": "提出了一种新颖的黑盒反事实生成框架，该框架基于最优编辑的理论保证，逐步提出编辑建议，以产生无需训练的人类级别的反事实解释。该框架采用预训练的图像编辑扩散模型，并且无需访问分类器的内部结构，从而实现可解释的反事实生成过程。", "result": "通过实验，尤其是在使用CNN、ViT和LVLM分类器的情况下，发现该框架能够减少人类推理和神经模型行为之间的解释差距，这一结论得到了广泛的人类评估的证实。", "conclusion": "研究结果表明，所提出的框架能够有效地产生可解释的反事实解释，而无需访问被解释模型的内部结构，并在多个分类器上进行了验证，表现出了良好的解释功效和人类评估一致性。"}}
{"id": "2509.16610", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16610", "abs": "https://arxiv.org/abs/2509.16610", "authors": ["Junhao Chen", "Jingbo Sun", "Xiang Li", "Haidong Xin", "Yuhao Xue", "Yibin Xu", "Hao Zhao"], "title": "LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts", "comment": "Accepted by EMNLP 2025 Findings", "summary": "As large language models (LLMs) advance across diverse tasks, the need for\ncomprehensive evaluation beyond single metrics becomes increasingly important.\nTo fully assess LLM intelligence, it is crucial to examine their interactive\ndynamics and strategic behaviors. We present LLMsPark, a game theory-based\nevaluation platform that measures LLMs' decision-making strategies and social\nbehaviors in classic game-theoretic settings, providing a multi-agent\nenvironment to explore strategic depth. Our system cross-evaluates 15 leading\nLLMs (both commercial and open-source) using leaderboard rankings and scoring\nmechanisms. Higher scores reflect stronger reasoning and strategic\ncapabilities, revealing distinct behavioral patterns and performance\ndifferences across models. This work introduces a novel perspective for\nevaluating LLMs' strategic intelligence, enriching existing benchmarks and\nbroadening their assessment in interactive, game-theoretic scenarios. The\nbenchmark and rankings are publicly available at https://llmsparks.github.io/.", "AI": {"tldr": "LLMsPark平台使用游戏理论方法评估多个大型语言模型的决策和社会行为，提供了评估LLMs战略智能的新视角。", "motivation": "随着大型语言模型在各种任务中的进步，对超越单一指标的全面评估的需求变得更加重要，提出此方法是为了全面评估LLMs的智力，特别是在互动动态和战略行为方面。", "method": "通过游戏理论评估多代理环境中大型语言模型(LLMs)的决策策略和社会行为，提出了LLMsPark平台来衡量LLMs在经典游戏理论环境中的表现。", "result": "对15个领先的LLMs进行了交叉评估，利用排行榜和评分机制区分行为模式和性能差异，更高的分数反映了更强的推理和战略能力。", "conclusion": "这项工作介绍了一个评估LLMs战略智能的新视角，丰富了现有的基准并在互动、游戏理论场景中扩展了评估方式。"}}
{"id": "2509.16582", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16582", "abs": "https://arxiv.org/abs/2509.16582", "authors": ["Antonio Scardace", "Lemuel Puglisi", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Ravì"], "title": "A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis", "comment": null, "summary": "Deep generative models have emerged as a transformative tool in medical\nimaging, offering substantial potential for synthetic data generation. However,\nrecent empirical studies highlight a critical vulnerability: these models can\nmemorize sensitive training data, posing significant risks of unauthorized\npatient information disclosure. Detecting memorization in generative models\nremains particularly challenging, necessitating scalable methods capable of\nidentifying training data leakage across large sets of generated samples. In\nthis work, we propose DeepSSIM, a novel self-supervised metric for quantifying\nmemorization in generative models. DeepSSIM is trained to: i) project images\ninto a learned embedding space and ii) force the cosine similarity between\nembeddings to match the ground-truth SSIM (Structural Similarity Index) scores\ncomputed in the image space. To capture domain-specific anatomical features,\ntraining incorporates structure-preserving augmentations, allowing DeepSSIM to\nestimate similarity reliably without requiring precise spatial alignment. We\nevaluate DeepSSIM in a case study involving synthetic brain MRI data generated\nby a Latent Diffusion Model (LDM) trained under memorization-prone conditions,\nusing 2,195 MRI scans from two publicly available datasets (IXI and CoRR).\nCompared to state-of-the-art memorization metrics, DeepSSIM achieves superior\nperformance, improving F1 scores by an average of +52.03% over the best\nexisting method. Code and data of our approach are publicly available at the\nfollowing link: https://github.com/brAIn-science/DeepSSIM.", "AI": {"tldr": "文章提出了一种新的自监督度量DeepSSIM，用于检测生成模型记忆训练数据的能力，在医学图像的合成数据中其表现优于现有的最佳方法。", "motivation": "近期的实证研究表明，深度生成模型存在一个关键弱点：这些模型可能会记住敏感的训练数据，从而引发未经授权的患者信息泄露。检测生成模型中的记忆痕迹尤为困难，需要一种能够识别大规模生成样本中训练数据泄露的可扩展方法。", "method": "我们提出了DeepSSIM，这是一种新颖的自监督度量标准，用于量化生成模型中的记忆痕迹。DeepSSIM被训练来：i) 将图像投影到一个学习到的嵌入空间中；ii) 使嵌入之间的余弦相似度匹配图像空间中计算的SSIM（结构相似性指数）得分。为了捕捉特定领域的解剖特征，训练中包含了结构保持增强，这使得DeepSSIM能够可靠地估计相似度，而无需精确的空间对齐。", "result": "通过使用2195张来自两个公开数据集（IXI和CoRR）的MRI扫描图像合成的脑部MRI数据的案例研究，DeepSSIM在评估合成数据记忆痕迹时，相较于现有的最佳方法，F1分数平均提升了52.03%。", "conclusion": "DeepSSIM在检测生成模型记忆痕迹方面表现优异，特别是在处理合成的医学图像数据时。"}}
{"id": "2509.16660", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16660", "abs": "https://arxiv.org/abs/2509.16660", "authors": ["Zuhair Hasan Shaik", "Abdullah Mazhar", "Aseem Srivastava", "Md Shad Akhtar"], "title": "Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation", "comment": "Accepted to the NeurIPS 2025 Research Track", "summary": "Large Language Models have demonstrated impressive fluency across diverse\ntasks, yet their tendency to produce toxic content remains a critical challenge\nfor AI safety and public trust. Existing toxicity mitigation approaches\nprimarily manipulate individual neuron activations, but these methods suffer\nfrom instability, context dependence, and often compromise the model's core\nlanguage abilities. To address these shortcomings, we investigate three key\nquestions: the stability of neuron-level toxicity indicators, the advantages of\nstructural (layer-wise) representations, and the interpretability of mechanisms\ndriving toxic generation. Through extensive experiments on Jigsaw and ToxiCN\ndatasets, we show that aggregated layer-wise features provide more robust\nsignals than single neurons. Moreover, we observe conceptual limitations in\nprior works that conflate toxicity detection experts and generation experts\nwithin neuron-based interventions. To mitigate this, we propose a novel\nprincipled intervention technique, EigenShift, based on eigen-decomposition of\nthe language model's final output layer. This method selectively targets\ngeneration-aligned components, enabling precise toxicity suppression without\nimpairing linguistic competence. Our method requires no additional training or\nfine-tuning, incurs minimal computational cost, and is grounded in rigorous\ntheoretical analysis.", "AI": {"tldr": "The paper presents EigenShift, a method using eigen-decomposition on the final output layer to suppress toxic content in large language models, maintaining their linguistic abilities without additional training.", "motivation": "The motivation behind this paper is to address the instability and context dependence of neuron-level modifications in large language models, which are used for mitigating toxic content while preserving core language abilities.", "method": "The paper proposes EigenShift, a method based on the eigen-decomposition of the language model's final output layer, to selectively target generation-aligned components for precise toxicity suppression.", "result": "The experiments on Jigsaw and ToxiCN datasets show that aggregated layer-wise features are more stable than single neurons, and that the proposed EigenShift method can suppress toxic content without impairing linguistic competence.", "conclusion": "The paper concludes that EigenShift is a robust, computationally efficient method for mitigating toxic content generation in large language models without compromising their language abilities."}}
{"id": "2509.16588", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.16588", "abs": "https://arxiv.org/abs/2509.16588", "authors": ["Haiming Zhang", "Yiyao Zhu", "Wending Zhou", "Xu Yan", "Yingjie Cai", "Bingbing Liu", "Shuguang Cui", "Zhen Li"], "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving", "comment": "NeurIPS 2025 (Spotlight)", "summary": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes\nexplicit dense BEV or volumetric construction, enabling highly efficient\ncomputation and accelerated inference. In this paper, we introduce SQS, a novel\nquery-based splatting pre-training specifically designed to advance SPMs in\nautonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian\nrepresentations from sparse queries during pre-training, leveraging\nself-supervised splatting to learn fine-grained contextual features through the\nreconstruction of multi-view images and depth maps. During fine-tuning, the\npre-trained Gaussian queries are seamlessly integrated into downstream networks\nvia query interaction mechanisms that explicitly connect pre-trained queries\nwith task-specific queries, effectively accommodating the diverse requirements\nof occupancy prediction and 3D object detection. Extensive experiments on\nautonomous driving benchmarks demonstrate that SQS delivers considerable\nperformance gains across multiple query-based 3D perception tasks, notably in\noccupancy prediction and 3D object detection, outperforming prior\nstate-of-the-art pre-training approaches by a significant margin (i.e., +1.3\nmIoU on occupancy prediction and +1.0 NDS on 3D detection).", "AI": {"tldr": "本文提出了SQS，一种新的预训练方法，旨在提高自动驾驶中的稀疏感知模型（SPM）的性能。通过实验验证了SQS的有效性。", "motivation": "现有的SPM在自动驾驶中的三维感知任务中取得了成功，但是对占用预测和三维物体检测等任务进行微调时，仍然存在较大的改进空间。为了提升这些任务的效果，本文提出了SQS，一种新的基于查询的预训练方法。", "method": "SQS是一种新的基于查询的预训练方法，专门设计用于提高自动驾驶中的SPM。SQS引入了一个插件模块，该模块在预训练期间从稀疏查询中预测3D高斯表示，利用自监督的涂抹技术来通过多视图图像和深度图的重建学习细粒度的上下文特征。在微调阶段，预训练的高斯查询通过查询交互机制无缝集成到下游网络中，明确地连接预训练查询与任务特定的查询。这样的设计能够有效满足占用预测和三维物体检测等任务的需求。", "result": "在自动驾驶基准测试中的大量实验表明，SQS在多个基于查询的三维感知任务中提供了显著的性能提升，尤其在占用预测和三维物体检测方面，优于之前的最先进预训练方法。具体的改进为占用预测提升+1.3个mIoU，三维检测提升+1.0 NDS。", "conclusion": "本文通过引入SQS，证明了在自动驾驶中的三维感知任务中，给SPM添加预训练的基于查询的涂抹模块能够展现出优越的性能。"}}
{"id": "2509.16666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16666", "abs": "https://arxiv.org/abs/2509.16666", "authors": ["Ahmet Yavuz Uluslu", "Tannon Kew", "Tilia Ellendorff", "Gerold Schneider", "Rico Sennrich"], "title": "Robust Native Language Identification through Agentic Decomposition", "comment": "Accepted at EMNLP* 2025", "summary": "Large language models (LLMs) often achieve high performance in native\nlanguage identification (NLI) benchmarks by leveraging superficial contextual\nclues such as names, locations, and cultural stereotypes, rather than the\nunderlying linguistic patterns indicative of native language (L1) influence. To\nimprove robustness, previous work has instructed LLMs to disregard such clues.\nIn this work, we demonstrate that such a strategy is unreliable and model\npredictions can be easily altered by misleading hints. To address this problem,\nwe introduce an agentic NLI pipeline inspired by forensic linguistics, where\nspecialized agents accumulate and categorize diverse linguistic evidence before\nan independent final overall assessment. In this final assessment, a goal-aware\ncoordinating agent synthesizes all evidence to make the NLI prediction. On two\nbenchmark datasets, our approach significantly enhances NLI robustness against\nmisleading contextual clues and performance consistency compared to standard\nprompting methods.", "AI": {"tldr": "本文通过引入一种新的主动型L1识别管道来提高大型语言模型在识别母语方面的准确性与对抗误导线索的鲁棒性。这种方法受到了法医语言学的启发，在最终评估阶段整合了所有证据。", "motivation": "由于大型语言模型在L1识别基准测试中往往依赖于类似名字、位置和文化刻板印象等表面上的语境线索而不是潜在的语言模式，因此我们希望通过这种方法改善模型在这种识别任务中的鲁棒性。", "method": "我们提出了一种受法医语言学启发的主动型L1识别管道，其中专门的代理会积累并分类各种语言证据，然后独立的最终评估阶段将整合所有证据，做出L1识别的预测。", "result": "该方法在两个基准数据集上显著提高了对抗误导性环境线索的健壮性和相对于标准提示方法的一致性。", "conclusion": "相较于标准编程方法，我们提出的方法在多个数据集上展示了更强的抗误导能力和更高的性能一致性。"}}
{"id": "2509.16602", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.16602", "abs": "https://arxiv.org/abs/2509.16602", "authors": ["Minji Heo", "Simon S. Woo"], "title": "FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection", "comment": null, "summary": "Multi-step or hybrid deepfakes, created by sequentially applying different\ndeepfake creation methods such as Face-Swapping, GAN-based generation, and\nDiffusion methods, can pose an emerging and unforseen technical challenge for\ndetection models trained on single-step forgeries. While prior studies have\nmainly focused on detecting isolated single manipulation, little is known about\nthe detection model behavior under such compositional, hybrid, and complex\nmanipulation pipelines. In this work, we introduce \\textbf{FakeChain}, a\nlarge-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using\nfive state-of-the-art representative generators. Using this approach, we\nanalyze detection performance and spectral properties across hybrid\nmanipulation at different step, along with varying generator combinations and\nquality settings. Surprisingly, our findings reveal that detection performance\nhighly depends on the final manipulation type, with F1-score dropping by up to\n\\textbf{58.83\\%} when it differs from training distribution. This clearly\ndemonstrates that detectors rely on last-stage artifacts rather than cumulative\nmanipulation traces, limiting generalization. Such findings highlight the need\nfor detection models to explicitly consider manipulation history and sequences.\nOur results highlight the importance of benchmarks such as FakeChain,\nreflecting growing synthesis complexity and diversity in real-world scenarios.\nOur sample code is available\nhere\\footnote{https://github.com/minjihh/FakeChain}.", "AI": {"tldr": "本研究介绍了FakeChain基准测试，分析了多步伪造检测中模型的性能，强调了检测模型需要更全面地考虑伪造操纵的复杂性。", "motivation": "鉴于以往研究大多聚焦于孤立单一操作的检测，对于由多步或混合深度伪造技术生成的伪造图像检测模型的行为了解甚少，因此本研究的动机是探讨在复杂复合伪造情况下的检测性能。", "method": "本研究开发了一个名为FakeChain的大型基准测试，该基准包含了使用五种代表性的生成器合成的1-, 2-, 3-步伪造图像。", "result": "研究发现检测性能很大程度上取决于最终的操纵类型，当伪造类型与训练分布不同时，F1-score可下降高达58.83%。", "conclusion": "研究结论认为检测模型必须考虑操纵历史和序列，而不仅仅是依赖于最后阶段的伪迹，表明了类似FakeChain这样能够反映现实世界合成复杂性和多样性的基准测试的重要性。"}}
{"id": "2509.16679", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.16679", "abs": "https://arxiv.org/abs/2509.16679", "authors": ["Keliang Liu", "Dingkang Yang", "Ziyun Qian", "Weijie Yin", "Yuchi Wang", "Hongsheng Li", "Jun Liu", "Peng Zhai", "Yang Liu", "Lihua Zhang"], "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle", "comment": "A Survey of Reinforcement Learning for Large Language Models", "summary": "In recent years, training methods centered on Reinforcement Learning (RL)\nhave markedly enhanced the reasoning and alignment performance of Large\nLanguage Models (LLMs), particularly in understanding human intents, following\nuser instructions, and bolstering inferential strength. Although existing\nsurveys offer overviews of RL augmented LLMs, their scope is often limited,\nfailing to provide a comprehensive summary of how RL operates across the full\nlifecycle of LLMs. We systematically review the theoretical and practical\nadvancements whereby RL empowers LLMs, especially Reinforcement Learning with\nVerifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL.\nSecond, we thoroughly detail application strategies for RL across various\nphases of the LLM lifecycle, including pre-training, alignment fine-tuning, and\nreinforced reasoning. In particular, we emphasize that RL methods in the\nreinforced reasoning phase serve as a pivotal driving force for advancing model\nreasoning to its limits. Next, we collate existing datasets and evaluation\nbenchmarks currently used for RL fine-tuning, spanning human-annotated\ndatasets, AI-assisted preference data, and program-verification-style corpora.\nSubsequently, we review the mainstream open-source tools and training\nframeworks available, providing clear practical references for subsequent\nresearch. Finally, we analyse the future challenges and trends in the field of\nRL-enhanced LLMs. This survey aims to present researchers and practitioners\nwith the latest developments and frontier trends at the intersection of RL and\nLLMs, with the goal of fostering the evolution of LLMs that are more\nintelligent, generalizable, and secure.", "AI": {"tldr": "本文综述了如何通过强化学习（RL）增强大型语言模型（LLMs），特别关注RL在LLMs不同阶段的应用，分析目前的数据集、评估基准以及开源工具，同时指出未来挑战和趋势。", "motivation": "虽然现有的综述提供了关于RL增强LLMs的概述，但它们的范围通常有限，未能全面总结RL在整个LLMs生命周期中的作用。本综述旨在全面分析RL在LLMs各个阶段的应用，以推动LLMs向更智能、更通用和更安全的方向发展。", "method": "系统地回顾了强化学习（RL）如何增强大型语言模型（LLMs）的理论和实践进展，特别关注使用可验证奖励的强化学习(RLVR)。包括理论简介、LLMs生命周期各阶段RL的应用策略、数据集和评估基准的总结、开源工具和培训框架的回顾以及未来挑战和趋势的分析。", "result": "展示了RL在预训练、微调和强化推理各阶段对LLMs的提升，特别是强化推理阶段的重要性，指出了目前使用的各类数据集和评估基准，以及可用的开源工具和框架。", "conclusion": "综述了RL在LLMs中的最新进展，展望未来的研究方向和挑战，以促进更智能、通用且安全的LLMs的发展。"}}
{"id": "2509.16609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.16609", "abs": "https://arxiv.org/abs/2509.16609", "authors": ["Shipeng Liu", "Zhonglin Zhang", "Dengfeng Chen", "Liang Zhao"], "title": "Describe-to-Score: Text-Guided Efficient Image Complexity Assessment", "comment": null, "summary": "Accurately assessing image complexity (IC) is critical for computer vision,\nyet most existing methods rely solely on visual features and often neglect\nhigh-level semantic information, limiting their accuracy and generalization. We\nintroduce vision-text fusion for IC modeling. This approach integrates visual\nand textual semantic features, increasing representational diversity. It also\nreduces the complexity of the hypothesis space, which enhances both accuracy\nand generalization in complexity assessment. We propose the D2S\n(Describe-to-Score) framework, which generates image captions with a\npre-trained vision-language model. We propose the feature alignment and entropy\ndistribution alignment mechanisms, D2S guides semantic information to inform\ncomplexity assessment while bridging the gap between vision and text\nmodalities. D2S utilizes multi-modal information during training but requires\nonly the vision branch during inference, thereby avoiding multi-modal\ncomputational overhead and enabling efficient assessment. Experimental results\ndemonstrate that D2S outperforms existing methods on the IC9600 dataset and\nmaintains competitiveness on no-reference image quality assessment (NR-IQA)\nbenchmark, validating the effectiveness and efficiency of multi-modal fusion in\ncomplexity-related tasks. Code is available at:\nhttps://github.com/xauat-liushipeng/D2S", "AI": {"tldr": "This paper presents D2S, a new approach for image complexity assessment using vision-text fusion that enhances accuracy and generalization by integrating visual and textual features, and outperforms existing methods.", "motivation": "The goal is to address the limitations of current image complexity assessment methods by incorporating semantic information from both visual and textual modalities, which enhances representational diversity and reduces hypothesis space complexity.", "method": "The paper proposes a vision-text fusion method for image complexity assessment called D2S (Describe-to-Score). It integrates visual and textual features by describing images with a pre-trained vision-language model and aligning features and entropy distributions.", "result": "Experimental results indicate that D2S outperforms existing methods on the IC9600 dataset and remains competitive on no-reference image quality assessment benchmarks, proving its effectiveness and efficiency.", "conclusion": "The D2S framework improves accuracy and generalization in image complexity assessment by effectively utilizing multimodal information during training while only requiring visual data during inference, leading to efficient assessments. It shows superior performance on the IC9600 dataset and is competitive on NR-IQA benchmarks."}}
