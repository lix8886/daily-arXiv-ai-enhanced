{"id": "2507.22910", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22910", "abs": "https://arxiv.org/abs/2507.22910", "authors": ["Sergio Di Meglio", "Aniello Somma", "Luigi Libero Lucio Starace", "Fabio Scippacercola", "Giancarlo Sperlì", "Sergio Di Martino"], "title": "Large Language Models in the Travel Domain: An Industrial Experience", "comment": "Manuscript accepted to the International Conference on Software\n  Engineering and Knowledge Engineering (SEKE) 2025", "summary": "Online property booking platforms are widely used and rely heavily on\nconsistent, up-to-date information about accommodation facilities, often\nsourced from third-party providers. However, these external data sources are\nfrequently affected by incomplete or inconsistent details, which can frustrate\nusers and result in a loss of market. In response to these challenges, we\npresent an industrial case study involving the integration of Large Language\nModels (LLMs) into CALEIDOHOTELS, a property reservation platform developed by\nFERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,\nfine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.\nBoth models were assessed based on their ability to generate consistent and\nhomogeneous descriptions while minimizing hallucinations. Mixtral 8x7B\noutperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision\n(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet\nmore concise content (249 vs. 277 words on average). However, this came at a\nsignificantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB\nand $0.16/hour for Mistral 7B. Our findings provide practical insights into the\ntrade-offs between model quality and resource efficiency, offering guidance for\ndeploying LLMs in production environments and demonstrating their effectiveness\nin enhancing the consistency and reliability of accommodation data.", "AI": {"tldr": "The paper discusses the use of Large Language Models (LLMs) to improve the consistency and reliability of accommodation data on property booking platforms by addressing issues with external data sources.", "motivation": "The motivation is to enhance user experience and prevent market loss due to inconsistent or incomplete accommodation information from third-party providers on booking platforms.", "method": "The researchers tested two LLMs, Mistral 7B (fine-tuned with QLoRA) and Mixtral 8x7B (with a refined system prompt), on their capacity to generate consistent and unbiased descriptions.", "result": "Mixtral 8x7B performed better than Mistral 7B in terms of description completeness, precision, and lower hallucination rate, albeit at a higher computational cost.", "conclusion": "The research offers practical guidance on the trade-offs in deploying LLMs in production environments to achieve higher model quality in accommodation data consistency."}}
{"id": "2507.22911", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22911", "abs": "https://arxiv.org/abs/2507.22911", "authors": ["Jinzhi Wang", "Qingke Peng", "Haozhou Li", "Zeyuan Zeng", "Qinfeng Song", "Kaixuan Yang", "Jiangbo Zhang", "Yaoying Wang", "Ruimeng Li", "Biyi Zhou"], "title": "ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing", "comment": null, "summary": "Electric power marketing customer service plays a critical role in addressing\ninquiries, complaints, and service requests. However, current systems, such as\nChina's 95598 hotline, often struggle with slow response times, inflexible\nprocedures, and limited accuracy in domain-specific tasks. While large language\nmodels (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,\nthey lack the domain expertise and empathy required in this field. To bridge\nthis gap, we introduce ElectriQ, the first benchmark designed to evaluate and\nenhance LLMs in electric power marketing scenarios. ElectriQ consists of a\ndialogue dataset covering six key service categories and introduces four\nevaluation metrics: professionalism, popularity, readability, and\nuser-friendliness. We further incorporate a domain-specific knowledge base and\npropose a knowledge augmentation method to boost model performance. Experiments\non 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and\naugmented, can surpass GPT-4o in terms of professionalism and\nuser-friendliness. ElectriQ establishes a comprehensive foundation for\ndeveloping LLMs tailored to the needs of power marketing services.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.22912", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50"], "pdf": "https://arxiv.org/pdf/2507.22912", "abs": "https://arxiv.org/abs/2507.22912", "authors": ["Navid Yazdanjue", "Morteza Rakhshaninejad", "Hossein Yazdanjouei", "Mohammad Sadegh Khorshidi", "Mikko S. Niemela", "Fang Chen", "Amir H. Gandomi"], "title": "A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms", "comment": "16 pages, 5 figures, 9 tables", "summary": "Illegal marketplaces have increasingly shifted to concealed parts of the\ninternet, including the deep and dark web, as well as platforms such as\nTelegram, Reddit, and Pastebin. These channels enable the anonymous trade of\nillicit goods including drugs, weapons, and stolen credentials. Detecting and\ncategorizing such content remains challenging due to limited labeled data, the\nevolving nature of illicit language, and the structural heterogeneity of online\nsources. This paper presents a hierarchical classification framework that\ncombines fine-tuned language models with a semi-supervised ensemble learning\nstrategy to detect and classify illicit marketplace content across diverse\nplatforms. We extract semantic representations using ModernBERT, a transformer\nmodel for long documents, finetuned on domain-specific data from deep and dark\nweb pages, Telegram channels, Subreddits, and Pastebin pastes to capture\nspecialized jargon and ambiguous linguistic patterns. In addition, we\nincorporate manually engineered features such as document structure, embedded\npatterns including Bitcoin addresses, emails, and IPs, and metadata, which\ncomplement language model embeddings. The classification pipeline operates in\ntwo stages. The first stage uses a semi-supervised ensemble of XGBoost, Random\nForest, and SVM with entropy-based weighted voting to detect sales-related\ndocuments. The second stage further classifies these into drug, weapon, or\ncredential sales. Experiments on three datasets, including our multi-source\ncorpus, DUTA, and CoDA, show that our model outperforms several baselines,\nincluding BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The\nmodel achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of\n0.95388, demonstrating strong generalization, robustness under limited\nsupervision, and effectiveness in real-world illicit content detection.", "AI": {"tldr": "提出了一种高效的非法市场内容分类模型，并在多个数据集上进行了验证，证明了模型的有效性和泛化能力。", "motivation": "由于标注数据有限、非法语言不断变化以及在线来源结构的异质性，对非法市场内容进行检测和分类极具挑战。因此，开发一种新的方法来有效地解决这一问题是必要的。", "method": "提出了一种结合微调语言模型和半监督集成学习策略的分层分类框架，用于检测和分类来自不同平台的非法市场内容。框架使用了在深度和暗网页面、Telegram频道、Subreddits和Pastebin帖子上微调的ModernBERT语言模型来提取语义表示，同时，框架还结合了手工特征如文档结构、嵌入式模式（包括比特币地址、电子邮件和IP）和元数据。分类流水线分为两个阶段：第一阶段使用XGBoost、随机森林和SVM的半监督集成通过基于熵的加权投票来检测与销售相关的文档；第二阶段进一步将这些文档分类为药物、武器或凭证销售。", "result": "模型在多数据集上的表现超出了多个现有模型，显示出了强劲的泛化能力、在有限监督下的鲁棒性和在现实世界中检测非法内容的有效性。", "conclusion": "实验结果表明，该模型在多个数据集上的表现优于包括BERT、ModernBERT、DarkBERT、ALBERT、Longformer和BigBird在内的多个基线模型，在准确率、F1分数和TMCC方面分别达到了0.96489、0.93467和0.95388，展示了模型在有限监督下的强泛化能力和现实世界非法内容检测的有效性。"}}
{"id": "2507.22913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22913", "abs": "https://arxiv.org/abs/2507.22913", "authors": ["Jinyu Liu", "Xiaoying Song", "Diana Zhang", "Jason Thomale", "Daqing He", "Lingzi Hong"], "title": "A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models", "comment": "13 pages, 2 figures, accepted by ASIST 2025", "summary": "Providing subject access to information resources is an essential function of\nany library management system. Large language models (LLMs) have been widely\nused in classification and summarization tasks, but their capability to perform\nsubject analysis is underexplored. Multi-label classification with traditional\nmachine learning (ML) models has been used for subject analysis but struggles\nwith unseen cases. LLMs offer an alternative but often over-generate and\nhallucinate. Therefore, we propose a hybrid framework that integrates\nembedding-based ML models with LLMs. This approach uses ML models to (1)\npredict the optimal number of LCSH labels to guide LLM predictions and (2)\npost-edit the predicted terms with actual LCSH terms to mitigate\nhallucinations. We experimented with LLMs and the hybrid framework to predict\nthe subject terms of books using the Library of Congress Subject Headings\n(LCSH). Experiment results show that providing initial predictions to guide LLM\ngenerations and imposing post-edits result in more controlled and\nvocabulary-aligned outputs.", "AI": {"tldr": "研究提出了一种混合框架，通过结合机器学习模型和大型语言模型，改善图书主题分析，获得了更好的预测和词汇一致性结果。", "motivation": "旨在利用传统机器学习模型和大型语言模型的优势来改善图书主题分析，解决LLM过度生成和幻觉的问题。", "method": "提出了一种结合基于嵌入的传统机器学习模型与大型语言模型（LLM）的混合框架。该方法使用机器学习模型（1）预测最优的LCSH标签数量以指导LLM预测，以及（2）使用实际的LCSH术语修正LLM生成的预测术语，以减少生成错误。", "result": "实验结果表明，通过提供初始预测以引导LLM生成和实施后期编辑，可以获得控制良好、词汇一致的结果。", "conclusion": "混合框架通过结合ML和LLM的优势改善了LLM在图书主题分析任务中的性能。"}}
{"id": "2507.22958", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 97D50", "I.2.7; I.4; K.3.1"], "pdf": "https://arxiv.org/pdf/2507.22958", "abs": "https://arxiv.org/abs/2507.22958", "authors": ["Ruslan Khrulev"], "title": "CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam", "comment": "15 pages, 3 figures, 10 tables. Code is available at:\n  https://github.com/Karifannaa/Auto-check-EGE-math", "summary": "This paper introduces a novel benchmark, EGE-Math Solutions Assessment\nBenchmark, for evaluating Vision-Language Models (VLMs) on their ability to\nassess hand-written mathematical solutions. Unlike existing benchmarks that\nfocus on problem solving, our approach centres on understanding student\nsolutions, identifying mistakes, and assigning grades according to fixed\ncriteria. We compile 122 scanned solutions from the Russian Unified State Exam\n(EGE) together with official expert grades, and evaluate seven modern VLMs from\nGoogle, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The\nresults reveal current limitations in mathematical reasoning and human-rubric\nalignment, opening new research avenues in AI-assisted assessment. You can find\ncode in https://github.com/Karifannaa/Auto-check-EGE-math", "AI": {"tldr": "该论文提出了一个新的基准测试，用于评估VLMs在评估手写数学解答方面的能力，揭示了现有模型在数学推理和与人类评分标准对齐方面的局限性，指出AI辅助评估的新研究方向。", "motivation": "与现有的专注于问题解决的基准测试不同，该研究关注理解学生解决方案、识别错误和根据固定标准分配分数。", "method": "该论文引入了EGE-Math Solutions Assessment Benchmark，这是一个新的基准测试，用于评估视觉语言模型（VLMs）评估手写数学解答的能力。基准测试包括来自俄罗斯国家统一考试的122个解答，附带官方专家评分，并在三种推理模式下评估了来自Google、OpenAI、Arcee AI和阿里云的七种现代VLMs。", "result": "研究表明当前模型在数学推理和与人类评分标准对齐方面存在局限性。", "conclusion": "该研究揭示了AI辅助评估的新研究方向。"}}
{"id": "2507.22914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22914", "abs": "https://arxiv.org/abs/2507.22914", "authors": ["Victor Eiti Yamamoto", "Hideaki Takeda"], "title": "Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs", "comment": null, "summary": "Knowledge graphs (KGs) are powerful tools for representing and reasoning over\nstructured information. Their main components include schema, identity, and\ncontext. While schema and identity matching are well-established in ontology\nand entity matching research, context matching remains largely unexplored. This\nis particularly important because real-world KGs often vary significantly in\nsource, size, and information density - factors not typically represented in\nthe datasets on which current entity matching methods are evaluated. As a\nresult, existing approaches may fall short in scenarios where diverse and\ncomplex contexts need to be integrated.\n  To address this gap, we propose a novel KG integration method consisting of\nlabel matching and triple matching. We use string manipulation, fuzzy matching,\nand vector similarity techniques to align entity and predicate labels. Next, we\nidentify mappings between triples that convey comparable information, using\nthese mappings to improve entity-matching accuracy. Our approach demonstrates\ncompetitive performance compared to leading systems in the OAEI competition and\nagainst supervised methods, achieving high accuracy across diverse test cases.\nAdditionally, we introduce a new dataset derived from the benchmark dataset to\nevaluate the triple-matching step more comprehensively.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.23006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23006", "abs": "https://arxiv.org/abs/2507.23006", "authors": ["Zhensheng Yuan", "Haozhi Huang", "Zhen Xiong", "Di Wang", "Guanghua Yang"], "title": "Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction", "comment": null, "summary": "We present a framework that enables fast reconstruction and real-time\nrendering of urban-scale scenes while maintaining robustness against appearance\nvariations across multi-view captures. Our approach begins with scene\npartitioning for parallel training, employing a visibility-based image\nselection strategy to optimize training efficiency. A controllable\nlevel-of-detail (LOD) strategy explicitly regulates Gaussian density under a\nuser-defined budget, enabling efficient training and rendering while\nmaintaining high visual fidelity. The appearance transformation module\nmitigates the negative effects of appearance inconsistencies across images\nwhile enabling flexible adjustments. Additionally, we utilize enhancement\nmodules, such as depth regularization, scale regularization, and antialiasing,\nto improve reconstruction fidelity. Experimental results demonstrate that our\nmethod effectively reconstructs urban-scale scenes and outperforms previous\napproaches in both efficiency and quality. The source code is available at:\nhttps://yzslab.github.io/REUrbanGS.", "AI": {"tldr": "本文提出了一种能够快速重建和实时渲染大规模城市场景的框架，同时解决了多视角捕获中的外观变化问题。", "motivation": "解决大规模城市场景重建中的效率和质量问题，特别是在多视角捕获中由于外观变化带来的挑战。", "method": "框架包括场景划分和并行训练，基于可见性的图像选择策略，可控范围的细节层次（LOD）策略和外观转换模块。此外，框架还包括深度正则化，尺度正则化和抗锯齿等模块以提高重建质量。", "result": "实验结果表明该方法有效地重建了大规模城市场景，并在效率和质量上优于先前的方法。", "conclusion": "提出的方法成功地解决了大规模城市场景重建中的效率和外观变化问题，为实际应用提供了强有力的工具。"}}
{"id": "2507.22915", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22915", "abs": "https://arxiv.org/abs/2507.22915", "authors": ["Esmail Gumaan"], "title": "Theoretical Foundations and Mitigation of Hallucination in Large Language Models", "comment": "12 pages", "summary": "Hallucination in Large Language Models (LLMs) refers to the generation of\ncontent that is not faithful to the input or the real-world facts. This paper\nprovides a rigorous treatment of hallucination in LLMs, including formal\ndefinitions and theoretical analyses. We distinguish between intrinsic and\nextrinsic hallucinations, and define a \\textit{hallucination risk} for models.\nWe derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes\nand Rademacher complexity). We then survey detection strategies for\nhallucinations, such as token-level uncertainty estimation, confidence\ncalibration, and attention alignment checks. On the mitigation side, we discuss\napproaches including retrieval-augmented generation, hallucination-aware\nfine-tuning, logit calibration, and the incorporation of fact-verification\nmodules. We propose a unified detection and mitigation workflow, illustrated\nwith a diagram, to integrate these strategies. Finally, we outline evaluation\nprotocols for hallucination, recommending datasets, metrics, and experimental\nsetups to quantify and reduce hallucinations. Our work lays a theoretical\nfoundation and practical guidelines for addressing the crucial challenge of\nhallucination in LLMs.", "AI": {"tldr": "论文区分了内在和外在幻觉，提出了幻觉风险的定义，并探讨了幻觉的检测和缓解策略，如检索增强生成、幻觉感知微调、logit校准和事实验证模块的引入。", "motivation": "大型语言模型（LLM）中幻觉问题的存在，即生成的内容与输入或现实世界事实不符，需要得到严格的处理，包括形式定义和理论分析。", "method": "本论文采用学习理论框架（例如PAC-Bayes和Rademacher复杂度）来界定和分析大型语言模型中的幻觉风险，并探讨了幻觉的检测和缓解策略。", "result": "论文提出了一个综合的幻觉检测和缓解工作流程，并推荐了一些评估协议以量化和减少幻觉。", "conclusion": "这项工作为解决大型语言模型中的幻觉问题提供了理论基础和实践指南。"}}
{"id": "2507.23021", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23021", "abs": "https://arxiv.org/abs/2507.23021", "authors": ["Giuseppe Cartella", "Vittorio Cuculo", "Alessandro D'Amelio", "Marcella Cornia", "Giuseppe Boccignone", "Rita Cucchiara"], "title": "Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision (ICCV), 2025", "summary": "Predicting human gaze scanpaths is crucial for understanding visual\nattention, with applications in human-computer interaction, autonomous systems,\nand cognitive robotics. While deep learning models have advanced scanpath\nprediction, most existing approaches generate averaged behaviors, failing to\ncapture the variability of human visual exploration. In this work, we present\nScanDiff, a novel architecture that combines diffusion models with Vision\nTransformers to generate diverse and realistic scanpaths. Our method explicitly\nmodels scanpath variability by leveraging the stochastic nature of diffusion\nmodels, producing a wide range of plausible gaze trajectories. Additionally, we\nintroduce textual conditioning to enable task-driven scanpath generation,\nallowing the model to adapt to different visual search objectives. Experiments\non benchmark datasets show that ScanDiff surpasses state-of-the-art methods in\nboth free-viewing and task-driven scenarios, producing more diverse and\naccurate scanpaths. These results highlight its ability to better capture the\ncomplexity of human visual behavior, pushing forward gaze prediction research.\nSource code and models are publicly available at\nhttps://aimagelab.github.io/ScanDiff.", "AI": {"tldr": "这篇论文介绍了 ScanDiff ，一种结合扩散模型和视觉变压器的新型架构，以生成多样的、真实的扫描路径，并在实验中展示了其超越现有方法的能力。", "motivation": "尽管深度学习模型在扫描路径预测方面取得了进展，但大多数现有方法产生的都是平均行为，未能捕捉到人类视觉探索的变异性。这项工作旨在解决这一问题。", "method": "我们的方法提出了一种新的架构 ScanDiff，它结合了扩散模型和视觉变压器，以生成多样且真实的扫描路径。该方法通过利用扩散模型的随机性质显式地建模扫描路径的变异性，并引入了文本条件，以实现任务驱动的扫描路径生成，使模型能够适应不同的视觉搜索目标。", "result": "实验结果显示，ScanDiff 在基准数据集上的自由视图和任务驱动场景中均超越了最先进的方法，生成了更多样化和准确的扫描路径。", "conclusion": "这些结果强调了 ScanDiff 在捕捉人类视觉行为复杂性方面的能力，推进了凝视预测研究的发展。"}}
{"id": "2507.22917", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.22917", "abs": "https://arxiv.org/abs/2507.22917", "authors": ["Kwun Hang Lau", "Ruiyuan Zhang", "Weijie Shi", "Xiaofang Zhou", "Xiaojun Cheng"], "title": "Reading Between the Timelines: RAG for Answering Diachronic Questions", "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) excels at injecting static,\nfactual knowledge into Large Language Models (LLMs), it exhibits a critical\ndeficit in handling longitudinal queries that require tracking entities and\nphenomena across time. This blind spot arises because conventional,\nsemantically-driven retrieval methods are not equipped to gather evidence that\nis both topically relevant and temporally coherent for a specified duration. We\naddress this challenge by proposing a new framework that fundamentally\nredesigns the RAG pipeline to infuse temporal logic. Our methodology begins by\ndisentangling a user's query into its core subject and its temporal window. It\nthen employs a specialized retriever that calibrates semantic matching against\ntemporal relevance, ensuring the collection of a contiguous evidence set that\nspans the entire queried period. To enable rigorous evaluation of this\ncapability, we also introduce the Analytical Diachronic Question Answering\nBenchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus\nof real and synthetic financial news. Empirical results on ADQAB show that our\napproach yields substantial gains in answer accuracy, surpassing standard RAG\nimplementations by 13% to 27%. This work provides a validated pathway toward\nRAG systems capable of performing the nuanced, evolutionary analysis required\nfor complex, real-world questions. The dataset and code for this study are\npublicly available at https://github.com/kwunhang/TA-RAG.", "AI": {"tldr": "本文提出了一个能够在检索增强生成系统中融合时间逻辑的新框架，适用于处理长期的查询。实验表明，我们的方法显着提高了问答任务中的答案准确率。", "motivation": "现有基于语义检索的方法无法有效处理长期查询，因为它们无法收集在指定时间段内主题相关且时间连贯的证据。我们提出了一个新的框架来解决这一问题。", "method": "我们的方法首先将用户的查询分解为核心主题和时间窗口。然后，使用一种专门的检索器，它在语义匹配和时间相关性之间进行校准，确保收集到的证据集在整个查询时间段内连贯一致。", "result": "通过ADQAB实验结果显示，我们的方法比标准RAG实现的准确率提高了13%到27%。", "conclusion": "研究为RAG系统处理复杂、现实世界问题提供了有效途径，可以通过对不同时间段内的信息进行细致和演化的分析来补充静态知识。"}}
{"id": "2507.23027", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23027", "abs": "https://arxiv.org/abs/2507.23027", "authors": ["Krishan Agyakari Raja Babu", "Om Prabhu", "Annu", "Mohanasankar Sivaprakasam"], "title": "Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging", "comment": "Accepted at the MICCAI Workshop on \"Medical Image Computing in\n  Resource Constrained Settings & Knowledge Interchange (MIRASOL)\" 2025", "summary": "Automated cardiac interpretation in resource-constrained settings (RCS) is\noften hindered by poor-quality echocardiographic imaging, limiting the\neffectiveness of downstream diagnostic models. While super-resolution (SR)\ntechniques have shown promise in enhancing magnetic resonance imaging (MRI) and\ncomputed tomography (CT) scans, their application to echocardiography-a widely\naccessible but noise-prone modality-remains underexplored. In this work, we\ninvestigate the potential of deep learning-based SR to improve classification\naccuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS\ndataset, we stratify samples by image quality and evaluate two clinically\nrelevant tasks of varying complexity: a relatively simple Two-Chamber vs.\nFour-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole\nvs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR\nmodels-Super-Resolution Generative Adversarial Network (SRGAN) and\nSuper-Resolution Residual Network (SRResNet), to enhance poor-quality images\nand observe significant gains in performance metric-particularly with SRResNet,\nwhich also offers computational efficiency. Our findings demonstrate that SR\ncan effectively recover diagnostic value in degraded echo scans, making it a\nviable tool for AI-assisted care in RCS, achieving more with less.", "AI": {"tldr": "本研究通过使用超分辨率技术改善了低质量二维超声图像的分类准确度，在资源受限环境中具有实用价值。", "motivation": "自动心脏解释在资源受限的环境下常被低质量的超声心动图图像所限制，影响了下游诊断模型的效果。虽然超分辨率技术已在MRI和CT上显示出潜力，但其在超声心动图上的应用尚未被充分探索。本文旨在探讨深度学习超分辨技术是否能改善低质量二维超声图的分类准确度。", "method": "本研究使用了公开的CAMUS数据集，将样本按照图像质量进行分层，并评估了两种临床相关的任务：简单的二腔心与四腔心视图分类，以及复杂的舒张末期与收缩末期分类。研究采用了两种广泛应用的超分辨率模型SRGAN与SRResNet来增强低质量图像，并观察到性能指标上的显著提高，尤其是计算效率更高的SRResNet模型。", "result": "研究发现超分辨率技术能有效恢复低质量超声图像的诊断价值，特别是在使用计算效率较高的SRResNet模型时效果更佳。", "conclusion": "本研究证明了超分辨率技术在提高低质量超声心动图的诊断准确性方面具有潜力，可以作为AI辅助诊断在资源受限环境中的一种有效工具。"}}
{"id": "2507.22918", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22918", "abs": "https://arxiv.org/abs/2507.22918", "authors": ["Daniel Son", "Sanjana Rathore", "Andrew Rufail", "Adrian Simon", "Daniel Zhang", "Soham Dave", "Cole Blondin", "Kevin Zhu", "Sean O'Brien"], "title": "Semantic Convergence: Investigating Shared Representations Across Scaled LLMs", "comment": "Submitted to ACL 2025 Student Research Workshop (poster)", "summary": "We investigate feature universality in Gemma-2 language models (Gemma-2-2B\nand Gemma-2-9B), asking whether models with a four-fold difference in scale\nstill converge on comparable internal concepts. Using the Sparse Autoencoder\n(SAE) dictionary-learning pipeline, we utilize SAEs on each model's\nresidual-stream activations, align the resulting monosemantic features via\nactivation correlation, and compare the matched feature spaces with SVCCA and\nRSA. Middle layers yield the strongest overlap, while early and late layers\nshow far less similarity. Preliminary experiments extend the analysis from\nsingle tokens to multi-token subspaces, showing that semantically similar\nsubspaces interact similarly with language models. These results strengthen the\ncase that large language models carve the world into broadly similar,\ninterpretable features despite size differences, reinforcing universality as a\nfoundation for cross-model interpretability.", "AI": {"tldr": "研究Gemma-2语言模型的特征普遍性，发现不同规模的模型在某些层面上能够收敛到相似的内部概念，表明大规模语言模型的普遍性有助于跨模型解释。", "motivation": "研究Gemma-2语言模型（Gemma-2-2B和Gemma-2-9B）中的特征普遍性，探讨规模相差四倍的模型是否仍然汇聚到相似的内部概念。", "method": "利用稀疏自编码器(SAE)字典学习流程，对每个模型的残差流激活进行SAE处理，通过激活关联对所得的单义特征进行对齐，并使用SVCCA和RSA比较匹配的特征空间。", "result": "中间层的特征重叠最强，而早期和晚期层则显示出较少的相似性。初步实验将分析从单个标记扩展到多个标记子空间，发现语义相似的子空间与语言模型的交互方式相似。", "conclusion": "这些结果强化了大规模语言模型能够将世界分割成广泛相似且可解释特征的观点，即使存在规模差异，这加强了普遍性作为跨模型可解释性的基础。"}}
{"id": "2507.23033", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.23033", "abs": "https://arxiv.org/abs/2507.23033", "authors": ["Ranxi Lin", "Canming Yao", "Jiayi Li", "Weihang Liu", "Xin Lou", "Pingqiang Zhou"], "title": "Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields", "comment": null, "summary": "Neural Radiance Fields (NeRF)-based models have achieved remarkable success\nin 3D reconstruction and rendering tasks. However, during both training and\ninference, these models rely heavily on dense point sampling along rays from\nmultiple viewpoints, resulting in a surge in floating-point operations and\nseverely limiting their use in resource-constrained scenarios like edge\ncomputing. Spiking Neural Networks (SNNs), which communicate via binary spikes\nover discrete time steps, offer a promising alternative due to their\nenergy-efficient nature. Given the inherent variability in scene scale and\ntexture complexity in neural rendering and the prevailing practice of training\nseparate models per scene, we propose a spike-based NeRF framework with a\ndynamic time step training strategy, termed Pretrain-Adaptive Time-step\nAdjustment (PATA). This approach automatically explores the trade-off between\nrendering quality and time step length during training. Consequently, it\nenables scene-adaptive inference with variable time steps and reduces the\nadditional consumption of computational resources in the inference process.\nAnchoring to the established Instant-NGP architecture, we evaluate our method\nacross diverse datasets. The experimental results show that PATA can preserve\nrendering fidelity while reducing inference time steps by 64\\% and running\npower by 61.55\\%.", "AI": {"tldr": "提出了一种基于脉冲神经网络的NeRF框架PATA，该框架可以通过调整时间步长来平衡渲染质量和计算资源消耗，在保持渲染质量的同时减少推理时间和功耗。", "motivation": "现有的NeRF模型在训练和推理过程中需要密集的点采样，这会导致大量的浮点运算，不适用于资源受限的环境。SNN由于其能量效率，被认为是可行的替代方案。同时，针对场景尺度和纹理复杂度的变化，提出一种适合场景自适应推理的方法。", "method": "提出名为PATA的脉冲NeRF框架，利用动态时间步长训练策略自动探索渲染质量和时间步长之间的平衡，以适应不同场景的自适应推理并减少计算资源消耗。建立在Instant-NGP架构上对方法进行评估。", "result": "实验结果显示，PATA可以保持渲染质量的同时减少推理时间步骤64%，运行功率降低61.55%。", "conclusion": "PATA框架有助于解决资源受限场景下的NeRF模型应用问题，提高了能量效率。"}}
{"id": "2507.22919", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22919", "abs": "https://arxiv.org/abs/2507.22919", "authors": ["Qixuan Hu", "Xumou Zhang", "Jinman Kim", "Florence Bourgeois", "Adam G. Dunn"], "title": "A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations", "comment": null, "summary": "Objectives: With accurate estimates of expected safety results, clinical\ntrials could be designed to avoid terminations and limit exposing participants\nto unnecessary risks. We evaluated methods for predicting serious adverse event\n(SAE) results in clinical trials using information only from their\nregistrations prior to the trial. Material and Methods: We analysed 22,107\ntwo-arm parallel interventional clinical trials from ClinicalTrials.gov with\nstructured summary results. Two prediction models were developed: a classifier\npredicting will experimental arm have higher SAE rates (area under the receiver\noperating characteristic curve; AUC) than control arm, and a regression model\nto predict the proportion of SAEs in control arms (root mean squared error;\nRMSE). A transfer learning approach using pretrained language models (e.g.,\nClinicalT5, BioBERT) was used for feature extraction, combined with downstream\nmodel for prediction. To maintain semantic representation in long trial texts\nexceeding localised language model input limits, a sliding window method was\ndeveloped for embedding extraction. Results: The best model\n(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a\nhigher proportion of patients with SAEs. When predicting proportion of\nparticipants experiencing SAE in the control arm, the same model achieved RMSE\nof 18.6%. The sliding window approach consistently outperformed methods without\nit. Across 12 classifiers, the average absolute AUC increase was 2.00%; across\n12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:\nSummary results data available at ClinicalTrials.gov remains underutilised. The\npotential to estimate results of trials before they start is an opportunity to\nimprove trial design and flag discrepancies between expected and reported\nsafety results.", "AI": {"tldr": "研究通过仅使用注册前信息预测临床试验中的严重不良事件（SAE）结果，开发了一种基于预训练语言模型的方法，取得了较好的预测准确性，表明该方法在改善试验设计和警示预期与报告的安全结果之间的不一致性方面具有潜力。", "motivation": "准确估计预期的安全结果，可以帮助临床试验的设计避免终止，并限制参与者的风险暴露。研究的动机是评估仅使用注册前信息来预测临床试验中严重不良事件（SAE）结果的方法。", "method": "该研究使用了转移学习方法，利用预训练语言模型（如ClinicalT5，BioBERT）进行特征抽取，并结合下游模型进行预测。为了在超过本地化语言模型输入限制的长试验文本中保持语义表示，开发了一种滑动窗口方法进行嵌入抽取。研究中开发了两个预测模型：一个分类器用来预测实验组是否有更高的严重不良事件（SAE）发生率，另一个回归模型用来预测对照组的严重不良事件发生率比例。", "result": "使用ClinicalT5+Transformer+MLP的最佳模型预测哪个实验组有更高的严重不良事件发生率的比例时，AUC达到了77.6%。预测对照组研究参与者中经历严重不良事件比例时，相同的模型达到了18.6%的RMSE。滑动窗口方法在性能上始终优于没有使用滑动窗口的方法；在12个分类器中，平均AUC绝对值增加了2.00%；在12个回归模型中，平均RMSE绝对值减少了1.58%。", "conclusion": "ClinicalTrials.gov上可用的总结结果数据仍未充分利用。在试验开始前估计结果的能力为改进试验设计和警示预期与报告的安全结果之间的不一致性提供了机会。"}}
{"id": "2507.23042", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO", "I.2.6; I.2.9; I.2.10; C.3.3"], "pdf": "https://arxiv.org/pdf/2507.23042", "abs": "https://arxiv.org/abs/2507.23042", "authors": ["Santosh Patapati", "Trisanth Srinivasan"], "title": "Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving", "comment": "6 pages", "summary": "Autonomous vehicles must react in milliseconds while reasoning about road\ngeometry and traffic intent to navigate complex situations. We introduce\nNovaDrive, a single-branch vision-language architecture that processes\nfront-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a\nsingle branch. A lightweight, two-stage cross-attention block first aligns\nwaypoint tokens with the HD map, then refines attention over fine-grained image\nand depth patches. Coupled with a novel smoothness loss that discourages abrupt\nsteering and speed changes, this design eliminates the need for recurrent\nmemory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language\nbackbone, enabling real-time inference. On the nuScenes / Waymo subset of the\nMD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts\npath-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from\n2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations\nconfirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention\nfusion each contribute the most to these gains. Beyond safety, NovaDrive's\nshorter routes (resulting from the novel smoothness loss) translate to lower\nfuel or battery usage, pointing toward leaner, more easily updated driving\nstacks. NovaDrive can be extended to other embodied-AI domains as well.", "AI": {"tldr": "NovaDrive 是一种改进的自动驾驶技术，它通过一种单分支视觉-语言架构，集合了图像、高精度地图、LiDAR 深度和路径点信息，高效地提升了自动驾驶的安全性、路径效率以及能源使用效率。", "motivation": "为了提高自动驾驶车辆在快速反应和复杂情况下的导航功能，同时提升驾驶安全性和效率，研究人员提出了 NovaDrive。这个系统试图通过整合多个输入源并优化交叉注意力过程来改进现有多传感器融合的自动驾驶技术。", "method": "NovaDrive 采用了一个轻量级的两阶段交叉注意力模块，首先将路径点与高精度地图对齐，然后通过细化对图像和深度补丁的注意力来增强整个系统的决策过程。项目团队还研发了一个专门用于平滑转弯和速度变化的损失函数，从而实现更加平稳的驾驶效果。最终，这个模型是通过微调 LLaMA-3.2 视觉语言骨干网络的顶层来实现高效实时推理的。", "result": "NovaDrive 是一种单分支视觉语言架构，专门设计用于处理自动驾驶所需的多种传感器数据输入，包括前向相机图像、高精度地图、LiDAR 深度和文字路径点。它通过轻量级的两阶段交叉注意力机制使路径点与高精度地图对齐，并进一步细化图像和深度补丁上的注意力。研究团队还包括一个新颖的平滑损失函数，以减少突然的方向和速度变化。实验结果显示，NovaDrive 在 nuScenes / Waymo 子集上提高了成功率、路径效率，并降低碰撞频率。此外，其较短的路径和能耗优化特性为油耗或电池使用提供了改进。", "conclusion": "NovaDrive 成功地将自动驾驶的成功率、路径效率以及降低碰撞频率进一步提升，证明了其在复杂驾驶情况下的有效性和可靠性。此外，研究还发现了路径优化带来的能源使用效率提升，这对于车辆的安全性、效率和成本节约具有积极意义。"}}
{"id": "2507.22920", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22920", "abs": "https://arxiv.org/abs/2507.22920", "authors": ["Jindong Li", "Yali Fu", "Jiahong Liu", "Linxiao Cao", "Wei Ji", "Menglin Yang", "Irwin King", "Ming-Hsuan Yang"], "title": "Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has intensified the\nneed for effective mechanisms to transform continuous multimodal data into\ndiscrete representations suitable for language-based processing. Discrete\ntokenization, with vector quantization (VQ) as a central approach, offers both\ncomputational efficiency and compatibility with LLM architectures. Despite its\ngrowing importance, there is a lack of a comprehensive survey that\nsystematically examines VQ techniques in the context of LLM-based systems. This\nwork fills this gap by presenting the first structured taxonomy and analysis of\ndiscrete tokenization methods designed for LLMs. We categorize 8 representative\nVQ variants that span classical and modern paradigms and analyze their\nalgorithmic principles, training dynamics, and integration challenges with LLM\npipelines. Beyond algorithm-level investigation, we discuss existing research\nin terms of classical applications without LLMs, LLM-based single-modality\nsystems, and LLM-based multimodal systems, highlighting how quantization\nstrategies influence alignment, reasoning, and generation performance. In\naddition, we identify key challenges including codebook collapse, unstable\ngradient estimation, and modality-specific encoding constraints. Finally, we\ndiscuss emerging research directions such as dynamic and task-adaptive\nquantization, unified tokenization frameworks, and biologically inspired\ncodebook learning. This survey bridges the gap between traditional vector\nquantization and modern LLM applications, serving as a foundational reference\nfor the development of efficient and generalizable multimodal systems. A\ncontinuously updated version is available at:\nhttps://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.", "AI": {"tldr": "The paper presents a comprehensive analysis of vector quantization techniques for LLMs, covering algorithmic principles, challenges, and impacts on LLM performance.", "motivation": "The motivation behind this paper is the lack of a systematic study that analyses vector quantization techniques for transforming continuous multimodal data into discrete representations compatible with LLM architectures, to leverage computational efficiency.", "method": "This paper introduces a structured taxonomy and analysis of discrete tokenization methods for LLMs, focusing on 8 vector quantization (VQ) techniques. It explores the algorithmic principles, training dynamics, and integration challenges, as well as discussing the impact of these techniques on alignment, reasoning, and generation performance in LLM-based systems.", "result": "Identifies key challenges such as codebook collapse and unstable gradient estimation, and explores emerging research directions like dynamic and task-adaptive quantization methods.", "conclusion": "This survey serves as a foundational reference for developing efficient and generalizable multimodal systems by bridging the gap between traditional vector quantization and LLM applications."}}
{"id": "2507.23058", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23058", "abs": "https://arxiv.org/abs/2507.23058", "authors": ["Alexandru Buburuzan"], "title": "Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation", "comment": "A dissertation submitted to The University of Manchester for the\n  degree of Bachelor of Science in Artificial Intelligence", "summary": "Safety-critical applications, such as autonomous driving and medical image\nanalysis, require extensive multimodal data for rigorous testing. Synthetic\ndata methods are gaining prominence due to the cost and complexity of gathering\nreal-world data, but they demand a high degree of realism and controllability\nto be useful. This work introduces two novel methods for synthetic data\ngeneration in autonomous driving and medical image analysis, namely MObI and\nAnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal\nObject Inpainting that leverages a diffusion model to produce realistic and\ncontrollable object inpaintings across perceptual modalities, demonstrated\nsimultaneously for camera and lidar. Given a single reference RGB image, MObI\nenables seamless object insertion into existing multimodal scenes at a\nspecified 3D location, guided by a bounding box, while maintaining semantic\nconsistency and multimodal coherence. Unlike traditional inpainting methods\nthat rely solely on edit masks, this approach uses 3D bounding box conditioning\nto ensure accurate spatial positioning and realistic scaling. AnydoorMed\nextends this paradigm to the medical imaging domain, focusing on\nreference-guided inpainting for mammography scans. It leverages a\ndiffusion-based model to inpaint anomalies with impressive detail preservation,\nmaintaining the reference anomaly's structural integrity while semantically\nblending it with the surrounding tissue. Together, these methods demonstrate\nthat foundation models for reference-guided inpainting in natural images can be\nreadily adapted to diverse perceptual modalities, paving the way for the next\ngeneration of systems capable of constructing highly realistic, controllable\nand multimodal counterfactual scenarios.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.22921", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.22921", "abs": "https://arxiv.org/abs/2507.22921", "authors": ["Lee Harris"], "title": "Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers", "comment": null, "summary": "Language models can capture complex relationships in given text, but these\nare notorious for being costly and for producing information that does not\nexist (i.e., hallucinations). Furthermore, the resources invested into\nproducing this information would be wasted if it were incorrect. We address\nthese issues by proposing, implementing, and applying the Language Model Chain\n(LMC) algorithm. In this, a language model's response to a given prompt about\ngiven text is only correct if it exists in the collection of possible (i.e.,\ncandidate) answers, and text corresponding to incorrect responses is fed into a\nmore predictive (but slower) language model. This process is repeated for a\ncollection of language models, or until all predictions about the text are\ncorrect. We used the LMC algorithm to extract patient dates of birth from\nmedical documents, and combining a collection of language models in a\nmulti-stage cascade significantly increased prediction speed and accuracy over\nindividual language models, while greatly reducing the number of corresponding\nhallucinations. We believe that the novel LMC algorithm significantly\ncontributes to the knowledge extraction field, and that this should be explored\nmuch further in the future.", "AI": {"tldr": "本文提出了语言模型链（LMC）算法，通过多阶段级联集合语言模型提高了预测的速度和准确性，并减少了幻觉现象，尤其在从医疗文档中提取患者出生日期的应用中效果显著。", "motivation": "本文旨在解决语言模型捕捉文本中复杂关系时存在的两大问题：计算成本高昂以及生成不存在信息（幻觉）。特别是在信息不正确的情况下，前期投入的资源将被浪费。为解决这一问题，作者提出了LMC算法。", "method": "本文提出了一种称为语言模型链（LMC）的新算法，通过多阶段级联集合语言模型来提高预测速度和准确性，并减少幻觉现象。具体来说，给定提示下的语言模型响应只有在其候选答案集中存在时才是正确的。对于错误响应对应的文字，则将其输入到一个更具有预测能力但速度较慢的语言模型中。此过程针对多个语言模型重复进行，直到关于文本的所有预测都正确为止。", "result": "通过使用LMC算法从医疗文档中提取患者的出生日期，研究发现与单独使用语言模型相比，多阶段级联集合语言模型显著提高了预测的速度和准确性，同时大幅减少了相应的幻觉现象。", "conclusion": "本文认为LMC算法对知识抽取领域有重大贡献，其进一步研究的可能性非常值得探索。"}}
{"id": "2507.23064", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO", "I.4.8; I.2.10; I.2.6; C.3.3; I.4.9"], "pdf": "https://arxiv.org/pdf/2507.23064", "abs": "https://arxiv.org/abs/2507.23064", "authors": ["Santosh Patapati", "Trisanth Srinivasan", "Murari Ambati"], "title": "Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints", "comment": "5 pages", "summary": "Autonomous cars need geometric accuracy and semantic understanding to\nnavigate complex environments, yet most stacks handle them separately. We\npresent XYZ-Drive, a single vision-language model that reads a front-camera\nframe, a 25m $\\times$ 25m overhead map, and the next waypoint, then outputs\nsteering and speed. A lightweight goal-centered cross-attention layer lets\nwaypoint tokens highlight relevant image and map patches, supporting both\naction and textual explanations, before the fused tokens enter a partially\nfine-tuned LLaMA-3.2 11B model.\n  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and\n0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and\nhalving collisions, all while significantly improving efficiency by using only\na single branch. Sixteen ablations explain the gains. Removing any modality\n(vision, waypoint, map) drops success by up to 11%, confirming their\ncomplementary roles and rich connections. Replacing goal-centered attention\nwith simple concatenation cuts 3% in performance, showing query-based fusion\ninjects map knowledge more effectively. Keeping the transformer frozen loses\n5%, showing the importance of fine-tuning when applying VLMs for specific tasks\nsuch as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs\nlane edges and raises crash rate.\n  Overall, these results demonstrate that early, token-level fusion of intent\nand map layout enables accurate, transparent, real-time driving.", "AI": {"tldr": "论文介绍了XYZ-Drive，一个单一的视觉语言模型，用于处理自动驾驶汽车的几何精度和语义理解。该模型结合了摄像头画面、俯视地图和下一个路径点信息，能够输出车辆的转向和速度，从而在复杂环境中导航。", "motivation": "现有的自动驾驶系统通常独立处理几何精度和语义理解。该研究的目标是展示通过视觉语言模型的联合处理方法可以提高性能，简化系统架构，并降低碰撞率。", "method": "XYZ-Drive模型使用了一种较轻量级的目标导向交叉注意力层，只需一次计算即可在模型的不同部分之间进行数据融合，包括摄像头画面、地图和车辆路径点位置。整个模型以一个部分微调过的LLaMA-3.2 11B为基础。", "result": "在MD-NEX Outdoor-Driving基准测试中，XYZ-Drive达到了95%的成功率和0.80的成功率加权路径长度（SPL），其性能超越了PhysNav-DG 15%，并将碰撞率降低了一半。多功能测试表明移除任何一种模态将导致高达11%的成功率下降；简单地将数据堆叠降低模型3%的表现，而冻结变压器则降低5%的性能。", "conclusion": "实验结果表明，意图和地图布局的早期、基于标记的融合可以实现准确、透明、实时的驾驶。这证明了单一模型处理多种模态信息的可行性及其增强自动驾驶性能的优点。"}}
{"id": "2507.22922", "categories": ["cs.CL", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.22922", "abs": "https://arxiv.org/abs/2507.22922", "authors": ["Mateusz Kmak", "Kamil Chmurzyński", "Kamil Matejuk", "Paweł Kotzbach", "Jan Kocoń"], "title": "Predicting stock prices with ChatGPT-annotated Reddit sentiment", "comment": "International Conference on Computational Science 2025", "summary": "The surge of retail investor activity on social media, exemplified by the\n2021 GameStop short squeeze, raised questions about the influence of online\nsentiment on stock prices. This paper explores whether sentiment derived from\nsocial media discussions can meaningfully predict stock market movements. We\nfocus on Reddit's r/wallstreetbets and analyze sentiment related to two\ncompanies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's\nrole, we employ two existing text-based sentiment analysis methods and\nintroduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model\ndesigned to better interpret the informal language and emojis prevalent in\nsocial media discussions. We use correlation and causality metrics to determine\nthese models' predictive power. Surprisingly, our findings suggest that social\nmedia sentiment has only a weak correlation with stock prices. At the same\ntime, simpler metrics, such as the volume of comments and Google search trends,\nexhibit stronger predictive signals. These results highlight the complexity of\nretail investor behavior and suggest that traditional sentiment analysis may\nnot fully capture the nuances of market-moving online discussions.", "AI": {"tldr": "本研究分析了两个公司GameStop和AMC在Reddit上的讨论情绪，并使用了三种不同的情绪分析模型。结果表明，社交媒体情绪与股价的相关性较弱，但评论量和谷歌搜索趋势显示出更强的预测信号。这表明零售投资者行为的复杂性，传统的情绪分析可能无法完全捕捉市场移动的在线讨论中的细微差别。", "motivation": "鉴于2021年GameStop短挤压事件引发的零售投资者在社交媒体上的活动激增，这篇论文探讨了社交媒体讨论中提取的情绪是否能有意义地预测股市走势。", "method": "本研究聚焦于Reddit的r/wallstreetbets板块，分析了GameStop (GME) 和AMC Entertainment (AMC)两家公司的社交媒体情绪。研究采用了两种现有文本情绪分析方法，并引入了一种基于ChatGPT注释和微调的RoBERTa模型，以更好地解读社交媒体中普遍存在的非正式语言和表情符号。使用相关性和因果性指标来确定这些模型的预测能力。", "result": "研究结果表明，社交媒体情绪与股价仅有弱相关性。与此相反，如评论量和谷歌搜索趋势等更简单的指标显示出更强的预测信号。", "conclusion": "这些结果强调了零售投资者行为的复杂性，并表明传统的情感分析可能无法完全捕捉到市场走势的在线讨论中的细微差异。"}}
{"id": "2507.23070", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23070", "abs": "https://arxiv.org/abs/2507.23070", "authors": ["Dmitry Demidov", "Zaigham Zaheer", "Omkar Thawakar", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model", "comment": "Accepted to ICCV 2025", "summary": "Fine-grained image classification, the task of distinguishing between\nvisually similar subcategories within a broader category (e.g., bird species,\ncar models, flower types), is a challenging computer vision problem.\nTraditional approaches rely heavily on fixed vocabularies and closed-set\nclassification paradigms, limiting their scalability and adaptability in\nreal-world settings where novel classes frequently emerge. Recent research has\ndemonstrated that combining large language models (LLMs) with vision-language\nmodels (VLMs) makes open-set recognition possible without the need for\npredefined class labels. However, the existing methods are often limited in\nharnessing the power of LLMs at the classification phase, and also rely heavily\non the guessed class names provided by an LLM without thorough analysis and\nrefinement. To address these bottlenecks, we propose our training-free method,\nEnriched-FineR (or E-FineR for short), which demonstrates state-of-the-art\nresults in fine-grained visual recognition while also offering greater\ninterpretability, highlighting its strong potential in real-world scenarios and\nnew domains where expert annotations are difficult to obtain. Additionally, we\ndemonstrate the application of our proposed approach to zero-shot and few-shot\nclassification, where it demonstrated performance on par with the existing SOTA\nwhile being training-free and not requiring human interventions. Overall, our\nvocabulary-free framework supports the shift in image classification from rigid\nlabel prediction to flexible, language-driven understanding, enabling scalable\nand generalizable systems for real-world applications. Well-documented code is\navailable on https://github.com/demidovd98/e-finer.", "AI": {"tldr": "本文提出了一种无训练方法Enriched-FineR(E-FineR)，通过综合利用大型语言模型和视觉语言模型的潜力，在细粒度图像分类中取得了SOTA结果，同时提高了可解释性，适用于缺乏专家注释的新领域。该方法还展示了在零样本和少样本分类中的出色表现。", "motivation": "当前细粒度图像分类方法依赖固定词汇和闭集分类，限制了其在新类别不断出现的实际设置中的可扩展性和适应能力。本文旨在改进现有方法，在分类阶段更好地利用大型语言模型，避免过度依赖未经详尽分析的语言模型预测类别名称。", "method": "本文提出了E-FineR，一种无训练的方法，通过结合大型语言模型和视觉语言模型，在不依赖预定义类别标签的情况下实现细粒度视觉识别。", "result": "E-FineR展示了在细粒度图像分类中的先进性能，特别是在零样本和少样本分类任务中表现优秀，展现了其在真实世界应用中的潜力。", "conclusion": "本文介绍的方法支持图像分类从刚性的标签预测转向灵活的语言驱动理解，为实际应用提供了可扩展和通用的系统。"}}
{"id": "2507.22923", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22923", "abs": "https://arxiv.org/abs/2507.22923", "authors": ["Aman Gupta", "Yingying Zhuang", "Zhou Yu", "Ziji Zhang", "Anurag Beniwal"], "title": "How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting", "comment": "Accepted at Prompt Optimization KDD '25", "summary": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs), their performance varies substantially across different languages and\ntasks. In multilingual retrieval-augmented generation (RAG)-based systems,\nknowledge bases (KB) are often shared from high-resource languages (such as\nEnglish) to low-resource ones, resulting in retrieved information from the KB\nbeing in a different language than the rest of the context. In such scenarios,\ntwo common practices are pre-translation to create a mono-lingual prompt and\ncross-lingual prompting for direct inference. However, the impact of these\nchoices remains unclear. In this paper, we systematically evaluate the impact\nof different prompt translation strategies for classification tasks with\nRAG-enhanced LLMs in multilingual systems. Experimental results show that an\noptimized prompting strategy can significantly improve knowledge sharing across\nlanguages, therefore improve the performance on the downstream classification\ntask. The findings advocate for a broader utilization of multilingual resource\nsharing and cross-lingual prompt optimization for non-English languages,\nespecially the low-resource ones.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.23110", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23110", "abs": "https://arxiv.org/abs/2507.23110", "authors": ["Zheyuan Zhang", "Linkai Peng", "Wanying Dou", "Cuiling Sun", "Halil Ertugrul Aktas", "Andrea M. Bejar", "Elif Keles", "Gorkem Durak", "Ulas Bagci"], "title": "Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation", "comment": null, "summary": "Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences\nwhose appearance differs more than the acquisition sites that produce them.\nExisting domain-generalization benchmarks focus almost on cross-center shifts\nand overlook this dominant source of variability. Pancreas segmentation remains\na major challenge in abdominal imaging: the gland is small, irregularly,\nsurrounded by organs and fat, and often suffers from low T1 contrast.\nState-of-the-art deep networks that already achieve >90% Dice on the liver or\nkidneys still miss 20-30% of the pancreas. The organ is also systematically\nunder-represented in public cross-domain benchmarks, despite its clinical\nimportance in early cancer detection, surgery, and diabetes research. To close\nthis gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas\nsegmentation dataset for investigating domain generalization in medical\nimaging. The dataset comprises 563 MRI scans from six institutions, spanning\nboth venous phase and out-of-phase sequences, enabling study of both\ncross-center and cross-sequence variations with pixel-accurate pancreas masks\ncreated by a double-blind, two-pass protocol. Through comprehensive analysis,\nwe reveal three insights: (i) limited sampling introduces significant variance\nthat may be mistaken for distribution shifts, (ii) cross-center performance\ncorrelates with source domain performance for identical sequences, and (iii)\ncross-sequence shifts require specialized solutions. We also propose a\nsemi-supervised approach that leverages anatomical invariances, significantly\noutperforming state-of-the-art domain generalization techniques with 61.63%\nDice score improvements and 87.00% on two test centers for cross-sequence\nsegmentation. PancreasDG sets a new benchmark for domain generalization in\nmedical imaging. Dataset, code, and models will be available at\nhttps://pancreasdg.netlify.app.", "AI": {"tldr": "提出了PancreasDG数据集用于研究医学影像中的领域泛化问题，揭示了领域泛化的一些见解，并提出了一种利用解剖不变性的半监督方法，在跨序列分割上显著优于最先进的领域泛化技术。", "motivation": "现有的领域泛化基准主要集中在跨中心转换上，忽略了T1和T2序列之间的主导变异性。胰腺分割在腹部成像中仍然是一个重大挑战，现有的深度网络在胰腺分割上的表现仍不尽如人意。", "method": "通过构建大规模多中心3D MRI胰腺分割数据集PancreasDG，研究医学影像中的领域泛化问题。该数据集包含6个机构的563个MRI扫描，覆盖静脉相和反相序列。", "result": "研究结果揭示了领域泛化在胰腺分割中的三个见解：有限采样引入的显著方差可能被误认为是分布转换，跨中心性能与相同序列的源领域性能相关，而跨序列转换需要专门的解决方案。此外，提出的半监督方法在跨序列分割中Dice分数提高了61.63%。", "conclusion": "PancreasDG为医学影像中的领域泛化设立了新的基准。"}}
{"id": "2507.22924", "categories": ["cs.CL", "I.2.7; K.3.1"], "pdf": "https://arxiv.org/pdf/2507.22924", "abs": "https://arxiv.org/abs/2507.22924", "authors": ["Brittney Exline", "Melanie Duffin", "Brittany Harbison", "Chrissa da Gomez", "David Joyner"], "title": "Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers", "comment": null, "summary": "Graduate-level CS programs in the U.S. increasingly enroll international\nstudents, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.\nstudents. Many of these students take online courses, where peer feedback is\nused to engage students and improve pedagogy in a scalable manner. Since these\ncourses are conducted in English, many students study in a language other than\ntheir first. This paper examines how native versus non-native English speaker\nstatus affects three metrics of peer feedback experience in online U.S.-based\ncomputing courses. Using the Twitter-roBERTa-based model, we analyze the\nsentiment of peer reviews written by and to a random sample of 500 students. We\nthen relate sentiment scores and peer feedback ratings to students' language\nbackground. Results show that native English speakers rate feedback less\nfavorably, while non-native speakers write more positively but receive less\npositive sentiment in return. When controlling for sex and age, significant\ninteractions emerge, suggesting that language background plays a modest but\ncomplex role in shaping peer feedback experiences.", "AI": {"tldr": "本论文探讨在线美国计算机课程中，英语母语者与非母语者在同行反馈体验三个指标上的差异，并揭示了语言背景在同行反馈体验中的作用。", "motivation": "鉴于越来越多的国际学生参加在线课程，并且很多学生在学习语言非母语的英语课程，作者希望通过研究语言背景在同行反馈体验中的作用，来优化在线教育中的互动和反馈机制。", "method": "本论文采用基于Twitter-roBERTa的模型分析来自和给随机抽取的500名学生的同行评审的语气，并将语气评分和同行反馈评价与学生的语言背景相关联。", "result": "研究结果表明，母语为英语的学生对反馈的评价较低，而非英语背景的学生写出的评价更加积极但接收到的反馈语气不太积极。当控制性别和年龄因素后，语言背景显示出一定程度的复杂且微妙的作用。", "conclusion": "语言背景对在线同行反馈体验具有微妙但复杂的影响。非英语背景学生写给同伴的反馈更积极，但其收到的反馈语气评分较低，这表明语言因素在同行反馈过程中扮演了重要角色。"}}
{"id": "2507.23134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23134", "abs": "https://arxiv.org/abs/2507.23134", "authors": ["Sanghun Jung", "Jingjing Zheng", "Ke Zhang", "Nan Qiao", "Albert Y. C. Chen", "Lu Xia", "Chi Liu", "Yuyin Sun", "Xiao Zeng", "Hsiang-Wei Huang", "Byron Boots", "Min Sun", "Cheng-Hao Kuo"], "title": "Details Matter for Indoor Open-vocabulary 3D Instance Segmentation", "comment": "ICCV 2025", "summary": "Unlike closed-vocabulary 3D instance segmentation that is often trained\nend-to-end, open-vocabulary 3D instance segmentation (OV-3DIS) often leverages\nvision-language models (VLMs) to generate 3D instance proposals and classify\nthem. While various concepts have been proposed from existing research, we\nobserve that these individual concepts are not mutually exclusive but\ncomplementary. In this paper, we propose a new state-of-the-art solution for\nOV-3DIS by carefully designing a recipe to combine the concepts together and\nrefining them to address key challenges. Our solution follows the two-stage\nscheme: 3D proposal generation and instance classification. We employ robust 3D\ntracking-based proposal aggregation to generate 3D proposals and remove\noverlapped or partial proposals by iterative merging/removal. For the\nclassification stage, we replace the standard CLIP model with Alpha-CLIP, which\nincorporates object masks as an alpha channel to reduce background noise and\nobtain object-centric representation. Additionally, we introduce the\nstandardized maximum similarity (SMS) score to normalize text-to-proposal\nsimilarity, effectively filtering out false positives and boosting precision.\nOur framework achieves state-of-the-art performance on ScanNet200 and S3DIS\nacross all AP and AR metrics, even surpassing an end-to-end closed-vocabulary\nmethod.", "AI": {"tldr": "The paper presents a new, state-of-the-art method for open-vocabulary 3D instance segmentation through a two-stage process of 3D proposal generation and instance classification, which outperforms existing approaches.", "motivation": "The paper aims to improve upon existing open-vocabulary 3D instance segmentation (OV-3DIS) approaches by synthesizing various concepts into a cohesive solution to address key challenges.", "method": "Our solution includes a two-stage scheme: first, robust 3D tracking-based proposal aggregation is used for generating 3D proposals, with iterative merging/removal of overlaps and partials. Second, Alpha-CLIP, incorporating object masks to reduce background noise, is employed for classification, along with the use of the standardized maximum similarity (SMS) score to improve precision.", "result": "The framework attains state-of-the-art performance on ScanNet200 and S3DIS in all AP and AR metrics, even surpassing an end-to-end closed-vocabulary method.", "conclusion": "The proposed method effectively combines various concepts into a novel, high-performing framework for OV-3DIS, demonstrating its superiority over current methods on standard benchmarks."}}
{"id": "2507.22925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22925", "abs": "https://arxiv.org/abs/2507.22925", "authors": ["Haoran Sun", "Shaoning Zeng"], "title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "comment": null, "summary": "Long-term memory is one of the key factors influencing the reasoning\ncapabilities of Large Language Model Agents (LLM Agents). Incorporating a\nmemory mechanism that effectively integrates past interactions can\nsignificantly enhance decision-making and contextual coherence of LLM Agents.\nWhile recent works have made progress in memory storage and retrieval, such as\nencoding memory into dense vectors for similarity-based search or organizing\nknowledge in the form of graph, these approaches often fall short in structured\nmemory organization and efficient retrieval. To address these limitations, we\npropose a Hierarchical Memory (H-MEM) architecture for LLM Agents that\norganizes and updates memory in a multi-level fashion based on the degree of\nsemantic abstraction. Each memory vector is embedded with a positional index\nencoding pointing to its semantically related sub-memories in the next layer.\nDuring the reasoning phase, an index-based routing mechanism enables efficient,\nlayer-by-layer retrieval without performing exhaustive similarity computations.\nWe evaluate our method on five task settings from the LoCoMo dataset.\nExperimental results show that our approach consistently outperforms five\nbaseline methods, demonstrating its effectiveness in long-term dialogue\nscenarios.", "AI": {"tldr": "本文提出了一种层次记忆结构，通过多层次的组织和位置索引编码，提高了大型语言模型的长期记忆推理能力，资源利用更高效，比五种基准方法有显著提升。", "motivation": "长期记忆力是影响大型语言模型代理（LLM Agent）推理能力的关键因素之一。尽管在记忆存储和检索方面已经取得了进展，但现有的方法在结构化记忆组织和检索效率上仍存在不足。为了应对这些问题，我们提出了这种新架构。", "method": "我们提出了一种层次记忆（H-MEM）架构，该架构根据语义抽象的程度以多层次方式组织和更新记忆。每个记忆向量都嵌入了一个位置索引编码，指向其在下一层的语义相关子记忆。在推理阶段，索引路由机制使得可以在不进行详尽相似性计算的情况下进行高效、逐层检索。", "result": "我们在LoCoMo数据集上的五个任务设置中进行了实验，结果表明，我们的方法在长期对话场景下，其表现优于五种基准方法，证明了其有效性。", "conclusion": "提出的H-MEM架构在长期记忆的组织和检索方面，为大型语言模型提供了更有效的解决方案，在具体对话任务中，其性能优于现有方法。"}}
{"id": "2507.23143", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23143", "abs": "https://arxiv.org/abs/2507.23143", "authors": ["Xiaochen Zhao", "Hongyi Xu", "Guoxian Song", "You Xie", "Chenxu Zhang", "Xiu Li", "Linjie Luo", "Jinli Suo", "Yebin Liu"], "title": "X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention", "comment": "ICLR 2025, code is available at\n  https://github.com/bytedance/x-nemo-inference", "summary": "We propose X-NeMo, a novel zero-shot diffusion-based portrait animation\npipeline that animates a static portrait using facial movements from a driving\nvideo of a different individual. Our work first identifies the root causes of\nthe key issues in prior approaches, such as identity leakage and difficulty in\ncapturing subtle and extreme expressions. To address these challenges, we\nintroduce a fully end-to-end training framework that distills a 1D\nidentity-agnostic latent motion descriptor from driving image, effectively\ncontrolling motion through cross-attention during image generation. Our\nimplicit motion descriptor captures expressive facial motion in fine detail,\nlearned end-to-end from a diverse video dataset without reliance on pretrained\nmotion detectors. We further enhance expressiveness and disentangle motion\nlatents from identity cues by supervising their learning with a dual GAN\ndecoder, alongside spatial and color augmentations. By embedding the driving\nmotion into a 1D latent vector and controlling motion via cross-attention\nrather than additive spatial guidance, our design eliminates the transmission\nof spatial-aligned structural clues from the driving condition to the diffusion\nbackbone, substantially mitigating identity leakage. Extensive experiments\ndemonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly\nexpressive animations with superior identity resemblance. Our code and models\nare available for research.", "AI": {"tldr": "本文介绍了一种改进的零样本扩散式肖像动画技术X-NeMo，该技术能够有效减少身份泄露并提高表达程度，生成高度相似的动画。", "motivation": "本文旨在解决当前方法中的关键问题，包括身份泄露和捕捉细微与极端表情的难度，以改进肖像动画生成技术。", "method": "本文提出了X-NeMo，这是一种新颖的零样本扩散式肖像动画生成管线，可以使用来自不同个体的驱动视频，对静态肖像进行动画处理。为了克服身份泄露和捕捉细微及极端表情的难题，本文引入了一个全流程训练框架，该框架能够从驱动图像中提取身份无关的1D潜藏运动描述符，并通过交叉注意力机制控制图像生成过程中的运动。此外，本文通过一个双生成对抗网络（Dual GAN）解码器监督学习来增强表达效果和分离运动潜藏与身份线索，同时还采用了空间和色彩增强技术。通过将驱动运动嵌入到1D潜藏向量中，通过对交叉注意力机制而非空间引导来进行运动控制，本方法显著减轻了身份泄露的问题。", "result": "大量实验结果表明，X-NeMo在生成高度表达的动画方面超越了现有的最高基线方法，同时保持了优秀的身份相似度。", "conclusion": "本文提出的X-NeMo通过一个完全新的训练框架，有效解决了身份泄露和表情细节捕捉的问题，在生成动画方面表现出卓越的性能。"}}
{"id": "2507.22926", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.22926", "abs": "https://arxiv.org/abs/2507.22926", "authors": ["Nilesh", "Atul Gupta", "Avinash C Panday"], "title": "Multi-Relation Extraction in Entity Pairs using Global Context", "comment": "11 pages, 9 figures", "summary": "In document-level relation extraction, entities may appear multiple times in\na document, and their relationships can shift from one context to another.\nAccurate prediction of the relationship between two entities across an entire\ndocument requires building a global context spanning all relevant sentences.\nPrevious approaches have focused only on the sentences where entities are\nmentioned, which fails to capture the complete document context necessary for\naccurate relation extraction. Therefore, this paper introduces a novel input\nembedding approach to capture the positions of mentioned entities throughout\nthe document rather than focusing solely on the span where they appear. The\nproposed input encoding approach leverages global relationships and\nmulti-sentence reasoning by representing entities as standalone segments,\nindependent of their positions within the document. The performance of the\nproposed method has been tested on three benchmark relation extraction\ndatasets, namely DocRED, Re-DocRED, and REBEL. The experimental results\ndemonstrated that the proposed method accurately predicts relationships between\nentities in a document-level setting. The proposed research also has\ntheoretical and practical implications. Theoretically, it advances global\ncontext modeling and multi-sentence reasoning in document-level relation\nextraction. Practically, it enhances relationship detection, enabling improved\nperformance in real-world NLP applications requiring comprehensive entity-level\ninsights and interpretability.", "AI": {"tldr": "This paper proposes a new approach to document-level relation extraction by capturing entities' positions across the entire document, rather than focusing only on their immediate context, improving relational accuracy and offering both theoretical advancements and practical applications in NLP.", "motivation": "The motivation is to address the limitation of previous approaches in document-level relation extraction where entities are considered only in the sentences they are mentioned, failing to capture the complete document context necessary for accurate relation extraction.", "method": "The proposed method introduces a novel input embedding approach that captures the positions of mentioned entities throughout the document, treating entities as standalone segments independent of their positions, thus leveraging global relationships and multi-sentence reasoning.", "result": "The experimental results on datasets like DocRED and Re-DocRED showed improved performance in predicting the relationships between entities across an entire document.", "conclusion": "The research not only advances the theoretical understanding of global context modeling in document-level relation extraction but also has practical implications for enhancing real-world NLP applications that require comprehensive entity-level insights."}}
{"id": "2507.23150", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23150", "abs": "https://arxiv.org/abs/2507.23150", "authors": ["Philip Wootaek Shin", "Vishal Gaur", "Rahul Ramachandran", "Manil Maskey", "Jack Sampson", "Vijaykrishnan Narayanan", "Sujit Roy"], "title": "Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery", "comment": null, "summary": "High-resolution satellite imagery is essential for geospatial analysis, yet\ndifferences in spatial resolution across satellite sensors present challenges\nfor data fusion and downstream applications. Super-resolution techniques can\nhelp bridge this gap, but existing methods rely on artificially downscaled\nimages rather than real sensor data and are not well suited for heterogeneous\nsatellite sensors with differing spectral, temporal characteristics. In this\nwork, we develop a preliminary framework to align and Harmonized Landsat\nSentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a\nreference from the HLS dataset. Our approach aims to bridge the resolution gap\nbetween these sensors and improve the quality of super-resolved Landsat\nimagery. Quantitative and qualitative evaluations demonstrate the effectiveness\nof our method, showing its potential for enhancing satellite-based sensing\napplications. This study provides insights into the feasibility of\nheterogeneous satellite image super-resolution and highlights key\nconsiderations for future advancements in the field.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.22927", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22927", "abs": "https://arxiv.org/abs/2507.22927", "authors": ["Zhehao Tan", "Yihan Jiao", "Dan Yang", "Lei Liu", "Jie Feng", "Duolin Sun", "Yue Shen", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge, where the LLM's ability to generate responses\nbased on the combination of a given query and retrieved documents is crucial.\nHowever, most benchmarks focus on overall RAG system performance, rarely\nassessing LLM-specific capabilities. Current benchmarks emphasize broad aspects\nsuch as noise robustness, but lack a systematic and granular evaluation\nframework on document utilization. To this end, we introduce\n\\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,\nemphasizing the following progressive dimensions: (1) multi-level filtering\nabilities, (2) combination abilities, and (3) reference reasoning. To provide a\nmore nuanced understanding of LLMs' roles in RAG systems, we formulate an\ninnovative placeholder-based approach to decouple the contributions of the\nLLM's parametric knowledge and the external knowledge. Experiments demonstrate\nthe limitations of representative LLMs in the RAG system's generation\ncapabilities, particularly in error resilience and context faithfulness. Our\nbenchmark provides a reproducible framework for developing more reliable and\nefficient RAG systems. Our code is available in\nhttps://github.com/Alipay-Med/PRGB.", "AI": {"tldr": "该研究引入了Placeholder-RAG-Benchmark，这是一个多层级细致的评估框架，用于系统化地评估RAG系统中大语言模型对文档的利用能力。通过创新的占位符方法，该框架能更细致地理解LLM在其处理外部知识时的作用。该研究揭示了代表性LLM在RAG系统中的生成能力存在的局限，特别是在错误弹性和上下文忠实度方面。", "motivation": "该论文的动机是改进对RAG系统中大语言模型能力的评估，特别是在文档利用能力方面的系统化和细分评估，现有的基准测试往往没有全面考虑这些方面。", "method": "提出了Placeholder-RAG-Benchmark，通过多层级的评估维度，包括多层级过滤能力、组合能力和参考推理能力，以及一种创新的占位符方法，来分解LLM的参数化知识和外部知识的贡献。", "result": "实验表明，代表性LLMs在RAG系统生成能力方面存在局限性，特别是在错误弹性和保持上下文忠实度方面。", "conclusion": "Placeholder-RAG-Benchmark提供了一个可复制的框架，用于更可靠和高效地开发RAG系统。"}}
{"id": "2507.23162", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23162", "abs": "https://arxiv.org/abs/2507.23162", "authors": ["Xu Cao", "Takafumi Taketomi"], "title": "Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues", "comment": "Accepted to ICCV 2025", "summary": "We propose a neural inverse rendering approach that jointly reconstructs\ngeometry, spatially varying reflectance, and lighting conditions from\nmulti-view images captured under varying directional lighting. Unlike prior\nmulti-view photometric stereo methods that require light calibration or\nintermediate cues such as per-view normal maps, our method jointly optimizes\nall scene parameters from raw images in a single stage. We represent both\ngeometry and reflectance as neural implicit fields and apply shadow-aware\nvolume rendering. A spatial network first predicts the signed distance and a\nreflectance latent code for each scene point. A reflectance network then\nestimates reflectance values conditioned on the latent code and angularly\nencoded surface normal, view, and light directions. The proposed method\noutperforms state-of-the-art normal-guided approaches in shape and lighting\nestimation accuracy, generalizes to view-unaligned multi-light images, and\nhandles objects with challenging geometry and reflectance.", "AI": {"tldr": "本文提出了一种神经逆渲染方法，可用于从多视角图像中重建几何结构、空间变化反射率和光照条件，超越现有方法，处理更复杂场景。", "motivation": "该研究的动力在于提出一种不需要光校准或中间提示（如每个视图的法线图）即可从原始图像中一次性优化所有场景参数的方法，从而改进之前多视角光度立体方法的局限性。", "method": "我们的方法是神经逆渲染技术，可以从多视角图像中同时重建几何图形、空间变化的反射率和光照条件。这种方法将几何图形和反射率表示为神经隐式场，并使用阴影感知的体积渲染技术。首先，一个空间网络预测每个场景点的符号距离和反射潜码，然后反射网络根据潜码和角度编码的表面法线、视线和光源方向来估计反射率值。", "result": "提出的方法在形状和光照估计准确度上优于现有的基于法线的先进技术，可以推广到视角不一致的多光源图像，能够处理具有挑战性的几何图形和反射率的对象。", "conclusion": "这种方法展示了一种有效的方式，提升了在复杂光照条件下从多视角图像重建场景质量和精度的能力。"}}
{"id": "2507.22928", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22928", "abs": "https://arxiv.org/abs/2507.22928", "authors": ["Xi Chen", "Aske Plaat", "Niki van Stein"], "title": "How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding", "comment": null, "summary": "Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on\nmulti-step tasks, yet whether the generated \"thoughts\" reflect the true\ninternal reasoning process is unresolved. We present the first feature-level\ncausal study of CoT faithfulness. Combining sparse autoencoders with activation\npatching, we extract monosemantic features from Pythia-70M and Pythia-2.8B\nwhile they tackle GSM8K math problems under CoT and plain (noCoT) prompting.\nSwapping a small set of CoT-reasoning features into a noCoT run raises answer\nlog-probabilities significantly in the 2.8B model, but has no reliable effect\nin 70M, revealing a clear scale threshold. CoT also leads to significantly\nhigher activation sparsity and feature interpretability scores in the larger\nmodel, signalling more modular internal computation. For example, the model's\nconfidence in generating correct answers improves from 1.2 to 4.3. We introduce\npatch-curves and random-feature patching baselines, showing that useful CoT\ninformation is not only present in the top-K patches but widely distributed.\nOverall, our results indicate that CoT can induce more interpretable internal\nstructures in high-capacity LLMs, validating its role as a structured prompting\nmethod.", "AI": {"tldr": "通过分析chain-of-thought (CoT)提示对不同规模模型的影响，研究揭示了CoT提示对大型语言模型内部推理过程的影响，并验证了该提示作为一种结构化提示方法的有效性。", "motivation": "探究chain-of-thought (CoT)提示生成的思维链条是否真实反映了大型语言模型内部的推理过程。", "method": "结合稀疏自编码器与激活修补技术，从处理GSM8K数学问题的Pythia-70M和Pythia-2.8B模型中提取单义特征，并对比chain-of-thought (CoT)提示和普通提示下的表现。通过交换少量CoT推理特征到普通提示运行中来测试其对模型性能的影响。", "result": "向2.8B模型的小批量普通提示运行中引入少量CoT推理特征会大幅提高答案的对数概率，但在70M模型中则没有显著效果，表明了明显的规模阈值。CoT提示在较大的模型中引发了更高的激活稀疏性和特征可解释性得分，显示出更加模块化的内部计算。例如，模型生成正确答案的信心从1.2提升至4.3。", "conclusion": "研究结果表明，CoT提示可以在高容量的大型语言模型中诱导出更可解释的内部结构，证实其作为结构化提示方法的有效性。"}}
{"id": "2507.23174", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23174", "abs": "https://arxiv.org/abs/2507.23174", "authors": ["Beatriz Díaz Peón", "Jorge Torres Gómez", "Ariel Fajardo Márquez"], "title": "CNN-based solution for mango classification in agricultural environments", "comment": null, "summary": "This article exemplifies the design of a fruit detection and classification\nsystem using Convolutional\n  Neural Networks (CNN). The goal is to develop a system that automatically\nassesses fruit quality for\n  farm inventory management. Specifically, a method for mango fruit\nclassification was developed using\n  image processing, ensuring both accuracy and efficiency. Resnet-18 was\nselected as the preliminary\n  architecture for classification, while a cascade detector was used for\ndetection, balancing execution speed\n  and computational resource consumption. Detection and classification results\nwere displayed through a\n  graphical interface developed in MatLab App Designer, streamlining system\ninteraction. The integration\n  of convolutional neural networks and cascade detectors proffers a reliable\nsolution for fruit classification\n  and detection, with potential applications in agricultural quality control.", "AI": {"tldr": "该论文展示了一种使用卷积神经网络设计的水果检测和分类系统，着重介绍了芒果分类方法，采用了Resnet-18及级联检测器，提升了农业中水果质量评估的准确性和效率，并进一步开发了交互界面，证明了其在农业生产中的应用潜力。", "motivation": "该研究的动机在于为农场库存管理开发了一个可以自动评估水果质量的系统。目的是利用先进的计算机视觉技术来提高水果分类的准确性和效率，并在农业生产质量控制中启用潜在的应用。", "method": "该论文采用卷积神经网络（CNN）设计了一种水果检测与分类系统，特别是使用图像处理技术开发了一种芒果分类方法。选择了Resnet-18作为初步分类架构，同时为了平衡执行速度和计算资源消耗，使用了级联检测器进行检测。成果通过在MatLab App Designer中开发的图形界面展示，简化了系统交互。", "result": "该系统实现了水果的精准分类，确保了效率的同时探测到水果的质量问题，证明了算法的有效性。通过图形界面使系统操作更加简单，为实际应用提供了新思路。", "conclusion": "研究结论认为，卷积神经网络与级联检测器的结合提供了可靠的方法来解决水果的分类和检测问题，这为农业中的品质控制提供了有力的支持，证明了其在实际应用中的可行性。"}}
{"id": "2507.22929", "categories": ["cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.22929", "abs": "https://arxiv.org/abs/2507.22929", "authors": ["Xiaoyu Pan", "Yang Bai", "Ke Zou", "Yang Zhou", "Jun Zhou", "Huazhu Fu", "Yih-Chung Tham", "Yong Liu"], "title": "EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow", "comment": "9 figures, 5 tables. submit/6621751", "summary": "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic\ndiagnosis, holding significant potential to address vision-threatening\ndiseases. However, their accuracy is constrained by hallucinations stemming\nfrom limited ophthalmic knowledge, insufficient visual localization and\nreasoning capabilities, and a scarcity of multimodal ophthalmic data, which\ncollectively impede precise lesion detection and disease diagnosis.\nFurthermore, existing medical benchmarks fail to effectively evaluate various\ntypes of hallucinations or provide actionable solutions to mitigate them. To\naddress the above challenges, we introduce EH-Benchmark, a novel ophthalmology\nbenchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'\nhallucinations based on specific tasks and error types into two primary\nclasses: Visual Understanding and Logical Composition, each comprising multiple\nsubclasses. Given that MLLMs predominantly rely on language-based reasoning\nrather than visual processing, we propose an agent-centric, three-phase\nframework, including the Knowledge-Level Retrieval stage, the Task-Level Case\nStudies stage, and the Result-Level Validation stage. Experimental results show\nthat our multi-agent framework significantly mitigates both types of\nhallucinations, enhancing accuracy, interpretability, and reliability. Our\nproject is available at https://github.com/ppxy1/EH-Benchmark.", "AI": {"tldr": "本研究聚焦于眼科领域的大型语言模型，介绍了一种新的基准——EH-Benchmark，用以评估和减轻这些模型中的误解。", "motivation": "现有的医疗基准无法有效评估大型语言模型中的各种误解，也无法提供有效的解决方案。为了解决这些问题，我们引入了EH-Benchmark，一个新型的、旨在评估眼科领域大型语言模型误解的基准。", "method": "我们提出了一种基于多智能体的三阶段框架，分别是知识水平检索阶段、任务水平案例研究阶段和结果水平验证阶段。由于大多数医疗大型语言模型主要依赖语言推理而非视觉处理，这种框架特别适用于减少模型中的误解。", "result": "实验结果表明，引入的多阶段框架能够显著提高大型语言模型在眼科疾病诊断中的准确性和可靠性。", "conclusion": "实验结果表明，这种多智能体框架显著减少了视觉理解和逻辑推理两种类型的误解，提高了模型的准确性、可解释性和可靠性。"}}
{"id": "2507.23185", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.23185", "abs": "https://arxiv.org/abs/2507.23185", "authors": ["Jongwook Si", "Sungyoung Kim"], "title": "Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network", "comment": "21 pages", "summary": "The problem of single-image rain streak removal goes beyond simple noise\nsuppression, requiring the simultaneous preservation of fine structural details\nand overall visual quality. In this study, we propose a novel image restoration\nnetwork that effectively constrains the restoration process by introducing a\nCorner Loss, which prevents the loss of object boundaries and detailed texture\ninformation during restoration. Furthermore, we propose a Residual\nConvolutional Block Attention Module (R-CBAM) Block into the encoder and\ndecoder to dynamically adjust the importance of features in both spatial and\nchannel dimensions, enabling the network to focus more effectively on regions\nheavily affected by rain streaks. Quantitative evaluations conducted on the\nRain100L and Rain100H datasets demonstrate that the proposed method\nsignificantly outperforms previous approaches, achieving a PSNR of 33.29 dB on\nRain100L and 26.16 dB on Rain100H.", "AI": {"tldr": "A new restoration network with Corner Loss and R-CBAM for rain streak removal, achieving better performance on specific datasets.", "motivation": "The motivation is to improve the quality of images affected by rain streaks while preserving fine structural details, which goes beyond simple noise suppression.", "method": "We propose a novel image restoration network for single-image rain streak removal that includes a Corner Loss to preserve object boundaries and a Residual Convolutional Block Attention Module (R-CBAM) to adjust feature importance dynamically.", "result": "Quantitative evaluations on Rain100L and Rain100H datasets show significant improvement over previous methods with PSNRs of 33.29 dB and 26.16 dB, respectively.", "conclusion": "The proposed method, utilizing a Corner Loss and R-CBAM in the network, demonstrates superior performance in removing rain streaks from images while preserving the fine details and overall quality."}}
{"id": "2507.22930", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.22930", "abs": "https://arxiv.org/abs/2507.22930", "authors": ["Shalini Jangra", "Suparna De", "Nishanth Sastry", "Saeed Fadaei"], "title": "Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection", "comment": "15 pages, 4 Figures, Accepted in \"The 17th International Conference\n  on Advances in Social Networks Analysis and Mining -ASONAM-2025\"", "summary": "Social platforms such as Reddit have a network of communities of shared\ninterests, with a prevalence of posts and comments from which one can infer\nusers' Personal Information Identifiers (PIIs). While such self-disclosures can\nlead to rewarding social interactions, they pose privacy risks and the threat\nof online harms. Research into the identification and retrieval of such risky\nself-disclosures of PIIs is hampered by the lack of open-source labeled\ndatasets. To foster reproducible research into PII-revealing text detection, we\ndevelop a novel methodology to create synthetic equivalents of PII-revealing\ndata that can be safely shared. Our contributions include creating a taxonomy\nof 19 PII-revealing categories for vulnerable populations and the creation and\nrelease of a synthetic PII-labeled multi-text span dataset generated from 3\ntext generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and\nzephyr-7b-beta, with sequential instruction prompting to resemble the original\nReddit posts. The utility of our methodology to generate this synthetic dataset\nis evaluated with three metrics: First, we require reproducibility equivalence,\ni.e., results from training a model on the synthetic data should be comparable\nto those obtained by training the same models on the original posts. Second, we\nrequire that the synthetic data be unlinkable to the original users, through\ncommon mechanisms such as Google Search. Third, we wish to ensure that the\nsynthetic data be indistinguishable from the original, i.e., trained humans\nshould not be able to tell them apart. We release our dataset and code at\nhttps://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster\nreproducible research into PII privacy risks in online social media.", "AI": {"tldr": "我们为在线社交媒体中的PII隐私风险研究提供了一个用于检测PII揭示文本的合成数据集。", "motivation": "研究中识别和检索这些风险自我披露的PII数据受制于缺乏开源标签数据集。为了促进关于PII揭示文本检测的可重复研究，我们开发了一种新方法来创建合成数据。", "method": "我们开发了一种新方法来创建可以安全共享的PII披露数据的合成等价物，包括建立一个包含19个PII披露类别的分类法，并使用Llama2-7B, Llama3-8B和zephyr-7b-beta三个大型语言模型生成合成PII标注多文本跨度数据集。", "result": "我们创建了一个可以用于评估模型性能的合成数据集，该数据集在可重复性、与原始用户无关以及与原始数据不可区分性这三个指标上得到了验证。", "conclusion": "我们发布了一个可以用于研究在线社交媒体中PII隐私风险的合成数据集和代码。"}}
{"id": "2507.23188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23188", "abs": "https://arxiv.org/abs/2507.23188", "authors": ["Shiyao Yu", "Zi-An Wang", "Kangning Yin", "Zheng Tian", "Mingyuan Zhang", "Weixin Si", "Shihao Zou"], "title": "Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space", "comment": "Accepted by IEEE TMM 2025", "summary": "Motion retrieval is crucial for motion acquisition, offering superior\nprecision, realism, controllability, and editability compared to motion\ngeneration. Existing approaches leverage contrastive learning to construct a\nunified embedding space for motion retrieval from text or visual modality.\nHowever, these methods lack a more intuitive and user-friendly interaction mode\nand often overlook the sequential representation of most modalities for\nimproved retrieval performance. To address these limitations, we propose a\nframework that aligns four modalities -- text, audio, video, and motion --\nwithin a fine-grained joint embedding space, incorporating audio for the first\ntime in motion retrieval to enhance user immersion and convenience. This\nfine-grained space is achieved through a sequence-level contrastive learning\napproach, which captures critical details across modalities for better\nalignment. To evaluate our framework, we augment existing text-motion datasets\nwith synthetic but diverse audio recordings, creating two multi-modal motion\nretrieval datasets. Experimental results demonstrate superior performance over\nstate-of-the-art methods across multiple sub-tasks, including an 10.16%\nimprovement in R@10 for text-to-motion retrieval and a 25.43% improvement in\nR@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, our\nresults show that our 4-modal framework significantly outperforms its 3-modal\ncounterpart, underscoring the potential of multi-modal motion retrieval for\nadvancing motion acquisition.", "AI": {"tldr": "This paper introduces a novel framework that leverages a sequence-level contrastive learning approach to align four modalities (text, audio, video, and motion) in a fine-grained embedding space, achieving superior performance in motion retrieval tasks compared to existing methods.", "motivation": "The motivation behind this research is to address the limitations of existing methods for motion retrieval which lack an intuitive user interaction mode and overlook the sequential nature of the modalities involved.", "method": "The paper proposes a framework that creates a fine-grained joint embedding space for text, audio, video, and motion modalities through a sequence-level contrastive learning approach. This is the first time audio is incorporated into motion retrieval frameworks, aiming to improve the user's immersive experience and make motion retrieval more intuitive.", "result": "The framework's effectiveness is validated on two newly created multi-modal motion retrieval datasets, demonstrating significant performance improvements over state-of-the-art methods in R@1 and R@10 retrieval metrics.", "conclusion": "The study concludes that the proposed multi-modal (text, audio, video, motion) framework, utilizing sequence-level contrastive learning, not only outperforms existing 3-modal approaches but also highlights the potential of multi-modal frameworks in advancing motion acquisition tasks."}}
{"id": "2507.22931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22931", "abs": "https://arxiv.org/abs/2507.22931", "authors": ["Shuyu Guo", "Zhaochun Ren"], "title": "Enhancing RAG Efficiency with Adaptive Context Compression", "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.", "AI": {"tldr": "提出了一种自适应上下文压缩框架(ACC-RAG)，该框架可以根据查询复杂度动态调整压缩率，优化大语言模型RAG的推理效率而不降低准确性，在保持或改善准确性的同时，推理速度提高了4倍以上。", "motivation": "虽然上下文压缩可以减轻RAG推理成本过高的问题，但是现有的方法使用固定的压缩率，这会导致简单查询过度压缩或复杂查询压缩不足。为了改善这一状况，提出了自适应上下文压缩的方法来优化RAG的效率。", "method": "提出了一种名为自适应上下文压缩RAG(ACC-RAG)的框架，该框架能够根据输入复杂度动态调整压缩率，从而在不牺牲准确性的情况下优化推断效率。ACC-RAG结合了用于多粒度嵌入的分层压缩器和上下文选择器，以保持最小的必要信息，类似于人类的略读过程。", "result": "在Wikipedia和五个问答数据集上评估ACC-RAG，结果显示其优于固定比率压缩方法，并提升了4倍以上的推理效率，同时保持或提升了准确性。", "conclusion": "评估结果表明，ACC-RAG优于固定比率的方法，并在与标准RAG对比时，推理速度至少快了四倍，同时保持或提高了准确性。"}}
{"id": "2507.23193", "categories": ["cs.CV", "I.4.6; I.2.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.23193", "abs": "https://arxiv.org/abs/2507.23193", "authors": ["Youngsun Jang", "Dongyoun Kim", "Chulwoo Pack", "Kwanghee Won"], "title": "A Novel Dataset for Flood Detection Robust to Seasonal Changes in Satellite Imagery", "comment": "8 pages, 2 figures. Presented at ACM RACS 2024 (Pompei, Italy, Nov\n  5-8, 2024)", "summary": "This study introduces a novel dataset for segmenting flooded areas in\nsatellite images. After reviewing 77 existing benchmarks utilizing satellite\nimagery, we identified a shortage of suitable datasets for this specific task.\nTo fill this gap, we collected satellite imagery of the 2019 Midwestern USA\nfloods from Planet Explorer by Planet Labs (Image \\c{opyright} 2024 Planet Labs\nPBC). The dataset consists of 10 satellite images per location, each containing\nboth flooded and non-flooded areas. We selected ten locations from each of the\nfive states: Iowa, Kansas, Montana, Nebraska, and South Dakota. The dataset\nensures uniform resolution and resizing during data processing. For evaluating\nsemantic segmentation performance, we tested state-of-the-art models in\ncomputer vision and remote sensing on our dataset. Additionally, we conducted\nan ablation study varying window sizes to capture temporal characteristics.\nOverall, the models demonstrated modest results, suggesting a requirement for\nfuture multimodal and temporal learning strategies. The dataset will be\npublicly available on\n<https://github.com/youngsunjang/SDSU_MidWest_Flood_2019>.", "AI": {"tldr": "该研究介绍了一个针对卫星图像中洪水区域分割的新数据集，旨在填补现有数据集在此任务中的空白。数据集基于2019年美国中西部洪水事件，共有10个卫星图像数据，涵盖了五个州的不同地点，用于评估语义分割性能。模型显示了良好的初步结果，但需要未来采用多模式和时间学习策略以提高效果。此数据集将在GitHub上公开。", "motivation": "研究的动机在于发现现有数据集在处理卫星图像中特定的洪水区域分割任务时存在不足，因此构建了一个新数据集以适应这一挑战。", "method": "通过Planet Explorer收集了2019年美国中西部5个州共10个地点的卫星图像，保证数据集中图像具有统一的分辨率和尺寸。使用计算机视觉和遥感领域的现有模型进行语义分割，同时进行了改变窗口大小用于捕捉时间特性的消融研究。", "result": "研究中的模型在新数据集上展示了中等程度的结果，表明在特定任务下现有模型仅能获得有限的性能。", "conclusion": "基于研究结果，表明未来的研究需要在多模式和时间学习策略上下功夫，以突破当前模型在卫星图像洪水区域分割上的局限性。数据集将在GitHub公开，供其他研究人员使用。"}}
{"id": "2507.22932", "categories": ["cs.CL", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2507.22932", "abs": "https://arxiv.org/abs/2507.22932", "authors": ["Baptiste Lefort", "Eric Benhamou", "Beatrice Guez", "Jean-Jacques Ohana", "Ethan Setrouk", "Alban Etienne"], "title": "FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification", "comment": "8 pages", "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.", "AI": {"tldr": "提出了一种结合LLMs和DRL的层级框架，用于处理混合金融数据，提升投资组合优化效果。该框架实现了优于市场基准的投资回报。", "motivation": "本文旨在通过混合金融新闻中的情感信号和传统的市场指标来改进投资组合优化方法。", "method": "本论文提出了一种新颖的层级框架，用于结合轻量级大型语言模型（LLMs）和深度强化学习（DRL）进行投资组合优化。这一三层架构利用基础RL代理来处理混合数据，元代理来汇总决策，超级代理根据市场数据和情感分析合并决策。", "result": "该框架在2018年至2024年的数据上进行了评估，其在2000年至2017年的训练后，实现了26%的年化回报率和1.2的夏普比率，优于等权重和S&P 500基准。", "conclusion": "关键贡献包括可扩展的跨模式集成，用于增强稳定性的层级RL结构，以及开源可复现性。"}}
{"id": "2507.23202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23202", "abs": "https://arxiv.org/abs/2507.23202", "authors": ["Chengwei Xia", "Fan Ma", "Ruijie Quan", "Kun Zhan", "Yi Yang"], "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks", "comment": null, "summary": "This paper addresses the challenge of generating adversarial image using a\ndiffusion model to deceive multimodal large language models (MLLMs) into\ngenerating the targeted responses, while avoiding significant distortion of the\nclean image. To address the above challenges, we propose an adversarial-guided\ndiffusion (AGD) approach for adversarial attack MLLMs. We introduce\nadversarial-guided noise to ensure attack efficacy. A key observation in our\ndesign is that, unlike most traditional adversarial attacks which embed\nhigh-frequency perturbations directly into the clean image, AGD injects target\nsemantics into the noise component of the reverse diffusion. Since the added\nnoise in a diffusion model spans the entire frequency spectrum, the adversarial\nsignal embedded within it also inherits this full-spectrum property.\nImportantly, during reverse diffusion, the adversarial image is formed as a\nlinear combination of the clean image and the noise. Thus, when applying\ndefenses such as a simple low-pass filtering, which act independently on each\ncomponent, the adversarial image within the noise component is less likely to\nbe suppressed, as it is not confined to the high-frequency band. This makes AGD\ninherently robust to variety defenses. Extensive experiments demonstrate that\nour AGD outperforms state-of-the-art methods in attack performance as well as\nin model robustness to some defenses.", "AI": {"tldr": "This paper introduces an adversarial-guided diffusion (AGD) approach that effectively generates adversarial images to attack MLLMs, ensuring less noticeable image distortion and robustness to defenses by incorporating adversarial signals into the noise component during reverse diffusion.", "motivation": "The motivation is to develop a method that can generate adversarial images to deceive MLLMs into generating targeted responses with less distortion to the original image and a higher resistance to common defenses against adversarial attacks.", "method": "The paper proposes an adversarial-guided diffusion (AGD) approach for attacking multimodal large language models (MLLMs). AGD introduces adversarial-guided noise that injects target semantics into the noise component of reverse diffusion, unlike traditional adversarial attacks that embed high-frequency perturbations directly into the clean image. This ensures attack effectiveness and robustness against defenses like low-pass filtering, as the adversarial signal spans the entire frequency spectrum.", "result": "Experiments show AGD outperforms state-of-the-art methods in terms of attack performance and robustness against specific defenses.", "conclusion": "AGD presents a novel way to conduct adversarial attacks on MLLMs, which not only maintains the quality of the original image but also demonstrates superior robustness to certain defensive measures compared to existing approaches."}}
{"id": "2507.22933", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22933", "abs": "https://arxiv.org/abs/2507.22933", "authors": ["Anthony C Davis", "Burhan Sadiq", "Tianmin Shu", "Chien-Ming Huang"], "title": "Augmented Vision-Language Models: A Systematic Review", "comment": null, "summary": "Recent advances in visual-language machine learning models have demonstrated\nexceptional ability to use natural language and understand visual scenes by\ntraining on large, unstructured datasets. However, this training paradigm\ncannot produce interpretable explanations for its outputs, requires retraining\nto integrate new information, is highly resource-intensive, and struggles with\ncertain forms of logical reasoning. One promising solution involves integrating\nneural networks with external symbolic information systems, forming neural\nsymbolic systems that can enhance reasoning and memory abilities. These neural\nsymbolic systems provide more interpretable explanations to their outputs and\nthe capacity to assimilate new information without extensive retraining.\nUtilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural\ncomponent, augmented by external systems, offers a pragmatic approach to\nrealizing the benefits of neural-symbolic integration. This systematic\nliterature review aims to categorize techniques through which visual-language\nunderstanding can be improved by interacting with external symbolic information\nsystems.", "AI": {"tldr": "本文提出了一种通过结合神经网络和外部符号信息系统来提升视觉语言理解的方法，并通过系统性文献回顾分类了相关技术。", "motivation": "当前的视觉语言机器学习模型虽然表现出色，但存在解释能力差、需要重新训练以整合新信息、资源消耗大以及逻辑推理能力不足的问题。为了解决这些问题，文章提出将神经网络与外部符号信息系统结合，形成神经符号系统，以提升推理和记忆能力。", "method": "本文通过系统性文献回顾，旨在分类能够通过与外部符号信息系统交互来提升视觉语言理解的技术。", "result": "暂无具体实验结果，因为这是一篇文献综述。", "conclusion": "文章确定了一个有希望的方法论，即将强大的预训练视觉语言模型作为核心神经组件，并通过外部系统增强，以实现神经符号集成的优势。"}}
{"id": "2507.23206", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23206", "abs": "https://arxiv.org/abs/2507.23206", "authors": ["Xiaoyu Ji", "Ali Shakouri", "Fengqing Zhu"], "title": "Confidence-aware agglomeration classification and segmentation of 2D microscopic food crystal images", "comment": null, "summary": "Food crystal agglomeration is a phenomenon occurs during crystallization\nwhich traps water between crystals and affects food product quality. Manual\nannotation of agglomeration in 2D microscopic images is particularly difficult\ndue to the transparency of water bonding and the limited perspective focusing\non a single slide of the imaged sample. To address this challenge, we first\npropose a supervised baseline model to generate segmentation pseudo-labels for\nthe coarsely labeled classification dataset. Next, an instance classification\nmodel that simultaneously performs pixel-wise segmentation is trained. Both\nmodels are used in the inference stage to combine their respective strengths in\nclassification and segmentation. To preserve crystal properties, a post\nprocessing module is designed and included to both steps. Our method improves\ntrue positive agglomeration classification accuracy and size distribution\npredictions compared to other existing methods. Given the variability in\nconfidence levels of manual annotations, our proposed method is evaluated under\ntwo confidence levels and successfully classifies potential agglomerated\ninstances.", "AI": {"tldr": "The paper proposes a combination of a supervised baseline model for generating segmentation pseudo-labels and an instance classification model for pixel-wise segmentation to improve the accuracy of agglomeration classification in 2D microscopic images of food crystals, dealing with the challenges posed by water transparency and limited perspective.", "motivation": "The research aims to address the difficulties in manually annotating agglomeration in 2D microscopic images of food crystals due to water transparency and the limited perspective of the images.", "method": "PaperAnalysis", "result": "The proposed method improves the true positive classification accuracy and size distribution predictions of agglomeration in comparison to existing methods, and it effectively classifies potential agglomerated instances under two confidence levels of manual annotations.", "conclusion": "The study concludes that the developed model, incorporating both classification and segmentation capabilities alongside a post-processing module, successfully enhances the precision and reliability of detecting food crystal agglomeration in microscopic images, providing a robust solution to manual annotation challenges in this field."}}
{"id": "2507.22934", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22934", "abs": "https://arxiv.org/abs/2507.22934", "authors": ["Jingwei Zhao", "Yuhua Wen", "Qifei Li", "Minchi Hu", "Yingying Zhou", "Jingyao Xue", "Junyang Wu", "Yingming Gao", "Zhengqi Wen", "Jianhua Tao", "Ya Li"], "title": "Deep Learning Approaches for Multimodal Intent Recognition: A Survey", "comment": "Submitted to ACM Computing Surveys", "summary": "Intent recognition aims to identify users' underlying intentions,\ntraditionally focusing on text in natural language processing. With growing\ndemands for natural human-computer interaction, the field has evolved through\ndeep learning and multimodal approaches, incorporating data from audio, vision,\nand physiological signals. Recently, the introduction of Transformer-based\nmodels has led to notable breakthroughs in this domain. This article surveys\ndeep learning methods for intent recognition, covering the shift from unimodal\nto multimodal techniques, relevant datasets, methodologies, applications, and\ncurrent challenges. It provides researchers with insights into the latest\ndevelopments in multimodal intent recognition (MIR) and directions for future\nresearch.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.23225", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23225", "abs": "https://arxiv.org/abs/2507.23225", "authors": ["Zicheng Lin", "Weichao Pan"], "title": "YOLO-ROC: A High-Precision and Ultra-Lightweight Model for Real-Time Road Damage Detection", "comment": null, "summary": "Road damage detection is a critical task for ensuring traffic safety and\nmaintaining infrastructure integrity. While deep learning-based detection\nmethods are now widely adopted, they still face two core challenges: first, the\ninadequate multi-scale feature extraction capabilities of existing networks for\ndiverse targets like cracks and potholes, leading to high miss rates for\nsmall-scale damage; and second, the substantial parameter counts and\ncomputational demands of mainstream models, which hinder their deployment for\nefficient, real-time detection in practical applications. To address these\nissues, this paper proposes a high-precision and lightweight model, YOLO - Road\nOrthogonal Compact (YOLO-ROC). We designed a Bidirectional Multi-scale Spatial\nPyramid Pooling Fast (BMS-SPPF) module to enhance multi-scale feature\nextraction and implemented a hierarchical channel compression strategy to\nreduce computational complexity. The BMS-SPPF module leverages a bidirectional\nspatial-channel attention mechanism to improve the detection of small targets.\nConcurrently, the channel compression strategy reduces the parameter count from\n3.01M to 0.89M and GFLOPs from 8.1 to 2.6. Experiments on the\nRDD2022_China_Drone dataset demonstrate that YOLO-ROC achieves a mAP50 of\n67.6%, surpassing the baseline YOLOv8n by 2.11%. Notably, the mAP50 for the\nsmall-target D40 category improved by 16.8%, and the final model size is only\n2.0 MB. Furthermore, the model exhibits excellent generalization performance on\nthe RDD2022_China_Motorbike dataset.", "AI": {"tldr": "本文提出了一种高精度轻量级模型YOLO-ROC，设计了BMS-SPPF模块以增强多尺度特征提取，并实施了层次通道压缩策略以降低计算复杂度，实验表明该模型在小尺度目标检测方面有显著提升。", "motivation": "为解决现有深度学习模型在道路损伤检测中多尺度特征提取不足和计算需求大的问题。", "method": "提出一种名为YOLO-ROC的模型，设计了BMS-SPPF模块并实现了层次通道压缩策略。", "result": "实验表明该模型在小目标类别D40的mAP50提升了16.8%，模型大小仅为2.0MB，同时在RDD2022_China_Motorbike数据集上表现良好。", "conclusion": "YOLO-ROC在道路损伤检测中表现出高精度和低计算需求的优越性。"}}
{"id": "2507.22935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22935", "abs": "https://arxiv.org/abs/2507.22935", "authors": ["Kathleen Mealey", "Jonathan A. Karr Jr.", "Priscila Saboia Moreira", "Paul R. Brenner", "Charles F. Vardeman II"], "title": "Trusted Knowledge Extraction for Operations and Maintenance Intelligence", "comment": null, "summary": "Deriving operational intelligence from organizational data repositories is a\nkey challenge due to the dichotomy of data confidentiality vs data integration\nobjectives, as well as the limitations of Natural Language Processing (NLP)\ntools relative to the specific knowledge structure of domains such as\noperations and maintenance. In this work, we discuss Knowledge Graph\nconstruction and break down the Knowledge Extraction process into its Named\nEntity Recognition, Coreference Resolution, Named Entity Linking, and Relation\nExtraction functional components. We then evaluate sixteen NLP tools in concert\nwith or in comparison to the rapidly advancing capabilities of Large Language\nModels (LLMs). We focus on the operational and maintenance intelligence use\ncase for trusted applications in the aircraft industry. A baseline dataset is\nderived from a rich public domain US Federal Aviation Administration dataset\nfocused on equipment failures or maintenance requirements. We assess the\nzero-shot performance of NLP and LLM tools that can be operated within a\ncontrolled, confidential environment (no data is sent to third parties). Based\non our observation of significant performance limitations, we discuss the\nchallenges related to trusted NLP and LLM tools as well as their Technical\nReadiness Level for wider use in mission-critical industries such as aviation.\nWe conclude with recommendations to enhance trust and provide our open-source\ncurated dataset to support further baseline testing and evaluation.", "AI": {"tldr": "本文评估了面向航空领域的NLP工具和LLM对设备故障或维护需求的知识提取性能，并讨论了其在关键任务行业应用中的技术成熟度以及面临的挑战。", "motivation": "由于数据保密与数据整合目标的二分法以及NLP工具相对于特定领域知识结构的功能局限性，从组织数据仓库中获取运营情报是一个关键挑战。", "method": "论文探讨了知识图谱构建，并将知识提取过程分解为命名实体识别、共指消解、实体链接和关系抽取等组件。作者评估了16种NLP工具与大型语言模型（LLMs）在零样本性能上的表现，并重点关注航空工业中运营和维护情报的可信应用。数据集来源于美国联邦航空管理局关于设备故障或维护需求的公开数据。", "result": "观察到NLP和LLM工具在特定专业领域有显著性能限制。", "conclusion": "论文最后提供了增强信任的建议，并开源了经过精心整理的数据集以支持进一步的基准测试和评估。"}}
{"id": "2507.23226", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23226", "abs": "https://arxiv.org/abs/2507.23226", "authors": ["Yanming Xiu"], "title": "Toward Safe, Trustworthy and Realistic Augmented Reality User Experience", "comment": "2 pages, 4 figures", "summary": "As augmented reality (AR) becomes increasingly integrated into everyday life,\nensuring the safety and trustworthiness of its virtual content is critical. Our\nresearch addresses the risks of task-detrimental AR content, particularly that\nwhich obstructs critical information or subtly manipulates user perception. We\ndeveloped two systems, ViDDAR and VIM-Sense, to detect such attacks using\nvision-language models (VLMs) and multimodal reasoning modules. Building on\nthis foundation, we propose three future directions: automated, perceptually\naligned quality assessment of virtual content; detection of multimodal attacks;\nand adaptation of VLMs for efficient and user-centered deployment on AR\ndevices. Overall, our work aims to establish a scalable, human-aligned\nframework for safeguarding AR experiences and seeks feedback on perceptual\nmodeling, multimodal AR content implementation, and lightweight model\nadaptation.", "AI": {"tldr": "研究开发了两个系统来检测有害AR内容，并提出了未来工作的三个方向。", "motivation": "研究动机在于解决AR内容可能造成的任务损害问题，特别是在遮挡关键信息或微妙操纵用户感知方面。", "method": "本研究开发了两个系统ViDDAR和VIM-Sense，利用视觉语言模型（VLMs）和多模态推理模块来检测有害的AR内容。", "result": "未给出具体的结果，但提出了未来工作的三个方向：自动感知对齐的质量评估、多模态攻击检测、以及在AR设备上实现高效的VLMs部署。", "conclusion": "研究旨在建立一个可扩展且符合人类感知的框架来保障AR体验，并寻求关于感知建模、多模态AR内容实现和轻量级模型适配的反馈。"}}
{"id": "2507.22936", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.22936", "abs": "https://arxiv.org/abs/2507.22936", "authors": ["Md Talha Mohsin"], "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis", "comment": "22 Pages, 6 Tables, 7 Figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide variety of Financial Natural Language Processing (FinNLP) tasks.\nHowever, systematic comparisons among widely used LLMs remain underexplored.\nGiven the rapid advancement and growing influence of LLMs in financial\nanalysis, this study conducts a thorough comparative evaluation of five leading\nLLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the\n'Magnificent Seven' technology companies. We create a set of domain-specific\nprompts and then use three methodologies to evaluate model performance: human\nannotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,\nJaccard), and model behavior diagnostics (prompt-level variance and\nacross-model similarity). The results show that GPT gives the most coherent,\nsemantically aligned, and contextually relevant answers; followed by Claude and\nPerplexity. Gemini and DeepSeek, on the other hand, have more variability and\nless agreement. Also, the similarity and stability of outputs change from\ncompany to company and over time, showing that they are sensitive to how\nprompts are written and what source material is used.", "AI": {"tldr": "本研究比较了五种主流大语言模型（GPT、Claude、Perplexity、Gemini 和 DeepSeek）在金融自然语言处理任务上的性能，发现GPT表现最佳，而Gemini和DeepSeek的输出更具变异性。", "motivation": "尽管大语言模型（LLMs）在金融自然语言处理（FinNLP）中展示了卓越的能力，但关于主流LLMs的系统性对比研究仍然较少。鉴于LLMs在金融分析中的快速发展和影响力，此研究旨在填补这一空白。", "method": "本研究对五种主流大语言模型（GPT、Claude、Perplexity、Gemini 和 DeepSeek）进行了全面的对比评估。研究采用了 'Magnificent Seven' 技术公司10-K文件，设计了一组领域特定的提示词，并使用三种方法来评估模型性能：人工标注、自动化词汇语义度量（ROUGE、余弦相似度、Jaccard），以及模型行为诊断（提示层级变异和跨模型相似度）。", "result": "评估结果显示，GPT提供的答案在连贯性、语义一致性和情境相关性方面表现最佳；接着是Claude和Perplexity。相比之下，Gemini和DeepSeek表现更不稳定，一致性较差。此外，输出的相似度和稳定性在不同的公司和时间段中有所不同，显示出它们对所使用的提示词和源材料更敏感。", "conclusion": "尽管GPT在连贯性、语义一致性和情境相关性表现最佳，但模型的表现会因为不同的提示词和源材料而有所不同，这显示出它们在处理金融自然语言处理任务时的具体能力。因此，在选择和使用大语言模型时，需要考虑其对不同公司和时间的敏感性。"}}
{"id": "2507.23237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23237", "abs": "https://arxiv.org/abs/2507.23237", "authors": ["Fan Lyu", "Linglan Zhao", "Chengyan Liu", "Yinying Mei", "Zhang Zhang", "Jian Zhang", "Fuyuan Hu", "Liang Wang"], "title": "Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised Few-Shot Class-Incremental Learning", "comment": "6 pages, 5 figures", "summary": "Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new\nconcepts from limited data while retaining knowledge of previous classes.\nRecently, many studies have started to leverage unlabeled samples to assist\nmodels in learning from few-shot samples, giving rise to the field of\nSemi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However,\nthese studies often assume that the source of unlabeled data is only confined\nto novel classes of the current session, which presents a narrow perspective\nand cannot align well with practical scenarios. To better reflect real-world\nscenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by\nincorporating both base and all the ever-seen novel classes in the unlabeled\nset. This change in the composition of unlabeled samples poses a new challenge\nfor existing methods, as they struggle to distinguish between unlabeled samples\nfrom base and novel classes. To address this issue, we propose an\nAmbiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC\ndynamically uses abundant base samples to correct biased feature distributions\nfor few-shot novel classes. Experiments on three benchmark datasets show that\nour method outperforms existing works, setting new state-of-the-art results.", "AI": {"tldr": "我们提出了GSemi-FSCIL框架和ALDC策略，纳入基础类别和所有先前见过的新类别，在未标记样本中进行动态特征校正，实验结果表明该方法优于现有方法。", "motivation": "之前的Semi-FSCIL研究仅假设未标记数据来自当前会话的新类别，这种观点过于狭隘且不符合实际情况。为了更好地反映现实场景，我们将Semi-FSCIL重新定义为Generalized Semi-FSCIL (GSemi-FSCIL)，并纳入了基础类别和所有之前见过的新类别。", "method": "我们提出了一个名为Ambiguity-guided Learnable Distribution Calibration (ALDC) 的策略，该策略动态使用大量的基础样本校正少量新类别样本的特征分布偏差。", "result": "在三个基准数据集上的实验表明，我们提出的方法优于现有工作，并取得了新的最优结果。", "conclusion": "我们通过在未标记集内引入基础类别和所有先前见过的新类别，构建了GSemi-FSCIL框架，并通过ALDC策略取得了显著性能提升，证明了其在实际应用中的潜力。"}}
{"id": "2507.22937", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22937", "abs": "https://arxiv.org/abs/2507.22937", "authors": ["Jinkun Zhao", "Yuanshuai Wang", "Xingjian Zhang", "Ruibo Chen", "Xingchuang Liao", "Junle Wang", "Lei Huang", "Kui Zhang", "Wenjun Wu"], "title": "CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering", "comment": null, "summary": "With the rapid evolution of artificial intelligence, AIOps has emerged as a\nprominent paradigm in DevOps. Lots of work has been proposed to improve the\nperformance of different AIOps phases. However, constrained by domain-specific\nknowledge, a single model can only handle the operation requirement of a\nspecific task,such as log parser,root cause analysis. Meanwhile, combining\nmultiple models can achieve more efficient results, which have been proved in\nboth previous ensemble learning and the recent LLM training domain. Inspired by\nthese works,to address the similar challenges in AIOPS, this paper first\nproposes a collaboration-of-expert framework(CoE-Ops) incorporating a\ngeneral-purpose large language model task classifier. A retrieval-augmented\ngeneration mechanism is introduced to improve the framework's capability in\nhandling both Question-Answering tasks with high-level(Code,build,Test,etc.)\nand low-level(fault analysis,anomaly detection,etc.). Finally, the proposed\nmethod is implemented in the AIOps domain, and extensive experiments are\nconducted on the DevOps-EVAL dataset. Experimental results demonstrate that\nCoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps\ntasks compared to existing CoE methods, delivers up to 8% accuracy enhancement\nover single AIOps models in DevOps problem resolution, and outperforms\nlarger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.", "AI": {"tldr": "本文提出了一种名为CoE-Ops的任务协作框架，该框架在AIOps领域内进行了实现，并在DevOps-EVAL数据集上进行了广泛实验。实验结果显示，CoE-Ops在处理高级AIOps任务时的路由准确性相较于现有方法提升了72%，在DevOps问题解决上的准确率也比单一模型提高了8%，并比大规模的混合专家模型(MoE)准确度高14%。", "motivation": "随着人工智能的快速发展，AIOps作为DevOps中的一个显著范式已经出现。尽管有许多工作专注于提高AIOps不同阶段的性能，但受限于领域的专业知识，单一模型只能处理特定的操作需求。本论文旨在通过结合多个模型提高效率，受到了集成学习和最近LLM训练领域的启发。", "method": "本论文提出了一种名为CoE-Ops的专家协作框架，该框架结合了通用大型语言模型的任务分类器。为了提高框架处理问答任务（包括高级任务如代码构建测试等和低级任务如故障分析异常检测等）的能力，引入了检索增强生成机制。", "result": "实验结果表明，与现有方法相比，CoE-Ops在高级AIOps任务的路由准确性上提高了72%，在DevOps问题解决上的准确率提高了8%，并且在准确度上比大规模混合专家模型(MoE)高出了14%。", "conclusion": "基于实验结果，CoE-Ops显著提升了AIOps领域中任务处理的效率和准确性，特别是在高级任务的路由选择和DevOps问题解决上表现出色。"}}
{"id": "2507.23242", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23242", "abs": "https://arxiv.org/abs/2507.23242", "authors": ["Sungguk Cha", "DongWook Kim", "Taeseung Hahn", "Mintae Kim", "Youngsub Han", "Byoung-Ki Jeon"], "title": "Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on effective query\nformulation to unlock external knowledge, yet optimizing queries for diverse,\nunstructured real-world documents remains a challenge. We introduce\n\\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query\nrewriting that eliminates the need for human-annotated datasets and extends\napplicability to both text-only and multi-modal databases. By synthesizing\nscenario-question pairs and leveraging Generalized Reward Policy Optimization\n(GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing\nretrieval performance across varied domains. Experiments on industrial in-house\ndata demonstrate significant improvements, with\n$\\text{RL-QR}_{\\text{multi-modal}}$ achieving an 11\\% relative gain in NDCG@3\nfor multi-modal RAG and $\\text{RL-QR}_{\\text{lexical}}$ yielding a 9\\% gain for\nlexical retrievers. However, challenges persist with semantic and hybrid\nretrievers, where rewriters failed to improve performance, likely due to\ntraining misalignments. Our findings highlight RL-QR's potential to\nrevolutionize query optimization for RAG systems, offering a scalable,\nannotation-free solution for real-world retrieval tasks, while identifying\navenues for further refinement in semantic retrieval contexts.", "AI": {"tldr": "提出RL-QR框架，用于检索增强生成系统中的查询重写，提升了多模态和文本检索性能，但对语义检索尚存挑战。", "motivation": "检索增强生成(RAG)系统依赖于有效的查询形成来利用外部知识。然而，针对多样化和非结构化的现实世界文档优化查询仍然是一个挑战。", "method": "本论文提出了一种名为RL-QR的强化学习框架，用于针对特定检索器的查询重写，该框架能够消除对人类注释数据集的需求，并扩展至纯文本和多模态数据库的应用。通过合成场景-问题对和利用广义奖励策略优化（GRPO），RL-QR对特定检索器进行了定制化的查询重写训练，增强了跨领域检索性能。", "result": "实验展示了在工业内部数据上的显著改善，其中RL-QR的多模态版本在多模态RAG中的NDCG@3指标相对提升了11%，而相对纯文本检索器而言提升了9%。然而，对于语义和混合检索器，重写器未能提升性能，可能是由于训练不匹配。", "conclusion": "本论文展示了RL-QR对RAG系统查询优化的巨大潜力，提供了一种可扩展且无需注释的数据解决方案用于实际检索任务，同时指出了在语义检索上下文中需要进一步改进的领域。"}}
{"id": "2507.22938", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22938", "abs": "https://arxiv.org/abs/2507.22938", "authors": ["Sumit Soman", "H. G. Ranjani", "Sujoy Roychowdhury", "Venkata Dharma Surya Narayana Sastry", "Akshat Jain", "Pranav Gangrade", "Ayaaz Khan"], "title": "A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents", "comment": "Accepted for publication at the KDD 2025 Workshop on Structured\n  Knowledge for Large Language Models", "summary": "Question-Answering (QA) from technical documents often involves questions\nwhose answers are present in figures, such as flowcharts or flow diagrams.\nText-based Retrieval Augmented Generation (RAG) systems may fail to answer such\nquestions. We leverage graph representations of flowcharts obtained from Visual\nlarge Language Models (VLMs) and incorporate them in a text-based RAG system to\nshow that this approach can enable image retrieval for QA in the telecom\ndomain. We present the end-to-end approach from processing technical documents,\nclassifying image types, building graph representations, and incorporating them\nwith the text embedding pipeline for efficient retrieval. We benchmark the same\non a QA dataset created based on proprietary telecom product information\ndocuments. Results show that the graph representations obtained using a\nfine-tuned VLM model have lower edit distance with respect to the ground truth,\nwhich illustrate the robustness of these representations for flowchart images.\nFurther, the approach for QA using these representations gives good retrieval\nperformance using text-based embedding models, including a telecom-domain\nadapted one. Our approach also alleviates the need for a VLM in inference,\nwhich is an important cost benefit for deployed QA systems.", "AI": {"tldr": "本文通过结合图形表示和基于文本的RAG系统，以提高技术文档中以流程图为答案的问答能力，实验显示该方法有效并具有成本效益。", "motivation": "文本基于的RAG系统在处理技术文档中涉及图表的答案时往往表现不佳，本文旨在通过引入流程图的图形表示来解决这一问题，特别是在电信领域中的问答任务。", "method": "本文提出了一种结合图形表示和基于文本的检索增强生成(RAG)系统的端到端方法，以提高从技术文档中进行问答的能力。具体来说，该方法包括处理技术文档、分类图像类型、构建图形表示，并将这些表示融入文本嵌入管道中，从而实现高效检索。", "result": "实验结果表明，使用微调的视觉大语言模型(VLM)获得的图形表示相对于地面真相的编辑距离较低，这说明了这些表示对于流程图图像的鲁棒性。此外，使用这些表示进行问答的检索性能良好，包括适应电信领域的文本嵌入模型。", "conclusion": "本文的方法可以有效解决基于文本的RAG系统在处理流程图或流程图类型图像相关的问答问题上的不足，并且在推理阶段无需使用VLM，这对部署的问答系统来说是一个重要的成本优势。"}}
{"id": "2507.23245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23245", "abs": "https://arxiv.org/abs/2507.23245", "authors": ["Lei Xie", "Jiahao Huang", "Jiawei Zhang", "Jianzhong He", "Yiang Pan", "Guoqiang Xie", "Mengjun Li", "Qingrun Zeng", "Mingchu Li", "Yuanjing Feng"], "title": "Automated Mapping the Pathways of Cranial Nerve II, III, V, and VII/VIII: A Multi-Parametric Multi-Stage Diffusion Tractography Atlas", "comment": null, "summary": "Cranial nerves (CNs) play a crucial role in various essential functions of\nthe human brain, and mapping their pathways from diffusion MRI (dMRI) provides\nvaluable preoperative insights into the spatial relationships between\nindividual CNs and key tissues. However, mapping a comprehensive and detailed\nCN atlas is challenging because of the unique anatomical structures of each CN\npair and the complexity of the skull base environment.In this work, we present\nwhat we believe to be the first study to develop a comprehensive diffusion\ntractography atlas for automated mapping of CN pathways in the human brain. The\nCN atlas is generated by fiber clustering by using the streamlines generated by\nmulti-parametric fiber tractography for each pair of CNs. Instead of disposable\nclustering, we explore a new strategy of multi-stage fiber clustering for\nmultiple analysis of approximately 1,000,000 streamlines generated from the 50\nsubjects from the Human Connectome Project (HCP). Quantitative and visual\nexperiments demonstrate that our CN atlas achieves high spatial correspondence\nwith expert manual annotations on multiple acquisition sites, including the HCP\ndataset, the Multi-shell Diffusion MRI (MDM) dataset and two clinical cases of\npituitary adenoma patients. The proposed CN atlas can automatically identify 8\nfiber bundles associated with 5 pairs of CNs, including the optic nerve CN II,\noculomotor nerve CN III, trigeminal nerve CN V and facial-vestibulocochlear\nnerve CN VII/VIII, and its robustness is demonstrated experimentally. This work\ncontributes to the field of diffusion imaging by facilitating more efficient\nand automated mapping the pathways of multiple pairs of CNs, thereby enhancing\nthe analysis and understanding of complex brain structures through\nvisualization of their spatial relationships with nearby anatomy.", "AI": {"tldr": "本文介绍了一种新的多阶段纤维聚类方法，实现了颅神经通路的自动化、全面映射，这对于改善手术前对颅神经与关键组织间空间关系的理解十分有益。实验表明，该方法生成的图谱与手动标注具有高度空间一致性。", "motivation": "由于颅神经独特的解剖结构和颅底环境的复杂性，创建一个全面且详细的颅神经图谱具有挑战性。因此，本研究旨在开发首个用于人类大脑中颅神经通路的自动化映射的全面扩散图谱。", "method": "本研究通过使用多参数纤维束成像技术生成每对颅神经的流线，并采用多阶段纤维聚类的新策略来生成一个全面的扩散图谱，以实现颅神经通路的自动化映射。这一策略应用于人类连接组计划（HCP）中的50名受试者生成的大约1,000,000个流线。", "result": "定量和视觉实验显示，该颅神经图谱在包括HCP、多壳扩散MRI（MDM）数据集以及两名垂体腺瘤患者的病例数据等多采集站点上，与专家手动注释具有高度的空间一致性。该图谱可以自动识别与5对颅神经相关的8个纤维束。", "conclusion": "此工作通过更高效和自动化的成对颅神经路径映射，促进了扩散成像领域的技术进步，从而通过可视化其与周围解剖结构的空间关系来增强对复杂大脑结构的分析和理解。"}}
{"id": "2507.22939", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22939", "abs": "https://arxiv.org/abs/2507.22939", "authors": ["Bastien Le Guellec", "Kokou Adambounou", "Lisa C Adams", "Thibault Agripnidis", "Sung Soo Ahn", "Radhia Ait Chalal", "Tugba Akinci D Antonoli", "Philippe Amouyel", "Henrik Andersson", "Raphael Bentegeac", "Claudio Benzoni", "Antonino Andrea Blandino", "Felix Busch", "Elif Can", "Riccardo Cau", "Armando Ugo Cavallo", "Christelle Chavihot", "Erwin Chiquete", "Renato Cuocolo", "Eugen Divjak", "Gordana Ivanac", "Barbara Dziadkowiec Macek", "Armel Elogne", "Salvatore Claudio Fanni", "Carlos Ferrarotti", "Claudia Fossataro", "Federica Fossataro", "Katarzyna Fulek", "Michal Fulek", "Pawel Gac", "Martyna Gachowska", "Ignacio Garcia Juarez", "Marco Gatti", "Natalia Gorelik", "Alexia Maria Goulianou", "Aghiles Hamroun", "Nicolas Herinirina", "Krzysztof Kraik", "Dominik Krupka", "Quentin Holay", "Felipe Kitamura", "Michail E Klontzas", "Anna Kompanowska", "Rafal Kompanowski", "Alexandre Lefevre", "Tristan Lemke", "Maximilian Lindholz", "Lukas Muller", "Piotr Macek", "Marcus Makowski", "Luigi Mannacio", "Aymen Meddeb", "Antonio Natale", "Beatrice Nguema Edzang", "Adriana Ojeda", "Yae Won Park", "Federica Piccione", "Andrea Ponsiglione", "Malgorzata Poreba", "Rafal Poreba", "Philipp Prucker", "Jean Pierre Pruvo", "Rosa Alba Pugliesi", "Feno Hasina Rabemanorintsoa", "Vasileios Rafailidis", "Katarzyna Resler", "Jan Rotkegel", "Luca Saba", "Ezann Siebert", "Arnaldo Stanzione", "Ali Fuat Tekin", "Liz Toapanta Yanchapaxi", "Matthaios Triantafyllou", "Ekaterini Tsaoulia", "Evangelia Vassalou", "Federica Vernuccio", "Johan Wasselius", "Weilang Wang", "Szymon Urban", "Adrian Wlodarczak", "Szymon Wlodarczak", "Andrzej Wysocki", "Lina Xu", "Tomasz Zatonski", "Shuhang Zhang", "Sebastian Ziegelmayer", "Gregory Kuchcinski", "Keno K Bressem"], "title": "PARROT: An Open Multilingual Radiology Reports Dataset", "comment": null, "summary": "Rationale and Objectives: To develop and validate PARROT (Polyglottal\nAnnotated Radiology Reports for Open Testing), a large, multicentric,\nopen-access dataset of fictional radiology reports spanning multiple languages\nfor testing natural language processing applications in radiology. Materials\nand Methods: From May to September 2024, radiologists were invited to\ncontribute fictional radiology reports following their standard reporting\npractices. Contributors provided at least 20 reports with associated metadata\nincluding anatomical region, imaging modality, clinical context, and for\nnon-English reports, English translations. All reports were assigned ICD-10\ncodes. A human vs. AI report differentiation study was conducted with 154\nparticipants (radiologists, healthcare professionals, and non-healthcare\nprofessionals) assessing whether reports were human-authored or AI-generated.\nResults: The dataset comprises 2,658 radiology reports from 76 authors across\n21 countries and 13 languages. Reports cover multiple imaging modalities (CT:\n36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical\nregions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)\nbeing most prevalent. In the differentiation study, participants achieved 53.9%\naccuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated\nreports, with radiologists performing significantly better (56.9%, 95% CI:\n53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the\nlargest open multilingual radiology report dataset, enabling development and\nvalidation of natural language processing applications across linguistic,\ngeographic, and clinical boundaries without privacy constraints.", "AI": {"tldr": "PARROT数据集由多语言的、虚构的放射学报告组成，可用于测试自然语言处理应用，数据集包含2,658份多样化的放射学报告，研究显示参与者能以53.9%的准确率区分人类与AI生成的报告，其中放射科医生表现最好。", "motivation": "开发和验证PARROT（多语言注释放射检查报告的开放测试数据集），用于测试放射学中的自然语言处理应用。这是一个大型的、多中心的、可公开访问的数据集，包含多语种的虚构放射学报告。", "method": "从2024年5月至9月，邀请放射科医生按照他们的标准报告惯例提供虚构的放射学报告。贡献者至少提供了20份包含元数据的报告，包括解剖区域、成像方式、临床背景等，对于非英语报告还需提供英语翻译。所有报告都被分配了ICD-10编码。进行了一个人类与AI报告区分研究，参与者包括154名放射科医生、医疗专业人员和非医疗专业人员，以确定报告是人类编写还是AI生成的。", "result": "该数据集包含来自21个国家76名作者的2,658份放射学报告，涵盖13种语言。涉及多种成像方式（CT: 36.1%，MRI: 22.8%，放射摄影: 19.0%，超声: 16.8%）和多个解剖区域，胸部、腹部、头部和骨盆是最主要的部分。在区分研究中，参与者以53.9%的准确率区分了人类和AI生成的报告（95% CI: 50.7%-57.1%），放射科医生表现更好（56.9%，95% CI: 53.3%-60.6%，p<0.05）。", "conclusion": "PARROT是最大的开放多语言放射学报告数据集，它支持开发和验证在语言、地理位置和临床界限上的自然语言处理应用，且无隐私约束。"}}
{"id": "2507.23251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23251", "abs": "https://arxiv.org/abs/2507.23251", "authors": ["Fereshteh Aghaee Meibodi", "Shadi Alijani", "Homayoun Najjaran"], "title": "A Deep Dive into Generic Object Tracking: A Survey", "comment": "55 pages, 29 figures, 9 tables", "summary": "Generic object tracking remains an important yet challenging task in computer\nvision due to complex spatio-temporal dynamics, especially in the presence of\nocclusions, similar distractors, and appearance variations. Over the past two\ndecades, a wide range of tracking paradigms, including Siamese-based trackers,\ndiscriminative trackers, and, more recently, prominent transformer-based\napproaches, have been introduced to address these challenges. While a few\nexisting survey papers in this field have either concentrated on a single\ncategory or widely covered multiple ones to capture progress, our paper\npresents a comprehensive review of all three categories, with particular\nemphasis on the rapidly evolving transformer-based methods. We analyze the core\ndesign principles, innovations, and limitations of each approach through both\nqualitative and quantitative comparisons. Our study introduces a novel\ncategorization and offers a unified visual and tabular comparison of\nrepresentative methods. Additionally, we organize existing trackers from\nmultiple perspectives and summarize the major evaluation benchmarks,\nhighlighting the fast-paced advancements in transformer-based tracking driven\nby their robust spatio-temporal modeling capabilities.", "AI": {"tldr": "本文综述了目标跟踪的三大类方法，并重点关注基于Transformer的方法，提供了一种新的技术分类框架以及定性和定量比较。", "motivation": "目标是提供一个全面的综述，尤其是针对快速发展的基于Transformer的方法，为理解现存的追踪技术及其优缺点提供参考。", "method": "Content分析涉及三个主要领域的目标跟踪技术综述：基于Siamese的追踪器、判别式追踪器、以及最近流行的基于Transformer的方法。文章着重讨论了这些技术的核心设计原则、创新点及局限性，并通过定性和定量的比较进行了分析。此外，研究提供了一个用于比较代表方法的新分类法，并从多个角度整理现有追踪器，概述了主要的评估基准。", "result": "通过对比分析不同方法的核心设计原则、创新点及局限性，研究为理解和评估现有目标追踪技术提供了新的见解和框架。", "conclusion": "基于Transformer的目标跟踪技术因其强大的时空建模能力正快速发展，该综述提供了全面的技术分类和比较，为未来研究提供了基础。"}}
{"id": "2507.22940", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22940", "abs": "https://arxiv.org/abs/2507.22940", "authors": ["Rui Jiao", "Yue Zhang", "Jinku Li"], "title": "Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes", "comment": null, "summary": "We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy\nfor Confidence Enhancement), a novel framework addressing a critical\nvulnerability in Large Language Models (LLMs): the prevalence of factual\ninaccuracies within intermediate reasoning steps despite correct final answers.\nThis phenomenon poses substantial risks in high-stakes domains including\nhealthcare, legal analysis, and scientific research, where erroneous yet\nconfidently presented reasoning can mislead users into dangerous decisions. Our\nframework integrates three core components: (1) a specialized fact-checking\nclassifier trained on counterfactually augmented data to detect subtle factual\ninconsistencies within reasoning chains; (2) a Group Relative Policy\nOptimization (GRPO) reinforcement learning approach that balances factuality,\ncoherence, and structural correctness through multi-dimensional rewards; and\n(3) a mechanistic interpretability module examining how factuality improvements\nmanifest in model activations during reasoning processes. Extensive evaluation\nacross ten state-of-the-art models reveals concerning patterns: even leading\nmodels like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of\nonly 81.93% and 82.57% respectively. RELIANCE significantly enhances factual\nrobustness (up to 49.90% improvement) while maintaining or improving\nperformance on challenging benchmarks including Math-500, AIME-2024, and GPQA.\nFurthermore, our activation-level analysis provides actionable insights into\nhow factual enhancements reshape reasoning trajectories within model\narchitectures, establishing foundations for future training methodologies that\nexplicitly target factual robustness through activation-guided optimization.", "AI": {"tldr": "RELICENSE 是一个新框架，旨在解决大型语言模型 (LLMs) 中的一个关键漏洞：尽管最终答案是正确的，中间推理步骤中的事实不准确。该框架包含三个核心组件：专门的事实检查分类器、多维奖励的 GRPO 强化学习方法和机制可解释性模块。通过实验评估，RELICENSE 显著提高了事实准确性，同时在挑战性基准测试中保持或提高了性能。", "motivation": "解决在高风险领域，如医疗保健、法律分析和科学研究中，即使最终答案正确，中间推理步骤中的事实不准确也可能误导用户的危险情况，对这些领域造成潜在风险。", "method": "包含三个部分：专门的事实检查分类器（检测推理链中的事实不一致性），GRPO 强化学习（通过多维奖励来平衡事实性、连贯性和结构正确性），以及机制可解释性模块（研究事实性改进如何在推理过程中反映在模型激活上）。", "result": "经过对十个最先进的模型的广泛评估，揭示了重要的模式：即使是领先的模型，如 Claude-3.7 和 GPT-o1，在推理事实准确性方面也仅为 81.93% 和 82.57%。相比之下，RELICENSE 显著提高了事实准确性（最高达 49.90% 的提升），并在数学、AIME 和 GPQA 等挑战性基准测试中保持或提高了性能。", "conclusion": "RELICENSE 不仅显著增强了事实准确性，还提供了有关如何通过激活指导优化来改进未来模型训练方法的洞察。"}}
{"id": "2507.23253", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23253", "abs": "https://arxiv.org/abs/2507.23253", "authors": ["Mingyang Yu", "Xiahui Guo", "Peng chen", "Zhenkai Li", "Yang Shu"], "title": "Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality", "comment": null, "summary": "Time Series forecasting is critical in diverse domains such as weather\nforecasting, financial investment, and traffic management. While traditional\nnumerical metrics like mean squared error (MSE) can quantify point-wise\naccuracy, they fail to evaluate the geometric structure of time series data,\nwhich is essential to understand temporal dynamics. To address this issue, we\npropose the time series Geometric Structure Index (TGSI), a novel evaluation\nmetric that transforms time series into images to leverage their inherent\ntwo-dimensional geometric representations. However, since the image\ntransformation process is non-differentiable, TGSI cannot be directly\nintegrated as a training loss. We further introduce the Shape-Aware Temporal\nLoss (SATL), a multi-component loss function operating in the time series\nmodality to bridge this gap and enhance structure modeling during training.\nSATL combines three components: a first-order difference loss that measures\nstructural consistency through the MSE between first-order differences, a\nfrequency domain loss that captures essential periodic patterns using the Fast\nFourier Transform while minimizing noise, and a perceptual feature loss that\nmeasures geometric structure difference in time-series by aligning temporal\nfeatures with geometric structure features through a pre-trained temporal\nfeature extractor and time-series image autoencoder. Experiments across\nmultiple datasets demonstrate that models trained with SATL achieve superior\nperformance in both MSE and the proposed TGSI metrics compared to baseline\nmethods, without additional computational cost during inference.", "AI": {"tldr": "本文提出了一种新的时间序列评估指标TGSI和多组成部分的损失函数SATL，以解决传统评估指标无法评估时间序列几何结构的问题，实验结果表明采用SATL训练的模型性能优于基线方法。", "motivation": "传统的诸如均方误差(MSE)这样的数值指标可以量化点精确度，但是它们无法评估时间序列数据的几何结构，该结构对于理解时间动态至关重要。", "method": "我们提出了一种新的评估指标时间序列几何结构指数(TGSI)，它将时间序列转换为图像以利用其固有的二维几何表示。为了解决图像转换过程中非可微的问题，我们还引入了Shape-Aware Temporal Loss (SATL)，它是一个多组成部分的损失函数，包括一阶差分损失、频域损失和感知特征损失。", "result": "实验结果表明，采用SATL训练的模型在多个数据集上，在MSE和提出的TGSI指标上都优于基线方法，并且在推理时没有额外的计算成本。", "conclusion": "通过引入SATL，我们能够在训练过程中增强时间序列的结构建模，并通过实验证明了SATL的有效性。"}}
{"id": "2507.22941", "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.22941", "abs": "https://arxiv.org/abs/2507.22941", "authors": ["Paul Minchella", "Loïc Verlingue", "Stéphane Chrétien", "Rémi Vaucher", "Guillaume Metzler"], "title": "SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology", "comment": "12 pages, 2 figures, accepted for ECML PKDD 2025", "summary": "Electronic medical reports (EHR) contain a vast amount of information that\ncan be leveraged for machine learning applications in healthcare. However,\nexisting survival analysis methods often struggle to effectively handle the\ncomplexity of textual data, particularly in its sequential form. Here, we\npropose SigBERT, an innovative temporal survival analysis framework designed to\nefficiently process a large number of clinical reports per patient. SigBERT\nprocesses timestamped medical reports by extracting and averaging word\nembeddings into sentence embeddings. To capture temporal dynamics from the time\nseries of sentence embedding coordinates, we apply signature extraction from\nrough path theory to derive geometric features for each patient, which\nsignificantly enhance survival model performance by capturing complex temporal\ndynamics. These features are then integrated into a LASSO-penalized Cox model\nto estimate patient-specific risk scores. The model was trained and evaluated\non a real-world oncology dataset from the L\\'eon B\\'erard Center corpus, with a\nC-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT\nintegrates sequential medical data to enhance risk estimation, advancing\nnarrative-based survival analysis.", "AI": {"tldr": "SigBERT is introduced as a new survival analysis framework that processes clinical reports to estimate patient-specific risk, leveraging advanced embedding and extraction techniques to capture temporal dynamics effectively.", "motivation": "The motivation behind SigBERT is to address the challenge of handling the complexity of textual data in electronic medical reports, especially its sequential nature, for the purpose of improving survival analysis in the healthcare domain.", "method": "SigBERT is a novel temporal survival analysis framework designed to handle a large volume of clinical reports per patient. It processes timestamped medical reports by converting them into sentence embeddings and then using signature extraction from rough path theory to capture temporal dynamics, resulting in geometric features that are used with a LASSO-penalized Cox model for estimating patient-specific risk scores.", "result": "The model was evaluated on a real-world oncology dataset from the Léon Bérard Center, achieving a C-index score of 0.75 (with a standard deviation of 0.014) on the independent test set, indicating a significant improvement in survival model performance.", "conclusion": "SigBERT integrates sequential medical data through a series of transformation and reduction techniques to enhance risk estimation, thereby advancing the field of narrative-based survival analysis."}}
{"id": "2507.23263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23263", "abs": "https://arxiv.org/abs/2507.23263", "authors": ["Haoxian Ruan", "Zhihua Xu", "Zhijing Yang", "Guang Ma", "Jieming Xie", "Changxiang Fan", "Tianshui Chen"], "title": "Learning Semantic-Aware Threshold for Multi-Label Image Recognition with Partial Labels", "comment": "15 pages, 13 figures, publish to ESWA (Expert Systems With\n  Applications)", "summary": "Multi-label image recognition with partial labels (MLR-PL) is designed to\ntrain models using a mix of known and unknown labels. Traditional methods rely\non semantic or feature correlations to create pseudo-labels for unidentified\nlabels using pre-set thresholds. This approach often overlooks the varying\nscore distributions across categories, resulting in inaccurate and incomplete\npseudo-labels, thereby affecting performance. In our study, we introduce the\nSemantic-Aware Threshold Learning (SATL) algorithm. This innovative approach\ncalculates the score distribution for both positive and negative samples within\neach category and determines category-specific thresholds based on these\ndistributions. These distributions and thresholds are dynamically updated\nthroughout the learning process. Additionally, we implement a differential\nranking loss to establish a significant gap between the score distributions of\npositive and negative samples, enhancing the discrimination of the thresholds.\nComprehensive experiments and analysis on large-scale multi-label datasets,\nsuch as Microsoft COCO and VG-200, demonstrate that our method significantly\nimproves performance in scenarios with limited labels.", "AI": {"tldr": "针对多标签图像识别中使用部分标签训练模型的问题，本文提出了一种新的算法SATL，该算法能够更准确地生成伪标签并提高模型性能。", "motivation": "传统的多标签图像识别方法依赖于语义或特征相关性为未知标签生成伪标签，这往往忽视了各个类别得分分布的差异，导致伪标签不准确和不完整，从而影响了性能。", "method": "引入了名为Semantic-Aware Threshold Learning (SATL) 的算法。该方法通过计算每个类别中正样本和负样本的得分分布来确定类特定阈值，并在整个学习过程中动态更新这些分布和阈值。此外，通过实施差异排名损失来显著拉开正样本和负样本得分分布的差距，提高阈值的判别性。", "result": "在大规模多标签数据集上进行了详尽的实验和分析，如Microsoft COCO和VG-200，结果展示了我们的方法在有限标签场景下显著提高了性能。", "conclusion": "实验证明，我们的方法在具有有限标签的场景中能够显著提升多标签图像识别的性能。"}}
{"id": "2507.22943", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.22943", "abs": "https://arxiv.org/abs/2507.22943", "authors": ["Shirley V Wang", "Georg Hahn", "Sushama Kattinakere Sreedhara", "Mufaddal Mahesri", "Haritha S. Pillai", "Rajendra Aldis", "Joyce Lii", "Sarah K. Dutcher", "Rhoda Eniafe", "Jamal T. Jones", "Keewan Kim", "Jiwei He", "Hana Lee", "Sengwee Toh", "Rishi J Desai", "Jie Yang"], "title": "A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies", "comment": null, "summary": "Background: One of the ways to enhance analyses conducted with large claims\ndatabases is by validating the measurement characteristics of code-based\nalgorithms used to identify health outcomes or other key study parameters of\ninterest. These metrics can be used in quantitative bias analyses to assess the\nrobustness of results for an inferential study given potential bias from\noutcome misclassification. However, extensive time and resource allocation are\ntypically re-quired to create reference-standard labels through manual chart\nreview of free-text notes from linked electronic health records. Methods: We\ndescribe an expedited process that introduces efficiency in a validation study\nus-ing two distinct mechanisms: 1) use of natural language processing (NLP) to\nreduce time spent by human reviewers to review each chart, and 2) a multi-wave\nadaptive sampling approach with pre-defined criteria to stop the validation\nstudy once performance characteristics are identified with sufficient\nprecision. We illustrate this process in a case study that validates the\nperformance of a claims-based outcome algorithm for intentional self-harm in\npatients with obesity. Results: We empirically demonstrate that the\nNLP-assisted annotation process reduced the time spent on review per chart by\n40% and use of the pre-defined stopping rule with multi-wave samples would have\nprevented review of 77% of patient charts with limited compromise to precision\nin derived measurement characteristics. Conclusion: This approach could\nfacilitate more routine validation of code-based algorithms used to define key\nstudy parameters, ultimately enhancing understanding of the reliability of\nfind-ings derived from database studies.", "AI": {"tldr": "The paper presents an expedited validation process for claims-based algorithms using NLP to decrease manual review time and adaptive sampling to optimize the number of reviewed charts, which was tested with an algorithm for identifying intentional self-harm in obese patients.", "motivation": "The motivation is to improve the efficiency of validating code-based algorithms used in large claims databases, which can enhance the reliability of findings from database studies without extensive time and resource allocation.", "method": "Content describes a method that uses natural language processing (NLP) to expedite the validation of claims-based outcome algorithms and a multi-wave adaptive sampling approach for identifying the performance characteristics of these algorithms with sufficient precision, focusing on reducing the manual review time required for the reference-standard label creation process.", "result": "The study results showed that the NLP-assisted annotation process reduced the review time per chart by 40%, and using the pre-defined stopping rule with multi-wave samples would have prevented 77% of patient charts from being reviewed, while still maintaining the precision of the derived measurement characteristics.", "conclusion": "The conclusion is that this NLP-assisted and multi-wave adaptive sampling method can make the validation of key study parameters more routine, ultimately aiding in the better understanding of the reliability of findings from database studies."}}
{"id": "2507.23268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23268", "abs": "https://arxiv.org/abs/2507.23268", "authors": ["Shuai Wang", "Ziteng Gao", "Chenhui Zhu", "Weilin Huang", "Limin Wang"], "title": "PixNerd: Pixel Neural Field Diffusion", "comment": "a single-scale, single-stage, efficient, end-to-end pixel space\n  diffusion model", "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet $256\\times256$ and 2.84 FID\non ImageNet $512\\times512$ without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.", "AI": {"tldr": "本文提出了PixelNerd，通过神经场在像素空间上进行逐块解码，解决了现有扩散变压器的错误和伪影问题，取得了优异的图像合成性能。", "motivation": "本文旨在解决依赖预训练变分自编码器的扩散变压器所引入的累积误差和解码伪影问题。", "method": "本文提出了名为PixelNerd的单尺度、单阶段、高效且端到端的解决方案，通过神经场建模逐块解码，以解决现有扩散变压器依赖预训练变分自编码器引入的累积误差和解码伪影等问题。", "result": "在ImageNet 256x256数据集上达到了2.15的FID，在ImageNet 512x512数据集上达到了2.84的FID，无需任何复杂的级联流水线或变分自编码器。在文生图应用中，PixNerd-XXL/16在GenEval基准测试中达到了0.73的总体评分，在DPG基准测试中达到了80.9的总体评分。", "conclusion": "实验结果表明，PixelNerd能够在无复杂级联流水线或变分自编码器的情况下，实现高质量的图像生成，并且在文生图应用中也展现出了强大的竞争力。"}}
{"id": "2507.22944", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.22944", "abs": "https://arxiv.org/abs/2507.22944", "authors": ["Naomi Omeonga wa Kayembe"], "title": "Opacity as Authority: Arbitrariness and the Preclusion of Contestation", "comment": null, "summary": "This article redefines arbitrariness not as a normative flaw or a symptom of\ndomination, but as a foundational functional mechanism structuring human\nsystems and interactions. Diverging from critical traditions that conflate\narbitrariness with injustice, it posits arbitrariness as a semiotic trait: a\nproperty enabling systems - linguistic, legal, or social - to operate\neffectively while withholding their internal rationale. Building on Ferdinand\nde Saussure's concept of l'arbitraire du signe, the analysis extends this\nprinciple beyond language to demonstrate its cross-domain applicability,\nparticularly in law and social dynamics. The paper introduces the \"Motivation\n-> Constatability -> Contestability\" chain, arguing that motivation functions\nas a crucial interface rendering an act's logic vulnerable to intersubjective\ncontestation. When this chain is broken through mechanisms like\n\"immotivization\" or \"Conflict Lateralization\" (exemplified by \"the blur of the\nwolf drowned in the fish\"), acts produce binding effects without exposing their\nrationale, thus precluding justiciability. This structural opacity, while\nappearing illogical, is a deliberate design protecting authority from\naccountability. Drawing on Shannon's entropy model, the paper formalizes\narbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern\ntheory of arbitrariness as a neutral operator central to control as well as\ncare, an overlooked dimension of interpersonal relations. While primarily\ndeveloped through human social systems, this framework also illuminates a new\npathway for analyzing explainability in advanced artificial intelligence\nsystems.", "AI": {"tldr": "该文重新定义了随意性，认为它是人类系统和互动中的功能性机制，而非规范性的缺陷或支配的标志。通过引入一系列链式结构分析随意性，并提出其理论模型，说明随意性在法律和社会动态中的普遍适用性。", "motivation": "文章试图纠正批评传统对随意性的理解误区，即随意性并不等同于不公，而是作为一种语义特征，使系统能够有效地运作，同时保留其内部理据的私密性。", "method": "基于索绪尔的语言符号随意性理论，文章提出了“动机->可验证性->可争议性”的链条机制来分析随意性的操作过程，并结合香农的信息熵模型定义了随意性的计算模型 A = H(L|M)。", "result": "通过分析发现，随意性原理不是无理性的表现，而是设计上的有意为之，用以保护权威免受问责。同时，这一理论框架也为分析人工智能系统的解释性提供了新途径。", "conclusion": "随意性作为中立的控制与关照的机制，在人际关系中是一个被忽视的维度。文章提出一套理论模型来解释这一机制在不同领域的应用方式。"}}
{"id": "2507.23272", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23272", "abs": "https://arxiv.org/abs/2507.23272", "authors": ["Solha Kang", "Eugene Kim", "Joris Vankerschaver", "Utku Ozbulak"], "title": "Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2", "comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI), 2nd Deep\n  Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in\n  Breast Care (DeepBreath), 2025", "summary": "Breast MRI provides high-resolution volumetric imaging critical for tumor\nassessment and treatment planning, yet manual interpretation of 3D scans\nremains labor-intensive and subjective. While AI-powered tools hold promise for\naccelerating medical image analysis, adoption of commercial medical AI products\nremains limited in low- and middle-income countries due to high license costs,\nproprietary software, and infrastructure demands. In this work, we investigate\nwhether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,\nminimal-input 3D tumor segmentation in breast MRI. Using a single bounding box\nannotation on one slice, we propagate segmentation predictions across the 3D\nvolume using three different slice-wise tracking strategies: top-to-bottom,\nbottom-to-top, and center-outward. We evaluate these strategies across a large\ncohort of patients and find that center-outward propagation yields the most\nconsistent and accurate segmentations. Despite being a zero-shot model not\ntrained for volumetric medical data, SAM2 achieves strong segmentation\nperformance under minimal supervision. We further analyze how segmentation\nperformance relates to tumor size, location, and shape, identifying key failure\nmodes. Our results suggest that general-purpose foundation models such as SAM2\ncan support 3D medical image analysis with minimal supervision, offering an\naccessible and affordable alternative for resource-constrained settings.", "AI": {"tldr": "本研究通过切片标注传播的方式在SAM2模型上实现了低成本的乳腺MRI肿瘤分割任务，尤其在中心向外的策略下获得了较好的分割效果，展示了通用模型在资源受限条件下应用的可能性。", "motivation": "研究动机在于，随着AI技术在加速医学影像分析中的应用，商业医学AI产品的高昂授权成本、专有软件和基础设施需求限制了其在中低收入国家的采用。", "method": "本研究探讨了Segment Anything Model 2 (SAM2) 是否可以被用于低成本、低输入的乳腺MRI三维肿瘤分割。通过在单一切片上使用单一边界框标注，将分割预测传播至整个3D体积，采用三种不同的一维追踪策略：从上到下、从下到上以及从中心向外。", "result": "研究结果表明，从中心向外的传播策略在患者大样本量上产生了最一致且准确的分割。尽管SAM2作为一个零样本模型并未针对体积医学数据进行训练，但在极低的监督下仍能实现强大的分割性能。", "conclusion": "研究结论强调，诸如SAM2这样的通用基础模型可以在最小监督下支持三维医学影像分析，为资源匮乏的环境提供了可访问和经济的替代方案。"}}
{"id": "2507.22968", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22968", "abs": "https://arxiv.org/abs/2507.22968", "authors": ["Chengqian Ma", "Wei Tao", "Yiwen Guo"], "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations", "comment": null, "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges.", "AI": {"tldr": "本文讨论了Spoken Dialogue Models（SDM）在理解和模拟人类对话方面的发展，并提出了一套基准数据集及评估方法以全面探索SDMs的实际效能。", "motivation": "尽管SDMs因其能够直接生成语音响应而在近期受到了越来越大的重视，但在对其理解和模拟人类对话的实际效果的研究上仍存在差距，尤其是与得到了广泛基准测试的基于文本的大语言模型相比。这些差距主要是由于语音对话特有的复杂性，包括语义和语音学方面的挑战，以及上下文依赖性。", "method": "本文提出了一套基准数据集，包含1,079个英语和中文实例，以及一种基于大语言模型的评估方法，旨在全面探索SDMs在解决实际挑战方面的表现。", "result": "通过引入新的基准数据集和评估方法，本文有望照亮SDM发展的当前状态，并解决语音对话模型面临的挑战。", "conclusion": "本文通过构建一个新的基准数据集和配套的基于大语言模型的评估方法，为全面探索SDMs在处理实际语音对话挑战方面的能力提供了新的视角。"}}
{"id": "2507.23277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23277", "abs": "https://arxiv.org/abs/2507.23277", "authors": ["Gyeongjin Kang", "Seungtae Nam", "Xiangyu Sun", "Sameh Khamis", "Abdelrahman Mohamed", "Eunbyung Park"], "title": "iLRM: An Iterative Large 3D Reconstruction Model", "comment": "Project page: https://gynjn.github.io/iLRM/", "summary": "Feed-forward 3D modeling has emerged as a promising approach for rapid and\nhigh-quality 3D reconstruction. In particular, directly generating explicit 3D\nrepresentations, such as 3D Gaussian splatting, has attracted significant\nattention due to its fast and high-quality rendering, as well as numerous\napplications. However, many state-of-the-art methods, primarily based on\ntransformer architectures, suffer from severe scalability issues because they\nrely on full attention across image tokens from multiple input views, resulting\nin prohibitive computational costs as the number of views or image resolution\nincreases. Toward a scalable and efficient feed-forward 3D reconstruction, we\nintroduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D\nGaussian representations through an iterative refinement mechanism, guided by\nthree core principles: (1) decoupling the scene representation from input-view\nimages to enable compact 3D representations; (2) decomposing fully-attentional\nmulti-view interactions into a two-stage attention scheme to reduce\ncomputational costs; and (3) injecting high-resolution information at every\nlayer to achieve high-fidelity reconstruction. Experimental results on widely\nused datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms\nexisting methods in both reconstruction quality and speed. Notably, iLRM\nexhibits superior scalability, delivering significantly higher reconstruction\nquality under comparable computational cost by efficiently leveraging a larger\nnumber of input views.", "AI": {"tldr": "提出了一个迭代的大型3D重建模型(iLRM)，解决了基于变压器架构的方法在处理大量视图输入时计算成本高的问题，通过分解全注意力机制，实现高效率和高质量的3D重建。", "motivation": "当前的3D重建方法，尤其是基于变压器的方法，受限于全注意力机制导致在处理多视图和高分辨率图像时计算成本过高。", "method": "iLRM主要通过三个核心原则来改进：(1)将场景表示从输入视图图像中解耦，使3D表示更加紧凑；(2)将全注意力多视图互动分解成两阶段注意力模式，减少计算成本；(3)在每一层注入高分辨率信息，实现高保真度重建。", "result": "在RE10K和DL3DV等常用数据集上的实验结果显示，iLRM在重建质量和速度上优于现有方法，特别是显示出更优的可扩展性，在与现有方法计算成本相当的情况下能有效利用更多输入视图达到更高的重建质量。", "conclusion": "iLRM模型通过引入迭代细化机制和高分辨率信息的逐层注入，解决了现有3D重建方法的计算成本和可扩展性问题，表明其是一个高效的解决方案。"}}
{"id": "2507.23063", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.23063", "abs": "https://arxiv.org/abs/2507.23063", "authors": ["Valeria de Paiva", "Qiyue Gao", "Hai Hu", "Pavel Kovalev", "Yikang Liu", "Lawrence S. Moss", "Zhiheng Qian"], "title": "Math Natural Language Inference: this should be easy!", "comment": "9 pages plus appendices", "summary": "We ask whether contemporary LLMs are able to perform natural language\ninference (NLI) tasks on mathematical texts. We call this the Math NLI problem.\nWe construct a corpus of Math NLI pairs whose premises are from extant\nmathematical text and whose hypotheses and gold labels were provided by people\nwith experience in both research-level mathematics and also in the NLI field.\nWe also investigate the quality of corpora using the same premises but whose\nhypotheses are provided by LLMs themselves. We not only investigate the\nperformance but also the inter-group consistency of the diverse group of LLMs.\nWe have both positive and negative findings. Among our positive findings: in\nsome settings, using a majority vote of LLMs is approximately equivalent to\nusing human-labeled data in the Math NLI area. On the negative side: LLMs still\nstruggle with mathematical language. They occasionally fail at even basic\ninferences. Current models are not as prone to hypothesis-only \"inference\" in\nour data the way the previous generation had been. In addition to our findings,\nwe also provide our corpora as data to support future work on Math NLI.", "AI": {"tldr": "论文探讨了当前LLMs在数学自然语言推理（Math NLI）任务上的表现，结果显示混合效果，既有积极发现也有挑战。同时也为该领域的未来研究提供了数据支持。", "motivation": "探究当前的大规模语言模型（LLMs）是否能执行数学文本上的自然语言推理（NLI）任务。这项任务称为Math NLI问题。", "method": "构建了一个数学自然语言推理（Math NLI）的数据集，数据集的前设来自于现存的数学文本，而假设和黄金标签是由既具有数学研究水平又熟悉NLI领域的人提供的。同时，也调查了由LLM生成假设的语料库的质量。", "result": "研究结果是混合的。正面发现包括：在某些设置下，使用多个LLM的多数投票结果与使用人类标注的数据在Math NLI领域中的效果相当。负面发现包括：LLM在处理数学语言方面仍存在困难，偶尔会在基本推理上出错。相对于上一代模型，当前模型更少依赖于假设的独立“推理”。", "conclusion": "该研究不仅提供了正面和负面的发现，还提供了支持未来Math NLI研究的语料库数据。"}}
{"id": "2507.23278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23278", "abs": "https://arxiv.org/abs/2507.23278", "authors": ["Hao Tang", "Chenwei Xie", "Xiaoyi Bao", "Tingyu Weng", "Pandeng Li", "Yun Zheng", "Liwei Wang"], "title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing", "comment": null, "summary": "In this paper, we propose UniLIP, which extends CLIP to reconstruction,\ngeneration and editing, thereby building a unified tokenizer upon its\nexceptional comprehension capabilities. Previous CLIP-based unified methods\noften require additional diffusion decoders or quantization to support\nreconstruction and generation tasks, leading to inconsistent reconstruction or\ndegradation of original comprehension performance.In contrast, we introduce a\ntwo-stage training scheme and a self-distillation strategy that progressively\nintegrates reconstruction capabilities into CLIP, allowing it to maintain\noriginal comprehension performance while achieving effective image\nreconstruction. Furthermore, we propose a dual-condition architecture to\nconnect the MLLM and diffusion transformer, using both learnable queries and\nthe last layer multimodal hidden states as joint conditions. This method not\nonly enables the utilization of the MLLM's strong reasoning capabilities in\ngeneration tasks, but also maximizes the exploitation of the rich information\nin UniLIP features during editing tasks. In text-to-image generation tasks,\nUniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark\nrespectively, surpassing all previous unified models of similar scale. In image\nediting, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark,\nsurpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP\neffectively expand the application scope of CLIP, enabling continuous CLIP\nfeatures to not only serve as the optimal choice for understanding tasks but\nalso achieve highly competitive performance in generation and editing tasks.", "AI": {"tldr": "本文提出了一种名为UniLIP的新模型，它基于CLIP，能够进行图像理解、重建、生成和编辑，其在各方面均表现出色，超过了许多现有的模型。", "motivation": "之前的基于CLIP的统一方法通常需要额外的扩散解码器或量化技术来支持重建和生成任务，这导致了重建不一致或原始理解性能下降。因此，我们提出UniLIP，旨在改进这些方面，即保持原始理解性能的同时，实现有效的图像重建。", "method": "我们提出了一种名为UniLIP的方法，它扩展了CLIP的功能，使其不仅能够理解和识别图像，还能进行图像重建、生成和编辑。UniLIP通过两阶段的训练方案和自蒸馏策略来实现这一目标，同时引入了双条件架构来更好地整合多模态大规模语言模型（MLLM）和扩散变换器（diffusion transformer）的功能，从而提高了生成任务中的推理能力和编辑任务中的特征利用效率。", "result": "在文本到图像生成任务中，UniLIP在GenEval和WISE基准上分别获得了0.87和0.53的分数，超过了同类规模的所有先前统一模型。在图像编辑任务上，UniLIP在ImgEdit Benchmark上的得分达到3.62，超越了如BAGEL和UniWorld-V1等近期的最先进模型。", "conclusion": "UniLIP大大扩展了CLIP的应用领域，使得连续的CLIP特征不仅是理解任务的最佳选择，还能在生成和编辑任务中达到高度竞争的表现。"}}
{"id": "2507.23082", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23082", "abs": "https://arxiv.org/abs/2507.23082", "authors": ["Diego Garat", "Guillermo Moncecchi", "Dina Wonsever"], "title": "Exploring In-Context Learning for Frame-Semantic Parsing", "comment": null, "summary": "Frame Semantic Parsing (FSP) entails identifying predicates and labeling\ntheir arguments according to Frame Semantics. This paper investigates the use\nof In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP\nwithout model fine-tuning. We propose a method that automatically generates\ntask-specific prompts for the Frame Identification (FI) and Frame Semantic Role\nLabeling (FSRL) subtasks, relying solely on the FrameNet database. These\nprompts, constructed from frame definitions and annotated examples, are used to\nguide six different LLMs. Experiments are conducted on a subset of frames\nrelated to violent events. The method achieves competitive results, with F1\nscores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers\na practical and effective alternative to traditional fine-tuning for\ndomain-specific FSP tasks.", "AI": {"tldr": "The paper investigates the use of In-Context Learning (ICL) with large language models to perform Frame Semantic Parsing (FSP), achieving competitive results on violent event-related frames without the need for model fine-tuning.", "motivation": "The motivation of this paper is to explore a practical and effective alternative to the traditional fine-tuning process in domain-specific Frame Semantic Parsing tasks, leveraging In-Context Learning (ICL) and Large Language Models (LLMs).", "method": "The paper employs In-Context Learning (ICL) technique with Large Language Models (LLMs) to perform Frame Semantic Parsing (FSP). It proposes an automatic generation of task-specific prompts for Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) using FrameNet database without the need for fine-tuning the LLMs.", "result": "The proposed method achieves high F1 scores of 94.3% for FI and 77.4% for FSRL on a subset of violent events-related frames.", "conclusion": "The study concludes that ICL with automatically generated prompts is a viable and effective approach for FSP, especially without the need for extensive model fine-tuning, demonstrating competitive performance in the context of violent event-related frames."}}
{"id": "2507.23284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23284", "abs": "https://arxiv.org/abs/2507.23284", "authors": ["Dohwan Ko", "Ji Soo Lee", "Minhyuk Choi", "Zihang Meng", "Hyunwoo J. Kim"], "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval", "comment": "ICCV 2025 Highlight", "summary": "Text-Video Retrieval aims to find the most relevant text (or video) candidate\ngiven a video (or text) query from large-scale online databases. Recent work\nleverages multi-modal large language models (MLLMs) to improve retrieval,\nespecially for long or complex query-candidate pairs. However, we observe that\nthe naive application of MLLMs, i.e., retrieval based on candidate likelihood,\nintroduces candidate prior bias, favoring candidates with inherently higher\npriors over those more relevant to the query. To this end, we propose a novel\nretrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),\nwhich leverages both query and candidate likelihoods by training the model to\ngenerate text from a given video as well as video features from a given text.\nFurthermore, we introduce Candidate Prior Normalization (CPN), a simple yet\neffective training-free score calibration module designed to mitigate candidate\nprior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,\nour BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4\nR@1 on average, effectively alleviating candidate prior bias and emphasizing\nquery-candidate relevance. Our in-depth analysis across various multi-modal\ntasks beyond retrieval highlights the broad applicability of CPN which enhances\nvisual understanding by reducing reliance on textual priors. Code is available\nat https://github.com/mlvlab/BLiM.", "AI": {"tldr": "本文提出的一种基于多模态语言模型的新的文本-视频检索框架BLiM，通过引入候选者先验归一化CPN模块来减少候选者先验偏见，改善了检索的准确性。", "motivation": "观察到直接使用多模态语言模型进行检索会导致候选者先验偏见，即更有利于具有较高固有先验概率的候选者。希望设计一个既能在给定视频的情况下生成文本，也能在给定文本的情况下生成视频特征的模型，从而更注重查询-候选者相关性而非候选者的固有概率。", "method": "提出了一种新的检索框架BLiM，利用双向似然估计来解决基于候选者似然性直接检索时引入的候选者先验偏见问题。此外，还引入了候选者先验归一化(CPN)模块，这是一种简单而有效的训练无关得分校准模块，旨在减少候选者先验偏见。", "result": "在四个文本-视频检索基准测试上，BLiM加上CPN的性能要比之前的最先进模型平均高出6.4 R@1，有效地减轻了候选者先验偏见，并强调了查询-候选者的相关性。", "conclusion": "该研究不仅证明了在文本-视频检索任务上的有效性，也展示了CPN在减少对文本先验依赖以增强视觉理解方面广泛的适用性。"}}
