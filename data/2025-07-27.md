<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

> This paper introduces Shop-R1, a reinforcement learning framework for improving the reasoning and action prediction of LLMs to simulate human behavior more accurately in online shopping environments.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the performance limitations of previous methods in enhancing LLMs' reasoning abilities, which are bounded by the reasoning capabilities of the model used for rationale generation.

**Method:** Shop-R1, a novel reinforcement learning (RL) framework that decomposes the human behavior simulation task into reasoning and action prediction stages, each guided by distinct reward signals for better simulation of human behavior in online shopping environments.

**Result:** Experimental results show a relative improvement of over 65% compared to the baseline method.

**Conclusion:** The Shop-R1 framework demonstrates significant improvement in simulating human-like behavior in online shopping environments compared to previous methods, achieving a substantial relative improvement over the baseline.

**Abstract:** Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [2] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

> 提出了动态和泛化的过程奖励模型（DG-PRM），采用了奖励树结构和帕累托优势估计，实现了在多任务中的优异性能和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的过程奖励模型（PRMs）主要依赖于启发式方法，难以跨领域泛化。尽管利用LLM作为评判者被提出以提供泛化的奖励，但当前研究主要关注反馈结果，而忽略了文本中的有意义指导。同时，静态和粗粒度的评估标准难以适应复杂的流程监督。

**Method:** Dynamic and Generalizable Process Reward Modeling (DG-PRM)采用奖励树来捕获和存储细粒度的多维奖励标准，并动态选择奖励信号进行逐步骤的奖励评分。为了处理多方面的奖励信号，首次采用帕累托优势估计来识别有区分度的正负样本对。

**Result:** 实验结果表明，DG-PRM在主流基准上取得了卓越性能，显著提升了具有密集奖励任务的模型性能。进一步分析表明，DG-PRM能够很好地适应分布外场景，表现出色的泛化能力。

**Conclusion:** DG-PRM通过引入奖励树和帕累托优势估计，有效地解决了现有PRMs跨域泛化难、静态评估标准不适应复杂情境的问题，证明了其在多个任务上以及面对分布外场景时的泛化能力和卓越性能。

**Abstract:** Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [3] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

> VeriMinder 是一个用于检测和缓解自然语言接口数据库系统中分析偏差的交互式系统，通过三个关键创新实现了这一目标：为特定分析场景创建语义映射框架、采用难以变化原理的分析框架以及通过多候选方案、批评反馈和自我反思生成高质量的任务特定提示词。实验结果显示，该系统显著提升了分析的质量、全面性和准确性。

<details>
  <summary>Details</summary>

**Motivation:** 许多没有统计分析背景的用户使用自然语言接口数据库系统进行数据分析时，往往会忽视认知偏差所带来的挑战。因此，解决这些潜在的偏差问题被认为是至关重要的。

**Method:** 通过引入三个关键创新来解决认知偏差问题，分别是：创建一个语义映射框架以识别特定分析环境中的偏差、操作化的难以变化分析框架以系统地指导数据分析、利用优化的LLM（大规模语言模型）系统生成高质量的任务特定提示词。

**Result:** 实验测评中，82.5%的参与者报告说该系统对分析质量产生了积极的影响，在比较评估中，VeriMinder的表现明显优于其他方法，至少在20%的数量级上更好地满足了分析质量和全面性的要求。

**Conclusion:** VeriMinder系统有效辅助用户在数据分析过程中避免因提出“错误问题”导致的漏洞，并作为开源软件提供给研究社区和从业者使用。

**Abstract:** Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [4] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

> 该研究提出了一种高效的整体自动口语评估方法，适用于多部分的二语测试，并在2025 Speak & Improve挑战赛中取得了优异效果。系统通过单一的Whisper-small编码器处理所有口语回应，并使用轻量级聚合器组合信息并预测最终分数，大幅减少了推理时间，提高了ASL学习系统的实用性。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有自动口语评估系统中存在的复杂模型结构和长时间推理问题，提高ASL系统在大规模数据中的应用效率。

**Method:** 使用Whisper-small编码器处理所有口语回应，通过轻量级聚合器将信息组合，并预测最终分数，同时提出了一种新的数据抽样策略。

**Result:** 系统达到了0.384的RMSE，优于文本基线系统（0.44），同时使用不超过168M参数。在仅使用语料库中44.8%的说话者数据训练时，系统仍能达到0.383的RMSE，提高了在不平衡分类中的性能和数据效率。

**Conclusion:** 提出的系统方法展示了高的自动口语评估性能和数据效率，在ASL系统中具有良好的应用前景。

**Abstract:** We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [5] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

> 此研究评估了六种AI检测工具（AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, GPTZero）对DeepSeek生成文本的检测效果，并发现QuillBot和Copyleaks在识别原版文本和改述文本时表现优异，而其他工具在检测对抗性改述和人性化处理时表现不佳。此外，使用DeepSeek自身进行文本分类显示出高准确度。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在填补文献中关于DeepSeek这一新发布的大型语言模型的检测技术的空白，同时评估现有AI检测工具在面对DeepSeek生成文本时的性能，特别是对抗技术的影响。

**Method:** 本研究调查了六个常见的AI检测工具（AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, GPTZero）对DeepSeek生成文本的检测能力，并探索了DeepSeek自身通过few-shot prompting和chain-of-thought (CoT)推理分类AI生成文本和人类编写文本的效能。研究数据包括49个人类编写的问答对及其对应的DeepSeek-v3生成的回答，以及通过对抗技术（如改述和人性化处理）额外增加的196个样本。

**Result:** 研究结果表明，QuillBot和Copyleaks在识别DeepSeek原始生成文本和改述文本时表现几乎完美，而AI Text Classifier和GPT-2则表现不一致。对抗技术中，人性化处理最有效地降低了检测准确率，分别为Copyleaks 71%，QuillBot 58%，GPTZero 52%。采用few-shot和CoT prompting方法识别AI生成文本的效果非常好，其中五次示例的最优秀结果仅将49个样本中的一个误分类。

**Conclusion:** 研究显示，现有的AI检测工具在检测DeepSeek生成文本时效果参差不齐，尤其是对抗性改述和人性化处理技术大幅降低了检测准确性。而DeepSeek通过few-shot和CoT推理方法能有效且准确地识别AI和人类编写的文本。

**Abstract:** Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling](https://arxiv.org/abs/2507.17801)
*Yi Xin,Juncheng Yan,Qi Qin,Zhen Li,Dongyang Liu,Shicheng Li,Victor Shea-Jay Huang,Yupeng Zhou,Renrui Zhang,Le Zhuo,Tiancheng Han,Xiaoqing Sun,Siqi Luo,Mengmeng Wang,Bin Fu,Yuewen Cao,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Yu Qiao,Peng Gao*

Main category: cs.CV

> Lumina-mGPT 2.0 是一种独立的，解码器仅有的自回归模型，专用于高质量图像生成及其他任务，其性能可媲美顶级扩散模型，并具有自回归建模的灵活性和组合性。通过统一标记方案和高效解码策略，该模型在多项基准测试中表现出色，展示了其作为统一多模态生成基础模型的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法依赖预训练组件或混合架构的问题，提供一个完全从零开始训练的自回归模型，以实现无限制的架构设计和许可证自由。

**Method:** 使用统一标记方案来处理各种任务（如主题驱动生成、图像编辑、可控合成和密集预测）并在单一生成框架内无缝操作。引入了如推理时缩放和推测Jacobi采样等高效解码策略以提高质量和速度。

**Result:** 在标准文本到图像基准测试（如GenEval、DPG）中，Lumina-mGPT 2.0 不仅匹配甚至在某些情况下超越了扩散模型。在Graph200K基准测试中，原生Lumina-mGPT 2.0 表现非常出色。

**Conclusion:** Lumina-mGPT 2.0 被定位为统一多模态生成的强大灵活的基础模型，其训练细节、代码和模型已在GitHub上公开。

**Abstract:** We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model
that revisits and revitalizes the autoregressive paradigm for high-quality
image generation and beyond. Unlike existing approaches that rely on pretrained
components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from
scratch, enabling unrestricted architectural design and licensing freedom. It
achieves generation quality on par with state-of-the-art diffusion models such
as DALL-E 3 and SANA, while preserving the inherent flexibility and
compositionality of autoregressive modeling. Our unified tokenization scheme
allows the model to seamlessly handle a wide spectrum of tasks-including
subject-driven generation, image editing, controllable synthesis, and dense
prediction-within a single generative framework. To further boost usability, we
incorporate efficient decoding strategies like inference-time scaling and
speculative Jacobi sampling to improve quality and speed, respectively.
Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)
demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses
diffusion-based models. Moreover, we confirm its multi-task capabilities on the
Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally
well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation
model for unified multimodal generation. We have released our training details,
code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.

</details>


### [7] [SV3.3B: A Sports Video Understanding Model for Action Recognition](https://arxiv.org/abs/2507.17844)
*Sai Varun Kodathala,Yashwanth Reddy Vutukoori,Rakesh Vunnam*

Main category: cs.CV

> 本文提出SV3.3B，一种轻量级的视频理解模型，适用于设备端部署的体育视频分析，实现了对运动动作的细致分析，超越了更大规模的专有模型的同时，计算需求更低。

<details>
  <summary>Details</summary>

**Motivation:** 解决传统体育视频分析模型计算密集、依赖服务器且缺乏对运动员动作精细理解的问题。

**Method:** 结合新颖的时序运动差异采样与自监督学习，通过DWT-VGG16-LDA方法提取关键帧，使用V-DWT-JEPA2编码器和语义生成解码器对体育动作描述进行预训练和微调。

**Result:** 在NSVA篮球数据集的子集上评估，SV3.3B在文本生成度量和体育特定位准方面均表现出色，尤其是在信息密度、动作复杂性和测量精度方面显著提升。

**Conclusion:** SV3.3B模型在生成技术详细和分析复杂度高的体育描述方面具有卓越能力，其表现优于GPT-4o模型，并在关键指标上提升了29.2%。

**Abstract:** This paper addresses the challenge of automated sports video analysis, which
has traditionally been limited by computationally intensive models requiring
server-side processing and lacking fine-grained understanding of athletic
movements. Current approaches struggle to capture the nuanced biomechanical
transitions essential for meaningful sports analysis, often missing critical
phases like preparation, execution, and follow-through that occur within
seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B
parameter video understanding model that combines novel temporal motion
difference sampling with self-supervised learning for efficient on-device
deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction
mechanism that intelligently identifies the 16 most representative frames from
sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through
mask-denoising objectives and an LLM decoder fine-tuned for sports action
description generation. Evaluated on a subset of the NSVA basketball dataset,
SV3.3B achieves superior performance across both traditional text generation
metrics and sports-specific evaluation criteria, outperforming larger
closed-source models including GPT-4o variants while maintaining significantly
lower computational requirements. Our model demonstrates exceptional capability
in generating technically detailed and analytically rich sports descriptions,
achieving 29.2% improvement over GPT-4o in ground truth validation metrics,
with substantial improvements in information density, action complexity, and
measurement precision metrics essential for comprehensive athletic analysis.
Model Available at https://huggingface.co/sportsvision/SV3.3B.

</details>


### [8] [Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models](https://arxiv.org/abs/2507.17853)
*Lifeng Chen,Jiner Wang,Zihao Pan,Beier Zhu,Xiaofeng Yang,Chi Zhang*

Main category: cs.CV

> 为了提高文本到图像生成模型在复杂提示场景中的性能，研究人员提出了Detail++，通过将复杂提示分解为一系列简化子提示，并采用质心对齐损失来减少绑定噪声，从而生成更复杂的图像。实验表明，该方法在多个指标上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管文本到图像生成模型取得了显著进步，但在处理涉及多个具有不同属性对象的复杂提示时仍面临挑战。为了提升生成图像的准确性和复杂性，提出了Detail++框架。

**Method:** Detail++框架通过引入Progressive Detail Injection (PDI)策略来解决复杂提示生成图像的挑战。该策略将复杂的提示分解为一系列简化子提示，指导生成过程分阶段进行。首先利用自注意力机制控制图像布局，确保全局组成，然后进行精确细化。为了减少绑定噪声并增强属性一致性，使用交叉注意力机制并在测试时引入质心对齐损失。

**Result:** 在T2I-CompBench和一个新构建的风格组合基准测试中的广泛实验表明，Detail++框架在涉及多个对象和复杂风格条件的情景下显著优于现有方法。

**Conclusion:** 通过Progressive Detail Injection策略和质心对齐损失，Detail++框架能够生成包含多种对象和复杂属性的高质量图像，尤其适用于文本到图像生成任务中的复杂提示场景。

**Abstract:** Recent advances in text-to-image (T2I) generation have led to impressive
visual results. However, these models still face significant challenges when
handling complex prompt, particularly those involving multiple subjects with
distinct attributes. Inspired by the human drawing process, which first
outlines the composition and then incrementally adds details, we propose
Detail++, a training-free framework that introduces a novel Progressive Detail
Injection (PDI) strategy to address this limitation. Specifically, we decompose
a complex prompt into a sequence of simplified sub-prompts, guiding the
generation process in stages. This staged generation leverages the inherent
layout-controlling capacity of self-attention to first ensure global
composition, followed by precise refinement. To achieve accurate binding
between attributes and corresponding subjects, we exploit cross-attention
mechanisms and further introduce a Centroid Alignment Loss at test time to
reduce binding noise and enhance attribute consistency. Extensive experiments
on T2I-CompBench and a newly constructed style composition benchmark
demonstrate that Detail++ significantly outperforms existing methods,
particularly in scenarios involving multiple objects and complex stylistic
conditions.

</details>


### [9] [FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains](https://arxiv.org/abs/2507.17859)
*Muayad Abujabal,Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

>  介绍FishDet-M基准和一个零样本模型选择策略以突破鱼类检测在水下场景中的瓶颈。

<details>
  <summary>Details</summary>

**Motivation:**  寻找一种方法来解决水下成像条件下鱼类检测面临的挑战，包括数据集分散、成像条件异构以及评估协议不一致。

**Method:**  danmark

**Result:** {{"name": "Structure", "arguments": {"tldr": "本文介绍了FishDet-M，这是一个最大的统一鱼类检测基准，包含多种类型的水下场景，用于克服数据集分散、异构成像条件和评估协议不一致的问题。文章系统地评估了28种目标检测模型，提出了基于CLIP的模型选择框架，以实现零样本下对于水下场景的适应性部署。", "motivation": "准确的鱼类检测在水下图像领域对于生态监测、养殖自动化和机器人感知至关重要。但实践部署受到数据集分散、异构成像条件和评估协议不一致的限制。", "method": "本文提出FishDet-M基准，整合了多种水下环境的公开数据集并统一标注格式。系统评估了28种当代目标检测模型，涵盖YOLOv8至YOLOv12系列以及不同的检测模型架构。还引入了基于CLIP的模型选择框架。", "result": "实验结果揭示了在FishDet-M上训练的不同模型的检测表现差异，展示了准确性和效率之间的权衡。基于CLIP的零样本模型选择策略在实际应用中到达了高表现，并且无需集成计算。", "conclusion": "FishDet-M为复杂的水下场景检测建立了一个标准化和可复现的评估平台，所有数据集、预训练模型和评估工具均公开，推动水下计算机视觉和智能海洋系统的未来研究。"}}}

**Conclusion:**  FishDet-M为提升鱼类检测性能提供了一个综合解决方案，推动相关技术的发展。

**Abstract:** Accurate fish detection in underwater imagery is essential for ecological
monitoring, aquaculture automation, and robotic perception. However, practical
deployment remains limited by fragmented datasets, heterogeneous imaging
conditions, and inconsistent evaluation protocols. To address these gaps, we
present \textit{FishDet-M}, the largest unified benchmark for fish detection,
comprising 13 publicly available datasets spanning diverse aquatic environments
including marine, brackish, occluded, and aquarium scenes. All data are
harmonized using COCO-style annotations with both bounding boxes and
segmentation masks, enabling consistent and scalable cross-domain evaluation.
We systematically benchmark 28 contemporary object detection models, covering
the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.
Evaluations are conducted using standard metrics including mAP, mAP@50, and
mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and
inference profiling in terms of latency and parameter count. The results
highlight the varying detection performance across models trained on FishDet-M,
as well as the trade-off between accuracy and efficiency across models of
different architectures. To support adaptive deployment, we introduce a
CLIP-based model selection framework that leverages vision-language alignment
to dynamically identify the most semantically appropriate detector for each
input image. This zero-shot selection strategy achieves high performance
without requiring ensemble computation, offering a scalable solution for
real-time applications. FishDet-M establishes a standardized and reproducible
platform for evaluating object detection in complex aquatic scenes. All
datasets, pretrained models, and evaluation tools are publicly available to
facilitate future research in underwater computer vision and intelligent marine
systems.

</details>


### [10] [Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis](https://arxiv.org/abs/2507.17860)
*Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel*

Main category: cs.CV

> 研究使用生成模型评估公开黑色素瘤分类器的公平性，强调使用合成数据评估的潜力以及面临的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 评估和改善此类系统公平性的重要性，尤其是在边缘设备上的深度学习技术应用于皮肤癌（如黑色素瘤）的常规筛查的背景下。

**Method:** 利用最先进的生成式AI模型（LightningDiT）来评估公开可用的黑色素瘤分类器的公平性。

**Result:** 研究结果表明，使用高度真实的合成数据评估公平性是一个有前景的方向，但当评估使用的黑色素瘤检测模型训练数据与生成合成图像的数据集不同时，验证公平性变得困难。

**Conclusion:** 研究提出了使用合成数据评估和增强医学影像生成式AI系统的公平性的一个有价值的新型途径。

**Abstract:** Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.

</details>


### [11] [DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration](https://arxiv.org/abs/2507.17892)
*Hanzhou Liu,Binghan Li,Chengkai Liu,Mi Lu*

Main category: cs.CV

> 本文提出了DiNAT-IR架构，它结合了通道感知模块来提升Dilated Neighborhood Attention（DiNA）在图像复原任务中的性能，该方法在多个基准测试中实现了有竞争力的结果。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于自我关注机制在处理图像复原任务时的计算成本高昂，特别是在高分辨率图像上，作者提出改善自我关注机制的效率，同时保持高质量的图像恢复，特别是解决局部细节遗漏的问题。

**Method:** Restormer采用通道自我关注机制来处理图像复原任务。然而，由于直接使用全局-局部设计在经典的去模糊任务上会导致准确复原的困难，本文提出了一种基于通道感知模块来补充局部注意的新方法，以有效整合全局上下文信息而不牺牲像素级精度。

**Result:** DiNAT-IR架构在多个基准测试中展示了其在处理低级计算机视觉问题中的高效率和高质量，特别在图像复原任务中取得了显著效果。

**Conclusion:** 研究表明，通过引入新的通道感知模块，可以有效提升基于Transformer的图像复原模型在全局上下文和局部细节处理上的平衡，实现了图像复原任务中的高质量恢复。

**Abstract:** Transformers, with their self-attention mechanisms for modeling long-range
dependencies, have become a dominant paradigm in image restoration tasks.
However, the high computational cost of self-attention limits scalability to
high-resolution images, making efficiency-quality trade-offs a key research
focus. To address this, Restormer employs channel-wise self-attention, which
computes attention across channels instead of spatial dimensions. While
effective, this approach may overlook localized artifacts that are crucial for
high-quality image restoration. To bridge this gap, we explore Dilated
Neighborhood Attention (DiNA) as a promising alternative, inspired by its
success in high-level vision tasks. DiNA balances global context and local
precision by integrating sliding-window attention with mixed dilation factors,
effectively expanding the receptive field without excessive overhead. However,
our preliminary experiments indicate that directly applying this global-local
design to the classic deblurring task hinders accurate visual restoration,
primarily due to the constrained global context understanding within local
attention. To address this, we introduce a channel-aware module that
complements local attention, effectively integrating global context without
sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based
architecture specifically designed for image restoration, achieves competitive
results across multiple benchmarks, offering a high-quality solution for
diverse low-level computer vision problems.

</details>


### [12] [AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2507.17957)
*Md. Al-Masrur Khan,Durgakant Pushp,Lantao Liu*

Main category: cs.CV

> 论文提出了一种Adaptive Feature Refinement（AFR）模块，旨在解决无监督领域适应语义分割问题中细节和全局信息的平衡问题，通过改进高分辨率特征的精确度，提升分割性能。

<details>
  <summary>Details</summary>

**Motivation:** 针对无监督领域适应语义分割中现有方法难以平衡细节与全局信息的问题，导致复杂区域分割错误。

**Method:** AFR模块通过使用来自低分辨率日志的语义先验来增强高分辨率特征的分割准确性，同时集成高频分量以捕捉细微结构并提供关键边界信息，通过不确定性驱动的注意力机制平衡局部和全局信息。

**Result:** AFR框架在GTA V到Cityscapes的分割准确度上提高了1.05%，在Synthia到Cityscapes上提高了1.04%。

**Conclusion:** AFR模块通过轻量级设计，能够无缝集成到HRDA基础的UDA方法中，实现分割性能的最先进水平。

**Abstract:** In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is
trained on labeled source domain data (e.g., synthetic images) and adapted to
an unlabeled target domain (e.g., real-world images) without access to target
annotations. Existing UDA-SS methods often struggle to balance fine-grained
local details with global contextual information, leading to segmentation
errors in complex regions. To address this, we introduce the Adaptive Feature
Refinement (AFR) module, which enhances segmentation accuracy by refining
highresolution features using semantic priors from low-resolution logits. AFR
also integrates high-frequency components, which capture fine-grained
structures and provide crucial boundary information, improving object
delineation. Additionally, AFR adaptively balances local and global information
through uncertaintydriven attention, reducing misclassifications. Its
lightweight design allows seamless integration into HRDA-based UDA methods,
leading to state-of-the-art segmentation performance. Our approach improves
existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on
Synthia-->Cityscapes. The implementation of our framework is available at:
https://github.com/Masrur02/AFRDA

</details>


### [13] [OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments](https://arxiv.org/abs/2507.17959)
*Ali Abedi,Sadaf Safa,Tracey J. F. Colella,Shehroz S. Khan*

Main category: cs.CV

> 论文介绍了OPEN数据集，这是针对老年人在虚拟学习中参与度识别的AI驱动数据集，包含超过35小时的数据。使用机器学习和深度学习模型进行的测试显示出高达81%的参与识别准确率。

<details>
  <summary>Details</summary>

**Motivation:** 由于准确测量虚拟群体中的参与度仍然是一个挑战，特别是对于老年人在虚拟和远程医疗学习环境中的参与度研究和数据集相对较少。现有方法往往忽视了参与度在不同会话中的情境相关性和纵向特性。

**Method:** 数据集来自11名老年人每周参加为期6周的虚拟小组学习会议，产生的数据包括面部、手部和身体关节标志点以及从视频中提取的情感和行为特征。提供了5秒、10秒、30秒和可变长度样本的版本。

**Result:** 该论文引入了OPEN（老年人参与）数据集，旨在支持AI驱动的参与识别。该数据集来自11名老年人每周参加为期6周的虚拟小组学习会议，产生超过35小时的数据，是该领域最大的数据集。数据包括面部、手部和身体关节标志点以及从视频中提取的情感和行为特征。注释包括二元参与状态、情感和行为标签以及情境类型指标。论文还展示了使用机器学习和深度学习模型进行参与识别的准确性，最高可达81%。该数据集为老年人口的个性化参与建模提供了可扩展的基础。

**Conclusion:** 该数据集为AI驱动的老年人参与度识别提供了基础，展示了其在个性化参与建模方面的应用潜力，并将对更广泛的研究领域作出贡献。

**Abstract:** Engagement in virtual learning is essential for participant satisfaction,
performance, and adherence, particularly in online education and virtual
rehabilitation, where interactive communication plays a key role. Yet,
accurately measuring engagement in virtual group settings remains a challenge.
There is increasing interest in using artificial intelligence (AI) for
large-scale, real-world, automated engagement recognition. While engagement has
been widely studied in younger academic populations, research and datasets
focused on older adults in virtual and telehealth learning settings remain
limited. Existing methods often neglect contextual relevance and the
longitudinal nature of engagement across sessions. This paper introduces OPEN
(Older adult Patient ENgagement), a novel dataset supporting AI-driven
engagement recognition. It was collected from eleven older adults participating
in weekly virtual group learning sessions over six weeks as part of cardiac
rehabilitation, producing over 35 hours of data, making it the largest dataset
of its kind. To protect privacy, raw video is withheld; instead, the released
data include facial, hand, and body joint landmarks, along with affective and
behavioral features extracted from video. Annotations include binary engagement
states, affective and behavioral labels, and context-type indicators, such as
whether the instructor addressed the group or an individual. The dataset offers
versions with 5-, 10-, 30-second, and variable-length samples. To demonstrate
utility, multiple machine learning and deep learning models were trained,
achieving engagement recognition accuracy of up to 81 percent. OPEN provides a
scalable foundation for personalized engagement modeling in aging populations
and contributes to broader engagement recognition research.

</details>
