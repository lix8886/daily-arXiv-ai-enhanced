<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [cs.CV](#cs.CV) [Total: 21]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)
*Gang Cheng,Haibo Jin,Wenbin Zhang,Haohan Wang,Jun Zhuang*

Main category: cs.CL

> 引入风险隐蔽攻击（RCA）来评估金融LLMs的监管风险，FIN-Bench基准测试显示了对多个主流LLM的高度成功率，强调了金融领域中更强的审查机制需求。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）越来越多地被集成到金融应用中，而现有的对抗性测试研究主要集中在有害内容上，大多数忽略了监管风险。我们旨在通过对抗性测试方法来研究金融LLMs的脆弱性。

**Method:** 引入了风险隐蔽攻击（RCA），这是一种多轮框架，旨在逐步隐藏监管风险，诱使LLM产生看似合规但实际上违规的响应。为了实现系统性评估，构建了FIN-Bench，这是一个特定于金融领域的基准，用于评估LLM在金融环境中的安全性。

**Result:** 在FIN-Bench上的广泛实验表明，RCA有效地绕过了包括GPT-4.1和OpenAI o1在内的九个主流LLMs，平均攻击成功率（ASR）为93.18%，GPT-4.1的成功率为98.28%，OpenAI o1的成功率为97.56%。

**Conclusion:** 这些发现揭示了当前对齐技术中的关键差距，并强调了在金融领域中迫切需要更强的审查机制。我们希望这项工作能够提供实用的见解，以提高LLM的安全性和领域适应性。

**Abstract:** Large Language Models (LLMs) are increasingly integrated into financial
applications, yet existing red-teaming research primarily targets harmful
content, largely neglecting regulatory risks. In this work, we aim to
investigate the vulnerability of financial LLMs through red-teaming approaches.
We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that
iteratively conceals regulatory risks to provoke seemingly compliant yet
regulatory-violating responses from LLMs. To enable systematic evaluation, we
construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in
financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA
effectively bypasses nine mainstream LLMs, achieving an average attack success
rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1.
These findings reveal a critical gap in current alignment techniques and
underscore the urgent need for stronger moderation mechanisms in financial
domains. We hope this work offers practical insights for advancing robust and
domain-aware LLM alignment.

</details>


### [2] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)
*Iván Vicente Moreno Cencerrado,Arnau Padrés Masdemont,Anton Gonzalvez Hawthorne,David Demitri Africa,Lorenzo Pacchiardi*

Main category: cs.CL

> 研究发现，大语言模型能在读取问题后预测其答案是否正确，且这种自我评估能力在中间层形成，但对需要数学推理的问题预测能力有限。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究的动机是探讨大语言模型在生成答案之前是否能够预知其正确性，从而增进对大规模语言模型内部运作的理解。

**Method:** 通过在模型读取问题但尚未生成任何令牌之前提取激活模式，并训练线性探测器来预测模型的答案是否正确，从而研究大语言模型是否能够预知其答案的正确性。

**Result:** 实验结果表明，在不同大小的三种开源模型家族上训练的探测器能够在分布内和分布外的各种知识数据集上成功预测模型的答案正确性，优于黑盒基线和口头表达的预测置信度。

**Conclusion:** 研究结果显示，自我评估在中间层计算中形成，针对需要数学推理的问题，预测能力失败。对于回答“I don’t know”的模型，其与探测器分数密切相关，表明同一方向也捕捉了置信度，该工作通过补充先前关于真实性及其他行为的研究结果，为阐明大语言模型的内部运作提供了重要的发现。

**Abstract:** Do large language models (LLMs) anticipate when they will answer correctly?
To study this, we extract activations after a question is read but before any
tokens are generated, and train linear probes to predict whether the model's
forthcoming answer will be correct. Across three open-source model families
ranging from 7 to 70 billion parameters, projections on this "in-advance
correctness direction" trained on generic trivia questions predict success in
distribution and on diverse out-of-distribution knowledge datasets,
outperforming black-box baselines and verbalised predicted confidence.
Predictive power saturates in intermediate layers, suggesting that
self-assessment emerges mid-computation. Notably, generalisation falters on
questions requiring mathematical reasoning. Moreover, for models responding "I
don't know", doing so strongly correlates with the probe score, indicating that
the same direction also captures confidence. By complementing previous results
on truthfulness and other behaviours obtained with probes and sparse
auto-encoders, our work contributes essential findings to elucidate LLM
internals.

</details>


### [3] [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)
*Enora Rice,Katharina von der Wense,Alexis Palmer*

Main category: cs.CL

> 本文提出计算形态学的研究成果在实际语言记录领域应用有限，强调了以用户为中心的设计对于开发更有效工具的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决计算形态学和语言记录之间的脱节问题，提出如果不系统地整合用户中心设计，该领域可能会变得脱离背景并且无效。

**Method:** 本文通过一个案例研究来展示用户中心设计（UCD）原则如何重塑研究议程，案例研究的对象是GlossLM模型，这是一个最先进的多语言IGT生成模型。

**Result:** 通过一项小型用户研究，研究发现尽管模型在指标上表现良好，但它并未满足实际语言记录环境中的核心可用性需求。

**Conclusion:** 本文认为，以用户为中心不仅可以产生更有效的工具，还能揭示更丰富、更有针对性的研究方向。

**Abstract:** Computational morphology has the potential to support language documentation
through tasks like morphological segmentation and the generation of Interlinear
Glossed Text (IGT). However, our research outputs have seen limited use in
real-world language documentation settings. This position paper situates the
disconnect between computational morphology and language documentation within a
broader misalignment between research and practice in NLP and argues that the
field risks becoming decontextualized and ineffectual without systematic
integration of User-Centered Design (UCD). To demonstrate how principles from
UCD can reshape the research agenda, we present a case study of GlossLM, a
state-of-the-art multilingual IGT generation model. Through a small-scale user
study with three documentary linguists, we find that despite strong metric
based performance, the system fails to meet core usability needs in real
documentation contexts. These insights raise new research questions around
model constraints, label standardization, segmentation, and personalization. We
argue that centering users not only produces more effective tools, but surfaces
richer, more relevant research directions

</details>


### [4] [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)
*Zineddine Tighidet,Andrea Mogini,Hedi Ben-younes,Jiali Mei,Patrick Gallinari,Benjamin Piwowarski*

Main category: cs.CL

> 本文研究了在解决上下文信息与参数化信息冲突中，熵神经元在抑制变压器中上下文复制行为的作用。结果表明，熵神经元对抑制上下文复制行为是负责的，并且消除它们会导致生成过程发生显著变化。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在面临与内部参数化知识冲突的上下文信息时的行为是不一致的，而且没有普遍接受的解释来说明预期结果的分布。因此，本文的研究动机在于理解熵神经元在解决冲突信息中的作用。

**Method:** 本文通过观察这些神经元在解决上下文信息和参数化信息冲突中的作用，来初步验证这些神经元参与抑制变压器中的上下文复制行为的假设。

**Result:** 研究表明，熵神经元负责抑制各种大型语言模型中的上下文复制行为，并且消除它们会导致生成过程发生显著变化。

**Conclusion:** 这些结果加深了我们对大型语言模型在处理冲突信息时内部动态的理解。

**Abstract:** The behavior of Large Language Models (LLMs) when facing contextual
information that conflicts with their internal parametric knowledge is
inconsistent, with no generally accepted explanation for the expected outcome
distribution. Recent work has identified in autoregressive transformer models a
class of neurons -- called entropy neurons -- that produce a significant effect
on the model output entropy while having an overall moderate impact on the
ranking of the predicted tokens. In this paper, we investigate the preliminary
claim that these neurons are involved in inhibiting context copying behavior in
transformers by looking at their role in resolving conflicts between contextual
and parametric information. We show that entropy neurons are responsible for
suppressing context copying across a range of LLMs, and that ablating them
leads to a significant change in the generation process. These results enhance
our understanding of the internal dynamics of LLMs when handling conflicting
information.

</details>


### [5] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)
*Jiayou Zhong,Anudeex Shetty,Chao Jia,Xuanrui Lin,Usman Naseem*

Main category: cs.CL

> 本文针对医疗领域中的语言模型多元化问题提出了EthosAgents方法，以适应多样化的价值观和视角，并证明了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 动机来源于现有对齐方法在医疗领域中的不足，特别是在考虑个人、文化和情境因素时。

**Method:** 提出了一种名为EthosAgents的轻量级、通用的、多元化的对齐方法，旨在模拟多样的观点和价值观。

**Result:** 实验证明，该方法在七种不同规模的开放和封闭模型上提升了全体三种模式的多元化对齐。

**Conclusion:** 研究发现，医疗卫生相关多元化需要适应性强且具有规范意识的方法，并为如何在其他高权重领域更好地尊重多样性提供了见解。

**Abstract:** As large language models are increasingly deployed in sensitive domains such
as healthcare, ensuring their outputs reflect the diverse values and
perspectives held across populations is critical. However, existing alignment
approaches, including pluralistic paradigms like Modular Pluralism, often fall
short in the health domain, where personal, cultural, and situational factors
shape pluralism. Motivated by the aforementioned healthcare challenges, we
propose a first lightweight, generalizable, pluralistic alignment approach,
EthosAgents, designed to simulate diverse perspectives and values. We
empirically show that it advances the pluralistic alignment for all three modes
across seven varying-sized open and closed models. Our findings reveal that
health-related pluralism demands adaptable and normatively aware approaches,
offering insights into how these models can better respect diversity in other
high-stakes domains.

</details>


### [6] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)
*Shuaiqi Wang,Vikas Raunak,Arturs Backurs,Victor Reis,Pei Zhou,Sihao Chen,Longqi Yang,Zinan Lin,Sergey Yekhanin,Giulia Fanti*

Main category: cs.CL

> 本文提出Struct-Bench框架，用于评估从包含自然语言数据的结构化数据集中生成的合成数据集，并提供了一个基准测试平台。

<details>
  <summary>Details</summary>

**Motivation:** 现有的合成数据评估技术难以捕捉到包含自然语言数据的结构化数据集的结构特性和相关性。

**Method:** 我们提出了Struct-Bench框架，它需要用户提供一种用上下文无关语法(CFG)表示的数据集结构。该基准包含了5个实际数据集和2个人工生成的数据集，每个都带有CFG注释。

**Result:** 这些数据集对最先进的DP合成数据生成方法提供了显著的挑战，并且通过案例研究展示了如何使用Struct-Bench来提高Private Evolution生成的结构化数据合成数据质量。

**Conclusion:** Struct-Bench为研究者提供了一个标准化的评估平台，用以衡量和研究隐私保护的合成数据生成方法。

**Abstract:** Differentially private (DP) synthetic data generation is a promising
technique for utilizing private datasets that otherwise cannot be exposed for
model training or other analytics. While much research literature has focused
on generating private unstructured text and image data, in enterprise settings,
structured data (e.g., tabular) is more common, often including natural
language fields or components. Existing synthetic data evaluation techniques
(e.g., FID) struggle to capture the structural properties and correlations of
such datasets. In this work, we propose Struct-Bench, a framework and benchmark
for evaluating synthetic datasets derived from structured datasets that contain
natural language data. The Struct-Bench framework requires users to provide a
representation of their dataset structure as a Context-Free Grammar (CFG). Our
benchmark comprises 5 real-world and 2 synthetically generated datasets, each
annotated with CFGs. We show that these datasets demonstrably present a great
challenge even for state-of-the-art DP synthetic data generation methods.
Struct-Bench also includes reference implementations of different metrics and a
leaderboard, thereby providing researchers a standardized evaluation platform
to benchmark and investigate privacy-preserving synthetic data generation
methods. Further, we also present a case study showing how to use Struct-Bench
to improve the synthetic data quality of Private Evolution (PE) on structured
data. The benchmark and the leaderboard have been publicly made available at
https://struct-bench.github.io.

</details>


### [7] [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)
*Pengcheng Jiang,Siru Ouyang,Yizhu Jiao,Ming Zhong,Runchu Tian,Jiawei Han*

Main category: cs.CL

> 该论文通过调查分析了RAS增强生成方法在改进大型语言模型（LLMs）应用性能方面的应用及其面临的挑战和潜在的研究机会。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大型语言模型（LLMs）在文本生成和推理方面表现出色，但部署到实际应用时存在幻觉生成、知识过时、领域专业知识有限等挑战，该研究旨在通过检索和结构化生成增强（RAS）方法解决这些问题。

**Method:** 该论文采用调查方法，探讨了检索机制、文本结构化技术和结构化表示与大型语言模型（LLMs）的集成方法。

**Result:** 该研究深入分析了稀疏、密集和混合检索方法，探讨了文本结构化技术，如分类法构建、层级分类和信息抽取，还研究了通过提示方法、推理框架和知识嵌入技术将结构化表示与LLMs集成的方式。

**Conclusion:** 该调查不仅指出了检索效率、结构质量、知识融合等技术挑战，还提出了多模态检索、跨语言结构和交互式系统等研究机会，为研究人员和实践者提供了RAS方法、应用和未来方向的见解。

**Abstract:** Large Language Models (LLMs) have revolutionized natural language processing
with their remarkable capabilities in text generation and reasoning. However,
these models face critical challenges when deployed in real-world applications,
including hallucination generation, outdated knowledge, and limited domain
expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these
limitations by integrating dynamic information retrieval with structured
knowledge representations. This survey (1) examines retrieval mechanisms
including sparse, dense, and hybrid approaches for accessing external
knowledge; (2) explore text structuring techniques such as taxonomy
construction, hierarchical classification, and information extraction that
transform unstructured text into organized representations; and (3) investigate
how these structured representations integrate with LLMs through prompt-based
methods, reasoning frameworks, and knowledge embedding techniques. It also
identifies technical challenges in retrieval efficiency, structure quality, and
knowledge integration, while highlighting research opportunities in multimodal
retrieval, cross-lingual structures, and interactive systems. This
comprehensive overview provides researchers and practitioners with insights
into RAS methods, applications, and future directions.

</details>


### [8] [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)
*Iman Barati,Mostafa Amiri,Heshaam Faili*

Main category: cs.CL

> 论文提出了SearchInstruct，一种用于生成高质量监督微调指令数据集的新方法，解决了特定领域训练数据集创建难题，提高了某些领域内大型语言模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决为特定领域创建合适训练数据集的挑战，由于领域约束和数据稀缺，这一过程变得很困难。

**Method:** SearchInstruct, 一种专门设计用于构造高质量指令数据集的方法，用于监督微调（SFT）。此方法首先使用有限的领域特定且由人类生成的问题，然后通过大型语言模型系统地扩展这些问题，并动态检索领域相关的资源以生成准确且上下文适当的答案。

**Result:** 该研究提高了特定领域训练数据集的质量和多样性，增强了大型语言模型在这些领域的性能。另外，此方法也促进了模型编辑任务。

**Conclusion:** 实验评估表明，SearchInstruct能增强SFT数据集的多样性和质量，从而在特定领域内提高大型语言模型的性能。此外，该方法还可促进如模型编辑等任务。

**Abstract:** Supervised Fine-Tuning (SFT) is essential for training large language models
(LLMs), significantly enhancing critical capabilities such as instruction
following and in-context learning. Nevertheless, creating suitable training
datasets tailored for specific domains remains challenging due to unique domain
constraints and data scarcity. In this paper, we propose SearchInstruct, an
innovative method explicitly designed to construct high quality instruction
datasets for SFT. Our approach begins with a limited set of domain specific,
human generated questions, which are systematically expanded using a large
language model. Subsequently, domain relevant resources are dynamically
retrieved to generate accurate and contextually appropriate answers for each
augmented question. Experimental evaluation demonstrates that SearchInstruct
enhances both the diversity and quality of SFT datasets, leading to measurable
improvements in LLM performance within specialized domains. Additionally, we
show that beyond dataset generation, the proposed method can also effectively
facilitate tasks such as model editing, enabling efficient updates to existing
models. To facilitate reproducibility and community adoption, we provide full
implementation details, the complete set of generated instruction response
pairs, and the source code in a publicly accessible Git repository:
[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)

</details>


### [9] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)
*Zaur Gouliev,Jennifer Waters,Chengqian Wang*

Main category: cs.CL

> 研究分析了多语言模型（mBERT, XLM, XLM-RoBERTa, RemBERT, mT5）在跨语言虚假信息检测中的性能，创建了PolyTruth Disinfo数据集包含多语言的假真陈述对。结果显示，RemBERT在语言资源较少的情况下表现更好，而mBERT和XLM则受限于稀缺训练数据。

<details>
  <summary>Details</summary>

**Motivation:** 现有的AI模型大多只在英语环境中进行基准测试，但虚假信息传播迅速跨越了语言的界限。本论文旨在填补这一空白。

**Method:** 对五个多语言变压器模型（mBERT、XLM、XLM-RoBERTa、RemBERT 和 mT5）进行了系统的比较分析，使用包含多种语言的真假陈述对的机器学习分类任务。

**Result:** 实验结果显示了性能的差异。例如，RemBERT在总体准确性上表现更好，特别是在资源较少的语言中；而mBERT和XLM在训练数据稀缺时显示出局限性。

**Conclusion:** 讨论了这些性能模式及其对实际部署的含义，并公开提供数据集以鼓励进一步的实验和进步。发现揭示了AI系统在多语言虚假信息检测方面的潜力及其当前限制。

**Abstract:** Disinformation spreads rapidly across linguistic boundaries, yet most AI
models are still benchmarked only on English. We address this gap with a
systematic comparison of five multilingual transformer models: mBERT, XLM,
XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning
classification task. While transformer-based language models have demonstrated
notable success in detecting disinformation in English, their effectiveness in
multilingual contexts still remains up for debate. To facilitate evaluation, we
introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs
(false claim vs. factual correction) spanning over twenty five languages that
collectively cover five language families and a broad topical range from
politics, health, climate, finance, and conspiracy, half of which are
fact-checked disinformation claims verified by an augmented MindBugs Discovery
dataset. Our experiments revealed performance variations. Models such as
RemBERT achieved better overall accuracy, particularly excelling in
low-resource languages, whereas models like mBERT and XLM exhibit considerable
limitations when training data is scarce. We provide a discussion of these
performance patterns and implications for real-world deployment. The dataset is
publicly available on our GitHub repository to encourage further
experimentation and advancement. Our findings illuminate both the potential and
the current limitations of AI systems for multilingual disinformation
detection.

</details>


### [10] [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)
*Mobina Pournemat,Keivan Rezaei,Gaurang Sriramanan,Arman Zarei,Jiaxiang Fu,Yang Wang,Hamid Eghbalzadeh,Soheil Feizi*

Main category: cs.CL

> 本文评估了大型语言模型（LLMs）在面临概率推理任务时的能力，发现虽然大型模型在推理和生成样本方面表现优异，但仍存在局限性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在语言理解和生成方面取得了广泛的成功，LLMs在需要概率推理的任务中表现出不清楚且经常不一致的行为。本研究旨在全面研究LLMs在显式离散概率分布上的推理能力。

**Method:** 本文通过精心设计的任务评估大型语言模型（LLMs）在面对显式离散概率分布时的推理能力。这些任务包括模式识别、最大似然估计和样本生成，通过提示模型对联合分布或其条件分布的查询做出响应来进行评估。

**Result:** 实验证明存在小模型和大模型之间的性能差距，大型模型在推理和样本生成方面表现出较强的能力。然而，也发现其存在对概率结果表示符号变化敏感和随着上下文长度增加性能下降超过60%的局限性。

**Conclusion:** 研究结果提供了关于大型语言模型概率推理能力的深入了解，并指出了未来改进的关键方向。

**Abstract:** Despite widespread success in language understanding and generation, large
language models (LLMs) exhibit unclear and often inconsistent behavior when
faced with tasks that require probabilistic reasoning. In this work, we present
the first comprehensive study of the reasoning capabilities of LLMs over
explicit discrete probability distributions. Given observations from a
probability distribution, we evaluate models on three carefully designed tasks,
mode identification, maximum likelihood estimation, and sample generation, by
prompting them to provide responses to queries about either the joint
distribution or its conditionals. These tasks thus probe a range of
probabilistic skills, including frequency analysis, marginalization, and
generative behavior. Through comprehensive empirical evaluations, we
demonstrate that there exists a clear performance gap between smaller and
larger models, with the latter demonstrating stronger inference and surprising
capabilities in sample generation. Furthermore, our investigations reveal
notable limitations, including sensitivity to variations in the notation
utilized to represent probabilistic outcomes and performance degradation of
over 60% as context length increases. Together, our results provide a detailed
understanding of the probabilistic reasoning abilities of LLMs and identify key
directions for future improvement.

</details>


### [11] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)
*Ozan Gokdemir,Neil Getty,Robert Underwood,Sandeep Madireddy,Franck Cappello,Arvind Ramanathan,Ian T. Foster,Rick L. Stevens*

Main category: cs.CL

> 研究提出了一种新的MCQA生成框架，并通过实验验证了该框架的有效性，尤其在基于推理轨迹的检索情况下，一些小型模型的表现甚至优于较大的GPT-4模型。

<details>
  <summary>Details</summary>

**Motivation:** 随着科学知识以前所未有的速度增长，评估基准需要发展以反映新的发现，并确保语言模型能够接受当前多样化的文献测试。

**Method:** 我们提出了一种可扩展、模块化的框架，用于从大量科学论文中自动生成多选题（MCQA）基准。该框架自动化了MCQA创作的每一个阶段，包括PDF解析、语义切块、问题生成和模型评估。

**Result:** 作为案例研究，我们从22,000篇开放获取的放射性和癌症生物学文章中生成了超过16,000个多选题。我们评估了一系列小型语言模型（1.1B-14B参数）在这些问题上的表现，并将基线准确度与基于论文衍生语义切块和GPT-4.1提取的推理轨迹的检索增强生成（RAG）进行比较。

**Conclusion:** 结果发现，基于推理轨迹的检索连贯性地提高了在合成和专家标注基准上的性能，使几个小型模型在2023年天体放射和癌症生物学考试中的表现优于GPT-4。

**Abstract:** As scientific knowledge grows at an unprecedented pace, evaluation benchmarks
must evolve to reflect new discoveries and ensure language models are tested on
current, diverse literature. We propose a scalable, modular framework for
generating multiple-choice question-answering (MCQA) benchmarks directly from
large corpora of scientific papers. Our pipeline automates every stage of MCQA
creation, including PDF parsing, semantic chunking, question generation, and
model evaluation. As a case study, we generate more than 16,000 MCQs from
22,000 open-access articles in radiation and cancer biology. We then evaluate a
suite of small language models (1.1B-14B parameters) on these questions,
comparing baseline accuracy with retrieval-augmented generation (RAG) from
paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1.
We find that reasoning-trace retrieval consistently improves performance on
both synthetic and expert-annotated benchmarks, enabling several small models
to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [12] [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)
*Adarsh Srinivasan,Jacob Dineen,Muhammad Umar Afzal,Muhammad Uzair Sarfraz,Irbaz B. Riaz,Ben Zhou*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models in healthcare often miss critical emotional cues,
delivering medically sound but emotionally flat advice. This is especially
problematic in clinical contexts where patients are distressed and vulnerable,
and require empathic communication to support safety, adherence, and trust. We
present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time
framework that adds structured emotional reasoning without retraining. By
decomposing empathy into transparent appraisal-theoretic stages and exposing
per-dimension Likert signals, RECAP produces nuanced, auditable responses.
Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by
22-28% on 8B models and 10-13% on larger models over zero-shot baselines.
Clinician evaluations further confirm superior empathetic communication. RECAP
shows that modular, theory-grounded prompting can systematically enhance
emotional intelligence in medical AI while preserving the accountability
required for deployment.

</details>


### [13] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)
*Yijun Liu,Yixuan Wang,Yuzhuang Xu,Shiyu Ji,Yang Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

> 本论文提出Judge Q方法解决大语言模型KV缓存替换问题，通过软标记列表训练捕获全局信息，保持解码质量，并在实验中证明了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型的键值缓存大小随序列增长呈现线性增长，这对内存使用和解码效率造成负面影响。现有的缓存替换方法通常过度关注局部信息，可能忽略关键的全局信息。

**Method:** Judge Q 方法通过在输入序列末尾连接软标记列表来训练语言模型的嵌入层，以较低的成本调整模型，使软标记的关注图与实际解码标记的接近，从而在缓存替换时捕获全局信息，准确评估键值对的重要性。

**Result:** 实验结果表明，与现有方法相比，在相同的替换预算下，该方法在LongBench基准测试中性能下降大约1个点，而在RULER基准测试中性能改善超过3个点。

**Conclusion:** 所提出的判别性Q方法能够有效地结合全局信息来评估KV缓存中键值对的重要性，在缓存替换时维持解码质量，且可轻松集成到现有的开源模型中，同时只需少量的训练开销。

**Abstract:** Large language models (LLMs) utilize key-value (KV) cache to store historical
information during sequence processing. The size of KV cache grows linearly as
the length of the sequence extends, which seriously affects memory usage and
decoding efficiency. Current methods for KV cache eviction typically utilize
the last window from the pre-filling phase as queries to compute the KV
importance scores for eviction. Although this scheme is simple to implement, it
tends to overly focus on local information, potentially leading to the neglect
or omission of crucial global information. To mitigate this issue, we propose
Judge Q, a novel training method which incorporates a soft token list. This
method only tunes the model's embedding layer at a low training cost. By
concatenating the soft token list at the end of the input sequence, we train
these tokens' attention map to the original input sequence to align with that
of the actual decoded tokens. In this way, the queries corresponding to the
soft tokens can effectively capture global information and better evaluate the
importance of the keys and values within the KV cache, thus maintaining
decoding quality when KV cache is evicted. Under the same eviction budget, our
method exhibits less performance degradation compared to existing eviction
approaches. We validate our approach through experiments conducted on models
such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks
including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an
improvement of approximately 1 point on the LongBench and over 3 points on
RULER. This proposed methodology can be seamlessly integrated into existing
open-source models with minimal training overhead, thereby enhancing
performance in KV cache eviction scenarios.

</details>


### [14] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)
*Dominic Petrak,Thy Thy Tran,Iryna Gurevych*

Main category: cs.CL

> 本文提出了SEEED方法，通过增强Soft Nearest Neighbor Loss和引入Label-Based Sample Ranking来提高未知错误检测的准确度。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型(LLMs)在检测错误和引导响应生成模型改进方面表现出色，但它们在识别未在指令中明确指定的错误时仍存在困难。本文旨在解决这一问题，提出自动错误发现框架和SEEED方法。

**Method:** 本文介绍了自动化错误发现框架，用于检测和定义对话AI中的错误，并提出了SEEED（Soft Clustering Extended Encoder-Based Error Detection），这是一种基于编码器的方法。SEEED通过增强Soft Nearest Neighbor Loss来放大负样本的距离权重，并引入了基于标签的样本排名以选择对比度高的样本用于更好的表示学习。

**Result:** SEEED在多个标注错误的对话数据集上击败了改编的基线方法，包括GPT-4o和Phi-4，未知错误检测准确性提高了8个百分点。

**Conclusion:** SEEED方法展示了在未来检测对话AI系统中未知错误的强大潜力，证明了其在未知意图检测中具有良好的泛化能力。

**Abstract:** Although LLM-based conversational agents demonstrate strong fluency and
coherence, they still produce undesirable behaviors (errors) that are
challenging to prevent from reaching users during deployment. Recent research
leverages large language models (LLMs) to detect errors and guide
response-generation models toward improvement. However, current LLMs struggle
to identify errors not explicitly specified in their instructions, such as
those arising from updates to the response-generation model or shifts in user
behavior. In this work, we introduce Automated Error Discovery, a framework for
detecting and defining errors in conversational AI, and propose SEEED (Soft
Clustering Extended Encoder-Based Error Detection), as an encoder-based
approach to its implementation. We enhance the Soft Nearest Neighbor Loss by
amplifying distance weighting for negative samples and introduce Label-Based
Sample Ranking to select highly contrastive examples for better representation
learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --
across multiple error-annotated dialogue datasets, improving the accuracy for
detecting unknown errors by up to 8 points and demonstrating strong
generalization to unknown intent detection.

</details>


### [15] [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](https://arxiv.org/abs/2509.10843)
*Can Wang,Yiqun Chen*

Main category: cs.CL

> 研究评估了大型语言模型在回答基于证据的临床问题时的表现，发现结构化指南建议的准确性最高，获取相关信息澄清来源和检索策略能够显著提高模型表现。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型在生物医学和临床应用中取得的进步，需要对其提供基于证据答案的能力进行严格的评估。

**Method:** 我们使用GPT-4o-mini和GPT-5模型对一个从Cochrane系统综述和临床指南中汇集的多源基准测试进行了评估。该测试涵盖了美国心脏协会的结构化建议和保险公司使用的叙述指导。我们还使用了检索增强的提示方法，提供黄金来源摘要和PubMed摘要，以测试它们对模型准确性的影响。

**Result:** 研究发现，模型在结构化指南建议上的准确性最高（90%），而在文献指南和系统综述问题上的准确性较低（60-70%）。准确性与底层系统综述的引文数量呈强相关性，每次引文数量翻倍，正确答案的概率约提高30%。提供黄金来源摘要可将先前错误项目的准确性提高到0.79；提供基于语义相似度排名的前3篇PubMed摘要可提高准确性到0.23，而随机摘要则会降低准确性（0.10）。这些影响同样体现在GPT-4o-mini上，突显出来源清晰度和有针对性的检索性能的重要性。

**Conclusion:** 研究结果表明，尽管存在局限性，大型语言模型在基于证据的临床问答方面具有巨大潜力。检索增强的提示法是提高事实准确性和对源证据对齐的有效策略。然而，需要分专业的评估和问题类型以理解当前的知识获取和模型表现的具体情况。

**Abstract:** Large Language Models (LLMs) have demonstrated substantial progress in
biomedical and clinical applications, motivating rigorous evaluation of their
ability to answer nuanced, evidence-based questions. We curate a multi-source
benchmark drawing from Cochrane systematic reviews and clinical guidelines,
including structured recommendations from the American Heart Association and
narrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe
consistent performance patterns across sources and clinical domains: accuracy
is highest on structured guideline recommendations (90%) and lower on narrative
guideline and systematic review questions (60--70%). We also find a strong
correlation between accuracy and the citation count of the underlying
systematic reviews, where each doubling of citations is associated with roughly
a 30% increase in the odds of a correct answer. Models show moderate ability to
reason about evidence quality when contextual information is supplied. When we
incorporate retrieval-augmented prompting, providing the gold-source abstract
raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed
abstracts (ranked by semantic relevance) improves accuracy to 0.23, while
random abstracts reduce accuracy (0.10, within temperature variation). These
effects are mirrored in GPT-4o-mini, underscoring that source clarity and
targeted retrieval -- not just model size -- drive performance. Overall, our
results highlight both the promise and current limitations of LLMs for
evidence-based clinical question answering. Retrieval-augmented prompting
emerges as a useful strategy to improve factual accuracy and alignment with
source evidence, while stratified evaluation by specialty and question type
remains essential to understand current knowledge access and to contextualize
model performance.

</details>


### [16] [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

> Propose GAPrune, a pruning framework that considers domain importance and general linguistic foundation in large models, demonstrating better performance and domain specialization after pruning compared to baselines.

<details>
  <summary>Details</summary>

**Motivation:** State-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions.

**Method:** We propose GAPrune, a pruning framework that considers both domain importance and preserves general linguistic foundation. It uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring.

**Result:** Experiments on FinMTEB and ChemTEB show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining, GAPrune achieves +4.51% on FinMTEB and +1.73% on ChemTEB.

**Conclusion:** Experiments on two domain benchmarks show that GAPrune maintains or even enhances domain-specific capabilities after pruning, demonstrating the effectiveness of our approach for model compression and enhanced domain specialization.

**Abstract:** Domain-specific embedding models have shown promise for applications that
require specialized semantic understanding, such as coding agents and financial
retrieval systems, often achieving higher performance gains than general
models. However, state-of-the-art embedding models are typically based on LLMs,
which contain billions of parameters, making deployment challenging in
resource-constrained environments. Model compression through pruning offers a
promising solution, but existing pruning methods treat all parameters
uniformly, failing to distinguish between general semantic representations and
domain-specific patterns, leading to suboptimal pruning decisions. Thus, we
propose GAPrune, a pruning framework that addresses this challenge by
considering both domain importance and preserving general linguistic
foundation. Our method uses Fisher Information to measure importance and
general-domain gradient alignment to assess parameter behavior, then combines
these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI
scores indicate that the parameter is either less important for the domain task
or creates conflicts between domain and general objectives. Experiments on two
domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance
within 2.5% of dense models in one-shot pruning at 50% sparsity, while
outperforming all baselines. With retraining in 100 steps, GAPrune achieves
+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our
pruning strategy not only preserves but enhances domain-specific capabilities.
Our findings demonstrate that principled pruning strategies can achieve model
compression and enhanced domain specialization, providing the research
community with a new approach for development.

</details>


### [17] [Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production](https://arxiv.org/abs/2509.10845)
*Liqian Feng,Lintao Wang,Kun Hu,Dehui Kong,Zhiyong Wang*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Sign language production (SLP) aims to translate spoken language sentences
into a sequence of pose frames in a sign language, bridging the communication
gap and promoting digital inclusion for deaf and hard-of-hearing communities.
Existing methods typically rely on gloss, a symbolic representation of sign
language words or phrases that serves as an intermediate step in SLP. This
limits the flexibility and generalization of SLP, as gloss annotations are
often unavailable and language-specific. Therefore, we present a novel
diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for
gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed
to generate sign language sequences from noisy latent sign codes and spoken
text jointly, reducing the potential error accumulation through a
non-autoregressive iterative denoising process. We also design a cross-modal
signing aligner that learns a shared latent space to bridge visual and textual
content in sign and spoken languages. This alignment supports the conditioned
diffusion-based process, enabling more accurate and contextually relevant sign
language generation without gloss. Extensive experiments on the commonly used
PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method,
achieving the state-of-the-art performance.

</details>


### [18] [A funny companion: Distinct neural responses to perceived AI- versus human- generated humor](https://arxiv.org/abs/2509.10847)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

> 研究使用EEG技术对比人类和AI幽默的处理方式，发现虽然参与者对AI和人类幽默的评价相似，但AI幽默引发了更少的认知努力和更多的惊喜情绪反应。

<details>
  <summary>Details</summary>

**Motivation:** 随着AI同伴的交流能力越来越接近人类，研究人们如何认知和情感上回应AI的幽默能力变得越来越重要。

**Method:** 研究使用EEG技术比较人们对来自AI和人类的幽默的处理方式。

**Result:** 行为分析显示参与者认为AI和人类的幽默一样有趣，但神经生理学数据显示AI幽默引发了较少的认知努力和更多的惊喜情绪反应。

**Conclusion:** 研究结果揭示了大脑如何动态更新对AI能力的预测模型，以及在这种情况下情绪奖励的增强，表明大脑对AI幽默有积极且强烈的反应。

**Abstract:** As AI companions become capable of human-like communication, including
telling jokes, understanding how people cognitively and emotionally respond to
AI humor becomes increasingly important. This study used electroencephalography
(EEG) to compare how people process humor from AI versus human sources.
Behavioral analysis revealed that participants rated AI and human humor as
comparably funny. However, neurophysiological data showed that AI humor
elicited a smaller N400 effect, suggesting reduced cognitive effort during the
processing of incongruity. This was accompanied by a larger Late Positive
Potential (LPP), indicating a greater degree of surprise and emotional
response. This enhanced LPP likely stems from the violation of low initial
expectations regarding AI's comedic capabilities. Furthermore, a key temporal
dynamic emerged: human humor showed habituation effects, marked by an
increasing N400 and a decreasing LPP over time. In contrast, AI humor
demonstrated increasing processing efficiency and emotional reward, with a
decreasing N400 and an increasing LPP. This trajectory reveals how the brain
can dynamically update its predictive model of AI capabilities. This process of
cumulative reinforcement challenges "algorithm aversion" in humor, as it
demonstrates how cognitive adaptation to AI's language patterns can lead to an
intensified emotional reward. Additionally, participants' social attitudes
toward AI modulated these neural responses, with higher perceived AI
trustworthiness correlating with enhanced emotional engagement. These findings
indicate that the brain responds to AI humor with surprisingly positive and
intense reactions, highlighting humor's potential for fostering genuine
engagement in human-AI social interaction.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [19] [A Real-Time Diminished Reality Approach to Privacy in MR Collaboration](https://arxiv.org/abs/2509.10466)
*Christian Fane*

Main category: cs.CV

> 本研究设计了一种实时虚化现实系统，通过语义分割、精确对象选择以及高度优化的视频修复算法，以支持共享空间混合现实会议中的隐私保护。系统无需固定观察视角或预先进行3D扫描，实现了实时处理并展示了其在隐私保护领域的应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** 论文动机是提供一种隐私控制的方法，尤其是在共享空间的混合现实会议中，允许主要头戴设备用户选择性地移除自己环境中的个人或敏感物品，从而确保这些物品不再对其他参与者可见。

**Method:** 本论文提出了一种基于实时图像修复的虚化现实系统，该系统利用语义分割和精确对象选择来移除头戴设备用户环境中的个人或敏感物品，并使用移动ZED 2i深度摄像头从第二观察者的视角进行实时图像修复。该系统利用YOLOv11进行对象检测，并采用改进的Decoupled Spatial-Temporal Transformer (DSTT)模型进行高质量的视频修复。

**Result:** 在720p分辨率下，该系统的管道维持每秒20帧以上的帧率，证明了实时虚化现实技术在实际隐私保护的混合现实应用中的可行性。

**Conclusion:** 研究结论表明，所提出的系统是便携且稳健的，不需要固定的第二视角或环境的先验3D扫描，可以在实时混合现实会议中实现隐私控制。

**Abstract:** Diminished reality (DR) refers to the digital removal of real-world objects
by compositing background content in their place. This thesis presents a
real-time, inpainting-based DR system designed to enable privacy control in
shared-space mixed reality (MR) meetings. The system allows a primary headset
user to selectively remove personal or sensitive items from their environment,
ensuring that those objects are no longer visible to other participants.
Removal is achieved through semantic segmentation and precise object selection,
followed by real-time inpainting from the viewpoint of a secondary observer,
implemented using a mobile ZED 2i depth camera. The solution is designed to be
portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D
scanning of the environment. The system utilises YOLOv11 for object detection
and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for
high-quality video inpainting. At 720p resolution, the pipeline sustains frame
rates exceeding 20 fps, demonstrating the feasibility of real-time diminished
reality for practical privacy-preserving MR applications.

</details>


### [20] [SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning](https://arxiv.org/abs/2509.10555)
*Alejandra Perez,Chinedu Nwoye,Ramtin Raji Kermani,Omid Mohareri,Muhammad Abdullah Jamal*

Main category: cs.CV

> SurgLaVi 是一个大规模、多样化的手术视觉语言数据集，包含近24万个片段-标题对，涵盖了超过200种手术过程，含有层次结构，包括阶段、步骤和任务级别。此外，还引入了 SurgCLIP，一个基于CLIP样式的视频文本对比框架，该模型在多个识别任务上表现优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的手术视觉语言预训练数据集在规模、多样性、语义质量和层次结构方面存在局限，因此需要一个新的大规模、多样的数据集 SurgLaVi 来克服这些限制，并开发手术基础模型。

**Method:** Structure

**Result:** {
  "tldr": "SurgLaVi 是一个大规模、多样化的手术视觉语言数据集，包含近24万个片段-标题对，涵盖了超过200种手术过程，含有层次结构，包括阶段、步骤和任务级别。此外，还引入了 SurgCLIP，一个基于CLIP样式的视频文本对比框架，该模型在多个识别任务上表现优于现有方法。", 
  "motivation": "现有的手术视觉语言预训练数据集在规模、多样性、语义质量和层次结构方面存在局限，因此需要一个新的大规模、多样的数据集 SurgLaVi 来克服这些限制，并开发手术基础模型。", 
  "method": "SurgLaVi 采用全自动管道生成外科视频的详细转录，并将它们分割成连贯的过程单元，同时使用双模态过滤来筛选和去除无关和噪音样本。", 
  "result": "SurgCLIP 在阶段、步骤、动作和工具识别任务上取得了显著的性能提升，超过了现有的最先进方法。", 
  "conclusion": "大规模、语义丰富、层次结构分明的数据集直接增强了模型的表现力和通用性，SurgLaVi 数据集成为开发手术基础模型的关键资源。" 
}

**Conclusion:** 大规模、语义丰富、层次结构分明的数据集直接增强了模型的表现力和通用性，SurgLaVi 数据集成为开发手术基础模型的关键资源。

**Abstract:** Vision-language pre-training (VLP) offers unique advantages for surgery by
aligning language with surgical videos, enabling workflow understanding and
transfer across tasks without relying on expert-labeled datasets. However,
progress in surgical VLP remains constrained by the limited scale, procedural
diversity, semantic quality, and hierarchical structure of existing datasets.
In this work, we present SurgLaVi, the largest and most diverse surgical
vision-language dataset to date, comprising nearly 240k clip-caption pairs from
more than 200 procedures, and comprising hierarchical levels at phase-, step-,
and task-level. At the core of SurgLaVi lies a fully automated pipeline that
systematically generates fine-grained transcriptions of surgical videos and
segments them into coherent procedural units. To ensure high-quality
annotations, it applies dual-modality filtering to remove irrelevant and noisy
samples. Within this framework, the resulting captions are enriched with
contextual detail, producing annotations that are both semantically rich and
easy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, an
open-source derivative of 113k clip-caption pairs constructed entirely from
public data, which is over four times larger than existing surgical VLP
datasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,
a CLIP-style video-text contrastive framework with dual encoders, as a
representative base model. SurgCLIP achieves consistent improvements across
phase, step, action, and tool recognition, surpassing prior state-of-the-art
methods, often by large margins. These results validate that large-scale,
semantically rich, and hierarchically structured datasets directly translate
into stronger and more generalizable representations, establishing SurgLaVi as
a key resource for developing surgical foundation models.

</details>


### [21] [Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses](https://arxiv.org/abs/2509.10620)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

> A general, high-resolution SSL model for 3D brain MRI scans, trained using SimCLR on a large dataset, outperforms other models in diverse prediction tasks with limited labeled data.

<details>
  <summary>Details</summary>

**Motivation:** To develop a generalizable, high-resolution foundation model for analyzing 3D brain MRI scans across different populations and tasks, overcoming limitations of specialized models with limited labeled data.

**Method:** SimCLR-based self-supervised learning model for 3D brain MRI, pretrained on a large dataset of 18,759 patients and 44,958 scans.

**Result:** The fine-tuned SimCLR model outperforms Masked Autoencoders (MAE) and supervised baselines in four downstream prediction tasks, including using only 20% labeled training samples for Alzheimer's disease prediction.

**Conclusion:** The SimCLR-based model is a broadly applicable and accessible foundation for clinical analysis of 3D brain MRI, demonstrating superior performance with lower amounts of labeled data compared to existing methods.

**Abstract:** 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly
acquired in clinical settings to monitor a wide range of neurological
conditions, including neurodegenerative disorders and stroke. While deep
learning models have shown promising results analyzing 3D MRI across a number
of brain imaging tasks, most are highly tailored for specific tasks with
limited labeled data, and are not able to generalize across tasks and/or
populations. The development of self-supervised learning (SSL) has enabled the
creation of large medical foundation models that leverage diverse, unlabeled
datasets ranging from healthy to diseased data, showing significant success in
2D medical imaging applications. However, even the very few foundation models
for 3D brain MRI that have been developed remain limited in resolution, scope,
or accessibility. In this work, we present a general, high-resolution
SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on
18,759 patients (44,958 scans) from 11 publicly available datasets spanning
diverse neurological diseases. We compare our model to Masked Autoencoders
(MAE), as well as two supervised baselines, on four diverse downstream
prediction tasks in both in-distribution and out-of-distribution settings. Our
fine-tuned SimCLR model outperforms all other models across all tasks. Notably,
our model still achieves superior performance when fine-tuned using only 20% of
labeled training samples for predicting Alzheimer's disease. We use publicly
available code and data, and release our trained model at
https://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadly
applicable and accessible foundation model for clinical brain MRI analysis.

</details>


### [22] [USCTNet: A deep unfolding nuclear-norm optimization solver for physically consistent HSI reconstruction](https://arxiv.org/abs/2509.10651)
*Xiaoyang Ma,Yiyang Chai,Xinran Qu,Hong Sun*

Main category: cs.CV

> 该论文提出了一种名为USCTNet的新方法，将RGB图像转换为高光谱图像（HSI）并在标准基准测试中展示了比现有方法更好的重建精度。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于，将单个RGB图像重建为高光谱图像（HSIs）是一个不适定问题，如相机光谱灵敏度（CSS）和场景照明不准确时则会变得物理上不一致。

**Method:** 该论文提出了一种名为USCTNet的方法，该方法将RGB图像转换为高光谱图像（HSI）视为一个基于物理的逆问题，通过核范数正则化在一个可学习的变换域中解决。该方法显式估计了相机光谱灵敏度（CSS）和照明，以避开每个迭代中全奇异值分解（SVD）的高成本和不稳定性，引入了一种数据自适应的低秩子空间奇异性阈值（SVT）操作符。USCTNet结合了参数估计模块与可学习的邻近更新，形成一个针对HSI的深层展开求解器。

**Result:** 论文通过广泛实验展示了USCTNet在标准基准数据集上的一致性改进，相较于最先进的RGB方法，恢复了更好的HSI。

**Conclusion:** 实验结果表明，相较于先进的RGB方法，USCTNet在高光谱图像重建的精度上显示出一致的改进。

**Abstract:** Reconstructing hyperspectral images (HSIs) from a single RGB image is
ill-posed and can become physically inconsistent when the camera spectral
sensitivity (CSS) and scene illumination are misspecified. We formulate
RGB-to-HSI reconstruction as a physics-grounded inverse problem regularized by
a nuclear norm in a learnable transform domain, and we explicitly estimate CSS
and illumination to define the forward operator embedded in each iteration,
ensuring colorimetric consistency. To avoid the cost and instability of full
singular-value decompositions (SVDs) required by singular-value thresholding
(SVT), we introduce a data-adaptive low-rank subspace SVT operator. Building on
these components, we develop USCTNet, a deep unfolding solver tailored to HSI
that couples a parameter estimation module with learnable proximal updates.
Extensive experiments on standard benchmarks show consistent improvements over
state-of-the-art RGB-based methods in reconstruction accuracy. Code:
https://github.com/psykheXX/USCTNet-Code-Implementation.git

</details>


### [23] [A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI](https://arxiv.org/abs/2509.10683)
*Felicia Liu,Jay J. Yoo,Farzad Khalvati*

Main category: cs.CV

> 研究发现，尽管大型语言模型（LLMs）在文本处理的医疗任务中表现出色，但在医学影像任务（如胶质瘤的分类和分割）中，其性能不如传统卷积神经网络（CNNs），特别是在空间理解上存在局限性。

<details>
  <summary>Details</summary>

**Motivation:** 探讨大型语言模型在医学影像任务中的潜力，尤其是在胶质瘤分类和分割上的表现，并将其与传统的CNNs进行对比。

**Method:** 使用BraTS 2020多模态脑MRI数据集，评估未微调和微调过的LLaMA 3.2 Instruct LLM，并使用3D CNNs作为基准模型。

**Result:** 在胶质瘤分类中，CNN达到了80%的准确率；LLM达到了76%准确率，但特异性仅为18%。微调后LLM的特异性提高到55%，但整体性能下降。对于分割任务，CNN们能够较好地定位肿瘤；而LLMs的输出靠近图像中心，无显著提升。

**Conclusion:** 研究结果显示，CNN们在分类和分割任务上优于LLMs，后者在空间理解上存在局限且微调效果不佳，表明LLMs需要更严格地微调或替代训练策略才能在医疗领域获得更好的性能。

**Abstract:** Large Language Models (LLMs) have shown strong performance in text-based
healthcare tasks. However, their utility in image-based applications remains
unexplored. We investigate the effectiveness of LLMs for medical imaging tasks,
specifically glioma classification and segmentation, and compare their
performance to that of traditional convolutional neural networks (CNNs). Using
the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a
general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after
fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma
classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and
balanced precision and recall. The general LLM reached 76% accuracy but
suffered from a specificity of only 18%, often misclassifying Low-Grade tumors.
Fine-tuning improved specificity to 55%, but overall performance declined
(e.g., accuracy dropped to 72%). For segmentation, three methods - center
point, bounding box, and polygon extraction, were implemented. CNNs accurately
localized gliomas, though small tumors were sometimes missed. In contrast, LLMs
consistently clustered predictions near the image center, with no distinction
of glioma size, location, or placement. Fine-tuning improved output formatting
but failed to meaningfully enhance spatial accuracy. The bounding polygon
method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in
both tasks. LLMs showed limited spatial understanding and minimal improvement
from fine-tuning, indicating that, in their current form, they are not
well-suited for image-based tasks. More rigorous fine-tuning or alternative
training strategies may be needed for LLMs to achieve better performance,
robustness, and utility in the medical space.

</details>


### [24] [Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation](https://arxiv.org/abs/2509.10687)
*Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani*

Main category: cs.CV

> SP4D是一种框架，用于从单目输入生成配对的RGB和运动部件视频，采用双分支扩散模型，增强了部件预测的一致性，适用于动画和运动相关任务。

<details>
  <summary>Details</summary>

**Motivation:** 传统的部件分割方法依赖于基于外观的语义线索，而SP4D旨在学习生成与物体运动部件对齐且视图间一致的部件。

**Method:** SP4D 采用双分支扩散模型，同步合成RGB帧及其对应的部件分割图。通过引入空间颜色编码方案，SP4D简化了架构，并允许不同的部件数量。BiDiFuse模块增强了跨分支一致性，同时使用对比部件一致性损失以促进部件预测的空间和时间对齐。

**Result:** 实验展示了SP4D在广泛场景中的强大泛化能力，包括真实世界的视频、新生成的对象及罕见的运动姿态。

**Conclusion:** SP4D生成的2D部件图可以提升到3D以获得骨骼结构和和谐的皮肤权重，需要轻微的手动调整即可。

**Abstract:** We present Stable Part Diffusion 4D (SP4D), a framework for generating paired
RGB and kinematic part videos from monocular inputs. Unlike conventional part
segmentation methods that rely on appearance-based semantic cues, SP4D learns
to produce kinematic parts - structural components aligned with object
articulation and consistent across views and time. SP4D adopts a dual-branch
diffusion model that jointly synthesizes RGB frames and corresponding part
segmentation maps. To simplify the architecture and flexibly enable different
part counts, we introduce a spatial color encoding scheme that maps part masks
to continuous RGB-like images. This encoding allows the segmentation branch to
share the latent VAE from the RGB branch, while enabling part segmentation to
be recovered via straightforward post-processing. A Bidirectional Diffusion
Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a
contrastive part consistency loss to promote spatial and temporal alignment of
part predictions. We demonstrate that the generated 2D part maps can be lifted
to 3D to derive skeletal structures and harmonic skinning weights with few
manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,
a curated dataset of over 20K rigged objects selected and processed from
Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part
video sequences. Experiments show that SP4D generalizes strongly to diverse
scenarios, including real-world videos, novel generated objects, and rare
articulated poses, producing kinematic-aware outputs suitable for downstream
animation and motion-related tasks.

</details>


### [25] [SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition](https://arxiv.org/abs/2509.10710)
*Sven Schreiber,Noha Sarhan,Simone Frintrop,Christian Wilms*

Main category: cs.CV

> 提出了一种名为SegSLR的新ISLR系统，结合了RGB数据和姿态信息，通过具体分割保持关键细节，实验显示其超越现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前的孤立手语识别(ISLR)方法主要依赖于RGB数据或签名人姿态信息，但结合这些模态往往导致关键细节如手势形状和方向的丢失。

**Method:** 通过可提示的零样本视频分割结合RGB和姿态信息，以保持所有相关形状信息，并专注于处理与ISLR最相关的身体部位，从而有效结合RGB和姿态信息。

**Result:** 在复杂的ChaLearn249 IsoGD数据集上的评估表明，SegSLR优于最先进的方法。进一步的消融研究表明，SegSLR从专注于签名人身体和手部中受益，这证明了我们的设计选择。

**Conclusion:** SegSLR系统通过视频分割结合了RGB和姿态信息，显著提升了ISLR的性能，凸显了其在复杂背景下的优越性。

**Abstract:** Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB
data or signer pose information. However, combining these modalities often
results in the loss of crucial details, such as hand shape and orientation, due
to imprecise representations like bounding boxes. Therefore, we propose the
ISLR system SegSLR, which combines RGB and pose information through promptable
zero-shot video segmentation. Given the rough localization of the hands and the
signer's body from pose information, we segment the respective parts through
the video to maintain all relevant shape information. Subsequently, the
segmentations focus the processing of the RGB data on the most relevant body
parts for ISLR. This effectively combines RGB and pose information. Our
evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR
outperforms state-of-the-art methods. Furthermore, ablation studies indicate
that SegSLR strongly benefits from focusing on the signer's body and hands,
justifying our design choices.

</details>


### [26] [SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation](https://arxiv.org/abs/2509.10748)
*Jecia Z. Y. Mao,Francis X Creighton,Russell H Taylor,Manish Sahu*

Main category: cs.CV

> 开发了一种结合语言模型与视觉基础模型的框架，以实现手术场景中的实时分割、标注和追踪。

<details>
  <summary>Details</summary>

**Motivation:** 手术场景中的准确元素分割和追踪对于提供语境感知的术中辅助和决策至关重要，但当前解决方案依赖特定领域的标签数据，限制新场景的应用。SCOPE框架旨在解决这一问题。

**Method:** 提出了一种语音引导的协作感知框架（SCOPE），将大型语言模型（LLM）的推理能力与开集视觉基础模型（VFM）的感知能力结合，支持手术视频流中手术器械和解剖结构的实时分割、标注和追踪。

**Result:** 在公开的Cataract1k数据集和内部的离体颅底数据集上进行了框架评估，并通过建模离体实验展示了其动态能力。

**Conclusion:** 这个人类-人工智能协作范式展示了开发适应性强、免手持和医生中心的手术环境工具的潜力。

**Abstract:** Accurate segmentation and tracking of relevant elements of the surgical scene
is crucial to enable context-aware intraoperative assistance and decision
making. Current solutions remain tethered to domain-specific, supervised models
that rely on labeled data and required domain-specific data to adapt to new
surgical scenarios and beyond predefined label categories. Recent advances in
prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot
segmentation across heterogeneous medical images. However, dependence of these
models on manual visual or textual cues restricts their deployment in
introperative surgical settings. We introduce a speech-guided collaborative
perception (SCOPE) framework that integrates reasoning capabilities of large
language model (LLM) with perception capabilities of open-set VFMs to support
on-the-fly segmentation, labeling and tracking of surgical instruments and
anatomy in intraoperative video streams. A key component of this framework is a
collaborative perception agent, which generates top candidates of VFM-generated
segmentation and incorporates intuitive speech feedback from clinicians to
guide the segmentation of surgical instruments in a natural human-machine
collaboration paradigm. Afterwards, instruments themselves serve as interactive
pointers to label additional elements of the surgical scene. We evaluated our
proposed framework on a subset of publicly available Cataract1k dataset and an
in-house ex-vivo skull-base dataset to demonstrate its potential to generate
on-the-fly segmentation and tracking of surgical scene. Furthermore, we
demonstrate its dynamic capabilities through a live mock ex-vivo experiment.
This human-AI collaboration paradigm showcase the potential of developing
adaptable, hands-free, surgeon-centric tools for dynamic operating-room
environments.

</details>


### [27] [Every Camera Effect, Every Time, All at Once: 4D Gaussian Ray Tracing for Physics-based Camera Effect Data Generation](https://arxiv.org/abs/2509.10759)
*Yi-Ruei Liu,You-Zhe Xie,Yu-Hsiang Hsu,I-Sheng Fang,Yu-Lun Liu,Jun-Cheng Chen*

Main category: cs.CV

> 4D-GRT is a novel pipeline for simulating real-world camera effects in computer vision systems, offering fast and high-quality rendering compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current computer vision systems in handling real-world camera effects such as fisheye distortion and rolling shutter, due to the lack of learning from training data with these camera effects. Existing approaches face challenges in cost, sim-to-real gaps, or accuracy in modeling these effects.

**Method:** 4D Gaussian Ray Tracing (4D-GRT), a two-stage pipeline that first reconstructs dynamic scenes from multi-view videos and then applies ray tracing to generate videos with controlled, physically accurate camera effects.

**Result:** 4D-GRT achieves the fastest rendering speed while maintaining better or comparable rendering quality compared to existing methods. A benchmark with eight synthetic dynamic scenes across four camera effects is used to evaluate generated videos.

**Conclusion:** The proposed 4D-GRT method efficiently simulates real-world camera effects and provides a benchmark for evaluating generated videos with these effects.

**Abstract:** Common computer vision systems typically assume ideal pinhole cameras but
fail when facing real-world camera effects such as fisheye distortion and
rolling shutter, mainly due to the lack of learning from training data with
camera effects. Existing data generation approaches suffer from either high
costs, sim-to-real gaps or fail to accurately model camera effects. To address
this bottleneck, we propose 4D Gaussian Ray Tracing (4D-GRT), a novel two-stage
pipeline that combines 4D Gaussian Splatting with physically-based ray tracing
for camera effect simulation. Given multi-view videos, 4D-GRT first
reconstructs dynamic scenes, then applies ray tracing to generate videos with
controllable, physically accurate camera effects. 4D-GRT achieves the fastest
rendering speed while performing better or comparable rendering quality
compared to existing baselines. Additionally, we construct eight synthetic
dynamic scenes in indoor environments across four camera effects as a benchmark
to evaluate generated videos with camera effects.

</details>


### [28] [EditDuet: A Multi-Agent System for Video Non-Linear Editing](https://arxiv.org/abs/2509.10761)
*Marcelo Sandoval-Castaneda,Bryan Russell,Josef Sivic,Gregory Shakhnarovich,Fabian Caba Heilbron*

Main category: cs.CV

> 提出了一种新的视频编辑方法，利用多智能体系统和语言驱动的方式，实现了视频的自动编辑，并且在多个指标上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前视频编辑工具主要集中在检索或用户界面，而缺乏自动化编辑的核心技术。本研究旨在自动化视频编辑的核心任务，通过多智能体协作实现语言驱动的自主编辑。

**Method:** 采用多智能体方法，设计了一个编辑器智能体和一个评论家智能体。编辑器智能体接收视频片段集合和自然语言指令，并通过使用视频编辑软件中的工具来生成编辑后的序列。评论家智能体基于生成的序列给出自然语言反馈或渲染最终结果。通过基于学习的方法实现专智能体之间的有效沟通，以应对语言驱动的视频编辑任务。

**Result:** 通过用户研究对生成的视频序列进行定性和定量评价，发现在覆盖率、时间约束满足和人类偏好方面，该系统远超现有方法。

**Conclusion:** 提出了一种基于多层次智能体协作的视频自动编辑方法，通过语言驱动的方式提高了编辑质量和效率。

**Abstract:** Automated tools for video editing and assembly have applications ranging from
filmmaking and advertisement to content creation for social media. Previous
video editing work has mainly focused on either retrieval or user interfaces,
leaving actual editing to the user. In contrast, we propose to automate the
core task of video editing, formulating it as sequential decision making
process. Ours is a multi-agent approach. We design an Editor agent and a Critic
agent. The Editor takes as input a collection of video clips together with
natural language instructions and uses tools commonly found in video editing
software to produce an edited sequence. On the other hand, the Critic gives
natural language feedback to the editor based on the produced sequence or
renders it if it is satisfactory. We introduce a learning-based approach for
enabling effective communication across specialized agents to address the
language-driven video editing task. Finally, we explore an LLM-as-a-judge
metric for evaluating the quality of video editing system and compare it with
general human preference. We evaluate our system's output video sequences
qualitatively and quantitatively through a user study and find that our system
vastly outperforms existing approaches in terms of coverage, time constraint
satisfaction, and human preference.

</details>


### [29] [Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging](https://arxiv.org/abs/2509.10767)
*Sajad Amiri,Shahram Taeb,Sara Gharibi,Setareh Dehghanfard,Somayeh Sadat Mehrnia,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: cs.CV

> 通过机器学习预测非对比MRI的对比增强能提供更安全、经济、易获得的胶质瘤成像方式。研究提出了一种稳定性框架，并使用多种数据集验证了这种方法的有效性和稳定性。

<details>
  <summary>Details</summary>

**Motivation:** 钆基对比剂(GBCAs)在胶质瘤成像中至关重要，但存在安全隐患、成本问题和获取障碍。通过机器学习预测非对比MRI的对比增强是一种更安全的替代方案，因为它能反映肿瘤的侵袭性并指导治疗计划的制定。扫描仪和队列差异阻碍了模型的选择。

**Method:** 提出了一种稳定性优先的框架，用于识别可重复的机器学习管道，以多中心预测胶质瘤MRI对比增强。

**Result:** 使用了4个TCIA数据集中的1,446例胶质瘤病例(UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG)。非对比T1WI作为输入，增強信号从配对的对比后T1WI得出。使用PyRadiomics在IBSI标准下提取了108个特征，并结合了48种降维方法和25种分类器，生成了1,200个管道。旋转验证在三个数据集上进行训练并在第四个数据集上进行测试。交叉验证的预测准确度范围为0.91到0.96，外部测试的准确度分别为UCSF-PDGM的0.87，UPENN-GB的0.98，BRATS-Africa的0.95，平均准确度为0.93。F1、精度和召回在0.87到0.96之间保持稳定，而ROC-AUC差异较大，反映队列异质性。

**Conclusion:** MI与ETr管道表现出最高的稳定性和准确性，表明稳定性优先的模型选择可实现可靠的胶质瘤MRI对比增强预测，减少对GBCA的依赖，增强多中心间的泛化能力，为神经肿瘤的机器学习提供了可扩展、可复制的模板。

**Abstract:** Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but
raise safety, cost, and accessibility concerns. Predicting contrast enhancement
from non-contrast MRI using machine learning (ML) offers a safer alternative,
as enhancement reflects tumor aggressiveness and informs treatment planning.
Yet scanner and cohort variability hinder robust model selection. We propose a
stability-aware framework to identify reproducible ML pipelines for multicenter
prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases
from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG).
Non-contrast T1WI served as input, with enhancement derived from paired
post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were
extracted and combined with 48 dimensionality reduction methods and 25
classifiers, yielding 1,200 pipelines. Rotational validation was trained on
three datasets and tested on the fourth. Cross-validation prediction accuracies
ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM),
0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1,
precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more
widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr
pipeline consistently ranked highest, balancing accuracy and stability. This
framework demonstrates that stability-aware model selection enables reliable
prediction of contrast enhancement from non-contrast glioma MRI, reducing
reliance on GBCAs and improving generalizability across centers. It provides a
scalable template for reproducible ML in neuro-oncology and beyond.

</details>


### [30] [Group Evidence Matters: Tiling-based Semantic Gating for Dense Object Detection](https://arxiv.org/abs/2509.10779)
*Yilun Xiao*

Main category: cs.CV

> 论文介绍了一种用于检测无人机图像中密集小目标的后处理框架，通过合并重叠造成的冗余，提高了检测的召回率，尽管以稍微降低精确率为代价。

<details>
  <summary>Details</summary>

**Motivation:** 无人机图像中的密集小目标常因远距离视野、遮挡和杂乱而丢失。为了提高这些目标的检测率，提出了这种框架。

**Method:** 此论文提出了一种与检测器无关的后处理框架，该框架首先通过重叠切片恢复低置信度候选对象，然后使用空间门（DBSCAN应用于框质心）和语义门（DBSCAN应用于ResNet-18嵌入）来验证组证据。经过验证的组在类感知NMS融合之前会接受精确的置信度重新加权。

**Result:** 实验结果显示在VisDrone数据集上的召回率从0.685提升至0.778，尽管精确率下降到0.595，但F1值为0.669，表明该框架具有优先提高召回率的性质，符合远距离计数和监控等应用的需求。

**Conclusion:** 框架无需重新训练现有检测器，可以与现代检测器集成。未来的研究将集中于降低语义门的成本并引入时间线索。

**Abstract:** Dense small objects in UAV imagery are often missed due to long-range
viewpoints, occlusion, and clutter[cite: 5]. This paper presents a
detector-agnostic post-processing framework that converts overlap-induced
redundancy into group evidence[cite: 6]. Overlapping tiling first recovers
low-confidence candidates[cite: 7]. A Spatial Gate (DBSCAN on box centroids)
and a Semantic Gate (DBSCAN on ResNet-18 embeddings) then validates group
evidence[cite: 7]. Validated groups receive controlled confidence reweighting
before class-aware NMS fusion[cite: 8]. Experiments on VisDrone show a recall
increase from 0.685 to 0.778 (+0.093) and a precision adjustment from 0.801 to
0.595, yielding F1=0.669[cite: 9]. Post-processing latency averages 0.095 s per
image[cite: 10]. These results indicate recall-first, precision-trade-off
behavior that benefits recall-sensitive applications such as far-field counting
and monitoring[cite: 10]. Ablation confirms that tiling exposes missed objects,
spatial clustering stabilizes geometry, semantic clustering enforces appearance
coherence, and reweighting provides calibrated integration with the
baseline[cite: 11]. The framework requires no retraining and integrates with
modern detectors[cite: 12]. Future work will reduce semantic gating cost and
extend the approach with temporal cues[cite: 13].

</details>


### [31] [InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts](https://arxiv.org/abs/2509.10813)
*Weipeng Zhong,Peizhou Cao,Yichen Jin,Li Luo,Wenzhe Cai,Jingli Lin,Hanqing Wang,Zhaoyang Lyu,Tai Wang,Bo Dai,Xudong Xu,Jiangmiao Pang*

Main category: cs.CV

> 提出了InternScenes，一个大规模可模拟的室内场景数据集，通过整合多种场景源，解决了现有数据集规模和多样性不足、布局简化的缺陷，增强了场景的交互性和真实感，展示了其在场景布局生成和点到点导航上的应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** 现有3D场景数据集在数据规模、多样性、布局真实性以及对象碰撞方面存在局限性，这些局限性阻碍了Embodied AI的发展。

**Method:** 通过整合真实世界扫描、程序生成场景和设计创建的场景，创建了包含约40,000个多样化场景的InternScenes数据集，并通过对数据的全面处理来确保数据可用于模拟和增强交互性。

**Result:** 展示了InternScenes在场景布局生成和点目标导航上的应用潜力，并解决了复杂和真实布局带来的新挑战。

**Conclusion:** InternScenes为这两种任务的模型训练规模化提供了可能，并计划开源数据、模型和基准测试以惠及整个社区。

**Abstract:** The advancement of Embodied AI heavily relies on large-scale, simulatable 3D
scene datasets characterized by scene diversity and realistic layouts. However,
existing datasets typically suffer from limitations in data scale or diversity,
sanitized layouts lacking small items, and severe object collisions. To address
these shortcomings, we introduce \textbf{InternScenes}, a novel large-scale
simulatable indoor scene dataset comprising approximately 40,000 diverse scenes
by integrating three disparate scene sources, real-world scans, procedurally
generated scenes, and designer-created scenes, including 1.96M 3D objects and
covering 15 common scene types and 288 object classes. We particularly preserve
massive small items in the scenes, resulting in realistic and complex layouts
with an average of 41.5 objects per region. Our comprehensive data processing
pipeline ensures simulatability by creating real-to-sim replicas for real-world
scans, enhances interactivity by incorporating interactive objects into these
scenes, and resolves object collisions by physical simulations. We demonstrate
the value of InternScenes with two benchmark applications: scene layout
generation and point-goal navigation. Both show the new challenges posed by the
complex and realistic layouts. More importantly, InternScenes paves the way for
scaling up the model training for both tasks, making the generation and
navigation in such complex scenes possible. We commit to open-sourcing the
data, models, and benchmarks to benefit the whole community.

</details>


### [32] [Well-Conditioned Polynomial Representations for Mathematical Handwriting Recognition](https://arxiv.org/abs/2509.10815)
*Robert M. Corless,Deepak Singh Kalhan,Stephen M. Watt*

Main category: cs.CV

> 文章探讨了不同基函数与多项式度数之间的权衡关系，以减少计算成本并提高模型精准度。

<details>
  <summary>Details</summary>

**Motivation:** 之前的研究使用参数化平面曲线多项式表示数学手写内容，使用Legendre或Legendre-Sobolev分级基进行表示，给出了数字墨迹的紧凑几何表示。还需要探索不同基的选择和多项式度数之间的权衡，以在保持计算效率的同时，实现精准建模。

**Method:** 本文研究了不同基函数（如Legendre，Legendre-Sobolev，Chebyshev，Chebyshev-Sobolev）与多项式度数之间的权衡，以实现准确建模并降低计算成本。为此，文章考察了这些基函数中的多项式评估的条件数，并计算了不同内积给定的符号变化的范数。

**Result:** 文章分析了不同基函数的多项式评估条件数，并评估了不同内积给定的符号变化的范数，以寻找最优的基函数和多项式度数组合。

**Conclusion:** 基于研究结果，论文为选择合适的基和多项式度数提供了有价值的指导，这对于数学手写建模非常重要。

**Abstract:** Previous work has made use of a parameterized plane curve polynomial
representation for mathematical handwriting, with the polynomials represented
in a Legendre or Legendre-Sobolev graded basis. This provides a compact
geometric representation for the digital ink. Preliminary results have also
been shown for Chebyshev and Chebyshev-Sobolev bases. This article explores the
trade-offs between basis choice and polynomial degree to achieve accurate
modeling with a low computational cost. To do this, we consider the condition
number for polynomial evaluation in these bases and bound how the various inner
products give norms for the variations between symbols.

</details>


### [33] [Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression](https://arxiv.org/abs/2509.10824)
*Aghiles Kebaili,Romain Modzelewski,Jérôme Lapuyade-Lahorgue,Maxime Fontanilles,Sébastien Thureau,Su Ruan*

Main category: cs.CV

> 本文提出了一种多任务扩散框架，用于预测胶质瘤的像素级进展，并通过数据增强和预训练的形变模块有效处理数据稀缺的问题。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机来源于胶质瘤这种进展迅速且预后不良的恶性脑肿瘤，其准确的进展预测带来了显著的挑战，特别是在临床实践中，由于纵向MRI数据稀疏且不规则获取，进一步增加了建模的难度。

**Method:** 本文提出了一个多任务扩散框架，用于时间无关的像素级胶质瘤进展预测。该模型同时生成未来任意时间点的FLAIR序列，并使用有符号距离场（SDF）估算空间概率肿瘤演变图，允许不确定性量化。为了捕捉任意时间间隔内的肿瘤演变动态，作者整合了一个经过预训练的形变模块，该模块使用形变场来建模扫描之间的变化。针对临床数据稀缺的常见问题，作者实施了一个有针对性的数据增强管道，该管道从现有的病人研究中合成完整的三份随访扫描序列并补全缺失的MRI模式，提升了预测模型的稳定性和准确性。

**Result:** 所提出的方法在公开数据集上进行了训练，并在内部私有数据集上进行了评估，在两种情况下都取得了有希望的结果。

**Conclusion:** 基于仅有的两个早期时间点的随访扫描，该框架可以生成灵活的时间依赖的概率图，允许临床医生在任意未来的时点询问肿瘤进展风险。

**Abstract:** Glioma, an aggressive brain malignancy characterized by rapid progression and
its poor prognosis, poses significant challenges for accurate evolution
prediction. These challenges are exacerbated by sparse, irregularly acquired
longitudinal MRI data in clinical practice, where incomplete follow-up
sequences create data imbalances and make reliable modeling difficult. In this
paper, we present a multitask diffusion framework for time-agnostic, pixel-wise
prediction of glioma progression. The model simultaneously generates future
FLAIR sequences at any chosen time point and estimates spatial probabilistic
tumor evolution maps derived using signed distance fields (SDFs), allowing
uncertainty quantification. To capture temporal dynamics of tumor evolution
across arbitrary intervals, we integrate a pretrained deformation module that
models inter-scan changes using deformation fields. Regarding the common
clinical limitation of data scarcity, we implement a targeted augmentation
pipeline that synthesizes complete sequences of three follow-up scans and
imputes missing MRI modalities from available patient studies, improving the
stability and accuracy of predictive models. Based on merely two follow-up
scans at earlier timepoints, our framework produces flexible time-depending
probability maps, enabling clinicians to interrogate tumor progression risks at
any future temporal milestone. We further introduce a radiotherapy-weighted
focal loss term that leverages radiation dose maps, as these highlight regions
of greater clinical importance during model training. The proposed method was
trained on a public dataset and evaluated on an internal private dataset,
achieving promising results in both cases

</details>


### [34] [Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios](https://arxiv.org/abs/2509.10841)
*Simone Mosco,Daniel Fusaro,Wanmeng Li,Emanuele Menegatti,Alberto Pretto*

Main category: cs.CV

> 本文提出了一种点-平面投影结合几何感知数据增强的技术，用于LiDAR点云语义分割，有效利用2D表示，降低计算复杂性，提高数据稀缺场景下的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现有方法通过使用不同的点云表示或结合来自其他传感器的数据取得了良好的性能，但这些方法通常计算复杂性高且需要大量训练数据，限制了在数据稀缺场景中的泛化能力。本研究旨在改进基于点的方法，通过点-平面投影有效学习特征，并进行几何感知的数据增强。

**Method:** 点-平面投影和几何感知数据增强技术，利用2D表示从LiDAR点云中有效学习特征，缓解类别不平衡问题。

**Result:** 实验表明，该方法在数据有限的场景下表现显著提升，并在两个公开的标准数据集SemanticKITTI和PandaSet上取得了有竞争力的结果。

**Conclusion:** 该方法通过点-平面投影技术和几何感知数据增强，改善了LiDAR点云语义分割的效果，尤其在数据有限的情况下表现突出，同时在标准数据集上也表现良好。

**Abstract:** LiDAR point cloud semantic segmentation is essential for interpreting 3D
environments in applications such as autonomous driving and robotics. Recent
methods achieve strong performance by exploiting different point cloud
representations or incorporating data from other sensors, such as cameras or
external datasets. However, these approaches often suffer from high
computational complexity and require large amounts of training data, limiting
their generalization in data-scarce scenarios. In this paper, we improve the
performance of point-based methods by effectively learning features from 2D
representations through point-plane projections, enabling the extraction of
complementary information while relying solely on LiDAR data. Additionally, we
introduce a geometry-aware technique for data augmentation that aligns with
LiDAR sensor properties and mitigates class imbalance. We implemented and
evaluated our method that applies point-plane projections onto multiple
informative 2D representations of the point cloud. Experiments demonstrate that
this approach leads to significant improvements in limited-data scenarios,
while also achieving competitive results on two publicly available standard
datasets, as SemanticKITTI and PandaSet. The code of our method is available at
https://github.com/SiMoM0/3PNet

</details>


### [35] [OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds](https://arxiv.org/abs/2509.10842)
*Chongyu Wang,Kunlei Jing,Jihua Zhu,Di Wang*

Main category: cs.CV

> 介绍OpenUrban3D，一种针对大规模城市场景的三维开放词汇语义分割框架，它可实现对任意文本查询的零样本分割，实验表明其在分割准确性和跨场景泛化性上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 提出OpenUrban3D，一个无需对齐的多视角图像、预训练的点云分割网络或手动注释，适用于大规模城市场景的三维开放词汇语义分割框架。

**Method:** 通过多视角、多粒度渲染、掩码级别的视觉语言特征提取以及样本平衡融合，直接从原始点云生成鲁棒的语义特征，这些特征随后被蒸馏到三维基础模型中，以实现对任意文本查询的零样本分割，同时捕捉语义丰富性和几何先验。

**Result:** 在SensatUrban和SUM等大规模城市基准上的实验表明，OpenUrban3D在分割准确性和跨场景泛化性上显著优于现有方法。

**Conclusion:** 展示了OpenUrban3D作为一个灵活和可扩展的解决方案，在三维城市场景理解方面的潜力。

**Abstract:** Open-vocabulary semantic segmentation enables models to recognize and segment
objects from arbitrary natural language descriptions, offering the flexibility
to handle novel, fine-grained, or functionally defined categories beyond fixed
label sets. While this capability is crucial for large-scale urban point clouds
that support applications such as digital twins, smart city management, and
urban analytics, it remains largely unexplored in this domain. The main
obstacles are the frequent absence of high-quality, well-aligned multi-view
imagery in large-scale urban point cloud datasets and the poor generalization
of existing three-dimensional (3D) segmentation pipelines across diverse urban
environments with substantial variation in geometry, scale, and appearance. To
address these challenges, we present OpenUrban3D, the first 3D open-vocabulary
semantic segmentation framework for large-scale urban scenes that operates
without aligned multi-view images, pre-trained point cloud segmentation
networks, or manual annotations. Our approach generates robust semantic
features directly from raw point clouds through multi-view, multi-granularity
rendering, mask-level vision-language feature extraction, and sample-balanced
fusion, followed by distillation into a 3D backbone model. This design enables
zero-shot segmentation for arbitrary text queries while capturing both semantic
richness and geometric priors. Extensive experiments on large-scale urban
benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves
significant improvements in both segmentation accuracy and cross-scene
generalization over existing methods, demonstrating its potential as a flexible
and scalable solution for 3D urban scene understanding.

</details>


### [36] [AutoOEP -- A Multi-modal Framework for Online Exam Proctoring](https://arxiv.org/abs/2509.10887)
*Aryan Kashyap Naveen,Bhuvanesh Singla,Raajan Wankhade,Shreesha M,Ramu S,Ram Mohana Reddy Guddeti*

Main category: cs.CV

> This paper presents AutoOEP, an automated proctoring system designed to enhance online exam integrity, achieving high accuracy and efficiency in detecting suspicious activities and prohibited items.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenges in ensuring academic integrity in online education, where traditional human proctoring is not scalable, and existing automated solutions are either intrusive or inadequate in detecting various forms of cheating.

**Method:** This paper introduces AutoOEP, an automated multi-modal proctoring framework that combines computer vision and machine learning techniques. It employs a dual-camera setup for comprehensive surveillance, integrating several modules such as Face Module for identity verification and suspicious behavior detection, and Hand Module for detecting prohibited items and tracking hand proximity. The system further employs an LSTM network to analyze temporal patterns and compute a real-time cheating probability score.

**Result:** AutoOEP demonstrates an accuracy of 90.7% in detecting suspicious activities and achieves a mean Average Precision of 0.57 for prohibited item detection. The system processes video at 2.4 frames per second without GPU support, highlighting its efficiency and feasibility in real-world applications.

**Conclusion:** The conclusion of the paper is that AutoOEP stands out as an efficient, resource-efficient, and effective solution for automated proctoring, significantly reducing the requirement for human proctoring and ensuring academic honesty in online exams.

**Abstract:** The burgeoning of online education has created an urgent need for robust and
scalable systems to ensure academic integrity during remote examinations.
Traditional human proctoring is often not feasible at scale, while existing
automated solutions can be intrusive or fail to detect a wide range of cheating
behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a
comprehensive, multi-modal framework that leverages computer vision and machine
learning to provide effective, automated proctoring. The system utilizes a
dual-camera setup to capture both a frontal view of the examinee and a side
view of the workspace, minimizing blind spots. Our approach integrates several
parallel analyses: the Face Module performs continuous identity verification
using ArcFace, along with head pose estimation, gaze tracking, and mouth
movement analysis to detect suspicious cues. Concurrently, the Hand Module
employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile
phones, notes) and tracks hand proximity to these objects. Features from these
modules are aggregated and fed into a Long Short-Term Memory (LSTM) network
that analyzes temporal patterns to calculate a real-time cheating probability
score. We evaluate AutoOEP on a custom-collected dataset simulating diverse
exam conditions. Our system achieves an accuracy of 90.7% in classifying
suspicious activities. The object detection component obtains a mean Average
Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework
processes video streams at approximately 2.4 frames per second without a GPU.
The results demonstrate that AutoOEP is an effective and resource-efficient
solution for automated proctoring, significantly reducing the need for human
intervention and enhancing the integrity of online assessments.

</details>


### [37] [Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System](https://arxiv.org/abs/2509.10897)
*Weiqiang Zhao,Tianzhu Liu,Yuzhe Gui,Yanfeng Gu*

Main category: cs.CV

> 本研究提出了一种结合全变差（TV）子梯度理论的双摄像头CASSI重建框架，改进了传统的模型依赖手工图像先验的方法和深度学习方法的黑盒特性。通过动态正则化策略和自适应参考生成机制，该框架提供了严格的数学基础和优化保证，提高了图像的空间-光谱结构连贯性。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在解决传统基于模型的方法和深度学习方法中存在的光谱、空间和时间分辨率之间的平衡问题。这些问题源于压缩感知理论带来的冗余表达问题以及深度学习方法的黑盒属性和物理可解释性的缺乏。

**Method:** 本研究提出了一种结合全变差（TV）子梯度理论的双摄像头编码孔径快照光谱成像（CASSI）重建框架。通过建立端到端SD-CASSI数学模型，该框架降低了求解逆问题的计算复杂性，并为多摄像头系统的分析提供了坚实的数学基础。此外，还引入了一种动态正则化策略，该策略采用从RGB/全色导出的参考图像中的归一化梯度约束，构造成一个具有严格凸优化保证的TV子梯度相似函数。依据辅助相机提供的空间先验，设计了一个自适应参考生成和更新机制，提供了子梯度指导。

**Result:** 实验结果表明，该方法能够有效保存空间-光谱结构的连贯性，并且该理论框架为计算光谱成像提供了一个可解释的数学基础，在多种重建场景中展示了强大的性能。

**Conclusion:** 研究展示了通过引入双摄像头系统和动态正则化策略来改进CASSI重建的效果。最终，该研究工作不仅提高了成像质量，还为计算光谱成像提供了一个坚实的数学分析框架。

**Abstract:** Spectral imaging technology has long-faced fundamental challenges in
balancing spectral, spatial, and temporal resolutions. While compressive
sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this
trade-off through optical encoding, high compression ratios result in ill-posed
reconstruction problems. Traditional model-based methods exhibit limited
performance due to reliance on handcrafted inherent image priors, while deep
learning approaches are constrained by their black-box nature, which
compromises physical interpretability. To address these limitations, we propose
a dual-camera CASSI reconstruction framework that integrates total variation
(TV) subgradient theory. By establishing an end-to-end SD-CASSI mathematical
model, we reduce the computational complexity of solving the inverse problem
and provide a mathematically well-founded framework for analyzing multi-camera
systems. A dynamic regularization strategy is introduced, incorporating
normalized gradient constraints from RGB/panchromatic-derived reference images,
which constructs a TV subgradient similarity function with strict convex
optimization guarantees. Leveraging spatial priors from auxiliary cameras, an
adaptive reference generation and updating mechanism is designed to provide
subgradient guidance. Experimental results demonstrate that the proposed method
effectively preserves spatial-spectral structural consistency. The theoretical
framework establishes an interpretable mathematical foundation for
computational spectral imaging, demonstrating robust performance across diverse
reconstruction scenarios. The source code is available at
https://github.com/bestwishes43/ADMM-TVDS.

</details>


### [38] [Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation](https://arxiv.org/abs/2509.10919)
*Mohanad Albughdadi*

Main category: cs.CV

> 本文研究了紧凑的EO模型架构，提出了一种只有2.5M参数的模型，其在多个数据集上的表现优于或可与大型模型相媲美，强调了元数据作用和小规模模型的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 随着地球观测领域的进步，大型基础模型受到了关注。然而，这些模型计算成本高，限制了它们在下游任务中的可访问性和重用性。本工作研究了紧凑架构作为向更小通用EO模型的一种实际途径。

**Method:** 提出了一种带有元数据感知混合专家掩码自动编码器(MoE-MAE)，只有2.5M参数。该模型结合了稀疏专家路由与地理时间校准，结合图像以及纬度/经度和季节/日循环编码。

**Result:** 在BigEarthNet-Landsat数据集上预训练MoE-MAE，并对其冻结编码器的嵌入使用线性探针进行评估。尽管模型规模小，它与更大模型表现相当，表明元数据感知预训练可以提高迁移和标签效率。在缺乏明确元数据的EuroSAT-Landsat数据集上，仍观察到与数亿参数模型相当的性能。

**Conclusion:** 结果表明，紧凑的、元数据感知的MoE-MAE是未来EO基础模型高效和可扩展步骤。

**Abstract:** Recent advances in Earth Observation have focused on large-scale foundation
models. However, these models are computationally expensive, limiting their
accessibility and reuse for downstream tasks. In this work, we investigate
compact architectures as a practical pathway toward smaller general-purpose EO
models. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder
(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routing
with geo-temporal conditioning, incorporating imagery alongside
latitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAE
on the BigEarthNet-Landsat dataset and evaluate embeddings from its frozen
encoder using linear probes. Despite its small size, the model competes with
much larger architectures, demonstrating that metadata-aware pretraining
improves transfer and label efficiency. To further assess generalization, we
evaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, and
still observe competitive performance compared to models with hundreds of
millions of parameters. These results suggest that compact, metadata-aware
MoE-MAEs are an efficient and scalable step toward future EO foundation models.

</details>


### [39] [Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging](https://arxiv.org/abs/2509.10961)
*Farhan Sadik,Christopher L. Newman,Stuart J. Warden,Rachel K. Surowiec*

Main category: cs.CV

> Develop a method to simulate and correct motion artifacts in HR-pQCT using machine learning, improving image quality significantly.

<details>
  <summary>Details</summary>

**Motivation:** Rigid-motion artifacts hinder in vivo assessment of bone microstructures in HR-pQCT, no motion correction methods exist due to the lack of standardized degradation models

**Method:** optimize a conventional sinogram-based method to simulate motion artifacts in HR-pQCT images, Edge-enhanced Self-attention Wasserstein Generative Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion artifacts

**Result:** achieves a mean SNR of 26.78, SSIM of 0.81, VIF of 0.76 for the source dataset, and SNR of 29.31, SSIM of 0.87, VIF of 0.81 for the target dataset

**Conclusion:** the methods represent an important initial step toward implementing deep learning-based motion correction in HR-pQCT despite not fully capturing the complexity of in vivo motion artifacts

**Abstract:** Rigid-motion artifacts, such as cortical bone streaking and trabecular
smearing, hinder in vivo assessment of bone microstructures in high-resolution
peripheral quantitative computed tomography (HR-pQCT). Despite various motion
grading techniques, no motion correction methods exist due to the lack of
standardized degradation models. We optimize a conventional sinogram-based
method to simulate motion artifacts in HR-pQCT images, creating paired datasets
of motion-corrupted images and their corresponding ground truth, which enables
seamless integration into supervised learning frameworks for motion correction.
As such, we propose an Edge-enhanced Self-attention Wasserstein Generative
Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion
artifacts in both simulated (source) and real-world (target) datasets. The
model incorporates edge-enhancing skip connections to preserve trabecular edges
and self-attention mechanisms to capture long-range dependencies, facilitating
motion correction. A visual geometry group (VGG)-based perceptual loss is used
to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean
signal-to-noise ratio (SNR) of 26.78, structural similarity index measure
(SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source
dataset, while showing improved performance on the target dataset with an SNR
of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a
simplified representation of real-world motion that may not fully capture the
complexity of in vivo motion artifacts. Nevertheless, because motion artifacts
present one of the foremost challenges to more widespread adoption of this
modality, these methods represent an important initial step toward implementing
deep learning-based motion correction in HR-pQCT.

</details>
